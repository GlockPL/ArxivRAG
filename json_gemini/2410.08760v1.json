{"title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation", "authors": ["KONSTANTIN BURLACHENKO", "PETER RICHT\u00c1RIK"], "abstract": "Federated Learning (FL) is an emerging paradigm that enables intelligent agents to collaboratively train Machine Learning (ML) models in a distributed manner, eliminating the need for sharing their local data. The recent work [51] introduces a family of Federated Newton Learn (FedNL) algorithms, marking a significant step towards applying second-order methods to FL and large-scale optimization. However, the reference FedNL prototype exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch a single experiment in a sever-grade workstation; (ii) The prototype only simulates multi-node setting; (iii) Prototype integration into resource-constrained applications is challenging. To bridge the gap between theory and practice, we present a self-contained implementation of FedNL, FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves the aforementioned issues and reduces the wall clock time by x1000. With this FedNL outperforms alternatives for training logistic regression in a single-node CVXPY [11], and in a multi-node - Apache Spark [37], Ray/Scikit-Learn [38]. Finally, we propose two practical-orientated compressors for FedNL - adaptive TopLEK and cache-aware RandSeqk, which fulfill the theory of FedNL.", "sections": [{"title": "1 INTRODUCTION", "content": "Convex optimization finds applications in science and engineering and additionally serves as a tool for tackling non- tractable global optimization and combinatorial optimization problems. Examples can be found in Bertsekas et al. [3], Boyd and Vandenberghe [5]. The second-order methods represent a category of continuous optimization techniques that go beyond utilizing gradient and function value information by incorporating details about the Hessians of the minimization objective. These methods offer advantages, such as invariance to affine changes in optimization variable coordinates and rapid local convergence. For instance, the pure Newton method exhibits a quadratic convergence rate near the solution. Existing ready-to-use systems within Mathematical Optimization and Machine Learning lack good support for large-scale distributed second-order optimization. Wytock et al. [55] attribute this issue to the following factors: (a) Distributing optimization using Newton and Quasi-Newton methods demands high bandwidth to transmit Hessian-like quantities across the communication network; (b) The memory requirement for forming and storing second-order information is substantial; (c) Practical low-level linear algebra libraries, have limited interfaces. However, these concerns are highly correlated with questions under study in Federated Learning.\nFederated Learning (FL), introduced by Kone\u010dn\u00fd et al. [32], is a specialized multidisciplinary subfield within ML. It facilitates intelligent clients to collaboratively train ML models in a distributed way, eliminating the need for centralized data collection. The FL optimization algorithms have internal mechanisms to balance memory transfers and computation. Importantly in FL, they are an integral part of the optimization algorithms. One of them is communication compression. Examples of first-order optimization algorithms that was co-designed with communication compression includes MARINA [18], COFIG/FRECON [56], EF-21 [46, 47]. These algorithms and also recently started to appear algorithms for second-order optimization methods with mechanisms for communication compression are still not spread enough,"}, {"title": "1.1 Contributions", "content": "Inspired by Safaryan et al. [51], we have explored the landscape of enhancements to make FedNL more practical. One line of possible research involves modifying optimization algorithms where step size computation explicitly avoids dependence on problem-specific constants. Notably, FedNL already exhibits this property. Next, during our experimentation with a reference implementation of FedNL, a significant challenge arose in launching numerical experiments. Launching FedNL experiments using provided prototypes took 4.8 hours for a single optimization process. Next, we noticed that the referenced prototype only simulates a distributed environment. The FedNL has a super-linear local convergence rate. With this level of theory development, the gain from further theoretical improvements might not be as substantial as those derived from a highly optimized implementation. These aspects motivated us to create a more well-developed FedNL implementation on top of the original work. We acknowledge the considerable challenges of simultaneously developing both a comprehensive theoretical framework and a proficient implementation. Therefore in the present work, we focused our efforts on enhancing its practical implementation and practical applicability of FedNL algorithm family. The practical implementation also serves other needs. First, the theoretical cost model may not be entirely accurate and needs to be adjusted (for examples of nuances in modern compute systems see Section 5.3, Appendix J.1). Second, without real implementation, scientific methods often remain confined to theory. As S. Boyd noted in [13], this is one reason why Control Theory has experienced limited adoption in the broad sense.\nSummary of our contributions:\n(1) Compute Effectiveness. We addressed the challenge of executing computationally intensive multi-node simulations for FedNL on a single workstation, achieving a remarkable \u00d71000 improvement in wall-clock time.\n(2) Self-Contained Design. We introduced a self-contained single-node and multi-node implementation of FedNL (Algorithm 1), FedNL-PP (Algorithm 3), FedNL-LS (Algorithm 2). Our design facilitates seamless integration into resource-constrained systems and eliminates the need for library dependencies management. Our solution relies only on OS interfaces. It is compatible with a number of OS, Compilers (Appendix I.1), CPUs (Appendix I.2). We provide native OS executable applications, dynamic libraries, and static libraries. In addition, we provide an easy way to generate extension modules for other programming languages (Appendix L.4).\n(3) Adaptive TopLEK Compressor. We introduced an extension of the Topk compression mechanism, termed Top-LEK. The core idea is to perform compression for Topk adaptively and to compress as much as theory allows, but not more (for details see Appendix D).\n(4) Cache-aware RandSeqK Compressor. We proposed a cache-aware version of Randk compressor, named as RandSeqk. The theory of Randk provides the need \"null space\", which we have exploited to make the algorithm cache-aware (for details see Appendix C).\n(5) Logistic Regression with FedNL Outperforms Best-Practice Solutions. In a single-node setup, our implementation practically outperforms solvers encapsulated within CVXPY [11], including the commercial MOSEK [1] for solving logistic regression. In a multi-node setup, our implementation surpasses the performance of Apache Spark MLlib [37] and Ray/Scikit-Learn [38]. Our implementation has an initialization time smaller by \u00d725\u00d750, and exhibits faster solving by factor \u00d77\u00d782 in wall clock time (see Section 9).\n(6) First Robust Practical Implementation. To the best of our knowledge, our implementation is the first robust, practical implementation for training (strongly) convex objectives Eq.1 in FL settings (see Section 9.4).\nContributions (3)-(6) are more focused toward FL. The principles from contributions (1)-(2) are valuable in scenarios when a theoretical compelling\u00b9 ML algorithm requires a strong realization. We believe some of our findings and improvements raise interesting questions for designing training algorithms because the achievement of improvements on the order of \u00d71000 (if the underlying compute and storage hardware is fixed) serves as an indicator of underlying fundamental issues. The broader impact of our work is elaborated in Appendix K."}, {"title": "1.2 The Importance of Considering Practical Aspects to Avoid Catastrophic Outcomes", "content": "First, we want to provide valuable insights from ML-related fields into why neglecting practical implementations can have disastrous consequences. In 1943, Kurt Lewin, a pioneer in social science, stated that \"There is nothing so practical as a good theory\" \u00b2, which sets one connection to how theory influences practice. It is well recognized that practical applications validate theories, and prompt theorists to refine their models. However, there are two less-known facts why there is a need to balance theory and practice discovered in communities close to Machine Learning."}, {"title": "2 BACKGROUND ON FEDNL", "content": "The optimization problem addressed by FedNL [51] has a finite sum structure:\n$\\min_{x \\in \\mathbb{R}^d} f(x) \\overset{\\text{def}}{=} \\frac{1}{n}\\sum_{i=1}^n f_i(x)$.\nIn Eq.(1), functions $f (x), f_1 (x), ..., f_n(x) \\in \\mathbb{R}^d \\rightarrow \\mathbb{R}$ satisfies to the following assumptions:\nAssumption 1.1. The f(x) is $\u00b5_f$ strongly convex. If f(x) is twice continuously differentiable it is equivalent to condition for $\\nabla^2 f (x) - \u00b5I$ be positive semi-definite $\u2200x \\in \\mathbb{R}^d$.\nAssumption 1.2. There exist Lipschitz constants $L_*, L_F, and L_\u221e$ such that $\u2200i \u2208 [n], \u2200x, y \u2208 \\mathbb{R}^d$:\n$\\|\\nabla^2 f_i (x) - \\nabla^2 f_i (y) \\|_2 \\leq L_*\\|x - y\\|$,\n$\\|\\nabla^2 f_i (x) - \\nabla^2 f_i (y) \\|_F \\leq L_F\\|x - y\\|$,\n$\\underset{j,l}{\\max} |(\\nabla^2 f_i(x) - \\nabla^2 f_i(y))_{j,l}| \\leq L_\u221e\\|x - y\\|$.\nTo the best of our knowledge, FedNL is the state-of-the-art second-order optimization method in terms of theoretical convergence guarantees in a class of algorithms striving to solve Eq.1 under Assumptions 1.1, 1.2. We will not reiterate the comparisons of FedNL with prior methods, as this has been comprehensively covered in Appendix A [51].\nFedNL supports communication compression for transferring Hessian information from clients. Its extensions include FedNL-PP, designed for partial participation among clients, and FedNL-LS, which ensures global convergence. The algorithm exhibits local superlinear convergence rates in terms of the squared Euclidean distance to the solution, independent of the Hessian's condition number. Practically, this is indistinguishable from the local quadratic rate achieved by the Newton method. Notably, executing FedNL does not require knowledge of any problem-specific constants. These factors make the FedNL algorithm family a promising choice for a range of practical applications in which Machine Learning problems fulfill Eq.1 under Assumptions 1.1,1.2."}, {"title": "3 COMPUTE AND STORAGE DEMANDS OF A SINGLE WORKER", "content": "Algorithm 1 encapsulates the essential procedures employed when multiple clients collaboratively solve the optimization problem Eq.(1). The algorithm's versatility is independent of specific computing hardware, but in realistic scenarios computing device should be fixed. In our implementation, we target modern general-purpose central processing units. This type of computing device is ubiquitous, spanning from server-grade machines to portable devices. Even when electronic components are integrated into a single System-on-Chip, the Central Processing Unit (CPU) remains a fundamental element. The architectural details at the assembly language level, have evolved sustainably over the decades, facilitating effective decoupling between Electrical Engineering and Computer Science and Engineering. In contrast, the landscape for Graphics Processing Units (GPUs) is the opposite. The Application Programming Interfaces\u00b3,\nmemory management rules, and computational organization principles evolve rapidly, even within a single vendor.\nTo evaluate our implementation of the FedNL a specific class of optimization problems must be chosen and we chose L2 regularized logistic regression. This selection is guided by the existence of experiments with this objective in the original FedNL paper, and the fact that it ensures strong convexity.\nThe logistic regression can be obtained from Eq. (1) through the following specialization:\n$f(x) = \\frac{1}{n_i} \\sum_{j=1}^{N_i} log(1+exp(-b_{ij} a_{i,j}^T x)) + \\frac{\\lambda}{2} ||x||_2^2$.\nFrom Eq.2 we can analytically compute $\\nabla f_i (x), \\nabla^2 f_i (x)$ by:\n$\\nabla f_i(x) = \\frac{-1/n_i}{exp(x^T (b_{i,1} a_{i,1})) + 1} \\cdot a_{i,1} \\\\\n+\\frac{-1/n_i}{exp(x^T (b_{i,n_i} a_{i,n_i})) + 1.} \\cdot a_{i,n_i} + \\lambda \\cdot x$"}, {"title": "4 COMPUTE ISSUES IN ORIGINAL IMPLEMENTATION", "content": "The reference implementation of FedNL was developed in Python [49] with NumPy [54] employed as the computational backbone. A single execution of the provided implementation for logistic regression utilized parameters d = 301, n = 142, derived by splitting the LIBSVM W8A dataset into chunks n\u2081 = 348, takes 19,770 seconds for TopK[K = 8d] and 17, 510 seconds for RandK[K = 8d], with r = 1000 rounds of repeating Lines 3-11 of Algorithm 1. This measurements were taken on a machine equipped with an Intel(R) Xeon(R) Gold 6246 CPU 4 with 12 physical computation cores. The CPU clock frequency was set to \u00b5 = 3.3 GHz (For preparation details see Appendix G.3).\nBack-of-the-Envelope calculation. To estimate the lower bound on the execution time of the simulation for a fixed algorithm, we need to examine the internal organization of the Intel(R) Xeon(R) Gold 6246 CPU. This organization is determined by the micro-architecture of Cascade Lake processors, which encompasses information about the components inside the CPU. The adders and multipliers are electrical circuits that perform corresponding operations at the rising edge of the clock that synchronize the operation of inside CPU. In the Cascade Lake micro-architecture, the number of Float Functional Units FPU = 3 5. These functional units, responsible for float add, subtract, and multiply have a throughput of 1 operation per clock cycle in modern CPU (See Fog [15]). Each FPU is a pipelined device. If the pipeline needs to be restarted, additional 4 clocks are incurred per operation. The computation demands of Algorithm 1 require each of n client compute hessian with O(d\u00b2 \u00b7 ni) arithmetic operations, full gradient with O(d\u00b7 ni) arithmetic operations, function values O(d \u00b7 ni) per round. Hessian compression takes O(d\u00b2), update the hessian shift O(d\u00b2) arithmetic operations. Clients' compute logic happened during r rounds requires:\n$O \\left(\\frac{(d^2 \\cdot n_i + d n_i + 2d^2) \\cdot r}{\\mu \\cdot cores \\cdot fpu} \\right) \\approx 0.26 \\text{ sec}.$\nThe master has to perform n additions of hessian with d k elements, and n additions of gradients with dimension d. The master's compute time for processing:\n$O \\left(\\frac{(d k + d)rn}{\\mu \\cdot cores \\cdot fpu} \\right) \\approx 0.0032 \\text{ sec}.$"}, {"title": "4.1 Computation Problems of Reference Implementation", "content": "After examining Python/NumPy reference implementations, we found a deeper issue. Major ML frameworks, burdened by extensive auxiliary management code, are suboptimal for complex system and algorithm creation with high- performance requirements. To tackle this, we've shifted away from general-purpose ML and FL middleware and the prevalent Python-centric design philosophy. For additional discussions on the computational problems see Appendix L.3."}, {"title": "5 STRUCTURE OF X1000 TIME IMPROVEMENT", "content": "We present more notable improvements from the FedNL simulation in one machine, focusing on the wall clock time improvement for training logistic regression Eq.(1), (2) with LIBSVM W8A dataset. For finer granularity of improvements see Appendix B. We augmented each sample in a dataset with an artificial feature equal to 1 to have an intercept term. After augmentation, W8A dataset contains d = 301 features. Number of rounds r = 1000. The dataset is reshuffled u.a.r and was split across n = 142 clients with n\u2081 = 350. The x\u00ba = 0 and regularization coefficient \u03bb = 0.001, for this problem \u03bb (\u2207\u00b2\u0192) \u2208 [0.001, 0.0058]. The overall time encompasses: (1) loading and parsing the dataset; (2) distributing the dataset and preparing runtime; (3) saving the outcome of the experiment to the disk; (4) training with Algorithm 1. The (1)-(3) takes 4.7%, and (4) 95.3% of the simulation time."}, {"title": "5.1 Naive C++ Implementation: x20", "content": "Targeting the CPU requires the FedNL ecosystem to be implemented in a formal programming language. Defining a programming language lacks universal consensus. The definition advocated by authors of compiler-based languages suggested that a language must directly translate logic into computation devices. Scripting languages lack this feature"}, {"title": "8 INTRODUCED COMPRESSORS", "content": "In our pursuit of bridging the gap between theory and practice we asked ourselves:\n\"Is any nullspace of the introduced compressors that can be exploited for practical purposes?\"\nWe observed that the Topk compressor employed in the original paper through contractive definition Safaryan et al. [51, p.11]. This absence of an unbiasedness requirement allows for the construction of \"any algorithm\" satisfying this property. We introduced TopLEK[k'] described in Appendix D. It adapts the TopK[k] compressor, transforming it into a search process for TopK[k' \u2264 k]. The selection of k' \u2208 [0, k] ensures that contractive inequality becomes a tight equality.\nNext, the Randk compressor exhibits an intriguing degree of freedom. While Randk selects a subset of coordinates of cardinality k u.a.r. from a total of d coordinates, zeroing out the rest and scaling the output to preserve unbiasedness, an alternative realization of item selection is possible. In RandSeqk schema, the group of coordinates is sequentially chosen from the start index s ~u.a.r. [d], but next k - 1 are sequential deterministic indices Zd. This RandSeqk schema is unbiased and satisfies Safaryan et al. [51, p.11] and has the same variance as Randk. But it is more appealing for practice because (a) it is cache-aware; (b) the number of invocations of the used pseudo-random generator is 1. For a more elaborate presentation of RandSeqk see Appendix C."}, {"title": "9 EXPERIMENTS", "content": "We trained L2 regularized logistic regression as defined in Eq.(2) with \u03bb = 0.001. The initial iterate x\u00ba = 0. When utilizing Randk and RandSeqk, we leveraged our implementation's ability to (optionally) reconstruct indices of sparsified information. For TopK and TopLEK, the indices are transferred as 32-bit integers. See Appendix H, G for details on the software and hardware environment. Employed steps for reliable time measurements are described in Appendix G.3.\nWe augmented implementation with Natural Compressor [28], which behaves remarkably well for FedNL. The selected datasets are example datasets used in the original FedNL paper by Safaryan et al. [51]."}, {"title": "9.1 Single-node: Baseline Vanilla FedNL Improvement", "content": "In a single-node simulation, we tackled logistic Rregression with a dimensionality of d = 301. The experiment involved a total of n = 142 simulated clients and the number of rounds r = 1000. The W8A dataset reshuffled u.a.r. and was partitioned into equal ni chunks. We utilized FedNL without Line Search, Option-B, a using option-2. Results are presented in Table 1. It can be observed that our FedNL/RandK[K=8d] implementation achieves a significant total speedup of \u00d7929.4 in a single-node setup compared to the baseline. Similarly, for FedNL/TopK[K=8d], the total speedup from our implementation is \u00d71053.9. The master aggregates the following data from all clients: (i) 2937.0 MBytes for Randk and Seqk compressors; (ii) 49 568.7 MBytes for Identical mapping C(x) x; (iii) 4 241.4 MBytes for TopK; (iv) 358.8 MBytes for TopLEK.\nWe employed a two-level schema for fi (x) aggregation in our implementation. After obtaining fi (x), the master dispatches updates to one of the 4 helper threads (configurable) in a round-robin fashion, incurring an extra O(d) memory storage per helper. Once all workers finish work, the master performs the final aggregation. For Hessian updates, the master utilizes another pool of configurable 4 (configurable) helpers responsible for decompression and atomic updates to Hk."}, {"title": "A PRACTICALLY IMPLEMENTED FEDNL EXTENSIONS", "content": "The first extension of FedNL that we implemented is FedNL-LS. We selected an algorithm in which the globalization technique remains independent of any problem-specific parameters. The pseudocode for this algorithm, presented as Algorithm 2, has been sourced from [51].\nThe FedNL family includes an extension incorporating a globalization strategy through cubic regularization FedNL-CR. However, its practical implementation requires knowledge of problem-dependent L*."}, {"title": "C RANDSEQK: A PRACTICAL CACHE-AWARE IMPROVEMENT OF RANDK", "content": "C.1 Background about RandK Compressor\nThe sparsification of the input matrix $M\\in \\mathbb{R}^{d \\times d}$ with Randk happens by selecting k items from possible $n = d(d + 1)/2$ elements from the upper triangular part of the input matrix M and scaling the result in a way to preserve unbiasedness. The distribution employed for selecting this subset is uniform across all possible subsets of cardinality k. The specific subset is selected with probability $\\binom{n}{k}^{-1}$ and the probability that a specific element will be included in the selection equals to:\n$p = \\frac{\\binom{n - 1}{k - 1}}{\\binom{n}{k}} = \\frac{\\frac{(n-1)!}{(k-1)! ((n-1) - (k-1))!}}{\\frac{n!}{k! (n - k)!}} = \\frac{\\frac{(n-1)!}{(k-1)! (n - k)!}}{\\frac{n!}{k! (n - k)!}} = \\frac{\\frac{k}{n}}{\\frac{n}{n}} = \\frac{k}{n}$.\nThis implies that specific items will be selected during sparsification can be described as indicator random variables which take 1 with probability $\\frac{k}{n}$, and 0 with probability $1 - \\frac{k}{n}$.\nRandK as a collection of Bernoulli Random Variables. The RandK(M) compressor applied for symmetric matrices from $\\mathbb{R}^{d \\times d}$ can be observed as an operator that selects elements from the upper triangular part and scales the results by scalar constant C. Let's use notation whereby with $e_{ij} \\in \\mathbb{R}^{d \\times d}$ we denoted the matrix from $\\mathbb{R}^{d \\times d}$ with the only non-zero element in positions (i, j) and (j, i) equal to 1. If $\\delta \\overset{\\text{def}}{=} \\mathbb{E} [Z_{mn}], \\forall m, n \\in [d]$ we proceed as follows:\n$\\mathbb{E} [RandK(M)] = \\mathbb{E} C \\sum_{i=1}^{d} \\sum_{j=i}^{d} e_{ij} Z_{ij} m_{ij} = C \\mathbb{E} \\sum_{i=1}^{d} \\sum_{j=i}^{d}  e_{ij} Z_{ij} m_{ij} = C \\cdot \\delta \\sum_{i=1}^{d} \\sum_{j=i}^{d} e_{ij} m_{ij} = C \\cdot \\delta \\cdot M$.\nTo ensure unbiased compression, there is only one choice for the constant $C = \\frac{1}{\\delta}$.\nC.2 Looking for a Degrees of Freedom in RandK Compressor\nThe theoretical derivation for Randk does not prescribe the joint distribution of Zij, i \u2264 j, therefore analysis of Randk is more general and can be applied to other compression algorithms. The only requirement to maintain previous mathematical analysis is that E [Zij] is constant equal to 8. This will imply automatically that w = 1/8 1 for a compressor that specifies input leavening non-zero (and scale by C) items from the upper triangular part of input matrix M specified by Zij.\nC.3 Compression Schema: Cache-Aware RandSeqK\nThe proposed modification of the Randk compressor introduces a sampling strategy that ensures a cache-aware memory access pattern during the compression of a matrix $M \\in \\mathbb{R}^{d \\times d}$ arranged densely in column-wise order. If we order all elements from the upper triangular part into the sequence E, its size becomes w = d(d + 1)/2. The sampling strategy begins by randomly selecting a start index s ~u.a.r. E. The subsequent set of indices is created deterministically:"}, {"title": "D TOPLEK: A RANDOMIZED AND ADAPTIVE IMPROVEMENT OVER TOPK", "content": "D.1 Background about TopK Compressor\nTo simplify exposition, let's initially focus on the class of contractive compressors to vectors:\n${C \\in \\mathbb{R}^d \\rightarrow \\mathbb{R}^d : \\mathbb{E} [\\|C(x) - x\\|^2] \\leq (1 - \\alpha)\\|x\\|^2, \\forall x \\in \\mathbb{R}^d, \\alpha \\in (0,1]}$.\nAn important example of a compress operator that satisfies this property is the Topk compress operator. This operator preserves the k largest (in absolute value) entries of the input, zeroing out the rest. In this case, the compression algorithm is deterministic (if breaks ties in the same way).\nThe deterministic Topk compressor satisfies condition in Eq. 6 deterministically:\n$\\|TopK(x) - x\\|^2 \\leq (1 - \\alpha)\\|x\\|^2, \\forall x \\in \\mathbb{R}^d$.\nFirstly, we observe that Topk is homogeneous in \u03b2 \u2208 R, \u03b2 \u2260 0. For \u2200x \u2208 Rd:\n$\\|TopK(x) -x\\|^2 \\leq (1 - \\alpha) \\|x\\|^2 \\Leftrightarrow \\|TopK(\\beta x) - \\beta x\\|^2 \\leq (1-\\alpha) \\|\\beta x\\|^2 \\Leftrightarrow \\|TopK(\\beta x)/\\beta - x\\|^2 \\leq (1-\\alpha) \\|x\\|^2$,\nComments. The Topk compressor returns the k largest values in absolute value. Scaling by any nonzero number does not change the result of the compress operator (if ties are broken in the same way). After returning the result, TopK(\u03b2x)/\u03b2 correctly reconstitutes the sign. Thus, we can conclude that Topk is a homogeneous function.\nD.2 Pessimism of TopK Analysis\nIf $x\\in \\mathbb{R}^d\\{0\\}^d$ and selected indices for Topk compressor is the set S(x) \u2208 2[d] then we have:\n$\\|TopK(x) - x\\|^2 = \\|x - TopK(x)\\|^2 \\leq (1 - \\alpha) \\|x\\|^2, \\forall x \\in \\mathbb{R}^d\\\\ \\sum_{j \\notin S(x)} x_j^2 \\leq (1-\\alpha) \\sum_{i=1}^{d} x_i^2 \\\\\\ \\Leftrightarrow \\alpha \\leq  \\frac{\\sum_{j \\in S(x)} x_j^2}{\\sum_{i=1}^{d} x_i^2}\\\\ a_{opt} = min_{\\mathbb{R}^{d}} =  \\frac{\\sum_{j \\in S(x)} x_j^2}{\\sum_{i=1}^{d} x_i^2}, subject to : \\| x \\|_{2} = 1$\nUsing Lagrangian for fixed sets S and S' can observe that stationary points are only attained when Vi, j : xi = xj. Because the problem is bounded there is no need to analyze behavior once x \u2192 inf in Rd. For whole Rd aopt attained minimum value only in the diagonal of the Rd.\nD.3 Compression Schema: Adaptive TopLEK Compressor"}, {"title": "O CONCLUSIONS", "content": "Our work represents a significant contribution to advancing the field of FL by addressing a crucial gap between theoretical advancements in FedNL optimization algorithm family and their practical implementation. Drawing inspiration from [51], our work is rooted in cutting-edge optimization theory, demonstrating the way of implementing theory into resource-constrained FL settings. Our work serves as a guiding beacon for researchers navigating the intricate path of translating theoretical algorithms into impactful implementations across diverse domains of Machine Learning. Our work emphasizes the multifaceted considerations involved in aiming to improve the actual wall clock time.\nAlso, our work challenges the predominant Python-centric design philosophy in Machine Learning. It underscores the significance of considering alternative languages when prioritizing computational and memory efficiency."}]}