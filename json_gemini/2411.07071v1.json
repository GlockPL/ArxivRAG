{"title": "Universal Response and Emergence of Induction in LLMs", "authors": ["Niclas Luick"], "abstract": "While induction is considered a key mechanism for in-context learning in LLMs, understanding its precise circuit decomposition beyond toy models remains elusive. Here, we study the emergence of induction behavior within LLMs by probing their response to weak single-token perturbations of the residual stream. We find that LLMs exhibit a robust, universal regime in which their response remains scale-invariant under changes in perturbation strength, thereby allowing us to quantify the build-up of token correlations throughout the model. By applying our method, we observe signatures of induction behavior within the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across all models, we find that these induction signatures gradually emerge within intermediate layers and identify the relevant model sections composing this behavior. Our results provide insights into the collective interplay of components within LLMs and serve as a benchmark for large-scale circuit analysis.", "sections": [{"title": "1 Introduction", "content": "Over the last few years, the capabilities of large language models (LLMs) have dramatically improved already reaching the level of human experts on a variety of tasks [1-3]. In comparison to this remarkable rise of LLMs, our understanding of such models has remained relatively limited.\nA promising approach to gain a better understanding of LLMs is mechanistic interpretability (MI), which aims at understanding a model's behavior through interpretable circuits of its components [4]. While in toy models such circuits were found to explain a variety of model behavior including in-context learning [5-8], understanding how such circuits can be extracted in larger models or for wider classes of behavior remains an open area of research [9, 10].\nWhat makes the composition of such circuits challenging for larger multi-layer models is a complex, non-linear interplay of multi-head attention (MHA), multi-layer perceptrons (MLPs), and skip- connections, via the residual stream [11]. This interplay is expected to give rise to superposition states of features [12], including attentional features [13], even across multiple layers [14], and thereby significantly complicates such interpretability studies. While recently, sparse autoencoders were successfully used to tackle the problem of superposition and find interpretable features as the variables for circuits in LLMs [14-17], many open questions remain [18].\nTherefore, although we have a good understanding of individual model components [15, 19\u201326], their collective downstream effect on the model behavior remains under active investigation [14, 27\u201336]. Consequently, while the field of MI is evolving rapidly and promising approaches to make progress on these questions are constantly emerging [37\u201339], a full macroscopic understanding of LLMs is still lacking."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Probing the response of LLMs", "content": "Our method to probe the response of multi-layer transformer models is summarized in Fig. 1. Our scheme relies on perturbing the residual stream directly at the input of the transformer (l = 0) and comparing the evolution of the perturbed vector $x'^{(l)} \\in \\mathbb{R}^{T\\times D}$ to the unperturbed state $x^{(l)} \\in \\mathbb{R}^{T\\times D}$ for each downstream position l = 1,2,..., 2L, where L is the number of layers, T is the total sequence length, and D is the model dimension.\nAs perturbation, we scale the residual stream vector, $x_i'^{(0)} (\\epsilon) = (x_0^{(0)}, ..., (1 - \\epsilon) \\cdot x_i^{(0)},...,x_{T-1}^{(0)} )$, at a single sequence position i, allowing us to continuously tune from weak ($\\epsilon \\approx 0$) to strong ($\\epsilon \\approx 1$) perturbations. Due to the attention mechanism of transformers, this perturbation also influences the residual stream at other token positions, j = 0, . . ., T \u2013 1, downstream in the model. We quantify the effect of perturbing at position i onto tokens at position j for each downstream location l of the residual stream using the response matrices\n$(C_{\\varphi}^{(l)} (\\epsilon))_{i,j} = ||\\Delta_j^{(l)} (\\epsilon)||_2$,\n$(C_{\\psi}^{(l)} (\\epsilon))_{i,j} = 1 - cos(\\varphi_j^{(l)} (\\epsilon))$,\n$(C_{\\nu}^{(l)} (\\epsilon))_{i,j} = cos(\\upsilon_j^{(l)} (\\epsilon))$,\nwhich measure the $l^2$-norm of the difference $\\Delta_j^{(l)} (\\epsilon) = x_j'^{(l)} (\\epsilon) - x_j^{(l)} \\in \\mathbb{R}^D$, as well as the cosine similarities, $cos(\\varphi_j^{(l)} (\\epsilon))$ and $cos(\\upsilon_j^{(l)} (\\epsilon))$, computed for $x_j'^{(l)} (\\epsilon)$ and $\\Delta_j^{(l)} (\\epsilon)$, respectively, relative to the unperturbed state $x_j^{(l)}$. Through this comparison to the unperturbed state, the response matrices (1-3) quantify accumulated correlations between tokens, which contain the full downstream contribution of all model components up to the specific position l in the residual stream."}, {"title": "2.2 Probing induction behavior in Gemma-2-2B", "content": "To demonstrate that the introduced response matrices (1-3) are indeed suitable metrics to study the emergence of induction behavior, we benchmark our method on repeated sequences of random tokens, using a pre-trained version of Gemma-2-2B [44] as our model. Figure 2a and 2d show typical response matrices in the weakly perturbed regime ($\\epsilon = \\epsilon_0 = 0.05$), for a repeated subsequence of $T_0$ = 64 random tokens at the final position of the residual stream (l = 2L). We observe that"}, {"title": "3 Scale-invariant response", "content": "To test the robustness of our introduced response metrics, we repeat the experimental protocol described above and measure the response functions in the entire range from weak to strong pertur- bations (s. Fig. 2b, 2e). Strikingly, we find that the shape of $C_{\\varphi}^{(2L)}$ and $C_{\\psi}^{(2L)}$ remains unchanged"}, {"title": "4 Emergence of induction signatures", "content": "Now that we have established the robustness of our method, we finally apply it to qualitatively show the emergence of induction signatures within LLMs. To do this, we map out the evolution of the model's response across the residual stream for a range of values around $\\Delta_j = T_0 \u2013 1$ (s. Fig. 4).\nIn the first layers of Gemma-2-2B, we observe a pronounced maximum in both $C_{\\varphi}^{(l)}$ and $C_{\\psi}^{(l)}$ at a value of $\\Delta_j = T_0$, signaling correlations between the same tokens of each subsequence. We note that for $C_{\\psi}^{(l)}$, this maximum is already present within the first layer of the model (l = 1) and therefore does not result from a composition of components across layers. As we proceed through the residual stream, we find that the position of this maximum gradually shifts to $\\Delta_j = T_0 \u2013 1$, predominately within a crossover region between l \u2248 30 and l \u2248 40. Simultaneously, in this crossover region, $C_{\\psi}^{(l)}$ changes sign. These results suggest that the induction behavior of Gemma-2-2B is predominately composed within a localized subset of layers within the second half of the residual stream.\nTo understand to what extent our findings universally hold across models, we apply our method to two other models of similar size, GPT-2-XL and Llama-3.2-3B, using the same data set as described above (s. Fig. 5). Across all models, we observe the same universal scaling of $C_{\\psi}^{(2L)}$ over two orders of magnitude in the scale parameter ($\\epsilon = 5 \\cdot 10^{-4} \u2013 5 \\cdot 10^{-2}$). This result confirms the applicability of our method across models and provides evidence that the scale-invariant regime of the response is in fact a universal property across LLMs.\nWe extend our measurement of $C_{\\varphi}^{(l)}$ across the residual stream, and observe that both GPT-2-XL and Llama-3.2-3B display the characteristic shift of the maximum from $\\Delta_j = T_0$ to $\\Delta_j = T_0 \u2013 1$, signaling the onset of induction behavior. For GPT-2-XL, this onset is characterized by a smooth transition, localized to positions l \u2248 30-60, while for Llama-3.2-3B, there exist stronger fluctuations between adjacent layers and the maximum of the response shifts multiple times already in early layers before settling to its final value at $\\Delta_j = T_0 \u2013 1$. These results suggest qualitative differences between multi-layer models in forming induction behavior and serve as a benchmark for future circuit analyses within these models."}, {"title": "5 Conclusion", "content": "In this work, we have observed the emergence of induction signatures within Gemma-2-2B, GPT-2-XL and Llama-3.2-3B by probing the models' response to weak perturbations of the residual stream. For each model, we found a pronounced universal regime, in which the response functions remain scale-invariant under changes in perturbation strength and thereby demonstrated the robustness of our method in quantifying the induction behavior. Beyond the induction mechanism and models studied within this work, we expect our method to provide valuable information about the complex collective interplay of components within LLMs."}, {"title": "Limitations", "content": "In this work, we focused solely on repeated sequences of random token to study induction behavior and, therefore, did not demonstrate the applicability of our method to real text sequences. Furthermore, while our method can be readily extended to study also higher order correlations between tokens, we only considered two-token response functions within this work. We expect such higher order correlations to be particularly relevant for actual text sequences containing more complex correlations between tokens.\nAdditionally, in our empirical work, we did not provide a theoretical foundation for the observed scale-invariant response. Therefore, it would be valuable to investigate how such remarkably robust, universal behavior can exist in the presence of the complex non-linear interplay of LLM components, and how it emerges during training.\nFinally, we did not demonstrate the universality of scale-invariance beyond the models Gemma-2-2B, GPT-2-XL, and Llama-3.2-3B, studied within this work. Therefore, it needs to be verified if the same scaling behavior and emergence of induction signatures can also be found in other models. In"}, {"title": "6 Related Work", "content": "Universality is a central research topic within mechanistic interpretability to understand if common features and circuits form across models and tasks [46]. For LLMs, universal behavior could be identified by finding common attention heads across models [5, 8, 26], the reuse of circuits for different tasks [47], by stitching parts of different models together [48], or by analyzing the emergence of capabilities during training [49]. At the same time, experiments focusing on modular addition tasks [50], group composition [51], and comparisons of neuron activations between different training instances of a model [52] showed mixed results for such universality claims.\nInduction heads and circuits were discovered in toy models [7] and later also analyzed in larger models using different approaches, including interventions on attention heads [8, 43]. Studies also revealed more nuanced attention heads such as anti-induction heads [8], negative name mover heads [5], successor heads [26], copy suppression heads [35], semantic induction heads [40], and provided evidence for the emergence of induction during training [41], and through the interplay of model components in toy models [42].\nFurthermore, our work uses weak perturbations to probe downstream token correlations in the residual stream, and therefore lies at the boundary between intervention and observation studies [53, 54].\nIn observation studies, layerwise interpretations of the model's output could be gained by applying the final classification layer [55], and learned affine transformations [56] to intermediate model layers or in between layers [57]. Observation studies also used layerwise projections onto the space of vocabulary items [58], cross-attention in encoder-decoder architectures [59], attention head specific transformations [60], and analyzed the information of tokens about future tokens [61]. Further post- hoc methods used attention scores [30] and attribution scores and trees [32] to uncover information flow in and between individual layers of transformers.\nIntervention studies enabled a study of the causal structure of LLMs [62]. A large part of this work focused on activation patching methods such as causal tracing [63], interchange intervention [64], causal mediation [65], and causal ablation [5]. Activation patching methods could also reveal a robustness of the model to removing the majority of heads [66], and removing and swapping adjacent layers [36], to gain insights into the layerwise interference process, including how attention heads and MLP sublayers form representations [67]. Further extensions of such activation patching work include path patching which enabled a study of the effect of attention heads on the model's output [68], and subspace activation patching [69]. Furthermore, attribution patching methods used linear approximations to improve the scalability of activation patching [70\u201372] and revealed the influence of single model components on the information flow within transformer models.\nFinally, also relevant to our work is the method of Deep Taylor Decomposition [73], which was used to extract relevancy maps of tokens for specific classification tasks [34]. In contrast, the correlation matrices (1-3) used within our work can be interpreted to contain the relevancy between tokens."}, {"title": "A Additional computational details", "content": "The experiments described within this work were performed using the TransformerLens library [74] which enables accessing and modifying activations within transformer models. All experiments described were performed on a single NVIDIA A100 GPU from a cloud provider. An overview of the pre-trained models used within this work can be found in Table 1."}]}