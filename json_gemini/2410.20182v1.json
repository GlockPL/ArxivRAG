{"title": "Chemical Language Model Linker: blending text and\nmolecules with modular adapters", "authors": ["Yifan Deng", "Spencer S. Ericksen", "Anthony Gitter"], "abstract": "The development of large language models and multi-modal models has enabled\nthe appealing idea of generating novel molecules from text descriptions. Generative\nmodeling would shift the paradigm from relying on large-scale chemical screening\nto find molecules with desired properties to directly generating those molecules.\nHowever, multi-modal models combining text and molecules are often trained from\nscratch, without leveraging existing high-quality pretrained models. That approach\nconsumes more computational resources and prohibits model scaling. In contrast,\nwe propose a lightweight adapter-based strategy named Chemical Language Model\nLinker (ChemLML). ChemLML blends the two single domain models and obtains\nconditional molecular generation from text descriptions while still operating in\nthe specialized embedding spaces of the molecular domain. ChemLML can tailor\ndiverse pretrained text models for molecule generation by training relatively few\nadapter parameters. We find that the choice of molecular representation used\nwithin ChemLML, SMILES versus SELFIES, has a strong influence on conditional\nmolecular generation performance. SMILES is often preferable despite not guar-\nanteeing valid molecules. We raise issues in using the large PubChem dataset of\nmolecules and their associated descriptions for evaluating molecule generation and\nprovide a filtered version of the dataset as a generation test set. To demonstrate\nhow ChemLML could be used in practice, we generate candidate protein inhibitors\nand use docking to assess their quality.", "sections": [{"title": "1 Introduction", "content": "Machine learning methods have emerged as powerful tools in biology and chemistry [Ching et al.,\n2018]. These methods have been applied extensively across a range of small molecule and protein\nproblems, from molecular property prediction [Gilmer et al., 2017] and molecule interaction pre-"}, {"title": "2 Related Work", "content": "Contrastive learning Contrastive learning has emerged as a powerful technique for learning rich,\nmeaningful representations in multiple domains. SimCLR [Chen et al., 2020] performs contrastive\nlearning between different augmented views of the same image to learn visual representations.\nMolCLR [Wang et al., 2022] extends a similar method to graph neural networks on molecule graphs.\nCLIP [Radford et al., 2021] introduces a novel approach to contrastive learning that unifies language"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Task definition", "content": "We define the task as generating molecules based on a text description such that the molecules match\nthe description. The text may contain information about the desired physical, functional, or chemical\nproperties of the molecule."}, {"title": "3.2 Preliminaries", "content": "Simplified molecular input line-entry system (SMILES) SMILES is a notation used to represent\nmolecules as strings [Krenn et al., 2020]. SMILES strings describe atomic composition, connec-\ntivity, and stereochemistry using simple rules. For example, the SMILES of 4-Methylphenol is\nCc1ccc(O)cc1.\nSelf-referencing embedded strings (SELFIES) SELFIES is an alternative representation for\nmolecules designed to overcome some limitations of SMILES [Krenn et al., 2020]. It ensures\nthe generation of syntactically and semantically valid molecular structures. This is achieved through\na self-referencing system that encodes rules and constraints within the string itself, making SELF-\nIES particularly useful for applications in generative models. The SELFIES of 4-Methylphenol is\n[C][C][=C][C][=C][Branch1][Branch1][C][=C][Ring1][=Branch1][O]. It is much longer than the\ncorresponding SMILES format."}, {"title": "3.3 ChemLML model architecture", "content": "Text pretrained models We used three pretrained text LLMs that are specialized for scientific text.\nSciBERT [Beltagy et al., 2019] is a lightweight model with 110M parameters. Galactica [Taylor\net al., 2022] was reported to have the best performance in a chemistry-related evaluation [Mirza et al.,\n2024]. Due to GPU memory constraints, we only used Galactica-125M, 1.3B, and 6.7B. We also\nused the T5 encoder from Text+ChemT5 [Christofidellis et al., 2023].\nMolecule pretrained models We focused on pretrained models that use SMILES or SELFIES string\nrepresentations. MolGPT [Bagal et al., 2021] is a GPT model. MolGen [Fang et al., 2023b] is a\nBART-style model that has two stages of pretraining. During the first stage, it randomly masks some\nof the SELFIES tokens, encodes the corrupted SELFIES using a bidirectional model, calculates\nthe likelihood of SELFIES string with a left-to-right autoregressive decoder, and calculates the\nreconstruction loss. In the second stage, it introduces the domain-agnostic molecular prefix as a\ndomain instructor to facilitate the transfer of knowledge across diverse domains. We only use the\nMolGen autoregressive decoder. MolXPT [Liu et al., 2023c] is another GPT-style model, which\nunifies text and molecules during pretraining. It replaces the molecule name with a SMILES string.\nWe only use the molecule decoding capability of MolXPT by feeding the start-of-molecule token at\nthe beginning during decoding.\nComplete architecture We use the text pretrained model as a text encoder and the molecule pretrained\nmodel as a molecule decoder. We add a chemical adapter to the last layer of the molecule decoder\nthat takes both text and molecule embeddings as input and outputs a molecule embedding. A similar\narchitecture was applied in the protein domain with LM-Design [Zheng et al., 2023] and ProtT3 [Liu\net al., 2024]. The model architecture is shown in Fig. 1."}, {"title": "3.4 ChemLML training and inference", "content": "Adapter module We use a cross-attention mechanism to construct the adapter between the text\nembedding and molecule embedding. Let $T = \\{t_1, t_2,..., t_m\\}$ be the set of text embeddings and\n$S = \\{8_1, 8_2, ..., s_n\\}$ be the set of molecule token embeddings. In cross-attention, the text embedding\nwill be projected to the same dimension as the molecule embedding, denoted as $T' = \\{t'_1, t'_2, ..., t'_m\\}$.\nWe can obtain the cross-attention matrix $a$ with size $m \\times n$. For $i = 1, 2, ..., m$ and $j = 1, 2, ..., n$,\nthe attention weights can be calculated as $a_{i,j} = \\frac{exp(t'_i s_j)}{\\sum_j exp(t'_i s_j)}$ and the molecule embedding will be\nupdated as $s' = \\sum_{j=1} a_{i,j}s_j$. We input the new molecule embedding into the language model head\nof the molecule pretrained model. The updated embeddings still reside within the embedding space\nof the original pretrained molecule model. The difference is that these molecules are now aligned\nwith the text embeddings, thereby inheriting the properties described by the text.\nDuring the training process, we use textual descriptions as input, which are first fed into the text LLM\nmodel to obtain their embeddings. Subsequently, we use the adapter to perform the cross-attention\noperation between the text embeddings and molecule embeddings from the last layer of the molecule\npretrained model to capture the relationship between the textual and molecular information. We adopt\nthe teacher-forcing strategy during training, using the ground truth token as the input for next step\nrather than using the model's own output from the last step."}, {"title": "3.5 Metrics", "content": "We intend generated molecules to exhibit specific properties. Following the molecular similarity\nprinciple [Duran-Frigola et al., 2020], this would result in the generated molecules having structures\nsimilar to the ground truth molecules, thereby achieving a high degree of similarity and low diversity\nduring evaluations. Therefore, we primarily focus on various similarity metrics and calculate the\nsimilarity between generated molecules and ground truth molecules. We use MACCS fingerprints,\nRDKit fingerprints [Landrum, 2018], and Morgan fingerprints as different molecule representations\nthat produce different similarity scores. The similarities between these fingerprints are calculated\nwith Tanimoto similarity. We discuss other metrics not used here in the Supplementary Methods.\nSMILES-based generation will generate syntactically invalid molecules sometimes, so we also include\nvalidity as a metric. Even though SELFIES is designed to include 100% validity, the SELFIES-based\nmodel might fail to generate a valid molecule, such as samples with an [EOS] token at the beginning.\nThe validity metric only evaluates syntactic correctness without accounting for the molecule's 3D\nstructure."}, {"title": "3.6 Datasets", "content": "Pretraining datasets The pretrained molecule generation models were trained on the following\ndatasets:"}, {"title": "3.7 Baseline methods", "content": "T5 The T5 model comes from the MolT5 paper [Edwards et al., 2022]. It is trained on the ChEBI-20\ndataset.\nMolT5 MolT5 loads the T5.1.1 checkpoint and is pretrained using the replace corrupted spans\nobjective. During each pretraining step, a minibatch comprising both natural language sequences and\nSMILES sequences is sampled and certain words are randomly selected for corruption. The task is\nto predict the spans of tokens that were corrupted. In the finetuning stage, the model is trained on\nChEBI-20.\nMolXPT MolXPT is pretrained on PubMed and PubChem data. Then, it is finetuned on ChEBI-20\ndataset."}, {"title": "3.8 Training and evaluation", "content": "We evaluate the model's performance on text-based molecule generation. For MolT5, Text+ChemT5,\nMolXPT, SciBERT, and different scales of MolGen, we use their checkpoints on HuggingFace. For\nthe Text+ChemT5 encoder, we add the prompt \"Write in SMILES the described molecule:\", which is\nused in the corresponding task in Text+ChemT5.\nFor ChemLML, we use SciBERT, the encoder from Text+ChemT5 and different scales of Galactica\nas the text encoders and combine them with MolGPT, MolXPT, MolGen, and MolGen-7B as the\nmolecule decoders. The weights of molecule decoder are always frozen. The weights of the adapter\nare trainable. The weights of the text encoder can be either trainable or frozen. We run experiments\non two datasets, ChEBI-20 and PubChem.\nFor ChEBI-20, we report the results from the MolT5, Text+ChemT5, MolXPT, and TGM-DLM\npapers. T5 is included in the MolT5's paper.\nMolGPT can not be finetuned in ChEBI-20 because it is pretrained on MOSES dataset so its tokenizer\ncan not deal with chirality information. We use MolGPT to compare SMILES- and SELFIES-based\nmodels. First, we use RDKit to remove stereochemistry information from all the molecules. Then,\nwe remove all the molecules that fail to be tokenized by MolGPT's tokenizer. Lastly, we transform all\nthe SMILES to SELFIES. This reduces the ChEBI-20 training set from 26,407 to 15,899 instances,\nvalidation set from 3,301 to 1,961 instances, and test set from 3,300 to 2,032 instances. We build the\nMolGPT model with 12 layers, 8 attention heads, and 256 as the embedding dimension. The only\ndifferences between MolGPT SMILES and SELFIES models are the output dimensions, depending\non the vocabulary lists of each method.\nWe use the Noam Optimizer with 4,000 warm up steps for training. For the sampling in the test set,\nwe use multinomial sampling with random seed 42 for all methods."}, {"title": "3.9 Docking case study setting", "content": "We filtered the PubChem dataset to identify text descriptions that pertain to molecules that inhibit\nspecific protein targets by initially retaining only descriptions with the string \"inhibit\". Then, we split\nall remaining instances into low (0.15 to 0.3), medium (0.3 to 0.5), and high (0.5 to 0.9) similarity\nbased on the Morgan fingerprint similarity between the ground truth and generated molecule from\nthe ChemLML (T5 encoder+MolGen) model. We selected one example per similarity bin (Table S2)\nas described in the Supplementary Methods.\nAfter we generated the first molecule with the temperature set to 1 and random seed 42, we set\nthe temperature to 1.5 and changed the random seed in order to sample different molecules. We\ncanonicalized the generated SMILES with RDKit and removed duplicates. We repeated this process\nuntil we obtained 99 additional unique and valid SMILES, resulting in a total of 100 ChemLML-\ngenerated molecules per target. The SMILES were then used to build 3D conformers with OpenEye's\nOmega2, and only a subset produced valid 3D structures for docking (Supplementary Methods). For\nChemLML, we used the T5 encoder+MolXPT and T5 encoder+MolGen settings. We omit \"encoder\"\nfor brevity in the docking experiments. We performed the same molecule generation process with the\nMolT5 and Text+ChemT5 baselines.\nFor each target protein, we docked the ground truth molecule, generated molecules, and control\nmolecules from two types of background distributions: FDA-approved compounds and ChemLML\nT5+MolGen-generated molecules from randomly-sampled descriptions. A consensus docking score\nwas computed for each molecule that combines results from four docking programs (Supplementary"}, {"title": "3.10 Code and data availability", "content": "The ChemLML code and PubChem-filtered dataset are available from https://github.com/\ngitter-lab/ChemLML and archived at https://doi.org/10.5281/zenodo.13925649. The\npretrained ChemLML models and PubChem-unfiltered dataset are available from https://\nzenodo.org/doi/10.5281/zenodo.11661517. The ChEBI-20 dataset is available from https:\n//github.com/cnedwards/text2mol, and a copy is in the ChemLML GitHub repository."}, {"title": "4 Results", "content": "We first compare baseline and ChemLML models trained on ChEBI-20 text descriptions on the ChEBI-\n20 test set (Table 1). The best performing ChemLML model is the T5 encoder finetune+MolXPT\nversion. With 114M trainable parameters, this model achieves 0.727 in Morgan FTS. It is not\nsurprising to find that models that include the T5 encoder work the best because the T5 encoder has\nbeen trained on multiple tasks. The ChemLML combination T5 encoder+MolXPT performs better\nthan the baseline MolT5 even though MolT5 has 52 times more trainable parameters."}, {"title": "4.1 Comparison among pretrained models used with ChemLML", "content": "When comparing the ChemLML models that use the MolGen model and do not finetune the text\nmodel, it is surprising that the cross-modal representation capability of similarly sized text models\ndoes not grow over time. The ChemLML variants SciBERT+MolGen and Galactica 125M+MolGen\nperformed similarly. SciBERT was published in 2018 with 110M parameters and is trained on 3.3B\ntokens. Galactica 125M is trained on 106B tokens, around 32 times more than SciBERT. A possible\nexplanation is that Galactica 125M is too small for the large training set. However, Galactica 1.3B\nalso outperforms SciBERT by only a small margin. Even Galactica 6.7B, which is far larger than\nGalactica 125M, only yields a slight improvement. LLMs perform well when combined with MolGen\n7B. ChemLML Galactica 125M+MolGen 7B achieves better results than MolT5.\nWe also consider the results from finetuning the pretrained language models along with the adapter.\nSciBERT outperforms Galactica 125M in this setting. Also, ChemLML SciBERT finetune+MolGen\noutperforms MolT5 with half the trainable parameters. The combination of the T5 encoder and\nMolGen is further strengthened by finetuning the T5 encoder. However, for the ChemLML T5\nencoder models that use MolGen 7B, the improvement from finetuning the T5 encoder is marginal."}, {"title": "4.2 Molecule similarity and validity trade-off", "content": "There is a trade-off between generated and ground truth molecule similarity and the generated\nmolecule validity (Table 1). T5 and MolT5 have the same model architecture. MolT5 has been\npretrained on a large amount of data with both SMILES and natural language while T5 has not. At\nfirst glance, it is therefore surprising that T5 outperforms MolT5 in the similarity metrics. However,\nMolT5 generates 11 percentage points more valid molecules than T5, which means more molecules\nare counted in the similarity calculation. These molecules that are more difficult to generate may\nhave lowered the overall similarity."}, {"title": "4.3 SMILES versus SELFIES representations", "content": "The choice of molecule representation, SMILES or SELFIES, is another factor that influences the\nsimilarity calculations. Skinnider [2024] recently showed that a molecule language model trained\non SMILES had Fr\u00e9chet ChemNet Distance (FCD) lower than 0.5, while one trained on SELFIES\nhad FCD greater than 2 because invalid SMILES are low-likelihood samples and they improve\nperformance on distribution-learning metrics like FCD. However, MolGen is trained on SELFIES,\nand there is not yet an equivalent model pretrained on SMILES that goes through multiple stages of\npretraining. Therefore, we use MolGPT to compare SMILES and SELFIES directly."}, {"title": "4.4 PubChem evaluation reveals pros and cons of LLMS", "content": "Because many of the PubChem molecule descriptions are overly ambiguous and generic, we prioritize\nthe PubChem-filtered version of the dataset but also evaluate the PubChem-unfiltered dataset to\nassess how the filtering affects the results. On PubChem-filtered, LLMs without finetuning perform\npoorly, while the finetuned LLMs encoders show better results (Table 3). PubChem-filtered has a\nsimilar distribution as the ChEBI-20 dataset, so the ChemLML T5 encoder+MolXPT model still\noutperforms other methods that have a frozen text encoder. The ChemLML combination T5 encoder\nfinetune+MolGen 7B does not perform well, potentially due to overfitting on the ChEBI-20 dataset.\nAnother notable finding is that ChemLML generates a higher proportion of exactly matched molecules\ncompared to the MolT5 and Text+ChemT5 baselines. We suggest that it is because MolGen and\nMolXPT have been pretrained on a large number of molecules, then ChemLML is able to retrieve\nthose molecules accurately based on the text embedding.\nWe are cautious interpreting results from the PubChem-unfiltered dataset (Table S1) due to the\nabundance of generic descriptions (Figure S1), but they do illuminate difference among the ChemLML"}, {"title": "4.5 Docking case study", "content": "To demonstrate how ChemLML could be used in practice, we applied it to generate chemical\ninhibitors of protein drug targets acetylcholinesterase, inosine-5'-monophosphate dehydrogenase,\nand heat shock protein HSP 90-alpha (Table S2). We obtained the experimental structures of these\nproteins and used FRED [McGann, 2011], Gnina [McNutt et al., 2021], PLANTS [Korb et al., 2009],\nand rDock [Ruiz-Carmona et al., 2014] to dock the ground truth inhibitor (co-crystallized ligand)\nfrom PubChem, the ChemLML-generated candidate inhibitors, and candidate inhibitors generated\nby two baseline methods. In addition, we docked control molecules generated by ChemLML and\nan FDA-approved screening library that are not expected to bind the targets. We generated a single\nconsensus docking score per compound (Figure 2).\nDocked ground truth and most generated molecules score poorly for the heat shock protein HSP\n90-alpha, likely due to poor handling of macrocycles by docking programs (Table S2). Here, control\ncompounds from the FDA and ChemLML background sets have median scores (0.00009 and 0.00043,\nrespectively) similar to the ground truth molecule (0.00006) and the ChemLML T5 + MolGen\ngenerated ensemble median (0.00004). The ChemLML T5+MolXPT generated ensemble scores\nslightly higher with a median of 0.00115 but not substantially better than the random ChemLML\nbackground. Considering the poor docking score of the ground truth molecule, we consider the HSP"}, {"title": "5 Discussion", "content": "This study proposes the ChemLML framework for using pretrained language and molecular models\nin text-guided molecule design. By reusing large-scale pretrained models, we enhance the flexibility\nof text-based molecule generation and reduce the training effort by supporting multiple types of\nfrozen text encoders. In our case study generating candidate inhibitors for three protein targets from\ntext, we find that for the two targets where docking performs well, ChemLML T5 encoder+MolGen\ngenerates molecules with docking scores that are better than ChemLML T5 encoder+MolXPT, two\nexisting molecule generation algorithms, and two distributions of control molecules. For the inosine-\n5'-monophosphate dehydrogenase target, the median ChemLML docking score is even better than\nthe ground truth inhibitor from PubChem.\nWhen finetuning LLMs, we find it is easy to finetune Galactica 125M. However, it becomes harder to\nfinetune the Galactica 1.3B model. Other finetuning methods such as Low-Rank Adaptation [Hu et al.,\n2022] may solve this problem. Also, we do not carry out experiments on Galactica 30B and 120B\ndue to hardware and training technique limitations. Improving the LLM finetuning and finetuning\nlarger LLMs are directions for future work.\nA limitation of ChemLML is that it only focuses on molecule generation whereas related methods\nare multi-task. For instance, MolT5 can perform molecule captioning, and Text+ChemT5 can also\nconduct forward reaction and retrosynthesis prediction. Conceptually, the ChemLML framework is\ncompatible with a multi-task setting and even more modalities including proteins [Liu et al., 2023b,\n2024]. This presents another avenue for future work.\nEven though ChemLML performs well on the CheBI-20 and PubChem datasets, there is still a gap\nbetween these experiments and real world applications. The molecular descriptions in these datasets\nfollow a relatively fixed format. We have not conducted user studies to assess how chemists' general\nexpectations for text-based molecule generation deviate from the types of descriptions in CheBI-20\nand PubChem. In addition, a limitation of text-based molecule generators like ChemLML is that\nthe text encoder can be sensitive to the prompt. The necessity to conduct evaluations on datasets\nlike CheBI-20 and PubChem, where molecules have existing text descriptions, makes it challenging\nto evaluate these models' robustness and generalizability to out-of-distribution data, both new text\nprompts and new molecules.\nEven after our PubChem filtering, data quality issues in this dataset likely remain (Supplementary\nMethods). PubChem-unfiltered offers a large potential training dataset, and similar datasets have\nbeen used in prior work. We intentionally decided to not train models on the generic text descriptions\nin PubChem and caution others to think carefully about whether PubChem-unfiltered or PubChem-\nfiltered is more appropriate for their training and evaluation goals."}, {"title": "6 Conclusion", "content": "ChemLML introduces the strategy of using an adapter between text LLMs and molecule decoders for\ntext-guided molecule design. Our approach is designed to capitalize on the rampant advances in both\nnatural language modeling and unconditional molecule generation. The adapter is lightweight and\ncompatible with different kinds of pretrained LLM encoders and molecule decoders. Looking forward,\nthe biological and chemical communities have shown strong interest in using natural language to\ncondition the generation of small molecules and even proteins with desired structural and functional\nproperties Pei et al. [2024]. ChemLML provides a path for combining the most advanced natural\nlanguage and domain-specific models as they continue to undergo rapid development for this type of\ntext-conditioned generation."}, {"title": "7 Appendix", "content": ""}, {"title": "7.1 Computational resources", "content": "For ChemLML models with less than 1B total parameters, we perform the training on one NVIDIA\nRTX 2080Ti, which has 11GB memory; for ChemLML models with greater than 1B total parameters,\nwe perform the training on one NVIDIA L40, which has 48GB memory."}, {"title": "7.2 Broader impacts", "content": "Like all molecule generation models, ChemLML has both positive and negative potential broader\nimpacts due to its potential to suggest novel chemicals [Urbina et al., 2022]. These impacts are\nespecially pertinent for text-based generative models such as ChemLML because they are designed\nto produce chemicals with desired properties from natural language without requiring experimental\ntraining data related to those properties for supervised training. As demonstrated in our case study,\none goal with ChemLML is to use it for beneficial purposes through applications in early stage drug\ndiscovery. However, we also see the potential to generate harmful molecules based on our partial\nmanual review of the ChEBMI-20 dataset used to train ChemLML models. We encountered text\ndescriptions related to opioids and carcinogens. Ultimately, molecules generated by ChemLML still\nhave to be manually reviewed and synthesized by a human chemist. Therefore, we believe it presents\nless relative risk than existing harmful chemicals and a fully-automated system like ChemCrow [Bran\net al., 2024] that plans and executes chemical synthesis."}, {"title": "7.3 Supplementary Figures", "content": ""}, {"title": "7.4 Supplementary Tables", "content": ""}, {"title": "7.5 Supplementary Methods", "content": ""}, {"title": "7.5.1 PubChem dataset", "content": "Generic PubChem descriptions may be reasonable for learning general molecule properties but\nviolate our assumptions for evaluating text-based molecule generation, which leverages the molecular\nsimilarity principle and assumes the generated molecule should be structurally similar to the ground\ntruth molecule. The biggest problem with PubChem-unfiltered is that many molecule descriptions\nare general. For example, the most frequent description, occurring 5,753 times is \"The molecule is a\npeptide\", which is uninformative. The top 10 most frequent descriptions are shown in Fig. S1. Also,\nthere are descriptions like \u201cThe molecule is a natural product found in\u201d when there can be hundreds\nof natural products produced by a single species. For instance, the fungal genera Aspergillus and\nPenicillium are associated with 3,091 and 2,550 natural products, respectively, and the bacterial genus\nStreptomyces is associated with 5,755 natural products [van Santen et al., 2022].\nIn addition, there are counterintuitive descriptions. One PubChem-unfiltered example has the\ndescription \"The molecule is a mineral\u201d and the SMILES of oxygen. It turns out that \u201cminerals\u201d is\nindeed in the PubChem page of oxygen (CID 977). This is because \u201corange\u201d 18O2 was crystallized\nunder special experimental conditions [Cromer et al., 1983]. Thus, oxygen is recorded in the American\nMinerallogist Crystal Structure Database, which is cross-referenced from PubChem. In order to obtain\na more meaningful dataset, we filter out descriptions less than 30 words and containing the description\n\"natural product\". This eliminates many, but not all, of the issues described above.\nEven the PubChem-filtered dataset contains other potential problems. A form of data leakage arguably\noccurs when a synonym of the molecule is in the text description. Our dataset review noted many\ndrug names such as loperamide, tretinoin, lanreotide, ambrisentan, nesbuvir, danusertib, and others in\nthe descriptions. Other descriptions state that the molecules is an enantiomer of another. Yet other\ndescriptions retain leftover IUPAC names, which can be used to generate molecules [Rothchild et al.,\n2021]. In all of these cases, the text descriptions gives more information than expected about the\nchemical structure, which may inflate similarity-based evaluation metrics.\nWe constructed the PubChem-filtered dataset independently from the ChEBI-20 dataset. However,\neven though we have transformed all the molecules into canonical form and make sure there are\nno overlapping canonical SMILES between the PubChem-filtered and the ChEBI-20 datasets, there\nare still molecules that have 100% similarity between PubChem-filtered and ChEBI-20. We did\nnot exhaustively remove these molecules in the datasets. Thus, we cannot guarantee that PubChem-\nfiltered is entirely disjoint from the ChEBI-20 training and validation splits.\nIn the evaluations, we directly test MolT5 and Text+ChemT5\u2019s performance on the test set without\nfurther finetuning. For fair comparison, we also train ChemLML models on ChEBI-20 and test on\nthe PubChem test set without finetuning. To make sure the PubChem test set corpus aligns with the\nChEBI-20 training corpus, we manually replace each molecule\u2019s IUPAC name with \u201cThis molecule\u201d\nand delete \u201cwith data available\u201d at the end of the sentence.\nNote that the typo \u201cmacrocyle\u201d in the high similarity description in Table S2 appears in the original\nPubChem data and was not introduced by our processing."}, {"title": "7.5.2 Evaluation metrics", "content": "Fr\u00e9chet ChemNet Distance [Preuer et al., 2018] is a common evaluation metric for generative\nmolecular models. It uses the embedding of the molecules in ChemNet [Mayr et al., 2018], a\nlong short-term memory-based model, to detect whether generated molecules are diverse and have\nsimilar properties as real molecules. We do not use this metric because our preliminary results and\nan independent evaluation [Holzgruber, 2024] found it is highly sensitive to the sample size and\nmolecule padding length.\nPrevious work in molecule generation also used metrics such as diversity and novelty for evaluating\nmolecule generation. However, these metrics primarily focus on sampling the chemical space to\ngenerate diverse molecules, which are not well-suited for description-guided molecule design.\nLevenshtein distance and BiLingual Evaluation Understudy (BLEU) scores have also been used\npreviously to compare string representations of molecules. However, these two scores that are\ncommon in NLP are not as suitable for molecules. Imagine the case where the generated molecule"}, {"title": "7.5.3 Text+ChemT5 tasks", "content": "The Text+ChemT5 [Christofidellis et al., 2023] tasks are:\n\u2022 mol2mol: This task contains forward reaction and retrosynthesis subtasks. In the forward\nreaction task, given reagents and/or enzymes, the model needs to generate the main product\nof the chemical reaction. For retrosynthesis, given the product of a chemical reaction, the\nmodel needs to find the reagents and/or enzymes.\n\u2022 mol2text: Given the molecule represented as SMILES, the model needs to generate a textual\ndescription of the molecule.\n\u2022 text2mol: Given the textual description of a molecule, the model needs to generate the\nSMILES representation the molecule.\n\u2022 text2text: Given the natural language description of a chemical reaction, the model needs to\ngenerates a step-wise execution protocol to carry out the reaction."}, {"title": "7.5.4 Docking case study", "content": "To select one instance from each similarity bin (low, medium, and high), we manually reviewed the\ninstances in each bin. We selected the first five descriptions that pertained to a single, specific protein\ntarget as opposed to multiple targets or inhibition of a biological process. Then, we examined the\nground truth molecules in PubChem. The example proteins selected for docking were those having\nexperimental ligand-bound structures cross-referenced from PubChem with unambiguous binding\nsites for search space specification in docking. Table S2 provides more information about these three\nselected examples.\nFor each target protein, we docked the ground truth molecule, generated molecules, and control\nmolecules from two types of background distributions. The first background distribution origi-\nnally contained 3,082 small molecules from the Selleck L1300 FDA-approved Library. The sec-\nond background distribution originally contained 1,000 generated molecules from the ChemLML\nChemT5+MolGen model sampling text descriptions from PubChem filtered. Due to molecule pre-\nprocessing, docking-based scoring was ultimately achieved on 2,956 and 657 compounds from\nthese background sets, respectively (Table S3). Additional components within generated SMILES,\nlike complexed waters or counterions on salts, were stripped from parent molecular species prior\nto RDKit SMILES canonicalization. Using OpenEye applications (Cadence Molecular Sciences,\nSanta Fe, NM), small molecule sets were processed from SMILES inputs"}]}