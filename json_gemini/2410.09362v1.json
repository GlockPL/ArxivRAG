{"title": "SERA: SELF-REVIEWING AND ALIGNMENT OF LLMS USING IMPLICIT REWARD MARGINS", "authors": ["Jongwoo Ko", "Saket Dingliwal", "Bhavana Ganesh", "Sailik Sengupta", "Sravan Bodapati", "Aram Galstyan", "WS AI Labs"], "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives for Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the preferences used in DAAs are usually collected before the alignment training begins and remain unchanged (off-policy). This design leads to two problems where the policy model (1) picks up on spurious correlations in the dataset (as opposed to learning the intended alignment expressed in the human preference labels), and (2) overfits to feedback on off-policy trajectories that have less likelihood of being generated by the updated policy model. To address these issues, we introduce Self-Reviewing and Alignment (SeRA), a cost-efficient and effective method that can be readily combined with existing DAAs. SeRA comprises of two components: (1) sample selection using implicit reward margins, which helps alleviate over-fitting to some undesired features, and (2) preference bootstrapping using implicit rewards to augment preference data with updated policy models in a cost-efficient manner. Extensive experimentation, including some on instruction-following tasks, demonstrate the effectiveness and generality of SeRA in training LLMs on offline preference datasets with DAAs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language models (LLMs; Achiam et al. 2023; Team et al. 2023; Jiang et al. 2024) have shown mastery on a multitude of tasks in artificial intelligence (AI), ranging from creative writing (Wang et al., 2024b) to code generation (Li et al., 2023a), and mathematical reasoning (Ahn et al., 2024). With success, comes concerns related to their safety, reliability, and potential for misuse in sensitive domains like social manipulation, cyber-attacks, etc. To address some of these challenges, works have considered aligning LLMs to human values/preferences using approaches like Reinforcement Learning from Human Feedback (RLHF; Ouyang et al. 2022; Bai et al. 2022a).\nInitial RLHF approaches, such as Proximal Policy Optimization (PPO), necessitated the need for two separate models\u2014 a policy model (the LLM model to be aligned) and a reward model (Schulman et al., 2017; Ouyang et al., 2022). Such approaches often result in challenges related to stability and scalability (Rafailov et al., 2023; Zhao et al., 2023; Llama Team, 2024). To mitigate these shortcomings, Direct Alignment Algorithms (DAAs), such as direct preference optimization (DPO; Rafailov et al. 2023), sequence likelihood calibration with human feedback (SLiC-HF; Zhao et al. 2023), and identity policy optimization (IPO; Azar et al. 2024), have emerged as popular alternatives. In DAA, we directly update the policy model/LLM using (some closed-from solution of) the pairwise preference data without the need for an explicit reward model, making the alignment process simpler, more efficient and stable compared to earlier methods (Rafailov et al., 2023).\nHowever, DAAs leverage preference rating on off-policy trajectories collected prior to alignment tuning (at times, generated by a different LLM/policy-model (Zhao et al., 2023; Tunstall et al., 2023)), resulting in two challenges. First, the off-policy preference data, which still require labor-intensive annotations, often contain noisy features orthogonal to the true preference objective (e.g. longer responses are more preferred; Park et al. 2024). Thus, training on this data can teach the"}, {"title": "2 BACKGROUND", "content": "In this section, we provide a brief overview of related works (\u00a72.1) and discuss some preliminaries (\u00a72.2) necessary to understand our contribution."}, {"title": "2.1 RELATED WORK", "content": "Alignment with preference data Actor-critic RLHF frameworks (Christiano et al., 2017; Stiennon et al., 2020; Bai et al., 2022a; Ouyang et al., 2022) seeks to align language models to human preferences, but is both memory-intensive (requires a policy model and a reward model to be on device simultaneously) and unstable during training. To mitigate this, several algorithms, such as direct preference optimization (DPO; Rafailov et al. 2023) and sequence likelihood calibration (SLiC-HF; Zhao et al. 2023), learn the contrastive preference in the offline setting using a closed-form loss function without the need for an explicit critic/reward model. Azar et al. (2024) argued that without regularization, a policy can easily overfit to deterministic preferences and introduced identity preference optimization (IPO) to directly optimize offline preference probabilities with regularization. In parallel, the availability of offline preference datasets like UltraFeedback (Cui et al., 2023), OpenOrca (Lian et al., 2023), Helpfulness and Harmless (Ganguli et al., 2022), and TL;DR Stiennon et al. (2020) has made aligning LLMs more accessible.\nAlignment with on-policy preference annotations. Despite its effectiveness, aforementioned approaches use reward signals over off-policy trajectories (or critic models trained to reward off-policy trajectories) that may have a distribution mismatch from on-policy trajectories generated by the policy model, which is improved in an iterative manner (Guo et al., 2024; Ko et al., 2024). A major impediment is the lack of human annotators (gold critic) to provide preference data for trajectories generated by iteratively-improving policy models. To address this, Guo et al. (2024) proposed online AI-feedback (OAIF) by using another LLM to annotate which of two online-sampled outputs from the current policy is preferred. This idea of distilling preference labels comes in various flavors. For example, Direct Nash optimization (DNO) distills preference signals for on-policy trajectories generated by the updated policy model using a strong teacher model (Rosset et al., 2024). While effective at distilling preference, this methods can incur high expenses for calling proprietary LLM APIs or violate (legal) terms-of-use, making them difficult to use practically.\nAlong these lines, cost-efficient alternatives encourage using the policy models itself as preference labelers or leveraging implicit preference superiority with high-quality human responses. Self-rewarding LMs (Yuan et al., 2024) studied the benefits of iteratively training on preferences derived from recent policy's sampled outputs. However, in their work, they used the LLM itself as the annotator based on prompting (Zheng et al., 2023), which is only valid for LLMs capable of following the prompt properly. Self-play fine-tuning (SPIN; Chen et al. 2024b) and adversarial preference optimization (APO; Cheng et al. 2023) are both iterative LLM training techniques that are compatible with contrastive losses. However, as highlighted in (Rosset et al., 2024), expecting high-quality predefined responses being better than the policy generation without considering of annotator feedback is a severe limitation. In general, methods that use a proxy reward model (whether a more capable LLM teach or the policy model itself) can suffer from reward-hacking that can significantly degrade the alignment quality in iterative improvement of policy models (Pan et al., 2024).\nSample Selection Approaches. In this work, SeRA partially relies on selecting off-policy samples based on an Internal Reward Margin (IRM) of policy models, as opposed to using the absolute reward. We highlight that this minimize distribution shift and, in turn, reduces performance deterioration when using offline preference datasets. While previous works have considered different mechanisms to sub-sample existing data, at times reducing the distributional shift across iterations, their selection approaches and goals differ from ours. For example, Cazenavette et al. (2022) and Chai et al. (2023) focused on improving training efficiency by using a sub-set of the data while guaranteeing performance similar to a model trained on the full data. Along similar lines, Kim et al. (2021); Xia et al. (2022) and Mirzasoleiman et al. (2020) formulate the problem of finding a a core subset of the entire training data that can capture the statistical properties of the original dataset, also aiming to enhance training efficiency. Recent work has also investigated using a weighted loss function over off-policy samples based on the recent policy model's judgements of how close they are to on-policy trajectories Zhou et al. (2024). Instead of focusing on improving training efficiency"}, {"title": "2.2 PRELIMINARIES", "content": "Reinforcement Learning From Human Feedback. The goal of RLHF is to optimize the policy LLM \\(\\pi_{\\theta}\\) such that it maximizes the expected value of the reward function. A common approach to modeling the reward function is using Bradley-Terry (BT; Bradley & Terry 1952) model:\n\\[p(y_w > y_l|x) = \\frac{exp (r(x, y_w))}{exp (r(x, y_w)) + exp (r(x, y_l))} = \\sigma(r(x, y_w) \u2013 r(x, y_l)), \\quad(1)\\]\nwhere \\(y_w\\) (and \\(y_l\\)) denotes the preferred/winning (and losing) policy, \\(p\\) denotes the preference distribution that approximates an unobserved latent reward \\(r(x, y)\\), and \\(\\sigma\\) is the logistic function. To achieve this, RLHF first trains a reward model \\(r_{\\phi}(x, y)\\). Then, RLHF updates \\(\\pi_{\\theta}\\) with an on-policy RL algorithm like PPO (Schulman et al., 2017), iteratively optimizing the model to provide responses more preferred by human. The most common objective is\n\\[L_{RLHF} = E_{x\\sim D,y\\sim \\pi_{\\theta}(\\cdot|x)} [r_{\\phi}(x, y)] \u2013 \\beta D_{KL} [\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)],\\quad (2)\\]\nwhich enforces a KL divergence (Kullback & Leibler, 1951) penalty with a reference distribution \\(\\pi_{ref}(y|x)\\), to prevent the LLM \\(\\pi_{\\theta}\\) from straying too far from its initialization. The hyper-parameter \\(\\beta\\) balances between exploiting the reward function and deviating from \\(\\pi_{ref}(y|x)\\).\nDirect Alignment Algorithms. RLHF is computationally expensive and unstable (Rafailov et al., 2023; Llama Team, 2024). Thus, many algorithms (Rafailov et al., 2023; Zhao et al., 2023; Azar et al., 2024) have been proposed to overcome these challenges. A common idea is to analytically derive the optimal policy and parameterize it using the reward function from Equation 2. In DPO (Rafailov et al., 2023), the optimal policy \\(\\pi^*\\) under the BT model satisfies:\n\\[p^* (y_{\\omega} \u2013 y_l|x) = (1+exp (\\beta log \\frac{\\pi^* (y_l|x)}{\\pi_{ref}(y_l|x)} - \\beta log \\frac{\\pi^* (y_{\\omega}|x)}{\\pi_{ref} (y_{\\omega}|x)}))^{-1}, \\quad (3)\\]\nwhere \\(p^*(y_w > y_l|x)\\) is underlying true preference, the probability that \\(y_w\\) is more preferred than \\(y_l\\). With human preference data expressed in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parameterized policy \\(\\pi_{\\theta}\\).\n\\[L_{DPO} (x, y_w, y_l) = - log \\sigma(\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}). \\quad (4)\\]\nCurrent methods for LLM alignment first collect a dataset of pairwise preferences by obtaining two responses to an input prompt \\(x\\) generated using an LLM. Then, human or AI annotators rank these responses, yielding a preferred response \\(y_w\\) and a less preferred one \\(y_l\\)."}, {"title": "3 OUR METHOD: SELF-REVIEWING AND ALIGNMENT", "content": "In contrast to recent works on DAAs (Rosset et al., 2024; Guo et al., 2024), we propose SeRA to improve alignment with DAA without the need for any external supervision. While self-verification highlights that a policy model can dual up as a reward model (Weng et al., 2022), we show that this self-supervision can be done implicitly for alignment using the notion of Implicit Reward Margin (IRM). After defining IRM in this section, we describe (1) an IRM-based off-policy sample selection, and (2) an IRM-based iterative preference data bootstrapping. Finally, we describe our approach SeRA and how it can be seamlessly incorporated into DAAs."}, {"title": "3.1 IMPLICIT REWARD MARGIN (IRM)", "content": "While works have shown that alignment datasets may have ambiguous preferences (Yang et al., 2023; Chowdhury et al., 2024), even the use of unambiguous preference data can result in policy models unintentionally learning spurious correlations (e.g. on response length) (Park et al., 2024; Rafailov et al., 2024). Recent works consider selecting preference samples \\((x, y_w, y_l)\\) where there exists a large difference in preferences (i.e. \\(y_w >> y_l\\)) to help mitigate the noise in the learning (Yang et al., 2023; Rosset et al., 2024). We leverage this observation and introduce a Implicit Reward Margin (IRM) to quantify the difference between the preference over two responses using the policy model \\(\\pi_{\\theta}\\) itself. Formally, we define IRM as follows where \\(\\pi_{ref}\\) is the reference model:\n\\[m(x, y_w, y_l) := \\frac{1}{\\beta} (r(x, y_w) \u2013 r(x, y_l)) = log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref} (y_w|x)} - log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)},\\quad (5)\\]\nwhich follows from Equation 3 and Equation 4. Unlike Rafailov et al. (2023), we omit the normalization term for the reward \\(r(x, y) := log (\\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)})\\) for tractability."}, {"title": "3.2 IRM-BASED OFF-POLICY SAMPLE SELECTION", "content": "We note that a policy model \\(\\pi_{\\theta}\\) can generate mis-calibrated rewards for individual trajectories (Panickssery et al., 2024), but we empirically study if the reward margin between preferred \\((y_w)\\) vs. less-preferred trajectories \\((y_l)\\) can provide relevant signals to choose samples for improving alignment. Finally, we propose to filter out training samples with lower IRMs. To motivate our choice, we compare against other sample selection methods in the Figure 2. IRM-based sample selection (in green) consistently leads to higher win rates against the SFT model, when compared to alignment on the entire preference samples (in sky-blue), or strong baseliense that using RLAIF with GPT-4 score (Zheng et al. (2023); in orange) or the log probability of \\(\\pi_{ref}\\) (Pattnaik et al. (2024); in pink) for sample selection. These gains hold true across model types, model size, DAAs, and datasets. Moreover, methods that use GPT-4 to obtain reward signals for sampling (Rosset et al., 2024; Zheng et al., 2023) incur high-cost; in comparison, our sample selection strategy that leverages \\(\\pi_{\\theta}\\) to compute IRM is inexpensive and more effective."}, {"title": "3.3 IRM-BASED PREFERENCE DATA BOOTSTRAPPING", "content": "As mentioned earlier, DAA's use of off-policy preference annotation suffers from distributional mismatch between the sequences observed during training (which are from different LLMs ahead of training) and those generated by the iteratively updated policy LLM (Arora et al., 2022; Ko et al., 2024), leading to reduced efficacy of DAAs (Guo et al., 2024). To address this, we present preference data bootstrapping that considers a decoding-time strategy to sample candidate pairs from the (updated) policy LLM, followed by a rejection sampling of pairs with low IRM.\nTo build a preference pair, we sample the \\(R\\) (\\(\\geq 2\\)) distinct candidate responses \\(y^{(i)}_1,..., y^{(i)}_R\\) ~ \\(\\pi_{\\theta_{t-1}}(\\cdot|x^{(i)})\\) from a query \\(x^{(i)}\\) by using decoding-time sampling. We then compute the implicit reward for each response \\(j(\\in \\{1, ..., R\\})\\) using the term \\(r(x^{(i)},y^{(i)}_j)\\) in Equation 5 and select the pair that maximizes the IRM.\nOur method bears similarity to self-rewarding LMs (SRLM; Yuan et al. 2024) that reward responses by using LLM-as-a-Judge (Zheng et al., 2023). Unfortunately, this approach works reliably only when one uses LLMs with sufficient instruction-following capability (see Table 5), which leads to significant costs and limited applicability for smaller LLMs. In contrast, our approach is versatile across LLMs with a wide range of capacities and boasts strong empirical efficacy (see Table 6)."}, {"title": "3.4 SERA: SELF-REVIEWING AND ALIGNMENT", "content": "We first describe how our two proposed components can be incorporated in an iterative manner with DPO and then extend it for other DAAs. Algorithm 1 provides an overview of SeRA . As our method leverages the policy model from previous iterations both for sample selection (\u00a73.2) and preference data bootstrapping (\u00a73.3), we consider an iterative approach to alignment training. Similar to recent works (Kim et al., 2024a; Pattnaik et al., 2024), we also update the reference model \\(\\pi_{ref}\\) to the latest updated policy model after every iteration. At iteration \\(t\\), we obtain \\(\\theta_t\\) using the following update step (instead of Equation 4 used in DPO):\n\\[\\theta_t \\leftarrow arg \\max_{\\theta} E_{(x,y_w,y_l)\\sim D_t} [-log \\sigma(\\beta log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{\\theta_{t-1}}(y_w|x)} - \\beta log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{\\theta_{t-1}}(y_l|x)}),],\\quad(6)\\]\nwhere \\(D_t\\) is a dataset obtained at the start of iteration \\(t\\), by combining k samples from off-policy dataset using \u00a73.2 and k bootstrapped samples using method \u00a73.3 with the highest IRM. At \\(t = 0\\), we consider the reference model to be the SFT model (i.e. \\(\\pi_{\\theta_0} = \\pi_{SFT}\\)) that we seek to align. Note that at \\(t = 1\\), \\(D_t\\) consists only of the original offline samples, as no aligned model is available yet.\nEnsemble of Reward Margin across Different Iteration. As SeRA depends on leveraging the policy model being trained instead of external reward model supervision, it risks introduction of undesired bias or reward hacking (Pan et al., 2024) that can manifest as part of the implicit rewards used to calculate IRM. To alleviate this, we apply \\(m_t(x, y_w, y_l) := (1/\\beta)\\cdot (r_t(x, y_w) \u2013 r_t(x, y_l))\\) rather than using Eqn. (5), where \\(r_t (t > 3)\\) is defined as follows:\n\\[r_t(x, y) = (1 \u2013 \\gamma) log \\frac{\\pi_{\\theta_{t-1}} (y|x)}{\\pi_{\\theta_{t-2}} (y|x)} + \\gamma log \\frac{\\pi_{\\theta_{t-2}} (y|x)}{\\pi_{\\theta_{t-3}} (y|x)} = log \\frac{\\pi_{\\theta_{t-1}} (y|x)^{(1 \u2013 \\gamma)}}{\\pi_{\\theta_{t-2}} (y|x)^{(1 \u2013 \\gamma)} \\pi_{\\theta_{t-3}}(y|x)^{\\gamma}},\\quad(7)\\]"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nSetup. Our setup resembles Tunstall et al. (2023) and Hong et al. (2024), where they consider three models- TinyLlama (Zhang et al., 2024), Pythia-2.8B (Biderman et al., 2023), and Mistral-7B (Jiang et al., 2023). Initially, these models are fine-tuned on UltraChat-200k (Tunstall et al., 2023) followed by an alignment with DAAs and preference pairs from UltraFeedback. We use the binary version of UltraFeedback (which contains two response pairs with corresponding ratings for"}, {"title": "4.2 RESULTS", "content": "DPO-baselines We assess the general instruction-following abilities of LLMs by comparing DPO-based preference alignment algorithms in Table 1. We observe that (1) the instruction-following"}, {"title": "5 ANALYSIS AND DISCUSSION", "content": "5.1 EFFECTS OF OFF-POLICY SAMPLING (k) AND PREFERENCE BOOTSTRAPPING (k)\nWe evaluate the efficacy of sample selection and preference bootstrapping in our proposed method. We plot the winning rate for different ratios of k and k in Figure 5a. The solid lines represent results obtained by different iterations in Sera with varying ratios of k and k maintaining k + k = N, while the dotted lines represent results obtained by using only IRM selection where k = 0.7N, k = 0. The results indicate that (1) IRM-based selection shows consistent improvement in comparison to using only offline samples (0% on x-axis), or only generated samples (100%), and (2) combining IRM-based sample selection and preference bootstrapping at moderate levels (i.e., 30-70%) leads to consistent improvement, highlighting the importance of both components in SeRA. Without offline samples, IRM and generated samples may lack valuable information for effective training, while relying solely on offline data can cause overfitting. Thus, the combination of selection and bootstrapping is crucial. Across iterations, this combination consistently outperforms"}, {"title": "5.2 ANALYSIS ON ENSEMBLE ACROSS DIFFERENT ITERATIONS", "content": "We also investigate the efficacy of the ensemble across different iterations, reporting the performance and similarity of selected samples for the TinyLlama case, when combining IRM in the second and third iterations. Although we see high winning rate in Figure 6a, a moderate range of ensemble coefficients \\(\\gamma\\) (i.e., 0.1-0.5) leads to greater performance improvements compared to higher coefficients (i.e., 0.7-0.9). This suggests that the reward margin generated by the latter policy models play a more significant role. However, employing an ensemble of reward margins can effectively mitigate the deterioration caused by undesirable model bias. Additionally, Figure 6b examines the similarity of selected samples across different \\(\\gamma\\) values. The results show that mild levels of \\(\\gamma\\) only slightly alter the selected samples, with similarity scores ranging from 0.88 to 0.94. This slight modification is crucial as it ensures the retention of important samples while preventing undesired bias in the model."}, {"title": "5.3 SELECTION BEHAVIOR", "content": "We checked the difference in the selected sample set with different training configurations such as DAAs, iterations, and LLMs. Figure 7a and Figure 7b depict the Jaccard similarity (Murphy,"}, {"title": "6 CONCLUSION", "content": "In this work, we proposed SeRA that incorporates two components into Direct Alignment Algorithms (DAAs)- (1) an implicit reward margin-based sample selection of offline datasets mitigates over-optimization, supported by empirical evidence; and (2) a self-reviewed preference bootstrapping to enable direct feedback that considers the training-inference mismatch. In addition, we use a reward ensemble over the updated policy models across iterations; this helps to develop a robust reward metric and in-turn a higher-quality on-policy dataset (although this increases the memory costs). Extensive experiments on diverse training configurations, across various LLMs, DAAs, and datasets, demonstrated the superior performance of SeRA. This highlights that even without additional supervision from humans or stronger LLMs, we can (1) reap further benefits from existing offline preference datasets and (2) obtain labels for on-policy data using the policy model's metrics (e.g. reward margins) to improve the policy model. Similar to Burns et al. (2023), our work demonstrates the possibility for self-evolution of LLMs to achieve better alignment with weak or minimal external supervision."}, {"title": "A PROOF FOR THEORETICAL INTUITION", "content": "Here, we provide the derivation of our theoretical results described in Theorem 1 that deepens the mathematical understanding on how our reward margin based sample selection can prevent the over-optimization.\nConsider a sample s defined as s = (x, yw, yl) where x is a given prompt/input, and (YwYl) are two outputs where one is preferred over the other yw > yi. We can now define a margin as f := r(x,yw) \u2013 r(x, yl) that encodes the difference is reward r assigned to the two output trajectories. We can now define a risk function R(f) over this margin by leveraging a loss function l over f as follows:\n\\[R(f) := Es~p [l(f(s))] := Es [p*(s)\u012bl(f(s))], \\quad(10)\\]\nAs the true preference distribution IP is unknown, we can approximate Equation 10 via the empirical risk over sample a N-sized set of samples S ~ PN as,\n\\[R(f; S) := \\frac{1}{N} \\sum_{n=1}^{N} l(f(s_n)), \\quad(11)\\]\nWe can also represent Equation 10 as the Bayes-distilled risk over the sample set S ~ PN as,\n\\[R(f; S) := \\frac{1}{N} \\sum_{n=1}^{N} p*(s_n)\u012bl(f(s_n)) \\quad(12)\\]\nNow, both the standard empirical risk R(f; S) (in Equation 11) and Bayes-distilled risk \u0125*(f; S) (in Equation 12) are unbiased estimates or the population risk R(f). First, we show that the Bayes-distilled risk has lower variance, compared to its empirical risk counterpart.\nLemma A.1 (Menon et al. 2021). For any fixed predictor f : S \u2192 R,\n\\[V_{S\\sim P_N} [R+(f; S)] \\leq V_{S\\sim P_N} [R(f; S)],\\]\nwhere V denotes variance, and equality holds if and only if \u2200s \u2208 S, the loss values l(f(s)) are constant on the support of p* (s).\nProof. The detailed proof is illustrated in Lemma 1 of Menon et al. (2021).\nGiven Lemma A.1, we note that the Bayes-distilled risk is more effective estimator that the standard empirical risk due to its small variance. However, since the value of p*(x) is unknown, we can approximate p*(x) with p(x), which is the estimated probability produced by the model. Additionally, we approximate Equation 12 using the distilled risk over a sample S ~ PN as,\n\\[R(f; S) := \\frac{1}{N} \\sum_{n=1}^{N} p(s_n)l(f(s_n))\\]\nNow, we provide the mathematical intuition that IRM-based sample selection can be effective by suggesting the upper bound of the distilled risk, computed by (\u00b7). For this, we first definte two terms.\nDefinition 1 (Covering Number). Let (A, d) be a metric space where A is a set of points and d is a distance measure. A set C is an e-cover of A if \u2200x \u2208 A, \u2203y \u2208 C such that d(x,y) < \u20ac. The covering number N(\u20ac, A, d) is defined as size of the smallest e-cover:\n\\[N(\u20ac, A, d) = min{|C| s.t. C is an e-cover}\\]"}, {"title": "B EXPERIMENTAL SETUP", "content": "Here, we elaborate the detailed experimental setup regarding the datasets used (\u00a7B.1), training details (\u00a7B.2), and evaluation details (\u00a7B.3).\nB.1 DATASET DESCRIPTION\nWe apply SeRA on preference datasets and instruction-following datasets. We provide detailed descriptions of the datasets used."}, {"title": "C ADDITIONAL RESULTS", "content": "C.1 COMPARISON WITH SELF-REWARDING LMS\nVersatility Comparison. We provide the LLM-as-a-Judge prompt, which is introduced in Yuan et al. (2024), in Fig. 10. As shown in Tab. 5, when LLMs do not have sufficient capacity to follow the instructions, it is difficult to exploit the self-rewarding mechanism introduced in Yuan et al. (2024). For example, TinyLlama-1.1B (Zhang et al., 2024) returned meaningless results, which cannot be properly used for rating the responses. On the other hand, Mistral-7B (Jiang et al., 2023) might"}]}