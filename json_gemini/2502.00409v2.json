{"title": "Doing More with Less \u2013 Implementing Routing Strategies in Large Language Model-Based Systems: An Extended Survey", "authors": ["Clovis Varangot-Reille", "Christophe Bouvard", "Antoine Gourru", "Mathieu Ciancone", "Marion Schaeffer", "Fran\u00e7ois Jacquenet"], "abstract": "Large Language Models (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component (e.g., conversational agents), are typically monolithic static architectures that rely on a single LLM for all user queries. However, they often require different preprocessing strategies, levels of reasoning, or knowledge. Generalist LLMs (e.g. GPT-4) trained on very large multi-topic corpora can perform well in a variety of tasks. They require significant financial, energy, and hardware resources that may not be justified for basic tasks. This implies potentially investing in unnecessary costs for a given query. To overcome this problem, a routing mechanism routes user queries to the most suitable components, such as smaller LLMs or experts in specific topics. This approach may improve response quality while minimising costs. Routing can be expanded to other components of the conversational agent architecture, such as the selection of optimal embedding strategies. This paper explores key considerations for integrating routing into LLM-based systems, focusing on resource management, cost definition, and strategy selection. Our main contributions include a formalisation of the problem, a novel taxonomy of existing approaches emphasising relevance and resource efficiency, and a comparative analysis of these strategies in relation to industry practices. Finally, we identify critical challenges and directions for future research.", "sections": [{"title": "1. Background", "content": "In the domain of computer network, a router can be defined as \u201ca device that sends data to the appropriate parts of a computer network\u201d. In LLM-based systems, a router is a system component that routes an item (i.e. user query) to the most appropriate element from a pool of elements candidates to carry out a task.\nThis paper focuses on one of the most common applications of LLM-based systems: conversational agents (Dam et al., 2024). Conversational agents respond to user queries by simulating human conversation (Caldarini et al., 2022; C.-C. Lin et al., 2023). They process the user's query and provide an answer that is relevant, given some metrics, to it (Caldarini et al., 2022). The Retrieval-Augmented Generation (RAG) architecture further improves the relevance of the response by adding an information retrieval step to the process (Gao et al., 2023; P. Lewis et al., 2020). This step involves retrieving the information most relevant to the user query from a knowledge database and including it into the prompt alongside the user query (see Figure 1 for a RAG architecture schema).\nWe can adapt all RAG steps to the user's query by routing it to the most appropriate element at each step (Figure 1). A router allows the whole system to dynamically adapt to different input types (C. Wang et al., 2024).\nGiven a set of n models M = {M1, ..., Mn}, for a given query q, the router function R aims to maximise the scoring function s (e.g. accuracy) while adhering to a budget constraint B:\nRM(q) = arg max s(q, M)\nMEM\ns.t. CM(q) \u2264 B\nwhere CM is the cost (i.e, $/token) to call the model M for a query q and B is the user's budget. The budget could be the amount of resources available.\nA straightforward optimisation of conversational agent architectures is the selection of the pre-trained LLM which has the ability to answer the user's query. Various LLMs exist"}, {"title": "2. Which elements should be optimised for routing?", "content": "The primary objectives of routing are to minimise unnecessary resource consumption while maximising performance by using a model or element appropriate to the task. In other words, it seeks to optimise the performance - cost trade-off."}, {"title": "2.1 A cost to minimise", "content": "Most existing LLM-based systems depend on API calls to closed models, such as those provided by OpenAI. The primary cost to minimise for a router is the price per token. To calculate the total cost of running the pipeline, it is essential to consider all invoked pre-trained models, including LLMs and embedding models in RAG settings.\nOther production-related costs, such as average latency and computational costs, can also be considered Irugalbandara et al., 2024. Latency is measured with respect to the delay between the user's request and the pipeline response. Effective routing could reduce the average latency by routing simple user queries, such as greetings, to LLMs that require fewer computing resources.\nAs LLM increases in use and size, the power and computing requirements increase significantly Luccioni et al., 2024, increasing the ecological impacts associated with LLM-based systems. This impact is often measured considering the energy consumption (kWh) or the global warming potential (kgCO2eq) of the application. Various tools\u00b3 have been proposed to estimate this environmental footprint."}, {"title": "2.2 A performance metric to maximise", "content": "The router function is also to maximise a scoring function that evaluates the model ability to produce accurate answers, as explained in equation 1.\nThere are several scoring function possibilities to maximise. In a traditional supervised learning framework, the evaluation process involves comparing these generated answers with the ground truth. In cases where the data lack ground truth or annotation, including a human evaluator in the evaluation loop allows us to assess whether the response is factually correct, in the expected format and consistent with the expected ground truth Chang et al., 2024. However, evaluating thousands of queries can require significant time and intense human involvement, affecting scalability. In addition, subjective bias might appear while evaluating, such as lack of expertise or preferences. More straightforward strategies include exact matching, partial matching, ROUGE score C.-Y. Lin, 2004, or even semantic similarity to the ground truth Chang et al., 2024; L. Zhang et al., 2024. These metrics may not adequately assess the factual accuracy of the generated content. As a result, previous studies have investigated automatic evaluation methods involving an LLM, where instructions and rating criteria are provided within the prompt along with the generated response Chiang & Lee, 2023. Their alignment with human evaluation remains uncertain, and factors such as instructions and sentence structure play an important role in the generated rating H. Wei et al., 2024. Many frameworks have been proposed, such as Retrieval Augmented Generation Assessment, also known as RAGAS Es et al., 2024. Preference learning, based on the preferences of human reviewers, can be used to assess the quality of responses in addition to traditional methods R. Jiang et al., 2024. Data related to preferences are typically represented as i < j, indicating that for a given query x, the user prefers the generated answer i over the alternative A; F\u00fcrnkranz & H\u00fcllermeier, 2012."}, {"title": "3. When should routing take place?", "content": "We propose two stages in the pipeline for the routing process: pre-generation routing (see Figure 2) and post-generation routing (also known as cascade routing) (see Figure 3). Pre-generation routing takes place before generating a response to the user query, while post-generation routing takes place after generating the response."}, {"title": "3.1 Routing as a Pre-generation Step", "content": "To implement pre-generation routing, we need to infer the ability of the LLM to answer a query a priori (see Figure 2). This method minimises latency by not waiting for the LLM response. There are two main approaches to achieve this: 1) infer the domain of knowledge of the query and route it to the associated LLMs trained as domain experts; and 2) assess the LLM candidate ability to answer a query of a given complexity and then route the query to the LLM with sufficient reasoning ability.\nIn this survey, we define the complexity of a query as the predicted performance score of the LLM for that specific query. The lower the score, the more complex the query. Within the context of RAG-based conversational agents, complexity can be categorised as follows: (a) low complexity user query may consist of a simple greeting that does not require retrieval; (b) intermediate complexity user query might involve extracting explicit information from a single document; (c) high complexity user query may necessitate the extraction of implicit information from multiple documents through reasoning."}, {"title": "3.2 Cascading: Routing as a Post-Generation Step", "content": "One might conceptualise routing as a post-generation step, where each model response is assessed iteratively (or in a cascade manner) by selecting progressively more advanced models until the response is considered pertinent (see Figure 3). In other words, the challenge is no longer to infer the ability of a potential LLM to meet a demand, but to assess the quality of the generated current response. By definition, this approach is less optimal than generating a single response with the pre-generation approach because, in some cases, several answers will be generated for the same query. This entails financial, computational, and latency costs.\nThe process can be enhanced by hybridising it with the pre-generation step (Dekoninck et al., 2024). We consider it as a multi-choice cascading method within the post-generation category.The evaluation of the response determines whether it should be routed to another LLM, similar to a post-generation approach. However, instead of routing the query to the next LLM in the predefined cascade sequence (as shown in Figure 3), the query can be routed to any available model within the set of LLMs at any stage of the cascade. This hybrid process outperforms simple sequential cascading or supervised pre-generation approaches (Dekoninck et al., 2024), but still requires multiple generations per query."}, {"title": "4. How routing should be implemented?", "content": "Figure 4 represents the different techniques discussed in this survey. The strategies are divided into high- and low-resource strategies. We define a high-resource strategy as one that (1) possibly generates multiple full-sequence responses to the same user query and/or (2) uses an LLM with an arbitrary threshold value chosen as a 1B parameter only for the routing process."}, {"title": "4.1 High-Resource Strategies", "content": ""}, {"title": "4.1.1 Supervised Routing", "content": "Answer confidence inference\nIn a cascading approach, at each iteration, a model, such as a fine-tuned DistilBERT (Sanh et al., 2020), classifies whether the generated answer aligns with the reference answer (L. Chen et al., 2023). L. Chen et al. (2023) proposed FrugalGPT4. This method infers the probability that the generated answer aligns with the reference answer using a DistilBERT regression model (Sanh et al., 2020), and compares this probability to an optimised threshold. The process of learning the threshold, denoted as t\u00bf for each model, is framed as a constrained optimisation problem. If the score exceeds the threshold, the response generated by Mi is retained; otherwise, a larger model, Mi+1, is called. They used 12 LLM APIs from 5 providers (OpenAI, AI21, Cohere, ForeFrontAI, Textsynth) with 10M input tokens cost ranging from $0.2 (Textsynth's GPT-J) to $30 (GPT-4). The authors reported that their framework could save between 59% and 98% of costs while maintaining similar accuracy to larger models, such as GPT-4. This strategy has often been used for comparisons, but it often underperforms against alternatives, such as LLM-"}, {"title": "4.1.2 Generative-based routing", "content": "This class of strategy uses a generative approach specifically for routing. Generative-based routing can potentially take advantage of LLM emergent capability for unsupervised multitasking to generalise to new contexts (Radford et al., 2019; J. Wei et al., 2022)."}, {"title": "Prompt-based routing", "content": "It can be used as either a pre-generation approach or as a post-generation approach.\nIn a pre-generation approach, the simplest method involves using a pre-trained LLM with prompt-based routing, also known as \"function calling\". This process entails passing descriptions of the routing options, with or without examples, in the prompt along with the user query. We route to the option returned by the LLM (Ning et al., 2024; Shen et al., 2023). Although it leverages an LLM, it is more energy-efficient and requires fewer resources than fine-tuning an LLM.Ning et al. (2024)6 prompt GPT-4 to determine whether a query possesses the necessary characteristics to be addressed by their prompt technique (SoT). Despite using a large LLM, the absence of examples affects inference. Furthermore, this strategy did not surpass a small language model, RoBERTa (Zhuang et al., 2021), specifically trained for task classification. Similarly, Shen et al. (2023) proposed HuggingGPT7, a framework for task planning and execution (Shen et al., 2023). An LLM is prompted to select between different models based on their descriptions.\nThis approach, despite its limitations, offers the advantage of performing inference with minimal examples in the prompt, known as \u201cfew-shot inference\u201d, or without any examples at all, known as \u201czero-shot inference\". This is particularly useful when limited resources are available for annotation.\nIn a post-generation approach, the LLM can be prompted to express its uncertainty when responding to user queries (Z. Li et al., 2024). This is known as \u201cverbalised confidence\" (Xiong et al., 2024). Z. Li et al. (2024) proposed Self-Route to decide whether retrieving traditional chunks using a RAG strategy is sufficient. Otherwise, they suggest using the entire document from which the chunks were extracted. This is an example of how routing can improve another step of a LLM-based conversational agent. The model was instructed to indicate whether the query is answerable based on the retrieved chunks. They used the prompt: \" Write 'unanswerable\u2019\" if the query cannot be answered based on the text. They ran a new generation with a larger context if the question was considered unanswerable. Their findings reveal that implementing this strategy outperformed a naive RAG system in terms of accuracy for the most commonly used LLMs (i.e. GPT-4o and GPT-3.5-turbo). For GPT-3.5, which has a smaller context window (16k), it also outperformed sending large contexts. In contrast, performance between Self-Route and sending a large context proved comparable for GPT-4o which has a larger context window (128k) while using an average of 61% fewer tokens. Finally, for gemini-1.5-pro, which features a context window of 1M, sending only large contexts appears to perform better than Self-Route, although the latter's performance remains reasonably close. The results for gemini-1.5-pro stem from its large context window, which allows the transmission of extensive contexts without truncation. Given these results, it may be tempting to send only large contexts. The authors show how it is possible to save costs while achieving comparable performance with fewer tokens.\nHowever, it has been reported that LLMs are often too confident in expressing their certainty (Xiong et al., 2024), suggesting caution about the reported effectiveness of this method. Furthermore, verbal confidence has been shown to be, in the best case, comparable to random routing to a larger LLM and could even perform worse (Chuang et al., 2024)."}, {"title": "Sequence probability", "content": "C.-H. Lee et al. (2024) proposed using the normalised sequence-level probability of a smaller LLM when determining whether to route a query to a larger LLM. The routing is based on the sequence uncertainty. However, this approach tends to rely too heavily on the larger LLM, resulting in an efficacy comparable to that of the larger LLM alone. Consequently, this approach is less effective in reducing call frequency to the larger LLM than the supervised methods discussed earlier."}, {"title": "LLM fine-tuning", "content": "If numerous resources are available, an LLM can be fine-tuned for routing tasks like classification or regression task (Liu et al., 2024; Ong et al., 2024), or for code generation to use providers API (Patil et al., 2024). Routing can be achieved by fine-tuning an LLM to add new routing-related tokens to its vocabulary: uncertainty-related tokens (Chuang et al., 2024) or token identifiers for domain experts (Chai et al., 2024). The LLM can also be fine-tuned for domain classification (Liu et al., 2024), but adding new domains will require a retraining of the model.\nLiu et al. (2024) suggested using Qwen1.5-1.8B-Chat (Bai et al., 2023), which they fine-tuned for a domain-classification tasks. This meta-model categorises the prompt, and the corresponding pre-trained expert associated with that category then generates a response. Similarly, Ong et al. (2024) fine-tuned a Llama-3-8B model (Grattafiori et al., 2024) on a scoring task designed to evaluate both the complexity of a query and the model's ability to answer it through LLM evaluation (Ong et al., 2024). This score is subsequently converted into the probability that the user has a preference for the larger model answer. While Ong et al. (2024) found that this strategy is not significantly more efficient regarding cost or quality compared to other non-LLM techniques, such as matrix factorisation, Liu et al. (2024) demonstrated a marked improvement in performance across all datasets through supervised fine-tuning. The accuracy increased from 15% to nearly 100% for the"}, {"title": "Repeated calls", "content": "In a post-generation approach, we could route to the next model whenever the uncertainty of the LLM for its answer is high. Smaller LLMs tend to give consistent answers to simple questions but show inconsistencies when confronted with more complex questions (Yue et al., 2024). Thus, the uncertainty of the model can be assessed by repeatedly querying the LLM with the same prompt, using temperatures ranging from 0.4 (Yue et al., 2024) to 1 (Aggarwal et al., 2024). By analysing the consistency of the responses, researchers can assess the level of uncertainty (Aggarwal et al., 2024; Yue et al., 2024). Aggarwal et al. (2024) introduced a three-step approach called Automix10 (Aggarwal et al., 2024). The models used by the authors were GPT-3.5, Llama-2-13B (Touvron et al., 2023), and Mistral-7B-Instruct-v0.2 (A. Q. Jiang et al., 2023), as the smaller language models, and GPT-4 as the larger language model. Given a set of N unique models with increasing size M = {M1, ..., MN}: (1) using the smaller model M; (j=1) to generate an answer Ai,j to a query qi based on a related context Ci; (2) initiating a self-verification process with the same model M\u2081 with a few-shot meta-prompt (verification prompt) to determine whether Aij aligns with the provided context Ci. To estimate a confidence score of M; aligning with Ci, they produced Ai,j k times (Aj, k > 1) at a high temperature and calculated the proportion of A aligning with Ci; (3) based on this confidence score, the router either retained the current answer A\u00bf or call a larger model M; (j>i). This procedure repeats until the confidence score reaches a satisfactory level or the entire set of LLM M series has been tested. The authors employed two routing strategies: a more complex one based on a Partially Observable Markov Decision Process (POMDP) and a simpler one grounded in a confidence cost/quality trade-off threshold.\nOn the other hand, Yue et al. (2024) did not assess the correctness of the answers finding this concept too challenging to evaluate. Instead, they scored the responses based on the ratio of identical answers among k samples for the same query, which reflects an-swer consistency\u00b9\u00b9. They assessed response consistency across different representations of thought by comparing responses generated by Chain-of-Thought (CoT) and Programme-of-Thought (PoT) prompts, with further details provided in Appendix A. The Mixture-of-Thoughts (MoT) representation employs both prompting strategies by mixing samples from CoT and PoT. For each query, they generated k samples for each prompting ap-proach (PoT, CoT, or MoT). The algorithm then calculates a consistency score, which is defined as the proportion of identical answers among k samples).\nThe authors reported that Automix (Aggarwal et al., 2024) achieves a higher F1 score on the QASPER and COQA datasets and superior accuracy across a range of costs compared to HybridLLM (Ding et al., 2024), FrugalGPT, (L. Chen et al., 2023) or stand-alone models such as GPT-4 and Llama-2-13B (Touvron et al., 2023), particularly the routing based on the POMDP approach. Even in low-resource scenarios with a small training dataset size, their method significantly outperforms both HybridLLM (Ding et al., 2024) and FrugalGPT (L. Chen et al., 2023). These findings are supported by Yue et al. (2024), who demonstrated that including various thought-based reasoning (MoT) when deciding whether to call a larger LLM resulted in similar accuracy at lower cost compared to assessing consistency between CoT or PoT samples alone (Yue et al., 2024). This underlines the need for diversity and complementarity between routing options. In both studies, users have to call an LLM multiple times for each query, and despite their efficiency, these approaches prove to be resource-intensive."}, {"title": "Code execution", "content": "This strategy applies exclusively to code generation tasks. It directly evaluates the confidence in the answer by executing the generated code. EcoAssistant12, an iterative multi-agent code generator designed to query external knowledge for question and answering, has been proposed by J. Zhang et al. (2023). In this generator, an LLM interacts with a code executor to generate and execute code. If the code generated by the smaller LLMs does not succeed, the request will be forwarded to a larger LLM. Here, success, rather than uncertainty, serves to guide routing. To determine the correctness of the generated code for a given query, the authors implemented an evaluation using GPT-4 alongside the success of code execution. They used two architectures: GPT-3.5-turbo + GPT-4 and Llama-2-13B-chat (Touvron et al., 2023) + GPT-3.5-turbo + GPT-4. The authors reported that their framework generated more successful code snippets at a lower cost than using GPT-4. Although more expensive than using GPT-3.5-turbo, EcoAssistant significantly exceeded the percentage of successful code generation..\nIn all the high-resources routing approaches discussed, the savings from the routing process may be outweighed by the implementation requirements of high-resource routing strategies."}, {"title": "4.2 Low-Resource Strategies", "content": "The drawbacks of the routing strategies presented in the previous section can be addressed using low-resource strategies. They attempt to avoid such resource requirements while maintaining routing performance. Most of them are pre-generation, except the last one."}, {"title": "4.2.1 Similarity-based Routing", "content": "Query similarity\nThe primary concept behind these approaches is to route a user's query to the LLM that has the best performance on similar queries answered in previous interactions (e.g. cosine similarity).\nIn its simplest form, the approach routes the user's query to the elements that have successfully responded to the n most similar previous queries, with the assumption that similar queries require similar processing. Stripelis et al. (2024) propose a 1NN router that routes the user query to the LLM identified as producing the most appropriate response for the most similar query in the training data, based on cosine similarity. However, this strategy fails to capture complex relationships between user queries and expert an-swers, and performs worse than randomly selecting from available models. Manias et al. (2024) suggests Semantic-Router, a framework developed by Aurelio AI13, which uses n examples. Semantic-Router detects intentions by routing the user's requests to an inten-tion that is associated with n similar previous requests. The authors demonstrated that Semantic-Router performed similarly to prompt-based intent detection, achieving around 90% accuracy without requiring LLM inference.\nSimilarly, Jang et al. (2023) proposes a method that routes to different expert adapters based on the training tasks that are most similar to the user query. Each training task is embedded and associated with one or more expert IDs. During the inference process, they identify the most similar training tasks and the most frequently associated expert IDs from the training tasks most similar to the user's query. They showed improved per-formance on non-generative tasks by using a smaller language model (t5-3B (Raffel et al., 2020)) compared to a fine-tuned multi-task t0-3B model (Sanh et al., 2022). However, their framework showed inferior performance on generative tasks.\nMalekpour et al. (2024) applied this approach within a text-to-SQL framework by ex-amining previous successes among similar SQL queries. They proposed selecting the cheapest LLM that yields a higher proportion of SQL generation matching the ground truth among the first n most similar SQL queries. To ensure minimal performance, the authors implemented a threshold that represents the minimum proportion of quer-ies a LLM must success to be considered. They utilised three LLMs as routing options: gpt-4o, gpt-4o-mini, and a quantised version of Llama3.1-8B-instruct (Grattafiori et al., 2024). The authors demonstrated that they could achieve performance close to that of the best stand-alone model for this task (gpt-4o)\u201460.1% compared to 61.0% at a lower cost (1.1 times cheaper). However, it is important to emphasise that most of the time, the strategy routed queries to the best LLM (gpt-4o) -81%. A routing strategy must not only minimise costs while maximising quality, but also avoid achieving higher quality by exclusively routing to the larger LLM. In addition, the choice of LLM did not include a specialist code generation LLM, which could potentially have led to a cheaper option with similar performance. Despite these limitations, employing similarity retrieval based on previous successes or utterances effectively reduces costs while maintaining performance. These approaches require minimal resources.\nThe query representation can be improved by optimising the encoding process using contrastive learning. The optimisation ensures that similar samples are close together in the multidimensional semantic space, while dissimilar samples are far apart (Le-Khac et al., 2020).\nC.-H. Lee et al. (2024) applied it to Dialogue State Tracking (DST) by fine-tuning a SenBERT-based bi-encoder (Reimers & Gurevych, 2019) within a framework called OrchestraLLM. DST is a task whose objective is to extract the user's intention and dialogue-related information in a structured representation (see Annex A for an example). Some dialogues may be too complex to be handled with a smaller LLM, and the conversation processing may therefore require a larger LLM. The authors assigned a series of utter-ances to either the smallest or largest LLM, depending on which model could accurately generate the correct dialogue state representation. In each iteration, they selected the appropriate LLM using a K-NN algorithm applied to the embedding of the new instance. The majority vote from the k closest neighbours determined which LLM had been most successful in similar past examples. OrchestraLLM shows a slight improvement in accur-acy compared to a direct call to the larger LLM, while also reducing the number of calls to this model by 50% to 80%. Optimising the representation strategy with the contrastive loss improves the assignment ratio to a smaller model by 8 points. Alternatively, S. Chen et al. (2024) proposes a framework called RouterDC, which improves user query repres-entation through multi-level contrastive learning. They map user queries into a shared space, learning embeddings of both the LLM representation and the query. This multi-level loss consists of two parts. The first is the contrastive loss between the query and the LLM to evaluate positive/negative LLM sets based on their performance compared to the query. The second is the contrastive loss between similar queries to ensure they are located closer together in this space. Subsequently, they generate a selection probab-ility distribution over the set of LLMs by applying a softmax function to the similarity between the user query and the learnable LLM representation embeddings. During infer-ence, it selects the LLM that maximises this similarity probability. The options include a mix of generalist and expert LLMs: Mistral-7B (A. Q. Jiang et al., 2023), MetaMath-Mistral-7B (Yu et al., 2024), zephyr-7b-beta (Tunstall et al., 2024), Chinese-Mistral-7B, dolphin-2.6-mistral-7b, Llama-3-8B (Grattafiori et al., 2024), and dolphin-2.9-llama3-8b. Additionally, RouterDC is compared to Zooter, a strategy described in section 4.2.3 (Lu et al., 2024), which employs a Reward-based inference approach. It is also evalu-ated against a majority voting method, a multiclass classification model, and a clustering technique for embeddings. Except for the MMLU dataset, RouterDC outperforms the best stand-alone LLMs and performs as well as or better than other routing methods on different datasets. It achieves the highest average performance across out-of-distribution datasets by effectively approximating the results of the best performing models on each dataset."}, {"title": "Clustering previous interactions", "content": "Incorporating unsupervised learning can enhance similarity-based routing by identifying the relevant cluster associated with the user query. Once the closest cluster is determined, the query is directed to the LLM that has exhibited the best performance in previous in-teractions within that cluster. The two studies that proposed cluster-based routing use a k-means algorithm. Pichlmeier et al. (2024) proposed the Expert Router framework, which employs TF-IDF encoding reduced to 100 dimensions through singular value decompos-ition. Using k-means is more energy efficient than using an LLM, but the researchers didn't train it from scratch. Instead, they used a pre-trained k-means model trained on the C4 dataset (Raffel et al., 2020) - a 300GB cleaned version of the Common Crawl web crawling corpus (Gururangan et al., 2023). While Pichlmeier et al. (2024) do not describe how they assign an LLM to each cluster, Srivatsa et al. (2024) evaluate which LLM has the most frequently generated answers that match the ground truth for each cluster de-rived from the training data set. The authors evaluated the framework's feasibility rather than its performance by testing latency, response time, and session throughput. They demonstrated that the infrastructure remains robust under high-load scenarios. Their work underscores the potential necessity of training on large datasets to enable smaller algorithms to effectively capture the information contained in queries.\nWhen Srivatsa et al. (2024) trained the K-means algorithm from scratch for the task, the clusters did not generalise effectively from the training dataset to the test dataset. The size and diversity of the training data set appear to be key factors when training clustering methods for routing. The authors did not observe any impact on the results from altering the encoding strategy, whether using a dense representation strategy with RoBERTa (Zhuang et al., 2021) or a sparse strategy with TF-IDF. Another significant point to highlight in this latest study is the selection of LLM for routing: the researchers did not compare a smaller LLM with a substantially larger one, using only smaller models such as gemma-7b and gemma-7b-it (Gemma-Team, Mesnard et al., 2024), metamath-7b (Yu et al., 2024), mistral-7b-it (A. Q. Jiang et al., 2023), llama-2-13B-chat and llama-2-76 (Touvron et al., 2023). The study did not involve topic experts, such as those in mathem-atical tasks, and relied on non-specialist models. Consequently, the clusters formed lacked sufficient distinctiveness to differentiate between the LLMs; often, the LLM assigned to a cluster was also the best-performing model across the dataset. Therefore, selecting com-plementary models based on topic or parameter values is essential for effective routing in the context of LLMs."}, {"title": "Preference similarity", "content": "User preferences can improve similarity retrieval by incorporating additional information. By analysing which model was favoured for similar queries", "RouteLLM": "a range of routing strategies based on user preferences15 16. They propose to reformulate the rout-ing problem between a smaller, Mixtral-8x7B, versus a larger LLM,GPT-4-1106-preview, as a binary classification task. This task involves predicting the probability of the larger LLM being preferred for a specific query. To determine this probability, the researchers employ a Bradley-Terry (BT) algorithm (Bradley & Terry, 1952) for similarity-weighted ranking. They weight the queries from the training dataset according to their similarity to the user's query and subsequently use these weighted queries to learn the BT coefficients. These coefficients enable the estimation of the probability of preferring a larger LLM for a specific query. The model parameters are learned through maximum likelihood estimation based on the preference data. Once this probability is inferred, the appropriate route is selected according to a cost threshold (\u03b1 \u2208 [0, 1"}]}