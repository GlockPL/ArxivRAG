{"title": "Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD)", "authors": ["Avshalom Manevich", "Reut Tsarfaty"], "abstract": "Large Vision-Language Models (LVLMs) are an extension of Large Language Models (LLMs) that facilitate processing both image and text inputs, expanding AI capabilities. However, LVLMs struggle with object hallucinations due to their reliance on text cues and learned object co-occurrence biases. While most research quantifies these hallucinations, mitigation strategies are still lacking. Our study introduces a Language Contrastive Decoding (LCD) algorithm that adjusts LVLM outputs based on LLM distribution confidence levels, effectively reducing object hallucinations. We demonstrate the advantages of LCD in leading LVLMs, showing up to 4% improvement in POPE F1 scores and up to 36% reduction in CHAIR scores on the COCO validation set, while also improving captioning quality scores. Our method effectively improves LVLMs without needing complex post-processing or retraining, and is easily applicable to different models. Our findings highlight the potential of further exploration of LVLM-specific decoding algorithms for improved multimodal performance.", "sections": [{"title": "1 Introduction", "content": "Large Vision-Language Models (LVLMs) are a multimodal extension of Large Language Models (LLMs), transforming textual prompts and image inputs into text. However, they frequently produce object hallucinations, where absent objects are mentioned in the output (Li et al., 2023b; Lovenia et al., 2023).\nWhile hallucination-mitigation techniques in LLMs are actively researched, specific strategies for LVLMs are less developed. Current methods involve model-specific adjustments, additional training, or auxiliary models for post-hoc correction, and are often proven inefficient, costly, or limited by training data and model biases (Wang et al., 2023; Zhou et al., 2023; Gunjal et al., 2023; Yin et al., 2023). Conversely, LVLM hallucination evaluation has seen progress with object hallucination benchmarks like NOPE (Lovenia et al., 2023) and POPE (Li et al., 2023b), and recent works that aim for more holistic LVLM hallucination evaluation such as FaithScore (Jing et al., 2023) and HallusionBench (Guan et al., 2023).\nA key reason for LVLM hallucinations is their tendency to over-rely on linguistic information, as was first observed by Guan et al. (2023). Based on this insight, we propose to intervene in the LVLM decoding phase so that model outputs are less informed by language biases. Specifically, we propose to use Contrastive Decoding (Li et al., 2023a;"}, {"title": "2 Motivation and Background", "content": "The integration of vision capabilities into LLMs has led to the development of Large Vision-Language Models, merging LLMs' textual understanding with vision-text encoders. This trend towards multimodal systems is exemplified in commercial platforms such as GPT4-V (OpenAI et al., 2023) and Google's Gemini (Team et al., 2023).\nLarge Vision-Language Models combine LLMs and vision-text encoders to generate text from textual prompts and visual inputs. An LVLM generally comprises three main components: a vision-text encoder like CLIP (Radford et al., 2021), an LLM such as LLAMA (Touvron et al., 2023) or Flan-T5 (Chung et al., 2022), and a cross-modal alignment module linking the vision-text encoder output with the LLM.\nInitially, LVLMs were fine-tuned for specific tasks (Li et al., 2022; Wang et al., 2022). However, advancements in LLMs have led to a shift towards general-purpose, instruction-tuned LVLMs. These models are designed to handle a wide range of tasks based on instructions, making them more versatile. Despite these advancements, LVLMs grapple with hallucinations of different types.\nLVLMs Hallucinations and their Mitigation Hallucinations in LVLMs, particularly object hallucinations where nonexistent entities are mentioned, are often attributed to LVLMs' reliance on spurious correlations and language biases, as demonstrated by Li et al. (2023c) and Zhou et al. (2023). Moreover, Guan et al. (2023) highlight LVLMs' tendency to prioritize language over visual data, leading to hallucinations.\nMitigation strategies proposed by Gunjal et al. (2023) and Wang et al. (2023) involve further model training with augmented datasets or reward models. Zhou et al. (2023); Yin et al. (2023) developed auxiliary models to correct outputs post-generation. These solutions often require dataset-specific work or additional model training, potentially leading to overfitting or new biases, and are not easily transferable across LVLMs.\nIn a concurrent work, Leng et al. (2023) develop an LVLM-specific decoding algorithm for mitigating hallucinations, using a noisy copy of the input image as a contrastive input. While their approach uses visual noise to guide the decoding process, LCD leverages the language modality to mitigate hallucinations. These approaches are orthogonal and can potentially be combined into a unified Language-Visual contrastive decoding algorithm, a direction we leave for future work."}, {"title": "3 Language Contrastive Decoding (LCD)", "content": "Before presenting LCD, we briefly introduce the essentials of decoding in LVLMs 3.1, followed by our formal proposal 3.2 and research hypothesis 3.3."}, {"title": "3.1 Decoding Techniques and Contrastive Decoding: Essential Preliminaries", "content": "Decoding in auto-regressive generative models is the stage that transforms an input representation into a sequence of output tokens. In LVLMs, this process involves a model M, an image I, a textual prompt X, and a particular timestamp t during generation. It can be described as a series of selections from the model's probability distribution, producing a token sequence T, as formalized in eq. (1).\n$$T_t \\sim P(.|I, X, T_{<t}; M)$$\nGreedy decoding, selecting the most probable token at each step (or the top k tokens in a beam"}, {"title": "3.2 Proposed Method", "content": "Our intuition, based on previous findings by (Guan et al., 2023; Rohrbach et al., 2018; Li et al., 2023b), is that an LVLM can be \"misled\" by its constituent LLM during the generation process.\nConsider for example an LVLM that is describing an image (see illustration 1). Mid-generation, given the text \"An image of a man walking his,\" it may predict \"dog\" due to language biases, even if it is a bear that is actually shown. A \u2018plain' LLM, without seeing the image, reinforces these biases by highly rating \"dog\". Our method builds on this insight to guide an LVLM towards more accurate predictions using Contrastive Decoding.\nOur method operates as follows: At each generation step t, for each token x, we first determine the next-token probabilities from the LVLM, $P_{LVLM}$, based on the current token sequence $T_{<t}$, text X, and image I. We then obtain a second distribution, $P_{LLM}$, by inputting all data except the image into the LLM. The LLM's conditional entropy $H_{LLM}$ informs the dynamic weight as per eq.(4). We then adjust token x's logits using the LCD formula in eq. (5).\n$$\\beta_t = \\frac{\\beta}{H_{LLM}(x|X,T_{<t})}$$\n$$LCD_t(x,T_{<t}, I, P_{LVLM}, P_{LLM}) =$$\n$$(1 + \\beta_t) log P_{LVLM}(x|I, X,T_{<t})$$\n$$- \\beta_t log P_{LLM} (x|X,T_{<t})$$\nIn our experiments, we generate text completions by sampling from the next token probabilities, which are obtained by applying the softmax function to the logits produced by the LCD algorithm."}, {"title": "3.3 Research Hypothesis", "content": "Our hypothesis is that contrasting LVLM outputs with LLM outputs conditioned only on the textual data, can mitigate language biases, therefore reducing hallucinations in LVLMs."}, {"title": "4 Experiments and Results", "content": "We set out to assess the effect of LCD on object hallucinations in LVLM outputs against popular decoding settings. Additionally, we verify that LCD does not degrade output quality. To this end, we asses LCD on the POPE benchmark (Li et al., 2023b), and on an image detailed-description task where we report hallucination and captioning metrics and conduct a GPT4-V assisted evaluation.\nPolling-based Object-Probing Evaluation POPE consists of object-presence binary questions on 500 COCO dataset images (Lin et al., 2015), with questions equally divided between present and absent objects. Absent objects are chosen based on three criteria: random, popular (common in COCO), and adversarial (commonly co-occurring with present objects). POPE's drawback is its one-word response structure, which limits the"}, {"title": "Image Detailed-Descriptions", "content": "To complement POPE, we introduce a long-form text generation task called \"Image Detailed-Descriptions,\" inspired by findings from Zhou et al. (2023) that more extensive context increases the likelihood of hallucinations. In this task, the input consists of an image from the COCO dataset and a text prompt requesting a detailed description of the image. The expected output is a long-form, detailed textual description of the given image, typically containing multiple sentences. The prompts used in this task are detailed in appendix A.1. By using the same COCO images as POPE, we maintain consistency in the visual domain while exploring LCD's effectiveness in a more challenging setting where the model is required to generate longer, more descriptive outputs."}, {"title": "Baselines and Metrics", "content": "For POPE, we use sampling as the baseline and report F1 scores. For the detailed-descriptions task, we use as a baseline the popular nucleus sampling algorithm and report CHAIR metrics (Rohrbach et al., 2018). To assess description quality, we use captioning metrics against COCO's gold captions, which serve as an approximation considering length differences. Additionally, following Yin et al. (2023), we use GPT4-V to evaluate the descriptions for Detailedness and Accuracy (see details in Appendix A.1)."}, {"title": "Models", "content": "We conduct our experiments with leading LVLMs: two versions of the InstructBLIP model (with Flan-T5 and Vicuna LLMs), LLAVA 1.5 and mPLUG-Owl2. The complete experimental details, such as exact model variants and generation hyper-parameters, are given in the Appendix."}, {"title": "5 Results and Discussion", "content": "For the POPE task, which evaluates object hallucinations using binary questions, LCD improves F1 scores across 11 out of 12 configurations compared to the baseline (Table 2). This suggests that LCD is effective in reducing object hallucinations in the POPE setting. It is worth noting that the POPE setting is highly constrained for decoding algorithms, as it consists of binary yes/no questions, and typically involves only a single decoding step. This limits the potential impact of decoding strategies on the model's performance in this specific task.\nIn the detailed-description task, which involves generating detailed descriptions of images, LCD significantly reduces hallucinations at both sentence and instance levels across all four models tested (Table 1). However, it is important to note that despite the improvements, the CHAIR scores, which measure hallucination rates (lower is better), remain relatively high. This indicates that object hallucinations are still prevalent in long-form LVLM outputs, even with the application of LCD. We observe that LCD is particularly effective in improving the performance of InstructBLIP models (InstructBLIPF and InstructBLIPy). We hypothesize that this may be due to the fact that the LLMs in these models are frozen during training, which results in a stronger language bias that LCD can effectively mitigate. When evaluating the overall generation quality using captioning metrics (METEOR, WMD, and ROUGEL), LCD outperforms the baseline in all cases except one (WMD in LLAVA 1.5, where the reduction is approximately 1%). This indicates that LCD not only reduces hallucinations but also maintains or improves the overall quality of the generated descriptions.\nFurthermore, in the GPT4-V assisted evaluation, which assesses the accuracy and detailedness of the generated descriptions, LCD improves the accuracy scores across all models. Interestingly, the detailedness scores remain similar to the baseline, suggesting that LCD reduces hallucinations without increasing the granularity of the descriptions."}, {"title": "6 Conclusion", "content": "In this paper we present Language Contrastive Decoding, a novel method to reduce hallucinations in LVLMs. By dynamically adjusting output probabilities using the LVLM's internal LLM, LCD significantly improves hallucination metrics across different LVLM architectures, enhancing the quality and reliability of generated content without necessitating retraining or auxiliary models and post-processing. This work highlights the potential of specialized decoding strategies in enhancing multimodal AI models and lays the groundwork for further exploration into more sophisticated LVLM decoding methods."}, {"title": "7 Limitations", "content": "Firstly, while LCD shows promise in reducing hallucinations, it only targets hallucinations caused by language biases, but hallucinations can arise from other sources. For instance, previous work has shown that some hallucinations are caused by poor visual understanding (Guan et al., 2023). We believe LCD can be used as a platform to craft LVLM-specific decoding algorithms that would mitigate hallucinations stemming from different factors, and leave this pursuit for future work.\nSecondly, our evaluation method primarily addresses object hallucinations, which are only one form of hallucination that LVLMs may exhibit. Preliminary results signal that LCD mitigates more complex manifestations of language-induced hallucinations as assessed by recent benchmarks such as FAITHSCORE (Jing et al., 2023) and HallusionBench (Guan et al., 2023), but further work is required to establish this.\nMoreover, LCD relies on current LVLM architectures that combine an LLM and a text-vision encoder, and requires access to an LLM that emits output probabilities on the same set of tokens as the LVLM. It is possible that the future generation of multimodal AI systems will have a different architecture that will make LCD obsolete. Additionally, LCD requires an LLM forward pass for each LVLM decoding step. The added latency could be mitigated with efficient inference techniques, and also by using a smaller LLM as the contrasting model. The effectiveness of LCD in this scenario is left for future work.\nFinally, there are ethical considerations related to the mitigation of hallucinations in LVLMs. As these models become more reliable, it is crucial to continue evaluating the potential impacts of their use, ensuring they do not perpetuate or exacerbate biases present in their training data. LCD indeed mitigates some biases, but it is important to keep in mind that it might amplify other biases, unknown to us. Responsible deployment of these models requires ongoing vigilance and a commitment to transparency and fairness."}]}