{"title": "Fine-tuning multilingual language models in\nTwitter/X sentiment analysis: a study on\nEastern-European V4 languages", "authors": ["Tom\u00e1\u0161 Filip", "Martin Pavl\u00ed\u010dek", "Petr Sos\u00edk"], "abstract": "The aspect-based sentiment analysis (ABSA) is a standard NLP task with numer-\nous approaches and benchmarks, where large language models (LLM) represent\nthe current state-of-the-art. We focus on ABSA subtasks based on Twitter/X\ndata in underrepresented languages. On such narrow tasks, small tuned language\nmodels can often outperform universal large ones, providing available and cheap\nsolutions.\nWe fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for\nclassification of sentiment towards Russia and Ukraine in the context of the\nongoing military conflict. The training/testing dataset was obtained from the\nacademic API from Twitter/X during 2023, narrowed to the languages of the V4\ncountries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their\nperformance under a variety of settings including translations, sentiment targets,\nin-context learning and more, using GPT4 as a reference model. We document\nseveral interesting phenomena demonstrating, among others, that some models\nare much better fine-tunable on multilingual Twitter tasks than others, and that\nthey can reach the SOTA level with a very small training set. Finally we identify\ncombinations of settings providing the best results.", "sections": [{"title": "1 Introduction", "content": "Aspect-based sentiment analysis (ABSA) (Nazir et al., 2020) includes a collection of\nmethods extracting sentiment towards a specific aspect associated with a given target\nin a message. Usual sources are text documents of various lengths, although other\nmedia (speech, video etc.) are included in multimodal sentiment analysis (Kaur and\nKautish, 2022). Methods of automated sentiment identification can be classified into\ninto three major categories: knowledge-based, machine learning, and hybrid models\n(Brauwers and Frasincar, 2022).\nWhile machine learning methods, especially attention-based models as transform-\ners, are considered the current state-of-the-art\u00b9 on sufficiently large training datasets\n(see, e.g. (Scaria et al., 2023)), the lack of training data (especially in languages other\nthan English) can decrease their efficiency significantly. In these cases, either simpler\nmachine-learning methods such as SVQ's, or hybrid methods augmenting machine-\nlearning models with knowledge bases and structures can be the choice. Another\nsolution may be a transfer of knowledge from other languages where sufficiently large\ndatabases exist (Barbieri et al., 2022).\nABSA can be described in three phases (Brauwers and Frasincar, 2022): aspect\ndetection/extraction, sentiment classification, and sentiment aggregation. In this paper\nwe use Twitter/X as the main (and rapid) source of public sentiment data, where the\naspect detection is usually simple due to their restricted length and the possibility to\nfollow re-tweets forming a thread dealing with a certain aspect/target. The sentiment\nclassification, however, can be challenging: when tweets are processed alone, we see\na lack of context, all the more so since the tweets relate to contemporary topics and\nassume the reader's knowledge of the current context. Therefore, even large universal\nmodels as GPT4 struggle with their specific style, and smaller fine-tuned models often\nperform better, especially in the case of non-English datasets. This fact is confirmed\nin several benchmarks and also in our experiments.\nWe focus on the analysis of sentiment towards the mostly publicised world event\nof the last few years: the Ukraine war crisis. The conflict even before it's kinetic\nbeginning was waged in cyberspace with methods of soft (information and narrative\ndissemination on social networks) and hard (ransomware, theft, DDoS) cyber attacks\n(Ducheine et al., 2022). Hence, a large online textual corpus is available for training\nand testing. There exist numerous studies devoted to ABSA in tweets, including those\nfocused on the recent Russo-Ukraine conflict. Nevertheless, we are not aware of any\nLLM-based sentiment analysis on this topic in Eastern European languages (the V4\ngroup), although these countries form the previous Russian sphere of influence which\nis now openly claimed back by Kremlin, and are subject of intensive cyber-bullying\nattacks from Russia. Neither we have met a comparison of performance of LLama and\nMistral models to the BERT-derived ones on the task of multilingual tweet analysis\nsimilar to ours.\nOur experimental pipeline started by downloading data from Twitter/X academic\nAPI during 2023, filtered by keywords and restricted to the V4 languages (Czech\nRepublic, Slovakia, Poland, Hungary). The obtained language-specific datasets were"}, {"title": "3 Methods", "content": "Data were collected using the academic Twitter/X API during the period 4/2/2023\nto 20/5/2023. We set the filters as follows: (i) languages \u2013 Czech, Slovak, Polish,\nHungarian; (ii) keywords Ukraine, Russia, Zelensky, Putin representing the topic we\nwanted to focus on. The total number of collected tweets was 34124.\nThe dataset was then split into three subsets based on the original language\nCzech/Slovak (CS), Polish (PL) and Hungarian (HU). We could not create a separate\nSlovak dataset as there was no filter available for Slovak so it was mixed with Czech,\nand an automated Google language recognition we tried was unable to distinguish\nreliably between these two close languages.\nFrom every language-specific subset we randomly chose a certain part for annotation.\nTweets were manually annotated due to their sentiment (1-negative, 2-neutral, 3-\npositive) towards Ukraine or Russia. The annotation intended to provide roughly equal\nnumber of tweets in all categories in each language and it did not reflect the overall\nratio of positive, negative and neutral tweets in that language. These ratios varied, e.g.,\nthe Hungarian subset contained lower ratio of positive tweets towards Ukraine than\nthe others. We tried also machine labelling using the pre-trained model GPT4 but the\nresults were inconclusive. Each annotated subset was further split into a training part\n(75%) and testing part (25%). The sizes of all annotated sets are listed in Table 2.\nThe annotated datasets were used for both training and testing in three different\nlanguage modes:\n\u2022 translated to English using the Helsinki Neural Machine Translation System\u00b2;\n\u2022 translated to English using the DeepL API\u00b3;\n\u2022 no translation, original languages (CS/SK, PL, HU).\nThe following table lists the models used in our experiments. Furthermore, GPT4\n(Achiam et al., 2023) was used as a reference model for tweet classification. As fine\ntuning was not available at the time of the experiments, instead we used in-context\nlearning described bellow."}, {"title": "4 Results", "content": "We conducted an extensive series of tweet sentiment classification experiments that\nvaried in the following settings:\n\u2022 sentiment aspect (Russia/Ukraine)\n\u2022 language of the tweet (CS/SK, HU, PL)\n\u2022 language model (BERT, BERTweet, Llama2, Llama3, Mistral, GPT4)\n\u2022 tweet translation (DeepL, Helsinki translator, none)\n\u2022 positive/neutral/negative classification, or only positive/negative\n\u2022 the presence of a reference tweet (to which the classified tweet reacted)\nThe goal was to study the influence of individual settings (and their combinations)\non the classification performance. We used standard metrics: accuracy and macro-\naveraged recall, precision and F1 score (Rainio et al., 2024). Unless stated otherwise,\nwe used three-valued (positive/neutral/negative) sentiment classification. After a few\ninitial tests, all experiments were run without in-context learning, with the exception of\nthe GPT4. Table 3 visualised at Fig. 2 summarises main results organised by language\nmodels and the type of translation.\nthree-valued sentiment classification, closely followed by BERTweet with recall 73.4.\nOur best setting (LLama2, translation by DeepL, averaged over all aspects and lan-\nguages) gave both recall and F1-score 72.8 (73.8 without the use of reference tweets).\nOur task is topically much narrower than TweetEVAL, on the one hand. On the other\nhand, TweetEVAl is monolingual and BERTweet was trained on 850M English tweets,\nwhile we fine-tuned our models using three underrepresented languages with small\nfine-tuning datasets (about 6K tweets in total).\nConsidering further the UMSAB multilingual Twitter benchmark\u00b9\u00b9 (Barbieri et al.,\n2022), the best reported model is XLM-Tw Multi with F1-score 69.4, macro-averaged\nover eight languages. Again, this task is wider than ours but XLM-Tw Multi used much\nlarger fine-tuning dataset (198M multilingual tweets), therefore we cannot provide an\nexact comparison which may be the subject of future work.\nOur best performing models were Llama2 and Mistral with almost equal average F1-\nscores. Surprisingly, Llama3 scored by approx. 6% F1 worse than Llama2. It is all\nthe more surprising since Llama3 was aware of the contemporary context (actions\nand names of politicians, related regional events etc.) which is essential to under-\nstand the tweets. Llama3 could also potentially access in its pre-training period the\nsame Twitter/X data we used for fine-tuning. Neither GPT4 (without fine-tuning but\nwith in-context learning) performed well compared to Llama2 or Mistral. Surprisingly,\nBERTweet large, the SOTA model according to the TweetEval (Barbieri et al., 2020),\nperformed even worse than BERT base (Table 2). It might be attributed to its size\nsmaller than Llama2 or Mistral, plus it was pre-trained on data until 2020 without\nrecent context.\nIt seems that some models (Llama2, Mistral, BERT) strongly benefited from fine-\ntuning while some others (Llama3, BERTweet) were more \"stiff\" and less tunable.\nThe proportion of PEFT-tuned parameters was slightly smaller in Llama3 than in\nLlama2 or Mistral (Table 1). However, this does not correspond to differences in their\nperformance.\nIn the overwhelming majority of settings (see Tables 3, A1 and A2), all tested LLMs\nperformed better with English-translated datasets (for both fine-tuning and testing),\nand the DeepL gave better results than the Helsinki translator. Therefore, despite\nsuccessful multilingual models as XLM-R (Conneau et al., 2020) or XLM-T (Barbieri\net al., 2022), a good translation to English still provided an advantage.\nFigure 3 summarises experiments on individual languages using the DeepL translation.\nRather surprisingly, almost all models performed poorer for the Polish language. These\nresults correlate neither to the support of fine-tuning sets of individual languages which\nwas almost the same for PL and HU (see Table 2) nor to the type of translation (the\nresults were similar for untranslated tweets), and they cannot be attributed to pre-\ntraining either (e.g., GPT4 gave very good results for Polish on the multilingual MMLU\nbenchmark (Achiam et al., 2023)). Note that also vanilla models (LLama2, LLama3,\nGPT4) put Polish tweets at a disadvantage compared to other languages (Table 4).\nA detailed analysis showed that many positive Polish tweets (either towards Ukraine\nor Russia) were classified as negative by the models. These tweets contained more\ncomplex thoughts reflecting the fact that Poland was historically more interconnected\nwith Ukraine than CS/SK or HU. A few examples of misclassified tweets can be found\nin Appendix C.\nHowever, when repeating fine-tuning with two-valued sentiment classification (Fig.\n4, some models (Llama2, Mistral, BERT) almost erased differences between languages.\nThis supports our hypothesis that these models are better tunable than the others.\nWe also relate this observation to the sizes of the training sets discussed bellow.\nTo get a comparison, we tested three vanilla models: LLama2, Llama3 and GPT4.\nFine-tuning improved dramatically the performance of Llama2/3 over vanilla models\n(compare Fig. 3 and Tab. 4). In contrast, we tried also in-context learning which\ndid not matter much for most tested models except GPT4 for which it improved the\naccuracy on all datasets by 5-10%. We therefore confirm the observations of (Liu et al.,\n2022) about prevalence of fine-tuning over in-context learning."}, {"title": "5 Conclusion", "content": "We addressed the problem of fine-tuning large language models for the task of aspect-\nbased classification of sentiment towards a given aspects. For our experiments we chose\nthe theme of the Russian-Ukrainian conflict which is still ongoing and it polarises\nsociety in a strong way. We narrowed our dataset to the V4 (Czech Republic, Slovakia,\nPoland, Hungary) language space, chosen due to its proximity to the conflict, and also\nbecause we are not aware of a similar study based on this underrepresented group of\nlanguages. Data were collected using the academic API from the social network Twitter\n(X) during the first half of 2023, and split into three independent language-specific\ndatasets which were manually annotated.\nWe performed fine-tuning of selected models (LLama2, LLama3, BERT, Bertweet,\nMistral) separately for each dataset in several variants, using either the original lan-\nguage or an English translation with the Helsinki or DeepL translator, and focusing\non sentiment towards a specific entity (Russia, Ukraine). Further detailed settings are\ndescribed in Section 3. The results were evaluated using basic metrics - accuracy and\nmacro-averaged precision, recall and F1. Furthermore, the GPT4 (with or without\nin-context learning) was used as a reference model. The best performing (SOTA-\nlevel) model was the Llama 2 followed closely by Mistral, both in combination with\nthe DeepL translation. Rather surprisingly, Llama3, BERTweet and GPT4 performed\nsignificantly worse.\nResults of the experiments revealed several interesting phenomena related to differ-\nences between models and between languages. One conclusion is that the fine-tuning,\neven with as few as hundreds of samples, was able to draw the model's attention to\nthe desired aspects and also to balance language and culture differences (at least for\nsome models). The results also indicate that the success of fine-tuning is highly model-\nand task-dependent, as found also in other studies such as (Zhang et al., 2024)."}, {"title": "Future work", "content": "The classification of sentiment in tweets (or similar short texts) can suffer from various\nsources of bias, such as the subconscious assumptions of cause-and-effect, the lack\nof contemporary context or human versus model bias towards a certain aspect. To\nunderstand and evaluate biases in models towards specific ongoing polarising themes,\nwe propose to create a synthetic dataset focused on Russo-Ukraine conflict. We would\nstudy various topics such as general sentiment of sentences, aspect based sentiment,\nsentiment of the implication etc. on several models in different setting using a more\ngranular approach.\nAnother recent trend is the distillation of knowledge to smaller models (such as\nMicrosoft Phi-3 (Abdin et al., 2024) or 1-bit models (Ma et al., 2024)) to make infer-\nence on narrow tasks cheaper and also to avoid the necessity of manual annotation.\nWe have already seen in our experiments that fine-tuned medium-sized models easily\nsurpass universal large ones as GPT4.\nWorking with different languages and information spaces and the use of different\nmethods requires integration of both methods and acquired experience to a more\ngeneral platform dealing with public information in cyberspace. Such a platform would\ncontain components that take care of the collection, storage, annotation, analysis and\ndownstream usage of data from heterogeneous sources (social networks, open internet).\nThese components would be anchored by a web interface through which the data\nwould be accessed in a unified and controlled manner (Pavl\u00ed\u010dek et al., 2020, 2021)."}]}