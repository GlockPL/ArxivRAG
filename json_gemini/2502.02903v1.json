{"title": "What is in a name? Mitigating Name Bias in Text Embeddings via Anonymization", "authors": ["Sahil Manchanda", "Pannaga Shivaswamy"], "abstract": "Text-embedding models often exhibit biases arising from the data on which they are trained. In this paper, we examine a hitherto unexplored bias in text-embeddings: bias arising from the presence of names such as persons, locations, organizations etc. in the text. Our study shows how the presence of name-bias in text-embedding models can potentially lead to erroneous conclusions in assessment of thematic similarity. Text-embeddings can mistakenly indicate similarity between texts based on names in the text, even when their actual semantic content has no similarity or indicate dissimilarity simply because of the names in the text even when the texts match semantically. We first demonstrate the presence of name bias in different text-embedding models and then propose text-anonymization during inference which involves removing references to names, while preserving the core theme of the text. The efficacy of the anonymization approach is demonstrated on two downstream NLP tasks, achieving significant performance gains. Our simple and training-optimization-free approach offers a practical and easily implementable solution to mitigate name bias.", "sections": [{"title": "Introduction", "content": "Text-embedding models, which convert raw text such as sentences/paragraphs into concise numerical representations, have become fundamental tools for downstream NLP tasks in fields such as healthcare, education, law and scientific research (Chrysostomou and Aletras, 2022; Reimers, 2019; Tenney, 2019; Nie et al., 2024; Sun et al., 2019). A cosine similarity between embeddings is typically used (Zhang et al., 2019; Mathur et al., 2019) although other types of similarities (Steck et al., 2024) are also possible. With a similarity measure, the goal is to find which two texts are similar to or different from one another. For simplicity, we will use text-embedding model to refer to models that convert text to an embedding.\nMany text-embedding models are often trained on large amounts of Internet text. This data can inadvertently contain biases of various kinds, reflecting social prejudices and stereotypes. As a result, these models can generate biased embeddings, reinforcing harmful stereotypes or discriminating against certain cultural groups, genders, etc. (Gallegos et al., 2024; Li et al., 2023; Rakivnenko et al., 2024). Furthermore, the presence of bias in models could lead to embeddings that disproportionately emphasize particular parts of the text, consequently failing to capture the true semantics and themes within the text (Rakivnenko et al., 2024).\nWhile important, existing studies on biases, predominantly examine biases in text-embedding models mostly related to gender, geography, race, religion etc. (Rakivnenko et al., 2024; May et al., 2019; Bolukbasi et al., 2016; Kotek et al., 2023; Nghiem et al., 2024). In this paper, we demonstrate that text-embedding models exhibit significant bias towards names within the text. To illustrate this, we begin with a motivating example in Table 1. We present a simple narrative (Story 1). We then show a similar plot while substituting the name of the main character in (Story 2). In the third narrative (Story 3), we introduce a distinct and contradicting storyline from Story 1 while retaining the original character names. We embed all three stories using text-embedding models. We observe that the similarity between Story 1 and Story 3, despite their differing plots, is consistently higher than the similarity between Story 1 and Story 2, which share highly semantically similar plots but differ in character names. This is very counter-intuitive since the text-embedding models seem to prioritize name similarity over the text's narrative structure. While this is admittedly an illustrative example, we proceed to generate numerous such narratives and conduct a thorough investigation of this bias in our experiments. We emphasize here that our study investigates thematic and semantic similarities within textual data while acknowledging certain applications involving text tied to specific individuals or locations, our primary focus lies on the broader thematic context rather than characters in the text.\nOur observation reveals a critical issue that can significantly impact applications that rely on semantic similarity, including semantic search, information retrieval, and plagiarism detection (Minaee et al., 2024; Pudasaini et al., 2024): consider the challenge of accurately assessing the similarity between two stories/plots with identical underlying meanings but distinct character names. Current methods may erroneously classify these stories as dissimilar, leading to inconsistent and unreliable results. Further, based upon our investigation, we would like to mention upfront that the issue is not confined to certain cultures, cross-culture, but is universal in the sense that the name bias issue occurs in a very broad sense."}, {"title": "Related Work", "content": "Biases in Text-embedding models: Text-embedding models while powerful, can inadvertently reflect and amplify existing biases and prejudices; there is vast research understanding and mitigating bias in such models. For example, there is work focusing on models that investigate under-representation or misrepresentation of specific groups, such as LGBTQ+ individuals, leading to skewed or inaccurate outcomes (May et al., 2019; Bolukbasi et al., 2016; Cheng et al., 2021). Another type of study focuses on gender bias in word embeddings models (Rakivnenko et al., 2024). The study highlights a concerning issue i.e many embedding models associate specific occupations with particular genders. Nikolaev and Pad\u00f3 (2023) studied biases at a sentence-level in sentence transformers influenced by different parts of speech such as common nouns, adverbs etc. While we discuss text-embedding model, it is important to highlight works that investigate bias within Large Language Models (LLMs) for text-generation which are a part of this ecosystem (Gallegos et al., 2024). Schw\u00f6bel et al. (2023) observed \"geographical erasure\" where certain regions are underrepresented in LLM outputs. Manvi et al. (2024) showed that LLMs often favor developed regions and exhibit negative biases towards locations with lower socioeconomic conditions, particularly on subjective topics such as attractiveness and intelligence. Further, some works have also investigated cross-cultural biases in LLMs for text generation (Naous et al., 2023; Ramezani and Xu, 2023; Cao et al., 2023; Arora et al., 2022). Compared to the above work, we investigate name-bias in text-embeddings, an area not previously explored in existing research to the best of our knowledge.\nDebiasing methods: Various approaches have been proposed to tackle different kinds of biases in text-embedding models highlighted above. One common technique to remove such biases is to update the training dataset and make it unbiased and re-train the model (Brunet et al., 2019; Ngo et al., 2021). Another paradigm involves applying approaches such as disentanglement or alignment where models are fine-tuned to remove biases associated with concepts such as gender, religion etc. (Kaneko and Bollegala, 2021; Guo et al., 2022; Kenneweg et al., 2024). An alternative approach involves post-processing of the embeddings. Specifically, it involves adding a debiasing module after encoders to filter out certain biases in the representations Cheng et al. (2021). For more details on this topic, we refer the reader to survey by Li et al. (2023) for more details.\nWe emphasize some key considerations based upon the discussions above. Firstly, all the aforementioned techniques require an optimization phase, involving either retraining the initial model, fine-tuning with a modified loss or post-processing of the generated embeddings. Secondly, these methods are often designed to address specific bias types, such as social, gender, or religious biases. Notably, the identification and mitigation of name bias has not been previously explored to our knowledge."}, {"title": "Understanding name bias", "content": "In this section, we investigate the presence of bias within text-embedding models related to names. Our primary objective is to investigate the influence of names containing identity-specific information on the resulting text embeddings, while ensuring the semantic structure of the text remains unchanged."}, {"title": "Benchmarking Methodology", "content": "To understand the impact of bias associated with names, we systematically replace instances of names in text with alternatives. For the sake of simplicity, in this section, we focus on person names and country names\u00b9. Given a text, we first identify instances of person and country names in the text.\u00b2 To study bias w.r.t. person names, we replace each person name in the text with a randomly sampled name from a list of person names. In the text, all instances of the same person are replaced by the same sampled name. Similarly, country names are replaced with a random country name sampled from a predefined list of countries. This process only changes the person names and countries and does not change the original structure or meaning of the text.\nFormally, given a universe of n person names  P = {P1, P2, P3\u00b7\u00b7\u00b7pn}, and l Country names C = {C1, C2, C3C1}, we apply algorithm 1 for a given text T to obtain a perturbed text T'.\nWe also study the impact of perturbation of person names only.\n\u00b2The datasets used for benchmarking are described in Sec. 3.3"}, {"title": "Candidate Text-embedding Models", "content": "We analyzed a diverse set of leading text embedding models from academia and industry. This includes models explicitly trained on diverse languages and tasks such as semantic search, question-answering etc. We include models such as multi-qa-distilbert-cos-v1 and multi-qa-mpnet-base-cos-v5 for question answering, and paraphrase-MiniLM-"}, {"title": "Analyzing Bias", "content": "In Table 3 and 4 we observe a significant deviation in the average cosine similarity which should be close to one if the cosine similarity captured the real semantic similarity rather than information in names present in the text. Any deviation from one indicates that the embeddings are heavily biased by the choice of names rather than from the similarity of the text. Models like msmarco-distilbert-cos-v5 exhibit significant sensitivity to changes in person and country names, as evidenced by an average cosine similarity\n\u00b3For each embedding model, we evaluate its performance only on samples which are within the limits of its maximum context window.\n\u2074We also experimented by using euclidean distance instead of cosine similarity in Tab. 19 in Appendix. The conclusion remained similar and therefore we proceeded with cosine similarity for remaining experiments."}, {"title": "Methodology: Overcoming Bias through Anonymization", "content": "Previously, we showed that how just changing person names/country names can impact the embeddings significantly. In this section, we introduce a simple inference-time anonymization technique to mitigate the bias caused by names. The core idea is to mitigate the influence of names on embeddings, and making the resulting debiased anonymized embeddings to be more generalizable and less prone to biases related to particular individuals or entities.\nThe anonymization of a text T during inference is achieved through the following process. We first identify in T, occurrences of desired entities such as person names, locations and organizations relevant to the use case. We anonymize the text by removing those occurrences from T. The anonymized text referred to as Tanon retains the overall structure and meaning of the original text T while removing any specific references to person names etc. This anonymization can be achieved via tools such as Large Language Models(LLMs) (Zhao et al., 2023) or Named Entity Recognition tools (Jehangir et al., 2023). In our work, we used gemini and anthropic.claude-3-5-sonnet text-generative models for anonymization using prompts. Depending upon the use-case, different names in text such as person names, cities, countries, organizations can be removed. We would like to clarify that the same process of anonymization can also be done through Named-Entity Recognition(NER) tools (Jehangir et al., 2023), however in our initial experiments we found LLMs to be more accurate. Sample prompts for anonymization are presented in Table 5. Post anonymization, the embeddings become independent of identity specific details such as person names/ country names etc. \u2075 Overall, the debiased embeddings generated on anonymized text promise reduced sensitivity to biases associated with particular individuals or entities. Note that the embeddings generated for sentences that differ solely in their named entities\n\u2075The type of anonymization i.e removing person names and/or country names and/or city names etc. used determines the exact level of independence."}, {"title": "Can anonymization help in down-stream tasks that use similarity from text-embedding models?", "content": "In this section, we investigate the performance of the anonymized text embeddings on two downstream tasks. Both the tasks are based on obtaining a similarity score between pieces of texts. These tasks are primarily based upon semantic similarity which find applications in areas such as information retrieval, clustering, plagiarism detection, question answering etc. (Reimers, 2019). The two tasks that we evaluate on differ in various aspects such as the nature of the task, evaluation methodology, the judgment score available, etc. On both these tasks, our experiments show that embeddings based on anonymized text can significantly help in downstream tasks."}, {"title": "Task 1: Semantic Similarity Between Query and Text-Pairs with Binary Labels.", "content": "Recall from Sec. 3 that altering only the names/locations in two otherwise identical stories/paragraphs significantly impacted their text embeddings. In this section, we investigate whether anonymization technique proposed in Sec. 3.4 can effectively mitigate this type of bias. Towards this, we explore the Semantic Similarity Task (STS).\nSemantic similarity seeks to determine the degree to which two pieces of text convey similar meaning (Muennighoff et al., 2022; Reimers, 2019). This goes beyond simple word matching, aiming to understand the underlying meaning within the text. In today's era of deep learning (Reimers et al., 2016; Muennighoff et al., 2022), achieving accurate semantic similarity relies heavily on high-quality embeddings, which represents sentences as dense vectors in a continuous space.\nIn this experiment we investigate whether the text-embeddings are able to capture the semantic nuances within the text or are they biased towards names? Ideally, a good embedding model should be able to differentiate reasonably well between two stories/paragraphs which have very different themes even if they contain same names. To investigate this, we create a dataset of 10 paragraph triplets. Each triplet includes a query paragraph, a positive paragraph that is highly semantically similar but with distinct person and location names, and a negative paragraph that is semantically dissimilar to the query text but has same person names/location names as in query text. For each triplet, (query, positive) pair is assigned a label 1(positive) and (query, negative) pair is assigned a label 0(negative). Two sample examples can be found in Table 16 in the the rows marked as Original. The entire set of generated triplets with labels are present in Appendix D. We evaluate the performance of different models on the STS task using AUC ROC score between cosine similarity scores of embeddings and the ground truth."}, {"title": "Task 2: Semantic Similarity With Graded Human Relevance.", "content": "In the previous task, a binary approach was employed to assess text pair similarity, categorizing text-pairs as either similar or dissimilar. In the task proposed in this section, we employ a more refined approach for evaluation by utilizing a graded relevance scale from 1 to 5 between a pair of text. The graded scale enables a more nuanced and granular assessment of semantic similarity between pairs, providing a richer understanding of their relationship. To evaluate this, we use the machine summary evaluation task from Muennighoff et al. (2022), which involves automatically assessing the relevance of machine-generated summaries, commonly assessed by calculating the semantic similarity between the embeddings of the summary and the original document/human summaries.\nFor this task, we follow the same evaluation setup as Muennighoff et al. (2022) which we describe next. We use the SummEval dataset (Fabbri et al., 2021; Muennighoff et al., 2022) with 100 text samples, each containing 16 machine and 10 human summaries. Human relevance scores (1-5) are assigned to each machine summary. We first obtain summary embeddings using text-embedding models for each machine summary and human summary in all 100 samples. Without loss of generality, for a given text sample out of the 100 samples, for each machine summary {mi | 1 \u2264 i \u2264 16}, we get its predicted score based on its maximum cosine similarity to any human summary {hj | 1 \u2264 j \u2264 10} within the same text sample i.e machine_pred_score(mi) = max1"}, {"title": "Conclusion", "content": "In this work, we highlight the bias in text embeddings stemming from the presence of names in the text. We showed concrete examples, over multiple text-embedding models, that similarities between embeddings can be dominated by names in the text rather than the semantic meanings of the text. We then proposed a method to mitigate bias by performing anonymization at inference time. This involved the removal of names from the text and using the anonymized text to create the embeddings. Our findings demonstrate that anonymized text embeddings significantly outperform deanonymized text embeddings on tasks involving semantic similarity. While we proposed one way to mitigate the issue through anonymization, a deeper question that remains is: how to train text-embedding models such that the embeddings capture the semantics more than the names in the text?"}, {"title": "Limitations", "content": "Below we discuss the limitations of the proposed work.\nIn this work we focused on evaluating/mitigating name bias in text-embedding models using texts from English language. The work presented here does not cover other languages. Further, the work also does not cover name bias issues arising in multi language texts.\nWhile our proposed anonymization solution enhances thematic similarity, it is not ideal for situations requiring the preservation of identity that we are removing through anonymization. A partial and straightforward solution might involve anonymizing only non-critical identifying information depending upon the use-case. Many real world use cases may require dynamically balancing identity and thematic preservation to suit the specific needs of each use case.\nIn our work, we adopted similarity between text-embeddings as a proxy for their semantic similarity. While commonly used, it is still an estimate of semantic similarity and may overlook deeper semantic relationships that require reasoning. A limitation of this work is that we capture thematic similarity only to the extent that it is captured by the cosine similarity (and the Euclidean distance similarity is studied in the Appendix)."}, {"title": "Semantic Similarity Task Dataset", "content": "Below we present the STS dataset consisting 10 samples used in Sec. 5.1. Each sample is a triplet of the form:\n< Query, Positive sample, Negative sample >.\nQuery: Nikolai and Deborah met on a rainy Tuesday in New York. The city's hustle and bustle couldn't dim the spark between them. Deborah, with her radiant smile and infectious laughter, had captured Nikolai's heart from the moment he saw her. Nikolai, a charming and witty gentleman, returned her affection with equal fervor.\n\u2022 Positive: Kashvi and Oluwafemi met on a rainy Tuesday in Northampton. The city's bustling streets couldn't dim the spark between them. Kashvi, with her radiant smile and infectious laughter, had captured Oluwafemi's heart from the moment he saw her. Oluwafemi, a charming and witty gentleman, returned her affection with equal fervor.\n\u2022 Negative: Nikolai and Deborah staying in New Jersey, once inseparable, were now worlds apart. Deborah, the trusted confidante, had betrayed Nikolai's trust, revealing his secrets to their rivals. The city's hustle and bustle mirrored the chaos within Nikolai's heart, as he grappled with the bitter reality of love turned treachery.\nQuery: Alejandro quickly ran to the store to buy a cold drink. He was eager to have a glass of cold drink.\n\u2022 Positive: Quickly, Hiroki dashed to the local market to procure some cold drinks. He was yearning for a chilled glass of cold drink.\n\u2022 Negative: Alejandro has stopped buying cold drinks from market. He only drinks cold drinks made at home.\nQuery: Mayatoshi and Alex had a deep, passionate love for each other. Their bond was unbreakable, a love that transcended all obstacles. They shared dreams, hopes, and aspirations, and their love was the foundation of their happiness.\n\u2022 Positive: Priyanka and Yuan were deeply in love. Their affection for each other was profound and unwavering. They shared a strong connection, a love that was the source of their joy and content-ment.\n\u2022 Negative: Despite their intense hatred for each other, Mayatoshi and Alex were bound by a strange, twisted connection. Their animosity fueled a toxic relationship, a constant battle of wills. Their lives were intertwined, a dark, destructive dance of love and hate.\nQuery: Amazon and Apple are two American corporations. Amazon's main business is online shopping and Apple is a phone maker giant\n\u2022 Positive: Alibaba and Xiaomi are two Chinese corporations. Alibaba's main business is online shopping and Xiaomi is a producer of phones\n\u2022 Negative: Amazon is a river in South America. Apples are not grown in the Amazon basin.\nQuery: Ganga and Yamuna are two mighty rivers. They are lifelines for millions of people in the region.\n\u2022 Positive: Yangtze is a mighty river. It is a long river and is the lifeline for millions of people in the region.\n\u2022 Negative: Ganga and Yamuna are two sisters. They had their schooling in the region and schooling provided a lifeline for them.\nQuery: Alice and Bob often helped each other financially. Recently, Alice lent Bob a significant sum of money. Bob promised to return it soon.\n\u2022 Positive: Yuri and Haruto frequently helped each other out, including with money. Lately, Yuri had loaned Haruto a substantial amount of money, which Haruto assured her he'd repay promptly.\n\u2022 Negative: Alice and Bob had a disagreement about money. Alice believed Bob owed her money, but Bob denied it.\nQuery: John, a renowned lawyer, is defending his client, Mike, who is accused of a serious crime. John is determined to prove Mike's innocence and secure his acquittal.\n\u2022 Positive: Armaan, a man falsely accused of a heinous crime, is relying on his skilled lawyer, Udit, to exonerate him. Udit is committed to presenting a strong defense and clearing Armaan's name.\n\u2022 Negative: John, a cunning lawyer, is manipulating the legal system to frame Mike for a crime he did not commit. John's goal is to secure a conviction and advance his own career, regardless of the truth.\nQuery: Dr. Alexander, a seasoned physician, meticulously analyzed patient Sarah's intricate heart condition. He prescribed a tailored regimen of medications and rigorous lifestyle modifications to significantly improve her cardiac health.\n\u2022 Positive: The esteemed doctor, Dr. Yerusha, conducted a thorough assessment of patient Reyan's complex symptoms of heart. She formulated a precise treatment plan, incorporating medications and day to day lifestyle changes, to alleviate Reyan's debilitating heart condition.\n\u2022 Negative: Dr. Alexander, a renowned doctor and surgeon, executed a high-risk heart surgical procedure on patient Sarah. After the complex operation Sarah did not recover.\nQuery: Mr. Smith, a dedicated teacher, guided his students, including the bright young minds of Miller and Pristina, towards academic excellence.\n\u2022 Positive: Mr. Yang, a committed educator, mentored his students, including the talented Shruti and Ren, to achieve academic success.\n\u2022 Negative: Mr. Smith, a rigid and punitive teacher, often unfairly targeted mischievous students like Miller and Pristina.\nQuery: Martinez gently examined the injured bird. He gave it food.\n\u2022 Positive: Yohan tenderly inspected the wounded bird and gave it a meal to eat.\n\u2022 Negative: The skilled hunter Martinez tracked the injured bird. He captured it for food."}, {"title": "Example of Semantic Similarity post-anonymization", "content": "In Table 16, we show impact of anonymization on STS tasks on embeddings crated by Open Al's text-embedding-3- small model. We observe that in all cases performance after anonymization is superior. Specifically, post anonymization, we obtain relatively higher score for positive samples and lower for negative samples."}, {"title": "Impact of Anonymization Strategy: Removal versus Replacement", "content": "This section investigates the effectiveness of remove of names vs. replacement of names in text for anonymization. In the replacement strategy, we replace names with non-identifying placeholder names instead of removing them from text. Example: person names with 'CHAR_ID', location names with 'LOC_ID' etc. Here ID can be replaced with {A, B, C' \u00b7 \u00b7 \u00b7 } or {1, 2, 3\u00b7\u00b7\u00b7 } etc. The detailed prompt is present in Table 17. In Table 18 we demonstrate that removal of names marginally outperforms replacement in the STS task. In the context of replacement strategy, one should note that the quality of embeddings derived is sensitive to the specific replacement placeholder terms used. For instance, substituting character names with with different placeholders such as \u201cCHAR_A\u201d / \u201cCHARACTER_B\u201d / \u201cCHARACTER_1\" or location names with \u201cLOC_1\u201d / \u201cLOC_B\" can impact the resulting embeddings differently. In order to mitigate this sensitivity and ensure consistent results and also based upon our findings we recommend using the name removal strategy for anonymization to mitigate name bias."}]}