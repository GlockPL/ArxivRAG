{"title": "EMMA-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning", "authors": ["Qi Sun", "Pengfei Hong", "Tej Deep Pala", "Vernon Y.H. Toh", "U-Xuan Tan", "Deepanway Ghosal", "Soujanya Poria"], "abstract": "Traditional reinforcement learning-based robotic control methods are often task-specific and fail to generalize across diverse environments or unseen objects and instructions. Visual Language Models (VLMs) demonstrate strong scene understanding and planning capabilities but lack the ability to generate actionable policies tailored to specific robotic embodiments. To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning. In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, EMMA-X. EMMA-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance. Additionally, we introduce a trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation. Experimental results demonstrate that EMMA-X achieves superior performance over competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning.", "sections": [{"title": "1 Introduction", "content": "The robotic policy model aims to generate sequences of low-level action manipulation policies for robots. Traditional reinforcement learning-based robotic control methods often focus on narrowly defined tasks within fixed environments (Ma et al., 2024), hindering their ability to generalize beyond task-specific training data and limiting their"}, {"title": "2 Problem Formulation", "content": ""}, {"title": "2.1 Policy Imitation Learning", "content": "Given a set of expert demonstrations D = {({st}Tti=1,{at}Tti=1)}Ni=1, where N is the number of demonstrations in the dataset, T is the number of states (image frames of the environment) for a data sample Di, si = image; represents the state consisting of an image of the environment, Ti is a natural language task instruction, and ai represents the action taken by the expert in that state, the goal is to learn a policy \\( \\pi_{\\theta}(a | s,T) \\) that mimics the expert's behavior.\nThe policy \\( \\pi_{\\theta} \\) is modeled by a Vision-Language-Action (VLA) model. In line with the OpenVLA"}, {"title": "2.2 Hierarchical Policy Imitation", "content": "We build on the above formulation by decomposing a general task T into a hierarchical structure consisting of finer-grained components: states, segments, and subtasks.\nA state at timestep t, denoted st, represents the scene. The sequence of states for the i-th trajectory is Si = {s1,s2,...,sT}, where T is the number of timesteps. An action at is taken at state st, and the corresponding sequence of actions is Ai = {a1,a2,...,aT}. A segment \\( \\sigma \\) is a series of consecutive states, {st,st+1,...,st+k}, contributing to a subgoal, with \\( \\sum_i = {\\sigma_1, \\sigma_2,..., \\sigma_\\eta} \\) representing the segment sequence for the i-th trajectory. In each segment, the robot performs similar actions. A subtask S consists of segments, {\\sigma_1,\\sigma_2,..., \\sigma_p}, to achieve a specific subgoal. Finally, a task T is a series of subtasks, {S1,S2,...,Sm}, required to complete the overall objective.\nOur Vision-Language-Action (VLA) model \\( \\pi_{\\theta}(a_t | S_t, T) \\) predicts actions at for each state st by hierarchically decomposing tasks into subtasks. This ensures the end-effector's motion aligns with subgoal intents, enhancing the model's ability to execute complex tasks through manageable subtasks. We create a dataset D = {Di}Ni=1, where Di = {Si, \\Sigmai, Ti}. Each state st \\( \\in \\) Si is labeled with its subtask. Without such labeling, chain-of-thought training is infeasible. During inference, the model generates reasoning chains, including subtasks and relevant spatial information derived from visual scenes."}, {"title": "3 Methodology", "content": "In this section, we introduce our proposed framework in detail. Our EMMA-X encompasses three crucial designs: (1) Segmenting the trajectory based on the states of the gripper and the motion trajectory of the robotic arm. (2) Generating hierarchical planning including grounded task reasoning, 2D gripper positions, and 3D spatial movements."}, {"title": "3.1 Trajectory Segmentation", "content": "Why Segment Trajectories? The overarching goal of our work is to enhance Vision-Language-Action (VLA) models with grounded chain-of-thought (CoT) reasoning. We identified two key limitations in existing VLAs: 1) While existing VLAs improve task decomposition by breaking a task into subtasks and solving each using CoT (Zawalski et al., 2024), their CoT reasoning relies exclusively on textual scene descriptions 1. This limits their reasoning capability for real-world scenarios. 2) They lack robust spatial reasoning abilities, essential for effective task planning and execution.\nTo address these limitations, we propose two key solutions: Incorporating visual scene information: Beyond textual prompts, we integrate visual inputs into Gemini to enable task decomposition into subtasks and generate high-level plans grounded in both visual and textual contexts. Fine-grained movement plans: We train the robot to determine where to go and how to reach a potential future state necessary for completing a subtask.\nTo implement these solutions, every state must be labeled with the subtask the robot is performing. However, our experiments revealed that directly annotating each individual frame via Gemini resulted in noisy labels, likely due to insufficient contextual information. To overcome this, we segment trajectories into sequences of consecutive states where the robot performs semantically similar ac-"}, {"title": "Our Segmentation Method", "content": "As shown in Figure 2(a) and Figure 2(b), we segment observation sequences by integrating the motion trajectory and the gripper states of the end effector. To achieve this, we utilize the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm (McInnes et al., 2017), which effectively handles noise stemming from small fluctuations caused by imperfections in human demonstration. The flexibility of HDBSCAN enables the discovery of diverse trajectory patterns within the data.\nWe define a custom distance measurement to segment the end effector's trajectory, capturing both spatial and temporal information. Let \\( p_i = (x_i, y_i, z_i) \\) denote the 3D position, and \\( r_i = (r_{ix}, r_{iy}, r_{iz}) \\) represent the 3D orientation of data point i. Additionally, let ti represent the timestamp of this data point. The distance between two data points i and j is given by the following expression:\n\\( d(i, j) = ||p_i - p_j||_2 + \\lambda ||r_i - r_j||_2 + \\beta |t_i - t_j|  \\) (1)"}, {"title": "3.2 Data Generation", "content": "After obtaining the segments, we generate hierarchical embodied planning data for each demonstration, as shown in Figure 2. For each segment of a demonstration, we produce the 2D end-effector position and 3D movements for the completion state of the current segment. Additionally, we generate grounded reasoning for the corresponding subtasks."}, {"title": "Why Look-ahead Spatial Reasoning?", "content": "Consider the robot as a delivery driver tasked with delivering a package to a specific destination (the goal). The driver has access to a detailed high-level map of the"}, {"title": "Look-ahead Gripper Position Generation.", "content": "Following (Zawalski et al., 2024), we also use OWLv2 (Minderer et al., 2024) and SAM (Kirillov et al., 2023) to detect 2D gripper position, which can be seen in Figure 2(e). The difference is that they train the model to output only the gripper position for the current input state, whereas, in our data construction process, we use the current gripper position"}, {"title": "Look-ahead Movement Plan Generation.", "content": "As shown in Figure 2(d), we infer the 3D spatial positions corresponding to the current state and the end state of the current segment using the state policy of the robot. Specifically, we calculate the displacement between these two positions to determine the direction and step size required for the manipulator to move from the current state to the end state. Following the motion language idea in RT-H (Belkhale et al., 2024), we encode our high-level motion plans using a standardized template in Appendix E. By integrating look-ahead spatial reasoning, the model incorporates both reactive and proactive decision-making. It combines immediate context at the current state st with a high-level plan that predicts likely future states st+k and the corresponding movement strategy \\( \\beta(st, St+k) \\). This dual focus enables the model to align immediate actions with the overarching goal, ensuring purposeful and adaptive task execution. Please note that this data is not directly executed as the robot's actions. Let's consider for every state st, we will obtain mt, the movement plan to the first state of the next segment."}, {"title": "Grounded Chain-of-Thought Reasoning.", "content": "As shown in Figure 2(f) and (g), we utilize Gemini 3 to derive the subtask corresponding to each segment, along with scene understanding and the reasoning behind the series of actions the robot needs to perform the subtask. Specifically, we take sequences of segmented images, and task descriptions as input to guide Gemini in generating the subtask and grounded reasoning for each segment. Compared to (Zawalski et al., 2024) that infer subtasks and their mapping to states solely from textual information, our approach first segments the sequence based on the robot's motion trajectory and gripper's state as explained in Section 3.1. After that, based on the given multimodal information, we generate the corresponding subtasks and the reasoning of each subtask. Note that each subtask can"}, {"title": "The Final Dataset.", "content": "The final dataset for the i-th trajectory in the training dataset is defined as: {Di}Ni=1 = {{Xi,Yi}Ti=1 {{(st, Ti), (mt, gt, GRt, at)}ti=1}Ni=1, where t = 1,2,..., T, and T is the total number of timesteps in the trajectory."}, {"title": "3.3 EMMA-X", "content": "In this section, we introduce the architecture of our proposed EMMA-X which is a 7B-parameter VLA model fine-tuned from OpenVLA using our constructed hierarchical embodiment data. As shown in Figure 3, we adjust the text prompt with the current gripper position and add chain-of-thought training to enhance the ability of spatial reasoning and scene understanding before predicting the next robot action policy.\nDuring the process of predicting for real robot testing, we input the task description, the current observation image, and the 2D gripper position detected in real-time by OWLv2 (Minderer et al., 2024) and SAM (Kirillov et al., 2023). EMMA-X first outputs the subtask and a description of the current scene, including the spatial relationship between the target object in the image and the robotic arm, as well as the operational instructions required for the gripper to reach the goal of the current subtask. Additionally, EMMA-X also predicts the target position the gripper needs to reach after completing the sub-task, including both the 2D location in the image and the 3D spatial movements. Finally, the model outputs the next 7D robot action policy for downstream manipulation."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Implementation Details", "content": "To create the hierarchical reasoning dataset, we employed our data creation pipeline on full BridgeData-v2, which consists of approximately 60,000 trajectories paired with task instructions, resulting in an augmented dataset.\nTo train our VLA models, we employed Open-VLA, a 7B vision-language-action (VLA) model"}, {"title": "4.2 Robot Setup and Metrics", "content": "We evaluate our approach using the 6-DoF WidowX robot arm, as introduced in the Bridge V2 paper, which represents a standard benchmark for assessing generalizable robotic policies. The policy takes as input a single third-person camera feed and a natural language instruction, predicting end-effector velocity actions to control the robot.\nTo rigorously test the generalization capabilities of the policies, we develop a suite of challenging evaluation tasks that span multiple aspects: in-domain scenarios, out-of-domain (OOD) objects, spatial relationships, and OOD instructions. All policies are assessed on identical real-world setups to ensure consistency in camera angle, lighting conditions, and background. Each task is conducted"}, {"title": "4.3 Baselines", "content": "To comprehensively evaluate the performance of our proposed EMMA-X, we conduct extensive experiments across 12 different tasks on the real robot with several competitive methods.\nOpenVLA (Kim et al., 2024): A VLA model based on large-scale VLM Prismatic-7b and pre-trained on the Open-X-Embodiment dataset(Collaboration et al., 2023).\nOpenVLA w/ FT: For a fair comparison, we finetuned the OpenVLA model on the BridgeV2 dataset for the same number of epochs following the same training setting in our method.\nECOT (Zawalski et al., 2024): A VLA model fine-tuned from OpenVLA on BridgeV2 dataset (Walke et al., 2023) with their generated chain-of-thought reasoning data."}, {"title": "4.4 EMMA-X Improves Policy Generalization", "content": "In this section, we compare EMMA-X with several baselines on 12 real-world robotic tasks. As shown in Table 1, our EMMA-X outperforms the strong baseline OpenVLA, with a 24.17% increase in task success rate and a 26.25% increase in half success rate. This demonstrates the effectiveness of our constructed hierarchical embodiment dataset. In addition, compared to ECOT, our EMMA-X shows significant gains, which can be caused by the following: 1) ECoT suffers from noisy training data, which causes hallucinations when faced with out-of-domain instructions or unfamiliar objects, leading to task failures. Interestingly, for IN DOMAIN tasks, it performs worse than in other categories and compared to other models, highlighting its limited reasoning capabilities. Our grounded task reasoning approach addresses this by incorporating the segmented visual images, ensuring more accurate task understanding. 2) EMMA-X enhances spatial reasoning by predicting the 2D gripper position of the end state of the current segment and 3D spatial movements to transit to it before predicting the next robot action policy.\nAs shown in Figure 4, we also compared the average performance across various categories of robotic tasks. Notably, our method achieved the most significant performance improvement in SPATIAL RELATION tasks, outperforming OpenVLA by 35% and ECoT by 43% in the h_Succ rate. These results strongly validate the effectiveness of our predicted 3D spatial movements. Furthermore, our method demonstrated substantial performance gains in OOD INSTRUCTION tasks, highlighting the efficacy of our grounded task reasoning."}, {"title": "4.5 Analysis", "content": "We trained several variants of EMMA-X to evaluate the roles of segmentation, look-ahead spatial reasoning, and grounded chain-of-thought (CoT) reasoning, which collectively constitute the core of EMMA-X. For this evaluation, we sampled 6 prompts across Spatial RELATION, OOD OBJECT, and OOD INSTRUCTION (prompts are indicated in magenta color in Section C). For each prompt, we conducted 10 rollouts under the same experimental setup as our main experiments.\nTo evaluate the effectiveness of our segmentation technique, we conducted an experiment where sequences were segmented solely based on the gripper's (end effector) open and close positions. The results, reported in Table 2 under the w/o HDBSCAN condition, show a general performance drop of 10% to 50%. Notably, spatial reasoning performance experienced the most significant decline, with a drop of 50%. These findings demonstrate that the distance metric introduced in Eq. 1 is crucial for the segmentation process.\nThe Impact of Look-ahead Spatial Reasoning. To evaluate the importance of look-ahead spatial reasoning, we conducted two experiments: 1) EMMA-X was trained without explicitly predicting the gripper's position in the next segment, relying only on the predicted movement plan to reach the future gripper position of that segment (denoted as w/o gt in Table 2). This assumes that EMMA-X implicitly infers the future gripper position. 2) We trained EMMA-X to predict the future end effector's position but without rolling out a movement plan to reach that position (denoted as w/o mt in Table 2). The results reveal significant performance"}, {"title": "The Importance of Grounded CoT Reasoning.", "content": "Grounded chain-of-thought (CoT) reasoning is a foundational element of EMMA-X. To assess its impact, we trained a variant of EMMA-X without grounded reasoning, while retaining look-ahead spatial reasoning in the data. The results show a marked performance drop by 43%-55%, highlighting that spatial reasoning alone is insufficient. Interestingly, the absence of grounded CoT reasoning resulted in a more severe decline in spatial reasoning performance compared to models where spatial reasoning capabilities were explicitly ablated. This underscores the critical role of grounded CoT in tackling complex reasoning tasks, including spatial reasoning. Therefore, we surmise that for enhancing the generalizable policies of Vision-Language-Action (VLA) models, it is essential to improve their broader reasoning capabilities, encompassing object recognition, color understanding, abstraction, commonsense knowledge, and more."}, {"title": "Fine-tuning does not Improve OpenVLA.", "content": "We sought to find whether fine-tuning OpenVLA on BridgeV2 could match the performance of EMMA-X. The results, shown in Table 2, reveal that OpenVLA's performance degrades by 5%-25% after fine-tuning with the worst performance observed for OOD INSTRUCTION. We hypothesize that this decline is due to overfitting, as BridgeV2 is already part of OpenVLA's pre-training dataset."}, {"title": "Qualitative Analysis on Real-world Robot Task.", "content": "To qualitatively evaluate the effectiveness of our spatial and task reasoning in guiding robotic actions, we present two successful trajectories and one failed trajectory in Figure 5. From the left case, we find that the predicted gripper position corresponds to the end state of the subtask \u201creaching for the blue cube\u201d. The 3D movement provides a detailed path, clearly directed toward the \u201cblue cube\u201d. We also include a failed trajectory where the"}, {"title": "5 Conclusion", "content": "We introduce EMMA-X, a 7B-parameter embodied multimodal action model designed to enhance spatial reasoning and task planning for robotic policy generation. We construct a hierarchical embodiment dataset enriched with grounded reasoning, including 2D gripper positions and 3D spatial movements. Furthermore, our proposed trajectory segmentation strategy reduces hallucination in task reasoning by grounding reasoning in visual images. The experimental results demonstrate the effectiveness of EMMA-X, showing significant improvements over existing baselines in tasks requiring long-horizon spatial reasoning."}, {"title": "Limitations", "content": "While EMMA-X shows promising performance, its latency remains higher compared to OpenVLA. This increased inference time primarily results from the additional tokens generated during the reasoning process. Specifically, EMMA-X generates approximately 10 times more tokens than OpenVLA. To mitigate this, a potential strategy is to predict all policies within a segment and only regenerate the policy if the predicted policy deviates significantly from the expected movement plan. Another limitation is the generalization capability of EMMA-X. Scaling the training process to incorporate a larger subset of the OXE dataset could enhance the model's ability to handle a broader range of tasks and robotic systems. Lastly, using SAM for detecting the gripper position can lead to inaccuracies. These errors may occur when the gripper is partially occluded by objects or positioned outside the image frame. Employing a more robust model for detecting and segmenting the robot hand could address these challenges and improve reliability."}, {"title": "A Related Work", "content": ""}, {"title": "Generalist Robot Policies.", "content": "Recent progress in robotics has shifted focus towards developing multi-task \u201cgeneralist\u201d robot policies capable of handling a wide variety of tasks across diverse robot embodiments (Brohan et al., 2023b,a; Ebert et al., 2021; Walke et al., 2023; Collaboration et al., 2023; Octo Model Team et al., 2024; Kim et al., 2024). For example, Octo (Octo Model Team et al., 2024) utilizes a compositional design to train a generalist policy capable of handling various tasks directly while supporting fine-tuning for new inputs and action spaces. Similarly, OpenVLA (Kim et al., 2024) adopts a streamlined end-to-end approach, fine-tuning vision-language models (VLMs) to produce robot actions by treating these actions as tokens within the language model's vocabulary. These studies highlight the potential of training robot policies on large and diverse datasets as a promising strategy for enhancing their performance."}, {"title": "Vision-Language-Action Models.", "content": "A number of recent works have explored fine-tuning large pretrained VLMs for predicting robot actions (Collaboration et al., 2023; Brohan et al., 2023a; Kim et al., 2024; Octo Model Team et al., 2024; Driess et al., 2023) Such models are often referred to as vision-language-action models (VLAs), as they fuse robot actions directly into VLM backbones and treating these actions as tokens within the language model vocabulary. This approach provides a simple yet scalable alternative, with models such as RT-2 (Brohan et al., 2023b), RT-2-X (Collaboration et al., 2023), and OpenVLA (Kim et al., 2024) demonstrating state-of-the-art performance and impressive generalization across diverse objects and environments. RT-2 integrates Internet-scale vision-language data with robotic trajectory data, while RT-2-X scales this further with a 55B-parameter policy trained on the Open X-Embodiment dataset (Collaboration et al., 2023). In contrast, OpenVLA integrates a robust open VLM backbone with an enriched robot pretraining dataset. Despite these advancements, current VLAs underutilize some of the most valuable features of their underlying language and vision-language models, specifically, their capacity to reason through the steps needed to solve complex tasks."}, {"title": "Reasoning for Robotics.", "content": "Prompting large language models (LLMs) to \u201cthink step-by-step\u201d (Kojima et al., 2022) when solving problems can significantly enhance their performance. Similar techniques have been explored in the context of high-level task planning for robotics (Liang et al., 2023; Ha et al., 2023). Expanding on this, Zawalski et al. (2024) introduced ECoT, a method that trains a VLA policy to autoregressively generate chain-of-thought (CoT) reasoning. ECoT combines high- and low-level reasoning with actionable steps, aligning these to an agent's environment based on input instructions and observations. While this equips VLAs with a better understanding of the current situation and task, it falls short in two key areas: long-horizon spatial reasoning for robot movement and visual grounding when augmenting reasoning data with Gemini. Building on these limitations, we build EMMA-X designed to enhance spatial reasoning and task planning in robotic policy generation. EMMA-X integrates grounded chain-of-thought reasoning to predict future states and transitions, enabling robots to develop more informed and effective action policies."}, {"title": "B Prompts and Sample Outputs", "content": "Template for prompting Gemini to generate subtasks and reasoning."}]}