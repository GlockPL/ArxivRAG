{"title": "WE-MATH: Does Your Large Multimodal Model Achieve Human-like Mathematical Reasoning?", "authors": ["Runqi Qiao", "Qiuna Tan", "Guanting Dong", "Minhui Wu", "Chong Sun", "Xiaoshuai Song", "Zhuoma GongQue", "Shanglin Lei", "Zhe Wei", "Miaoxuan Zhang", "Runfeng Qiao", "Yifan Zhang", "Xiao Zong", "Yida Xu", "Muxi Diao", "Zhimin Bao", "Chen Li", "Honggang Zhang"], "abstract": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has received widespread attention from the Large Multimodal Models (LMMs) community. Existing benchmarks focus more on the result-oriented performance, but neglecting the underlying principles in knowledge acquisition and generalization. Inspired by human-like mathematical reasoning, we introduce WE-MATH, the first benchmark specifically designed to explore the problem-solving principles beyond the end-to-end performance. We meticulously collect and categorize 6.5K visual math problems, spanning 67 hierarchical knowledge concepts and 5 layers of knowledge granularity. We firstly decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate Generalization (IG), Complete Mastery (CM), and Rote Memorization (RM) to hierarchically assess inherent issues in LMMs' reasoning process. With WE-MATH, we conduct a thorough evaluation of existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving step and problem-specific performance. We confirm the IK issue of LMMs can be effectively improved via knowledge augmentation strategy. More notably, the primary challenge of GPT-40 has significantly transitioned from IK to IG, establishing it as the first LMM advancing towards the knowledge generalization stage. In contrast, other LMMs exhibit a marked inclination towards Rote Memorization \u2013 they correctly solve composite problems involving multiple knowledge concepts, yet fail in answering sub-problems. We anticipate that WE-MATH will open new pathways for advancements in visual mathematical reasoning for LMMs.", "sections": [{"title": "1 Introduction", "content": "Human cognitive and reasoning patterns have profoundly shaped the progress of deep learning [1]. Initially, the design of neural networks [2] is inspired by the brain's neuronal mechanisms. It uses convolution kernels and hierarchical network to mimic human cognitive process of knowledge acquisition. Recently, Transformers [3] employ attention mechanisms to handle multiple information flows and quickly focus on critical content, thereby achieving more efficient and in-depth sequential learning. Owing the scalability of the Transformer architecture and pre-training techniques, Large Language Models (LLMs) [4, 5, 6, 7] and Large Multimodal Models (LMMs) [8, 9, 10, 11, 12, 13, 14, 15, 16] showcases strong reasoning abilities that parallel human performance across a wide range of tasks and provide a glimpse into the early outlines of Artificial General Intelligence (AGI).\nMathematical reasoning is a critical capability of foundational models. Existing methods employ Chain of Thought (COT) [17], Program of Thought (POT) [18, 19], Tool-integrated techniques [20, 21] and data augmentation strategies [22, 23, 24, 25] to guide LLMs towards emulating human-like reasoning patterns. In a more challenging scenario, Visual mathematical reasoning requires the model to accurately decode the visual information in image and perform reasoning based on the textual problem. With the rapid advancements of large multimodal models (LMMs) [26, 27], researchers progressively utilize the LMMs for solving visual mathematical problems [28, 29]. These studies provide valuable insights into the ongoing improvements in multi-modal logical thinking capabilities.\nTo systematically evaluate visual mathematical reasoning capabilities, previous efforts [30, 31, 32, 33] have focused on challenging geometric problems. Recently, several benchmarks [34, 35] expand the scope to include a wider range of disciplines. However, these benchmarks rely solely on end-to-end results for assessment, which fails to identify inherent issues within the LMMs' reasoning process. Moreover, MathVerse [35] attempt to directly evaluate reasoning paths based on reference answers, but limitations remain due to the knowledge-intensive nature of mathematical reasoning. While noticing that humans solve complex math problems through gradually mastering and generalizing the knowledge concepts [36], we claim a fair evaluation of a model's reasoning process should be based on knowledge concepts. Therefore, we pose two questions about mathematical reasoning evaluation:\nQ1: Does the correct answer truly reflect LMM's ability to reason through such problems accurately?\nQ2: Does an incorrect answer suggest a lack of foundational knowledge in LMM's reasoning process?\nAs the response, we present WE-MATH, a pioneering benchmark for conducting an in-depth analysis of the underlying principles of LMMs in visual mathematical reasoning. WE-MATH consists of over 6.5K meticulously selected visual math problems, which can be categorized into 5 layers of knowledge granularity across 67 knowledge concepts for ensuring a comprehensive coverage. We observe that real-world math problems typically encompass multiple foundational knowledge concepts, and their difficulty is directly related to the number of concepts involved. Upon this, we decouple the model's ability to solve composite problems with k knowledge concepts into two stages:\n1) LMMs can solve k individual sub-problems corresponding its knowledge concept;\n2) LMMs reason out the final answer by integrating the k individual knowledge concepts.\nThe above process can be formulated as follows:\n$P(Y|X) = \\prod_{i=1}^{k} P(Y_i|X_i) \\cdot P_{reason}$,"}, {"title": "2 WE-MATH", "content": "Overview of WE-MATH. As previously mentioned, existing benchmarks tend to be result-oriented, while overlooking the essence of solving mathematical problems. This leads to the generation of some counterintuitive evaluation conclusions. For example, conclusions in MathVista [34] indicate that LMMs exhibit superior performance on university-level problems compared to elementary-level ones. Different from existing benchmarks, WE-MATH is constructed around textbook knowledge units, decomposing composite problem solutions into sub-problems based on the knowledge concepts. WE-MATH has the following characteristics:\n(1) Hierarchical Knowledge Structure. WE-MATH strictly adheres to the knowledge presented in mathematics textbooks, featuring a rigorous hierarchical and multi-category architecture. It ensures the independence of knowledge concepts within the same level, while establishing logical relationships among concepts at different hierarchical levels.\n(2) Knowledge based Reasoning Evaluation. WE-MATH is designed to explore how LMMs solve problems. Drawing upon that humans tackle problems incrementally by leveraging fundamental knowledge concepts, we break down complex mathematical problems into more manageable sub-problems. Furthermore, we employ diverse measurement dimensions for meticulous evaluations.\n(3) Knowledge Concept Augmentation. To alleviate the inherent issues during the problem-solving process, we heuristically introduce descriptions for 67 knowledge concepts from Wikipedia and textbooks, thereby providing essential knowledge support for the reasoning processes of LMMs."}, {"title": "2.1 Hierachial Structured Dataset Composition", "content": "Hierachial Knowledge Structure. WE-MATH emphasizes fundamental math skills, believing that complex mathematical reasoning is built upon foundation of basic mathematical reasoning processes."}, {"title": "2.2 Knowledge based Reasoning Evaluation", "content": "Problem Definition. For the visual mathematical reasoning task, given text question $Q_i$, image $I_i$ and corresponding answer $A_i$. We define the LMMs evaluation dataset $D_{eval} = \\{(Q_i, I_i, A_i)|K_i, C_i\\}_{i=1}^{N}$ where $K_i$ and $C_i$ are two prior constraints for question $Q_i$. In detail, $K_i = \\{k_i\\}_{m=1}^{M}$ denote $M$ knowledge concepts within the question. $C_i$ represents the prerequisite conditions needed to solve the problem $Q_i$ (see Figure 3 for example). For the convenience of presenting, we define the problem containing k knowledge concepts as a \"k-step problem\" in our paper.\nKnowledge-based Data Decomposition. Real-world mathematical problems are composed of multiple atomic knowledge concepts. However, existing benchmarks usually overlook this in- formation, leading to unreasonable evaluation results. Inspired by Euclid's Elements [36], we argue that the evaluation of mathematical reasoning ability in LMMs essentially involves assessing their mastery of fundamental knowledge concepts. It is quite a natural and objective way to exploit basic knowledge concepts for reasoning evaluation of LMMs. Given an i-th test sample $\\{(Q_i, I_i, A_i)|K_i,C_i\\} \\in D_{WE-MATH}$ with M concepts $K_i = \\{k_i\\}_{m=1}^{M}$, we ask human experts to decompose each problem step by step into M sub-problems based on knowledge concepts, which can be formulated as:\n$\\{(q_m, i_m, a_m)| k_i,c\\}_{m=1}^{M} = \\underset{D_{WE-MATH}}{Decompose} \\{(Q_i, I_i, A_i)|K_i, C_i\\}$"}, {"title": "Metric for Reasoning Evaluation.", "content": "Based on the decomposed multi-step problems, we further reveal the inherent issues of LMMs in problem-solving process. We feed both the M one-step sub-problems and the original problem into LMMs, and classifying the responses into four categories:\n1. Insufficient Knowledge (IK): Part of one-step problems contain errors, and the multi-step problem is wrong. It is reasonable because model's insufficient grasp of single knowledge concept may lead to errors in multi-step problem.\n2. Inadequate Generalization (IG): One-Step problems are all correct, but the multi-step problem is incorrect. This is also considered reasonable. While LMMs are capable of understanding individual knowledge concepts, they may struggle to generalize that knowledge to solve composite problems."}, {"title": "2.3 Knowledge Concept Augmentation", "content": "In the previous section, we identify the Insufficient Knowledge (IK) as the foundation challenge in mathematical reasoning. To heuristically tackle this issue, we enlist human experts to create 67 knowledge concept cards, which is essential for LMM's reasoning process. Initially, expert annotators offer precise summaries derived from the definitions in Euclid's Elements [36], Wikipedia and textbooks. Subsequently, these experts further condense the content examined by a series of questions related to a specific knowledge concept, extracting crucial knowledge hints for incorporation into the knowledge cards. After several rounds of review, we confirm the accuracy and utility of each card."}, {"title": "3 Experiment", "content": "Evaluation Protocols. To accelerate the evaluation speed, WE-MATH comprises a testmini set with 1740 samples, including 1215 one-step samples, 360 two-step samples, and 165 three-step samples. In subsequent experiments, we utilize the WE-MATH testmini subset for evaluation. For automated evaluation, we standardize all samples into a multiple-choice format. We use regex to match the LMMs' predictions and then calculate their accuracy against the ground-truth answers for main results. For analyses in section 3.2 and 3.3, we utilize the four-dimensional metric described in section 2.2 for assessment. To avoid LMMs deduce answers from options, we introduce an extra uncertain option to mitigate this issue.\nEvaluation Models. We examine the performance of foundation models across two distinct cate- gories on WE-MATH: (a) Closed-source LMMs: GPT-40 [38], GPT-4V [26], Gemini 1.5 Pro [40], Qwen-VL-Max [13], (b) Open-source LMMs: LLaVA-NeXT-110B, LLaVA-NeXT-70B [39], LLaVA- 1.6-13B, LLaVA-1.6-7B [41], DeepSeek-VL-1.3B, DeepSeek-VL-7B [42], Phi3-Vision-4.2B [43], MiniCPM-Llama3-V 2.5 [44], InternLM-XComposer2-VL-7B [45], InternVL-Chat-V1.5 [46], GLM- 4V-9B [47], LongVA [48], G-LLaVA-13B [29]."}, {"title": "3.1 Main Result", "content": "Table 1 shows the overall performance of different LMMs on One-Step / Two-Step / Three-Step problems and different problem domains. We have the following observations:\nThe Nums of Knowledge Concepts are negatively correlated with LMMs' Performance. Regard- ing problems of varying complexities (one-step vs. two-step vs. three-step), GPT-40 consistently achieve an advantage across all settings. Other closed-source models, such as GPT-4V and Gemini 1.5 Pro, also demonstrate competitive performance. However, most LMMs perform significantly worse on multi-step problems compared to one-step problems. For instance, GPT-4o's accuracy drops from 72.84% to 43.64%. This trend is even more pronounced in stronger models like LLaVA-NeXT-110B and InternVL-Chat-V1.5. These observations suggest that the number of knowledge concepts in a question is positively correlated with its difficulty and negatively correlated with LMMs' performance, supporting the rationale for decomposing questions to a certain extent."}, {"title": "3.2 Knowledge based Reasoning Analysis", "content": "Table 2 and Figure 7, 8, 9 illustrate the results of knowledge based reasoning evaluation, including four distinct conditions (IK, IG, CM, RM). We have the following observations:\nIK is the Greatest Vulnerability of LMMs. All LMMs consistently demonstrate an Insufficient Knowledge issue during the reasoning process, especially in models with smaller parameter scales (LLaVA-1.6-7B, DeepSeek-VL-1.3B). As discussed in section 2.2, addressing IK is crucial for progressing towards Inadequate Generalization (IG) and Complete Mastery (CM). This knowledge gap in solving one-step problems hinders further progress in reasoning about more composite mathematical problems. This finding also supports the rationale behind our proposed KCA strategy.\nGPT-40's Main Challenge has gradually shifted from IK to IG, highlighting it as the First LMM towards the Knowledge Generalization Stage. Focusing on IK and IG, GPT-40 exhibits a substantial lead in addressing the IK issue, but the weakest performance in IG. Further analyzing"}, {"title": "3.3 Quantitative Analysis", "content": "The Effectiveness on KCA. Figure 10 displays the quantitative analysis of the LMMs with knowl- edge concept augmented (KCA). We find that LMMs with different parameter scales show con- sistent performance improvements on both strict and loose metrics after introducing the KCA strat- egy. Moreover, the KCA strategy significantly alleviates IK issues but does not noticeably im- prove IG. This aligns with human intuition, as the knowledge descriptions primarily address gaps in reasoning knowledge. Nevertheless, alleviating IG issues requires a comprehensive enhancement of the LMMs' knowledge generalization abilities, which we consider a direction for future explo- ration.\nError Anaysis. Figure 11 shows the occurrence of the four types of errors across the 67 knowledge concepts. Knowledge errors are the most frequent, appearing in over 45 knowledge concepts. Notably, although visual errors are the second most common, they are more concentrated in specific concepts (e.g., \"Understanding Angles\" >10), and over 38 concepts have no visual errors. This finding underscores the urgent need to enhance the fine-grained measurement capabilities of visual encoders in LMMs for mathematical reasoning, rather than blindly improving their overall capabilities."}, {"title": "4 Related Work", "content": "Mathematical Reasoning Benchmarks. Assessing mathematical reasoning abilities is crucial for the development of large foundational models (LLMs and LMMs). Early efforts, such as MathQA [49], focus on solving mathematical word problems and highlight the importance of operation-based reasoning. Following this, datasets like GSM8K [50] and MATH [51] set the stage for evaluating text- based mathematical problems at various difficulty levels. Other benchmarks, such as MMLU [52] and MT-Bench [53], also consider mathematical evaluation as a key part of assessing LLMs. Beyond text- only evaluations, datasets like GeoQA [32], UniGeo [33], and Geometry3K [30] have pioneered the evaluation of geometric problems. Recently, several benchmarks [34] [54] have expanded their scope to cover a broader range of subjects. Additionally, MathVerse [35] aims to evaluate reasoning paths based on reference answers. However, challenges remain due to the complex nature of mathematical reasoning. In this paper, we introduce WE-MATH, a comprehensive benchmark designed to evaluate the reasoning abilities of LMMs across a wide range of mathematical categories.\nBenchmarks for Large Multimodal Model. The rapid advancement of Large Language Mod- els (LLMs) and Large Multimodal Models (LMMs) have highlighted the necessity for more comprehensive evaluation benchmarks. At first, the emergence of a series of text-only bench- marks and evaluations give us a clearer understanding of the strengths and weaknesses of large language models [50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62]. Focusing on the visual as- pect, early benchmarks predominantly focused on narrow tasks like Visual Question Answering (VQA) [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75] and image captioning [76, 77, 78], showcas- ing significant progress but not fully addressing the broader spectrum of multimodal perception and reasoning. This gap has driven recent research to assess LMMs from multiple angles. Notable efforts include MMBench [79] and SEED-bench [80, 81], which probe models' abilities through common- sense queries incorporating multiple-choice formats. For domain-specific expertise, MMMU [54] utilize academic content to gauge deeper knowledge levels. Yet, benchmark such as MMStar [82] reveals that certain evaluations allow models to respond without images, risking data leakage and failing to adequately measure logic and reasoning skills. The challenge of understanding image impli- cations, requiring multi-hop reasoning and theory of mind (ToM) [83, 84, 85, 85, 86, 87], underscores this shortfall. In parallel, the intersection of large language models (LLMs) and Large Multimodal Models (LMMs) has surged, extending the applicability of LMMs evaluations across diverse modal- ities including 2D images [88, 89, 90], 3D point clouds [91, 92, 93], audio [94, 95, 96, 97], and video [98, 99, 100]. Moreover, a series of works have positioned LMMs as agents with various tools, such as APIs [101, 102, 103], retrievers [104, 105], thereby broadening the development avenues for the model evaluation community [106, 107, 108, 109]."}, {"title": "5 Conclusion", "content": "In this paper, we propose WE-MATH, a comprehensive benchmark for in-depth analysis of LMMs in visual mathematical reasoning. WE-MATH encompasses 6.5K visual math problems, covering 5 layers and 67 knowledge concepts. Moreover, we pioneeringly decompose composite problems into sub-problems according to the required knowledge concepts and introduce a novel four-dimensional metric for fine-grained reasoning evaluation. With WE-MATH, we thoroughly evaluate existing LMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and problem-specific performance. Furthermore, we identify IK issues as the greatest vulnerability of LMMs. However, GPT-4o's main challenge has shifted from IK to IG, highlighting it the first LMM towards the next stage. Lastly, analyses on KCA strategy and error cases further heuristically guides existing LMMs towards human-like visual mathematical reasoning."}, {"title": "A Broaden Impact", "content": "Bridging Human-Like Inspiration and Reliability. As previously mentioned, works such as neural networks [2] and attention mechanisms [3] draw their design inspiration from human thinking patterns. This is fundamentally because the purpose of designing AI is to assist humans. Currently, LMMs have already been helping people in various scenarios, which was unimaginable in the past. Therefore, we firmly believe that a new era is coming, where people will focus not only on the performance of models in specific fields but also on the reliability of a model. In some fundamental scenarios, a reliable model is more important, which is one of the primary motivations behind the creation of WE-MATH. Furthermore, after completing our experiments, we find that in a loose setting, GPT-40's RM metric is only 1.07%, showing us the possibility of a reliable and accurate model emerging in the future.\nFine-grained Evaluation and Versatile Applications. From the model's perspective, WE-MATH can provide LMMs with an assessment of mathematical abilities. Additionally, WE-MATH'S IK, IG, and CM metrics offer a fine-grained evaluation of the model's capabilities. Furthermore, the RM metric reflects a model's reliability to address our concern of not desiring a model that can solve complex problems but makes errors on sub-problems within the solution process. Ultimately, we introduce the Scoreaverage metric to quantify the model's overall performance. Moreover, since WE-MATH is constructed from the decomposition of a multi-step problem's necessary solution process, it provides new perspectives for interactive tasks (multi-turn dialogues), self-supervised learning, information extraction, and other tasks. It also offers crucial references and support for the deployment of models in education and other fields.\nEthics Statement. We ensure that WE-MATH complies with legal and ethical guidelines throughout its construction process, with no violations. We provide fair compensation to all annotators involved. WE-MATH focuses on elementary mathematics problems, and during its construction, data collection was sourced from publicly available test questions, textbooks, and professional websites. Since mathematics problems inherently have standard answers, they are not subject to cultural differences. Additionally, we guarantee that WE-MATH is solely for academic research purposes, and we up- hold the strict prohibition of any commercial use. Additionally, we declare that we will bear full responsibility in the event of any rights violations and confirm the data license."}, {"title": "B More Details on WE-MATH", "content": "B.1 Hierarchical Knowledge Structure\nFigure 12, 13 shows the detailed hierarchical structure of WE-MATH, which includes 5 levels, 99 nodes, and 67 leaf nodes.\nIn the initial stages of constructing the benchmark, we aimed to address two key objectives. We believe that the purpose of designing a benchmark is to evaluate the performance of models and provide guidance on areas that need improvement. However, existing benchmarks offer only broad guides in these aspects. Additionally, the core contribution mentioned earlier is that WE-MATH is the first benchmark specifically designed to study the mathematical problem-solving mechanisms of models. Inspired by the learning paradigm of humans, which is based on knowledge concepts, WE-MATH constructs its dataset with knowledge concepts as the basic unit, resulting in evaluations with rigorous scientific accuracy and better guidance.\nB.2 Knowledge-based Data Decomposition\nFigures 14, 15 illustrate the process of Knowledge-based Data Decomposition.\nCollection. In each example, the Collection section presents specific information about each multi- step problem in the dataset.\nHuman reasoning. The Human reasoning section shows the process required before decomposing each multi-step problem, where educational experts extract the key information needed for each sub-problem based on the reasoning path for the knowledge concepts included in the multi-step problem."}, {"title": "B.3 Knowledge Concepts Augmentation", "content": "Table 3 report the prompt templates in our experiments. We concatenate the textual descriptions into the prompt. Additionally, each knowledge concept description is accompanied by its corresponding visual content, which helps the experimenter understand and facilitates further enhancement when models can incorporate sufficient visual information as part of the prompt in the future.\nIn section F.1, we illustrates the specific content of descriptions for 67 knowledge concepts. For example, as shown in Figure 46, for the knowledge concept \"Perimeter of Squares,\" it is necessary to know that \"c=4a\", relying solely on textual descriptions is insufficient for understanding this concept, so we include visual information to aid comprehension."}, {"title": "B.4 Details of Data Collection", "content": "With the hierarchical knowledge structure, we select geometric problems with images from publicly authoritative mathematics websites from various countries, including professional exams and practice tests (detailed sources list can be found in section F.2). To ensure comprehensive coverage of fundamental and critical areas in primary math, we select the five most foundational and prevalent domains within the field of primary geometry, including:\n\u2022 Plane figures: Questions involving identification and properties of two-dimensional shapes.\n\u2022 Solid figures: Questions related to the recognition and characteristics of three-dimensional objects.\n\u2022 Transformation and motion of figures: Problems focusing on geometric transformations such as translation, rotation, and reflection.\n\u2022 Position and direction: Questions that involve understanding spatial relationships and directions.\n\u2022 Measurement: Problems requiring the measurement of length, area, volume, and angles.\nThe selection criteria are as follows: (1) The problems include multiple knowledge concepts and can be decomposed into steps for solution. (2) The problems and images are consistent. (3) The correct answer is unique, and the distractor options are highly confusing."}, {"title": "B.5 Details of Data Statistics", "content": "Question distribution. The WE-MATH con- sists entirely of English questions, all newly collected from public authoritative mathe- matics websites, and presented in the format of multiple-choice questions. As illustrated in Table 4, the average number of words in the English questions within WE-MATH is 25.81, with the maximum number of words in a question reaching 143. Figure 16 fur- ther elaborates on the distribution of word counts, highlighting the diverse patterns of the questions.\nAdvantages of Multiple-Choice Questions.\nIn WE-MATH, all problems are presented as multiple-choice questions. Even if some problems did not originally conform to the multiple-choice format during the initial se- lection, our researchers manually converted them into the format. Using multiple-choice questions offers several advantages:\nStandardization: Ensures uniformity across all questions, facilitating consistent assess- ment and comparison across different hierar- chical subjects.\nObjective Grading: The use of single cor- rect answers eliminates subjectivity in grad- ing, enhancing the reliability of the evalua- tion.\nEfficiency: Allows for rapid and scalable assessment, suitable for large datasets and automated systems.\nFocused Assessment: Carefully designed distractors help in accurately identifying specific knowl- edge gaps and common misconceptions."}, {"title": "C More Details on the Metrics", "content": "Distinguishing Metric. Considering the model's instability, Figure 4 and Figure 17, 18 illustrate the two metrics we propose for distinguishing between RM and CM metrics. Figure 4 represents the"}, {"title": "D More Details on Experiment Setup", "content": "D.1 Details of the Evaluated Models\nTo evaluate the mathematical reasoning abilities of various LMMs, we selected their latest model versions. Table 5 presents their release dates and specific sources. Given the intuition that smaller models (with parameters of 7B or less) perform poorly on Insufficient Knowledge (IK), we also included evaluations of the latest models with 7B, 4.2B, and 1.3B parameters. This was done to explore whether these models could achieve significant improvement under the KCA strategy.\nD.2 Details of the Model Hyperparameters\nFor all closed-sourced models with API access, we adopt the generation scheme shown in Table 6 and simply run the inference with CPUs, which typically completes within a day. For all open-source models, we utilize a cluster with 8 NVIDIA A800-SXM4-80GB GPUs to run the inference, and we follow the hyper-parameter settings specified in the model source's inference samples. If no specific instructions are provided, we use the default settings."}, {"title": "E More Details on Experiment Results", "content": "E.1 Details of Model Performance\nThe Leaderboard on WE-MATH. We present the visualization results of the Scoreaverage under the loose and strict metric in Figure 7, respectively. GPT-4o shows a significant lead under both metric, and LLaVA-NeXT-110B performs the best among open-source models. Impressively, InternVL-Chat- V1.5 and GLM-4V-9B achieved excellent scores, surpassing the closed-source model Qwen-VL- Max. Additionally, some recently proposed smaller models (such as Phi-3-Vision-4.2B, InternLM- XComposer2-VL-7B, and MiniCPM-LLaMA3-V 2.5) also demonstrated outstanding performance, suggesting that optimizing training methods might partially substitute for the performance gains typically achieved by increasing the parameter count.\nDetailed Performance of Four-Dimensional Metrics. Figure 8 and Figure 9 display the specific performance of LMMs under both loose and strict metric across four metrics. Focusing on the IK metric, GPT-4o has the fewest instances under both metric, indicating that GPT-40 has the best grasp of the knowledge concepts. Furthermore, for the IG metric, we find that GPT-40 and GPT-4V have the highest exposure compared to other models. As discussed in the previous Section C, IG issues only arise after addressing IK issues, which further indicates that GPT-4 is progressing to the next stage. Focusing on the CM and RM metrics, GPT-40 and GPT-4V continue to show significant leadership. Both models excel in the CM metric, where the number of correctly answered multi-step problems and their corresponding sub-questions is significantly higher than that of other models. Additionally, comparing GPT-40 and GPT-4V under strict metric, GPT-40 consistently outperforms GPT-4V, aligning with the Scoreaverage results.\nDetailed Performance on Each Category. In Figure 6, we present the performance of open-source and closed-source models under the second-level nodes. In Figure 19 to Figure 35, we detail the specific performance of 17 models across 67 knowledge concepts (based on statistics from one-step problem questions). It is evident that GPT-40 consistently leads in overall performance, but its main issue lies in measurement-related tasks. Notably, some open-source models perform worse on the simpler \"Understanding and Conversion of Units\" knowledge concepts compared to \"Angles and Length\" related concepts, while InternVL-Chat-V1.5 and MiniCPM-LLaMA3-V 2.5 exhibit more logically consistent results."}, {"title": "E.2 Specific Error Analysis", "content": "Error Types. To delve into the failure cases of models, we detailed four typical error types in Table 9. Furthermore, to facilitate a better understanding of each error type, we provide examples of each error made by GPT-40 from Figure 36 to Figure 39. Since a single thought process in a problem can involve multiple errors and a single logical error is enough to derail a much larger solution, we consider the first error that occurs in the reasoning steps as the key error and include only this error in our statistics.\nCorrespondence of Errors in Multi-Step and One-Step Problems. Focusing on Insufficient Knowledge, the errors in multi-step problems often correspond to those in one-step problems. This supports our approach of decomposing problems to accurately associate error types with specific knowledge concepts. Furthermore, we observe a positive correlation between the quantity of knowledge concepts and the errors in the reasoning process. As the complexity of knowledge concepts increases, the difficulty for the model to perform multi-step reasoning also increases, leading to a higher likelihood of visual recognition errors and incorrect application of knowledge concepts."}]}