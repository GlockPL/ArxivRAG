{"title": "WE-MATH: Does Your Large Multimodal Model\nAchieve Human-like Mathematical Reasoning?", "authors": ["Runqi Qiao", "Qiuna Tan", "Guanting Dong", "Minhui Wu", "Chong Sun", "Xiaoshuai Song", "Zhuoma GongQue", "Shanglin Lei", "Zhe Wei", "Miaoxuan Zhang", "Runfeng Qiao", "Yifan Zhang", "Xiao Zong", "Yida Xu", "Muxi Diao", "Zhimin Bao", "Chen Li", "Honggang Zhang"], "abstract": "Visual mathematical reasoning, as a fundamental visual reasoning ability, has\nreceived widespread attention from the Large Multimodal Models (LMMs) com-\nmunity. Existing benchmarks focus more on the result-oriented performance,\nbut neglecting the underlying principles in knowledge acquisition and generaliza-\ntion. Inspired by human-like mathematical reasoning, we introduce WE-MATH,\nthe first benchmark specifically designed to explore the problem-solving princi-\nples beyond the end-to-end performance. We meticulously collect and categorize\n6.5K visual math problems, spanning 67 hierarchical knowledge concepts and\n5 layers of knowledge granularity. We firstly decompose composite problems\ninto sub-problems according to the required knowledge concepts and introduce a\nnovel four-dimensional metric, namely Insufficient Knowledge (IK), Inadequate\nGeneralization (IG), Complete Mastery (CM), and Rote Memorization (RM) to hi-\nerarchically assess inherent issues in LMMs' reasoning process. With WE-MATH,\nwe conduct a thorough evaluation of existing LMMs in visual mathematical rea-\nsoning and reveal a negative correlation between solving step and problem-specific\nperformance. We confirm the IK issue of LMMs can be effectively improved\nvia knowledge augmentation strategy. More notably, the primary challenge of\nGPT-40 has significantly transitioned from IK to IG, establishing it as the first\nLMM advancing towards the knowledge generalization stage. In contrast, other\nLMMs exhibit a marked inclination towards Rote Memorization \u2013 they correctly\nsolve composite problems involving multiple knowledge concepts, yet fail in an-\nswering sub-problems. We anticipate that WE-MATH will open new pathways for\nadvancements in visual mathematical reasoning for LMMs. The WE-MATH data\nand evaluation code are available at https://github.com/We-Math/We-Math.", "sections": [{"title": "1 Introduction", "content": "\"I think", "evaluation": "nQ1: Does the correct answer truly reflect LMM's ability to reason through such problems accurately?\nQ2: Does an incorrect answer suggest a lack of foundational knowledge in LMM's reasoning process?\nAs the response", "stages": "n1) LMMs can solve k individual sub-problems corresponding its knowledge concept;\n2) LMMs reason out the final answer by integrating the k individual knowledge concepts.\nThe above process can be formulated as follows:\n$\\P(Y|X) = \\prod_{i=1"}, {}, {"title": "2 WE-MATH", "content": "Overview of WE-MATH. As previously mentioned, existing benchmarks tend to be result-oriented,\nwhile overlooking the essence of solving mathematical problems. This leads to the generation of\nsome counterintuitive evaluation conclusions. For example, conclusions in MathVista [34] indicate\nthat LMMs exhibit superior performance on university-level problems compared to elementary-level\nones. Different from existing benchmarks, as shown in Figure 2, WE-MATH is constructed around\ntextbook knowledge units, decomposing composite problem solutions into sub-problems based on\nthe knowledge concepts. WE-MATH has the following characteristics:\n(1) Hierarchical Knowledge Structure. WE-MATH strictly adheres to the knowledge presented\nin mathematics textbooks, featuring a rigorous hierarchical and multi-category architecture. It\nensures the independence of knowledge concepts within the same level, while establishing logical\nrelationships among concepts at different hierarchical levels.\n(2) Knowledge based Reasoning Evaluation. WE-MATH is designed to explore how LMMs solve\nproblems. Drawing upon that humans tackle problems incrementally by leveraging fundamental\nknowledge concepts, we break down complex mathematical problems into more manageable sub-\nproblems. Furthermore, we employ diverse measurement dimensions for meticulous evaluations.\n(3) Knowledge Concept Augmentation. To alleviate the inherent issues during the problem-solving\nprocess, we heuristically introduce descriptions for 67 knowledge concepts from Wikipedia and\ntextbooks, thereby providing essential knowledge support for the reasoning processes of LMMs."}, {"title": "2.1 Hierachial Structured Dataset Composition", "content": "Hierachial Knowledge Structure. WE-MATH emphasizes fundamental math skills, believing that\ncomplex mathematical reasoning is built upon foundation of basic mathematical reasoning processes."}, {"title": "2.2 Knowledge based Reasoning Evaluation", "content": "Problem Definition. For the visual mathematical reasoning task, given text question $Q_i$, image $I_i$ and\ncorresponding answer $A_i$. We define the LMMs evaluation dataset $D_{eval} = \\{(Q_i, I_i, A_i)|K_i, C_i\\}_{i=1}^{N}$\nwhere $K_i$ and $C_i$ are two prior constraints for question $Q_i$. In detail, $K_i = \\{k_i\\}_{i=1}^{M}$ denote M\nknowledge concepts within the question. $C_i$ represents the prerequisite conditions needed to solve\nthe problem $Q_i$ (see Figure 3 for example). For the convenience of presenting, we define the problem\ncontaining k knowledge concepts as a \"k-step problem\" in our paper.\nKnowledge-based Data Decomposition. Real-world mathematical problems are composed of\nmultiple atomic knowledge concepts. However, existing benchmarks usually overlook this in-\nformation, leading to unreasonable evaluation results. Inspired by Euclid's Elements [36], we\nargue that the evaluation of mathematical reasoning ability in LMMs essentially involves assess-\ning their mastery of fundamental knowledge concepts. It is quite a natural and objective way to\nexploit basic knowledge concepts for reasoning evaluation of LMMs. Given an i-th test sample\n$\\{(Q_i, I_i, A_i)|K_i,C_i\\} \\in D_{WE-MATH}$ with M concepts $K_i = \\{k_i\\}_{i=1}^{M}$, we ask human experts to\ndecompose each problem step by step into M sub-problems based on knowledge concepts, which\ncan be formulated as:\n$\\Decompose \\{(Q_i, I_i, A_i)|K_i, C_i\\} = \\{(q_m, i_m, a_m) | k_i^m, c\\}_{m=1}^{M}$ (2)"}, {"title": "2.3 Knowledge Concept Augmentation", "content": "In the previous section, we identify the Insufficient Knowledge (IK) as the foundation challenge\nin mathematical reasoning. To heuristically tackle this issue, we enlist human experts to create 67\nknowledge concept cards, which is essential for LMM's reasoning process. Initially, expert annotators\noffer precise summaries derived from the definitions in Euclid's Elements [36], Wikipedia and\ntextbooks. Subsequently, these experts further condense the content examined by a series of questions\nrelated to a specific knowledge concept, extracting crucial knowledge hints for incorporation into the\nknowledge cards. After several rounds of review, we confirm the accuracy and utility of each card."}, {"title": "3 Experiment", "content": "Evaluation Protocols. To accelerate the evaluation speed, WE-MATH comprises a testmini set\nwith 1740 samples, including 1215 one-step samples, 360 two-step samples, and 165 three-step\nsamples. In subsequent experiments, we utilize the WE-MATH testmini subset for evaluation. For\nautomated evaluation, we standardize all samples into a multiple-choice format. We use regex to\nmatch the LMMs' predictions and then calculate their accuracy against the ground-truth answers for\nmain results. For analyses in section 3.2 and 3.3, we utilize the four-dimensional metric described\nin section 2.2 for assessment. To avoid LMMs deduce answers from options, we introduce an extra\nuncertain option to mitigate this issue.\nEvaluation Models. We examine the performance of foundation models across two distinct cate-\ngories on WE-MATH: (a) Closed-source LMMs: GPT-4o [38], GPT-4V [26], Gemini 1.5 Pro [40],\nQwen-VL-Max [13], (b) Open-source LMMs: LLaVA-NeXT-110B, LLaVA-NeXT-70B [39], LLaVA-\n1.6-13B, LLaVA-1.6-7B [41], DeepSeek-VL-1.3B, DeepSeek-VL-7B [42], Phi3-Vision-4.2B [43],\nMiniCPM-Llama3-V 2.5 [44], InternLM-XComposer2-VL-7B [45], InternVL-Chat-V1.5 [46], GLM-\n4V-9B [47], LongVA [48], G-LLaVA-13B [29]."}, {"title": "3.1 Main Result", "content": "Table 1 shows the overall performance of different LMMs on One-Step / Two-Step / Three-Step\nproblems and different problem domains. We have the following observations:\nThe Nums of Knowledge Concepts are negatively correlated with LMMs' Performance. Regard-\ning problems of varying complexities (one-step vs. two-step vs. three-step), GPT-4o consistently\nachieve an advantage across all settings. Other closed-source models, such as GPT-4V and Gemini 1.5\nPro, also demonstrate competitive performance. However, most LMMs perform significantly worse\non multi-step problems compared to one-step problems. For instance, GPT-4o's accuracy drops from\n72.84% to 43.64%. This trend is even more pronounced in stronger models like LLaVA-NeXT-110B\nand InternVL-Chat-V1.5. These observations suggest that the number of knowledge concepts in a\nquestion is positively correlated with its difficulty and negatively correlated with LMMs' performance,\nsupporting the rationale for decomposing questions to a certain extent."}, {"title": "3.2 Knowledge based Reasoning Analysis", "content": "Table 2 and Figure 7, 8, 9 illustrate the results of knowledge based reasoning evaluation, including\nfour distinct conditions (IK, IG, CM, RM). We have the following observations:\nIK is the Greatest Vulnerability of LMMs. All LMMs consistently demonstrate an Insufficient\nKnowledge issue during the reasoning process, especially in models with smaller parameter scales\n(LLaVA-1.6-7B, DeepSeek-VL-1.3B). As discussed in section 2.2, addressing IK is crucial for\nprogressing towards Inadequate Generalization (IG) and Complete Mastery (CM). This knowledge\ngap in solving one-step problems hinders further progress in reasoning about more composite\nmathematical problems. This finding also supports the rationale behind our proposed KCA strategy.\nGPT-40's Main Challenge has gradually shifted from IK to IG, highlighting it as the First\nLMM towards the Knowledge Generalization Stage. Focusing on IK and IG, GPT-40 exhibits a\nsubstantial lead in addressing the IK issue, but the weakest performance in IG. Further analyzing"}, {"title": "3.3 Quantitative Analysis", "content": "The Effectiveness on KCA. Figure 10 displays\nthe quantitative analysis of the LMMs with knowl-\nedge concept augmented (KCA). We find that\nLMMs with different parameter scales show con-\nsistent performance improvements on both strict\nand loose metrics after introducing the KCA strat-\negy. Moreover, the KCA strategy significantly\nalleviates IK issues but does not noticeably im-\nprove IG. This aligns with human intuition, as the\nknowledge descriptions primarily address gaps in\nreasoning knowledge. Nevertheless, alleviating\nIG issues requires a comprehensive enhancement\nof the LMMs' knowledge generalization abilities,\nwhich we consider a direction for future explo-\nration.\nError Anaysis. Figure 11 shows the occurrence of the four types of errors across the 67 knowledge\nconcepts. Knowledge errors are the most frequent, appearing in over 45 knowledge concepts. Notably,\nalthough visual errors are the second most common, they are more concentrated in specific concepts\n(e.g., \"Understanding Angles\" >10), and over 38 concepts have no visual errors. This finding\nunderscores the urgent need to enhance the fine-grained measurement capabilities of visual encoders\nin LMMs for mathematical reasoning, rather than blindly improving their overall capabilities."}, {"title": "4 Related Work", "content": "Mathematical Reasoning Benchmarks. Assessing mathematical reasoning abilities is crucial for the\ndevelopment of large foundational models (LLMs and LMMs). Early efforts, such as MathQA [49],\nfocus on solving mathematical word problems and highlight the importance of operation-based\nreasoning. Following this, datasets like GSM8K [50] and MATH [51] set the stage for evaluating text-\nbased mathematical problems at various difficulty levels. Other benchmarks, such as MMLU [52] and\nMT-Bench [53], also consider mathematical evaluation as a key part of assessing LLMs. Beyond text-\nonly evaluations, datasets like GeoQA [32], UniGeo [33], and Geometry3K [30] have pioneered the\nevaluation of geometric problems. Recently, several benchmarks [34] [54] have expanded their scope\nto cover a broader range of subjects. Additionally, MathVerse [35] aims to evaluate reasoning paths\nbased on reference answers. However, challenges remain due to the complex nature of mathematical\nreasoning. In this paper, we introduce WE-MATH, a comprehensive benchmark designed to evaluate\nthe reasoning abilities of LMMs across a wide range of mathematical categories.\nBenchmarks for Large Multimodal Model. The rapid advancement of Large Language Mod-\nels (LLMs) and Large Multimodal Models (LMMs) have highlighted the necessity for more\ncomprehensive evaluation benchmarks. At first, the emergence of a series of text-only bench-\nmarks and evaluations give us a clearer understanding of the strengths and weaknesses of large\nlanguage models [50, 51, 52, 53, 55, 56, 57, 58, 59, 60, 61, 62]. Focusing on the visual as-\npect, early benchmarks predominantly focused on narrow tasks like Visual Question Answering\n(VQA) [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75] and image captioning [76, 77, 78], showcas-\ning significant progress but not fully addressing the broader spectrum of multimodal perception and\nreasoning. This gap has driven recent research to assess LMMs from multiple angles. Notable efforts\ninclude MMBench [79] and SEED-bench [80, 81], which probe models' abilities through common-\nsense queries incorporating multiple-choice formats. For domain-specific expertise, MMMU [54]\nutilize academic content to gauge deeper knowledge levels. Yet, benchmark such as MMStar [82]\nreveals that certain evaluations allow models to respond without images, risking data leakage and\nfailing to adequately measure logic and reasoning skills. The challenge of understanding image impli-\ncations, requiring multi-hop reasoning and theory of mind (ToM) [83, 84, 85, 85, 86, 87], underscores\nthis shortfall. In parallel, the intersection of large language models (LLMs) and Large Multimodal\nModels (LMMs) has surged, extending the applicability of LMMs evaluations across diverse modal-\nities including 2D images [88, 89, 90], 3D point clouds [91, 92, 93], audio [94, 95, 96, 97], and\nvideo [98, 99, 100]. Moreover, a series of works have positioned LMMs as agents with various tools,\nsuch as APIs [101, 102, 103], retrievers [104, 105], thereby broadening the development avenues for\nthe model evaluation community [106, 107, 108, 109]."}, {"title": "5 Conclusion", "content": "In this paper, we propose WE-MATH, a comprehensive benchmark for in-depth analysis of LMMs\nin visual mathematical reasoning. WE-MATH encompasses 6.5K visual math problems, covering 5\nlayers and 67 knowledge concepts. Moreover, we pioneeringly decompose composite problems into\nsub-problems according to the required knowledge concepts and introduce a novel four-dimensional\nmetric for fine-grained reasoning evaluation. With WE-MATH, we thoroughly evaluate existing\nLMMs in visual mathematical reasoning and reveal a negative correlation between solving steps and\nproblem-specific performance. Furthermore, we identify IK issues as the greatest vulnerability of\nLMMs. However, GPT-4o's main challenge has shifted from IK to IG, highlighting it the first LMM\ntowards the next stage. Lastly, analyses on KCA strategy and error cases further heuristically guides\nexisting LMMs towards human-like visual mathematical reasoning."}, {"title": "A Broaden Impact", "content": "Bridging Human-Like Inspiration and Reliability. As previously mentioned, works such as\nneural networks [2", "3": "draw their design inspiration from human thinking\npatterns. This is fundamentally because the purpose of designing AI is to assist humans. Currently,\nLMMs have already been helping people in various scenarios, which was unimaginable in the past.\nTherefore, we firmly believe that a new era is coming, where people will focus not only on the\nperformance of models in specific fields but also on the reliability of a model. In some fundamental\nscenarios, a reliable model is more important, which is one of the primary motivations behind the\ncreation of WE-MATH. Furthermore, after completing our experiments, we find that in a loose setting,\nGPT-40's RM metric is only 1.07%, showing us the possibility of a reliable and accurate model\nemerging in the future.\nFine-grained Evaluation and Versatile Applications. From the model's perspective, WE-MATH\ncan provide LMMs with an assessment of mathematical abilities. Additionally, WE-MATH'S IK,\nIG, and CM metrics offer a fine-grained evaluation of the model's capabilities. Furthermore, the\nRM metric reflects a model's reliability to address our concern of not desiring a model that can\nsolve complex problems but makes errors on sub-problems within the solution process. Ultimately,\nwe introduce the Scoreaverage metric to quantify the model's overall performance. Moreover, since\nWE-MATH is constructed from the decomposition of a multi-step problem's necessary solution\nprocess, it provides new perspectives for interactive tasks (multi-turn dialogues), self-supervised\nlearning, information extraction, and other tasks. It also offers crucial references and support for the\ndeployment of models in education and other fields.\nEthics Statement. We ensure that WE-MATH complies with legal and ethical guidelines throughout\nits construction process, with no violations. We provide fair compensation to all annotators involved.\nWE-MATH focuses on elementary mathematics problems, and during its construction, data collection\nwas sourced from publicly available test questions, textbooks, and professional websites. Since\nmathematics problems inherently have standard answers, they are not subject to cultural differences.\nAdditionally, we guarantee that WE-MATH is solely for academic research purposes, and we up-"}]}