{"title": "Nearest Neighbor CCP-Based Molecular Sequence Analysis", "authors": ["Sarwan Ali", "Prakash Chourasia", "Bipin Koirala", "Murray Patterson"], "abstract": "Molecular sequence analysis is crucial for comprehending several biological processes, including protein-protein interactions, functional annotation, and disease classification. The large number of sequences and the inherently complicated nature of protein structures make it challenging to analyze such data. Finding patterns and enhancing subsequent research requires the use of dimensionality reduction and feature selection approaches. Recently, a method called Correlated Clustering and Projection (CCP) has been proposed as an effective method for biological sequencing data. The CCP technique is still costly to compute even though it is effective for sequence visualization. Furthermore, its utility for classifying molecular sequences is still uncertain. To solve these two problems, we present a Nearest Neighbor Correlated Clustering and Projection (CCP-NN)-based technique for efficiently preprocessing molecular sequence data. To group related molecular sequences and produce representative supersequences, CCP makes use of sequence-to-sequence correlations. As opposed to conventional methods, CCP doesn't rely on matrix diagonalization, therefore it can be applied to a range of machine-learning problems. We estimate the density map and compute the correlation using a nearest-neighbor search technique. We performed molecular sequence classification using CCP and CCP-NN representations to assess the efficacy of our proposed approach. Our findings show that CCP-NN considerably improves classification task accuracy as well as significantly outperforms CCP in terms of computational runtime.", "sections": [{"title": "I. INTRODUCTION", "content": "Molecular sequences are a crucial part of the dynamic changes in sequence composition that control biological processes. Researchers have analyzed these molecular sequences and made progress toward a better understanding of physiology, biological development, and disease [1]. To understand various biological mechanisms and disorders, it is essential to understand how proteins interact with one another and carry out certain functions [2]. Unfortunately, because of their vastness, complexity, and lack of distinct patterns, it is still difficult to analyze thousands of molecular sequences at once. The development of medicines and treatments for human diseases is hampered by these obstacles. Deciphering the functions that proteins serve in various physiological and pathological circumstances is the goal of the field of proteomics [3]. Large sets of molecular sequences may now be made feasible by recent technical advancements to be analyzed to find patterns that could ultimately result in the creation of brand-new medications and vaccines [4]. However, processing such massive amounts of data necessitates the use of cutting-edge computing tools and statistical techniques to extract pertinent information.\nAdditionally, It is well known that the data with a highly dimensional feature space will become sparse, which makes it difficult for statistical analysis to identify statistical significance and key factors. To facilitate prediction, analysis, and visualization, it is, therefore, preferable to minimize the dimensionality of high-dimensional data. Due to these challenges, a variety of dimensionality reduction (DR) techniques has been developed that can accurately reflect the inherent correlations in the original data on a low-dimensional space. There are several linear and non-linear DR methods have been proposed such as principal component analysis (PCA), Linear discriminant analysis (LDA), Multidimensional Scaling (MDS) [5], LargeVis [6] are a few linear dimensionality reduction methods. Whereas kernel PCA [7], Sammon mapping [8] and spectral embedding [9] are non-linear dimensionality reduction techniques.\nCorrelated Clustering and Projection (CCP), a non-linear dimensionality reduction approach, computes the pairwise correlation matrix of samples and imposes a cutoff distance to prevent the global summation during the projection to increase computational efficiency [10], [11]. CCP has several benefits to offer, such as handling the dimensionality reduction of high sample sizes (because it avoids matrix diagonalization and instead solves a matrix to lower the dimensionality), employing statistical metrics like covariances to quantify the high-level dependence between random feature vectors [12], and it can be used in conjunction with a frequency-domain method for secondary dimensionality reduction to improve the preservation of data's global structures and increase accuracy [13].\nThis research is focused on creating a pipeline for analyzing molecular sequences using a method based on Nearest neighbor CCP-NN to preprocess the data and produce condensed representations that accurately reflect the original sequences. Here instead of computing the distance between data points, we use Nearest Neighbor (NN) to compute the nearest neighbor distance. We propose a method we name CCP-NN which is based on CCP to give an extra edge over the original CCP. The main contributions of our research are as follows:\n1) We propose a novel approach based on CCP to preprocess molecular sequence data, which leverages"}, {"title": "II. RELATED WORK", "content": "Sequence classification is a well-researched issue in bioinformatics [14]. A phylogenetic approach is frequently used in more conventional techniques of analyzing sequencing data [15], however, they are not scalable due to higher computational cost. To counter the issue, some machine learning (ML) approaches, including alignment-based [16], [17] and alignment-free [18] embedding approaches have become popular for ML tasks such as classification and clustering. Due to the extremely high dimensionality of the feature vector, these techniques do, however, also have scalability issues. The classification of biological sequences also makes use of the kernel matrix technique [19]. The Wasserstein distance (WD) is used in [20] to extract the features. Some efforts have been made to improve computational performance, such as Locality Sensitive Hashing (LSH) [21], which can train models faster and more accurately. The hash function in [22] is used to generate an approximate word embedding for language processing. However, collisions might occur in the resulting vectors, which reduces the embedding's effectiveness. The use of bloom filters for mistake correction in raw read data to aid a de novo assembly was demonstrated by authors in [23]. However, these methods are prone to cause loss of information. Some of the most popular methods are principal component analysis (PCA) [24], Multidimensional Scaling [5], and LargeVis [6]. The curse of dimensionality and difficulties with the analysis of outliers are another issue [25]. When it comes to data noise, missing data, and poor-quality data, it can be quite unstable [25]. Sequence analysis is still difficult to perform despite extensive effort because of the high dimensionality and quantity of data [26]. Therefore, methods that can manage the dimensionality reduction of high sample numbers and do not involve matrix diagonalization are required."}, {"title": "III. PROPOSED APPROACH", "content": "We divide this section into two parts where we first discuss the original Correlated Clustering and Projection (CCP) method proposed by [10], [11]. After that, we discuss the proposed method, called Nearest Neighbor CCP-NN in detail."}, {"title": "A. Correlated Clustering and Projection (CCP) Algorithm", "content": "The Correlated Clustering and Projection (CCP) algorithm [10], [11] is a data clustering and dimensionality reduction technique that identifies and groups correlated features within a high-dimensional dataset. The algorithm operates by partitioning the features into clusters based on their correlation patterns and then projecting the data onto the subspace spanned by the identified clusters. The purpose of this algorithm is to capture the underlying structure of the data by focusing on feature subsets that exhibit strong correlations, thereby facilitating meaningful analysis and visualization. Given a dataset with N samples and M features represented by the matrix X, the CCP proceeds as follows:\na) Step 1 (Data Preprocessing): The algorithm begins by calculating the variance of each feature to identify non-zero variance features, which are essential for meaningful clustering.\nb) Step 2 (Selecting Features for Clustering): The next step is to select a subset of features for clustering based on the variance. The algorithm chooses the top numCuto f f features, which is a user-defined parameter representing the percentage of non-zero variance features to retain.\nc) Step 3 (K-Means Clustering): The selected features are clustered using the K-Means algorithm with n_components \u2013 1 clusters, where n_components (a hyper-parameter) is the desired number of clusters.\nd) Step 4 (Partitioning Non-Clustered Features): The features that were not assigned to any cluster due to low variance are grouped into a separate cluster. This makes a new cluster that contains the remaining features.\ne) Step 5 (Computing Density Map): For each cluster, the algorithm computes a density map to capture the correlation between features within the cluster (see Algorithm 1). The density map is estimated using either an exponential kernel or a Lorentz kernel, which are defined as follows:\nExponential Kernel => K(x) = e-(scale) Power\nLorentz Kernel => K(x) =  $\\frac{1}{1+(\\frac{x}{scale})^{power}}$\nwhere x represents the pairwise distance between two features, scale is a scaling factor, and power is a user-defined parameter.\nIn Algorithm 1, the CCP performs several steps to compute the correlation and estimate the density map based on the given inputs and parameters. It begins by calculating the pairwise distances between the selected features. If the transformation flag is set to true, it calculates the distances between the features in the input data and the reference data. Otherwise, it calculates the distances between the features in the reference data. Next, if the scaling factor is not already calculated for the specified component, it proceeds to compute the average minimum distance, which is important for scaling the density estimation. Similarly, if the cutoff value is not already set for the specified component, it computes the average and standard deviation (SD) of the pairwise distances. The cutoff is then defined as the average plus three times the SD. It helps determine the threshold beyond which correlation values are considered negligible. The algorithm calculates the scaling factor by multiplying the user-defined scaling parameter with the previously computed average minimum distance. The scaling factor is used to adjust the scale of the density estimation."}, {"title": "f) Step 6 (Density-based Clustering):", "content": "The density map obtained for each cluster is used for a final density-based clustering. Features are assigned to clusters based on their density values, where higher density indicates a stronger correlation."}, {"title": "g) Step 7 (Projection):", "content": "Finally, the data is projected onto the subspace spanned by the identified clusters. Each sample is represented as a vector of density values corresponding to each cluster. This new projected representation into the subspace, called OCCP is used as the low dimensional embedding representation for the given data point."}, {"title": "B. Nearest Neighbors Based CCP", "content": "In the nearest neighbor (NN) version of CCP (our proposed method), all steps from 1 to 7 are followed from the original CCP as described in the above subsection. The main modification is made in Step 5, where we compute the density map using the NN algorithm for efficient and fast computation of the density map. The pseudocode for computing the density map is given in Algorithm 2, where the NearestNeighborComputeCorr function incorporates the use of an NN search technique, specifically the AnnoyIndex data structure [27], to calculate the correlation and estimate the density map. The steps involved in this process are as follows: An AnnoyIndex is created with the specified metric, and the features from the reference data are added to the index. The function checks if the transformation flag is set to true then we find a correlation by vector else if it is set to false, we find a correlation by item.\nOnce the AnnoyIndex is constructed, the function retrieves the NN for each feature in the input data. If the transformation flag is true, it retrieves the NN based on the features in the input data. If the flag is false, it retrieves the NN based on the features in the reference data. The retrieved nearest neighbors (NN) are reshaped into a proper format for further processing. Similar to Algorithm 1, the function checks if the scaling factor needs to be computed for the specified component. If the scaling factor is not already calculated, it proceeds to compute the average minimum distance, which is crucial for scaling the density estimation.\nIf the cutoff value is not set, it computes the average and SD of the correlation values obtained from the NN. The cutoff is then defined as the average plus three times the SD. This cutoff value helps determine the threshold beyond which correlation values are considered negligible. The scaling factor is calculated by multiplying the user-defined scaling parameter with the previously computed average minimum distance. This scaling factor is used to adjust the scale of the density estimation.\nFinally, the function estimates the density map, also known as the correlation, by applying the density estimation function to the correlation values obtained from the NN. This density map represents the correlation between the selected features, taking into account the scaling factor and the cutoff value. In summary, Algorithm 2 incorporates the use of NN to calculate the correlation and estimate the density map. It involves constructing an AnnoyIndex, retrieving the NN, reshaping the obtained correlations, computing the scaling factor and cutoff value, and estimating the density map based on these values. This approach allows for efficient computation of correlations and density estimation, particularly for high-dimensional data.\nAfter computing the density map, Step 6 and Step 7 are followed similarly to the original CCP as described above"}, {"title": "C. Algorithm Complexity", "content": "1) CCP: To begin, the computation of variance along each feature axis takes O(NM), where N is the number of samples and M is the number of features. Sorting the variance values to select the top f (where f < M) features takes O(f log f). Following this, the K-Means clustering step typically depends on the number of iterations niter, the number of features f, the number of clusters nc, and the number of samples N. This step has a time complexity of O(nc\u00b7f\u00b7niter\u00b7N).\nTo this end, the overall time complexity of the algorithm can be expressed as:\nO(NM + f log f + nc \u00b7 f \u00b7 Niter.N)\nSince the K-Means setup dominates the variance computation and sorting steps, this simplifies to:\nO(nc.niterf. N)\nThe total space complexity is dominated by the size of the input matrix and the memory used for K-Means clustering. Therefore, the overall space complexity is:\nO(N\u00b7M)\nFurthermore, in order to compute the density map to capture the correlation between features within a cluster, pairwise distance calculation is required, which is the most computationally expensive operation. If fi is the number of features selected for the i-th cluster, the pairwise distance calculation takes O(N2. fi). This is because pairwise distance calculation compares each of the N samples with every other sample across fi features. Summing over all ne components, the total time complexity becomes:\n$\\Sigma_{i=1}^{nc}$ \u039d\u00b2. fi = O(N\u00b2 \u2022 f)\nwhere f =  $\\Sigma_{i=1}^{nc}$ fi is the total number of features across all components. The space complexity associated with this procedure is O(N2). Therefore, the overall complexity is;\nO(N.f(nc.niter + N))\nand the space complexity is;\nO(N(M + N))\n2) CCP-NN: On the other hand, if we were to use Approximate Nearest Neighbor (ANN) via Annoy Index as a proxy for pairwise distance computation, we would significantly benefit in terms of computation time. Building an Annoy Index takes O(N log N. f), because the algorithm builds a forest of random projection trees. Querying the Annoy Index for nearest neighbors is approximately O(log N) per query, and since this is repeated for all N samples, the complexity for querying is O(N log N).\nWith nc total clusters, the total time complexity becomes\n$\\Sigma_{i=1}^{nc}$\u039f(nc niter f\u00b7N)+O( N log N.fi) = O(N.f(nc.niter+log N))\nand the space complexity is;\nO(N(M + log N. f))\nClearly, CCP-NN has an advantage over CCP in terms of speed and memory requirement."}, {"title": "D. Convergence Analysis", "content": "The key step in CCP-NN is the estimation of the density map, which is used to capture the correlation between features within a cluster. This is done using the nearest neighbor search, especially using the Annoy Index data structure [27].\nLet X \u2208 RN\u00d7M be a dataset with N samples and M features (dimension). Given a set of features {$x_i$}#1 within a cluster, the density estimate at each point xi is determined by the proximity of its nearest neighbors. The Annoy Index facilitates the retrieval of the nearest neighbors, denoted by Nk(xi), where k is the number of neighbors considered. Assume the following;\n\u2022 X ~ P(X) with a well defined density function p(x)\n\u2022 Nearest neighbor search in CCP-NN provides a close approximation to the true nearest neighbors, with an error margin e\nNow, let Nk(xi) represent the nearest neighbors returned by the Annoy Index, and let Nk represent the true nearest neighbors. The accuracy of the Annoy Index guarantees that:\nE[||Nk(xi) \u2013 \u00d1k(xi)||] \u2264 \u0454\nwhere \u0454 > 0 depends on the dimensionality of the data and the parameters of the Annoy Index.\nGiven the convergence of the nearest neighbor search, we can now analyze the consistency of the density estimation process. Let p(xi) be the density estimate at xi obtained by CCP-NN, and let p(xi) be the true density. The density estimate is given by;\np(xi) =  $\\frac{1}{hk}\\Sigma_{xj\u2208Nk (xi)}K(\\frac{Xi - Xj}{h})$\nwhere K is a kernel function and h is a bandwidth parameter. Using Triangle-Inequality:\n|p(xi)-p(xi)| \u2264 |p(xi)-p(xi, \u00d1k(xi))|+|p(xi, \u00d1k(xi))-P(xi)|\nwhere p(xi, \u00d1k(xi)) denotes the density estimate using the true nearest-neighbor. We take expectation on both sides and the first term of R.H.S in the inequality above can be bounded by O(\u20ac) due to the accuracy of the Annoy Index. The second term can be bounded using standard kernel density estimation convergence results [28].\nTherefore, the upper bound on the error estimate is:\nE[[p(xi) - p(xi)|] < O(e + h\u2074 + 1/kh)"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "In this section, we describe the different datasets used for experiments. We also go through the baseline methods and evaluation metrics we use for the classification. A Windows 10 64-bit machine with an Intel(R) Core i5 processor operating at 2.10 GHz and 32 GB of memory is used for all experiments. Our pre-processed datasets and code are available online for reproducibility 1.\nWe use three datasets in this study to assess the effectiveness of the proposed method. We employ t-distributed stochastic neighbor embedding (t-SNE) [29] to examine the data for any natural (hidden) grouping. The t-SNE plots for various embedding techniques are displayed in Figure 1, 2, and 3 for Protein Subcellular, Coronavirus Host, and Human DNA datasets, respectively. We can observe that t-SNE can group similar classes in the case of Autoencoder with CCP-NN.\nTo classify the molecular sequences, we employed several ML models, including Support Vector Machine (SVM), Naive Bayes (NB), Multi-Layer Perceptron (MLP), K-Nearest Neighbors (KNN), Random Forest (RF), Logistic Regression (LR), and Decision Tree (DT). We evaluated classification performance using average accuracy, precision, recall, F1 (weighted), F1 (macro), Receiver Operator Characteristic Curve (ROC) Area Under the Curve (AUC), and training runtime. To preserve the original data distribution, the data for each classification task is divided into 60-10-30% train-validation-test sets using stratified sampling. To obtain more consistent findings, we also conduct our tests by averaging the performance outcomes of 5 runs. We carefully considered baselines from several embedding generation categories, including feature engineering, conventional kernel matrix generation, neural networks, pre-trained language models, and pre-trained transformers for protein sequences."}, {"title": "A. Baseline Methods", "content": "Among the baseline methods discussed in Table I, we selected 4 popular embedding generation models, including One Hot Encoding (OHE) [16], Spike2Vec [18], PWM2Vec [17], and Autoencoder [30] to be used as input to both vanilla CCP and CCP-NN for dimensionality reduction. These methods were simple to use (in terms of implementation compared to complex models like SeqVec, TAPE, and Protein Bert), easy and fast to compute (compared to WDGRL, which is computationally expensive and takes long computational time), and generate embeddings directly, which can be used for dimensionality reduction (unlike String kernel, which generates a kernel matrix, which has to be converted to embeddings using kernel PCA, which could cause loss of information)."}, {"title": "B. Dataset Statistics", "content": "We use 3 datasets for our experiments. The datasets employed are listed below:\nProtein Subcellular Locations The dataset Protein Subcellular Locations Dataset we employ consists of 5959 unaligned protein sequences, each corresponding to a different subcellular location [34]. The labels for the classification task are these subcellular sites and there are 11 unique labels in our dataset. These labels correspond to the proteins of plant cells and fungal cells, while animal cells share all localizations with them."}, {"title": "V. RESULTS AND DISCUSSION", "content": "In this section, we report classification and runtime results for both CCP and CCP-NN using different datasets and embedding models."}, {"title": "A. Results For Protein Subcellular dataset", "content": "The classification results (averaged over 5 runs) for the proposed CCP-NN and its comparison with the CCP approach for the Protein Subcellular dataset are shown in Table V. We can observe that the proposed CCP-NN outperforms the original CCP-based low-dimensional representation for all evaluation metrics and achieves a near-perfect predictive classification performance. The performance gain for CCP-NN (using OHE with Decision Tree classifier and using Autoencoder with Decision Tree classifier), compared to the best CCP-based results (Spike2Vec with Random Forest classifier) is 50.3%, which highlights a significant improvement in terms of predictive accuracy.\nThe comparison of the best performing proposed method from Table V (i.e. CCP-NN with Autoencoder) with the existing baseline models (without CCP or CPP-NN) is shown in Table VI. We can observe that the proposed method significantly outperforms all baselines for all evaluation metrics other than the training runtime. Specifically, in terms of average accuracy, the proposed method with Autoencoder embedding achieves 28% improvement compared to the second best (i.e. Protein Bert, a pre-trained transformer-based model) and achieves a near-perfect average accuracy score in the case of the Protein Subcellular dataset.\nThe standard deviation (SD) results (for 5 runs) for the baselines and the proposed method are shown in Table VII for the Protein Subcellular dataset. We can observe that in the majority of the cases, the SD values are towards the lower end (i.e. < 0.02), which shows that there is not much variation in the results for different experimental runs having a random train-test split."}, {"title": "B. Results For Coronavirus Host Data", "content": "The classification results (averaged over 5 runs) for the proposed CCP-NN and its comparison with the CCP approach for the Coronavirus Host dataset are shown in Table VIII. The best values for each embedding method are underlined while overall best values among all methods are shown in bold. We can observe that the proposed CCP-NN outperforms the original CCP-based low-dimensional representation for all evaluation metrics (other than classifier training runtime) and achieves a near-perfect predictive classification performance. Although the performance gain for CCP-NN compared to the best CCP-based results is not significant, however, it still outperforms CCP for all evaluation metrics other than training runtime.\nThe comparison of the best performing proposed method from Table VIII i.e. CCP-NN with Spike2Vec, with the existing baseline models is shown in Table IX for the Coronavirus Host dataset. We can observe that the proposed method significantly outperforms all baselines for all evaluation metrics other than the training runtime. Specifically, in terms of average accuracy, the CCP-NN with Spike2Vec embedding achieves 1.7% improvement compared to the second-best results (i.e. original Spike2Vec with Random Forest and Logistic Regression classifiers) and achieves a higher average accuracy score in the case of the Coronavirus Host dataset."}, {"title": "C. Results For Human DNA Data", "content": "Table XI presents the classification results for both the proposed CCP-NN and the conventional CCP approach on the Human DNA dataset. The results are averaged over 5 runs. The best values for each embedding method are underlined, and the overall best values among all methods are shown in bold. It is evident that the proposed CCP-NN consistently outperforms the original CCP-based low-dimensional representation for all evaluation metrics, except for the classifier training runtime. The classification accuracy achieved by CCP-NN is notably higher, with the best-performing results using CCP-NN (with Autoencoder and Random Forest Classifier) showing a significant improvement of 10.8% compared to the best results obtained from the original CCP (using One-Hot Encoding with Random Forest classifier).\nThe comparison of the best performing proposed method from Table XI, i.e. CCP-NN with Autoencoder, with the existing baseline models is shown in Table XII for the Human DNA dataset. We can observe that the proposed method significantly outperforms all baselines for all evaluation metrics other than the training runtime. Specifically, in terms of average accuracy, the CCP-NN with Autoencoder embedding achieves 11.8% improvement compared to the second-best results (i.e. original Spike2Vec with Random Forest classifier).\nIt is noteworthy that the pre-trained Protein Bert exhibited significantly poorer performance on the Human DNA dataset compared to its performance on the Protein Subcellular and Coronavirus Host datasets. The underlying reason for this discrepancy lies in the fact that the Protein Bert model is designed and trained specifically on molecular sequence data. Consequently, when faced with nucleotide sequences of Human DNA, the model struggles to generalize effectively, leading to its subpar performance. In contrast, the proposed method demonstrated the highest performance among all approaches, outperforming the baseline methods on the Human DNA dataset. This indicates the robustness and efficacy of our proposed method in handling diverse biological sequence data.\nTable XIII presents the standard deviation (SD) results (averaged over 5 runs) for the baseline methods and our proposed approach to the Human DNA dataset. The findings reveal that the majority of the standard deviations. values"}, {"title": "D. Runtime Evaluation", "content": "For all datasets, we additionally report % improvement for running the $CCP_NN compared to QCCP in terms of runtime. For computing the runtime performance gain, we use the following expression:\n% improvement =  $\\frac{ROCCP - ROCCP_NN}{ROCCP}$ \u00d7 100\nwhere R&CCP represents the runtime of the original CCP method while R$CCP_NN corresponds to the runtime for our NN-based CCP computation.\nThe computational runtime for R&CCP_NN and R&CCP along with the performance gain is reported in Table XIV, XV, and XVI for the Protein Subcellular, Human DNA, and Coronavirus Host datasets, respectively. For protein subcellular data, we can observe that for all 4 embedding methods as input to the CCP and CCP-NN, the performance gain (i.e. Percentage improvement) for our CCP-NN is 23.32%, 72.34%, 55.93%, and 92.88%, for OHE, Spike2Vec, PWM2Vec, and Autoencoder, respectively. For the Coronavirus Host dataset, we can again observe that in terms of computations runtime performance gain, the proposed CCP-NN significantly outperforms the original CCP by 39.429%, 55.950%, 97.865%, and 93.989% for OHE, Spike2Vec, PWM2Vec, and Autoencoder, respectively. Similarly, for the Human DNA dataset, we can observe 94.385% (OHE), 90.243% (for Spike2Vec), 91.456% (PWM2Vec), and 85.425% (for Autoencoder) improvement in"}, {"title": "F. Discussion", "content": "The observed improvement in classification results when using the proposed method over the original CCP method can be attributed to several technical and logical factors.\na) Efficient Nearest Neighbor Search: The proposed method employs the nearest neighbor (NN) search technique using the AnnoyIndex. By leveraging NN, the algorithm reduces the computational complexity of pairwise distance calculations. In high-dimensional spaces like biological sequencing data where feature dimensions can be large, the NN-based approach significantly speeds up the computation, allowing for better handling of the complexity and potentially better capturing of correlations between features.\nb) Handling High-Dimensional Data: In sequence classification, feature spaces can often be high-dimensional due to the representation of various attributes. The proposed method can better preserve the feature information in the low-dimensional space than the original CCP due to efficient nearest-neighbor computation, leading to more meaningful density estimations. The nearest neighbor search focuses on capturing local patterns in data, which are particularly relevant for correlations between features. Using the density estimation from an NN-based neighbor search, the method effectively identifies relationships between relevant features, thereby representing the underlying patterns better.\nc) Robustness to Noisy Data: The data might contain noise and outliers, which can negatively impact the accuracy of correlation estimation. The NN-based method can be more robust to noisy data as it focuses on local patterns rather than global distances. It implicitly handles noise using neighbor relationships, leading to more reliable density estimations and better classification results.\nd) Optimal Feature Clustering: The proposed method partitions the features into clusters based on their correlation patterns. By employing NN-based density estimation, the method identifies more optimal feature clusters, which in turn can enhance the separation of target classes. The ability to detect relevant feature subsets for classification can contribute to improved accuracy.\nIn general, the superior classification results obtained with the proposed method can be attributed to its efficient handling of high-dimensional data, the use of NN for nearest neighbor search, enhanced correlation estimation, robustness to noise, and optimal feature clustering. These advantages collectively enable the method to capture the underlying patterns and provide more discriminative representations for improved classification performance as demonstrated in a variety of datasets including protein subcellular localization data, Coronavirus Host data, and Human DNA in our experiments."}, {"title": "VI. CONCLUSION", "content": "In this study, we addressed the challenges of analyzing molecular sequence data, which involves a large number of sequences and complex protein structures. We proposed an efficient and fast method called Nearest Neighbor Correlated Clustering and Projection (CCP-NN). The CCP-NN method is based on the original Correlated Clustering and Projection (CCP) technique, but it incorporates an NN search for computing the density map and correlations. Through a series of experimental evaluations, we compared the performance of CCP and CCP-NN in classifying molecular sequences. The results demonstrated that CCP-NN outperforms CCP in terms of classification accuracy while also reducing computational runtime. Future work includes further enhancements to the CCP-NN framework, such as incorporating additional information or integrating it with other machine-learning methods for comprehensive analysis of sequences."}]}