{"title": "Learning to Route for Dynamic Adapter Composition in Continual Learning with Language Models", "authors": ["Vladimir Araujo", "Marie-Francine Moens", "Tinne Tuytelaars"], "abstract": "Parameter-efficient fine-tuning (PEFT) methods are increasingly used with pre-trained language models (PLMs) for continual learning (CL). These methods involve training a PEFT module for each new task and using similarity-based selection to route modules during inference. However, they face two major limitations: 1) interference with already learned modules and 2) suboptimal routing when composing modules. In this paper, we introduce a method that isolates the training of PEFT modules for task specialization. Then, before evaluation, it learns to compose the previously learned modules by training a router that leverages samples from a small memory. We evaluate our method in two CL setups using several benchmarks. Our results show that our method provides a better composition of PEFT modules, leading to better generalization and performance compared to previous methods.", "sections": [{"title": "1 Introduction", "content": "CL aims to continuously learn new tasks without forgetting previously learned knowledge from old tasks (McCloskey and Cohen, 1989). Recent advancements in PLMs and PEFT methods have shown potential in CL for NLP (Zhou et al., 2024). PEFT methods, like prompts or adapters, are lightweight modules that can be tuned for downstream tasks while keeping the PLM frozen. Recent work shows that these methods can be competitive with, or even superior to, full fine-tuning of PLMs (Li and Liang, 2021; Hu et al., 2022). Additionally, research (Wang et al., 2023, 2024, 2022b; Smith et al., 2023) demonstrates their effectiveness in CL.\nHowever, PEFT approaches have two significant limitations: 1) When a new task arises, a new PEFT module is added, and the previously learned ones remain frozen but activated (Razdaibiedina et al., 2023). Previous modules may interfere with the optimal task knowledge acquisition by the current PEFT module. 2) They rely on a similarity-based selection mechanism (Wang et al., 2024) that may not accurately represent a routing function that effectively composes the PEFT modules.\nTo address these issues, we introduce Learning to Route for dynamic PEFT composition (L2R). L2R is a simple but effective method that trains PEFT modules in isolation to avoid interference from previous knowledge when learning new tasks (Wang et al., 2023). Then, it uses previous examples stored in memory to learn a routing function for dynamically composing PEFT modules during inference, a process analogous to local adaptation (d'Autume et al., 2019). Our extensive evaluations show that L2R is competitive against other PEFT-based methods across benchmarks and CL setups.\nUnlike existing methods that focus on developing routing mechanisms within the training phase, our method learns to route adapters prior to test-time inference. From a practitioner's perspective, the ultimate goal of CL is to develop a model that can learn new tasks while maintaining high performance (Prabhu et al., 2020). Our method addresses this by incrementally adding PEFT modules for new tasks. Additionally, in real-world applications, systems are primarily constrained by computational and time budgets rather than storage (Prabhu et al., 2023; Verwimp et al., 2024). By leveraging a memory along PEFT, our approach achieves a better trade-off between performance and efficiency."}, {"title": "2 Related Work", "content": "Continual Learning with PLMs CL in NLP has grown with the advent of robust PLMs from pre-training, beneficial for CL (Zhou et al., 2024). Existing NLP approaches include replay-based (d'Autume et al., 2019; Araujo et al., 2022b), meta-learning-based (Wang et al., 2020), amount others. Recent techniques use PEFT for CL, adding new modules as tasks arise while freezing previous ones to maintain a strong backbone and leverage past knowledge (Razdaibiedina et al., 2023). This has been extended for models to learn module composition for inference (Wang et al., 2024).\nOur work focuses on the PEFT setup. We rely on the parameter isolation strategy (Wang et al., 2023; Cheng et al., 2024) to let the PLM learn task-specific modules independently. We also adopt a memory as in replay-based methods, not to rehearse examples but to learn a routing function for effective module composition before inference.\nLocal Adaptation Memory-based parameter adaptation (MbPA) (Sprechmann et al., 2018), also known as local adaptation, allows a model to deal with changes and shifts in data distributions. In CL, it leverages labeled data stored in memory for brief fine-tuning before making predictions to improve model performance (d'Autume et al., 2019).\nOur work builds on the concept of local adaptation to maintain model generalization and performance across tasks in CL. Rather than retraining model components, we introduce a new component (a router) that learns to dynamically compose appropriate PEFT modules based on input.\nLearning to Route In mixture-of-experts (MoE) models, routing activates subnetworks based on their specialized capabilities for prediction (Shazeer et al., 2017a). Recently, adapters\u2014a class of PEFT methods\u2014have been employed to implement MoE-style models (Wang et al., 2022a) and routing functions (Ponti et al., 2023).\nExisting approaches in CL use task centroids (Cheng et al., 2024), learnable task vectors (Wang et al., 2024), or record task distributions (Wang et al., 2023) to select modules. However, these act as a proxy for routing and may not effectively direct input to modules based on their specialties.\nOur work builds on studies showing the benefits of a well-learned router in MoE models (Dikkala et al., 2023). We propose a router learning approach that takes place prior to inference and enables dynamic utilization of modules by the model."}, {"title": "3 Method", "content": "We consider a CL setup for NLP (d'Autume et al., 2019), where a model needs to learn from a sequence of T tasks: $D = {D_1, ..., D_T}$. Each task t consists of a new data distribution $D_t = (X_t, Y_t)$, where $X_t$ are the input instances and $Y_t$ are the labels. A model in this setup consists of a PLM that processes input x and a classifier for predicting class y. We extend this setup by PEFT modules, specifically adapters, which consistently achieve superior predictive performance than other techniques in CL (Wistuba et al., 2024).\nTask-specific Adapters L2R employs a set of adapters $A = {A_1, ..., A_T}$ that learn task-specific knowledge from a data stream (Figure 1a). Specifically, an adapter $A_t$ is activated and trained when a new task $D_t$ emerges. Previous adapters are deactivated, allowing $A_t$ to learn in isolation and specialize in task t (Wang et al., 2023). We keep the backbone frozen and train only the adapter $A_t$. For simplicity, we omit the fact that each task uses L adapters that match the number of PLM layers.\nAdapters offer significant benefits. Their robust modeling capabilities allow them to effectively capture distribution-specific details from each task and the high parameter efficiency enables efficient training of our method whenever a new task emerges.\nMemory Our method is equipped with a non-parametric memory M that stores previously seen training examples. A similar component is used in replay methods (d'Autume et al., 2019), where memory is used to perform replay during training. However, our memory is leveraged prior to test inference to learn a routing function resembling the local adaptation to improve generalization.\nWe determine the memory capacity as a percentage of the total dataset size and populate it by randomly sampling the training data. Random sampling captures the global distribution of the data stream (Araujo et al., 2022a), which will help in the effective training of routers. While more advanced memory population methods could enhance performance (Hurtado et al., 2023; Hayes and Kanan, 2021), we leave this exploration for future work.\nMemory-based Router Learning Once the adapters A have been trained, they may be combined for inference (Figure 1b). For this, we add a router network R to each adapter across the model. This router computes the probability of the input being sent to each adapter. Our router takes the [CLS] representation as input to decide how to route the whole input. This provides two advantages: 1) the routers learn to route based on input structure, and 2) can produce different combinations at different layers based on the model's abstraction hierarchy.\nInstead of using a softmax across adapters, where modules compete for activation, we adopt an approach inspired by Ponti et al. (2023), emphasizing task-level modularity that respects task hierarchy, where more complex tasks encompass simpler ones as sub-tasks. Each output of the router is a binary scalar indicating whether an adapter is active for a given input. This is implemented as a collection of Bernoulli distributions, relaxed through a Gumbel-sigmoid, which ensures stochasticity.\nTo obtain a competent router, we propose a memory-based router learning process. Inspired by local adaptation, we leverage the elements in memory to train the router network parameters. However, unlike local adaptation that adapts the model for each input, our method performs this process only once after the training phase to learn the routing functions. After this, the model is ready to perform inference at any time.\nAdapter Composition Given routing probabilities, we can compose our adapters in different ways. We consider two options: 1) a weighted average of their outputs (Shazeer et al., 2017b), denoted as L2R-wavg, and 2) merging adapter parameters via a weighted average of their weights (Wortsman et al., 2022), referred to as L2R-merge."}, {"title": "4 Experimental Setup", "content": "Benchmarks We adopt two CL setups: 1) Class-Incremental Learning (CIL) and 2) Task-Incremental Learning (TIL). Both setups aim to incrementally learn new tasks (and thus new classes), but TIL always has access to task identity, making CIL a more challenging and realistic scenario.\nBased on this, we consider three benchmarks. MTL5 (d'Autume et al., 2019): 5 text classification tasks. WOS (Kowsari et al., 2017): 7 document classification tasks. AfriSenti (Muhammad et al., 2023): 12 (multilingual) sentiment analysis tasks.\nBaselines We focus our experimentation on comparing PEFT-based CL methods. Lower-Bound trains task-specific adapters, summing their outputs during inference. Upper-Bound trains task-specific adapters, using only the corresponding adapter for inference. ProgPrompt (Razdaibiedina et al., 2023) progressively adds new prompts for new tasks. All prompts are used for inference (only for TIL). DAM (Cheng et al., 2024) learns task-specific adapters and creates task vectors by averaging training example representations. At inference, it uses similarity scores between the input and task vectors to route (only for CIL). EPI (Wang et al., 2023) learns task-specific prompts and records task distributions. At inference, it selects the nearest task's prompt based on input comparison with all distributions. MoCL (Wang et al., 2024) progressively adds new prompts and learns a task vector. At inference, it uses task vectors to compute similarity scores to combine the prompts.\nImplementation Details We use BERT (Devlin et al., 2019) for MTL5 and WOS while AfroXLMR (Alabi et al., 2022) for AfriSenti. To implement our adapters, We adopt LoRA (Hu et al., 2022) due to its efficiency and performance. Our memory is similar to that of (d'Autume et al., 2019), but we accumulate only 10% of each task's, training dataset a negligible amount when using efficient storage formats. Our router network consists of a linear trans-"}, {"title": "5 Results", "content": "In this section, we introduce our results with a summarized Table 1. For full results, see Appendix C, and for an analysis of efficiency, see Appendix D.\nResults on CIL Table 1 presents the results under the CIL setup. Both L2R-wavg and L2R-merge outperform comparable baselines such as DAM, \u0395\u03a1\u0399, and MOCL. DAM shows the worst performance of the group, as it relies on task vectors created from training data, which are insufficient for properly routing the adapters. However, for Afrisenti, DAM achieves competitive results since this benchmark resembles a domain incremental learning scenario where task vectors effectively help distinguish task identity. EPI and MoCL perform better than DAM but still fall short of our method despite their sophisticated techniques. This highlights the importance of a well-learned router, which enables the router's ability to intelligently route and confer a significant performance advantage (Dikkala et al., 2023).\nRegarding L2R versions, we observe that L2R-wavg has a clear advantage over L2R-merge. This may be due to interference between the parameters of multiple adapters, which can cause performance drops when they are merged (Yadav et al., 2024).\nResults on TIL Table 1 presents the results under the TIL setup. L2R-wavg performs best in the MTL5 and AfriSenti benchmarks, while it lags behind in the WOS benchmark. Interestingly, both L2R-wavg and MoCL reach the Upper-Bound performance, suggesting that the learned routing function likely emulates the full activation of the corresponding adapter or an effective combination of the adapters. We hypothesize that L2R-wavg tends toward the latter, as evidenced by its performance in AfriSenti, where it surpasses the Upper-Bound.\nIn the WOS benchmark, our methods perform competitively but fall behind MoCL. We attribute this to the amount of data used to train the routing function. WOS consists of very small datasets, resulting in a memory populated with approximately 100 elements per task, which may not be sufficient for router learning. In fact, Dikkala et al. (2023) have shown that as the number of examples increases, the routing function reaches its optimal performance. We explore this in the next section.\nImpact of Memory Size Figure 2 shows the performance for all benchmarks with memory sizes ranging from 1% to 30%. As expected, we find that the performance of L2R-wavg increases with larger memory sizes. Notably, the performance on WOS improves to match that of MoCL, supporting our assertion about the importance of using more examples for effective router adaptation. For MTL5 and AfriSenti, their performance gets close to the Upper-Bound, demonstrating that L2R may be learning better compositions for the adapters, leading to high performance across tasks.\nRouter Analysis As discussed in section 3, we utilize Gumbel-sigmoid activation in our routers to theoretically enhance task-level modularity over activation competency offered by Softmax. To validate this, we compare the routing probabilities from L2R-wavg in CIL with the scores from a version trained using Softmax activation. shows the router probabilities generated for the task 5 test set in MTL5. We observe that L2R-wavg scores activate task 5 moderately, but also leverage knowledge from other tasks. In contrast, the Softmax version predominantly activates the adapter for task 5. These results confirm that L2R provides a composition that leverages knowledge from another task and not only relies on a unique adapter. We found similar behavior across tasks."}, {"title": "6 Conclusion", "content": "We introduce L2R, a method for learning to route in CL that enables a PLM to dynamically combine adapters, providing better generalization and performance. Our experiments across various benchmarks demonstrate the proficiency of our method in TIL and in the challenging CIL setup.\nLimitations\nOur method focuses on developing an effective routing function to enhance generalization and performance on downstream tasks learned from a stream. To achieve this, we use a non-parametric memory to store previous examples, similar to replay-based methods. This approach may be limited in environments with storage constraints or data privacy concerns.\nOur experimental setup primarily focuses on small-scale PLMs, which is a limitation since exploring large language models (LLMs) would be desirable given their dominance in the current NLP landscape. Although we have not tested our method with LLMs due to computational resource limitations, we anticipate similar results."}, {"title": "A Additional Benchmark Details", "content": "MTL5 is a benchmark for text classification. It consists of five datasets from (Zhang et al., 2015). AGNews classification, Yelp sentiment analysis, Amazon sentiment analysis, DBPedia article classification, and Yahoo questions and answers categorization. Both sentiment analysis tasks share the same labels. In line with d'Autume et al. (2019), we use 575,000 training and 38,000 test examples with 33 classes from all datasets using 4 task orders:\n(i) AGNews \u2192 Yelp \u2192 Amazon \u2192 Yahoo \u2192 DBpedia\n(ii) Yelp\u2192 Yahoo \u2192 Amazon \u2192 DBpedia \u2192 AGNews\n(iii) DBPedia \u2192 Yahoo \u2192 AGNews \u2192 Amazon \u2192 Yelp\n(iv) Yelp \u2192 AGNews \u2192 DBPedia \u2192 Amazon \u2192 Yahoo\nWeb-of-science (WOS) (Kowsari et al., 2017) initially began as a hierarchical dataset for categorizing documents. It includes research papers from seven distinct fields: biochemistry, civil engineering, computer science, electrical engineering, medical science, mechanical engineering, and psychology. Each of these fields represents a high-level category for document classification, with multiple subcategories within each field. This dataset has about 11,967 instances and 33 classes. In line with Wang et al. (2023), we split to train/val/test set in the ratio of 0.6:0.2:0.2 and structured 7 sequential learning tasks based on its high-level categories:\n(i) 1\u21922\u21923\u21924\u21925\u21926\u21927\nAfriSenti (Muhammad et al., 2023) is a multilingual sentiment analysis dataset comprising 12 low-resource African languages. These languages include Amharic (am), Algerian Arabic (dz), Hausa (ha), Igbo (ig), Kinyarwanda (kr), Moroccan Arabic (ma), Nigerian Pidgin (pcm), Mozambican Portuguese (pt), Swahili (sw), Xitsonga (ts), Twi (twi), and Yoruba (yo). The dataset contains over 110,000 annotated tweets and spans three sentiment classes across all languages. In line with (Wang et al., 2023), we use 3 task orders:\n(i) am \u2192 dz \u2192 ha \u2192 ig\u2192 kr \u2192 ma \u2192 pcm \u2192 pt \u2192 sw \u2192ts \u2192 twi\u2192 yo\n(ii) ma \u2192 pcm \u2192 kr \u2192 pt \u2192 ig \u2192 sw \u2192 ha \u2192 ts \u2192 dz \u2192 twi\u2192 am \u2192 yo\n(iii) amdz \u2192 ha \u2192 ma \u2192 ig \u2192 kr \u2192 sw \u2192 ts \u2192 twi \u2192 yo\u2192 pcm \u2192 pt\nFollowing Wang et al. (2023), we categorize benchmarks based on task domain similarity. WOS and Afrisenti serve as near-domain benchmarks due to their closely related tasks. MTL5, on the other hand, represents far-domain benchmarks where distinct domain boundaries among tasks are evident."}, {"title": "B Additional Implementation Details", "content": "As backbones, we use BERT-base (Devlin et al., 2019) for MTL5 and WOS and AfroXLMR-base (Alabi et al., 2022) for AfriSenti. Additionally, we adopt LORA (Hu et al., 2022) as our PEFT modules, using a rank of 8 and dropout of 0.1. We applied LORA to $W_q$ and $W_v$, and it has been demonstrated to be a competitive configuration. These adapters allow efficient fine-tuning, representing less than 0.7% of the PLM's parameters in our experiments.\nOur memory is non-parametric, which means that it stores raw examples with labels sampled from training examples. We only sample 10% of each task's training dataset, representing less than 3.8MB per dataset in our experiments, as we use Parquet, an efficient storage format.\nDuring the training phase, we use a batch size of 8 and a learning rate of 3e-4. Additionally, we use a linear scheduler with warmup and AdamW optimizer. On the other hand, we use the same configuration to train the router networks. However, we use a learning rate of 3e-4 or 3e-5 for MTL5 and AfriSenti, and a learning rate of 3e-3 or 3e-4 for WOS."}, {"title": "C Full Results", "content": "Table 2 and Table 3 show the full results for CIL and TIL setups, respectively. We observe that our method consistently outperforms the best baseline (MoCL) across orders in CIL. Interestingly, our model achieves a similar performance across orders. This is because the isolation strategy helps the model to have a very specialized adapter regardless of the order of training. However, order (iii,iv) of MTL5 and order (ii) of AfriSenti show slightly superior performance, indicating that memory population methods could be leveraged to further improve performance (Araujo et al., 2022a).\nRegarding TIL experiments, we find that L2R-wavg obtains a similar performance to MoCL and Upper-Bound. Interestingly, L2R-merge lags behind the performance of MoCL, potentially due to the need for more data to properly train its router, as shown in our experiments. This is more evident for our L2R-merge, which has also been shown to be less effective to L2R-wavg possible for interference produced by merging the adapters."}, {"title": "D Efficiency Comparison: FLOPs Analysis", "content": "We compute the theoretical amount of FLOPS (floating-point operations) of our methods and some baselines for the MTL5 benchmark (i.e., 5 tasks). In Table 4, we observe BERT for classification as the more efficient model. This is expected as all the other methods augment BERT with PEFT modules. Our methods are the second most efficient models. L2R-wave is the less efficient one, as it always uses all the adapters and composes the input them. L2R-merge and DAM have the same amount of FLOPs as these models merge the adapters in one, making the computation more efficient. Although MoCL is the direct competitor to our methods in performance, it is not in terms of efficiency as this model extends the input, resulting in more operations."}]}