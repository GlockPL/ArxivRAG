{"title": "AlignGroup: Learning and Aligning Group Consensus with Member Preferences for Group Recommendation", "authors": ["Jinfeng Xu", "Zheyu Chen", "Jinze Li", "Shuo Yang", "Hewei Wang", "Edith C. H. Ngai"], "abstract": "Group activities are important behaviors in human society, providing personalized recommendations for groups is referred to as the group recommendation task. Existing methods can usually be categorized into two strategies to infer group preferences: 1) determining group preferences by aggregating members' person-alized preferences, and 2) inferring group consensus by capturing group members' coherent decisions after common compromises. However, the former would suffer from the lack of group-level con-siderations, and the latter overlooks the fine-grained preferences of individual users. To this end, we propose a novel group rec-ommendation method AlignGroup, which focuses on both group consensus and individual preferences of group members to infer the group decision-making. Specifically, AlignGroup explores group consensus through a well-designed hypergraph neural network that efficiently learns intra- and inter-group relationships. More-over, AlignGroup innovatively utilizes a self-supervised alignment task to capture fine-grained group decision-making by aligning the group consensus with members' common preferences. Extensive ex-periments on two real-world datasets validate that our AlignGroup outperforms the state-of-the-art on both the group recommenda-tion task and the user recommendation task, as well as outperforms the efficiency of most baselines.", "sections": [{"title": "1\nIntroduction", "content": "As social media becomes ubiquitous [25, 28], the online platform has become the main position and key information source of group ac-tivities, such as Mafengwo facilitating travel planning and Meetup hosting social gatherings [30]. Traditional recommendation sys-tems, designed for individuals, struggle to cope with the complexi-ties of group decision-making. This gap has spurred the need for group recommendation systems, which aim to balance members' individual preferences and group consensus to deliver fine-grained recommendations. In the e-commerce domain, such systems are crucial, as seen from Facebook Events to Yelp, where they must respect individual tastes while capturing group desires to ensure the accuracy of recommendations [7, 32].\nEchoing the methods of conventional recommender systems, latent factor models are pivotal in the domain of group recommen-dations. Specifically, groups and items are represented as vectors by mapping into the same semantic space. Then estimating the suit-able items closes with the group decision-making. Recent works [1-6, 11, 14, 24] on group recommendations have largely concen-trated on aggregating the preferences of members to predict the decision-making of a group. Classic aggregation techniques rely on pre-defined heuristic rules, such as averaging [3], minimizing mis-ery [1], or maximizing satisfaction [2]. More recently, there are also advanced deep-learning models [4, 5, 11, 14, 24] that employ atten-tion mechanisms to facilitate preference aggregation. For example, AGREE [4] proposes an attentive group recommendation that dy-namically aggregates the preferences of users within a group, and HyperGroup [10] explores the superior representational capabilities of hypergraphs for group recommendation. CubeRec [6] leverages the geometrically rich structure of hypercubes [20, 33] to integrate multifaceted member preferences. It is worth mentioning that Con-sRec [26] realizes that group consensus cannot be integrated by"}, {"title": "2\nRelated work", "content": "In this section, we review three related lines of research that con-tribute to group recommendation, namely methods for preference aggregation, hypergraph learning, and self-supervised learning."}, {"title": "2.1 Preference Aggregation", "content": "Earlier works use pre-defined heuristic rules to aggregate the pref-erences of users within a group to infer the group decision-making, such as averaging [3], minimizing misery [1], or maximizing satis-faction [2]. However, in the real world, each member has a different status and role within the group. Therefore, pre-defined aggregation rules cannot dynamically adjust the weights. Recently, many deep learning-based aggregation methods have been proposed, which dy-namically adjust the weight share of each user in a group through a learnable weight matrix. For example, MoSAN [24] first utilizes the attention mechanism to capture fine-grained interactions among group members dynamically. Nevertheless, it only considers users as group members. AGREE [4] broadens the scope of group mem-bers based on MoSAN, which considers both users and items as group members, and SoAGREE [5] further exploits the relation-ships among users to enhance the user representation in AGREE. SIGR [29] exploits and integrates the global and local social influ-ence of users. CAGR [30] first tries to learn centrality-aware user representations and then leverages it to refer to group decision-making. GroupIM [22] adopts mutual information maximization between users and groups to overcome the data sparsity problem between items and groups. HyperGroup [10] proposes a hyper-edge embedding-based method to enhance the representations. It is worth noting that some recent works break the constraint of aggregating information, such as CubeRec [6] exploits the geomet-ric representational power of hypercubes to frame the scope of group consensus, and ConsRec [26] utilizes multilevel views to col-lectively expose group consensus to infer group decision-making. However, we note that relying solely on group consensus or aggre-gating the preferences of group members is insufficient. Therefore, only by taking into account both individual preferences and group consensus can group decision-making be effectively inferred."}, {"title": "2.2 Hypergraph Learning", "content": "The relationship between the edge and vertex in the hypergraph is naturally similar to the relationship between group and member."}, {"title": "2.3 Self-supervised Learning", "content": "Recent works point out that self-supervised learning can also be effective in mitigating the data sparsity problem on the group rec-ommendation task, GroupIM [22] utilizes self-supervised signals to maximize the mutual information of users and groups to improve the representation, S2-HHGR [32] utilizes self-supervised signals to efficiently align user representations at different granularities, and CubeRec [6] proposes a novel self-supervised signal to capture the common preferences of users within a hypercube."}, {"title": "3 Preliminary", "content": "Formally, we use bold lowercase letters (e.g., e) and bold capital letters (e.g., E) to represent vectors and matrices, respectively. None-bold letters (e.g., x) and squiggle letters (e.g., X) are used to denote scalars and sets, respectively."}, {"title": "3.1 Task Definition", "content": "Let $U = \\{u_1, ..., u_{|U|}\\}$, $I = \\{i_1, ..., i_{|I|}\\}$, and $G = \\{g_1, ..., g_{|G|}\\}$ be the sets of users, items, and groups respectively. There are two types of observed interactions. We use $R_U \\in \\mathbb{R}^{|U|\\times|I|}$ to denote the user-item interactions between $U$ and $I$, where the element $R_U (n, m) = 1$ if user $u_n$ has interacted with item $i_m$ otherwise $R_U(n, m) = 0$. Similarly, we use $R_G \\in \\mathbb{R}^{|G|\\times|I|}$ to denote the group-item interactions between $G$ and $I$. The group $g\\in G$ consists of a set of user members $U_g = \\{U_1, ..., U_p, ..., U_{|U_g|}\\}$ where $u_p \\in U$. We denote the interacted item set of $g$ as $I_g = \\{i_1, ..., i_q, ..., i_{|I_g|}\\}$ where $i_q \\in I$. The task for group recommendation is defined as recommending items that target group $g$ may be interested in."}, {"title": "3.2 Hypergraph", "content": "Hypergraph is a more complex and enriched topological structure. Each hyperedge can contain more than one vertex. Formally, we define the hypergraph as $G = (V, E, H)$ where $V = U + I$ is the vertex set, $& is the hyperedge set, and $H\\in \\mathbb{R}^{|V|\\times|&|}$ depicts the connectivity of the hypergraph as $H(v, e) = 1$ if the hyperedge $e$ connects the vertex $v$ otherwise $H(v, e) = 0$. Furthermore, let $E_v$ denote a set of related hyperedges that connect to the node $v$ (i.e., $E_v = \\{e \\in E|H(v, e) = 1\\}$) and $V_e$ denote a set of nodes that connect to the hyperedge $e$ (i.e., $V_e = \\{v \\in V|H(v, e) = 1\\}$). In the group recommendation scenario, all members of a group together form a hyperedge, each member is a vertex, and each vertex can belong to more than one hyperedge. Therefore, we use $g \\in &$ to represent hyperedge instead of $e \\in &$ in some cases."}, {"title": "4 Methodology", "content": "In this section, we elaborate on AlignGroup architecture and de-scribe each component in detail. Specifically, AlignGroup first pro-vides a hypergraph neural network to capture both group con-sensus embedding and member embedding (including both users and items) via both intra-group relation learning (IntraRL) and inter-group relation learning (InterRL). To ensure that the user's personalization preferences are not compromised too much for group consensus, we further introduce a self-supervised alignment signal to steer the group consensus closer to the users' preferences."}, {"title": "4.1 Hypergraph Neural Network", "content": "In this subsection, we introduce a hypergraph neural network that aims to capture both group consensus and member preferences. Specifically, this hypergraph neural network aims to capture group consensus embedding \u2020$E_g$, refined user embedding \u2020$E_u$, and refined item embedding \u2020$E_i$ via graph propagation."}, {"title": "4.1.1 Group Consensus Embedding Learning via Hypergraph Neural\nNetwork", "content": "The group consensus is determined by the joint decision of all relevant members of the group, including users and items. At the same time, there is some similarity between groups with overlapping members.\nWe aim to extract both intra- and inter-group relations to effec-tively capture the group consensus.\nIntra-group Relation Learning (IntraRL): First, we consider intra-group relation learning. We represent each group as a hyper-edge $g\\in &$, including both users and items. However, user and item nodes preserve different semantic information. Therefore, we split the aggregation process of the hypergraph neural network into user-side and item-side. We feed the initial user embedding $E_u \\in \\mathbb{R}^{|U|\\times d}$, initial item embedding $E_i \\in \\mathbb{R}^{|I|\\times d}$, and initial group embedding $E_g \\in \\mathbb{R}^{|G|\\times d}$ to the hypergraph neural network.\nFormally, for hyperedge $g$, we compute the message for the user/item side within group $g$ via aggregation process:\n$e_{up} = E_u (p, :), e_{iq} = E_i (q, :)$ (1)\n$\\hat{e}_{g,u} = Aggr(e_{up}|u_p \\in U_g) = \\sum_{u_p\\in U_g} \\alpha_u e_p$ (2)\n$\\hat{e}_{g,i} = Aggr(e_{iq}|i_q \\in I_g) = \\sum_{i_q\\in I_g} \\alpha_i e_{iq}$ (3)\nwhere $a_i$ and $a_u$ are the simple average operation (i.e., $a_u = 1/|U_g|$ and $a_i = 1/|I_g|$). Then we fuse the user-side message and the item-side message through a linear transformation:\n$\\hat{e}_g = Concat(\\hat{e}_{g,u}|\\hat{e}_{g,i})W,$ (4)\nwhere $W \\in \\mathbb{R}^{2d\\times d}$ denotes the trainable weight matrix for message fusion and Concat is the concatenation operation. Then we stack all $\\hat{e}_g(g \\in G)$ to construct group consensus embeddings $E_g$.\nMany works [12, 16, 27, 31] verify that multiple-layer Graph Convolution Networks (GCNs) can improve the expressiveness of representation. Both user/item representation and group consensus representation can benefit from high-order neighbors. To this end, we average the embedding obtained at each layer to compute the final group representation \u2020$E_g$:"}, {"title": "Inter-group Relation Learning (InterRL)", "content": "Inspired by ConsRec [26], we noticed that groups may have their inter-group rela-tions. For example, as depicted in Fig 3, since Bob and Tom both like sports and mountain views, group 1 always chooses to climb the steeper Mount Hua and Mount Tai. However, in group 2, Bob is the only one who likes sports, and both Jessica and Lily like Japan, so they finally choose to enjoy Mount Fuji in Japan, which has a beautiful mountain view. It means that the degree of member overlap plays a significant role in the similarity between groups. To capture the group inherent preference among similar groups, we introduce a group overlap graph to capture the similarity between groups from a member overlap perspective.\nFormally, we build an adjacency matrix $W_g$ to discriminate rele-vance between groups. $W_g(g_p, g_q)$ means the overlap ratio between group $g_p$ and group $g_q$:\n$W_g (g_p, g_q) = \\frac{|V_{g_p} \\bigcap V_{g_q}|}{ |V_{g_p} \\bigcup V_{g_q}|}$ (6)\nWe further combine inter- and intra-group relations. Specifically, we utilize a group overlap graph to explore the inter-group relations together with intra-group consensus embeddings $E_g$, formally:\n$^{\\dagger}E_g = \\frac{1}{L+1}( \\sum_{l=1}^{L} E_g^{(l)} + E_g^{(0)} W_g).$ (7)"}, {"title": "4.1.2 User/Item Embedding Learning via Hypergraph Neural\nNetwork", "content": "Intuitively, the preferences of a user or the properties of an item can be jointly modeled by the behavior of all groups to which it belongs. For example, as described in Fig 4, the preferences of user u can be modeled by group consensus. Specifically, we can infer the preference of user u for Japanese cuisine by observing their observed liking for yakitori in Group 1 and for ramen and sushi in Group 3. Additionally, the preference of user u for noodles can be deduced from their fondness for knife-cut noodles in Group 2 and ramen in Group 3. It is worth noting that the pizza in Group 2 may mean user u chooses to compromise with the group consensus.\nTo enhance the representation of user/item preference, we ag-gregate all related representations of each user/item by collecting the message from related hyperedges:\n$\\hat{e}_u = Aggr(\\hat{e}_g|g \\in &u) = \\frac{1}{\\sum_{g'\\in &u} \\frac{(|U_{g'}| + |I_{g'}|)}{|&u|(|U_g|+|I_g|)}} \\sum_{g\\in &u} \\frac{(|U_{g}| + |I_{g}|)}{|&u|(|U_g|+|I_g|)} \\hat{e}_g,$ (8)\n$\\hat{e}_i = Aggr(\\hat{e}_g|g \\in &i) = \\frac{1}{\\sum_{g'\\in &i} \\frac{(|U_{g'}| + |I_{g'}|)}{|&i|(|U_g|+|I_g|)}} \\sum_{g\\in &i} \\frac{(|U_{g}| + |I_{g}|)}{|&i|(|U_g|+|I_g|)} \\hat{e}_g,$ (9)\nwhere $\\hat{e}_u$ and $\\hat{e}_i$ means the enhanced embedding of user u and item i, respectively. In the real world, the more people in a group, the lower the correlation between group consensus and individ-ual preferences. Therefore, we utilize group size to dynamically adjust the influence weights of different group consensus on individual preferences. $\\sum_{g'\\in &u} (|U_{g'}| + |I_{g'}|)/|&u|(|U_g| + |I_g|)$ and $\\sum_{g' \\in &i} (|U_{g'}| + |I_{g'}|)/|&i|(|U_g|+|I_g|)$ are the adjust weights. Then we stack all $e_u (u \\in U)$ and $e_i (i \\in I)$ to construct user and item embeddings $E_u$ and $E_i$, respectively.\nWe further enhance the expressiveness of $E_u$ and $E_i$ by GCNs. Specifically, we average the embedding obtained at each layer to get the final user representation \u2020$E_u$ and item representation \u2020$E_i$:\n$E_u^{(l+1)} = \\hat{E}_u^{(l)}, ^{\\dagger}E_u = \\frac{1}{L+1} \\sum_{l=0}^{L} E_u^{(l)}$ (10)\n$E_i^{(l+1)} = \\hat{E}_i^{(l)}, ^{\\dagger}E_i = \\frac{1}{L+1} \\sum_{l=0}^{L} E_i^{(l)}$ (11)\nwhere L is the total number of convolutional layers, $\\hat{E}_u$ and $\\hat{E}_i$ are the next layer representation of $E_u$ and $E_i$, respectively."}, {"title": "4.2 Self-supervised Alignment Task", "content": "Unfortunately, in many cases, members have to compromise their individual preferences to reach the group consensus. To satisfy both group consensus and member preferences, we propose a self-supervised alignment task to align group consensus and members' common preferences.\nTwo important problems need to be identified, a) the scope of group members, and b) how to calculate the common prefer-ences of members within a group."}, {"title": "4.2.2 How to Calculate the Common Preferences of Members within\na Group", "content": "For a given group $g\\in G$, it consists of a set of members $M_g = \\{m_1, ...m_k, ..., m_{|M_g|}\\}$, where $m_k \\in U_g$ or $m_k \\in U_g \\cup I_g$.\nThanks to the hypergraph neural network part, we have pro-jected the embedding of item, user, and group into the same seman-tic space. Therefore, we consider defining the common preferences of group members from a physical space perspective.\nGeometric Centroid: The geometric centroid for group $g$ can be computed as the following:\n$e_{g,c} = \\frac{Max(^{\\dagger}e_{m_k}|m_k \\in M_g) + Min(^{\\dagger}e_{m_k}|m_k \\in M_g)}{2}$ (12)"}, {"title": "4.2.3 Loss Function", "content": "We adopt InfoNCE [19] to align group con-sensus embeddings \u2020$E_g$ and members' common preferences em-beddings \u2020$E_c$. Formally, the alignment learning loss is defined as:\n$L_{align} = \\sum_{g_p \\in G} -log \\frac{exp (^{\\dagger}e_{g_p,c} \\cdot ^{\\dagger}e_{g_p} /\\tau)}{\\sum_{g_q \\in G} exp (^{\\dagger}e_{g_q,c} \\cdot ^{\\dagger}e_{g_q} /\\tau)},$ (14)\nwhere $^{\\dagger}e_{g_p,c}$ and $^{\\dagger}e_{g_q,c}$ are the vectors of $E_c$, and $^{\\dagger}e_{g_p}$ and $^{\\dagger}e_{g_q}$ are the vectors of $E_g$. \u03c4 is the temperature hyper-parameter."}, {"title": "4.3 Optimization", "content": "We introduce our optimization strategy that jointly learns user-item and group-item interactions. Specifically, we provide a shared Multi-layer Perceptron (MLP) as a prediction function to compute both user-item final scores and group-item final scores. Formally:\n$MLP(e) = LeakyReLU(eW_1) W_2,$ (15)\n$\\hat{y}_{u,i} = \\sigma(MLP(^{\\dagger}e_u \\odot ^{\\dagger}e_i)), \\hat{y}_{g,i} = \\sigma(MLP(^{\\dagger}e_g \\odot ^{\\dagger}e_i)),$ (16)\nwhere $W_1 \\in \\mathbb{R}^{d\\times8}$ and $W_2 \\in \\mathbb{R}^{8\\times1}$ denote the trainable weight matrices, $^{\\dagger}e_u, ^{\\dagger}e_i$, and $^{\\dagger}e_g$ are the vectors of $^{\\dagger}E_u, ^{\\dagger}E_i$, and $^{\\dagger}E_g$, respectively. $\\sigma$ denotes the Sigmoid function, and $\\odot$ is element-wise multiplication. We choose LeakyReLU [18] to solve the neuron death problem for ReLU.\nWith both the user-item interaction and group-item interaction data, we utilize the Bayesian Personalized Ranking (BPR) [21] loss for optimization:\n$L_u = - \\sum_{u\\in U} \\sum_{(p,n) \\in D_u} log \\sigma (\\hat{y}_{u,p} - \\hat{y}_{u,n}),$ (17)\n$L_g = - \\sum_{g\\in G} \\sum_{(p,n) \\in D_g} log \\sigma (\\hat{y}_{g,p} - \\hat{y}_{g,n}),$ (18)\nwhere $D_u$ and $D_g$ denote the user-item training set sampled for user u and group-item training set sampled for group g, respectively. For each (p, n) pair, p and n denote the observed item and unobserved item, respectively.\nWe jointly train $L_u$ and $L_g$ on all the user-item and group-item interactions as:\n$L_{rec} = L_u + L_g.$ (19)\nFinally, we combine the $L_{rec}$ with auxiliary self-supervised align-ment task $L_{align}$ to jointly training:\n$L = L_{rec} + \\delta_{align}L_{align}$ (20)\nwhere $\\delta_{align}$ is the balancing hyper-parameter."}, {"title": "5 Experiment", "content": "In this section, we conduct a comprehensive experiment to validate the effectiveness of our proposed AlignGroup. Specifically, we aim to answer the following research questions (RQs):\n\u2022 RQ1: Does AlignGroup outperform the state-of-the-art group recommendation methods?\n\u2022 RQ2: Whether the InterRL is necessary for AlignGroup?\n\u2022 RQ3: How do different strategies and scopes of our self-supervised alignment task affect the performance?\n\u2022 RQ4: Can AlignGroup effectively align group consensus and member preferences?\n\u2022 RQ5: How efficient is AlignGroup compared with other group recommendation methods?\n\u2022 RQ6: How do different hyper-parameter settings impact the per-formance of AlignGroup?"}, {"title": "5.1 Experimental Settings", "content": "To evaluate our AlignGroup in both group and user recommendation tasks, we conduct extensive experiments on two real-world public datasets, Mafengwo and CAMRa2011 [4].\n1. Mafengwo: The Mafengwo dataset encompasses travel records from a social tourism platform, detailing 5,275 users, 995 groups, and 1,513 locations with interactions between users and visited sites. This dataset was curated by selecting groups with at least two members who have visited three or more venues, averaging 7.19 users per group.\n2. CAMRa2011: The CAMRa2011 dataset provides a collection of movie ratings from individual and household users, comprising 602 users in 290 groups with a total of 7,710 movies rated on a scale from 0 to 100. To focus on communal viewing preferences, only users belonging to groups were retained, with ratings converted into binary feedback, resulting in 116,344 user-item and 145,068 group-item interactions, with an average group size of 2.08."}, {"title": "5.2 Overall Performance (RQ1)", "content": "achieves higher performance than all baselines, which demonstrates the effectiveness of our AlignGroup. We attribute this to the excellent ability of our IntraRL and InterRL in hyper-graph neural networks to capture group consensus. In addition, we note that the improvement of AlignGroup compared to other baselines is huge for NDCG, which is more concerned with fine-grained ranking order than HR. Therefore, we believe this implies that merely obtaining group consensus lacks fine-grained infer-ence over group behavior, such as ConsRec and CubeRec. On the contrary, our self-supervised alignment task can effectively capture the fine-grained behavior of the group by aligning the group consensus with the members' common preferences."}, {"title": "5.3 Ablation Study (RQ2 & RQ3 & RQ4)", "content": "In this section, we conduct exhaustive experiments to examine the effectiveness of various modules in our AlignGroup."}, {"title": "5.3.1 Effectiveness of InterRL (RQ2)", "content": "To verify the effectiveness of the InterRL in our hypergraph neural network, we design the following variant of AlignGroup.\nAlignGroup w/o InterRL: We remove the InterRL part and only retain intra-group relation learning. Formally, we utilize Eq 5 to compute group representation \u2020$E_g$ instead of Eq 7.\nAs shown in Fig 5, we can observe that removing InterRL de-grades the performance, showing that both intra-group and inter-group relations play a distinct role in group consensus learning."}, {"title": "5.3.2 Performance of Different Strategies and Scopes of Self-supervised\nAlignment Task (RQ3)", "content": "To analyze the performance of different strategies and scopes of our self-supervised alignment task, we in-troduce an ablation study. In terms of scope, we refer to considering only the user as \"small\" and considering both the user and item as \"big\". Therefore we try to combine two different scopes and two different strategies for computing common preferences:\nCentroid-small: We choose geometric centroid as common preferences for each group including small scope members.\nCentroid-big: We choose geometric centroid as common pref-erences for each group including big scope members.\nBarycenter-small: We choose geometric barycenter as com-mon preferences for each group including small scope members.\nBarycenter-big: We choose geometric barycenter as common preferences for each group including big scope members."}, {"title": "5.4 Efficiency Study (RQ5)", "content": "Following ConsRec [26] settings, we estimate the efficiency of Align-Group by directly comparing the total running time (including both training and testing) with all baselines. Fig 7 illustrates the per-formance (NDCG@5) and runtime (in seconds or minutes) for the group recommendation task on two experimental datasets. Our pro-posed AlignGroup achieves exceptional efficiency and performance, outperforming all baseline methods in terms of efficiency-with the sole exception of GroupIM-and significantly surpassing all baseline methods in performance."}, {"title": "5.5 Hyper-parameter Analysis (RQ6)", "content": "Layer number affects the performance of the hypergraph neural network. More layers can aggregate more information from high-order neighbors. However, as the layer number increases, it faces the over-smoothing problem [17] where the discrimina-tive of node representations is insufficient. Fig 8 illustrates the performance trends of AlignGroup with different settings of L. Considering both the group recommendation and user recommen-dation tasks, L = 3 is the optimal setting on both Mafengwo and CAMRa2011 datasets."}, {"title": "6 Conclusion", "content": "In this paper, we propose a novel group recommendation method, named AlignGroup. It extracts group consensus by learning intra- and inter-group relations and further refining the group decision-making by aligning group consensus and members' common pref-erences. In particular, AlignGroup effectively captures the group consensus from both intra- and inter-group relations by a well-designed hypergraph neural network. Moreover, AlignGroup pro-poses a novel and simple self-supervised alignment task to capture fine-grained group decision-making by aligning the group consen-sus with members' common preferences. Experimental results on two real-world datasets rigorously verify the effectiveness and ef-ficiency of AlignGroup for both group and user recommendation tasks."}]}