{"title": "Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and Cost", "authors": ["Maier, Jannis", "M\u00f6ller, Felix", "Purucker, Lennart"], "abstract": "Automated Machine Learning (AutoML) significantly simplifies the deployment of machine learning models by automating tasks from data preprocessing to model selection to ensembling. AutoML systems for tabular data often employ post hoc ensembling, where multiple models are combined to improve predictive accuracy. This typically results in longer inference times, a major limitation in practical deployments. Addressing this, we introduce a hardware-aware ensemble selection approach that integrates inference time into post hoc ensembling. By leveraging an existing framework for ensemble selection with quality diversity optimization, our method evaluates ensemble candidates for their predictive accuracy and hardware efficiency. This dual focus allows for a balanced consideration of accuracy and operational efficiency. Thus, our approach enables practitioners to choose from a Pareto front of accurate and efficient ensembles. Our evaluation using 83 classification datasets shows that our approach sustains competitive accuracy and can significantly improve ensembles' operational efficiency. The results of this study provide a foundation for extending these principles to additional hardware constraints, setting the stage for the development of more resource-efficient AutoML systems.", "sections": [{"title": "1 Introduction", "content": "Automated Machine Learning (AutoML) aims to automate the entire machine learning pipeline- from data preprocessing and feature selection to model selection and hyperparameter tuning- thereby reducing the need for manual intervention and expertise. State-of-the-art AutoML systems often use post hoc ensembling to further enhance model predictive accuracy (Purucker et al., 2023; Purucker and Beel, 2023). Post hoc ensembling involves creating ensembles from models generated during the model selection phase. By combining the predictions of multiple models, post hoc ensembling leverages the strengths of individual models and mitigates their weaknesses, resulting in improved predictive accuracy and generalization ability. Here, greedy ensemble selection (GES) (Caruana et al., 2004) is used in prominent AutoML frameworks like Auto-Sklearn (Feurer et al., 2020) and AutoGluon (Erickson et al., 2020) to further improve predictive accuracy over the single best model (Feurer et al., 2020; Purucker et al., 2023; Purucker and Beel, 2023).\nYet, post hoc ensembling also further increases the predictive cost of AutoML systems w.r.t. disk footprint, memory requirement, and inference time. GES for Auto-Sklearn requires, on average, ~8.5 models for inference (Purucker et al., 2023), whereby each model has to be stored on disk for deployment, in memory for inference, and has its own inference overhead. The problem is that practitioners cannot trade off improved predictive accuracy and increased predictive cost with existing post hoc ensembling algorithms. Furthermore, a greedy search for improved predictive accuracy, like in GES, might not provide a good set of options to choose from.\nWe aim to enable practitioners to balance improved predictive accuracy and increased predictive cost in AutoML. Therefore, in this work, we study hardware-aware ensemble selection, an approach to obtain a Pareto front (defined in Appendix A) for predictive accuracy and cost in AutoML. We focus on ensemble selection because it natively produces smaller, less expensive ensembles (Purucker and Beel, 2023) and is most often used in state-of-the-art AutoML systems (Purucker et al., 2023)."}, {"title": "2 Related Work", "content": "Integrating hardware constraints into AutoML and Neural Architecture Search (NAS) has gained significant attention over the last few years (Zhang et al., 2019; Benmeziane et al., 2021; Schneider et al., 2022; Sukthanker et al., 2024), reflecting the diverse demands of deployment environments. Schneider et al. (2022) introduced the application of QDO to NAS, aimed at generating a diverse set of architectures, each optimized for specific hardware constraints. Unlike traditional multi-objective NAS that seeks to approximate a Pareto front, this method focuses on optimizing for predefined, niche-specific requirements. The study illustrates that QDO's targeted approach potentially offers improvements over traditional methods in terms of efficiency and solution quality for hardware-aware NAS.\nIn the domain of multi-objective ensemble selection, several studies have explored optimizing model robustness and diversity without directly addressing hardware constraints. Works by Cavalcanti et al. (2016); Li et al. (2012); Partalas et al. (2010); Mart\u0131nez-Munoz and Su\u00e1rez (2004); Partridge and Yates (1996) have contributed to the understanding of ensemble pruning and selection by focusing on performance and uncertainty measures, yet typically do not consider hardware efficiency as a critical objective. To the best of our knowledge, no research has been done on hardware awareness in ensemble selection. We extend this focus by integrating hardware constraints, thereby enhancing the scope of multi-objective optimization in ensemble methods for AutoML solutions.\nOur approach to hardware-aware ensemble selection builds on applying QDO-ES as proposed by Purucker et al. (2023). QDO-ES uses principles from QDO and concepts from ensemble diversity to enhance the performance and robustness of ensemble selection. We extend this by including hardware-aware metrics, like inference time or ensemble size, when applying QDO to ensemble selection to create hardware-aware QDO-ES."}, {"title": "3 Method", "content": "In this study, we are comparing the performance of five ensembling methods and their ability to find an optimal Pareto front for predictive accuracy and inference time: (i) GES; (ii) QO-ES; (iii) QDO-ES; (iv) QDO-ES with ensemble size (Size-QDO-ES); and (v) QDO-ES with inference time (Infer-QDO-ES).\nGreedy Ensemble Selection. GES was first introduced by Caruana et al. (2004). Starting from the single best model, GES iteratively adds models to the ensemble that lead to the largest improvement. Compared to the other methods, GES usually produces only one solution. Thus, we extend GES by recording the ensemble produced at every iteration. Then, we obtain a set of ensembles that we use to compute a Pareto front.\nQuality (Diversity) Optimization Ensemble Selection. QO-ES and QDO-ES implemented by Purucker et al. (2023) maintain a population of ensembles, which both methods' evolution strategies iterate over to improve performance. QDO-ES extends QO-ES in that it makes sure the population includes diverse ensembles. In their application of QO-ES and QDO-ES, the algorithms only returned one solution. We extended both methods to extract the last population of ensembles of QO-ES and QDO-ES. Then, we use this population as the set of ensembles to compute the Pareto front. Apart from this, we left the remaining parameters of the evolutionary algorithms default, following the prior work. We also include the single best model in this set, i.e., an ensemble with only one member. This follows the set for GES, which starts with the single best model.\nHardware-Aware Quality Diversity Optimization Ensemble Selection. GES, QO-ES, and QDO-ES solely optimize for predictive accuracy. Thus, no aforementioned method is hardware-aware\u00b9. Therefore, we propose a variant of QDO-ES by adjusting its population management. Instead of creating a population that includes diverse ensembles, we guarantee that QDO-ES ensures that the population consists of ensembles with varying degrees of predictive cost.\nTo this end, we adjust the behavior space (Chatzilygeroudis et al., 2021) that QDO-ES operates on. The behavior space determines which individuals (i.e., ensembles) are kept in the population after each iteration. To guarantee that the behavior space becomes hardware-aware, we replace one of its two dimensions with a measure of predictive cost. In detail, we replace the config space similarity metrics, an ensemble diversity metric used by QDO-ES, with either inference time or ensemble size to obtain Infer-QDO-ES and Size-QDO-ES, respectively. Consequently, after each iteration, the population will contain ensembles with varying degrees of inference time or ensemble size. As for QO-ES and QDO-ES, we used the last population together with the single best model as the set of models for computing the Pareto front. While we could have used all ensembles from all iterations of the genetic algorithm, we want to provide a manageable set of solutions to a user.\nWe include Size-QDO-ES as an ablation for a different predictive cost measure. Ensemble size is generally a proxy for predictive cost and inference time but does not necessarily represent the true predictive cost. We follow the defaults of the implementation provided by Purucker et al. (2023) for population size and individual selection method."}, {"title": "4 Experiments", "content": "We use TabRepo (Salinas and Erickson, 2023) as a foundation for our experiments. TabRepo includes prediction probabilities on validation and test data for up to 1416 model configurations and 200 datasets, making it an invaluable tool for evaluating and simulating ensemble selection methods. We use its data D244_F3_C1530_100, which includes the results for 100 datasets with three-fold cross-validation and 1416 models. We omitted the 17 regression datasets from our testing because QDO-ES does not support regression, though it could be extended in the future. Each method was executed across 10 different seeds per fold to ensure robustness in our findings.\nTo evaluate Pareto fronts for predictive accuracy and cost for each method, we measure predictive accuracy using the ROC AUC on test data and predictive cost by inference time. Inference times are calculated based on the data in TabRepo, where inference times are recorded per dataset and model. We then define an ensemble's inference time as the sum of the inference times of all its models. To measure the quality of Parteo fronts, we employ hypervolume (defined in Appendix A)."}, {"title": "5 Results", "content": "Balancing Predictive Accuracy and Cost. To determine which method best balances predictive accuracy and cost, we examined the hypervolumes of the Pareto fronts produced by each method, as illustrated in Figure 1. Infer-QDO-ES demonstrated superior performance, outpacing other methods with a statistically significant margin. It excelled in identifying solutions that balance predictive performance and cost. GES, QDO-ES, and GES showed similar results regarding hypervolume, indicating a lower but comparable capability in optimizing the trade-off between efficiency and performance. QO-ES performed worst, likely attributed to the lack of diversity compared to QDO-ES.\nPredictive Accuracy after Balancing. Using Infer-QDO-ES and Size-QDO-ES does not detrimentally affect predictive performance when returning only the best ensemble of the Pareto front. These methods even surpassed the performance of QDO-ES, as shown in the Appendix, Figure 2. We additionally provided ROC AUC results for each method per dataset in the Appendix, Table 2. We observe that our methods, alongside GES, QDO-ES, and QO-ES, generally yield comparable, not significantly different high-quality results across the tested datasets. Moreover, including hardware-aware metrics in QDO-ES might even improve the robustness of the generated ensembles.\nOur results, as shown in Figure 2, reproduce the conclusions by Purucker et al. (2023) that QO-ES and QDO-ES outrank GES w.r.t. ROC AUC on test data. This is particularly interesting since Purucker et al. (2023) used data from Auto-Sklearn while we used TabRepo's data from AutoGluon.\nPredictive Cost after Balancing. When assessing the efficiency of the best ensembles, particularly in inference times, our methods outperformed those created by QDO-ES and QO-ES. This is depicted in the Appendix, Figure 3. However, they did not achieve the efficiency levels of GES, which typically produces a smaller final ensemble requiring fewer models during inference. Given the larger ensemble sizes typically generated by QO-ES and QDO-ES (Purucker et al., 2023), this outcome aligns with our expectations. Moreover, as seen in Figure 1, practitioners could also choose a comparable or better ensemble from the Pareto fronts when using Infer-QDO-ES or Size-QDO-ES.\nSummary. Our results demonstrate that Size-QDO-ES and Infer-QDO-ES enhance the efficiency of AutoML solutions and maintain competitive accuracy, effectively managing the trade-offs between accuracy and operational speed. Our methods' ability to generate superior Pareto fronts underscores their effectiveness and provides greater flexibility for practitioners, allowing for choosing models to meet specific hardware or business constraints."}, {"title": "6 Conclusion", "content": "Our study demonstrated the viability of hardware-aware post hoc ensemble selection in AutoML, presenting a significant advancement in ensemble selection methodologies that integrate hardware efficiency considerations. By incorporating a hardware constraint like inference time into the ensemble selection process, we have shown that it is possible to maintain competitive accuracy and significantly enhance the models' operational efficiency.\nIntegrating quality diversity optimization principles into ensemble selection has proven particularly effective, yielding Pareto fronts that reflect an optimal balance between predictive performance and hardware efficiency. Our proposed variant of QDO-ES, Infer-QDO-ES, has outperformed traditional ensemble methods by producing ensembles that are not only diverse and high-performing but also tailored for specific hardware limitations. These findings underscore the importance of considering hardware constraints in the ensemble selection process, particularly in resource-constrained deployment environments.\nThe implications of this research extend beyond the immediate improvements in ensemble selection; they suggest a paradigm shift in how AutoML systems are designed and deployed. By demonstrating that hardware-aware ensembles can achieve comparable or even superior performance to traditional methods, this study paves the way for more sustainable and efficient AutoML solutions, reducing both computational costs and environmental impacts.\nOur work is limited to the evaluation of data from TabRepo and classification tasks. Additionally, we only considered two hardware constraints, omitting others like FLOPs, area, and energy consumption. Finally, the effectiveness of hardware-aware model selection compared to our post hoc hardware-aware ensemble selection approach remains unclear. For future work, we suggest several directions: (1) To broaden their applicability, extend hardware-aware ensemble methods to other types of machine learning tasks, such as regression. (2) Further exploring hardware constraints or multiple constraints simultaneously, such as energy consumption or memory usage, to better tailor models to deployment scenarios. (3) Investigating these methods' integration and potential user interface into mainstream AutoML frameworks, making them accessible to a wider range of users.\nIn conclusion, hardware-aware ensemble selection represents a promising advancement in AutoML. It aligns model performance with the practical realities of deployment environments. This approach not only enhances the practicality of AutoML solutions but also contributes to the broader goal of making machine learning more accessible and sustainable.\nBroader Impact Statement. After careful reflection, we determined that this work does not have new negative broader impacts that are not already present for existing state-of-the-art AutoML systems. However, we hope that our work will have a positive, broader impact by enabling practitioners to balance costs and profits when applying AutoML systems. Thus, practitioners can select, e.g., less energy-consuming ensembles with only minimal loss in performance."}, {"title": "A Additional Definitions", "content": "Pareto Optimality. : A solution is Pareto optimal if no other solution exists that can improve some objectives without worsening others. Such solutions are efficient and provide a set of equally valid alternatives based on different priority scenarios (Van Veldhuizen et al., 1998).\nPareto Front. : The Pareto Front is defined as the collection of all solutions in a multiobjective optimization problem that are Pareto optimal. A solution is Pareto optimal if no other feasible solution exists that improves at least one objective without worsening another. Thus, the Pareto Front represents the boundary in the objective space beyond which no further improvements can be made without trade-offs (Van Veldhuizen et al., 1998).\nHypervolume. : Hypervolume measures the volume enclosed by the Pareto front and a reference point, capturing the extent of the objective space covered. It quantifies both the convergence to the Pareto front and the diversity of solutions, increasing as the set of solutions better approximates the true Pareto front (Bader and Zitzler, 2011)."}, {"title": "B Resource Details", "content": "The experiments were conducted on a variety of server configurations at [removed], including 1. Dell R740xd and Asus ESC4000 with Xeon 6254/6354, 756GB/1TB RAM; 2. Dell R920 with E7-4880, 1TB RAM; 3. Dell R740 with Xeon 6134, 756GB RAM; and ran for roughly 4 days. We can use diverse hardware since TabRepo precalculates the inference times. Code for the experiments can be found at https://github.com/Atraxus/HA-ES."}, {"title": "C Tabrepo Details", "content": "In this section, we provide detailed statistics related to the distribution of dataset classes in the context of D244_F3_C1530_100. Table 1 enumerates the datasets according to the number of classes they contain. Entries are only included for class counts where datasets are available. The absence of a particular class count indicates that there are no corresponding datasets in this context."}]}