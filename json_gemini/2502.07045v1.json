{"title": "Scalable and Ethical Insider Threat Detection\nthrough Data Synthesis and Analysis by LLMs", "authors": ["Haywood Gelman", "John D. Hastings"], "abstract": "Insider threats wield an outsized influence on orga-\nnizations, disproportionate to their small numbers. This is due\nto the internal access insiders have to systems, information, and\ninfrastructure. Signals for such risks may be found in anonymous\nsubmissions to public web-based job search site reviews. This\nresearch studies the potential for large language models (LLMs)\nto analyze and detect insider threat sentiment within job site\nreviews. Addressing ethical data collection concerns, this research\nutilizes synthetic data generation using LLMs alongside existing\njob review datasets. A comparative analysis of sentiment scores\ngenerated by LLMs is benchmarked against expert human\nscoring. Findings reveal that LLMs demonstrate alignment with\nhuman evaluations in most cases, thus effectively identifying\nnuanced indicators of threat sentiment. The performance is lower\non human-generated data than synthetic data, suggesting areas\nfor improvement in evaluating real-world data. Text diversity\nanalysis found differences between human-generated and LLM-\ngenerated datasets, with synthetic data exhibiting somewhat\nlower diversity. Overall, the results demonstrate the applicability\nof LLMs to insider threat detection, and a scalable solution for\ninsider sentiment testing by overcoming ethical and logistical\nbarriers tied to data acquisition.", "sections": [{"title": "I. INTRODUCTION", "content": "Insider threats place organizations at great risk of attack\ndue to internal employee access to systems, information, and\ninfrastructure [1]. Research conducted on insider threats takes\nmany paths to detection and mitigation through technological,\npsychological, behavioral, and educational tools [2]. Despite\navailable remedies, insider threat detection remains elusive due\nto the difficulty in perceiving technical and human indicators\nthat signal the presence of an insider threat [3], [4].\nOne possible source of employee related sentiment is job\nsearch sites where current and past employees rate their\nemployers. Usernames are anonymized, encouraging honest\npoints of view in reviews [5]. Many reviews express negative\nsentiment toward current and former employers, which though\nhopefully honest, can cause damage to an employer's reputa-\ntion [6]-[8]. Considerable negative reviews on Glassdoor can\nalso appear during times of a company's monetary misman-\nagement [6]-[8], sentiment that pervades long after assuaging\nof monetary concerns [8].\nThe potential for anonymous employee reviews to reveal\ninsider threat sentiment has seen minimal research. Reviews\nmay express dissatisfaction or highlight organizational vul-\nnerabilities, but some may also inadvertently or intentionally\ndisclose confidential corporate data, proprietary secrets, or\nsecurity risks [5], [6]. This creates an opportunity to analyze\nsuch reviews for indicators of insider threats. However, ethical\nand legal concerns with web scraping and dataset validity [9],\n[10] pose challenges to leveraging these sources directly.\nIn addition, through website restrictions, automation of non-\nauthenticated access can produce only limited data [11].\nThis research proposes that indicators of insider threats can\nbe found in job reviews, including confidential and propri-\netary information that can harm an organization by intellect,\nfinances, and reputation. Many existing insider threat detection\nmethods use machine learning methods for sentiment analysis\nacross various web platforms [12], but none, thus far, have\nused LLMs for this task. In addition, none of the work has\nlooked at synthesizing data for insider threat research pur-\nposes. Inferred use can be drawn from [13] for cybersecurity\ndetection and response, and the promise of refined sentiment\nanalysis in [14]. To address these research challenges, this\nresearch utilizes LLMs to synthetically generate employer re-\nviews through carefully crafted prompts [13] to minimize bias,\nthereby overcoming ethical and logistical barriers associated\nwith sourcing data at scale through web scraping or other\nmeans. The following research questions guide the study:\nRQ1: How can LLMs synthesize job site reviews that sim-\nulate human insider threat sentiment using carefully\ncrafted prompts to minimize bias?\nRQ2: How representative are the LLM synthesized review\ndatasets compared to human data sets?\nRQ3: How can LLMs be utilized to analyze synthetic and\nexisting datasets for insider threat sentiment?\nThis research is significant in its approach to uncovering\ninsider threat sentiment in employer reviews, and expanding\nthe scale of data available for insider threat research. This\nknowledge can assist in the formation of more effective\ncorporate policy, to improve security of proprietary data, and\nincrease public awareness of insider threats. By advancing the\nuse of LLMs for security-focused sentiment analysis, this work"}, {"title": "II. METHODOLOGY", "content": "Claude Sonnet 3.5 [15] synthetically generated a job review\ndataset, and was compared to an aggregate pair of existing\nGlassdoor review datasets. Two LLMs, Sonnet 3.5 and GPT-40\n[16], were employed to analyze both datasets for insider threat\nsentiment. The methodology is composed of search criteria for\nrelated work, LLM prompts used for review generation and\nsentiment analysis, existing datasets employed, and methods\nused for sentiment analysis. The LLMs are used to analyze\nboth synthetic and publicly available (yet limited) real-world\ndata for insider threat sentiment, with results compared to\nhuman expert evaluations to assess validity.\nA. LLM Selection\nResearch was performed on LLMs that met specific use\ncriteria that would be suitable for review generation and\nsentiment analysis. The most important LLM criteria were\nnatural language processing with native sentiment analysis\ncapabilities, an API for interface with a python script to pass\ndata from a comma separated value (CSV) file to the LLM,\nand the ability write results back to a CSV file. For sentiment-\nrelated tasks, Sonnet 3.5 and GPT-40 are outstanding [17],\nwhile demonstrating that they can capably produce synthetic\nsentiment data [18]. For the tailored prompts described in this\nsection, prompts from [18] were used as a starting point.\nB. Glassdoor Dataset\nExisting Glassdoor review datasets were chosen from Kag-\ngle [19], [20] containing approximately 10.8 million records.\nLoosely defined insider threat related keywords including\n\u201chate\u201d, \u201ctoxic\u201d, \u201ccaught\u201d, \u201csteal\u201d, \u201ccorrupt\u201d, \u201ccollu", "stole\u201d,": "elet", "pay\u201d, \u201cpaid\u201d, and \u201cfraud\u201d reduced the dataset to\n1.8 million. The given approach involved selecting multi-\nmatch keywords (\"delet\u201d for": "elete", "deletion": "deleted", "or": "eleting", "collu": "or \u201ccolluding\u201d, \u201ccolluded", "collusion": "nto minimize bias in keyword choices. For a population of 10.8\nmillion records, a sample size of 385 (rounded up from 384.14)\nwas determined based on [21]. The sorted order of the 1.8\nmillion record set was randomized with the first 385 records\nchosen as the sample and used as the matching set. Matching is\nrequired in this situation because a control set is not possible.\nC. Expert Scoring Criteria\nHuman insider threat manual scoring of Glassdoor reviews\nwas performed by one expert insider threat researcher as the\ngold standard, and used to create a suitable comparison to\nLLM-generated reviews. Criteria include scores 0.0 to 1.0\n(0.0 as most negative, 1.0 as most positive) with general\nlevels of critical, high, medium, low, and nominal, respectively.\nCritical level indicators (0.0-0.2) include revenge motivations,\nunambiguous intent to commit an insider threat act, and\ndemonstration of knowledge to damage systems, information,\ninfrastructure, organizational finances, or reputation. High\nlevel indicators (0.2-0.4) manifest as explicit objections to\npolicy or managerial malfeasance, antipathy, and either a\ndesire to seek employment elsewhere or a recognized bias\nafter separation. Medium level indicators (0.4-0.6) include\ndisturbance, minor policy disagreements, and specificity in\ncriticism. Low level indicators (0.6-0.8) include productive\ndisapproval, non-specific criticism, and statement of common\nconcerns (pay complaints being the most common). Nominal\nlevel indicators (0.8-1.0) indicate neutral to positive employer\nattitudes with negligible insider threat risk. Crossover scores,\nwhere a score bordered between two scoring groups (0.2,\n0.4, 0.6, and 0.8) were used to indicate a borderline threat\ndetermination between two groups.\nSentiment analysis prompts in this study intentionally avoid\nassigning a weight to scoring values so as not to bias the\nresults. This was an important consideration in order to study\nthe inherent understanding of LLM insider threat sentiment.\nThe above level indicators determine that a neutral score\nindicates no insider threat risk in the 0.8-1.0 range. Single\nscore results that follow will demonstrate the LLMs inherently\nunderstood this critical insider threat sentiment concept.\nD. Synthetic Reviews (RQ1)\nFor synthetic review generation, Sonnet 3.5 was chosen\nbased its performance in ad hoc testing. Using the API, 385\nsynthetic reviews were generated with the prompt in Table\nI. The prompt requests that Sonnet 3.5 generate a job review\nusing a given sentiment score between 0.0 and 1.0, with 0.0 as\nmost negative and 1.0 as most positive. The prompt was sent\n35 times per each of the 11 scoring positions (i.e., 0.0, 0.1,...,\n1.0) until 385 reviews were generated. To construct datasets\nwith a similar configuration to the Glassdoor datasets, the\nprompt requests a randomly generated date, employee status,\nand job title, in addition to pros and cons that are common to\nGlassdoor reviews. The prompt was also given a word limit of\n40 words per pro or con to minimize API cost, and asked to\noutput the data in CSV-readable format for post-processing.\nE. Text Diversity (RQ2)\nText diversity is an analysis of the unique lexical charac-\nteristics of LLM-generated content [18], and can be scored\nthrough a variety of measurements including CR (compression"}, {"title": "F. Single Score Sentiment Analysis (RQ3)", "content": "The Glassdoor and synthetic datasets were fed through\nthe APIs for GPT-40 and Sonnet 3.5 to score insider threat\nsentiment in the reviews. Initially, the LLMs were asked to\nscore the pros and cons fields separately. However, insider\nthreat activity might appear in either the pros or cons fields,\nso generating one overall sentiment score by having the LLM\nlook holistically at the full review made more sense than\nattempting to numerically combine pro and con scores. There-\nfore, the final prompt shown in Table II was used to produce\nsingle insider threat sentiment scores. In brief, this prompt\nrequests that datasets be analyzed for insider threat sentiment\ncontained in each review. A score range is provided to reflect\npositive or negative sentiment and a concise explanation for\neach sentiment score is requested.\n1) Alignment Evaluation: In assessing alignment between\nthe gold standard scoring and LLM sentiment scoring of\nGlassdoor reviews, and between target sentiment scores and\nLLM sentiment scoring for synthetic reviews, well-established\nmetrics for pairwise numerical data analysis were used:\nmean absolute difference (MAD) and mean squared difference\n(MSD) were employed\u00b9 . These metrics provide a comprehen-\nsive analysis of outcomes in each approach."}, {"title": "III. RESULTS", "content": "A. Synthetic Reviews (RQ1)\nThe approach described in the prior section was used by\nSonnet 3.5 to synthesize reviews. An illustrative example\ncontaining a variety of interesting potential insider threats\nappears in Table III."}, {"title": "IV. DISCUSSION", "content": "A. Sentiment Analysis\nAddressing RQ1, data analysis was expected to reveal a\ndegree of bias, indicating limitations in current LLM devel-\nopment. Bias in synthetic reviews and sentiment analysis was\nminimized through careful prompt creation [10]. Human error\nis common in manual processing of large datasets. Aside from\nnoted human errors, lessons learned can be applied to future\nwork. Key findings determined that in many examples, expert\nanalysis viewed indicators similarly to LLM sentiment anal-\nysis, and some examples differently. Similar scoring include\nan employer who demonstrated dishonest business dealings,\ndiscriminatory behavior, ethical and moral concerns exposed\nwith specificity, a combative environment that creates disillu-\nsionment, threatening behavior from management, nepotism,\nand theft of code. Differences in interpretation of indicators in-\nclude the previously noted pay complaint (\"there's no amount\nof money that is enough\"), corporate revenge, evidence of\ndark trait characteristics of narcissism and psychopathy with\ndisdain for people with education [3], and confrontational\nissues. One important area of commonality is that in agreement\nor disagreement, expert and LLM-generated reviews surfaced\nvalid concerns of insider threats.\nIndicators of insider threats in organizations are small, so a\nsmall degree of correlation was expected in the collected data\nbetween the two datasets, addressing RQ3. Results demon-\nstrated indicators of insider threats proportional with levels\nof such threats observed within organizations through use of\nnatural language processing inherent to LLMs.\nB. Research Implications\nThis research has applicability to significance and social\nconstruct by assisting organizations in gaining a better un-\nderstanding of technical and reputational risks in job site\nreviews, as well as raising public awareness of insider threats.\nThe use of human and synthetic LLM-generated job reviews\nto test LLM-based sentiment analysis is effective based on\nan expected small degree of insider threat occurrence within\norganizations. A wide margin of disagreement was seen in\nonly 1.3% of reviews between human and LLM-generated\nsentiment analysis (5 out of 385), answering RQ3.\nAddressing, RQ1 through RQ3, the research presented a\nrepeatable methodology. Generalizability lies in the global\nsources for the existing dataset, variables of interest, and the\nability to reuse the given prompts so researchers may generate\ntheir own datasets. For global generalizability, prompts would\nbe translated to a target language."}, {"title": "V. RELATED WORK", "content": "A. Trends\nResearch by [23] in their review of insider threat research\nwith natural language processing determine the viability of\nresearch using the CERT dataset [24]. Use of the CERT\ndataset is common among NLP insider threat researchers"}, {"title": "VI. CONCLUSION", "content": "Insider threats place organizations at grave risk of cyber,\nfinancial, and reputational risk. Job site reviews that contain\nindicators of threats to an organization's intellectual property,\nreputation, and cybersecurity provide an opportunity to gain\nanonymized insight into these threats. Insider threat research\nis a well-studied field, but little research exists on natural\nlanguage processing of job site reviews that can cause degrees\nof harm to an organization by way of insider threats. Research\nutilized the the Sonnet 3.5 model to synthetically generate\na dataset, as well as an existing dataset from Glassdoor,\nwhich were subsequently processed through Sonnet 3.5 and\nGPT-40 models for insider threat sentiment. Results were\nanalyzed and compared using available data analysis tools to\nestablish connections between collected data and insider threat\nsentiment. Results demonstrated a percentage of job reviews\nwith insider threat sentiment proportional to the number of\ninsider threats seen in organizations. The importance of this\nresearch is to create awareness for organizations as to their\nlevel of insider threat risk to influence policy in areas of\nemployment, employee job satisfaction, data protection, pro-\ntection of intellectual property, and protection of confidential\ninformation. Research provides a new insider threat dataset,\nopenly available to insider threat researchers as a method to\ncompare tool effectiveness. This research will also stimulate a\ndialog on the importance of insider threats, and to raise aware-\nness with the public as to their criticality. The potential impact\non policy will assist organizations in protecting information,\nimprove employee job satisfaction, and protect organizations\nfrom financial and reputational loss."}]}