{"title": "Distribution-Level Memory Recall for Continual\nLearning: Preserving Knowledge and Avoiding\nConfusion", "authors": ["Shaoxu Cheng", "Kanglei Geng", "Chiyuan He", "Zihuan Qiu", "Linfeng Xu", "Heqian Qiu", "Lanxiao Wang", "Qingbo Wu", "Fanman Meng", "Hongliang Li"], "abstract": "Abstract\u2014Continual Learning (CL) aims to enable Deep Neu-\nral Networks (DNNs) to learn new data without forgetting previ-\nously learned knowledge. The key to achieving this goal is to avoid\nconfusion at the feature level, i.e., avoiding confusion within old\ntasks and between new and old tasks. Previous prototype-based\nCL methods generate pseudo features for old knowledge replay\nby adding Gaussian noise to the centroids of old classes. However,\nthe distribution in the feature space exhibits anisotropy during\nthe incremental process, which prevents the pseudo features\nfrom faithfully reproducing the distribution of old knowledge\nin the feature space, leading to confusion in classification bound-\naries within old tasks. To address this issue, we propose the\nDistribution-Level Memory Recall (DMR) method, which uses a\nGaussian mixture model to precisely fit the feature distribution\nof old knowledge at the distribution level and generate pseudo\nfeatures in the next stage. Furthermore, resistance to confusion\nat the distribution level is also crucial for multimodal learning,\nas the problem of multimodal imbalance results in significant\ndifferences in feature responses between different modalities,\nexacerbating confusion within old tasks in prototype-based CL\nmethods. Therefore, we mitigate the multi-modal imbalance\nproblem by using the Inter-modal Guidance and Intra-modal\nMining (IGIM) method to guide weaker modalities with prior\ninformation from dominant modalities and further explore useful\ninformation within modalities. For the second key, We propose\nthe Confusion Index to quantitatively describe a model\u2019s ability\nto distinguish between new and old tasks, and we use the\nIncremental Mixup Feature Enhancement (IMFE) method to\nenhance pseudo features with new sample features, alleviating\nclassification confusion between new and old knowledge. We\nconduct extensive experiments on the CIFAR100, ImageNet100,\nand UESTC-MMEA-CL datasets, and achieve state-of-the-art\nresults.", "sections": [{"title": "I. INTRODUCTION", "content": "EEP neural networks (DNNs) have excelled in learning\ncomplex patterns and representations from data, leading\nto numerous breakthroughs in industry [1], [2], [3], [4], [5].\nHowever, complex data emerges continuously in the form of\ndata streams. In this scenario, deep neural networks (DNNs)\nhave been found to struggle with processing continuous data.\nOne of the challenges in training DNNs is the phenomenon\nwhere they tend to forget previously learned knowledge when\nnew data is introduced, known as \u201ccatastrophic forgetting\".\nVarious methods have been proposed to address this issue from\ndifferent perspectives. One approach involves storing a small\nportion of old samples while continuously inputting new sam-\nples into the network. Another method involves using knowl-\nedge distillation to constrain the student model to retain old\nknowledge at the input-output level. Regularizing network pa-\nrameters, continually expanding and adjusting network struc-\ntures, and other methods focus on preserving old knowledge by\nmanipulating model parameters or structures. Prototype-based\nmethods also exist, which maintain old knowledge by using\nprototypes of old categories. The exemplar-free prototype-\nbased method is favored for its privacy-preserving character-\ntics. However, it lacks a thorough consideration from the\nperspective of preserving classification boundaries of old tasks.\nPrior prototype-based methods generate pseudo features in the\nincremental task phase that cannot accurately reproduce old\nknowledge, leading to confusion in classification boundaries\nwithin old tasks. Therefore, for prototype-based methods,\ninstead of combining them with more complex network struc-\ntures, regularization strategies, or using networks from both\nold and new stages for knowledge distillation, it is more ef-\nfective to seek solutions from the distribution of old knowledge\nitself. We propose a method for preserving and reproducing\nknowledge at the distribution level in the representation space.\nAt each stage, we use GMM to fit the distribution of high-\ndimensional features, and improve it by adaptively determining\nthe number of Gaussian components. Furthermore, we apply\ntwice information degradation to preserve the distribution. In\nthe next task stage, we generate pseudo features. The resulting\npseudo features closely approximate the true distribution of\nold samples, maintaining the classification boundaries between\nclasses within the old task and maximizing the preservation of\n\u201ctrue memories\u201d. Furthermore, Artificial intelligence has made\nsignificant breakthroughs in the industrial sector, sparking a\nwave of applications [6], [7], [8], [9]. We continuously study\nits application in real-world scenarios of multimodal behavior\nrecognition. Building on Peng et al.'s work [10], we delve\ninto the issue of imbalance in multimodal networks, which\ncan lead to suboptimal performance. We observe that the\nimbalance between modalities results in significant differences\nin the response of different modal features 12, rendering\nprevious prototype-based methods ineffective for reproducing\nold knowledge. Therefore, we propose a dual information\nD"}, {"title": "II. RELATED WORK", "content": "In the realm of continual learning, the goal is to enable\nartificial neural networks to continuously \u201cevolve\" in their\nlearning, acquiring new knowledge without forgetting. To\nmitigate forgetting, several approaches have been proposed:\nFirst is the Dynamic Network Approaches, they extend\nthe network backbone or some network layers, enabling the\nextended parts to accommodate new knowledge while the orig-\ninal parts maintain old knowledge. DEN [19] trains neurons\nrelevant to incremental tasks purposefully, mitigating the drift\nof individually optimized neurons. DER [20] also expands the\nbackbone continuously during incremental learning to retain\nold knowledge, but with a broader linear classifier aggregat-\ning features from multiple backbones. Inspired by gradient\nboosting algorithms, FOSTER [21] dynamically adjusts the\nnetwork and employs knowledge distillation to retain old\nknowledge. However, these methods may face issues with\nexcessive memory consumption due to the continual expansion\nof the backbone network. MEMO [22] observes that only\ndeeper network layers exhibit specificity to different incre-\nmental tasks, thus saving storage by expanding only specified\nnetwork layers.\nParameter regularization methods impose regularization on\nnetwork parameters to prevent parameter drift, EWC [23]\nevaluates the importance of parameters for old knowledge\nusing the Fisher information matrix, protecting parameters\ndeemed more crucial for old knowledge, thus safeguarding\nold knowledge from a parameter perspective. SI [24] defines\nthe importance of parameters for old knowledge based on the\nmagnitude of their influence on the loss function, proposing an\nonline method for importance assessment. IADM [25] assesses\nthe importance of parameters in different network layers,\ncontrary to EWC [23], which regularizes global parameters.\nKnowledge distillation is a mainstream method in continual\nlearning. These methods add regularization distillation loss\nfunctions to the inputs and outputs. LwF [26] was the first to\napply distillation strategies in continual learning. It maintains\nold knowledge by using the frozen old model as the teacher\nmodel to guide the student model in the incremental phase at\nthe input and output levels. The iCaRL [16] builds upon LwF\n[26] by adding an exemplar set for replay to further protect\nold knowledge. COIL [27] employs bidirectional distillation to\nleverage semantic relationships in both new and old models.\nIn addition to logits distillation, UCIR [28] employs a more\npotent feature distillation, stipulating that the encoder's outputs\nshould be similar. It discovers weight drift in the classifier and\naddresses this issue by using a cosine classifier. Building upon\nthis, WA [29] continuously normalizes weights and introduces\nweight clipping to prevent drift. ISM-Net [30] combines\ndistillation with incremental semantic mining between taskes,\nthereby continuously retaining old knowledge while expanding\nthe network to learn new knowledge. PODNet [31] pools and\nreduces the differences in feature maps from different network\nlayers before distillation. IL2A [32] evaluates the feature\nvalues obtained from the spectral decomposition of features of\nnew and old classes, finding that maintaining relatively large\nfeature values can mitigate forgetting, further alleviated by\ndistillation.\nThese methods can be classified into two classes: replay-\nbased and exemplar-free methods. Strategies involving replay\nexemplars are often combined with other approaches, such\nas the FOSTER [21] combining with dynamic networks and\nthe LwM [33] method combining with parameter regulariza-\ntion, especially in conjunction with distillation. Because these\nmethods can directly access old data during the incremental\nprocess, they generally outperform methods that do not use\nreplay strategies.\nPrototype-based methods primarily operate in the embed-\nding space, without direct access to the raw data of old classes.\nHowever, when IL2M [34] first introduced methods that store\nprototypes and simultaneously use sample replay strategies,\nsubsequent prototype-based methods mostly avoided using\nreplay strategies. The PASS [35] first obtains a generalized\nencoder and features through self-supervised methods in the\ninitial stage. It then stores the class centers and mean squared\ndeviations of old classes, and enhances the prototypes with\nGaussian noise in the incremental stage to preserve old knowl-\nedge. Subsequent SSRE [36] method selects samples based\non the similarity between prototypes and new samples, and\nthen distills to enhance the Discriminability between new and\nold classes. Additionally, it extends and reorganizes network\nlayers to maintain old features. The FeTrIL [37] enhances old\nprototypes by computing the similarity between new and old\nclass prototypes and shifting all features of new samples to the\npositions of old class prototypes, thereby maintaining knowl-\nedge of old classes. Prototype-based methods can maintain\nknowledge through prototypes of old classes at the feature\nlevel."}, {"title": "B. Multimodal Continual Learning", "content": "In addition to the problem of class imbalance, the work on\nmultimodal continual learning has also attracted considerable\nattention, especially in the field of vision and language.\nSrinivasan et al. [38] proposed the CLIMB framework, which\nmodifies the vision-language transformer to implement sev-\neral continual learning methods. They provide a benchmark\nfor continual learning in vision-language tasks, aiming to\npromote research on novel continual learning algorithms in\nthis challenging multimodal environment. Zhu et al. [39]\ncontributed a unified multimodal dataset for continual learning."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Problem Statement", "content": "Continual learning is a learning paradigm that requires data\nto be continuously inputted as a series of tasks, mimicking\nthe scenario of continually receiving new data for learning.\nWe partition the data into different tasks $D = \\{D_\\tau\\}_{\u03c4=1}^{T}$,\nwhere $D_\\tau = \\{X_{\\tau,i}, Y_{\\tau,i}\\}_{i=1}^{N_\\tau}$ represents the input of $N_\\tau$ data\n$X_{\\tau,i} \\in \\mathbb{R}^D$ at stage \u03c4, with the corresponding label $Y_{\\tau,i} \\in C_\\tau$,\nwhere $C_\\tau$ is the class set in task \u03c4, and different stages\nsatisfy $C_{\\tau_1} \\cap C_{\\tau_2} = \\emptyset$. We decouple the entire model into\nan encoder and a classifier: $f(X_{\\tau,i}) = W^T\\phi(x_{\\tau,i})$, where\n$\\phi(\u00b7) : \\mathbb{R}^D \\rightarrow \\mathbb{R}^d$, $W \\in \\mathbb{R}^{d \\times |y_\u03c4|}$."}, {"title": "B. Distribution-level Memory Recall", "content": "Revisiting Prototype-Based CIL Methods: Transitioning\nfrom (a) to (b), Prototype-based methods, which do not\ninvolve privacy concerns and can preserve old knowledge sim-\nilar to rehearsal-based methods, are widely used in continual\nlearning and few-shot continual learning. For example, PASS\n[35] obtains pseudo features in the class-incremental phase by\nusing class centers plus the mean squared deviation multiplied\nby Gaussian noise. In FeTrIL [37], all features of new classes\nare shifted to the class centers of similar old classes to\nobtain pseudo features of old samples. However, these methods\nfail to consider that the samples of new classes exhibit an\nanisotropic distribution due to fixed encoder parameters, and\nthere are inherent differences in distributions between new and\nold classes. This can lead to confusion in the classification\nboundary when shifting, resulting in incomplete preservation\nof old knowledge. Fig. 2 is a schematic diagram related to Fig.\n1, where (a) represents the actual distribution of feature space\nfor two classes, and (b) illustrates the method of enhancing\nclass centers with Gaussian noise to generate pseudo features.\nIt can be seen that this method fails to effectively reproduce the\nactual situation in (a), leading to confusion in the classification\nboundary between classes, indicating incomplete preservation\nof knowledge from the old classes.\nHow Does Our Method Progress from (a) to (c)? To\naddress the above issues, we first aim to preserve complete\nold knowledge, as shown in subfigure (c), to maintain more\naccurate old knowledge and thus maintain approximately true\ninter-class relationships when generating pseudo features in the\nincremental phase. We propose the Distribution-level Memory\nRecall method to effectively preserve the feature space distri-\nbution of old knowledge and prevent inter-class confusion.\nWe fit all features in the incremental phase of task \u03c4 using\nGMM. Assuming that the feature distribution in incremental\nlearning can be modeled and fitted by a mixture model\nconsisting of K Gaussian distributions, and there is a dataset\ncontaining n samples $\\{x_1,x_2,...,x_n\\}$, where each sample $x_i$\nis a d-dimensional vector (i.e., $X_i = (X_{i1}, X_{i2}, ..., X_{id})$). Let's\nassume that the probability density function (PDF) of the\nGaussian distribution corresponding to the k-th (0 < k \u2264 K)\nmixture component is $N(\\mu_k, \\Sigma_k)$, where $\\mu_k$ is a d-dimensional\nmean vector, and $\\Sigma_k$ is a $d \\times d$ covariance matrix. Then, the\nPDF of the k-th mixture component can be represented as:\n$p(x|\\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma_k|^{1/2}} exp \\bigg(-\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k)\\bigg),$ (1)"}, {"title": "", "content": "the PDF of the entire GMM can be expressed as a linear\ncombination of the PDFs of each mixture component:\n$p(x) = \\sum_{k=1}^K \\pi_k p(x|\\mu_k, \\Sigma_k),$ (2)\nwhere $\u03c0_k$ is the weight of the k-th mixture component, satis-\nfying $\\sum_{k=1}^K \u03c0_k = 1$. Our goal is to estimate the parameters of\nthe GMM by maximizing the joint PDF of all samples in the\ndataset, i.e., maximizing the likelihood function:\n$L(\\Theta) = \\prod_{i=1}^n p(x_i|\\Theta),$ (3)\nwhere \u0398 represents all parameters of the GMM, including the\nmeans $\u03bc_k$, covariance matrices $\u03a3_k$, and weights $\u03c0_k$ of each\nmixture component.\nIn practice, to avoid numerical issues, the logarithm of the\nlikelihood function is often taken, and the negative is mini-\nmized, transforming the problem of maximizing the likelihood\nfunction into minimizing the negative log-likelihood function\n$\\text{argmin}\u00a0_{\\Theta} (-\\log L(\\Theta))$:\n$\\text{argmin}\u00a0_{\\Theta} \\bigg(-\\sum_{i=1}^n \\log \\Big(\\sum_{k=1}^K \\pi_k p(x_i|\\mu_k, \\Sigma_k)\\Big)\\bigg)$ (4)\nFor the minimization of the negative log-likelihood function,\nthe EM algorithm (Expectation-Maximization algorithm) is\ncommonly used to estimate the parameters of the GMM.\nThe EM algorithm iteratively alternates between two steps:\nthe E-step (Expectation step) and the M-step (Maximization\nstep), until convergence to a local optimum. In the E-step,\nthe probability that each sample belongs to each mixture\ncomponent is estimated using the current parameters, and in\nthe M-step, the model parameters are updated by maximizing\na lower bound of the current likelihood function.\nWe utilize GMM to model and fit the distributions of these\nvectors, obtaining a Gaussian Mixture Model that describes\nthese distributions. We save the mean, covariance matrix,\nand weight coefficient of each Gaussian component. In the\nnext incremental phase, we generate samples that conform to\nthe mixed Gaussian distribution, thus preserving knowledge.\nHowever, considering the issue of the number of feature sub-\nspaces, i.e., not every class necessarily requires K Gaussian\ncomponents for fitting, we achieve adaptive selection of the\nnumber of Gaussians during the GMM model fitting process.\nThat is, to obtain the optimal number of feature subspaces\nfor the feature distribution, we first calculate the closeness of\nsample i to other samples in the same class:\n$a(i) = \\frac{1}{|C_i|-1}\\sum_{j \\in C_i, j\\neq i} d(i, j)^2,$ (5)\nwhere $C_i$ denotes the set of samples in the same class as\nsample i, $|C_i|$ is the number of samples in set $C_i$, and $d(i, j)$\nrepresents the Euclidean distance between sample i and sample\nj. We then calculate the separateness of sample i from all\nsamples in every other different class:\n$b(i) = \\min_{C_k,k\\neq i}\u00a0\\frac{1}{|C_k|} \\sum_{j \\in C_k} d(i, j)^2.$ (6)\nWe obtain the silhouette coefficient of sample i:\n$s(i) = \\frac{b(i) - a(i)}{\\text{max}\\{a(i), b(i)\\}\\}\u02d9$ (7)\nFinally, by averaging the silhouette coefficients of all sam-\nples, and comparing the average silhouette coefficients under\ndifferent numbers of Gaussians, we can determine the optimal\nvalue of K. Additionally, we need to consider that if there are\nno longer multiple feature subspaces within a class, meaning\nthe true features may only need to be modeled and fitted with\na multivariate Gaussian distribution, we set a threshold for\nthe silhouette coefficient. Only when it exceeds this threshold\ndo we believe that K Gaussian components are needed;\notherwise, only one Gaussian component is used.\nFurthermore, if using a multivariate Gaussian distribution,\nwe need to store the covariance matrix. For example, using\nResNet18 [47] as the backbone network, we need to store a\n512-dimensional class center vector and a 512\u00d7512 covariance\nmatrix. This imposes a higher storage burden compared to\nprototype-based methods. To address this, we perform twice\ndegradation of the covariance matrix, continuously aligning it\nfor dimensionality reduction, while preserving the centers and\nweights information obtained from the GMM (as there is no\nstorage burden). We then analyze this process, resulting in a\nmore comprehensive solution. Details will be provided in the\nexperimental section."}, {"title": "", "content": "Algorithm 1 DMR Modeling and Fitting in Continual Learn-\ning\nprocedure FITTING FEATURE DISTRIBUTION IN\nCURRENT-TASK\nInput: Features set $\\{X_1, X_2, ..., X_n\\}_{c_i}$ of class $c_i$, task_id\nfor $c_i$ in task t do\nSilhouette Score Algorithm($\\{X_1, X_2, ..., X_n\\}_{c_i}$)\n$K^* = adaptive\\_K(silhouette\u00a0scores)$\n$\\{\\mu_{c_i}, \\Sigma_{c_i}, \\pi_{ic_i}\\}_{i=1}^{K^*}=$\nGMM Fitting($\\{x_1, x_2, ..., x_n\\}_{c_i}$, $K^*$\nStore parameters: $\\{\\mu_{c_i}, \\Sigma_{c_i}, \\pi_{ic_i}\\}_{i=1}^{K^*}$\nreturn Stored parameters\nprocedure GENERATE PSEUDO FEATURES FOR OLD-\nTASK\nInput: Store parameters:$\\left\\{\\mu_{c_i}, \\Sigma_{c_i}, \\pi_{i c_i}\\right\\}_{i=1}^{K^*}$\nfor task 0, 1,..., \u03c4 \u2212 1 do\nfor $c_i$ in task do\nGMM Generation Algorithm ($\\{\\mu_{c_i}, \\Sigma_{c_i}, \\pi_{i c_i}\\}_{i=1}^{K^*}$)\nreturn Generate pseudo Features"}, {"title": "C. Avoiding Old-New Confusion", "content": "Through the method in III-B, we can effectively retain\nold knowledge, aiming to minimize confusion between old\nclasses. However, as shown in Fig. 3, we should also enhance\ndiscriminability between new and old tasks. The representation\nof old knowledge by pseudo features is fixed in the embedding\nspace. When training with new samples, confusion may arise,\nleading to classifier drift. Therefore, we aim to make the"}, {"title": "", "content": "classifier more discriminative towards new and old knowledge.\nTo achieve this, we enhance the prototype with features from\nnew classes outside the old knowledge domain.\nIn task t + 1, the representation $e_n^{c_1}$ of the n-th sample\nfrom class $c_1$ in the current task is obtained. We then generate\nthe pseudo features $e_2^{c_2}$ for a specific old class $c_2$ based on\nIII-B. Subsequently, in the incremental feature space, we blend\nthe obtained pseudo-prototype $e_2^{c_2}$ of the old class with the\nfeature $e_n^{c_1}$ of the new class in a linear manner, akin to the\nMixup approach [45]. The training label is the hard label of\nthe new class, which is utilized to distinguish between new\nand old classes.\n$\\phi_m = \\lambda \\cdot e_n^{c_1} + (1 - \\lambda) \\cdot e_2^{c_2},$ (8)\nwhere $\u03bb \u2208 [0, 1]$ follows a Beta distribution. In Fig. 4, the three\ntypes of data, $e_n^{c_1}$, $e_2^{c_2}$, and $\u03c6_m$, representing the new class, the\nold class, and the mixture of new and old classes, respectively,\nare input to the linear classifier. They are supervised by three\ncorresponding loss functions, $L_{cls}$, $L_p$, and $L_m$, all computed\nusing cross-entropy. In the feature space, $\u03c6_m$, enhanced by\nnew knowledge prototypes, lies between the features of the\nnew and old classes. This approach mitigates overfitting of the\nclassifier to the prototypes and enhances its discriminability\nbetween new and old classes. In summary, the loss function\nis composed as:\n$L = \\xi (L_{cls} + L_p) + (1 - \\xi) L_m,$ (9)\nwhere $\u03be \u2208 [0, 1]$ is a hyperparameter.\nAdditionally, we categorize classification errors into two\ntypes: confusion within tasks and confusion between new and\nold tasks. Let N represent the total number of samples in the"}, {"title": "", "content": "new task, O represent the total number of samples in the old\ntask, $M_{new}$ represent the number of samples misclassified from\nthe old task to the new task, and $M_{old}$ represent the number of\nsamples misclassified from the new task to the old task. The\nconfusion index between the tasks can be calculated as:\n$C\\_I = \\frac{M_{new}}{N} + \\frac{M_{old}}{O}.$ (10)\nWhen accumulating the confusion across different incremental\nstages, you can sum the confusion values of each stage.\nAssuming there are K incremental stages (tasks), with each\nstage having a confusion index of C_I, the total confusion\ncan be expressed as:\n$C\\_{Itotal} = \\sum_{\u03c4=1}^K C\\_I_\u03c4.$ (11)"}, {"title": "D. Generalization Is Prerequisite.", "content": "In continual learning or Multimodal Continual Learning, it\nis essential to ensure that the network performs well in each\nstage, which requires adaptability, the ability to adapt to new\ntasks. In existing research, it has been found that the balance of\nmultimodal is one of the key factors affecting the performance\nof multimodal training. We also can see in Fig. 12, the\nfeature response difference between different modalities is\nvery large, as a result, the distribution of old knowledge\ncannot be easily reproduced. Therefore, maintaining the bal-\nance between modalities is one of the sufficient conditions\nfor multimodal continual learning. In multimodal continual\nlearning, the $D_\u03c4 = \\{X_v^{\u03c4,i}, X_s^{\u03c4,i}, Y_\u03c4,i\\}_{i=1}^{N_\u03c4}$ represents the input\nof $N_\u03c4$ paired visual modality data and sensor modality data\n$X_v^{\u03c4,i}, X_s^{\u03c4,i} \\in \\mathbb{R}^D$ at stage \u03c4, with the corresponding label\n$Y_{\u03c4i} \u2208 C_\u03c4$. We decouple the entire model into an encoder,\nan Intra-modal mining module, and a classifier:\n$f(x^{\u03c4,i}_v, x^{\u03c4,i}_s) = W^T M(\\phi_v(x^{\u03c4,i}_v), \\phi_s(x^{\u03c4,i}_s)),$ (12)\nwhere $\\phi_*(\u00b7) : \\mathbb{R}^D \\rightarrow \\mathbb{R}^{H\u00d7W\u00d7d}$, $M(\u00b7) : \\mathbb{R}^{H\u00d7W\u00d7d} \\rightarrow \\mathbb{R}^{d}$, $W \u2208 \\mathbb{R}^{d \u00d7 |y_\u03c4|}$, and the encoder consists of two parallel\nsingle-modality encoders $\\phi_v(\u00b7)$ and $\\phi_s(\u00b7)$, outputting feature\nmaps. Firstly, as shown in Fig. 5 inspired by transfer learning\nmethods, since there is an imbalance between dominant and\nweak modalities, we transfer effective prior information ob-\ntained from dominant modalities to weak modalities, thereby\nalleviating the insufficient information in weak modalities\ncaused by under-optimization. The input of the visual modality\n$x_v^{\u03c4,i} = \\{x_k\\}_{k=1}^{K}$ contains k frames extracted at intervals from the\nsame video arranged in chronological order, which is aligned\nin time with the sensor signal. The images containing time\ninformation are inputted into the Evaluation function $E(\u00b7)$.\nInspired by [48], we directly use pretrained models as part\nof $E(\u00b7)$. Through $E(\u00b7)$, we obtain the weight information of\nthe visual modality on the timeline and weight this information\nonto the temporal dimension of the spectrogram obtained from\nthe sensor signal as the input of the sensor modality network:\n$\\text{IS}_{\u03c4,i} = w_v^{\u03c4} \\cdot x_v^{\u03c4,i} = E(x_v^{\u03c4,i}) \\cdot X_s^{\u03c4,i} \\\\\u00a0\\qquad w_v^{\u03c4} = \\frac{e^{\\phi_v(x^{\u03c4,i}_v)}}{\\sum_{j=1}^{T}e^{\\phi_v(x^{\u03c4,i}_v)}}.$ (13)\n$\\text{IS}_{\u03c4,i}$"}, {"title": "", "content": "where $w_v^{\u03c4}$ denotes the information extracted from the visual\nmodality over time, $p(\u00b7)$ denotes pooling over the width and\nheight of the feature map output by $\\phi_v(\u00b7)$. After obtaining\n$I_v^{\u03c4,i}$ and the time-guided $I_s^{\u03c4,i}$, we then perform intra-modal\ninformation mining. From this perspective, inspired by [49],\nas shown in Fig. 6, we explore and enhance information in\nthe sensor network input $I_s$ in both the time and frequency\ndomains. The feature map of the sensor modality contains\nthree dimensions: channel, time, and frequency. Same as\nour previous method [18], in Time-Frequency attention, we\nperform pooling in both the time and frequency dimensions,\nthen feed them into the small net shown in the Fig. 5. Finally,\nwe add them back to the original input feature map in the\ntime and channel dimensions, forming the sensor modality's\ninformation miner. The information that the visual modality\nshould focus on comes from spatial aspects, so we refer to\n49]. After obtaining the enhanced feature map, we simply\nconcatenate them and input them into the channel attetion for\nfusion."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Datasets", "content": "We utilized two commonly used datasets in continual learn-\ning, CIFAR100 and ImageNet100. Additionally, considering\nthe challenges of modal imbalance in our multimodal appli-\ncations, which affect the generalization capability of multi-\nmodal models, we extended our study to the UESTC-MMEA-\nCL dataset for multimodal behavior recognition. When the\ngeneralization ability of multimodal models is ensured, we\nare willing to extend our continual learning methods through\nexperimental validation.\nCIFAR100 [14]is a dataset widely used for class-\nincremental learning. It contains 100 classes, each with 600\nRGB images. For each class, 500 images are used for training\nand 100 for testing.\nImageNet100 [16] is a subset of ImageNet-1K [15], con-\nsisting of 100 classes randomly selected after shuffling the\noriginal 1000 classes. Each class in ImageNet100 contains an\naverage of 1300 training samples and 50 test samples. The\nImageNet dataset is widely recognized in the field of computer\nvision and is extensively used in continual learning research.\nUESTC-MMEA-CL [17] is a multimodal activity recog-\nnition dataset designed for continual learning. It contains 32\ndaily behavior classes, with approximately 200 samples per\nclass. The training and testing ratio is 7:3, and each sample\nconsists of temporally aligned multimodal segments, including\nvideo segments, accelerometer signal segments, and gyroscope\nsignal segments. Same with He et al. [43], we sample 8\nRGB images from each video segment and obtain optical\nflow data generated from the videos, and also perform short-\ntime Fourier transform on the sensor signals to obtain time-\nfrequency spectrograms, which are then input into the model."}, {"title": "B. Implementation Details", "content": "We need to compare our work with different works on\ndifferent datasets. Since the tasks on each dataset are not\nexactly the same, the model structures and task settings for\neach dataset are not exactly the same. However, we can ensure\na fair comparison to highlight the effectiveness of our work.\nTask Settings: In the CIFAR100 dataset, consistent with the\nprototype-based continual learning method setting, we divide\nthe 100 classes into T = 5, T = 10, T = 20, and T = 60,\nspecifically set as 50 + 10 \u00d7 5, 50 + 5 \u00d7 10, and 40 + 3 \u00d7 20\n(50+10 \u00d7 5 means the training process is divided into one\nbase phase and 5 incremental phases, with 50 classes in the\nbase phase and 10 classes in each incremental phase). On the\nImageNet100 dataset, it is also divided into T = 5, T = 10\nand T = 20, the detailed settings are also consistent with\nthose used on CIFAR100. In the UESTC-MMEA-CL dataset,\nwe shuffle the 32 classes with a fixed random seed, consistent\nwith [17]. We divide them into three cases: T = 16, T = 8,\nand T = 4, specifically set as 2 + 2 \u00d7 15, 4 + 4 \u00d7 7, and\n8+8 x 3.\nArchitecture and train details: On the CIFAR100 and\nImageNet100 dataset, we use ResNet18 for training, with pa-\nrameters set the same as in FeTrIL [37]. In the base phase, the\nlearning rate is 0.1 for 200 iterations, and in the incremental\nphase, it is 0.05 for 60 iterations. All the models are built\nbased on pytorch framework, and the method implementation\nand experiment are mainly implemented based on PyCIL [50].\nIn the research on the UESTC-MMEA-CL dataset, the encoder\nis BN-Inception pre-trained on KICHENS-100, and the entire\nbackbone network structure is consistent with AID [18]. The\ntraining details are also consistent with it, training 20 epochs"}, {"title": "C. Analysis on DMR and DMR-L Method", "content": "In III-B", "with-\nGMM": "determining the number of Gaussians for modeling\nand the size of the covariance matrix. We also proposed\ncorresponding solutions. As shown in the Fig. 7, we conducted\nextensive experiments on different numbers of Gaussians and\nthe degradation of information in the covariance matrix.\nFirstly, by observing the Fig. 7 horizontally, we noticed that\nas K increases from 1, the average accuracy generally rises.\nHowever, the significant improvement occurs mainly between\n1 and 2, with little improvement afterward and even some\nfluctuations, indicating a saturation phenomenon"}]}