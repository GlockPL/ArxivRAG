{"title": "Distribution-Level Memory Recall for Continual Learning: Preserving Knowledge and Avoiding Confusion", "authors": ["Shaoxu Cheng", "Kanglei Geng", "Chiyuan He", "Zihuan Qiu", "Linfeng Xu", "Heqian Qiu", "Lanxiao Wang", "Qingbo Wu", "Fanman Meng", "Hongliang Li"], "abstract": "Continual Learning (CL) aims to enable Deep Neural Networks (DNNs) to learn new data without forgetting previously learned knowledge. The key to achieving this goal is to avoid confusion at the feature level, i.e., avoiding confusion within old tasks and between new and old tasks. Previous prototype-based CL methods generate pseudo features for old knowledge replay by adding Gaussian noise to the centroids of old classes. However, the distribution in the feature space exhibits anisotropy during the incremental process, which prevents the pseudo features from faithfully reproducing the distribution of old knowledge in the feature space, leading to confusion in classification boundaries within old tasks. To address this issue, we propose the Distribution-Level Memory Recall (DMR) method, which uses a Gaussian mixture model to precisely fit the feature distribution of old knowledge at the distribution level and generate pseudo features in the next stage. Furthermore, resistance to confusion at the distribution level is also crucial for multimodal learning, as the problem of multimodal imbalance results in significant differences in feature responses between different modalities, exacerbating confusion within old tasks in prototype-based CL methods. Therefore, we mitigate the multi-modal imbalance problem by using the Inter-modal Guidance and Intra-modal Mining (IGIM) method to guide weaker modalities with prior information from dominant modalities and further explore useful information within modalities. For the second key, We propose the Confusion Index to quantitatively describe a model's ability to distinguish between new and old tasks, and we use the Incremental Mixup Feature Enhancement (IMFE) method to enhance pseudo features with new sample features, alleviating classification confusion between new and old knowledge. We conduct extensive experiments on the CIFAR100, ImageNet100, and UESTC-MMEA-CL datasets, and achieve state-of-the-art results.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) have excelled in learning complex patterns and representations from data, leading to numerous breakthroughs in industry [1], [2], [3], [4], [5]. However, complex data emerges continuously in the form of data streams. In this scenario, deep neural networks (DNNs) have been found to struggle with processing continuous data. One of the challenges in training DNNs is the phenomenon where they tend to forget previously learned knowledge when new data is introduced, known as \u201ccatastrophic forgetting\". Various methods have been proposed to address this issue from different perspectives. One approach involves storing a small portion of old samples while continuously inputting new samples into the network. Another method involves using knowledge distillation to constrain the student model to retain old knowledge at the input-output level. Regularizing network parameters, continually expanding and adjusting network structures, and other methods focus on preserving old knowledge by manipulating model parameters or structures. Prototype-based methods also exist, which maintain old knowledge by using prototypes of old categories. The exemplar-free prototype-based method is favored for its privacy-preserving character-istics. However, it lacks a thorough consideration from the perspective of preserving classification boundaries of old tasks. Prior prototype-based methods generate pseudo features in the incremental task phase that cannot accurately reproduce old knowledge, leading to confusion in classification boundaries within old tasks. Therefore, for prototype-based methods, instead of combining them with more complex network structures, regularization strategies, or using networks from both old and new stages for knowledge distillation, it is more effective to seek solutions from the distribution of old knowledge itself. We propose a method for preserving and reproducing knowledge at the distribution level in the representation space. At each stage, we use GMM to fit the distribution of high-dimensional features, and improve it by adaptively determining the number of Gaussian components. Furthermore, we apply twice information degradation to preserve the distribution. In the next task stage, we generate pseudo features. The resulting pseudo features closely approximate the true distribution of old samples, maintaining the classification boundaries between classes within the old task and maximizing the preservation of \u201ctrue memories\u201d. Furthermore, Artificial intelligence has made significant breakthroughs in the industrial sector, sparking a wave of applications [6], [7], [8], [9]. We continuously study its application in real-world scenarios of multimodal behavior recognition. Building on Peng et al.'s work [10], we delve into the issue of imbalance in multimodal networks, which can lead to suboptimal performance. We observe that the imbalance between modalities results in significant differences in the response of different modal features 12, rendering previous prototype-based methods ineffective for reproducing old knowledge. Therefore, we propose a dual information enhancement approach within and between modalities to address the imbalance caused by the insufficient information in weaker modalities. When our method can maintain the feature distribution and classification boundaries of the old task, and the imbalance in the multimodal distribution can be alleviated, we observe through the Confusion Index that the confusion between the new and old tasks, leading to classification errors, cannot be ignored. Therefore, we use features of the new task's samples to perform data augmentation on pseudo features of the old task's knowledge to alleviate the confusion between the new and old tasks.\nIn summary, to address the aforementioned challenges, we categorize the paradigm of human intuition overcoming forgetfulness into three aspects, which serve as the overall framework. Firstly, it is essential to retain true old knowledge to ensure that the replayed old knowledge aligns as closely as possible with reality. Secondly, there must be a learning ability, which is also a prerequisite for maintaining valuable memory. In the case of multimodal scenario applications, the imbalance of multimodalities may fail to meet this prerequisite. Finally, there should be no confusion between new and old knowledge. We develop strategies to alleviate the forgetting problem based on this paradigm:\n1) Memories must not \"deform\": For the incremental-frozen [12] method, the distribution of the feature space exhibits anisotropy [13], making it extremely difficult for the original prototype-based method to maintain knowledge from previous tasks solely by relying on simple data from class centers. As shown in Fig. 1, subfigure (a) shows the distribution of features obtained from real samples from four classes in the embedding space, (b) simulates the pseudo features generated in the feature space based on class centers and mean squared deviations alone, and (c) shows the simulated results of (a) using our method. It can be observed that generating data solely based on class centers cannot restore the feature space distribution of the old task, loses the knowledge learned within classes, and confuses the decision boundary. Therefore, we propose the Distribution-level Memory Recall (DMR) method to fit the real feature space distribution, better preserving the knowledge learned within classes in the old task, and better restoring the decision boundaries between classes in the old task.\n2) Overcoming confusion: In the context of feature space, the memorization of old classes is already established. Therefore, distinguishing between old and new knowledge is crucial when acquiring new knowledge, as otherwise, there is a risk of confusion between the two. To address this issue, we propose the Incremental Mixup Feature Enhancement (IMFE) method. This method utilizes the features of new classes to enhance pseudo features, thereby improving the classifier's ability to distinguish between old and new knowledge.\n3) Enhancing learning capabilities: We propose the Inter-Modal Guidance and Intra-Modal Mining (IGIM) method, which enhances information by considering both inter- and intra-modal aspects. Our approach first categorizes modalities into dominant and weak based on their performance, we select prior effective knowledge to guide the learning of weak modal-ities. Additionally, we deeply mine useful information within each modality. These dual approaches reduce the gap between dominant and weak modalities, leading to more generalizable features.\nOur contributions can be summarized as follows:\n\u2022 DMR method we proposed is the first to fit the distri-bution of representations in the feature space from the perspective of maintaining the old knowledge distribution and thereby maintaining the classification boundaries within the old task. We also propose an optimization plan for the fitted storage burden, which can ultimately maintain realistic classification boundaries with almost no increase in burden.\n\u2022 By transferring prior information from dominant modali-ties and mining the time-frequency information of sensor modalities, we alleviate the imbalance between modalities and obtain generalizable features, meeting the prerequi-sites of multimodal continual learning.\n\u2022 Observation is conducted through a customized confusion index, and the classifier's ability to distinguish between new and old knowledge is enhanced using the IMFE method, reducing the confusion between new and old classes.\n\u2022 We conducted extensive experiments on the CIFAR100 [14], ImageNet100 [15], [16], and UESTC-MMEA-CL [17] datasets, achieving SOTA results, and performed detailed ablation studies and analysis.\nThis paper is an extended version of our previous work reported at [18], with the following major improvements compared to the previous version: (1) From the perspective of knowledge distribution, we employ GMM to protect knowl-edge at the distribution level and use an adaptive algorithm to calculate the number of feature subspaces. We also use dual information degradation to avoid storage overhead. We evaluate and analyze the similarity between pseudo-features and the original real distribution. (2) By using the Con-fusion Index between new and old categories, we analyze the confusion between new and old categories, providing a basis for the methods in the original work. (3) We extend the original work to the theory of continual learning and propose a paradigm for continual learning, demonstrating its effectiveness on more general datasets. (4) In multi-modal applications, we address the issue of multi-modal imbalance by guiding weak modalities with prior knowledge to achieve deeper solutions."}, {"title": "II. RELATED WORK", "content": "In the realm of continual learning, the goal is to enable artificial neural networks to continuously \u201cevolve\" in their learning, acquiring new knowledge without forgetting. To mitigate forgetting, several approaches have been proposed: First is the Dynamic Network Approaches, they extend the network backbone or some network layers, enabling the extended parts to accommodate new knowledge while the orig-inal parts maintain old knowledge. DEN [19] trains neurons relevant to incremental tasks purposefully, mitigating the drift of individually optimized neurons. DER [20] also expands the backbone continuously during incremental learning to retain old knowledge, but with a broader linear classifier aggregat-ing features from multiple backbones. Inspired by gradient boosting algorithms, FOSTER [21] dynamically adjusts the network and employs knowledge distillation to retain old knowledge. However, these methods may face issues with excessive memory consumption due to the continual expansion of the backbone network. MEMO [22] observes that only deeper network layers exhibit specificity to different incre-mental tasks, thus saving storage by expanding only specified network layers.\nParameter regularization methods impose regularization on network parameters to prevent parameter drift, EWC [23] evaluates the importance of parameters for old knowledge using the Fisher information matrix, protecting parameters deemed more crucial for old knowledge, thus safeguarding old knowledge from a parameter perspective. SI [24] defines the importance of parameters for old knowledge based on the magnitude of their influence on the loss function, proposing an online method for importance assessment. IADM [25] assesses the importance of parameters in different network layers, contrary to EWC [23], which regularizes global parameters.\nKnowledge distillation is a mainstream method in continual learning. These methods add regularization distillation loss functions to the inputs and outputs. LwF [26] was the first to apply distillation strategies in continual learning. It maintains old knowledge by using the frozen old model as the teacher model to guide the student model in the incremental phase at the input and output levels. The iCaRL [16] builds upon LwF [26] by adding an exemplar set for replay to further protect old knowledge. COIL [27] employs bidirectional distillation to leverage semantic relationships in both new and old models. In addition to logits distillation, UCIR [28] employs a more potent feature distillation, stipulating that the encoder's outputs should be similar. It discovers weight drift in the classifier and addresses this issue by using a cosine classifier. Building upon this, WA [29] continuously normalizes weights and introduces weight clipping to prevent drift. ISM-Net [30] combines distillation with incremental semantic mining between taskes, thereby continuously retaining old knowledge while expanding the network to learn new knowledge. PODNet [31] pools and reduces the differences in feature maps from different network layers before distillation. IL2A [32] evaluates the feature values obtained from the spectral decomposition of features of new and old classes, finding that maintaining relatively large feature values can mitigate forgetting, further alleviated by distillation.\nThese methods can be classified into two classes: replay-based and exemplar-free methods. Strategies involving replay exemplars are often combined with other approaches, such as the FOSTER [21] combining with dynamic networks and the LwM [33] method combining with parameter regulariza-tion, especially in conjunction with distillation. Because these methods can directly access old data during the incremental process, they generally outperform methods that do not use replay strategies.\nPrototype-based methods primarily operate in the embed-ding space, without direct access to the raw data of old classes. However, when IL2M [34] first introduced methods that store prototypes and simultaneously use sample replay strategies, subsequent prototype-based methods mostly avoided using replay strategies. The PASS [35] first obtains a generalized encoder and features through self-supervised methods in the initial stage. It then stores the class centers and mean squared deviations of old classes, and enhances the prototypes with Gaussian noise in the incremental stage to preserve old knowl-edge. Subsequent SSRE [36] method selects samples based on the similarity between prototypes and new samples, and then distills to enhance the Discriminability between new and old classes. Additionally, it extends and reorganizes network layers to maintain old features. The FeTrIL [37] enhances old prototypes by computing the similarity between new and old class prototypes and shifting all features of new samples to the positions of old class prototypes, thereby maintaining knowl-edge of old classes. Prototype-based methods can maintain knowledge through prototypes of old classes at the feature level."}, {"title": "B. Multimodal Continual Learning", "content": "In addition to the problem of class imbalance, the work on multimodal continual learning has also attracted considerable attention, especially in the field of vision and language. Srinivasan et al. [38] proposed the CLIMB framework, which modifies the vision-language transformer to implement sev-eral continual learning methods. They provide a benchmark for continual learning in vision-language tasks, aiming to promote research on novel continual learning algorithms in this challenging multimodal environment. Zhu et al. [39] contributed a unified multimodal dataset for continual learning. They also introduced the Compatible Momentum Contrast method for Topology Preservation, which updates the features of modalities by absorbing knowledge from old and new tasks separately.\nThe popular vision-language pretraining model CLIP has also garnered attention. Wang et al. [40] proposed a contin-ual learning model based on the CLIP model, which uses probabilities for fine-tuning to alleviate forgetting issues. It demonstrates outstanding performance in detecting new data and example selection. The S-prompt [41] method explores domain-incremental learning using the CLIP model but does not address class incremental learning.\nWith the development of sensors and other devices, in the field of multimodal continual activity recognition, HarMl [42] considered the significant storage consumption in training multimodal models and proposed corresponding optimizations. They used an attention mechanism to handle data from differ-ent sensors and also employed elastic weight consolidation and exemplar relevance analysis to overcome catastrophic for-getting. Xu et al. [17] found that multimodal data exacerbates the problem of forgetting in continual learning, and different modal fusion methods have different effects on the forgetting problem. Modal fusion makes features contain more useful information, enhancing the expressive power of each modality. He et al. [43] found that by masking the representation of dominant modalities in multimodal data and prolonging the optimization time of weaker modalities, the forgetting problem in the incremental process can be effectively mitigated. Wang et al. [44] generated confusion samples through mixup [45] to encourage the fusion network to learn more discriminative features to alleviate feature confusion between different tasks. AID [18] used a time-frequency attention mechanism for sensor signals, thus alleviating unbalance, and used a mix of new and old classes of features and pseudo features to reduce confusion between different tasks. In the AV-CIL [46], benchmarks were established on three multimodal datasets using pretrained models. They maintained semantic similarity between modalities by constraining the similarity between two modalities and used distillation methods to alleviate forgetting problems."}, {"title": "III. METHODOLOGY", "content": ""}, {"title": "A. Problem Statement", "content": "Continual learning is a learning paradigm that requires data to be continuously inputted as a series of tasks, mimicking the scenario of continually receiving new data for learning. We partition the data into different tasks $\\mathcal{D} = {\\mathcal{D}_\\tau}_{\\tau=1}^T$, where $\\mathcal{D}_\\tau = {\\mathcal{X}\\_{\\tau,i}, Y\\_{\\tau,i}}_{i=1}^{N_\\tau}$ represents the input of $N_\\tau$ data $X_{\\tau,i} \\in \\mathbb{R}^D$ at stage $\\tau$, with the corresponding label $Y_{\\tau,i} \\in C_\\tau$, where $C_\\tau$ is the class set in task $\\tau$, and different stages satisfy $C_{\\tau_1} \\cap C_{\\tau_2} = \\emptyset$. We decouple the entire model into an encoder and a classifier: $f(X_{\\tau,i}) = W^T\\phi(x_{\\tau,i})$, where $\\phi(\\cdot) : \\mathbb{R}^D \\rightarrow \\mathbb{R}^d$, $W \\in \\mathbb{R}^{d \\times |y_\\tau|}$."}, {"title": "B. Distribution-level Memory Recall", "content": "Revisiting Prototype-Based CIL Methods: Transitioning from (a) to (b), Prototype-based methods, which do not involve privacy concerns and can preserve old knowledge sim-ilar to rehearsal-based methods, are widely used in continual learning and few-shot continual learning. For example, PASS [35] obtains pseudo features in the class-incremental phase by using class centers plus the mean squared deviation multiplied by Gaussian noise. In FeTrIL [37], all features of new classes are shifted to the class centers of similar old classes to obtain pseudo features of old samples. However, these methods fail to consider that the samples of new classes exhibit an anisotropic distribution due to fixed encoder parameters, and there are inherent differences in distributions between new and old classes. This can lead to confusion in the classification boundary when shifting, resulting in incomplete preservation of old knowledge. Fig. 2 is a schematic diagram related to Fig. 1, where (a) represents the actual distribution of feature space for two classes, and (b) illustrates the method of enhancing class centers with Gaussian noise to generate pseudo features. It can be seen that this method fails to effectively reproduce the actual situation in (a), leading to confusion in the classification boundary between classes, indicating incomplete preservation of knowledge from the old classes.\nHow Does Our Method Progress from (a) to (c)? To address the above issues, we first aim to preserve complete old knowledge, as shown in subfigure (c), to maintain more accurate old knowledge and thus maintain approximately true inter-class relationships when generating pseudo features in the incremental phase. We propose the Distribution-level Memory Recall method to effectively preserve the feature space distri-bution of old knowledge and prevent inter-class confusion.\nWe fit all features in the incremental phase of task $\\tau$ using GMM. Assuming that the feature distribution in incremental learning can be modeled and fitted by a mixture model consisting of $K$ Gaussian distributions, and there is a dataset containing n samples {$x_1,x_2,...,x_n$}, where each sample $x_i$ is a d-dimensional vector (i.e., $X_i = (X_{i1}, X_{i2}, ..., X_{id})$). Let's assume that the probability density function (PDF) of the Gaussian distribution corresponding to the k-th (0 < k \u2264 K) mixture component is $N(\\mu\\_k, \\Sigma\\_k)$, where $\\mu\\_k$ is a d-dimensional mean vector, and $\\Sigma\\_k$ is a dxd covariance matrix. Then, the PDF of the k-th mixture component can be represented as:\n$p(x | \\mu\\_k, \\Sigma\\_k) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma\\_k|^{1/2}} exp \\left( -\\frac{1}{2} (x-\\mu\\_k)^T \\Sigma\\_k^{-1} (x-\\mu\\_k) \\right),$ (1)\nthe PDF of the entire GMM can be expressed as a linear combination of the PDFs of each mixture component:\n$p(x) = \\sum_{k=1}^K \\pi_k p(x | \\mu_k, \\Sigma_k),$ (2)\nwhere $\\pi_k$ is the weight of the k-th mixture component, satis-fying $\\sum_{k=1}^K \\pi_k = 1$. Our goal is to estimate the parameters of the GMM by maximizing the joint PDF of all samples in the dataset, i.e., maximizing the likelihood function:\n$L(\\Theta) = \\prod_{i=1}^n p(x_i | \\Theta),$ (3)\nwhere $\\Theta$ represents all parameters of the GMM, including the means $\\mu_k$, covariance matrices $\\Sigma_k$, and weights $\\pi_k$ of each mixture component.\nIn practice, to avoid numerical issues, the logarithm of the likelihood function is often taken, and the negative is mini-mized, transforming the problem of maximizing the likelihood function into minimizing the negative log-likelihood function argmine (-log L(\u0398)):\n$\\operatorname{argmine} \\left( -\\log \\left( \\prod_{i=1}^n \\sum_{k=1}^K \\pi_k p(X_i | \\mu_k, \\Sigma_k) \\right) \\right)$ (4)\nFor the minimization of the negative log-likelihood function, the EM algorithm (Expectation-Maximization algorithm) is commonly used to estimate the parameters of the GMM. The EM algorithm iteratively alternates between two steps: the E-step (Expectation step) and the M-step (Maximization step), until convergence to a local optimum. In the E-step, the probability that each sample belongs to each mixture component is estimated using the current parameters, and in the M-step, the model parameters are updated by maximizing a lower bound of the current likelihood function.\nWe utilize GMM to model and fit the distributions of these vectors, obtaining a Gaussian Mixture Model that describes these distributions. We save the mean, covariance matrix, and weight coefficient of each Gaussian component. In the next incremental phase, we generate samples that conform to the mixed Gaussian distribution, thus preserving knowledge. However, considering the issue of the number of feature sub-spaces, i.e., not every class necessarily requires K Gaussian components for fitting, we achieve adaptive selection of the number of Gaussians during the GMM model fitting process. That is, to obtain the optimal number of feature subspaces for the feature distribution, we first calculate the closeness of sample i to other samples in the same class:\n$a(i) = \\frac{1}{|C_i|-1} \\sum_{j \\in C_i, j \\neq i} d(i, j)^2,$ (5)\nwhere Ci denotes the set of samples in the same class as sample i, |Ci| is the number of samples in set Ci, and d(i, j) represents the Euclidean distance between sample i and sample j. We then calculate the separateness of sample i from all samples in every other different class:\n$b(i) = \\min_{j \\in C_k} d(i, j)^2.$ (6)\n$k \\neq C_i$\nWe obtain the silhouette coefficient of sample i:\n$s(i) = \\frac{b(i) - a(i)}{\\max{a(i), b(i)}}$. (7)\nFinally, by averaging the silhouette coefficients of all sam-ples, and comparing the average silhouette coefficients under different numbers of Gaussians, we can determine the optimal value of K. Additionally, we need to consider that if there are no longer multiple feature subspaces within a class, meaning the true features may only need to be modeled and fitted with a multivariate Gaussian distribution, we set a threshold for the silhouette coefficient. Only when it exceeds this threshold do we believe that K Gaussian components are needed; otherwise, only one Gaussian component is used.\nFurthermore, if using a multivariate Gaussian distribution, we need to store the covariance matrix. For example, using ResNet18 [47] as the backbone network, we need to store a 512-dimensional class center vector and a 512\u00d7512 covariance matrix. This imposes a higher storage burden compared to prototype-based methods. To address this, we perform twice degradation of the covariance matrix, continuously aligning it for dimensionality reduction, while preserving the centers and weights information obtained from the GMM (as there is no storage burden). We then analyze this process, resulting in a more comprehensive solution. Details will be provided in the experimental section."}, {"title": "C. Avoiding Old-New Confusion", "content": "Through the method in III-B, we can effectively retain old knowledge, aiming to minimize confusion between old classes. However, as shown in Fig. 3, we should also enhance discriminability between new and old tasks. The representation of old knowledge by pseudo features is fixed in the embedding space. When training with new samples, confusion may arise, leading to classifier drift. Therefore, we aim to make the classifier more discriminative towards new and old knowledge. To achieve this, we enhance the prototype with features from new classes outside the old knowledge domain.\nIn task t + 1, the representation $e_n^{c_1}$ of the n-th sample from class $c_1$ in the current task is obtained. We then generate the pseudo features $\\phi_2^{old}$ for a specific old class $c_2$ based on III-B. Subsequently, in the incremental feature space, we blend the obtained pseudo-prototype $\\phi_2^{old}$ of the old class with the feature $e_n^{c_1}$ of the new class in a linear manner, akin to the Mixup approach [45]. The training label is the hard label of the new class, which is utilized to distinguish between new and old classes.\n$\\varphi_m = \\lambda \\cdot e\\_n^{c\\_1} + (1-\\lambda) \\cdot \\varphi\\_2^{old},$ (8)\nwhere $\\lambda \\in [0, 1]$ follows a Beta distribution. In Fig. 4, the three types of data, $e_n^{c\\_1}$, $\\phi_2^{old}$, and $\\varphi\\_m$, representing the new class, the old class, and the mixture of new and old classes, respectively, are input to the linear classifier. They are supervised by three corresponding loss functions, $L_{cls}$, $L_p$, and $L_m$, all computed using cross-entropy. In the feature space, $\\varphi\\_m$, enhanced by new knowledge prototypes, lies between the features of the new and old classes. This approach mitigates overfitting of the classifier to the prototypes and enhances its discriminability between new and old classes. In summary, the loss function is composed as:\n$L = \\xi (L_{cls} + L_p) + (1 - \\xi) L_m,$ (9)\nwhere $\\xi \\in [0, 1]$ is a hyperparameter.\nAdditionally, we categorize classification errors into two types: confusion within tasks and confusion between new and old tasks. Let N represent the total number of samples in the new task, O represent the total number of samples in the old task, $M_{new}$ represent the number of samples misclassified from the old task to the new task, and $M_{old}$ represent the number of samples misclassified from the new task to the old task. The confusion index between the tasks can be calculated as:\n$\\text{C\\_I} = \\frac{M_{new}}{N} + \\frac{M_{old}}{O}$ (10)\nWhen accumulating the confusion across different incremental stages, you can sum the confusion values of each stage. Assuming there are K incremental stages (tasks), with each stage having a confusion index of C_Ir, the total confusion can be expressed as:\n$\\text{C\\_Itotal} = \\sum\\_{\\tau=1}^K \\text{C\\_I}\\_{\\tau}.$ (11)"}, {"title": "D. Generalization Is Prerequisite.", "content": "In continual learning or Multimodal Continual Learning, it is essential to ensure that the network performs well in each stage, which requires adaptability, the ability to adapt to new tasks. In existing research, it has been found that the balance of multimodal is one of the key factors affecting the performance of multimodal training. We also can see in Fig. 12, the feature response difference between different modalities is very large, as a result, the distribution of old knowledge cannot be easily reproduced. Therefore, maintaining the bal-ance between modalities is one of the sufficient conditions for multimodal continual learning. In multimodal continual learning, the $D_\\tau = {X^v_{\\tau,i}, X^s_{\\tau,i}, Y_{\\tau,i}}_{i=1}^{N_\\tau}$ represents the input of $N_\\tau$ paired visual modality data and sensor modality data $x^v_{\\tau,i}x^s_{\\tau,i} \\in \\mathbb{R}^D$ at stage $\\tau$, with the corresponding label $Y_{\\tau,i} \\in C_\\tau$. We decouple the entire model into an encoder, an Intra-modal mining module, and a classifier:\n$f(x^v_{\\tau,i}, x^s_{\\tau,i}) = W^T M(\\phi_v(x^v_{\\tau,i}), \\phi_s(x^s_{\\tau,i})),$ (12)\nwhere $\\phi\\_v(\\cdot) : \\mathbb{R}^D \\rightarrow \\mathbb{R}^{H \\times W \\times d}$, $M(\\cdot) : \\mathbb{R}^{H \\times W \\times d} \\rightarrow \\mathbb{R}^{d}$, $W \\in \\mathbb{R}^{d \\times |y_\\tau|}$, and the encoder consists of two parallel single-modality encoders $\\phi\\_v(\\cdot)$ and $\\phi\\_s(\\cdot)$, outputting feature maps. Firstly, as shown in Fig. 5 inspired by transfer learning methods, since there is an imbalance between dominant and weak modalities, we transfer effective prior information ob-tained from dominant modalities to weak modalities, thereby alleviating the insufficient information in weak modalities caused by under-optimization. The input of the visual modality $x\\_i^v$ contains k frames extracted at intervals from the same video arranged in chronological order, which is aligned in time with the sensor signal. The images containing time information are inputted into the Evaluation function E(\u00b7). Inspired by [48], we directly use pretrained models as part of E(\u00b7). Through E(\u00b7), we obtain the weight information of the visual modality on the timeline and weight this information onto the temporal dimension of the spectrogram obtained from the sensor signal as the input of the sensor modality network:\n$\\text{IS}\\_{\\tau,i} = w\\_{\\tau}^v \\cdot x^s_{\\tau,i} = E(x^v_{\\tau,i}) \\cdot x^s_{\\tau,i}$\n$\\text{TX}* = \\frac{e^{\\rho(\\phi\\_v(x))}}{\\sum\\_{j=1}^T e^{\\rho(\\phi\\_v(x))}},$ (13)\n$IV\\_{\\tau,i} = Xi$, (14)\nwhere $w^v_{\\tau}$ denotes the information extracted from the visual modality over time, $\\rho(\\cdot)$ denotes pooling over the width and height of the feature map output by $\\phi_v(\\cdot)$. After obtaining $IV\\_{\\tau,i}$ and the time-guided $IS\\_{\\tau,i}$, we then perform intra-modal information mining. From this perspective, inspired by [49], as shown in Fig. 6, we explore and enhance information in the sensor network input $IS\\_{\\tau}$ in both the time and frequency domains. The feature map of the sensor modality contains three dimensions: channel, time, and frequency. Same as our previous method [18], in Time-Frequency attention, we perform pooling in both the time and frequency dimensions, then feed them into the small net shown in the Fig. 5. Finally, we add them back to the original input feature map in the time and channel dimensions, forming the sensor modality's information miner. The information that the visual modality should focus on comes from spatial aspects, so we refer to [49]. After obtaining the enhanced feature map, we simply concatenate them and input them into the channel attetion for fusion."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Datasets", "content": "We utilized two commonly used datasets in continual learn-ing, CIFAR100 and ImageNet100. Additionally, considering the challenges of modal imbalance in our multimodal appli-cations, which affect the generalization capability of multi-modal models, we extended our study to the UESTC-MMEA-CL dataset for multimodal behavior recognition. When the generalization ability of multimodal models is ensured, we are willing to extend our continual learning methods through experimental validation.\nCIFAR100 [14]is a dataset widely used for class-incremental learning. It contains 100 classes, each with 600 RGB images. For each class, 500 images are used for training and 100 for testing.\nImageNet100 [16] is a subset of ImageNet-1K [15], con-sisting of 100 classes randomly selected after shuffling the original 1000 classes. Each class in ImageNet100 contains an average of 1300 training samples and 50 test samples. The ImageNet dataset is widely recognized in the field of computer vision and is extensively used in continual learning research.\nUESTC-MMEA-CL [17] is a multimodal activity recog-nition dataset designed for continual learning. It contains 32 daily behavior classes, with approximately 200 samples per class. The training and testing ratio is 7:3, and each sample consists of temporally aligned multimodal segments, including video segments, accelerometer signal segments, and gyroscope signal segments. Same with He et al. [43], we sample 8 RGB images from each video segment and obtain optical flow data generated from the videos, and also perform short-time Fourier transform on the sensor signals to obtain time-frequency spectrograms, which are then input into the model."}, {"title": "B. Implementation Details", "content": "We need to compare our work with different works on different datasets. Since the tasks on each dataset are not exactly the same", "Settings": "In the CIFAR100 dataset", "17": ".", "cases": "T = 16", "details": "On the CIFAR100 and ImageNet100 dataset, we use ResNet18 for training, with pa-rameters set the same as in FeTrIL [37"}]}