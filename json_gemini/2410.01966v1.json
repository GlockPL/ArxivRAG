{"title": "Enhancing Screen Time Identification in Children with a Multi-View Vision Language Model and Screen Time Tracker", "authors": ["Xinlong Hou", "Sen Shen", "Xueshen Li", "Xinran Gao", "Ziyi Huang", "Steven J. Holiday", "Matthew R. Cribbet", "Susan W. White", "Edward Sazonov", "Yu Gan"], "abstract": "Being able to accurately monitor the screen exposure of young children is important for research on phenomena linked to screen use such as childhood obesity, physical activity, and social interaction. Most existing studies rely upon self-report or manual measures from bulky wearable sensors, thus lacking efficiency and accuracy in capturing quantitative screen exposure data. In this work, we developed a novel sensor informatics framework that utilizes egocentric images from a wearable sensor, termed the screen time tracker (STT), and a vision language model (VLM). In particular, we devised a multi-view VLM that takes multiple views from egocentric image sequences and interprets screen exposure dynamically. We validated our approach by using a dataset of children's free-living activities, demonstrating significant improvement over existing methods in plain vision language models and object detection models. Results supported the promise of this monitoring approach, which could optimize behavioral research on screen exposure in children's naturalistic settings.", "sections": [{"title": "Introduction", "content": "Screen exposure has garnered an increasing attention in the past decades due to the dramatic rise in the digital technology. Extensive screen exposure has been associated with healthy and psychological problems [1] such as eye problems [2], language disorders [3], sleep disorders [4], obesity [5], and cognitive impairments [6]. In particular, following the suggestions from World Health Organization (WHO), children with age between 2 and 4 should have less than 1 hour of screen exposure time per day [7]. However, this age group has been reported to spends an average of 2.5 hours in front of screens [8]. Therefore, it is important for parents to objectively monitor children's screen exposure and manage children's screen activity. In addition, there is also an unmet need for scientists to accurately measure screen time to better understand the association between health concerns and screen exposure [9]. Existing research methods for measuring screen exposure rely on users' self-reporting, experimental technologies (e.g., eye-tracking glasses), or built-in device apps. Whereas self-reporting is prone to bias, eye-tracking glasses can be invasive, and built-in apps can't measure exposure across devices or confirm users' identity. There is currently no non-invasive and automatic solution for accurate and robust measurement of screen exposure on cross-device screens.\nA wearable camera with the capability to capture egocentric images is a promising non-invasive candidate to monitor screen presence. Egocentric images are captured from first-person perspective and can demonstrate the wearer's viewpoint. Although there is existing work that uses wearable cameras to track children's screen exposure [10, 11], those solutions are not ideal due to their bulky size and lack of automated analysis. Artificial intelligence (AI) enables computers to automatically identify objects with high accuracy, making it promising for screen exposure measurement. While AI can identify object types from egocentric images, no automated algorithm has been customized for children's screen exposure. Recently, vision language models have shown great potential for robustly identifying objects and events from images [12], but no such efforts have been made specifically for egocentric images for screen type identification. Convolutional neural network (CNN), as a major branch of AI, has advanced automated object detection methods in various applications by providing the backbone architectures for the development of the regional based CNN (R-CNN) [13] and You Only Look Once (YOLO)[14] detection systems. However, such methods work on individual frames and relies on local features extracted from CNN, lacking the capability to associate long range dependency of features or high-level objects.\nIn this paper, we proposed a combination of an egocentric camera, namely the screen time tracker (STT), and a vision language model (VLM) to identify screen existence among multiple screen devices. The STT device and representative images are shown in Fig. 1. In particular, we devised a multi-view VLM (MV-VLM) to process sequential frames from egocentric images. We took advantage of a customized wearable sensor and egocentric images to identify children's screen exposure. In comparison with existing single view input vision language models, we developed a unique multi-view vision language model that process egocentric image sequences from wearable devices. Notably, a contrastive learning-based view selection module and a screen type identification module are specifically designed to address the challenges in existing method and to develop a robust approach to detecting children's screen exposure. The proposed approach innovatively takes multi-view rather than single-view images as input compared to the existing screen identification work [15, 16]. The innovation contributes to the model's capability in extracting long-dependent textual features alongside spatial-temporal features from multi-view imaging, achieving the highest performance among all three types of screen scenarios.\nOur major contributions are summarized as follows:\n\u2022 We proposed a wearable sensor solution to identifying children's screen exposure. Our lightweight wearable sensor is children-friendly and captures egocentric images to monitor children's electronic screen activities. We created a dataset collected from children's free-living activities over two days,"}, {"title": "2 Related Work", "content": "Screen Exposure and Wearable Cameras. Understanding the link between health issues and screen exposure is crucial, necessitating precise tracking of screen usage. Several studies have explored the extent of children's screen exposure using wearable cameras, which are non-invasive and less dependent on bias compared to self-reporting [17, 18]. Wearable cameras also indicate a majority distribution of screen exposure goes to TV (42.4%) of children's daily screen exposure in [19]. Overall, very limited work [17] has explored the solution to automate the screen detection from wearable cameras.\nTraditional Neural Networks in Egocentric Videos. However, while many of these works rely on manual annotations, an automated solution for data analysis could largely reduce human workload. Deep learning models have been largely adapted in processing the videos from the egocentric wearable camera. Chen et al. [20] utilizes CNN and random decision forest for activity recognition; Song et al [21] utilizes CNN-based and VLP-based models in extracting surrounding information for visually impaired person; Bock et al[22] utilizes ActionFormer for outdoor sports recognition;[23] utilizes an LSTM-based encoder-decoder framework to predict movement trajectory of a targeted person. The integration of multi-modal data such as inertial data [22] and IMU readings [23] demonstrated an improved performance compared to using vision-based images alone.\nMulti-view based Neutral Networks. A multi-view deep learning model leverages data from multiple perspectives to enhance learning in tasks such as classification and recognition by integrating diverse sources of information. Although it has been demonstrated in exsiting studies that the incorporation of multi-view in CNN-based models outperforms models with single-view [24, 25], multi-view network has not been fully explored in processing egocentric image sequences. A critical problem in applying multi-view network is how to select multi-view images from egocentric image sequences.\nLarge language model (LLM) in Egocentric Videos. Large language model is a rising field and incorporating LLMs into image processing has the potential to significantly improve upon the studies using traditional models such as CNNs. By leveraging the contextual capabilities of LLMs, researchers can achieve more accurate and comprehensive egocentric image understanding. LifelongMemory was proposed in [26] as a novel framework that uses multiple pre-trained models to answer queries from egocentric video content. Research in [27] addresses Ego4D natural language queries challenge with image and video captioning models. Most of the studies focuses on answer queries, thus lacking the capability to systematically analyze daily life for specific applications. Moreover, there is limited vision language model to address the identification of electronic screen type, even from a single view."}, {"title": "3 Method", "content": "We collected data from an egocentric device, screen time tracker (STT), following our previous work [15, 16]. The key component used in the sensor was a miniature 5 Megapixel camera with 120-degree wide-angle gaze-aligned lens with the resolution of 2592 x 1944. The camera was attached to clothing with a magnetic clip mount, as shown in Fig. 1 (left panel), and boasted a battery life exceeding 48 hours.\nWe redesigned the device's casing by resembling a badge to reduce user burden, particularly for children. Positioned on the chest, this badge-like device captured egocentric images every 10 seconds. Our device measured a dimension of 54mm x 35mm \u00d7 12mm. Notably, the device only takes 30% of the size of commercial cameras used in other wearable senor informatics work [10].\nYoung children between age of 3 and 5 were eligible to participate in data acquisition. The experimental protocol was approved by the local Institutional Review Board (IRB) at home institution. We collected data from 30 participants. Each participant wore the device for two days. The whole dataset roughly includes children's participation in free-living activities. After the two day activities, a survey on the comfort of the wearable device is conducted following a conventional protocol [28]. The screen exposures related to TV, computers, and smartphones were captured.\nOur framework collected multiple images from different views to identify the existence of electronic screens. As shown in Fig. 2, we developed a novel screen identification framework that consists of a view selection module, a vision model, a language model, and a screen identification module. The core concept involves selecting and inputting images from multiple egocentric viewpoints to enhance the robustness of electronic screen identification."}, {"title": "3.2.1 Conceptual Rationale for Multi-View Vision Language Model", "content": "In the screen exposure scenarios, multi-view image processing could be beneficial in several aspects. For instance, when screens are only partially captured, multi-view images may provide complementary data from different parts of the screen, enabling deep learning models to better understand and integrate the screen's various structural components. Additionally, in cases of low-quality images, such as those that are blurry, it is challenging for deep learning models to classify the images. Utilizing multi-view images allows the model to synthesize features from various perspectives, thereby enhancing feature representation and improving classification accuracy. To identify the existence of electronic screens, it is essential to consider the spatial relationships among static objects like TVs, computers, windows, walls, and shelves. These objects maintain consistent spatial relationships in the real world, which are reflected in 2D images based on perspective principles. To capitalize on these features and their spatial interrelations, we employed a vision transformer coupled with a language model. This approach leveraged the model's capability to explore long-range dependencies in text descriptions and image processing."}, {"title": "3.2.2 View Selection", "content": "Image sequences captured from STT are in time series, representing objects from different views and timestamps. Images taken from the same scene with small variations of position, height, and orientation are considered similar, though environmental condition like light, time of the day may change. We sought for a mapping rule that is robust enough to reveal spatial relationships and partial or occluded screen information. To capture coherent image features, we chose a contrastive learning-based embedding approach to encode the images and analyze their similarity. Assuming the egocentric image sequences $I_1, I_2, I_3, ..., I_n$, for each image $I_i$, we used contrastive language-image pre-training (CLIP) to convert images to embeddings $CLIP(I_i)$. For any two images $I_m$ and $I_n$, we measured the cosine similarity between two embeddings via:\n$Sim(I_m, I_n) = \\frac{CLIP(I_m) \\cdot CLIP(I_n)}{||CLIP(I_m)|| ||CLIP(I_n)||}$ (1)\nWhere () represents dot product and $||. ||$ is the magnitude. Then to split the image sequences into multi-view image groups, we built a graph G(V, E) to represent the images with their similarity according to values of $Sim(I_m, I_n)$. In Graph G, each node $v_i$ represents an image $I_i$. V corresponds to the set of nodes (i.e., images) and E corresponds to the set of weighted edges that connect the each pair of nodes within a time window. For two nodes $v_m$ and $v_n$, the edge $E_{m,n}$ of them is valid only if $Sim(I_m, I_n)$ is within a range between an upper bound $t_h$ and a lower bound $t_l$. The upper bound ensured that static images in consecutive frames would not be selected. The lower bound ensured these two images could capture the same screen-related scene. We built an undirected graph and selected connected components of size k as multi-view image groups. To get the most number of multi-view groups, we first sorted the all connected components of size k according to the sum of edge degrees in each k-size component ascendingly. Then we greedily selected and split the components starting from the components with least edge degrees from G, and updated G accordingly. The detailed procedures are illustrated in Algorithm 1."}, {"title": "3.2.3 Vision Language Model", "content": "The VLM model includes three components: a ViT model for vision embedding, a MiniLM model for text processing, and a Llama model for text generation. The three modules are connected by alignment layers.\nVisual Embedding (ViT). We utilized the Swin Transformer [29] for image embedding. The transformer model used in this paper is a pre-trained model on a conclusive natural image dataset [30]. Images were divided into smaller non-overlapping patches and learnt by the transformer blocks. The transformer blocks operated in a hierarchical manner and each block applied multi-head self-attention mechanisms based on shifted windows. These shifted windows enabled a set of self-attention models to dynamically learn and extract image features which include long-range dependence. In our model, the features extracted from different views were embedded to a fixed length and sent to fusion layer. In the feature fusion layer, features with the same length were normalized and stacked. Then an alignment layer, which was built by a fully connected layer, fused them into a constant dimension. In particular, the features were averaged to create an input for the following vision-language generating task. This process fused multi-view images equally to enhance a complementary integration of features extracted from Swin Transformer.\nText Embedding (MiniLM). In the training phase, we employed a MiniLM [31] for extracting textual features from annotations associated with multi-view images. In particular, each images were associated with a sentence of textual annotation during training. MiniLM is made of a set of distilled self-attention model in transformer implementation. In this study, our analysis was constrained by a limited sample space, as all egocentric images were sourced from indoor home environments. Furthermore, the sample set mainly comprises screen-related instances. In addition, textual information involved in this study was mainly that relevant to screen events. This made MiniLM particularly well-suited for our application, as its distilled architecture efficiently captured the necessary textual embeddings during the training phase. In the Feature Synthesis layer, text embeddings were normalized and concatenated to ensure the brevity of image captions. These concatenated embeddings were then processed through a linear alignment layer, ensuring dimensional consistency with the embeddings used in the Swin Transformer model. In particular, the text embeddings extracted from this phase serve as the annotation for the following text generation model.\nText Generation (Llama). In this study, we used a large language model, Llama2 \u2013 7B [32], to efficiently produce fast scene description regarding the contents in the multi-view images. We used one set of fully connected layers, which served as a soft prompt, to align the visual features with the large language model. Similarly, another set of alignment layers connected the text embeddings with the large language model. The training of text generation model was an optimization process for alignment layers, where $\u03b8_v$ and $\u03b8_t$ corresponded to the weights in alignment for vision model and the weights in alignment layers for text mode. The loss function for the report generation task was jointly optimized by the combination of loss terms from each task:\n$L_{report} (\u03b8_v, \u03b8_\u03c4; X_v, X_t, X_p, X_r) = - \\sum_{l=1}^{M} log P_{\u03b8_v, \u03b8_\u03c4}(x_l; X_v, X_t, X_p, X_{r<l})$ (2)\nwhere $x_l$ is a variable related to the predicted token, M represents the length of the generated text, $X_r$ represents the current prediction text, $X_v$ represents the visual embedding, $X_t$ represents the textual inputs, $X_p$ represents prompts, and $X_{r<l}$ represents the token before the predicted token. This loss function considered textual input, visual input, and token generated, thus seeking an optimized parameter setting to generate reliable scene descriptions."}, {"title": "3.2.4 Screen Type Identification", "content": "The text description of the multi-view images was further processed to identify screen types. First, we extracted key words from generated description."}, {"title": "4 Experiments and Results", "content": "The raw dataset has 1191 images, corresponding to 397 groups of multi-view images, including different screen types of TV, smartphone, computer use in free-living environments. To build a multi-view image dataset with caption describing the screen"}, {"title": "4.3 Multi-view selection", "content": "Our model selected images from CLIP embedding. We picked a subset of egocentric images to input VLM. A representative example of each screen type is shown in Fig. 3. We noted that the selected images complemented each other in terms of field of view and spatial features, demonstrating that CLIP was an effective feature extractor for evaluating similarity and selecting views. Moreover, we analyzed the CLIP generated features using t-distributed Stochastic Neighbor Embedding (t-SNE) [36], to visualize the distribution of typical multi-view images. It is observed that not only the features from the same multi-view group is complement to each other but also the features among screen types are distinctive to each other. Such observation lays a foundation of text generation and screen type identification."}, {"title": "4.4 Text generation and screen type identification", "content": "We evaluated the generated text description from Llama with the ground truth using BiLingual Evaluation Understudy (BLEU) [37]. BLEU scores evaluate the similarity of the generated description by Llama and the reference caption. A higher BLEU indicates higher similarity to ground truth. BLEU N, where N corresponds to contiguous sequences of N word, indicates an evaluation of quality in a context. In Fig. 4, while value decreases when N increases, comparatively high BLEU scores are obtained from BLEU 1 to BLEU 4. Moreover, our MV-VLM model was able to generated scene description with logic and smoothness.\nFigure 5 shows representative scene descriptions in each screen type categories. Key words from each scene description is highlighted in red and categories to the three screen types (i.e., TV, smarpthone, and computer). Our vision language model effectively identified various screen types, their spatial relationships, and action events with objects. For instance, it identified a typical setup in the first row where a TV and a fireplace are typical living room setting. In the second row, it captured a scene of a hand interacting with a laptop, recognizing a person seated in front of the laptop. In the final row, the action of holding a cell phone was precisely detected, along with the accurate identification of a smartphone."}, {"title": "4.5 Comparison with existing methods", "content": "To validate the superiority of MV-VLM over existing methods, we compared the performance of identifying screen types with YOLOv8[33] and single-view vision language model (MiniGPT) [34] in Fig. 4 (b). YOLOv8 is an object detection approach that uses convolution neural network (CNN) as backbone to output the bounding box and screen type. It is currently the state of the art that provides high accuracy in object detection. The performance is around 20% lower than our method in accuracy, indicating that VLM is much powerful than conventional CNN-based approach. As shown in Fig. 5, MV-LVM identified spatial relationship among objects (e.g., objects around the screen) and action events (e.g., holding a phone). The capability of identifying spatial relationship among objects and action events is the key that our approach could outperform conventional CNN-based object detection approach.\nMiniGPT took single view image as input and used a pre-trained VLM to generate text followed by the same key word extraction and identification process. The performance of MiniGPT is much worse than proposed method, indicating that it is necessary to retain a VLM model with alignment layer specifically for screen type identification. Importantly, we note a consistently higher of 10% accuracy among all types of screens in Fig. 4 (c). The superiority of our method also demonstrates the need for multi-view processing in vision language model as MiniGPT only process images from single-view. In comparison with other VLM, egocentric image sequences have the advantages of acquiring multi-view images over time for processing. Egocentric images are thus considered as an ideal data source for MV-VLM."}, {"title": "4.6 Ablation study", "content": "We conducted an ablation study to investigate the contributions of multi-view and multi-caption elements to detection accuracy. The results are summarized in Table 2. For comparative purposes, we modified two models: 1) we removed the view selection module and directly used a single image as input (denoted by Proposed w/o MV, CLIP); 2) we employed three temporally consecutive images as input instead of using the CLIP framework to select multiple views (denoted by Proposed w/o CLIP). It is important to note that the model without the multi-view functionality differs from the MiniGPT described in the previous section, specifically in its underlying large language model architecture. Both alternative methods showed lower accuracy than the proposed model. Without a sophisticated view selection mechanism, the performance can be comparable (i.e., computer) to or even worse (i.e., smartphone) than that achieved with a single-view approach, particularly for smaller screens whose locations can vary across consecutive images. Overall, the results highlight the importance of both the multi-view structure and the CLIP-based view selection in achieving high identification accuracy for different screen types."}, {"title": "4.7 Binary classification", "content": "In behavioral study, it is more important to determine the existence of electronic screens rather than identifying the screen types. To this end, we conducted a binary classification test to evaluate the performance of the MV-VLM in detecting the existence of screens or not. We had 154 groups of multi-view images in the test set. The results, including a confusion matrix, are presented in Table 3. The model achieved an overall accuracy of 75.3%. Notably, we achieved a high sensitivity of 88.2%, indicating a stronger capability in detecting the screen existence with a small fraction of false negative detection."}, {"title": "4.8 Assessment of comfort on wearable device", "content": "We conduct an assessment of comfort on the wearable device. As shown in Table 4, the device receives high evaluation scores in movement, and emotional response, anxiety, and harm. The results suggest that the device is comfortable and acceptable for regular use by children. The high scores demonstrate the effectiveness of our tailored design for children, which includes light weight, a magnetic clip mount for secure attachment, a smaller and lighter badge-like design, and customized shapes to appeal to children."}, {"title": "5 Discussion", "content": "In this study, we propose MV-VLM, the first known vision-language model that is deployed for screen type identification. The dual extraction performance distinctly surpasses that of other existing models. The efficacy of MV-VLM in processing and integrating these diverse data types indicates potential utility in applications where comprehensive understanding from multiple perspectives is crucial. Moreover, MV-VLM only requires a minimal amount of training samples, as the training is on the alignment layers and the other models are fine-tuned on a pre-trained models (Swin Transformer and MiniLM). Such feature enhances the feasibility of deploying our model in resource-constrained environments. Besides the data analysis, we also improve the data collection process. Unlike previous studies that collected data from adults in controlled environments, this study focuses on data collected from children in a free-living environments, making the dataset more representative of real-world usage and future behavioral studies. To protect children's privacy, guardians and children may delete images that they feel uncomfortable at the end of data acquisition following conventional protocols in [16, 15].\nThere are a few limitations of this study. First, we only examine the case of single screen type is involved. We will expand the framework to multi-task classification when multiple screens are involved in sequential images. Second, our data collection has been restricted to indoor environments, which limits the exposure of our model to varied environmental contexts. In future research, we plan to expand our data acquisition efforts to outdoor settings, thereby enriching the model's training dataset with a broader range of environmental dynamics. This expansion is anticipated to booster the model's generalization capabilities and enhance its performance across more diverse real-world scenarios.\nTo ensure a fair evaluation on the algorithm development, we exclude samples with severe motion blurring and images accidentally covered by clothes or other objects. We consider those images as outliers which do not correspond to features captured by wearable sensors. In the integration of a hardware system, we plan to use accelerometers to inform the exclusion of low quality blurry images and use ambient light sensors to instruct the system that there are occlusion and the multi-view images are not needed. Future study will seek to integrate behavioral research to measurement obtained from MV-VLM. By correlating our model's outputs with behavioral data, we aim to deepen our understanding of the interactions between environmental contexts and individual behaviors. This approach is promising not only to validate MV-VLM but also to extend to more complex, behaviorally-driven studies."}, {"title": "6 Conclusion", "content": "We proposed a lightweight and comfortable wearable sensor solution using screen time tracker to monitor children's screen exposure. We devised a multi-view vision language model to identify existence of screens from egocentric image sequences. We collected data from children's free living activities. We explored multi-view images to enhance features of screen identification. This multi-view model is integrated with vision language model, generating image descriptions that are related to screen existence. Our experiments indicate superiority in comparison with conventional vision language model and object detection model. Future work will include accumulative measurements of screen time exposure and associate the screen measurements with other factors in behavioral studies."}]}