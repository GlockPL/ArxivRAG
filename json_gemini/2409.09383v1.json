{"title": "LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free Approach", "authors": ["Kunlong Chen", "Junjun Wang", "Zhaoqun Chen", "Kunjin Chen", "Yitian Chen"], "abstract": "We participated in the KDD CUP 2024 paper source tracing competition and achieved the 3rd place. This competition tasked participants with identifying the reference sources (i.e., ref-sources, as referred to by the organizers of the competition) of given academic papers. Unlike most teams that addressed this challenge by fine-tuning pre-trained neural language models such as BERT or ChatGLM, our primary approach utilized closed-source large language models (LLMs). With recent advancements in LLM technology, closed-source LLMs have demonstrated the capability to tackle complex reasoning tasks in zero-shot or few-shot scenarios. Consequently, in the absence of GPUs, we employed closed-source LLMs to directly generate predicted reference sources from the provided papers. We further refined these predictions through ensemble learning. Notably, our method was the only one among the award-winning approaches that did not require the use of GPUs for model training.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving landscape of scientific research, tracing the origins of ideas of academic papers has become increasingly crucial. The ability to accurately identify the source references of a paper not only enhances our understanding of the pathways of scientific progress but also holds significant scientific and societal value. As the volume of scientific publications continues to grow exponentially, there is an urgent need for efficient algorithms that can swiftly identify the sources of papers. Such tools are particularly valuable for emerging researchers, enabling them to quickly grasp the developmental trajectory of specific technologies and situate their work within the broader scientific context.\nTraditionally, researchers have used methods such as random forests or a neural network training to identify key citations for each paper. However, these approaches often require substantial time and effort in designing intricate features or fine-tuning pre-trained models to achieve satisfactory performance. The advent of large language model (LLM) technologies has opened new avenues for addressing this challenge. Recent studies have demonstrated that when the parameter count is sufficiently high, LLMs can solve complex problems through zero-shot learning.\nIn this paper, we propose a novel approach that leverages pre-trained LLMs as unsupervised reasoners to identify a paper's source references. Our method utilizes the paper's text and related information to perform direct reasoning, eliminating the need for extensive feature engineering or model fine-tuning. This approach is particularly beneficial for researchers with limited GPU resources, as it offers a balance between computational efficiency and performance accuracy.\nThe efficacy of our proposed method is validated by its success in the paper source tracing competition of KDD CUP 2024, where it secured third place. Notably, our team was the only award-winning entry that did not rely on GPUs, underscoring the method's ability to achieve competitive results with minimal computational resources. This achievement demonstrates that our approach effectively balances resource consumption and performance, making it a valuable tool for a wide range of researchers and institutions."}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 Data Description", "content": "The dataset we used in this study is from the Open Academic Graph dataset, which includes training and test sets for multiple tasks under the academic paper mining theme, such as Author Name Disambiguation, Academic question answering, and Paper source tracing. The method we employ is applicable to the data from the Paper source tracing task in the OAG dataset, which is referred to as the PST dataset in the following text. The PST dataset contains 1576 papers related to computer science and 55014 associated citations."}, {"title": "2.2 Task Description", "content": "The primary objective of our study is to identify the most significant reference paper for a given academic work. To formalize this task, we introduce the following notation.\n\u2022 Let A denote the set of all references that have been cited at least once across the entire dataset.\n\u2022 For each paper Pi, we define Ri = [ri,1, ri,2,\u00b7\u00b7\u00b7,ri,j] as the set of its references, where each ri,j \u2208 A.\n\u2022 We denote Si C Ri as the subset of references that serve as source references for paper Pi.\nOur task is to assign a probability score pi,j \u2208 [0, 1] to each reference ri,j in Ri, where higher values indicate a greater likelihood of the reference being a key source for the paper. To evaluate the performance of our model, we employ the following metrics:\n\u2022 For each paper Pi, we calculate the Average Precision (AP), denoted as AP\u00a1.\n\u2022 The overall model performance is assessed using the Mean\nAverage Precision (MAP), computed as $MAP = \\frac{1}{N} \\Sigma_{i=1}^{N} AP_i$, where N is the total number of papers in the dataset."}, {"title": "3 METHOD", "content": "Given the constraints of limited access to high-performance GPUs, we developed an innovative approach that diverges from traditional methods such as fine-tuning large pre-trained models or employing graph machine learning techniques. Our methodology primarily leverages closed-source LLMs in conjunction with feature engineering and ensemble learning to address the paper source tracing problem effectively."}, {"title": "3.1 Feature Engineering", "content": "While our method relies heavily on LLMs for text comprehension and problem-solving, we found that incorporating carefully engineered features from the papers enhances performance. Our feature extraction process focuses on the following key aspects:\n(1) Paper Metadata: We extract information such as the publication venue (journal/conference name), citation count, and author details (nationality and affiliation) for each paper.\n(2) Citation Statistics: We compute the total occurrences of each cited reference within the paper, as well as its distribution across different sections or chapters.\n(3) Reference Metadata: Similar to the main paper, we extract publication venue and author information for each cited reference.\n(4) Contextual Keywords: We count the occurrences of specific keywords (e.g., motivated by, inspired by) in the vicinity of each citation to gauge its importance.\nThese features provide a rich, structured representation of the papers and their citations, complementing the semantic understanding capabilities of the LLMs."}, {"title": "3.2 LLM-generated Answers", "content": "To generate diverse and high-quality answers, we employ four state-of-the-art LLMs: GPT-4 Turbo, GPT-40, Gemini 1.5 Pro, and Claude 3 Opus. Our selection criterion was based on empirical evidence suggesting that LLMs with an MMLU (Massive Multitask Language Understanding) score above 85 demonstrate superior logical reasoning capabilities in academic contexts."}, {"title": "3.2.1 Prompt Engineering", "content": "We designed a set of prompts to elicit nuanced responses from the LLMs:\n(1) Base prompt: A carefully crafted instruction set (illustrated in Figure 1) that guides the LLM in identifying source citations.\n(2) Inspiration-focused prompt: This variant asks the LLM to categorize citations as \"direct inspiration\", \"indirect inspiration\", or \"other inspiration\", emphasizing the keyword \"inspiration\".\n(3) Title-enriched prompt: An extension of the base prompt that includes the titles of cited articles for additional context.\n(4) Meta-optimized prompt: We utilize GPT-4 to refine the base prompt itself, leveraging the LLM's meta-learning capabilities.\n(5) Notes-based prompt: For papers with available \"notes\" fields (potentially containing annotators' insights), we instruct the LLM to identify key citations based on these descriptions."}, {"title": "3.2.2 Answer Generation", "content": "Let NLLM denote the number of LLM types and Nprompt the number of prompt variants. Theoretically, for each academic paper, we could generate up to $M_p = N_{LLM} \\cdot N_{prompt} \\cdot M$ answers, where M is the maximum number of answers per LLM-prompt combination. However, due to practical constraints, we limit this to Mp answers per paper.\nFor each cited reference ri,j in the citation set Ri of paper Pi, we generate a probability list $P_{i,j}^{LLM}$ of length Mp, where each element represents the LLM-estimated probability of ri, j being a source citation. Formally, we have:\n$P_{i,j}^{LLM} = [p_1, p_2, ..., p_{M_p}], p_k \\in [0, 1]$"}, {"title": "3.3 Base Models", "content": "To complement the LLM-generated probabilities, we employ two gradient boosting frameworks: LightGBM and CatBoost . These models operate on the engineered features described in Section 3.1.\nFor each paper-citation pair (Pi, ri,j) in the training set, we construct a feature vector Xi,j and a corresponding binary label Yi,j \u2208 {0, 1}, where Yi,j = 1 indicates that ri,j is a reference source of Pi. We train two classifiers:\n$f_{lgb}(X_{i,j}) \\rightarrow [0, 1] \\quad (LightGBM)$\n$f_{cb}(X_{i,j}) \\rightarrow [0, 1] \\quad (CatBoost)$"}, {"title": "3.4 Ensemble Approach", "content": "Our final prediction model combines the strengths of LLM-generated answers and traditional machine learning classifiers. For each paper-citation pair (Pi, ri,j), our goal is to find a function that combines the prediction results of different methods as:\n$p_{i,j} = f_{ensemble}((P_{i,j}^{LLM}), f_{lgb}(X_{i,j}), f_{cb}(X_{i,j}))$"}, {"title": "3.5 Our framework", "content": "In this section, we present our complete approach. The algorithm takes as input the main text of paper Pi and its list of cited references Ri, and outputs the probability pi,j for each reference ri,j being a source citation of paper Pi.\n(1) Using $f_{lgb}$ and $f_{cb}$ trained on the training set, we score each reference ri,j, where the score represents the probability of it being a source citation. This score is represented as $p_{i,j}^{base} = \\frac{p_{lgb} \\cdot p_{lgb}^{xw} + p_{cb} \\cdot p_{cb}^{xw}}{p_{lgb} + p_{cb}}$.\n(2) We modify $p_{i,j}^{base}$ using the results from LLMs. This process involves two main steps:\n\u2022 First, we group the LLM outputs into N VLLM groups based on prompt type and base model type. Within each group,\nwe ask each LLM to provide a confidence score for its predicted key citations, as represented by the prob2score function (Equation 5). Additionally, we assign a weight\n$w_{lm}$ to each group based on expert knowledge, representing the importance of that LLM group. Finally, we aggregate the results of all LLM groups according to their weights to obtain $score_{bonus}$, which serves as a bonus to be added to $p_{i,j}^{base}$.\n\u2022 In the second step, for each candidate reference, we aggregate the results from all LLMs. If the score given by the LLM at the $p_{neg}$-th percentile is less than $P_{threshold}$, we reduce the bonus magnitude from the previous step. Our approach is to divide $score_{bonus}$ by a constant $C_{neg}$.\n$prob2score(x) = \\begin{cases} 3, & \\text{if } x \\geq 0.9 \\\\ 2, & \\text{if } 0.5 \\leq x < 0.9 \\\\ 1, & \\text{if } 0.4 < x < 0.5 \\\\ 0, & \\text{if } x \\leq 0.4 \\end{cases}$\n(3) Our final prediction can be expressed as:\n$p_{i,j}^{final} = p_{i,j}^{base} + of\\cdot \\frac{score_{bonus}}{C_{neg}}$\nwhere of is the weight used to adjust the base score using the LLM prediction results."}, {"title": "4 Result", "content": "We present the performance of our proposed method on the validation set in Table 1. As can be observed, using LightGBM or LLM individually does not yield high scores. When the two approaches are combined, however, a significant improvement in the score can be achieved. This demonstrates the effectiveness of ensemble learning techniques. In addition, we present the configuration of parameters in Table 1."}, {"title": "5 Conclusion", "content": "In this paper, we have briefly introduced our method that secured third place in the KDD CUP 2024 paper source tracing competition. Our approach leverages the zero-shot capabilities of LLMs, achieving impressive performance without the need for GPU-intensive model training. We demonstrate that ensemble learning techniques can significantly enhance the zero-shot performance of LLMs.\nWe believe that our work highlights the potential of combining multiple LLMs to harness their collective intelligence for complex reasoning tasks. This approach is particularly valuable in resource-constrained environments, as it enables sophisticated inference without the need for extensive computational resources.\nLooking ahead, we encourage the machine learning community to focus on developing methods that effectively integrate the strengths of diverse LLMs for inference tasks. Such research directions hold promise for expanding the applicability of advanced AI systems across a wide range of domains and computational settings."}]}