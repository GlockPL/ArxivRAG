{"title": "TANGOFLUX: SUPER Fast and FAITHFUL TEXT TO AUDIO GENERATION WITH FLOW MATCHING AND CLAP-RANKED PREFERENCE OPTIMIZATION", "authors": ["Chia-Yu Hung", "Navonil Majumder", "Zhifeng Kong", "Ambuj Mehrish", "Rafael Valle", "Bryan Catanzaro", "Soujanya Poria"], "abstract": "We introduce TANGOFLUX, an efficient Text-to-Audio (TTA) generative model with 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio in just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models lies in the difficulty of creating preference pairs, as TTA lacks structured mechanisms like verifiable rewards or gold-standard answers available for Large Language Models (LLMs). To address this, we propose CLAP-Ranked Preference Optimization (CRPO), a novel framework that iteratively generates and optimizes", "sections": [{"title": "1 INTRODUCTION", "content": "Audio plays a vital role in daily life and creative industries, from enhancing communication and storytelling to enriching experiences in music, sound effects, and podcasts. However, creating high-quality audio, such as foley effects or music compositions, demands significant effort, expertise, and time. Recent advancements in text-to-audio (TTA) generation and offer a transformative approach, enabling the automatic creation of diverse and expressive audio content directly from textual descriptions. This technology holds immense potential to streamline audio production workflows and unlock new possibilities in multimedia content creation. However, many existing models face challenges with controllability, occasionally struggling to fully capture the details in the input prompts, especially when the prompts are complex. This can sometimes result in generated audio that omits certain events or diverges from the user intent. At times, the generated audio may even contain input-adjacent, but unmentioned and unintended, events, that could be characterized as hallucinations.\nIn contrast, the recent advancements in Large Language Models (LLMs) have been significantly driven by the alignment stage after pre-training and supervised fine-tuning. This alignment stage, often leveraging reinforcement learning from human feedback (RLHF) or other reward-based optimization methods, endows the generated outputs with human preferences, ethical considerations, and task-specific requirements. Despite the rapid progress in TTA models, until recently alignment, that could mitigate the aforementioned issues with audio outputs, has not been a mainstay in TTA model training.\nOne critical challenge in implementing alignment for TTA models lies in the creation of preference pairs. Unlike LLM alignment, where off-the-shelf reward models and human feedback data or verifiable gold answers are available, TTA domain as yet lacks such tooling. For instance, in general, LLM alignment settings, such as safety or instruction following, tools exist for categorizing specific safety risks . Frontier LLMs like GPT-4 are often used directly to judge the candidate outputs.\nWhile audio language models can process audio inputs and generate textual outputs, they often produce noisy feedback, unfit for preference pair creation for audio. BATON employs human annotators to assign a binary score of 0 or 1 to each audio sample based on its alignment with a given prompt. However, such labor-intensive manual approach is often economically not viable at a large scale.\nTo address these issues, we propose CLAP-Ranked Preference Optimization (CRPO), a simple yet effective approach to generate audio preference data and perform preference optimization on rectified flows. CRPO consists of iterative cycles of data sampling, generating preference pairs, and performing preference optimization, resembling a self-improvement algorithm. We first demonstrate that the CLAP model can serve as a proxy reward model for ranking generated audios by alignment with the text description. Using this ranking, we construct an audio preference dataset that yields superior performance after preference optimization, as compared to other audio preference datasets, such as, BATON and Audio-Alpaca . Finally, we demonstrate the effectiveness of this iterative optimization, emphasizing the importance of each component, including the modified loss function compared to conventional preference optimization loss.\nAdditionally, many TTA models are trained on proprietary data , with their weights often unavailable to the public or accessible only through private APIs, posing challenges for public use and foundational research. Moreover, the diffusion-based TTA models are known to require too many denoising steps to generate a decent output, consuming much GPU compute and time."}, {"title": "2 METHOD", "content": "TANGOFLUX consists of FluxTransformer blocks which are Diffusion Transformer (DiT) and Multimodal Diffusion Transformer (MMDIT), conditioned on textual prompt and duration embedding to generate audio at 44.1kHz up to 30 seconds. TANGOFLUX learns a rectified flow trajectory from audio latent representation encoded by a variational autoencoder (VAE) . TANGOFLUX training pipeline consists of three stages: pre-training, fine-tuning then preference optimization. TANGOFLUX is aligned via CRPO which iteratively generates new synthetic data and constructs preference pairs to perform preference optimization. The overall pipeline is depicted in Fig. 1."}, {"title": "2.1 AUDIO ENCODING", "content": "We use the VAE from Stable Audio Open, which is capable of encoding stereo audio waveforms at 44.1kHz into audio latent representations. Given a stereo audio $X \\in \\mathbb{R}^{2 \\times d \\times s_r}$ with d as the duration and $s_r$ as the sampling rate, the VAE encodes X into a latent representation $Z \\in \\mathbb{R}^{L \\times C}$, with L, C being the latent sequence length and channel size, respectively. The VAE decodes the latent representation Z back into the original stereo audio X. The entire VAE is kept frozen during TANGOFLUX training."}, {"title": "2.2 MODEL CONDITIONING", "content": "To enable the controllable generation of audio of varying lengths, we employ textual conditioning and duration conditioning. Textual conditioning controls the event present of the generated audio based on a provided description, while duration conditioning specifies the desired audio length, up to a maximum of 30 seconds.\nTextual Conditioning. Given the textual description of an audio, we obtain the text encoding $C_{text}$ from a pretrained text-encoder. Given the strong performance of FLAN-T5 as conditioning in text-to-audio generation, we select FLAN-T5 as our text encoder.\nDuration Encoding. Inspired by the recent works , to generate audios with variable length, we firstly use a small neural network to encode the audio duration into a duration embedding $C_{dur}$. This is concatenated with the text encoding $C_{text}$ and fed into TANGOFLUX to control the duration of audio output."}, {"title": "2.3 MODEL ARCHITECTURE", "content": "Following the recent success of FLUX models in image generation 1, we adopt a hybrid MMDiT and DiT architecture as the backbone for TANGOFLUX. While MMDiT blocks demonstrated a strong performance, simplifying some of them into single DiT block improved scalability and parameter efficiency 2. These lead us to select a model architecture consisting of 6 blocks of MMDiT, followed by 18 blocks of DiT. Each block uses 8 attention heads, with each attention head dimension of 128, resulting in a width of 1024. This configuration results in a model with 515M parameters."}, {"title": "2.4 FLOW MATCHING", "content": "Several generative models have been successfully trained under the diffusion framework. However, this approach is known to be sensitive to the choice of noise scheduler, which may significantly affect performance. In contrast, the flow matching (FM) framework has been shown to be more robust to the choice of noise scheduler, making it a preferred choice in many applications, including text-to-audio (TTA) and text-to-speech (TTS) tasks.\nFlow matching builds upon the continuous normalizing flows framework . It generates samples from a target distribution by learning a time-dependent vector field that maps samples from a simple prior distribution (e.g., Gaussian) to a complex target distribution. Prior work in TTA, such as AudioBox and Voicebox, has predominantly adopted the Optimal Transport conditional path proposed by . However, in our approach, we utilize rectified flows instead, which is a straight line path from noise to distribution, corresponding to the shortest path.\nRectified Flows. Given a latent representation of an audio sample $x_1$, a noise sample $x_o \\sim N(0, I)$, time-step $t \\in [0, 1]$, we can construct a training sample $x_t$ where the model learns to predict a velocity $v_t = \\frac{dx_t}{dt}$ that guides $x_t$ to $x_1$. While there exist several methods of constructing transport path $X_t$, we used rectified flows (RFs) which the forward process are straight paths between target distribution and noise distribution, defined in Eq. (1). It was empirically demonstrated that rectified flows are sample efficient and degrade less compared to other formulations when reducing lesser number of sampling steps . We use $\\theta$ to denote the model u's parameter. The model directly regresses the predicted velocity $u(x_t, t;\\theta)$ against the ground truth velocity $v_t$ where the loss is shown in Eq. (2).\n$x_t = (1-t)x_1 + tx_o, v_t = \\frac{dx_t}{dt} = x_o - x_1, \\text{(1)}$"}, {"title": "2.5 CLAP-RANKED PREFERENCE OPTIMIZATION (CRPO)", "content": "CLAP-Ranked Preference Optimization (CRPO) leverages a text-audio joint-embedding model like CLAP as a proxy reward model to rank the generated audios by similarity with the input description and subsequently construct the preference pairs.\nWe firstly set a pre-trained checkpoint of TANGOFLUX architecture as the base model to align, denoted by $\\pi_o$. Thereafter, CRPO iteratively aligns checkpoint $\\pi_k := u(\\cdot; \\theta_k)$ into checkpoint $\\pi_{k+1}$, starting from k = 0. Each of such alignment iterations consists of three steps: (i) batched online data generation, (ii) reward estimation and preference dataset creation, and (iii) fine-tuning $\\pi_k$ into $\\pi_{k+1}$ via direct preference optimization.\nThis approach to aligning rectified flow is inspired by a few LLM alignment approaches . However, there are key distinctions to our work: (i) we focus on aligning rectified flows for audio generation, rather than autoregressive language models; (ii) while LLM alignment benefits from numerous off-the-shelf reward models, which facilitate the construction of preference datasets based on reward scores, LLM judged outputs, or programmatically verifiable answers, the audio domain lacks such models or method for evaluating audio. We demonstrate that the CLAP model can serve as an effective proxy audio reward model, enabling the creation of preference datasets (see Section 4.3). Finally, we highlight the necessity of generating online data at every iteration, as iterative optimization on offline data leads to quicker performance saturation and subsequent degradation."}, {"title": "2.5.1 CLAP AS A REWARD MODEL", "content": "CLAP reward score is calculated as the cosine similarity between textual and audio embeddings encoded by the model. Thus, we assume that CLAP can serve as a reasonable proxy reward model for evaluating audio outputs against the textual description. In Section 4.3, we demonstrate that using CLAP as a judge to choose the best-of-N inferred policies improves performance in terms of objective metrics."}, {"title": "2.5.2 BATCHED ONLINE DATA GENERATION", "content": "To construct a preference dataset at iteration k, we first sample a set of prompts $M_k$ from a larger pool $B$. Subsequently, we generate N audios for each prompt $y_i \\in M_k$ using $\\pi_k$ and use CLAP to rank those audios by similarity with $y_i$. For each prompt $y_i$, we select the highest-rewarded or -ranking audio $x^w$ as the winner and the lowest-rewarded audio $x^l$ as the loser, yielding a preference dataset $D_k = {(x^w, x^l, y_i) | y_i \\in M_k}$."}, {"title": "2.5.3 PREFERENCE OPTIMIZATION", "content": "Direct preference optimization (DPO) is shown to be effective at instilling human preferences in LLMs . Consequently, DPO is successfully translated into DPO-Diffusion for alignment of diffusion models. The DPO-diffusion loss is defined as\n$L_{DPO-Diff} = -E_{(x^w,x^l)\\sim D} logo (\\frac{\\rho_{\\theta}(x_{0:T})}{\\rho_{ref}(x_{0:T})}) - logo(\\frac{\\rho_{\\theta}(x^l_{0:T})}{\\rho_{ref}(x^l_{0:T})}) ) \\text{(3)}$"}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 MODEL TRAINING", "content": "We pretrained TANGOFLUX on Wavcaps for 80 epochs with the AdamW , $\\beta_1 = 0.9, \\beta_2 = 0.95$, a max learning rate of $5 \\times 10^{-4}$. We used a linear learning rate scheduler for 2000 steps. We used five A40 GPUs with a batch size of 16 on each device, resulting in an overall batch size of 80. After pretraining, TANGOFLUX was finetuned on the AudioCaps training set for 65 additional epochs. Several works find that sampling timesteps t from the middle of its range [0, 1] leads to superior results, thus, we sampled t from a logit-normal distribution with a mean"}, {"title": "3.2 DATASETS", "content": "Training dataset. We use complete open source data which consists of approximately 400k audios from Wavcaps and 45k audios from the training set of AudioCaps. For audios shorter than 30 seconds, we pad the remaining audio with silence. For audios longer than 30 seconds, we perform center cropping of 30 seconds. Since the audio files are all mono, we duplicated the channel to create \"stereo\" audio for compatibility with our model.\nCRPO dataset. We initialize the prompt bank as the prompts of AudioCaps training set, with a total of 45k prompts. At the start of each iteration of CRPO, we randomly sample 20k prompts from the prompt bank and generate 5 audios per prompt, and use the CLAP model to construct 20k preference pairs.\nEvaluation dataset. For the main results, we evaluated TANGOFLUX on the AudioCaps test set, using the same 886-sample split as . Objective metrics are reported on this subset. Additionally, we categorized AudioCaps prompts using GPT-4 to identify those with multiple distinct events, such as \"Birds chirping and thunder strikes,\" which includes \u201csound of birds chirping\u201d and \u201csound of thunder.\u201d Metrics for these multi-event captions are reported separately. Subjective evaluation was conducted on an out-of-distribution dataset with 50 challenging prompts."}, {"title": "3.3 OBJECTIVE EVALUATION", "content": "Baselines. We compare TANGOFLUX to four existing strong baselines for text-to-audio generation: Tango 2, AudioLDM 2, and Stable Audio Open, including the previous SOTA models. For all of our baseline evaluations, we use the default recommended classifier free guidance (CFG) scale and number of steps. For TANGOFLUX, we use a CFG scale of 4.5 and 50 steps for inference. Since TANGOFLUX and Stable Audio Open allow variable audio generation length, we set the duration conditioning to 10 seconds and use the first 10 seconds of generated audio to perform the evaluation. We also report the effect of CFG scale in the appendix A.1.\nEvaluation metrics. We evaluate TANGOFLUX using both objective and subjective metrics. For objective metrics, we report the 4 metrics: Fr\u00e9chet Distance (FDopenl3) , Kull-back-Leibler divergence (KLpasst), CLAPscore and Inception Score (IS) . We chose this set of metrics proposed by due to the capabilities to perform high-quality audio evaluation up to 48kHz. FDopenl3 evaluates the similarity between the statistics of a generated audio set and another reference audio set in the feature level space. A low FDopenl3 indicates that the generated audio is realistic and closely resembles the reference audio. KLpasst computes the KL divergence over the probabilities of the labels between the generated and the reference audio given the state-of-the-art audio tagger PaSST. A low KLpasst signifies the generated and reference audio share similar semantics tags. CLAPscore is a reference-free metric that measures the cosine similarity between the audio as well as the text prompt. High CLAP score score denotes the generated audio aligns with the textual prompt. IS measures the specificity and coverage for a set of samples. A high IS score represents the diversity of the generated audio. We use stable-audio-metrics to compute FDopenl3, KLpasst, CLAPscore and AudioLDM evaluation toolkit to compute IS. Note that we use different CLAP checkpoints to create our preference dataset (630k-audioset-best) and to perform the evaluation (630k-audioset-fusion-best)."}, {"title": "3.4 HUMAN EVALUATION", "content": "To evaluate the instruction-following capabilities and robustness of TTA models, we created 50 out-of-distribution complex captions, such as \"A pile of coins spills onto a wooden table with a metallic clatter, followed by the hushed murmur of a tavern crowd and the creak of a swinging door.\" These captions describe multiple events (ranging from 3 to 6 per caption) and go beyond conventional or overused sounds, such as simple animal noises, footsteps, or city ambiance. Events were identified using GPT4o to evaluate the captions generated. Each of the generated prompts contains multiple events including several where the temporal order of the events must be maintained. Details of our caption generation template and samples of generated captions can be found in the Appendix A.2.\nFollowing prior studies, our subjective evaluation focuses on two primary attributes of the generated audio: overall audio quality (OVL) and relevance to the text input (REL). The OVL metric evaluates the general sound quality, including clarity and naturalness, irrespective of the alignment with the input prompt. In contrast, the REL metric specifically measures the alignment of the generated audio with the provided text input. Annotators rated each audio sample on a scale from 0 (worst) to 100 (best) for both OVL and REL. This evaluation was performed on 50 GPT4o-generated prompts, with each sample independently assessed by at least four annotators.\nAdditional details on the evaluation instructions and annotators can be found in Appendix A.2."}, {"title": "3.4.1 \u039cETRICS", "content": "We report three key metrics for subjective evaluation:\nScores: The average of the scores assigned by individual annotators. Due to the subjective nature of these scores and the significant variance observed in the annotator scoring patterns, the ratings were normalized to z-scores at the annotator level: $z_{ij} = (S_{ij} \u2212 \u03bc_i)/\u03c3_i$. $Z_{ij}$: The z-score for annotator i's score of model $M_j$. This is the score after applying z-score normalization. $s_{ij}$: The raw score assigned by annotator i to model j. This is the original score before normalization. $\u03bc_i$: The mean score assigned by annotator i across all models. It represents the central tendency of the annotator's scoring pattern. $\u03c3_i$: The standard deviation of annotator i's scores across all models. This measures the variability or spread in the annotator's ratings.\nThis normalization procedure adjusts the raw scores, centering them around the annotator's mean score and scaling by the annotator's score spread (standard deviation). This ensures that scores from different annotators are comparable, helping to mitigate individual scoring biases.\nRanking: Despite z-score normalization, the variability in annotator scoring can still introduce noise into the evaluation process. To address this, models are also ranked based on their absolute scores. We utilize the mean (average rank of a model), and mode (the most common rank of a model) as metrics for evaluating these rankings.\nElo: Elo-based evaluation, a widely adopted method in language model assessment, involves pairwise model comparisons. We first normalized the absolute scores of the models using z-score normalization and then derived Elo scores from these pairwise comparisons. Elo score mitigates the noise and inconsistencies observed in scoring and ranking techniques. Specifically, Elo considers the relative performance between models rather than relying solely on absolute or averaged scores, providing a more robust measure of model quality under subjective evaluation. While ranking-based evaluation provides an ordinal comparison of models, determining the order of performance (e.g., Model A ranks first, Model B ranks second), it does not capture the magnitude of differences between ranks. For instance, if the difference between the first and second rankers is minimal, this is not evident from ranks alone. Elo scoring addresses this limitation by integrating both ranking and pairwise performance data. In ranking-based systems, the rank $R_i$ of a model $M_i$ is determined purely by its position relative to others: $R_i = position \\text{ of } M_i \\text{ in the sorted list of models based on performance}$. However, this approach fails to quantify: 1) The gap in performance between consecutive ranks. 2) The consistency of relative performance across different pairwise comparisons. Elo scoring provides a probabilistic measure of model performance based on pairwise comparisons. By leveraging annotator scores, Elo assigns a continuous score Ei to each model Mi, capturing its relative strength."}, {"title": "4 RESULTS", "content": ""}, {"title": "4.1 MAIN RESULTS", "content": "Table 1 compares TANGOFLUX with prior text-to-audio generation models on AudioCaps in terms of the objective metrics. Model performance on the prompts with more than one event, namely multi-event prompts, are reported in Table 2.\nThe results suggest that TANGOFLUX consistently outperforms the prior works on all objective metrics, except Tango 2 on KLpasst. Interestingly, the margin on CLAP score between TANGOFLUX and baselines is higher when evaluated on multi-event prompts. This suggests that TANGOFLUX excels at understanding and generating audio for complex instructions involving multiple events, effectively capturing nuanced details and relationships within the text compared to the baselines."}, {"title": "4.2 BATCHED ONLINE DATA GENERATION IS NECESSARY", "content": "To show the impact of generating new samples at each iteration, in Fig. 2 we present the results of 5 training iterations of CRPO, both with and without generating new data at each iteration. Our findings suggest that training on the same dataset over multiple iterations leads to quick performance saturation and eventual degradation. Specifically, for offline CRPO, the CLAP score decreases after the second iteration, while the KL increases significantly. By the final iteration, performance degradation is evident, with both the CLAP score and KL worse than the first iteration, emphasizing the limitations of using offline data. In contrast, the online CRPO with data generation at the beginning of each iteration consistently outperforms the offline CRPO in terms of both CLAP score and KL.\nA possible explanation of this performance degradation could be reward over-optimization . Previous work by demonstrated that the reference model serves as a lower bound in DPO training for language models. Several iterations of updating the reference model (lower bound) with the same dataset cause the current model to excessively minimize the loss in unexpected ways. In Section 4.5, we identify unexpected phenomena in loss minimization that could explain over-optimization. This over-optimization ultimately leads to performance degradation as shown by the spike in KL and drop in CLAP score."}, {"title": "4.3 CLAP AS REWARD MODEL", "content": ""}, {"title": "4.4 CRPO DATASET IS BETTER THAN OTHER AUDIO PREFERENCE DATASETS", "content": "To validate the effectiveness of CRPO in constructing preference datasets, we compared the performance of CRPO with two other audio preference datasets: Audio-Alpaca and BATON.\nBATON: BATON collects human-annotated data by asking labelers to assign a binary score of 0 or 1 to each audio sample based on its alignment with a given prompt. A score of 1 indicates alignment, while 0 indicates misalignment. From this data, we construct a preference dataset by pairing audio samples scored as 1 (winners) with those scored as 0 (losers) for the same prompt, creating a set of winner-loser pairs.\nAudio-Alpaca: Audio-Alpaca, in contrast, is already structured as a preference dataset, requiring no further processing.\nWe use the base model TANGOFLUX-base for preference optimization, conducting only one iteration since Audio-Alpaca and BATON are fixed datasets. Table 3 reports objective metrics FDopenl3, KLpasst, and CLAPscore, demonstrating that preference optimization with the CRPO dataset outperforms the other two audio preference datasets across all metrics. Despite its simplicity, CRPO proves highly effective for constructing audio preference datasets for optimization."}, {"title": "4.5 LCRPO VS LDPO-FM", "content": ""}, {"title": "4.6 INFERENCE TIME AND PERFORMANCE COMPARISON", "content": "We compare inference times, CLAP scores, and FD scores across models for steps 10, 25, 50, 100, 150, and 200, as shown in Figure 5. TANGOFLUX demonstrates a remarkable balance between efficiency and performance, consistently achieving higher CLAP scores and lower FD scores while requiring significantly less inference time compared to other models. For example, at 50 steps, TANGOFLUX achieves a CLAP score of 0.480 and an FD score of 75.1 in just 3.7 seconds. In comparison, Stable Audio Open requires 4.5 seconds for the same step count but only achieves a"}, {"title": "4.7 HUMAN EVALUATION RESULTS", "content": "The results of the human evaluation are presented in Table 5, with detailed comparisons of the models across the evaluated metrics: z-scores, rankings, and Elo scores for both overall audio quality (OVL) and relevance to the text input (REL). Below, we provide an analysis of the findings."}, {"title": "5 RELATED WORKS", "content": "Text-To-Audio Generation. TTA Generation has garnered attention lately due to models such as AudioLDM series model, Tango series model and Stable Audio series model. These models commonly adopt the diffusion framework , which trains a latent diffusion model conditioned on either T5 embedding or CLAP embedding. However, another common framework for TTA generation is the flow matching framework which was employed in models such as VoiceBox , AudioBox , FlashAudio .\nAlignment Method. Preference optimization is the standard approach for aligning LLMs, achieved either by training a reward model to capture human preferences or by using the LLM itself as the reward model . Recent advances improve this process through iterative alignment, leveraging human annotators to construct preference pairs or utilizing pre-trained reward models. Verifiable answers can enhance the construction of preference pairs. For diffusion and flow-based models, Diffusion-DPO shows that these models can be aligned similarly . However, constructing preference pairs for TTA remains challenging due to the lack of \"gold\" audio for given text prompts and the subjective nature of audio. Tango2 addresses this by using prompt perturbation, while BATON relies on human annotation to construct preference pairs which is not a scalable solution."}, {"title": "6 CONCLUSION", "content": "We introduce TANGOFLUX, a fast flow-based text-to-audio model aligned using synthetic preference data generated online during training. Objective and human evaluations show that TANGOFLUX produces audio more representative of user prompts than existing diffusion-based models, achieving state-of-the-art performance with significantly fewer parameters. Additionally, TANGOFLUX demonstrates greater robustness, maintaining performance even when sampling with fewer time steps. These advancements make TANGOFLUX a practical and scalable solution for widespread adoption."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 EFFECT OF CFG SCALE", "content": "We conduct an ablation of the effect of CFG scale for TANGOFLUX and show the result in Table 6. It reveals a trade-off: higher CFG values improve FD score (lower FD) but slightly reduce semantic alignment (CLAP score), which peaks at CFG=3.5. The results emphasize CFG=3.5 as the optimal balance between fidelity and semantic relevance."}, {"title": "A.2 HUMAN EVALUATION", "content": "The human evaluation was performed using a web-based Gradio app. Each annotator was presented with 20 prompts, each having four audio samples generated by four distinct text-to-audio models, shuffled randomly, as shown in Fig. 6. Before the annotation process, the annotators were instructed with the following directive:\nWelcome username\n# Instructions for evaluating audio clips\nPlease carefully read the instructions below.\n## Task\nYou are to evaluate four 10-second-long audio outputs to each of the 20 prompts below. These four outputs are from four different models. You are to judge each output with respect to two qualities:\n\u2022 Overall Quality (OVL): The overall quality of the audio is to be judged on a scale from 0 to 100: 0 being absolute noise with no discernible feature. Whereas, 100 is perfect. Overall fidelity, clarity, and noisiness of the audio are important here.\n\u2022 Relevance (REL): The extent of audio alignment with the prompt is to be judged on a scale from 0 to 100: with 0 being absolute irrelevance to the input description. Whereas, 100 is a perfect representation of the input description. You are to judge if the concepts from the input prompt appear in the audio in the described temporal order.\nYou may want to compare the audios of the same prompt with each other during the evaluation.\n## Listening guide\n1. Please use a head/earphone to listen to minimize exposure to the external noise.\n2. Please move to a quiet place as well, if possible.\n## UI guide"}]}