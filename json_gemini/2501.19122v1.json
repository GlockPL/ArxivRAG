[{"title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling", "authors": ["Hong Huang", "Hai Yang", "Yuan Chen", "Jiaxun Ye", "Dapeng Wu"], "abstract": "Federated Learning (FL) enables collaborative\nmodel training across distributed clients without\ndata sharing, but its high computational and com-\nmunication demands strain resource-constrained\ndevices. While existing methods use dynamic\npruning to improve efficiency by periodically ad-\njusting sparse model topologies while maintain-\ning sparsity, these approaches suffer from issues\nsuch as greedy adjustments, unstable topolo-\ngies, and communication inefficiency, result-\ning in less robust models and suboptimal perfor-\nmance under data heterogeneity and partial client\navailability. To address these challenges, we pro-\npose Federated Robust pruning via combinatorial\nThompson Sampling (FedRTS), a novel frame-\nwork designed to develop robust sparse models.\nFedRTS enhances robustness and performance\nthrough its Thompson Sampling-based Adjust-\nment (TSAdj) mechanism, which uses probabilis-\ntic decisions informed by stable, farsighted infor-\nmation instead of deterministic decisions reliant\non unstable and myopic information in previous\nmethods. Extensive experiments demonstrate that\nFedRTS achieves state-of-the-art performance in\ncomputer vision and natural language process-\ning tasks while reducing communication costs,\nparticularly excelling in scenarios with hetero-\ngeneous data distributions and partial client par-\nticipation.", "sections": [{"title": "1. Introduction", "content": "Federated learning (FL) (McMahan et al., 2017; Li et al.,\n2019; Kairouz et al., 2021) enables edge devices with private\nlocal data to collaboratively train a model without sharing\ndata. However, substantial computational and communica-\ntion demands for training deep learning models often exceed\nthe resource limitations of edge devices in cross-device FL\nscenarios. Although neural network pruning (Han et al.,\n2015; Molchanov et al., 2019a; Ma et al., 2021) offers a\npromising solution by removing redundant parameters, tradi-\ntional pruning methods depend on resource-intensive dense\nmodel training, rendering them impractical for privacy-\npreserved and resource-constrained FL environments.\nTo address these challenges, recent federated pruning frame-\nworks (Bibikar et al., 2022; Qiu et al., 2022; Tian et al.,\n2024; Jiang et al., 2022; Huang et al., 2024; 2023; Munir\net al., 2021) have adopted dynamic pruning techniques (Evci\net al., 2020; Raihan & Aamodt, 2020; Jayakumar et al.,\n2020) within FL. These frameworks employ a two-loop\ntraining process: in the inner loop, model weights are up-\ndated through standard FL rounds with fixed model topol-\nogy; in the outer loop, the server adjusts the model topology\nby pruning and reactivating parameters (Evci et al., 2020),\nas illustrated in Fig. 1 (left). This iterative process generates\na specialized sparse model without dense model training,\nsignificantly reducing computational costs.\nDespite these advancements, existing frameworks suffer\nfrom three critical challenges in model topology adjust-\nment (Bibikar et al., 2022; Qiu et al., 2022; Tian et al.,\n2024; Jiang et al., 2022; Huang et al., 2024; 2023; Munir\net al., 2021), as illustrated in Fig. 1 (left). 1). Greedy ad-\njustment: Current methods rely on myopic, aggregated\ninformation from a small subset of participating clients, ig-\noring data from the majority of unseen clients and prior\nknowledge. This leads to greedy adjustments and reduced\nrobustness (Kairouz et al., 2021). 2). Unstable topology:\nDeterministic adjustments based solely on aggregated infor-\nmation are prone to instability due to heterogeneous data\ndistributions, resulting in unstable model topologies. 3).\nCommunication inefficiency: Transmitting extensive aux-\niliary data (e.g., full gradient matrices) for topology updates\nimposes high communication costs. These limitations hin-\nder the ability of current methods to handle client availabil-\nity, data heterogeneity, and communication costs effectively,\nultimately leading to suboptimal performance and inefficient\nresource utilization.\nTo address these challenges, we reframe federated prun-"}, {"title": "2. Preliminary and Challenges", "content": "2.1. Federated Dynamic Pruning\nIn federated pruning, N resource-constrained clients col-\nlaboratively train a sparse model using their local datasets\nDn, n \u2208 {1,2,...,N}. Given the target density d', the\nobjective is to solve the following optimization problem:\n$\\min_{W,m}\\sum_{n=1}^{N}p_n.f_n (W, m, D_n), s.t. d \\leq d' ,$ (1)\nwhere W represents the model weights, m \u2208 {0,1}^{|W|}\ndenotes the sparse model topology (also called mask), fn (\u00b7)\nis the objective function for client n, d is the density of the\ntopology m, and pn is set as the proportion of the dataset\nsize of client n, typically set as $p_n = \\frac{|D_n|}{\\sum_{i=1}^{|N|}(|D_i|)}.$ \nTo solve the problem in Eq. 1 under the target density con-\nstraint d', existing federated pruning frameworks (Bibikar\net al., 2022; Qiu et al., 2022; Tian et al., 2024; Jiang et al.,\n2022; Huang et al., 2024; 2023; Munir et al., 2021) apply\ndynamic pruning techniques (Evci et al., 2020; Raihan &\nAamodt, 2020; Jayakumar et al., 2020) within FL. These\nframeworks iteratively adjust sparse on-device models dur-\ning training while maintaining the density level d. The\nprocess begins with a sparse model initialized via random\npruning and follows a two-loop training procedure: In the\ninner loop, the model topology m remains fixed, and the\nmodel weights are updated through traditional FL rounds.\nAfter AT inner loops, as illustrated in Fig. 1 (left), the\nframework enters the outer loop, where clients upload addi-\ntional data (specifically, the gradients of the pruned weights)"}, {"title": "2.2. \u0421\u041c\u0410\u0412-Based Problem Formulation", "content": "The Combinatorial Multi-Armed Bandit (CMAB) (Chen\net al., 2013; Slivkins et al., 2019; Kong et al., 2021) is a se-\nquential decision-making framework where an agent selects\ncombinations of arms (called super arms) with unknown re-\nward distributions. The objective is to maximize cumulative\nrewards by balancing the exploration of uncertain arms and\nthe exploitation of high-performing ones.\nWe formulate the topology adjustment as a CMAB problem,\nwhere each link mi in the model topology serves as an\narm. In each outer loop t for topology adjustment, the\nserver selects an action St, which includes K arms, where\nK = d' |W|. Selected arms i \u2208 St are activated (mt,i = 1),\nwhile others i \u2209 St are deactivated (mt,i = 0). After\nplaying action, the new sparse weights Wt = Wa99 *\nmt will be distributed to the clients for further training,\nwhere Wagg is the aggregated weights. The server then\nobserves outcomes Xt = (Xt,1,\u2026, Xt,m), drawn from\ndistributions with unknown expectation \u00b5. After that, the\nserver obtains the unknown rewards Rt = R(St, Xt). At\nthe next round t + 1, the previous information {X\u03c4 |1 <\n\u03c4 \u2264 t} is the input to the adjustment algorithm to select the\naction St+1. Following previous work (Kong et al., 2021),\nwe assume that the expected reward of an action St only\ndepends on St and and the mean vector \u00b5, i.e., there exists a\nfunction r such that E[Rt] = Ex\u2081 [R(St, Xt)] = r(St, \u00b5)."}, {"title": "2.3. Challenges in Previous Methods", "content": "From the CMAB aspect, after taking the action St, previous\nfederated pruning methods observe the delay outcomes Xt\nat the next outer loop t + 1, where Xt,i = 1 if i is the\nindex of the top magnitude of aggregated weights |Wt\u22121|\nor gradients |Gt\u22121|; otherwise, Xt,i = 0. Treating the\noutcomes {X} as the feedback, the next action St+1 is\nselected, i.e., St+1 = {i|Xt,i = 1}. Thereby these methods\nface three significant challenges:\nGreedy Adjustment: Existing frameworks only observe the\noutcomes in the outer loop, ignoring the inner loop. More-\nover, they discard all previous information {X\u03c4 |1 \u2264 \u03c4 \u2264\nt \u2212 1}, which is myopic, excluding information from un-\nseen clients and previous data, leading to a greedy topology\nadjustment that is difficult to exploit from a global view.\nUnstable Topology: Previous ignore the expectation of\ndistribution \u00b5 and make a deterministic decision based on\nunreliable outcomes Xt. It is hard to maximize the reward\nexpectation r(St, \u00b5) and may select an arm with a low ex-\npectation, leading to an unstable topology."}, {"title": "3. Methodology", "content": "In this section, we present FedRTS, a novel federated prun-\ning framework designed to develop robust model topologies,\nas illustrated in Fig. 2. We begin by detailing the TSAdj\nmechanism, which serves as the core component for search-\ning robust sparse topologies. Next, we provide an overview\nof the FedRTS algorithm that integrates TSAdj. Finally, we\nprovide the theoretical regret upper bound of TSAdj.\n3.1. Thompson Sampling-based Adjustment\nWe find that the core source of the aforementioned chal-\nlenges in existing federated dynamic pruning frameworks\nlies in deterministic decision strategies and myopic observa-\ntions. Therefore, we propose the Thompson Sampling based\nAdjustment (TSAdj) mechanism, which includes probabilis-\ntic decision strategy and farsighted observations. Moreover,\nTSAdj effectively reduces the communication overhead.\nCompared to directly using myopic outcomes to make deter-\nministic decisions, our proposed TSAdj maintains individual\nprior probability distributions P for each link in the model\ntopology m during training. The higher feedback a link i\nhas gained from previous observations, the closer the ex-\npectation of its probability E[Pi] approaches 1, indicating\na higher likelihood of being selected. In the outer loop for\ntopology adjustment, instead of deterministically selection\nbased on outcomes Xt, the server samples random variables\n\u03be ~ P and selects an action St on the topology as follows:\n$S_t = {i | \\xi_i \\in Top(\\xi, K)},$ (2)\nwhere Top(\u03be, K) denotes the set of top K elements of \u03be.\nThe action St is derived from the expectation of the prior\ndistribution, E[P], which provides a more stable topology\ncompared to existing approaches.\nUnlike existing approaches that develop the outcomes using\nimmediate and myopic data in the outer loop, TSAdj derives\nmuch more comprehensive outcomes Xt from both the inner\nand outer loops. After performing action St, the outcome\nXt is observed in the end of loop t, and is formulated as\na combination of individual outcomes X and aggregated"}, {"title": "Full-size\nWeights", "content": "ts, significantly reducing the communication\noverhead.\nTo construct a posterior probability distribution, TSAdj up-\ndates the prior distribution P based on outcomes Xt. Fol-\nlowing the Thompson Sampling framework, each Pi is mod-\neled as a Beta distribution with factors \u03b1i, \u03b2i, representing\nthe likelihood of selecting an arm into St: a higher \u03b1i shifts\nE[Pi] towards 1, whereas a higher \u03b2i shifts it towards 0. The\n(\u03b1i, \u03b2i) are updated via Bayes' rule, leveraging its conju-\ngacy properties and using the outcomes Xt,i as follows:\n$(\\alpha_i, \\beta_i) \\leftarrow (\\alpha_i + \\lambda X_{t,i}, \\beta_i + \\lambda (1 - X_{t,i})),$ $X_{t,i} \\neq None,$ (7)\nwhere X is a scaling factor that determines the influence of\noutcomes. Over time, the distribution gradually converges\ntowards the true distribution, aligning with the observed\noutcomes. The overflow of TSAdj is shown in Algorithm 1."}, {"title": "3.2. FedRTS", "content": "FedRTS is a novel federated learning framework designed\nto address the limitations of existing dynamic sparse train-\ning methods by integrating the Thompson Sampling-based\nAdjustment (TSAdj) mechanism, as illustrated in Fig. 2.\nThe framework begins with an initialized model weight and\nsparse topology sampled from Beta distributions P. Fe-\ndRTS employs a two-loop training process to iteratively\nupdate model weights and refine the sparse topology.\nIn the inner loop, the model topology remains fixed while\nthe server updates the model weights and distribution P\nbased on semi-outcomes. In the outer loop, the server col-\nlects the full outcomes by requiring clients to upload the\ntop-k gradient indices of inactivated weights. Using this in-\nformation, the server applies TSAdj to make robust topology\nadjustments, and the updated topology is then distributed to\nclients. FedRTS continues this iterative process until model\nconvergence. The details of FedRTS are shown in Algo. 2\nand Sec. B.2.\nBy integrating TSAdj, FedRTS introduces a novel approach\nto sparse topology adjustment. Leveraging Thompson Sam-\npling, it mitigates the instability caused by unstable aggres-\nsive topology, ensuring smoother convergence through prob-\nabilistically guided adjustments based on historical infor-\nmation. This adaptive adjustment strategy enhances the ro-\nbustness of the sparse topology and improves overall model\nperformance in FL tasks."}, {"title": "3.3. Theoretical Analysis", "content": "For simplifications, we omit the inter loop of FedRTS\nto isolate the TSAdj mechanism and consider only semi-\noutcomes, i.e., ignore the outcomes Xti that i \u2209 St. We\nalso consider the following assumption and approximation:\nAssumption 3.1. (Independent Magnitude) Magnitude of\nmodel weights are mutually independent.\nAssumption 3.2. (Threshold Approximation) The discrimi-\nnative function h(\u00b7) in Eq. 4 and 5 can be approximated by\na threshold \u03c3, i.e., h(i, x, \u03ba) \u2248 1x\u2265\u03c3\u00b7\nAssumption 3.1 aligns with standard magnitude pruning\npractices, while Assumption 3.2 holds empirically, as\nweights often exhibit a Gaussian-like distribution under\nL2 regularization and normalization techniques. Therefore,\nunder assumptions 3.1 and 3.2, the outcomes Xt,i become:\n$X_{t,i} = {1|W_{t,i}|\\geq \\sigma + (1 - \\gamma) \\sum_{n}^{N}p_n 1|W_{n,i}|\\geq \\sigma, i \\in S_t,$ (8)"}, {"title": "communication overhead.", "content": "which are mutually independent. We also include the Lips-\nchitz assumption 3.3 as in previous work (Kong et al., 2021).\nAssumption 3.3. (Lipschitz continuity) \u2203L \u2208 R, \u2200S, \u00b5, \u00b5'\nsatisfies |r(S, \u00b5) \u2013 r(S, \u00b5')| \u2264 L||\u00b5 \u2013 \u00b5' ||1.\nThe cumulative regret Reg(T) is defined as:\n$Reg(T) = E\\sum_{t=1}^{T} \\Delta_{S_t},$ (9)\nwhere \u2206S\u2081 = max{r(S*, \u00b5) \u2013 r(St, \u03bc), 0} and S* is the\ngreedy optimal action based on \u00b5, which is obtained by\nAlgo. 3. The maximum reward gap is set as \u2206max =\nmaxs \u2206s. The maximum reward gap of actions containing\narm s is \u2206max = maxs:ses \u2206s. We define S* and St are\nsequences, i.e., S* = (s\u2081, ..., sk), St = (st,1,..., st,K),\nand s\u2208 S* is the k-th element added to S*. We denote\nS= (s,...,s) and St,k = (st,1,...,st,k). For any\narm s, define the marginal reward gap \u2206s,k = r(S, \u00b5) \u2013\nr(S-1\u222a {s}, \u03bc).\nInspired by previous work in combinatorial Thompson sam-\npling (Chen et al., 2013; Kong et al., 2021), we provide a\nregret upper bound for TSAdj.\nTheorem 3.4. (Upper Bound) Under assumptions 3.1, 3.2\nand 3.3 and with outcomes Xt defined in Eq. 8, the regret\nReg(T) of TSAdj can be upper bounded by:\n$Reg(T) \\leq \\sum_{s\\neq s} max_{k: s \\notin S} (\\frac{6\\Delta_{max} L^2 log T}{(\\Delta_{s,k}- 2BK\\epsilon)^2}) \\\\+ (2K + 4|\\mathcal{W}| + \\frac{K U + 8K \\epsilon^4}{\\epsilon^6}) \\Delta_{max} = \\mathcal{O} (\\sum_{s \\neq s} max_{k: s \\notin S} (\\frac{\\Delta_{max} L^2 log T}{\\Delta_{s,k}^2}) \\Delta_{max}$ (10)\nfor any \u03f5 such that \u2200s \u2260 s\u2081 and s \u2209 S, \u2206s,k > 2BK\u03f5,\nwhere U is a universal constant.\nThe proof detail is provided in Sec. C.2."}, {"title": "5. Conclusion", "content": "In this paper, we analyze federated pruning from the per-\nspective of combinatorial multi-armed bandits and identify\nkey limitations in existing methods, including greedy adjust-\nments, unstable topologies, and communication inefficien-\ncies. To address these challenges, we propose Federated\nRobust Pruning via Combinatorial Thompson Sampling (Fe-\ndRTS), a novel framework for developing robust sparse\nmodels. The core component of FedRTS, the Thompson\nSampling-based Adjustment (TSAdj) module, enhances ro-\nbustness and performance by making probabilistic decisions\nbased on stable, farsighted information rather than deter-\nministic decisions driven by unstable, myopic observations.\nWe provide a theoretical regret upper bound for TSAdj and\nconduct extensive experiments, demonstrating that FedRTS\nachieves state-of-the-art performance in both computer vi-\nsion and natural language processing tasks."}, {"title": "A. Related Work", "content": "A.1. Neural Network Pruning\nNeural network pruning (Mozer & Smolensky, 1988; LeCun\net al., 1989; Janowsky, 1989) optimizes network structures\nby removing redundant components like connections (Han\net al., 2015), neurons (Yu et al., 2018), or filters (Li et al.,\n2016). Traditional pruning methods focus on inference, re-\nquiring dense training followed by importance-based prun-\ning and retraining. These importance scores are derived\nfrom weight magnitudes (Janowsky, 1989; Han et al., 2015),\ngradients (Mozer & Smolensky, 1988; Molchanov et al.,\n2019b), Fisher information (Singh & Alistarh, 2020), and\nother variants (Louizos et al., 2018; Yu et al., 2018). How-\never, training a dense model first increases computational\nand memory costs.\nTherefore, Recent works propose dynamic sparse train-\ning (Mocanu et al., 2018; Dettmers & Zettlemoyer, 2019;\nEvci et al., 2020) to bypass the dense training by iteratively\nadjusting sparse topologies during training while maintain-\ning a fixed model size. Yet, existing methods rely on deter-\nministic adjustments, leading to high variance and resource-\nintensive searches, exacerbated in federated learning under\ndata heterogeneity and partial client availability. To address\nthis, we propose TSAdj, which employs probabilistic adjust-\nments for a more robust and stable model topology."}, {"title": "A.2. Federated Pruning", "content": "Federated Pruning is a technique that applies neural net-\nwork pruning within the Federated Learning (FL) process.\nTo avoid the requirements of dense training in traditional\npruning methods, some previous methods determine the\nsparse model topology before federated training and do not\nadjust it during the FL process, such as SNIP (Lee et al.,\n2018) and Synflow (Tanaka et al., 2020). This approach of\npruning at initialization may overlook crucial data informa-\ntion, potentially leading to suboptimal model structures.\nDynamic sparse training methods, such as those employed\nby PruneFL (Jiang et al., 2022), adjust the model dur-\ning FL rounds. However, these methods require the en-\ntire model gradient to be sent to the server to ensure final\nperformance, resulting in high communication costs. Fed-\nDST (Bibikar et al., 2022) and FedSGC (Tian et al., 2024)\nalleviate this issue by applying dynamic sparse training on\nclients. FedTiny (Huang et al., 2023) and FedMef (Huang\net al., 2024) require clients only to upload the Top gradients.\nDespite these advancements, these methods adjust the model\ntopology based on limited information from the currently\nselected clients, leading to issues of greedy adjustment and\nunstable topology. FedRTS addresses these challenges by\napplying TSAdj, significantly mitigating the problems as-\nsociated with limited client information and enhancing the"}, {"title": "A.3. Combinatorial Multi-Arm Bandit", "content": "The Combinatorial Multi-Armed Bandit (CMAB) problem\nextends the classical MAB to combinatorial settings, where\nrewards depend on selected arm combinations. (Gai et al.,\n2012) introduced the CMAB framework with linear rewards\nand proposed a UCB-type algorithm that allows approxi-\nmation oracles for decision-making. This work established\nthe foundation for CMAB analysis. (Chen et al., 2013;\n2016) and (Wang & Chen, 2017) generalized the CMAB\nframework by introducing probabilistically triggered arms\nand relaxing reward assumptions to monotonicity. They pro-\nposed the CUCB algorithm, achieving regret bounds with\napproximation oracles and enabling broader applications of\n\u0421\u041c\u0410\u0412.\nDespite these advancements, UCB-based algorithms, such\nas CUCB, rely on assumptions like monotonicity for reward\nfunctions. However, these assumptions may not hold in\nmore complex scenarios, such as non-monotonic rewards\nor high-dimensional combinatorial action spaces. (Wang\n& Chen, 2018) pointed out that these limitations restrict\nthe practicality of UCB algorithms, particularly in settings\nwhere reward functions are highly nonlinear or lack smooth-\nness.\nThompson Sampling (TS) offers greater flexibility by avoid-\ning monotonicity. TS has been shown to achieve re-\ngret bounds comparable to CUCB, as demonstrated by\n(Komiyama et al., 2015) and (Wang & Chen, 2018), while\n(Perrault et al., 2020) further improved its performance co-\nefficients. (Kong et al., 2021) extended the theoretical\nunderstanding of CMAB by conducting a detailed hardness\nanalysis on regret and bounds when using greedy oracles.\nTheir study quantified the impact of greedy oracles on re-\ngret and provided analytical results for scenarios where\nexact solutions are unavailable. Additionally, their analysis\naddressed the convergence of TS in combinatorial settings,\nhighlighting its robustness in handling complex reward struc-"}, {"title": "B. Algorithm", "content": "B.1. TSAdj\nIn each loop t, the aggregated weights Wa99 will be cal-\nculated in each loop when the server receives the weights\nW+1 from each client n \u2208 Ct, and TSAdj is invoked to\nadjust the sparse topology mt only in the outer loop. The\noutcomes Xt,i for link mi are computed as described in\nEquation 3, which balances global and client-specific con-\ntributions. Specifically, outcomes X99 and X are calcu-\nlated based on Equations 4 and 5 on the server, using the"}, {"title": "C. Theoretical Details", "content": "C.1. Greedy Optimal Action\nAlgorithm 3 Greedy Optimal Action\nInput: The mean vector \u00b5 and action size K\nReturn: The greedy optimal action S*\nInitialize: S* = \u00d8\nfor k \u2208 1,..., K do\nS* = S* \u222a {argmaxi\u2260s*r(S* \u222a {i}, \u03bc)}\nend for"}, {"title": "C.2. Proof of Theorem 3.4", "content": "The cumulative regret Reg(T) is defined as:\n$Reg(T) = E\\sum_{t=1}^{T} \\Delta_{S_t},$ (11)\nwhere \u2206S\u2081 = max{r(S*, \u00b5) \u2013 r(St, \u03bc), 0} and S* is the\ngreedy optimal action based on \u00b5, which is obtained by\nAlg. 3. The maximum reward gap is set as \u2206max =\nmaxs \u2206s. The maximum reward gap of actions containing\narm s is \u2206max = maxs:ses \u2206s. We define S* and St are\nsequences, i.e., S* = (s\u2081, ..., sk), St = (st,1,..., st,K),\nand s\u2208 S* is the k-th element added to S*. We denote\nS=(s,...,s) and St,k = (st,1,...,st,k). For any\narm s, define the marginal reward gap \u2206s,k = r(S, \u03bc) \u2013\nr(S-1\u222a {s}, \u03bc)."}, {"title": "Communication Cost", "content": "Theorem. (Upper Bound) Under assumptions 3.1, 3.2\nand 3.3 and with outcomes defined in Eq. 8, the regret\n$Reg(T) \\leq \\sum_{s\\neq s} max_{k: s \\notin S} (\\frac{6\\Delta_{max} L^2 log T}{(\\Delta_{s,k}- 2BK\\epsilon)^2}) \\\\+ (2K + 4|\\mathcal{W}| + \\frac{K U + 8K \\epsilon^4}{\\epsilon^6}) \\Delta_{max} = \\mathcal{O} (\\sum_{s \\neq s} max_{k: s \\notin S} (\\frac{\\Delta_{max} L^2 log T}{\\Delta_{s,k}^2}) \\Delta_{max}$ (12)"}, {"title": "Communication Overhead", "content": "for any \u03f5 such that \u2200s \u2260 s\u2081 and s \u2209 S, \u2206s,k > 2BK\u03f5,\nwhere U is a universal constant.\nProof. Our proof follows (Kong et al., 2021). For any arm\ns\u2260 s\u2081, we apply the exploration price F(i) as\n$F(s) = max_{k: s \\notin S} (\\frac{6L^2logT}{(\\Delta_{s,k}- 2LK\\epsilon)^2}$ (13)\nTherefore, at t-th iteration, there are two event\n$A_t := { \\exists i \\in {1,..., |\\mathcal{W}|} \\Rightarrow | \\hat{\\mu}_{t,i} - \\mu_{i} |^2 > \\frac{3 log T}{2N_{t,i}} },$ (14)\n$B_t := { \\exists i \\in {1,..., |\\mathcal{W}|} \\Rightarrow | \\hat{\\mu}_{t,i} - \\mu_{i} |^2 > \\frac{3 log T}{2N_{t,i}} },$ (15)\nwhere Nt,s = \u03a3=11s\u2208s, is the number of oberserva-\ntions of arms and \u00b5t,s = \u039d. \u03a3\u03c4=1ses, X7,5 is the\nempirical mean outcome of s at the start of round t.\nBased on Eq. 14 and 15, the regret Reg(T) can be devided\ninto three terms as\n$Reg(T) \\leq E \\sum_{t=1}^{T} 1_{At \\cap \\overline{B_t}} \\Delta_{S_t} + E \\sum_{t=1}^{T} 1_{\\overline{At}} \\Delta_{St}  + E \\sum_{t=1}^{T} 1_{Bt \\overline{St}}  $ (16)"}, {"title": "Communication and Computational Cost", "content": "We provide the Lemma C.1, C.2 and C.3 to bound these\nthree terms one by one. Therefore, we can obtain\n$Reg(T) \\leq \\sum_{s\\neq s} max_{k: s \\notin S} (\\frac{6\\Delta_{max} L^2 log T}{(\\Delta_{s,k}- 2BK\\epsilon)^2}) + (2K + 4|\\mathcal{W}| + \\frac{K U + 8K \\epsilon^4}{\\epsilon^6}) \\Delta_{max}$ (17)"}, {"title": "Communication Channel", "content": "Lemma C.1. Under all assumptions and settings in Theo-"}, {"title": "Computer Networks", "content": "rem 3.4, we have\n$\\sum_{s \\neq s} (\\frac{\\Delta_{max} L^2 log T}{(\\Delta_{s,k}- 2BK\\epsilon)^2}) $ (18)"}, {"title": "Communication Complexity", "content": "Proof. To bound this term, we should study the difference\nbetween st,k and s, that is, this term can be bounded by\n$E [\\sum_{t=1}^{T} 1_{At \\cap \\overline{B_t}} \\Delta_{St} \\leq  \\sum_{k=1}^{K} E[\\sum_{t=1}^{T} 1_{At \\cap \\overline{B_t} \\cap C_{t,k} \\Delta_{S_t} + \\sum_{t=1}^{T} 1_{At \\cap \\overline{B_t} \\cap \\overline{C_{t,k}} } ]$ (19)\nwhere Ct,k denotes as an event which is defined as\n$C_{t,k} := {S_{t-1} = S^*_{t,k-1} \\cap s \\neq s_{t,k} \\cap ||E_{t,S_{t-1}} - M_{S_{t-1}}||_{\\infty} \\leq \\epsilon}.$ (20)\nWe define Ft,s = {\u00b5t,ii \u2208 S} and \u03bc\u03b5 = {\u00b5\u03b5\u03af \u2208 S}.\nAccording to Lemma C.4, the first term in Eq. 19 can be\nbounded by:\n$\\sum_{k=1}^{K} E [\\sum_{t=1}^{T} \\Delta_{S_{t}} 1_{At \\cap \\overline{B_t} \\cap C_{t,k} \\Delta_{S_t}  \\leq \\sum_{k=1}^{K} E[\\sum_{t=1}^{T} \\sum_{s \\notin \\mathcal{S^*}} \\Delta_{S_t} 1_{A_t^{S = S_{t,k} \\cap  N_{t,s} <  F(s) \\cap  N_{t,s} + \\sum_{k=1}^{T} E [\\sum_{t=1}^{T} \\sum_{s \\notin \\mathcal{S^*}} 1_{A_t^{S = S_{t,k} \\cap  N_{t,s} <  F(s) }$"}, {"title": "Communication over Wireless", "content": "Therefore, the\n$\\sum_{k=1}^{K} E [\\sum_{t=1}^{T} 1_{\\hat{S} = S_{t,k}} |"}, {"title": "Communication Technologies", "content": "Based on Lemma C.5, the\n$\\sum_{i=1}^{T} B_{i,k}^r \\cup  1_{A_t^{S = S_{t,k} | +  (2_k + \\frac{K^V + 8K\\epsilon}{\\epsilon^3"}, "Delta_{max}.$"]}, {"title": "Communication Networks", "content": "Combining Eq. 21 and Eq. 22, we have\n$E [\\sum_{t=1}^{T} 1_{At \\cap \\overline{B_t}} \\Delta_{St}  \\leq \\sum_{s \\neq s} \\sum_{k} \\Delta_{max}( \\frac{6U K^1 L^2 log T}{(\\Delta_{s,k} - 2BK \\epsilon})^2})$\\+ (2K + \\frac{K U + 8K\\epsilon}{\\epsilon^3"}, {}, {"title": "Communications", "content": "Lemma C.2. Under all assumptions and setting in Theo-\nrem 3.4, we have\n$E [\\sum_{t=1}^{T} 1_{At \\cap \\overline{B_t}} \\Delta_{St}  \\leq 2A max(W). $ (24)"}, {"title": "49 and"}]