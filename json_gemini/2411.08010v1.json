{"title": "ExpressivityArena: Can LLMs Express Information Implicitly?", "authors": ["Joshua Tint", "Som Sagar", "Kelly Raines", "Bimsara Pathiraja", "Aditya Taparia", "Caleb Liu", "Ransalu Senanayake"], "abstract": "While Large Language Models (LLMs) have demonstrated remarkable performance in certain dimensions, their ability to express implicit language cues that human use for effective communication remains unclear. This paper presents ExpressivityArena, a Python library for measuring the implicit communication abilities of LLMs. We provide a comprehensive framework to evaluate expressivity of arbitrary LLMs and explore its practical implications. To this end, we refine the definition and measurements of \"expressivity,\u201d and use our framework in a set of small experiments. These experiments test LLMs in creative and logical tasks such as poetry, coding, and emotion-based responses. They are then evaluated by an automated grader, through ExpressivityArena, which we verify to be the most pragmatic for testing expressivity. Building on these experiments, we deepen our understanding of the expressivity of LLMs by assessing their ability to remain expressive in conversations. Our findings indicate that LLMs are capable of generating and understanding expressive content, however, with some limitations. These insights will inform the future development and deployment of expressive LLMs. We provide the code for ExpressivityArena alongside our paper.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (OpenAI, 2023; Touvron et al., 2023) are disrupting many domains where human communication is essential, including education (OpenAI, 2023), customer support (Radford et al., 2019), legal services (Chern et al., 2024), and healthcare (Bubeck et al., 2023). Increasing parameter count in LLMs has resulted in better performance in a multitude of downstream tasks such as language translation, text summarizing, and question-answering (Devlin et al., 2018; Brown et al., 2020). This performance is typically measured in terms of the number of errors (OpenAI, 2023), contextual understanding (Brown et al.,"}, {"title": "2 Related Work", "content": "In this section, we explore scattered literature for definitions of expressivity related to natural language processing in order to formalize our own definition of expressivity. Most methods that delve into expressivity of language models typically focus on emotions, as studied in affective computing devices (Picard, 2000). This includes recognizing emotions from language or facial expressions and body language (Plaza-del Arco et al., 2024). In social robotics, facial expressions on robots act as a method of communicating emotion and personality (Wang et al., 2024; Venture and Kuli\u0107, 2019). However, this limited focus of emotions on expressivity does not capture other aspects that we use in our day-to-day communication. Our study focuses on diverse aspects of expressivity, ranging from emotions to computer programming paradigms.\nIn particular, we adapt a definition from linguistics, to term \u201cexpressivity\" as the state of communicating information implicitly: showing, not telling (Sanders; and Taniguchi, 2022). To further clarify, Yus offers a framework for distinguishing implicit and explicit communication: implicit information must be derived by the interlocutor, using contextual or pragmatic information (Yus, 1999). This is in contrast to explicit communication, which is represented immediately in the semantics of text. For instance, the words \"cheap\" and \"affordable\" may have the same literal meaning, but \u201ccheap\" may have a more negative connotation. The word \"greetings\" might communicate a more formal context than \"hello.\u201d However, these meanings must be interpreted by the listener or reader in context to be understood. Given that LLMs may struggle with contextual understanding, studying expressivity provides a lens to explore the limitations of language models (Zhu et al., 2024)."}, {"title": "3 Expressivity Arena", "content": "ExpressivityArena is a Python-based framework that allows for simple, scalable, and flexible testing of LLM expressivity. To measure whether a piece of information was correctly conveyed implicitly in a piece of LLM-generated text, Expressivity Arena implements an experiment which tests whether a grader can accurately guess the implicitly conveyed information from the original text.\nIn order to perform an expressivity experiment in ExpressivityArena, the user first specifies an LLM, $f_{test}(x_{in})$, that takes a user prompt, $x_{in}$, to generate a model response, $x_{out}$. The user prompt must contain two critical instructions: a domain, $d$, and an expressive signal, $s$. The domain $d$ is simply a string naming the context in which the text must be written. An example might be a \u201csong\u201d or a \"recipe.\" In order to test a given signal against alternatives, the user then defines a signal category. The signal category, $S_c$, is a set which contains various expressive signals that each will be tested. The elements of the signal category set, $s \\in S_c$, should belong to the same qualitative category, for instance a set of emotions, or a set of genres. For each signal $s$, the language model will be prompted to generate a piece of text in the domain $d$ expressing the signal $s$. A complete prompt takes the form of:\n\u201cPlease write a (d) which conveys (s), without using o not explicitly mention (s) in your response. Do not also convey any of the following signals: ($S_c \\setminus s$)\"\nWe iterate this prompt for all $s \\in S_c$. For instance, the user may prompt the model: $x_{in}$ = \u201cPlease write a \u3008letter\u3009 which conveys \u3008patriotism\u3009\u201d or $x_{in}$ = \u201cPlease write a \u3008 short story \u3009 which conveys \u3008a Daoist philosophy\u3009.\u201d The response $x_{out} = f_{test}(x_{in})$ is then collected. To avoid unintentionally leaking s in the response, if $x_{out}$, contains an explicit mention of the signal s, the response will be regenerated.\nOnce the response has been generated, it is then given to a blind grader, another LLM, $f_{grader}(x_{out})$, that is unaware of the original prompt. The grader is then asked to guess, out of a set of all possible sig-"}, {"title": "4 Experiments", "content": "Each of our experiments was conducted on a off-the-shelf laptop with a midrange GPU."}, {"title": "4.1 Experiment 1: Grader Validation", "content": "We use an LLM as an automated grader in ExpressivityArena. This enables test far more samples than would be possible with a human grader, as well as domains requiring specialized knowledge, such as paradigms. However, because ultimately expressivity is understood by humans, in order for an automated grader to be useful, it must perform at a comparable accuracy to a human grader. Experiment 1 is therefore designed to validate this use of an automated grader. LLMs have been successful in evaluating other LLMs for other tasks (Lee et al., 2023; Bai et al., 2024; Chern et al., 2024), so we expect them to be similarly successful when evaluating expressivity. This experiment will also inform high-quality grader selection, ensuring that ExpressivityArena results reflect the LLM being graded and not the grader itself.\nWe begin by having a set of LLMs generate"}, {"title": "4.2 Experiment 2: Single-Prompt Scenarios", "content": "The purpose of experiment 2 is to answer RQ1: Are LLMs capable of exhibiting expressivity? To this end, we consider single-prompt scenarios-the user prompts only once and $f_{test}()$ generates a single response without back and forth communication. We evaluate two domains, these being poetry generation and code generation."}, {"title": "4.2.1 Poetry Generation", "content": "Poetry, as a highly expressive domain, serves as a testbed for assessing LLMs' expressive capabilities. We evaluate LLM performance across two signal types: emotion and writing style.\nFor the emotion category, we use the set of emotions from the GoEmotions dataset (Demszky et al., 2020) as our set of signals. This set includes 28 different emotions. 30 different poems were generated for each emotion as a signal. The grader was then prompted to choose, from the full set of emotions, which one was expressed.\nwere typically emotions with similar semantics. However,"}, {"title": "4.2.2 Code Generation", "content": "As opposed to poetry, programming is not traditionally considered an expressive domain. This gives us a way of understanding how LLMs perform in low-expressivity domains. We studied expressivity in two subcategories for program generation: skill level and programming style: two features that are implicitly shown in programs that could be inferred by a skilled programmer. Both experiments were structurally similar to poetry generation. The model was prompted to provide a Python program which would print out the Fibonacci numbers in order, while also expressing a particular constraint. The resulting program was then evaluated by an automated grader which guessed the signal expressed in the program. Python was chosen for this task as it is a multiparadigm language that facilitates the expression of many distinct programming styles."}, {"title": "4.3 Experiment 3: Conversations", "content": "In experiment 3, we answer RQ2: Can LLMs remain expressive through the course of a conversation? We conversations between professions, as defined in Appendix A.2, and emotions, as defined in Appendix A.2. To evaluate conversational skills, we assign a specific profession as signals to a LLM and facilitate a dialogue between two such models. For emotional signals, we utilize the set of emotions from the GoEmotions dataset (Demszky et al., 2020), for profession signals, we selected a list of professions as detailed in the Appendix A.2. In both experiments, the LLMs are configured to avoid explicitly stating the emotion or profession they have been assigned.\nFor each domain, we developed ExpressivityArena to join the output of two LLMs so that they could read each others responses as though they were in a conversation. This conversation was segmented into time steps, where each model responded to the output generated at the previous step by the other model. At the beginning of the conversation we apply prompting to each LLM to implicitly express a particular signal from the chosen domain. We then allow the LLMs to communicate with one another for a chosen number of iterations. After the conversation has completed, we use the grader to analyze each LLM's response at each time step to obtain an expressivity rate. We then compare expressivity rate values at each time step to understand whether they change over time.\nWhen prompting LLMs to express emotions in conversation,"}, {"title": "5 Discussion", "content": "Experiment 1 revealed that top automated graders matched human proficiency in identifying implicit signals, occasionally even outperforming humans. This could be due to automated graders' better alignment with LLM associations or human fatigue during evaluation (Boksem et al., 2005). The highest performing models were GPT-4 and GPT-40,\nIn experiment 2, we utilized ExpressivityArena to evaluate the expressivity of LLMs, in logical and creative domains. In more logical areas such as code, expressivity becomes a correctness issue if it cannot write code in similar paradigms or skill-level styles, making it more difficult to integrate with existing programs. Notably, in programming tasks, expressivity rates were consistently low, despite there being fewer possible labels in those experiments. This may be because code is a less expressive domain. In particular, emulating a particular skill level had the lowest expressivity rate. All models had their outputs consistently rated as having a lower skill level than they were prompted to create. This has implications for the application of LLMs to code generation; our results suggest that LLMs may be less able to write code matching a particular style than they would be with natural language. In use cases such as generating idiomatic code in a particular language, or generating code to match the style of an existing module, this may become an issue.\nIn the poetry domain, confusion between female poets impacted model accuracy, indicating potential bias in expressivity. For instance, the models struggled to differentiate between poets like Emily Dickinson and Sappho, possibly due to overgeneralization based on gender and underrepresentation in training data (Dong et al., 2024). This suggests that bias may negatively impact expressivity. Certain emotions in experiment 2 were also frequently confused, typically ones with similar semantics. However, when any GPT model was prompted to give a poem expressing disapproval, the output was most often identified as expressing approval. This was a significant instance where two emotions of conflicting meanings were frequently confused. As a whole, models performed best on the emotion category. This may be because emotions are more commonly expressed in conversations than poetic styles, meaning that each model had more training data to draw on. Yet, there remains significant concern for the expressiveness of current models in this area as it confuses two quite drastically contrasting emotions.\nIn simulated LLM conversations, models showed a decline in emotional expressivity over time, possibly due to prioritizing neutrality in pro-"}, {"title": "6 Conclusion", "content": "As LLMs become more integrated into daily life, understanding their capabilities and limitations is crucial for harnessing their full potential. One key area is expressivity-the ability to convey implied information-which is essential for natural communication. ExpressivityArena provided a platform to analyze expressivity in single-prompt responses and LLM-LLM conversations. Our experiments show that while LLMs can communicate implicitly to a degree, their expressivity rate remains around 30-60%, indicating room for improvement. This may be due to biases like race, age, and sex underrepresentation. Since expressivity in LLMs is important to communicate with humans effectively, we believe ExpressivityArena and our findings will help to improve LLMs to convey more complex or abstract concepts properly. Future research will garner further understanding and possible methods to increase expressivity in LLMs, requiring expertise from several areas such as linguistics, psychology, and machine learning."}, {"title": "Limitations", "content": "Though we do not see alternatives to grade, our use of an automated grader introduces several limitations into our method. Experiment 1 suggests that automated graders are not less perceptive of expressive signals than human graders, but they still may grade in qualitatively different ways that may introduce degrees of bias. Without a more comprehensive comparison of human and automated graders, it would be difficult to discern whether there are certain kinds of signals that automated graders are less sensitive towards. The very fact that the grader we chose, GPT-40, outperformed the average human grader may show that it is oversensitive to expressive signals. However, the overperformance is not dramatic in any case, and there are persuasive reasons why an LLM grader is required. Surveying a set of humans with the requisite poetic or coding knowledge to complete our Experiment 2 would be next-to-impossible, and less reproducible.\nOur initial experiments are focused on validating our method and testing ExpressivityArena in a variety of contexts; they do not constitute a benchmark nor a comprehensive ranking of LLMs in expressivity. In order to form such an expressivity benchmark, far more domains would need to be tested on more samples. The design and execution of this is left as future work. ExpressivityArena is presented as a utility to evaluate LLMs in sets of user-defined signals.\nIn this study, we use the multiple choice metric, which has been shown to be nonlinear and discontinuous for NLP tasks (Schaeffer et al., 2023). However, because we are not investigating emergent expressive ability in LLMs, accuracy is still a suitable metric for comparison of models. Future work may consider using Brier Scoring to study emergent expressive capabilities of model families (Shao et al., 2024)."}, {"title": "Ethical Considerations", "content": "Since our paper is a generic algorithmic evaluation, we do not foresee direct negative societal impacts. Human graders who were surveyed for experiment 1 were all given a privacy statement notifying them of their confidentiality and of the purpose of the experiment. No identifying information was solicited or collected. The statement read as follows:\nThank you for considering participation in our survey. Please read the following information carefully before proceeding."}]}