{"title": "Hateful Meme Detection through Context-Sensitive Prompting and Fine-Grained Labeling", "authors": ["Rongxin Ouyang", "Kokil Jaidka", "Subhayan Mukerjee", "Guangyu Cui"], "abstract": "The prevalence of multi-modal content on social media complicates automated moderation strategies. This calls for an enhancement in multi-modal classification and a deeper understanding of understated meanings in images and memes. Although previous efforts have aimed at improving model performance through fine-tuning, few have explored an end-to-end optimization pipeline that accounts for modalities, prompting, labeling, and fine-tuning. In this study, we propose an end-to-end conceptual framework for model optimization in complex tasks. Experiments support the efficacy of this traditional yet novel framework, achieving the highest accuracy and AUROC. Ablation experiments demonstrate that isolated optimizations are not ineffective on their own.", "sections": [{"title": "Introduction", "content": "Recent years have seen a significant increase in visual content on social media (Peng, Lu, and Shen 2023; Heley, Gaysynsky, and King 2022), particularly visual misinformation (Yang, Davis, and Hindman 2023). Up to 30% of the content on platforms like X includes images or videos (Pfeffer et al. 2023), highlighting the need for a multi-modal research on social media.\nHowever, while increasingly more studies have recognized the visual moderation challenge (Gonz\u00e1lez-Aguilar, Segado-Boj, and Makhortykh 2023; Solea and Sugiura 2023), most prior work has either unimodal (Muddiman, McGregor, and Stroud 2019), or focused on fine-tuning only (Lippe et al. 2020; Hermida and Santos 2023). Prior work on prompt engineering (Furniturewala et al. 2024) indicates the relative advantage of multi-stage prompts that act to pre-empt biases in Large Language Model (LLM) outputs. Yet, these studies focus on unimodal content, and it is unclear whether using multi-stage prompts suffices to improve the classification accuracy in Vision Language Model (VLM) outputs for multimodal input. In the case of Large- and Multimodal Language Models, it is unclear whether fine-grained categories for prompting and labeling outweigh the effect of fine-tuning. To address these research gaps, our experiment design systematically evaluates the contributions of each factor to determine whether combining them enhances performance in multimodal hate speech detection, with applications for content moderation."}, {"title": "Methodology", "content": "The principle underlying the proposed framework is captured by Equation 1: we consider performance (8) as a multivariate optimization problem dependent on modalities (M), prompting (P), labeling (L), and fine-tuning (F).\n\n$8 = f(M, P, L, F)$ (1)\n\nMore specifically, we first compare modalities, between a visual model, InternVL 2 (Chen et al. 2023) (8B), with another text-based model, DistilBERT (Sanh et al. 2020) (66M); and expect a better performance of the multi-modal approach for more information recognized. Second, we use both prompting and fine-tuning on the same model, InternVL 8B, with identical prompting and labeling strategies. Last we compiled a 2\u00d72 matrix of construct by both prompting (simple question or categories defined in details) and labeling (binary output or outputs in an interval scale). A simple prompt asks a plain question while categories provide detailed definitions of sub-categories of hateful content (see SI). To get labels in scales for fine-tuning, we used GPT4-0-mini to generate answers in scales and excluded incorrectly annotated cases in training according to the ground truth, ensuring the quality of the extended annotations. More details about prompts hyper-parameters are documented in Supplementary Information."}, {"title": "Results and Future Work", "content": "As shown in Table 1, the best model is not the one with highest complexity, highlighting the necessity of our framework. Among all the models, the model M achieves the highest accuracy (68.933%, +19.611 p.p.) and AUROC (66.827%, +19.449 p.p.). Comparisons of ablation show that this improvement results from fine-tuning, categorical prompting, and binary labels. However, components beneficial to model M do not universally enhance performance (e.g., scaled outputs generally improve performance but not always), underscoring the need for the end-to-end framework we proposed.\nIn summary, our study introduces an end-to-end optimization pipeline for complex, multi-modal tasks like hateful meme detection. Our experiments demonstrate the effectiveness of a global optimization strategy within this framework. Moreover, our ablation studies indicate that isolated optimizations are not better by themselves (e.g., scales improve performance in most settings but not on the best model). We therefore argue that this traditional wisdom is both beneficial and necessary for such complicated, modern tasks."}, {"title": "Supplementary Material", "content": "This supplementary material includes detailed prompts (Table 1), experimental settings (section 2), and simplified core codes for both training and evaluations (section 3)."}, {"title": "Prompting Components", "content": "All the prompting strategies in this paper are divided into specific modules (see Table 1). The \u201csimple\u201d prompting component asks the model a straightforward question. The \u201ccategory\u201d component breaks down the question into specific subcategories of hatefulness. The \u201cscale\u201d labeling component requires numerical outputs (e.g., 0-9), while the \u201cbinary\u201d component expects a boolean value. To minimize conversion errors, we implemented output constraints to either the \u201cscale\u201d or \u201cbinary\u201d group. To control confounders, we restrict the prompts to clean combinations of several components, even though some other strategies perform better. All of them were developed based on prior works (Furniturewala et al. 2024)."}, {"title": "Detailed Settings", "content": "The experiments includes three dimensions. The first dimension is model category: multi-modal prompting on a large pre-trained model, InternVL (Chen et al. 2023), multi-modal fine-tuning of the same large pre-trained model, and a unimodal fine-tuning on a smaller textual model, DistilBERT (Sanh et al. 2020). To effectively simulate the common computation capacity, we used LoRA (Hu et al. 2021), low-rank adaptations of large language models, to reduce computation costs in the fine-tuning of 8B models.\nThe second dimension is the prompting strategy: either a basic prompt for this classification task (prompt component simple), or a detailed version defining potential categories of hatefulness (prompt component category; see Table 1). Then we introduce the third dimension of output format: binary or scales. Binary label refers to a direct question about the result of the classification (i.e., True or False, label component binary); on the other hand, scales potentially capture more fine-grained levels of hatefulness (label component scale). For comparison, we keep the same settings for each component across all combinations.\nSince the original dataset lacks scaled outputs for verifying the pipeline, we used GPT-40-mini as a teacher model to generate labels. To ensure 100% accuracy in this annotation process, we manually filtered out incorrect entries using the binary ground-truth. For example, if the ground-truth is hateful but the model rates it 1 (not hateful) on a 0-9 scale, we removed it. This process not only produced accurate outputs but also provided additional information from multi-modal representations to scales from the teacher model.\nHyper-parameters were defined as follows. For InternVL, we used the default hyper-parameters for fine-tuning. For DistilBERT, we applied the same settings listed below to all models. These decisions were intended to control confounders."}, {"title": "Simplified Codes of Training and Evaluation", "content": "Here, we illustrate the automatic, batched fine-tuning and evaluation process with a simplified structure. Full code is available on GitHub."}]}