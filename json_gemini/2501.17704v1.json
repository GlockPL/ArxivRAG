{"title": "Inferring Implicit Goals Across Differing Task Models", "authors": ["Silvia Tulli", "Stylianos Loukas Vasileiou", "Mohamed Chetouani", "Sarath Sreedharan"], "abstract": "One of the significant challenges to generating value-aligned behavior is to not only account for the specified user objectives but also any implicit or unspecified user requirements. The existence of such implicit requirements could be particularly common in settings where the user's understanding of the task model may differ from the agent's estimate of the model. Under this scenario, the user may incorrectly expect some agent behavior to be inevitable or guaranteed. This paper addresses such expectation mismatch in the presence of differing models by capturing the possibility of unspecified user subgoal in the context of a task captured as a Markov Decision Process (MDP) and querying for it as required. Our method identifies bottleneck states and uses them as candidates for potential implicit subgoals. We then introduce a querying strategy that will generate the minimal number of queries required to identify a policy guaranteed to achieve the underlying goal. Our empirical evaluations demonstrate the effectiveness of our approach in inferring and achieving unstated goals across various tasks.", "sections": [{"title": "1 Introduction", "content": "Humans often omit details they consider obvious, unavoidable, or not worth mentioning when providing instructions. This omission leads to implicit goals and unstated preferences that AI systems must navigate to truly align with human intentions. One potential source of such unstated subgoals or preferences could be behaviors that the human user setting the agent objectives may identify as inevitable. The user would never bother stating anything regarding such behaviors, since they believe that it cannot be avoided. One class of such behavior is that of visiting bottleneck states in the context of goal-based Markov Decision Process (MDP). Here, bottleneck states refer to environment states that the agent must pass through to reach the stated goal. In many cases, the human user may want the agent to pass through or visit some bottleneck states in addition to the goal, thus forming a set of intermediate subgoals. However, the user may never specify them since, as far as the user is concerned, every trace of non-zero probability that leads to the goal passes through all the bottleneck states. This should be all well and good, provided the human bottleneck states are also bottleneck states for the agent. Otherwise, the agent must make an effort to figure out what the user's underlying subgoals may be.\nTo see how such problems may arise, consider an agent tasked with guiding a tourist to a famous art museum. The tourist simply says, \"Get me a plan to get to the art museum,\" unaware of the city's metro system and expecting an above-ground route passing certain landmarks. The agent, however, might plan a route using the metro system. For the agent's metro route, bottlenecks might include entering the metro, making transfers, and exiting at the correct station. For the tourist's expected route, they might include crossing a river and passing through the city center. This misalignment stems from differing world models: the agent's comprehensive transit data versus the tourist's limited knowledge of the city's layout. The challenge in AI alignment lies in bridging this gap - identifying and accounting for implicit aspects of the task that weren't mentioned. This paper explores how an agent can learn and achieve the implicit subgoals of another agent, particularly when their understandings of the environment differ.\nTraditional approaches to goal-conditioned reinforcement learning and planning often rely on explicit goal specification and assume shared world models between agents [Liu et al., 2022; Chane-Sane et al., 2021]. This means trying to purely optimize for the achievement of specified human goals could lead to suboptimal or even incorrect task execution in complex, real-world scenarios. We propose a novel approach that introduces and formalizes a specific type of implicit subgoals within the MDP framework. Our method aims to compute policies that align with implicit subgoals by utilizing incomplete knowledge about another agent's model of the world. We formalize the problem using two distinct MDPs: the executing agent's model, the robot, and the goal-setting agent's model, the human. We will use the bottlenecks in the human model as a basis for potential hypotheses about the implicit subgoals of the human. When potential implicit subgoals cannot be achieved, we will also make use of minimal querying to refine the agent's hypotheses about the human subgoals.\nTo evaluate our approach, we propose empirical evaluations in simulated scenarios. These evaluations will assess"}, {"title": "2 Background", "content": "The problems studied in the paper will be ones that can be modeled as infinite horizon discounted Markov Decision Processes (MDP), particularly ones where the primary objectives correspond to achieving some goals. Here, goal achievement corresponds to visiting specific goal states. The algorithms used in the paper will also exploit more traditional MDPs with rewards; as such, it is worth providing a more general definition. Specifically, an infinite horizon MDP is a tuple M = (S, A,T, s_0, \\gamma, R), where S is the state space, A is the action space, T: S\\times A \\times S \\rightarrow [0,1] is the transition function (e.g., $T(s_i, a_i, s')$ provides the probability of transitioning from state $s$ to $s'$ under action $a$), R: S \\rightarrow R is the reward function, $s_0 \\in S$ is the initial state of the agent, and $\\gamma \\in [0, 1)$ is the discount factor. Note that we will generally limit our attention to models where both S and A are finite sets.\nIn this setting, the solution takes the form of a deterministic, stationary policy $\\pi : S \\rightarrow A$ mapping states to actions. A value of a policy $\\pi$, denoted as $V^{\\pi} : S \\rightarrow R$ provides the expected cumulative discounted reward obtained by following the policy from a given state. A policy is considered optimal if no policy with a value higher than the current one exists.\nIn this paper, we focus on goal-directed problems, where the set of goal states is given as $S_G \\subset S$. In such problems, the reward function is sparse, i.e., it returns a small positive value for all states in $S_G$, and 0 otherwise. The states in $S_G$ are treated as an absorbing state in that all transitions out of that state have zero probability. In such cases, we will represent the MDP equivalently in goal terms by dropping the reward function, i.e., M = (S, A, T, $s_0$, \\gamma, $S_G$).\nA concept that we will leverage throughout the paper is that of goal-reaching traces. A goal-reaching trace of a policy from a state s, denoted as $\\tau \\sim_{M \\pi}(s)$, where $\\tau = (s_0, \\pi(s), ..., s_k)$, corresponds to a state-action sequence with non-zero probability that terminates at a goal state. Since the reward is only provided by the goal, the value of the policy in a state is directly proportional to the probability of reaching"}, {"title": "3 Planning for Implicit Subgoals", "content": "As mentioned before, we are interested in a scenario where the robot's model of the task $M_R = (S, A, T_R, s_0, \\gamma, S_G)$ is different from the user's beliefs about the task $M_H = (S, A, T_H, s_0, \\gamma, S_G)$ in terms of the underlying transition function. This difference leads the user to overlook specifying subgoals they are confident are bound to happen. We will leverage the notion of a bottleneck state, where a state is said to be a bottleneck state if it is reached in every path from the initial state to the goal. More formally,\nDefinition 1. For a given MDP model M = (S, A, T, $s_0$, \\gamma, $S_G$), we define a bottleneck state as a state s \u2208 S that must be visited by any valid trace starting from $s_0$, for any policy, with non-zero probability of reaching a state in $S_G$. We denote the set of all bottlenecks as $B \\subset S$.\nNow, we assume that implicit subgoals (IG) are a subset of B that the user wants the robot to achieve on its way to the goal. From the user's point of view, there is no need to spell out these implicit subgoals since they are bottleneck states in their own model and thus cannot be avoided. However, as the robot and user models differ, what is a bottleneck state in one model may not be in the other. What are interested in here is finding a policy that achieves the subgoals in the robot's models. Formally,\nDefinition 2. For a given robot model $M_R = (S, A, T_R, s_0, \\gamma, S_G)$, a policy $\\pi$ is said to achieve a set of implicit subgoal(s) IG, i.e., $\\pi \\models_{M_R} IG$, if every goal reaching trace from $s_0$, $\\tau \\sim_{M_R \\pi}(s_0)$ passes through every state s \u2208 IG.\nThe main challenge we face is to find a policy that achieves all the implicit subgoals when we do not know the implicit subgoals or the exact user model. To achieve this, we make the following two reasonable assumptions: (1) We are given a set of potential user models $M_H$ corresponding to different transition functions; and (2) We can query the user about potential implicit subgoals. We will represent the latter using an oracle function of the form $O_{IG} : S \\rightarrow \\{0,1\\}$, where it can tell you whether a state s is an implicit subgoal or not, i.e.,\n$O_{IG}(s) = \\{\n    1 \\text{ if } s \\in IG \\\\\n    0 \\text{ otherwise}\n\\}$\nNow, it is worth looking at how reasonable these assumptions are. It is always possible to have access to $M_H$ by taking into account all possible well-formed transition functions"}, {"title": "4 Query Identification for Implicit Subgoals Set", "content": "Next, we will look at algorithms that will allow us to identify such queries. Our primary approach here would involve first finding a set of potential implicit subgoals and then querying the user until we are able to whittle it down to a set that can be simultaneously achieved.\nMoving onto the problem of identifying potential implicit subgoals, we know that implicit subgoals are a subset of bottleneck states in the human model. Unfortunately, here, we might be given an infinite set of potential human models. However, in this setting, we can exploit the fact that bottleneck states are preserved through all outcome determinization [Keller and Eyerich, 2011]. Under all outcome determinization, every possible stochastic transition under an action is converted into a separate deterministic action. More formally, we construct a determinized version of a given model as follows:\nDefinition 5. For a given MDP model M = (S, A, $s_0$, T, $S_G$), a determinized model $\\delta(M)$ is given as"}, {"title": "5 Related Work", "content": "Our work intersects with three primary areas of research: reward misspecification, planning with different world models (including insights from Theory of Mind), and query mechanisms in the context of assistance.\nReward Misspecification\nThe challenge of aligning AI systems with human values and intentions has been a growing concern in recent years. Reward misspecification, where the specified reward function does not fully capture the user's true objectives, has been identified as a critical issue in this domain. [Hadfield-Menell et al., 2017] introduced the concept of inverse reward design, which aims to infer the true objective function from an observed reward function. This work highlights the importance of considering the context in which rewards are specified. Building on this, [Majumdar et al., 2017] proposed a risk-sensitive inverse reinforcement learning framework, addressing uncertainties in the reward function. More recently, [Shah et al., 2019] explored the idea of preferences implicit in the design of the reward function, introducing methods to infer these implicit preferences. Similarly, [Gleave et al., 2021] investigated the problem of reward hacking, where agents exploit misspecified rewards in unexpected ways. A significant contribution to this area is the work of [Sreedharan and Mechergui, 2024]. This research directly addresses the challenge of reward misspecification when there is a discrepancy between the agent's and the user's expectations of the task or environment. The authors propose a framework that not only identifies potential misalignments in the reward function but also considers the differences in model understanding between the agent and the user. This approach is particularly relevant to our work as it bridges the gap between reward misspecification and model reconciliation. Our work extends these ideas by specifically focusing on the identification of unspecified subgoals, which may arise due to differences between the user's and agent's understanding of the task model.\nPlanning with Different World Models\nThe problem of agents and humans having different world models has been explored in various contexts within AI planning and human-robot interaction. [Chakraborti et al., 2017] introduced the concept of model reconciliation in the context of explainable planning. Their work focuses on generating explanations that bridge the gap between the agent's and the human's model of the world. This line of research was further developed by [Sreedharan et al., 2018b], who explored the generation of minimal explanations for model reconciliation. In a related vein, [Bobu et al., 2018] investigated the problem of learning from corrections in domains where the human and robot have different feature spaces. Recent work aims to address the representation misalignment [Peng et al., 2024]. Their work highlights the importance of considering model"}, {"title": "6 Evaluation", "content": "We evaluate our approach on a set of standard Markov Decision Process (MDP) benchmarks.\nEnvironments\nThe base environment is a Maze, a basic setting where agents navigate a grid with randomly placed obstacles, providing our baseline scenario with simple navigation challenges and clear bottleneck states at narrow passages. Building on this, Four-Rooms extends Maze by dividing it into four quadrants connected by doorways. The fixed room structure with randomized door placements creates natural bottlenecks, testing our method's ability to identify critical transition points. PuddleWorld introduces additional complexity by adding puddles that incur penalties when traversed. This environment forces trade-offs between path length and safety, creating interesting bottleneck scenarios where avoiding puddles competes with finding shortest paths. Finally, RockWorld features two types of rocks - valuable rocks that provide rewards when collected and dangerous rocks that incur penalties. This tests bottleneck identification in scenarios with resource management and risk-reward trade-offs.\nMethodology\nWe conduct parallel experiments across different configurations for each environment\u00b2. Our framework runs multiple independent trials with varying parameters including grid sizes chosen to balance complexity and tractability, obstacle"}, {"title": "7 Discussion", "content": "The paper presents a way a planning system can identify hidden subgoals of users, even when the human model may not be exactly known. We present algorithms to both identify potential candidates and generate an optimal number of queries. We evaluate the effectiveness of the proposed method on a set of standard benchmark problems. In terms of future work, one of the immediate next steps would be to run user studies. We plan to do them in realistic and everyday scenarios, possibly a robotic one, with significant population size. This would allow us to capture the effectiveness of our method in terms of the load placed on the humans and also test another related hypothesis. For example, one could test whether people would be open to more queries if it significantly improves the agent efficiency. This paper also focuses on the exact method that identifies optimal solutions. It would be interesting to see if we could leverage approximate methods. It would also be interesting to see if we could use other knowledge sources like pre-trained large language models, to get more information about user knowledge and preferences [Zhou et al., 2024]."}]}