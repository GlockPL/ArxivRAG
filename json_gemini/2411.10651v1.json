{"title": "UNDERSTANDING LEARNING WITH SLICED-WASSERSTEIN REQUIRES RETHINKING INFORMATIVE SLICES", "authors": ["Huy Tran", "Yikun Bai", "Ashkan Shahbazi", "John R. Hershey", "Soheil Kolouri"], "abstract": "The practical applications of Wasserstein distances (WDs) are constrained by their sample and com- putational complexities. Sliced-Wasserstein distances (SWDs) provide a workaround by projecting distributions onto one-dimensional subspaces, leveraging the more efficient, closed-form WDs for one-dimensional distributions. However, in high dimensions, most random projections become un- informative due to the concentration of measure phenomenon. Although several SWD variants have been proposed to focus on informative slices, they often introduce additional complexity, numerical instability, and compromise desirable theoretical (metric) properties of SWD. Amidst the growing literature that focuses on directly modifying the slicing distribution, which often face challenges, we revisit the classical Sliced-Wasserstein and propose instead to rescale the 1D Wasserstein to make all slices equally informative. Importantly, we show that with an appropriate data assumption and no- tion of slice informativeness, rescaling for all individual slices simplifies to a single global scaling factor on the SWD. This, in turn, translates to the standard learning rate search for gradient-based learning in common machine learning workflows. We perform extensive experiments across various machine learning tasks showing that the classical SWD, when properly configured, can often match or surpass the performance of more complex variants. We then answer the following question:  \"Is Sliced-Wasserstein all you need for common learning tasks?\"", "sections": [{"title": "Introduction", "content": "Data representation in machine learning concerns the identity of individual data points and the relationship between them in the context of downstream tasks. Optimal transport (OT) theory (Peyr\u00e9 et al., 2019; Villani et al., 2009) compares data distributions by finding an optimal transportation plan that minimizes the expected cost of moving mass between them, leading to the popular Wasserstein distance (WD) central to many learning applications (Khamis et al., 2024). However, the computational complexity of OT solvers poses a significant bottleneck when calculating the WD. In cases of discrete measures or sample-based scenarios, which are common in machine learning, the problem typically reduces to linear programming with time complexity $O(N^3 log N)$, space complexity $O(N^2)$, and sample complexity $O(N^{-1})$, where $N$ is the number of support points and $d$ is the data dimensionality. These unfavorable scaling properties, particularly the curse of dimensionality in sample complexity, make WD impractical for many real-world applications. To address these challenges, several approaches have been proposed, including entropic regularized OT (Cuturi, 2013), smooth OT (Blondel et al., 2018; Manole et al., 2024)), and sliced OT (Bonneel et al., 2015).\nThe Sliced-Wasserstein distances (SWD), (Bonneel et al., 2015; Rabin et al., 2012) project high-dimensional distribu- tions onto 1D subspaces and aggregate the closed-form OT solutions in these subspaces. This method is particularly attractive because 1D Wasserstein distances can be computed efficiently with a time complexity of $O(N log N)$ and a space complexity of $O(N)$ for discrete measures. Additionally, SWD provides a metric between probability dis- tributions that retains many desirable properties of the Wasserstein distance (WD), such as being statistically and topologically equivalent to WD, while being more computationally tractable (Nadjahi et al., 2020). Notably, with a"}, {"title": "Introduction", "content": "sample complexity of $O(N^{-1/2})$, SWD avoids the curse of dimensionality. However, a key drawback of SWD is its projection complexity, which requires exponentially more slices as the data dimensionality increases.\nThe projection complexity of SWD has motivated several lines of work that aim to enhance the effectiveness of the slicing approach, especially in addressing variance reduction (Nguyen and Ho, 2023), approximation error reduction (Nguyen et al., 2023), and slicing complexity (Deshpande et al., 2019; Kolouri et al., 2019; Nguyen and Ho, 2024; Nguyen et al., 2020, 2024a,b). This is particularly relevant in high-dimensional machine learning settings where data often has supports in low-dimensional subspaces. These SW variants are data-driven, focusing on identifying the most informative slices for capturing distributional differences in the data. For instance, Max-SW (Deshpande et al., 2019) and DSW (Nguyen et al., 2020) seek to find slices/projections that maximize the differences between the data distributions. GSW (Kolouri et al., 2019) and ASW (Chen et al., 2020) extend SW by allowing 'non-linear' projections to capture complex data structures. EBSW (Nguyen and Ho, 2024) designs an energy-based slicing distribution that is parameter-free and has the density proportional to an energy function of the projected 1D distance. MSW (Nguyen et al., 2024a) imposes a first-order Markov structure to avoid redundant, independent projections. More recently, RPSW (Nguyen et al., 2024b) proposes using the normalized differences between random samples from the two distributions to ensure that the projections are sampled from the subspace in which the data resides. These methods improve the performance of SW in various downstream tasks and have significantly expanded the tools at disposal for both researchers and practitioners alike. Nonetheless, the elegant extensions also come with increased computational cost, numerical instability, complicated design choices, and often losing the metricity of the original SW.\nIn this paper, we argue that the standard SW, with proper hyperparameters, can often match or surpass the perfor- mance of more complex variants in many learning tasks while retaining its simplicity and theoretical guarantees. Our key insight is that when $d$-dimensional data have $k$-dimensional supports, where $k \\ll d$, almost all random slices $\\theta \\sim U(\\mathbb{S}^{d-1})$ can be decomposed into an informative component $\\theta_D \\in \\mathbb{R}^k$ within the data subspace and its orthogonal complement $\\theta_{\\perp} \\in \\mathbb{R}^{d-k}$. This implies most slices still carry relevant information for distinguishing distributions, pro- portional to $|\\theta_D||$. By appropriately scaling the distance per slice, we get better gradient for learning. In expectation, we show that, with our defined notion of informativeness, scaling for all slices (based on their informativeness) simpli- fies to scaling the SWD by a single scalar factor. In gradient-based learning, this means finding an appropriate learning rate is equivalent to getting informative slices for free. This allows the classical SWD to adapt to the data's intrinsic dimensionality without explicitly limiting the computation to the subspace. We provide theoretical justification and empirical evidence, offering a fresh perspective on SW, particularly in high-dimensional settings.\nBy revisiting the celebrated SW with these insights, we aim to elucidate the performance gap between the original formulation and recent variants in the existing literature. We emphasize that our work does not diminish the valuable contributions of these variants, which have greatly advanced our understanding of Sliced-Wasserstein. Rather, we offer a complementary perspective that highlights the potential of the standard SW when properly integrated into learning tasks. Along that line, we remark that the related line of specialized methods that respects the data geometry (Bonet et al., 2022, 2024; Martin et al., 2023; Quellmalz et al., 2023; Rabin et al., 2011; Tran et al., 2024) remains valuable when the manifold constraint on the data is readily known.\nIn common ML settings where data is supported, or nearly supported, on a $k$-dimensional subspace embedded in a $d$-dimensional space, our specific contributions can be summarized as follows:\n*   We introduce the $\\varphi$-weighting formulation unifying various SW variants. In this framework, we propose reweighing all one-dimensional Wasserstein distances based on slice informativeness instead of directly mod- ifying the slicing distribution, as commonly done in the literature. We show that with an appropriate notion of slice informativeness, in expectation, this leads to an equivalence between the SWD in the ambient space and the data effective subspace. (See $\\textsection 25$).\n*   Our findings translate to scaling the classic SW by a single global constant to get better learning gradients. We show that this reduces solving the problem of non-informative slices to the learning rate search for the classic SW, a process that is already a standard in ML workflows. In other words, we get informative slices for free with the classic SW.\n*   We perform a comprehensive learning rate sweep across a wide range of experiments, including gradient flow (on 3 classic toy datasets, MNIST images, CelebA images), color transfer (3 sets of images), deep generative modeling on the FFHQ dataset (unconditional generation and unpaired translation with SW). We show that the classic SW, with appropriate hyperpameters, perform competitively with more advanced methods in these settings."}, {"title": "Introduction", "content": "Notations. We let $\\mathbb{R}^d$ denote a $d$-dimensional inner product space, and we denote the unit hypersphere in this space by $\\mathbb{S}^{d-1} = {\\theta \\in \\mathbb{R}^d : ||\\theta||_2 = 1}$. Additionally, we denote by $\\mathcal{P}(\\mathbb{R}^d)$ the set of probability measures on $\\mathbb{R}^d$ endowed with the $\\sigma$-algebra of Borel sets, and by $\\mathcal{P}_p(\\mathbb{R}^d) \\subset \\mathcal{P}(\\mathbb{R}^d)$ the subset of those measures with finite $p$-th moments."}, {"title": "Background on Sliced-Wasserstein", "content": "For a measurable function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ defined by $f(x) = \\theta^\\top x$ such that $\\theta \\in \\mathbb{S}^{d-1}$, we denote the pushforward of a measure $\\mu \\in \\mathcal{P}(\\mathbb{R}^d)$ through $f$ as $f_\\#\\mu$. Particularly, $\\theta_\\#\\mu$ is the pushforward measure of $\\mu$ under the projection $x \\rightarrow \\theta^\\top x$.\n2 Background on Sliced-Wasserstein\nLet $\\mu \\in \\mathcal{P}_p (\\mathbb{R}^d)$ and $\\nu \\in \\mathcal{P}_p(\\mathbb{R}^d)$ be two probability measures of interest.\nThe Wasserstein distance (WD). The $p$-WD between $\\mu$ and $\\nu$ is:\n$W_p(\\mu, \\nu) = \\inf_{\\pi \\in \\Pi(\\mu, \\nu)} \\mathbb{E}_{(x, y) \\sim \\pi} [||x - y||^p]^{\\frac{1}{p}}$,\nwith $\\Pi(\\mu, \\nu) = {\\pi \\in \\mathcal{P}_2(\\mathbb{R}^d \\times \\mathbb{R}^d) : \\pi(A \\times \\mathbb{R}^d) = \\mu(A), \\pi(\\mathbb{R}^d \\times A) = \\nu(A)}$ for all measurable sets $A \\subset \\mathbb{R}^d$. In one dimension ($d = 1$), the $p$-WD admits the following closed-form solution:\n$W^p_p(\\mu, \\nu) = \\int_{\\mathbb{R}} |F^{-1}_\\mu(z) - F^{-1}_\\nu(z)|^p dz$,\nwhere $F_\\mu, F_\\nu$ are the cumulative distribution functions (CDF) of $\\mu$ and $\\nu$, respectively. For empirical measures, 2 becomes a Monte Carlo sum that can be calculated by averaging the $d^p(\\cdot, \\cdot)$ between sorted samples. In general, this translates to a highly favorable time complexity of $O(N log N)$ and gives rise to the following Sliced-Wasserstein distance.\nSliced-Wasserstein (SW). The SW distance between $\\mu$ and $\\nu$ is defined as:\n$\\text{SW}_p(\\mu, \\nu; \\sigma) := (\\mathbb{E}_{\\theta \\sim \\sigma} [W_p^p(\\theta_\\#\\mu, \\theta_\\#\\nu)])^{\\frac{1}{p}}$,\nwhere $\\sigma \\in \\mathcal{P}(\\mathbb{S}^{d-1})$ is the reference measure for slicing vector $\\theta$. In default setting, $\\sigma$ is set to be uniform distribution, denoted as: $\\sigma = \\mathcal{U}(\\mathbb{S}^{d-1})$ and we use $\\text{SW}_p(\\mu, \\nu)$ to denote $\\text{SW}_p(\\mu, \\nu; \\sigma)$ for simplicity. The intractable expectation implies (3) admits a Monte Carlo estimator:\n$\\text{SW}_p(\\mu, \\nu; \\sum_{i=1}^L \\delta_{\\theta_i} /L) = (\\frac{1}{L} \\sum_{l=1}^L W_p^p(\\theta_l\\#\\mu, \\theta_l\\#\\nu))^{\\frac{1}{p}}$,\nwhere {$\\theta_l$}$^L_{l=1}$ $\\stackrel{\\text{i.i.d.}}{\\sim}$ $\\sigma$. The MC scheme has the estimation error decreases as $\\frac{1}{\\sqrt{L}}$, where $L$ is the number of samples. The main issue becomes how much one can simulate (for large $d$), which proves to be challenging since most slices are known to be non-informative. As a result, $\\text{SW}_p(\\mu, \\nu; \\sum_{i=1}^L \\delta_{\\theta_i})$ often underestimates the distance between $\\mu$ and $\\nu$ in practice. Moreover, $L$ should be sufficiently large compared to $d$, which is undesirable since the time complexity of SW scales linearly with $L$."}, {"title": "Other Related Work", "content": "Subspace-constrained Optimal Transport. Recent works propose computing optimal transport (OT) in lower- dimensional subspaces (Bonet et al., 2021b; Muzellec and Cuturi, 2019; Paty and Cuturi, 2019) to improve both efficiency and robustness for high-dimensional data. 1) Subspace Detours (Bonet et al., 2021b; Muzellec and Cuturi, 2019) constrain transport plans to be optimal when projected onto a chosen subspace. This enables efficient exten- sion of low-dimensional transport solutions to the full space. 2) Subspace Robust Wasserstein (Paty and Cuturi, 2019) considers the worst-case transport cost over all possible low-dimensional projections. Interestingly, this can be computed by minimizing the sum of the $k$ largest eigenvalues of the transport plan second-order moment matrix $\\mathbb{S}(\\mu, \\nu) = \\min_{\\pi \\in \\Pi(\\mu, \\nu)} \\sum_{i=1}^k \\lambda_i(V_\\pi)$ where $V_\\pi := \\int (x - y)(x - y)^\\top d\\pi(x, y)$ is the second-order displacement matrix for a coupling $\\pi$, and $\\lambda_i(V)$ is its $l$-th largest eigenvalue.\nGaussian Sliced-Wasserstein. Earlier works ((Diaconis and Freedman, 1984; Reeves, 2017; Sudakov, 1978)) es- tablish several central limit theorems showing that under mild conditions, low-dimensional projections of high- dimensional data converge to Gaussians. Nadjahi et al. (2021) leverages this concentration of measure phe- nomenon and shows the Gaussian SW distance is equivalent to the classical SW distance: $\\text{SW}_p(\\mu, \\nu; I_d) = C_{d,p}\\text{SW}_p(\\mu, \\nu; \\mathcal{U}(\\mathbb{S}^{d-1}))$, where $C_{d,p}$ is the dimensionality-dependent constant. This leads to an efficient approx- imation given by $\\text{SW}_2^2(\\mu, \\nu) = W_2^2(\\mathcal{N}(0, \\Sigma_2(\\mu)), \\mathcal{N}(0, \\Sigma_2(\\nu))) + |\\mu - \\nu|^2$, where $\\Sigma_2(\\mu)$ denotes the 2nd moment of $\\mu$."}, {"title": "Revisiting Sliced-Wasserstein: A Subspace Perspective", "content": "4 Revisiting Sliced-Wasserstein: A Subspace Perspective\nThe main challenge. Many machine learning problems involve high-dimensional data that has a low-dimensional structure. Formally, this phenomenon, known as the manifold hypothesis, states that for a dataset $\\mathcal{X} \\subset \\mathbb{R}^d$, there exists a $k$-dimensional manifold $\\mathcal{M}$ where $k \\ll d$ such that $\\mathcal{X}$ approximately lies on $\\mathcal{M}$ (Fefferman et al., 2016). For instance, rigorous dimensionality estimation methods applied to common datasets like MS-COCO (Lin et al., 2014) and ImageNet (Deng et al., 2009) suggest $k < 50$ (Pope et al., 2021), despite their ambient dimension $d$ being orders of magnitude larger. While these manifolds are generally nonlinear, they admit local linear approximations via their tangent spaces. Moreover, in practice, data features typically have strong linear correlations, allowing techniques like Principal Component Analy- sis (PCA) to identify a principal subspace that captures most of the data variance. This subspace approximation is particularly relevant in the context of Sliced-Wasserstein distance (SWD). It is known from Kolouri et al. (2019) that when slices $\\theta$ are sampled uniformly from $\\mathbb{S}^{d-1}$, the probability that a random slice is nearly orthogonal to any fixed direction increases exponentially with dimension. Specifically, for a unit vector $x_0$ representing a principal direction in the data subspace:\n$\\text{Pr}(\\left| \\langle \\theta, x_0 \\rangle \\right| \\leq \\epsilon) > 1 - e^{-d \\epsilon^2}$, $\\theta \\sim \\mathcal{U}(\\mathbb{S}^{d-1})$.\nThis concentration of measure phenomenon implies that as dimensionality $d$ grows, most random slices become nearly orthogonal to the principal directions of the data subspace. Consequently, the corresponding 1D Wasserstein distances contribute minimally to the overall SWD. This effect, which we refer to as the non-informativeness of the slices, fundamentally limits the effectiveness of SWD in high-dimensional spaces.\nCurrent approaches: Directly modifying the slicing distribution. Various sampling-based methods seek to define a non-uniform slicing distribution that focuses on discriminative directions. This is done in a data-driven manner with or without an explicit optimization. Methods that are optimization-free (Nguyen and Ho, 2024; Nguyen et al., 2024b) are objectively faster but do not yield true metrics. Other methods (Nguyen et al., 2020, 2024a) yield proper metrics but are more computationally expensive due to the optimization involved. In the limit, the Max variants use discrete slicing distributions that require global optimality to be metrics, which is generally intractable in practice. Empirically, without careful hyperparameter tuning, the different variants face numerical instability in the larger learning rate regimes, likely because of the overemphasizing on directions with large projected distances.\nA novel perspective: Rescaling one-dimensional Wasserstein distances. These challenges in directly redefining the slicing distribution motivates us to take a second look at the conventional wisdom of sampling informative slices. We propose an alternative formulation that reweights each one-dimensional Wasserstein based on the informativeness of the corresponding slice/projecting direction (See Figure 1 for illustration). By defining the notion of an informative slice based on its alignment with the effective data subspace, we demonstrate that it is possible to reweight for all slices by a single global constant on the SWD. This maintains the efficiency and theoretical properties of the classical Sliced-Wasserstein. The implications of this finding for using SWD in gradient-based learning will be discussed subsequently.\nTo formalize this approach, we introduce the following assumption and definitions:"}, {"title": "Revisiting Sliced-Wasserstein: A Subspace Perspective", "content": "Assumption 4.1 (Effective Subspace Structure). Let $\\mu^d, \\nu^d \\in \\mathcal{P}(\\mathbb{R}^d)$ be probability measures. We say $(\\mu^d, \\nu^d)$ has $k$-dimensional effective structure if:\n1.  There exists a semi-orthogonal matrix $U \\in \\mathbb{R}^{d \\times k}$ (i.e., $U^\\top U = I_k$) such that\n$\\text{supp}(\\mu^d), \\text{supp}(\\nu^d) \\subset V_k := \\text{col-span}(U)$\n2.  $k$ is minimal, meaning that there does not exist any $U' \\in \\mathbb{R}^{d \\times k'}$ with $k' < k$ such that (1) holds.\n3.  For any measurable set $A \\subset \\mathbb{R}^d$, we have that\n$\\mu^d(A) = \\mu^d(P_{V_k}(A)) \\text{ and } \\nu^d(A) = \\nu^d(P_{V_k}(A))$\nwhere $P_{V_k}(x) = UU^\\top x$ is the orthogonal projection onto $V_k$.\n4.  Let $\\mu^k = (U^\\top)\\_\\# \\mu^d$ and $\\nu^k = (U^\\top)\\_\\# \\nu^d$ be the pushforward measures under $U^\\top : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$. Then, we have that\n$W_p(\\mu^k, \\nu^k) < \\infty \\text{ for some } p \\geq 1$"}, {"title": "Revisiting Sliced-Wasserstein: A Subspace Perspective", "content": "We refer to $V_k$ as the effective subspace (ES) of $\\mu^d, \\nu^d$, and $k$ as their effective dimensionality (ED).\nDefinition 4.2 (Informative slices). Let $\\phi : \\mathbb{S}^{d-1} \\rightarrow \\mathbb{R}\\_+$ be a function that assigns importance values to projection directions $\\theta \\in \\mathbb{S}^{d-1}$ based on their relevance in comparing data distributions. Different approaches compute this importance in various ways:\n*   Max-SW (Deshpande et al., 2019), Markovian SW Nguyen et al. (2024a), and EBSW (Nguyen and Ho, 2024) implicitly use\n$\\phi_{\\mu, \\nu}(\\theta) = W_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu)$,\nto measure the informativeness of $\\theta$. In other words, slices that have higher projected distances are considered more important/informative.\n*   On the other hand, RPSW (Nguyen et al., 2024b) implicitly uses\n$\\phi_{\\mu, \\nu}(\\theta; \\mu, \\nu, \\gamma\\_\\kappa) = \\mathbb{E}_{(X, Y) \\sim \\mu \\times \\nu} [\\Psi\\_\\kappa(\\theta; P_{\\mathbb{S}^{d-1}}(X - Y))]$,\nwhere $\\Upsilon\\_k$ is a location-scale distribution (i.e., von Mises-Fisher) and $P_{\\mathbb{S}^{d-1}}$ is the projection onto $\\mathbb{S}^{d-1}$. Slices that align well with random paths connecting 2 distributions are considered more important/informative.\nFor the above instances, one may also refer to $\\phi_{\\mu, \\nu}$ as the discriminant function. However, note that we define the concept of informativeness more broadly, allowing for a broader set of assumptions about data structure rather than strictly referring to the ability to discriminate between distributions. Other ways to quantify informativeness may be appropriate depending on the context where prior information on the data is available. We refer to further related details in Remark 4.6.\nDefining $\\phi$ in terms of 1D Wassersteins may not always be desirable. It requires calculating them to find out how informative the slices are, even when the calculations are not always used in computing the final distances (Nguyen et al., 2024a). Additionally, defining $\\phi$ based on the input measures $\\mu, \\nu$ could introduce complex dependencies that make the triangle inequality challenging to prove (Nguyen and Ho, 2024; Nguyen et al., 2024b). Motivated by 4.1, we propose a principled notion of informativeness that avoids both issues (i.e., redundant calculations, complex dependencies) and lead to a significantly simplified solution.\nDefinition 4.3 (ES-aligned informative slices). Given a $k$-dimensional subspace $V_k = \\text{span}(U)$, where $U \\in \\mathbb{R}^{d \\times k}$ is an orthogonal matrix, we define the ES-aligned informative function $\\phi\\_U : \\mathbb{S}^{d-1} \\rightarrow [0, 1]$ as:\n$\\phi\\_U(\\theta) = ||U^\\top \\theta||$.\nIntuitively speaking, $\\phi\\_U$ corresponds to how much information $\\theta$ contains about the data if it is projected into the space spanned by $U$. Higher $\\phi\\_U(\\theta)$ is considered more (ES-aligned) informative.\nRemark 4.4. $\\Phi\\_U(\\theta)$ has the following basic properties:\n1.  $0 \\leq \\phi\\_U(\\theta) \\leq 1$ for all $\\theta \\in \\mathbb{S}^{d-1}$\n2.  $\\phi\\_U(\\theta) = 1$ iff $\\theta \\in \\text{span}(U) \\cap \\mathbb{S}^{d-1}$\n3.  $\\phi\\_U(\\theta) = 0$ iff $\\theta \\bot \\text{span}(U)$\n4.  For any orthogonal matrix $Q \\in \\mathbb{R}^{k \\times k}$, $\\phi\\_U(\\theta) = \\phi\\_{UQ}(\\theta)$"}, {"title": "Revisiting Sliced-Wasserstein: A Subspace Perspective", "content": "4.1 The $\\varphi$-weighting formulation\nStarting from the classical SWD definition in Equation (3), we propose a general framework for reweighting slice contributions:\n$\\text{SW}\\_p(\\mu, \\nu; \\sigma, \\rho\\_\\phi) = (\\int_{\\mathbb{S}^{d-1}} \\rho(\\phi(\\theta)) W^p\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu) d\\sigma(\\theta))^{\\frac{1}{p}}$,\nwhere $\\rho : [0, 1] \\rightarrow \\mathbb{R}\\_+$ is the $\\varphi$-weighting function that rescales the contribution of each slice based on how (un)informative it is."}, {"title": "Revisiting Sliced-Wasserstein: A Subspace Perspective", "content": "Example 4.5. If the goal is to make all slices informative, an appropriate choice for $p$ can be the multiplicative inverse of $\\phi(\\theta)$ (i.e., more informative slices are scaled less). That is,\n$\\rho\\_\\phi(\\phi(\\theta)) = \\begin{cases} \\frac{1}{\\phi(\\theta)^p}, & \\text{if } \\phi(\\theta) > 0, \\\\ 0, & \\text{if } \\phi(\\theta) = 0. \\end{cases}$\nRemark 4.6. Equation (9) notably does not rely on Assumption 4.1 (only our choice of $\\phi(\\cdot) = \\phi\\_U(\\cdot)$ does). By defining the appropriate $\\rho\\_\\phi(\\cdot)$ and $\\phi(\\cdot)$, the $\\phi$-weighting formulation can be seen as a unifying formulation that recovers different SW variants.\n*   Classical SWD: We set $\\phi = 1$ and obtain the classical Sliced-Wasserstein distance:\n$\\text{SW}(\\mu, \\nu; \\sigma, \\rho\\_\\phi) = \\text{SW}\\_p(\\mu, \\nu; \\sigma)$.\n*   Max-SW (Deshpande et al., 2019): We set $\\phi_{\\mu, \\nu}(\\theta) = W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu)$ and $\\rho\\_\\phi(r) = \\delta\\_{r\\_{\\text{max}}}, \\sigma = \\mathcal{U}(\\mathbb{S}^{d-1})$ where $r_{\\text{max}} = \\text{max} \\phi_{\\mu, \\nu}(\\theta)$. Then, we have that\n$\\text{SW}(\\mu, \\nu; \\sigma, \\rho\\_\\phi) = \\text{Max-SW}\\_p(\\mu, \\nu)$.\n*   EBSW (Nguyen and Ho, 2024): We set $\\phi_{\\mu, \\nu}(\\theta) = W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu)$, $\\rho\\_\\phi(r) = \\frac{f(r)}{\\int_{\\mathbb{S}^{d-1}} f(W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu)) d\\sigma(\\theta)}$, $\\sigma = \\mathcal{U}(\\mathbb{S}^{d-1})$ where $f : [0, \\infty) \\rightarrow (0, \\infty)$ is an increasing energy function (e.g., $f(x) = e^x$). Then, we have that\n$\\text{SW}(\\mu, \\nu; \\sigma, \\rho\\_\\phi) ) = \\int_{\\mathbb{S}^{d-1}} \\rho\\_\\phi(\\Phi_{\\mu, \\nu}(\\theta)) W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu) d\\sigma(\\theta) = \\int_{\\mathbb{S}^{d-1}} \\frac{f(W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu))}{\\int_{\\mathbb{S}^{d-1}} f(W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu)) d\\sigma(\\theta)} W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu) d\\sigma(\\theta) = \\text{EBSW}(\\mu, \\nu; f)$.\n*   RPSW (Nguyen et al., 2024b): We set $\\phi_{\\mu, \\nu}(\\theta) = \\mathbb{E}\\_{(X, Y) \\sim \\mu \\times \\nu} [\\sqrt{\\gamma\\_\\kappa(\\theta; P_{\\mathbb{S}^{d-1}}(X - Y))}]$, $\\rho\\_\\phi(r) = r$, $\\sigma = \\mathcal{U}(\\mathbb{S}^{d-1})$, where $\\gamma\\_\\kappa$ is a location-scale distribution with parameter $\\kappa$. Then, we have that\n$\\tilde{\\text{SW}}(\\mu, \\nu; \\sigma, \\rho\\_\\phi) = \\int_{\\mathbb{S}^{d-1}} \\Phi_{\\mu, \\nu}(\\theta) W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu) d\\sigma(\\theta) = \\mathbb{E}\\_{\\theta \\sim \\Phi_{\\mu, \\nu}(\\cdot)} [W\\_p(\\theta\\_\\# \\mu, \\theta\\_\\# \\nu)] = \\text{RPSW}(\\mu, \\nu; \\gamma\\_\\kappa)$,\nwhere we abuse the notation $\\Phi_{\\mu, \\nu}$ to denote the distribution induced by function $\\Phi_{\\mu, \\nu}(\\theta)$."}, {"title": "Revisiting Sliced-Wasserstein: A Subspace Perspective", "content": "4.2 Misaligned random projections are implicitly downweighed by a scalar\nUnder Assumption 4.1, we will show that the 1D Wasserstein corresponding to each random projection is weighted by a scalar related to the (ES-aligned) informativeness of that projection.\nThe case for one-dimensional effective subspaces. Let $V\\_1 = \\text{span}(u)$ where $u \\in \\mathbb{S}^{d-1}$, and suppose $\\text{supp}(\\mu^d), \\text{supp}(\\nu^d) \\subset V\\_1$. Given $\\theta \\in \\mathbb{S}^{d-1}$, we can decompose it uniquely as $\\theta = \\theta\\_{V\\_1} + \\theta\\_{\\perp}$, where $\\theta\\_{V\\_1} = (u^\\top \\theta)u$ and $\\theta\\_{\\perp} \\bot V\\_1$. For any $x \\in V\\_1$, we have $x = (x^\\top u)u$, and $\\theta^\\top x$ can thus be decomposed as:\n$\\theta^\\top x = (\\theta\\_{V\\_1} + \\theta_{\\perp})^\\top x = \\theta\\_{V\\_1}^\\top x = (u^\\top \\theta) (u^\\top x)$.\nThis implies that for any slice $\\theta$, the projected distributions $\\theta\\_\\# \\mu^d$ and $\\theta\\_\\# \\nu^d$ are equivalent (up to scaling) to the distributions obtained by projecting $\\mu^d$ and $\\nu^d$ onto $u$. Specifically:\nW\\_p(\\theta\\_\\# \\mu^d, \\theta\\_\\# \\nu^d) = |u^\\top \\theta|^p W\\_p(u\\_\\# \\mu^d, u\\_\\# \\nu^d).\nGeneralizing to higher-dimensional effective subspaces. We extend the idea from one dimension to a $k$-dimensional subspace $V_k$ and investigate how the reweighting function $\\rho\\_\\phi(\\phi\\_U(\\theta)) = ||U^\\top \\theta||^{-p}$ adjusts the contributions of slices in higher dimensions."}, {"title": "Revisiting Sliced-Wasserstein: A Subspace Perspective", "content": "Proposition 4.7. Under Assumption 4.1", "that": "nW\\_p(\\theta\\_\\# \\mu^d, \\theta\\_\\# \\nu^d) = W\\_p(((U^\\top \\theta^d)\\_\\# \\mu^k, (U^\\top \\theta^d)\\_\\# \\nu^k) = ||U^\\top \\theta^d||^p W\\_p(\\theta\\_\\# \\mu^k, \\theta\\_\\# \\nu^k),\nwhere $\\theta^k = \\frac{U^\\top \\theta^d"}]}