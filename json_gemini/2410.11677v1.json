{"title": "UNDERSTANDING LIKELIHOOD OVER-OPTIMISATION\nIN DIRECT ALIGNMENT ALGORITHMS", "authors": ["Zhengyan Shi", "Sander Land", "Acyr Locatelli", "Matthieu Geist", "Max Bartolo"], "abstract": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms\nsuch as Proximal Policy Optimisation (PPO) for aligning language models to hu-\nman preferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred) comple-\ntions while discouraging worse (non-preferred) ones, while staying close to the\noriginal model's behaviour. In this work, we explore the relationship between\ncompletion likelihood and model performance in state-of-the-art DAAs, and iden-\ntify a critical issue of likelihood over-optimisation. Contrary to expectations, we\nfind that higher likelihood of better completions and larger margins between better\nand worse completion likelihoods do not necessarily lead to better performance,\nand may even degrade it. Our analysis reveals that while higher likelihood cor-\nrelates with better memorisation of factual knowledge patterns, a slightly lower\ncompletion likelihood tends to improve output diversity, thus leading to better\ngeneralisation to unseen scenarios. Moreover, we identify two key indicators that\nsignal when over-optimised output diversity begins to harm performance: De-\ncreasing Entropy over Top-k Tokens and Diminishing Top-k Probability Mass.\nOur experimental results validate that these indicators are reliable signs of de-\nclining performance under different regularisation schemes, helping prevent over-\noptimisation and improve alignment with human preferences.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs) (Touvron et al., 2023; Achiam et al., 2023;\nRoziere et al., 2023; Dubey et al., 2024; Land & Bartolo, 2024) have significantly expanded their\ncapabilities, enabling applications such as code generation, tool use, and interactive communication.\nAs LLMs become increasingly powerful, the challenge of aligning them with human preferences has\ngrown in importance. Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) (Rafailov et al., 2023) and Identity Preference Optimisation (IPO) (Azar et al., 2024), have\nemerged as alternatives to Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al.,\n2019; Bai et al., 2022) for training LMs on human preference data. These methods aim to bypass the\ntraditional RLHF pipeline by directly optimising the policy without the explicit reward modelling.\nDAAs are designed to increase the likelihood of better completions while reducing the likelihood of\nworse ones, all while staying close to the original model's behaviour. However, a known issue with\nstandard DAAs is that they may decrease the likelihood of better completions as long as the relative\nprobability between better and worse completions increases (Rafailov et al., 2023; Pal et al., 2024).\nRecent research has sought to address this by focusing on maintaining a high likelihood for better"}, {"title": "2 RELATED WORK", "content": "Preference learning. Recent years have seen significant progress in aligning large language mod-\nels (LLMs) with human preferences (Hosking et al., 2024; Kirk et al., 2024a; Wu et al., 2024).\nRLHF, pioneered by Christiano et al. (2017); Ziegler et al. (2019) and developed in subsequent\nworks (Stiennon et al., 2020; Bai et al., 2022; Ouyang et al., 2022b), typically consists of three\nstages: supervised fine-tuning (SFT), reward modelling, and RL fine-tuning (Schulman et al., 2017;\nMnih, 2016; Shi & Lipani, 2023; Aryabumi et al., 2024; Ahmadian et al., 2024). The reward model\nis trained to predict human preferences between pairs of model outputs, while the RL phase opti-\nmises the model to maximise the reward (Ye et al., 2024; Lambert et al., 2024; Zhou et al., 2024a;\nLiu et al., 2024b). More recently, researchers have proposed Direct Alignment Algorithms (Rafailov\net al., 2023; Zhao et al., 2023; Azar et al., 2024) that aim to simplify RLHF by directly optimising\nthe policy without a reward modelling or RL phase.\nOver-optimisation for preference learning. Over-optimisation in preference learning occurs\nwhen a model's performance on a proxy measure improves while its true performance declines.\nGao et al. (2023) were the first to extensively characterise this issue in the context of RLHF, where\noptimisation against a learned reward model leads to increased proxy rewards, while actual task\nperformance plateaus or worsens, a phenomenon termed \u201creward over-optimisation", "alignment tax\" in LM fine-tuning. Bai et al.\n(2022); Ouyang et al. (2022a); Bai et al. (2023); Kotha et al. (2023) observed that aligning models\nwith human preferences, through RLHF, can sometimes degrade performance on specific tasks, es-\npecially for smaller models. Various approaches have been proposed to mitigate the alignment tax\n(Noukhovitch et al., 2023; Shi & Lipani, 2024; Qi et al., 2024). For example, Ouyang et al. (2022a)\nsuggested incorporating pretraining data into RLHF fine-tuning to minimise performance regres-\nsions on standard NLP datasets. However, these studies have not explored how the optimisation of\ncompletion likelihood correlates with model performance, including diversity and generalisation.\"\n    },\n    {\n      \"title\": \"3 PRELIMINARIES\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"3.1 DIRECT ALIGNMENT ALGORITHMS\",\n      \"content\": \"Direct Alignment Algorithms (DAAs) are a family of methods designed to train LMs to align with\nhuman preferences without the need for explicit reward modelling. These algorithms aim to optimise\na policy model to maximise the probability of better completions over worse ones.\nDirect Preference Optimisation. Direct Preference Optimisation (DPO) (Rafailov et al., 2023) is\na foundational DAA method. The DPO loss function is defined as follows:\n$\\displaystyle L_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,y_w,y_l) \\sim D} [log \\sigma (\\beta\\Delta(x, y_w, y_l))]$,\n$\\displaystyle \\Delta(x, y_w, y_l) = log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)},$\"\n    },\n    {\n      \"title\": \"Identity Preference Optimisation.\",\n      \"content\": \"Identity Preference Optimisation (IPO) (Azar et al., 2024) is\na variant of DAA methods. Specifically, IPO uses a quadratic loss function, which is defined as:\n$\\displaystyle L_{IPO}(\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_w,y_l) \\sim D} [\\tau\\Delta(x, y_w, y_l) - \\frac{1}{2}]^2,$\nwhere \\( \\tau \\) is a temperature hyperparameter. This formulation aims to push the difference in log prob-\nabilities \\( \\Delta(x, y_w, y_l) \\), defined within the DPO framework, towards a target value of \\( \\frac{1}{2\\tau} \\).\"\n    },\n    {\n      \"title\": \"Hinge Loss.\",\n      \"content\": \"The hinge loss method (Zhao et al., 2023; Liu et al., 2024a) represents another vari-\nation within the DAA framework. Specifically, we adopt the loss function from SLIC-HF (Zhao\net al., 2023), which is defined as follows:\n$\\displaystyle L_{Hinge} (\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_w,y_l) \\sim D} \\max (0, \\gamma - log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{\\theta}(y_l|x)}),$ where \\( \\gamma \\) is a hyperparameter and we set to \\( \\gamma = 1 \\) for simplicity. In line with Zhao et al. (2023), we\nincorporate a regularisation term into the hinge loss, defined as follows:\n$\\displaystyle L_{reg}(\\pi_{\\theta}; \\pi_{ref}) = E_{(x,y_w,y_l) \\sim D} [log (1 + exp (1 - log (\\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)})))],$ which represents a smoothed version of hinge loss (Huber, 1992; Cristianini & Shawe-Taylor, 2000).\nThis term encourages the likelihood of better completions to remain higher than that of the reference\nmodel. The total hinge loss is given by \\( L_{Hinge}(\\pi_{\\theta}; \\pi_{ref}) = L_{Hinge}(\\pi_{\\theta}; \\pi_{ref}) + \\alpha L_{reg}(\\pi_{\\theta}; \\pi_{ref}) \\), where\n\\( \\alpha \\) is a scaling coefficient.\"\n    },\n    {\n      \"title\": \"3.2 BETTER LIKELIHOOD SUPPORT\",\n      \"content\": \"Standard DAAs do not guarantee an increase in the absolute probability of better completions. This\ncan lead to scenarios where the model assigns very low probabilities to both better and worse com-\npletions, as long as the better completion has a higher relative probability.\nNegative Log-Likelihood Loss. To mitigate this issue, Negative Log-Likelihood (NLL) loss is\ncommonly employed as a regularisation term in DAA (Hong et al., 2024; Pang et al., 2024; Adler\net al., 2024; Dubey et al., 2024). It encourages the policy to maintain a high likelihood of better\ncompletions. The NLL loss is formulated as:\n$\\displaystyle L_{NLL}(\\pi_{\\theta}) = -E_{(x,y_w) \\sim D} [log \\pi_{\\theta}(y_w|x)],$ where \\( y_w \\) represents the better completion for a given input \\( x \\). This loss term is typically combined\nwith the primary objective of the DAA using a scaling coefficient \\( \\lambda \\).\nSeveral other regularisation methods have been proposed to address this issue. For example, Pal\net al. (2024) introduces an additional term, \\( max (0, log(\\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)})) \\), to \\( \\Delta(x, y_w, y_l) \\), to ensure that\nthe log-likelihood of better examples remains high relative to that of the reference model. In this\nwork, we mainly discuss the impact of Negative Log-Likelihood Loss.\"\n    },\n    {\n      \"title\": \"4 UNDERSTANDING THE IMPACT OF COMPLETION LIKELIHOOD\",\n      \"content\": \"\"\n    },\n    {\n      \"title\": \"4.1 EXPERIMENTAL SETUP\",\n      \"content\": \"Model and Datasets. In our experiments, we utilise two instruction-tuned models: a closed-\nweight model (7B) and Cohere Command R (35B) (Cohere For AI, 2024). We train and evalu-\nate them on two datasets: (1) A binarised version of ULTRAFEEDBACK (Tunstall et al., 2024),\"\n    },\n    {\n      \"title\": \"Training and Evaluation Details.\",\n      \"content\": \"For each method (Hinge, DPO, and IPO), we test six different\nvalues for its hyper-parameter (i.e., \\( \\alpha \\), \\( \\beta \\), or \\( \\tau \\)), respectively. We use a batch size of 32 for both\ntraining and evaluation, with a maximum sequence length of 8192. The model is trained with a\npeak learning rate of either \\( 5 \\times 10^{-6} \\) or \\( 1 \\times 10^{-5} \\) and an end learning rate ratio of 0.1. Following\nrecent studies (Ouyang et al., 2022a; Howard & Whitaker, 2023; Shi et al., 2024), we train all\nmodels within a single epoch. The learning rate warms up over 128 steps. We monitor the model\ntraining every 50 steps to apply early stopping. We use the Adam optimiser (Kingma, 2014) with\n\\( \\beta_1 = 0.9 \\), \\( \\beta_2 = 0.95 \\), \\( \\epsilon = 1 \\times 10^{-8} \\), an additive weight decay of 0.1, and a gradient clipping\nnorm of 1.0. The model training is conducted on TPU v5-128 for the 7B model and TPU v5-256\nfor the 35B model, utilising the flash attention (Dao et al., 2022) to improve training efficiency. For\nboth DPO and IPO, we use the sum of the token log-likelihoods as the completion log-likelihood\nduring training. For the Hinge method, we compute the average token log-likelihood instead for\nbetter performance. During evaluation, we calculate the log-likelihood for both the better and worse\ncompletions from the validation set. For all methods, we report the average of token log-likelihoods\nfor better and worse completions respectively, without normalising against the reference model's\nlikelihood. Additionally, we monitor the difference in log-likelihood between better and worse\ncompletions.\"\n    },\n    {\n      \"title\": \"Generalisation Evaluation.\",\n      \"content\": \"Following the previous work (Kirk et al., 2024b), we evaluate the\nmodel in open-ended text generation tasks to assess generalisation ability. Specifically, we employ\nthe LLM-as-a-Judge framework (Zheng et al., 2023; Taori et al., 2023) based on the reward model to\ncompare our models' outputs against leading models, including GPT-3.5-Turbo, GPT-40 (Achiam\net al., 2023), Claude-3-Sonnet (Claude, 2024), Llama-3-8B, and Llama-3-70B-Chat (Dubey et al.,\n2024). The evaluation uses the closed-source reward model, which ranked the top position on RE-\nWARDBENCH (Lambert et al., 2024). This validates that the evaluation provides a reliable proxy for\nhuman preferences. We use win probability, denoted as \\( P_{win} \\), as the primary evaluation metric. It is\ncomputed as:\n$\\displaystyle P_{win} = \\sigma(r_c - r_v),$ where \\( \\sigma(\\cdot) \\) is the sigmoid function, \\( r_c \\) is the reward assigned to the competitor model's output, and\n\\( r_v \\) is the reward assigned to the trained policy model's output by the same reward model. We prompt\nmodels with 433 diverse prompts, including code generation, chain-of-reasoning questions, closed\nQA, and length control (see Appendix A for examples and details). During the decoding, we use a\ntop-p probability threshold of \\( p = 0.75 \\), a temperature of 0.5, and a maximum limit of 2048 tokens.\"\n    },\n    {\n      \"title\": \"Diversity Evaluation.\",\n      \"content\": \"To assess output diversity, we also measure Per-Input Diversity, defined\nas the average diversity of the output sets over inputs, and Cross-Input Diversity, which captures\nthe diversity of outputs across different inputs, similar to previous works (Kirk et al., 2024b; Hong\net al., 2024). However, instead of generating a set of K outputs from the model, we take a more\nefficient way to measure Per-Input Diversity. Specifically, we compute the entropy over the top k\ntokens with the highest probability in the model's next token distribution (Kuhn et al., 2023). Let \\( p_k \\)\nrepresent the probability distribution over the top k tokens, and \\( H(p_k) \\) represent the entropy of the\ndistribution. The entropy is calculated using the following formula:\n$\\displaystyle H(p_k) = - \\sum_{i=1}^{n} p_i log_b(p_i),$ where b is the logarithm base. Here we set b = 2 and k = 10. This formula quantifies the un-\ncertainty within the top k token predictions as a proxy for Per-Input Diversity. This entropy is\nhighest when the output is minimally informative: predicting the same probability for all possible\ntokens, indicating more diverse outputs. To evaluate Cross-Input Diversity, we use distinct N-grams\n(Li et al., 2016), which counts the unique N-grams across model outputs and averages them over\nn = 1,2,3,4,5. Following Kirk et al. (2024b), we use the expectation-adjusted distinct N-grams\n(EAD) formula to remove the bias towards shorter outputs.\"\n    },\n    {\n      \"title\": \"4.2 EVALUATING LIKELIHOOD OVER-OPTIMISATION\",\n      \"content\": \"In this section, we explore the relationship between model likelihood and performance. Below, we\ndiscuss our key findings in detail.\n1) Higher likelihood for better completions and larger gaps between better and worse comple-\ntions do not necessarily improve model performance. As shown in Figure 1, we plot the likeli-\nhood of better completions against the win probability (compared to GPT-3.5-Turbo) with different\"\n    },\n    {\n      \"title\": \"2) Length Correlation.\",\n      \"content\": \"We investigate the relationship between the mean log-likelihood of better\ncompletions and the average number of tokens in completions, as shown in Figure 1. To quantify this\nrelationship, we calculate the Pearson correlation coefficient and perform its associated significance\ntest. The null hypothesis posits no linear relationship between these two variables. For the 7B model,\nwe find a weak negative correlation (r = -0.114, p-value = 0.266), while the 35B model shows a weak\npositive correlation (r = 0.198, p-value = 0.173). In both cases, the p-values exceed the conventional\nsignificance level of 0.05, indicating insufficient evidence to reject the null hypothesis.\"\n    },\n    {\n      \"title\": \"3) Training Negative Log-Likelihood Loss on better completions has limited influence on the\nmodel when it cannot affect completion likelihood.\",\n      \"content\": \"As shown in Figure 4, we experiment with\nDPO using three different values of \\( \\beta \\), adding NLL loss as an auxiliary loss with four \\( \\lambda \\) coefficients.\nOur results indicate that when there is limited impact on the likelihood (from the left column to the\nright column), the NLL loss has minimal impact on model performance. This suggests that NLL\nloss can be seen as a tool to regulate completion likelihood, but it remains susceptible to likelihood\nover-optimisation: higher likelihood may lead to a sub-optimal performance. We observe similar\nresults on BINARIZEDPREF using the 35B model, as shown in Figure 11 of Appendix \u00a7B.\"\n    },\n    {\n      \"title\": \"4.3 GENERALISATION AND DIVERSITY\",\n      \"content\": \"In this section, we explore the impact of model likelihood on generalisation and diversity.\n1) Lower Completion likelihood improves the models' Cross-Input Diversity. Figure 2\npresents Cross-Input Diversity (measured by distinct N-grams) of the model outputs throughout\ntraining. Specifically, within each DAA, models with lower likelihood tend to produce more diverse\noutputs. For example, the pink lines for DAAs indicate that models with lower completion likeli-\nhood typically show the highest level of Cross-Input Diversity scores throughout training. Better\noutput diversity tends to improve their generalisation to unseen scenarios, as reflected in increased\nwin probability at the early stage of the training phase. Figure 4 further demonstrates that output\ndiversity follows a similar trend under the different regularisation (i.e., Negative Log-Likelihood\nLoss), suggesting a strong correlation between likelihood and model diversity. However, it is worth\nnoting that the relationship between diversity and win probability is not linear. While some diversity\nis beneficial for generalisation, excessive diversity can lead to performance degradation, similar to\nour previous discussion in \u00a74.2. We will explore this phenomenon further in \u00a74.4.\"\n    },\n    {\n      \"title\": \"2) Higher Likelihood tends to have better memorisation of factual patterns.\",\n      \"content\": \"Figure 5 show-\ncases the relationship between model performance on NATURALQUESTIONSOPEN and TRIVIAQA\"\n    },\n    {\n      \"title\": \"4.4 SIGNALS FOR LIKELIHOOD OVER-OPTIMISATION\",\n      \"content\": \"We have demonstrated that a slight decrease in the likelihood of better completion correlates with\nimproved performance, which can be explained by an increase in output diversity. However, the\ncritical question remains: when should we stop lowering the likelihood of better completion? In this\nsection, we outline two key indicators that signal the over-optimisation of likelihood.\n1) Decreasing Entropy over Top-k tokens (Per-Input Diversity). Figure 2 and 4 presents\nPer-Input Diversity (measured by the entropy) of the model outputs throughout training. For DPO\nand IPO curves, at the beginning of the training, the Per-Input Diversity increases, signifying a\nbroader distribution of selected tokens and a more uniform output distribution for the next token\nprediction. Considering that the better completion likelihood is decreasing across the training, the\nincrease of entropy at the beginning phase indicates that those tokens from better completion have\na higher probability at the initial policy model over other tokens in the top k (here k = 10). The\ndecrease better completion likelihood gives the model a better chance to select other tokens, which\nincreases diversity and enhances generalisation, as reflected in the win probability. However, at\na certain point in training, this trend reverses. As Per-Input Diversity (entropy) starts decreasing,\nthe model begins to over-prioritise certain tokens. This suggests that those tokens in the better\ncompletion now have an overly low likelihood, lower than other tokens in the top k. Despite this,\nCross-Input Diversity keeps increasing, which indicates that the model is still generating diverse\noutputs, but now it includes tokens that are less relevant or nonsensical, i.e., tokens that humans do\nnot prefer. Notably, the turning points of entropy often coincide with those of win probability for\nDPO and IPO, as the model's outputs become less aligned with desirable outcomes.\n2) Decreasing in Probability Mass in Top k Tokens. In another scenario, the entropy of the\ntop 10 tokens continues to increase, suggesting a progressively broader and more uniform output\ndistribution (refer to the hinge curves in Figure 2). This suggests that even as the likelihood of\nbetter completions decreases, the model does not tend to over-prioritise any specific tokens during\ntraining. However, this can result in degraded model performance. As depicted in the bottom row\nof the figure, the probability mass of all top-10 tokens diminishes, leading to more random outputs,\nwith an increased likelihood of selecting tokens outside the top 10. This can introduce issues such\nas code-switching, where the model becomes prone to world-level language confusion when the\"\n    },\n    {\n      \"title\": \"5 EPILOGUE\",\n      \"content\": \"Limitations. This study primarily focuses on two models (7B and 35B), which may not fully rep-\nresent the broader spectrum of LLMs available. However, most LLMs are very standard transformers\n(Vaswani et al., 2017), and we would not expect other LLMs to behave differently.\nImplications for Practical Applications. The findings of this study have several implications for\nenhancing offline preference learning methods in practical applications: (1) Early stopping signal.\nIn practice, we can integrate entropy/probability mass monitoring into the training loop. Training\ncan employ adaptive methods like early stopping once entropy falls below a specific threshold. (2)\nAdaptive regularisation for over-optimisation. Rather than using a fixed coefficient for the NLL\nloss (Dubey et al., 2024), we could implement an adaptive regularisation based on the entropy and\nprobability mass, i.e., adding dropout or noise to prevent over-prioritisation of tokens or adding an\nexplicit regularisation term that maintains a certain degree of entropy and the probability mass of\nthe top-k tokens. While maintaining a certain degree of entropy and probability mass of the top-k\ntokens is important, care should be taken not to overly constrain the model, as some tasks inherently\nrequire a broader token distribution (e.g., give me a random number between 0 and 10).\nREPRODUCIBILITY STATEMENT\nTo ensure the reproducibility of our results, we have taken comprehensive steps to provide detailed\ninformation about our experimental setup. In Section 4.1, we offer full details on the models used\n(7B and 35B parameter models) and the datasets (ULTRAFEEDBACK and BINARIZEDPREF), in-\ncluding exact versions and sizes. While the 7B model and reward model are closed-source, and\nthe 433 prompts for the LLM-as-a-Judge framework are proprietary, we provide a summary of the\nprompt dataset to give insight into its composition. All hyperparameters for training, including\nlearning rates, batch sizes, and optimizer settings, are specified. We detail the hardware used (TPU\nv5-128/256) and provide comprehensive descriptions of all evaluation metrics. Statistical analyses,\nincluding Pearson correlation coefficients and p-values, are reported in Section 4.2. The ULTRA-\nFEEDBACK dataset is publicly available, and while BINARIZEDPREF is proprietary, we describe its\ncontents and size. Importantly, we test our findings on ULTRAFEEDBACK, which is a public dataset,\nindicating that our findings are generalisable. While some aspects could not be fully open-sourced\ndue to the use of proprietary models or data, we have described these in as much detail as possible.\nFurthermore, we posit that our findings are likely generalisable to other LLMs, as most LLMs (e.g.,\nLlama, Gemini) are based on standard transformer architectures (Vaswani et al., 2017). For exam-\nple, the Llama model family has very standard features such as RoPE embeddings (Su et al., 2024).\nIndeed, the designers note that they tried to avoid innovating on the model architecture (Dubey et al.,\n2024). As such, we would not expect significantly different behaviours. We welcome questions from\nthe community and are committed to providing additional clarification.\"\n    },\n    {\n      \"title\": \"A EVALUATION DATASETS\",\n      \"content\": \"This section provides an in-depth look at the datasets used in our experiments, focusing on the\nLLM-as-a-Judge framework, NATURALQUESTIONSOPEN, and TRIVIAQA datasets.\nLLM-as-a-Judge Framework Dataset. We utilize a diverse set of prompts for the LLM-as-a-\nJudge framework. Figure 1 illustrates a representative example from this dataset, showcasing dif-\nferent generations from various competitor models. To provide insight into the composition of our\nLLM-as-a-Judge dataset, Figure 6 presents the distribution of prompt examples. This visualisation\nhelps to understand the variety and balance of the prompts used in our evaluation framework.\nNATURAL QUESTIONSOPEN Dataset. Table 2 presents examples from the NATURALQUES-\nTIONSOPEN dataset\u00b2, showcasing the types of questions and answers used in our evaluation. The\nNATURALQUESTIONSOPEN dataset, introduced by Kwiatkowski et al. (2019), is an open-domain\nquestion-answering benchmark. It consists of English questions paired with possible answer strings,\nall answerable using English Wikipedia content. Each data instance contains a question field and an\nanswer field with potential correct responses. We use the validation set for our evaluation. Table 2\npresents representative examples from this dataset, illustrating the types of questions and answers\nused in our evaluation.\"\n    },\n    {\n      \"title\": \"C FURTHER INVESTIGATIONS FOR QUESTION ANSWERING TASKS\",\n      \"content\": \"Case studies for NATURALQUESTIONSOPEN and TRIVIAQA tasks. Table 5 provides two ex-\namples for NATURALQUESTIONSOPEN and TRIVIAQA tasks, respectively.\nLLM-as-a-Judge for the NATURALQUESTIONS OPEN task. We implement a more flexible eval-\nuation method to understand the potential issue of stylistic variations in answers. Instead of rely-\ning on exact string matching, which can be overly rigid, we employ an LLM-as-a-Judge using the\nGPT40 model. As shown in Table 4, this LLM-based evaluation system is presented with the orig-\ninal question, the reference answer, and the model's output. It then assesses whether the model's\noutput is correct, incorrect, or if there's not enough information to make a determination, responding\nwith \\\"Yes\\\", \\\"No": "or \u201cUnsure\u201d respectively. We compute the model performance based on the per-\ncentage of \"Yes\". Figure 12 shows the model performance on the ULTRAFEEDBACK dataset using\nthe 7B model. Our analysis reveals that while the LLM-as-a-Judge evaluation method demonstrates\na trend similar to the F\u2081 score, it consistently yields higher performance metrics."}]}