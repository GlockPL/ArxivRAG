{"title": "A Survey on Time-Series Distance Measures", "authors": ["JOHN PAPARRIZOS", "HAOJUN LI", "FAN YANG", "KAIZE WU", "JENS E. D'HONDT", "ODYSSEAS PAPAPETROU"], "abstract": "Distance measures have been recognized as one of the fundamental building blocks in time-series analysis tasks, e.g., querying, indexing, classification, clustering, anomaly detection, and similarity search. The vast proliferation of time-series data across a wide range of fields has increased the relevance of evaluating the effectiveness and efficiency of these distance measures. To provide a comprehensive view of this field, this work considers over 100 state-of-the-art distance measures, classified into 7 categories: lock-step measures, sliding measures, elastic measures, kernel measures, feature-based measures, model-based measures, and embedding measures. Beyond providing comprehensive mathematical frameworks, this work also delves into the distinctions and applications across these categories for both univariate and multivariate cases. By providing comprehensive collections and insights, this study paves the way for the future development of innovative time-series distance measures.", "sections": [{"title": "1 Introduction", "content": "With the advancement of techniques in sensing, networking, storage, and data processing, it has become feasible to collect, store, and process massive collections of measurements over time [106, 134, 150, 157, 170], referred to as time series. Time series analysis, with its ability to capture temporal relationships between data points, has attracted significant interest across various academic and industrial domains [78, 97, 129, 143, 161, 162, 212, 222] such as electrical engineering [104, 170], astronomy [5, 199], finance [37, 74], energy [10, 12], environment [77, 90, 147, 206], bioinformatics [16, 17, 67], medicine [49, 165, 171], and psychology [109]. In various domains and Internet-of-Things applications, the increasing volume of time series data has prompted the need for efficient techniques in data processing and analysis [61, 98, 99, 115, 133, 137].\nDistance functions, as one of the fundamental building blocks in time-series analysis, are engineered to define dissimilarity between signals [62, 159]. They have been widely used in every time-series downstream task such as similarity search [4, 58], indexing [38, 64, 152], clustering [8, 18, 103, 154, 155, 160], classification [132, 153, 169] and anomaly detection [29-35, 54, 135, 136, 151, 156, 185]. Understanding the relationships between instances, such as similarities or dissimilarities, offers valuable insights for uncovering intra-class and inter-class patterns across different domains. Since the notion of similarity is highly dependent on the context of the data and downstream tasks, a large diversity in distance measures has emerged. Different measures have been created to capture diverse notions of similarity, where the degree of similarity between two time series can vary greatly between any two measures.\nHowever, determining the dissimilarity of two time series is not a trivial task. In real-world applications, time series data can be distorted in various ways, which increases the challenge in the analysis process [154]. Distortions can include (1) scaling and translation, where two time series have different amplitudes (scaling) and/or offsets (translation); (2) shifting, where two sequences have different phases and alignment should be considered; (3) occlusion, where some subsequences of time series are missing in the dataset; (4) uniform scaling, where time series have different lengths; (5) complexity, where time series with similar shape exhibit complexity at different levels, e.g., one time series could suffer from more noise perturbation while the other may experience less. Neglecting these distortions can result in practical issues, underscoring the importance of addressing them properly. To illustrate this, consider the classic measure of Euclidean Distance (ED), which evaluates the similarity of each element of a time series compared to the corresponding element at the same time point in another series. We will refer to measures of this type as lock-step. Such measure design may suffer significant performance degradation when two compared time series exhibit the same pattern at a different temporal point (shift-distortion). Additionally, various distortions across time steps and dimensions can create further complications in the process. In special cases, significant noise levels can make it difficult to extract meaningful information from the raw data. These issues highlight the need for proper data normalization, which functions as one crucial preprocessing component to alleviate the noise issue, e.g., scaling and translation distortions.\nFor those distortions that cannot be addressed in the preprocessing stage, strategies like elaborate time series alignment have shown significant benefits in the distance measure design. For example, Dynamic Time Warping (DTW) [23] utilizes dynamic programming to find the optimal alignment between two time series. Compared with conventional lock-step measures, these \"elastic\" characteristics enable the distance measure to capture not only one-to-one mapping but also one-to-many mapping across time steps. However, this mapping process may lead to high complexity, i.e., O(n\u00b2) time with time series length n, which hinders its application in time-sensitive tasks. To reduce the time complexity, Shape-based Distance (SBD) [154], as one of the sliding measures, efficiently aligns two time series by leveraging the merit of Fast Fourier Transform (FFT). This approach reduces the time complexity to O(nlog(n))."}, {"title": "2 Preliminaries and Notations", "content": "We now introduce the formal notation for time series and distance measures. We define a univariate times-series X = {x\u2081, x\u2082,...,x\u2099} as a sequence of real-valued numbers, where n = |X| is the length of the time-series and x\u1d62 \u2208 \u211d for i \u2208 [1, n]. Given this definition, we further define a multivariate, or c-dimensional time-series X = [X\u207d\u00b9\u207e, X\u207d\u00b2\u207e, . . .,X\u207d\u1d9c\u207e] as a set of c univariate time-series of length n. Each row of the multivariate time series X represents a univariate time series X\u207d\u2071\u207e = [x\u207d\u2071\u207e\u2081, x\u207d\u2071\u207e\u2082, ..., x\u207d\u2071\u207e\u2099] for j \u2208 [1, c]. We denote X\u207d\u2071\u207e\u2c7c as the ith data point on channel j and X\u207d\u02b2\u207e the univariate time series on channel j.\nWith the definition of univariate and multivariate time series, we can define the distance measure as follows. Take univariate time series as an example. A time-series distance measure d is a function d: \u211d\u207f \u00d7 \u211d\u207f \u2192 \u211d, where d(X, Y) is the distance between time series X and Y. The multivariate time-series distance measure can be defined similarly."}, {"title": "3 Taxonomy of Time-series Distance Measure", "content": "In this section, we describe our proposed taxonomy of time-series distance measures which differentiates between measures based on 7 categories: (i) lock-step, (ii) elastic, (iii) sliding, (iv) kernel, (v) feature-based, (vi) model-based, and (vii) embedding.  This taxonomy extends beyond those found in previous surveys and evaluations [40, 58, 76, 158, 177], not only by incorporating established categories like lock-step and elastic measures, but also by introducing two additional classes - feature-based and model-based measures reflecting their widespread use in practical applications. We start by reviewing the definitions of these categories:\nLock-step measures: Lock-step measures assess the distance between two time series by comparing the i-th time step of one series with the i-th time step of the other, and aggregating their distances. This \"lock-step\" fashion assumes two time series are well aligned in the temporal order. The most well-known examples of the lock-step measure are the Euclidean Distance and Manhattan Distance.\nSliding measures: Sliding measures calculate the lock-step distances between a time series and all shifted versions of another time series, converting the distance between the two time series into the minimum of these calculated distances. This design offers robustness to noise perturbation such as shift and translation. One of the classic examples of sliding measures is the Shape-based Distance (SBD), which serves as the distance measure for the state-of-the-art clustering algorithm known as k-Shape [154].\nElastic measures: Elastic measures are based on the notion of temporal alignment, where the time series is first matched across its temporal range before similarity is computed. This addresses the phase alignment problem in time series data and allows the measure to conceptually \u201cstretch\" or \"squeeze\u201d the time axis to find the optimal alignment to maximize the similarity of the compared time series. This temporal elasticity is used to compare the time points in a one-to-one or one-to-many manner. In extending to the multivariate case, the alignment of time series can be approached in two ways: an \"independent\u201d version, where each channel is aligned separately, or a \"dependent\" version, where all channels are aligned together as a single temporal axis, accounting for their interdependencies.\nKernel measures: Kernel measures employ a mapping function that projects the time series into a higher-dimensional space, before computing their distances at this space. Such measures become instrumental in some application scenarios - such as clustering - to project via a non-linear function to a space where the clusters are more easily separable.\nFeature-based measures: Feature-based measures involve identifying and extracting descriptive (predominantly statistical) attributes, such as the mean value, overall trend, and other characteristics, to represent an entire time series."}, {"title": "4 Lock-step Measures", "content": "Lock-step measures rely on element-wise comparison between the time series. Due to their relatively low cost, i.e., O(n) complexity where n denotes the time series length, these measures have been widely applied across various fields, with the most widely used example being ED. In this section, we first review 9 well-known categories of lock-step measures [40]. We also discuss three additional lock-step measures that do not belong in these 9 categories: the Dissimilarity Metric (DISSIM) [69], the Autocorrelation Distance (ACD), and the Markovian Distance (MD) [146].\n4.1 Minkowski-based measures\nThe Euclidean distance is the classic example of a lock-step distance measure [40]. It can be calculated using the Pythagorean theorem, therefore occasionally being called the Pythagorean distance. In the late 19th century, Hermann Minkowski considered the Manhattan distance [114], which has the advantage that outliers skew the result less than using the Euclidean distance. Other names for the Manhattan distance include rectilinear distance, taxicab norm, and city block distance. Minkowski expanded the formulas for ED and Manhattan distance by using p to denote the order of the norm, thereby generalizing these distance calculations [40]. When p approaches infinity, it becomes Chebyshev distance, equivalent to finding the maximum absolute difference across all time steps. It is worth noting that, for applications with high dimensionality, lower p might be more favorable; for instance, the Manhattan distance (L1) is preferable to the Euclidean distance (L2) in these high dimensional applications because it does not extensively penalize outliers and noise [2, 158].\n4.2 L1 Functions\nThe L\u2081 functions all involve adapted versions of the Manhattan metric. S\u00f8rensen distance [186] is a normalized adaptation of the L1 distance that confines its values to the range [0, 1]. It is widely used in the fields of ecology and environmental sciences [138]. Similar to S\u00f8rensen distance, Gower distance [80] also normalizes the Manhattan distance, but in this case by the length of the time series. It has been shown to be effective for mixed continuous and categorical variables [194]. The Soergel distance [196] normalizes the L\u2081 distance using the sum of the maximum values of the corresponding elements in the two compared time series, whereas the Kulczynski distance [63] normalizes by the sum of the minimum values of respective elements from the two time series. Canberra distance has a strong sensitivity to small changes near zero, so Canberra distance is often used for data scattered around an origin [79]. Lorentzian distance [63] applies a logarithm operation to the L\u2081 distance, and the constant term is added to avoid log(0) issues. Compared with other L1 measures, Lorentzian has shown robustness to noise and outliers. In prior evaluation studies, it has been shown that Lorentzian distance, when applied with normalization strategies such as z-score, significantly outperforms Euclidean Distance in the classification downstream task [158].\n4.3 Intersection Functions\nThe intersection family of functions shares a strong connection with the L\u2081 family. Although there exist some exceptions, many similarity measures within the intersection family can be converted into the distance measures in the L\u2081 family by using the formula [40], i.e., d(X, Y) = 1 \u2212 s(X, Y), where d and s denote the dissimilarity and similarity measure between two time series. The Czekanowski distance, derived from Czekanowski similarity, $S_{Cze} = \\frac{2 \\sum_{i=1}^{n} \\min(x_i, y_i)}{\\sum_{i=1}^{n}(x_i+y_i)}$, is an application of this transformation rule. This measure is equivalent to S\u00f8rensen distance in the L\u2081 family through a mathematical formula transformation. Another transformation example could also be found in the Tanimoto distance and its corresponding member of the L\u2081 family, the Soergel distance. In prior evaluation studies [76, 158], the intersection family functions are not able to surpass the basic Euclidean distance in downstream applications. However, there is still value in these distances for specific tasks and circumstances [60, 84].\n4.4 Inner Product Functions\nThis family of methods incorporates the inner product for similarity measures, which is the sum of the element-wise multiplication of two vectors or time series (with proper transformation, the functions can be applied to measure dissimilarity). From a geometric perspective, the inner product boils down to the ratio of magnitudes between a vector, and the projection of another vector onto that first vector. By normalizing these vectors' L2-norms, this function effectively captures the angular information in space between two time series, e.g., cosine similarity. The inner product also has a direct connection to Pearson correlation, which is a measure of linear dependence between time series. Specifically, Pearson correlation boils down to the inner product of two z-normalized time series; where the normalization ensures that the time series are of the same scale and variance, and the inner product then measures the linear dependence between the two time series (i.e., vectors). This connection is further demonstrated in the following paragraph. Harmonic Mean similarity [63] calculates element-wise harmonic means between the time series. It is often used when focusing on rates of change. Kumar-Hassebrook [118] is similar to harmonic mean distance but measures the Peak-to-correlation energy; a frequently used algorithm for comparing patterns in signals from digital image sensors. Jaccard [149] and Dice distance [57] are widely used in research fields such as information retrieval [57].\nAs mentioned above, inner product functions have shown strong relationships with other families. Here, we examine the relationship between inner product, ED, Pearson correlation coefficient (PCC, will be discussed later in more detail), and Cosine similarity, and discuss how these lead to equivalence of different problems, under certain conditions. Let us first consider Inner product and Squared ED:\nInner_Product(X, Y) = $\\sum_{i=1}^{n} x_i y_i = X^T Y$\nSquared_ED(X, Y) = $\\sum_{i=1}^{n} (x_i - y_i)^2 = ||X||_2 + ||Y||_2 - 2 X^T Y$.\nWhen time series X and Y have unit lengths, the squared ED becomes 2(1 \u2013 X\u1d40Y) = 2(1 \u2013 Inner_Product(X, Y)). This means that, if ||X||\u2082 = ||Y||\u2082 = 1, the nearest-neighbor search (NSS) problem is equivalent to the problem of finding the pair of time series that maximizes the inner product, i.e., the maximum inner product search problem [180]. The Pearson correlation coefficient (PCC) also has a relationship with inner product, as follows:\nPCC(X, Y) = $\\frac{\\sum_{i=1}^{n} (x_i - \\mu_X)(y_i - \\mu_Y)}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\mu_X)^2 \\sum_{i=1}^{n} (y_i - \\mu_Y)^2}} = \\frac{X^T Y}{||X|| ||Y||}$, if X, Y are zero-centered\n= $\\frac{X^T Y}{\\sqrt{X^2} \\sqrt{Y^2}}$, if X, Y are zero-centered\n= X\u1d40Y, if X, Y are zero-centered, with unit lengths\nwhere \u03bcX and \u03bcY denote the mean value of two time series. Finally, Cosine similarity $Cosine(X, Y) = cos(\\theta) = \\frac{X^T Y}{||X|| \\sqrt{Y^2}}$, equals to InnerProduct(X, Y) when ||X||\u2082 = ||Y||\u2082 = 1.\n4.5 Squared Chord Functions\nThe squared chord functions are a collection of measures that incorporate the sum of geometric means of two time series, which is computed by summing the square roots of the products of corresponding elements from two time series. Fidelity similarity [63] is built by the sum of geometric means given two time series. Bhattacharyya distance"}, {"title": "4.6 Squared L2 Functions", "content": "The squared L2 functions, or \u03c7\u00b2 functions, are a group of distance measures that have the squared Euclidean distance as the dividend. Squared Euclidean distance takes basic form without normalization. Clark distance [63] normalizes the Euclidean distance with the sum of element pairs from two time series. Neyman \u03c7\u00b2 [148] and Pearson \u03c7\u00b2 divergence [164] derive their denominators from either the first or the second of the two time series under comparison. As these two formulas are asymmetric, they are categorized as divergence functions. To address this issue, several symmetric versions have been proposed. Probabilistic Symmetric \u03c7\u00b2 distance [63], Squared \u03c7\u00b2 distance [56], and Divergence distance [50] compute the sum of the element pairs as the denominator. This can be considered a symmetric version of the Neyman \u03c7\u00b2 distance. To the same end, Additive Symmetric \u03c7\u00b2 distance [63] computes the product of the element pairs as the denominator. Among this family, measures like Clark have been widely used in various downstream tasks such as light curve classification in astronomy [41]. Previous evaluation studies on K-Nearest Neighbor (KNN) classifier [7], have demonstrated that the symmetric structures within the Squared L2 family, including the Additive Symmetric \u03c7\u00b2 distance and Probabilistic Symmetric \u03c7\u00b2, do not inherently guarantee superior performance compared to asymmetric divergence. Among them, Squared \u03c7\u00b2 and Clark Distance have shown supreme performance compared to others."}, {"title": "4.7 Shannon's Entropy Functions", "content": "The following functions are based on Shannon's Entropy measure which has to deal with how much information a variable contains and the probabilistic uncertainty of information. Kullback-Leibler (KL) divergence [117], also relative entropy, has been widely adopted for capturing how one probability distribution diverges from the other. However, its asymmetrical nature may pose challenges in specific applications where a metric and its associated properties are needed. To solve this problem, Jeffreys distance [95, 117, 189] is considered to be the symmetric version of Kullback-Leibler distance. Jensen-Shannon distance and Tops\u00f8e distance [63] are symmetric versions of K divergence distance, which shows a structure similar to KL divergence. Tops\u00f8e distance is also referred to as information statistics [56]. Investigating the concept of information radius, which emerges from the concavity of Shannon's entropy, Sibson [181] introduced the equation of Jensen difference [40]. For its entropy nature, functions in Shannon's Entropy family are widely used in numerous downstream tasks such as time series classification, clustering, and anomaly detection [19, 122, 207]."}, {"title": "4.8 Vicissitude Functions", "content": "This group of functions is based on Vicis-Wave Hedges function [40]. Vicis-Wave Hedges distance, also named Emanon 1 distance, is a variant of the Wave Hedges function. It is built by applying the relationship between Sorensen and Canberra to Kulczynski [40]. Emanon 2 and 3 distance [40] are variants of Vicis-Wave Hedges where the squared operation is applied for the numerator and denominator. Emanon 4 distance [40] takes the sum of the squared difference over the maximum of the element pairs. Max-Symmetric \u03c7\u00b2 distance [40] takes the maximum of the Pearson and Neyman \u03c7\u00b2, while Min-Symmetric \u03c7\u00b2 distance [40] takes the minimum. Prior evaluation studies [158] demonstrate that Emanon 4 outperforms other functions within the Vicissitude family, offering significantly better performance than ED in the classification task."}, {"title": "4.9 Combination Functions", "content": "The combination functions integrate techniques from various aforementioned families. For example, Taneja measure [189] is also known as arithmetic-geometric mean divergence measure. Kumar-Johnson [119] combines strategies from symmetric \u03c7\u00b2, arithmetic-geometric mean divergence. Avg(L1,L\u221e) is the average between the L\u2081 distance and Chebyshev distance. By seizing elements from different methods, combination functions can leverage the diverse benefits each family offers, potentially yielding more versatile and robust measures under different scenarios. In prior evaluation studies [76, 158], Avg(L1,L\u221e) shows superior classification performance compared with Taneja and Kumar-Johnson."}, {"title": "4.10 Other Functions", "content": "In this section, we present three other measures that show differences from the previously mentioned families. These metrics offer more flexible handling of time.\nConventional Lockstep Measures are not designed to address variations in the temporal dimension of time series, typically assuming uniform lengths and sampling rates across time steps. Dissimilarity Metric (DISSIM) [69] accounts for these issues by calculating the definite integral of the function of time of the ED between two time series during each time interval and sums up the distance in all intervals. Formally, DISSIM [69] is defined in Table 3, where [t\u2081, tn] is the time period, DX,Y is the Euclidean distance between two points moving with linear functions of time between consecutive timestamps, and a, b, c (a > 0) are factors of the trinomial. In prior evaluation studies, it has shown to be effective on various downstream tasks such as classification and clustering [113, 158]\nPearson correlation coefficient (PCC) is a widely used similarity measure to capture the linear correlation between two data samples (with proper transformation it can be applied to quantify the distance). The formula is shown in Table 3, where \u03bcX and \u03bcY denote the mean value of each data. It has enlightened the design of many following correlation-based functions. Autocorrelation Distance (ACD) [146] calculates the autocorrelation vector that consists of autocorrelation coefficients with different lags, where the number of coefficients included is a parameter to tune. ACD distance is then defined by calculating computing the Euclidean distance between autocorrelation vectors. The autocorrelation vector, autocorrelation coefficients [146] are defined in Table 3, where u and \u03c3\u00b2 are the mean and variance of the time series. Research has shown it to be robust against both stationary and non-stationary time series [172].\nMarkovian Distance (MD) [146] defines the similarity between two time series X and Y as the probability that Y is generated using a Markov model characterized by X. MD computes such probability by first estimating a transition probability matrix M that characterizes a Markov Chain by estimating the conditional probabilities of query X. This is achieved by calculating frequencies of all sequences of length k and k + 1, where k (a parameter to be estimated) is the number of previous states the current state depends on. The distance between X and Y is then calculated as the probability of generating Y using the model of X. Since there is only one query series X, it is hard to estimate the initial states, which are instead set to be equitable at first. Entries in the transition matrix with the same prefix [Xt-1, ..., Xt-k] form a probability distribution, and the \"equally-shared probability\" of prefixes not observed in the query will be divided among other observed prefixes. Logarithmic operation is used to avoid the accumulation of machine errors. The Markovian Distance [146] is formally defined in Table 3. In real applications, both ACD and MD have been widely utilized to detect dissimilarities between data streams [9]."}, {"title": "5 Elastic Measures", "content": "For time series data with phase misalignments, stretching or squeezing of observations over the time range, or fluctuations, lock-step comparisons are not always a suitable distance measure due to the inability to consider these distortions [13, 58, 158, 201]. Figure 3 illustrates the linear mapping of ED and the one-to-many alignment of elastic measures such as Dynamic Time Warping (DTW) that captures the shape similarity of two time series.\nAmong dozens of distance measures proposed to align such distortions, elastic measures, which create a non-linear mapping between time series to align or stretch their points, have shown to be effective in numerous downstream tasks such as classification and clustering. In addition, contrary to prior beliefs, ED may not converge to the high accuracy of elastic measures with increasing dataset sizes [158].\nOne limitation of elastic measures is their quadratic time complexity (i.e., O(n\u00b2), for time series of length n), whereas the lock-step measures like ED have linear time complexity (i.e., O(n)). In large-scale settings, the higher complexity of elastic measures results in a runtime overhead often between one to three orders of magnitude over ED [13, 158, 188], which usually prevents applications from using elastic measures and rely instead on less accurate measures. Consequently, several acceleration methods, such as lower bounding and early abandoning, are developed to speedup the application of elastic measures in tasks like the K-NN search in large-scale settings.\nThis section reviews elastic measures by starting with Dynamic Time Warping (DTW), the earliest and most popular elastic measure, and provides a generalized formula to showcase the recursive (dynamic programming) computation shared by all elastic measures and highlights the different cost functions across elastic measures. Based on these different cost functions, elastic measures developed after DTW are categorized into threshold-based elastic measures and metric elastic measures [163]. Acceleration methods of elastic measurs, including lower bounding and early abandoning, are reviewed at the end of this section."}, {"title": "5.1 Dynamic Time Warping (DTW)", "content": "Dynamic Time Warping (DTW) addresses distortions or phase differences by permitting one-to-many point matching to achieve local alignment, so the two time series are aligned based on their optimal shape similarity. To find the local alignment, DTW finds an optimal alignment path and the minimum distance between two time series by computing a distance matrix, D, using the following recursive computation:\nD(i, j) = $\\begin{cases}\n (x_i - y_j)^2 & \\text{if } i, j = 1 \\\\\n D(i - 1, j) + (x_i - y_j)^2 & \\text{if } i \\neq 1 \\text{ and } j = 1 \\\\\n D(i, j - 1) + (x_i - y_j)^2 & \\text{if } i = 1 \\text{ and } j\\neq 1 \\\\\n  \\min \\begin{cases}\n D(i-1, j - 1) + (x_i - y_j)^2 \\\\\n D(i-1, j) + (x_i - y_j)^2 \\\\\n D(i, j - 1) + (x_i - y_j)^2\n  \\end{cases} & \\text{if } i, j \\neq 1\n\\end{cases}$\n(1)\nThe optimal alignment path, W = {w\u2081, w\u2082, ..., w\u209a}, which starts from the bottom-left corner and ends at the top-right corner in the matrix where the distance of alignments add up to the cell in the top-right corner:\nDTW(X, Y) = D(nx, ny) = $\\sum_{i=1}^{P} (x_{w_i[1]} - y_{w_i[2]})^2$\nThe warping path follows two properties [187]:\nBoundary Constraints: w\u2081 = (1, 1) and w\u209a = (nx, ny), meaning the optimal warping path starts on the bottom-left corner of D and ends on the upper-right corner of D.\nContinuity and Monotonicity: if w\u1d62 = (i, j) for i \u2208 [2, p \u2212 1], then w\u1d62\u208a\u2081 \u2208 {(i + 1, j), (i, j + 1), (i + 1, j + 1)}, meaning that the warping path, starting from bottom-left, only moves vertically upwards, horizontally towards the right, or diagonally towards top-right continuously until arriving at the top-right corner. DTW uses the same distance function (i.e., squared difference) in each matrix cell regardless of whether the optimal path arrives at that cell horizontally, vertically, or diagonally.\nSeveral extensions are developed for DTW to enhance its performance in terms of speed and/or accuracy, and these extensions are applicable to other elastic measures as they share the same dynamic programming structure with DTW. DTW variants with such extensions include:\nConstrained DTW: Locality constraints are commonly applied to DTW to reduce runtime and improve classification accuracy [158]; locality constraints limit the range of warping allowed to avoid unreasonably far-reaching alignments. This approach is referred to as Constrained DTW (cDTW) [173], and the most widely adopted locality constraint is the Sakoe-Chiba band [173]. Commonly referred to as the warping window, the Sakoe-Chiba band is mathematically defined as the maximum possible deviation of the alignment path from the diagonal of D, and cells further away are not computed.\nWeighted DTW: Weighted DTW (WDTW) [96] creates a weighted vector that penalizes the differences between i and j and thereby better captures the shape similarity of two time series. WDTW computes weights by adjusting the parameters of a logistic function and is capable of giving linear weights, sigmoid weights, two distinct weights, or constant weights to alignments in the warping path.\nDerivative DTW: Derivative DTW [108] improves DTW's ability to capture shape similarities by replacing each element of the original two time series with a \"derivative\" value that measures the changes in the shape of the time series at that time step. For noisy datasets, exponential smoothing can be applied before computing the \"derivative\" to further improve shape similarity capture."}, {"title": "5.2 Threshold-based and Metric Elastic Measures", "content": "Numerous elastic measures are proposed after DTW to overcome its limitations such as not being a metric or poor performance on noisy datasets. To these ends, subsequent elastic measures adopt different cost functions for diagonal and vertical/horizontal movements (whereas DTW uses the same squared difference cost function for all movements) and sometimes introduce additional parameters; however, they all use a dynamic programming approach to find the optimal warping path, which could be generalized as:\nD(i, j) = $\\begin{cases}\ninitial_distance(x_i, y_j) & \\text{if } i, j = 1 \\\\\n  \\min \\begin{cases}\n D(i - 1, j) + dist_V(x_i, y_j) \\\\\n D(i, j - 1) + dist_H(x_i, y_j) \\\\\n D(i - 1, j - 1) + dist_D(x_i, y_j)\n  \\end{cases} & \\text{if } i, j \\neq 1 \\\\\n D(i - 1, j) + dist_V(x_i, y_j) & \\text{if } i \\neq 1 \\text{ and } j = 1 \\\\\n D(i, j - 1) + dist_H(x_i, y_j) & \\text{if } i = 1 \\text{ and } j\\neq 1\n\\end{cases}$\n(2)\nwhere distD(x\u1d62, y\u2c7c), distV(x\u1d62, y\u2c7c), and distH(x\u1d62, y\u2c7c) are the cost functions for diagonal, vertical, and horizontal movements, respectively, and initial_distance(x\u1d62, y\u2c7c) is the cost function for initial alignment in D(1, 1). Based on different types of cost functions, elastic measures are categorized into Threshold-based and metric elastic measures.\nThreshold-based Elastic Measures: To improve DTW's ability to handle outliers in noisy datasets, threshold-based elastic measures use a threshold parameter \u03f5 to decide whether two elements match or not; such binary classification of the relationship between two elements from two time series regardless of their numerical difference makes threshold-based elastic measures robust against outliers. In computing the cost matrix, a match and mismatch correspond to a diagonal cost or horizontal/vertical cost respectively.\nLongest Common Subsequence (LCSS) [198], initially developed for pattern matching within text strings, has been adapted to evaluate similarity between time series data. In this context, LCSS increases the similarity score by 1 for each matching and by 0 for each mismatch. The resulting LCSS distance measures the similarity of the two time series, and the transformation function is applied to the upper-corner cell in the diagonal matrix to convert the similarity score to a distance measure.\nEdit Distance on Real Sequences (EDR) [44] is an adaptation of edit distance for strings to time series distance measure. EDR achieves robustness against outliers by quantizing the distance between elements to either 0 or 1, thus reducing the impact of outliers.\nSequence Weighted Alignment (SWALE) [58] generalizes EDR by incorporating a parameter r for a match and a penalty parameter p for a mismatch, instead of fixed 1 and 0 as in EDR.\nMetric Elastic Measures: DTW and threshold-based measures are not metric distances and cannot use the triangle inequality [182] to take advantage of generic indexing methods [87, 88, 215], clustering methods [36, 71, 93], and pruning methods [43] designed for metric distances. Therefore, several metric elastic measures with different cost functions were proposed."}, {"title": "5.3 Acceleration Methods for Elastic Measures", "content": "The most common methods for accelerating elastic measures in applications such as classification involves the use of lower bounding or early abandoning.\nEarly Abandoning (EA): In nearest neighbor search tasks", "167": "monitors the accumulated distance during computation of a full elastic measure distance and abandons the computation when the accumulated distance is greater than the \"abandoning criterion\". Instead of returning an exact distance", "LB)": "LBs are distance measures that approximate the corresponding elastic measure distance without computing the full distance matrix. The LB distance is always less than or equal to the full elastic measure distance and thus can be used to filter out unpromising candidates in the nearest neighbor search, where full elastic measure distance is not computed for data time series with LB distances greater than existing nearest neighbor. Research efforts have concentrated in developing LBs for DTW, and DTW LBs have inspired the development of several"}]}