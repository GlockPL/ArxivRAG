{"title": "An Effective Information Theoretic Framework for Channel Pruning", "authors": ["Yihao Chen", "Zefang Wang"], "abstract": "Channel pruning is a promising method for accelerating and compressing convolutional neural networks. However, current pruning algorithms still remain unsolved problems that how to assign layer-wise pruning ratios properly and discard the least important channels with a convincing criterion. In this paper, we present a novel channel pruning approach via information theory and interpretability of neural networks. Specifically, we regard information entropy as the expected amount of information for convolutional layers. In addition, if we suppose a matrix as a system of linear equations, a higher-rank matrix represents there exist more solutions to it, which indicates more uncertainty. From the point of view of information theory, the rank can also describe the amount of information. In a neural network, considering the rank and entropy as two information indicators of convolutional layers, we propose a fusion function to reach a compromise of them, where the fusion results are defined as \"information concentration\". When pre-defining layer-wise pruning ratios, we employ the information concentration as a reference instead of heuristic and engineering tuning to provide a more interpretable solution. Moreover, we leverage Shapley values, which are a potent tool in the interpretability of neural networks, to evaluate the channel contributions and discard the least important channels for model compression while maintaining its performance. Extensive experiments demonstrate the effectiveness and promising performance of our method. For example, our method improves the accuracy by 0.21% when reducing 45.5% FLOPs and removing 40.3% parameters for ResNet-56 on CIFAR-10. Moreover, our method obtains loss in Top-1/Top-5 accuracies of 0.43%/0.11% by reducing 41.6% FLOPs and removing 35.0% parameters for ResNet-50 on ImageNet. In object detection, our method yields an mAP of 37.6% with 25.55M parameters for RetinaNet on COCO2017.", "sections": [{"title": "I. INTRODUCTION", "content": "CONVOLUTIONAL Neural Networks (CNNs) demonstrates its superiority in computer vision tasks such as image classification [1]\u2013[4], object detection [5]\u2013[8], semantic segmentation [9]\u2013[11]. However, these models have a large number of parameters and a high computational cost, making them difficult to deploy on mobile and embedded devices. Even if the network is specially designed with an efficient architecture (e.g., residual connection [4], inception module [3]), over-parametrization still remains a problem. To solve this problem, a model with a small memory footprint and low computation overhead but high accuracy is required. There are two categories of network pruning that can be used to accelerate and compress a model, i.e., unstructured pruning [12]\u2013[18] and structured pruning [19]\u2013[23]. Unstructured pruning methods (e.g., weight pruning) prune the unimportant weights of the network to produce a sparse tensor. Nevertheless, the model achieved by weight pruning can only be integrated into BLAS libraries [24] and can only be accelerated with specialized hardware [25]. On the contrary, structured pruning (e.g., filter pruning, channel pruning) methods discard the entire filters or channels in a network. As a result, since the entire filters or channels are discarded, they can achieve a lightweight network that does not require specialized software or hardware. To this end, we concentrate on channel pruning for the purpose of reducing parameters and computational costs while maintaining model accuracy, with the goal of providing a scheme for deploying the neural network in a resource-limited device.\nHow to appropriately assign the pruning rate per layer is a difficult problem of channel pruning. Recent studies [26]\u2013[28] empirically pre-define the pruning rate for each layer. However, they do not provide a reliable explanation for the setting of the layer-wise pruning rates. Heuristic tuning is typically required to determine how to set an appropriate pruning rate per layer [29]. Another critical issue is how to identify unimportant channels. Previous algorithms used multiple criteria to describe the importance of the channels [26], [30]\u2013[32], but these properties lack of interpretability when used to describe the importance of the channels in a convolutional layer. In other words, they do not provide a full explanation of the reason that the channels contribute less and are decided to be discarded. For the purpose of solving the two aforementioned issues, we present a novel method of channel pruning. Fig. 1 demonstrates the framework of our method, which uses the information concentration of convolutional layers to assign layer-wise pruning ratios and channel contributions to prune the layers. During forward propagation, each layer makes different contributions to the final output of the neural network, which can be thought of as the information flowing through each layer. As a result, depending on the information that the convolutional layers contain, it is possible to assign various numbers of channels for pruning multiple layers. The information redundancy and amount in a tensor are described by the rank and information entropy (which we refer to as \u201centropy\u201d for convenience) [26], [33]. We use a \"fusion\" of these two indicators to summarize how they characterize the data from various convolutional layers in the pre-trained model. The fusion values indicate the information that the convolutional layers contain, where the larger ones denote the corresponding layers are more important. According to the results, we employed the channel pruning method, selecting a larger number of channels for pruning in the less informative layers. In the pruning stage, we propose a theoretical foundation: pruning the channels that have the least contributions to the optimization of loss. This approach differs from previous methods, such as magnitude-based pruning, where channels are removed based on their importance scores [26], [30], [31], [33]. Shapley values are a potent tool in interpretability of deep learning [34], [35], and are naturally suited for assessing the channel contributions because they can fairly distribute the average marginal con-tributions to them and explicitly model the importance of the channels with feature attribution explanation. We also calculate the Shapley values along with the rank and entropy of the layers. Pruning has a less adverse effect on the performance of a model for the channels with the lowest Shapley values since they have fewer contributions to optimization. For the purpose of reconstructing the accuracy of the pruned model, we re-train the network again.\nContributions: The following is a summary of our contri-butions: (1) For the purpose of obtaining an overall indicator of the information concentration of convolutional layers, we propose a fusion function that reaches a compromise of the information indicated by rank and entropy. According to the fusion values, we determine the layer-wise pruning rates. (2) We propose that an appropriate pruning criterion can be the contributions to the optimization of loss and recommend using Shapley values as a potent tool with the interpretability of CNNs to assess the channel contributions. (3) Extensive experiments for pruning architectures on CIFAR-10 [36] and ImageNet [37] using VGGNet [2], ResNet [4], and DenseNet [38] for image classification, and COCO2017 [39] using RetinaNet [40], FSAF [41], ATSS [42], and PAA [43] for object detection, demonstrate the effectiveness and superiority of our method."}, {"title": "II. RELATED WORKS", "content": "Pruning criteria: Previous methods in terms of structured pruning prune a model at multiple levels (e.g., filter and channel), where channel pruning correlates to filter pruning [44], [45]. The capacity and complexity of a model are reduced due to discarding the filters or channels, but this inevitably degrades the model's accuracy. Therefore, it is generally ac-cepted that removing the least important filters or channels will minimize the loss in accuracy. For the purpose of achieving this target, the question of how to recognize the unimportant filters or channels is raised. Prior study [46] approximates the change in the loss function to determine the importance of the filters, then removes the least important ones. The importance of the filters is estimated using a variety of criteria, including the geometric median [32], l\u2081-norm [30], l2-norm [31], rank [26], and entropy [33]. The Average Percentage of Zeros (APoZ) is used as the importance score for the channel-level pruning method [47], which determines each channel's sparsity based on it. In addition, CP [27] discards the filters based on LASSO regression that produce the less informative channels along with the corresponding channels in the next layer, and ThiNet [28] removes a channel underlying the statistics information calculated from the next layer. The most recent works [48], [49] also make use of fisher information as an importance scoring metric and CHEX [50] uses column subset selection to remove the channels that are not impor-tant. White-Box [51] and MFP [52] empirically assign layer-wise pruning ratios using class contribution and geometric distance, respectively. CLR-RNF [53] employs a computation-aware measurement to determine pruning ratios, pruning more weights in less computation-intensive layers. CATRO [54] pre-defines layer-wise pruning ratios using class information from a few samples to measure the joint impact of multiple channels. ABP [55] discards fewer filters in significant layers and more in less significant ones, after examining each layer's training importance. The MDP [56] framework enables multi-dimensional pruning in convolutional neural networks. The CLFIP [57] method incorporates classification loss and feature importance into the layer-wise channel pruning process. PCP [58] iteratively prunes channels and estimates the subsequent accuracy drop. For point cloud neural networks, the JP [59] framework reduces redundancies along multiple dimensions. The 3D-P [60] framework prunes based on the importance of each frame or point for 3D action recognition.\nPruning rate: Recent research assigns the layer-wise prun-ing rates for convolutional layers in accordance with various rules when the model is enforced with a filter or channel sparsity. The pruning rate for the convolutional layers is pre-defined in rule-based methods, demonstrating that we are aware of the proportion (number) of filters or channels which will be discarded beforehand [29]. Early works [31]\u2013[33], [61] adopt a constant percentage to discard the filters or channels in each layer. On the contrary, HRank [26], ThiNet [28] and CP [27] empirically assign various layer-wise pruning ratios. In addition, NISP [62] pre-defines the layer-wise pruning ratios based on a variety of requirements, including FLOPs, memory and accuracy. After examining the sensitivity of each convolutional layer, CC [63] and PFEC [30] discard fewer filters in the sensitive layers and remove more filters in the insusceptible layers. For the purpose of calculating the layer-wise pruning ratios, SPP [64] performs Principal Component Analysis (PCA) on each layer and uses the reconstruction error to evaluate the sensitivity. Network Slimming [65] first trains with channel-level sparsity-induced regularization before establishing a global pruning rate to discard the channels for the entire network. In addition, AMC [66] uses DDPG [67] introduced in Reinforcement Learning to learn a precise pruning ratio per layer.\nPruning schedule: A network can be pruned using a variety of pruning schedules, which can generally be divided into three categories. (1) One-shot [29], [30]: remove the filters or channels of multiple convolutional layers in the network at once. NISP [62] prunes the CNNs by removing the least important neurons and then fine-tuning the compact network to preserve its predictive power. PFEC [30] and CC [63] prune the layers at once and further fine-tune the pruned network. Different from PFEC [30] which prunes the unimportant weights according to l\u2081-norm only, GReg [68] first imposes a l2-norm penalty to drive the unimportant weights to zero and prune those with the least l\u2081-norm until the penalty factor for the weight is updated to reach a pre-set ceiling. By using generative adversarial learning, GAL [69] prunes the filters along with other structures, such as channels, branches, and blocks, in an end-to-end and label-free paradigm. (2) Progressive [29]: the network under a simultaneous training and pruning process, and the network sparsity gradually goes from zero until the target number. The method [46] alternates iterations of pruning the least important neuron and fine-tuning until the the pre-determined desired trade-off between accuracy and pruning objective is reached. SFP [31] and FPGM [32] prune the filters based on their importance at the end of each training epoch. DSA calculates the gradient-based optimization to discover the layer-wise pruning ratios and prunes the network when training the network from scratch. Prior work [48] discards the least important channel and the corresponding coupled ones at every several iterations until the FLOPs reduces to the desired amount. (3) Iterative [29], [30]: discard the unimportant filters or channels and fine-tune the pruned network, then the process is repeated until the model achieves the desired sparsity. Previous studies [28], [33] prune the pre-trained network and fine-tune the pruned model layer by layer with one or two epochs, and finally re-train the model with more additional epochs when all the layers are pruned. Without retraining, HRank [26] fine-tunes the pruned network in a layer-by-layer fashion. An iterative two-step process is used by CP [27] to prune each layer, and after pruning the network is fine-tuned. Additionally, there are works [66], [70] which prune and fine-tune the network repeatedly up to the pre-determined number of iterations, and the pruned network is fine-tuned after thorough pruning. SCL [71] applies PCA to each layer, using network connections as regularization. It initially trains with sparsity-induced regularization, then sets a global pruning rate, and finally uses reinforcement learning to optimize layer-specific pruning ratios.\nDiscussion: In contrast to unstructured pruning, the model produced by structured pruning algorithms can be easily inte-grated into BLAS libraries and deployed to resource-limited devices. As far as we know, our method, focusing on channel pruning, is the first to assign layer-wise pruning ratios for convolutional layer pruning based on the rank and entropy for the convolutional layers in the unpruned model, which offers a more reliable method on the setting of the layer-wise pruning ratios. In addition, our method uses Shapley values to explain the contributions of the channels rather than importance-based criteria, and it discards the channels with the fewest contributions to accelerate the model during training and inference while retaining accuracy."}, {"title": "III. METHODOLOGY", "content": "In a CNN-based model, for the ith convolutional layer Ci where 1 \u2264 i \u2264 L and L denotes the total number of convolutional layers, we assume that the number of input channels and output channels are represented by Ci and Ci+1, respectively. In C\u2081, the height and weight of the feature maps are denoted as hi and wi, respectively. A set of filters K\u2081 = {K1, K2,..., Kci } \u2208 Rci \u00d7 Ci\u22121\u00d7ki\u00d7ki is in the ith convolutional layer, where K \u2208 Rci-1\u00d7ki\u00d7ki represents the jth filter of the ith convolutional layer, j \u2208 {1, 2, ...,ci}and ki denotes the kernel size of the filter. Applying Ci+1 filters on Ci channels transforms the input feature maps Mi \u2208 RCi \u00d7 hi \u00d7 Wi into output feature maps Mi+1 \u2208 RCi+1\u00d7hi+1\u00d7Wi+1. In the convolutional layers, ci channels are split into two groups, i.e., the discarded channels Qi = {C1, C2,..., Cqi } and the remaining channels Ui = {CU1, CU2,..., CUui }, where qi and ui are the number of discarded and remaining channels, respectively. For Qi, Ui, qi and ui, we have:\n$\\begin{cases}\nQ_i \\cup U_i = C_i, \\\\\nQ_i \\cap U_i = \\emptyset, \\\\\nQ_i + U_i = C_i.\n\\end{cases}$\nLet an indicator function I denote whether the jth channel in the ith convolutional layer is pruned, then we have:\n$I = \\begin{cases}\n0, C \\in Q_i, \\\\\n1, C \\in U_i.\n\\end{cases}$\nAssume that \u03c3(C) represents the importance of the jth channel in the ith convolutional layer, and we sort \u03c3(C) to prune ui unimportant channels. Therefore, channel pruning can be defined as the optimization problem as below:\n$\\min_I \\sum_{i=1}^L \\sum_{j=1}^{C_i} I \\sigma(C_i^j), \\\\\ns.t. \\sum_{j=1}^{C_i} I = q_i$\nwhich demonstrates that our target is to discover the least im-portant channels for the purpose of minimizing the information of the removed channels."}, {"title": "B. Information Concentration", "content": "Previous works [28], [31], [32], [61] remove the same proportions of filters in the convolutional layers, and the works [26], [27] empirically assign layer-wise pruning ratios. Different from them, we intend to assign layer-wise pruning ratios with information theory. We start by investigating the rank and entropy of the convolutional layer outputs in the pre-trained model to identify the information redundancy and amount that is contained in a layer. Then, we obtain the multiplication of them as a \u201cfusion\", which can be seen as the information concentration of the layers, taking into account the rank and entropy as two information indicators of convolu-tional layer outputs. If a convolutional layer is less informative, we can assign a larger number to prune the channels in the corresponding layer. A low-rank tensor typically contains lots of redundant information. If we suppose a matrix as a system of linear equations, a higher-rank matrix represents there exist more solutions to it, which indicates more uncertainty. From the point of view of information theory, the rank can describe the amount of information. In the case of channel pruning, a higher-rank tensor of the channel in a convolutional layer is more informative than the lower-rank one [26]. Additionally, a layer with a low average rank per channel suggests that multiple channels in it probably contain repetitive information, making it possible to compress it into a more compact layer with negligible loss in important information. Identifying the importance of features via rank provides a way to identify the redundant features with interpretability. To accurately calculate the rank of a matrix, we use the Singular Value Decomposition (SVD) method. First, we decompose the matrix A using SVD, yielding orthogonal matrices U,\u03a3, and VT. Then, we count the number of non-zero singular values in the diagonal matrix, which is the rank of matrix A. Compared to directly calculating the matrix rank, this method avoids the NP-hard problem and can obtain a relatively accurate rank estimate. The key insight in SVD is that the number of non-zero elements in the \u2211 matrix is equivalent to the rank of the matrix A. Hence, we can compute the rank of the matrix A by performing SVD decomposition and counting the number of non-zero elements in the \u2211 matrix. Previous work [26] demonstrated the rank of each convolutional layer almost remains the same under various image batches. Inspired by the conclusion, we discover that a small portion of images can generate the outputs to estimate the rank for convolutional layers, as demonstrated in Figs. 2a ~ 2d. Therefore, in the dataset with N images, we first randomly fetch out B images and feed them to the network for the sum of rank for the outputs per layer. Then, for the ith convolutional layer Ci, we can get the average rank per channel as:\n$R(C_i) = \\frac{\\sum_{b=1}^B \\sum_{j=1}^{C_i} \\text{Rank}(K(b, j, :, :)) }{C_i}$ \nGenerally, entropy measures the disorder of a system or the uncertainty of an event. In information theory, entropy can be regarded as the expected amount of information [72]. A low-entropy convolutional layer suggests that the channels inside it contain less information in the channel pruning scenario. Thus, we could remove more channels in such convolutional layers. Entropy is naturally fit for selecting features by the amount of information they contain, therefore, it provides a more interpretable solution for identifying less informative convolutional layers. Additionally, we find that similar to the calculation for the rank of convolutional layers, the estimation of entropy for the convolutional layers may be generated by feeding only one or more batches of images, as shown in Figs. 2e ~ 2h. Given B input images, we first use the softmax function to map C between 0 and 1, after which the outputs can be regarded as the probability distribution for the channels in Ci:\n$p(C_i^j) = \\text{Softmax}(C_i^j) = \\frac{e^{K(b,:,:,:) }}{\\sum_{b=1}^B \\sum_{j=1}^{C_i} e^{K(b, j, :, :)}}$\nAfter obtaining the probability distribution of the channels in a convolutional layer, we can get the average entropy per channel of Ci as:\n$E(C_i) = \\frac{\\sum_{j=1}^{C_i} p(C_i^j) \\log p(C_i^j)}{C_i}$\nAs an off-line analysis, we first obtain the average statistics of rank and entropy per channel of convolutional layer outputs via Eqn. 4 and Eqn. 6 on the pre-trained model. They can be considered as two information indicators of the convolutional layers, which measure the information from different aspects. The sub-figures in Fig. 2 illustrate that under various image batches, the rank and entropy for convolutional layer outputs nearly remain unchanged with minor fluctuations. Addition-ally, the internal variations in rank and entropy are not entirely consistent. Therefore, we normalize them to [l, m] and propose a fusion function that reaches a compromise of these two indicators to obtain an overall indicator as the information con-centration of convolutional layers, reducing the inconsistencies and employing the information complementarity between these two indicators:\n$O(C_i) = \\prod_{X, Y} \\left((m - l) \\frac{X - \\min Y}{\\max Y - \\min Y} + l\\right)$\nwhere X represents R(Ci) and E(Ci), and Y represents {R(C\u2081)} and {E(C\u2081)} di \u2208 {1,2, . . ., L}, respectively. After obtaining O(Ci), we also normalize it to [l,m]. The overall indicator offers a trustworthy representation of the information that the channels in the convolutional layers contain since it includes rank and entropy characterizations. Though rank and entropy can separately provide solutions with interpretability for uninformative layers, combining them eliminates some potential errors they themselves may have partially caused. After obtaining the fusion value, we assign ui channels to prune the ith convolutional layer. Since we consider the layers with lower fusion values as being less informative, ui in such layers should be set to a higher value. Otherwise, we prune fewer channels in the layers."}, {"title": "C. Channel Pruning via Shapley Values", "content": "The concept of Shapley value comes from the field of cooperative game theory and is based on a scenario in which members cooperate in a team. They receive a reward that is meant to be equally allocated to each player based on their unique contributions, where a contribution is known as Shapley value [34]. Shapley values are frequently used to interpret a \u201cblack-box\u201d deep neural network [73], [74], due to its rationality in evaluating the importance of features. Therefore, we extend the concept to the case of channel pruning: By considering a convolutional layer in a CNN-based model as a game in which the channels collaborate to produce an output, we can distribute the layer-wise outcomes to each of them. We suppose that a set P = {1,2,..., n} is comprised of n members who participate in cooperation, and the subsets C P represents an alliance consisting of two or more members. If for any s, a corresponding real number e(s) satisfies:\n$\\begin{cases}\ne(\\emptyset) = 0. \\\\\n\\forall \\text{disjoint subsets } s_1, s_2 \\subset P, \\\\\ne(s_1 \\cup s_2) \\ge e(s_1) + e(s_2).\n\\end{cases}$\nthen e(s) can be defined as a characteristic equation on P. The probability if the player a will join the alliance s\\{a} is calculated as:\nv(s) = \\frac{(|s| - 1)!(n - |s|)!}{n!}\nwhere |s| represents the number of elements in s. The player a's marginal contribution to all of the alliances which comprise a is calculated as:\ng_a(e) = \\sum_{s \\in S_a} (e(s) - e(s\\{a})),\nwhere Sa denotes the set that contains the player a from all subsets, and e(s) \u2013 e(s\\{a}) denotes the marginal player's contribution, i.e., the player's contribution in alliance s. Thus, the Shapley value that the player a generates is calculated as:\nw_a(e) = \\sum_{s \\in S_a} v(s)(e(s) - e(s\\{a})) x g_a(e).\nSince the Shapley value wa(e) is proportional to the marginal contribution ga(e), it reflects a player's contribution to the cooperation. In the case of convolutional neural networks, for simplicity, a deep CNN-based neural network F can be written as:\nF = F(1)F(2)\u3002... F(L)\nwhere F(1), F(2),...,F(L) is the forward functions layer by layer. We regard ci channels {C1, C2,...,Cci } as Ci members in the set Ci. The function F maps each subset rCi of channels from activation outputs to real numbers for modeling the outcomes. The Shapley value of the channel C is calculated as:\nw_{c_i}(F) = \\sum_{r \\in S_c \\atop r \\subset C_i} \\frac{(|r| - 1)!(c_i - |r|)!}{C_i!} (F(r) - F(r\\{C_i^j})).\nTherefore, $w_{C_i}(F)$ represents the importance of the channel, which provides us a reliable basis to discard the least important channels in network pruning. Thus, we can reformulate Eqn. (3) as:\n$\\min \\sum_{i=1}^L \\sum_{j=1}^{C_i} I w_{C_i^j}(F), \\\\\ns.t. \\sum_{j=1}^{C_i} I = q_i$"}, {"title": "D. Pruning Schedules", "content": "In the pruning phase for image classification, three pruning schedules are employed by our method to prune the channels over the convolutional layers [29]:\n1) One-shot: We discard the channels from the convolu-tional layers with the lowest Shapley values representing contributions at once.\n2) Iterative static: We prune each layer and fine-tune the compact network for several epochs iteratively, and the unimportant channels are determined by the Shapley val-ues calculated in the pre-trained model. \"Static\" implies that the channel contributions are represented by the initial Shapley values of them.\n3) Iterative dynamic: We prune and fine-tune the network similar to the iterative static paradigm, but the channel contributions are generated during the training and prun-ing procedure. \"Dynamic\" implies the Shapley values are calculated after fine-tuning, but not from the initial pre-trained network.\nAlg. 1 demonstrates our method with an iterative static pruning schedule, which we use in the experiments on CIFAR-10. First, we use the weights of the pre-trained model to initialize the network. Next, we propose a fusion function to get the overall indicator of rank and entropy for the convolutional layers under image batches. According to the fusion values, we assign the number of channels to be removed for the layers. By forward-passing the model to the batches of sampled images, we also obtain the Shapley values for the channels in the convolutional layers. During the pruning stage, we considered that the channels in a convolutional layer can be regarded as the team members in cooperation. Therefore, the Shapley values can be the reflection of their average marginal contributions to the layer output. After sorting the Shapley values, the lowest ones indicate the corresponding channels with the fewest contributions. Then, we discard the least important channels. After pruning a layer, to lessen the loss brought on by pruning, we fine-tune the model with several epochs. The procedure repeats until the final layer is pruned. Until the process of layer-wise pruning and fine-tuning ends, we retrain the pruned model to decrease the error caused by pruning. The accuracy of the compressed model is then restored. In our experiments, we demonstrated initializing with weights of pruned models outperformed random initialization, which we decided as the initialization method in re-training."}, {"title": "IV. EXPERIMENTS", "content": "Benchmark datasets and models: We evaluate the perfor-mance of our pruning method on CIFAR-10 and ImageNet for image classification. The CIFAR-10 and ImageNet dataset contains 60,000 32 \u00d7 32 images with 10 classes and 1.28 million 224 \u00d7 224 images with 1,000 classes, respectively. We conduct the experiments on VGGNet with a plain structure, ResNet with a residual structure, and DenseNet with dense blocks. We randomly select 1,024 and 128 images for the architectures on CIFAR-10 and ImageNet for the purpose of estimating the average statistics of rank and entropy for convolutional layer outputs, and then we combine these two indicators to obtain the results. We set [1,10] as the scaling range of rank, entropy, and fusion values as Eqn. 7 introduced. In addition, we also evaluate the performance of our method in object detection on COCO2017, where the architectures are RetinaNet, FSAF, ATSS and PAA. COCO2017 contains 163,957 images with 80 categories, and 118,287 and 5,000 images are in the training set and validation set, respectively. Following the previous works [5]\u2013[7], we train the models on the training set and evaluate their detection capabilities on the validation set.\nEvaluation metrics: We use Top-1 accuracy and Top-1/Top-5 accuracies on the test set to assess the image classifi-cation capabilities on CIFAR-10 and ImageNet, respectively. We use Float Points Operations (denoted as FLOPs) and the parameters to assess the computational overhead and the capacity of the pruned models, respectively. We evaluate the accuracy drop for performance, FLOPs reduction for acceler-ation and parameters reduction for compression in different methods. For object detection, we use mAP (mean average precision) to evaluate the detection capabilities of the models, and also use the memory footprint to estimate the scale of the models."}, {"title": "B. Results and Analysis", "content": "VGGNet: In comparison to SSS and CP", "channels.\nResNet": "For ResNet-20", "0.14%.\nDenseNet": "Based on the results for pruning DenseNet-40", "blocks.\nFSAF": "For FSAF, when normalizing the Shapley values"}]}