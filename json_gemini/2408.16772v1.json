{"title": "An Effective Information Theoretic Framework for Channel Pruning", "authors": ["Yihao Chen", "Zefang Wang"], "abstract": "Channel pruning is a promising method for accelerating and compressing convolutional neural networks. However, current pruning algorithms still remain unsolved problems that how to assign layer-wise pruning ratios properly and discard the least important channels with a convincing criterion. In this paper, we present a novel channel pruning approach via information theory and interpretability of neural networks. Specifically, we regard information entropy as the expected amount of information for convolutional layers. In addition, if we suppose a matrix as a system of linear equations, a higher-rank matrix represents there exist more solutions to it, which indicates more uncertainty. From the point of view of information theory, the rank can also describe the amount of information. In a neural network, considering the rank and entropy as two information indicators of convolutional layers, we propose a fusion function to reach a compromise of them, where the fusion results are defined as \"information concentration\". When pre-defining layer-wise pruning ratios, we employ the information concentration as a reference instead of heuristic and engineering tuning to provide a more interpretable solution. Moreover, we leverage Shapley values, which are a potent tool in the interpretability of neural networks, to evaluate the channel contributions and discard the least important channels for model compression while maintaining its performance. Extensive experiments demonstrate the effectiveness and promising performance of our method. For example, our method improves the accuracy by 0.21% when reducing 45.5% FLOPs and removing 40.3% parameters for ResNet-56 on CIFAR-10. Moreover, our method obtains loss in Top-1/Top-5 accuracies of 0.43%/0.11% by reducing 41.6% FLOPs and removing 35.0% parameters for ResNet-50 on ImageNet. In object detection, our method yields an mAP of 37.6% with 25.55M parameters for RetinaNet on COCO2017.", "sections": [{"title": "I. INTRODUCTION", "content": "CONVOLUTIONAL Neural Networks (CNNs) demonstrate its superiority in computer vision tasks such as image classification [1]\u2013[4], object detection [5]\u2013[8], semantic segmentation [9]\u2013[11]. However, these models have a large number of parameters and a high computational cost, making them difficult to deploy on mobile and embedded devices. Even if the network is specially designed with an efficient architecture (e.g., residual connection [4], inception module [3]), over-parametrization still remains a problem. To solve this problem, a model with a small memory footprint and low computation overhead but high accuracy is required. There are two categories of network pruning that can be used to accelerate and compress a model, i.e., unstructured pruning [12]\u2013[18] and structured pruning [19]\u2013[23]. Unstructured pruning methods (e.g., weight pruning) prune the unimportant weights of the network to produce a sparse tensor. Nevertheless, the model achieved by weight pruning can only be integrated into BLAS libraries [24] and can only be accelerated with specialized hardware [25]. On the contrary, structured pruning (e.g., filter pruning, channel pruning) methods discard the entire filters or channels in a network. As a result, since the entire filters or channels are discarded, they can achieve a lightweight network that does not require specialized software or hardware. To this end, we concentrate on channel pruning for the purpose of reducing parameters and computational costs while maintaining model accuracy, with the goal of providing a scheme for deploying the neural network in a resource-limited device.\nHow to appropriately assign the pruning rate per layer is a difficult problem of channel pruning. Recent studies [26]\u2013[28] empirically pre-define the pruning rate for each layer. However, they do not provide a reliable explanation for the setting of the layer-wise pruning rates. Heuristic tuning is typically required to determine how to set an appropriate pruning rate per layer [29]. Another critical issue is how to identify unimportant channels. Previous algorithms used multiple criteria to describe the importance of the channels [26], [30]\u2013[32], but these properties lack of interpretability when used to describe the importance of the channels in a convolutional layer. In other words, they do not provide a full explanation of the reason that the channels contribute less and are decided to be discarded. For the purpose of solving the two aforementioned issues, we present a novel method of channel pruning. Fig. 1 demonstrates the framework of our method, which uses the information concentration of convolutional layers to assign layer-wise pruning ratios and channel contributions to prune the layers. During forward propagation, each layer makes different contributions to the final output of the neural network, which can be thought of as the information flowing through each layer. As a result, depending on the information that the convolutional layers contain, it is possible to assign various numbers of channels for pruning multiple layers. The information redundancy and amount in a tensor are described by the rank and information entropy (which we refer to as \u201centropy\u201d for convenience) [26], [33]. We use a \"fusion\" of these two indicators to summarize how they characterize the data from various convolutional layers in the pre-trained model. The fusion values indicate the information that the convolutional layers contain, where the larger ones denote the corresponding layers are more important. According to the results, we employed the channel pruning method, selecting a larger number of channels for pruning in the less informative layers. In the pruning stage, we propose a theoretical foundation: pruning the channels that have the least contributions to the optimization of loss. This approach differs from previous methods, such as magnitude-based pruning, where channels are removed based on their importance scores [26], [30], [31], [33]. Shapley values are a potent tool in interpretability of deep learning [34], [35], and are naturally suited for assessing the channel contributions because they can fairly distribute the average marginal con-tributions to them and explicitly model the importance of the channels with feature attribution explanation. We also calculate the Shapley values along with the rank and entropy of the layers. Pruning has a less adverse effect on the performance of a model for the channels with the lowest Shapley values since they have fewer contributions to optimization. For the purpose of reconstructing the accuracy of the pruned model, we re-train the network again.\nContributions: The following is a summary of our contributions: (1) For the purpose of obtaining an overall indicator of the information concentration of convolutional layers, we propose a fusion function that reaches a compromise of the information indicated by rank and entropy. According to the fusion values, we determine the layer-wise pruning rates. (2) We propose that an appropriate pruning criterion can be the contributions to the optimization of loss and recommend using Shapley values as a potent tool with the interpretability of CNNs to assess the channel contributions. (3) Extensive experiments for pruning architectures on CIFAR-10 [36] and ImageNet [37] using VGGNet [2], ResNet [4], and DenseNet [38] for image classification, and COCO2017 [39] using RetinaNet [40], FSAF [41], ATSS [42], and PAA [43] for object detection, demonstrate the effectiveness and superiority of our method."}, {"title": "II. RELATED WORKS", "content": "Pruning criteria: Previous methods in terms of structured pruning prune a model at multiple levels (e.g., filter and channel), where channel pruning correlates to filter pruning [44], [45]. The capacity and complexity of a model are reduced due to discarding the filters or channels, but this inevitably degrades the model's accuracy. Therefore, it is generally accepted that removing the least important filters or channels will minimize the loss in accuracy. For the purpose of achieving this target, the question of how to recognize the unimportant filters or channels is raised. Prior study [46] approximates the change in the loss function to determine the importance of the filters, then removes the least important ones. The importance of the filters is estimated using a variety of criteria, including the geometric median [32], l\u2081-norm [30], l2-norm [31], rank [26], and entropy [33]. The Average Percentage of Zeros (APoZ) is used as the importance score for the channel-level pruning method [47], which determines each channel's sparsity based on it. In addition, CP [27] discards the filters based on LASSO regression that produce the less informative channels along with the corresponding channels in the next layer, and ThiNet [28] removes a channel underlying the statistics information calculated from the next layer. The most recent works [48], [49] also make use of fisher information as an importance scoring metric and CHEX [50] uses column subset selection to remove the channels that are not impor-tant. White-Box [51] and MFP [52] empirically assign layer-wise pruning ratios using class contribution and geometric distance, respectively. CLR-RNF [53] employs a computation-aware measurement to determine pruning ratios, pruning more weights in less computation-intensive layers. CATRO [54] pre-defines layer-wise pruning ratios using class information from a few samples to measure the joint impact of multiple channels. ABP [55] discards fewer filters in significant layers and more in less significant ones, after examining each layer's training importance. The MDP [56] framework enables multi-dimensional pruning in convolutional neural networks. The CLFIP [57] method incorporates classification loss and feature importance into the layer-wise channel pruning process. PCP [58] iteratively prunes channels and estimates the subsequent accuracy drop. For point cloud neural networks, the JP [59] framework reduces redundancies along multiple dimensions. The 3D-P [60] framework prunes based on the importance of each frame or point for 3D action recognition.\nPruning rate: Recent research assigns the layer-wise pruning rates for convolutional layers in accordance with various rules when the model is enforced with a filter or channel sparsity. The pruning rate for the convolutional layers is pre-defined in rule-based methods, demonstrating that we are aware of the proportion (number) of filters or channels which will be discarded beforehand [29]. Early works [31]\u2013[33], [61] adopt a constant percentage to discard the filters or channels in each layer. On the contrary, HRank [26], ThiNet [28] and CP [27] empirically assign various layer-wise pruning ratios. In addition, NISP [62] pre-defines the layer-wise pruning ratios based on a variety of requirements, including FLOPs, memory and accuracy. After examining the sensitivity of each convolutional layer, CC [63] and PFEC [30] discard fewer filters in the sensitive layers and remove more filters in the insusceptible layers. For the purpose of calculating the layer-wise pruning ratios, SPP [64] performs Principal Component Analysis (PCA) on each layer and uses the reconstruction error to evaluate the sensitivity. Network Slimming [65] first trains with channel-level sparsity-induced regularization before establishing a global pruning rate to discard the channels for the entire network. In addition, AMC [66] uses DDPG [67] introduced in Reinforcement Learning to learn a precise pruning ratio per layer.\nPruning schedule: A network can be pruned using a variety of pruning schedules, which can generally be divided into three categories. (1) One-shot [29], [30]: remove the filters or channels of multiple convolutional layers in the network at once. NISP [62] prunes the CNNs by removing the least important neurons and then fine-tuning the compact network to preserve its predictive power. PFEC [30] and CC [63] prune the layers at once and further fine-tune the pruned network. Different from PFEC [30] which prunes the unimportant weights according to l\u2081-norm only, GReg [68] first imposes a l2-norm penalty to drive the unimportant weights to zero and prune those with the least l\u2081-norm until the penalty factor for the weight is updated to reach a pre-set ceiling. By using generative adversarial learning, GAL [69] prunes the filters along with other structures, such as channels, branches, and blocks, in an end-to-end and label-free paradigm. (2) Progressive [29]: the network under a simultaneous training and pruning process, and the network sparsity gradually goes from zero until the target number. The method [46] alternates iterations of pruning the least important neuron and fine-tuning until the the pre-determined desired trade-off between accuracy and pruning objective is reached. SFP [31] and FPGM [32] prune the filters based on their importance at the end of each training epoch. DSA calculates the gradient-based optimization to discover the layer-wise pruning ratios and prunes the network when training the network from scratch. Prior work [48] discards the least important channel and the corresponding coupled ones at every several iterations until the FLOPs reduces to the desired amount. (3) Iterative [29], [30]: discard the unimportant filters or channels and fine-tune the pruned network, then the process is repeated until the model achieves the desired sparsity. Previous studies [28], [33] prune the pre-trained network and fine-tune the pruned model layer by layer with one or two epochs, and finally re-train the model with more additional epochs when all the layers are pruned. Without retraining, HRank [26] fine-tunes the pruned network in a layer-by-layer fashion. An iterative two-step process is used by CP [27] to prune each layer, and after pruning the network is fine-tuned. Additionally, there are works [66], [70] which prune and fine-tune the network repeatedly up to the pre-determined number of iterations, and the pruned network is fine-tuned after thorough pruning. SCL [71] applies PCA to each layer, using network connections as regularization. It initially trains with sparsity-induced regularization, then sets a global pruning rate, and finally uses reinforcement learning to optimize layer-specific pruning ratios.\nDiscussion: In contrast to unstructured pruning, the model produced by structured pruning algorithms can be easily integrated into BLAS libraries and deployed to resource-limited devices. As far as we know, our method, focusing on channel pruning, is the first to assign layer-wise pruning ratios for convolutional layer pruning based on the rank and entropy for the convolutional layers in the unpruned model, which offers a more reliable method on the setting of the layer-wise pruning ratios. In addition, our method uses Shapley values to explain the contributions of the channels rather than importance-based criteria, and it discards the channels with the fewest contributions to accelerate the model during training and inference while retaining accuracy."}, {"title": "III. METHODOLOGY", "content": "In a CNN-based model, for the ith convolutional layer Ci where 1 \u2264 i \u2264 L and L denotes the total number of convolutional layers, we assume that the number of input channels and output channels are represented by Ci and Ci+1, respectively. In C\u2081, the height and weight of the feature maps are denoted as hi and wi, respectively. A set of filters K\u2081 = {K}, K,..., Ki } \u2208 R\u00a9\u00a1 \u00d7 Ci\u22121\u00d7ki\u00d7ki is in the ith con-volutional layer, where K \u2208 RCi-1\u00d7ki\u00d7ki represents the jth filter of the ith convolutional layer, j \u2208 {1, 2, ...,ci}and ki denotes the kernel size of the filter. Applying Ci+1 filters on Ci channels transforms the input feature maps Mi \u2208 R C i \u00d7 h i \u00d7 Wi into output feature maps Mi+1 \u2208 RCi+1\u00d7hi+1\u00d7Wi+1. In the convolutional layers, ci channels are split into two groups, i.e., the discarded channels Q\u2081 = {Chi Bi Q C} and the remaining channels U\u2081 = {CUCU,..., U}, where qi and ui are the number of discarded and remaining channels, respectively. For Qi, Ui, qi and ui, we have:\nLet an indicator function I denote whether the jth channel in the ith convolutional layer is pruned, then we have:\nAssume that o(C) represents the importance of the th channel in the ith convolutional layer, and we sort o(C) to prune u\u017c unimportant channels. Therefore, channel pruning can be defined as the optimization problem as below:\nwhich demonstrates that our target is to discover the least im-portant channels for the purpose of minimizing the information of the removed channels."}, {"title": "B. Information Concentration", "content": "Previous works [28], [31], [32], [61] remove the same proportions of filters in the convolutional layers, and the works [26], [27] empirically assign layer-wise pruning ratios. Different from them, we intend to assign layer-wise pruning ratios with information theory. We start by investigating the rank and entropy of the convolutional layer outputs in the pre-trained model to identify the information redundancy and amount that is contained in a layer. Then, we obtain the multiplication of them as a \u201cfusion\", which can be seen as the information concentration of the layers, taking into account the rank and entropy as two information indicators of convolutional layer outputs. If a convolutional layer is less informative, we can assign a larger number to prune the channels in the corresponding layer. A low-rank tensor typically contains lots of redundant information. If we suppose a matrix as a system of linear equations, a higher-rank matrix represents there exist more solutions to it, which indicates more uncertainty. From the point of view of information theory, the rank can describe the amount of information. In the case of channel pruning, a higher-rank tensor of the channel in a convolutional layer is more informative than the lower-rank one [26]. Additionally, a layer with a low average rank per channel suggests that multiple channels in it probably contain repetitive information, making it possible to compress it into a more compact layer with negligible loss in important information. Identifying the importance of features via rank provides a way to identify the redundant features with interpretability. To accurately calculate the rank of a matrix, we use the Singular Value Decomposition (SVD) method. First, we decompose the matrix A using SVD, yielding orthogonal matrices U,\u03a3, and VT. Then, we count the number of non-zero singular values in the diagonal matrix, which is the rank of matrix A. Compared to directly calculating the matrix rank, this method avoids the NP-hard problem and can obtain a relatively accurate rank estimate. The key insight in SVD is that the number of non-zero elements in the \u2211 matrix is equivalent to the rank of the matrix A. Hence, we can compute the rank of the matrix A by performing SVD decomposition and counting the number of non-zero elements in the \u2211 matrix. Previous work [26] demonstrated the rank of each convolutional layer almost remains the same under various image batches. Inspired by the conclusion, we discover that a small portion of images can generate the outputs to estimate the rank for convolutional layers, as demonstrated in Figs. 2a ~ 2d. Therefore, in the dataset with N images, we first randomly fetch out B images and feed them to the network for the sum of rank for the outputs per layer. Then, for the ith convolutional layer Ci, we can get the average rank per channel as:\nGenerally, entropy measures the disorder of a system or the uncertainty of an event. In information theory, entropy can be regarded as the expected amount of information [72]. A low-entropy convolutional layer suggests that the channels inside it contain less information in the channel pruning scenario. Thus, we could remove more channels in such convolutional layers. Entropy is naturally fit for selecting features by the amount of information they contain, therefore, it provides a more interpretable solution for identifying less informative convolutional layers. Additionally, we find that similar to the calculation for the rank of convolutional layers, the estimation of entropy for the convolutional layers may be generated by feeding only one or more batches of images, as shown in Figs. 2e ~ 2h. Given B input images, we first use the softmax function to map C between 0 and 1, after which the outputs can be regarded as the probability distribution for the channels in Ci:"}, {"title": "C. Channel Pruning via Shapley Values", "content": "The concept of Shapley value comes from the field of cooperative game theory and is based on a scenario in which members cooperate in a team. They receive a reward that is meant to be equally allocated to each player based on their unique contributions, where a contribution is known as Shapley value [34]. Shapley values are frequently used to interpret a \u201cblack-box\u201d deep neural network [73], [74], due to its rationality in evaluating the importance of features. Therefore, we extend the concept to the case of channel pruning: By considering a convolutional layer in a CNN-based model as a game in which the channels collaborate to produce an output, we can distribute the layer-wise outcomes to each of them. We suppose that a set P = {1,2,..., n} is comprised of n members who participate in cooperation, and the subsets C P represents an alliance consisting of two or more members. If for any s, a corresponding real number e(s) satisfies:\nthen e(s) can be defined as a characteristic equation on P. The probability if the player a will join the alliance s\\{a} is calculated as:\nwhere |s| represents the number of elements in s. The player a's marginal contribution to all of the alliances which comprise a is calculated as:"}, {"title": "D. Pruning Schedules", "content": "In the pruning phase for image classification, three pruning schedules are employed by our method to prune the channels over the convolutional layers [29]:\n1) One-shot: We discard the channels from the convolutional layers with the lowest Shapley values representing contributions at once.\n2) Iterative static: We prune each layer and fine-tune the compact network for several epochs iteratively, and the unimportant channels are determined by the Shapley val-ues calculated in the pre-trained model. \"Static\" implies that the channel contributions are represented by the initial Shapley values of them.\n3) Iterative dynamic: We prune and fine-tune the network similar to the iterative static paradigm, but the channel contributions are generated during the training and prun-ing procedure. \"Dynamic\" implies the Shapley values are calculated after fine-tuning, but not from the initial pre-trained network.\nAlg. 1 demonstrates our method with an iterative static pruning schedule, which we use in the experiments on CIFAR-10. First, we use the weights of the pre-trained model to initialize the network. Next, we propose a fusion function to get the overall indicator of rank and entropy for the convolutional layers under image batches. According to the fusion values, we assign the number of channels to be removed for the layers. By forward-passing the model to the batches of sampled images, we also obtain the Shapley values for the channels in the convolutional layers. During the pruning stage, we considered that the channels in a convolutional layer can be regarded as the team members in cooperation. Therefore, the Shapley values can be the reflection of their average marginal contributions to the layer output. After sorting the Shapley values, the lowest ones indicate the corresponding channels with the fewest contributions. Then, we discard the least important channels. After pruning a layer, to lessen the loss brought on by pruning, we fine-tune the model with several epochs. The procedure repeats until the final layer is pruned. Until the process of layer-wise pruning and fine-tuning ends, we retrain the pruned model to decrease the error caused by pruning. The accuracy of the compressed model is then restored. In our experiments, we demonstrated initializing with weights of pruned models outperformed random initialization, which we decided as the initialization method in re-training.\nThe models in object detection contain not only \"backbone\" structures, but also \"neck\" and \"head\". As a result, the number of stages (blocks consisting of identical layers) becomes greatly larger than the ones in the image classification models, which results in many more hyper-parameters needing to be tuned. Semantic segmentation models incorporate not only 'base' structures to interpret input images but also 'upsam-pling' components to generate detailed classifications. This results in a greater depth of stages (blocks comprised of similar layers) compared to image classification models, necessitating a larger number of hyper-parameters to be finetuned. As is known to all, the more hyper-parameters we manually tune, the more errors it may cause. Finally, we decided to try another method in pruning models for those tasks that are more complex than image classification. Finally, we decided to try another method in pruning models for those tasks that are more complex than image classification. Inspired by SFP [31] and Fisher [48], we employ the progressive pruning paradigm [29] without pre-defining layer-wise pruning ratios, as summarized in Alg. 2. Different from iterative pruning, we prune the network along with network training. In the training and pruning process, we calculate the Shapley values for all channels in the network and discard the least important one at every iteration. When the process ends, the unimportant channels are all removed, then we obtain a compact model. For the purpose of reconstructing the detection capability, we retrain the pruned model again."}, {"title": "IV. EXPERIMENTS", "content": "Benchmark datasets and models: We evaluate the performance of our pruning method on CIFAR-10 and ImageNet for image classification. The CIFAR-10 and ImageNet dataset contains 60,000 32 \u00d7 32 images with 10 classes and 1.28 million 224 \u00d7 224 images with 1,000 classes, respectively. We conduct the experiments on VGGNet with a plain structure, ResNet with a residual structure, and DenseNet with dense blocks. We randomly select 1,024 and 128 images for the architectures on CIFAR-10 and ImageNet for the purpose of estimating the average statistics of rank and entropy for convolutional layer outputs, and then we combine these two indicators to obtain the results. We set [1,10] as the scaling range of rank, entropy, and fusion values as Eqn. 7 introduced. In addition, we also evaluate the performance of our method in object detection on COCO2017, where the architectures are RetinaNet, FSAF, ATSS and PAA. COCO2017 contains 163,957 images with 80 categories, and 118,287 and 5,000 images are in the training set and validation set, respectively. Following the previous works [5]\u2013[7], we train the models on the training set and evaluate their detection capabilities on the validation set.\nEvaluation metrics: We use Top-1 accuracy and Top-1/Top-5 accuracies on the test set to assess the image classifi-cation capabilities on CIFAR-10 and ImageNet, respectively. We use Float Points Operations (denoted as FLOPs) and the parameters to assess the computational overhead and the capacity of the pruned models, respectively. We evaluate the accuracy drop for performance, FLOPs reduction for acceler-ation and parameters reduction for compression in different methods. For object detection, we use mAP (mean average precision) to evaluate the detection capabilities of the models, and also use the memory footprint to estimate the scale of the models.\nConfigurations: We use PyTorch [75] as the framework to implement our method. In the training process, we use SGD as the network optimizer and set 0.1 as the initial learning rate. Additionally, we set the batch size of 256. The momentum is 0.9 and weight decay is 10-4. On CIFAR-10, we train the model for 200 epochs, where the learning rate is divided by 10 at epochs 100 and 150. On ImageNet, we train the model for 90 epochs and the learning rate is divided by 10 at epochs 30 and 60. On COCO2017, we train the model for 12 epochs and the batch size is 2. The initial learning rate of 0.005. In terms of the pruning schedule, we adopt the iterative pruning manner on CIFAR-10, where the fine-tuning epochs are 20. Besides, we adopt the one-shot pruning schedule on ImageNet and progressive pruning paradigm on COCO2017. We use four NVIDIA RTX 3090 GPUs and four Tesla V100 GPUs for conducting the experiments."}, {"title": "B. Results and Analysis", "content": "The classification performance on CIFAR-10 is demonstrated in Tab. I.\nVGGNet: In comparison to SSS and CP", "channels.\nResNet": "For ResNet-20, our method is superior than SFP and FPGM w.r.t. loss in accuracy (0.35% v.s. 1.37% by SFP and 1.11% by FPGM) and FLOPs reduction (45.2% v.s. 42.2% by SFP and 42.2% by FPGM). Additionally, our method outperforms DSA, FPGM and PGMPF in accuracy drop (0.93% v.s. 1.06% by DSA, 1.76% by FPGM and 1.35% by PGMPF), with larger FLOPs reduction (57.9% v.s. 50.3% by DSA, 54.0% by FPGM and 54.0% by PGMPF). In comparison to SFP and FPGM, which obtain accuracy decreases of 0.55% and 0.32%, respectively, our method increases the accuracy by 0.01% for ResNet-32. Moreover, our method reduces more FLOPs (38.0% by ACTD, 46.8% v.s. 41.5% by SFP and 41.5% by FPGM). Compared with PScratch and FPGM, our method yields a loss in accuracy of 0.70% lower than PScratch (1.00%) and equal to FPGM (0.70%). In addition, 43.9% of parameters and 58.0% of FLOPs are reduced by our method, where the FLOPs reductions are larger than Pscratch (50.0%) and FPGM (53.2%). For pruning ResNet-56, our method improves the accuracy more than ACTD, SFP and GCNNA (0.21% v.s. 0.07% by ACTD, 0.19% by SFP and 0.13% by GCNNA), while GAL and AMC obtain the loss in accuracy of 0.35% and 0.90%, respectively, and DECORE retains the accuracy. In addition, under a larger acceleration ratio of 58.1% and compression ratio of 43.9%, our method obtains a drop in ac-curacy less than FPGM and DBP (0.28% v.s. 0.33% by FPGM and 0.42% by DBP), while Graph increases the accuracy by 0.11% with parameters reduction of 43.0%. Compared to the methods pruning ResNet-110, our method performs the best in accuracy. Specifically, our method increases the accuracy of 0.88%, outperforming SFP (0.18%) and HRank (0.73%), reducing more FLOPs (45.6% v.s. 41.2% by HRank and 40.8% by SFP). Additionally, our method increases the accuracy larger than FPGM (0.48% v.s. 0.16% by FPGM) with greater FLOPs reduction (58.1% v.s. 52.3%), while GAL degrades the accuracy by 0.76% and removes fewer FLOPs (48.5%), and DECORE retains the accuracy with 61.8% FLOPs drop higher than our method. HRank obtains a slightly higher FLOPs reduction of 58.2% than our method, but it harms the accuracy of 0.14%.\nBased on the results for pruning DenseNet-40, we observe that our method has the potential to remove more parameters. To be specific, although CC obtains an accuracy loss of 0.14%, it achieves a FLOPs reduction that is close to our method (44.4% v.s. 47.0% by CC) but a compression ratio which is lower than our method (60.8% v.s. 51.9% by CC). In addition, our method degrades the accuracy drop slightly more than HRank (1.68% v.s. 1.28% by HRank), but outperforms HRank w.r.t. FLOPs reduction (59.6% v.s. 54.7% by HRank) and parameters reduction (68.6% v.s. 53.8% by HRank). Thus, our method can more effectively compress models with dense blocks.\nThe classification performance on ImageNet is demonstrated in Tab. II.\nFor ResNet-18, our method yields Top-1/Top-5 accuracy drops of 1.49%/0.82%, outperforming SFP (3.18%/1.85%) and FPGM (1.94%/1.10%). Moreover, our method achieves a FLOPs reduction of 40.7% and a parameters reduction of 40.6%. In addition, our method still outperforms SFP and FPGM with greater FLOPs and parameter reductions of 45.1% and 41.7%, respectively, with Top-1/Top-5 accuracy drops of 1.80%/0.98%. For ResNet-34, our method obtains a Top-1 accuracy drop of 0.57%, which is better than SFP (2.09%) and FPGM (1.38%), removing larger FLOPs reduction (45.2% v.s. 41.1% by SFP and 41.1% by FPGM) and reducing 36.3% parameters. Additionally, our method achieves Top-1/Top-5 accuracy drops of 1.06%/0.73% with a FLOPs reduction of 50.1% and a parameters drop of 36.0%, outperforming the Top-1/Top-5 accuracy drops of PGMPF (1.68%/0.98%) with 52.7% FLOPs reduction. ResNet-50 is a popular network for analyzing the performance of pruning methods. There-fore, more methods for comparison are listed. Our method obtains Top-1/Top-5 accuracy drops of 0.43%/0.11%, out-performing DSA (0.92%/0.41%), SFP (1.54%/0.81%), FPGM (0.56%/0.24%), GAL (4.20%/1.93%), HRank (1.17%/0.54%) and DECORE (1.57%/0.69%), among which the FLOPs re-ductions are similar. In addition, our method achieves Top-1/Top-5 accuracy drops of 0.84%/0.39%, outperforming DSA (1.33%/0.80%) with a parameter reduction of 44.2%. In addi-tion, our method achieves a loss in Top-1 accuracy slightly higher than TPP (0.53%) and Fisher (0.37%), lower than GNN-RL (1.82%) and PGMPF (0.90%), but with a larger FLOPs reduction of 50.4% than DSA (50.0%) and Fisher (50.0%), and a parameters reduction of 44.2%. ResNet-101 is a network with a large quantity of parameters, and our method outperforms FPGM in Top-1/Top-5 accuracy drops (0.02%/-0.04% v.s. 0.05%/0.00% by FPGM) but removing more FLOPS (43.7% v.s."}]}