{"title": "INTERPRETING LANGUAGE REWARD MODELS VIA CONTRASTIVE EXPLANATIONS", "authors": ["Junqi Jiang", "Tom Bewley", "Saumitra Mishra", "Freddy Lecue", "Manuela Veloso"], "abstract": "Reward models (RMs) are a crucial component in the alignment of large language models' (LLMs) outputs with human values. RMs approximate human preferences over possible LLM responses to the same prompt by predicting and comparing reward scores. However, as they are typically modified versions of LLMs with scalar output heads, RMs are large black boxes whose predictions are not explainable. More transparent RMs would enable improved trust in the alignment of LLMs. In this work, we propose to use contrastive explanations to explain any binary response comparison made by an RM. Specifically, we generate a diverse set of new comparisons similar to the original one to characterise the RM's local behaviour. The perturbed responses forming the new comparisons are generated to explicitly modify manually specified high-level evaluation attributes, on which analyses of RM behaviour are grounded. In quantitative experiments, we validate the effectiveness of our method for finding high-quality contrastive explanations. We then showcase the qualitative usefulness of our method for investigating global sensitivity of RMs to each evaluation attribute, and demonstrate how representative examples can be automatically extracted to explain and compare behaviours of different RMs. We see our method as a flexible framework for RM explanation, providing a basis for more interpretable and trustworthy LLM alignment.", "sections": [{"title": "1 INTRODUCTION", "content": "The training of safe and capable large language models (LLMs) typically involves a fine-tuning step to align their outputs with human preferences. Recent work by Xu et al. (2024) suggests that fine-tuning by reinforcement learning using a language reward model (RM), which represents these preferences by rating the quality of LLM responses to user prompts, remains the state-of-the-art alignment method. In such frameworks, the effectiveness of alignment heavily depends on the quality of the RM itself (Chaudhari et al., 2024). While a growing body of research aims at improving the performance of RMs (Bai et al., 2022; Chan et al., 2024; Wang et al., 2024a), evaluating and understanding RMs has received \"relatively little study\" (Lambert et al., 2024). This is despite it being identified as a key aspect of the AI safety research agenda (Curtis et al., 2024). Among the limited prior works, several focus on characterising certain failure modes of RMs, such as unwanted biases towards longer answers (Singhal et al., 2023), an inability to generalise to slightly perturbed responses (Pikus et al., 2023), and a susceptibility to being over-optimised (Gao et al., 2023). Recently, Zeng et al. (2024) and Park et al. (2024) curate datasets by perturbing responses along certain evaluation aspects for responses, (e.g. instruction following, response length, correctness), and systematically evaluate RM responses to these changes to gain more insights.\nA complementary approach for achieving better understandings of complex machine learning models is to apply explainable AI (XAI) techniques, which aim to uncover reasons behind their predictions. In addition to the complexity of the language domain, a particular challenge for understanding RMs is the lack of direct meaning in their predicted rewards. RMs are typically trained and evaluated on binary response comparisons (Ouyang et al., 2022) using the preference model of Bradley & Terry (1952), which converts the RM's scalar reward for each response into a probabilistic preference for one response or the other. Outside of such a comparison, the meaning of the output scalars"}, {"title": "2 CONTRASTIVE EXPLANATIONS FOR LANGUAGE REWARD MODELS", "content": "2.1 PRELIMINARIES\nLet x denote a natural language prompt and y denote an LLM-generated response. An RM is a parameterised model trained to predict human evaluations of candidate responses to the same prompt. In the standard training pipeline of Bradley-Terry (BT) modelling (Bradley & Terry, 1952), RMs are typically modified LLMs topped with scalar output heads (Ouyang et al., 2022; Bai et al., 2022). Given a prompt-response pair (x, y), the RM assigns a scalar reward, denoted as $r_{BT}(x, y)$. Given two responses $y_+$ and $y_-$ such that $r_{BT}(x,y_+) > r_{BT}(x,y_-)$, we say that the RM has a preference for $y_+ \\succ y_-$ as a response to prompt x. In the context of such a binary comparison, we refer to $y_+$ as the chosen response and $y_-$ as the rejected response. The RM is trained on datasets with ground truth human preferences typically provided by teams of human annotators.\nAlternatively, multi-dimensional regression RMS ($r_{MR}$), implemented as language models with regression heads (Dong et al., 2023; Wang et al., 2023; 2024b;a), are trained to predict a vector of rewards, with each dimension representing an evaluation aspect of the response (Cui et al., 2023). To compare responses overall, a scalarisation function (e.g. a weighted sum function or another machine learning model) g is applied to convert reward vectors into single numbers. Given a prompt x, an overall preference $y_+ \\succ y_-$ is predicted if $g(r_{MR}(x,y_+)) > g(r_{MR}(x,y_-))$.\nIn this work, we focus on explaining binary comparisons made by RMs because this is their most basic functionality and facilitates a definition of contrastive explanation. In doing so, we need not distinguish between the two types of RM, and use r to denote either $r_{BT}(\\cdot, \\cdot)$ or $g(r_{MR}(\\cdot,\\cdot))$.\n2.2 CONTRASTIVE EXPLANATIONS\nWe now introduce contrastive explanations for any binary comparison made by an RM, involving a user prompt x and a pair of chosen and rejected responses $y_+$, $y_-$ such that the RM's preference is $y_+ \\succ y_-$. We assume access to two sets of perturbed responses, $Y_+$ and $Y_-$, which are perturbations of $y_+$ and $y_-$ respectively. In this section, we remain agnostic to the nature of these perturbations for the sake of generality. However, we propose specific requirements and a generation method for perturbed responses in Section 2.3 below. Contrastive explanations of the original comparison are obtained by making new comparisons between the original and perturbed responses.\nAs discussed in Section 1, we can distinguish two classes of perturbation, namely counterfactuals (CFs) and semifactuals (SFs), based on whether the RM's preference flips for the new comparisons. Concretely, given $Y_+$ and $Y_-$, we denote $Y_+^{CF} = \\{y'_+ \\in Y_+ | r(x,y'_+) < r(x,y_-)\\}$ and $Y_-^{CF} = \\{y'_{-} \\in Y_- | r(x,y'_-) > r(x,y_+)\\}$ as the sets of CF perturbations of the original chosen and rejected responses respectively. Similarly, we refer to $Y_+^{SF} = \\{y'_+ \\in Y_+ | r(x,y'_+) \\geq r(x,y_-)\\}$ and $Y_-^{SF} = \\{y'_{-} \\in Y_- | r(x,y'_-) < r(x, y_+)\\}$ as the sets of SF perturbations. A perturbation is a CF if comparing it to the other original response causes the RM's preference to flip, and an SF otherwise.\nCollectively, the sets of categorised perturbations $Y_+^{CF}$, $Y_-^{CF}$, $Y_+^{SF}$ and $Y_-^{SF}$ can be understood as a dataset of new binary comparisons in the neighbourhood of the original one, revealing sensitivities in the RM's local behaviour and thereby helping to explain the original preference. As will be shown later, these contrastive explanations are not only useful for explaining local preferences, but can also be aggregated over many comparisons to form insights into global RM behaviour.\n2.3 GENERATING PERTURBED RESPONSES\nThe preceding conceptual formulation requires a set of perturbed responses $Y_+$ and $Y_-$ for categorising into CFs and SFs. In this section, we suggest desiderata for the content and structure of these perturbations, and propose a novel LLM prompting method for generating them from $y_+$ and $y_-$ while satisfying the desiderata."}, {"title": "3 QUANTITATIVE EVALUATION", "content": "In this section, we demonstrate the effectiveness of our method for generating high-quality contrastive explanations. Our experiments are conducted on three open source human preference datasets and three RMs. For each dataset, we randomly select 30 binary comparisons from the training set serving as test comparisons (repeated five times with different random seeds, making 150 test comparisons in total), for which we then generate contrastive explanations using our method and the baselines for each RM. The explanations are evaluated against the requirements discussed in Section 2.3 using popular metrics from the text CF literature (Nguyen et al., 2024).\nDatasets. We use HelpSteer2 (hs2) (Wang et al., 2024b), HH-RLHF-helpful, and HH-RLHF-harmless\u00b9 (Bai et al., 2022). We focus on explaining single-turn conversations in our experiments. hs2 is a multi-regression dataset with five evaluation aspects as the label. We filter hs2 for pairs of responses where the preference is clear, i.e. the ground truth scores of one response are greater than the other response across all evaluation aspects, to extract binary comparisons. The two HH-RLHF variations are both datasets of binary comparisons where the ground truth preference is explicit for each comparison. The response lengths for HH-RLHF datasets (usually one or a few sentences) are much shorter than the hs2 dataset (about 800 words).\nReward Models. We generate explanations for three open source RMs from the OpenAssistant community. DeBERTa-large (v1)\u00b2 and DeBERTa-large-v2\u00b3, comprising 0.4 billion parameters, are RMs fine-tuned from the DeBERTa model (He et al., 2021) using different datasets (see their model cards for more details). pythia-1.4b-epoch-2.54 is a larger model with 1.4 billion parameters.\nMetrics. We use a set of metrics derived from the desiderata in Section 2.3. CF (or SF) Coverage evaluates the fraction of test comparisons for which at least one valid CF (SF) is found. Having both high CF and SF coverage indicates that an explainer is better at obtaining a balanced mix of"}, {"title": "4 QUALITATIVE ANALYSIS", "content": "In this section, we demonstrate how our proposed contrastive explanation approach can be useful for investigating global behaviours and understanding local preferences of RMs. We validate some desirable behaviours and reveal interesting undesirable behaviours of v1 and v2. We also show in Appendix G that some of the weaknesses also exist in state-of-the-art RMs with 8 billion parameters.\n4.1 GLOBAL SENSITIVITY\nThe key intuition we leverage is that the changed attributes in a CF are more causally relevant to an original prediction, while those in an SF are less causally relevant (McCloy & Byrne, 2002; Aryal & Keane, 2023). Therefore in our case, for any given binary response comparison (x,y+, y : r(x,y+) > r(x, y)), the attributes associated with the CF responses, $Y_+^{CF}$ and $Y_-^{CF}$, can be labelled as strongly relevant, and those of the SF responses, $Y_+^{SF}$ and $Y_-^{SF}$, as weakly relevant. Note that we need to separately consider $Y_+$ and $Y_-$ for this analysis because perturbing $y_+$ to become less preferred than y\u2013 is an independent process from perturbing y\u2013 to become more preferred than $y_+$ (see Appendix F). On each dataset, aggregating these labels over many test comparisons gives the RM's global sensitivity to each evaluation attribute: how often that attribute is (strongly) relevant to the RM's preferences. This type of analysis (aggregating local explanations for global insights) has proven effective for providing rich CF summaries for tabular data settings (Rawal & Lakkaraju, 2020) and global feature attributions (Ribeiro et al., 2016).\nFollowing the experimental setup in Section 3, we randomly select 500 comparisons from each dataset and filter for the comparisons where the three RMs predict the same preference. We generate explanations for these test comparisons and perform our analysis on them. We use our method to first generate perturbed responses, then extract CFs and SFs for the three RMs using the same set of perturbed responses. This allows us to fairly compare the global behaviour of the models. Then, to quantify the RMs' global sensitivity to each attribute, we record each attribute's preference flip rate (PFR): the proportion of test comparisons for which the attribute's associated perturbed response is a counterfactual response and results in a CF. The PFR is separately calculated for $Y_+$ and $Y_$.\n4.2 FINDING REPRESENTATIVE EXAMPLES FOR RM'S GLOBAL BEHAVIOUR\nFollowing on from a global analysis, for each test comparison in a dataset, we record r(x, y) \u2013 r(x, y'+) and r(x, y'-) \u2013 r(x, y+) for every perturbed response $y'_+ \\in Y_+$ and $y'_{-} \\in Y_-$, indicating how much the perturbation causes the RM output to change in the opposite direction. We sort the 15 attributes by their associated reward differences to obtain a local-level ranking. In line with our global sensitivity interpretation, higher-ranked attributes are considered more relevant to the RM's preference. We calculate the ranking similarity (Kendall's correlation coefficient) between the global and local rankings. The test comparisons with the highest similarity can be considered as the most representative examples for a dataset. This process is completed separately for $Y_+$ and $Y_$.\n4.2.1 REPRESENTATIVE EXAMPLES FOR ONE MODEL\nTo find representative examples capturing global patterns on both sides ($Y_+$ and $Y_-$) of one dataset, we calculate the sum of the ranking similarities for both responses in each test comparison."}, {"title": "5 CONCLUSION", "content": "In this work, we formulate contrastive explanations for language reward models and use them for interpretation. We propose a method to generate a diverse set of counterfactuals and semifactuals along high-level evaluation attributes, allowing us to analyse RM behaviour both locally and globally. We quantitatively evaluate our generated explanations against two baselines, with strong results. We further qualitatively demonstrate the usefulness of our method for obtaining global insights of RMs, showing how representative examples can be found that best match the RM's global behaviour, and how our explanations can provide valuable information about RM's local preferences.\nThis work comes with limitations. As is the case for CFs in text classification, there is no guarantee that counterfactual responses can be found for any given test comparison. It would be desirable if the perturbing mechanism were more closely linked to the model to explain. Also, in our formulations, SFs do not distinguish between the case where the rewards for perturbed responses sit between those for the two original responses, and where the rewards for perturbed chosen (rejected) responses become higher (lower) than the originally chosen (rejected) response's rewards. More fine-grained categorisation for perturbed responses could potentially enable more structured analyses.\nThis work presents a novel method and example workflow to systematically interpret behaviours of RMs, which open up exciting avenues for future work. Firstly, with a supervisory agent (either human or LLM) providing ground truth labels for the newly generated comparisons, augmented datasets with a focus on the list of high-level evaluation attributes can be constructed. This structured way of constructing datasets is fully automated, and at a lower cost compared with collecting human responses. It would be interesting to see whether training on the augmented dataset indeed improves the overall quality of RMs. Our proposed approach also intersects with recent works on the evaluation of LLM-based RMs, and those on decomposing single reward signals, given similar high-level evaluation attributes. Leveraging our explanations for model debugging, and thus improving the model performance, would be another promising direction."}, {"title": "A THE HIGH-LEVEL EVALUATION ATTRIBUTES", "content": "We adopted a semi-automated workflow to obtain the list of attributes presented in Section 2.3. Using the same test sets of the three datasets (described in Section 3), we prompt GPT-40 to identify relevant attributes which potentially caused one response to be more preferred than the other response in each test comparison. Outside the scope of reward modelling, similar techniques have been used in the thematic analysis of texts (Xiao et al., 2023; Dai et al., 2023; Chew et al., 2023). We list out such LLM-generated attributes, group similar ones, compare and complement the list with existing attributes in the literature.\nWe used the following prompt:\nIn the task of response quality scoring, a trained deep learning model assigns real-valued scores for responses to questions. The higher the score, the better the response quality. The question is '{question}'. The model assigned a score {chosen_reward} for response A: '{chosen_response}\u2019. The model assigned a score {rejected_reward} for response B: '{rejected_response}\u2019. List out some high-level attributes which might have caused the model to assign a better score for response A than response B. Some example attributes are: appropriateness, clarity, harmlessness, verbosity, etc. Only output the attributes in a comma-separated list.\nFrom the test sets, we obtain the following attributes, sorted by the number of occurrences in the LLM responses in descending order:\nclarity, relevance, harmlessness, informativeness, detail, conciseness, specificity, completeness, structure, helpfulness, verbosity, engagement, coherence, appropriateness, comprehensiveness, empathy, accuracy, sensitivity, creativity, legality, ethicality, readability, factual-accuracy, persuasiveness, privacy, humour, assertiveness\nThis list encompasses all attributes in the hs2 dataset Wang et al. (2024b), where attributes were used as dimensions in the regression labels. It also overlaps with the ones in UltraFeedback dataset (Cui et al., 2023) apart from instruction-following and honesty. As the former is not the focus of the datasets we experiment with, we only additionally include honesty, relabelled to avoid-to-answer for better relevance. The list of attributes deviates from the list in the OASST2 dataset (K\u00f6pf et al., 2023), which focuses on distinguishing various types of harmful contents, e.g. hate speech, violence, etc. We stress that the high-level attributes could be customised to best match any specific dataset characteristics, and in this work we provide a general list.\nWe conclude our final list below, sorted in alphabetical order:\n\u2022\n\u2022\n\u2022 avoid-to-answer: whether or not the response is avoiding to give direct answers to the question,\n\u2022 appropriateness: the extent to which the response is appropriate in terms of language style, politeness, and whether it contains any sarcasm,\n\u2022 assertiveness: the extent to which the response sounds very certain and contains judgements,\n\u2022 clarity: whether or not the response is clear and easy to read,\n\u2022 coherence: whether or not the contents in the response are self-contained and clear,\n\u2022 complexity: the intellectual burden required by a person to understand this response,\n\u2022 correctness: whether or not the response is factually correct,\n\u2022 engagement: the extent to which the language style of the response is trying to engage with the person who wrote the question,\n\u2022 harmlessness: whether or not the response is relevant to any potentially unsafe, immoral or illegal behaviours,"}, {"title": "B PROMPTS USED IN OUR METHODS", "content": "We present the prompts we use for the two steps described in Section 2.3. Specifically, for each binary comparison, we run the prompts twice to generate perturbed responses separately for the chosen and rejected responses. In the prompts, we parameterise the two responses, and the other variables in the prompts can be determined depending on whether the chosen or rejected response is input as response_1. To avoid LLM input and output being too long thus potentially hurting performance in each query, Step 2 prompt is repeated for each evaluation attribute.\nStep 1 prompt:\nIn the task of response quality scoring, a trained deep learning model assigns real-valued scores for responses to questions, the higher the score the better the response quality.\nThe question is \u2019{question}'. The model assigned a score {score_1} for response A: '{response_1}'. The model assigned a score {score_2} for response B: '{response_2}\u2032.\nThe high-level attributes that potentially caused the model to assign a {better/worse} score for response A than response B are {attribute_list}.\nYour task: for each attribute in this list, identify the words in response A that are relevant to it.\nOnly output the attributes and their associated words like this: 'attribute: word1, word2, word3\u2032. Each line should contain a comma-separated word list for one attribute.\nIt is fine to have repeated words in the words identified for each attribute, but you need to keep them in their original order of occurrence in the response A.\nStep 2 prompt:\nIn the task of response quality scoring, a trained deep learning model assigns real-valued scores for responses to questions, the higher the score the better the response quality.\nThe question is \u2019{question}'. The model assigned a score {score_1} for response A: '{response_1}'. The model assigned a score {score_2} for response B: '{response_2}'.\nThe potential high-level attributes that caused the model to assign a {better/worse} score for response A than response B is: {attribute}. This attribute concerns {attribute_description}.\nYour task is to modify response A. Here is a list of requirements for the modification:\n- The modified response A becomes a {better/worse} response to the question than response B."}, {"title": "C PROMPTS USED IN THE RANDOM PERTURBATION BASELINE", "content": "Generate a random perturbation of this piece of text: {response}.\nOnly output the perturbed text.\nDo not output any characters other than English texts and common punctuation."}, {"title": "D ABLATION ANALYSIS", "content": "Different prompts can have large impacts on the perturbations generated. In our prompt, we specify that\nThe changes made to response A should be centered around the following words: {relevant words for this attribute identified in Step 1}.\nIn this ablation analysis, we highlight the importance and effectiveness of this additional requirement in keeping the perturbations close to the original responses while balancing the CF and SF coverages.\nFollowing the same experimental setup described in Section 3), we quantitatively compare CFs and SFs resulting from perturbations obtained with three different prompts. Our final prompt is referred to as the center-prompt. One of the different prompt (only) replaces the above sentence by:\nResponse A can only be modified by deleting, replacing, or inserting words, at the locations of all or a subset of the following words: {relevant words},\nposing a stronger restriction on the changes made in the perturbations. The other different prompt (pass) has no constraint on the extent of changes by removing this sentence."}, {"title": "E EXPLANATION EVALUATION RESULTS FOR CORRECT AND WRONG PREFERENCES", "content": "In this analysis, we look at evaluation metrics for explanations respectively for correct and wrong preferences made by RMs to further understand the effectiveness of our approach. Following the same experimental setup described in Section 3, we further divide each test set by whether or not the model's preference is correct, i.e. matches the ground truth label. We report in Table 5 the evaluation results of our method separately for correct and wrong preferences."}, {"title": "F SEPARATELY CONSIDER PERTURBING CHOSEN AND REJECTED RESPONSE FOR GLOBAL SENSITIVITY ANALYSIS", "content": "In Section 4, we have separately considered global sensitivity for $Y_+$ and $Y_-$ because perturbing $y_+$ to become worse than y\u2013 is an independent process from perturbing y\u2013 to become better than $y_+$. We validate this intuition by calculating the ranking similarities (Kendall's $\\tau$ correlation coefficient) between each dataset's $Y_+$ and $Y_-$ branches. The results for each model are presented in Table 6. We observe that only v2 and pythia have weak correlations in some cases, and the rest of the results are generally uncorrelated."}, {"title": "G FURTHER QUALITATIVE ANALYSIS", "content": "In this section, we present a further qualitative analysis, using our contrastive explanation method to discover failure cases in state-of-the-art RMs.\nFollowing the findings on the undesirable behaviours of model v2 in Section 4.2.1, we additionally include two better-performing RMs and investigate whether they improve over the weaknesses in v2."}]}