{"title": "TransDAE: Dual Attention Mechanism in a Hierarchical Transformer for Efficient Medical Image Segmentation", "authors": ["BOBBY AZAD", "POURYA ADIBFAR", "Kaiqun Fu"], "abstract": "In healthcare, medical image segmentation is crucial for accurate disease diagnosis and the development of effective treatment strategies. Early detection can significantly aid in managing diseases and potentially prevent their progression. Machine learning, particularly deep convolutional neural networks, has emerged as a promising approach to addressing segmentation challenges. Traditional methods like U-Net use encoding blocks for local representation modeling and decoding blocks to uncover semantic relationships. However, these models often struggle with multi-scale objects exhibiting significant variations in texture and shape, and they frequently fail to capture long-range dependencies in the input data. Transformers designed for sequence-to-sequence predictions have been proposed as alternatives, utilizing global self-attention mechanisms. Yet, they can sometimes lack precise localization due to insufficient granular details. To overcome these limitations, we introduce TransDAE: a novel approach that reimagines the self-attention mechanism to include both spatial and channel-wise associations across the entire feature space, while maintaining computational efficiency. Additionally, TransDAE enhances the skip connection pathway with an inter-scale interaction module, promoting feature reuse and improving localization accuracy. Remarkably, TransDAE outperforms existing state-of-the-art methods on the Synaps multi-organ dataset, even without relying on pre-trained weights.", "sections": [{"title": "I. INTRODUCTION", "content": "EARLY disease diagnosis is paramount in healthcare, as it enables the detection of disorder severity and spread at initial stages [1]. Medical image segmentation serves as a critical component in automating computer-aided disease diagnosis (CAD), treatment planning, and surgical pre-assessment. The segmentation process involves parti- tioning target organ and tissue shapes and volumes through pixel-wise classification [2]. Traditional manual annotation is labor-intensive, time-consuming, and susceptible to human error [3]. Consequently, automating medical image segmen- tation has become a research focus to alleviate this burden. Recent studies have explored the potential of deep learning in CAD, given its successful application in various medical fields.\nOver the years, convolutional neural networks (CNNs) and fully convolutional networks (FCNs) have been the dominant architectures in medical image segmentation, largely due to their ability to learn hierarchical features through convolu- tional operations. Among these, U-Net has emerged as a par- ticularly effective model due to its U-shaped structure with symmetric encoding and decoding paths [4]. This architec- ture, with its skip connections, facilitates the fusion of low- level and high-level features, improving context modeling and producing accurate segmentation results. Variants such as Res-UNet [5], Dense-UNet [6], U-Net++ [7], and UNet3+ [8] have further improved performance by addressing specific limitations, such as the loss of spatial information in deeper layers.\nHowever, despite these advances, CNN-based models face inherent limitations. The localized nature of convolution operations means that these models often struggle with cap- turing long-range dependencies and multi-scale variations within medical images, which are crucial for segmenting"}, {"title": "II. LITERATURE REVIEW", "content": "In recent years, deep learning methods have gained promi- nence in image segmentation tasks, replacing hand-crafted- feature based machine learning approaches. CNNs have be- come a popular choice for various medical image segmenta- tion tasks, primarily due to the success of U-Net [16]. The U-shaped symmetric structure of U-Net incorporates skip connections between each block of the encoder and decoder, enabling the concatenation of higher-resolution feature maps from the encoder network with upsampled features for more accurate representations. The success of U-Net has inspired researchers to adapt its architecture and improve its perfor- mance by applying various strategies, such as Res-UNet [5], Dense-UNet [6], U-Net++ [7], and UNet3+ [8]. The 3D U- net [17] was proposed as an enhanced version of U-Net, specifically designed for volumetric segmentation in three dimensions.\nOktay et al. [11] introduced attention gates to U-Net's skip connections, emphasizing the importance of specific objects by focusing on critical objects and disregarding irrelevant ones. Alryalat et al. [12] employed a dual-attention strategy in U-Net skip connections to make the network concentrate on more representative channels using channel attention and to identify the most informative spatial regions in images using spatial attention. Zhou et al. developed U-Net++ [7] and demonstrated that using nested and dense skip connec- tions to inject encoder feature maps into the decoder, rather than directly fetching them, improves network performance. However, due to the limited receptive field size of convolu- tion operations, CNN-based methods primarily capture local dependencies and struggle to represent long-range depen- dencies. Although the dimensional size of images changes in different network blocks to cover a varying range, the operation remains limited to local information and not global contexts. The locality of convolution operations and their weight-sharing property hinder them from capturing global contexts.\nTo overcome the limitations of CNN networks, various approaches have been developed in recent years. Yu et al. [9] attempted to expand the receptive field size to capture global contexts without downsampling images and losing resolution, by employing atrous convolution with a dilation rate. Zhao et al. [10] used pyramid pooling at different feature scales to model global information. Wang et al. [18] proposed a non-local network to capture long-range dependencies by calculating response at a position as a weighted sum of all features within the input feature maps. Some studies, [12], [19], have discovered self-attention modules' potential to address the deficiency of CNNs in long-range dependency modeling. Despite these efforts to mitigate the shortcomings of CNNs, they remain unable to fully satisfy clinical applica-"}, {"title": "A. CNN-BASED SEGMENTATION NETWORKS", "content": "In recent years, deep learning methods have gained promi- nence in image segmentation tasks, replacing hand-crafted- feature based machine learning approaches. CNNs have be- come a popular choice for various medical image segmenta- tion tasks, primarily due to the success of U-Net [16]. The U-shaped symmetric structure of U-Net incorporates skip connections between each block of the encoder and decoder, enabling the concatenation of higher-resolution feature maps from the encoder network with upsampled features for more accurate representations. The success of U-Net has inspired researchers to adapt its architecture and improve its perfor- mance by applying various strategies, such as Res-UNet [5], Dense-UNet [6], U-Net++ [7], and UNet3+ [8]. The 3D U- net [17] was proposed as an enhanced version of U-Net, specifically designed for volumetric segmentation in three dimensions.\nOktay et al. [11] introduced attention gates to U-Net's skip connections, emphasizing the importance of specific objects by focusing on critical objects and disregarding irrelevant ones. Alryalat et al. [12] employed a dual-attention strategy in U-Net skip connections to make the network concentrate on more representative channels using channel attention and to identify the most informative spatial regions in images using spatial attention. Zhou et al. developed U-Net++ [7] and demonstrated that using nested and dense skip connec- tions to inject encoder feature maps into the decoder, rather than directly fetching them, improves network performance. However, due to the limited receptive field size of convolu- tion operations, CNN-based methods primarily capture local dependencies and struggle to represent long-range depen- dencies. Although the dimensional size of images changes in different network blocks to cover a varying range, the operation remains limited to local information and not global contexts. The locality of convolution operations and their weight-sharing property hinder them from capturing global contexts.\nTo overcome the limitations of CNN networks, various approaches have been developed in recent years. Yu et al. [9] attempted to expand the receptive field size to capture global contexts without downsampling images and losing resolution, by employing atrous convolution with a dilation rate. Zhao et al. [10] used pyramid pooling at different feature scales to model global information. Wang et al. [18] proposed a non-local network to capture long-range dependencies by calculating response at a position as a weighted sum of all features within the input feature maps. Some studies, [12], [19], have discovered self-attention modules' potential to address the deficiency of CNNs in long-range dependency modeling. Despite these efforts to mitigate the shortcomings of CNNs, they remain unable to fully satisfy clinical applica-"}, {"title": "B. TRANSFORMERS", "content": "The success of Transformer methods in natural language processing, where high dependency exists between words in text, has encouraged researchers to leverage these models' long-range dependency capabilities for image segmentation and recognition tasks. ViT [14] served as a foundational method, introducing Transformer approaches to machine vi- sion and outperforming traditional CNN-based architectures. This method partitions input images into segments called patches and embeds each patch's location so that the network can consider the spatial dependence between patches. These patches are then fed into a Transformer encoder, which employs multi-head attention modules, followed by a multi- layer perceptron for classification.\nTo improve the performance of this novel approach, sev- eral enhanced versions of ViT have been proposed, including Swin Transformer [20], LeViT [21], and Twins [22]. Given the complexity of these models, the Swin Transformer [20] sought to reduce the number of model parameters by dividing image patches into windows and applying the Transformer exclusively within patches inside each window. An additional step was suggested to allow adjacent windows to interact with each other, based on the fundamental principle of CNNs: shifting the window and then reapplying the Transformer module.\nAlthough Transformer-based methods have demonstrated great success in various domains, they are not without their limitations. One notable shortcoming is their weakness in capturing local information representation. Unlike CNNs, which inherently focus on local features due to their con- volution operations, Transformers primarily excel at cap- turing long-range dependencies. This limitation can lead to suboptimal performance in tasks where local information plays a crucial role in understanding and processing the data. Consequently, it has become imperative to explore hybrid models that can effectively leverage the strengths of both CNNs and Transformers in order to address these limitations and improve overall performance."}, {"title": "C. HYBRID CNN-TRANSFORMER APPROACHES", "content": "Recent advances in medical image segmentation have sought to harness the strengths of both CNNs and Transformer archi- tectures by incorporating Transformer layers into the encoder component of CNN networks. This enables the combined model to capture local information while also effectively modeling long-range dependencies. TransUNet [23] serves as a pioneering approach in this regard, utilizing a ResNet-50 backbone to generate low-resolution feature maps, which are then encoded using a ViT model. The encoded features are subsequently upsampled via cascaded upsampling layers to produce the final segmentation map. However, integrating a pure Transformer-based model alongside a CNN model can increase network complexity by up to eight times. To address this challenge, Cao et al. [24] introduced Swin-UNet, which computes attention within a fixed window (analogous to the Swin-Transformer approach). As an added feature, Swin- UNet includes a patch-expanding layer that reshapes adjacent feature maps into higher-resolution feature maps during the upsampling process. In another related approach, Wu et al. [25] incorporated a Transformer module into the encoding layers by replacing the single-branch encoder with a dual encoder containing both CNN and Transformer branches. Furthermore, the researchers devised a feature adaptation module (FAM) and a memory-efficient decoder to overcome the computational inefficiency associated with fusing these branches and the decoding component. In a similar vein, Azad et al. [26] tackled the limitations of traditional CNN- based methods by introducing a \"Context Bridge\". This feature merges the U-Net's local representation capability into a transformer model, overcoming issues in modeling long-range dependencies and handling diverse objects. Fur- thermore, they substituted the standard attention mechanism with an \"Efficient Self-attention\" strategy, simplifying the architecture without compromising performance.\nCNN-based methods are proficient in capturing local in- formation but struggle with modeling long-range depen- dencies essential for medical image analysis. In contrast, Transformer-based methods excel in long-range dependency representation but lack local information-capturing mecha- nisms. Therefore, our research aims to develop a method that combines the strengths of both models while maintaining acceptable network complexity. We propose a dual atten- tion module for handling spatial input features and chan- nel context, utilizing Wang et al.'s efficient self-attention method [27] and enhanced self-attention module [28] to minimize complexity. Our redesigned Transformer block is incorporated into a U-Net-like architecture, highlighting the significance of skip connections for improved performance and accurate feature reconstruction. By integrating a large kernel approach, we enhance information transfer, increase low-level localization information effectiveness, and ulti- mately strengthen the model's overall performance via better encoder-decoder communication."}, {"title": "III. PROPOSED METHOD", "content": "Figure 1 presents an overview of our proposed model, a hierarchical Transformer model with a U-Net-like structure that leverages both local and global feature representation along with an enhanced skip connection module. Given an input image $x^{H \\times W \\times C}$ with spatial dimensions H \u00d7 W and C channels, the model employs the patch embedding module [24], [28] to obtain overlapping patch tokens of size 4 \u00d7 4. The tokenized input ($x_{n \\times d}$) then passes through the encoder module, which comprises three stacked encoder blocks, each containing two sequential dual Transformer layers and a patch merging layer. Patch merging combines 2 \u00d7 2 patch tokens, reducing the spatial dimension while doubling the channel dimension, enabling the network to achieve a multi- scale representation hierarchically. In the decoder, tokens are"}, {"title": "A. DUAL ATTENTION TRANSFORMER BLOCK", "content": "The motivation for incorporating dual attention in our model comes from the realization that both channel and spatial attention are essential in medical image segmentation tasks. Accurate segmentation results rely on the efficient represen- tation of feature tensors. Channel attention enables the model to focus selectively on the most informative representation, fostering a deeper understanding of the structures within medical images. In contrast, spatial attention emphasizes spatial relationships between features, allowing the model to capture vital contextual information and dependencies across various regions in the image. By integrating dual attention, our model effectively combines the strengths of both channel and spatial attention, ultimately enhancing its performance in medical image segmentation tasks. This approach enables the development of a more robust and efficient network capable of representing feature tensors effectively, leading to improved segmentation outcomes. A visual representation of the dual attention mechanism is illustrated in Figure 2. This figure illustrates how the channel and spatial attention com- ponents work in tandem to boost the model's segmentation capabilities. It is important to note that our design applies the attention mechanisms sequentially, not in parallel, leading to improved performance.\nTo harness the advantages of the dual attention mechanism without incurring its associated complexity, we use an effi- cient attention module for channel attention and an enhanced transformer block for spatial attention. The limitation of the standard self-attention mechanism is its quadratic computa- tional complexity ($O(N^2)$), as shown in Equation (1). This restricts the architecture's applicability to high-resolution medical images.\n$S(Q, K, V) = softmax(\\frac{Q K^T}{\\sqrt{d_k}}) V.$\nIn Equation (1), Q, K, and V denote the query, key, and value vectors, respectively, while $d_k$ represents the embedding dimension. By adopting the efficient attention mechanism, we reduce the computational overhead without sacrificing the benefits offered by the channel attention approach. This allows our model to process feature maps more effectively and deliver enhanced performance in medical image segmen- tation tasks. Furthermore, the efficient attention mechanism ensures that the model remains scalable, enabling its applica- tion to a broader range of use cases and datasets. We utilize"}, {"title": "B. INTER-SCALE INTERACTION MODULE", "content": "The attention mechanism fundamentally serves as a dynamic selector, adept at emphasizing relevant features across scales while marginalizing redundant ones by relying on input features. An essential byproduct of this mechanism is the at- tention map, which operates akin to a spotlight, highlighting the relative significance of various features across different scales. This spotlight subsequently aids in deciphering how different features correlate.\nThe keys are represented as d attention maps kTj, where j denotes the position j in the input feature.  The standard self attention, (ON()) RCR1\u00d71"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "This section provides details about the training process, the metrics we used during our experimental evaluation, and a detailed analysis of the experimental results."}, {"title": "A. TRAINING PROCESS", "content": "In this study, we implemented the proposed method using PyTorch on an NVIDIA Tesla V100 GPU with a 24-batch size without any data augmentation. For 400 epochs, we trained all the models at a learning rate of 1e - 3 and a decay rate of 1e - 4. The weight of the model was initialized using a standard normal distribution, which is stable from the start and ensures less fluctuation in weight. Furthermore, if the validation performance does not change in ten consecutive epochs during the training process, the training process stops. On both training and validation datasets, the optimization algorithm gradually decreased the loss value and eventually converged to the optimal solution during the training process. There was no evidence of instability during the training process."}, {"title": "B. DATASET", "content": "The proposed method was evaluated on Synapse multi-organ segmentation dataset [36]. Beyond the Cranial Vault (BTCV) abdomen challenge, dataset [36] includes 30 abdominal CT scans for a total of 3779 axial contrast-enhanced abdominal"}, {"title": "C. QUANTITATIVE AND QUALITATIVE RESULTS", "content": "Table 1 showcases a comparative analysis of our proposed approach against several benchmarking methods. This in- cludes both our preliminary models (baselines) and a few top-performing, state-of-the-art architectures.\nFor a comprehensive understanding of our method's effec- tiveness, we evaluated three distinct baselines:\nBaseline: This model forms the foundation of our approach and excludes the enhancements of dual attention and ISIM. Instead, it only employs an efficient attention module in each transformer block.\nProposed Method (without ISIM): An evolution of the ini- tial model, this version incorporates both channel and spatial attention. As we describe it, this combines the efficiencies of both attention mechanisms.\nProposed Method: This embodies our holistic approach, utilizing all features, including ISIM.\nSequential enhancements in our model evidently enhanced its performance. Incorporating dual attention and subse- quently, the ISIM, empirically validated our strategy's po- tency in addressing medical image segmentation challenges."}, {"title": "D. ABLATION STUDY", "content": "A defining trait of our technique is its ability to adeptly capture long-range dependencies. The superior prediction capabilities for larger organs, such as the liver, compared to other models, stand testament to this. The model's prowess in accommodating these long-range dependencies within its predictive realm is significant.\nAdditionally, we observed that for smaller organs, such as the aorta, U-Net models tend to outshine other Transformer- based methodologies. This highlights the indispensable role of local feature representation when predicting smaller enti- ties and the consequential need to assimilate this information into the prediction matrix.\nReinforcing our point on the model's capability to harness long-range information, it's imperative to note our method's adeptness in segmenting both small and large organs. This demands a considerable receptive field size for precision in object prediction. We further elucidate this with a class acti- vation map for both organ types in Figure 4, shedding light on our model's enhanced ability to discern local patterns, resulting in meticulous segmentation."}, {"title": "V. CONCLUSION", "content": "In this study, we presented and assessed a new architecture designed for medical image segmentation, which harmo- niously combines efficient and enhanced attention mecha-"}]}