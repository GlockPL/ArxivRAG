{"title": "Multimodal Chain-of-Thought Reasoning via ChatGPT to Protect Children from Age-Inappropriate Apps", "authors": ["Chuanbo Hu", "Bin Liu", "Minglei Yin", "Yilu Zhou", "Xin Li"], "abstract": "Mobile applications (Apps) could expose children to inappropriate themes such as sexual content, violence, and drug use. Maturity rating offers a quick and effective method for potential users, particularly guardians, to assess the maturity levels of apps. Determining accurate maturity ratings for mobile apps is essential to protect children's health in today's saturated digital marketplace. Existing approaches to maturity rating are either in-accurate (e.g., self-reported rating by developers) or costly (e.g., manual examination). In the literature, there are few text-mining-based approaches to maturity rating. However, each app typically involves multiple modalities, namely app description in the text, and screenshots in the image. In this paper, we present a framework for determining app maturity levels that utilize multimodal large language models (MLLMs), specifically ChatGPT-4 Vision. Powered by Chain-of-Thought (CoT) reasoning, our framework systematically leverages ChatGPT-4 to process multimodal app data (i.e., textual descriptions and screenshots) and guide the MLLM model through a step-by-step reasoning pathway from initial content analysis to final maturity rating determination. As a result, through explicitly incorporating CoT reasoning, our framework enables ChatGPT to understand better and apply maturity policies to facilitate maturity rating. Experimental results indicate that the proposed method outperforms all baseline models and other fusion strategies.", "sections": [{"title": "1. Introduction", "content": "The widespread use of mobile devices (e.g., smartphones and tablets) has reshaped our engagement with digital content, typically provided in the format of mobile apps, in our daily lives. As of 2024, a staggering number of apps are available, with approximately 3.95 million on Google Play [1] and about 1.83 million on Apple App Store [2]. In particular, children are increasingly using these devices for both educational and recreational activi-ties. Despite the benefits, the proliferation of age-inappropriate content poses serious risks for young children. Exposure to apps featuring mature content such as violence, sexual content, gambling, or drug use can have detrimental effects on children's psychological and behavioral development. Research in developmental psychology underscores that young children, when exposed to such content, may not only mimic these behaviors but also suffer long-term developmental consequences [3, 4]. Therefore, ensuring that children interact only with age-appropriate apps is crucial for safeguarding their growth and well-being in an increasingly digital world.\nTo help mitigate these risks, mobile platforms like Apple and Google have implemented app maturity rating systems, akin to those used in the video game and movie industries. These systems are designed to inform users about the content of apps and to suggest the appropriate age group for their use. For example, App Store implements a maturity rating policy that includes four levels: 4+, 9+, 12+, and 17+. These categories are designed to guide users on the appropriate age group for each app, helping to safeguard younger users from content that may be unsuitable for their age group. In addition, App Store also provides detailed descriptions of the content that justify these ratings. This transparency helps users understand the nature of an app and the reasons behind its specific maturity classification. Similarly, Google Play employs an age-based maturity rating that determined by different territories 2. However, unlike the ratings issued by the Motion Picture Association of America (MPAA) and the ESRB, which are based on evaluations by independent rating committees, app ratings are often based on developers' self-reported assessments. This self-reporting approach can"}, {"title": "2. Background and Literature Review", "content": "2.1. Mobile App Maturity Rating\nApps could expose users to mature themes. Therefore, app platforms such as the App Store and Google Play implement maturity rating systems.\nApp Store's Maturity Rating Policy: The App Store requires all published apps to submit maturity ratings according to its maturity rating policy. As outlined in Table 2, the App Store categorizes apps into four maturity levels: 4+, 9+, 12+, and 17+. The categorization is based on the content's nature and intensity, such as violence, sexual themes, profanity, and substance use. For instance, an app may be rated 12+ for mild violence but 17+ if the violence is intense. The highest level of mature content detected determines the app's final maturity rating.\nGoogle Play's Maturity Rating Policy: Google Play employs an age-based rating system, with maturity levels influenced by regional regulatory bodies 3. For example, in the United States, Google Play adheres to the Entertainment Software Rating Board (ESRB) standards, categorizing apps into ratings such as \u2018Everyone', \u2018Everyone 10+', \u2018Teen', \u2018Mature', and 'Adults Only'. If no regional authority is present, ratings are assigned by the International Age Rating Coalition."}, {"title": "2.2. Text Mining Based App Maturity Rating", "content": "The lack of accuracy and reliability in current self-reported app ratings underscores the need for an accurate automatic system for assessing app ma-turity. There has been limited research conducted on app maturity rating systems. Chen et al. [5] proposed a keyword matching-based approach to rate app maturity, which often resulted in limited accuracy due to a lack of seman-tic understanding. Hu et al. [6] proposed a text classification-based maturity rating method, which combines an augmented keyword list and traditional bag-of-words models to access app maturity levels from app descriptions. Zhou et al. [7] proposed a machine learning-based method by integrating both app metadata and features of the apps. Despite these advancements, previous methods predominantly focused on textual data from app descrip-tions and overlooked the rich information available in other modalities like screenshots. Our work distinguishes itself by integrating both text and im-age data, leveraging the complementary information from both to enhance maturity rating accuracy."}, {"title": "2.3. Multimodal Large Language Models", "content": "Multimodal learning has significantly advanced AI capabilities by effec-tively integrating diverse data types such as text, images, and audio and has led to success in various applications [8, 9, 10, 11, 12, 13]. Recently, the emergence of Large Language Models (LLMs) has further propelled this field, particularly through zero-shot learning abilities, where these models perform tasks they haven't been explicitly trained on, demonstrating remarkable flex-ibility and breadth of application [14, 15, 16, 17]. The introduction of spe-cialized Multimodal LLMs, such as ChatGPT-4 Vision (GPT-4V), marks a pivotal development [18]. These models are not only adept at process-ing mixed data types (i.e., text and image) but also excel in generating coherent, contextually accurate outputs across different modalities, setting a new standard in AI's operational capabilities. Recent studies have begun to demonstrate the potential of GPT-4V in tackling domain-specific challenges"}, {"title": "2.4. Chain-of-Thought Reasoning Based on Large Language Model", "content": "The Chain-of-Thought (CoT) reasoning approach has significantly en-hanced the capabilities of LLMs, enabling them to tackle complex, multi-step problems by mimicking human-like reasoning processes [25]. Kojima et al. [26] demonstrate how these models excel in zero-shot reasoning tasks, ad-vancing the field significantly. A self-consistency method has been proposed that refines CoT reasoning, improving the analytical accuracy of these mod-els [27]. Zhou et al. [28] further expand on this by exploring least-to-most prompting, enhancing complex reasoning in LLMs. Despite these advance-ments, the integration of CoT with multimodal models, which process both text and visual data, remains largely untapped, promising significant poten-tial for areas requiring nuanced analysis, such as app maturity rating."}, {"title": "3. Methods", "content": "3.1. Problem Formulation\nFocusing on App Store, we aim to design an effective framework that leverages the multimodal data associated with apps, namely app descriptions and screenshot images, to predict mature ratings for the apps under the guideline of the maturity rating policy of an app market (e.g., App Store). Specifically, for each app i, we have access to its textual description Di, and its n screenshot images Si = [S1, S2, ..., Sn]. We also have knowledge of the maturity rating policy as shown in Table 2. The policy establishes a direct correlation between the Maturity Content (MC) indicators and their corresponding maturity ratings (R), where the intensity (I) of each MC influences the level of maturity rating assigned. For each app i, there are a set of 12 MC indicators MC\u2081 = [MC1, MC2, ..., MC12], and its maturity rating R\u00bf \u2208 {R1, R2, R3, R4} which is aligned with four age groups: 4+,9+,12+, and 17+. Our goal is to design an effective system to determine the maturity levels for apps."}, {"title": "3.2. Overview of the Proposed Framework", "content": "Building on aforementioned problem formulation, we propose a frame-work that utilizes multimodal large language models (MLLMs), specifically ChatGPT-4 Vision, to determine the maturity levels for apps. As shown in Figure 1, our framework takes the multimodal input including textual de-scriptions D and screenshots S = [S1, S2, ..., Sn] of apps, which are crucial in reflecting the app's content under the specific rating policy of an app platform, for ChatGPT-4V to access maturity levels. On the one hand, the maturity information in the descriptions could be ambiguous and obscure. On the other hand, different screenshots would contain different levels of maturity contents, and only a subset of the screenshots are useful in deter-mining the maturity levels. Therefore, we further endow our framework with chain-of-thought (CoT) reasoning, with which the ChatGPT-4V follows a se-quence of logical steps to derive maturity levels for apps. The detailed steps are summarized as follows:\nStep 1: Maturity Content and Intensity Extraction. Given the screen-shots S = [S1, S2, ..., Sn] of an app, we utilize CoT reasoning to enable GPT-4V to process each screenshot sequentially to deduce the maturity content MC and its intensity levels I. Since different screenshots would contain dif-ferent levels of maturity contents, this step-by-step reasoning will identify and rank the screenshots according to the exist and intensity of maturity contents in the given app according to the maturity rating policy.\nStep 2: Maturity Rating Determination. The maturity ratings R are computed by combing the top screenshot(s) S* \u2208 {S1, S2, ..., sn} identified in step 1 and the textual descriptions D."}, {"title": "3.3. Chain-of-Thought (CoT) Endowed Prompting for App Maturity Rating", "content": "The effectiveness of using a MLLM for app maturity rating relies heavily on the design of specialized prompts that analyze both textual description D and screenshot images S = [S1, S2, ..., Sn] with CoT reasoning. As shown in Figure 2, our CoT-endowed prompting enables maturity rating reasoning through a sequence of reasoning steps, and it is composed of maturity content prompt and maturity intensity prompt to identify the maturity content and its intensity in each screenshot, and a maturity rate prompt, which combines the top screenshot(s) and the textual descriptions to determine the final maturity rating for the app.\nMaturity Content Prompt: The goal of maturity content prompt is to identify the maturity content indicator MC = [MC1, MC2, ..., MC12] for a given screenshot s \u2208 [S1, S2, ..., sn]. We design maturity content prompt Pmc according to the maturity rating policy as shown in Table 2. As shown in Figure 2(a), the maturity content prompt PMC instructs the MLLM \u0192MLLM to output the maturity content MC as follows:\nMCs = fMLLM(MC|Xs, PMC)\nwhere X, is the image of screenshot s \u2208 [S1, S2, ..., Sn].\nMaturity Intensity Prompt: After deriving maturity content MCs for screenshots, we continue to evaluate the intensity of the identified ma-turity content. As shown in Figure 2(b), the maturity intensity prompt PMI instructs the MLLM model fmllm to output the maturity intensity I as follows:\nIs = fMLLM(I|X5, MC's, PMI).\nFor example, if we identified \u201cCartoon or Fantasy Violence\u201d in a screenshot, then maturity intensity prompt PMI is designed as follows:\nPlease response the maturity level of content: Cartoon or Fantasy Violence, ranging from mild to intense in the screenshot image.\nMaturity Rate Prompt: After identifying the maturity content and its intensity in each screenshot s\u2208 [S1, S2, ..., sn], we rank them according to (MCs, Is)se{1,...,n} values, and obtain the top screenshot(s) S* \u2208 {S1, S2, ..., Sn}. Then the maturity ratings R are computed by combing the top screenshot(s) S* and the textual descriptions D. As shown in Figure 2(c), the maturity rating prompt PMR instructs the MLLM model fmLLM to output the maturity rating R as follows:\nR = fMLLM(R|Xs,s\u2208S*, D, PMR).\nThis prompt directs the MLLM to classify the app into one of four age-appropriate categories {4+,9+,12+,17+} based on the evaluated content. This prompt encourages the model to use its inherent knowledge to distill the extracted content themes and intensities into a clear age rating, employing a straightforward fusion strategy to assess the overall suitability for potential user groups."}, {"title": "4. Experiments", "content": "In this section, we present empirical evaluations to carefully assess our proposed method.\n4.1. Experimental Setup\nDataset. We crawled data from App Store and curated a dataset for our empirical evaluations in this study. The dataset consists of 1,281 mobile apps, categorized into four age groups: 324 apps rated 4+, 334 apps rated 9+, 394 apps rated 12+, and 229 apps rated 17+. Each app includes between 2 to 7 screenshot images along with a description. These elements serve as the multimodal inputs to the MLLM to assess the maturity level of the content. The apps were categorized across various genres to ensure diversity in content type (see Table 2), which aids in generalizing the model's applicability.\nBaseline Models. App maturity rating is a multi-class classification prob-lem, where we classify each app into one of the four age-appropriate categories {4+,9+,12+,17+}. To benchmark the performance of our proposed CoT-endowed method, we compare it against several established baseline models with different large language models. These models are selected based on their prior success in similar tasks such as text classification, image classifi-cation, and multimodal classification.\n\u2022 Vicuna [29] is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations and we use Vicuna to process app textual descriptions for maturity rating.\n\u2022 GPT-3.5 and GPT-4 [30]. We respectively use GPT-3.5 and GPT-4 to input app textual descriptions to return the maturity ratings.\n\u2022 LLaVa-1.5 [31] is a large multimodal model trained by combining a vision encoder and Vicuna for general-purpose visual and language un-derstanding. We apply LLaVa-1.5 to process the app screenshots to return the maturity ratings.\n\u2022 GPT-4V [30] is the Multimodal LLM developed by OpenAI by incorpo-rating image modalities into large language models. We instruct GPT-4V to analyze single-modality (screenshot-only) and dual-modality (screen-shot+description) to return the maturity ratings.\nEvaluation Metrics. Since app maturity rating is a multi-class classifica-tion problem, we employ a variety of classification metrics to comprehensively"}, {"title": "4.2. Comparison Against Baseline Methods", "content": "Table 3 shows the results of our proposed CoT-endowed MLLM frame-work for app maturity rating against the baseline approaches. Our approach consistently outperforms the baseline methods across all metrics with sig-nificant margins. Specifically, we first observe that different LLM models would lead to different performances. For example, for methods that only use textual description for maturity rating, GPT-4 outperforms GPT-3.5 and Vicuna; for methods that only use screenshots for maturity rating, GPT-4V outperforms LLaVa-1.5 (F1-score 59.96% vs 29.38%). Second, for the same LLM platform, the text-based method (GPT-4 with F1-score of 63.94%) out-performs the image-based method (GPT-4V with F1-score of 59.96%). Third,"}, {"title": "4.3. Comparative Analysis of Different Multimodal Fusion Strategies for App Maturity Rating", "content": "Typically, different multimodal fusion strategies would lead to different multimodal learning performances. This subsection evaluates the effective-ness of different fusion strategies in classifying app maturity, focusing on how they integrate multimodal information. We compare following multimodal fusion strategies.\n\u2022 Basic Fusion Strategy: This baseline method uses a single screenshot combined with the app description for maturity rating.\n\u2022 Image-Caption Fusion Strategy: This method involves using GPT-4V to create a caption for a selected screenshot, which is then combined with the app's main description to predict the maturity rate. This strategy leverages the descriptive power of GPT-4V to enhance textual understanding of visual content.\n\u2022 Global CoT Fusion: This strategy analyzes all available screenshots along with the description, offering a comprehensive assessment by con-sidering all visual data for a complete understanding of the app's con-tent.\n\u2022 Selective CoT Fusion: This approach combines textual description with the most indicative content through a selection of the top-ranked screenshot(s) for maturity rating."}, {"title": "4.4. Impact of Chain-of-Thought (CoT) Reasoning on Maturity Rating", "content": "Note that in our proposed CoT endowed prompting for app maturity rating, we first identify and rank the screenshots according to the exist and intensity of maturity contents and then combine the top screenshot(s) and the textual descriptions to determine the final maturity rating. To further eval-uate the effectiveness of our proposed CoT on maturity rating, we conduct experiments on different CoT reasoning strategies. Table 5 shows the per-formances of maturity rating with different CoT reasoning strategies. In the \u201cText understanding + Image\u201d CoT strategy, similar to the way to prompt MLLM to understand the exist and intensity of maturity content in an image, we use the same method to comprehend app descriptions before we combine with the screenshot for final maturity rating. Accordingly, in the \u201cImage se-lection + Text understanding\" CoT strategy, prompt MLLM to understand both images and text, and combine the results to access maturity levels. We can see that the performance of the \u201cText understanding + Image\" CoT strategy is notably poor, which in turn affects the overall effectiveness when combined with image selection-based CoT. Specifically, the combination of CoT applied to both text and image modestly improves accuracy and pre-cision but remains significantly lower than desired. Surprisingly, using CoT to identify and rank the screenshots and then combing text yields much bet-ter results. These findings highlight the challenges of implementing CoT in text processing within this context and suggest that while CoT enhances image analysis, its application in the text needs further refinement to avoid diminishing the benefits of multimodal fusion strategies.\nTo further investigate the importance of the proposed CoT method, we conduct an ablation study by comparing two image selection methods for app maturity rating. Table 6 shows the results of the ablation study. The \"w/ CoT Image\u201d method applies CoT reasoning to systematically select the"}, {"title": "4.5. Case studies", "content": "4.5.1. Case Study 1: Evaluating Multimodal Fusion Strategies for Maturity Rating\nIn this subsection, we present case studies from our experimental eval-uations to illustrate the application of our MLLM for app maturity rating (see Table 7). These cases highlight how different fusion strategies compare against each other and the ground truth (GT) ratings. Each case study involves a specific mobile game featuring both visual and textual content, which our framework processes to predict the appropriate age rating."}, {"title": "4.5.2. Case Study 2: Comparative Analysis of CoT-based method and Non-CoT-based method", "content": "Some case studies further explore the impact of selected images on the performance of our maturity evaluation framework (see Table 8). Each case contrasts the effect of using different images from the same app on the pre-dicted maturity ratings, examining the role of CoT in this process.\nCase 1 in Table 8 shows that the GT rating for this game is 17, likely due to its horror and violent themes. Interestingly, both with and without the use of CoT, the model rated the app as suitable for 12+. This discrepancy suggests that while the selected images were significant, they did not fully capture the maturity level expected by the GT, indicating a need for more comprehensive visual analysis or an adjustment in how images are weighted by the model. Case 2 demonstrates the effectiveness of CoT in interpreting more complex or nuanced content accurately when presented with more ex-plicit imagery. Case 3 shows that the GT for this app is surprisingly low at 9, despite the model with CoT assigning a rating of 17, influenced perhaps by the aggressive thematic content of the selected image. The rating without CoT resulted in a more moderate 12, indicating sensitivity to image selection and the challenge of aligning model perception with community standards. Case 4 presents an interesting reversal where the model with CoT suggested a 12+ rating, while without CoT, the rating matched the GT at 9. This suggests that CoT might be emphasizing the warfare aspect more heavily, impacting the perceived suitability for younger players.\nThese case studies underline the critical role of image selection in auto-mated content rating systems and highlight how different images can lead to varying interpretations of app content. They also emphasize the importance of implementing robust reasoning mechanisms like CoT, which can discern subtleties in content that might otherwise lead to underestimation or overes-timation of maturity levels. This analysis is crucial for refining the accuracy of our model, ensuring that it not only performs well across a broad spectrum of apps but also aligns closely with established content rating standards."}, {"title": "4.6. Discussion", "content": "Multimodal large language models (MLLMs) represented by GPT-4V have shown impressive performances in different tasks such as image and text understanding. As shown in our study, applying MLLMs to a real-world application still requires careful consideration. For example, in the maturity rating task in this study, CoT reasoning can improve the performance further than the direct application of GPT-4V. Also, the effectiveness of these fusion strategies can vary based on the type of content and the specific requirements of the application. For example, the analysis of Basic Fusion, Image-Caption Fusion, and Global CoT Fusion strategies highlights the varying degrees of success and challenges, suggesting that no one-size-fits-all solution exists. This insight is crucial for developers and researchers looking to optimize content classification systems for specific user needs and content types.\nAdditionally, the exploration of more advanced multimodal learning tech-niques that seamlessly integrate various data types (text, image, video, etc.) could further refine the effectiveness of maturity rating systems. Such ad-vancements could pave the way for more nuanced and context-aware AI sys-tems capable of adapting to the ever-evolving landscape of digital content. These future directions not only promise to enhance the technical capabil-ities of maturity assessment models but also contribute to safer and more appropriate digital environments for diverse user groups."}, {"title": "5. Conclusion and Future Work", "content": "In this work, we presented the first systematic study on app maturity rat-ing with multimodal large language models (MLLMs). We designed chain-of-thought (CoT) endowed prompting to instruct MLLM to better understand the images and text associated with apps step by step to determine maturity levels for apps. Our comprehensive experiments have demonstrated that the integration of CoT with multimodal data significantly improves the accu-racy and reliability of maturity ratings. The CoT methodology, particularly when applied to a combination of textual and visual data, provides a deeper understanding of content, enabling more precise classifications and proving superior to traditional approaches.\nLimitations and Future Research. There are several limitations in this work and some interesting future research directions. First, due to the limit on GPT-4V access, we evaluated our method on a relatively small dataset. A better model evaluation can be achieved by experiments with more data. Sec-ond, more advanced methods could be applied to improve the maturity rating performances. Finally, the implications of this study are vast, suggesting av-enues for further enhancements in AI-driven content evaluation systems. The success of the Selective CoT Fusion strategy in our tests highlights the poten-tial for targeted data processing techniques to refine AI assessments in vari-ous digital environments. As digital content continues to grow in complexity and volume, the need for advanced, nuanced, and reliable content filtering technologies becomes increasingly critical. Our research contributes to this ongoing development, laying the groundwork for future innovations that will continue to enhance safe and appropriate content consumption across digital platforms."}]}