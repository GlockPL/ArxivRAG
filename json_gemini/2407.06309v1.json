{"title": "Multimodal Chain-of-Thought Reasoning via ChatGPT to Protect Children from Age-Inappropriate Apps", "authors": ["Chuanbo Hu", "Bin Liu", "Minglei Yin", "Yilu Zhou", "Xin Li"], "abstract": "Mobile applications (Apps) could expose children to inappropriate themes such as sexual content, violence, and drug use. Maturity rating offers a quick and effective method for potential users, particularly guardians, to assess the maturity levels of apps. Determining accurate maturity ratings for mobile apps is essential to protect children's health in today's saturated digital marketplace. Existing approaches to maturity rating are either inaccurate (e.g., self-reported rating by developers) or costly (e.g., manual examination). In the literature, there are few text-mining-based approaches to maturity rating. However, each app typically involves multiple modalities, namely app description in the text, and screenshots in the image. In this paper, we present a framework for determining app maturity levels that utilize multimodal large language models (MLLMs), specifically ChatGPT-4 Vision. Powered by Chain-of-Thought (CoT) reasoning, our framework systematically leverages ChatGPT-4 to process multimodal app data (i.e., textual descriptions and screenshots) and guide the MLLM model through a step-by-step reasoning pathway from initial content analysis to final maturity rating determination. As a result, through explicitly incorporating CoT reasoning, our framework enables ChatGPT to understand better and apply maturity policies to facilitate maturity rating. Experimental results indicate that the proposed method outperforms all baseline models and other fusion strategies.", "sections": [{"title": "1. Introduction", "content": "The widespread use of mobile devices (e.g., smartphones and tablets) has reshaped our engagement with digital content, typically provided in the format of mobile apps, in our daily lives. As of 2024, a staggering number of apps are available, with approximately 3.95 million on Google Play [1] and about 1.83 million on Apple App Store [2]. In particular, children are increasingly using these devices for both educational and recreational activities. Despite the benefits, the proliferation of age-inappropriate content poses serious risks for young children. Exposure to apps featuring mature content such as violence, sexual content, gambling, or drug use can have detrimental effects on children's psychological and behavioral development. Research in developmental psychology underscores that young children, when exposed to such content, may not only mimic these behaviors but also suffer long-term developmental consequences [3, 4]. Therefore, ensuring that children interact only with age-appropriate apps is crucial for safeguarding their growth and well-being in an increasingly digital world.\nTo help mitigate these risks, mobile platforms like Apple and Google have implemented app maturity rating systems, akin to those used in the video game and movie industries. These systems are designed to inform users about the content of apps and to suggest the appropriate age group for their use. For example, App Store implements a maturity rating policy that includes four levels: 4+, 9+, 12+, and 17+ 1. These categories are designed to guide users on the appropriate age group for each app, helping to safeguard younger users from content that may be unsuitable for their age group. In addition, App Store also provides detailed descriptions of the content that justify these ratings. This transparency helps users understand the nature of an app and the reasons behind its specific maturity classification. Similarly, Google Play employs an age-based maturity rating that determined by different territories 2. However, unlike the ratings issued by the Motion Picture Association of America (MPAA) and the ESRB, which are based on evaluations by independent rating committees, app ratings are often based on developers' self-reported assessments. This self-reporting approach can lead to inaccuracies. Developers may underestimate the maturity level of their apps, either inadvertently or to broaden their market reach, leading to a misclassification of content [5]. This situation has caused growing concern among parents and educators, who frequently encounter apps with maturity ratings that do not accurately reflect their content.\nTo the best of our knowledge, there has been limited research conducted on app maturity rating systems. Chen et al. [5] studied maturity ratings across Android and iOS applications, identifying discrepancies and proposing methods to improve content safety for children. Hu et al. [6] proposed a text classification based automatic maturity rating framework to access app maturity levels from app descriptions. Zhou et al. [7] improved previous methodologies by integrating a more advanced machine learning model that uses both app metadata and user-generated content. However, previous studies on app maturity rating have significant limitations. First, these approaches often fail to capture the full spectrum of content (both textual description and screenshot images) that can influence an app's maturity rating. Second, these approaches are based on conventional supervised machine learning methods, which typically required large amount of labeled data to train the models, and may not fully leverage the capabilities of more advanced analytical techniques.\nResponding to these challenges, in this paper, we present a framework that utilizes multimodal large language models (MLLMs), specifically ChatGPT-4V, to take input of multimodal data (i.e., textual descriptions and screenshots) of apps to determine the maturity levels for apps. Unique difficulties arise when applying the textual descriptions and screenshots information for app maturity rating. First, the main purpose of app description and its screenshots is to introduce its functionality and to promote the app on an app market (e.g, App Store) rather than to directly provide information regarding its maturity level. For example, the maturity information, if any, in the descriptions could be ambiguous and obscure. Meanwhile, unlike maturity images in the film industry, app screenshots reveal maturity content in a more complex way. Second, different screenshots would contain different levels of maturity contents. As such, only a subset of the screenshots are useful in determining the maturity levels. To this end, we design chain-of-thought (CoT) endowed prompting for app maturity rating. CoT prompting enables ChatGPT-4V to follow a sequence of logical steps to derive maturity levels for apps. Specifically, we first identify and rank the screenshots according to the exist and intensity of maturity contents for a given app according to the maturity rating policy. Accordingly, based on the policy, we design maturity content prompt and maturity intensity prompt to identify the maturity content and its intensity in each screenshot. Then we design a maturity rate prompt, which combines the top screenshot(s) and the textual descriptions, to determine the final maturity rating for the app.\nWe assess our proposed method through extensive experiments on a dataset that we collected from the App Store. The results show that the proposed approach consistently outperforms baseline models by a significant margin and demonstrates the effectiveness of the CoT-endowed prompting with MLLM for app maturity rating.\nIn summary, our key contributions are as follows:\n\u2022 We conduct the first systematic study on app maturity rating with multimodal large language models (MLLMs).\n\u2022 We design chain-of-thought (CoT) endowed prompting to instruct MLLM to follow a sequence of logical steps to derive maturity levels for apps.\n\u2022 We conduct extensive experiments on datasets collected from the App Store with different LLM models and different multimodal fusion strategies, and the results demonstrate the effectiveness of the proposed method."}, {"title": "2. Background and Literature Review", "content": "Apps could expose users to mature themes. Therefore, app platforms such as the App Store and Google Play implement maturity rating systems.\nThe App Store requires all published apps to submit maturity ratings according to its maturity rating policy. As outlined in Table 2, the App Store categorizes apps into four maturity levels: 4+, 9+, 12+, and 17+. The categorization is based on the content's nature and intensity, such as violence, sexual themes, profanity, and substance use. For instance, an app may be rated 12+ for mild violence but 17+ if the violence is intense. The highest level of mature content detected determines the app's final maturity rating.\nGoogle Play employs an age-based rating system, with maturity levels influenced by regional regulatory bodies 3. For example, in the United States, Google Play adheres to the Entertainment Software Rating Board (ESRB) standards, categorizing apps into ratings such as \u2018Everyone', \u2018Everyone 10+', \u2018Teen', \u2018Mature', and 'Adults Only'. If no regional authority is present, ratings are assigned by the International Age Rating Coalition."}, {"title": "2.2. Text Mining Based App Maturity Rating", "content": "The lack of accuracy and reliability in current self-reported app ratings underscores the need for an accurate automatic system for assessing app maturity. There has been limited research conducted on app maturity rating systems. Chen et al. [5] proposed a keyword matching-based approach to rate app maturity, which often resulted in limited accuracy due to a lack of semantic understanding. Hu et al. [6] proposed a text classification-based maturity rating method, which combines an augmented keyword list and traditional bag-of-words models to access app maturity levels from app descriptions. Zhou et al. [7] proposed a machine learning-based method by integrating both app metadata and features of the apps. Despite these advancements, previous methods predominantly focused on textual data from app descriptions and overlooked the rich information available in other modalities like screenshots. Our work distinguishes itself by integrating both text and image data, leveraging the complementary information from both to enhance maturity rating accuracy."}, {"title": "2.3. Multimodal Large Language Models", "content": "Multimodal learning has significantly advanced AI capabilities by effectively integrating diverse data types such as text, images, and audio and has led to success in various applications [8, 9, 10, 11, 12, 13]. Recently, the emergence of Large Language Models (LLMs) has further propelled this field, particularly through zero-shot learning abilities, where these models perform tasks they haven't been explicitly trained on, demonstrating remarkable flexibility and breadth of application [14, 15, 16, 17]. The introduction of specialized Multimodal LLMs, such as ChatGPT-4 Vision (GPT-4V), marks a pivotal development [18]. These models are not only adept at processing mixed data types (i.e., text and image) but also excel in generating coherent, contextually accurate outputs across different modalities, setting a new standard in AI's operational capabilities. Recent studies have begun to demonstrate the potential of GPT-4V in tackling domain-specific challenges across various fields. For example, Schramm et al. explored the impact of multimodal prompts in brain MRI diagnostics, demonstrating the efficacy of GPT-4V in analyzing complex medical imagery [19]. Similarly, Pillai et al. evaluated the capabilities of GPT-4V in dermatology, particularly for diagnostic and treatment recommendations, marking a significant advancement in medical AI applications [20]. Furthermore, Jia et al. investigated the use of GPT-4V in media forensics, specifically its ability to detect deep-fakes, revealing its utility in digital content verification [21]. Lastly, Lian et al. introduced a zero-shot benchmark for emotion recognition using GPT-4V, expanding its uses into emotional AI interactions [22]. These studies collectively highlight the versatility of GPT-4V and its transformative impact across different fields. However, despite its advancements, ChatGPT still faces significant challenges, particularly in sophisticated reasoning and contextual understanding. It sometimes struggles with complex problem-solving that requires nuanced, logical deduction, highlighting a critical area for further development to enhance its effectiveness and reliability [23, 24]."}, {"title": "2.4. Chain-of-Thought Reasoning Based on Large Language Model", "content": "The Chain-of-Thought (CoT) reasoning approach has significantly enhanced the capabilities of LLMs, enabling them to tackle complex, multi-step problems by mimicking human-like reasoning processes [25]. Kojima et al. [26] demonstrate how these models excel in zero-shot reasoning tasks, advancing the field significantly. A self-consistency method has been proposed that refines CoT reasoning, improving the analytical accuracy of these models [27]. Zhou et al. [28] further expand on this by exploring least-to-most prompting, enhancing complex reasoning in LLMs. Despite these advancements, the integration of CoT with multimodal models, which process both text and visual data, remains largely untapped, promising significant potential for areas requiring nuanced analysis, such as app maturity rating."}, {"title": "3. Methods", "content": "Focusing on App Store, we aim to design an effective framework that leverages the multimodal data associated with apps, namely app descriptions and screenshot images, to predict mature ratings for the apps under the guideline of the maturity rating policy of an app market (e.g., App Store). Specifically, for each app i, we have access to its textual description \\(D_i\\), and its n screenshot images \\(S_i = [S_1, S_2, ..., S_n]\\). We also have knowledge of the maturity rating policy as shown in Table 2. The policy establishes a direct correlation between the Maturity Content (MC) indicators and their corresponding maturity ratings (R), where the intensity (I) of each MC influences the level of maturity rating assigned. For each app i, there are a set of 12 MC indicators \\(MC_i = [MC_1, MC_2, ..., MC_{12}]\\), and its maturity rating \\(R_i \\in \\{R_1, R_2, R_3, R_4\\}\\) which is aligned with four age groups: 4+,9+,12+, and 17+. Our goal is to design an effective system to determine the maturity levels for apps."}, {"title": "3.2. Overview of the Proposed Framework", "content": "Building on aforementioned problem formulation, we propose a framework that utilizes multimodal large language models (MLLMs), specifically ChatGPT-4 Vision, to determine the maturity levels for apps. As shown in Figure 1, our framework takes the multimodal input including textual descriptions D and screenshots \\(S = [S_1, S_2, ..., S_n]\\) of apps, which are crucial in reflecting the app's content under the specific rating policy of an app platform, for ChatGPT-4V to access maturity levels. On the one hand, the maturity information in the descriptions could be ambiguous and obscure. On the other hand, different screenshots would contain different levels of maturity contents, and only a subset of the screenshots are useful in determining the maturity levels. Therefore, we further endow our framework with chain-of-thought (CoT) reasoning, with which the ChatGPT-4V follows a sequence of logical steps to derive maturity levels for apps. The detailed steps are summarized as follows:\nGiven the screenshots \\(S = [S_1, S_2, ..., S_n]\\) of an app, we utilize CoT reasoning to enable GPT-4V to process each screenshot sequentially to deduce the maturity content MC and its intensity levels I. Since different screenshots would contain different levels of maturity contents, this step-by-step reasoning will identify and rank the screenshots according to the exist and intensity of maturity contents in the given app according to the maturity rating policy.\nThe maturity ratings R are computed by combing the top screenshot(s) \\(S^* \\in \\{S_1, S_2, ..., s_n\\}\\) identified in step 1 and the textual descriptions D."}, {"title": "3.3. Chain-of-Thought (CoT) Endowed Prompting for App Maturity Rating", "content": "The effectiveness of using a MLLM for app maturity rating relies heavily on the design of specialized prompts that analyze both textual description D and screenshot images \\(S = [S_1, S_2, ..., S_n]\\) with CoT reasoning. As shown in Figure 2, our CoT-endowed prompting enables maturity rating reasoning through a sequence of reasoning steps, and it is composed of maturity content prompt and maturity intensity prompt to identify the maturity content and its intensity in each screenshot, and a maturity rate prompt, which combines the top screenshot(s) and the textual descriptions to determine the final maturity rating for the app.\nThe goal of maturity content prompt is to identify the maturity content indicator \\(MC = [MC_1, MC_2, ..., MC_{12}]\\) for a given screenshot \\(s \\in [S_1, S_2, ..., s_n]\\). We design maturity content prompt \\(P_{MC}\\) according to the maturity rating policy as shown in Table 2. As shown in Figure 2(a), the maturity content prompt \\(P_{MC}\\) instructs the MLLM \\(\u0192_{MLLM}\\) to output the maturity content MC as follows:\n\\(MC_s = f_{MLLM}(MC|X_s, P_{MC})\\) (1)\nwhere \\(X_s\\) is the image of screenshot \\(s \\in [S_1, S_2, ..., S_n]\\).\nAfter deriving maturity content \\(MC_s\\) for screenshots, we continue to evaluate the intensity of the identified maturity content. As shown in Figure 2(b), the maturity intensity prompt \\(P_{MI}\\) instructs the MLLM model \\(f_{mllm}\\) to output the maturity intensity I as follows:\n\\(I_s = f_{MLLM}(I|X_s, MC_s, P_{MI}).\\) (2)\nFor example, if we identified \u201cCartoon or Fantasy Violence\u201d in a screenshot, then maturity intensity prompt \\(P_{MI}\\) is designed as follows:\nPlease response the maturity level of content: Cartoon or Fantasy Violence, ranging from mild to intense in the screenshot image.\nAfter identifying the maturity content and its intensity in each screenshot \\(s \\in [S_1, S_2, ..., s_n]\\), we rank them according to \\(\\{ (MC_s, I_s) \\}_{s\\in\\{1,...,n\\}}\\) values, and obtain the top screenshot(s) \\(S^* \\in \\{S_1, S_2, ..., S_n\\}\\). Then the maturity ratings R are computed by combing the top screenshot(s) \\(S^*\\) and the textual descriptions D. As shown in Figure 2(c), the maturity rating prompt \\(P_{MR}\\) instructs the MLLM model \\(f_{MLLM}\\) to output the maturity rating R as follows:\n\\(R = f_{MLLM}(R|X_{s,s\\in S^*}, D, P_{MR}).\\) (3)\nThis prompt directs the MLLM to classify the app into one of four age-appropriate categories \\{4+,9+,12+,17+\\} based on the evaluated content. This prompt encourages the model to use its inherent knowledge to distill the extracted content themes and intensities into a clear age rating, employing a straightforward fusion strategy to assess the overall suitability for potential user groups."}, {"title": "4. Experiments", "content": "In this section, we present empirical evaluations to carefully assess our proposed method."}, {"title": "4.1. Experimental Setup", "content": "We crawled data from App Store and curated a dataset for our empirical evaluations in this study. The dataset consists of 1,281 mobile apps, categorized into four age groups: 324 apps rated 4+, 334 apps rated 9+, 394 apps rated 12+, and 229 apps rated 17+. Each app includes between 2 to 7 screenshot images along with a description. These elements serve as the multimodal inputs to the MLLM to assess the maturity level of the content. The apps were categorized across various genres to ensure diversity in content type (see Table 2), which aids in generalizing the model's applicability.\nApp maturity rating is a multi-class classification problem, where we classify each app into one of the four age-appropriate categories \\{4+,9+,12+,17+\\}. To benchmark the performance of our proposed CoT-endowed method, we compare it against several established baseline models with different large language models. These models are selected based on their prior success in similar tasks such as text classification, image classification, and multimodal classification.\n\u2022 Vicuna [29] is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations and we use Vicuna to process app textual descriptions for maturity rating.\n\u2022 GPT-3.5 and GPT-4 [30]. We respectively use GPT-3.5 and GPT-4 to input app textual descriptions to return the maturity ratings.\n\u2022 LLaVa-1.5 [31] is a large multimodal model trained by combining a vision encoder and Vicuna for general-purpose visual and language understanding. We apply LLaVa-1.5 to process the app screenshots to return the maturity ratings.\n\u2022 GPT-4V [30] is the Multimodal LLM developed by OpenAI by incorporating image modalities into large language models. We instruct GPT-4V to analyze single-modality (screenshot-only) and dual-modality (screenshot+description) to return the maturity ratings.\nSince app maturity rating is a multi-class classification problem, we employ a variety of classification metrics to comprehensively evaluate the effectiveness of our proposed method in predicting app maturity levels. These metrics include accuracy, precision, recall, and F1-score; each provides insights into different aspects of model performance. Such metrics help us gauge not only the accuracy but also the consistency and reliability of the maturity ratings."}, {"title": "4.2. Comparison Against Baseline Methods", "content": "Table 3 shows the results of our proposed CoT-endowed MLLM framework for app maturity rating against the baseline approaches. Our approach consistently outperforms the baseline methods across all metrics with significant margins. Specifically, we first observe that different LLM models would lead to different performances. For example, for methods that only use textual description for maturity rating, GPT-4 outperforms GPT-3.5 and Vicuna; for methods that only use screenshots for maturity rating, GPT-4V outperforms LLaVa-1.5 (F1-score 59.96% vs 29.38%). Second, for the same LLM platform, the text-based method (GPT-4 with F1-score of 63.94%) outperforms the image-based method (GPT-4V with F1-score of 59.96%). Third, the multimodal method outperforms unimodal methods. By combining both screenshot and description, GPT-4V achieves an F1-score of 70.38%, while the F1-score for the description-only method is 63.94% and for the screenshot-only method is 59.96%. Finally, our proposed approach achieves much more accurate maturity ratings than the best baseline that integrates multimodal data, including screenshots and descriptions (F1-score 72.00% vs 70.38%).\nThe performance improvement shows that CoT-endowed prompting is much more effective for maturity rating reasoning through a sequence of reasoning steps.\nWe further study the performance of our proposed method in predicting different maturity levels. Figure 3 shows the confusion matrix of predicted versus actual age ratings, including 4+, 9+, 12+, and 17+. Analyzing the matrix, we observe a high number of correct predictions (diagonal elements), indicating robust model accuracy across all age categories. Specifically, for the 4+ category, the framework achieved accurate predictions (precision of 94.7% and recall of 99.38%), which shows a strong ability to identify content suitable for all ages. In addition, for the 229 age 17+ apps, we observe decrease in predictions (precision of 62.73.7% and recall of 74.23%). For example, 52 (22.7%) of the age 17+ apps were predicted as 12+ app; 94 out of the 394 (23.85%) age 12+ apps were classified as age 17+ apps. We observe similar confusion for classifying age 9+ apps and age 12+ apps. We speculate the following reasons that the performance decreases in classifying age 9+ apps, age 12+ apps, and age 17+ apps. First, maturity rating is a challenging task by nature. For example, according to rating policy, \u201cInfrequent/mild Mature or suggestive content\" is categorized as age 9+ apps; meanwhile, \"Infrequent/mild Sexual content or nudity\u201d is categorized as age 12+ apps. The boundary between these two concepts is not always clear, which makes it difficult to distinguish these two concepts by looking at app images and descriptions. Second, app developers might intentionally obscure mention of maturity contents in-app images and descriptions to promote their apps to more users."}, {"title": "4.3. Comparative Analysis of Different Multimodal Fusion Strategies for App Maturity Rating", "content": "Typically, different multimodal fusion strategies would lead to different multimodal learning performances. This subsection evaluates the effectiveness of different fusion strategies in classifying app maturity, focusing on how they integrate multimodal information. We compare following multimodal fusion strategies.\nThis baseline method uses a single screenshot combined with the app description for maturity rating.\nThis method involves using GPT-4V to create a caption for a selected screenshot, which is then combined with the app's main description to predict the maturity rate. This strategy leverages the descriptive power of GPT-4V to enhance textual understanding of visual content.\nThis strategy analyzes all available screenshots along with the description, offering a comprehensive assessment by considering all visual data for a complete understanding of the app's content.\nThis approach combines textual description with the most indicative content through a selection of the top-ranked screenshot(s) for maturity rating."}, {"title": "4.4. Impact of Chain-of-Thought (CoT) Reasoning on Maturity Rating", "content": "Note that in our proposed CoT endowed prompting for app maturity rating, we first identify and rank the screenshots according to the exist and intensity of maturity contents and then combine the top screenshot(s) and the textual descriptions to determine the final maturity rating. To further evaluate the effectiveness of our proposed CoT on maturity rating, we conduct experiments on different CoT reasoning strategies. Table 5 shows the performances of maturity rating with different CoT reasoning strategies. In the \u201cText understanding + Image\u201d CoT strategy, similar to the way to prompt MLLM to understand the exist and intensity of maturity content in an image, we use the same method to comprehend app descriptions before we combine with the screenshot for final maturity rating. Accordingly, in the \u201cImage selection + Text understanding\" CoT strategy, prompt MLLM to understand both images and text, and combine the results to access maturity levels. We can see that the performance of the \u201cText understanding + Image\" CoT strategy is notably poor, which in turn affects the overall effectiveness when combined with image selection-based CoT. Specifically, the combination of CoT applied to both text and image modestly improves accuracy and precision but remains significantly lower than desired. Surprisingly, using CoT to identify and rank the screenshots and then combing text yields much better results. These findings highlight the challenges of implementing CoT in text processing within this context and suggest that while CoT enhances image analysis, its application in the text needs further refinement to avoid diminishing the benefits of multimodal fusion strategies.\nTo further investigate the importance of the proposed CoT method, we conduct an ablation study by comparing two image selection methods for app maturity rating. Table 6 shows the results of the ablation study. The \"w/ CoT Image\u201d method applies CoT reasoning to systematically select the screenshot that is most indicative of the app's content maturity. Conversely, the \"w/o CoT Image\" method randomly selects a screenshot from the remaining images that were not identified by the CoT approach. This design allows us to demonstrate the precision and effectiveness of the CoT methodology in enhancing the accuracy of maturity assessments by comparing it to a non-discriminative selection approach. Comparing the approach with and without CoT on images reveals that using CoT results in higher accuracy, precision, recall, and F1-score. This not only validates the efficacy of incorporating CoT but also emphasizes its critical role in improving the reliability and accuracy of automated content maturity assessments.\nThrough above two studies, we can see that our CoT approach leads to best results, emphasizing the need for its holistic application for accurate digital content classification."}, {"title": "4.5. Case studies", "content": "In this subsection, we present case studies from our experimental evaluations to illustrate the application of our MLLM for app maturity rating (see Table 7). These cases highlight how different fusion strategies compare against each other and the ground truth (GT) ratings. Each case study involves a specific mobile game featuring both visual and textual content, which our framework processes to predict the appropriate age rating.\nTable 7 indicates that for an app suitable for older teenagers (17+) featuring casino-related content or realistic violence, all methods-Basic Fusion (PBF), Image-Caption Fusion (PCF), Global CoT (PGloCoT), and our approach (Pselco\u0442(ours)) \u2014uniformly rated this app as 17+, which matches the ground truth (refer to cases No.1 and No.2). Case No.3 demonstrates that although the GT rates this app as suitable for ages 9+, all models predicted a rating of 12+. This discrepancy may arise because the image lacks clear information related to age-appropriate content, leading to a prediction bias in every fusion strategy employed. Case No.4 indicates that the GT recommends a 9+ rating, appropriate for children able to understand strategic game mechanics. However, our model accurately predicted this rating, while the other models overestimated the age suitability at 12+.\nThese case studies demonstrate the varied capabilities of different fusion strategies in our multimodal LLM framework. Our method shows a consistent alignment with ground truth ratings, emphasizing its robustness in accurately assessing app content through detailed analysis of both text and imagery. The discrepancies observed in other models highlight the challenges in multimodal content evaluation, especially in interpreting the subtleties of visual elements and narrative context. This comparative analysis not only validates our model's effectiveness but also illustrates the potential for further refinement in automated maturity rating systems."}, {"title": "4.5.2. Case Study 2: Comparative Analysis of CoT-based method and Non-CoT-based method", "content": "Some case studies further explore the impact of selected images on the performance of our maturity evaluation framework (see Table 8). Each case contrasts the effect of using different images from the same app on the predicted maturity ratings, examining the role of CoT in this process.\nCase 1 in Table 8 shows that the GT rating for this game is 17, likely due to its horror and violent themes. Interestingly, both with and without the use of CoT, the model rated the app as suitable for 12+. This discrepancy suggests that while the selected images were significant, they did not fully capture the maturity level expected by the GT, indicating a need for more comprehensive visual analysis or an adjustment in how images are weighted by the model. Case 2 demonstrates the effectiveness of CoT in interpreting more complex or nuanced content accurately when presented with more explicit imagery. Case 3 shows that the GT for this app is surprisingly low at 9, despite the model with CoT assigning a rating of 17, influenced perhaps by the aggressive thematic content of the selected image. The rating without CoT resulted in a more moderate 12, indicating sensitivity to image selection and the challenge of aligning model perception with community standards. Case 4 presents an interesting reversal where the model with CoT suggested a 12+ rating, while without CoT, the rating matched the GT at 9. This suggests that CoT might be emphasizing the warfare aspect more heavily, impacting the perceived suitability for younger players.\nThese case studies underline the critical role of image selection in automated content rating systems and highlight how different images can lead to varying interpretations of app content. They also emphasize the importance of implementing robust reasoning mechanisms like CoT, which can discern subtleties in content that might otherwise lead to underestimation or overestimation of maturity levels. This analysis is crucial for refining the accuracy of our model, ensuring that it not only performs well across a broad spectrum of apps but also aligns closely with established content rating standards."}, {"title": "4.6. Discussion", "content": "Multimodal large language models (MLLMs) represented by GPT-4V have shown impressive performances in different tasks such as image and text understanding. As shown in our study, applying MLLMs to a real-world application still requires careful consideration. For example, in the maturity rating task in this study, CoT reasoning can improve the performance further than the direct application of GPT-4V. Also, the effectiveness of these fusion strategies can vary based on the type of content and the specific requirements of the application. For example, the analysis of Basic Fusion, Image-Caption Fusion, and Global CoT Fusion strategies highlights the varying degrees of success and challenges, suggesting that no one-size-fits-all solution exists. This insight is crucial for developers and researchers looking to optimize content classification systems for specific user needs and content types.\nAdditionally, the exploration of more advanced multimodal learning techniques that seamlessly integrate various data types (text, image, video, etc.) could further refine the effectiveness of maturity rating systems. Such advancements could pave the way for more nuanced and context-aware AI systems capable of adapting to the ever-evolving landscape of digital content. These future directions not only promise to enhance the technical capabilities of maturity assessment models but also contribute to safer and more appropriate digital environments for diverse user groups."}, {"title": "5. Conclusion and Future Work", "content": "In this work, we presented the first systematic study on app maturity rating with multimodal large language models (MLLMs). We designed chain-of-thought (CoT) endowed prompting to instruct MLLM to better understand the images and text associated with apps step by step to determine maturity levels for apps. Our comprehensive experiments have demonstrated that the integration of CoT with multimodal data significantly improves the accuracy and reliability of maturity ratings. The CoT methodology, particularly when applied to a combination of textual and visual data, provides a deeper understanding of content, enabling more precise classifications and proving superior to traditional approaches.\nThere are several limitations in this work and some interesting future research directions. First, due to the limit on GPT-4V access, we evaluated our method on a relatively small dataset. A better model evaluation can be achieved by experiments with more data. Second, more advanced methods could be applied to improve the maturity rating performances. Finally, the implications of this study are vast, suggesting avenues for further enhancements in AI-driven content evaluation systems. The success of the Selective CoT Fusion strategy in our tests highlights the potential for targeted data processing techniques to refine AI assessments in various digital environments. As digital content continues to grow in complexity and volume, the need for advanced, nuanced, and reliable content filtering technologies becomes increasingly critical. Our research contributes to this ongoing development, laying the groundwork for future innovations that will continue to enhance safe and appropriate content consumption across digital platforms."}]}