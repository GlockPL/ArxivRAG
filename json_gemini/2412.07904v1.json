{"title": "Score Change of Variables", "authors": ["Stephen Robbins"], "abstract": "We derive a general change of variables formula for score functions, showing that for a smooth, invertible transformation $y = \\phi(x)$, the transformed score function $\\nabla_y \\log q(y)$ can be expressed directly in terms of $\\nabla_x \\log p(x)$. Using this result, we develop two applications: First, we establish a reverse-time It\u00f4 lemma for score-based diffusion models, allowing the use of $\\nabla_x \\log p_t(x)$ to reverse an SDE in the transformed space without directly learning $\\nabla_y \\log q_t(y)$. This approach enables training diffusion models in one space but sampling in another, effectively decoupling the forward and reverse processes. Second, we introduce generalized sliced score matching, extending traditional sliced score matching from linear projections to arbitrary smooth transformations. This provides greater flexibility in high-dimensional density estimation. We demonstrate these theoretical advances through applications to diffusion on the probability simplex and empirically compare our generalized score matching approach against traditional sliced score matching methods.", "sections": [{"title": "Introduction", "content": "The change of variables formula is a cornerstone of mathematical analysis, crucial for understanding how quantities transform under coordinate mappings [10, 13]. Its applications span numerous fields, from physics to probability theory. In probability theory, this formula describes how probability density functions (PDFs) relate under a differentiable invertible transformation : if a random variable X follows a density p(x), then $Y = \\phi(X)$ follows a density\n$q(y) = p(\\phi^{-1}(y)) |\\text{det } J_{\\phi^{-1}}(y)|,$\n(1)\nwhere $J_{\\phi^{-1}}$ denotes the Jacobian matrix of the inverse transformation $\\phi^{-1}$.\nRecently, the score function $\\nabla_x \\log p(x)$ has gained significant attention in machine learning, playing a pivotal role in diffusion models for generative modeling, statistical inference, and density estimation [25, 14, 26, 9]. State-of-the-art diffusion models, such as those in [7], have demonstrated remarkable success in generating high-fidelity samples by leveraging the score function. Similarly, score matching techniques [16, 26, 23] offer an efficient way to estimate the score directly from data, bypassing the need for explicit density estimation and enabling scalability to high-dimensional problems.\nDespite the increasing prominence of the score function, its transformation properties under coordinate changes remain widely unexplored. In this work, we address this limitation by deriving a change of variables formula for score functions. Our formula expresses the score function in the transformed space, $\\nabla_y \\log q(y)$, directly in terms of the score function in the original space, $\\nabla_x \\log p(x)$, and derivatives of the transformation $\\phi$. This result provides a mathematical tool for adapting score-based methods to transformed spaces, analogous to how the classical change of variables formula facilitates the transformation of probability densities.\nFurthermore, we explore the implications of our score function transformation formula in the context of diffusion models and score matching. We derive a reverse-time It\u00f4 lemma that enables efficient sampling in transformed spaces using score functions learned in the original space, effectively decoupling the forward and reverse processes. This decoupling allows for greater flexibility in designing diffusion models tailored to specific geometries or constraints. Additionally, we introduce Generalized Sliced Score Matching (GSSM), which extends the idea of linear projections in sliced score matching. Similar to how sliced score matching reduces dimensionality using a linear projection, GSSM achieves this by projecting onto a one-dimensional space defined by the gradient of a general smooth function, providing a non-linear generalization of the original concept. This generalization expands the applicability of sliced score matching, allowing for more flexible and potentially more accurate score estimation in diverse settings.\nOur main contributions are:\n1.  Score Function Change of Variables: We establish a general and explicit formula for transforming score functions under smooth invertible mappings.\n2.  Reverse-Time It\u00f4 Lemma: We develop a reverse-time It\u00f4 lemma that allows sampling in transformed spaces by leveraging score functions learned in the original space, enabling more flexible diffusion models.\n3.  Generalized Sliced Score Matching: We extend sliced score matching [23] by generalizing the linear projection in the original method to the gradient of an arbitrary smooth function from $\\mathbb{R}^n$ to $\\mathbb{R}$.\nBy providing these theoretical contributions, our work paves the way for a deeper understanding and broader application of score-based methods in various machine learning domains. The ability to seamlessly transform score functions across coordinate systems opens up new possibilities for developing more sophisticated and effective generative models and inference algorithms."}, {"title": "Main Result", "content": "We present a change of variables formula for score functions under smooth, bijective mappings, beginning with the necessary assumptions."}, {"title": "Assumptions", "content": "Assumption 1 (Bijectivity and Smoothness in $\\mathbb{R}^n$). Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, and let $X : \\Omega \\rightarrow \\mathbb{R}^n$ be a random vector with probability density function p. Assume that $\\nabla_x \\log p(x)$ is well-defined on $\\mathbb{R}^n$ and that $\\phi : U \\rightarrow U_1$, with $U, U_1 \\subset \\mathbb{R}^n$, is a bijective and twice continuously differentiable mapping. Let q(y) denote the probability density function of the transformed variable $Y = \\phi(X)$."}, {"title": "Score Change of Variables", "content": "Theorem 1 (Score Change of Variables: $\\mathbb{R}^n \\rightarrow \\mathbb{R}^n$). Suppose Assumption 1 holds. Then, the score function $\\nabla_y \\log q(y)$ can be expressed as:\n$\\nabla_y \\log q(y) = J_{\\phi^{-1}}(y) \\nabla_x \\log p(x) + \\nabla_x \\cdot (J_{\\phi^{-1}}(\\phi(x)))^{T}|_{x = \\phi^{-1}(y)}.$\nRemark 1. Throughout this paper:\n*   $J_{\\phi}$ denotes the Jacobian matrix of the transformation $\\phi$.\n*   For any matrix-valued function $A(x) : \\mathbb{R}^n \\rightarrow \\mathbb{R}^{n \\times n}$, we define its divergence row-wise:\n$\\nabla_x \\cdot A(x) := [\\nabla_x \\cdot A_i(x)]_{i=1}^{n} \\in \\mathbb{R}^n,$\nwhere $A_i$ is the i-th row of A.\nProof. Starting from the change of variables formula for densities, $q(y) = p(\\phi^{-1}(y))| \\text{det}(J_{\\phi^{-1}}(y))|$, we take the gradient of the log-density and apply the chain rule and the logarithmic derivative of the determinant. Detailed calculations are in appendix A."}, {"title": "Special Cases", "content": "The multidimensional result also covers important special cases, presented as corollaries.\nCorollary 1 (Score Change of Variables: $\\mathbb{R} \\rightarrow \\mathbb{R}$). Let $X : \\Omega \\rightarrow \\mathbb{R}$ be a random variable with probability density function p. Assume that $\\nabla_x \\log p(x)$ is well-defined on $\\mathbb{R}$ and that $\\phi : U \\rightarrow U_1$, with $U, U_1 \\subset \\mathbb{R}$, is a bijective and twice continuously differentiable mapping. Let q(y) denote the probability density function of the transformed variable $Y = \\phi(X)$. Then:\n$\\nabla_y \\log q(y) = (\\phi^{-1})'(y)\\nabla_x \\log p(\\phi^{-1}(y)) + \\frac{(\\phi^{-1})''(y)}{(\\phi^{-1})'(y)},$\nor, equivalently,\n$\\nabla_y \\log q(y) = \\frac{1}{(\\phi'(x))^2} (\\phi'(x) \\nabla_x \\log p(x) - \\phi''(x))|_{x=\\phi^{-1}(y)}.$\nCorollary 2 (Score Change of Variables: $\\mathbb{R}^n \\rightarrow \\mathbb{R}$). Let $(\\Omega, \\mathcal{F}, \\mathbb{P})$ be a probability space, and let $X : \\Omega \\rightarrow \\mathbb{R}^n$ have probability density function p. Let $v : U \\rightarrow U_1$, where $U \\subset \\mathbb{R}^n$ and $U_1 \\subset \\mathbb{R}$, be a twice continuously differentiable mapping. Assume that $|\\|\\nabla v(x)|\\|^2 \\neq 0$ for all $x \\in U$. Let q(y) be the density of $Y = v(X) \\in \\mathbb{R}$. Then:\n$\\nabla_y \\log q(y) = \\frac{1}{|\\|\\nabla v(x)|\\|^2} (\\nabla v(x) \\nabla_x \\log p(x) - \\Delta v(x) - \\sum_{i=1}^{n} \\frac{\\partial v}{\\partial x_i} \\frac{\\partial}{\\partial y} \\log p((x_j)_{j\\neq i} | y)).$\nProof. The proof constructs a sequence of transformations involving v, applies Theorem 1 to each step, and then combines the results. See Appendix A for the full derivation."}, {"title": "Applications", "content": "We present two applications of the score change of variables: a reverse-time It\u00f4 lemma for transformed diffusion processes and a generalized sliced score matching method (Section 3.2). Both applications leverage the score change of variables formula for more flexible training and sampling."}, {"title": "Reverse-Time It\u00f4 Lemma", "content": "Score-based diffusion models [7, 15, 21] generate samples by reversing a forward SDE that progressively adds noise to the data. This reversal relies on the score function of the marginal distributions, $\\nabla_x \\log p_t(x)$. Given a forward SDE:\n$dX = f(X, t) dt + G(X, t) dW(t),$\n(2)\nwhere $W(t)$ is a standard Wiener process, the corresponding reverse-time SDE [1, 25] is:\n$dX = [f(X, t) - G(X, t)G(X, t)^T \\nabla_x \\log p_t(X) - \\nabla_x \\cdot (G(X, t)G(X, t)^T)] dt + G(X, t) d\\bar{W}(t),$\n(3)\n$\\bar{W}(t)$ is a Wiener process in reverse time. This reverse process allows us to recover the original data distribution by starting from noise.\nIn many scenarios, we prefer to perform this generative diffusion process in a transformed space, for instance when data lies on a manifold [18, 6]. Consider a bijective, twice continuously differentiable transformation $\\phi : \\mathbb{R}^n \\times [0, T] \\rightarrow \\mathbb{R}^n$. The transformed process $Y(t) = \\phi(X(t), t)$ for $t \\in [0, T]$ satisfies a new SDE [17]:\n$dY = \\tilde{f}(Y, t) dt + \\tilde{G}(Y, t) dW(t),$\nwhere the transformed drift $\\tilde{f}$ and diffusion $\\tilde{G}$ are obtained via It\u00f4's Lemma:\n$\\tilde{f}(y, t) = (\\frac{\\partial \\phi(x, t)}{\\partial t} + J_{\\phi}(x, t)f(x, t) + \\frac{1}{2} \\text{Tr} [G(x, t)H(x, t)G(x, t)^T])|_{x = \\phi^{-1}(y, t)},$\n(5)\n$\\tilde{G}(y, t) = J_{\\phi}(x, t)G(x, t)|_{x = \\phi^{-1}(y, t)},$"}, {"title": "Transformed Score Matching", "content": "While the previous section applied Theorem 1 to continuous-time processes, we now generalize sliced score matching using Corollary 2. This Generalized Sliced Score Matching (GSSM) framework offers greater flexibility in data projection while retaining computational efficiency. We first briefly review score matching and sliced score matching, and then introduce GSSM."}, {"title": "Background: Score Matching", "content": "Score matching [16] estimates the score function $\\nabla_x \\log p_{\\theta}(x)$ of a probability density $p_{\\theta}(x)$ from samples $x \\sim p_{\\theta}$, without requiring an explicit density form. Consider the loss for a score model $s_{\\theta}(x)$:\n$\\mathcal{L}_{SM}(s_{\\theta}) = \\frac{1}{2} \\mathbb{E}_{p_{\\theta}} [||s_{\\theta}(X) - \\nabla_x \\log p_{\\theta}(X)||^2].$\n(8)\nSince $\\nabla_x \\log p_{\\theta}(x)$ is unknown, we cannot compute this directly. Using integration by parts and suitable boundary conditions, the loss can be rewritten without directly involving $\\nabla_x \\log p_{\\theta}(x)$:\n$\\mathcal{L}_{SM}(s_{\\theta}) = \\frac{1}{2} \\mathbb{E}_{p_{\\theta}} [||s_{\\theta}(X)||^2] + \\mathbb{E}_{p_{\\theta}} [\\nabla_x \\cdot s_{\\theta}(X)].$\n(9)"}, {"title": "Sliced Score Matching", "content": "In high-dimensional settings, evaluating $\\mathbb{E}_{p_{\\theta}} [\\nabla_x \\cdot s_{\\theta}(X)]$ can be prohibitively expensive. Sliced score matching [23] addresses this by projecting onto random one-dimensional subspaces. Specifically, it uses random vectors $v \\in \\mathbb{R}^n$ drawn from a distribution $p_v$, where $\\mathbb{E}_{p_v} [||v||^2] < \\infty$ and $\\mathbb{E}_{p_v} [vv^T] > 0$. Under these conditions, the loss becomes:\n$\\mathcal{L}_{SSM}(s_{\\theta}) = \\frac{1}{2} \\mathbb{E}_{p_{\\theta}} \\mathbb{E}_{p_v} [(v^T s_{\\theta}(X))^2] + \\mathbb{E}_{p_{\\theta}} \\mathbb{E}_{p_v} [v^T \\nabla_x (v^T s_{\\theta}(X))].$\n(10)\nThis objective is equivalent to the original score matching loss up to a constant, yet is more tractable computationally in high dimensions."}, {"title": "Generalized Sliced Score Matching", "content": "We now introduce Generalized Sliced Score Matching (GSSM), which extends sliced score matching to use arbitrary smooth transformations. Let $v : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ be a twice continuously differentiable random function drawn from an independent distribution $p_v$. Using Corollary 2, we derive:\n$\\mathcal{L}_{GSSM}(s_{\\theta}) = \\frac{1}{2} \\mathbb{E}_{p_{\\theta}} \\mathbb{E}_{p_v} [((\\nabla_x v(X))^T s_{\\theta}(X))^2] + \\mathbb{E}_{p_{\\theta}} \\mathbb{E}_{p_v} [(\\nabla_x v(X))^T \\nabla_x ((\\nabla_x v(X))^T s_{\\theta}(X))]\\\\\n+ \\mathbb{E}_{p_{\\theta}} \\mathbb{E}_{p_v} [s_{\\theta}(X)^T H_v(X) \\nabla_x v(X)] + \\mathbb{E}_{p_{\\theta}} \\mathbb{E}_{p_v} [(\\nabla_x v(X))^T s_{\\theta}(X) \\Delta v(X)].$\n(11)\nThe extra terms compared to SSM account for the nonlinearity of v via its Hessian $H_v$ and Laplacian $\\Delta v$.\nWhen $v(x) = v^T x$ is linear, we have $\\nabla_x v(x) = v$, $H_v = 0$, and $\\Delta v = 0$, reducing GSSM to the original sliced score matching loss (10). Thus, GSSM strictly generalizes sliced score matching, providing more flexibility and potentially improved performance in complex, high-dimensional scenarios."}, {"title": "Derivation Outline", "content": "To derive GSSM, we first apply standard score matching in the transformed space $y = v(x)$. Then, using Corollary 2, we substitute the transformed score back into the original space. Integration by parts under appropriate conditions (see Appendix B) yields Equation 11. The complete derivation is in Appendix B."}, {"title": "Examples", "content": ""}, {"title": "Diffusion on the Probability Simplex for Chess Positions", "content": "We demonstrate our reverse-time It\u00f4 lemma by generating chess positions through diffusion on a probability simplex. Prior work has explored diffusion on the simplex by training directly in a constrained space [12], but our approach enables training in an unconstrained space with a Gaussian score model while still sampling in the simplex."}, {"title": "Representing Chess Positions", "content": "A chess position can be represented as a point in a 13-dimensional probability simplex. We focus on the projected simplex defined as:\n$\\left\\{y \\in [0, 1]^{12} | \\sum_{i=1}^{12} y_i \\leq 1\\right\\}.$\nHere, each element y specifies probabilities for 12 possible piece types (6 for White and 6 for Black) that could occupy a single square on the chessboard. The probability of the square being empty is implicitly given by $1 - \\sum_{i=1}^{12} y_i$. Extending this construction to all 64 squares, we represent a full chess position as a collection of such probability vectors across the board."}, {"title": "Transformation Between Spaces", "content": "We employ the additive logistic transformation [3] and its inverse to map between $\\mathbb{R}^{12}$ and the projected simplex. For $i = 1, ..., 12$:\n$\\phi_i(x) = \\frac{e^{x_i}}{1 + \\sum_{j=1}^{12} e^{x_j}},$\n(12)\n$\\psi_i(y) = \\log(\\frac{y_i}{1 - \\sum_{j=1}^{12} y_j}).$\n(13)\nThis mapping allows us to freely train a score-based model in an unconstrained Euclidean space, then transform and sample in the simplex domain where the probabilities must sum to at most one."}, {"title": "Training and Sampling", "content": "We train a score-based model $s_{\\theta}(x, t)$ in $\\mathbb{R}^{12}$. Chess positions are represented with a softened one-hot encoding to avoid infinite values when applying the inverse transformation $\\psi(\\cdot)$. The score model is trained using a Variance Preserving (VP) SDE [25]:\n$dX = -\\frac{1}{2} \\beta(t) X dt + \\sqrt{\\beta(t)} dW,$\n(14)\nwhere $\\beta(t)$ is the noise schedule. During inference, we apply our reverse-time It\u00f4 lemma to perform diffusion directly in the projected simplex. By leveraging the learned scores in the unconstrained space and the known transformation $\\phi$, we obtain the appropriate transformed SDE coefficients (derived in Appendix E) and sample valid chess positions on the simplex."}, {"title": "Controlling Piece Density", "content": "The transformed drift term, $\\tilde{f}(Y, t)$, interacts with the simplex geometry to influence piece distribu- tions. To gain intuitive control, we introduce a scaling factor $\\omega$:\n$\\tilde{f}(Y, t) = \\omega \\tilde{f}(Y, t)$.\n(15)\nIncreasing $\\omega$ biases the distribution toward fewer pieces (increasing empty squares), while decreasing it encourages more pieces. This simple scalar parameter provides interpretable geometric control over the final configurations, allowing practitioners to shape the sampling process easily. The generated positions in Figure 5 demonstrate how varying $\\omega$ changes piece density without losing consistency in the underlying chess position representation."}, {"title": "Density Estimation with Generalized Sliced Score Matching", "content": "We evaluate Generalized Sliced Score Matching (GSSM) on density estimation tasks using deep kernel exponential families (DKEFs), comparing its performance against traditional score matching (SM) and sliced score matching (SSM) [23]. We also introduce and evaluate a variance-reduced version of GSSM (GSSM-VR)."}, {"title": "Choice of Transformation", "content": "For GSSM, we use a quadratic form as our nonlinear transformation:\n$v(x) = \\frac{1}{2} x^T A x + b^T x,$\n(16)\nwhere A is a random symmetric matrix and b is a random vector. The entries of A and b are drawn from zero-mean distributions with specified variances: $\\sigma_1^2$ for diagonal entries of A, $\\sigma_2^2$ for off-diagonal entries, and $\\sigma_b^2$ for entries of b. The gradient of this transformation, which determines the direction of projection in GSSM, is:\n$\\nabla_x v(x) = Ax + b.$\n(17)"}, {"title": "Variance Reduction", "content": "Depending on the chosen transformation, we may reduce the variance of our estimator by analytically integrating out some randomness. For linear transformations (as in standard sliced score matching), integrating out the random directions yields the known variance-reduced SSM objective [23]:\n$\\mathcal{L}_{SSM-VR}(s_{\\theta}) = \\mathbb{E}_{p_{\\theta}} [\\frac{1}{n} ||s_{\\theta}(X)||^2] + \\mathbb{E}_{p_v} [v^T \\nabla_x s_{\\theta}(X) v]].$\n(18)\nSimilarly, for the quadratic transformation, integrating out some of the randomness from A and b leads to a variance-reduced GSSM (GSSM-VR) objective:\n$\\mathcal{L}_{GSSM-VR}(s_{\\theta}) = \\frac{1}{2} \\mathbb{E}_{p_{\\theta}}[(\\sigma_1^2 - 2\\sigma_2^2) \\sum_i s_{\\theta i}(X)^2 x_i^2 + \\sigma_2^2 (||X||^2 ||s_{\\theta}(X)||^2 + (s_{\\theta}(X)^T X)^2) + \\sigma_b^2 ||s_{\\theta}(X)||^2] \\\\\n+ \\mathbb{E}_{p_{\\theta}}[(2\\sigma_1^2 + (n - 1)\\sigma_2^2) s_{\\theta}(X) X] \\\\\n+ \\mathbb{E}_{p_{\\theta}} \\mathbb{E}_{A, b} [(AX + b)^T \\nabla_x s_{\\theta}(X) (AX + b)].$\n(19)\nThe full derivation for the quadratic case is provided in Appendix C."}, {"title": "Future Directions", "content": "Several promising directions can further advance this line of work.\nFirst, while we focused on analytically tractable mappings, exploring more expressive transformations, such as invertible neural networks, could offer greater flexibility and adaptability to complex data geometries. This data-driven approach could lead to significant performance improvements, but also increase computational costs.\nSecond, deeper theoretical study is needed to understand the interplay between transformations and diffusion processes. Such insights could guide the selection of optimal transformations, challenging the prevailing assumption that the same space should be used for both training and sampling. This may lead to improved sampling efficiency and better-tailored geometries for specific applications.\nThird, extending the score change of variables framework to other score-based methods, including variational inference [19] and energy-based models [24], presents a compelling direction for future research and could unlock new capabilities within these frameworks.\nIn summary, the theoretical foundations established here open the door to more versatile and effective score-based methodologies, offering opportunities for improved performance, stability, and interpretability across a range of applications."}]}