{"title": "Adapting Unsigned Graph Neural Networks for Signed Graphs: A Few-Shot Prompt Tuning Approach", "authors": ["Zian Zhai", "Sima Qing", "Xiaoyang Wang", "Wenjie Zhang"], "abstract": "Signed Graph Neural Networks (SGNNs) are powerful tools for signed graph representation learning but struggle with limited generalization and heavy dependence on labeled data. While recent advancements in \"graph pre-training and prompt tuning\" have reduced label dependence in Graph Neural Networks (GNNs) and improved their generalization abilities by leveraging pre-training knowledge, these efforts have focused exclusively on unsigned graphs. The scarcity of publicly available signed graph datasets makes it essential to transfer knowledge from unsigned graphs to signed graph tasks. However, this transfer introduces significant challenges due to the graph-level and task-level divergences between the pre-training and downstream phases. To address these challenges, we propose Signed Graph Prompt Tuning (SGPT) in this paper. Specifically, SGPT employs a graph template and a semantic prompt to segregate mixed link semantics in the signed graph and then adaptively integrate the distinctive semantic information according to the needs of downstream tasks, thereby unifying the pre-training and downstream graphs. Additionally, SGPT utilizes a task template and a feature prompt to reformulate the downstream signed graph tasks, aligning them with pre-training tasks to ensure a unified optimization objective and consistent feature space across tasks. Finally, extensive experiments are conducted on popular signed graph datasets, demonstrating the superiority of SGPT over state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "In many real-world scenarios, interactions between entities are inherently complex, often involving opposing relationships, such as friendships and enmities, trust and distrust, or agreement and disagreement (Liu et al. 2021; Sun et al. 2022b). Such dynamics are often modeled as signed graphs, where links carry both positive and negative signs to represent these opposite connections. To learn the effective representations of signed graphs, Signed Graph Neural Networks (SGNNs) have attracted increasing research interest (Derr, Ma, and Tang 2018; Shu et al. 2021; Huang et al. 2021). SGNNs incorporate link sign signals into the message-passing process, updating node features by independently aggregating information from neighbors with different semantics. This approach yields expressive representations for a variety of signed graph tasks, such as node classification (Mercado, Bosch, and Stoll 2020) and link sign prediction (Huang et al. 2019, 2021; Li et al. 2023). Although SGNNs have achieved remarkable success, they suffer from limitations of label dependence and overfitting problems. On the one hand, SGNNs are trained in a supervised manner, which requires a large amount of task-specific labels for supervision. These labels, such as node and link sign labels, are typically expensive to obtain in real-world applications (Tang et al. 2016; Tang, Aggarwal, and Liu 2016; Zhang et al. 2024). On the other hand, compared to unsigned graphs, signed graph datasets exhibit exceptionally sparser structures and more noises (Shu et al. 2021; Zhang et al. 2023). This makes SGNNs trained on signed graphs more prone to memorizing these non-universal patterns, leading to overfitting problems.\nInspired by the success of \u201cpre-training and prompt tuning\" in Large Language Models (LLMs), recent research has applied such a paradigm to graph tasks to overcome label reliance and overfitting problems in conventional Graph Neural Networks (GNNs) (Sun et al. 2022a; Fang et al. 2024). In the pre-training phase, GNNs are trained to learn task-agnostic priors from the graph structure in an unsupervised manner (Hu et al. 2020; You et al. 2021). In the prompt tuning phase, the downstream tasks are reformulated into the same form as the pre-training task, maximizing the utilization of prior knowledge. Meanwhile, with the pre-trained model frozen, several lightweight prompt vectors are introduced and tuned with task-specific labels. Compared to supervised training, this paradigm allows GNNs to learn transferable knowledge, enhancing their generalization abilities. Moreover, with fewer tunable parameters, it can adapt the pre-trained model to different downstream tasks when the task-specific labels are limited (Sun et al. 2023; Liu et al. 2023). However, current research only focuses on unsigned graphs, overlooking the scenarios where downstream tasks involve signed graphs. Such oversight is significant, as signed graphs are typically limited, with far fewer publicly available datasets (Zhang et al. 2023), making it challenging to gather sufficient data to directly train a generalized SGNN. In contrast, unsigned graphs are more abundant and easier to access, enabling effective pre-training of generalized GNNs (Sun et al. 2023; Yu et al. 2024a). This makes transferring general graph knowledge to address signed graph tasks a promising approach.\nHence, this study focuses on prompt tuning pre-trained GNNs for few-shot signed graph tasks, namely node classification and link sign prediction. This problem is non-trivial due to two types of divergence between the pre-training and downstream tasks.\n\u2022 Graph-level divergence. The first challenge arises from the divergence between the graph types used in pre-training and those in the downstream stage, as downstream signed graphs carry more complex semantic information. In unsigned graphs, links consistently represent the connectivity between nodes (Hu et al. 2020). GNNs pre-trained on unsigned graphs learn node representation based on the social theory homophily (McPherson, Smith-Lovin, and Cook 2001). In other words, they aggregate features from connected neighbors uniformly. However, downstream tasks involve both positive and negative link signs. The negative links contain the opposite semantics to the positive links, violating the key assumptions of homophily in GNNs. Therefore, dedicated efforts are needed to narrow down such graph-type divergence when applying the pre-trained GNNs.\n\u2022 Task-level divergence. The uniqueness of signed graph tasks presents the second challenge: how to transfer prior knowledge to unseen tasks that are specific to signed graphs. The pre-training tasks on unsigned graphs aim to preserve the instinct graph structures such as graph similarities and node connectivity (Zhang and Chen 2018; You et al. 2021). These tasks primarily focus on node features and structural information. Comparatively, downstream signed graph tasks aim to optimize the task-specific loss function, i.e., fit the ground truth of node labels or link sign labels (Derr, Ma, and Tang 2018; Huang et al. 2019). They require the model to handle not only the node features but also the semantic complexity of link signs in graphs. Such inconsistent tasks between the pre-training and downstream phases may produce the suboptimal performance of pre-trained GNNs, even \"negative transfer\".\nTo address these challenges, we propose Signed Graph Prompt Tuning (SGPT), a novel transfer learning framework to adapt the pre-trained unsigned GNNs to signed graph tasks. This approach employs two unification templates (graph and task templates) and two prompts (feature and semantic prompts) to bridge the gaps between the pre-training and downstream phases. Specifically, to reduce the graph-level divergence, we propose a graph template to address the complex node relationships in signed graphs. Based on the balance theory, the graph template generates samples from the original signed graph: positive, negative, and topological, each capturing distinct relational semantics. These segregated samples are then fed into the pre-trained GNN in parallel, ensuring each input graph contains only a type of single link semantics, thereby aligning with the homophily assumption in the pre-trained GNN. We then use a semantic prompt to adaptively integrate the embeddings of these graph samples according to the specific requirements of downstream tasks, generating more effective representations. To narrow down the task-level divergence, we apply a task template. It unifies the pre-training and downstream tasks by reformulating the latter to the same form as the pre-training task, ensuring a consistent optimization objective across both. Additionally, we introduce a feature prompt to modify the downstream input space to better align with that of the pre-training tasks, reducing the feature variance across the tasks.\nTo the best of our knowledge, we are the first to study the problem of prompting pre-trained unsigned GNNs towards few-shot signed graph tasks. The contributions of this paper are summarized as follows: (1) We propose SGPT, a prompt tuning framework that adapts pre-trained unsigned GNNs to signed graphs with limited task-specific labels. (2) We design a graph template and a semantic prompt to unify the pre-training and downstream graphs and effectively integrate complex semantic information tailored to the downstream tasks. (3) We propose a task template and a feature prompt to unify forms of both tasks and align their input feature space. (4) We conduct extensive experiments on popular signed graph datasets, demonstrating the effectiveness of our framework."}, {"title": "2 Related Work", "content": "Graph pre-training. The recent success of LLMs inspired researchers to apply pre-training methods to GNNs to enhance their generalization capabilities. Essentially, graph pre-training leverages easily accessible graph structure data to train a GNN in a self-supervised manner, e.g., link prediction (Hu et al. 2020), node attribute reconstruction (Grover and Leskovec 2016), and graph pattern discrimination (You et al. 2021). Pre-trained GNNs learn universal, transferable patterns and serve as an effective initialization for different downstream tasks. They are subsequently fine-tuned with the specific downstream objectives. For signed graphs, although there are studies that utilize contrastive learning to train SGNNs, e.g., SGCL (Shu et al. 2021) and SBGCL (Zhang et al. 2023), they essentially rely on the link sign labels for graph augmentation and supervision. As a result, they still suffer from the reliance on link sign labels.\nGraph prompt tuning. In LLMs, prompt tuning aims to bridge the gap between pre-training and downstream task objectives. This is achieved by introducing lightweight tokens into the inputs of downstream tasks and reformulating these tasks to align with the pre-training ones, while keeping the pre-trained model frozen (Sun et al. 2023). This approach maximizes the utilization of pre-training knowledge, thereby reducing the supervision requirements in the downstream phase. Recent research in graph analysis has adopted this method to relieve the label reliance in supervised GNNs. GPPT (Sun et al. 2022a) transforms the downstream tasks to link prediction, but its design is limited to node classification tasks. GraphPrompt (Liu et al. 2023) employs the subgraph similarity as the pre-training task and extends prompt designs to graph classification tasks. ProG (Sun et al. 2023) reformulates different downstream problems to the graph-level tasks and learns lightweight prompt tokens via meta-learning. However, none of these studies address scenarios where downstream tasks involve signed graphs."}, {"title": "3 Preliminaries", "content": "A signed graph is denoted as $G = (\\mathcal{V},\\mathcal{E}^+,\\mathcal{E}^-)$, where $\\mathcal{V} = \\{v_1, v_2, ..., v_n\\}$ denotes a set of n nodes in G. $\\mathcal{E}^+$ and $\\mathcal{E}^-$ represent sets of positive and negative links respectively, such that $\\mathcal{E}^+ \\cap \\mathcal{E}^- = \\emptyset$. We use $A^+ \\in \\mathbb{R}^{n \\times n}$ and $A^- \\in \\mathbb{R}^{n \\times n}$ to represent the positive and negative binary (0/1) adjacency matrices. $X \\in \\mathbb{R}^{n \\times d_{in}}$ is the feature matrix, where $d_{in}$ is the feature dimension. Similarly, an unsigned graph disregards link signs, which can be represented as $G = (\\mathcal{V}, \\mathcal{E})$ with node feature matrix $X \\in \\mathbb{R}^{n \\times d_{in}}$. A binary adjacency matrix $A$ can be constructed where $a_{ij} = 1$ if node $v_i$ connects to node $v_j$, otherwise $a_{ij} = 0$.\nProblem statement. In this study, we focus on adapting the pre-trained unsigned GNNs to two widely studied signed graph tasks, namely Node Classification (NC) and Link Sign Prediction (LSP). In particular, we consider few-shot settings due to the scarcity of labels in real-world scenarios.\n\u2022 Few-shot Node Classification. Let $C = \\{C_1, C_2, ..., C_m \\}$ denote the set of node classes. Each node in the signed graph G is associated with a label $c \\in C$. In the few-shot setting, the training set includes only a small number of nodes per class, significantly fewer than the total number of nodes. Our goal is to predict the unknown node labels in the test set given that all link signs are known.\n\u2022 Few-shot Link Sign Prediction. Let $C = \\{P, N\\}$ denote the positive and negative link signs. Each linked node pair in the signed graph G has a link sign $c \\in C$. We mask a portion of the link signs in G and only have a small number of link instances for both positive and negative classes in the training set, which is significantly fewer than the total number of linked node pairs. In the test set, we already know node pairs are connected, and our task is to predict signs of these known linked node pairs."}, {"title": "Graph Neural Networks", "content": "Graph Neural Networks (GNNs) learn the node representation by recursively aggregating features from its neighbors, as known as message-passing (Sun et al. 2022a). Formally, the representation of node v at the l-th layer is:\n$h_v^{(l)} = AGG(\\{h_u^{(l-1)}, u \\in \\mathcal{N}(v) \\cup v\\}, \\Theta^{(l)})$, (1)\nwhere $h_v^{(0)}$ is give by the initial node feature $x$; $\\mathcal{N}(v)$ is the neighbor set of node $v$; and $\\Theta^{(l)}$ is the parameters of the l-th layer of the GNN. The aggregation function AGG(\u00b7) combines the embedding of node v and its neighbors, which is typically sum, mean, or max pooling."}, {"title": "4 Proposed Method", "content": "4.1 Overall Framework\nWe first introduce the overall framework of SGPT, as shown in Fig. 1. During the pre-training stage, we implement Link Prediction (LP) on unsigned graphs to pre-train an unsigned GNN in Fig. 1(a). During the downstream stage in Fig. 1(b), we first design two unification templates to unify the pre-training and downstream phases. First, a graph template is proposed to unify the graph types across both tasks. It converts the downstream signed graph to multiple graph samples, each with consistent internal link semantics: positive, negative, and topological, thereby satisfying the homophily assumption in the pre-trained GNN. These segregated samples are then fed into the pre-trained GNN in parallel to generate their corresponding representations. Second, a task template is introduced to unify the pre-training and downstream task forms. It reformulates link sign prediction and node classification into the same form as link prediction, ensuring a consistent optimization objective. With unified graph types and task forms, we further introduce two prompts to adapt the pre-trained model, which reduces the feature variance across tasks and integrates the complex semantic information respectively. The feature prompt is added to the input node features before the encoding process to align the downstream feature space with that of the pre-training task. The semantic prompt is applied after the GNN encoding to adaptively integrate the embeddings of graph samples generated by the graph template, outputting the representations tailored to the needs of downstream tasks.\n4.2 Graph Pre-training\nIn this paper, we employ Link Prediction (LP) as the pre-training task (Hu et al. 2020). Let $G_{pre}$ denote the pre-training graph with node feature matrix $X \\in \\mathbb{R}^{n \\times d_{in}}$ and the adjacency matrix $A_{pre}$, where a portion of links are masked as the training set, and the corresponding elements in the adjacency matrix are set to $a_{ij} = 0$. The node embedding matrix can be generated by the GNN based on the unmasked links, denoted as $H = f_{pre}(G_{pre}, X)$, where $H \\in \\mathbb{R}^{n \\times d_{out}}$, and $d_{out}$ is the dimension of the output embeddings.\nGiven a node $v$ in the pre-training graph $G_{pre}$, we randomly sample one positive node $a$ from $v$'s neighbors, and a negative node $b$ from the graph which does not link to $v$, forming the triplet $(v, a, b)$ in the training set $D_{pre}$. The objective of LP is to predict higher similarities to $v$ and $a$ while giving lower similarities to $v$ and $b$. The GNN is optimized to minimize the loss function $L_{pre} =$\n$\\sum_{(v,a,b) \\in D_{pre}} -ln \\frac{exp(Sim(h_v, h_a))}{\\sum_{u \\in \\{a,b\\}} exp(Sim(h_v, h_u))}$, (2)\nwhere $Sim(h_v, h_u)$ denotes the similarity of node embeddings, which can be calculated using various similarity measures, such as cosine similarity.\n4.3 Template Design\nGraph Template. Negative links in signed graphs violate the homophily assumption in pre-trained GNNs and introduce more complex node relationships. To address the graph-level divergence, we start from the inherent semantics of links and propose a graph template. Based on balance theory (Heider 1946), it explicitly extracts node relationships at different hops to generate multiple graph samples, each maintaining consistent internal link semantics: positive, negative, and topological, segregating the mixed relationships in the original signed graphs.\nAccording to balance theory, a balanced path is defined as one where the number of negative links is even, while an unbalanced path has an odd number of negative links. If a source node and a target node have a balanced (unbalanced) path, they will have an explicit or implicit positive (negative) link (Derr, Ma, and Tang 2018). Formally, given the downstream signed graph G with positive and negative adjacency matrix $A^+$ and $A^-$, the 1-hop positive and negative graph samples can be generated by selecting nodes' positive and negative neighbors respectively. Their adjacent matrix can be denoted as:\n$A_1^+ = A^+, A_1^- = A^-$. (3)\nFurthermore, we consider higher hops to capture global information. For k-hop (k \u2265 2) graph samples, adding a negative link to a balanced path makes it unbalanced, while an unbalanced path with a negative link becomes balanced. Conversely, adding a positive link preserves the path's balance status Thus, the adjacency matrices of k-hop positive (negative) graph sample are calculated by:\n$A_k^+ = I(A_1^{k-1}A_1^+ + A_1^{-1}A_1^-)$, (4)\n$A_k^- = I(A_1^+A_1^{k-1} + A_1^{-1}A_1^+)$, (5)\nwhere I(\u00b7) denotes setting to non-zero elements to 1. In this way, we explicitly extract the connections between nodes and their 1 to k hop neighbors and generate k positive and k negative graph samples where links represent only positive and negative relationships respectively.\nFor simplicity, we refer to the set of positive samples as the positive channel and the set of negative samples as the negative channel, which is denoted as:\n$G_P = \\{G_1^+, \\dots, G_k^+: G_i^+ = (X, A_i^+) \\text{ and } 1 \\leq i \\leq k\\}$, (6)\n$G_N = \\{G_1^-, \\dots, G_k^-: G_i^- = (X, A_i^-) \\text{ and } 1 \\leq i \\leq k\\}$. (7)\nAdditionally, to preserve the original graph topology, we mask the link signs in the downstream signed graph and convert it to an unsigned sample in the topological channel. In this channel, links represent only the connection between nodes, which is denoted as:\n$G_T = (X, A^+ \\cup A^-)$. (8)\nOverall, the graph template segregates the downstream signed graph into multiple samples across three channels. Each channel represents a consistent link semantics: positive, negative, and topological, denoted as $S = \\{P, N, T\\}$. This process is represented as $G_T(G) = \\{G_P, G_N, G_T\\}$.\nTask Template. The task template converts the downstream link sign prediction and node classification tasks to the same form used in the pre-training link prediction.\n\u2022 Link Prediction (LP): Given an pre-training unsigned graph and a node triplet (v, a, b), there exists a link between v and a, whereas v and b are not connected. With the optimization objective in Equation 2, the model is expected to predict higher similarities for linked node pairs compared to unlinked ones, which is denoted as:\n$Sim(h_v, h_a) > Sim(h_v, h_v)$. (9)\n\u2022 Link Sign Prediction (LSP): Given a downstream signed graph with two link sign classes $C = \\{P, N\\}$, we introduce two prototypes into the link embedding space for each link label class. Their embeddings are defined as $E = [e_P, e_N] \\in \\mathbb{R}^{2 \\times 2d_{out}}$. Here, we use the concatenation of the embeddings of node v and node u in the queried link as the link embedding (Derr, Ma, and Tang 2018; Shu et al. 2021), which is denoted as $e_{(v,u)} = [h_v||h_u]$. Then, the model predicts the link sign by selecting the prototype that has the highest similarity to the queried link:\n$C_{(v,u)} = \\arg \\max_{c \\in \\{P, N\\}} Sim(e_{(v,u)}, e_c)$. (10)\n\u2022 Node Classification (NC): Given a downstream signed graph with m node classes $C = \\{C_1, ..., C_m\\}$, we adopt m class prototypes in the graph. Their embeddings are defined as $E = [e_{c_1},...,e_{c_m}] \\in \\mathbb{R}^{m \\times d_{out}}$. Subsequently, a node can be classified by selecting the class prototype with the highest similarity:\n$C = \\arg \\max_{C_i \\in C} Sim(h_v, e_{c_i})$. (11)\n4.4 Prompt Design\nFeature Prompt. With the task template, we have unified the forms of pre-training and downstream tasks. However, due to the inherently different natures of these tasks, they may emphasize different aspects of input features (Yu et al. 2024a). To bridge the gap, we design a feature prompt that involves adding token vectors to modify the input node features before the GNN encoding process, thereby aligning the downstream input space with that of the pre-training.\nFor the downstream signed graph, we assume that we obtain graph samples of three channels via the graph template $G_T(G)$. Based on the observation that the feature aggregation process in GNNs may emphasize distinct feature facets for each type of link semantics (Shu et al. 2021; Huang et al. 2021), we tailored a feature prompt for each type of link semantics. Formally, for each type of link semantics $s \\in S$, we employ a feature prompt $P_s$ consisting of r independent basis vectors. Each basis vector has the same dimension of $d_{in}$ as the input node features. The feature prompt P is denoted as:\n$P = \\{P_s | s \\in S\\}, P_s = \\{p_1^s, ..., p_r^s\\}$. (12)\nFor each node v in the graph sample G, we add the attentive aggregation of the prompt basis in $P_s$ to its input feature x. Then the prompted feature $x_v^s$ can be obtained by:\n$x_v^s = x + \\sum_{j=1}^{r} \\alpha_j^s p_j^s$, (13)\n$\\alpha_j^s = \\frac{exp((q_j^s)^T x_v)}{\\sum_{j=1}^{r} exp((q_j^s)^T x_v)}$, (14)\nwhere $q_j^s$ is the learnable linear projection head for the prompt basis $p_j^s$ to calculate the attention weight. Subsequently, we pass prompted node features of each graph sample G, represented as $X_s$, to the pre-trained unsigned GNN model to obtain the prompted node embedding matrix: $H_s = f_{pre}(G_s, X_s)$.\nAs downstream tasks may emphasize information from different levels, i.e., the local structure contained from closer neighbors or global information from further nodes, we utilize a learnable hop-wise coefficient $w_s \\in \\mathbb{R}^{1 \\times K}$ for both positive and negative channels to fuse the prompted embedding of graph samples with different hops. The fused embeddings of positive and negative channels are represented as:\n$H_P = \\sum_{i=1}^{k} w_s H_P^i, H_N = \\sum_{i=1}^{k} w_s H_N^i$ (15)\nAs we do not extract multi-hop relationships for the graph sample in the topological channel, we pass its prompted node features $X_T$ to the GNN and only use the prompted embedding $H_T = f_{pre}(G_T, X_T)$ as the embedding of the topological channel.\nSemantic prompt. Given the embeddings of three channels, the semantic prompt is designed to aggregate these embeddings according to the needs of the downstream tasks. To achieve the objective, a straightforward approach is to concatenate their embeddings. However, such aggregation might not effectively capture the varying importance of different channels for the downstream tasks, as it assumes that all channels contribute equally. To address the problem, we adopt an adapter as the semantic prompt.\nFormally, an adapter includes a down projection $W_{down}: \\mathbb{R}^{3d_{out}} \\rightarrow \\mathbb{R}^{d_{mid}}$, a ReLU activation function, and an up projection $W_{up}: \\mathbb{R}^{d_{mid}} \\rightarrow \\mathbb{R}^{d_{out}}$. To reduce the learnable parameters, the mid-dimension $d_{mid}$ is usually set to a small number as a bottleneck. With the adapter, the integrated node embedding matrix is calculated as follows:\n$E = H_T + BN(W_{up}(ReLU(W_{down}([H_T||H_P||H_N]))))$, (16)\nwhere BN denotes batch normalization. The addition of $H_T$ to the output of the adapter serves a dual purpose. First, $H_T$ preserves the topology without the influence of link signs, providing a neutral representation that captures the graph structure (Perozzi, Al-Rfou, and Skiena 2014). It is then enriched by the adapter's integration of the semantic channels. Second, this addition acts as a part of a residual network, which helps to prevent gradient vanishing.\nWhile more complex prompts, such as a learnable linear transformation matrix or an attention layer, can be alternatives. In few-shot scenarios, prompt designs with fewer parameters are generally preferred due to their lower tuning difficulties and better generalization abilities (Yu et al. 2024b). Therefore, we adopt the adapter here for its lightweight nature and tuning efficiency.\nPrompt tuning. Given the training set $D_{down} = \\{(X_1,Y_1), (X_2,Y_2), ...\\}$, where each $x_i$ can be either a node or a linked node pair instance, and $y_i$ is its corresponding label from the task label set Y. With the parameters of pre-trained GNNs frozen, the prompts are optimized by minimizing the loss function $L_{down} =$\n$\\sum_{(X_i, Y_i) \\in D_{down}} -ln \\frac{exp(Sim(e_{X_i}, e_{Y_i}))}{\\sum_{c \\in Y} exp(Sim(e_{X_i}, e_c))}$, (17)\nwhere $e_{X_i}$ can be either the node or link embedding, and $e_c$ is the embedding of the class prototype."}, {"title": "5 Experiments", "content": "5.1 Experimental Setup\nDatasets. We conduct experiments on widely used publicly available signed graph datasets, including Bitcoin-Alpha, Bitcoin-OTC, Slashdot, Epinions, Wikipedia-RfA, Wikipedia-Elec, and Wikipedia-Editor. Details of these datasets can be found in Appendix A."}]}