{"title": "LEARNING LINEAR ATTENTION IN POLYNOMIAL TIME", "authors": ["Morris Yau", "Ekin Aky\u00fcrek", "Jiayuan Mao", "Joshua B. Tenenbaum", "Stefanie Jegelka", "Jacob Andreas"], "abstract": "Previous research has explored the computational expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the learnability of these simulators from observational data has remained an open question. Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention. We show that linear attention may be viewed as a linear predictor in a suitably defined RKHS. As a consequence, the problem of learning any linear transformer may be converted into the problem of learning an ordinary linear predictor in an expanded feature space, and any such predictor may be converted back into a multiheaded linear transformer. Moving to generalization, we show how to efficiently identify training datasets for which every empirical risk minimizer is equivalent (up to trivial symmetries) to the linear Transformer that generated the data, thereby guaranteeing the learned model will correctly generalize across all inputs. Finally, we provide examples of computations expressible via linear attention and therefore polynomial-time learnable, including associative memories, finite automata, and a class of Universal Turing Machine (UTMs) with polynomially bounded computation histories. We empirically validate our theoretical findings on three tasks: learning random linear attention networks, key-value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformers, and show that flexible and general models of computation are efficiently learnable.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers have become a ubiquitous tool in a range of learning problems, due to their versatility in generating structured sequences (including text) conditioned on complex inputs (including natural language instructions). A large body of work has attempted to explain the behavior of trained Transformers and characterize their expressive power . Here one central problem\u2014particularly useful for understanding Transformers' ability to follow instructions in language is understanding how reliably they can simulate execution of automata like universal Turing machines, which in turn can execute arbitrary programs (i.e., the input \u201cinstructions\u201d). While several recent papers have shown that Transformers are expressive enough to implement important models of computation, it remains an open question whether these machines may be effectively learned. Even verifying that a trained model has successfully learned a generalizable computation procedure has remained challenging.\nConcretely, existing work has shown positive results on how Transformer-like architectures can realize many algorithmic computations, including simulating universal Turing machines (Li et al., 2024), evaluating sentences of first-order logic (Barcel\u00f3 et al., 2020), and recognizing various formal languages (Strobl et al., 2024). However, these proofs rely on manual constructions of network weights. For questions about learnability, the most comprehensive result to date is due to Luo et al. (2023), who proposed a proof under very strong assumptions about the data distribution (i.e., under complete presentation by enumerating of all Turing machines up to a constant size). While the result is positive in the sense that training a Transformer model on a finite dataset enables generalization"}, {"title": "2 TECHNICAL OVERVIEW", "content": "We start with basic definitions of a multi-head linear attention (MHLA) module, a stackable attention-based neural module simplified from the standard Transformer module by removing the softmax activation. MHLA has been a standard subject of study for expressivity and learning theory.\nDespite its simplicity, this model class retains significant expressive power: we show that it can realize a restricted but expressive class of universal Turing machines with bounded computation history size. Our results imply that Transformer models can efficiently (in polynomial time) learn these machines with polynomial sample complexity. Moreover, we show that under checkable conditions, minimizing the empirical risk is guaranteed to recover a model that correctly simulates arbitrary new input machines.\nWe first show that the computation performed by MHLAs can be reformulated as an elementwise product between two larger matrices (W, X(Z)), where $W = \\sum_{h\\in [H]} \\text{flatten} (V_n) \\text{flatten}(Q_h)$ and X(Z) is a fixed cubic polynomial function of Z. Consequently, optimizing over the class of H-head MHLA models is equivalent to optimizing over the class of rank-H matrices W. Furthermore, in the full-rank space of $d^2 \\times d^2$ matrices, optimization of W can be performed via linear regression with time polynomial in the inverse target error and size of the dataset. Finally, decomposing an optimal W via SVD recovers an MHLA model with no more than $d^2$ heads that is then guaranteed to compete against the best MHLA parameters-establishing our agnostic learning result (the learned model competes against the best choice of parameters in the hypothesis class).\nNext we define a certificate of identifiability for MHLAs an efficiently checkable condition for any dataset D that, if true, ensures every empirical risk minimizer in the class of MHLAs computes the same function on all possible inputs. More specifically, let $A_D$ be the second moment of the flattening of the data in the X-feature space: $A_D = E_D [X(Z)X(Z)^T]$. We show that, if $A_D$ is full rank, it is guaranteed that MHLA is identifiable with respect to the data. We call this second moment condition \"certifiable identifiability\". When combined with existing results on realizability, this leads to a polynomial-time algorithm for learning computations expressible as linear attention, with checkable certificates of identifiability. Given a dataset satisfying checkable identifiability, if a computational procedure can fit the dataset and can be realized by a MHLA, then all empirical risk minimizers will be equivalent to that procedure. This applies to any function realizable by single-layer linear Transformers."}, {"title": "2.1 POLYNOMIAL-TIME LEARNABILITY", "content": "Our main result is that MHLA is learnable in polynomial time. Colloquially, Algorithm 1 returns an MHLA that attains the global minimum of the training loss, and requires as few as poly(d, \u20ac-1, log(8-1)) samples to achieve e generalization error with probability 1 \u03b4. Here our algorithmic guarantees do not require the data to be \u201crealizable\" (that is, there need be no underlying MHLA that generates the data).\nBelow we describe the high-level intuition behind this proof; a formal statement is given Appendix A. Note additionally that, if we are purely concerned with guaranteeing that we can find a global minimum of the training loss, we may remove the i.i.d. assumption: Algorithm 1 is always within error e of the training loss. This is also detailed in Appendix A. Specific issues related to generalization over autoregressive sequences rather than i.i.d. data are handled in the UTM learning result: see Section C.2."}, {"title": "2.2 IDENTIFIABILITY", "content": "Our algorithmic result also sheds light on a closely related question about generalization in MHLAs: Is there an efficiently checkable condition on any given dataset, such that empirical risk minimization is guaranteed to learn the true data-generating process and generalize even to out-of-distribution examples? Without any qualifiers, \"out of distribution\" generalization is ill-defined. Nevertheless, if a model class is identifiable-if the empirical risk minimizers of the class all compute the same function on all inputs-we can at least know that the empirical risk minimization algorithm produces models with the same behavior (in- and out-of-distribution) on held-out data. Furthermore, if we have a specific computational procedure (e.g., a Universal Turing Machine) that can fit the data and can be realized by some setting of model parameters, then, assuming the dataset is identifiable, all empirical risk minimizers will be equivalent to that procedure. (For a more formal definition of realizability see Definition 4).\nWe show that, as a direct implication of our algorithmic result, it is possible to produce such an efficiently checkable condition (a certificate) on the data that guarantees every empirical risk minimizer in a family of MHLAs computes the same function. Let $A_D$ be the second moment of the flattening of the data, denoted H(Z), in feature space:\nThen if $A_D$ is full rank, it is guaranteed that MHLA is identifiable with respect to the data.\nClearly MHLAs with very different parameters can compute the same function, due to simple symmetries like reciprocal scaling of the V and Q matrices. The polynomial p defines the equivalence class of parameters that compute the same function. For a formal statement of Lemma 2.1 see Lemma 4.1. For handling of errors for approximate empirical risk minimization see Lemma 4.4. Moreover, the certificate given by Algorithm 2 is not the only choice of feature mapping H that would certify identifiability; for a general lemma on certifiable identifiability see Lemma B.1. One way to interpret Corollary 1 is that two MHLA models parameterized by and \u0398' compute the"}, {"title": "2.3 REALIZABILITY OF UNIVERSAL AUTOMATA IN MHLA", "content": "We also include an application of our theory on learnability and identifiability to the problem of learning a universal Turing machine (UTMs) with polynomially bounded computation length. We prove such a UTM is expressible via MHLA in Lemma 2.2, and show that for certifiably identifiable data the learned MHLA generalizes to any TM M and input word x in Lemma 5.2.\nOur construction bears similarities to (P\u00e9rez et al., 2019; Hahn, 2020; Merrill & Sabharwal, 2023; Merrill et al., 2022; 2021; Liu et al., 2022; Feng et al., 2023); the high-level idea is write down every letter in the computation history of M on x. If we use orthogonal vectors to encode every letter, state, and positional embedding we arrive at a natural construction involving a few basic primitives copy, lookup, and if-then-else. For details see discussion section C and Proof C.1\nWe now proceed to a more detailed discussion of the main technical result Theorem 2.1."}, {"title": "3 POLYNOMIAL ALGORITHM FOR LEARNING MHLA", "content": "We first show that, given any dataset D, there exists a learning algorithm that can recover the optimal parameter of an MHLA with a fixed latent dimension d, in the sense of empirical risk minimization."}, {"title": "4 CERTIFICATE FOR IDENTIFIABILITY OF LINEAR ATTENTION", "content": "We begin by defining identifiability of a model class with respect to a dataset."}, {"title": "5 APPLICATION TO LEARNING UNIVERSAL TURING MACHINES", "content": "We apply our algorithmic and identifiability machinery to show that an important computational procedure is representable and learnable as an MHLA: namely, a restricted class of universal Turing machines (UTMs) with bounded computation history. We must first generalize our previous MHLA definition to enable multi-step computation:"}, {"title": "6 EXPERIMENTS", "content": "The theoretical analysis establishes three key results rephrased in the context of empirical validation:\nIn our experiments, we validate these theoretical predictions in practical settings where Transformers are trained using stochastic gradient descent (SGD), as follows:"}, {"title": "6.1 DO EXTRA HEADS HELP INDEPENDENT OF THE LEARNING ALGORITHM?", "content": "Algorithm 1 returns a $d^2$ head MHLA which competes with the optimal model on the training data. If the data is generated by a single-head linear attention, our method can be viewed as learning with an over-parameterized model. This raises the question: Are other forms of over-parameterization equally effective in learning linear attention networks? To address this, we train three types of over-parameterized models with SGD on data generated from a single-layer linear attention network: (1) multi-layer linear attention networks, (2) multi-head linear attention networks, (3) a full Transformer layer. The results are presented in Figure 1."}, {"title": "6.2 DOES CERTIFIABLE IDENTIFIABILITY PREDICT GENERALIZATION?", "content": "In Lemma 4.1, we developed a certificate that provides a sufficient condition for identifiability. However, it gives a sufficient, not necessary, condition for generalization. To assess the practical relevance of this certificate, we conducted an empirical analysis of convergence in cases where the condition is not satisfied. The results of this analysis are presented in Figure 2.\nAssociative Memory Associative Memory (Bietti et al., 2023; Cabannes et al., 2024) is a task of looking up a value in a table with a query. As a single head one-layer linear attention it can be represented with ground truth parameters $\\Theta = \\{V, Q\\}$ where $V, Q \\in \\mathbb{R}^{2d\\times2d}$."}, {"title": "6.3 LEARNING THE COMPUTATION HISTORY OF DETERMINISTIC FINITE AUTOMATA", "content": "Universal automata (like the universal Turing machine discussed in Appendix C.2) receive descriptions of other automata as input, and simulate them to produce an output. Here we empirically evaluate the ability of MHLA models to perform universal simulation of deterministic finite automata (DFAs). We limit our study to DFAs with a maximum number of states (N), alphabet size (V), and input length (L). While recent work on in-context learning (Aky\u00fcrek et al., 2024) has focused on inferring DFA behavior from input-output examples, here, we aim to simulate DFAs given explicit descriptions of their state transitions as input\u2014a task somewhat analogous to instruction following in large scale language models."}, {"title": "7 RELATED WORK", "content": "We break down the literature on learning transformers. First, there is the literature on statistical learnability, where the focus is on the amount of data required to learn without considering whether"}, {"title": "7.1 FORMAL EXPRESSIVITY OF TRANSFORMERS", "content": "A large body of work has been trying to tackle the problem of quantifying what algorithmic tasks can a Transformer do, in terms of various kinds of circuit families (P\u00e9rez et al., 2019; Edelman et al., 2022b; Hahn, 2020; Merrill & Sabharwal, 2023; Merrill et al., 2022; 2021; Liu et al., 2022; Feng et al., 2023). In particular, researchers have studied how Transformers can realize specific DSLs (Weiss et al., 2021), logic expressions (Dong et al., 2019; Barcel\u00f3 et al., 2020; 2024), Turing machines (Dehghani et al., 2018; Giannou et al., 2023; P\u00e9rez et al., 2021), formal language recognition (Hao et al., 2022; Chiang et al., 2023), as well as automata and universal Turing machines (Liu et al., 2022; Li et al., 2024). However, while these works primarily focus on determining the types of problems whose solutions a Transformer can express, they often overlook the crucial question of how these solutions can be learned from data. Moreover, there is limited discussion on the sufficiency of the dataset itself-whether the data available can identify the underlying \"true\" function or algorithm that we aim to capture."}, {"title": "7.2 LEARNING TRANSFORMERS", "content": "We break down the literature on learning transformers. First, there is the literature on statistical learnability, where the focus is on the amount of data required to learn without considering whether"}, {"title": "D ADDITIONAL DEFINITIONS", "content": "We adopt a naive \"rounding\u201d scheme for converting vectors into tokens. This can be done in a variety of ways, and we choose to simply round the entries of the vector embeddings to the nearest token embedding."}, {"title": "C PROGRAMS EXPRESSIBLE AS FIXED DEPTH LINEAR TRANSFORMER", "content": "In this section we build out examples of programs that can be expressed as fixed depth linear transformers. Expressibility results can be carried out in a variety of equivalent ways. The main takeaway, is that the computation history of TM M on word x, when written down \"step by step\" can be captured by next token prediction of linear attention. This is because the key-query-value naturally implements a table lookup sometimes referred to as \"associative memory\" or \"in context linear regression\" in the linear case.\nWe write such programs in an object oriented syntax with each token representing an object with multiple attributes. Attributes can be updated and looked up from other objects using a generalized lookup akin to associative memory."}, {"title": "C.1 CONSTRUCTION OF UTM", "content": "Now we proceed with our construction of an Autoregressive MHLA-Program for UTM. The UTM requires a small number of operations captured by an Autoregressive MHLA-Program."}]}