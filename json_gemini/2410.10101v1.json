{"title": "LEARNING LINEAR ATTENTION IN POLYNOMIAL TIME", "authors": ["Morris Yau", "Ekin Aky\u00fcrek", "Jiayuan Mao", "Joshua B. Tenenbaum", "Stefanie Jegelka", "Jacob Andreas"], "abstract": "Previous research has explored the computational expressivity of Transformer mod-\nels in simulating Boolean circuits or Turing machines. However, the learnability\nof these simulators from observational data has remained an open question. Our\nstudy addresses this gap by providing the first polynomial-time learnability results\n(specifically strong, agnostic PAC learning) for single-layer Transformers with\nlinear attention. We show that linear attention may be viewed as a linear predictor\nin a suitably defined RKHS. As a consequence, the problem of learning any linear\ntransformer may be converted into the problem of learning an ordinary linear pre-\ndictor in an expanded feature space, and any such predictor may be converted back\ninto a multiheaded linear transformer. Moving to generalization, we show how to\nefficiently identify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated the\ndata, thereby guaranteeing the learned model will correctly generalize across all in-\nputs. Finally, we provide examples of computations expressible via linear attention\nand therefore polynomial-time learnable, including associative memories, finite\nautomata, and a class of Universal Turing Machine (UTMs) with polynomially\nbounded computation histories. We empirically validate our theoretical findings\non three tasks: learning random linear attention networks, key-value associations,\nand learning to execute finite automata. Our findings bridge a critical gap between\ntheoretical expressivity and learnability of Transformers, and show that flexible\nand general models of computation are efficiently learnable.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers have become a ubiquitous tool in a range of learning problems, due to their versatility\nin generating structured sequences (including text) conditioned on complex inputs (including natural\nlanguage instructions). A large body of work has attempted to explain the behavior of trained\nTransformers and characterize their expressive power . Here one central problem\u2014particularly useful\nfor understanding Transformers' ability to follow instructions in language is understanding how\nreliably they can simulate execution of automata like universal Turing machines, which in turn can\nexecute arbitrary programs (i.e., the input \u201cinstructions\u201d). While several recent papers have shown\nthat Transformers are expressive enough to implement important models of computation, it remains\nan open question whether these machines may be effectively learned. Even verifying that a trained\nmodel has successfully learned a generalizable computation procedure has remained challenging.\nConcretely, existing work has shown positive results on how Transformer-like architectures can\nrealize many algorithmic computations, including simulating universal Turing machines (Li et al.,\n2024), evaluating sentences of first-order logic (Barcel\u00f3 et al., 2020), and recognizing various formal\nlanguages (Strobl et al., 2024). However, these proofs rely on manual constructions of network\nweights. For questions about learnability, the most comprehensive result to date is due to Luo et al.\n(2023), who proposed a proof under very strong assumptions about the data distribution (i.e., under\ncomplete presentation by enumerating of all Turing machines up to a constant size). While the result\nis positive in the sense that training a Transformer model on a finite dataset enables generalization"}, {"title": "2 TECHNICAL OVERVIEW", "content": "We start with basic definitions of a multi-head linear attention (MHLA) module, a stackable attention-\nbased neural module simplified from the standard Transformer module by removing the softmax\nactivation. MHLA has been a standard subject of study for expressivity and learning theory.\nDefinition (Multi-Head Linear Attention). Let \\(Z \\in \\mathbb{R}^{d \\times n}\\) be a matrix of input data. Let \\(\\Theta =\\{(V_h, Q_h)\\}\\_{h \\in [H]}\\) be a set of parameters where each \\(V_h, Q_h \\in \\mathbb{R}^{d \\times d}\\) denotes value and key-query\nmatrices for all heads \\(h \\in [H]\\). We say \\(\\Theta \\in \\Omega\\_H\\) where \\(\\Omega\\_H\\) is the space of sets of H ordered\ntuples of of d x d matrices. We define multi-head linear attention (MHLA) to be the function\nMHLA: \\(\\mathbb{R}^{dxn} \\rightarrow \\mathbb{R}^{dxn}\n\\[Y = MHLA_{\\Theta}(Z) = \\sum_{h \\in [H]} V_hZ(Z^TQ_hZ),\\]\nwhere \\(\\hat{Y} \\in \\mathbb{R}^{d \\times n}\\) is the output of the one layer linear attention. We will primarily be interested in\nthe rightmost column vector output by MHLA_{\\Theta} (e.g., as in auto-regressive language models), which is:\n\\[\\hat{y} = MHLA_{\\Theta}(Z) = \\sum_{h \\in [H]} V_hZ(Z^TQ_hZ[:,n]),\\]\nNote that MHLA is a uniform circuit family: it can take inputs of any length dimension n and fixed\nembedding dimension d. It is uniform in the sense that there is a polynomial time algorithm that\nmaps from the parameters to the circuit that processes inputs of length n. We will occasionally\nabuse terminology and refer to MHLA as a function rather than as a family of functions."}, {"title": "2.1 POLYNOMIAL-TIME LEARNABILITY", "content": "Our main result is that MHLA is learnable in polynomial time. Colloquially, Algorithm 1 re-\nturns an MHLA that attains the global minimum of the training loss, and requires as few as\npoly(\\(d, \\epsilon^{-1}, log(\\delta^{-1})\\)) samples to achieve \\(\\epsilon\\) generalization error with probability \\(1 - \\delta\\). Here our\nalgorithmic guarantees do not require the data to be \u201crealizable\" (that is, there need be no underlying\nMHLA that generates the data).\nTheorem 2.1 (Learnability of Linear Attention). Let \\(D\\) be a dataset \\(D = \\{(Z_i,Y_i)\\}\\_{i\\in[N]}\\) drawn\ni.i.d. from a distribution \\(D\\) where each \\(Z_i \\in \\mathbb{R}^{d \\times n_i}, Y_i \\in \\mathbb{R}^{d}\\). Here the embedding dimension\nd is fixed across the dataset, whereas \\(n_i\\) can be different for each datapoint. Let \\(N\\_{max}\\) be the\nmaximum sequence length \\(|n_i|\\) for \\(i \\in [N]\\), and let \\(\\Omega\\_H\\) be the space of H pairs of value and\nkey-query matrices \\(\\{(V_h, Q_h)\\}\\_{h\\in[H]}\\) for any \\(H \\in [1,\\infty\\)\\). Then there is an algorithm (Algorithm 1)\nthat runs in time \\(O(Nd^4n\\_{max}\\epsilon^{-1})\\) and that, given input-output pairs \\(\\{(Z_i, Y_i)\\}\\_{i\\in[N]}\\), returns \\(\\hat{\\Theta} =\\{(V_h, Q_h)\\}\\_{h\\in[\\hat{H}]} \\in \\Omega\\_{\\hat{H}}\\) for \\(\\hat{H} < d^2\\) such that with probability \\(1 - \\delta\\),\n\\[E_{(Z,y)\\sim D} [||MHLA_{\\hat{\\Theta}}(Z) - y||^2] - min_{\\Theta\\in\\Omega\\_H} E_{(Z,Y)\\sim D} [||MHLA_{\\Theta}(Z) - y||^2] < \\epsilon\\]\nwith sample complexity \\(N = O(\\frac{d^4}{\\epsilon} + \\frac{log(\\delta^{-1})}{\\epsilon})\\).\nBelow we describe the high-level intuition behind this proof; a formal statement is given Appendix A.\nNote additionally that, if we are purely concerned with guaranteeing that we can find a global\nminimum of the training loss, we may remove the i.i.d. assumption: Algorithm 1 is always within\nerror \\(\\epsilon\\) of the training loss. This is also detailed in Appendix A. Specific issues related to generalization\nover autoregressive sequences rather than i.i.d. data are handled in the UTM learning result: see\nSection C.2."}, {"title": "2.2 IDENTIFIABILITY", "content": "Our algorithmic result also sheds light on a closely related question about generalization in MHLAs:\nIs there an efficiently checkable condition on any given dataset, such that empirical risk minimization\nis guaranteed to learn the true data-generating process and generalize even to out-of-distribution\nexamples? Without any qualifiers, \"out of distribution\" generalization is ill-defined. Nevertheless,\nif a model class is identifiable-if the empirical risk minimizers of the class all compute the same\nfunction on all inputs-we can at least know that the empirical risk minimization algorithm produces\nmodels with the same behavior (in- and out-of-distribution) on held-out data. Furthermore, if we have\na specific computational procedure (e.g., a Universal Turing Machine) that can fit the data and can be\nrealized by some setting of model parameters, then, assuming the dataset is identifiable, all empirical\nrisk minimizers will be equivalent to that procedure. (For a more formal definition of realizability see\nDefinition 4).\nWe show that, as a direct implication of our algorithmic result, it is possible to produce such\nan efficiently checkable condition (a certificate) on the data that guarantees every empirical risk\nminimizer in a family of MHLAs computes the same function. Let \\(A\\_D\\) be the second moment of the\nflattening of the data, denoted \\(H(Z)\\), in feature space:\n\\[A\\_D = \\mathbb{E}\\[H(Z) H(Z)^T] = \\frac{1}{N}\\sum_{Z\\in D} \\[H(Z) H(Z)^T\\].\\]\nThen if \\(A\\_D\\) is full rank, it is guaranteed that MHLA is identifiable with respect to the data.\nLemma 2.1 (Certificate of Identifiability-Informal). Let dataset \\(D = \\{(Z_i, Y_i)\\}\\_{i\\in[N]}\\) be realizable\n(see Definition 4) by an H-head MHLA for any H > 1. Let \\(H\\) be the uniform family of polynomials\n\\(H\\_n : \\mathbb{R}^{d \\times n} \\rightarrow \\mathbb{R}^{\\psi}\\) for \\(\\psi := \\binom{d}{2} + d^2\\) defined as in Algorithm 2, and for convenience write\n\\(H(Z) = H\\_n(Z)\\) for \\(Z \\in \\mathbb{R}^{d \\times n}\\) (there is one \\(H\\_n \\in H\\) for each length-n input). Finally, define\n\\(A\\_D \\in \\mathbb{R}^{\\psi \\times \\psi}\\) to be the second moment of the data features:\n\\[A\\_D := \\mathbb{E}\\_D \\[H(Z)H(Z)^T] .\\]\nThen if the eigenvalue \\(\\lambda\\_{min}(A\\_D) > 0\\), we say that MHLA_{\\Theta} is certifiably identifiable with respect to\nD. That is, for every pair of empirical risk minimizers \\(\\Theta, \\Theta' \\in \\Omega\\_{H}\\)\n\\[MHLA_{\\Theta} = MHLA_{\\Theta'}\\]\ni.e., the two models have the same outputs on all inputs.\nCorollary 1. There is a polynomial \\(p : \\Omega \\rightarrow \\mathbb{R}^{\\psi}\\) such that for any pair of parameters \\(\\Theta, \\Theta' \\in \\Omega\\_{H}\\)\nwe have \\(MHLA_{\\Theta} = MHLA_{\\Theta'}\\) if and only if \\(p(\\Theta) = p(\\Theta')\\).\nClearly MHLAs with very different parameters can compute the same function, due to simple\nsymmetries like reciprocal scaling of the V and Q matrices. The polynomial p defines the equivalence\nclass of parameters that compute the same function. For a formal statement of Lemma 2.1 see\nLemma 4.1. For handling of errors for approximate empirical risk minimization see Lemma 4.4.\nMoreover, the certificate given by Algorithm 2 is not the only choice of feature mapping H that\nwould certify identifiability; for a general lemma on certifiable identifiability see Lemma B.1. One\nway to interpret Corollary 1 is that two MHLA models parameterized by \\(\\Theta\\) and \\(\\Theta'\\) compute the"}, {"title": "2.3 REALIZABILITY OF UNIVERSAL AUTOMATA IN MHLA", "content": "We also include an application of our theory on learnability and identifiability to the problem of\nlearning a universal Turing machine (UTMs) with polynomially bounded computation length. We\nprove such a UTM is expressible via MHLA in Lemma 2.2, and show that for certifiably identifiable\ndata the learned MHLA generalizes to any TM M and input word x in Lemma 5.2.\nLemma 2.2 (UTM Expressibility). Let \\(\\Delta(Q, \\Sigma, \\tilde{n}, \\Phi)\\) be the set of Turing machines M\n\\[= \\{\\delta, \\Sigma, Q, q\\_{start}, q\\_{accept}, q\\_{reject}\\}\\] and words \\(x \\in \\Sigma^*\\) with number of states, size of alphabet, size\nof input, and number of steps in computation history bounded by \\(Q, \\Sigma, \\tilde{n}, \\Phi\\) respectively. For\nany \\((M,x) \\in \\Delta\\), let \\(\\{x\\_t\\}\\_{t\\in[\\Phi]}\\) be the computation history of the UTM on \\((M,x)\\). Let the\nautoregressive computation history (see Definition 5) of MHLA_{\\Theta} on input \\((M,x)\\) be denoted\n\\(CH(M, x) = \\{Z\\_1, Z\\_2, ..., Z\\_\\Phi\\}\\). Then there exists a set of parameters \\(\\Theta \\in \\Omega\\_H\\) for \\(H = O(\\tilde{n}\\Phi\\Sigma)\\)\nand embedding dimension \\(d = O(\\tilde{n}\\Phi \\Sigma \\max(\\Sigma, 2))\\), such that for all \\((M, x) \\in \\Delta\\), the TM compu-\ntation history at time step t is equivalent to the autoregressive computation history at time step \\(c(t)\\)\nwhere \\(c(t) \\le O((\\tilde{n} + t)t)\\) i.e. \\(Z\\_{c(t)} [: -length(x\\_t)] = x\\_t\\). Furthermore, this can be achieved with 2\nbits of precision.\nOur construction bears similarities to (P\u00e9rez et al., 2019; Hahn, 2020; Merrill & Sabharwal, 2023;\nMerrill et al., 2022; 2021; Liu et al., 2022; Feng et al., 2023); the high-level idea is write down every\nletter in the computation history of M on x. If we use orthogonal vectors to encode every letter, state,\nand positional embedding we arrive at a natural construction involving a few basic primitives copy,\nlookup, and if-then-else. For details see discussion section C and Proof C.1\nWe now proceed to a more detailed discussion of the main technical result Theorem 2.1."}, {"title": "3 POLYNOMIAL ALGORITHM FOR LEARNING MHLA", "content": "We first show that, given any dataset \\(D\\), there exists a learning algorithm that can recover the optimal\nparameter of an MHLA with a fixed latent dimension d, in the sense of empirical risk minimization.\nTheorem 2.1 (Learnability of Linear Attention). Let \\(D\\) be a dataset \\(D = \\{(Z_i,Y_i)\\}\\_{i\\in[N]}\\) drawn\ni.i.d. from a distribution \\(D\\) where each \\(Z_i \\in \\mathbb{R}^{d \\times n_i}, Y_i \\in \\mathbb{R}^{d}\\). Here the embedding dimension\nd is fixed across the dataset, whereas \\(n_i\\) can be different for each datapoint. Let \\(N\\_{max}\\) be the\nmaximum sequence length \\(|n_i|\\) for \\(i \\in [N]\\), and let \\(\\Omega\\_H\\) be the space of H pairs of value and\nkey-query matrices \\(\\{(V_h, Q_h)\\}\\_{h\\in[H]}\\) for any \\(H \\in [1,\\infty\\)\\). Then there is an algorithm (Algorithm 1)\nthat runs in time \\(O(Nd^4n\\_{max}\\epsilon^{-1})\\) and that, given input-output pairs \\(\\{(Z_i, Y_i)\\}\\_{i\\in[N]}\\), returns \\(\\hat{\\Theta} =\\{(V_h, Q_h)\\}\\_{h\\in[\\hat{H}]} \\in \\Omega\\_{\\hat{H}}\\) for \\(\\hat{H} < d^2\\) such that with probability \\(1 - \\delta\\),\n\\[E_{(Z,Y)\\sim D} [||MHLA_{\\hat{\\Theta}}(Z) - y||^2] - min_{\\Theta\\in\\Omega\\_H} E_{(Z,Y)\\sim D} [||MHLA_{\\Theta}(Z) - y||^2] < \\epsilon\\]\nProof Idea: First we write down the loss, and observe that a one-layer attention network is a\nquadratic polynomial in \\(\\{V\\_h, Q\\_h\\}\\_{h\\in[H]}\\) of input features \\(X\\_{i,a}\\)."}, {"title": "4 CERTIFICATE FOR IDENTIFIABILITY OF LINEAR ATTENTION", "content": "We begin by defining identifiability of a model class with respect to a dataset.\nDefinition (Identifiability). Let \\(D = \\{(Z_i, Y_i)\\}\\_{i\\in[N]}\\). Let \\(\\mathcal{U}\\_{\\Theta}\\) denote a model class which is a\nuniform circuit family parameterized by parameters \\(\\Theta \\in \\Omega\\). Let L be a loss function and \\(\\Omega\\_{ERM}\\) be\nthe set of empirical risk minimizers:\n\\[\\Omega\\_{ERM} = \\{\\hat{\\Theta} \\in \\Omega | \\mathcal{U}\\_{\\hat{\\Theta}} = argmin_{\\Theta \\in \\Omega} L(\\mathcal{U}\\_{\\Theta}, D)\\}\\]\nWe say model class \\(\\mathcal{U}\\_{\\Theta}\\) is identifiable with respect to the dataset \\(D\\) if for all \\(Z \\in \\mathbb{R}^{d \\times n'}\\), and for all\npairs of empirical risk minimizers \\(\\Theta, \\Theta' \\in \\Omega\\_{ERM}\\) we have \\(\\mathcal{U}\\_{\\Theta}\\) and \\(\\mathcal{U}\\_{\\Theta'}\\) compute the same function,\ni.e., they agree on all inputs (are the same uniform circuit family):\n\\[\\mathcal{U}\\_{\\Theta}(Z) = \\mathcal{U}\\_{\\Theta'}(Z).\\]"}, {"title": "5 APPLICATION TO LEARNING UNIVERSAL TURING MACHINES", "content": "We apply our algorithmic and identifiability machinery to show that an important computational\nprocedure is representable and learnable as an MHLA: namely, a restricted class of universal Turing\nmachines (UTMs) with bounded computation history. We must first generalize our previous MHLA\ndefinition to enable multi-step computation:"}, {"title": "C PROGRAMS EXPRESSIBLE AS FIXED DEPTH LINEAR TRANSFORMER", "content": "In this section we build out examples of programs that can be expressed as fixed depth linear\ntransformers. Expressibility results can be carried out in a variety of equivalent ways. The main\ntakeaway, is that the computation history of TM M on word x, when written down \"step by step\"\ncan be captured by next token prediction of linear attention. This is because the key-query-value\nnaturally implements a table lookup sometimes referred to as \"associative memory\" or \"in context\nlinear regression\" in the linear case.\nThe notion of an Autoregressive MHLA Program is useful for condensing the proofs of expressibility.\nWe write such programs in an object oriented syntax with each token representing an object with\nmultiple attributes. Attributes can be updated and looked up from other objects using a generalized\nlookup akin to associative memory."}, {"title": "C.1 CONSTRUCTION OF UTM", "content": "Now we proceed with our construction of an Autoregressive MHLA-Program for UTM. The UTM\nrequires a small number of operations captured by an Autoregressive MHLA-Program.\nWe define an embedding function that takes as input a TM M and word x such that"}, {"title": "D ADDITIONAL DEFINITIONS", "content": "Definition (Orthogonal Embeddings). Let Embed be a function Embed : \\(\\Sigma \\rightarrow \\mathbb{R}^{|\\Sigma|}\\). Let \\(\\Sigma\\) be an\nalphabet and let \\(e\\_1, e\\_2, ..., e\\_{|\\Sigma|} \\in \\mathbb{R}^{|\\Sigma|}\\) be a basis of orthogonal unit vectors. Then for each letter a in\nan alphabet \\(\\Sigma\\), we define Embed(a) = e\\_a where we associate a different unit vector to each letter.\nWe adopt a naive \"rounding\u201d scheme for converting vectors into tokens. This can be done in a variety\nof ways, and we choose to simply round the entries of the vector embeddings to the nearest token\nembedding.\nDefinition (Rounding). For any vector \\(v = (v\\_1, v\\_2, ..., v\\_d) \\in \\mathbb{R}^d\\), let Round(v) = e\\_j for j =\narg max\\(i\\in [d]\\) \\((v, e\\_i)\\). Since we use orthogonal unit vectors for token embeddings we will refer to\nRound(v) as a token. We will often refer to a matrix \\(Z \\in \\mathbb{R}^{d \\times n}\\) as being equivalent to a series of n\ntokens \\(a\\_1, a\\_2, ..., a\\_n\\) to mean Round(Z[:, i]) = a\\_i for all \\(i \\in [n]\\)."}, {"title": "D.1 TRAINING DETAILS OF ATTENTION NETWORKS", "content": "We use Adam Kingma & Ba (2014) optimizer to train linear attention model Equation (1) and the full\nTransformer Vaswani et al. (2017) models."}, {"title": "D.2 TRAINING DETAILS IN DFA EXECUTION", "content": "We use the Llama variant of the Transformer arhitecture from Touvron et al. (2023). We run\neach setting with N number of training examples with the following different values N\u2208\n\\{16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 6144, 8192, 12290, 16384, 20480, 32768, 65536\\}. The\nother hyper parameters are given in the below table."}]}