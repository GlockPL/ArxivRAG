[{"title": "LEARNING LINEAR ATTENTION IN POLYNOMIAL TIME", "authors": ["Morris Yau", "Ekin Aky\u00fcrek", "Jiayuan Mao", "Joshua B. Tenenbaum", "Stefanie Jegelka", "Jacob Andreas"], "abstract": "Previous research has explored the computational expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the learnability of these simulators from observational data has remained an open question. Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention. We show that linear attention may be viewed as a linear predictor in a suitably defined RKHS. As a consequence, the problem of learning any linear transformer may be converted into the problem of learning an ordinary linear predictor in an expanded feature space, and any such predictor may be converted back into a multiheaded linear transformer. Moving to generalization, we show how to efficiently identify training datasets for which every empirical risk minimizer is equivalent (up to trivial symmetries) to the linear Transformer that generated the data, thereby guaranteeing the learned model will correctly generalize across all inputs. Finally, we provide examples of computations expressible via linear attention and therefore polynomial-time learnable, including associative memories, finite automata, and a class of Universal Turing Machine (UTMs) with polynomially bounded computation histories. We empirically validate our theoretical findings on three tasks: learning random linear attention networks, key-value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformers, and show that flexible and general models of computation are efficiently learnable.", "sections": [{"title": "1 INTRODUCTION", "content": "Transformers have become a ubiquitous tool in a range of learning problems, due to their versatility in generating structured sequences (including text) conditioned on complex inputs (including natural language instructions). A large body of work has attempted to explain the behavior of trained Transformers and characterize their expressive power . Here one central problem\u2014particularly useful for understanding Transformers' ability to follow instructions in language is understanding how reliably they can simulate execution of automata like universal Turing machines, which in turn can execute arbitrary programs (i.e., the input \u201cinstructions\u201d). While several recent papers have shown that Transformers are expressive enough to implement important models of computation, it remains an open question whether these machines may be effectively learned. Even verifying that a trained model has successfully learned a generalizable computation procedure has remained challenging.\nConcretely, existing work has shown positive results on how Transformer-like architectures can realize many algorithmic computations, including simulating universal Turing machines (Li et al., 2024), evaluating sentences of first-order logic (Barcel\u00f3 et al., 2020), and recognizing various formal languages (Strobl et al., 2024). However, these proofs rely on manual constructions of network weights. For questions about learnability, the most comprehensive result to date is due to Luo et al. (2023), who proposed a proof under very strong assumptions about the data distribution (i.e., under complete presentation by enumerating of all Turing machines up to a constant size). While the result is positive in the sense that training a Transformer model on a finite dataset enables generalization"}, {"title": "2 TECHNICAL OVERVIEW", "content": "We start with basic definitions of a multi-head linear attention (MHLA) module, a stackable attention-based neural module simplified from the standard Transformer module by removing the softmax activation. MHLA has been a standard subject of study for expressivity and learning theory.\nDefinition (Multi-Head Linear Attention). Let $Z \\in \\mathbb{R}^{d \\times n}$ be a matrix of input data. Let $\\Theta = \\{(V_h, Q_h)\\}_{h \\in [H]}$ be a set of parameters where each $V_h, Q_h \\in \\mathbb{R}^{d \\times d}$ denotes value and key-query matrices for all heads $h \\in [H]$. We say $\\Theta \\in \\Omega_H$ where $\\Omega_H$ is the space of sets of $H$ ordered tuples of of $d \\times d$ matrices. We define multi-head linear attention (MHLA) to be the function $\\text{MHLA} : \\mathbb{R}^{d \\times n} \\rightarrow \\mathbb{R}^{d \\times n}$\n$Y = \\text{MHLA}(Z) = \\sum_{h\\in [H]} V_hZ(Z^TQ_hZ),$ (1)\nwhere $\\hat{Y} \\in \\mathbb{R}^{d \\times n}$ is the output of the one layer linear attention. We will primarily be interested in the rightmost column vector output by $\\text{MHLA}_{\\Theta}$ (e.g., as in auto-regressive language models), which is:\n$\\hat{y} = \\text{MHLA}(Z) = \\sum_{h\\in [H]} V_hZ(Z^TQ_hZ[:,n]),$ (2)\nwhere $Z[:, n]$ is the nth column of $Z$.\nNote that MHLA is a uniform circuit family: it can take inputs of any length dimension $n$ and fixed embedding dimension $d$. It is uniform in the sense that there is a polynomial time algorithm that maps from the parameters to the circuit that processes inputs of length $n$. We will occasionally abuse terminology and refer to MHLA as a function rather than as a family of functions."}, {"title": "2.1 POLYNOMIAL-TIME LEARNABILITY", "content": "Our main result is that MHLA is learnable in polynomial time. Colloquially, Algorithm 1 returns an MHLA that attains the global minimum of the training loss, and requires as few as $poly(d, \\epsilon^{-1}, log(\\delta^{-1}))$ samples to achieve $\\epsilon$ generalization error with probability $1 - \\delta$. Here our algorithmic guarantees do not require the data to be \u201crealizable\" (that is, there need be no underlying MHLA that generates the data).\nTheorem 2.1 (Learnability of Linear Attention). Let $D$ be a dataset $D = \\{(Z_i,Y_i)\\}_{i\\in[N]}$ drawn i.i.d. from a distribution $D$ where each $Z_i \\in \\mathbb{R}^{d \\times n_i}, Y_i \\in \\mathbb{R}^{d}$. Here the embedding dimension $d$ is fixed across the dataset, whereas $n_i$ can be different for each datapoint. Let $N_{max}$ be the maximum sequence length $|n_i|$ for $i \\in [N]$, and let $\\Omega_H$ be the space of $H$ pairs of value and key-query matrices $\\{(V_h, Q_h)\\}_{h\\in[H]}$ for any $H \\in [1,\\infty)$. Then there is an algorithm (Algorithm 1) that runs in time $O(Nd^4n_{max}\\epsilon^{-1})$ and that, given input-output pairs $\\{(Z_i, Y_i)\\}_{i\\in[N]}$, returns $\\Theta = \\{(V_h, Q_h)\\}_{h\\in[\\hat{H}]} \\in \\Omega_{\\hat{H}}$ for $\\hat{H} < d^2$ such that with probability $1 - \\delta$,\n$\\mathbb{E}_{(Z,Y)\\sim D} [||\\text{MHLA}(Z) - y||^2] - \\min_{\\Theta\\in\\Omega_H} \\mathbb{E}_{(Z,Y)\\sim D} [||\\text{MHLA}_{\\Theta}(Z) - y||^2] < \\epsilon$ (3)\nwith sample complexity $N = O (\\frac{1}{\\epsilon^2} (d^4 + log(\\delta^{-1})))$.\nBelow we describe the high-level intuition behind this proof; a formal statement is given Appendix A. Note additionally that, if we are purely concerned with guaranteeing that we can find a global minimum of the training loss, we may remove the i.i.d. assumption: Algorithm 1 is always within error $\\epsilon$ of the training loss. This is also detailed in Appendix A. Specific issues related to generalization over autoregressive sequences rather than i.i.d. data are handled in the UTM learning result: see Section C.2."}, {"title": "2.2 IDENTIFIABILITY", "content": "Our algorithmic result also sheds light on a closely related question about generalization in MHLAs: Is there an efficiently checkable condition on any given dataset, such that empirical risk minimization is guaranteed to learn the true data-generating process and generalize even to out-of-distribution examples? Without any qualifiers, \"out of distribution\" generalization is ill-defined. Nevertheless, if a model class is identifiable-if the empirical risk minimizers of the class all compute the same function on all inputs-we can at least know that the empirical risk minimization algorithm produces models with the same behavior (in- and out-of-distribution) on held-out data. Furthermore, if we have a specific computational procedure (e.g., a Universal Turing Machine) that can fit the data and can be realized by some setting of model parameters, then, assuming the dataset is identifiable, all empirical risk minimizers will be equivalent to that procedure. (For a more formal definition of realizability see Definition 4).\nWe show that, as a direct implication of our algorithmic result, it is possible to produce such an efficiently checkable condition (a certificate) on the data that guarantees every empirical risk minimizer in a family of MHLAs computes the same function. Let $A_D$ be the second moment of the flattening of the data, denoted $H(Z)$, in feature space:\n$A_D = \\mathbb{E}[H(Z) H(Z)^T] = \\frac{1}{N} \\sum_{Z\\in D} [H(Z) H(Z)^T].$ (5)\nThen if $A_D$ is full rank, it is guaranteed that MHLA is identifiable with respect to the data.\nLemma 2.1 (Certificate of Identifiability-Informal). Let dataset $D = \\{(Z_i, Y_i)\\}_{i\\in[N]}$ be realizable (see Definition 4) by an $H$-head MHLA for any $H > 1$. Let $H$ be the uniform family of polynomials $H_n : \\mathbb{R}^{d \\times n} \\rightarrow \\mathbb{R}^{\\psi}$ for $\\psi := (\\binom{d}{2}d + d^2$ defined as in Algorithm 2, and for convenience write $H(Z) = H_n(Z)$ for $Z \\in \\mathbb{R}^{d \\times n}$ (there is one $H_n \\in H$ for each length-$n$ input). Finally, define $A_D \\in \\mathbb{R}^{\\psi \\times \\psi}$ to be the second moment of the data features:\n$A_D := \\mathbb{E}_D [H(Z)H(Z)^T] .$ (6)\nThen if the eigenvalue $\\lambda_{min} (A_D) > 0$, we say that MHLA is certifiably identifiable with respect to $D$. That is, for every pair of empirical risk minimizers $\\Theta, \\Theta' \\in \\Omega_H$\n$\\text{MHLA}_{\\Theta} = \\text{MHLA}_{\\Theta'}$ (7)\ni.e., the two models have the same outputs on all inputs.\nCorollary 1. There is a polynomial $p : \\Omega \\rightarrow \\mathbb{R}^{\\psi}$ such that for any pair of parameters $\\Theta, \\Theta' \\in \\Omega_H$ we have $\\text{MHLA}_{\\Theta} = \\text{MHLA}_{\\Theta'}$ if and only if $p(\\Theta) = p(\\Theta')$.\nClearly MHLAs with very different parameters can compute the same function, due to simple symmetries like reciprocal scaling of the $V$ and $Q$ matrices. The polynomial $p$ defines the equivalence class of parameters that compute the same function. For a formal statement of Lemma 2.1 see Lemma 4.1. For handling of errors for approximate empirical risk minimization see Lemma 4.4. Moreover, the certificate given by Algorithm 2 is not the only choice of feature mapping $H$ that would certify identifiability; for a general lemma on certifiable identifiability see Lemma B.1. One way to interpret Corollary 1 is that two MHLA models parameterized by $\\Theta$ and $\\Theta'$ compute the"}, {"title": "2.3 REALIZABILITY OF UNIVERSAL AUTOMATA IN MHLA", "content": "We also include an application of our theory on learnability and identifiability to the problem of learning a universal Turing machine (UTMs) with polynomially bounded computation length. We prove such a UTM is expressible via MHLA in Lemma 2.2, and show that for certifiably identifiable data the learned MHLA generalizes to any TM M and input word x in Lemma 5.2.\nLemma 2.2 (UTM Expressibility). Let $\\Delta(\\mathcal{Q}, \\hat{\\Sigma}, \\hat{n}, \\Phi)$ be the set of Turing machines $M = \\{\\delta, \\Sigma, Q, q_{start}, q_{accept},q_{reject}\\}$ and words $x \\in \\Sigma^*$ with number of states, size of alphabet, size of input, and number of steps in computation history bounded by $\\mathcal{Q}, \\hat{\\Sigma}, \\hat{n}, \\Phi$ respectively. For any $(M,x) \\in \\Delta$, let $\\{x_t\\}_{t\\in[\\Phi]}$ be the computation history of the UTM on $(M,x)$. Let the autoregressive computation history (see Definition 5) of $\\text{MHLA}_{\\Theta}$ on input $(M,x)$ be denoted $CH(M, x) = \\{Z_1, Z_2, ..., Z_{\\Phi}\\}$. Then there exists a set of parameters $\\Theta \\in \\Omega_H$ for $H = O(n\\Phi\\Sigma)$ and embedding dimension $d = O(\\hat{n} \\Phi \\Sigma \\max(\\hat{\\Sigma}, \\mathcal{Q}))$, such that for all $(M, x) \\in \\Delta$, the TM computation history at time step $t$ is equivalent to the autoregressive computation history at time step $c(t)$ where $c(t) \\leq O((\\hat{n} + t)t)$ i.e $Z_{c(t)}[: -length(x_t))] = x_t$. Furthermore, this can be achieved with 2 bits of precision.\nOur construction bears similarities to (P\u00e9rez et al., 2019; Hahn, 2020; Merrill & Sabharwal, 2023; Merrill et al., 2022; 2021; Liu et al., 2022; Feng et al., 2023); the high-level idea is write down every letter in the computation history of M on x. If we use orthogonal vectors to encode every letter, state, and positional embedding we arrive at a natural construction involving a few basic primitives copy, lookup, and if-then-else. For details see discussion section C and Proof C.1\nWe now proceed to a more detailed discussion of the main technical result Theorem 2.1."}, {"title": "3 POLYNOMIAL ALGORITHM FOR LEARNING MHLA", "content": "We first show that, given any dataset $D$, there exists a learning algorithm that can recover the optimal parameter of an MHLA with a fixed latent dimension $d$, in the sense of empirical risk minimization.\nTheorem 2.1 (Learnability of Linear Attention). Let $D$ be a dataset $D = \\{(Z_i,Y_i)\\}_{i\\in[N]}$ drawn i.i.d. from a distribution $D$ where each $Z_i \\in \\mathbb{R}^{d \\times n_i}, Y_i \\in \\mathbb{R}^{d}$. Here the embedding dimension $d$ is fixed across the dataset, whereas $n_i$ can be different for each datapoint. Let $N_{max}$ be the maximum sequence length $|n_i|$ for $i \\in [N]$, and let $\\Omega_H$ be the space of $H$ pairs of value and key-query matrices $\\{(V_h, Q_h)\\}_{h\\in[H]}$ for any $H \\in [1,\\infty)$. Then there is an algorithm (Algorithm 1) that runs in time $O(Nd^4n_{max}\\epsilon^{-1})$ and that, given input-output pairs $\\{(Z_i, Y_i)\\}_{i\\in[N]}$, returns $\\Theta = \\{(V_h, Q_h)\\}_{h\\in[\\hat{H}]} \\in \\Omega_{\\hat{H}}$ for $\\hat{H} < d^2$ such that with probability $1 - \\delta$,\n$\\mathbb{E}_{(Z,Y)\\sim D} [||\\text{MHLA}(Z) - y||^2] - \\min_{\\Theta\\in\\Omega_H} \\mathbb{E}_{(Z,Y)\\sim D} [||\\text{MHLA}_{\\Theta}(Z) - y||^2] < \\epsilon$ (3)\nwith sample complexity $N = O (\\frac{1}{\\epsilon^2} (d^4 + log(\\delta^{-1})))$.\nProof Idea: First we write down the loss, and observe that a one-layer attention network is a quadratic polynomial in $\\{(V_h, Q_h)\\}_{h\\in[H]}$ of input features $X_{i,a}$."}, {"title": "4 CERTIFICATE FOR IDENTIFIABILITY OF LINEAR ATTENTION", "content": "We begin by defining identifiability of a model class with respect to a dataset.\nDefinition (Identifiability). Let $D = \\{(Z_i, Y_i)\\}_{i\\in[N]}$. Let $\\mathcal{U}_{\\Theta}$ denote a model class which is a uniform circuit family parameterized by parameters $\\Theta \\in \\Omega$. Let $\\mathcal{L}$ be a loss function and $\\Omega_{NERM}$ be the set of empirical risk minimizers:\n$\\Omega_{NERM} = \\{\\Theta \\in \\Omega | \\mathcal{U}_{\\Theta} = arg\\min_{\\Theta'\\in \\Omega} \\mathcal{L}(\\mathcal{U}_{\\Theta'}, D)\\}$. (13)\nWe say model class $\\mathcal{U}_{\\Theta}$ is identifiable with respect to the dataset $D$ if for all $Z \\in \\mathbb{R}^{d \\times n'}$, and for all pairs of empirical risk minimizers $\\Theta, \\Theta' \\in \\Omega_{NERM}$ we have $\\mathcal{U}_{\\Theta}$ and $\\mathcal{U}_{\\Theta'}$ compute the same function, i.e., they agree on all inputs (are the same uniform circuit family):\n$\\mathcal{U}_{\\Theta}(Z) = \\mathcal{U}_{\\Theta'}(Z)$. (14)"}, {"title": "5 APPLICATION TO LEARNING UNIVERSAL TURING MACHINES", "content": "We apply our algorithmic and identifiability machinery to show that an important computational procedure is representable and learnable as an MHLA: namely, a restricted class of universal Turing machines (UTMs) with bounded computation history. We must first generalize our previous MHLA definition to enable multi-step computation:"}, {"title": "6 EXPERIMENTS", "content": "The theoretical analysis establishes three key results rephrased in the context of empirical validation:\nAn overparameterized family of linear attention networks can learn linear attention in polynomial time and samples (Theorem 2.1).\nIn the realizable setting, there are sufficient and checkable conditions under which empirical risk minimization recovers the equivalence class of the ground truth parameter values (Lemma 4.1).\nLinear attention networks can a restricted class of universal Turing machines with poly-nomial hidden size, using polynomially bounded computation histories for state tracking (Lemma 5.1).\nIn our experiments, we validate these theoretical predictions in practical settings where Transformers are trained using stochastic gradient descent (SGD), as follows:\nOne interpretation of Theorem 2.1 is that relaxing MHLA learning into an \"easy\" linear regression problem corresponds to adding extra attention heads, and suggests that adding extra heads might provide optimization benefits even when learning MHLA models in their native form. We investigate the role of over-parameterization in multi-head and multi-layer linear attention networks. For random data generated from linear attention networks, we observe that adding more heads achieves faster convergence of training loss than adding more layers. This suggests that while depth is important for expressiveness, the number of heads is important for optimization (Figure 1)."}, {"title": "6.1 DO EXTRA HEADS HELP INDEPENDENT OF THE LEARNING ALGORITHM?", "content": "Algorithm 1 returns a $d^2$ head MHLA which competes with the optimal model on the training data. If the data is generated by a single-head linear attention, our method can be viewed as learning with an over-parameterized model. This raises the question: Are other forms of over-parameterization equally effective in learning linear attention networks? To address this, we train three types of over-parameterized models with SGD on data generated from a single-layer linear attention network: (1) multi-layer linear attention networks, (2) multi-head linear attention networks, (3) a full Transformer layer. The results are presented in Figure 1.\nExperimental Setup: We initialize a single-layer linear attention network with parameters $V \\in \\mathbb{R}^{1\\times d}$ and $Q \\in \\mathbb{R}^{d\\times d}$, sampled from a Gaussian distribution $N(0, 1)$. Input sequences $Z^i \\in \\mathbb{R}^{T\\times d}$ are sampled from $N(0, \\frac{1}{d})$, where $i = 1,..., N$, $T = 100$ is the maximum number of time steps, and $N$ is the dataset size. We generate outputs by running the ground-truth network auto-regressively: $y^i = V Z^i_t (Z^i[:, : t]QZ^i[:, t])$, creating our dataset $D = \\{(Z^i, y^i)\\}_{i=1}^N$.\nIn addition to learning with Algorithm 1, we train three types of models on this data using SGD:\nMulti-head linear attention as in Equation (1).\nMulti-layer linear attention with a single head.\nAn ordinary Transformer network (Vaswani et al., 2017) with softmax attention, multi-layer perceptron blocks, and layer normalization.\nFor detailed hyperparameters and optimization procedures, please refer to Appendix D.1."}, {"title": "6.2 DOES CERTIFIABLE IDENTIFIABILITY PREDICT GENERALIZATION?", "content": "In Lemma 4.1, we developed a certificate that provides a sufficient condition for identifiability. However, it gives a sufficient, not necessary, condition for generalization. To assess the practical relevance of this certificate, we conducted an empirical analysis of convergence in cases where the condition is not satisfied. The results of this analysis are presented in Figure 2.\nAssociative Memory Associative Memory (Bietti et al., 2023; Cabannes et al., 2024) is a task of looking up a value in a table with a query. As a single head one-layer linear attention it can be represented with ground truth parameters $\\Theta = \\{V, Q\\}$ where $V, Q \\in \\mathbb{R}^{2d\\times2d}$.\n$V = \\begin{bmatrix} 0 & 0 \\ 0 & I_{d\\times d} \\end{bmatrix} \\qquad Q = \\begin{bmatrix} I_{d\\times d} & 0 \\ 0 & 0 \\end{bmatrix}$\nThe data $Z$ is drawn as follows: let $k_1,k_2, ..., k_d \\in \\mathbb{R}^d$ be random variables corresponding to keys in a lookup table, let $v_1, v_2, ..., v_d \\in \\mathbb{R}^d$ be random variables corresponding to values in a lookup table,"}, {"title": "6.3 LEARNING THE COMPUTATION HISTORY OF DETERMINISTIC FINITE AUTOMATA", "content": "Universal automata (like the universal Turing machine discussed in Appendix C.2) receive descrip-tions of other automata as input, and simulate them to produce an output. Here we empirically evaluate the ability of MHLA models to perform universal simulation of deterministic finite automata (DFAs). We limit our study to DFAs with a maximum number of states (N), alphabet size (V), and input length (L). While recent work on in-context learning (Aky\u00fcrek et al., 2024) has focused on inferring DFA behavior from input-output examples, here, we aim to simulate DFAs given explicit descriptions of their state transitions as input\u2014a task somewhat analogous to instruction following in large scale language models.\nThe construction in Lemma 5.1 shows that a linear attention layer can output the polynomially bounded computation history of any TM (and therefore any DFA). Our construction requires embed-ding size linear with maximum length of computation history, number of states and alphabet size. Therefore, we predict the data requirements are polynomial in each of N, V and L.\nDataset Our dataset consists of strings containing three components: the input DFA's transition function $\\delta : Q \\times \\Sigma \\rightarrow Q$, the input word $x \\in \\Sigma^L$ and the computation history $h \\in Q^L$ which is the"}, {"title": "7 RELATED WORK", "content": "We break down the literature on learning transformers. First, there is the literature on statistical learnability, where the focus is on the amount of data required to learn without considering whether"}, {"title": "7.1 FORMAL EXPRESSIVITY OF TRANSFORMERS", "content": "A large body of work has been trying to tackle the problem of quantifying what algorithmic tasks can a Transformer do, in terms of various kinds of circuit families (P\u00e9rez et al., 2019; Edelman et al., 2022b; Hahn, 2020; Merrill & Sabharwal, 2023; Merrill et al., 2022; 2021; Liu et al., 2022; Feng et al., 2023).\nIn particular, researchers have studied how Transformers can realize specific DSLs (Weiss et al., 2021), logic expressions (Dong et al., 2019; Barcel\u00f3 et al., 2020; 2024), Turing machines (Dehghani et al., 2018; Giannou et al., 2023; P\u00e9rez et al., 2021), formal language recognition (Hao et al., 2022; Chiang et al., 2023), as well as automata and universal Turing machines (Liu et al., 2022; Li et al., 2024). However, while these works primarily focus on determining the types of problems whose solutions a Transformer can express, they often overlook the crucial question of how these solutions can be learned from data. Moreover, there is limited discussion on the sufficiency of the dataset itself-whether the data available can identify the underlying \"true\" function or algorithm that we aim to capture."}, {"title": "7.2 LEARNING TRANSFORMERS", "content": "We break down the literature on learning transformers. First, there is the literature on statistical learnability, where the focus is on the amount of data required to learn without considering whether"}, {"title": "A PROOF OF MAIN THEOREM", "content": "Theorem 2.1 (Learnability of Linear Attention). Let $D$ be a dataset $D = \\{(Z_i,Y_i)\\}_{i\\in[N]}$ drawn i.i.d. from a distribution $D$ where each $Z_i \\in \\mathbb{R}^{d \\times n_i}, Y_i \\in \\mathbb{R}^{d}$. Here the embedding dimension $d$ is fixed across the dataset, whereas $n_i$ can be different for each datapoint. Let $N_{max}$ be the maximum sequence length $|n_i|$ for $i \\in [N]$, and let $\\Omega_H$ be the space of $H$ pairs of value and key-query matrices $\\{(V_h, Q_h)\\}_{h\\in[H]}$ for any $H \\in [1, \\infty)$. Then there is an algorithm (Algorithm 1) that runs in time $O(Nd^4n_{max}\\epsilon^{-1})$ and that, given input-output pairs $\\{(Z_i, Y_i)\\}_{i\\in[N]}$, returns $\\hat{\\Theta} = \\{(V_h, Q_h)\\}_{h\\in [\\hat{H}]} \\in \\Omega_{\\hat{H}}$ for $\\hat{H} < d^2$ such that with probability $1 - \\delta$,\n$\\mathbb{E}_{(Z,y)\\sim D} [||\\text{MHLA}(Z) - y||^2] - \\min_{\\Theta\\in\\Omega_H} \\mathbb{E}_{(Z,Y)\\sim D} [||\\text{MHLA}_{\\Theta}(Z) - y||^2] < \\epsilon$ (3)\nwith sample complexity $N = O (\\frac{1}{\\epsilon^2} (d^4 + log(\\delta^{-1})))$.\nProof. First we write down the loss.\n$\\mathcal{L}_{\\Theta}(\\{(Z_i, Y_i)\\}_{i\\in[N]}) := \\frac{1}{N} \\sum_{i\\in[N]} || \\sum_{h\\in[H]} V_hZ_i (Z_i^TQ_hZ_i[:, n_i]) - Y_i ||_F^2$\n$\\hspace{1.7cm} = \\frac{1}{N} \\sum_{i\\in[N]} \\sum_{a\\in[d]} \\left( \\sum_{h\\in[H]} V_{h,a}^T Z_i (Z_i^TQ_hZ_i[:, n_i]) - Y_{i,a} \\right)^2$ (29)\nObserve that the one layer attention network is a quadratic polynomial in $\\{(V_h, Q_h)\\}_{h\\in[H]}$.\n$= \\frac{1}{N} \\sum_{i\\in[N]} \\sum_{a\\in[d]} ((\\text{T}_{\\Theta}, X_{i,a}) - Y_{i,a})^2$ (30)\nHere\n$\\text{T}_{\\Theta} := \\sum_{h \\in [H]} flatten(V_h) flatten(Q_h)^T = \\sum_{h \\in [H]} \\begin{bmatrix} V_{h,00}Q_{h,00} & V_{h,00}Q_{h,01} & ... & V_{h,00}Q_{h,dd} \\ V_{h,01}Q_{h,00} & V_{h,01}Q_{h,01} & ... & V_{h,01}Q_{h,dd} \\ : & : & & : \\ V_{h,dd}Q_{h,00} & V_{h,dd}Q_{h,01} & ... & V_{h,dd}Q_{h,dd} \\end{bmatrix}$ (31)\nNow we relax this objective by replacing $\\text{T}_{\\Theta}$ with an unconstrained matrix $W \\in \\mathbb{R}^{d^2\\times d^2}$. Another way to put it is that $\\text{T}_{\\Theta}$ is rank-$H$ but $W$ can be a general matrix. Because the space of general rank matrices is larger, we have written down a relaxation guaranteed to have a smaller loss. Furthermore the loss can be optimized via ordinary least squares.\n$\\min_{W\\in \\mathbb{R}^{d^2 \\times d^2}} L_W(\\{(Z_i, Y_i)\\}_{i\\in[N]}) := \\frac{1}{N} \\sum_{i\\in[N]} \\sum_{a\\in[d]} ((W, X_{i,a}) - Y_{i,a})^2$\n$\\leq \\min_{\\Theta\\in\\Omega_H} L_{\\Theta}(\\{(Z_i, Y_i)\\}_{i\\in[N]}) + \\epsilon$ (32)\nThus the optimum of the regression with respect to the data achieves optimum of the loss to error $\\epsilon$ in time $O(d^4N)$. The sample complexity to achieve error $\\epsilon$ is then $O(\\frac{1}{\\epsilon^2} (d^4 + log(\\delta^{-1})))$ with probability $1 - \\delta$ over the data distribution. Furthermore, if we take the SVD of $W = \\sum_{i\\in[\\hat{H}]} A_i B_i^T$ where we absorb the singular values into the left and right singular vectors we have for $\\hat{\\Theta} =$"}, {"title": "B PROOFS FROM IDENTIFIABILITY SECTION", "content": "First", "H(Z)H(Z)^T": "is full rank then the model class is identifiable with respect to $D$.\nThe following is the certificate of identifiability written in an abstract form involving polynomials to map parameters to feature space and polynomials to map data to feature space. The proof does not require the model to be an MHLA", "Y_i)\\}_{i\\in[N": ""}, "be a dataset realizable by $\\Theta \\in \\Omega_H$. Let $p := \\{p_a\\}_{a\\in[d"]}, {"H_n": "mathbb{R"}, {"p": "Omega \\rightarrow \\mathbb{R}^{\\psi}$ such that $\\text{MHLA}_{\\Theta} = \\text{MHLA}_{\\Theta'}$ if and only if $p(\\Theta) = p(\\Theta')$. Then we show that any empirical risk minimizer $\\Theta_{ERM}$ and the ground truth $\\Theta$ satisfy $p(\\Theta_{ERM}) = p(\\Theta)$."}]