{"title": "SIMBA: SIMPLICITY BIAS FOR SCALING UP\nPARAMETERS IN DEEP REINFORCEMENT LEARNING", "authors": ["Hojoon Lee", "Dongyoon Hwang", "Donghu Kim", "Hyunseung Kim", "Jun Jet Tai", "Kaushik Subramanian", "Peter R.Wurman", "Jaegul Choo", "Peter Stone", "Takuma Seno"], "abstract": "Recent advances in CV and NLP have been largely driven by scaling up the num-\nber of network parameters, despite traditional theories suggesting that larger net-\nworks are prone to overfitting. These large networks avoid overfitting by integrat-\ning components that induce a simplicity bias, guiding models toward simple and\ngeneralizable solutions. However, in deep RL, designing and scaling up networks\nhave been less explored. Motivated by this opportunity, we present SimBa, an\narchitecture designed to scale up parameters in deep RL by injecting a simplic-\nity bias. SimBa consists of three components: (i) an observation normalization\nlayer that standardizes inputs with running statistics, (ii) a residual feedforward\nblock to provide a linear pathway from the input to output, and (iii) a layer nor-\nmalization to control feature magnitudes. By scaling up parameters with SimBa,\nthe sample efficiency of various deep RL algorithms\u2014including off-policy, on-\npolicy, and unsupervised methods\u2014is consistently improved. Moreover, solely\nby integrating SimBa architecture into SAC, it matches or surpasses state-of-the-\nart deep RL methods with high computational efficiency across DMC, MyoSuite,\nand HumanoidBench. These results demonstrate SimBa's broad applicability and\neffectiveness across diverse RL algorithms and environments.", "sections": [{"title": "INTRODUCTION", "content": "Scaling up neural network sizes has been a key driver of recent advancements in computer vision\n(CV) (Dehghani et al., 2023) and natural language processing (NLP) (Google et al., 2023; Achiam\net al., 2023). By increasing the number of parameters, neural networks gain enhanced expressivity,\nenabling them to cover diverse functions and discover effective solutions that smaller networks might\nmiss. However, this increased capacity also heightens the risk of overfitting, as larger networks can\nfit intricate patterns in the training data that do not generalize well to unseen data.\nDespite this risk of overfitting, empirical evidence shows that neural networks tend to converge\ntoward simpler functions that generalize effectively (Kaplan et al., 2020; Nakkiran et al., 2021).\nThis phenomenon is attributed to a simplicity bias inherent in neural networks, where standard opti-\nmization algorithms and architectural components guide highly expressive models toward solutions\nrepresenting simple, generalizable functions (Shah et al., 2020; Berchenko, 2024). For instance, gra-\ndient noise in stochastic gradient descent prevents models from converging on sharp local minima,\nhelping them avoid overfitting (Chizat & Bach, 2020; Gunasekar et al., 2018; Pesme et al., 2021).\nArchitectural components such as ReLU activations (Hermann et al., 2024), layer normalization (Ba\net al., 2016), and residual connections (He et al., 2020) are also known to amplify simplicity bias.\nThese components influence the types of functions that neural networks represent at initialization,\nwhere networks that represent simpler function at initialization are more likely to converge to simple\nfunctions (Valle-Perez et al., 2018; Mingard et al., 2019; Teney et al., 2024).\nWhile scaling up network parameters and leveraging simplicity bias have been successfully applied\nin CV and NLP, these principles have been underexplored in deep reinforcement learning (RL),\nwhere the focus has primarily been on algorithmic advancements (Hessel et al., 2018; Hafner et al.,\n2023; Fujimoto et al., 2023; Hansen et al., 2023). Motivated by this opportunity, we introduce the\nSimBa network, a novel architecture that explicitly embeds simplicity bias to effectively scale up\nparameters in deep RL. SimBa comprises of three key components: (i) an observation normalization\nlayer that standardizes inputs by tracking the mean and variance of each dimension, reducing over-\nfitting to high-variance features (Andrychowicz et al., 2020); (ii) a pre-layer normalization residual\nfeedforward block (Xiong et al., 2020), which maintains a direct linear information pathway from\ninput to output and applies non-linearity only when necessary; and (iii) a post-layer normalization\nbefore the output layer to stabilize activations, ensuring more reliable policy and value predictions.\nTo verify whether SimBa amplifies simplicity bias, we compared it against the standard MLP ar-\nchitecture often employed in deep RL. Following Teney et al. (2024), we measured simplicity bias\nby (i) sampling random inputs from a uniform distribution; (ii) generating network outputs; and\n(iii) performing Fourier decomposition on these outputs. A smaller sum of the Fourier coefficients\nindicates that the neural network represents a low-frequency function, signifying greater simplicity.\nWe define the simplicity score as the inverse of this sum, meaning a higher score corresponds to a\nstronger simplicity bias. As illustrated in Figure 2.(a), our analysis revealed that SimBa has a higher\nsimplicity score than the MLP (Further details are provided in Section 2)."}, {"title": "PRELIMINARY", "content": "Simplicity bias refers to the tendency of neural networks to prioritize learning simpler patterns over\ncapturing intricate details (Shah et al., 2020; Berchenko, 2024). In this section, we introduce metrics\nto quantify simplicity bias; in-depth definitions are provided in Appendix A."}, {"title": "MEASURING FUNCTION COMPLEXITY", "content": "We begin by defining a complexity measure $c : \\mathcal{F} \\rightarrow [0, \\infty)$ to quantify the complexity of functions\nin $\\mathcal{F} = {f|f : \\mathcal{X} \\rightarrow \\mathcal{Y}}$ where $\\mathcal{X} \\subseteq \\mathbb{R}^n$ denote the input space and $\\mathcal{Y} \\subset \\mathbb{R}^m$ the output space.\nTraditional complexity measures, such as the Vapnik-Chervonenkis dimension (Blumer et al.,\n1989) and Rademacher complexity (Bartlett & Mendelson, 2002), are well-established but of-\nten intractable for deep neural networks. Therefore, we follow Teney et al. (2024) and adopt\nFourier analysis as our primary complexity measure. Given $f(x) := (2\\pi)^{d/2} \\int \\hat{f}(k) e^{ik\\cdot x}dk$ where\n$\\hat{f}(k) := \\int f(x) e^{-ik\\cdot x}dx$ is the Fourier transform, we perform a discrete Fourier transform by uni-\nformly discretizing the frequency domain, $k \\in {0,1,..., K}$. The value of $K$ is chosen by the\nNyquist-Shannon limit (Shannon, 1949) to ensure accurate function representation. Our complexity\nmeasure $c(f)$ is then computed as the frequency-weighted average of the Fourier coefficients:\n$c(f) = \\frac{\\sum_{k=0}^K \\hat{f}(k)\\cdot k}{\\sum_{k=0}^K \\hat{f}(k)}.$                                                        (1)\nIntuitively, larger $c(f)$ indicates higher complexity due to a dominance of high-frequency compo-\nnents such as rapid amplitude changes or intricate details. Conversely, lower $c(f)$ implies a larger\ncontribution from low-frequency components, indicating a lower complexity function."}, {"title": "MEASURING SIMPLICITY BIAS", "content": "In theory, simplicity bias can be measured by evaluating the complexity of the function to which the\nnetwork converges after training. However, directly comparing simplicity bias across different archi-\ntectures after convergence is challenging due to the randomness of the non-stationary optimization\nprocess, especially in RL, where the data distribution changes continuously.\nEmpirical studies suggest that the initial complexity of a network strongly correlates with the com-\nplexity of the functions it converges to during training (Valle-Perez et al., 2018; De Palma et al.,\n2019; Mingard et al., 2019; Teney et al., 2024). Therefore, for a given network architecture $f$ with\nan initial parameter distribution $\\Theta$, we define the simplicity bias score $s(f) : \\mathcal{F} \\rightarrow (0,\\infty)$ as:\n$s(f) \\approx E_{\\Theta \\sim \\Theta_0} [\\frac{1}{c(f_{\\Theta})}].$                                                                                (2)\nwhere $f_{\\Theta}$ denotes the network architecture $f$ parameterized by $\\Theta$.\nThis measure indicates that networks with lower complexity at initialization are more likely to ex-\nhibit a higher simplicity bias, thereby converging to simpler functions during training."}, {"title": "RELATED WORK", "content": ""}, {"title": "SIMPLICITY BIAS", "content": "Initially, simplicity bias was mainly attributed to the implicit regularization effects of the stochastic\ngradient descent (SGD) optimizer (Soudry et al., 2018; Gunasekar et al., 2018; Chizat & Bach, 2020;"}, {"title": "DEEP REINFORCEMENT LEARNING", "content": "For years, deep RL has largely focused on algorithmic improvements to enhance sample efficiency\nand generalization. Techniques like Double Q-learning (Van Hasselt et al., 2016; Fujimoto et al.,\n2018), and Distributional RL (Dabney et al., 2018) have improved the stability of value estima-\ntion by reducing overestimation bias. Regularization strategies\u2014including periodic reinitialization\n(Nikishin et al., 2022; Lee et al., 2024), Layer Normalization (Lee et al., 2023; Gallici et al., 2024),\nBatch Normalization (Bhatt et al., 2024), and Spectral Normalization (Gogianu et al., 2021)-have\nbeen employed to prevent overfitting and enhance generalization. Incorporating self-supervised ob-\njectives with model-based learning (Fujimoto et al., 2023; Hafner et al., 2023; Hansen et al., 2023)\nhas further improved representation learning and sample efficiency.\nDespite these advances, scaling up network architectures in deep RL remains underexplored. While\nlarger models improved performance in supervised learning by leveraging simplicity bias, this prin-\nciple has not been fully explored in RL. Several recent studies have attempted to scale up network\nsizes-through ensembling (Chen et al., 2021; Obando-Ceron et al., 2024), widening (Schwarzer\net al., 2023), and deepening networks (Bjorck et al., 2021; Nauman et al., 2024). However, they\noften rely on computationally intensive layers like spectral normalization (Bjorck et al., 2021) or\nrequire sophisticated training protocols (Nauman et al., 2024), limiting their applicability.\nIn this work, we aim to design an architecture that amplifies simplicity bias, enabling us to effectively\nscale up parameters in RL, independent of using any other sophisticated training protocol."}, {"title": "SIMBA", "content": "This section introduces the SimBa network, an architecture designed to embed simplicity bias\ninto deep RL. The architecture comprises Running Statistics Normalization, Residual Feedforward\nBlocks, and Post-Layer Normalization. By amplifying the simplicity bias, SimBa allows the model\nto avoid overfitting for highly overparameterized configurations."}, {"title": "Running Statistics Normalization (RSNorm)", "content": "First, RSNorm standardizes input observations by\ntracking the running mean and variance of each input dimension during training, preventing features\nwith disproportionately large values from dominating the learning process.\nGiven an input observation $o_t \\in \\mathbb{R}^{d_o}$ at timestep $t$, we update the running observation mean $\\mu_{\\epsilon} \\in\n\\mathbb{R}^{d_o}$ and variance $\\sigma_{\\epsilon} \\in \\mathbb{R}^{d_o}$ as follows:\n$\\mu_t = \\mu_{t-1} + \\frac{1}{t} \\delta_t, \\ \\sigma_t^2 = (\\sigma_{t-1}^2 + \\frac{1}{t} \\delta_t^2)$                                                                           (3)\nwhere $\\delta_t = o_t - \\mu_{t-1}$ and $d_o$ denotes the dimension of the observation."}, {"title": "Residual Feedforward Block", "content": "The normalized observation $\\bar{o}_t$ is first embedded into a $d_h$-\ndimensional vector using a linear layer:\n$x = Linear(\\bar{o}_t).$                                                                                              (5)\nAt each block, $l \\in {1, ..., L}$, the input $x$ passes through a pre-layer normalization residual feed-\nforward block, introducing simplicity bias by allowing a direct linear pathway from input to output.\nThis direct linear pathway enables the network to pass the input unchanged unless non-linear trans-\nformations are necessary. Each block is defined as:\n$x^{l+1} = x^l + MLP(LayerNorm(x^l)).$                                                                                  (6)\nFollowing Vaswani (2017), the MLP is structured with an inverted bottleneck, where the hidden\ndimension is expanded to $4 \\cdot d_h$ and a ReLU activation is applied between the two linear layers."}, {"title": "Post-Layer Normalization", "content": "To ensure that activations remain on a consistent scale before predicting\nthe policy or value function, we apply layer normalization after the final residual block:\n$z_t = LayerNorm(x^L).$                                                                                                (7)\nThe normalized output $z_t$ is then processed through a linear layer, generating predictions for the\nactor's policy or the critic's value function."}, {"title": "ANALYZING SIMBA", "content": "In this section, we analyze whether each component of SimBa amplifies simplicity bias and allows\nscaling up parameters in deep RL. We conducted experiments on challenging environments in DMC\ninvolving Humanoid and Dog, collectively referred to as DMC-Hard. Throughout this section, we\nused Soft Actor Critic (Haarnoja et al., 2018, SAC) as the base algorithm."}, {"title": "ARCHITECTURAL COMPONENTS", "content": "To quantify simplicity bias in SimBa, we use Fourier analysis as described in Section 2. For each\nnetwork $f$, we estimate the simplicity bias score $s(f)$ by averaging over 100 random initializations\n$\\Theta \\sim \\Theta_0$. Following Teney et al. (2024), the input space $\\mathcal{X} = [-100,100]^2 \\subset \\mathbb{R}^2$ is divided into a\ngrid of 90,000 points, and the network outputs a scalar value for each input which are represented\nas a grayscale image. By applying the discrete Fourier transform to these outputs, we compute the\nsimplicity score $s(f)$ defined in Equation 2 (see Appendix B for further details)."}, {"title": "COMPARISON WITH OTHER ARCHITECTURES", "content": "To assess SimBa's scalability, we compared it to BroNet (Nauman et al., 2024), SpectralNet (Bjorck\net al., 2021), and MLP. Detailed descriptions of each architecture are provided in Appendix B.\nThe key differences between SimBa and other architectures lie in the placement of components\nthat promote simplicity bias. SimBa maintains a direct linear residual pathway from input to out-\nput, applying non-linearity exclusively through residual connections. In contrast, other architectures\nintroduce non-linearities within the input-to-output pathway, increasing functional complexity. Ad-\nditionally, SimBa employs post-layer normalization to ensure consistent activation scales across all\nhidden dimensions, reducing variance in policy and value predictions. Conversely, other architec-\ntures omit normalization before the output layer, which can lead to high variance in their predictions.\nTo ensure that performance differences were attributable to architectural design choices rather than\nvarying input normalization strategies, we uniformly applied RSNorm as the input normalization\nmethod across all models. Additionally, our investigation primarily focused on scaling the critic\nnetwork's hidden dimension, as scaling the actor network showed limited benefits (see Section7.2)."}, {"title": "EXPERIMENTS", "content": "This section evaluates SimBa's applicability across various deep RL algorithms and environments.\nFor each baseline, we either use the authors' reported results or run experiments using their rec-\nommended hyperparameters. Detailed descriptions of the environments used in our evaluations are\nprovided in Appendix F. In addition, we also include learning curves and final performance for each\ntask in Appendix H and I, along with hyperparameters used in our experiments in Appendix G."}, {"title": "OFF-POLICY RL", "content": "Experimental Setup. We evaluate the algorithms on 51 tasks across three benchmarks: DMC (Tassa\net al., 2018), MyoSuite (Caggiano et al., 2022), and HumanoidBench (Sferrazza et al., 2024). The"}, {"title": "Integrating SimBa into Off-Policy RL", "content": "The SimBa ar-\nchitecture is algorithm-agnostic and can be applied to any\ndeep RL method. To demonstrate its applicability, we in-\ntegrate SimBa into two model-free (SAC, DDPG) and one\nmodel-based algorithm (TD-MPC2), evaluating their per-\nformance on the DMC-Hard benchmark. For SAC and\nDDPG, we replace the standard MLP-based actor-critic\nnetworks with SimBa. For TD-MPC2, we substitute the\nshared encoder from MLP to SimBa while matching the\nnumber of parameters as the original implementation."}, {"title": "Comparisons with State-of-the-Art Methods", "content": "Here, we compare SAC + SimBa against state-of-\nthe-art off-policy deep RL algorithms, which demonstrated the most promising results in previous\nexperiments. Throughout this section, we refer to SAC + SimBa as SimBa.\nFigure 8 presents the results, where the x-axis represents computation time using an RTX 3070 GPU,\nand the y-axis denotes performance. Points in the upper-left corner ($ \\nwarrow $) indicate higher compute\nefficiency. Here, SimBa consistently outperforms most baselines across various environments while\nrequiring less computational time. The only exception is TD-MPC2 on HumanoidBench; however,\nTD-MPC2 requires 2.5 times the computational resources of SimBa to achieve better performance,\nmaking SimBa the most efficient choice overall.\nThe key takeaway is that SimBa achieves these remarkable results without the bells and whis-\ntles often found in state-of-the-art deep RL algorithms. It is easy to implement, which only re-\nquires modifying the network architecture into SimBa without additional changes to the loss func-\ntions (Schwarzer et al., 2020; Hafner et al., 2023) or using domain-specific regularizers (Fujimoto\net al., 2023; Nauman et al., 2024). Its effectiveness stems from an architectural design that leverages\nsimplicity bias and scaling up the number of parameters, thereby maximizing performance."}, {"title": "Impact of Input Dimension", "content": "To further identify in which cases SimBa offers significant benefits,\nwe analyze its performance across DMC environments with varying input dimensions. As shown in\nFigure 9, the advantage of SimBa becomes more pronounced as input dimensionality increases. We\nhypothesize that higher-dimensional inputs exacerbate the curse of dimensionality, and the simplic-\nity bias introduced by SimBa effectively mitigates overfitting in these high-dimensional settings."}, {"title": "ON-POLICY RL", "content": "Experimental Setup. We conduct our on-policy RL experiments in Craftax (Matthews et al., 2024),\nan open-ended environment inspired by Crafter (Hafner, 2022) and NetHack (K\u00fcttler et al., 2020).\nCraftax poses a unique challenge with its open-ended structure and compositional tasks. Following\nMatthews et al. (2024), we use Proximal Policy Optimization (Schulman et al., 2017, PPO) as the\nbaseline algorithm. When integrating SimBa with PPO, we replace the standard MLP-based actor-\ncritic networks with SimBa and train both PPO and PPO + SimBa on 1024 parallel environments\nfor a total of 1 billion environment steps.\nResults. As illustrated in Figure 10, integrating SimBa into PPO significantly enhances performance\nacross multiple complex tasks. With SimBa, the agent learns to craft iron swords and pickaxes\nmore rapidly, enabling the early acquisition of advanced tools. Notably, by using these advanced\ntools, the SimBa-based agent successfully defeats challenging adversaries like the Orc Mage using\nsignificantly fewer time steps than an MLP-based agent. These improvements arise solely from the\narchitectural change, demonstrating the effectiveness of the SimBa approach for on-policy RL."}, {"title": "UNSUPERVISED RL", "content": "Experimental Setup. In our unsupervised RL study, we incorporate SimBa for online skill dis-\ncovery, aiming to identify diverse behaviors without relying on task-specific rewards. We focus our\nexperiments primarily on METRA (Park et al., 2023), which serves as the state-of-the-art algorithm\nin this domain. We evaluate METRA and METRA with SimBa on the Humanoid task from DMC,\nrunning 10M environment steps. For a quantitative comparison, we adopt state coverage as our main\nmetric. Coverage is measured by discretizing the x and y axes into a grid and counting the number of\ngrid cells covered by the learned behaviors at each evaluation epoch, following prior literature (Park\net al., 2023; Kim et al., 2024a;b)."}, {"title": "ABLATIONS", "content": "We conducted ablations on DMC-Hard with SAC + SimBa, running 5 seeds for each experiment."}, {"title": "OBSERVATION NORMALIZATION", "content": "A key factor in SimBa's success is using RSNorm for observation normalization. To validate its ef-\nfectiveness, we compare 5 alternative methods: (i) Layer Normalization (Ba et al., 2016); (ii) Batch\nNormalization (Ioffe & Szegedy, 2015); (iii) Env Wrapper RSNorm, which tracks running statis-\ntics for each dimension, normalizes observations upon receiving from the environment, and stores\nthem in the replay buffer; (iv) Initial N Steps: fixed statistics derived from initially collected tran-\nsition samples, where we used $N$ = 5,000; and (v) Oracle Statistics, which rely on pre-computed\nstatistics from previously collected expert data.\nAs illustrated in Figure 12, layer normalization and batch\nnormalization offer little to no performance gains. While\nthe env-wrapper RSNorm is somewhat effective, it falls short\nof RSNorm's performance. Although widely used in deep\nRL frameworks (Dhariwal et al., 2017; Hoffman et al., 2020;\nRaffin et al., 2021), the env-wrapper introduces inconsisten-\ncies in off-policy settings by normalizing samples with dif-\nferent statistics based on their collection time. This causes\nidentical observations to be stored with varying values in\nthe replay buffer, reducing learning consistency. Fixed ini-\ntial statistics also show slightly worse performance than\nRSNorm, potentially due to their inability to adapt to the\nevolving dynamics during training. Overall, only RSNorm\nmatched the performance of oracle statistics, making it the\nmost practical observation normalization choice for deep RL."}, {"title": "SCALING THE NUMBER OF PARAMETERS", "content": "Here, we investigate scaling parameters in the actor-critic architecture with SimBa by focusing on\ntwo aspects: (i) scaling the actor versus the critic network, and (ii) scaling network width versus\ndepth. For width scaling, the actor and critic depths are set to 1 and 2 blocks, respectively. For depth\nscaling, the actor and critic widths are fixed at 128 and 512, respectively, following our default setup."}, {"title": "SCALING REPLAY RATIO", "content": "According to scaling laws (Kaplan et al., 2020), performance can be enhanced not only by scaling\nup the number of parameters but also by scaling up the computation time, which can be done by\nscaling up the number of gradient updates per collected sample (i.e., replay ratio) in deep RL.\nHowever, in deep RL, scaling up the replay ratio has been shown to decrease performance due to the\nrisk of overfitting the network to the initially collected samples (Nikishin et al., 2022; D'Oro et al.,\n2022; Lee et al., 2023). To address this issue, recent research has proposed periodically reinitializing\nthe network to prevent overfitting, demonstrating that performance can be scaled with respect to the\nreplay ratio. In this section, we aim to assess whether the simplicity bias induced by SimBa can\nmitigate overfitting under increased computation time (i.e., higher replay ratios).\nTo evaluate this, we trained SAC with SimBa using 2,\n4, 8, and 16 replay ratios, both with and without peri-\nodic resets. For SAC with resets, we reinitialized the en-\ntire network and optimizer every 500,000 gradient steps.\nWe also compared our results with BRO (Nauman et al.,\n2024), which incorporates resets.\nSurprisingly, as illustrated in Figure 14, SimBa's per-\nformance consistently improves as the replay ratio in-\nc"}, {"title": "LESSONS AND OPPORTUNITIES", "content": "Lessons. Historically, deep RL has suffered from overfitting, necessitating sophisticated training\nprotocols and implementation tricks to mitigate these issues (Hessel et al., 2018; Fujimoto et al.,\n2023). These complexities serve as significant barriers for practitioners with limited resources, hin-\ndering the effective usage of deep RL methods. In this paper, we improved performance by solely\nmodifying the network architecture while keeping the underlying algorithms unchanged. This ap-\nproach simplifies the implementation of SimBa, making it easy to adopt. By incorporating simplicity\nbias through architectural design and scaling up the number of parameters, the network converges\nto simpler, generalizable functions, matching or surpassing state-of-the-art deep RL methods. We\nalso believe that SimBa follows the insights from Richard Sutton's Bitter Lesson (Sutton, 2019): al-\nthough task-specific designs can yield immediate improvements, an approach that scales effectively\nwith increased computation may provide more sustainable benefits over the long term.\nOpportunities. Our exploration of simplicity bias has primarily concentrated on the network ar-\nchitecture; however, optimization techniques are equally vital. Strategies such as dropout (Hiraoka\net al., 2021), data augmentation (Kostrikov et al., 2020), and diverse optimization algorithms (Foret\net al., 2020) can further enhance convergence to simpler functions during training. Integrating these\ntechniques with continued parameter scaling presents a promising avenue for future research in\ndeep RL. Furthermore, while our focus has been on the basic model-free algorithm SAC, there ex-\nists considerable empirical success with model-based algorithms (Schwarzer et al., 2020; Hansen\net al., 2023; Hafner et al., 2023). Preliminary investigations into applying SimBa to model-based\nRL, specifically TD-MPC2 (Hansen et al., 2023), have shown promise. We encourage the research\ncommunity to collaboratively pursue these avenues to advance deep RL architectures and facilitate\ntheir successful application in real-world scenarios."}, {"title": "REPRODUCIBILITY", "content": "To ensure the reproducibility of our experiments, we provide the complete source code to run SAC\nwith SimBa. Moreover, we have included a Dockerfile to simplify the testing and replication of our\nresults. The raw scores of each experiment including all baselines are also included. The code is\navailable at https://github.com/SonyResearch/simba."}, {"title": "PLASTICITY ANALYSIS", "content": "Recent studies have identified the loss of plasticity as a significant challenge in non-stationary train-\ning scenarios, where neural networks gradually lose their ability to adapt to new data over time\n(Nikishin et al., 2022; Lyle et al., 2023; Lee et al., 2024). This phenomenon can severely impact the\nperformance and adaptability of models in dynamic learning environments such as RL. To quantify\nand understand this issue, several metrics have been proposed, including stable-rank (Kumar et al.,\n2022), dormant ratio (Sokar et al., 2023), and L2-feature norm (Kumar et al., 2022).\nThe stable-rank assesses the rank of the feature matrix, reflecting the diversity and richness of the\nrepresentations learned by the network. This is achieved by performing an eigen decomposition\non the covariance matrix of the feature matrix and counting the number of singular values $\\sigma_j$ that\nexceed a predefined threshold $\\tau$:\ns-Rank = $\\sum_{j=1}^m \\mathbb{I}(\\sigma_j > \\tau)$                                                                                                    (16)\nwhere $F \\in \\mathbb{R}^{d \\times m}$ is the feature matrix with $d$ samples and $m$ features, $\\sigma_j$ are the singular values,\nand $\\mathbb{I}(\\cdot)$ is the indicator function.\nThe dormant ratio measures the proportion of neurons with negligible activation. It is calculated as\nthe number of neurons with activation norms below a small threshold $\\epsilon$ divided by the total number\nof neurons $D$:\nDormant Ratio = $\\frac{|{i | ||a_i|| < \\epsilon}|}{D}$                                                                                     (17)\nwhere $a_i$ represents the activation of neuron $i$.\nThe L2-feature norm represents the average L2 norm of the feature vectors across all samples:\nFeature Norm = $\\frac{1}{N} \\sum_{i=1}^N ||F_i||^2$                                                                                  (18)\nwhere $F_i$ is the feature vector for the i-th sample. Large feature norms can signal overactive neurons,\npotentially leading to numerical instability.\nTo assess how different architectures impact plasticity, we conducted experiments in DMC-Hard.\nWe compared Soft Actor-Critic (SAC) implemented with a standard MLP architecture against SAC\nwith the SimBa architecture. Each configuration was evaluated across 5 random seeds."}, {"title": "COMPARISON TO EXISTING ARCHITECTURES", "content": "Here we provide a more in-depth discussion related to the architectural difference between SimBa,\nBroNet, and SpectralNet. As shown in Figure 16, SimBa differs from BroNet in three key aspects:\n(i) the inclusion of RSNorm, (ii) the implementation of pre layer-normalization, (iii) the utilization\nof a linear residual pathway, (iv) and the inclusion of a post-layer normalization layer. Similarly,\ncompared to SpectralNet, SimBa incorporates RSNorm, employs a linear residual pathway, and\nleverages spectral normalization differently."}, {"title": "COMPUTATIONAL RESOURCES", "content": "The training was performed using NVIDIA RTX 3070, A100, or H100 GPUs for neural network\ncomputations and either a 16-core Intel i7-11800H or a 32-core AMD EPYC 7502 CPU for running\nsimulators. Our software environment included Python 3.10, CUDA 12.2, and Jax 4.26.\nWhen benchmarking computation time, experiments were conducted on a same hardware equipped\nwith an NVIDIA RTX 3070 GPU and 16-core Intel i7-11800H CPU."}, {"title": "ENVIRONMENT DETAILS", "content": "This section details the benchmark environments used in our evaluation. We list all tasks from each\nbenchmark along with their observation and action dimensions. Visualizations of each environment\nare provided in Figure 6, and detailed environment information is available in Table 1."}, {"title": "DEEPMIND CONTROL SUITE", "content": "DeepMind Control Suite (Tassa et al., 2018, DMC) is a standard benchmark for continuous con-\ntrol, featuring a range of locomotion and manipulation tasks with varying complexities, from simple\nlow-dimensional tasks ($s \\in \\mathbb{R}^3$) to highly complex ones ($s \\in \\mathbb{R}^{223}$). We evaluate 27 DMC tasks, cat-\negorized into two groups: DMC-Easy&Medium and DMC-Hard. All Humanoid and Dog tasks are\nclassified as DMC-Hard, while the remaining tasks are grouped under DMC-Easy&Medium. The\ncomplete lists of DMC-Easy&Medium and DMC-Hard are provided in Tables 2 and 3, respectively.\nFor our URL experiments, we adhere to the protocol in Park et al. (2023), using an episode length\nof 400 steps and augmenting the agent's observations with its x, y, and z coordinates. For the\nrepresentation function $\\phi$ (i.e., reward network) in METRA, we only used x, y, and z positions as\ninput. This approach effectively replaces the colored floors used in the pixel-based humanoid setting\nof METRA with state-based inputs."}, {"title": "MYOSUITE", "content": "MyoSuite (Caggiano et al., 2022) simulates musculoskeletal movements with high-dimensional state\nand action spaces, focusing on physiologically accurate motor control. It includes benchmarks for\ncomplex object manipulation using a dexterous hand. We evaluate 10 MyoSuite tasks, categorized\nas \"easy\" when the goal is fixed and \"hard\" when the goal is randomized, following Hansen et al.\n(202"}]}