{"title": "COGEN: Learning from Feedback with Coupled Comprehension and Generation", "authors": ["Mustafa Omer Gul", "Yoav Artzi"], "abstract": "Systems with both language comprehension and generation capabilities can benefit from the tight connection between the two. This work studies coupling comprehension and generation with focus on continually learning from interaction with users. We propose techniques to tightly integrate the two capabilities for both learning and inference. We situate our studies in two-player reference games, and deploy various models for thousands of interactions with human users, while learning from interaction feedback signals. We show dramatic improvements in performance over time, with comprehension-generation coupling leading to performance improvements up to 26% in absolute terms and up to 17% higher accuracies compared to a non-coupled system. Our analysis also shows coupling has substantial qualitative impact on the system's language, making it significantly more human-like.", "sections": [{"title": "Introduction", "content": "Language comprehension and generation are closely related processes. Indeed, observations such as the ability to finish incomplete partner utterances in dialogue (Clark and Wilkes-Gibbs, 1986; Howes et al., 2011), as well as neuroscientific evidence (Paus et al., 1996; Opitz et al., 2003; Menenti et al., 2011) have led to integrated accounts of comprehension and generation in cognitive science (Pickering and Garrod, 2013; Pickering and Gambi, 2018), where processes related to generation are active during comprehension and vice versa. This suggests a potential for coupling the two in computational systems, and creating a virtuous cycle, where the improvement of one capability drives learning and performance in the other. This is particularly compelling in systems that continually learn and improve through interaction with users, where the dynamics between the two capabilities play out over time.\nWe study the dynamics of this coupling in a continual learning\u00b9 setting, where trends in learning and behavior can be observed over time. We design an interaction scenario where models can take both listener (comprehension) and speaker (generation) roles, and receive feedback while interacting with human partners. We couple comprehension and generation through several mechanisms, and observe the impact this coupling has on the long-term dynamics of performance and language.\nWe instantiate comprehension and generation as the listener and speaker roles of a two-player reference game (Krauss and Weinheimer, 1964; Clark and Wilkes-Gibbs, 1986) involving abstract visual stimuli (Ji et al., 2022), which remain challenging"}, {"title": "Interaction Scenario and Overview", "content": "We study the coupling of comprehension and generation by training and deploying an agent that interacts with human users, and continually learns from these interactions. This allows us to observe how the interplay between comprehension and generation evolves over time, and what the long-term effects of coupling the two processes are.\nInteraction Scenario We use a reference game as our interaction scenario (Figure 1). Each game involves two players: a speaker and listener. Both"}, {"title": "Continual Learning", "content": "We combine the continual learning approaches of Kojima et al. (2021) (generation) and Suhr and Artzi (2024) (comprehension). Both approaches map feedback to rewards, treat learning as a contextual bandit problem, and use REINFORCE (Williams, 1992), a relatively simple policy gradient algorithm. We adopt these design choices. A key difference of our process is that we combine the comprehension and generation objectives to train a single model.\nDeployment and learning are interleaved. Each round p starts with deploying the model parameterized by \u03b8p to collect interactions with humans. We record feedback signals from these interactions. Upon collecting a set of interactions, we re-train the model given all data collected so far to estimate new parameters 0p+1. The model is then deployed for the next round, and the process continues."}, {"title": "Feedback Collection", "content": "Feedback collection is part of the model interacting with human partners (i.e., the system deployment), and differs depending on the model's role. As the listener, the model is given context I and a human-generated utterance u and predicts the index of the target image t = arg maxt P\u2081(t|I, u; \u03b8\u03c1). The game then indicates if the selection was correct or not, and terminates. We treat this indication as feedback, and directly map it to a binary reward to create a comprehension datapoint: (I, u, t, r), with r = 1 upon game success and r = -1 otherwise.\nLikewise, as the speaker, the model samples an utterance \u00fb ~ P\u2084(u|I, t; 0p) given context I and tar-"}, {"title": "Learning", "content": "We estimate the next round's model parameters 0p+1 by re-training from the initial weights (i.e., the original IDEFICS2 weights). The comprehension training dataset is a union of all collected feedback data so far Di,<p = U=1 Di,i. The production task dataset Ds,<p is similarly defined.\nWe frame learning as a contextual bandit problem with a multi-task additive objective combining the comprehension and generation components. We optimize with a REINFORCE-style policy gradient algorithm (Williams, 1992). This choice follows prior work (Kojima et al., 2021; Suhr and Artzi, 2024), and is motivated by the simplicity of REINFORCE, critical in a setting where humans are part of the iterative learning process. The gradient for a comprehension example (I, u, t, r) ~ Di,<p collected at round m is:\n\u0394\u03b9 = car\u2207 log P\u2081(t|I, u; 0),\nwhere ca is the cased inverse propensity score (IPS) coefficient introduced by Kojima et al. (2021) to mitigate the effect of negative examples (i.e., r = -1) allowing for unbounded loss:\nCl = { (\u03a1\u03b9(\u0399,\u03c5;0) / Pi(t1,u;0m))   if r = -1, 1 otherwise, }  ,\nwhere P\u2081(t|I, u; 0m) is the probability of the target t when it was sampled during the interaction at round m. Without this coefficient, negative examples (i.e., r = -1) can dominate the loss and destabilize learning as their probabilities decrease, because limp()\u2192o log P\u2081(\u00b7) = \u2212\u221e. The coefficient ci decreases the importance of such examples as their probability decreases. The gradient \u2206, for generation datapoints (I, \u00fb, t, r) is identical, except using the generation distribution P\u2084(\u00fb|I, t; 0)."}, {"title": "Coupling Comprehension and Generation", "content": "We couple comprehension and production during both learning and inference. We also use one model for both tasks, creating a coupling at the parameter level, which is common in contemporary methods, partially due to high memory needs."}, {"title": "Learning with Data Sharing", "content": "We convert comprehension datapoints to generation datapoints, and vice versa, to fully utilize the data models are exposed to in interactions. For example, consider the case of an agent in the role of a listener. If the speaker partner generates the utterance the target is a swan facing right and the listener correctly guesses the target image (as in Figure 2), the listener does not only receive positive feedback for their guess, but also can learn that a swan facing right is a valid description for the current context-target pair.\nGiven datasets for comprehension Di,p and generation Ds,p collected at round p, we expand both:\nDl,p = Di,p \u222a {(I, \u00fb, t,r) \u2208 Ds,p | r = 1},\nDs,p = Ds,p \u222a {(I, u, t,r) \u2208 D\u0131,p | r = 1}.\nWe only convert positively labeled feedback (r = 1), because we generally find positive rewards to be more reliable. A negative reward for a generated utterance could be because the utterance is incorrect or ambiguous, or the human listener made a mistake. The listener task is essentially classification. Creating a comprehension example with negative reward from such an example indicates to the model the utterance is a valid description for another target. This is a misleading signal, and in early pilot studies we found it not to be helpful, so we only convert examples with positive reward.\nAn important result of this process is introducing human language into the training data of the speaker model. Generally, if a generating model learns from feedback only (Kojima et al., 2021), it is only exposed to language it has generated. This can lead to its language drifting from human language, even if its accuracy and legibility to human partners increase. Taking advantage of human utterances for the purpose of generation training opens up this closed system. We further discuss this in our results and analysis (Section 6)."}, {"title": "Joint Inference", "content": "We couple the two distributions P\u2081 and Ps during inference by sampling from one distribution (i.e., Pr in the case of comprehension) and then re-rank with a weighted geometric mean of the two distributions. The weight controlling the geometric mean is a hyper-parameter: A for generation and \u03bb\u03b9 for comprehension. In the case of comprehension, the joint probability distribution is:\nPi (t|I, u; 0) = (P\u2081(t|I, u; 0)\u03bb\u03b9 * P\u2084(u|I, t;0)1\u2212\u03bb\u03b9) / (\u03a3_{t'=1}^{N} Pi(t'|1, \u03ba; \u03b8)\u03bb\u03b9 * Ps(u|I, t'; 0)1\u2212\u03bb\u03b9),\nwhere N is the number of targets. The joint generation distribution P (u|T, t; 0) is defined in a similar fashion, but with the \u5165 hyperparameter. Enumerating all possible utterances for the normalization of the joint generation distribution is intractable, so we sample k utterances from Ps(u|I, t; 0) and sum over them to compute the normalization. In the case of comprehension, we can compute the joint distribution exactly because the number of outputs is small (i.e., 10 targets). However, if the number of targets was intractably large, the same approximation could also be performed for comprehension.\nIn practice, we observe the multiplicative generation distribution to skew inference heavily towards short utterances when doing joint inference, and find A = 0 to be the best combination for the joint generation distribution (Section 5). Although this eliminates the term Ps from the joint probability, Ps is still influential as the source of samples.\nThis joint formulation is similar to a rational speech act model (RSA; Goodman and Frank, 2016) with a single level of recursion. RSA is a model of pragmatic reasoning, and has been evaluated extensively in reference games (Cohn-Gordon et al., 2018; McDowell and Goodman, 2019). We analyze this property for our speaker model in Section 6.2. Our approximation of the joint speaker distribution is inspired by similar approaches that were applied to RSA (Fried et al., 2018a)."}, {"title": "Experimental Setup", "content": "Game Construction We construct reference game contexts using the KILOGRAM dataset (Ji et al., 2022) of 1,016 abstract tangram shapes. Each context comprises 10 images drawn from this dataset. We use a CLIP model (Radford et al., 2021) finetuned on KILOGRAM annotations from Ji et al. (2022) to ensure visual similarity between images in each context and increase task difficulty. Appendix B provides further details.\nModel and Initialization We fine-tune the instruction-tuned IDEFICS2-8B (Lauren\u00e7on et al., 2024). The tasks are delineated via prompting. Training hyperparameters are kept fixed throughout different continual learning rounds and system variants. For systems with joint inference, we set XL = 0.5 and As = 0. Appendix A details prompt design, hyperparameters, and training. Before the first round of interactions, we initialize the model by fine-tuning IDEFICS2 with a small set of 104 successful human-human games. We also add this data to the later rounds of re-training, by assigning all these examples a reward of 1. We use 280 successful human-human games as a validation set for model selection throughout our experiments.\nSystem Variants We refer to our proposed system coupling comprehension and generation with joint inference and data sharing as FULL. We compare against three other systems: ablations without data sharing (No-DS; Section 4.1) or joint inference (NO-JI; Section 4.2), and a baseline that uses neither (BASELINE). We additionally collect human-human interaction data (HUMAN) to contextualize performance over time relative to human performance. We also use this human-human data for language analysis.\nDeployment We conduct four rounds of deployment, including interactions with human partners and learning. All interactions for each round are collected concurrently in a randomized experiment. We collect an equal number of interactions for the speaker and listener roles for each system and round. We collect 2,000 interactions for each role for each system in the first round, and increase the number by 500 each round, as the marginal benefit of more examples decreases as the data grows. Because data sharing is not applicable for the first round, the FULL and NO-DS, and the NO-JI and BASELINE systems are identical on the first round.\nWe deploy our systems to interact with human workers on MTurk, at a total cost of $12,980USD. Appendix E provides crowdsourcing details.\nEvaluation At each round, we evaluate comprehension performance from interactions in the listener role using the target selection accuracy. We"}, {"title": "Results and Analysis", "content": "We focus on two broad questions: (a) does coupling influence the rate of improvement on task performance (Section 6.1) and (b) does it lead to quantifiable differences in the generated language over time (Section 6.2). Overall, we find the answer to both questions is positive, with strong effects."}, {"title": "Performance Analysis", "content": "Figure 3 shows model performance over time. All systems show dramatic improvement in performance for both comprehension and generation. Immediately, we observe significant effect from joint inference, with FULL and No-DS outperforming NO-JI and BASELINE on the first round (53.31% vs. 42.64% comprehension, 52.00% vs. 48.45% generation). FULL achieves the highest performance at the end of the study, with comprehension improving 53.31\u219272.79% (19.48% absolute improvement) and generation 52.00\u219278.07% (26.07% improvement). For generation, FULL shows the biggest performance delta, even though it starts with already higher performance compared to variants without joint inference. With comprehension, No-JI (42.64\u219266.86%) shows the biggest delta (24.22%). Coupling dramatically increases learning sample efficiency: FULL at the second round already performs better than BASELINE at the end of study, even though it trained on less than one third of the data BASELINE has seen at the end.\nOverall, the gap in performance between FULL and BASELINE only increases over time. For comprehension, the gap widens 10.67\u219214.80%, but it is much more dramatic for generation with 3.55\u219217.10%. Both coupling strategies play a role in this widening gap in performance, but between the two strategies the relation changes over time. Although No-DS starts with higher performance than NO-JI, they are essentially equivalent at the end, with NO-JI showing a trend of outperforming No-DS. This may be because NO-JI is exposed to more data from the opposing role with data sharing, compensating for the lack of joint inference.\nUser adaptation is an important potential confounder, potentially explaining any improvements in system performance. During the final round, we deploy the initial FULL model in a concurrent randomized deployment. We observe that human adaptation cannot explain model improvement, seeing very limited improvement due to adaptation: 0.42% and 2.56% for comprehension and generation (cross and dashed curve in Figure 3).\nDuring deployment, a recurring complaint from workers was about the models' inconsistent spatial reasoning, echoing recent evaluations of vision-"}, {"title": "Language Analysis", "content": "We study trends in language use over time. Throughout this section, except the pragmatic reasoning analysis, we eliminate factors that can complicate the analysis by generating new utterances on the same set of context-target pairs per round for all systems. We randomly sample 2,000 context-target pairs from the human-human games for each round, and generate utterances for them with each system using the same inference process as during deployment. Figure 5 plots the observed trends.\nWe observe a decrease in utterance length for all variants. Humans also show a downward trend in length, likely reflecting the participants becoming experts and therefore more economical in their language. This is a known phenomenon in reference games (Krauss and Weinheimer, 1964; Clark and Wilkes-Gibbs, 1986), and was also observed in other collaborative scenarios (Effenberger et al., 2021). FULL and NO-JI track the human trends best, but generally generate shorter utterances throughout.\nThe effective vocabulary of all systems, that is the number of unique words generated for the set of context-target pairs, is also decreasing. This has been observed in prior studies for generation systems that are exposed only to their output in continual learning (Kojima et al., 2021). We expected this effect to be less strong or even reversed once the system is exposed to human utterances, either through data sharing or through joint inference with a comprehension model trained on human utterances. The decrease in the vocabulary size is much smaller for the coupled variants, and the smallest for FULL, but it remains present. We also plot, for each round, how many words a model added to the cumulative set of words it generated until that round (third panel). More new words appear for the coupled variants throughout the study. All systems display a significantly less rich vocabulary compared to humans, leaving an important direction for future work.\nWe use MAUVE (Pillutla et al., 2021), a reference-less generation evaluation metric, to evaluate the similarity of each model's language to human language. For each round and system, we compute the metric between the model- and human-produced utterances for that round. We use GPT2-Large as the embedding model (Radford et al., 2019), similar to Pillutla et al. (2021), and keep the number of clusters fixed at 200. We find coupling avoids the drift from human language the BASELINE displays. The FULL system not only does not stray further from human language, but actually moves closer to it over time. Data sharing is particularly critical, but the combination of joint inference further helps to align the model language with human language.\nFinally, we briefly look into whether coupling affects the model's pragmatic reasoning. In reference games, the pragmatic information is the images in the context that are not the target. A speaker that employs pragmatic reasoning well will take into account the other images so to help the speaker make the right selection in the specific context they share (i.e., the speaker will refer to properties of the target that specifically distinguish from the other images). We operationalize this question by measuring the diversity of model descriptions for a specific tangram within different context sets. We use the Shape Naming Divergence (SND) metric, introduced by (Ji et al., 2022) to measure the diversity of human annotations for individual tangrams. Roughly speaking, high SND means high lexical diversity between the descriptions of a specific image. For each system and each round, we generate utterances for every context-target pair observed in all human-human games throughout continual"}, {"title": "Related Work", "content": "Our joint inference strategy (Section 4.2) is technically based on approximations (Fried et al., 2018a,b) of the Rational Speech Acts framework (RSA; Goodman and Frank, 2016; Yuan et al., 2018), which frames pragmatic reasoning as a recursive process between listener and speaker models. RSA has been studied extensively with the focus of developing models that reason pragmatically (e.g., Monroe et al., 2017; Andreas and Klein, 2016), including through incorporation in learning (McDowell and Goodman, 2019) and inference (White et al., 2020). We use it for different aims, as one of two strategies to couple comprehension and generation. Liu et al. (2023) studied the incorporation of joint inference for generation learning, which is a component of our study, with a static model listener. In contrast, we study learning dynamics for both comprehension and generation, evaluate data sharing as an additional coupling mechanism, and deploy for continual learning with humans, who constitute non-static partners.\nContinually learning from interactions with human users has been studied in the context of instruction generation (Kojima et al., 2021) and following (Suhr and Artzi, 2024), question answering (Gao et al., 2023), and ad-hoc adaptation (Hawkins et al., 2020). In our work, continual learning enables us to study long-term dynamics that arise from coupling comprehension and generation. Our continual learning setup is different from the Reinforcement Learning from Human Feedback framework (RLHF; Ziegler et al., 2019) in relying on binary signals derived from interactions with users, while RLHF requires external annotators that compare output pairs.\nThe reference game scenario has been extensively used in cognitive studies as a prototypical, but simple interaction design (Rosenberg and Cohen, 1964; Krauss and Weinheimer, 1964). It has been used to study convention formation at dyadic (Clark and Wilkes-Gibbs, 1986; Wilkes-Gibbs and Clark, 1992) and population-levels (Hawkins et al., 2023), and demonstrate com-"}, {"title": "Conclusion", "content": "We study the dynamics of coupling language comprehension and generation at inference- and training-time through a continual learning setting where an agent learns from interactions with humans. Coupling has significant impact over time, leading to improved agent performance, sample efficiency, and similarity to human language.\nOur work points to multiple directions for future work, including coupling the processes through the training objective in addition to data at training-time, developing more efficient alternatives to sampling utterances during joint inference for generation, and the study of alternative interaction scenarios, including multi-turn settings where dynamics between comprehension and generation can affect an interaction throughout its duration. Scaling up our approach and experimental setting to a real-world deployment featuring a wider range of tasks and a broader set of feedback signals, such as natural language feedback, constitutes a particularly important direction."}, {"title": "Limitations", "content": "Our work does not touch on an important factor in deployed systems: the addition of new participants into the system. To simplify the crowdsourcing setup, we keep the set of workers fixed during our experiments. This does not allow us to observe the effect of new participants joining the population and the impact of the data they create interacting with our agents. This is an important direction for future work. While our methods are not specifically designed for English, our study is only done in English. We restrict the language to English and recruit workers from English-majority locales only. This qualifies our findings, both with regard to the language choice and the impact of the culture of the participants. These are also important variables for future studies.\nUnlike how RL is usually studied in the research community, our continual learning process involves humans in the loop. This entails restrictions in terms of time and cost. We opt for simplicity and choose to train models with a REINFORCE-style policy gradient algorithm (Williams, 1992) and retrain models from scratch on the cumulative set of collected data with each round of continual learning. A more extensive (and costly) search over methods might impact results. We leave the study of more complex RL algorithms, such as PPO (Schulman et al., 2017), as well as different strategies for incorporating data from previous rounds to future work.\nWe invested significant effort and resources in running our study for a significant amount of interactions and rounds. While we show consistent trends, it is hard to predict trends at much larger scale (e.g., thousands of rounds or millions of interactions). This is beyond the resource available for this research. That said, even if trends change dramatically with such a long horizon, our approach remains useful for faster learning (i.e., reduce regret) in the early life of the system."}, {"title": "Ethical Considerations", "content": "Our work studies how the coupling of comprehension and generation affects the dynamics of performance and model language. Through coupling at training time, our model trains on both its own generations alongside generations its human partners produced at interaction time. A naive implementation of this strategy during real-world deployment risks aligning model behavior with the biases of its human interlocutors at best and exposing the model to adversarial actors at worst. Appropriate guardrails or further research for selecting when to apply data sharing should be implemented before deployment is considered to ensure safety."}, {"title": "Appendix A Training and Inference Details", "content": "We use the instruction-tuned IDEFICS2-8B model (Lauren\u00e7on et al., 2024) for all our experiments, and optimize with AdamW (Loshchilov and Hutter, 2019) with a learning rate of 0.0001 and a weight decay of 0.1. Each gradient step is computed over independently sampled minibatches of size 32 for comprehension and generation tasks.\nWe use LORA for finetuning (Hu et al., 2022), where r = 16 and a = 8. We apply adapters to all feedforward layers in the vision encoder, the modality projection and the perceiver-resampler block, but only to the key, query and value projections of the text decoder. We found applying further adapters for the text decoder to exacerbate overfitting. We load and train models with BF16 precision to reduce memory and compute costs.\nWe observe the IPS term for negatively rewarded examples (Section 3.2) to infrequently attain high values in early epochs during pilot experiments. To increase training stability, we clip the IPS term at 5. During our main experiment, clipping is activated for 2-3% of negatively rewarded generation"}, {"title": "Stopping Criterion", "content": "Each model is trained for a maximum of 15 epochs. An epoch is a complete pass over data for the comprehension task. We use patience stopping, ending training when model validation accuracy for the comprehension task does not improve for five epochs. For models with joint inference, we compute validation accuracy with the joint listener model Pi (t|I, u; 0), while for models without joint inference, we compute it with the base listener P\u2081(t|I, u; 0). We exclusively use comprehension accuracy. Pilot experiments showed it correlates well with deployment performance."}, {"title": "Hyperparameter Search", "content": "Hyperparameter search is done on the seed initialization data, using comprehension accuracy on the validation set as the metric. We vary learning rates {1e - 5, 5e - 5,1\u0435 \u2014 4, 2\u0435 \u2013 4}, weight decay {0, 1e - 3, 1e \u2013 1}, LoRA a {8,32} and adapter placements (only key-query-value projections; all forward projections; and all forward projections except for the language decoder, which used key-query-value projections) and prompt designs. The selection of LoRA adapters is the most important hyperparameter, showing a strong impact on overfitting at the data scales we work in.\nThe \u5165\u2081 hyperparameter for the joint comprehension distribution is tuned on the seed data with the hyperparameters described on Appendix A.1. We save model checkpoints for each epoch of training and inspect comprehension accuracy values on the validation set for different settings of \u03bb\u03b9. We find \u03bb\u03b9 = 0.5 consistently perform well.\nWe choose As by training models with the joint inference strategy with \u03bb\u03b9 = 0.5 and using the hyperparameters and stopping criterion from Appendix A.1 and Appendix A.2. We sample utterances on the validation set and inspect the reranking behavior of the joint generation distribution P (u|I, t; 0) with different \u5165\u300f values. We observe that the utterance the joint distribution P (u|I, t; 0) ranked as the best was often equivalent to the utterance the base generation distribution P\u2084(u|I, t; 0) ranked as the most likely. This skew towards the base generation distribution is additionally exacerbated with longer training times.\nTo determine A, in light of this, we probe how accurately the joint generation distribution could"}, {"title": "Prompt Design", "content": "We use the same model (i.e., same architectures and same parameters) for comprehension and generation and designate which task the model should perform through prompting. Figure 6 and Figure 7 show the prompts for comprehension and generation."}, {"title": "Generation Sampling Details", "content": "We sample utterances autoregressively using a temperature of 7 = 0.7. We sample k = 10 utterances to generate with the joint inference procedure. To isolate the influence of reranking with the comprehension model, we also sample k = 10 utterances when not performing joint inference and return the utterance with the highest probability."}, {"title": "Computational Resources", "content": "Each model is trained with a single GPU, RTX A6000 or NVIDIA A100. Hyperparameter tuning experiments took 100-200 GPU hours total, while training for the main continual learning experiment took approximately 225 GPU hours. For deployment, on the other hand, Models are deployed using RTX A6000 and V100 GPUs, with Ray for inference parallelization (Moritz et al., 2018)."}, {"title": "Context Construction", "content": "Each reference game round involves a context of size N = 10 comprising 3 blocks (two of size 3 and one of size 4) of visually similar tangrams. We use a CLIP model (Radford et al., 2021) finetuned by Ji et al. (2022) on annotations from the KILOGRAM dataset to construct these sub-blocks. The blocks increase the difficulty of the context, because elements within each block have high visual similarity, making both comprehension and generation more challenging.\nEach similarity block is constructed by randomly sampling a tangram, and sampling the rest of the block members from all other tangrams. The sampling is done using a distribution of normalized sim-"}, {"title": "Experiment Details", "content": "We curate the set of words relating to spatial reasoning by parsing the set of all human and model-generated utterances using spaCy with the en_core_web_sm pipeline (Honnibal et al., 2020). We collect the set of all words marked with an ADP (adposition) part-of-speech tag, which predominantly contained terms for spatial reasoning in our task, and manually filtered out the words such as \"like\" that are irrelevant to spatial reasoning. We then added words relating to notions of \"left\" and \"right,\" which were not captured under the ADP tag.\nThe full set of words we used was: 'from', 'towards', 'thru', 'to', 'through', 'until', 'next', 'above', 'along', 'about', 'out', 'inside', 'behind', 'outside', 'forward', 'back', 'around', 'beneath', 'atop', 'up', 'apart', 'near', 'at', 'below', 'into', 'onto', 'toward', 'past', 'upwards', 'before', 'within', 'against', 'between', 'beside', 'on', 'after', 'by', 'over', 'across', 'down', 'opposite', 'underneath', 'in', 'under', 'left', 'leftward', 'leftwards', 'right', 'rightward', 'rightwards'."}, {"title": "Shape Naming Divergence Metric", "content": "We analyze pragmatic reasoning using the Shape Naming Divergence (SND) metric (Ji et al., 2022), which measures how much the naming of individual tangrams varies across different annotations. We repurpose it to probe pragmatic reasoning by measuring how much a model's description of a given tangram varies across different contexts. Instead of descriptions from different annotators, we compute SND over descriptions of that tangram in different contexts. This gives insight into the impact of the context (i.e., via pragmatic reasoning) on the description of the individual tangram."}, {"title": "Estimating Performance on Future Rounds", "content": "The decisions of experiment length (i.e., in the number of rounds) requires to balance costs and research utility. Our main experiment included four rounds of deployment and learning, which was suf-"}, {"title": "Worker Recruitment", "content": "We recruit workers with a minimum HIT (Human Intelligence Task) approval rate of 98% and at least 1,000 approved HITs. We restrict the pool to workers from English-majority locales (United States, Canada, Great Britain, Ireland, Australia, and New Zealand). Workers complete a video tutorial and a qualification quiz to qualify for our tasks."}, {"title": "Payment Details", "content": "The HIT base pay is $0.60USD. For each round of reference games played within a HIT, workers receive a bonus of $0.125USD upon success or $0.05USD upon failure. The estimated hourly pay was $18.31 USD for games between humans, and $20.55 USD for games between humans and models at the final round. We set the base pay and bonuses through pilot studies among researchers and tuned the values based on estimates of hourly pay during pilot studies."}, {"title": "Game Interface", "content": "The reference game interface is built using the Empirica framework (Almaatouq et al., 2021). It includes a chatbox at the left hand of the screen and the context tangrams at the center. When in the speaker role, the target is indicated to the speaker with a black square. The speaker has 45 seconds to type and send an utterance through the chatbox. After the speaker sends a message, the listener is given 15 additional seconds to make a selection. Each round lasts at most 60 seconds. The listener makes a selection by clicking on a tangram image. If successful, the target flashes green for both players. Upon failure, the target tangram flashes red for the speaker and the chosen tangram flashes red for the listener. Workers in the speaker role are not revealed their partners' choice and work-"}, {"title": "Deployment Details", "content": "In each deployment, we give each worker access to an equal number of HITs to uniformly sample from the worker pool. Within a given HIT, a worker plays 40 rounds of reference games, either against a human or model partner. If playing against a model, the worker plays against each system variant an equal number of times and in a random order. During the final round of deployment, we additionally evaluate the initial FULL system, and therefore increase the number of rounds per HIT to 50.\nThroughout the execution of a HIT, players alternate between roles every 3\u20134 rounds. In each group of 3-4 rounds, the underlying context is kept fixed, with the targets changing each round. This balances the cognitive load of observing a completely new context while preventing workers from being able to guess targets based on what has not been mentioned yet. If a worker is playing against a model, the system they are playing against is kept fixed within this group of 3-4 rounds. Workers are not revealed whether they are playing against a human or a model."}]}