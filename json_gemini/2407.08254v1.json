{"title": "United We Stand: Decentralized Multi-Agent Planning With Attrition", "authors": ["Nhat Nguyen", "Duong Nguyen", "Gianluca Rizzo", "Hung Nguyen"], "abstract": "Decentralized planning is a key element of cooperative multi-agent systems for information gathering tasks. However, despite the high frequency of agent failures in realistic large deployment scenarios, current approaches perform poorly in the presence of failures, by not converging at all, and/or by making very inefficient use of resources (e.g. energy). In this work, we propose Attritable MCTS (A-MCTS), a decentralized MCTS algorithm capable of timely and efficient adaptation to changes in the set of active agents. It is based on the use of a global reward function for the estimation of each agent's local contribution, and regret matching for coordination. We evaluate its effectiveness in realistic data-harvesting problems under different scenarios. We show both theoretically and experimentally that A-MCTS enables efficient adaptation even under high failure rates. Results suggest that, in the presence of frequent failures, our solution improves substantially over the best existing approaches in terms of global utility and scalability.", "sections": [{"title": "1 Introduction", "content": "Cooperative multi-agent systems (MAS) are systems where multiple agents (such as autonomous vehicles/drones) work together to achieve a common goal such as maximizing a shared utility [17]. These agents can communicate and coordinate with each other, either directly or indirectly, to solve complex tasks that are beyond a single agent's capabilities. Examples are drone swarms for autonomous aerial surveillance or disaster relief operations, or teams of robots that collaborate to explore unknown environments, harvest data from sensors, or manipulate objects, among others [47].\nCentralized approaches for addressing the MAS planning problem do not scale with the number of agents, as the amount of computational resources required to solve it quickly becomes prohibitive. In addition, the amount of information exchange required for a centralized planner to manage all agents can be unfeasibly high in large-scale settings, particularly in remote areas and disaster scenarios [44]. Thus, recent research has focused on decentralized approaches for online MAS planning [49]. Indeed, they offer enhanced robustness, reduced computational burden, and lower communication load, particularly on infrastructure-based communications such as cellular radio access networks [11].\nThe main challenge in decentralized approaches for cooperative MAS is optimizing agents' actions in a distributed manner to maximize a global reward function. This problem can typically be modeled as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) [34]. However, the computational complexity of Dec-POMDPs presents a significant hurdle, making direct optimal solution search infeasible in polynomial-time [5]. Several sampling-based planning algorithms have also been proposed to improve computational efficiency, particularly for special classes of Dec-POMDPs, such as multi-robot active perception [49]. A first family of approaches to address this is given by point-based methods, which scale well, but they may not cover the entire belief space well [35, 42]. Another set of algorithms is based on policy search [40, 1] based on a parameterized policy representation. They can handle problems with large action spaces, but they may get stuck in local optima or require many samples [1]. Thus, attention has turned towards algorithms based on Monte Carlo tree search (MCTS), due to their ability to effectively explore long planning horizons, their anytime nature [25], and their excellent performance in decentralized settings [11, 7, 15], effectively overcoming the limitations of other approaches.\nIn many present-day MAS application scenarios, the departure of agents from the system (henceforth denoted as attrition, and due to e.g. failures or energy depletion) is a very common feature. However, all of the main approaches to decentralized MAS planning assume agents are always available and actively contributing to the joint planning process. When applied to scenarios with attrition, they perform in a heavily suboptimal manner and they often do not converge at all [14, 33]. In swarm robotics, for instance, agent attrition due to robot failures, damage, or energy depletion affects the overall swarm behavior and task completion. Designing robust algorithms for decentralized MAS planning capable of effectively handling agent loss is critical for their successful deployment many in practical scenarios.\nThe common approaches for scenarios with attrition are based on periodically resetting agents' learned behavior, and restarting the learning process [2, 38, 21]. In volatile settings with frequent failures, such a feature may significantly hamper the overall performance of the active perception task, by slowing down the convergence rate and by keeping the system far from adapting and thus from achieving optimal operating conditions. Therefore, how to efficiently and effectively perform online MAS planning in the presence of attrition, while achieving fast convergence, is a key open issue.\nIn this paper, we develop a novel decentralized planning algorithm that achieves both of these objectives. Our approach is based on MCTS and the use of the global reward instead of the local one in the estimation of each agent's local contribution. Moreover, it exploits regret matching (RM) [22] to coordinate the actions between agents. We prove that our approach guarantees that the average joint"}, {"title": "2 Related Work", "content": "Information-gathering problems are often modeled as sequential decision-making problems [48]. When there are multiple agents, decentralized information gathering can be viewed as a decentralized POMDP [5]. The dominant approach to Dec-POMDP is to first solve the centralized, offline planning over the joint multi-agent policy space, and then push these policies to agents to execute them in a decentralized fashion [34]. When the state of the environment or agents is not known ahead of time, these approaches become infeasible. Fully decentralized Dec-POMDP solvers exist [43]. However, they require significant memory and incur high computational complexity due to the requirements to compute and store all the reachable joint state estimations [26].\nRecently, simultaneous distributed approaches based on MCTS have gained significant interest due to their flexibility in trading off computation time for accuracy. The key idea is to use the upper confidence bound (UCB) [25] for planning the best course of action. To implement cooperation between agents, these methods usually keep a predefined model of the teammates, which can be heuristic or machine learning trained [11, 15, 10]. However, as they are based on trained knowledge, they are unsuitable for online planning in settings that change unpredictably, such as in disastrous environments.\nTo address this, new approaches based not on apriori knowledge about agents' behavior, but on information sharing among them, have been proposed (Dec-MCTS [7]). A key aspect of Dec-MCTS and all its subsequent variations [27, 28, 33] is that each agent is assigned a local utility function, which does not measure the total team reward but the contribution of that agent only. To deal with any uncertainty that arises during the mission, Dec-MCTS algorithms allow for online replanning during execution, by having agents update their beliefs about the system.\nUnder high uncertainty scenarios, a growing body of literature review in the area of multi-drone systems [45, 29, 16, 20] explores the significant challenges posed by agent attrition \u2013 the loss or removal of individual drones (due to mechanical failures, environmental factors, and human errors), and highlights the need to address attrition for robust system performance. In systems with attrition in which agents may fail abruptly, all of the above-mentioned approaches do not apply, as they do not allow adjusting to attrition in agents' populations. In the present work, we show that the suboptimality of current Dec-MCTS algorithms is due to the usage of the marginal contribution combined with the submodular properties of the global utility functions.\nAnother body of literature on related works concerns Open Agent Systems (OASYS), in which agents can enter and leave over time. Most solutions to OASYS are either fully or partially offline, i.e., offline planning with online execution [12, 9] or online planning with precomputed offline policies [18]. This is not feasible in applications with significant sources of uncertainty, particularly when the environment's state or mental models of the agents are unknown in advance, and when the agents' failures occur abruptly during execution. A recent work [23] proposed a fully online approach that leverages communication between agents related to their presence to predict the actions of others. This approach assumes that agents can communicate their existence implicitly. In scenarios where the communication is intermittent or agents vanish without warning, the unforeseeable failure can significantly impact coordination and planning, jeopardizing the overall performance. In addition, the computational complexity of modeling each other's presence and predicting their actions can make the system computationally intractable on a large scale. Thus, it isn't directly equipped to handle sudden agent failures and may require further refinement to be viable in large-scale or highly dynamic environments.\nOur paper focuses on problems where agents experience hard failures in an abrupt and unforeseeable fashion. This unpredictable nature makes the existing techniques not directly applicable. Therefore, more research is needed to enhance the robustness and resilience of multi-agent systems in such uncertain attrition scenarios. Our work explicitly tackles this challenge by providing a new approach for online decentralized planning for multi-agents that does not require pre-computed offline policies and mental models. Instead, agents reason about the actions and existence of others using directly communicated information. We employ a computational-effective game-based technique to coordinate agents, enabling adaptive decision-making in the presence of peer failures while ensuring fast convergence in polynomial time relative to system size."}, {"title": "3 Problem Formulation", "content": "In this paper, we consider a set N of N autonomous agents moving within a given area of space. We consider a set of R regions of interest in a given area, where \\(R_k\\) is the k-th element of the set. We assume each region is a sphere with an equal radius, however, the formulation could extend to more complex models. Without loss of generality, we assume agents move along an undirected graph \\(G = (V, E)\\) that is placed in the same space as the regions of interest. Each vertex \\(v_i \\in V\\) represents a location, and each edge \\(e_{ij}\\) represents a feasible route from vertex \\(v_i\\) to \\(v_j\\). A key property of this graph is that it traverses at least once every region of interest. This graph typically models constraints to agent trajectories due to the morphology of the monitored environment, presence of obstacles, regions of interest distribution, and characteristics of agent movements, among others. The specific way in which the graph is derived is thus application and context-dependent [33]. The graph is defined at the beginning of the mission, it does not change over time and it is known by all agents.\nThe path of agent n, denoted as \\(p^n\\), is an ordered list of edges \\(p^n = (e^n_1, e^n_2, ...)\\), such that two adjacent edges in the path are"}, {"title": "3.1 Multi-agent planning with attrition", "content": "We denote the information-gathering task as a mission, for which each agent performs independent actions to achieve a collective goal maximizing the global utility for the whole team. Each agent n plans its path \\(p^n\\) and coordinates with others in a decentralized manner while satisfying the given budget constraint B on path length. This formulation of the information gathering problem generalizes many multi-agent path planning problems, such as team orienteering problem [6]. We consider a scenario, in which mission planning is performed in a decentralized manner for scalability and computational feasibility, as mentioned in Section 1. Thus, each agent plans its path while considering the potential actions of other agents and the team's total utility. We consider scenarios where a subset F of the N agents fail during the mission. We focus on hard failures, where agents interrupt reward collection and information exchange. We assume the set of agents that fail F is unknown in advance and the time at which they fail to be determined by any arbitrary criteria or distribution. Therefore, our solution does not rely on knowing its size and probability distribution. In the occurrence of a failure, all the utility collected by the failed agent is lost, i.e. it is not considered anymore in the computation of the global utility of the mission. This models a typical setup in information gathering, in which data collected by agents is relayed to data sinks only at the end of the mission.\nOur goal is to provide an efficient planning and coordination mechanism that can quickly adapt to agent failures and maximize the global utility of the mission within the agent's budget constraint. Let \\(P = (P_1, ..., P_n, ..., P_N)\\), with \\(P_n\\) denote the set of all possible paths of length B which starts at agent n starting position. We define the following problem:\nProblem 1. (Multi-agent planning in attrition settings)\n\\[\\begin{array}{ll}\\\\text{maximize} & U_g(\\textbf{p}) \\\\\\\\text{Subject to:} & b(p^n) \\leq B, \\forall n \\in N \\\\\\\\ & 0 \\leq |F| \\leq N\\end{array}\\]\nConstraint (2) derives from imposing that the total path length for the agent n to be less than the travel budget B available to each agent. Intuitively, our goal is to find a path for each agent such that the global utility associated with all regions observed by all agents during the mission is maximized, while a subset F of agents fail. Such an optimization problem cannot be solved efficiently. Indeed it is easy to see that Problem 1 is a variant of the well-known NP-hard traveling salesman problem."}, {"title": "4 Attritable MCTS with Regret Minimization", "content": "In this section, we first give a brief introduction to Monte Carlo Tree Search and its most popular decentralized version. We then show the root cause of the inefficiency of existing decentralized MCTS approaches with attrition.\nMCTS is an excellent approach for online planning problems [25]. The tree \\(T_n\\) for agent n is defined such that each node s of the tree represents a state and each edge a starting from that node represents an available action. A branch from the root node to another node represents a valid action sequence. The tree is incrementally grown via a four-step process: selection, expansion, rollout, and backpropagation. Decentralized Monte Carlo Tree Search (Dec-MCTS) [7] extends the power of MCTS to MAS using intention sharing. Specifically, agent n maintains a probability mass function \\(q_n (x_n)\\) over the set of all possible action sequences \\(X_n\\), where \\(x_n \\in X_n\\) is a primitive action sequence. The intentions of other agents except agent n are denoted by \\(q_{-n}\\) and \\(X_{-n}\\). By taking a probabilistic sampling from the communicated intention, each agent can reconstruct the global utility. To create better coordination, rather than optimizing directly for the global utility \\(U_g\\) of the entire team, each agent n instead optimizes for a local marginal contribution utility function \\(U_n\\). That is, agent n estimates the rollout score for executing \\(x_n\\) as:\n\\[F_n(x_n) = U_n(x_n, x_{-n}) = U_g (x_n, x_{-n}) - U_g (x_{-n}), \\quad (4)\\]\nwhere \\(U_g (x_{-n})\\) is the global utility without the contribution of agent n.\nWe now analyze Dec-MCTS asymptotic behavior when agents fail. We are particularly interested in submodular reward functions, frequently arising in data collection problems [13, 39]. Submodular set function, which is defined in Definition 1, satisfies the diminishing returns property. Regarding the information-gathering problem discussed in this paper (see Section 3), the marginal gain of adding a new location to the set of visited locations decreases as the number of locations visited increases.\nDefinition 1 (Submodular set function). Let \\(g : 2^{\\Omega} \\rightarrow R\\) be a set function where \\(2^{\\Omega}\\) is the power set of \\(\\Omega\\). Then g is a submodular function if for every \\(X, Y \\subseteq \\Omega\\) with \\(X \\subset Y\\) and every \\(x \\in \\Omega \\setminus Y\\) the following inequality holds\n\\[g(X\\cup x) - g(X) \\geq g(Y \\cup x) \u2013 g(Y) .\\]\nIn particular, at iteration t, let \\(x_n\\) denote the chosen action sequence of agent n and \\(x_{-n}\\) denote the combined sampled action sequences of other agents. Assume that at the next iteration t + 1, a subset of agents fails. Let \\(x'_{-n}\\) be the combined sampled action sequences of all agents except agent n and the lost agents (i.e., that is \\(X_n \\cup x'_{-n}\\) ).\nProposition 1. If the global objective function \\(U_g\\) is submodular, then \\(F_{n}^{(t+1)}(x_n) \\geq F_{n}^{(t)}(x_n)\\) by the diminishing return property due to submodularity, where \\(F_n(x_n)\\) is defined in (4)."}, {"title": "4.1 Overview of the A-MCTS algorithm", "content": "We develop Attritable MCTS (A-MCTS), an online decentralized MCTS algorithm that quickly adapts to agents' attrition and efficiently coordinates action between the remaining agents. Its performance mainly relies on two key factors, including the joint-utility-guided decentralized tree search, and the best response policy given the shared intentions of others. Each agent runs A-MCTS distributedly to plan for itself a path that is expected to maximize the total utility of the whole mission. Agents then execute the first planned action and observe any changes. After that, they perform replanning from their new state and update the planned paths based on newly available information. The search tree may be pruned by removing all children of the root except the selected branch. This cycle of planning and execution continues until the travel budget expires. The pseudo-code of A-MCTS for agent n is shown in Algorithm 1.\nThe tree \\(T_n\\) of agent n is incrementally built over its action sequences space \\(X_n\\) while considering the possible behaviors of others \\(X_{-n}\\) (Line 6-9). In the selection phase, the discounted upper confidence bound on Tree (D-UCT) [7] is applied to handle the abrupt changes in reward values caused by the actions of other agents.\nThe key idea of our proposed algorithm is to have the search trees of every agent be guided by the same utility of the joint action sequences. This is achieved by letting all agents optimize their local actions using the global utility \\(U_g\\) directly (Line 8). Each agent can then decide its path \\(x_n\\) independently to maximize \\(U_g\\) and be aware of the change in the global rewards immediately if there are failures in the system. However, the uncertainty in other agents' plans has also been shown to degrade the overall performance when using the global objective function to optimize local actions [46]. To overcome this issue, we propose to let each agent improve its policy iteratively while assuming others keep their policies fixed.\n\nProblem 2. (Best joint policy for multi-agent planning)\n\\[\\begin{array}{ll}\\\\text{maximize} & U_g (x_1, x_2, ..., x_N)\\\\\\\\(x_1,x_2,...,x_N)\\\\text{Subject to:} & x_n \\in \\mathbb{X}_n, \\forall n \\in N\\end{array}\\]\nThe objective is to find an action profile \\((x_1,x_2,...,x_N)\\) that maximizes the global utility \\(U_g(.)\\). Such an optimization problem cannot be solved efficiently. Indeed it is NP-hard to maximize a submodular function [37]. Seeking a Nash equilibrium (NE) (where each agent policy is the best response to the others) that achieves a good efficiency compared to the optimal solution is more accessible [36]. A greedy algorithm is usually employed to find an approximation solution [31]. However, we will show later with simulations that"}, {"title": "4.2 Regret Matching For Cooperative Coordination", "content": "In this paper, we consider the distributed solution of the optimization problem 2 where each agent decides its path based on local information and limited communication from its peers. We aim to design a decision-making method that is capable of operating and adapting with occasional communication or less, where every agent acts solely based on its local observation and does not need to constantly communicate every decision with the others. This is to guarantee that the algorithm can effectively handle the agent attrition situation described in Section 3.1. The main difficulty here is how to ensure that the independent decisions of the agents lead to jointly optimal decisions for the group. To address this challenge, we formulate the problem of finding for each agent an action sequence that collectively maximizes the joint utility as a multi-agent cooperative game. We then propose a distributed mechanism, where every agent independently simulates a multi-player cooperative game based on the local information available to itself and solves the game by self-play. For this purpose, a game theory learning algorithm based on the Regret Matching technique [22] is employed to approximate the Nash equilibrium of the game.\nLet \\(\\mathbb{X} = (\\mathbb{X}_n, \\mathbb{X}_{-n})\\) denote the joint set of action sequences that are shared between all agents, and \\(x_{nm}\\) denote the action sequence m of agent n. In our approach, periodically, every agent independently constructs a matrix game in which the set of players contains all the active agents and the set of actions is \\(\\mathbb{X}\\). At this stage, each agent applies the Regret Matching (RM) procedure as proposed in [22] to its estimated matrix game to compute a best response joint decision. The pseudo-code of our RM game is shown in Algorithm 2.\nTo further improve the performance of RM in cooperative settings, we let the agents use the global utility to calculate the regrets instead of the local utility. At each iteration t, an action \\(x_{nm} \\in \\mathbb{X}\\) is sampled for each agent based on a probability distribution. Let p denote this probability distribution where \\(p(x_{nm})\\) is the probability for \\(x_{nm}\\) and \\(\\sum_{m=1}^{M} P(X_{nj}) = 1, \\forall n \\in N\\). With \\(x(t) = {x_n^{(t)}, x_{-n}^{(t)}\\)\\), we denote the sampled set at iteration t, where \\(x_n^{(t)}\\) is the sample action for agent n and \\(x_{-n}^{(t)}\\) is the sampled actions for all agents except agent n. We then define the regret of agent n for not taking action m at iteration t as \\(R_{nm}^{(t)} = U_g(x_{nm}, x_{-n}^{(t)}) \u2013 U_g(x^{(t)})\\). Denote R as the cumulative regret matrix where an element \\(R_{nm}\\) is the regret for \\(x_{nm}\\) and \\(R_{nm}^+ = max{R_{nm},0}\\). Then, the probability distribution p used at the next iteration will be updated as\n\\[p(x_{nm}) = \\begin{cases} \\frac{R_{nm}^+}{\\sum_{M=1}^{M} R_{nm}^+}& \\text{if } \\sum_{1}^{M} R_{nm}^+ > 0, \\\\frac{1}{M} & \\text{otherwise}. \\end{cases}\\]\nDenote the joint decision computed using RM by agent n, which is the set of action sequences, one per agent, that has the highest probability \\(p(x_{nm})\\), as \\(x^{RM}\\). Similarly, let \\(x^{RM}_{-n}\\) be the computed set for all agents except agent n. Finally, these sets are exchanged between every agent, and the most payoff-dominant solution is chosen as the best response joint decision \\(x^{BR} = (x^{BR}, x^{BR}_{-n})\\)."}, {"title": "4.3 Analysis and Discussion", "content": "It has been shown in [4] that there exists no polynomial time algorithm to compute a pure NE in multiplayer nonzero-sum stochastic games. Hence, we employ an approximate method of finding the NE by proposing a decentralized Nash selection method based on Regret Matching for making choices in a multiplayer matrix game formulated at each decision-making state. Regret Matching is a regret-based algorithm for learning strategies in games, and is often used to compute correlated equilibria in multi-player repeated games with imperfect information. Although the regret matching technique has been widely used for non-cooperative games, its application in cooperative games, such as the problem studied in our paper with a submodular utility function, has only been recently explored [32]. In this work, by leveraging the submodularity property of the joint objective function, we employ Regret Matching as a self-play technique to independently learn a Nash-based strategy for each player. We theoretically prove a stronger result of convergence using Regret Matching to an approximate pure-strategy Nash solution (see Definition 2), rather than the commonly-used correlated equilibrium, in games where players' utility functions are submodular.\nDefinition 2 (Pure-Strategy Nash Equilibrium). A pure-strategy Nash equilibrium (PSNE) is a joint action profile \\(x^* = (x_n, x_{-n}) \\in \\mathbb{X}\\) if for all \\(n \\in N\\) and all \\(x_n \\in \\mathbb{A}_n\\) such that: \\(U_n(x_n, x_{-n}) \\geq U_n (X_n, X_{-n})\\).\nTheorem 2. The best response joint decision \\(x^{BR}\\) computed using RM, under the assumption of submodular utility functions, is a PSNE solution of the matrix-game representation generated by the set of best feasible paths \\(\\mathbb{A}_n \\subseteq \\mathbb{X}_n\\) chosen by every agent at each decision point\u00b2."}, {"title": "5 Experimental Evaluation", "content": "To assess our A-MCTS algorithm, we consider the task of data collection from underwater wireless sensor networks (UWSN). Such tasks usually call for a collaboration of multiple autonomous underwater vehicles (AUVs) to traverse the environment and gather data from sensors. The scenario consists of 200 randomly distributed sensors in a 4000 m \u00d7 4000 m plane, with a transmission radius of 50 m (typical for UWSN, e.g. [8]). The graph G of feasible paths is constructed using a probabilistic roadmap with a Dubins path model [24]. This model employs curves to refine the straight-line"}, {"title": "5.1 Performance Benchmarking", "content": "In the first evaluation of our algorithm's adaptability to failures, we examine the impact of the failure intensity (i.e. fractions of agents that fail during the mission) on the IRC at the mission end, illustrated in Figure 2a. As expected, all algorithms experience performance declines with increasing intensity, reflecting reduced reward coverage due to fewer remaining agents in the system. Notably, with over 50% of agents failing, Dec-MCTS-Reset surpasses the non-reset version due to the smaller system size which requires fewer iterations for exploration. Conversely, larger systems necessitate more time for agents to learn about the environment, hence frequent resets hamper the algorithm's performance.\nTo further elaborate on this matter, we assessed the impact of planning time on the IRC at the mission end. As Figure 2b shows, other baseline algorithms improved as planning time increased, with Dec-"}, {"title": "5.2 Trade Off Between Communication Loss and Attrition for A-MCTS Analysis", "content": "In our approach, repeated communication loss is used as an indication of attrition. However, in practical settings, inter-agent communication can be unreliable and intermittent. If the algorithm is more sensitive to communication loss, it can mistakenly treat delayed messages as agent failures.\nTo better understand such impact, in this section, we study how tolerance to communication loss affects the performance of A-MCTS. Specifically, we parameterize this tolerance level by the number of times an agent must experience communication loss with another agent before treating it as attrition. Fig. 3 shows the IRC at the end of the mission against different numbers of allowed inter-agent communication loss. With up to 5 allowed messages loss, A-MCTS still shows no noticeable degradation. However, as the algorithm is more communication loss tolerant, the performance declines. This is expected because the remaining agents can not recognize churns fast enough and adapt efficiently."}, {"title": "6 Conclusions", "content": "Achieving efficient coordination in multi-agent planning for information gathering is a critical challenge in practical settings with high attrition rates. In this work, we proposed a new approach to tackle this issue. Our proposed algorithm, Attritable MCTS (A-MCTS), effectively coordinates actions among agents while adapting to agent failures by allowing all agents to jointly optimize the global utility directly with a new coordination technique based on regret matching. Our empirical evaluation demonstrates that A-MCTS improves substantially over the best existing approaches in terms of global utility and scalability in scenarios with frequent agent failures. As a follow-up, we intend to extend A-MCTS to more dynamic systems where new agents can join and the communication is probabilistic. Another line of inquiry is to study the performance of our algorithm in problems with inter-agent dependency, where the actions of an agent can only be enabled by the actions of another."}, {"title": "A Technical Results", "content": "A.1 Details of Discounted Upper Confidence Bound on Tree (D-UCT)\nConsider an arbitrary node s with a set of child nodes \\(A_n (s)\\). Whenever s is visited", "gamma)": "F_n^{(t)"}, "s', \\gamma) + c_n^{(t)} (s', \\gamma),\\]\nwhere \\(F_n^{(t)} (s', \\gamma)\\) is the average empirical reward for choosing s', and \\(c_n^{(t)} (s', \\gamma)\\) is the exploration bonus.\nDenote the discounted number of times the child node s' of the parent node has been visited as\n\\[N_n^{(t)} (s', \\gamma) := \\sum_{\\tau=1}^{t} \\gamma^{t-\\tau} 1_{\\{a_n^{(t-1)} (s)=\\gamma\\}}, \\quad (10)\\]\nwhere \\(1_{\\{a_n^{({\\tau}}) (s)=s' \\}}\\) is the indicator function that returns 1 if node s' was selected at round \\(\\tau\\) and 0 otherwise.\nLet \\(F_n^{(t)}\\) be the rollout score at iteration \\(\\tau < t\\) and \\(N_n^{(\\tau)}(s, \\gamma)\\) be the discounted number of times the parent node s has been visited. Then at time t, the average reward for node s' is computed as\n\\[F_n^{(t)} (s',\\gamma) = \\frac{1}{N_n^{(t)} (s',\\gamma)} \\sum_{\\tau=1}^{t} \\gamma^{t-\\tau} 1_{\\{a_n^{(\\tau)} (s)=s'\\}} F_n^{(\\tau)} \\quad (11)\\]\nand exploration bonus as\n\\[c_n^{(t)} (s', \\gamma) = 2C_p \\sqrt{\\frac{\\log N_n^{(t)} (s, \\gamma)}{N_n^{(t)} (s',\\gamma)}} \\quad (12)\\]\nA.2 Analysis of Dec-MCTS Performance in Attrition Settings\nAs shown in [7], Dec-MCTS has vanishing regret and converges as \\(t\\rightarrow\\infty\\). We prove here the behavior of Dec-MCTS after convergence. Recall that by convergence we mean each agent stays with the same action sequence (i.e., the UCB score for each action in such sequence is the highest at that corresponding decision node).\nAssume that Dec-MCTS converges at iteration \\(\\tau_0\\) (finite) for all agents. At iteration \\(t > \\tau_0\\), let \\(x^n\\) denote the converged action sequence of agent n, and \\(x^*_{-n}\\) denotes the converged action sequences of every other agent except agent n. The rollout score received by agent n for executing the action sequence \\(x^n\\) given by the marginal utility will then be a constant L:\n\\[F_n^{(t)}(x_n) = U_n (x_n, x_{-n}) = U_g (x_n, x_{-n})-U_g(x_{-n}) = L .\\quad (13)\\]\nAssume that at the next iteration t + 1, a subset of agents becomes unavailable due to failures. Let \\(x'_{-n}\\) be the combined sequences of actions taken by all agents except agent i and the lost agents. That is\n\\[x_n \\cup x'_{-n}.\\]\nand the rollout score agent i receives for the same action sequence now is\n\\[F_n^{(t+1)}(x_n) = U_n (x_n, x_{-n}) = U_g(x_n, x_{-n}) \u2013 U_g(x_{-n}).\\quad (14)\\]\nLemma 3. Assume that the Dec-MCTS algorithm has converged on all the agents at time step \\(\\tau_0\\) and that the global objective function \\(U_g\\) is submodular. Then\n\\[X_n^{(t+1)} (x,y) \\geq X_n^{(t)} (x, y), \\forall t \\geq \\tau_0.\\]\nLemma 3 essentially states that once Dec-MCTS converges, the D-UCB score calculated by agent n for its converged action sequence \\(x_n\\) is non-decreasing even if it detects that some of the other agents have failed. Hence, it always picks and updates the same action sequence (i.e., the series of actions that has the highest D-UCB scores at each corresponding decision node).\nLet \\(c, p \\in x\\) be two nodes in the converged action sequence of agent i, with c being the child node of p. After the algorithm converges at iteration \\(\\tau_0\\), by definition, the nodes c and p are going to be selected repeatedly. Thus, at iteration t, the discounted number of times c is visited can be written as\n\\[N_n^{(t)} (c, \\gamma) = \\gamma^{t-\\tau_0} N_n^{(\\tau_0)} (c) + \\sum_{\\tau =0}^{t-\\tau_0} \\gamma^{\\tau} \\quad (15)\\]\n\\[= \\gamma^{t-\\tau_0} N_n^{(\\tau_0)} + \\frac{1 - \\gamma^{t-\\tau_0+1}}{1-\\gamma}\\]\nwith the constant \\(N_n^{(\\tau_0)}\\) is the discounted number of times node c is chosen at \\(\\tau_0\\). In addition, the discounted number of times p is visited at iteration t is\n\\[N_n^{(t)} (p, \\gamma) = \\gamma^{t-\\tau_0} N_n^{(\\tau_0)} + \\sum_{\\tau =0}^{t-\\tau_0} \\gamma^{\\tau} \\quad (16)\\]\n\\[= \\gamma^{t-\\tau_0} N_n^{(\\tau_0)} + \\frac{1 - \\gamma^{t-\\tau_0+1}}{1```json\n{\n      \"title\": \"C Further Analysis of Regret Matching Coordination Algorithm\",\n      \"content\":", "C.1 Rationale behind Nash equilibrium solution\nNash equilibrium is particularly useful in situations where agents have incomplete information about the strategies of other agents, i.e., coordination when agents can not maintain perfect communication. In such cooperative situations with limited communication, NE provides an effective way to find a set of strategies for each agent that is robust to uncertainty and incomplete information. In a Nash solution, no agent can improve its outcome by unilaterally deviating from the NE strategy. Thus, NE is a stable outcome for the agents involved in the planning process where all the agents share the same common objective. Yet, a remaining challenge is that there often exists multiple Nash equilibria and thus how to make sure the combination of these individual Nash-based strategies, which each agent independently computes, defines an optimal equilibrium. The selection of a good Nash equilibrium among the many options, known as an equilibrium selection problem, remains an open question for further investigation. Within the scope of this work, to address this dilemma, we propose that the agents synchronize their reached Nash points to identify the most payoff-dominant Nash solution. The agents then"]}