{"title": "SIn-NeRF2NeRF: Editing 3D Scenes with Instructions through Segmentation\nand Inpainting", "authors": ["Jiseung Hong", "Changmin Lee", "Gyusang Yu"], "abstract": "TL;DR Perform 3D object editing selectively by disen-\ntangling it from the background scene.\nInstruct-NeRF2NeRF [3] (in2n) is a promising method\nthat enables editing of 3D scenes composed of Neural\nRadiance Field (NeRF) [7] using text prompts. How-\never, it is challenging to perform geometrical modifica-\ntions such as shrinking, scaling, or moving on both the\nbackground and object simultaneously. In this project,\nwe enable geometrical changes of objects within the 3D\nscene by selectively editing the object after separating it\nfrom the scene. We perform object segmentation and back-\nground inpainting respectively, and demonstrate various ex-\namples of freely resizing or moving disentangled objects\nwithin the three-dimensional space.", "sections": [{"title": "1. Introduction", "content": "The current leading method for representing 3D scenes\nis Neural Radiance Fields (NeRF), which can generate re-\nalistic novel views from a sparse set of images, given the\ncamera parameters. The ability to freely and stably edit\nsuch 3D scenes is one of the most critical technologies for\nimplementing the real world in VR/AR applications.\nThe introduced Instruct-NeRF2NeRF is a powerful tool\nthat allows humans to edit the scene itself based on given in-\nput. Additionally, SPIn-NeRF [8] has created a background\nscene by interactively removing selected objects from the\nentire scene. InpaintNeRF360 [11] is a recent work that ap-\npropriately fuses the concepts of both. Unlike SPIn-NeRF,\nwhich is limited to inpainting results of frontal scenes, In-\npaintNeRF360 demonstrated the ability to perform inpaint-\ning across 360-degree scenes using human prompts.\nThe 3D scene edition results from Instruct-NeRF2NeRF\nshows promising results in geometrical addition and minor\nperturbations, while some of the updates in objects were\nnot reflected in the background scene. For instance, when\nan object goes through geometrical shrinking, the original\npart of the background wouldn't fill the perturbed part of\nthe object.\nIn this project, our objective is to achieve a refined object\nedition by separating the object from the scene, restoring\nthe original portion of the background without the object\nthrough inpainting, and selectively editing the object. Ul-\ntimately, this approach enables the disentanglement of the\nobject and background, facilitating more precise and effec-\ntive modifications in 3D scene reconstruction.\nTherefore, we propose SIn-NeRF2NeRF (sn2n) that\nmodifies Instruct-NeRF2NeRF (in2n) based on the code of\nSPIn-NeRF to perform object edition. The key challenge\nlied in the modification of in2n framework to get RGBA im-\nage as input, and yield the object scene with no background.\nWe elaborate the detailed implementation of this and other\nprocesses in the subsequent sections. We verify the fidelity\nof our model based on the datasets provided by in2n and\nSPIn-NeRF. Furthermore, we confirm its functionality on\na custom dataset, thereby demonstrating its robustness and\nscalability."}, {"title": "2. Method Summary", "content": "As shown in Figure 2, sn2n receives images of a scene\nfrom multiple views and object masks as input. Addition-\nally, it takes a text prompt as an input to output an edited\n3D scene where the object is modified. During this process,\ntechniques such as multiview segmentation and SPIn-NeRF\nwere utilized to disentangle the object and the background."}, {"title": "2.1. Problem Setup", "content": "As specified above, sn2n takes a text prompt and a NeRF\nscene as inputs to separate the scene into object and back-\nground components. For the object, it performs scene edit-\ning and scaling, while for the background, it applies 3D in-\npainting, and then merges the two scenes. In the process of\nediting the object scene, it employs a method $I_{obj \\ object,i+1}\nU_\\theta(I_{object,it}; I_{object,0}, c_T)$, for an unedited object image\n$I_{object,0}$, a text instruction $c_T$, and a noisy input $z_t$, where\n$z_0 = (I_{object,i})$. Here, $U_\\theta$ is defined as DDIM sampling,\nconsistent with the definition in in2n. Meanwhile, for the\nbackground scene, a 3D inpainting method called SPIn-\nNeRF was used. For further details in SPIn-NeRF, refer\nto Section 2.3. Consequently, we merge two NeRF scenes\nwhere the object is disentangled from the background."}, {"title": "2.2. Baseline Inspection", "content": "Instruct-NeRF2NeRF is a novel work performing ob-\nject editing within NeRF scenes and serves as the base-\nline method for our project. In the Section 4, we com-\npare the outcomes of applying in2n after separately learn-\ning the object and background scene using the SPIn-NeRF\nframework, against when in2n applied to the original scene.\nInstruct-NeRF2NeRF takes a typical NeRF scene as input\nand follows an iterative dataset update pipeline to update the\nscene. Per each iteration step, this dataset update pipeline\ntakes the novel scene image with added random gaussian\nnoise and prompt from the rendered view as input, and out-\nputs an image updated via InstructPix2Pix [1] (ip2p). Ini-\ntially the difference between the text prompt and the origi-\nnal scene is huge, and 2D images reconstructed according to\nthe prompt turns out 3D inconsistent. However as learning\ncontinues, the scene gradually changes starting from aspects\nacross views. This can be seen in Figure 3. Iterative dataset\nupdate is showing incremental changes to (such as pointy\nears, blue eyes, green t-shirts, etc.) per iteration."}, {"title": "2.3. SPIn-NeRF", "content": "SPIn-NeRF plays a pivotal role in our project by inpaint-\ning the background of the original scene with sparse object\nselection. SPIn-NeRF starts with training the given NeRF\nscene and acquires disparity maps for novel views. Conse-\nquently, the the object mask is given as input and it perform\nLaMA inpainting in every training views. Additional LaMa\ninpainting is done to the initially acquired disparity map. Fi-\nnally the background scene is constructed using these two\ninpainted rgb image and disparity inpainted image sets."}, {"title": "3. Implementation Details", "content": "We have implemented the entire process from data\npreparation/preprocessing (2D image multiview segmenta-\ntion) to applying in2n on the object scene and synthesiz-"}, {"title": "3.1. 2D Image Multiview Segmentation", "content": "We implemented code that allows interactive 2D image\nmultiview segmentation using the Segment Anything Model\n(SAM) [5]. It takes the original 2D image set as input, sep-\narates the object and background by providing sparse anno-\ntations for each frame, and retrieves the object mask."}, {"title": "3.2. Object Scene Reconstruction & Applying in2n", "content": "For the object scene, we train a DSNeRF [2] using\nan RGBA image set concatenated from the object mask\nand object image. During this process, we implement the\nrandom background color technique borrowed from the\ninstant-ngp [9] code to ensure effective training of the object\nscene with a transparent background. The random back-\nground color technique involves alpha blending each view\nwith a random color. Figure 4 shows significant difference\nin results compared to when technique is not applied."}, {"title": "3.3. Background Scene Reconstruction", "content": "Using the code provided by SPIn-NeRF, we train the\nDSNERF for the inpainted background scene. We perform\n2D inpainting on the original 2D background images, where\nthe object part is segmented out, using LaMa, and then\ntrained the 3D inpainted background scene using RGB loss,\ndisparity loss, LPIPS loss, and true depth loss."}, {"title": "3.4. 3D NERF Scene Synthesis", "content": "The method for synthesizing the edited object scene\nfrom Section 3.2 with the inpainted background scene us-\ning SPIn-NeRF was adopted from ml-neuman [4]. Since\nboth scenes share the same camera parameters, we sort the\nsampled points for the same rays generated in the object and\nbackground scenes by their depth values Z as in Equation 1.\n$P_{sorted} = {t_i | t_i \u2208 P_{object} U P_{bkg} \\ and \\ Z(t_i) \\ is \\ sorted}$"}, {"title": "3.5. Object Transformation", "content": "The process of transforming (scaling, translation, and\nrotation) a disentangled object scene from the background\ncommences with the utilization of COLMAP to ascertain\nthe coordinates P of the 3D object. Subsequently, these\ncoordinates are transformed around the centroid O using\na transformation factor scale, rotate, and trans, thereby\nacquiring the 3D coordinates of the transformed object P'.\nThis transformation doesn't change the actual 3D model but\nalters how it is perceived from the camera's viewpoint dur-\ning the rendering process."}, {"title": "4. Experimental Results", "content": "We confirm the robustness of our framework by dataset\nprovided by SPIn-NeRF team and in2n team. We focus\nprimarily on presenting qualitative results, complementing\nthese with quantitative results using CLIP metric. The face\nscene is mainly presented due to its intuitiveness. The\nmethod can be also applied for other datasets. We train\nour Segmented NeRF (object scene) for 2k iterations, which\ntakes about 30 minutes on a single RTX 4060 Ti. Running\nmultiview inpainter (background scene) also takes about 30\nminutes on a same device."}, {"title": "4.1. Qualitative Results", "content": "The advantage of our method is that by separately train-\ning the object and background and then merging them, it\nallows for not only editing through prompt input but also\nfree adjustment of position and scaling. In Figure 5, we can\nobserve the free panning of an object in the scene."}, {"title": "Baseline Comparison", "content": "It can be confirmed from Figure 6 that in2n has been sta-\nbly implemented within the SPIn-NeRF framework. When\ncomparing the results of sn2n with in2n for various prompts,\nit can be observed that the outcomes are similar because\nboth employ the same instruct pix2pix's 2D diffusion im-\nage editing. However, in the detailed aspects below, it is\nevident that sn2n has a geometric advantage over in2n, such\nas smoother background reconstruction."}, {"title": "4.2. Quantitative Results", "content": "As mentioned in the baseline paper [3], editing is a\nsubjective task, so the focus should be on qualitative re-\nsults rather than quantitative results when measuring perfor-\nmance improvement. However, as in ip2p and in2n, we an-\nalyze quantitatively in two ways. The results of comparing\nsn2n to the baseline model in2n based on the alignment of\n3D edits with text instructions and the temporal consistency\nof edits across views are shown in Table 1. We evaluated the\nmetrics based on the face scene with the text prompt \"Make\nhim into a clown\" and \"Put him in a Tuxedo\"."}, {"title": "5. Conclusion", "content": "In this project, we aim to achieve high-quality object\nediting within 3D scene editing, utilizing interactively seg-\nmented input and human prompts. We perform parallel\nNeRF learning for the received object and scene, where the\nobject is edited using the Instruct-NeRF2NeRF technique\nas desired, and the scene undergoes inpainting using the\nSPIn-NeRF method. We demonstrate enhanced object edi-\ntion (such as translation, scaling, etc.) on people, objects,\nand large-scale scenes as observed in in2n.\nHowever, the results of object editing significantly var-\nied depending on the performance of iterative dataset up-\ndate using ip2p. Also, due to the gradual nature of up-\ndates, changes are limited to texture and feature modifica-\ntions rather than dynamic changes like altering the pose.\nAdvancing towards more robust and dynamic object edition\ncapabilities is a direction for future research in this field,\nand we expect this can be done with RGBA image updating\nmethods like LayerDiffusion [6]."}]}