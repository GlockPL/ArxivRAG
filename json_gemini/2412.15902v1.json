{"title": "On the Suitability of pre-trained foundational LLMs for Analysis in German Legal Education", "authors": ["Lorenz Wendlinger", "Christian Braun", "Abdullah Al Zubaer", "Simon Alexander Nonn", "Sarah Gro\u00dfkopf", "Christofer Fellicious", "Michael Granitzer"], "abstract": "We show that current open-source foundational LLMs possess instruction capability and German legal background knowledge that is sufficient for some legal analysis in an educational context. However, model capability breaks down in very specific tasks, such as the classification of \"Gutachtenstil\" appraisal style components, or with complex contexts, such as complete legal opinions. Even with extended context and effective prompting strategies, they cannot match the Bag-of-Words baseline. To combat this, we introduce a Retrieval Augmented Generation based prompt example selection method that substantially improves predictions in high data availability scenarios. We further evaluate the performance of pre-trained LLMs on two standard tasks for argument mining and automated essay scoring and find it to be more adequate. Throughout, pre-trained LLMs improve upon the baseline in scenarios with little or no labeled data with Chain-of-Thought prompting further helping in the zero-shot case.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are trained on web-scale corpora that encompass a significant cross-section of human knowledge and communication. They offer generalization abilities that allow natural language instruction without any or with little need for fine-tuning. With Ainslie et al.'s (2023) Grouped-Query Attention, context windows are large enough to provide exhaustive and relevant background information. For this reason, they are poised to be a replacement for task-specific models in niche and evolving domains.\nLegal analysis is such a domain, with a large quantity of new laws and court decisions that make human analysis time consuming, while the structure and style of legal appraisal remain largely static. We want to empirically verify the assumption that the current crop of open-source local LLMs can understand this appraisal style by checking if they can correctly recognize it.\nThe German Gutachtenstil (appraisal style) is similar in structure and purpose to the IRAC (Issue, Rule, Application, Conclusion) method of legal analysis. It starts with a Major claim that names the legal question that is analyzed, the Definition that defines the requirements that need to be fulfilled to affirm the major claim. After that, in the Subsumtion, the facts of the case are applied to the different elements of the definition. Lastly, the Conclusion is reached by answering the question raised in the major claim. It is intended as a guide to help structure the thinking of law students. In German law, this schema is complex enough to be a focus of entry and intermediate level college lectures, with some universities even offering dedicated courses to teach the appraisal style. As it is seldom found in legal practice outside of universities, but rather substituted with the modified Urteilsstil, that starts with the conclusion and then uses the steps of definition and subsumption to justify the result, legal corpora may not contain enough examples to implicitly encode this information.\nWe consider this to be the minimum level of evidence required as proof for legal understanding. If a legal practitioner cannot appropriately structure the constituents of arguments into this appraisal style, they show a lack of understanding which may also result in content-specific mistakes. Similarly, if any student cannot recognize the elements of the appraisal style and fails to apply it in their own reasoning, they run a risk of exhibiting lapses in logic, of overlooking facts or legal requirements. Therefore, we start our analysis by testing whether LLMs can reliably identify the elements of this argumentation style.\nTo test the upper bounds of LLM capability, we perform a specific test to check the assessment capabilities of the models in the complex but controlled context of a challenging new dataset. The task of grading full legal opinions requires close to complete legal understanding. It can only be facilitated with a large catalog of background knowledge, comprised of the legal context, concurring and dissenting legal opinions and secondary literature. The examination of multiple statutes and constituent facts also necessitates the ability to appropriately contextualize and structure the components of the legal network that is established around an appraisal. For this purpose, we introduce a dataset of full legal papers written and graded in the context of legal education.\nWe further investigate the performance of pre-trained LLMs in two simpler related tasks in the realm of argument mining and automated essay scoring. This provides some context on the influence of task difficulty, language barriers, and domain-specific adaptations.\nThe used codebase as well as code for all experimental setups with results are available at https://github.com/wendli01/legal_analysis_llm."}, {"title": "2 Related Work", "content": "Recent work explores using generative LLMs for argument mining and essay scoring tasks, though most methods focus on extractive models instead. We summarize relevant results from automated essay scoring and argument mining below.\nRodriguez et al. (2019) apply two early transformer models to the Automatic Essay Scoring (AES) dataset from (Taghipour and Ng, 2016). They find that these models are competitive with Long Short-Term Memory Networks (LSTMs) and can outperform them as well as human scoring in this task if combined into a heterogeneous ensemble. Yang et al. (2020) use multiple losses to fine-tune language models on the Automated Student Assessment Prize (ASAP) dataset. Ormerod et al. (2021) use efficient Transformer variants to score essays. They find that they can achieve equivalent performance at a significant efficiency increase.\nLai et al. (2023) provide a comprehensive overview of LLMs in Chinese legal applications. They find that there are multiple models that are fine-tuned on Chinese legal QA corpora as well as more general models for legal consultation and reasoning. They point out the major problems with legal LLMs, mainly the difficulty of data quality control as well as bias and interpretability issues. They further allude to the inherent uncertainty of legal concepts, that makes the legal domain challenging to adapt LLMs to.\nMizumoto and Eguchi (2023) explore the applicability of GPT-3.5 to AES on the TOEFL11 corpus. They find that it generally lags behind the baseline Bayesian regression model operating on linguistic features only, but that combining the two leads to a slight improvement.\nColombo et al. (2024) fine-tune Mistral 7B on a large English legal corpus and publish the resulting SaulLM-7B. They perform general instruction fine-tuning followed by specific legal instruction fine-tuning with synthetic data generated by the base model. They find that the resulting model outperforms the base in the newly introduced LegalBench-Instruct, Legal-MMLU and perplexity, i.e. quality-of-fit, on legal documents.\nXiao et al. (2024) experiment with using GPT models for automated essay scoring and semi-supervised grading systems. They find that a fine-tuned variant of GPT-3.5 outperforms other models and the traditional baseline in most ASAP sets as well as on a private Chinese student English essay dataset.\nStahl et al. (2024) explore the impact of different prompts and prompting strategies in automated essay scoring. They find that the specific prompt has a measurable effect, while more complex strategies can be detrimental.\nThe use of LLMs for argument mining has seen growing interest in recent times. For instance, Al Zubaer et al. (2023) analyze the performance of GPT-3.5 and GPT-4 for argument component classification via prompting in the legal domain. They find that open-source local models perform better than propriety models and demonstrate the significance of including similar examples (in contrast to random and dissimilar) examples in prompts to boost the model's performance.\nOtiefy and Alhamzeh (2024) explore another sub-component of argument mining in relation detection between argument components using fine-tuned transformer-based models and GPT-4 for the financial domain. The find GPT-4's performance to be better than the fine-tuned transformer-based models.\nSimilarly, Gorur et al. (2024) demonstrate the effectiveness of Llama 2 and Mixtral for argument relation identification where given a pair of argument components, the model classifies the relation between them (support or attack). They find that Llama 2 and Mixtral outperformed traditional ROBERTA-based baseline in different domains ranging from essays to online debate forums."}, {"title": "3 Methods", "content": "We use multiple prompt engineering techniques for both the design of our analysis systems and post-mortem analysis. These include forays into few-shot prompting, chain-of-thought prompting, retrieval augmented generation, instruction following and pseudonymization.\nWe consider an excerpt (Table 8) of relevant categories from White et al.'s (2024) LiveBench\u00b9, a contamination-free LLM leaderboard. While Llama 3 and GPT-3.5 are close in the overall score, they exhibit opposite and complementary strengths in the two most relevant tasks, reasoning and language comprehension. This allows for speculation as to which of the two skills is more relevant in legal analysis. Mixtral 8x7B is competitive in reasoning but falls short in the language comprehension task."}, {"title": "3.1 Generative Pre-trained Transformer", "content": "The GPT family of LLMs is based on a decoder-only architecture that is trained on a web-scale corpus of documents and made available for commercial use by OpenAI. GPT-3.5 is a model that is optimized for assistant tasks."}, {"title": "3.2 Llama 3", "content": "AI@Meta's (2024) Llama 3 is an improved decoder-only transformer with a more advanced tokenizer using Ainslie et al.'s (2023) grouped query attention. It is trained using a context window of size 8,192 tokens and boasts an effective inference maximum of approx. 128k. We use the Meta-Llama-3-70B-Instruct checkpoint 2."}, {"title": "3.3 Mixtral", "content": "Mixtral (AI, 2024) is a Sparse Mixture-of-Experts (SMOE) model built from 8 Mistral-7B experts and purported to be on par with or outscore GPT-3.5 as well as Llama2 70B in several language understanding and LLM assistant tasks. It boasts a context length of 32k tokens through local attention and efficient sparse inference via a router network that selects 2 out of 8 experts for each layer and token. It exhibits low bias and hallucination tendencies while showing fluency in, among other languages, German. We make use of the Mixtral-8x7B-Instruct-v0.1 checkpoint 3."}, {"title": "3.4 Jina Embeddings", "content": "Mohr et al. (2024) propose the more focused and lightweight BERT-based Jina Embedding model. We use the German-English bilingual 161 million base-de version to generate embeddings for Retrieval augmented Generation."}, {"title": "3.5 Prompting Strategies", "content": "We use and describe here several prompting strategies from the literature, of which Sahoo et al. (2024) provide a systematic survey, with slight adaptations to our domain.\nIn Radford et al.'s (2019) zero-shot prompting, a model is only instructed with the task plus context and background knowledge, if available. This requires no labeled training data and is therefore compelling for novel and niche applications.\nIn contrast, Brown et al. (2020) show that integrating even a few examples can enhance model responses. This adaptation is quantitatively cheap but requires carefully chosen high"}, {"title": "4 Results", "content": "We evaluate the suitability of pre-trained generative LLMs on 4 datasets across the two tasks of argument mining and essay scoring. We first describe the datasets and accompanying evaluation methodology before reporting the main results as well as ablation studies."}, {"title": "4.1 Datasets", "content": "We evaluate the effectiveness of pre-trained generative LLMs on 4 datasets, with 2 each for argumentation mining and essay scoring, c.f. table 1 for their properties. For both tasks, we included a simpler and standard dataset to investigate the impact of outright task difficulty."}, {"title": "4.1.1 SPWSLE", "content": "The Dataset introduced in Structured Persuasive Writing Support in Legal Education: A Model and Tool for German Legal Case Solutions (Weber et al., 2023) comprises 413 student submitted solutions to 4 different legal cases with a mean length of 60.8 sentences. We abbreviate the classes as follows: MC = Major Claim, C = Conclusion, D = Definition, S = Subsumption, LC = Legal Claim, P = Premise, N = None.\nThis dataset splits the class Subsumption into two different subclasses, Premise and Legal Claim. The premise serves as a statement of the facts as they relate to the Definition, and the Legal Claim then matches the facts of the Premise to the Definition."}, {"title": "4.1.2 Graded Strafrecht Hausarbeiten", "content": "We present here the Graded Strafrecht HausArbeiten dataset which is distinguished by its small size as well as the complexity of its samples and the task. It is intended as a transfer learning challenge for models with high instruction and language capability as it does not provided enough data for full fine-tuning on the main task. As such, it encompasses 76 distinct student solutions to a legal problem complex with a final grade on an 18 point scale attached to each.\nThe student solutions were created by intermediate and advanced German law students in the course of their regular studies. They were provided with the facts of a criminal law case and tasked with solving it over several weeks. Use of legal commentaries, textbooks and databases was allowed. Out of a larger cohort, 76 students then provided their consent for the use of their work and its grade for our research. Afterwards, the works were corrected by research assistants on a scale from 0-18, with 18 being the best, and 4 being the minimum score required to pass."}, {"title": "4.1.3 ASAP AES", "content": "The Automated Student Assessment Prize (Hamner et al., 2012) contains English student essays for 8 different tasks. They are assigned combined scores between 10 and 60 points each. Here we select sets 1,2 and 8 as they are all from the Persuasive/Narrative/Expository category as per (Xiao et al., 2024). We focus on set 8 as it contains the longest and most complex documents, c.f. table 1."}, {"title": "4.1.4 CIMT", "content": "Romberg and Conrad's (2021) CIMT argument mining dataset consists of German language public feedback statements to 5 different infrastructure projects or concepts. These contributions are annotated on a sentence level with the categories Major Position, Premise, Major Position + Premise or None. It is therefore similar to SPWSLE, but with a simplified annotation schema and much shorter texts."}, {"title": "4.2 Evaluation Methodology", "content": "For all experiments, we report the performance of a linear SVM with Bag-of-Words features as an established baseline.\nWe re-phrase the classification of appraisal style components as a joint task, that involves the recognition of both the subsumption component as well as its sub-components in one pass. This is in contrast to (Weber et al., 2023), where the authors propose a two-tiered task that first extracts the main components and further sub-divides the subsumption. It is not exactly clear how they do this, however the presence of a None category in both tiers in their evaluation coupled with the absence of a respective category in the annotations hints at a usage as a masking placeholder. Furthermore there is no mapping of annotation categories to classes included. Our estimate is the following: e1=Major Claim, e2=Conclusion, e4=Definition, e5=Subsumption, e6=Legal Claim, e7=Premise, with e3 remaining unused throughout all data. We choose the joint task as it allows us to investigate the overall error of all components, rather than relying on the results of the previous stage or re-examining all components. However, we also report the results of our pre-trained LLM approach in the two-tiered setting as a point of comparison.\nIn contrast to the AES dataset used in (Rodriguez et al., 2019) and (Ormerod et al., 2021), the GSHA dataset does not contain scores from multiple annotators. We therefore substitute the Quadratic Weighted Kappa (QWK) with linear and rank correlation and accuracy for evaluation."}, {"title": "4.3 Gutachtenstil Argument Mining", "content": "Pre-trained LLMs cannot match the baseline extractive systems in SPWSLE Gutachtenstil argument mining. In both the two-tiered extraction task (cf. table 2) and the joint task (cf. table 3) they fall behind the Bag-of-Words model and underperform especially for the subsumption categories. This can partially be explained by the entanglement of these categories and removing the relevant context, i.e. Gutachtenstil explanation, counter-intuitively increases scores (c.f. table 2, Llama 310 NE). This can be viewed as a manifestation of the inaccurate interpretation of legal concepts identified in (Lai et al., 2023). While Retrieval Augmented Generation helps performance by selecting the most relevant example shots, the overall delta in accuracy and F1 score is still significant at 18.1% and 7.4% respectively.\nHowever, we observe the BoW model to be on par with Weber et al.'s (2023) BERT-based extractive classifier, offering a much more efficient mining system at equivalent effectiveness."}, {"title": "4.3.1 Retrieval Augmented Generation", "content": "To investigate the influence of RAG example selection, we perform a brittleness test by comparing the best and worst possible selection under the chosen criterion (cf. Table 10).\nWe observe this maximum performance margin to be 14.4% macro F1 and 17.1% accuracy overall with major regressions for the Major Claim and Legal Claim categories. With inverse RAG, providing the supposed least relevant 10 shots decreases performance below that of random example selection with a delta of 3.4% macro F1 and 4.6% accuracy. This hints that there are closely related or otherwise helpful samples in the training set, despite the disjointed documents, and that cosine similarity over embeddings can identify them. The retrieved example selection is not perfectly homogeneous, cf. figure 5, so there is still reasoning required from the model.\nThe worst-case selection of shots is still helpful in prompting, with a delta of 16.8% F1 and 23.9% accuracy over zero-shot prompting. These results confirm the overall benefit of RAG-based example selection and show that the penalty for sub-optimal selection is low."}, {"title": "4.3.2 Chain-of-Thought Prompting", "content": "The CoT strategy from (Wei et al., 2022) is not natively applicable to our few-shot prompting approach. We find that, if the model is presented with sample responses that do not also explain their reasoning, it will fail to do so for the current request as well. There are not detailed explanations available for SPWSLE, and while they could be generated for all training data, this would lead to a very expensive cold-start problem. As a compromise, we limit the generation of artificial reasoning data to a small portion of the available training data. This ensures that examples are provided to the model with sufficient quality and diversity, while remaining tractable. We further investigate whether the advantages of CoT prompting can outweigh the lack of example prompts by implementing a CoT zero-shot model.\nWe find that with CoT prompting, the Llama 3 shows some deficiencies following instructions. It is necessary to enforce an output format that ends with the category name to provide the explanation context before the final conclusion. This routinely leads to malformed responses with fictional categories and, despite the explicit target language instruction (c.f. prompt 4), in English instead of German. With increased distance between the instruction and result, attention naturally lowers with the positional encodings diverging. The effect that this has on instruction following outweighs any benefit of the more complex reasoning afforded by CoT prompting.\nWe therefore employ pattern matching and report the most frequent category in the model answer as the result.\nWhile the generated artificial reasoning shots are generally of high quality, they do not improve consistency. Effectively, this shows that there is error in the chain from feedback request to feedback to result. Therefore either the models capability in providing feedback or integrating feedback is not no par, which we view as a critical issue in education use."}, {"title": "4.3.3 Pseudonymization", "content": "With generative pre-trained models, providing new context is difficult. Especially in the legal domain, certain terms carry significant meaning that cannot adequately be expressed in a short definition. In the case of Gutachtenstil argument mining, we can check the presence of such meaningful internal representations in two ways. Firstly, we remove the Gutachtenstil explanation from the system prompt, relying on knowledge acquired at training time and through example shots alone, and find that it only has a small impact on performance (cf. table 2 Llama 310 NE).\nHowever, we can effectively remove internal information about the categories by pseudonymizing them via random permutation for each query to the model. With pseudonymization the model cannot recover any meaningful information, cf. table 5, confirming that this internalized knowledge is immediately necessary."}, {"title": "4.4 Public Feedback Argument Mining", "content": "As the CIMT dataset contains only 4 categories and their structure is clearer than SPWSLE, we find performance improved generally, cf. table 6. The features used for the SVM in (Romberg and Conrad, 2021) are not specified, so we were not able to reproduce their results. However, Llama 3 can be made competitive with the BoW+SVM baseline and even surpass it with 10 example shots curated via RAG. Again we find that CoT prompting is beneficial for the zero-shot case, however only marginally, but does not offer much, if any, improvement in few-shot prompting."}, {"title": "4.5 Legal Essay Scoring", "content": "Our selection of pre-trained LLMs does not perform well in legal essay scoring. Even though their context windows are large enough to include up to 6 examples, they cannot achieve prediction quality beyond random guessing (cf. fig. 6). Various techniques to lower task difficulty, such as simpler grading, additive scoring scale, partial scoring and chain-of-thought prompting did not yield any improvement. We surmise that this grading task is too complex and noisy to be solved with pre-trained generative models of this calibre."}, {"title": "4.6 Student Essay Scoring", "content": "The Automated Student Assessment Prize dataset represents a set of much easier scoring tasks than the GSHA. As the texts are both more plentiful and generally much shorter, long contexts and instruction following are less problematic. Accordingly, Llama 3 with 10 example shots outperforms the BoW baseline here with considerable margin, cf. table 7. However the zero-shot scenario regresses considerably in linear correlation and CoT does not offer any improvement. This suggests that even with the slightly more complex set 8 we are observing the limits of model capability.\nThe improvements over the baseline with Llama 310 are still a good sign for LLM-based AES in education settings."}, {"title": "5 Conclusion", "content": "We identified that instruction following and especially German language understanding, are key factors for a significant gap between LLM capabilities and the demands of German legal education. The models studied fail to demonstrate the logical structuring needed for legal understanding and thus cannot effectively score full legal essays. However, simpler English tasks show more promising results, suggesting that pre-trained LLMs could be useful with basic tasks or more advanced models.\nOur findings indicate that Llama 3's superior language comprehension trumps GPT-3.5's reasoning capabilities, highlighting the importance of linguistic skills in natural language communication. Most models benefit from few-shot prompting, while Chain-of-Thought prompting only helps in zero-shot scenarios. The failure of our Auto-CoT inspired approach providing generated reasoning before extracting final results shows a lack of feedback integration capability.\nWe identified certain limitations such as inference efficiency and target language understanding in the current generation of LLMs. Even with the efficiency gains provided by Grouped-Query Attention and Mixture-of-Expert models, we find that for incremental tasks, this makes them prohibitively expensive. For languages outside of the main training language, even with multilingual models, proficiency is still lacking.\nOverall, prompt-based extractive use of LLMs works well for simple tasks but underperforms in complex tasks with long contexts due to limited language understanding and instruction following. This can be partially mitigated with careful selection of examples.\nLimitations While pre-trained LLMs lift the large burden of training, the inference compute is still expensive, which cannot be solved with efficient prompting or fine-tuning alone.\nAs the examined models benefit from abundant training data, knowledge transfer to new within-domain tasks should be checked to gauge the efficiency of adaptation. However, it was not possible for us to conduct such a study as the linking to the case is not included in the documents or annotations.\nAs shown by much improved results in English language AES tasks, even with multilingual models, the proficiency in any but the main training language is still lacking."}, {"title": "A Appendix", "content": null}]}