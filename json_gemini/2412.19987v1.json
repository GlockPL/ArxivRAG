{"title": "DELAYED RANDOM PARTIAL GRADIENT AVERAGING FOR FEDERATED LEARNING", "authors": ["Xinyi Hu"], "abstract": "Federated learning (FL) is a distributed machine learning paradigm that enables multiple clients to train a shared model collaboratively while preserving privacy. However, the scaling of real-world FL systems is often limited by two communication bottlenecks: (a) while the increasing computing power of edge devices enables the deployment of large-scale Deep Neural Networks (DNNs), the limited bandwidth constraints frequent transmissions over large DNNs; and (b) high latency cost greatly degrades the performance of FL. In light of these bottlenecks, we propose a Delayed Random Partial Gradient Averaging (DPGA) to enhance FL. Under DPGA, clients only share partial local model gradients with the server. The size of the shared part in a local model is determined by the update rate, which is coarsely initialized and subsequently refined over the temporal dimension. Moreover, DPGA largely reduces the system run time by enabling computation in parallel with communication. We conduct experiments on non-IID CIFAR-10/100 to demonstrate the efficacy of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated learning (FL) [1]\u2013[3] is a privacy-preserving machine learning paradigm that enables multiple clients to jointly train a global model without sharing their local data. In contrast to conventional machine learning approaches that run on a central server, FL executes the training process on edge devices, where the intermediate parameters (e.g., weights or gradients) are exchanged between the central server and the clients. Although the ever-increasing computational capacities of edge devices make it possible to deploy a large Deep Neural Network at the edge devices, the communication bottleneck, including the limited bandwidth and severe latency incurred by the spectral scarcity and long distances connection between clients and central server, constraints the scalability of FL systems. In response, numerous methods [4]\u2013[7] have been proposed to address this crucial issue in FL from two perspectives.\nApproaches [6], [8] aim to improve communication effi- ciency via reducing the number of parameters uploaded per client to accommodate bandwidth limitations. For example, LG-Fed [9] only uploads the deep layer parameters, CD2- pFed [10] decouples the channels and only shares part of channel parameters, and gradient compression [11] upload partial gradient parameters. However, once the local parameters are uploaded to the central server, clients need to wait for the results before they can proceed to the next round of local computations, as Fig. 1.(a) shows, in these approaches with high network latency still. To reduce the system run time, a few approaches are proposed to take into account of the latency, such as DGA [5] delays the gradient averaging step and allows local computation in parallel to communication, zero-wait SFWFL [7] allows clients to continuously perform local computing without being interrupted by the global pa- rameter uploading on a server free wireless FL scenario, while these approaches all upload full model parameters as Fig. 1.(b) shown. It is worth noting that there is another asynchronous [12] FL approach that seems to solve a sim- ilar problem of latency, but what they actually solve is the heterogeneity between fast and slow clients, such as DAve- QN [13] propose a distributed asynchronous averaging scheme of decision vectors and gradients in a way to effectively capture the local Hessian information of the objective function.\nIn summary, the above methods only address one aspect of the communication limitation in terms of bandwidth or latency, resulting in the communication of FL will still be constrained.\nIn this paper, we propose Delayed Random Partial Gradient Averaging (DPGA) to simultaneously overcome the latency and bandwidth bottlenecks. On the one hand, to address the bandwidth limitation issue, we introduce an update rate to determine the number of uploaded parameters. Recognizing that during different communication rounds, the global model has a different influence on the clients' local computing performance, we employ the dynamic update rate based on the random walk process [14] to accelerate the global training process while concurrently reducing the overall communication overhead. It is worth noting that the architecture of the weights/gradient of client uploads varies in each commu- nication round. On top of the above, we proposed delayed partial gradient averaging so that clients continue their local computing during partial gradient communication and use these extra calculations to reduce the system run time as Fig.1.(c) shown. We evaluate our approach on CIFAR-10/100 datasets with non-IID settings. The results validate that our DPGA consistently outperforms the state-of-the-art methods in accuracy, communication cost, and run time."}, {"title": "II. PROPOSED METHOD", "content": "A. System model\nConsider an FL system consisting of a server and $N$ clients, in which client $i$ holds a local loss function $f_i(\\cdot): \\mathbb{R}^d \\rightarrow \\mathbb{R}$ constructed from its local dataset $D_i$. The objective of all the entities in this system is to find a global model $w \\in \\mathbb{R}^d$ that solves the minimization problem\n$\\min _{w} f(w)=\\sum_{i=1}^{N} \\frac{n_{i}}{n} f_{i}(w)$\n(1)\nwhere $n_i = |D_i|$ denotes number of local data samples of client $i$ and $n = \\Sigma_{i=1}^{N}|D_i|$ is the total number of training samples across the system. In a typical communication round $t$, a subset of clients $S_t$ are selected to conduct local training based on the latest global model weights $w_t$. Let $w_i^t$ denote the weights of client $i$'s model after local training. At the end of communication round $t$, the server would collect local models from the selected clients to update the global model via Federated Averaging (FedAvg [1]), i.e., $w^{t+1}=\\sum_{i \\in S_t} \\beta_{i} w_{i}^{t}$, in which $\\beta_i = |D_i|/\\sum_{j \\in S_t} |D_j|$ represents the ratio of the local data samples in client $i$ over the total number of data samples in the selected subset in communication round $t$.\nIn the partial gradient averaging paradigm of personalized federated learning, each client\u2019s gradient contains the personal and shared parts, although only the shared gradient will be exchanged between the clients and the global server. The ratio of the shared gradient to the total model gradient is defined as the update rate:\n$p^t=\\frac{|G_i^t|}{|G_i^t|+|G_i^{'t}|}$\n(2)\nwhere $|G_i^t|$ and $|G_i^{'t}|$ denote the amount of shared and personal gradient in the communication round $t$, respectively.\nB. Partial gradient averaging with dynamic update rates\nWe assign dynamic update rates $p \\in (0,1]$ for local models based on the random walk process to renew the portion of the personal and shared gradient periodically.\n1) Sample update rates based on the random walks: A one- dimension random walk in discrete time is comprised of a walker that flips a fair coin and moves one step to the right or the left, depending on the result of the coin toss [14]. In consequence, the direction taken by the walker at each step is independent of the direction of the previous ones [15]. The walker's position after $m$ steps can be expressed as\n$x' = x + X_1 + \\ldots + X_m$,\n(3)\nwhere $x$ is the initial position and $X_i \\in \\{-1,1\\}$ is a discrete random variable.\nApart from the probabilistic perspective, a one-dimension random walk can also be modeled by a Markov chain [16] to represent the stochastic process of sampling update rates. To be more concrete, in communication round $t$, we have a state $Q^t(p_t)$, denoting that the update rate equals $p_t$. Then, the update rate $p_{t+1}$ in the $t+1$-th communication round is obtained by an $m$-step random walk with starting point $p_t$ (the step size is set to 0.1). The transition probability from state $Q^t(p_t)$ to state $Q^{t+1}(p_{t+1})$ is given by\n$\\Phi_{\\tau}(p_{t+1})=\\frac{\\binom{m}{\\frac{m+10(p_{t+1}-p_{t})}{2}}}{2^{m}}$\n(4)\nwhere $C_x^y = \\frac{y!}{x!(y-x)!}$, $A = 10$. Equation (4) indicates that the larger the gap between $p_t$ and $p_{t+1}$, the smaller the transition probability. Moreover, the transition probabilities are normalized as\n$\\sum_{p_{t+1}} \\Phi_{\\tau}(p_{t+1})=1$.\n(5)\n2) Partial gradient averaging: Recall that the server and clients only exchange global shared gradient during the partial gradient averaging process.\nDue to the diverse architectures of the received partial gradi- ent, the server would update the global model via component- wise aggregation. For each component (i.e., layer, channel, weight, etc.), the global gradient $g_t$ are updated over the subset of clients whose architecture contains the corresponding component. For communication round $t$, client $i$ downloads the partial global gradient $g_t^i(p)$ and merges it with the partial local gradient $g_t^i(1-p)$. The merged result is given by\n$g_t^i = g_t^i(p) \\cup g_t^i(1 - p) = g_t^i + g_t^i(1-p)$,\n(6)\nThe expression of a typical partial gradient is given as\n$g_t^i(p_t) = g_{t m}^i(p_t)$,\n(7)\nin which $g_i$ is the gradient of the full model, $m_i(p_t)$ denotes the local personal mask of the client $i$. The local personal and global shared masks are complementary. We determined the location of the upload gradient parameters, whose local personal mask is set to 0, by Top-K sparsification [17]. And the dynamic 'K' determined by the update rate $p^t$. Then the rest of the global shared mask is set to 1.\nC. Delayed random partial gradient averaging\nConsider of the inevitable high communication latency of the gradient averaging, we propose the DPGA, which allows local updates during the download/upload partial gradient, to reduce the system run time. Specifically, the global gradient $g_t$ is delayed to a later iteration so that clients can immediately start the next round and a partial gradient correction term is designed to compensate the staleness.\nTo simplify the notation representation, we assume that all clients spend the same amount of time uploading and downloading partial gradients in each round of global com- munication. The communication time spreads over $D$ local computing rounds, which is in total $tD$ iterations. Then, as shown in Fig. 1.(c), the DPGA no longer freezes the local computation power during the communication. To make the discussion more explicit, we denote the model weights on the i-th client at the k-th iteration within the t-th round by $w_i^{t,k}$, and the corresponding stochastic gradient as $g_i^{t,k}$. In the first round of local computing, each client $i$ executes $K$ stochastic gradient descent (SGD) iterations and arrives at the following:\n$w_i^{1,K+1} = w_i^{1,K} - \\eta g_i^{1}(w_i^{1,k})$\n$= w_i^{0} - \\eta \\sum_{k=1}^{K} g_i^{1}(w_i^{1,k})$\n$= w_i^{0} - \\eta z_i^1$\n(8)\nwhere, $w_i^0$ is the initial weights of the local models. With the end of the first round's computation, the accumulated partial gradient $z_i^1 := \\sum_{k=1}^{K} g_i^{1}(w_i^{1,k})$ is sent to all the clients, i.e. partial gradient averaging. Then, we immediately execute the second round's local updates, leaving the first round averages in transmission. When the global partial gradient is received, client $i$ has already performed $D$ extra rounds of local updates and its model weights can be written as:\n$w_i^{1+D,1} = w_i^{1,K} - \\eta f_i^{1}(w_i^{1,K})$\n$= w_i^{1} - \\eta z_i^{1+D}$\n$= w_i^{0} - \\eta (z_i^1 + z_i^2 + \\ldots + z_i^D)$.\n(9)\nAs the global gradient $g^1$, of the first round is now available, client $i$ can substitute the shared part of the first round local gradients by the global one\n$w_i^{1+D,1} = (w_i^{0} - \\eta z_i^{1}) - \\eta (z_i^{2} + \\ldots + z_i^D)$\n$= w_i^{1} - \\eta z_i^2 - z_i^1(p) + g_i^1(p)),$\n(10)\nwhere, $z_i^t(p) = \\sum_{k=1}^{K} g_k^i(p)$ represents the global shared part gradient of the client $i$ in the t-th communication round.\nThe details of DPGA is elucidated in Algorithm 1. This algorithm contains two major component: (a) The partial gradient communication with dynamic update rates, which breakthrough the bandwidth bottleneck, and (b) Delayed gradi- ent aggregation, which allows local computation in parallel to communication. A comparison among the sequential training procedure where computations are suspended during global communications, parallel training procedure and our DPGA is provided in Fig. 1."}, {"title": "III. NUMERICAL RESULTS", "content": "In this section, we conduct simulations to evaluate the performance of DPGA on a LeNet-5 [18] over the CIFAR- 10 [19] and on a ResNet-34 [20] over the CIFAR-100 [19] with IID and non-IID data heterogeneity settings. We adopt Dirichlet distribution-based non-IID data partition method [21] for CIFAR-10/100, where the identicalness of local data dis- tribution and the class imbalance could be controlled by the parameter pair $(\\alpha, \\rho)$. We compare our DPGA with conven- tional FL method (FedAvg), partial gradient averaging (LG- Fed) methods, and delayed gradient averaging (DGA) methods from three dimensions, i.e., test accuracy, communication time, and communication parameters. The total number of bytes transmitted through uplink (clients-to-server) and downlink (server-to-clients) connections represents the communication cost throughout the training. Also, we express the commu- nication time as a ratio of the communication to the local computation time. Since the FedAvg, LG-Fed, DGA, and DPGA are trained locally with the full model, it can be assumed that the local computation time per round is the same for these four methods. All simulations are conducted on NVIDIA TITAN Xp GPU.\nFirst, we compare the performance of four different fed- erated learning methods on CIFAR-10. The results showed that DPGA outperformed the other methods in all settings, achieving the highest ac- curacy. For example, in the IID setting, DPGA achieved a test accuracy of 93.08%, while the other methods FedAvg, DGA, and LG-Fed achieved 60.08%, 61.02%, and 62.14%, respectively. As the degree of data heterogeneity increases, such as at $(\\alpha, \\rho)=(0.1,1)$, DPGA still maintains a high test accuracy (86.01%) while the test accuracy of other methods does not exceed 60%.\nTo provide a comprehensive evaluation, we also compared our method with the other methods on CIFAR-100. The same as CIFAR-10, we conducted simulations on four dif- ferent data heterogeneity settings, including IID, three set- tings based on Dirichlet distribution partitioning $(\\alpha, \\rho) = (10, 0.7), (1, 1), (0.1,1)$. The whole CIFAR-100 dataset is participated in 100 non-overlapped portions and assigned to the $N = 100$ clients in all settings. Table II shows that our DPGA still outperforms other methods in all data heterogene- ity settings, achieving the highest accuracy. Specifically, in the highly non-IID setting $(\\alpha, \\rho) = (0.1,1)$, the DPGA achieves an excellent accuracy of 80.06%, which is significantly higher than the second-best performing algorithm, LG-Fed, with an accuracy of 50.39%.\nMoreover, our DPGA significantly improves communication efficiency. Figure 2 visualize the training process of the four methods using the non-IID CIFAR-10/100 setting $((\\alpha, \\rho) = (10,0.7))$ as an example. To achieve 60% test accuracy, the required communication time of DPGA is only 150 commu- nication time, while LG-Fed and DGA require about 500, and FedAvg requires more. In addition, DPGA requires only 6.7 Gb of communication parameters, which is 4% of FedAvg and DGA. Even the second-best performing LG-Fed requires three times the parameters of DPGA to achieve 60% accuracy. Figure 2 (c)&(d) visualize the training process under the CIFAR- 100 dataset. When achieving 40% test accuracy, the communi- cation time of DPGA is only 36, while FedAvg, DGA, and LG- Fed are 349, 234, and 152, respectively. From the perspective of communication parameters, our DPGA also has a much smaller overhead than the other three methods. Specifically, DPGA requires only 67.3 Gb of parameters to achieve the target accuracy, while the other three methods require com- munication parameters ranging from 247 Gb to 357 Gb.\nCompare to the conventional FL method (FedAvg), the method for solving latency problems (DGA) achieves the target accuracy in a shorter time. Since it exchanges the full model between clients and server, the communication parameters are not reduced. And the method for solving bandwidth limitations (LG-Fed) achieves the target accuracy with fewer communication parameters but requires a longer communication time than DGA. Our DPGA solves both la- tency and bandwidth bottlenecks and performs much better than DGA and LG-Fed in terms of communication time and communication parameters when achieving the target accu- racy. In addition, DPGA achieves much higher test accuracy in a variety of heterogeneous settings than the other methods.\nComprehensive simulations on different datasets demon- strate that our DPGA consistently outperforms those conven- tional FL methods with the extremely much less communica- tion time and parameters."}, {"title": "IV. CONCLUSION", "content": "In this paper, we proposed DPGA for FL. By adopting partial gradient exchange and the parallel strategy for commu- nication and local computation, our scheme is able to break through both bandwidth and latency bottlenecks. We evaluate our approach on CIFAR-10/100 datasets with non-IID settings. The results validate that our DPGA consistently outperforms the state-of-the-art methods in accuracy, communication cost, and run time."}]}