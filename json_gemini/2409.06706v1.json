{"title": "Discovering Long-Term Effects on Parameter Efficient Fine-tuning", "authors": ["Gaole Dai", "Yiming Tang", "Chunkai Fan", "Qizhe Zhang", "Zhi Zhang", "Yulu Gan", "Chengqing Zeng", "Shanghang Zhang", "Tiejun Huang"], "abstract": "Pre-trained Artificial Neural Networks (ANNs) exhibit robust pattern recognition capabilities and share extensive similarities with the human brain, specifically Biological Neural Networks (BNNs). We are particularly intrigued by these models' ability to acquire new knowledge through fine-tuning. In this regard, Parameter-efficient Fine-tuning (PEFT) has gained widespread adoption as a substitute for full fine-tuning due to its cost reduction in training and mitigation of over-fitting risks by limiting the number of trainable parameters during adaptation. Since both ANNs and BNNs propagate information layer-by-layer, a common analogy can be drawn: weights in ANNs represent synapses in BNNs, while features (also known as latent variables or logits) in ANNs represent neurotransmitters released by neurons in BNNs. Mainstream PEFT methods aim to adjust feature or parameter values using only a limited number of trainable parameters (usually less than 1% of the total parameters), yet achieve surprisingly good results. Building upon this clue, we delve deeper into exploring the connections between feature adjustment and parameter adjustment, resulting in our proposed method Synapses & Neurons (SAN) that learns scaling matrices for features and propagates their effects towards posterior weight matrices. Our approach draws strong inspiration from well-known neuroscience phenomena - Long-term Potentiation (LTP) and Long-term Depression (LTD), which also reveal the relationship between synapse development and neurotransmitter release levels. We conducted extensive comparisons of PEFT on 26 datasets using attention-based networks as well as convolution-based networks, leading to significant improvements compared to other tuning methods (+8.5% over fully-finetune, +7% over Visual Prompt Tuning, and +3.2% over LORA). The codes would be released.", "sections": [{"title": "Introduction", "content": "The use of large-scale, pre-trained models demonstrates their robust adaptability to various downstream datasets and tasks through fine-tuning techniques. However, performing full fine-tuning by adjusting all parameters imposes significant computational and data costs. In this context, the concept of parameter-efficient fine-tuning (PEFT) aims to reduce the number of adjustable parameters during fine-tuning, resulting in fewer parameters, faster training speed, and a lower risk"}, {"title": "", "content": "of overfitting [9]. By adhering to this rule-of-thumb strategy, the cost burdens above can be alleviated simultaneously.\nCompared to full fine-tuning, in the early stage of PEFT history, most methods were referred to as \"partial fine-tuning\" and focused on releasing a subset of parameters for adjustment. For example, linear probing was the simplest method that only released the head parameters of the model for fine-tuning. More advanced approaches like Bitfit (i.e., bias tuning) [32] chose to release biases and achieved better global adjustment capabilities. Recent concepts such as \"sparse training\" introduced more sophisticated selection mechanisms by utilizing gradients or parameter magnitudes [11]. However, these methods still faced challenges where the subset of parameters lacked representative abilities of the entire model. Consequently, researchers shifted their focus towards directly adjusting the output features of each layer. The most popular approach in this regard is the Low-rank Adapter (LoRA) family [13], which employs two low-rank sequential learnable matrices (down and up) alongside each layer to simulate additional parameter activities while simultaneously handling input with original parameters; thus resulting in final features being a summation of LoRA's output and original output. The success of LoRA indicated that adjusting features would be more efficient and implementation-friendly. Other notable works in feature adjustment include Visual Prompt Tuning (VPT) [14,20], where extra learnable tokens are concatenated with each layer's feature, and Scale and Shift Features (SSF) [19], which applies a linear transformation to features using learnable shift and scale factors.\nFrom a mathematical perspective, we can consider feature transformations as approximation of parameters tuning. Operations such as addition, concatenation, and linear transformation on features essentially perform unified transformations on the weighted sum results of each channel in the parameter matrices. This viewpoint provides a more generalized framework for understanding PEFT methods. For instance, let W be the original parameter matrix of a layer, and x be the input feature. The output feature y is typically computed as y = Wx. In PEFT methods that transform features:\nAddition (as in LoRA):\n\u2022 y' = Wx + Ax, where A is a low-rank matrix and can be viewed as a transformation on W: W' = W + A.\nConcatenation (as in VPT):\n\u2022 y' = W[x; p], where p is the prompt vector and effectively extends W to W' = [W, Wp], where Wp is the weight for the prompt.\nLinear transformation (as in SSF):\n\u2022 y' = \\alpha (Wx) + \\beta, where \\alpha and \\beta are learnable scale and shift factors and can be seen as transforming W to W' = \\alpha W, with an additional bias term.\nThese feature-level operations can be interpreted as implicit transformations of the entire parameter space, offering a more flexible and efficient way to adapt the model [35]. By operating on features, we're essentially performing a form of"}, {"title": "", "content": "meta-learning, where the model learns how to model its original parameters indirectly through the additional parameters created for feature modifications.\nThe success of feature-based PEFT methods raises an intriguing question: Should those additional parameters created for feature modifications in one layer affect the parameters of subsequent layers? We found these aforementioned methods overlooked this and focused only on the bandwidth of the current layer, yet it finds significant resonance in neuroscience, particularly in the phenomena of Long-Term Depression (LTD) and Long-Term Potentiation (LTP). In neuroscience, LTD and LTP are well-established mechanisms of synaptic plasticity that play crucial roles in learning and memory formation [4,24]. LTP refers to the strengthening of synaptic connections, while LTD refers to their weakening. Specific patterns of short-term neural activity typically induce these processes and can persist for extended periods, hence the term \"long-term\" [6].\nA key aspect of LTD and LTP is their ability to induce changes in the immediate synaptic connection and subsequent neurons along the pathway. For instance, studies have shown that by modulating the neurotransmitter release levels of presynaptic neurons (often through pharmacological or optogenetic methods), researchers can observe changes in the synaptic development of downstream neurons [25, 29]. This trans-synaptic effect, known as Heterosynaptic Plasticity, suggests that local changes can propagate and influence broader neural networks [5].\nDrawing an analogy between neural networks and biological neural systems, we can consider features analogous to neurotransmitters and parameter matrices as synapses. Following this, a natural extension of current PEFT methods would be to allow feature adjustments in one layer to propagate and influence the parameters of subsequent layers explicitly. As a brief introduction to our proposed SAN method, for each layer, we first conduct trainable scaling for each feature to mimic the rapid change in pre-synaptic neuron's neurotransmitter level when exposed to a stimulant. Further on, we propagate those scaling factors to the next layer's parameters. This simulates the further effect of post-synaptic neuron development i.e. Heterosynaptic Plasticity. Notice the trainable parameters for SAN are the scaling factors with the exact shape as the feature size, this is very efficient and can be considered as a degraded LoRA with a bottleneck i.e. rank size equal to one. However, we outperformed it by a large margin.\nIn the upcoming Sec. 3, we will show how SAN gives more fine-grained parameter adjustment abilities from the aspect of reparameterization [36] and the regularization abilities of SAN to prevent over-fitting. We will also demonstrate our hypothesis that the existing PEFT method implicitly propagated adjustment to further layers. In contrast, SAN propagates this explicitly and simplifies the tuning task by narrowing the searching area for an optimally tuned subspace."}, {"title": "Related works", "content": "Parameter-efficient fine-tuning The most straightforward method for PEFT is linear probing [1]. By simply adding or modifying a trainable new head, the pre-trained model can adapt to new tasks. However, the expressiveness of the linear probing method is limited. An intuitive improvement proposed in Bitfit [32] involves unfreezing the bias. A more efficient approach is prompt tuning [14,20], which adjusts the inputs. Recent work, such as sensitivity-aware PEFT, analyzes weight magnitudes or accumulated gradients on specific datasets to discover model sparsity and focuses only on tuning those areas. In addition to focusing on adjustment locations, there are also differences in how adjustments are made. The adapter [34] is a commonly used approach that adds an extra layer along or inserts it into the pre-trained model; features pass through this adapter and output an adjustment value for different adjustment operations (etc. addition, production, and concatenation). Another efficient adapter style is Low-Rank Adapter (LoRA) [13], which uses two low-rank matrices to equalize a dense layer. This adapter style can be further quantized into Q-LORA [8] and other types of"}, {"title": "", "content": "matrix decompositions. Reparameterization offers a completely different style of adjustment by directly shifting and scaling features; this remapping technique is known as Shift and Scale PEFT [19].\nLong-Term Depression/Potentiation and Heterosynaptic Plasticity Long-Term Depression (LTD) and Long-Term Potentiation (LTP) are fundamental mechanisms of synaptic plasticity in biological neural networks, playing crucial roles in learning and memory formation [24]. LTP refers to the strengthening of synaptic connections, while LTD involves their weakening, both persisting for extended periods [3]. These processes are primarily triggered by specific patterns of neuronal activity and are often considered Hebbian in nature, following the principle that \"neurons that fire together, wire together\" [12]. However, the discovery of heterosynaptic plasticity has expanded our understanding of synaptic modulation beyond this simple associative rule [5]. Heterosynaptic plasticity refers to changes in synaptic strength that occur at synapses that are not directly involved in the inducing activity [2]. This form of plasticity allows for more complex and distributed forms of information storage and processing in neural networks [23]. For instance, when LTP is induced at one set of synapses, heterosynaptic LTD may occur at nearby inactive synapses, potentially serving as a homeostatic mechanism to maintain overall network stability [28]. The molecular mechanisms underlying these forms of plasticity involve complex cascades of intracellular signalling. Heterosynaptic plasticity, involves unique signalling pathways, including the spread of intracellular messengers and the release of diffusible factors that can affect nearby synapses [27]."}, {"title": "Methods", "content": "3.1 Preliminaries\nTransformers: For vision transformers (VIT), RGB input image with shape I \\in R^{3\\times H\\times W} is divided into N \\times N patches. A convolution layer is used to convert patches to embeddings, with an extra class token appended to the end. The input for transformer blocks is x \\in R^{(N^2+1)\\times d}, where d is the dimension for each embedding. These embeddings use self-attention algorithms to calculate the dependencies.\nThe attention mechanism is defined as:\nAttention(Q, K, V) = Softmax(\\frac{Q K^T}{\\sqrt{d}}) V \\qquad(1)\nwhere queries, keys, and values Q, K, V \\in R^{(N^2+1)\\times d}.\nLORA & Adapter: These PEFT methods use two low-rank learnable matrices (Down and Up) to simulate the full rank dense layers.\nout = [W_{up}(W_{down}x^T)]^T, \\qquad(2)\nwhere W_{down} \\in R^{d'\\times d}(d' < d), 4, and W_{up} \\in R^{d \\times d} represent the down-projection matrix, non-linear function, and up-projection matrix, respectively."}, {"title": "", "content": "Visual Prompt Tuning: This PEFT method concatenates a learnable prompt p\\in R^{n\\times d} to each input x, resulting in x' = [x; p] \\in R^{(N^2+n+1)\\times d}.\nScale & Shift Features: This PEFT method applies a learnable linear transformation to each layer's output y' = \\gamma y + \\beta \\in R^{(N^2+1)\\times d}, where \\gamma, \\beta \\in R^d are the scaling and shifting factors, respectively, and is the element-wise product. The reparameterize formula is:\ny' = \\gamma \\odot y + \\beta = \\gamma \\odot (w * x + b) + \\beta = (\\gamma \\odot w) * x + \\gamma \\odot b + \\beta, \\qquad(3)\nwhere b, w, and x are bias, weight, and input for this layer. * is convolution or multiplication operation in convolution or MLP layer. SSF is indeed highly efficient, however, this method, which we considered, ignores the effect of presynaptic stimulus and causes post-synaptic development, so our major modification would be conducted on it."}, {"title": "SAN", "content": "Basic Formula: Similar to SSF, the scaled output y\" of layer I can be described as y'\" = y' \\odot \\gamma', where \\gamma is the scaling factor (we initialize it to one) of our SAN adapter and y' is the original output of the linear transformation. Then, the output goes through a set of operations such as activation function or normalization, denoted \\sigma(\\cdot). We consider the scaling of output in this layer, i.e., \\gamma' would pose a further effect towards the next layer's parameters (similar to the LTD/P effect found in BNNs), so we further apply it to scale the parameters in the next layer w^{l+1}. The parameters for the next layer would be scaled to w^{l+1} = \\gamma' \\odot w^{l+1}; therefore the output for the next layer is:\ny'^{l+1} = \\gamma^{l+1} \\odot (w'^{l+1} * \\sigma(y'\") + b^{l+1}), \\qquad(4)\nwhere b^{l+1} is the bias of the next layer.\nReparameterization: As introduced in Eq. 3, the reparameterization of SSF implies a strong assumption: for each row of the current layer's weight matrix, the scaling & shifting factors would be the same. In contrast, our SAN method introduces a critical re-application process of the scaling factor to the next layer's weight matrix. This approach allows us to achieve a unique adjustment value for every individual parameter without incurring any extra training burden.\nBy propagating the scaling factor \\gamma^{l-1} from the previous layer to the current layer's weight, we can overcome the strong assumption of SSF and achieve a more fine-grained adjustment. The reparameterization formula of SAN can be expressed as:\ny = \\gamma' \\odot (\\gamma'^{l-1} \\odot w^l * x + b^l) + \\beta^l\n= (\\gamma' \\odot \\gamma'^{l-1} \\odot w^l) * x + \\gamma^l \\odot b^l + \\beta^l, \\qquad(5)\nwhere \\gamma^{l-1} is the scaling factor from the previous layer, \\gamma^l and \\beta^l are the scaling and shifting factors for the current layer, w^l and b^l are the weight and bias of the current layer, and x is the input."}, {"title": "", "content": "Regularization: The re-application of scaling factors in SAN not only provides fine-grained parameter adjustment but also introduces an implicit regularization effect to prevent overfitting. This regularization emerges from the approximate quadratic nature of the scaling factor's influence when propagated through layers. To illustrate this, let's consider a simplified two-layer linear network scenario without any activation and normalization:\ny'^{l+1} = \\gamma^{l+1} \\odot ((\\gamma' \\odot w^{l+1}) * (\\gamma' \\odot x^{l+1}) + b^{l+1}) \\qquad(6)\nRearranging this equation, we get:\ny'^{l+1} = (\\gamma^{l+1} \\odot \\gamma' \\odot \\gamma' \\odot w^{l+1}) * x^{l+1} + \\gamma^{l+1} \\odot b^{l+1} \\qquad(7)\nThe presence of (\\gamma')^2 in this formulation reveals a crucial property: the effect of the scaling factor is essentially squared when propagated through layers. This quadratic influence acts as a soft constraint on the magnitude of \\gamma', discouraging extreme values and promoting stability. To formalize this regularization effect, we can express it as an implicit regularization term R(\\gamma) added to the loss function:\nR(\\gamma) = \\lambda \\sum_{l} ||\\gamma^l - 1||_2 \\qquad(8)\nwhere \\lambda is a hyperparameter controlling the strength of regularization, this regularization term penalizes large deviations of \\gamma from its initial value of 1, effectively limiting the model's capacity to make extreme adaptations.\nExplicit Propagation: The key innovation of our SAN method lies in explicitly propagating the scaling factor of the current layer to the parameters of the subsequent layer. This approach is motivated by a fundamental insight into the nature of linear transformations in neural networks: any linear transformation applied to features for Parameter-Efficient Fine-Tuning (PEFT) implicitly affects the subsequent layer's parameters.\nTo elaborate, consider a linear transformation T applied to the features f of layer l:\nf' = T(f) \\qquad(9)\nThe output of the subsequent layer l + 1 with weight W and bias b can be expressed as:\ny = WT(f) + b \\qquad(10)\nDue to the linearity of the operations, this is equivalent to:\ny = (W \\cdot T) \\cdot f + b = W' \\cdot f + b \\qquad(11)\nwhere W' = WT is an adjusted weight matrix."}, {"title": "", "content": "This equivalence reveals that any linear transformation of features in layer l can be equalized as an adjustment to the weights of layer l+1, assuming no non-linear activations are applied between these operations. In essence, methods that apply linear transformations to features are implicitly learning an adjustment matrix for the subsequent layer's weights.\nWhile this principle is straightforward in purely linear scenarios, real-world neural networks incorporate non-linear activations and normalization layers. However, we argue that our approach remains approximately valid even in these more complex settings. This approximation is based on two key observations:\n1. Near-linear behavior of modern activation functions: Many popular activation functions, such as ReLU and its variants, exhibit approximately linear behavior in certain regions. This near-linearity allows our linear transformation principle to hold to a good approximation over significant portions of the input space.\n2. Adaptive re-calibration of scaling factors: To account for the effects of non-linearities and normalization, SAN introduces an additional learnable linear transformation before re-applying the scaling factor to the next layer's weights. This can be expressed as:\n\\gamma' = A\\gamma^2 + b^2 \\qquad(12)\nwhere \\gamma'' is the recalibrated scaling factor, and A^l and b^l are learnable parameters. The weight adjustment for the next layer then becomes:\nW^{l+1} = W^{l+1} \\odot \\gamma' \\qquad(13)\nThis adaptive re-calibration allows SAN to:\nCompensate for the non-linear effects introduced by activation functions and normalization layers.\nFine-tune the propagation of scaling factors to better suit the specific characteristics of each layer and the task.\nMaintain the benefits of weight adjustment while adapting to the complexities of modern neural architectures."}, {"title": "Experimental Evaluation", "content": "To assess the efficacy of our proposed SAN, we conducted extensive experiments across a diverse range of visual datasets. This section outlines our experimental framework, including the datasets utilized, the backbone architectures employed, and the baseline methods we compared against. We then present our findings, demonstrating SAN's performance and versatility. Additionally, we provide an in-depth analysis of various scaling strategies and their impacts through comprehensive ablation studies."}, {"title": "Experimental Framework", "content": "Dataset Selection Our evaluation leverages a variety of datasets, categorized into three distinct groups:\nFine-Grained Visual Classification (FGVC): This category comprises five specialized tasks, utilizing datasets such as CUB-200-2011 [31], NABirds [30], Oxford Flowers [26], Stanford Dogs [15], and Stanford Cars [17].\nVisual Task Adaptation Benchmark (VTAB-1k): [33] This benchmark encompasses 19 diverse visual classification tasks, organized into Natural, Specialized, and Structured subsets.\nGeneral Image Classification: We include CIFAR-100 [18] and ImageNet-1k [7], two representive benchmarks in the field.\nModel Architectures To ensure a fair comparison with existing methods, our primary experiments employ ViT-B/16 [10], pre-trained on ImageNet-21K [7]. To further demonstrate SAN's adaptability, we extend our experiments to include Swin Transformer (Swin-B) [21] and ConvNeXt-B [22], representing state-of-the-art Transformer-based and CNN-based architectures, respectively.\nComparative Methods We evaluate SAN against a spectrum of fine-tuning approaches, broadly classified into three categories:\nFull Model Tuning: This conventional approach involves updating all model parameters during the fine-tuning process.\nParameter Tuning Methods: These techniques fine-tune a subset of the original model's parameters. Examples include linear probing and Bias tuning [32]. While computationally efficient, these methods have historically shown limited effectiveness. Our proposed SAN falls within this category, aiming to overcome previous limitations while maintaining efficiency and broad applicability.\nFeature Tuning Methods: These methods introduce additional trainable parameters to the model, such as Adapter [34] and Visual Prompt Tuning (VPT) [14]. While effective, they often incur extra computational costs during both training and inference. Some variants, like LORA [13] and SSF [19], allow for parameter reparameterization, potentially mitigating inference-time overhead.\nImplementation Specifics Our image processing pipeline follows the protocol established by SSF for the FGVC, VTAB-1k, and CIFAR-100 datasets. We optimize our models using Adam/AdamW [16] with a cosine learning rate decay schedule over 100 epochs, incorporating a linear warm-up phase for the initial 10 epochs. All experiments were conducted using a distributed setup across four NVIDIA RTX 3090 GPUs to ensure the timely completion of our comprehensive study."}, {"title": "Performance with Vision Transformer as Backbone", "content": "Tab 1 presents a comprehensive comparison of our proposed SAN method against other state-of-the-art fine-tuning approaches using Vision Transformer (ViT-B) as the backbone. The results clearly demonstrate the effectiveness and efficiency of SAN across a wide range of tasks and datasets.\nOne of the striking aspects of SAN's performance is its parameter efficiency. While LORA, we maximum its bottleneck dimension around the 1% constraint and serves as a strong baseline, SAN achieves superior performance using only 0.20% of the parameters. This parameter efficiency is comparable with SSF since re-apply operations do not introduce extra burdens. Nevertheless, SAN shows remarkable improvements over its competitors, even in challenging subsets of the VTAB dataset such as Specialized and Structure.\nSAN also shows consistent performance across diverse datasets - from FGVC which focuses on fine-grained classification tasks with moderate training images to VTAB-1k which focuses on challenging varieties of subset and limited training images and more general image classification tasks like CIFAR100 and ImageNet-1k with sufficient training images - underscores its versatility and robustness. Notably, SAN outperforms full fine-tuning in many cases, despite using only a fraction of the parameters, we believe the key is SAN have a great balance between expressiveness and preventing overfitting."}, {"title": "Performance with Different Backbones", "content": "To demonstrate the versatility of our SAN method, we conducted experiments using three different backbone architectures: Vision Transformer (ViT-B), Swin Transformer (SWIN-B), and ConvNeXt (ConvNeXt-B). Figure 2 illustrates the performance of various fine-tuning methods across these backbone architectures.\nAs evident from the radar chart, SAN consistently outperforms other fine-tuning methods across all three backbone architectures. This performance consistency demonstrates the robustness and adaptability of our proposed method. The chart also reveals interesting patterns in the performance of different methods. For instance, while LoRA shows competitive performance with transformer-based models, its effectiveness slightly diminishes with the ConvNeXt-B architecture. In contrast, SAN maintains its leading position across all backbones, suggesting a more generalized approach to parameter-efficient fine-tuning."}, {"title": "Ablation Studies", "content": "To investigate the effectiveness of our proposed SAN method, we conduct ablation studies focusing on separating two key components: modeling of the current layer by a set of learnable scaling factors and propagation of the learned scaling factors to the next layer. These studies aim to quantify the contribution of each component and validate our design choices. Fig 3 illustrates the performance of various methods, including the aforementioned settings and some other fine-tuning strategies."}, {"title": "", "content": "It is clear that both modeling the current layer strategy and propagate to the next layer strategy can work as a decent PEFT method alone, however, when used together, the improvement would be more complete with a higher expressivity."}, {"title": "Conclusion and Future Work", "content": "The primary contribution of our paper is the introduction of the concept of propagating the feature adjustment value forward to the parameters of the next layer. This concept is motivated by Heterosynaptic Plasticity observed in BNN during LTP/D occurrences. We have conducted an analysis on the properties of this propagation, demonstrating its regularization abilities and how it enhances fine-grained expressivity through reparameterization perspectives. Moreover, we hypothesize that current feature tuning methods implicitly propagate, but by making this propagation explicit, we can simplify the learning process. Our experiments validate our concept, and we believe future work should focus on discovering how to propagate and re-apply additional parameters created for adjusting certain layer's features to more layers. By doing so, we can achieve even lower training costs and emphasize the interconnections between different layers."}]}