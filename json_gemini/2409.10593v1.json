{"title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios", "authors": ["Luning Wang", "Shiyao Li", "Xuefei Ning", "Zhihang Yuan", "Shengen Yan", "Guohao Dai", "Yu Wang"], "abstract": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV cache\ncompression methods typically focus on quantization and token pruning, which\nhave compression limits, and excessive sparsity can lead to severe performance\ndegradation. Other methods design new architectures with le ss KV overhead but\nrequire significant training overhead. To address the above two drawbacks, we\nfurther explore the redundancy in the channel dimension and apply an architecture-\nlevel design with minor training costs. Therefore, we introduce CSKV, a training-\nefficient Channel Shrinking technique for KV cache compression: (1) We first\nanalyze the singular value distribution of the KV cache, revealing significant\nredundancy and compression potential along the channel dimension. Based on this\nobservation, we propose using low-rank decomposition for key and value layers\nand storing the low-dimension features. (2) To preserve model performance, we\nintroduce a bi-branch KV cache, including a window-based full-precision KV\ncache and a low-precision compressed KV cache. (3) To reduce the training costs,\nwe minimize the layer-wise reconstruction loss for the compressed KV cache\ninstead of retraining the entire LLMs. Extensive experiments show that CSKV\ncan reduce the memory overhead of the KV cache by 80% while maintaining\nthe model's long-context capability. Moreover, we show that our method can be\nseamlessly combined with quantization to further reduce the memory overhead,\nachieving a compression ratio of up to 95%.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been widely adopted in various natural language processing\ntasks, particularly those requiring long-context capabilities, such as document analysis and fact\nretrieval [6]. However, the key-value (KV) cache mechanism used in transformer-based LLMs poses\nsignificant efficiency challenges as its memory overhead grows linearly with the sequence length,\noften replacing the weights to be the memory bottleneck in long-context scenarios. For instance,\nprocessing a sequence with 200K tokens using LLaMA-2-7B [17] results in a KV cache occupying\naround 100GB, compared to 14GB required for model weights. Compressing the KV cache by over\n10x is necessary to fit such a sequence on a single NVIDIA RTX 4090 GPU with 24GB of memory.\nExisting KV cache compression methods, mainly training-free techniques like token pruning [22, 12,\n18, 8] and quantization [10, 13, 11, 15], struggle to maintain model performance at high compression\nratios, particularly in long-context tasks. Alternatively, training-required techniques, such as MLA [3]"}, {"title": "2 Method", "content": ""}, {"title": "2.1 Inference with Bi-Branch KV Cache", "content": "To reduce the memory overhead, we design to reduce the memory overhead of the KV cache by\nusing low-rank decomposition for both the Key and Value weight matrix. Without loss of generality,\nwe will detail the workflow of compressing the key cache, as the process is identical to that of the\nvalue cache.\nAs shown in Figure 1, we use two matrices, \u0410\u043a \u2208 \u211d^{h_{in}\u00d7h_{comp}} and BK \u2208 \u211d^{h_{comp}\u00d7h_{out}}, to\napproximate the weight matrix of WK \u2208 \u211d^{h_{in}\u00d7h_{out}}. Here the $h_{in}, h_{out},h_{comp}$ are the input\ndimension of WK, the output dimension of WK, and the intermediate dimension of the low-rank\ndecompression. Keeping the $h_{comp}$ smaller than $h_{out}$ and storing the intermediate features as\nthe compressed Key cache, we can significantly save the memory overhead, especially in the long\ncontext scenario.\nTo maintain the high performance, we propose to follow the prior research by preserving the\nrecently used tokens [1, 18] because they are crucial for accurate next-token prediction. To prevent\nthe degradation of this local information during inference, we propose the bi-branch KV cache that\npreserves the recently used tokens effectively during both the prefilling and decoding stages. With a\npre-defined window size lw, we compress the KV cache only after the tokens fill a complete window\nwhile retaining the residual tokens in their original hidden dimensions.\nSpecifically, for the prefilling stage, as shown in Figure 1(a), given an input sequence with n tokens,\nwe first use the AK to generate the compressed Key matrix and store it in the Compressed Key Cache\nKc. In this case, the Compressed Key Cache contains all of the historical information of the given\nsentence. On the other branch, we use the original WK to generate the full-precision Key matrix\nK for computation, which can guarantee that the computation results of the prefilling stage are the\nsame as the original LLMs. Then, we only store the full-precision Key activation of the last m tokens\n$K_{local}$ to preserve the local information for the decoding stage.\nMoreover, during the decoding stage, as shown in Figure 1(b), we only process one token during each\nforward pass. We take the process of the (n + 1)-th token as an example. For the cache update, we\ncompute both the compressed Key activation Kc and full-precision Key activation K and update\nboth Key caches with the new activations. In this case, the compressed Key cache has (n + 1) tokens,\nand the full-precision Key cache has (m + 1) tokens. To get the (n + 1) tokens' Key matrix, we\nuse the (m + 1) tokens from the full-precision Key cache as $K_{local}$ and use the BK to process the\noldest (n \u2013 m) tokens in the compressed Key cache as $K$. By concatenating the K and $K_{local}$, we"}, {"title": "2.2 Efficient Fine-tuning by SVD-based Initialization", "content": "Directly applying low-rank decomposed weight matrices for KV cache compression would result\nin the degradation of model performance when the compression ratio becomes high. To further\nenhance the model performance, we propose to introduce an efficient training process. We find\nthat the initialization method to the proposed AK and BK is of great importance for convergence\nand final performance. In this case, we proposed to use the ASVD-based decomposition results\nfor initialization. As shown in Figure 2, we train LLMs in a layer-wise manner by minimizing the\nlayer-wise reconstruction loss for the compressed keys and values.\nSpecifically, for each layer, we can use the WK to generate the full-precision Key matrix K = XWK\nand use AK, BK to generate the lossy key matrix K = XAK BK. The local reconstruction loss of\nthis layer could be defined as Equation 1:\n$L_K = MSELoss(K, \\hat{K})$\n(1)\nwhere LK denotes the loss of keys in this layer, and MSELoss(, ) is the Mean Square Error (MSE)\nloss function. Finally, define the loss of keys and values in the i-th layer as $L_{K,i}, L_{V,i}$, the loss for\nthe whole model is shown in Equantion 2:\n$L_{all} = \\sum_{j=0}^{n} (L_{K,j} + L_{V,j})$\n(2)\nwhere $L_{all}$ denotes the loss for the whole model, and ni denotes the number of layers."}, {"title": "3 Experiment", "content": ""}, {"title": "3.1 Experimental Setup", "content": "We evaluate our method on LongChat-7B-v1.5-32k [9] and Mistral-7B-Instruct-v0.2 [7]. We eval-\nuate our method on three widely-used long-context benchmarks: LongEval [9], LongBench [21],\nand LVEval [19]. For comparison, we include results from StreamingLLM [18], H2O [22], and\nASVD [20]. The first two are token pruning methods, while the latter is a SOTA channel-shrinking\nmethod. More details can be found in the Appendix."}, {"title": "3.2 Main Results", "content": "We apply compression ratios of 50% and 80% consistently for both keys and values. The results are\npresented in Table 1.\nAccording to the evaluation results in table 1, the token pruning methods are especially not skilled in\nretrieval tasks like LongEval, even at a 50% compression ratio, when ASVD and CSKV only incur\nminor performance loss. As the compression ratio reaches 80%, all methods except for CSKV suffer\ngreat performance degradation on all three tasks. To dive deeper, we examine the failure cases of\ntoken pruning methods, and found that although the model could generate coherent sentences based\non instructions, a great deal of the retrieved answers deviate from the ground truth by a small portion,\nlike answering \"4244\" when the label is \"42440\", or give an irrelevant answer such as \"1386\". This\nmight be caused by their token eviction mechanisms which inherently have to discard the information\nof some tokens completely, facing great risk of losing the ground truth information. In contrast, the\nabundant failure cases of ASVD at 80% compression are mainly caused by the loss of the model's\nlanguage modeling capabilities, like responding with dozens of tokens that could hardly form a\nsentence. Different from the aforementioned methods, CSKV consistently enables the model to\ngenerate instruction-following responses and give accurate answers on either retrieval tasks or QA\ntasks, showing its superior capability of keeping the model's long-context abilities even at high\ncompression ratios."}, {"title": "3.3 Ablation Studies", "content": "We conduct several ablation studies to further explore the potential of our method, and the main\nconclusions include: 1) The SVD-based initialization methods is crucial to the success of training; 2)\nThe model performance is positively correlated with the window size, while the benefit would become\nless significant after it reaches a certain level; 3) In most cases, it would be better to compress the key\ncache more than the value cache given a certain budget; 4) CSKV could be seamlessly integrated\nwith 4-bit QAT with very small performance loss. See Appendix for details."}, {"title": "4 Limitation and Future Directions", "content": "While demonstrating competitive performance, the proposed method's compression ratio assignment\nis user-defined and might not be optimal, offering the potential to achieve higher compression ratios.\nFuture work could explore the application of automated search algorithms to dynamically assign\ncompression ratios to individual layers, accounting for their varying sensitivity to compression. Simi-"}, {"title": "Appendix", "content": ""}, {"title": "A. Distribution of Singular Values of key cache", "content": "We visualize the distribution of singular values of key cache in the 14-th layer of LLaMA-2-7B-chat\nmodel, using data randomly sampled from the Pile [4] dataset. We find that the singular value of\nthe key cache has a significant long-tailed distribution, and a similar phenomenon also appears in\nthe value cache. In this case, only a tiny fraction of singular values have large magnitudes, while\nthe vast majority are around zero, which can be removed without significant degradation of model\nperformance."}, {"title": "B. Details of Experimental Setup", "content": "We evaluate our method on widely used long-context models, including LongChat-7B-v1.5-32k [9]\nand Mistral-7B-Instruct-v0.2 [7]. For fine-tuning, we use a scaled-down version of the Pile [4]\ndataset [14] and is conducted with both the epoch and batch size set to 1, using the AdamW optimizer\nwith an initial learning rate of 5e-5. The entire fine-tuning process for each 7B model is completed\nwithin 90 minutes on a single NVIDIA A100-80G GPU, resulting in minimal training costs. We\ninitialize the model with ASVD [20], selecting 256 samples from the fine-tuning dataset as calibration\ndata. We set a = 0.5 and use the Absolute Mean Value method for configuring the scaling matrix S.\nThe evaluation of our method is performed on three widely-used long-context benchmarks, including\nLongEval [9], LongBench [21] and LVEval [19]. Specifically, we choose the 200,300,400,500 lines\nsubsets in LongEval (with an average length of 4k,6k,8k,10k), the qasper, hotpotqa, multifieldqa_en,\ngov_report, triviaqa subset of LongBench-E, along with the 16K subset of LVEval. To compare the\nresults with other methods, we choose StreamingLLM[18], H2O[22] and ASVD[20], in which the\nfirst two are token pruning methods and the last one could be regarded as a channel shrinking method\u00b9.\nWe select compression ratios of 50% and 80% for the experiments, with the same compression ratios\nfor keys and values."}, {"title": "C. Ablation Study", "content": "Without loss of generality, we perform an ablation study on LongEval with the Longchat-7b-v1.5-32k\nmodel. The window size is set to 32 and the compression ratio is evenly distributed on keys and\nvalues by default. The \"Avg.Acc\" column in the following tables indicates the average accuracy on\nthe four chosen subsets of LongEval."}, {"title": "C.1 Effect of Initialization Methods", "content": "We test three initialization methods for the low-rank decomposed matrices: 1) random initialization,\n2) standard SVD initialization, and 3) ASVD initialization. We keep their fine-tuning settings the\nsame as mentioned in the Experimental Setups. The loss curves of 80% compression are shown in\nFigure 4, and the evaluation results for the trained models with a bi-branch strategy are shown in\nTable 2."}, {"title": "C.2 Effect of Window Size", "content": "The window size determines how much local information could be preserved, which is of vital\nimportance to the quality of generated content. We fix the compression ratio to 80% and evaluate\nthe performance of the bi-branch trained model with multiple window size settings. The results are\nshown in Table 3."}, {"title": "C.3 Effect of Compression Ratio Allocation for KV", "content": "Different from the token pruning methods that have to keep or discard a certain token's keys and\nvalues simultaneously, our channel shrinking method allows for the key cache and value cache to\nhave different compression ratios. To investigate the impact of allocating a certain compression ratio\nto the key cache and value cache in different proportions, we conduct experiments by fixing the total\ncompression rate at 50% and 75%, respectively. We then evaluate the model's performance under\nvarious combinations of compression ratios for keys and values. The results are shown in Table 4."}, {"title": "C.4 Compatibility with Quantization", "content": "As the low-bit quantization methods are orthogonal with our method, we further demonstrate that\nquantization could be seamlessly combined with our method. Specifically, we apply KIVI [13]\nwith 4-bit quantization on the compressed keys and values, using per-channel quantization for the\nformer and per-token quantization for the latter. Both the window size and the residual size are set\nto 32. We separately perform the experiments with two quantization manners: PTQ (Post-Training\nQuantization) and QAT (Quantization-Aware Training). The results are shown in Table 5, where the\n\"None\" rows are the referenced results from the full-precision model."}]}