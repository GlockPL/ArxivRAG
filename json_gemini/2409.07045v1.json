{"title": "Beyond IID: Optimizing Instruction Learning from the Perspective of Instruction Interaction and Dependency", "authors": ["Hanyu Zhao", "Li Du", "Yiming Ju", "Chengwei Wu", "Tengfei Pan"], "abstract": "With the availability of various instruction datasets, a pivotal challenge is how to effectively select and integrate these instructions to fine-tune large language models (LLMs). Previous research mainly focuses on selecting individual high-quality instructions. However, these works overlooked the joint interactions and dependencies between different categories of instructions, leading to suboptimal selection strategies. Moreover, the nature of these interaction patterns remains largely unexplored, let alone optimize the instruction set with regard to them. To fill these gaps, in this paper, we: (1) systemically investigate interaction and dependency patterns between different categories of instructions, (2) manage to optimize the instruction set concerning the interaction patterns using a linear programming-based method, and optimize the learning schema of SFT using an instruction dependency taxonomy guided curriculum learning. Experimental results across different LLMs demonstrate improved performance over strong baselines on widely adopted benchmarks.", "sections": [{"title": "1 Introduction", "content": "Supervised fine-tuning (SFT) is the key to aligning large language models (LLMs) with human beings, enabling them to complete various downstream tasks and adapt to specific domains such as healthcare and finance (Zhao et al., 2023a). The effectiveness of the SFT process relies on a high-quality instruction set, so as to ensure the performance of LLMS (Longpre et al., 2023; Wang et al., 2023; Xu et al., 2023). Temporarily, with the availability of various instruction sets, a new challenge has been raised, i.e., how to select and integrate existing datasets to obtain an optimized instruction set. To address this issue, previous research typically works by selecting and combining individual \u201chigh-quality\" instructions. Then often construct proxy indicators to evaluate different aspects of quality, such as factual correctness, complexity, and informativeness. Then the raw instruction set could be refined by selecting instructions with the highest relative quality scores (Latif and Zhai, 2024; Li et al., 2024; Lu et al.; Zhao et al., 2023b).\nHowever, emerging evidence (Dong et al.; Yuan et al., 2023) and our analyses indicate that complex correlation and dependency relationships exist between different categories of instructions. Therefore, considering the quality of individual instructions alone can be a suboptimal approach for building a fine-tuning instruction set. Research indicates that these categories are interrelated; incorporating one category of instructions may enhance or diminish the model's performance in others (Dong et al., 2023; Huang and Chang, 2023). Additionally, the skills required for different tasks often form hierarchical taxonomies. For instance, solving a bioinformatics problem requires both biological knowledge and coding skills. Consequently, instructions are interconnected and collectively influence model performance. Ignoring these correlations can reduce the efficiency of instruction selection, as incorporating one category of instruction may even degrade the model's performance in another category. Moreover, the dependency between skills necessitates that models acquire foundational knowledge before progressing to more complex tasks; otherwise, the effectiveness of instruction tuning will be compromised (Longpre et al., 2023).\nHence, it is crucial to account for these joint effects for optimizing the instruction set. However, two main challenges remain unaddressed: (1) The potential correlation and dependency patterns are largely unknown; (2) How to optimize the instruction set while considering these correlation and dependency patterns remains an unexplored area. To fill these gaps, as Figure 1 (b)-(e) shows, we systemically investigated the correlation patterns between different categories of instructions, and induced an ability taxonomy of instructions based on causal interventions on the distribution of the instruction set. Then, with the guidance of the correlation patterns and dependency taxonomy, we optimized the instruction set by adjusting the proportion of different categories of instructions, and arranging the order of learning different categories of instructions.\nSpecifically, we construct an automatic tagging system to assign the instruction with tags describing the detailed capability and knowledge required to complete this instruction. With the tags, we do interventions in the dataset distribution by adding or removing instructions with certain tags, so that we can observe how the LLM's performance changes with the incorporation of each category of instructions, as well as how the performance of the LLM on one category of instruction changes depending on another category of instructions. Given the correlation patterns, we"}, {"title": "2 Causal Intervention based Instruction Correlation Analysis and Ability Taxonomy Induction", "content": "In the SFT stage, the LLM is trained to learn given instruction set \\(D = \\{C_i\\}_{i=1}^N\\), where \\(C_i\\) is the ith category of instructions. With the maximum likelihood estimation and independent identical distribution (iid) assumption, the training objective could be formalized as \\(\\max_\\theta \\prod_i[P(C_{ij})]\\), where \\(C_{ij}\\) is the jth instruction of \\(C_i\\).\nDue to the inherent correlation and dependency between the knowledge and skills involved in different categories of instructions, the distribution of instruction set \\(D\\) should be characterized using an joint distribution \\(P(C_1, ..., C_i, C_N)\\), and \\(P(C_1,..., C_i,...,C_N) \\neq \\prod_i P(C_{ij})\\). Such discrepancy would lead to ineffectiveness and inefficiency of the SFT process, as: (1) Under the iid assumption, the training objective deviates from the actual distribution of instruction set; (2) The joint distribution could be decomposed into a sequential manner as \\(P(C_1, ..., C_i,...,C_N) = \\prod_i P(C_k | C_{j},...)\\), which indicates a sequential learning schema, i.e., advanced skills could be learned only enough preliminary knowledge is equipped. Nevertheless, in the current SFT schema, different categories of instructions are randomly distributed in the whole epoch, making the advanced skills been trained before equipped with enough prior knowledge. This limits the efficiency of the SFT process. Thus, it would be necessary to consider the optimization of instruction set beyond the iid assumption.\nHowever, the category of instructions are unknown priorly. Thus, to induce the correlation pattern and dependency taxonomy between different categories of instructions, we first systematically collect publicly available high-quality instructions and design an ability tagging system to automatically confer instruction a set of tags, which describes the ability and knowledge necessary for completing the instruction. So that we can categorize the instructions with the tags. Then based on the category tags, by adding or removing certain categories of instructions (Cause), we could obtain how the performances of other categories of instructions change (Effect) brought by such intervention, and induce the correlation and dependency pattern."}, {"title": "2.1 Instruction Collection and Automatic Ability Tagging System", "content": "The prerequisites of analyzing the relationship patterns between different categories of instructions are collecting a"}, {"title": "2.2 Causal Intervention based Instruction Correlation Analysis", "content": "Previous research suggests that instructions from different domains and tasks are interconnected. After incorporating one category of instructions, after SFT, performance across other categories would also be influenced. The source of such correlations could be rather complex Dong et al. (2023); Huang and Chang (2023). In this paper, rather than"}, {"title": "2.3 Causal Intervention based Large Language Model Ability Taxonomy Induction", "content": "Heuristically, human beings could not learn advanced knowledge before mastering the necessary preliminary knowledge. For example, it would be rather hard for a student to learn advanced math before he has acquired basic arithmetic calculations. Such a phenomenon inspires us to investigate whether the dependency also exists when LLMs learn different categories of instructions in the SFT process.\nTo induce the dependency taxonomy of different instruction categories, given an instruction set \\(D_{\\text{train}}\\), we sequentially remove one category of instructions \\(C_i\\) and obtain a set of ablation instruction sets \\(\\{D_{\\text{train}} \\setminus C_i\\}_{i=1}^N\\), N is the total number of instructions, then compare the performance change of LLM fine-tuned on \\(D_{\\text{train}}\\) with that fine-tuned on \\(D_{\\text{train}} \\setminus C_i\\).\nThis is because: (1) If the exclusion of \\(C_i\\) causes significant performance degradation on"}, {"title": "3 Category Relationship Guided Instruction Set Optimization", "content": "With the relationship patterns, we further explore optimizing the SFT process of LLM. We investigate optimizing the category distribution of the instruction set based on the correlation pattern between instructions, and concerning the dependency taxonomy between different categories of instructions, we explore optimizing the instruct tuning process by curriculum learning."}, {"title": "3.1 Effect Equivalence-based Category Proportion Optimization", "content": "With the correlation patterns between different categories of instructions, we aim to optimize the instruction set by adjusting the proportion of each instruction category. The objective function of the optimization could be formalized as:\n\n\n\n\n\\(Obj = s(f_a(w| \\{\\gamma_{ij}\\}_{i,j \\in [1,N]}, D, D_{\\text{candidate}}))\\)\n\n\n\n\nwhere w is the optimized weight of each category of instruction, \\(\\gamma_{ij}\\) is the equivalence effect coefficient measuring the correlation strength between category i and j, D is the original instruction set, \\(f_a(\\cdot)\\) is the weight adjust function, s(\\( \\cdot \\)) is a score function evaluating the effectiveness of the weight adjustment. By adjusting the proportion of each category of instructions according to w, certain categories of instructions are removed from D, or incorporated into the new instruction set from \\(D_{\\text{candidate}}\\). However, it could be a challenging task, as the score function is not clearly defined. Generally, it should be related to the performance of LLM finetuned using the adjusted instruction set. Whereas it is rather hard to be modeled using a parametric function and thus obstructs solving w.\nTo address this issue, we propose an Effect Equivalence-based Category Proportion Optimization (EE-CPO) method. We notice that, since different categories of instructions are"}, {"title": "3.2 Ability Dependency Taxonomy Guided Curriculum Instruction Learning", "content": "The dependency between instruction categories underscores the need to optimize the SFT process, as learning efficiency would be hindered by the lack of preliminary skills. To address this issue, we resort to Curriculum Learning. Rather than simply repeating the instruction set with several epochs, Curriculum Learning aims at arranging the samples with different content and difficulty in a sequential manner, so that the model can acquire enough preliminary skills before learning the more complex instructions.\nSpecifically, as shown in Figure 1 (e), given the dependency taxonomy, and an already existing instruction set D to equip LLM with enough preliminary skills, we adjust the learning sequence of the SFT process by increasing the proportion of preliminary categories in the early stage of the SFT process. Correspondingly, the proportion of subsequential categories is accordingly decreased. In contrast, at the later stage of SFT, the weight of subsequential categories is increased, and the weight of preliminary categories is decreased, so that the LLM is trained to complete the complex tasks using preliminary skills. For brevity, we abbreviate our proposed approach as DT-CSFT (Dependency Taxonomy guided Curriculum SFT). Thus, DF-CSFT makes adjustments only by adjusting the learning sequential of different categories."}, {"title": "4 Related Work", "content": "With the availability of various instruction sets, one crucial issue is how to optimize the existing instruction sets. To address this issue, most current methods focus on selecting high-quality instructions to obtain a refined instruction set (Latif and Zhai, 2024; Li et al., 2024; Lu et al.). While the \"quality\" of instruction could be a comprehensive concept containing multiple aspects. Pioneer works use proxy indicators such as length and perplexity to evaluate the quality of instructions (Huang and Chang, 2021; Wang et al., 2024). However, such indicators would not be enough to comprehensively measure the instructions' quality. Another line of work aims at measuring the complexity of instructions, as there is no need to focus too much on learning simple instructions, while overly difficult instructions cannot be"}, {"title": "5 Conclusion", "content": "In this paper, we systemically investigate the correlations and dependency taxonomy between different categories of instructions. Analyses results show the widespread of such interactions across multiple categories of instructions and different LLMs, suggesting the necessity of taking the correlation and dependency in the optimization of the content distribution of the instruction set and learning schema of the SFT process. Hence, we further managed to optimize the category proportion and the learning sequence of the instruction set with regard to the correlation and dependency patterns. The improved performance in turn supports the existence of correlation and dependency patterns, together with the reasonability of our investigation and instruction set optimization method. Considering the numerous categories of instruction data, their interaction patterns could be quite complex. Our work might serve as a pioneer and call for further research to conduct a more comprehensive exploration."}, {"title": "7 Appendix", "content": null}, {"title": "7.1 Comparison between Correlation Pattern Induced from Different LLMs", "content": "Figure 5 shows the correlation pattern derived from Qwen-1.5-7B and Llama3-8B, respectively. Correlation patterns derived from different LLMs demonstrate a high similarity. This shows the widespread and generality of the correlation patterns. Such similarity would partly be brought by the significant overlap between the pretraining of different LLMs, and the inherent logical and knowledge relationships between different categories of instructions."}, {"title": "7.2 Comparison between Ability Dependency Taxonomy Induced from Different LLMs", "content": "Table 5 shows the dependency taxonomy derived from Qwen-1.5-7B and Llama3-8B, respectively. Comparison between the two taxonomies suggests a high similarity between two taxonomies, especially in the preliminary categories. This shows the widespread and generality of such dependency taxonomy, as the dependency patterns are essentially decided by the inherent relationship of knowledge and skills for completing different categories of instructions."}, {"title": "7.3 Instruction Collection", "content": "In this paper, before analyzing the interaction relationships between instruction categories, a prerequisite is collecting enough instructions so that the main categories of instructions can be covered. provide a comprehensive list. Based on their list, we exclude all instructions that are not constructed by human annotation or advanced LLMs such as GPT-4 or ChatGPT. Beyond their list, we also include Logi-QA, Wild-Chat, and COIG-CQIA. Table 6 provided a detailed list of the included instruction set.\nGiven the instruction set, we employ SimHash with a threshold=0.95 to remove the potential duplicated instructions. After the duplication process, 9,509,526 instances in total are left in the instruction collection."}, {"title": "7.4 Tag Generation", "content": "Considering the vast amount of instruction, we construct an automatic tagging system that employs an LLM to generate tags for a given instruction.\nSpecifically, given an instance from the instruction collection which is composed by (several) {Instruct-Response} pair(s), we concatenate them into a string, and as Figure 6 shows, using the following prompt, to demand the LLM to generate tags describing the necessary knowledge and skills for completing the dialogue described by the {Instruct-Response} pair(s):\nWe employ Qwen-1.5-72B-instruct as the tagger."}, {"title": "7.5 Tag Normalization", "content": "The LLM would describe one kind of knowledge or skill with different expressions, for example, \"math calculation\" with \"mathematical calculation\". To address this issue, we propose to combine these tags according to their semantic similarity. Specifically, we obtain the embedding of the tags using BGE (Xiao et al., 2023). Then semantically similar tags are recognized if their cosine similarity of embeddings is larger than an empirical threshold X = 0.85. For a set of semantically similar tags, they are normalized to the one with the highest frequency among them. After the normalization process, tags With a frequency lower than 100 are considered as long-tail and are filtered out Lu et al.. After the normalization and filtering, a total of 21,378 tags are left. We manually selected 29 categories across 7 domains for analysis."}, {"title": "7.6 Experimental Settings", "content": "Before analysis, we excluded the instructions that were semantically similar to the test instances in the benchmarks AlpacaEval and MTBench using semantic similarity. Instructions with a cosine similarity larger than 0.3 are excluded from further analysis. Without generality, we only select instructions in English for analysis.\nIn the causal intervention-based instruction correlation analysis, the base instruction set D are constructed by randomly sampling 1,000 instances of each category from the whole instruction collection. The same base instruction set is also adopted in the ability taxonomy induction, to control potential confounders.\nTo induce the correlation pattern of each category of instruction with the others, at each time, 2,000 instructions of one category are added into the base instruction set to obtain \\(D \\cup C_i\\). Then we use \\(D \\cup C_i\\) to fine-tune an LLM M and obtain \\(M \\cup C_i\\). During the fine-tuning process, M is fine-tuned for 3 epochs, with a batch size of 32, the initial learning rate of 9.65e-6, optimized with the Adam optimizer, \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.95\\) Ouyang et al. (2022). This group of hyperparameters is also adopted in all other sections when the fine-tuning process is involved.\nFor the baseline quality score-based instruction selection method IFD and Instag, we implemented their using their codes on our instruction collection. As we included more instructions in the collection set, our implementations outperformed the original implementations on the MT-Bench and AlpacaEval2.0.\nIn the ability dependency taxonomy guided curriculum instruction learning, assume the base instruction set D contains \\(N_{pre}\\), \\(N_{inter}\\), and \\(N_{sub}\\) preliminary, intermediate, and subsequential category instructions. Note that D = \\(N_{pre}\\) +"}, {"title": "7.7 Changes in weights of Categories after Optimization", "content": "As Figure 7 shows, in general, the weights of categories that can be well substituted such as NLU, Concept Understanding, or Commonsense Reasoning are decreased. On the contrary, the weights of Text Summarization, Academic Writing which could not be well substituted by another category of instructions, and the preliminary categories, such as Mathematical Modeling and mathematical Reasoning, are increased."}]}