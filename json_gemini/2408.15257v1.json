{"title": "Text classification optimization algorithm based on\ngraph neural network", "authors": ["Erdi Gao", "Haowei Yang", "Dan Sun", "Haohao Xia", "Yuhan Ma", "Yuanjing Zhu"], "abstract": "In the field of natural language processing, text\nclassification, as a basic task, has important research value and\napplication prospects. Traditional text classification methods\nusually rely on feature representations such as the bag of words\nmodel or TF-IDF, which overlook the semantic connections\nbetween words and make it challenging to grasp the deep\nstructural details of the text. Recently, GNNs have proven to be\na valuable asset for text classification tasks, thanks to their\ncapability to handle non-Euclidean data efficiently. However,\nthe existing text classification methods based on GNN still face\nchallenges such as complex graph structure construction and\nhigh cost of model training. This paper introduces a text\nclassification optimization algorithm utilizing graph neural\nnetworks. By introducing adaptive graph construction strategy\nand efficient graph convolution operation, the accuracy and\nefficiency of text classification are effectively improved. The\nexperimental results demonstrate that the proposed method\nsurpasses traditional approaches and existing GNN models\nacross multiple public datasets, highlighting its superior\nperformance and feasibility for text classification tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Text classification is a core task in Natural Language\nProcessing [1], which is widely used in sentiment analysis[2],\nsubject detection[3], spam filtering and other fields.\nTraditional text classification methods mostly rely on vector\nrepresentations such as Bag-of-Words or TF-IDF, which treat\ntext as an independent lexical set. By overlooking the\nsemantic relationships and contextual information between\nwords, it becomes challenging to capture the deep structural\nfeatures of text. Convolutional Neural Networks [4-5] and\nRecurrent Neural Networks [6] have been applied to text\nclassification tasks. Although these methods have improved\nclassification performance to some extent, they still face\nlimitations in capturing long-distance dependencies and\nprocessing non-Euclidean data.\nGraph Neural Networks, as a kind of deep learning model\nthat can effectively process non-Euclidean data, have made\nremarkable progress in many fields in recent years[7]. GNNs\nrealizes the learning and representation of graph-structured\ndata through the information transfer and aggregation between\nnodes and their neighbors, which is especially suitable for text\ndata with complex relational structure[8]. However, the\nexisting GNN-based text classification methods still face\nseveral challenges in practical application. On the one hand,\nthe graph structure construction of text usually relies on word\nco-occurrence or syntactic dependency, which has problems\nof high computational complexity and unstable graph\nconstruction quality; On the other hand, the training and\nreasoning process of GNN model is time-consuming and\nrequires high resources, which may not adequately address the\nrequirements of large-scale text classification tasks.\nThis paper presents an optimized text classification\nalgorithm leveraging graph neural networks. The objective is\nto improve the efficiency and accuracy of text classification\nby optimizing the graph construction strategy and the graph\nconvolution process. Specifically, this paper uses the adaptive\ngraph construction method to dynamically adjust the\nrelationship between nodes and edges according to the text\ncontent, so as to build a more accurate text graph structure.\nAdditionally, an efficient graph convolution operation is\nincorporated to enhance the information transfer and\naggregation process, thereby reducing computational\noverhead.\nThis paper contributes in three main aspects: 1)\nintroducing an adaptive strategy for constructing text graph\nstructures to enhance accuracy; 2) designing a streamlined\ngraph convolution operation to mitigate computational\ncomplexity; 3) The effectiveness and feasibility of the\nproposed method in actual text classification tasks are verified\nby a large number of experiments. It is hoped that the research\nin this paper can provide new ideas and technical support for\ntext classification based on graph neural network, and promote\nthe development of NLP field."}, {"title": "II. RELATED WORK", "content": "As artificial intelligence continues to evolve at a rapid pace,\na growing number of researchers and academics are delving\ninto deep learning technologies. These technologies have\nfound widespread application across various domains,\nincluding medical diagnosis [9-11], image classification[12-\n14], financial risk management [15-16], natural language\nprocessing (NLP), speech recognition, and text classification.\nWithin the scope of NLP, text classification emerges as a\npivotal research area, encompassing a wide array of methods\nand techniques. This section provides a comprehensive\noverview of traditional text classification approaches,\nunderscores recent\nadvancements in deep learning\nmethodologies, and traces the development of Graph Neural\nNetwork-based methods. Furthermore, it critically examines\ntheir respective advantages, limitations, and the trajectories of\ntheir historical evolution.\nEarly methods of text classification relied heavily on\nstatistical and machine learning models. Bag-of-Words model\n[17] and TF-IDF [18] are the most commonly used text\nrepresentation methods. These methods transform text into\nword frequency vectors or TF-IDF vectors, ignoring the order\nand semantic relationships between words. Subsequently,\ntraditional machine learning algorithms such as SVMs. While\nthese methods perform well when dealing with small-scale\ndata, they fall significantly short when it comes to capturing\nthe deep semantics of text and processing large-scale data.\nWith the development of deep learning, CNNs and RNNS\nhave been introduced into text classification tasks. CNN\ncaptures the local features of text, which is suitable for short\ntext classification. However, CNNS are limited in their\neffectiveness when dealing with long text and long-distance\ndependencies. RNNS and their variants, such as Long Short-\nTerm Memory [19] and Gated Recurrent Unit [20], effectively\ncapture sequence information and context through cyclic\nstructures. Significant progress has been made in text\nclassification. However, these methods still have limitations\nwhen dealing with non-Euclidean structured data.\nGraph Neural Networks have excelled in recent years in\nhandling non-Euclidean data. GNN realizes information\ntransfer and aggregation between nodes and their neighbors\nthrough graph convolution operation, which is suitable for\nprocessing text data with complex relational structure. The\nexisting GNN-based text classification methods usually\ninclude the following steps: First, the construction of the text\ngraph involves both word co-occurrence and syntactic\ndependency relationships; Secondly, models such as Graph\nConvolutional Networks and Graph Attention Networks are\nused to learn and classify node features[21].\nAlthough GNN-based text classification methods show\nunique advantages in capturing semantic relationships and\nglobal structure information of text, there are still several\nchallenges. First of all, the construction process of text graph\nstructure is complicated and often needs to rely on external\nknowledge or pre-trained model, which leads to high\ncomputational cost. Secondly, the training and inference\nprocess of GNN model requires high resources, especially\nwhen dealing with large-scale text data, and there are\nbottlenecks in training efficiency and model performance.\nThe focus of this paper is a text classification optimization\nalgorithm grounded in graph neural networks. It encompasses\nan adaptive strategy for graph construction and an efficient\napproach to graph convolution operations. The adaptive graph\nconstruction strategy dynamically adjusts the relationship\nbetween nodes and edges according to the text content, which\nimproves the accuracy and construction efficiency of the\ngraph structure. Based on the experimental findings, the\nproposed approach demonstrates clear superiority over\ntraditional methods and existing GNN models across various\npublic datasets. These results substantiate the method's\neffectiveness and advantages in enhancing text classification\ntasks.\nIn conclusion, this paper introduces a novel optimization\nalgorithm that integrates traditional text classification\nmethods, deep learning techniques, and graph-based neural\nnetwork approaches. This proposal offers innovative ideas and\ntechnical foundations to enhance the precision and efficiency\nof text classification tasks."}, {"title": "III. THEORETICAL BASIS", "content": "A. Graph neural networks\nThe spectral domain-based graph neural network is a\ntechnique used for feature extraction and representation\nlearning by leveraging spectral information inherent in graphs.\nIt analyzes the eigenvalues and eigenvectors of the Laplacian\nmatrix of the graph to comprehend its structure and features,\nenabling tasks such as node classification and graph\nclassification to be performed effectively. This approach\nprovides insights into the underlying characteristics of the\ngraph, facilitating accurate classification tasks based on\nspectral properties.\nFirst with regard to the representation of graphs and the\nLaplacian matrix, we assume that there is a G = (V, E) where\nV is nodes and E is edges. Figure Gcan be represented as the\nadenjency matrix A, where $A_{ij}$ = 1 indicates that there is an\ni and j, otherwise $A_{ij}$ = 0. The Laplace matrix L can be\ndefined as L = D - A.\nFor Laplacian matrix L, its eigenvalues and eigenvectors\ncan be obtained by spectral decomposition. Let $\u03bb_1 \u2264 \u03bb_2 \u2264$\n<$\u03bb_n$ be the eigenvalue of L, and the corresponding\neigenvectors are $u_1, u_2, ..., u_n$. These eigenvectors form the\nspectral space of the graph G.\nGraph convolution operations based on spectral domains\ncan be expressed in the following form:\n$H^{(l+1)} = \u03c3(D^{-1/2}AD^{-1/2}H^{(l)}W^{(l)})$\n(1)\n$H^{(l)}$is the nodal eigenmatrix of the $l$ layer. A = A + I is\nthe result of adding A self-join to the adjacency matrix A.\nInterlayer propagation of Graph Convolutional Networks\n(GCN) Interlayer propagation of GCN can be expressed in the\nfollowing form:\n$H^{(l+1)} = \u03c3(D^{-1/2}\u00c2D^{-1/2}H^{(l)}W^{(l)})$\n(2)\nAmong them: \u00c2 = D -1/2 \u00c2D -1/2 adjacency matrix is\nsymmetric normalization. D is the degree matrix of \u00c2.GCNs\ncan be trained for graph node classification tasks:\n$L = -\\sum_{i=1}^{N}\\sum_{k=1}^{K}y_{ik} log(\\hat{y}_{ik})$\n(3)\nWhere: $y_{ik}$ is the label of whether the node i belongs to\nthe class k. $\\hat{y}_{ik}$ is the probability that the model predicts that\nthe node i belongs to the class k. By optimizing the loss\nfunction, the weight parameters in the graph neural network\ncan be learned to complete the task of node classification.\nWhile this is an overview of spectral domain based graph\nneural networks, recent research efforts have turned to\nexploring alternative matrix structures to optimize the\nperformance of graph convolutional network (GCN) models.\nAmong them, the model proposed by Mei et al. [22]"}, {"title": "IV. TEXT CLASSIFICATION MODEL BASED ON GNN-\nMULTIMODAL INFORMATION", "content": "This paper combines Graph Neural Networks (GNNs) and\nMulti-Modal Information. A novel text classification model,\nGNN-MultiModal TextClassifier (GNN-MMC), is proposed.\nThe objective of the model is to enhance both the accuracy\nand robustness of text classification, not only utilizing text\ndata, but also integrating other modal information (such as\nimages, audio, metadata, etc.) to comprehensively understand\nand classify text content.\nThe model architecture begins by transforming text into a\ngraph structure, where nodes denote fundamental text\nelements (such as words, phrases, and sentences), and edges\nsignify the connections among these elements (such as word\nco-occurrence and grammatical dependencies). Through the\nconstruction of the graph structure, the model is adept at\ncapturing intricate semantic relationships and contextual\ninformation within the text. Specifically, given a text, a graph\n(G = (V, E)) is constructed from the co-occurrence of words\nor syntactic dependencies. Each node (v \u2208 V) is initialized as\na word embedding vector (h()).\nIn addition to text data, the model also introduces\ninformation from other modes. For example, in social media\nanalytics, text may be accompanied by images, audio, and user\nmetadata. The information of these modes is transformed into\ncorresponding feature vectors and integrated with the text\ngraph. Suppose there is (N) mode information, and the\neigenvector of each mode is expressed as (mi), where (i =\n1, ..., N). These modal features are transformed by a specific\ntransformation function ($\\Phi_i(\u00b7)$) into vectors (m\u2081 = $\\Phi_i(mi)$)\nof the same dimension as the node features of the text graph.\nIn the graph neural network (GNN) layer, node features\nare updated by graph convolution operations. GNN learns the\nglobal semantic information by gradually updating the node\nrepresentation through information aggregation of neighbor\nnodes. Specifically, the model uses Graph Attention Networks\n(GATs) architecture. In each layer of graph convolution, the\nfeature update formula of node (v) is:\n$h_v^{(k+1)} = \u03c3(\\sum_{u\u2208N (v)}\u03b1_{vu}^{(k)}W^{(k)}h_u^{(k)})$\n(6)\nWhere (N(v)) represents the set of neighbor nodes of the\nnode (v), (W(k)) is the weight matrix of the (k) layer, (\u03c3) is\nthe activation function, ($\u03b1_{vu}^{(k)}$) is the attention weight,\ncalculated by the similarity between nodes:\n$\u03b1_{vu}^{(k)} = \\frac{exp(LeakyReLU(a^{(k)} [W^{(k)}h_v^{(k)} || W^{(k)}h_u^{(k)}]))}{\\sum_{j\u2208N (v)} exp (LeakyReLU(a^{(k)} [W^{(k)}h_v^{(k)} || W^{(k)}h_j^{(k)}]))}$\n(7)\nIn order to integrate the information of different modes\neffectively, the model adopts the feature fusion strategy.\nCommon methods include feature splicing, weighted fusion,\nand attention mechanisms. The multi-modal characteristics\nafter fusion are expressed as follows:\n$h_{v}^{fused} = Concat(h_{v}^{(K)}, m_1, m_2,...,m_N)$\n(8)\nWhere ($h_{v}^{(K)}$) is the node feature updated by the\nconvolution of the (K) layer graph. The fused multimodal\nfeatures are input into subsequent classifiers, and finally, the\nfused features are input into the fully connected layer or other\nclassifiers (such as SVM, Softmax classifier), and the category\nlabel of the output text is:\n$\\hat{y}$ = Softmax($W_{cls}h_{v}^{fused} + b_{cls}$)\n(9)\nGNN-MMC offers several advantages. First of all, it can\ncomprehensively process multi-modal information, which\nimproves the comprehensiveness and accuracy of text\nclassification. By integrating information such as images,\naudio, and metadata, models can capture important features\nbeyond text. Secondly, through the graph neural network, the\nmodel\ncan effectively capture the complex semantic\nrelationships and global structure information in the text,\nwhich makes GNN-MMC have a significant advantage in\ndealing with long text and text with complex relationships.\nFinally, the graph neural network layer and multi-modal\nfeature fusion layer of the model can learn and adjust the\nfeature representation adaptively, which reduces the\ndependence on artificial feature engineering.\nIn summary, GNN-MMC provides an efficient and\naccurate text classification solution by combining graph\nneural networks and multimodal information. Based on the\nexperimental outcomes, the proposed model exhibits\nsignificant superiority over traditional text classification\nmethods and existing multimodal classification models across\nmultiple datasets. In the future, GNN-MMC can be further\nexpanded and optimized to deal with more diverse and\ncomplex practical application scenarios."}, {"title": "V. EXPERIMENTAL ANALYSIS", "content": "A. Data set\nTo assess and validate the performance of the GNN-\nMultiModal TextClassifier (GNN-MMC) model, this study\nopted to utilize the Twitter Multimodal Sentiment Analysis\n(TMMSA) dataset as the experimental dataset. The TMMSA\ndataset is a widely used standard dataset for multimodal text\nclassification tasks and is particularly suitable for studying\nsocial media sentiment analysis. The TMMSA dataset\ncontains a large number of emotionally tagged Twitter posts,\neach containing not only text content, but also associated\nimages and other metadata, such as user information and post\ntimestamps. This multimodal nature makes datasets\nparticularly useful for models studying the integration of text\nand non-text information (such as images, metadata). The\nGNN-MMC model is designed to deal with this kind of\ncomplex information.\nIn the experiment, the TMMSA data set is preprocessed in\ndetail. The dataset is managed using the Linked Data\nmethodology [23], which consolidates various essential\nformats for academic research. This structured approach\nfacilitates the interconnection of data across multiple datasets,\nthereby enhancing interoperability. This capability is\nparticularly advantageous in fields such as machine learning\nand artificial intelligence, where the quality of data plays a\ncritical role in training models and obtaining precise results.\nThe text data goes through a standard text cleaning process,\nincluding the removal of stops, punctuation, stem extraction\nand word vectorization. The image data is preprocessed,\nincluding scaling, normalization, and image feature extraction\nusing convolutional neural networks (e.g. ResNet, VGG). In\naddition, metadata such as user information and time stamps\nare transformed into appropriate numerical features for\nintegration with text and image information. In experiments,\nthe GNN-MMC model works by representing text as a graph\nstructure, where nodes represent the basic units of text (such\nas words, phrases, sentences), and edges represent the\nsemantic relationships between these units. The model uses\ngraph neural network (GNN) for information aggregation and\nlearning to capture global semantic information and complex\ncontextual relationships. By combining multimodal\ninformation from images and metadata, GNN-MMC is able to\nunderstand and utilize information more comprehensively in\ntext classification tasks, thereby improving classification\naccuracy and generalization.\nData preprocessing is a crucial step in any machine\nlearning project, ensuring that the data is clean and valid\nbefore it is fed into the model. In this paper, we carried out a\ndetailed data preprocessing process for the TMMSA data set\nused to prepare the data for use by the GNN-MultiModal\nTextClassifier (GNN-MMC) model. The following are the\nspecific steps of data preprocessing:\na. Remove invalid characters and punctuation marks\nFirst, remove invalid characters and punctuation from the\ntext that could interfere with model training, such as special\nsymbols, HTML tags, and so on. This step can easily be done\nwith regular expressions or pre-processing libraries such as\nNLTK or Spa\u0441\u0443.\nb. Participle\nDividing text into sequences of words or phrases is a basic\nstep in text processing. Word segmentation can be done using\nan off-the-shelf word divider (such as NLTK's\nword_tokenize), ensuring that the text is split into meaningful\nunits.\nc. Remove the stop word\nStop words are words that appear frequently in text\nanalysis but usually do not contain useful information, such as\n\"and\", \"the\", etc. In the preprocessing process, removing the\nstops can reduce the noise.\nd. Stem extraction or morphology reduction:\nCommonly used techniques such as stemming or\nlemmatization are employed to simplify vocabulary\ncomplexity and reduce the feature space. They can convert\nwords to their basic form, for example, converting both\n\"running\" and \"ran\" to \"run.\"\ne. Text vectorization\nTo convert processed text into numerical form, each word\nis mapped as a vector, usually using a Bag of Words (BoW)\nmodel or a word embedding model (e.g. Word2Vec, GloVe).\nThese vectorized representations transform text information\ninto a form that a machine learning model can process.\nB. Evaluation indicators\nOne of the elements of our study is the evaluation of the\nclassification performance which is based on the confusion\nmatrix[24]. This matrix is used as a fundamental tool for\npredicting information into different categories. It organizes\ninstances in four main quadrants which are: True Positives,\nTrue Negatives, False Positives, and False Negatives. In this\ncase, TP indicates the instances of the right predicted positive\ncases, whilst TN means the count of the actual negative cases\ncorrectly labeled as negative in the dataset. On the other-hand\nFP is for those instances where the model by fault classifies\nnegatives as positives and FN stands for the positive instances\nthat have been incorrectly characterized as negatives.\nThe form of summing up some data, otherwise named a\nconfusion matrix, is a set of ways to comprehend a model's\nprecision level in various sections. A deep insight into these\nmetrics (TP, TN, FP, and FN) allows us to have a complete\nknowledge concerning the program's predictive capabilities. It\ngives the possibility of measuring not only general exactness\nbut also individual dimensions of the classifier's performance,\nconsequently enabling a complete evaluation. The\ncorresponding category Table 1 is as follows:\nAccuracy is a major factor considered when assessing a\nmodel's performance. Together with F1 Score, it enables a\ndeeper analysis of the precision and recall of the model. This\nwidely used classification metric represents the ratio of correct\npredictions that are predicted correctly with the real outcomes\nregarding every single data point.\nAccuracy = $\\frac{TP + TN}{TP + TN+FP + FN}$\n(10)\nConversely, the formula is such that, assess the model's\ncapacity for categorization tasks, are both algorithms of the\ncommon practice. Classification accuracy indicates the\npercentage of correct predictions made.\nAs an explanation that is still well-structured, the term\nbeing specified is the best explanation of expected accuracy in\nthe assessment of a model. Model performance evaluation\nusually implies the use of multiple metrics, such as Accuracy\nand F1 Score, which help in determining the effectiveness of\na model. Here, the use of Accuracy predominates since its core\nidea is the percentage of correct forecasts out of the total\nsamples. Basically, its computation is made using a specific\nformula.\nThe F1 score is created by the mean weighted of Precision\nand Recall, which brings both accuracy and recall into\nconsideration. F1 scores are computed by the next formula:\nF1= $\\frac{2x Precision \u00d7 Recall}{Precision + Recall}$\n(11)\nThe Precision in model evaluation reads as the proportion\nof actual positive predictions among all the positive instances\nidentified by the model; the Recall on the other hand is a\ndistance function that provides an effective measure of the\nproportion of the real positive items that are properly\ndesignated as positive by the classification. Instead of\nAccuracy, which just focuses on the number of correct\npredictions, the F1 Score establishes a balance between\nPrecision and Recall, consequently providing a more complex\nview of the model performance for different classes. The focus\non both aspects makes it especially useful when evaluating the\nclassification accuracy, especially in cases where the proper\ndistinction between real positives and negatives that are not\nmistakenly labeled as positives is critical."}, {"title": "C. Experimental setup", "content": "In the experimental part of this paper, we design the\nspecific parameter Settings of the GNN-MultiModal\nTextClassifier (GNN-MMC) model in detail, and the\nnecessary equipment requirements to ensure the repeatability\nand effectiveness of the experiment. First, we chose Graph\nAttention Networks (GATs) as the main graph neural network\n(GNN) layer structure. Compared with traditional GCN,\nGATs has better ability of neighborhood information\naggregation and is suitable for processing complex\nunstructured data such as text. In the configuration of GAT,\nwe set up 2 layers of GAT with 128 and 64 hidden units per\nlayer and LeakyReLU as the activation function. These\nSettings are designed to take full advantage of the power of\nmulti-layer networks to effectively capture complex semantic\nrelationships and contextual information in text data. For\nmulti-modal information fusion, we adopt a simple feature\nsplicing strategy. Specifically, we spliced preprocessed text\nfeature vectors, image features (2048-dimensional feature\nvectors extracted through ResNet), and metadata features\n(such as numerical representations of user information)\ndirectly together. This simple and direct fusion method helps\nto preserve the original characteristics of each mode\ninformation, while reducing the complexity and\ncomputational cost.\nAdditionally, a learning rate decay strategy was\nimplemented to gradually adjust the learning rate throughout\nthe training process, enhancing the model's convergence. The\nBatch Size of 64 was selected to strike a balance between\navailable computational resources and model complexity.\nThis ensures efficient processing of data batches in each\niteration. For device requirements, we recommend using an\nNVIDIA GPU that supports CUDA acceleration for model\ntraining. Especially when dealing with large data sets, Gpus\ncan significantly improve computational efficiency and speed\nup model training and inference processes. It is recommended\nto use a workstation or cloud server with sufficient memory to\nmeet the large memory requirements that the GNN-MMC\nmodel may produce. In summary, with the above detailed\nparameter Settings and equipment requirements, we were able\nto fully evaluate and verify the performance of the GNN-\nMMC model on the TMMSA dataset. These Settings not only\nhelp ensure the reliability and validity of the experiment, but\nalso provide a solid foundation and guidance for us to further\nexplore and optimize the model."}, {"title": "D. Experimental result", "content": "According to the experimental results of different baseline\nmodels in Table 2, we can deeply analyze the performance of\neach model and its comparison: First, the GNN-MMC model\nshows the best performance, with an accuracy of 96.01% and\nan F1 score of 97.88%. This shows that the GNN-MMC can\nsignificantly improve the accuracy and overall performance of\ntext classification under the design of combining graph neural\nnetwork and multi-modal information fusion. By using graph\nstructure to capture complex semantic relationships and\ncontext information in text, and effectively integrating multi-\nmodal information such as images and metadata, GNN-MMC\ncan understand and classify text content more\ncomprehensively, which is significantly reflected in the\nexperimental results.\nAs a conventional graph neural network model, the GCN\nachieves an accuracy of 94.11% and an F1 score of 92.58%.\nThis underscores the effectiveness of the GCN in traditional\ngraph-based tasks. Although GCN is good at capturing local\nstructures and relationships in graph data, it is somewhat poor\nat integrating multimodal information and processing global\nsemantics, and thus slightly inferior to GNN-MMC in\nmultimodal text classification tasks.\nFinally, the RNN model performed relatively poorly, with\nan accuracy of 93.68% and an F1 score of 91.37%. As a\ntraditional sequence model, RNN is strong in feature\nextraction and classification of short texts, but its performance\nis inferior to that of graph neural network model in processing\nlong texts and complex semantic relationships. Especially\nwithout considering the multimodal information, the\nperformance of RNN model is limited by the limitation of its\nsequence modeling. In summary, by comparing the\nexperimental results of different baseline models, we can\nclarify the advantages of GNN-MMC model in multimodal\ntext classification tasks. It combines advanced graph neural\nnetwork technology and effective multi-modal information\nfusion strategy to provide an effective solution for processing\ncomplex text data, and has significant performance\nadvantages and potential application prospects."}, {"title": "E. Ablation experiment", "content": "According to the ablation experiment results in Table 3,\nwe can compare and analyze the performance of the GNN-\nMMC model and its components (GNN and MMC), so as to\ndeeply understand the contribution and influence of each part\nin the overall performance of the model.\nFirst, the GNN-MMC model showed the highest accuracy\nand F1 scores, at 96.01% and 97.88%, respectively. The\nresults show that when GNN and MMC are applied at the\nsame time, the model achieves the best comprehensive results\non text classification tasks. GNN-MMC uses graph structure\nto capture complex semantic relationships in text effectively,\nand improves comprehensive understanding and classification\nof text content by integrating multi-modal information such as\nimages and metadata.\nThe GNN model achieves an accuracy of 91.81% and an\nF1 score of 93.09%. However, when compared to GNN-MMC,\nwhich integrates multimodal information, the standalone\nGNN shows slight inadequacy, highlighting the critical role of\nmultimodal fusion in enhancing overall classification\nperformance. GNN performs well in learning local structures,\nbut has limitations in dealing with global semantics and multi-\nmodal information integration.\nFurthermore, the MMC model achieves an accuracy of\n92.99% and an F1 score of 92.58%. While the MMC model\nexcels in integrating multimodal information, it lacks the\ngraph neural network's capability to learn complex semantic\nrelationships. This limitation hinders its performance in\ncomprehending and accurately classifying long texts, which is\nwhere the GNN-MMC model excels.\nIn summary, the ablation experiment clearly demonstrated\nthe advantages of GNN-MMC model using graph neural\nnetwork and multi-modal information fusion. Gnn-mmc\nachieves excellent performance in text classification tasks by\neffectively combining GNN's global semantic learning with\nMMC's multimodal information integration. This study\nprovides insights into the role of the model's components in\ncomplex tasks and points the way to designing more efficient\nmultimodal text classification models in the future."}, {"title": "VI. CONCLUSION", "content": "Based on the research and experiment of graph neural\nnetwork and multimodal information fusion in text\nclassification, we draw the following conclusions: This paper\nproposes an innovative text classification model, GNN-\nMultimodal TextClassifier (GNN-MMC), which combines\nthe advantages of graph neural networks (GNN) and\nMultiModal information fusion (MMC). By introducing graph\nstructure into the model, we can capture complex semantic\nrelationships and contextual information in text data, while\nintegrating multi-modal information (such as images, audio,\nmetadata) further enriches the understanding of text content.\nAccording to the experimental results, the GNN-MMC model\ndemonstrates significantly superior performance compared to\ntraditional methods on the TMMSA dataset, achieving an\naccuracy rate of 96.01% and an F1 score of 97.88%. Ablation\nexperiments further validated the contribution of GNN and\nMMC to the model performance. The performance of GNN\nand MMC when used alone is lower than that of GNN-MMC\nmodel when used in combination, which shows that\ncomprehensive use of graph neural network and multi-modal\ninformation fusion are effective strategies when dealing with\ncomplex text tasks. Overall, the research in this paper not only\npromotes the application of graph neural networks in text\nclassification, but also explores the potential of multimodal\ninformation in improving text understanding and\nclassification accuracy. Future work could further optimize\nthe structure and algorithms of the model and explore broader\nand complex application scenarios, such as sentiment analysis,\nevent detection, etc., with a view to providing more powerful\nand flexible multimodal text analysis tools."}]}