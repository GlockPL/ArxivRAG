{"title": "Cluster-guided Contrastive Class-imbalanced Graph Classification", "authors": ["Wei Ju", "Zhengyang Mao", "Siyu Yi", "Yifang Qin", "Yiyang Gu", "Zhiping Xiao", "Jianhao Shen", "Ziyue Qiao", "Ming Zhang"], "abstract": "This paper studies the problem of class-imbalanced graph classification, which aims at effectively classifying the categories of graphs in scenarios with imbalanced class distribution. Despite the tremendous success of graph neural networks (GNNs), their modeling ability for imbalanced graph-structured data is inadequate, which typically leads to predictions biased towards the majority classes. Besides, existing class-imbalanced learning methods in visions may overlook the rich graph semantic substructures of the majority classes and excessively emphasize learning from the minority classes. To tackle this issue, this paper proposes a simple yet powerful approach called C\u00b3GNN that incorporates the idea of clustering into contrastive learning to enhance class-imbalanced graph classification. Technically, C\u00b3GNN clusters graphs from each majority class into multiple subclasses, ensuring they have similar sizes to the minority class, thus alleviating class imbalance. Additionally, it utilizes the Mixup technique to synthesize new samples and enrich the semantic information of each subclass, and leverages supervised contrastive learning to hierarchically learn effective graph representations. In this way, we can not only sufficiently explore the semantic substructures within the majority class but also effectively alleviate excessive focus on the minority class. Extensive experiments on real-world graph benchmark datasets verify the superior performance of our proposed method.", "sections": [{"title": "Introduction", "content": "Graphs are widely recognized as highly effective structured data for representing complex relationships among objects in various domains (Ju et al. 2024a), including social analysis, bioinformatics, and recommender systems. As such, there is considerable interest in exploring the potential of analyzing graph data, covering a broad spectrum of graph learning tasks, such as node classification (Luo et al. 2023a), graph classification (Mao et al. 2023), and graph clustering (Yi et al. 2023a). Among these, graph classification is one of the most interesting also popular topics, which aims to classify the class labels of graphs and has emerged as a significant research focus in various scenarios like molecular property prediction and protein functional analysis.\nTo well solve this task, graph neural networks (GNNs) (Kipf and Welling 2016) have recently emerged as powerful approaches attributed to the message propagation and feature transformation mechanisms (Gilmer et al. 2017). However, the success of existing GNN methods typically relies on the assumption that the class distribution in the dataset is balanced. Unfortunately, this assumption fails in many real-world scenarios, where a large portion of classes have a small number of labeled graphs (minority classes) while a few classes have a significantly large number of labeled graphs (majority classes), exhibiting severely skewed class distributions. For instance, the NCI dataset comprises graphs of chemical compounds (Wale, Watson, and Karypis 2008), where only approximately 5% of molecules are labeled as active in the anti-cancer bioassay test, while the remaining molecules are labeled as inactive. This class-imbalanced issue can lead to the notorious prediction bias phenomenon (Zhou et al. 2020), where the GNN classifier favors the majority classes and ignore the minority classes. Consequently, directly applying GNNs to class-imbalanced graphs poses a significant challenge in real-world situations.\nActually, the issue of imbalance in visions has received increasing attention and efforts have been devoted to addressing it over a significant period. In general, the solutions can be categorized into three main groups: re-sampling, re-weighting, and ensembling learning. Re-sampling strategies (Chawla et al. 2002; Han, Wang, and Mao 2005; Guo and Wang 2021) attempt to balance the class distribution by generating synthetic training data. Re-weighting strategies (Cui et al. 2019; Cao et al. 2019; Hou et al. 2023) focus on adjusting the loss function to assign different weights to the training samples from various classes. For ensembling learning approaches (Xiang, Ding, and Han 2020; Wang et al. 2020), they integrate multiple classifiers within a multi-expert framework to achieve robust predictions."}, {"title": "", "content": "However, directly applying existing class-imbalanced learning methods from vision to graph domains is challenging due to two key limitations. First, due to the complexity and diversity of the graph topology, there typically exist hierarchical substructures in graph samples within the majority classes. For example, considering the polarity of molecules, there are numerous naturally occurring molecules with the functional group \u201c-OH\u201d. These molecules possess polarity (class label), but they are further classified into different hierarchical levels based on the strength of their polarity (subclasses). When connected to the functional group \u201c-C=O\u201d, they exhibit strong polarity, whereas when connected to the \u201c-CH2\u201d, they exhibit relatively weak polarity. As a result, within the same class label, the samples of majority class display varying levels of semantic substructure. Second, existing re-sampling and re-weighting strategies excessively focus on minority classes, usually at the cost of sacrificing the accuracy of majority classes. However, the samples of minority classes in the graph domain, such as those containing the adamantyl group, are scarce and mostly lack polarity, resulting in a relatively simple semantic structure. These molecules often cannot represent all the semantic structures of the minority classes. Overlearning from such samples introduces certain redundant information and causes prediction bias. In addition, although there are many class-imbalanced methods designed for graphs (Shi et al. 2020; Liu, Nguyen, and Fang 2021; Park, Song, and Yang 2021; Zhou and Gong 2023; Zeng et al. 2023), they are primarily developed for node-level classification on a single graph. However, the promising yet challenging task of graph-level classification has largely remained unexplored, and it serves as the main focus of this paper.\nTo address these challenges, in this paper we propose a Cluster-guided Contrastive Class-imbalanced framework called C\u00b3GNN for graph classification. The key idea of C3GNN is to learn effective graph-level representations by incorporating the principle of clustering into supervised contrastive learning (Khosla et al. 2020). Specifically, to capture hierarchical semantic substructures of the majority classes, C\u00b3GNN first adaptively clusters graphs of each majority class into multiple subclasses, ensuring that the sample sizes in each subclass are comparable to the minority class. Then, we utilize the Mixup technique to generate synthetic samples, further enriching the semantic information within each subclass and preventing representation collapse due to sparse subclass samples. Based upon this, we leverage supervised contrastive learning to hierarchically learn graph-level representations, encouraging a graph to be (i) closer to graphs from the same subclass than to any other graph, and (ii) closer to graphs from different subclasses but the same class than to graphs from any other subclass. In this way, we are able to capture rich graph semantic substructures of the majority classes and alleviate overlearning in the minority classes, achieving well class-imbalanced learning. To summarize, the main contributions of our works are as follows:\n\u2022 We explore an intriguing and relatively unexplored challenge: class-imbalanced graph classification, with the aim of providing valuable insights for future research.\n\u2022 We propose a novel class-imbalanced learning approach on graphs via clustering and contrastive learning to preserve the hierarchical class substructures as well as achieve balanced learning for all classes.\n\u2022 We conduct extensive experiments to demonstrate that C3GNN can provide superior performance over baseline methods in multiple real-world benchmark datasets."}, {"title": "Problem Definition & Preliminaries", "content": "Notations. Given a graph dataset $G = \\{G_i, Y_i\\}_{i=1}^N$ comprising $N$ graphs, each associated with a ground-truth class label $y_i \\in \\{1, . ., K\\}$. Without loss of generality, denote $N_j$ the number of graphs in the j-th class, and assume that $N_1 \\geq N_2 \\geq \\dots \\geq N_K$ following a descending order. To quantify the degree of class imbalance, we define the imbalance factor (IF) of the dataset as the ratio $N_1/N_K$. A class-imbalanced dataset consists of an imbalanced training set and a balanced test set, where the training set satisfies:\n$\\int p(G|y = k_1) dG > \\int p(G|y = k_2) dG, \\quad k_1 < k_2,$\n$\\lim_{k \\to \\infty} \\int p(G|y = k) dG = 0,$\nwhich reveals a successive decay in class volumes as the class indexes ascend, and the probability ultimately converges to zero in the final few classes.\nClass-imbalanced Graph Classification. The objective is to develop an unbiased classifier based on the graph dataset characterized by imbalanced class distributions. The trained classifier should learn robust and discriminative graph representations, ensuring that it remains capable of accurately classifying graphs across all classes, without being dominated by the abundant majority classes. Moreover, the trained classifier should exhibit strong generalization capabilities when tested on a balanced dataset.\nGNN-based Encoder. Recent GNNs leverage both the graph structure and node features to learn an effective representation for a given graph via message-passing mechanism, which entails iteratively updating the embedding of a node $h_v$ by aggregating embeddings from its neighboring nodes:\n$h_v^l = \\text{AGGREGATE}(h_v^{l-1}, h_u^{l-1} | u \\in \\mathcal{N}(v)),$\nwhere $h_v^l$ is the embedding of node $v$ at the $l$-th layer of the GNN. $\\mathcal{N}(v)$ is the neighbors of node $v$, AGGREGATE refers to an aggregation function. By iterating $L$ times, the graph-level representation $h_G$ can be obtained by aggregating node representations $h_v^L$ using the READOUT function:\n$h_G = \\text{READOUT}(h_v^L | v \\in V)$\nwhere $V$ is the node set of graph $G$. Afterward, the derived graph-level representation $h_G$ can be well used for downstream graph-level classification."}, {"title": "Methodology", "content": "Our C\u00b3GNN mainly includes three modules, i.e., adaptive clustering for subclass-balancing, subclass mixup interpolation, and hierarchical subclass contrastive learning. The framework overview of our proposed method is illustrated in Figure 1. In the subsequent sections, we provide a detailed explanation of the three components."}, {"title": "Adaptive Clustering for Subclass-balancing", "content": "Most existing class-imbalanced methods typically sacrifice the performance of the majority classes to improve the accuracy of the minority classes (Mao et al. 2023; Yi et al. 2023b). However, we argue that due to the complexity and diversity of the graph topology, the samples in the majority class often exhibit rich hierarchical substructures. It is crucial to sufficiently explore the hierarchical semantic information within them and we achieve this by decomposing each majority class into multiple semantically coherent subclasses, thereby balancing the various classes.\nTo this end, we leverage the idea of clustering to adaptively decompose the graph samples of each majority class into multiple subclasses (Hou et al. 2023). Technically, considering a majority class $c$ and its corresponding graph dataset $G_c$, we first feed these graphs to the GNN-based encoder to obtain graph-level representations $Z^c = \\{z_1^c, z_2^c, ..., z_{N_c}^c\\}$ by Eq. (3), where $N_c$ is the number of graphs in class $c$. Then we employ a selected clustering algorithm (e.g., k-means) based on the graph representations, to partition $G_c$ into multiple subclasses. To ensure a balanced distribution of samples among the subclasses, we set a threshold value $M$ as an upper limit of sample size within each subclass defined as:\n$M = \\text{max} (n_k, \\delta)$\nwhere $n_k$ represents the number of the last minority classes sorted in descending order. The hyperparameter $\\delta$ serves to regulate the minimum sample size within clusters, thereby preventing the formation of excessively small clusters. Notably, the cluster centers via the clustering algorithm can be progressively updated every several epochs during the learning process, such that we can adaptively decompose the optimal subclasses. Additionally, we only apply the clustering algorithm to majority classes that contain multiple instances, while keeping the minority classes unchanged. Therefore, this approach ensures that the number of samples in each resulting subclass is approximately equal to that of the minority class, thus alleviating class imbalance.\nIn this way, we can achieve subclass balance by decomposing"}, {"title": "Subclass Mixup Interpolation", "content": "After eliminating class imbalance through adaptive clustering, the subclasses of the majority class roughly have a similar number of samples as the minority class. However, due to the sparsity of the minority class, directly optimizing on this basis can easily cause the learned representations to collapse into trivial solutions, leading to severe performance degradation. Existing class-imbalanced methods such as re-sampling and re-weighting strategies (Chawla et al. 2002; Cui et al. 2019), often employ oversampling or increasing the weight of the minority class to enhance the emphasis on them. Nevertheless, a problem arises in that minority classes typically have a relatively simple semantic structure, making it difficult to represent all the minority classes by replicating or increasing the weight of these samples. This makes the learned representations sensitive and poorly generalized when faced with samples from other minority classes.\nTo address this issue, we introduce the Mixup technique (Zhang et al. 2017) to synthesize new samples within subclasses, thereby increasing the diversity of samples and enriching their semantic structure, as well as avoiding the collapse of representation caused by sample sparsity. Specifically, given a subclass $s$ from class $c$, its feature representations is represented as $Z_s^c = \\{z_1, z_2, \\dots, z_{N_{cs}}\\}$, where $N_{cs}$ is the number of graphs in subclass $s$ from class $c$. Then we conduct interpolation between the feature representations $z_i^c$ and $z_j^c$ within each subclass, denoted as:\n$\\tilde{z} = \\alpha \\cdot z_i^c + (1 - \\alpha) \\cdot z_j^c,$\nwhose subclass label is annotated to be the same as $z_i^c$ and $z_j^c$, and $\\alpha$ is a scalar mixing ratio, sampled from a uniform distribution $\\mathcal{U} [0, 1]$ (Zhang et al. 2017).\nIn this way, Mixup extends the training samples within each subclass by incorporating the prior knowledge based on the smoothing assumption, enhancing the diversity and semantic richness of samples in each subclass."}, {"title": "Hierarchical Subclass Contrastive Learning", "content": "By adaptively clustering the majority classes into multiple subclasses with similar sizes, we can now treat all subclasses (including the original minority classes) in a balanced manner. This allows us to address class imbalance while also harvesting abundant semantic information with different hierarchical structures: coarse-grained class labels and fine-grained subclass labels.\nAs such, inspired by the powerful representation learning capability of contrastive learning, which learns to discriminate the samples by contrasting the negative ones (Chen et al. 2020; You et al. 2020; Ju et al. 2024c). We leverage supervised contrastive learning (Khosla et al. 2020) to effectively learn discriminative representations from both intrasubclass and inter-subclass perspectives.\nFrom the intra-subclass view, we aim to encourage a graph to be closer to graphs from the same subclass than to any other graph. Specifically, given a batch $\\mathcal{B} = \\{G_i, \\Omega(G_i)\\}_{i=1}^B$ where $\\Omega(G_i)$ denotes the subclass label of graph $G_i$. To conduct effective graph contrastive learning, for each graph $G_i$ we first perform stochastic graph augmentations $\\mathcal{T}(\\cdot|G_i)$ to obtain two correlated views $\\tilde{G}^1_i$ and $\\tilde{G}^2_i$ as a positive pair, where $\\mathcal{T}(\\cdot|G_i)$ is pre-defined augmentation distribution (e.g., node dropping, edge perturbation, attribute masking, subgraph following (You et al. 2020)). Then, graph-level representations of the resulting 2B augmented graphs are extracted by the GNN-based encoder denoted as $\\{z_1, z_2, ..., z_{2B}\\}$ with an abuse of notation. Let $i \\in \\mathcal{I} = \\{1, ..., 2B\\}$ be the index of an arbitrary augmented graph in a batch, we formulate the supervised contrastive loss for i-th graph from the intra-subclass view as:\n$\\mathcal{L}_i^{\\text{Cintra}} = - \\frac{1}{|Q(i)|} \\sum_{j \\in Q(i)} \\log \\frac{e^{z_i \\cdot z_j/\\tau}}{\\sum_{a \\in A(i)} e^{z_i \\cdot z_a/\\tau}},$\nwhere $A(i) = \\mathcal{I} \\setminus \\{i\\}$, $Q(i) = \\{q \\in A(i) : \\Omega(G_q) = \\Omega(G_i)\\}$ is the set of indices of all positives distinct from $i$ with the same subclass label as $G_i$, and $\\tau$ is a scalar temperature hyper-parameter.\nFrom the inter-subclass view, we aim to encourage a graph to be closer to graphs from different subclasses but the same class than to graphs from any other subclass. Analogously, we formulate the supervised contrastive loss for i-th graph from the inter-subclass view as:\n$\\mathcal{L}_i^{\\text{Cinter}} = \\frac{1}{|P(i)| - |Q(i)|} \\sum_{j \\in P(i)/Q(i)} \\log \\frac{e^{z_i \\cdot z_j/\\tau}}{\\sum_{a \\in A(i)/Q(i)} e^{z_i \\cdot z_a/\\tau}},$\nwhere $P(i) = \\{p \\in A(i) : Y_q = Y_i\\}$ is the set of indices of all positives distinct from $i$ with the same class label as $G_i$.\nNotably, Eq. (6) achieves subclass-level balance, where positive graphs are pairs within the same subclass, enabling the model to understand the intra-subclass relationships. On the other hand, Eq. (7) achieves class-level balance, which leverages the class information but excludes graphs from the same subclass to focus on those within the same class but"}, {"title": "", "content": "belonging to different subclasses, facilitating the learning of inter-subclass relationships from complementary view.\nJoint Optimization. To enhance class-imbalanced graph classification by learning effective representations in a hierarchical way, we formally combine the two supervised contrastive losses from both intra-subclass and inter-subclass perspectives in a batch, which is defined as:\n$\\mathcal{L}^\\mathcal{B} = \\sum_{i=1}^B (\\mathcal{L}_i^{\\text{Cintra}} + \\beta \\cdot \\mathcal{L}_i^{\\text{Cinter}}),$\nwhere $\\beta$ is the balance hyper-parameter to adjust the relative importance of each loss component, and we set it to 1 by default in the experiments. We summarize the whole optimization algorithm of C\u00b3GNN in Algorithm 1.\nComputational Complexity Analysis\nWith $B$ as the batch size and $|V|$ as the average number of nodes in input graphs, obtaining embeddings from the GNN encoder has a time complexity of $O(ND|V|)$, where $N$ is the number of graphs, and $D$ is the embedding dimension. The time complexity of subclass clustering is $O(IKND)$, where $I$ is the iterations until convergence and $K$ is the total number of clusters. Since clustering algorithms converge quickly in practice, $I$ and $K$ can be considered constants, and the complexity can be simplified to $O(ND)$. Subclass contrastive learning computes the loss in $O(NDB)$ time. Therefore, the overall time complexity of C\u00b3GNN is $O(ND(|V| + B))$."}, {"title": "Experiment", "content": "Experimental Settings\nDatasets. We evaluate the effectiveness of our proposed model by examining it on both synthetic and real-world datasets from various domains. Specifically, the datasets are categorized into three groups: (a) synthetic: Synthie (Morris et al. 2016), (b) bioinformatics: ENZYMES (Schomburg et al. 2004), and (c) computer vision: MNIST (Dwivedi"}, {"title": "Hyper-parameter Sensitivity", "content": "Here we investigate the sensitivity of our proposed C\u00b3GNN to hyper-parameters. Specifically, we study the influence of varying different cluster sizes on six datasets, which are essential to contrastive learning performance. We vary the cluster size in the range of $\\{25, 50, 75, 100, 125\\}$ for the MNIST dataset, and in the range of $\\{3, 4, 5, 6, 7\\}$ for other datasets. The results are shown in Figure 2.\nThe accuracy curves exhibit a similar trend across all datasets. We can observe that the performance of our C3GNN gradually improves as the cluster size increases until it reaches saturation. However, choosing a cluster size that is too large or too small may have a detrimental effect on model performance. We argue that this is because a large cluster size can result in some subclasses containing significantly more instances than the minority classes, leading to an imbalance among subclasses, which in turn leads to suboptimal performance. Moreover, a small cluster size can result in similar instances being assigned to different clusters,"}, {"title": "Analysis of Feature Distribution", "content": "To analyze the learned representation of C\u00b3GNN, we begin by defining the intra-class and inter-class distances using the Euclidean distance between a given sample and other samples from the same or different classes. Specifically, we calculate the Euclidean distance between a sample $z_i$ and a set $S$ as $D(z_i, S) = \\frac{1}{|S|} \\sum_{z_j \\in S} ||z_i \u2013 z_j||_2$. Next, we define the intra- and inter-class distance of sample $z_i$ as $D(z_i, P(i))$ and $D(z_i,G/P(i))$, respectively. Similarly, we define the intra-and inter-subclass distance of sample $z_i$ as $D(z_i, Q(i))$ and $D(z_i, P(i)/Q(i))$, respectively.\nWe visualize the feature distance distributions using the MNIST dataset with an imbalance factor of 50. We first divided the classes into the many, medium, and few regions based on the number of samples. Figure 3a demonstrates the intra-class/inter-class feature distances among different data groups and their corresponding distributions, while Figure 3b displays the intra-subclass/inter-subclass feature distances. From the results, we can draw several conclusions: (i) The average intra-subclass distance is lower than the average intra-class distance, which implies that C\u00b3GNN encourages samples from the same subclass to have similar representations. This observation suggests that C\u00b3GNN successfully captures and emphasizes the finer distinctions present within each class. (ii) The feature distances in subclasses exhibit a higher degree of uniformity across different data subsets compared to the feature distances in classes. This key finding indicates that the subclasses belonging to both majority and minority classes occupy similar volumes of the learned feature space. Consequently, this balance among the subclasses contributes to the overall class balance."}, {"title": "Visualization of Clustering Results", "content": "In this subsection, we visualize the clustering results to demonstrate the effectiveness of our subclass clustering and subclass contrastive module. Figure 4 illustrates eight graphs drawn from the MNIST dataset, where the nodes represent super-pixels of images and the colors indicate the grey-scale values of the super-pixels. Notably, while graphs within the same class may not exhibit significant structural similarities, those within the same subclass demonstrate remarkable resemblances. For instance, graph (e) and graph (f) show distinct writing styles compared to graph (g) and graph (h), thereby representing different variations within class 7. The clustering results emphasize the effectiveness of our proposed subclass clustering module, which successfully identifies and groups graphs with similar topological structures within specific subclasses. Moreover, our subclass contrastive module not only mitigates the negative impact of class imbalance while preserving sample balance but also enables the representations to cluster into finer-grained subclasses within the representation space. This demonstrates the capability of our approach to capture and leverage the subtle variations and distinctions among different subclasses, leading to more informative representations and enhanced classification performance."}, {"title": "Related Work", "content": "Graph Classification poses a pivotal challenge within the domain of graph analysis, aiming to recognize the class label for an entire graph. Existing methodologies for graph classification can be broadly classified into two fundamental streams: graph kernel-based approaches and GNN-based techniques. The former aims to measure the similarity between graphs by decomposing them into substructures (e.g., shortest paths (Kashima, Tsuda, and Inokuchi 2003),"}, {"title": "", "content": "graphlets (Shervashidze et al. 2009), or subtrees (Shervashidze et al. 2011)). The latter has gained significant attention in recent years due to the great capability to capture both structural and attribute information (Ju et al. 2022, 2024b; Luo et al. 2024a, 2023b), whose key idea is to iteratively update node features by aggregating information from neighboring nodes (Gilmer et al. 2017). Despite the tremendous success, these methods are prone to prediction bias for imbalanced data while our C\u00b3GNN alleviates this issue by clustering to balance the quantities of different classes.\nClass-imbalanced Learning (also known as long-tailed learning) endeavors to mitigate the influence of imbalanced class distributions, and there are broadly three main strategies: re-sampling (Chawla et al. 2002; Han, Wang, and Mao 2005; Guo and Wang 2021), re-weighting (Cui et al. 2019; Cao et al. 2019; Hou et al. 2023), and ensemble learning (Xiang, Ding, and Han 2020; Wang et al. 2020; Yi et al. 2023b; Mao et al. 2024). To facilitate class-imbalanced learning for graph-structured data, many GNN-based methods have been proposed for node classification (Shi et al. 2020; Liu, Nguyen, and Fang 2021; Park, Song, and Yang 2021; Song, Park, and Yang 2022; Yun et al. 2022; Zhou and Gong 2023; Zeng et al. 2023; Ju et al. 2024d). However, these methods often overlook capturing the rich semantic structure of the majority classes and excessively focus on learning from the minority classes. Furthermore, existing GNN methods are designed for node classification, while our approach C\u00b3GNN explores the unexplored domain of class-imbalanced graph classification.\nContrastive Learning have garnered considerable attention due to their remarkable performance in representation learning and downstream tasks (Chen et al. 2020; He et al. 2020; Khosla et al. 2020; Luo et al. 2024b). The widely adopted loss function in this domain is the InfoNCE loss (Oord, Li, and Vinyals 2018), which effectively pulls together augmentations of the same samples while simultaneously pushing away negative samples. Moreover, a multitude of recent advancements have emerged that extend contrastive learning to the graph domains (You et al. 2020; Chen and Kou 2023; Gong, Yang, and Shi 2023; Gu et al. 2024; Ju et al. 2024c). Different from the above works, our C\u00b3GNN leverages supervised contrastive learning to hierarchically learn graph representations, thereby mitigating class imbalance."}, {"title": "Conclusion", "content": "In this paper, we focus on class-imbalanced graph classification and propose a novel framework C\u00b3GNN to fully capture the semantic substructures within the majority class while effectively mitigating excessive focus towards the minority class. We first leverage clustering to balance the imbalance among different classes. Then we employ mixup to alleviate data sparsity and enrich intra-subclass semantics. Finally, we utilize supervised contrastive learning to effectively learn hierarchical graph representations from both intra-subclass and inter-subclass views. Extensive experiments demonstrate the superiority of C\u00b3GNN over the baseline methods in a series of real-world graph benchmarks."}]}