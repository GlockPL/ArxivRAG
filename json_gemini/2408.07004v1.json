{"title": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models", "authors": ["Chun Jie Chong", "Chenxi Hou", "Zhihao (Zephyr) Yao", "Seyed Mohammadjavad Seyed Talebi"], "abstract": "Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience. Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services. However, the privacy consequences associated with these services and their third-party plugins are not well understood. Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins. In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services. Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services. At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier. We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Model (LLM) has been widely adopted in various online applications, such as chatbots, search engines, and translation tools. The success of LLM is a result of recent advancements in machine learning, allowing transformer models [1] to train on a large corpus of data, measured in tens and hundreds of trillion tokens [2]. Cloud-based training and inference services have made LLM readily accessible to regular users, who can utilize the powerful language models without maintaining prohibitively expensive computational resources.\nCloud-based LLM services offer convenience, but unfortunately, they also raise privacy concerns. As they are black-box systems, users have little control over how their prompts are processed and stored. According to their privacy policies, LLM service providers collect the prompts and share user's inputs with third-party service providers for business-related purposes [3, 4]. We will take a closer look at the privacy policies of mainstream LLM service providers in Section 2.1.\nIndeed, the prompt collection is essential for improving the quality of LLM responses. Training data for LLM models are collected from a broad range of sources to ensure the model's generalization and flexibility. Without proper context, LLM may generate irrelevant or even nonsensical responses [5]. Using previous prompts to fine-tune and perform in-context learning improves the coherence and relevance of subsequent responses, effectively serving as a few-shot learning mechanism [2], but these features raise privacy concerns, as the refined model may leak sensitive information [6].\nIn addition, LLM plugins, such as online shopping portal and travel arrangement service, provide rich functionalities that allow LLM to interact with the real world, and these plugins may require users to share their inputs with third-party service providers to fulfill the requested tasks [7, 8]. Once the user's inputs are shared with third-party service providers, the providers may store and process them in accordance with their respective privacy policies. For example, Expedia has developed a plugin for ChatGPT for streamlined travel booking through LLM chat experience [9]. When users enable the plugin and ask for travel accommodation, their prompts are processed by Expedia. Although Expedia's privacy policy does not specifically mention LLM prompts, it does state data that enables booking may be collected and used [10].\nSimilar privacy concerns have been raised and addressed in voice assistant systems [11]. Voice assistants constantly listen to users' voice commands, and send them to the cloud for processing. If requests are intended for third-party services, their transcript may be sent to third-party skills for further processing [12]. Megamind [11] addresses this privacy issue by interposing on the voice commands and filtering out sensitive information locally, so that only non-sensitive information is shared with the cloud and third-party skills.\nTo address the urgent need for privacy-preserving online LLM services, we design and implement a lightweight, yet effective, privacy-preserving system for online LLM services, called Casper. Casper is a browser extension that runs entirely on user's device."}, {"title": "2 BACKGROUND", "content": "Cloud-based LLM services, such as OpenAI's ChatGPT[24] and Microsoft's Copilot [25], have dramatically improved the accessibility of LLM to regular users through web interfaces. These online services enable a personal device to access powerful LLM models hosted on the cloud, which are trained with the entire human knowledge available on the internet [2]. Public health [26] and education [27] are examples of many areas that have benefited from the use of online LLM services.\nThe LLM service providers specify their data collection and usage practices in their privacy policies. For example, Microsoft's privacy statements indicate that Bing's generative AI feature, namely Copilot, follows the same data collection policy as Bing web search, which may include user's \"searches or commands\" \"(in the form of text, voice data, or an image),\" and the de-identified data are shared with \"selected third parties\" [3].\nOpenAl's privacy policy states that usage data may be collected for improvement, research, development, misuse prevention, and other business purposes, and the data may be provided to third parties, such as \u201cproviders of hosting services, customer service vendors, cloud services, email communication software, web analytics services, and other information technology providers,\" and business account administrators [4].\nLikewise, Anthropic (which runs Claude.ai) states in its privacy policy that it collects \"Inputs and Outputs\" of its LLM models for service improvement, research, and marketing purposes, and the data may be retained \"for as long as reasonably necessary\" [28]."}, {"title": "2.2 Benefits and Risk of Prompt Collection", "content": "Users are increasingly concerned about their privacy in using online services. Microsoft privacy report indicates that 14 million users have visited Microsoft privacy dashboard in a six-month period in 2023, representing a 21% increase from the previous reporting period [29]. The industry and academia have proposed various solutions to mitigate privacy issues in cloud-based Artificial Intelligence (AI), such as differential privacy [30] and federated learning [31]. However, online LLM services are not able to fully leverage these solutions due to the fact that they require user prompts in plaintext for training and providing third-party services.\nWell-structured prompts provided by users are valuable resources. [32, 33]. As stated in their privacy policies, LLM service providers collect prompts for service improvement and research purposes [3, 4]. Because of the black-box nature of LLM service providers, the usage of prompts is largely unknown to users. Past incidents have highlighted that user queries made available to research may be leaked inadvertently. In 2006, 20 million AOL web search queries, intended for research purposes, were accidentally released to the public, resulting in the identification of individuals in real life [34].\nOne of the widely-adopted usage of prompts is to improve the quality of subsequent LLM responses for the same user. Fine-tuning and in-context learning are commonly used techniques to improve the relevance of LLM responses to user's previous inputs [2]. The prompts used to fine-tune models are kept and processed by LLM service providers' infrastructure. While these models are not directly exposed to other users, there is a risk of sensitive information being leaked through data breaches or novel attacks that exploit shared infrastructure.\nThere is a increased risk of personal information leakage through adversarial ML attacks if other users can interact with the fine-tuned model. Model inversion attacks are a type of privacy attack that aims to recover the training data of a machine learning model by querying the model with maliciously crafted inputs [35]. Prompts provided for training may contain PIIs, medical records, legal documents, and other sensitive information, which can be recovered by adversaries. We discuss our threat model in Section 3.\nBesides advanced ML model inversion attacks, data breaches are another major concern of prompt collection. Recent server-side data breaches [36-38] have exposed a massive amount of sensitive personal information, including social security numbers, credit card numbers, and personal history. A security breach in 2023 at OpenAI exposes internal data [39]. These incidents highlight the risk of retaining PIIs and other sensitive information on servers.\nEven legitimate uses of sensitive personal information can be detrimental to user's interest. Personal web browsing and searching history are commonly used to provide personalized advertisements, and they may be used for price discrimination [40, 41] and even in insurance decisions [42]. As a result, users are increasingly concerned about sharing personal information with third parties [43].\nFortunately, it is not necessary for users to provide PIIs to use online LLM services, and users can avoid sharing sensitive information by crafting prompts carefully. However, this requires users to carefully examine every prompt that they provide to LLM service providers, which is not a scalable solution for users to protect their privacy."}, {"title": "2.3 Third-Party Extensions to AI Services", "content": "AI-based online services, such as voice assistant and LLM, on their own are limited in functionalities. Third-party extensions are needed to enhance the capabilities of these services. Third-party extensions running on top of Amazon's voice assistant, Alexa, are called skills [12]. Skills have been widely adopted for Internet of Things (IoT) control, entertainment, and productivity [12], but they have also raised privacy concerns.\nThe poor privacy practice of voice assistant extensions has been widely reported: according to a study in 2021 by Lentzsch et al., only 24.2% of all skills provide a privacy policy link, and approximately \"23.3% of privacy policies are not fully disclosing the data types associated with permissions requested by the skill\" [44]. This observation corroborates the findings of a 2017 study by Alhadlaq et al., that 25% of around 10,000 skills have no privacy policy [45]. Although the number of skills has rapidly increased from 135 in 2016 to about 125,000 in 2021 [46], as the statistics from [44] and [45] show, the availability of privacy policies of Alexa skills has not improved from 2017 to 2021.\nTo address the privacy concerns in third-party skills, since 2023, Amazon has required all skill developers to provide a privacy policy link before their skills can be published at the Alexa Skill Store [47]. However, privacy policies may not be sufficient to protect users' privacy, as many of them are not accurate [44], and the actual enforcement of privacy policies is unclear. Megamind [11] is a system solution deployed at the voice assistant devices to enforce privacy control before the voice data is sent to the cloud. It provides strong privacy guarantees by ensuring that the voice data is processed locally and only necessary information is sent to the cloud."}, {"title": "2.3.2 LLM Plugins.", "content": "Parallel to Alexa skills, third-party plugins are increasingly popular in online LLM services to enable LLM to interact with the real world.\nAs shown in Figure 1a, online LLM service providers send live or stored LLM interactions to third-party providers, so that they can use the prompts and responses to provide services [7, 8]. LLM plugins share some similarities with voice assistant skills, but their functionalities are more diverse, exposing users to a wider range of privacy risks. Although both Microsoft Copilot and OpenAI ChatGPT's plugin features are currently in their early stages, they are expected to gain popularity in the near future.\nOne special plugin is the ChatGPT retrieval plugin. As mentioned in the plugin's security statements, instead of retrieving information from the Internet, the plugin searches \"a vector database of content\" to ensure that \"it doesn't have any external effects\" [7]. This security consideration acknowledges and mitigates the risk of exposing user's prompts to an Internet search engine, but for third-party plugins, the risk is unchartered territory.\nMicrosoft Copilot has required third-party providers to provide privacy policies for their plugins [8]. However, as we have seen in the case of voice assistant skills, mandating third-party providers to provide privacy policies may not be sufficient to protect sensitive information in user's prompts. Indeed, according to our study, although all plugins showcased on ChatGPT Plugins page [7] have privacy policies, only 27.3% of plugins' privacy policies mention LLM. All the surveyed privacy policies further mention that they share user data with their business affiliates, partners, or service providers (which are fourth parties from the perspective of LLM users). We show the statistics in Table 1."}, {"title": "3 THREAT MODEL", "content": "We consider users to be honest, are using a not compromised system and browser to access online LLM services, and are willing to share their prompts with LLM service providers and certain third-party providers (through LLM extensions). However, they are concerned about personal privacy and do not want to burden the providers with the responsibility of protecting their sensitive information in the prompts.\nWe consider LLM service providers to be only trusted for correctly fulfilling user's requests, but not trusted for withstanding malicious attacks. Under such assumption, LLM service providers communicate with users and third-party providers through encrypted channels, and use industrial standard technologies to protect user's data from unauthorized access. However, LLM providers may collect and use user's prompts for business-related purposes, such as improvement and research, which may unintentionally disclose sensitive information through data breaches or ML model inversion attacks [35].\nWe assume the same threat model for third-party providers as LLM service providers, but they are at a higher risk due to the fact that they may have various privacy and security policies and practices."}, {"title": "4 MOTIVATION AND DESIGN GOALS", "content": "One obvious solution to the privacy concerns of online LLM services is to perform all LLM inference locally on user's device. Indeed, local LLM inference has been made possible by open-source LLM models [2]. WebLLM leverages WebGPU to accelerate LLM inference using local CPU or GPU through a web browser [48]. Likewise, TinyChat is a framework for local LLM inference on edge devices, such as robots and in-car systems [49].\nHowever, local LLM inference is not comparable to cloud-based LLM services in terms of performance and scalability. The training cost of OpenAI's GPT-4 model costs over 100 million dollars [50], and so does the training cost of Google's Gemini Ultra model [51]. Inference of these models requires an (undisclosed) large amount of computational resources, which are not possible to be run on user's devices."}, {"title": "4.2 Straw-Man Solution 2: Cloud-based PII Redaction", "content": "Another solution to address the privacy concern is to perform PII redaction and other sensitive information sanitization on the LLM service providers' side. The LLM service providers can deploy black-box algorithms to remove sensitive information from users' prompts before they are processed by LLM models. The black-box algorithm may even utilize an LLM model to identify sensitive topics in the prompts.\nHowever, this approach creates a trust issue between users and LLM service providers. As mentioned in our threat model in Section 3, users may not trust LLM service providers to protect their privacy, as the providers have different privacy policies, and may even have incentives to retain sensitive information."}, {"title": "4.3 Research Challenges and Design Goals", "content": "Casper's goal is to address the privacy concerns in submitting sensitive information to online LLM services. Compared with running LLM inference locally, cloud-based LLM services are necessary for users to access prohibitively large models and to leverage the latest model updates.\nIt might be tempting to perform PII redaction on the LLM service providers' side, but this approach is not ideal. The LLM service provider may not have the incentive to protect users' privacy as much as the users themselves. Also, users may not trust LLM service providers to use black-box algorithms to protect their privacy. We identify the following research problems, which are also design goals, for Casper to address:\nGoal 1: Comprehensive Privacy Protection. The primary goal of Casper is to provide comprehensive privacy protection for cloud-based LLM services, by filtering out sensitive personal information with or without a fixed pattern.\nGoal 2: Client-Side Filtering. Casper should run entirely on user's device without using any cloud-based services, so that user's sensitive information never leaves their devices. This includes the LLM inference for topic identification and any other filtering processes.\nGoal 3: Lightweight and Efficient. The system design and implementation should not introduce significant performance overhead to the user's device, and should not burden users with complex configurations or software installations. The best user experience is achieved when Casper is readily deployable on a web browser as a browser extension.\nGoal 4: Compatibility. The system architecture should be compatible with existing online LLM services. Casper should not require any modification to the existing web interfaces of online LLM services. Other than privacy protection, the user interaction with the LLM services should remain the same.\nGoal 5: User Awareness and Control. If a prompt contains sensitive topics but not specific keywords or patterns, the user should be informed of the potential privacy risks. The warning should be clear and informative to raise user awareness of the privacy risks. Users should have the ability to customize the filtering rules to include user-specific patterns and topics that they do not want to share with LLM services."}, {"title": "5 DESIGN", "content": "In this section, we present the design of Casper. We present a system overview, followed by an overview of our three-layered prompt sanitization mechanism. We then discuss the design decisions and trade-offs in each of the three layers, i.e., rule-based filtering, ML-based named-entity detection, and local LLM-based topic identification. We also discuss the research challenges in the data preparation for local LLM topic identification, and the user interface design for awareness-raising. Finally, we present the feature-preserving redaction mechanism in Casper."}, {"title": "5.1 System Overview", "content": "As shown in Figure 1b, Casper examines the prompts provided by users when they interact with online LLM services. Casper identifies two types of private information in the prompts: PIIs and sensitive topics.\nCasper redacts PIIs from the prompts by replacing them with unique placeholders before sending them to LLM services. By sharing only the placeholders with LLM services, Casper ensures that they can still generate relevant responses without knowing the original PIIs. And when the LLM service returns the responses, Casper reverts any placeholders in the response back to their original forms before displaying the responses to the users.\nFor sensitive topics, such as personal medical and legal information in user's prompts, Casper identifies them and alerts users to review and confirm before sending them to the LLM service. The user interface of Casper clearly indicates the sensitive topics involved in the prompts, and helps users to make informed decisions about sharing them with LLM services (fulfilling our goal of user awareness and control, Goal 5).\nAs shown in the example in Figure 2, Casper filters out sensitive identifiers, alerts users of sensitive topics, and then forwards the sanitized prompts to an unmodified LLM service pipeline (fulfilling our goal of compatibility with existing LLM services, Goal 4). As the prompts are sanitized before they leave the user's device, the LLM service provider, plugins, and other third-party providers only receive sanitized prompts and do not have access to the original prompts."}, {"title": "5.2 Three-Layered Prompt Sanitization", "content": "Casper sanitizes the prompts in three separate stages: rule-based filtering, ML-based named-entity detection, and local LLM-based topic identification. The rule-based filtering identifies fixed patterns, as well as user-defined patterns and keywords. The ML-based named entity detection identifies names and other identifiers that do not follow fixed patterns.\nThe combination of these three stages is based on a sound rationale: As shown in Figure 3, rule-based filtering and named-entity detection cover a wide range of sensitive PIIs and may overlap in identifying known patterns and named entities of unknown patterns in the prompts. However, a prompt may contain information about certain sensitive topics that are not readily identifiable with fixed patterns, and may not contain any named entities. Local LLM-based topic identification is designed to catch the information that may be missed by the first two stages. These three stages are designed to complement each other, fulfilling our design goal of comprehensive privacy protection (Goal 1).\nCasper runs entirely on user's device, and does not rely on any cloud-based services. All these three stages are self-contained within our browser extension, fulfilling our design goal of client-side filtering and lightweight and efficient design (Goal 2 and Goal 3). Although prompts go through three stages of filtering, we demonstrate that the performance overhead of Casper is minimal (14.7%). We present a comprehensive evaluation of the effectiveness and performance overhead of Casper in Section 7."}, {"title": "5.3 Rule-Based Filtering", "content": "Rule-based filtering is the first stage of prompt sanitization. It identifies generic patterns that are commonly associated with a person's identity, such as alphanumeric patterns that resemble a phone number, social security number, credit card number, and email address. Users can further customize the rules in the form of regular expressions to include or exclude specific patterns. The rule-based filtering also identifies keywords that users provide, which they may consider sensitive, such as their names, cellphone numbers, and other keywords that they do not intend to share with LLM services."}, {"title": "5.4 Named-Entity Detection", "content": "Named entities are words that refer to a certain entity, such as a person, a location, an organization, or a miscellaneous entity. Miscellaneous entities can be any entities that do not fall into a specific category. For example, a product name or an event name is a miscellaneous entity. Named-entity cannot be readily identified by rule-based filtering, as they can be any word, and may not follow a specific pattern. Therefore, to complement the rule-based filtering, we adopt an ML-based named-entity detection model to identify them in the prompts. A named entity detection model is trained on top of a large dataset of named entities, and the accuracy of the model depends on the quality and quantity of training data.\nNamed entity detection is an evolving field in natural language processing, and there are many pre-trained models available [52-55]. One research challenge is to create an architecture that can plug and play different named entity detection models. To address this challenge, we design a modular architecture using Transformers.js [56] to load a pre-trained model in our browser extension. We leverage the common design of named entity detection models, where the model takes a string as input and returns a list of identified entities in the string. We tailor Casper's programming interface to be compatible with the common design, so that Casper can easily adopt different named entity detection models that fit user's need and language preference."}, {"title": "5.5 Sensitive Topic Identification", "content": "The topic identification is the final stage of prompt sanitization. The reason behind this stage is that prompts may reveal sensitive personal situations without containing any keywords, patterns, or named entities. One such example we raised in Section 1 is that a user may ask questions about their medical symptoms without mentioning any specific medical jargon. As mentioned in Section 2.2, this information will be shared with third-party providers and may be used against the user's interest, such as price discrimination [40, 41] and insurance decisions [42].\nOur design addresses this issue by deploying a local LLM model running entirely in a browser extension to identify sensitive topics in the prompts. Casper adopts WebGPU-based LLM inference architecture [48] to accelerate the LLM inference. Users have full control over the LLM model and can choose to use various pre-trained models of different model sizes and context lengths available online that fit their computers' configurations.\nWe show the template of the local LLM query for topic identification below,\nLocal LLM Query for Topic Identification: Below is a prompt. Answer only yes or no based on if the prompt contains TOPIC information. \"DATA\""}, {"title": "5.6 Data Preparation for Local LLM Topic Identification", "content": "We face a research challenge as we feed our query to the local LLM model. Our query template (as shown in Section 5.5) is a combination of our request to identify a certain topic in the prompt, and the prompt itself. The prompt may contain questions, statements, data, and other information, that confuses the LLM model. This challenge has been documented by [58] where the authors show that LLMs cannot distinguish between user's questions and attached data, and therefore often generate irrelevant responses. The suggested solution in [58] is to improve the LLM model in explainability, but this is a long-term research direction, and not applicable to our current work as we are using a pre-trained model.\nHowever, we discover a lightweight solution to our unique use case of topic identification. Psycholinguistic research has shown that the informativeness of a sentence is mainly carried by the nouns (including Direct Object (DO) nouns) [59]. Inspired by this observation, we extract the nouns from the user's prompt, and use them as the data field of our local LLM query. This design decision not only resolved the model confusion problem mentioned earlier, but also reduced the token count of the prompt and improved the processing time of the local LLM model.\nIn our design, we also address the potential issue of long prompts. As LLM models have token limits, and online LLM services may have a bigger limit than local LLM models, we truncate the prompt to the maximum token limit of the local LLM model, and query the local LLM model for one truncated section of the prompt at a time.\n```\nAlgorithm 1: Local LLM Topic Identification\n1 extract nouns;\n2 count currTokens;\n3 update totalTokens;\n4 if totalTokens > limit then\n5 divide nouns into chunks of maximum token limit;\n6 for each chunk do\n7 reload model;\n8 reset totalToken;\n9 checkTopic(chunk);\n10 else\n11 checkTopic(nouns);\n12 return response;\n13 Function checkTopic (nouns):\n14 for each topic do\n15 if totalTokens > limit then\n16 reload model;\n17 send nouns to local LLM;\n18 wait for response;\n19 update totalToken;\n20 clear chat history;\n21 if sensitive topic detected then\n22 return response;\n```"}, {"title": "5.7 Awareness-Raising User Interface", "content": "Casper warns users about privacy risks when their prompts contain PIIs or sensitive topics. The warning message highlights the private and sensitive information detected in the prompt, and provides a reference to the online LLM service's privacy policy. The warning message is designed to be clear and informative, and its user interface ensures that users can quickly understand the privacy risks if they decide to share the prompts with LLM services. If a user decides to proceed, they can click a button to acknowledge the warning and send the prompt to the LLM service."}, {"title": "5.8 Feature-Preserving Redaction", "content": "Casper replaces PIIs detected by pattern-based filtering and named-entity detection with unique placeholders. For person names and other named entities, pseudo names are generated using Faker.js [60]. Casper maintains a mapping of the original PIIs and the placeholders, allowing it to revert the placeholders to the original PIIs when a response is received from the LLM services.\nThis design has several benefits: First, it ensures that LLM services can still generate relevant responses without knowing the actual PIIs. Second, the redaction process is behind the scenes and does not affect the user experience. Because of the user-transparent design of the PII redaction, (although the false positive rate of named-entity detection is relatively low (13.3%)), Casper tolerates false positives in the redaction process as all placeholders are reverted to their original forms in the responses displayed to users. Users can still interact with the LLM services as they normally would, and the responses are displayed with their original PIIs as if the redaction process never happened."}, {"title": "6 IMPLEMENTATION AND PROTOTYPE", "content": "Casper Browser Extension\nCasper is implemented as a browser extension for Google Chrome version 124. The extension is written in TypeScript, HTML, and CSS in approximately one thousand lines of code. The browser extension features an icon in the browser toolbar, which users can click to open Casper's configuration panel. The configuration panel features personal information setup, regular expression customization, sensitive topic configurations, model selections for local LLM, and local LLM and named entity detection settings.\nAlthough Casper's design applies to other browsers and online LLM services, we focus on Google Chrome and OpenAI's ChatGPT in our prototype implementation. The extension activates itself when user visits the ChatGPT website, and it analyzes user's prompts using the three-layered filtering mechanism (Section 5). We implement all three layers of prompt sanitization in TypeScript in the browser extension, and therefore, there is no need for any software installation or cloud-based services."}, {"title": "6.2 Rule-Based Filtering", "content": "The filtering rules are implemented as keyword search and regular expression match in TypeScript. Starting with common PII patterns, such as email addresses, phone numbers, and passwords, we iteratively improve the regular expression patterns to include more PII patterns with the help of LLM. Note that we use LLM merely as a development tool and do not introduce LLM in the rule-based filtering process.\nWe ask LLM to generate worldwide PII examples, evaluate our regular expression patterns against the generated strings, and add the missing patterns to our regular expression pool. We repeat this process until we are confident that our regular expression patterns cover most PII patterns. Our regular expression pool currently consists of 10 patterns. Each pattern covers several formats of a specific PII such as various formats of phone numbers. Users may include their own keywords or patterns in the filtering rules through the configuration panel."}, {"title": "6.3 Named-Entity Detection", "content": "We use bert-base NER [61] which is a fine-tuned BERT model [55] for named-entity detection. The model has 110 million parameters and is trained on CoNLL-2003 Named Entity Recognition [62] dataset, which includes 6600 person names, 7140 locations, 6321 organizations, and 3438 miscellaneous examples. Although the model is implemented for TensorFlow [63] toolchain, we utilize Trasnformers.js [56] to load the model from within our browser extension. Casper sends user's prompts to the local Transformers.js interface, and synchronously receives results from the model in the form of a list of recognized entities in the string. Each of the recognized entities is tagged with its type and a score indicating the model's confidence in the recognition. This score provides an opportunity for future enhancement, such as only reporting recognized entities above a certain threshold. Casper provides a default score threshold of 90 out of 100, which can be adjusted by users.\nThrough Casper's configuration panel, users can turn on or off the named-entity detection feature. As discussed in Section 5.4, the named-entity detection model can be easily replaced with other models, as long as the model is compatible with the Transformers.js library."}, {"title": "6.4 Local LLM-based Topic Identification", "content": "Casper uses a local LLM model for topic identification. We implement local LLM inference by leveraging the WebLLM framework [48]. We port the WebLLM framework to our browser extension. The WebLLM framework provides high flexibility in changing the LLM model, therefore, it makes switching between different LLM models easy in Casper. Users can select from a few LLM models on the configuration page of Casper based on their needs. The local LLM model Casper uses is the Llama 3 model with 8 billion parameters. As shown in Section 7.3, we choose the Llama 3 model for its balance between performance, accuracy, and resource consumption.\nCasper sends user's prompts to the local LLM model through Web LLM API and synchronously reads a binary result from the model which indicates whether the prompt contains privacy-sensitive topics. Users can optionally turn on or off the topic identification feature and the default Llama 3 model can be replaced with other models through our configuration panel. Indeed, before the Llama 3 release on April 18, 2024 [64], the best trade-off among our criteria was the Llama 2 model, which we used in our early prototype development. With less than 10 lines of code modification and approximately 15 minutes of human effort, we successfully switched to the Llama 3 model.\nDuring our development, we notice that the detection accuracy degrades significantly when we ask the LLM model to identify more than one topic, such as both medical and legal topics. We show the results of this experiment in Table 2. To address this issue, we perform topic identification for each topic separately. For example, if a user asks if a prompt contains both medical and legal topics, we first ask the local LLM model to identify medical topics, and then ask it to identify legal topics. To avoid previous inputs from influencing the detection in the subsequent prompt, Casper clears the chat history after each run of topic identification."}, {"title": "6.5 User Interface Integration", "content": "We implement Casper to tailor the user interface of OpenAI's ChatGPT. Our browser extension locates the prompt input box in ChatGPT web interface and analyzes user's prompts before they are submitted. Due to the limitations of Chrome extension API", "limitation": 1}]}