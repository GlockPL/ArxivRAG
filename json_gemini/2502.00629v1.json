{"title": "Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic Mathematical Reasoning", "authors": ["Yuxuan Wu", "Hideki Nakayama"], "abstract": "In recent years, neuro-symbolic methods have become a popular and powerful approach that aug- ments artificial intelligence systems with the capability to perform abstract, logical, and quantitative deductions with enhanced precision and controllability. Recent studies successfully performed sym- bolic reasoning by leveraging various machine learning models to explicitly or implicitly predict intermediate labels that provide symbolic instructions. However, these intermediate labels are not always prepared for every task as a part of training data, and pre-trained models, represented by Large Language Models (LLMs), also do not consistently generate valid symbolic instructions with their intrinsic knowledge. On the other hand, existing work developed alternative learning techniques that allow the learning system to autonomously uncover optimal symbolic instructions. Nevertheless, their performance also exhibits limitations when faced with relatively huge search spaces or more challenging reasoning problems. In view of this work, we put forward an advanced practice for neuro-symbolic reasoning systems to explore the intermediate labels with weak supervision from problem inputs and final outputs. Our experiments on the Mathematics dataset illustrated the effectiveness of our proposals from multiple aspects.", "sections": [{"title": "1 Introduction", "content": "In recent years, rapid progress in deep learning has yielded remarkable achievements across various areas and tasks from computer vision to natural language processing. Nevertheless, the solutions to mathematical problems are still often considered a formidable challenge due to the demand for precise logical and quantitative calculation and deduction, which is discordant with the fundamental nature of deep learning models that rely on stacked layers of neurons for estimations and fittings. To address these constraints of neural networks, researchers gravitated towards neuro-symbolic methods. The concept of neuro-symbolic learning is vast, and its origin dates back to the last century [1]. Modern neuro-symbolic practices also have integrated symbolic assistance at various learning stages adapting to the distinct characteristics and requirements of varied tasks [2, 3]. Specific to mathematical reasoning tasks, neuro-symbolic approaches typically comprise symbolic practice by employing neural networks to receive and interpret question texts and generate symbolic notations as instructions. Subsequently, these symbolic notations are deciphered with a calculator or interpreter in a rule-based manner to acquire the final answer. This approach empowers the learning system to leverage the generalization capacity and end-to-end training of neural networks along with the precision and controllability of symbolic calculations. This advantageous feature allows neuro-symbolic methods to achieve impressive performance across various mathematical reasoning tasks and datasets [4, 5, 6].\nHowever, a pivotal concern underlying the success of neuro-symbolic methods is associated with the training of their inference models. From the perspective where mathematical reasoning tasks are viewed holistically, the final answers, as the destination of reasoning, are naturally regarded as the inherent labels in these tasks. In contrast, the symbolic"}, {"title": "2 Related Work", "content": "Mathematical reasoning tasks comprise a series of tasks developed to examine the ability of machine learning systems to solve mathematical problems and conduct abstract, logical, and quantitative reasoning. Early studies in this field started with collecting and solving applied mathematical problems described in natural languages, which are also known as math word problems. Several representative datasets for math word problems include MAWPS [7], Math23K [4], and MathQA [5]. Pushing the boundaries beyond applied mathematical problems, Saxton et al. constructed a large-scale dataset offering mathematical problems generated in a wide spectrum of mathematical areas [11], and other work has also introduced datasets specialized to geometric problems [12] and theorem proving [13, 14]. Upon entering the era of LLMs, recent work published datasets collating mathematical problems together with their solutions annotated in natural languages [6]. Such tasks have also become an important benchmark in the evaluation of LLMs [15]."}, {"title": "2.1 Mathematical Reasoning Tasks", "content": "Mathematical reasoning tasks comprise a series of tasks developed to examine the ability of machine learning systems to solve mathematical problems and conduct abstract, logical, and quantitative reasoning. Early studies in this field started with collecting and solving applied mathematical problems described in natural languages, which are also known as math word problems. Several representative datasets for math word problems include MAWPS [7], Math23K [4], and MathQA [5]. Pushing the boundaries beyond applied mathematical problems, Saxton et al. constructed a large-scale dataset offering mathematical problems generated in a wide spectrum of mathematical areas [11], and other work has also introduced datasets specialized to geometric problems [12] and theorem proving [13, 14]. Upon entering the era of LLMs, recent work published datasets collating mathematical problems together with their solutions annotated in natural languages [6]. Such tasks have also become an important benchmark in the evaluation of LLMs [15]."}, {"title": "2.2 Mathematical Problem Solvers", "content": "While it is feasible to train machine learning models to solve mathematical problems through end-to-end predictions, existing studies have revealed significant limitations in their generalization capacity, which is generally because end-to-"}, {"title": "3 Problem Definition", "content": "In this work, we study the practice of neuro-symbolic methods and verify our proposals on the mathematical reasoning task specific to question-answering problems. In this task, the basic unit of learning and evaluation is the pair of questions and answers. An example is presented in Table 1. As for the preprocessing, we extract the numerical tokens present in each question and substitute these tokens with token (Ni), where i indicates the index of the original token within the question text. We let q denote the question text with replaced tokens, and num denote the tuple of extracted numbers. We also refer to q as the \"template\" because multiple raw questions can be converted to the same q if the differences in their question texts are confined to the numerical tokens. In this case, these questions are consolidated to a single q, and their corresponding num and a are collected into sets {num} and {a}.\nOn the other hand, we let f denote the symbolic instructions expected to be produced by the learning system for solving the corresponding question. They are also referred to as \"formula\" in the following discussion. Theoretically, f can be arranged in any format as long as it conveys unambiguous semantic meaning and can be executed by an associated interpreter. In this work, we implement f with a sequential and functional Domain Specific Language (DSL). A formal definition of this DSL is provided by Equations 1 to 4. As shown by the example in Table 1, a formula f comprises a sequence of basic terms [S0, ..., Sn]. Each of these terms Si can be regarded as a function call, which comprises an operator opr and a certain number of operands opd\u2081 to opdk. Given an interpreter and specific num, these terms can be computed in order from S0 to Sn, and the computation result of the last term Sn yields the result of the entire formula. With I(\u00b7) denotes a formula interpreter, we have a = I(f, num)."}, {"title": "4 Proposed Method", "content": "The general learning procedure adopted in our study is formulated with Algorithm 1. Its fundamental idea is to implement a recurrent exploration process in which optimal f for each q is gradually acquired and the inference model is continuously optimized. Here, PolicyNet denotes the inference model that takes q as input and predicts the formula f. D denotes the preprocessed dataset containing learning instances {(q, {num}, {a});}. L denotes a dictionary recording the optimal formula f together with the accuracy it achieved for each q. L is initialized as empty.\nAfter initialization, the learning procedure is organized into numerous basic loops. Within each loop, we first employ curriculum learning to determine the instance (q, {num}, {a}) for the learning in the current loop with a Sample function, of which the details are presented in Section 4.2. Then, we conduct a Search procedure with the purpose of searching for an optimal formula f for the sampled instance, of which the details are presented in Section 4.3. After the formula f and its corresponding accuracy accuf is returned by Search, L is updated with f and accuf. Concretely, if f is not None, and if either no f has been recorded for q in IL or accuf surpassed the previously recorded accuracy, f and accuf will be updated for q in L. After this, PolicyNet is trained with the (q, f) pairs recorded in L. At the end of each learning loop, we introduced a reflection mechanism to reduce the redundancy of formulas and try rectifying formulas whose effectiveness is misjudged. The details of this Relect procedure are presented in Section 4.4."}, {"title": "4.1 General Learning Procedure", "content": "The general learning procedure adopted in our study is formulated with Algorithm 1. Its fundamental idea is to implement a recurrent exploration process in which optimal f for each q is gradually acquired and the inference model is continuously optimized. Here, PolicyNet denotes the inference model that takes q as input and predicts the formula f. D denotes the preprocessed dataset containing learning instances {(q, {num}, {a});}. L denotes a dictionary recording the optimal formula f together with the accuracy it achieved for each q. L is initialized as empty.\nAfter initialization, the learning procedure is organized into numerous basic loops. Within each loop, we first employ curriculum learning to determine the instance (q, {num}, {a}) for the learning in the current loop with a Sample function, of which the details are presented in Section 4.2. Then, we conduct a Search procedure with the purpose of searching for an optimal formula f for the sampled instance, of which the details are presented in Section 4.3. After the formula f and its corresponding accuracy accuf is returned by Search, L is updated with f and accuf. Concretely, if f is not None, and if either no f has been recorded for q in IL or accuf surpassed the previously recorded accuracy, f and accuf will be updated for q in L. After this, PolicyNet is trained with the (q, f) pairs recorded in L. At the end of each learning loop, we introduced a reflection mechanism to reduce the redundancy of formulas and try rectifying formulas whose effectiveness is misjudged. The details of this Relect procedure are presented in Section 4.4."}, {"title": "4.2 Problem Sampling", "content": "Given the fact that the difficulty of mathematical problems within a dataset may vary largely, and the solutions to relatively difficult problems usually lead to extremely huge spaces of formulas that cannot be well-explored in a"}, {"title": "4.3 Formula Search", "content": "As for the Search procedure executed in line 5 of Algorithm 1, its detail is presented by Algorithm 2. For this procedure, we basically followed and improved the search algorithm proposed by prior study [10]. The general idea of this algorithm is to organize a heuristic search on a graph structure with consideration of the confidence level of each formula and the formulas being close."}, {"title": "4.3.1 Formula Graph", "content": "In Algorithm 2, G denotes a graph structure utilized to store the formulas under exploration in each Search attempt. G is maintained under the following two rules:\n\u2022 Each of its nodes nf represents a unique formula f.\n\u2022 There is an edge between two nodes if and only if the distance between the formulas they represent is one."}, {"title": "4.3.2 Graph Initialization", "content": "As shown in line 2 of Algorithm 2, the graph G is initialized at the beginning of each Search attempt. During initialization, nodes representing formulas of the following three types are added to G:\n\u2022 [(+, NO, N1)], which is a universal starting point of search by default.\n\u2022 N formulas predicted by PolicyNet given q with top-N likelihood.\n\u2022 M formulas recorded in IL for the M templates that are semantically closest to the current given q.\nFollowing this configuration, G should be initialized with a maximum of (N+M+1) initial nodes, while some of them can be merged if there are overlaps among these three types of initial nodes and duplications in the (N+M+1) formulas. Here, we determine the semantic distance between two templates q and q' by calculating the Euclidean distance between their sentence embedding generated by PolicyNet. That is, with Ep(\u00b7) denotes the encoder of PolicyNet, distance(q, q') = ||Ep(q) - Ep(q')||2."}, {"title": "4.3.3 Formula Sampling", "content": "As shown in line 4 of Algorithm 2, we sample a formula fexp from G as the formula to be explored in each search iteration. Following the idea of observing the confidence level of each formula and the formulas being close, prior study proposed an Expectation value defined by Equation 8 on each node to estimate how reliable the formula it represents can be for solving q [10]. In each iteration of search, the node with the greatest Expection value among unexplored nodes is selected as the node whose formula fexp is going to be explored."}, {"title": "4.3.4 Formula Mutation", "content": "As shown in line 5 of Algorithm 2, mutations from fexp are generated to expand the graph G at the end of each search iteration. Here, we generate new formulas whose distance from fexp is one by inserting, removing, or modifying one single formula term. The freshly generated formulas are added to G if they do not exist in G yet, and the relevant edges should also be updated to keep G conforming to its definition.\nAfter the search reaches the maximum number of iterations, the formula on which the highest accuracy is achieved is considered the optimal formula fbest for solving q. fbest is returned with the accuracy it achieves after CleanUp as the result of Search. Together with Reflect, we proposed this CleanUp procedure to reduce the redundancy of the formulas obtained through a heuristic search. The details of this CleanUp procedure are presented in Section 4.4. If none of the formulas achieve a non-zero accuracy, Search returns None."}, {"title": "4.4 Formula Clean-Up and Reflection", "content": "Through our study and experiment on mathematical problems with the learning method introduced above, a phenomenon we observed is that the formulas derived via a heuristic search occasionally exhibit unnecessary prolixity. Table 2 illustrates this phenomenon on a simple problem. In contrast to fo, which can be considered a valid and concise formula for solving the given q, even though f1 expresses the same computation as fo, its first term does not virtually contribute to the subsequent computations so that it becomes redundant. This kind of redundant terms appears more or less in the search result because the search process advances by generating mutations via inserting, removing, and modifying formula terms. f2 illustrates another case of prolixity, whereas it is caused by unnecessary inverse operations instead of unused terms. Including these prolix formulas into the training data of inference models without proper processing can adversely affect the learning efficiency and generalization capacity. In view of this, we proposed the CleanUp and Reflect procedures in our study.\nConcretely, with the first case of prolixity caused by unused terms named \u201ctype-I prolixity\u201d and the second case caused by redundant computations named \"type-II prolixity\", CleanUp is utilized to eliminate the type-I prolixity, we first perform CleanUp in line 9 of Algorithm 2 to eliminate the type-I prolixity before a formula is returned as the search result. For CleanUp, we simply verify whether each term Si is ever referred to as Mi in the subsequent terms, and remove the terms that are not. Note that the reference tokens referring to formula terms after the removed terms should be adjusted correspondingly to keep the reference intact.\nAs for the type-II prolixity, given that it is highly related to the particular properties of certain operations and thus may not be easily identified without injecting external knowledge, which is a practice we are committed to avoiding to uphold the universal applicability of the learning framework, we propose to leverage the inference model to make this judgment.\nAs for the Reflect procedure performed at the end of each learning loop in line 8 of Algorithm 1, we let PolicyNet reperform the inference on each q \u2208 L. With fo and accufo denote the original formula and accuracy recorded for q in L, and \u0192 denotes the fresh inference of formula made by PolicyNet, we first examine the effectiveness of f with I on {num} and {a} to obtain its accuracy accuf. Then, if accu \u00ee \u2265 accuf, and the length (i.e., the number of terms) of f is not longer than fo, we update the formula and accuracy recorded for q in L with (f, accuf). Intuitively, in this case, we consider that the inference model suggests an alternative formula that also effectively solves the relevant problem while consuming fewer calculation terms or exhibiting higher consistency with existing formulas. As a result, the previously recorded formula can be reasonably updated. This mechanism also contributes to the exclusion of formulas whose effectiveness is misjudged. We studied the effect of this reflection mechanism with an ablation study and reported the result in Section 6.3.2."}, {"title": "5 Asynchrony and Parallelization in Search Process", "content": "In addition to the techniques introduced above, we also incorporate the asynchrony and parallelization practices in the search process to further improve the search efficiency with engineering efforts. Here, our proposal is derived from a fact we observed in the search process that the workloads on CPU and GPU occur alternately. Specifically, the computational load on CPU primarily arises during the update of the formula graph and the evaluation of formulas. On the other hand, the computational load on GPU primarily arises during the scoring of formulas and the calculation of p(f|q, 0p) given PolicyNet. In consideration that these computations also occur sequentially in the original search process, the overall usage of both CPU and GPU ends up being low. This leaves us room for improvement by employing asynchrony and parallelization practices.\nTo make full use of the computational resources on both CPU and GPU, we first reimplement the formula scoring in an asynchronous manner as presented in Algorithm 3. Here, the Score function, which is a subprocedure of the main search process, and the GPU_Handler function are executed in two separate processes. They communicate with a data structure pipe. Generally, after receiving the formula f to be scored and the associated (q, {num}, {a}), the formula scoring procedure first delivers the task of calculating the likelihood of f with inference model to the GPU_Handler. Then, the formula evaluation is performed during the time spent waiting for the calculation on GPU_Handler to be done. The asynchronous execution of these two tasks enables the concurrent usage of computational resources on both CPU and GPU.\nFurthermore, we also implement a more general parallelized learning framework for the entire learning process. In this framework, the Search procedure in line 5 of Algorithm 1 is parallelized into multiple search processes, which allows multiple search attempts on different learning instances to be performed concurrently. In addition, there can also be multiple GPU Handlers to maximize resource utilization in a multi-GPU environment and serve more search processes. To achieve this, a central controller is responsible for not only organizing all the other procedures presented in Algorithm 1 but also the synchronization of inference models across multiple GPU Handlers. Concretely, after all the search processes finish and the PolicyNet is optimized as shown by line 7, the central controller broadcasts the optimized parameters of PolicyNet to all the GPU Handlers to keep inference models synchronized."}, {"title": "6 Experiments", "content": "In this study, we assesses the effectiveness of our methods on the Mathematics dataset [11]. We chose the Mathematics dataset for our experiments for several reasons. Firstly, the Mathematics dataset offers mathematical problems of diverse difficulties across multiple domains and problem categories, which allows us to evaluate our approach in various scenarios. Secondly, although neuro-symbolic methods are highly suitable for solving the problems in the Mathematics dataset, the original training data provided by the Mathematics dataset only comprises the question texts and final answers. This allows us to verify our weakly-supervised approach for learning intermediate labels in neuro-symbolic methods and make fair comparisons with other methods. Lastly, the Mathematics dataset affords millions of pre-generated question-answer samples for each problem category and supports unlimited data generation. This ensures a sufficient amount of training data for organizing and verifying the learning of formulas from scratch."}, {"title": "6.1 Experimental Setup", "content": "In consideration of the data types and computations that can be supported, as well as the requirements for formula examination, we collected a subset from the Mathematics dataset that satisfies two criteria. First, the answers to questions should be a single rational number. Second, each template should correspond to a minimum of three raw questions on average after data preprocessing. For the inference model, We employ the Memory-Interactive Learning Engine (MILE) [21] with an LSTM [22] encoder as the PolicyNet because its architecture is naturally compatible with the structure of the DSL we adopt. For the learning process, following the conventional practice adopted by existing research, we conducted the training and evaluation on each problem category separately. As for the hyperparameters, we let max_loop = 100000, max_iter = 10000, \u03b2 = 1.0, the number for initial nodes in formula graph N = 10 and M = 10, and the acceptable threshold for question answering accuracy \u03b3 = 0.99. The set of operations we utilized is shown in Appendix A and B."}, {"title": "6.2 Primary Result", "content": "The primary results of our experiments are presented in Table 3. In this table, in the first three rows, we first present the number of unique question templates obtained in the training set of each problem category after data preprocessing and the number and proportion of templates solved by the formula exploration in the training stage. Generally, these results indicate the effectiveness of the formula exploration and weakly-supervised formula learning. In the following rows, we show the test accuracy achieved by our methods and baselines on the test sets. Here, \u201cTransformer\" and \u201cLSTM\" are the baselines provided by [11] that implement end-to-end training. Moreover, we also investigated the performance of modern LLMs. Specifically, we first employ the GPT-3.5 model [23] to perform zero-shot and few-shot in-context learning and report the result as \u201cGPT-3.5 / zs\" and \u201cGPT-3.5 / fs\". The prompts we use for GPT-3.5 are shown in Appendix C. On the other hand, we leverage the GPT-4 model [24] to perform the Chain-of-Thought reasoning by utilizing the three prompts suggested by [19] that resulted in the best performance. The details of these prompts are shown in Appendix D. For generating the response with the GPT-3.5 and GPT-4 models, we set the temperature to 0.5 and top-p to 1.0.\nBased on these results, it can first be concluded that the weakly-supervised formula learning is capable of acquiring valid formulas in most problem categories. The proportion of question templates that are solved drops to some degree in some challenging problem categories represented by \u201cmixed\", which contains problems involving nested and mixed"}, {"title": "6.3 Ablation Study", "content": "In addition to the primary experiment, we also conducted ablation studies to verify the effectiveness of several crucial proposals and mechanisms employed in our learning framework. The first factor we investigated here is the effectiveness of the graph-based formula exploration technique, which is a fundamental proposal made in our work. To perform an ablation study, we set the weight w for summarizing formula score in Equation 9 from its default scale [0.4, 0.3, 0.2, 0.1] to [1.0, 0.0, 0.0, 0.0]. This prevents the formula sampling in line 4 of Algorithm 2 from observing the neighbors of each formula and makes the graph structure degenerate into an ordinary collection of independent formula nodes. In this experiment, we investigated the number of search iterations consumed in finding valid formulas for each learning sample with regular and degenerate w on three representative problem categories: mul_div_multiple, linear_1d, and conversion. The results are presented with histograms in Figure 1. From this result, it can be noticed that the search procedure with a regular w generally consumed fewer search iterations to discover each valid formula. This can be observed both from the distribution presented by the histogram and the average number of consumed search iterations. This result demonstrates that the efficiency of the formula search is improved by the search mechanism organized on the formula graph."}, {"title": "6.3.1 Effectiveness of Formula Graph", "content": "In addition to the primary experiment, we also conducted ablation studies to verify the effectiveness of several crucial proposals and mechanisms employed in our learning framework. The first factor we investigated here is the effectiveness of the graph-based formula exploration technique, which is a fundamental proposal made in our work. To perform an ablation study, we set the weight w for summarizing formula score in Equation 9 from its default scale [0.4, 0.3, 0.2, 0.1] to [1.0, 0.0, 0.0, 0.0]. This prevents the formula sampling in line 4 of Algorithm 2 from observing the neighbors of each formula and makes the graph structure degenerate into an ordinary collection of independent formula nodes. In this experiment, we investigated the number of search iterations consumed in finding valid formulas for each learning sample with regular and degenerate w on three representative problem categories: mul_div_multiple, linear_1d, and conversion. The results are presented with histograms in Figure 1. From this result, it can be noticed that the search procedure with a regular w generally consumed fewer search iterations to discover each valid formula. This can be observed both from the distribution presented by the histogram and the average number of consumed search iterations. This result demonstrates that the efficiency of the formula search is improved by the search mechanism organized on the formula graph."}, {"title": "6.3.2 Effectiveness of Reflection Mechanism", "content": "Furthermore, we investigated the effect of the reflection mechanism we proposed in Section 4.4. In this experiment, we simply activated and deactivated the Reflect procedure adopted in line 8 of Algorithm 1. This experiment is conducted on several relatively challenging problem categories with diverse question templates, and the result is presented in Table 4. Based on these results, it can be concluded that the reflection mechanism generally enhances the generalization capacity of inference thanks to the mitigation of the prolixity issue and the improvement in the consistency of formulas."}, {"title": "6.3.3 Asynchrony and Parallelization", "content": "To evaluate the improvement in the efficiency of formula searches resulting from the asynchrony and parallelization practices we introduced in Section 5, we measured the rate of formula examination achieved under difference hardware expenses. Here, the experiment is conducted on a computing server operating with Ubuntu 18.04 and Python 3.10.5. The model of the CPU is 32-Core AMD EPYC 7452, and the model of the GPU is 40GB NVIDIA A100. The results are reported in Table 5. In these results, the formula examining rate is averaged across three problem categories: mul_div_multiple, linear_1d, and conversion. \u201cLegacy\" refers to the classic implementation without any asynchrony and parallelization practice.\nFrom these results, it can first be concluded that the asynchrony and parallelization practices we introduced in Section 5 largely accelerate the formula examination. Moreover, we can observe that the formula examining rate is majorly influenced by the number of GPUs, which virtually affects the progress rate of search iterations as each search iteration must wait for the computation on GPUs to be finished to obtain the scoring results for the new formulas. When the GPUs have reached their maximum workload, further increasing the number of search processes contributes marginally to the overall exploration efficiency. Accordingly, we suggest prioritizing fulfilling the computing power requirements on GPUs to maximize the efficiency of formula exploration."}, {"title": "6.3.4 Comparison with Previous Work", "content": "Finally, we performed a direct comparison between our proposals and the existing formula exploration approach [10] with a quantitative metric to assess the test accuracy as well as a qualitative metric to illustrate the formula search progress during the learning stage. The results are shown in Table 6 and Figure 2, respectively. Both these results demonstrate that our refined learning methods achieve higher formula exploration efficiency and better test performance compared to the classic learning approach."}, {"title": "7 Further Discussion", "content": "In light of the rapid advancements in the techniques and applications of LLMs nowadays, we consistently remain concerned with the correlation between our proposals and LLMs. Generally, we consider that the major contribution of our proposals does not conflict with the application of LLMs, and the reasons are as follows. Firstly, as we mentioned at the beginning, even though LLMs exhibit a rich practicality in generating symbolic instructions for neuro-symbolic reasoning, the reliability of these instructions is not always assured. Accordingly, in cases that the prediction of LLMs involves false symbolic instructions, our proposals can act in eliminating these noises. Secondly, our methods can also work in conjunction with LLMs through employing LLMs as our PolicyNet to suggest reasonable prior formula inferences for the initialization of our formula graph, or utilizing LLMs to estimate the likelihood of formulas for the formula scoring. The search process can also benefit from the extensive prior knowledge embedded in pretrained LLMs.\nAs for the potential limitation of our work, the major concern we retain still lies in the feasibility of weak-supervised learning in extremely large search spaces. This limitation arises due to two primary facts: the size of the search space increases exponentially with the growth of the count of necessary calculations required for solving associated problems, and the learning approaches we studied in this work majorly focus on the problem setting that no prior knowledge is provided, which requires the learning process to start completely from scratch. In practice, we also observe that our learning practice rarely succeeds in discovering valid formulas exceeding ten operations in their length. With respect to this limitation, we consider endowing the learning procedure with appropriate prior knowledge to be a feasible solution, which also serves as a significant direction for future work."}, {"title": "8 Conclusion", "content": "This paper discussed the application and limitation of neuro-symbolic approaches in mathematical reasoning tasks. In these approaches, intermediate symbolic labels, named formulas, are indispensable for the training process but can be hard to acquire. Moreover, existing weakly-supervised learning techniques proposed by previous work can be inadequate to solve more complex and challenging mathematical problems, and their compatibility with computations in more arbitrary and non-tree-structured forms is also questionable. To address these problems, in this work, we developed an advanced learning framework on the basis of prior work to facilitate the weakly-supervised learning of"}]}