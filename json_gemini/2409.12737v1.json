{"title": "MEXMA: Token-level objectives improve sentence\nrepresentations", "authors": ["Jo\u00e3o Maria Janeiro", "Benjamin Piwowarski", "Patrick Gallinari", "Lo\u00efc Barrault"], "abstract": "Current pre-trained cross-lingual sentence encoders approaches use sentence-level objectives only. This\ncan lead to loss of information, especially for tokens, which then degrades the sentence representation.\nWe propose MEXMA, a novel approach that integrates both sentence-level and token-level objectives.\nThe sentence representation in one language is used to predict masked tokens in another language,\nwith both the sentence representation and all tokens directly updating the encoder. We show that\nadding token-level objectives greatly improves the sentence representation quality across several tasks.\nOur approach outperforms current pre-trained cross-lingual sentence encoders on bi-text mining as\nwell as several downstream tasks. We also analyse the information encoded in our tokens, and how\nthe sentence representation is built from them.", "sections": [{"title": "1 Introduction", "content": "Creating general-purpose multilingual embeddings has attracted significant attention from the research\ncommunity in recent years, driven by the growing need for efficient and effective cross-lingual representations.\nCross-Lingual Sentence Encoders (CLSE) create fixed-size sentence representations that are able to capture\nthe relevant information in a sentence, and are aligned across languages. By capturing relevant sentence\ninformation in a shared multilingual space, these aligned representations enable efficient comparison and\nretrieval based on distance measures, thereby facilitating their effective utilization in various downstream\napplications.\nCurrent CLSE (Duquenne et al., 2023; Feng et al., 2022) typically build upon pre-trained encoders, often\nlanguage models (Conneau et al., 2020; Devlin et al., 2019) or translation models (NLLB Team et al., 2022).\nThese pre-trained encoders have been trained using objectives that focus on individual words or tokens, i.e.\ntoken-level objectives. Examples of such objectives include unmasking, where the model is required to predict\neach token individually, and all predictions are used to update the encoder directly. However, Muennighoff\net al. (2023); Hu et al. (2020) show that pre-trained encoders without objectives that consider entire sentences,\ni.e. sentence-level objectives, do not create good sentence representations. This means that CLSE need to\ntrain sentence-level representations, in order to effectively capture the relevant information of the sentences.\nAlthough CLSE start from encoders pre-trained with token-level objectives, they are commonly trained with\nsentence-level objectives that only update the encoder through the sentence representation (Duquenne et al.,\n2023; Feng et al., 2022; Yang et al., 2019; Artetxe and Schwenk, 2019a), without any objective for each token\nindividually. We hypothesize that token-level objectives should be kept during the training of CLSE, coupled\nwith the sentence-level objectives, to better update the encoder and improve sentence representation quality\nand alignment. The intuition is that only using sentence-level objectives leads to a degradation of token level\ninformation, especially lexical information, which in turn can impact the sentence representation.\nRecently, there have been approaches exploring the use of both token-level and sentence-level objectives for\nbetter sentence representations. DAP (Li et al., 2023) uses both objectives, but the token-level objective\nis only used to update the token representations in the encoder, without influencing directly the sentence\nrepresentation. On the other hand, RetroMAE (Xiao et al., 2022) also employs both objectives, but uses\ntwo different token objectives to update the individual tokens and the sentence, with the latter having to be\ncreated from a masked input."}, {"title": "2 Related Work", "content": "Sentence embeddings have been well studied in the last decade. Initially, recurrent networks were trained\nto predict previous and next sentence (Kiros et al., 2015) or sentence entailment (Conneau et al., 2017).\nUniversal Sentence Encoder (Cer et al., 2018) trains a transformer network on both tasks. Reimers and\nGurevych (2019) propose to continue the training of a BERT model to include a sentence-level objective.\nThese initial works have been extended to multilingual settings, to capture the relevant information in the\nsentences, and to have aligned representations across languages. These new approaches are called cross-lingual\nsentence encoder. We describe those works next.\nUPDATE VIA SENTENCE REPRESENTATION Most current cross-lingual sentence encoder approaches only\nupdate their encoder via the sentence representation objective, without having any token-level objective in\nthe output of the encoder that would update each token individually (Guo et al., 2018; Yang et al., 2019;\nFeng et al., 2022; Artetxe and Schwenk, 2019a; Duquenne et al., 2023; Heffernan et al., 2022). They are\nmost commonly based on contrastive learning (Hadsell et al., 2006) methods, that aim to reduce the distance\nbetween positive pairs (translations) and increase the distance between negative pairs (non-translations) (Guo\net al., 2018; Yang et al., 2019; Feng et al., 2022). Notably, LaBSE (Feng et al., 2022) uses the contrastive loss,\nwith the additive margin softmax approach of Yang et al. (2019). A common non-contrastive solution is to\nuse translation (Artetxe and Schwenk, 2019a; Duquenne et al., 2023) with a fixed-size sentence representation\nafter the encoder (bottleneck), assuming that a model can translate a sentence into many languages only if a\ngood sentence-level conceptual representation is learned. The bottleneck however prevents gradients from the\ndecoder to directly update the individual token representations of the encoder, which we hypothesize leads\nto a degradation of token level information and consequently of the sentence representation. Our method\nalso uses a sentence representation as context for the unmasking, but allows direct token-level gradients to\npropagate to the encoder token representations.\nUPDATE VIA SENTENCE AND TOKEN REPRESENTATIONS Recent approaches (Li et al., 2023; Xiao et al.,\n2022) have shown that combining token and sentence level objectives can improve sentence representations.\nRetroMAE (Xiao et al., 2022), is an Information Retrieval (IR) method that utilizes fixed-size sentence"}, {"title": "3 Methodology", "content": "We propose MEXMA, a novel multilingual alignment technique based on both token-level and sentence-level\nobjectives. The goal is to create a sentence representation that is able to encode the syntactic, semantic\nand lexical information in a sentence, with representations well aligned across languages. To achieve this\ngoal, inspired by monolingual masked auto-encoding techniques (Xiao et al., 2022), we use the sentence\nrepresentation in one language to unmask the tokens in another language, updating both the sentence and\nindividual tokens, while forcing the sentence representation to encode the relevant parts of the sentence.\nUsing masking also allows us to use a non-contrastive loss to align sentence representations, since it prevents\nthe collapse. Both sentence and token-level objectives are used to improve the quality of the sentence\nrepresentation. Our architecture is depicted in Figure 1, and is composed of several components, that we\ndescribe now. For the explanation, we refer to inputs (and the output of their encoders) that have no masking\nas clean, and masked for their masked counterparts. Additionally, we consider two languages, language A and\nlanguage B, which are associated with the sentence representations SA and SB (from the clean encoders)."}, {"title": "THE CROSS-UNMASKING", "content": "To ensure that our sentence vector captures the meaningful information of the\nsentence, we mask a significant portion of the input tokens in language A. This makes it challenging for\nthe encoder and the MLM head to recover the missing tokens without any additional context. To overcome\nthis challenge, we provide the unmasking head with the sentence vector S\u00df, derived from the clean sentence\nin language B. This forces the model to leverage the information in SB to predict the masked tokens in\nlanguage A. By doing so, we encourage the sentence vector to capture the essential information of the sentence.\nFurthermore, by alternating languages, we enforce the sentence vector to encode information that is useful\nacross languages. We formulate this component into a symmetrical cross-entropy loss (CE), applied over the\noutputs of the encoders:\n\\mathcal{L}_{mlm} = CE([S_B, \\hat{A}], A) + CE([S_A, \\hat{B}], B),\nwhere A and B are the outputs of the masked encoders without the CLS embedding, A and B the token\ntargets, and [X, Y] represents the concatenation of X and Y."}, {"title": "THE ALIGNMENT LOSS", "content": "The cross-unmasking generates an implicit alignment due to the switching of languages\nto perform the unmasking. However, as is, that implicit alignment does not strongly enforce the same sentence\nrepresentations in two different languages to be equal in the embedding space. Following SONAR Duquenne\net al. (2023), to further reinforce the spatial proximity of semantically equivalent sentences across languages, we\nuse an additional non-contrastive alignment objective. The two losses, unmasking and alignment, complement\neach other to provide both aligned and meaningful vector representations of sentences in multiple languages.\nWe formulate this component as a Mean Squared Error (MSE) loss between sentence representations:\n\\mathcal{L}_{alignment} = MSE(S_A, S_B),"}, {"title": "THE SYMMETRICAL ARCHITECTURE", "content": "To align all languages and maximize data usage, we adopt a symmetrical\napproach that unmasks the tokens of language A with SB, and vice versa, simultaneously. We thus create four\ninstances of the encoder (with shared parameters). For each language, we have two versions of each sentence:\none heavily masked and one clean. This allows us to generate two clean sentence vectors, SA and SB, which is\nessential for aligning representations between languages. A non-symmetrical approach with only two encoders\n(one per language) would not produce the desired alignment as it would force the model to align a heavily\nmasked sentence vector with a clean one, which is not ideal."}, {"title": "THE KOLEO LOSS", "content": "In preliminary experiments, we noticed that our representations exhibited more anisotropy\nthan those learned with contrastive approaches. This has been shown to impact the quality of the representa-\ntions (Godey et al., 2024). Inspired by DINOv2 (Oquab et al., 2024), we employ the KoLeo loss (Sablayrolles\net al., 2019) to encourage sentence representations to spread out evenly in the latent space. The KoLeo loss is\nbased on the Kozachenko-Leonenko differential entropy estimator (see Beirlant et al. (1997)). We define below\nthe KoLeo loss, \\mathcal{L}_{KoLeo}, for a set of n representations, as well as the symmetrical version, \\mathcal{L}_{K}, we use to train\nour models:\n\\mathcal{L}_K = \\mathcal{L}_{KOLeo}(S_A) + \\mathcal{L}_{KOLeo}(S_B) \\text{ with } \\mathcal{L}_{KoLeo} = \\frac{1}{n}\\sum_{i=1}^n log(d_{n,i})\nwhere dn,i = minj\u2260i || Xi xj || is the distance between xi and its nearest point in the batch.\nOur training loss is a weighted combination of all previous losses:\n\\mathcal{L}_{MEXMA} = \u03b1\\cdot \\mathcal{L}_{alignment} + \u03b2\\cdot \\mathcal{E}_{mlm} + \u03b3\\cdot \\mathcal{L}_{K}\nwhere a, \u1e9e and y are hyper-parameters that control the weight of each loss term. To show that \u039c\u0395\u03a7\u039c\u0391\ncan be used on top of existing alignment approaches, we provide in Section 5.2 experimental results when\nreplacing the MSE alignment loss in MEXMA with a contrastive loss."}, {"title": "3.1 Experimental setup", "content": "ENCODER BACKBONE As our encoder, we utilize a modified version of the XLM-ROBERTa model (Conneau\net al., 2020) provided by HuggingFace that uses a more efficient attention (details in Appendix A). Our\nsentence representation from the encoder is obtained via the CLS embedding of the last layer, without any\nfurther processing."}, {"title": "4 Results", "content": "To assess the quality and alignment of our embeddings, we evaluate them on a range of tasks. These tasks\nfall into two categories: mining tasks and other downstream tasks. Mining tasks measure how aligned our\nrepresentations are across languages, while downstream tasks evaluate the generalization power and overall\nquality of our embeddings."}, {"title": "4.1 Multilingual alignment through mining", "content": "We evaluate on three alignment tasks, namely xsim\u00b9, xsim++ (Chen et al., 2023) and BUCC (Zweigenbaum\net al., 2018, 2017). xsim and BUCC are composed of sentences translated in many languages, and the goal is\nto be able to retrieve the correct translation of a query sentence. xsim++ extends this task by introducing\nvariations in the existing sentences in English, creating hard negatives that are difficult to distinguish from\nthe correct sentence. We follow Heffernan et al. (2022) and do not evaluate on Tatoeba because of the small\namount of data available for some language pairs and the low-quality translations created by non-professional\nvolunteers.\nxsim and xsim++ use a margin-based similarity approach (Artetxe and Schwenk, 2019b). We use the same\nsetup as described in Heffernan et al. (2022); Duquenne et al. (2023). For BUCC, the similarity is the cosine\nsimilarity as is commonly done. The xsim and xsim++ scores are the error rate of wrongly aligned sentences\nin the test set. For BUCC, the score is the F1 score of the alignment, computed using the MTEB benchmark\n(Muennighoff et al., 2023).\nBUCC evaluates on 4 languages: German, French, Russian and Chinese. The detailed results (per language)\nare available in Appendix E. We evaluate our model using xsim and xsim++ on the FLORES200 dataset,\ncovering the 81 languages supported by our model (listed in Appendix C). For fairer comparison, we also\nreport results for the 72 languages supported by LaBSE, SONAR, and MEX\u039c\u0391 (\"o-xsim\"), and separately\nfor the 34 languages common to DAP and the other models (\"d-xsim\").\nThe results are shown in Table 1. MEXMA outperforms previous SOTA on all three benchmarks, showcasing\nthe improved alignment achieved in our new approach. The improvements in xsim and BUCC suggest that our"}, {"title": "4.2 Downstream tasks", "content": "To understand the quality of our embeddings and how generic they are, we evaluate them on several tasks\nfrom the MTEB benchmark (Muennighoff et al., 2023). We report the averaged results for each language. For\nthe full list of results for every task, see Appendix E.\nSINGLE SENTENCE CLASSIFICATION We evaluate our model's classification performance on two benchmarks.\nFirst, the SentEval suite (Conneau and Kiela, 2018) is used to assess the performance across various tasks in\nEnglish. We only evaluate on the tasks considered in LaBSE. Second, we evaluate the multilingual classification\ncapabilities using the available datasets from the MTEB benchmark. Table 2 shows the aggregated results.\nWe can see that MEXMA outperforms all baseline models on average, and more specifically gains +2.33%\nwhen compared with SONAR.\nPAIRWISE SENTENCE CLASSIFICATION We further evaluate on the pair classification task. This task consists\nin classifying two sentences, e.g. determining if a pair of sentences are duplicates or not. The metric, as\nreported in MTEB, is the Average Precision (AP) based on the distance between sentence representations.\nThe results are in Table 3. MEXMA consistently outperforms all baselines on average, by at least +1.85%.\nThese results, combined with our single sentence classification results, suggest that our model can effectively\nencode the relevant information in the sentence vectors.\nSEMANTIC TEXTUAL SIMILARITY (STS) The STS task evaluates the model's ability to replicate human\njudgments on sentence similarity. The metric, as reported in MTEB, is the Spearman correlation based on\ndistance. The results are in Table 4. We can see that LaBSE outperforms all other methods, and in particular"}, {"title": "5 Ablations and Analyses", "content": "In this section, we conduct a comprehensive analysis of our MEXMA architecture, examining the impact\nof its individual components, how it scales with varying model and data sizes, and its potential to improve\nother alignment approaches. We also examine the characteristics of the token embeddings and sentence\nrepresentations learned by our model."}, {"title": "5.1 Model components", "content": "In Table 5 we ablate the impact of having direct token-level gradients in MEXMA. In model 1, we have all\nof MEXMA's components, as covered in Section 3, without the KoLeo loss. However, the gradients from the\nunmasking task are only back propagating through the sentence representations back to the encoder, and\nare deactivated for the individual tokens the encoder outputs, i.e. in the \\mathcal{L}_{mlm} mentioned in Section 3, A/B\nhave no gradients flowing back to the encoder. This model already achieves results that are competitive with\ncurrent state of the art, but does not outperform them. However, if we allow the gradients to flow through\nthe tokens directly, model 2, we are able to outperform the current state-of-the-art. As we hypothesized,\nadding updates on the tokens directly, coupled with the sentence updates largely improves results across all\ntasks. Additionally, we also show that adding the KoLeo loss, model 3, also slightly improves results across\nall tasks. The ablation on all components of the model is provided in Appendix B."}, {"title": "5.2 Contrastive alignment loss", "content": "To further assess the improvements given by the direct token updates in MEXMA, and understand MEXMA's\nscalability to other alignment approaches, we replaced our alignment loss, MSE, with a contrastive loss (also\ndropping the KoLeo loss). We used a siamese network with XLM-ROBERTa-large trained on the symmetric\ncross-entropy loss (InfoNCE from van den Oord et al. (2019)) as the baseline model, having an architecture\nsimilar to LaBSE (Feng et al., 2022). Our training used a batch size of 1.2k, with the rest of the parameters\nthe same as reported in Section 3.1. The results are presented in Table 6. Our baseline model performed well\non xsim and SentEval but struggled with xsim++. Switching to the MEXMA architecture without token-level\ngradients, as done in model \u2460 in Section 5.1, improved performance, already close to state-of-the-art xsim++\nperformance. Moreover, incorporating token-level gradients, allowing the full MEXMA architecture with\ncontrastive loss, as done in model 2 in Section 5.1, resulted in competitive performance, already outperforming\nprevious approaches in SentEval and xsim++. This demonstrates the positive impact of direct token-level\ngradients and shows that MEXMA can be easily integrated with existing alignment approaches, such as\ncontrastive learning, to improve their results."}, {"title": "5.3 Model and data sizes", "content": "Table 7 shows how our model's results scale with the model size. We train two models, MEXMA-base\nwith 277M parameters, based on XLM-ROBERTa-base, and MEXMA with 559M parameters, based on\nXLM-ROBERTa-large. It is possible to see that even the smaller model (277M parameters) outperforms\nLaBSE (471M parameters), on both xSIM and xSIM++, and gets a close result in SentEval, with a 0.3%\ndecrease in performance, with 58.81% of the size. This smaller model also gets surprisingly close to the results\nof SONAR, which has 766M parameters, i.e. \u22482.77 times its size. These results show that our approach\nworks on smaller and larger models, and it seems to enable quite powerful small models, due to our stronger\ntraining signal. Our larger model, MEXMA, with \u224873% the size of SONAR, is able to largely outperform it\nacross all tasks.\nTo investigate the impact of training data, we conducted experiments using two different language subsets of\nthe FLORES200. We trained separate MEXMA models on each subset, using the same hyperparameters as\nreported in Section 3.1. For comparison, we evaluated the publicly available SONAR model, which was trained\non all available 200 languages, on both language subsets. The results, presented in Table 8, demonstrate that\nMEXMA outperforms SONAR on both subsets, highlighting the adaptability and robustness of our approach\nto varying training data."}, {"title": "5.4 Masking ratio", "content": "NLP models typically use masking percentages around 15%, whereas vision papers have explored much higher\nmasking ratios, ranging from 40% in BEiT (Bao et al., 2022) to as high as 90% in MAE (He et al., 2022)\nand V-JEPA (Bardes et al., 2024), usually aligning augmentations. For text, there is less redundancy and\nthe representations are more information-dense. In our case, we are aligning the same sentence in several\nlanguages, which can be viewed as augmentations of a pivot sentence, i.e. the sentence in English. We need\nto know how much we can mask, to make the unmasking task hard, but to not deteriorate the performance of\nour encoder. Table 12 shows the results we obtained for the different masking ratios. The range 30%-60%\nseems to be the best operating region. We selected 40% for all experiments conducted in this paper, since it\nhad the best balance between alignment and classification. More information is provided in Appendix B."}, {"title": "5.5 Token embeddings analysis", "content": "Sentence vectors are pooled representations of their tokens. In this section, we investigate the information\nencoded in the tokens from the last layer across different models. Our goal is to determine whether the\ntokens primarily convey semantic, lexical, and/or contextual information. Although these categories can be\nintertwined, understanding the dominant characteristics of each model's tokens provides valuable insights into\ntheir behavior."}, {"title": "6 Conclusion", "content": "We introduced MEXMA, a novel multilingual alignment technique that leverages both token-level and\nsentence-level objectives. We show that integrating token-level objectives into the training of cross-lingual\nsentence encoders greatly improves their sentence representation quality, achieving new state-of-the-art results\nin bitext mining and other downstream tasks. We additionally validate these improvements via ablations.\nNotably, MEXMA also achieves strong token alignment across languages and effectively encodes meaningful\ninformation within each token. Since the sentence representation is built from these tokens, as we analysed,\nthis leads to better sentence representations. Looking ahead, we plan to explore MEXMA's scalability to\nmore languages, and potentially modalities."}, {"title": "A Experimental Setup", "content": ""}, {"title": "A.1 Encoder backbone", "content": "The available implementation of XLM-ROBERTa in HuggingFace employs an inefficient attention mechanism,\nwhich we have modified to incorporate the memory-efficient attention from xFormers (Lefaudeux et al., 2022).\nThis modification was necessary due to the random batching process used in our training, which results in a\nsignificant amount of padding and increased computational cost. To address this issue and eliminate padding,\nwe have employed the BlockDiagonalMask 2, which through custom CUDA kernels, avoids computations in\npadding altogether. With this change we are able to increase our batch size in each GPU by a factor of \u2248 8."}, {"title": "A.2 Unmasking head", "content": "For the unmasking head, we use 6 transformer layers, also leveraging the memory-efficient attention."}, {"title": "A.3 Compute and training length", "content": "Our models were trained on a single node of 8 A100 GPUs. Each GPU had a batch size of 150, totalling\n1,200 batch size across all GPUs. We accumulated two gradients, making our effective batch size 2,400. We\ntrained our models for 300k steps."}, {"title": "A.4 Losses", "content": "Our models were trained with a = 1, \u03b2 = \u00bd and y = \\frac{0.01}{2}."}, {"title": "A.5 Training parameters", "content": "We utilize the AdamW optimizer for our training process. The learning rate is linearly increased from 1e-5 for\nthe 300k steps. To optimize memory usage, we employ mixed precision training, where the model is stored in\nfloat32, while most computations are performed in float16. The maximum sequence length for our input data\nis set to 200 tokens. Finally, we apply a masking ratio of 40% to the input data."}, {"title": "B Ablations", "content": ""}, {"title": "B.1 Model components", "content": "In Table 11, we ablate the different components of our architecture described in Section 3. We briefly explain\neach entry in the table. Model \u2460 has only two encoder instances, one for each language, where one of the\ninputs is masked, and the other is left clean. The token unmasking is performed with the clean sentence\nrepresentation as context. The languages are randomly swapped at every new sample, to eliminate potential\nbiases. The gradients from the unmasking task are only propagated back to the encoder via the sentence\nrepresentation, and there is no gradient propagation from the individual tokens back to the encoder. There\nis also neither alignment nor koleo losses. Model \u2461 adds two additional encoder instances, totalling four\ninstances, two for each language, where now each language has its clean and masked input. This allows to\nunmask language A with language B, and vice-versa, and will also allow (once added) to align two clean\nsentence representations. Model 3 adds the alignment loss, but it is performed between the masked sentence\nrepresentation of language A and the clean sentence representation of language B, to better emphasize the\nadvantages of having a symmetrical architecture with an alignment loss between two clean representations.\nModel\u2463 then changes the alignment loss to be performed between the two clean sentence representations of\neach language. In model \u2464 we allow the gradients from the unmasking to be propagated to the encoder via\neach individual token, as well as its sentence representation. Finally, model \u2465 adds the KoLeo loss.\nThe results indicate that each component always enhances performance on at least two out of the three tasks.\nNotably, the alignment loss, \u2462-\u2463, and token-level gradients, \u2464, emerge as the most critical components.\nMore precisely, the alignment loss yields substantial improvements on two tasks while also resulting in a\nnotable decline in performance on another task. In contrast, the token-level gradients consistently provide\nsignificant performance gains across all three tasks."}, {"title": "B.2 Masking ratio", "content": "Classical NLP masked encoders like BERT use a small masking percentage, usually \u2248 15%, without aligning\nany augmentations. Recent vision approaches use much higher masking percentages. BEiT (Bao et al., 2022)\nwas one of the first masked image modelling (MIM) approaches, in a BERT-style training, and masked 40%.\nMAE (He et al., 2022) is another BERT-like model for images, and masks 75%, but shows that even masking"}, {"title": "C Language information appendix", "content": "In this section, we cover the languages used by our model. The list of languages used to train our model is\nreported in Table 13. The list used to conduct the experiments with 90 languages is available in Table 14."}, {"title": "D Datasets", "content": "In this section we report the data used to train our models. Table 15 reports all the datasets used to train the\nmodels."}, {"title": "E MTEB datasets", "content": "In this section, we report the scores for each task of the MTEB benchmark reported in Section 4. We report\nthe scores per task, with every dataset used per task, and per language. MEXMA is able to outperform the\nprevious SOTA results on mining, while also improving the downstream results on classification and pair\nclassification. LaBSE outperforms all other models on STS."}, {"title": "E.1 Bitext Mining", "content": "Results for mining are in Table 16, for the BUCC dataset. We report the scores on the four available languages:\nGerman, French, Russian and Chinese."}, {"title": "E.2 Classification", "content": "Classification results for English are available in Table 17, for SentEval, and in Table 18 for the English MTEB\nclassification datasets. Classification results for Chinese, French, Danish, Norwegian and Polish are reported\nin Table 19, Table 20, Table 21, Table 22, Table 23, respectively. MEXMA outperforms all other models on\naverage."}, {"title": "E.3 Pair Classification", "content": "Pair classification results for English, French and Chinese are reported in Table 24, Table 25, and Table 26,\nrespectively. MEXMA outperforms all other models on average."}, {"title": "E.4 Semantic Textual Similarity (STS)", "content": "Semantic Textual Similarity (STS) results are reported in Table 27, Table 29, Table 30 and Table 28 for\nEnglish, French, Polish and Chinese, respectively. LaBSE outperforms MEXMA and the remaining models on\nSTS. MEXMA and LaBSE outperform SONAR by large margins."}, {"title": "F Token level analysis", "content": "In this section, we illustrate the behaviour of each model by visualizing the closest tokens in the space. We\nobserve that MEXMA matches tokens in translations but also different contexts when tokens are used with\nthe same meaning. This is further broken down in Table 9, which distinguishes between two types of matches\nMEXMA does: (1) \"same language\" matches, where the model identifies the same token used in a different\ncontext (monolingual), and (2) \"other\" matches, where it recognizes translated tokens in a sentence in another\nlanguage that is not a translation (multilingual). We observe that SONAR primarily matches tokens across\ntranslations, but does not tend to match the same token when it appears in different sentences within the same\nlanguage. Examples of MEXMA and SONAR comparisons of matching the same token in other sentences is\nin Figure 5, and both models matching translations in Figure 6. In both figures, we show the three closest\ntokens to the selected token, denoted as query on the green box, with the blue text. The closest tokens are in\nthe purple boxes with the pink text. Additionally, we show examples of how LaBSE and MEXMA without\ndirect token-level gradients (no-tok MEXMA), match adjacent tokens in the same sentence regularly. These\nare shown for LaBSE in Figure 2, and for no-tok MEXMA in Figure 3. Lastly, we show how XLM-ROBERTa\nmostly matches the same tokens in other sentences in the same language, presented in Figure 4. For these last\nthree models, we show the top-2 closest tokens, with the same color scheme as mentioned above. Each image\nhas two examples for the given model."}, {"title": "G Attention distribution over tokens", "content": "In this section, we provide some examples of MEXMA and LaBSE's attention probabilities given by the CLS\ntoken to the word tokens. The examples are provided in Figures 7, 8, 9 and 10. Across all figures, it is possible\nto see that LaBSE tends to be more uniform across all tokens, while MEXMA tends to focus more attention\non a smaller subset of the tokens. All examples are taken from the FLORES200 test set with the xsim++\nextension, where some words in the original sentences are replaced, and the models have to be able to still\nmatch the correct translation, and not a sentence with a small change. From Figure 7 to Figure 8 \"nineteen\"\nis replaced with \"twenty nine\". From Figure 9 to Figure 10 the word \"white\" is replaced with \"black\".\nFigure 7 shows the attention placed by MEXMA and LaBSE on the same sentence in English and Portuguese.\nIt is possible to see that MEXMA in Portuguese places most of the attention in two tokens, \"governador\" and\n\"19\", where the token in \"19\" is very relevant here since it is the one needed to distinguish the examples in\nxsim++. LaBSE seems to have many tokens with a lot of attention, and does not have \"19\" as one of the\ntokens with the most attention.\nIn Figure 8, we have the English example with nineteen (as previously shown in Figure 7) compared to the\nsame sentence with nineteen replaced by twenty-nine. Interestingly, LaBSE places more attention on the\n\"##teen\" token than the \"nine\" token, but similar attention to the \"twenty\", \"-\" and \"nine\" tokens. \u039c\u0395\u03a7\u039c\u0391\nplaces similar attention in all nineteen tokens, and in twenty nine it places a small amount of attention on the\nirrelevant \"-\", with a higher degree of attention in \"nine\" and a smaller amount of attention in \"twenty\".\nMEXMA also seems to do a good job ignoring irrelevant tokens like \"of\", while La"}]}