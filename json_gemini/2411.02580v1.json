{"title": "Social Support Detection from Social Media Texts", "authors": ["Zahra Ahani", "Moein Shahiki Tasha", "Fazlourrahman Balouchzahi", "Luis Ramos", "Grigori Sidorov", "Alexander Gelbukh"], "abstract": "Social support, conveyed through a multitude of interactions and platforms such as social media, plays a pivotal role in fostering a sense of belonging, aiding resilience in the face of challenges, and enhancing overall well-being. This paper introduces Social Support Detection (SSD) as a Natural language processing (NLP) task aimed at identifying supportive interactions within online communities. The study presents the task of Social Support Detection (SSD) in three subtasks: two binary classification tasks and one multiclass task, with labels detailed in the dataset section. We conducted experiments on a dataset comprising 10,000 YouTube comments. Traditional machine learning models were employed, utilizing various feature combinations that encompass linguistic, psycholinguistic, emotional, and sentiment information. Additionally, we experimented with neural network-based models using various word embeddings to enhance the performance of our models across these subtasks.The results reveal a prevalence of group-oriented support in online dialogues, reflecting broader societal patterns. The findings demonstrate the effectiveness of integrating psycholinguistic, emotional, and sentiment features with n-grams in detecting social support and distinguishing whether it is directed toward an individual or a group. The best results for different subtasks across all experiments range from 0.72 to 0.82.", "sections": [{"title": "1. Introduction", "content": "Social support is the provision of behaviors, communication, and interactions that convey care and value to individuals, fostering a sense of belonging and aiding in coping with life's challenges (Ko et al., 2013). Social support manifests in diverse ways, ranging from expressions of care and encouragement to practical assistance or guidance. Recognizing the presence of supportive individuals who offer various forms of aid can serve as a buffer against stress and safeguard both emotional and physical health. The support patients receive from shared content plays a crucial role in enhancing their self-care practices and overall health results (Jadad et al., 2000). Specifically, individuals coping with chronic illnesses, disabilities, or cancer often find social media platforms invaluable, as they offer opportunities to connect with peers or professionals for guidance in managing their long-term conditions effectively (Merolli et al., 2013).\nIn recent years, a heightened awareness of the detrimental impacts of hate speech, abusive language, and misogyny on social media platforms has led to a surge in research efforts focused on their detection through Natural Language Processing (NLP) (Shashirekha, 2020). While social media platforms offer users the freedom and anonymity to express their opinions and engage in instant feedback, this liberty also fosters an environment where individuals may exploit the platform to propagate discriminatory or harmful views targeting specific demographics (Chakravarthi, 2020). Consequently, developing tools and techniques for detecting and mitigating such content has become imperative in creating safer digital environments and promoting respectful online discourse.\nHowever, some argue that this approach can infringe on users' freedom of expression (Chakravarthi, 2020; Balouchzahi et al., 2023). Instead of solely focusing on identifying and removing negative content, an alternative strategy could involve promoting positive interactions and supporting content that contributes to social good. By encouraging and amplifying constructive and respectful communication, social media platforms can foster a more positive online environment while still respecting users' rights to freely express their opinions. This dual approach not only mitigates the spread of harmful content but also actively contributes to a more supportive and inclusive digital community.\nDespite the importance of promoting positive and supportive content, not many tasks have been done in this research area. In response to these challenges, our proposed approach offers an alternative but under-explored strategy to combat the negative atmosphere on social media platforms by promoting social support comments. Rather than solely focusing on identifying and filtering out negative content, our approach seeks to cultivate a more positive and supportive online environment by encouraging users to provide emotional comfort, encouragement, and advice to those facing challenges.\nOnline social support encompasses the assistance and emotional comfort offered via digital platforms such as social media, forums, and messaging apps. This type of support is crucial for individuals and groups facing various challenges, such as victims of wars, black community, or minorities. Through these digital channels, individuals can connect with others who"}, {"title": "2. Definitions", "content": "Albrecht and Goldsmith (2003) define social support as verbal and nonverbal communication between recipients and providers that reduces uncertainty about the situation, the self, the other, or the relationship and functions to enhance the perception of personal control in one's experience.\nAlthough social support is helpful during stressful situations, Barnes and Duck (1994) pointed out that the exchange of support does not only manifest during the crisis but is also an everyday occurrence in personal relationships.\nCutrona and Suhr (1992) delineate a social support framework comprising five primary categories: informational, emotional, esteem, social network, and tangible support. Informational support entails messages conveying knowledge or advice, emotional support involves expressions of care and empathy, esteem support boosts self-worth and abilities, social network support fosters a sense of belonging within a group, and tangible support entails physically providing goods or services as needed.\nThe current research aligns with (Barnes and Duck, 1994) definition of social support and views the exchange of comments and feedback between users and audiences as a form of social support occurring within their communication. Social support refers to \u201cinformation leading the subject to believe that he is cared for and loved, esteemed, and a member of a network of mutual obligations\u201d (Cobb, 1976). It is formed by the exchange of resources (i.e., verbal and nonverbal messages) between two or more individuals Shumaker and Brownell (1984).\nStudies have demonstrated that social support offers advantages to patients, such as dealing with challenging life circumstances (Thoits, 1982), enhancing compliance with recommended treatment plans McCorkle et al. (2008), and fostering better mental health Cohen and Wills (1985). In the management of chronic illnesses, social support plays a critical role in encouraging healthy behaviors and attaining favorable health results for patients. For instance, McCorkle et al. (2008). discovered that social support enhances individuals' quality of life and diminishes psychological distress among those facing severe mental health issues.\nHence, social support is defined as \u201cthe emotional, informational, or practical assistance offered by others, including peers or community members. This aid can be extended to individuals or groups, such as women, religious communities, or racial minorities like black community, aiding them in navigating challenges, enhancing their overall well-being, and fostering resilience.\""}, {"title": "3. Related work", "content": "Despite the critical importance of promoting positive and supportive content, research in this area remains relatively sparse. This concept serves as a counterpoint to hate speech. While there is no directly comparable work specifically focused on positive content within NLP, related research has been conducted on tasks such as hope speechArif et al. (2024), which offers some insights into this area. Therefore, in this section, we summarize some of the hate and hope speech research.\nThe phenomenon of hate speech and violent communication online is commonly referred to as cyberhate (Mir\u00f3-Llinares and Rodriguez-Sala, 2016). It involves the use of electronic communication technologies to propagate discriminatory or extremist messages, targeting not only individuals but also entire communities Blaya (2019). Hate speech encompasses various linguistic styles and actions, including insults, provocation, and aggression (Anis, 2017). It can be categorized into different types, such as gendered hate speech, which targets specific genders or promotes misogyny, religious hate speech,"}, {"title": "4. Dataset development", "content": "This research focuses on analyzing data collected from YouTube comments across 17 videos spanning various categories such as nation, black community, women, religion, LGBTQ, and others. The video selection was based on topics potentially related to supportive content. For instance, videos concerning the war between Israel and Palestine, events involving the Black Community, Christiano Ronaldo, LGBTQ+ issues, and Women were chosen. Initially, we amassed 66,272 comments. After filtering out duplicate and non-English comments, the dataset was refined to 42,695 comments.\nSubsequently, we proceeded to select 5,000 comments containing specific keywords, and another 5,000 comments were chosen randomly The following keywords were selected for analysis: \"support\", \"stay strong\", \"I'm here to help\", \"I believe in you\", and \"inspiring\", and their synonyms. These keywords capture the essence of social support and convey various aspects of emotional assistance.\nIt is worth noting that comments associated with the selected videos underwent no additional filtering or selection process. This approach enabled us to accurately gauge the distribution of supportive comments for each video while also exploring related aspects."}, {"title": "4.2. Annotator selection", "content": "For annotator selection, two annotators, one male and one female, both holding master's degrees in computer science and possessing proficient English language skills, were hired. Initially, each annotator was provided with 100 sample tweets and detailed annotation guidelines. Subsequently, the labeled samples from two annotators were analyzed. Individual meetings and interviews were then conducted with these annotators to address any confusion and ensure a thorough understanding of the annotation task.\nAs a third annotator, one of the authors of the paper, who had comprehensive knowledge of the concept, was selected. This annotator, a Ph.D. student in natural language processing with advanced English proficiency, contributed to the annotation process.\nFinally, the 10,000 data points were divided into five parts, with each annotator completing one part. After each part, comments were randomly selected to verify the accuracy of their annotations."}, {"title": "4.3. Annotation guidelines", "content": "The SSD task was structured as a three-step classification process. First, supportive comments were identified. Next, it was determined whether these supportive comments were directed toward an individual, a group, or a community. Finally, if the supportive comment was identified as being directed toward a group, the specific group was further identified. The guidelines for this process are described below.\n\u2022 Subtask 1 - Binary social support detection: In this subtask, a given text is classified as either supportive or non-supportive:\n- Social Support (label = SS): Supportive statements promote understanding, empathy, and positive action. Therefore, a supportive comment is a statement or message that offers support, encouragement, admiration, or assistance to individuals or groups that are encountering difficulties or have accomplished something noteworthy. These comments aim to provide emotional support, boost morale, or acknowledge the achievements of others.\n- Not Social Support (label = NSS): The text does not convey any form of support, admiration, or encouragement.\n\u2022 Subtask 2 - Individual vs. Group: In this subtask, each pre-identified in Subtask 1 supportive comment is further classified as support for an individual or support for a group:\n- Individual: If the text expresses support for a specific person or individual (e.g., Cristiano Ronaldo, Trump), it is labeled as Support for Individual.\n- Group: If the text expresses support for a group of people, community, tribe, nation, etc. (e.g., Muslims, Real Madrid, Black nations, LGBTQ), it is labeled as Support for Group.\n\u2022 Subtask 3 - Multiclass SS for Groups: In this subtask, we aim to identify which community or group of people are targeted for social support by classifying the group supportive comments identified in Subtask 2 into the following categories:\n- Women: The text expresses support for women and promotes women's rights and feminism.\n- Black community: The text expresses support for the black community and promotes black community rights.\n- LGBTQ: The text expresses support for the LGBTQ community and promotes LGBTQ community rights.\n- Religion: The text expresses support for a religion and its rights.\n- Other: The text expresses support for a community other than those listed above."}, {"title": "4.5. Inter-annotator agreement", "content": "Inter-annotator agreement (IAA) assesses how much annotators agree, factoring in chance agreement. Cohen's Kappa Coefficient scores of 85% for subtask 1, 82% for subtask 2, and 65% for subtask 3 demonstrate the robustness of the datasets, reflecting the rigorous annotation process."}, {"title": "5. Feature Analysis", "content": "This section delves into the complex interplay of psycholinguistic, emotionalTash et al. (2024b), and sentiment features utilized in our research. It not only outlines these features but also explores how they relate to various forms of social support. By understanding these dynamics, we gain a nuanced understanding of how language and emotions intersect with mechanisms of social support. Psycholinguistic attributes involve linguistic cues intertwined with psychological processes, extracted using LIWC software from social supportive comments. Emotional features encompass the expression of emotions Tash et al. (2024d), intensity, and valence within communication,"}, {"title": "5.1. LIWC", "content": "The LIWC model has transformed psychological research by making language data analysis more robust, accessible, and scientifically rigorous. LIWC-22 evaluates over 100 textual dimensions, all validated by esteemed research institutions worldwide. With over 20,000 scientific publications utilizing LIWC, it has become a widely recognized and trusted tool, enabling novel analytical approaches. Despite its advantages, LIWC has limitations. One issue is its reliance on predefined linguistic categories, which may not capture the nuances and variations of natural language (Lyu et al., 2023; Boji\u0107, 2023). Additionally, LIWC can struggle with accurately interpreting sarcasm, irony, and other subtle language forms, potentially leading to misinterpretations. In recent years, machine learning and LIWC have risen as formidable assets in the realm of Psychology. Their advent has brought about groundbreaking advancements in mental disorder diagnosis, while also making substantial contributions to enhancing human well-being (Bahar and \u00dclker). LIWC stands as a widely embraced computerized text analysis software, facilitating researchers in scrutinizing the emotional, cognitive, structural, and process components inherent in language.\nThis is achieved through the frequency analysis of words in written text or speech (Thompson and Hartwig, 2023). Recent studies suggest an increasing trend in utilizing both LIWC and machine learning for diagnosing mental disorders. Research has concentrated on analyzing large volumes of text to uncover links between everyday language usage and traits such as personality, social interactions, and cognitive patterns (Bahar and \u00dclker).\nIn this study, we aim to employ machine learning and LIWC to detect social support. We employed a wide-ranging set of exit analysis features, encompassing Summary Variables, Linguistic Dimensions, Drives, Cognition, Affect, Social Processes, Culture, Lifestyle, Physical Attributes, States, Motives, Perception, and Conversation extracted from LIWC. The average statistical relationships between these features and various types of social supportive are detailed in Table 4.\nWord count Word Count (WC) measures user engagement and fluency by evaluating the diversity and range of vocabulary used. It reveals that supportive interactions generally have higher word counts than non-supportive ones, suggesting that people give more detailed responses when being supportive. This trend is consistent in both individual and group settings. Additionally, within group interactions, there is a tendency to provide more detailed responses when offering support to women.\nThe Function words analyzed in LIWC include pronouns, impersonal pronouns, articles, prepositions, auxiliary verbs, common adverbs, conjunctions, and negations. Pronouns, in particular, provide insights into users' personalities and communication styles. Supportive comments use fewer function"}, {"title": "5.2. Emotions", "content": "This study employed the NRC Emotion Lexicon (Mohammad and Turney, 2013) to analyze emotions associated with different types of social support. Table 5 presents emotions categorized into supportive and non-supportive groups. It shown that certain emotions such as joy and trust generally have higher intensity levels in supportive contexts compared to nonsupportive ones. For instance, joy tends to be significantly higher in supportive contexts across various categories like individual, group, nation, and others. Conversely, emotions like"}, {"title": "5.3. Sentiment Analysis", "content": "The Social Support dataset was analyzed for sentiment using VaderSentiment (Hutto and Gilbert, 2014). Table 6 showcases the distribution of sentiment (negative, neutral, and positive) within different categories. Across most categories, neutral sentiment appears to be the most prevalent, followed by either positive or negative sentiment, though the balance between the two varies. Non-supportive contexts generally exhibit slightly higher proportions of negative sentiment compared to supportive contexts, where neutral sentiment tends to dominate. Interestingly, individual and group contexts show a relatively balanced distribution between neutral and positive sentiments. There are also noticeable differences in sentiment distribution across demographic groups, with variations particularly evident among black community and women, where negative sentiment appears more pronounced. This suggests that sentiment dynamics may be influenced by factors such as social context and demographic characteristics."}, {"title": "6. Experiments", "content": "Five traditional machine learning classifiers, including LR and SVM with both radial basis function (RBF) and linear kernels, DT, and RFC, are employed for the task of detecting hope speech. We experimented with their soft and hard voting as ensemble methods to create more robust models. All classifiers are utilized with their default parameters and trained on the TF-IDF of unigrams. Additionally, features derived from LIWC, emotion analysis, and sentiment analysis are incorporated to study their contribution to the classification task in the proposed dataset."}, {"title": "6.2. Preprocessing", "content": "Initially, the data preprocessing involved the removal of duplicate comments and the selection of English tweets. Following this, tokenization, lowercasing, punctuation removal, stop word elimination, and stemming or lemmatization were performed to standardize the text data. Additionally, emojis and emoticons were converted into textual representations using the emotion library Mohammad and Turney (2013). Subsequently, abbreviations were expanded to their full forms utilizing a predefined dictionary, while punctuation marks and stopwords were further removed to refine the text data."}, {"title": "6.3. Feature extraction", "content": "Psycholinguistic, emotional, and sentiment attributes were extracted from the textual corpus through the utilization of established lexicons and sentiment analysis tools, namely LIWC by Tausczik and Pennebaker (2010), the NRC Emotion Lexicon devised by Mohammad and Turney (2013), and VaderSentiment introduced by Hutto and Gilbert (2014)Hutto and Gilbert (2014). These metrics were employed to construct comprehensive feature vectors for each text, encapsulating the psychological, emotional, and sentiment dimensions inherent in every tweet. Furthermore, to enhance the scope of analysis and facilitate comparative assessments across different feature sets, ngram features were also derived. The quantification of the feature space dimensions is elaborated upon in detail within Table 9. module."}, {"title": "6.4. Model training and predictions", "content": "In all experiments, we utilized a 5-fold cross-validation approach for both training and evaluating the ML models. Evaluation and comparison were conducted based on the average weighted and macro scores across all folds. Comprehensive results are elaborated upon in the Results section (7)."}, {"title": "6.5. Deep learning", "content": "Two deep learning models, namely Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM), were trained separately using Global Vectors for Word Representation (GloVe) and FastText embeddings. A Keras tokenizer was fitted on the dataset texts to convert all texts into sequences. The maximum sequence length was set to the maximum length of tweets in the dataset, and all sequences were padded to this length. Vectors were obtained from the word embedding matrix for each tweet, after which the input sequences were created and fed to the deep learning models. The parameters used for both models are detailed in Table 7, and the models were trained for 50 epochs for each fold."}, {"title": "7. Results", "content": "The machine learning models were rigorously evaluated across three distinct classification steps as discussed in section 4.3. Importantly, each level of our experiment was augmented with different feature combinations, namely LIWC+Emotions and sentiment features only, TF-IDF only, and a combination of all features. This meticulous approach allowed us to systematically investigate the impact of different feature sets on the performance of various models across different classification tasks. Through this comprehensive analysis, we aimed to identify the most effective model-feature combinations for accurate and reliable social support detection. We conducted experiments using CNN and BiLSTM models with GloVe and FastText embeddings across three subtasks, with results detailed in Section 7.3.1."}, {"title": "7.1. Social Support Detection with LIWC, emotions, and sentiments features", "content": "In Table 8, where we incorporated LIWC, emotions, and sentiment analysis as features, our analysis delved into different traditional machine learning models across various levels of our experiment. Notably, in subtask 1, the LR model emerges as the frontrunner, boasting a macro F1 score of 0.7061. This trend persists in subtask 2, where LR again outshines other models with an impressive macro F1 score of 0.7751. Extending this trend, LR also leads the pack in the subtask 3, securing a macro F1 score of 0.5666. These consistent superior performances by the LR model underscore its efficacy across different levels of our experiment. The LR model's strengths in maintaining competitive macro F1 scores across diverse classification tasks suggest its adaptability and robustness in capturing nuanced patterns within the data. This reaffirms the importance of leveraging LR's simplicity and interpretability to achieve reliable and consistent results in social support detection tasks."}, {"title": "7.2. Social Support Detection using Unigram with TF-IDF values", "content": "In this section, we conducted our experiment using Unigram with TF-IDF values, yielding distinct outcomes compared to the previous section. Table 9 showcases notable variations in model performances across different classification tasks. For subtask 1, the Soft Voting model emerges as the frontrunner, surpassing other models in terms of performance metrics. Similarly, in subtask 2, the Soft Voting model exhibits superior results compared to its counterparts. This trend persists in subtask 3, where, once again, the Soft Voting model demonstrates the highest performance. These findings underscore the efficacy of Soft Voting in leveraging Unigram with TF-IDF values for social support detection across various classification tasks. The superior performance of the Soft Voting ensemble model across all classification tasks could be attributed to its ability to aggregate predictions from multiple base models in a balanced manner. Soft Voting combines the predictions of individual models by taking into account their probabilities rather than simple majority voting, thus leveraging the collective wisdom of diverse classifiers. This ensemble approach often leads to more robust and generalized predictions, as it mitigates the biases and weaknesses inherent in individual models. Additionally, Soft Voting can effectively exploit the complementary strengths of different base models, resulting in enhanced overall performance"}, {"title": "7.3. Social Support Detecion with the combination of all features", "content": "In this section, we integrated a combination of all features including LIWC, emotions, sentiment scores, and Unigram with TF-IDF values. Table 10 presents the outcomes of our experiments across different classification tasks. In the first subtask, SVM (Linear) emerges as the top-performing model, surpassing others in terms of performance metrics. Moving to the second subtask, soft voting stands out with the highest performance value among the models considered. Transitioning to the third subtask, hard voting demonstrates superior results compared to other models. These findings highlight the varied strengths of different models when leveraging a comprehensive feature set, underscoring the importance of selecting appropriate models tailored to the specific task requirements."}, {"title": "7.3.1. Deep learning", "content": "Table 11 provides an overview of model performance across various tasks, with a focus on the macro Fl-score, which is considered a robust metric for evaluating model performance in tasks with imbalanced classes. Across different word embeddings (GloVe and FastText) and model architectures (CNN and BiLSTM), the configurations yielding the highest macro F1-scores are of particular interest. For example, in Task1, using FastText embeddings with the BiLSTM model resulted in a macro F1-score of 0.7611, indicating a balanced performance in terms of precision and recall across both support and non-support classes. Similarly, in Task 2 and Task 3, certain configurations achieved macro F1-scores of 0.8184 and 0.7235"}, {"title": "8. Error analysis", "content": "The detailed results of all experiments are presented in Section 7. Here, we analyze the performance of the bestperforming model for each subtask.\nTable 12 presents the classwise scores for the bestperforming models. For Subtask 1 and Subtask 2, the CNN with a Glove embedding. In Subtask 3, the best performance was achieved through soft-voting of models using only Unigram with TF-IDF values.\nComparing the results for each category with the label distribution across the dataset in Table 3 reveals the influence of these imbalanced distributions on performance. In all subtasks, categories with a higher population were predicted more accurately.\nWe also provided the confusion matrices for these models in Figures 2, 3, and 4. The analysis of these confusion matrices reveals several key patterns of misclassification.\nFor subtask 1, supportive comments were frequently misclassified as not supportive. This indicates that the model struggles to accurately distinguish between these two categories, likely due to subtle differences in language or context that are challenging for the algorithm to capture."}, {"title": "9. Discussion", "content": "The dataset and experiments proposed in this paper have several characteristics and limitations that are briefly discussed in the following:\n\u2022 The dataset utilized in this paper was exclusively gathered from YouTube comments within a specific timeframe and targeted videos, which may introduce biases. To enhance the diversity dataset and its richness, incorporating posts from other social media platforms, such as X and Reddit, in an open timeframe would be beneficial.\n\u2022 The dataset is relatively small and imbalanced in terms of supportive comments, which has affected the performance of the machine learning models. In future work, this issue could be addressed by increasing the dataset size, particularly for supportive comments. Additionally, techniques should be proposed to manage this imbalanced distribution of data in the experiments.\n\u2022 The current study reveals that users on YouTube tend to express more support for groups of people than for individuals. Additionally, users show support for different nations without being heavily influenced by religious affiliations. The data indicates that recent support has not been predominantly directed towards any specific religion. Instead, people are more concerned with nations and communities, such as LGBTQ+ individuals and Black people. This trend highlights a broader social focus on national and community identities over religious considerations.\n\u2022 This study serves as a foundational step in introducing the task of social support detection aimed at fostering support and positivity as an alternative to merely filtering out hate speech Tash et al. (2024a); Ahani et al. (2024b). Consequently, the paper primarily focuses on the introduction of this concept, assessing the feasibility of the task, and examining it from a psychological perspective. As such, experiments involving state-of-the-art models like transformers and large language models have been deferred to future works."}, {"title": "10. Conclusion and future work", "content": "This study marks a significant step forward in understanding and promoting social support within online communities. By introducing the task of SSD and conducting experiments on a dataset of YouTube comments, we have gained valuable insights into the dynamics of supportive interactions in digital spaces.\nOur findings reveal that YouTube users predominantly express support for groups of people, with less emphasis on individual support and religious affiliations. This highlights the importance of considering broader societal contexts when analyzing social support interactions online.\nWhile our experiments have provided promising results, there are notable limitations to address. The dataset used is relatively small and imbalanced, limiting the generalizability of our findings. Additionally, biases inherent in the dataset, stemming from its exclusive focus on YouTube comments within specific parameters, need to be addressed through diversification of data sources.\nLooking ahead, future research in SSD should focus on expanding and diversifying datasets, exploring advanced modeling techniques such as deep learning approaches, and designing interventions to promote positive interactions in online communities. By addressing these challenges, we can further our understanding of social support dynamics online and contribute to fostering supportive and inclusive digital spaces."}, {"title": "Declarations", "content": "Funding\nThe work was done with partial support from the Mexican Government through the grant A1-S-47854 of CONACYT,"}, {"title": "Conflict of Interest", "content": "I declare that the authors have no competing interests as defined by Nature Research, or other interests that might be perceived to influence the results and/or discussion reported in this paper."}, {"title": "Ethics approval", "content": "Not applicable."}, {"title": "Consent to participate", "content": "Not applicable."}, {"title": "Consent for publication", "content": "Not applicable."}, {"title": "Availability of data and materials", "content": "The dataset utilized in this study can be obtained upon request from the corresponding author. Please reach out to Moein Shahiki Tash at [mshahikit2022@cic.ipn.mx]."}, {"title": "Author Contributions Statement", "content": "Z.A. and M.S.T. contributed equally to the work. Z.A. and M.S.T. developed the study concept and design, performed the data analysis, and wrote the main manuscript text. L.R. contributed to data collection, curation, and preprocessing. F.B. provided significant contributions to the methodological framework and data interpretation. G.S. was responsible for software development and technical validation. A.G. supervised the project, provided critical revisions to the manuscript, and contributed to the theoretical framework. All authors reviewed and approved the final manuscript."}]}