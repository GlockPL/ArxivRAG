{"title": "DIFFUSION GENERATIVE MODELING FOR SPATIALLY RESOLVED GENE EXPRESSION INFERENCE FROM HISTOLOGY IMAGES", "authors": ["Sichen Zhu", "Yuchen Zhu", "Molei Tao", "Peng Qiu"], "abstract": "Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA sequence abundance by systematically connecting cell morphology depicted in Hematoxylin and Eosin (H&E) stained histology images to spatially resolved gene expressions. ST is a time-consuming, expensive yet powerful experimental technique that provides new opportunities to understand cancer mechanisms at a fine-grained molecular level, which is critical for uncovering new approaches for disease diagnosis and treatments. Here, we present Stem (SpaTially resolved gene Expression inference with diffusion Model), a novel computational tool that leverages a conditional diffusion generative model to enable in silico gene expression inference from H&E stained images. Through better capturing the inherent stochasticity and heterogeneity in ST data, Stem achieves state-of-the-art performance on spatial gene expression prediction and generates biologically meaningful gene profiles for new H&E stained images at test time. We evaluate the proposed algorithm on datasets with various tissue sources and sequencing platforms, where it demonstrates clear improvement over existing approaches. Stem generates high-fidelity gene expression predictions that share similar gene variation levels as ground truth data, suggesting that our method preserves the underlying biological heterogeneity. Our proposed pipeline opens up the possibility of analyzing existing, easily accessible H&E stained histology images from a genomics point of view without physically performing gene expression profiling and empowers potential biological discovery from H&E stained histology images.", "sections": [{"title": "1 INTRODUCTION", "content": "Histology imaging of Hematoxylin and Eosin (H&E) stained tissues has been an important, long-standing tool in biomedical research and clinical diagnosis. H&E stained histology images provide rich information about the tissue composition and cell morphology at a cellular, microscopic level. In recent years, the emergence of Spatial Transcriptomics (ST) technology has provided an opportunity to deepen our understanding of these H&E images and tissue slides to a more fine-grained molecular level. ST technology segments centimeter-size Whole Slide Images (WSIs) into hundreds of spots with a micrometer-size diameter and generates gene expression profiling of the tissue within each spot (St\u00e5hl et al., 2016). ST has seen prominent applications in biomedical and clinical scenarios. By connecting genomics information to cells' spatial location within the tissue, ST captures the underlying biological heterogeneity across various cells in different locations and reveals the cancer microenvironment for better targets in treatment (Lewis et al., 2021; Williams et al., 2022).\nWhile being a promising tool to explore potential relationships between cell morphology and gene expression patterns, existing ST technology such as Visium (St\u00e5hl et al., 2016) are less accessible due to its substantial cost in time and experimental preparation work in wet labs. On the other hand, H&E stained images are enriched in clinical settings due to their low cost and wide application."}, {"title": "2 RELATED WORK", "content": "Machine Learning Prediction of Gene Expression from Histology Images The task of predicting spatially resolved gene expression at a near single-cell resolution using patch-level H&E stained images has been approached as a regression task by several works. The seminal work of ST-Net (He et al., 2020) utilizes transfer learning to directly predict gene expression values from encoded histology images starting from a pre-trained model on ImageNet. HisToGene (Pang et al., 2021) and Hist2ST (Zeng et al., 2022) improve upon ST-Net by additionally introducing correlation among different patches and spatial location information into the model. Taking a step further towards including more information into the model, TRIPLEX (Chung et al., 2024) and M2ORT (Wang et al., 2024a) integrate nonlocal, holistic information of the histology image with the local information by extracting hierarchical, multi-resolution image features. In a different direction, BLEEP (Xie et al., 2024) and EGN (Yang et al., 2023) attempt to enhance prediction accuracy by retrieving gene expression values from the training set that are most similar to the test histology image query. BLEEP achieves this goal by aligning image and gene expression embedding through a CLIP-like contrastive learning loss (Radford et al., 2021) while EGN adopts the path of exemplar learning. We remark that while these mentioned approaches have shown promising performance on the gene expression prediction task, their idea has deep roots in the regression framework, which potentially hinders them from achieving more satisfactory results. Our proposed approach is inherently different from all the works mentioned above by taking a different route through generative modeling.\nComputational Pathology Foundation Models One of the central tasks in computational pathology is to obtain general-purpose embedding of histology image patches that can be used for downstream tasks such as gene expression prediction, cell phenotyping, and prognosis prediction (Jaume et al., 2024). Thanks to the abundance of histology images, powerful pathology foundation models have been trained on large-scale datasets (Ciga et al., 2022; Filiot et al., 2023; Vorontsov et al., 2023; Xu et al., 2024; Huang et al., 2023; Chen et al., 2024; Lu et al., 2024) with unimodal or multimodal self-supervised learning objectives, such as image-text contrastive learning (Radford et al., 2021), image captioning (Yu et al., 2022), DINO (Oquab et al., 2023), etc. Through massive pertaining, computational pathology foundation models implicitly learn to encode the tissue cell morphology into embeddings with rich information. Since cell morphology largely determines cell types, which further substantially affects the associated gene expression values, the embedding also partially encodes gene-related information that is highly beneficial for gene expression inference. In this work, we mainly leverage two foundational models, UNI (Chen et al., 2024) and CONCH (Lu et al., 2024) for extracting and aggregating histology image information. We also experiment with large-scale foundation models such as Virchow & Virchow-2 (Vorontsov et al., 2023; Zimmermann et al., 2024) and H-Optimus-0 (Saillard et al., 2024), which are trained with similar self-supervised methods as UNI but using a larger vision encoder.\nDiffusion Model for Multimodal data and Conditional Generation Diffusion models have shown remarkable performance in multimodal generation through the power of conditional generation, such as text-to-image (Rombach et al., 2022; Saharia et al., 2022; Esser et al., 2024), text-to-video (Singer et al., 2022; Ho et al., 2022), text-to-audio (Kreuk et al., 2022) and many more. One central element of conditional diffusion models is the conditioning mechanism since it affects how well information from different modalities fuses together. The flexibility in diffusion model design space allows the introduction of conditioned data modality into the model in various ways, where the cross-attention and modulation mechanisms are two mainstream approaches. For example, taking the literature of text-to-image diffusion model for demonstration, GLIDE (Nichol et al., 2021), Imagen (Saharia et al., 2022) and Stable-Diffusion (Rombach et al., 2022) take the route of cross attention and incorporates conditional information by attending the model to text condition embedding extracted with either learned or pre-trained encoders. In a different vein, PGv3 (Liu et al., 2024) recently introduced a new way to fuse information by performing joint attention between data and conditions using KV concatenation. In this work, we are also faced with the need for a conditioning mechanism to fuse the histology image information with the gene expression data. We select the modulation approach with adaptive LayerNorm, the same module used in DiT (Peebles & Xie, 2023), since it's parameter-efficient and fast for inference."}, {"title": "3 BACKGROUND", "content": "Diffusion Model Diffusion models have shown tremendous success in generating complex data distribution, including numerous science applications such as protein design (Yim et al., 2023; Watson et al., 2023), quantum science (Zhu et al., 2024a;b), single cell analysis (Luo et al., 2024), chemistry (Duan et al., 2023) and neural science (Wang et al., 2024b). Before introducing our main algorithm and architecture, we first review some basics of the diffusion model in the setting of discrete time denoising diffusion (DDPMs) (Ho et al., 2020; Sohl-Dickstein et al., 2015).\nDiffusion models are probabilistic models that are designed to learn data distribution $P_{data}(x)$ from samples. A diffusion model consists of two stochastic processes: forward noising and backward denoising processes. First, the forward process $q(x_t|x_o)$ is chosen to perturb the data distribution $P_{data}$ into a simple distribution $q_{ref}$. A common choice is to apply Gaussian noise to data $x_o$ gradually in T steps and turns it into $p_T \\approx q_{ref} = \\mathcal{N}(0,I)$: $q(x_t|x_o) = \\mathcal{N}(x_t; \\sqrt{\\bar{a}_t}x_o, (1 - \\bar{a}_t)I)$, where $\\bar{a}_t$ are constant hyperparamters. With the parameterization trick, $x_t$ can be sampled by $x_t = \\sqrt{\\bar{a}_t}x_o + \\sqrt{1 - \\bar{a}_t}\\epsilon_t, \\epsilon_t \\sim \\mathcal{N}(0, I)$.\nThe backward process reverts the forward noising process by iterative denoising $q_{ref}$ into $P_{data}(x)$. We parameterize the backward process as $p_{\\theta}(x_{t-1}|X_t) = \\mathcal{N}(\\mu_{\\theta}(x_t,t), \\Sigma_{\\theta}(x_t,t))$, where neural networks are used to predict the statistics of distribution from noisy data $x_t$. $P_{\\theta}(x_{t-1}|x_t)$ is learned by maximizing the variational lower bound of the log-likelihood of true data $x_o$, which reduces to the minimization of following objective $L(\\theta) = \\sum_t D_{KL} (q(x_{t-1}|X_t, x_o) || P_{\\theta}(x_{t-1}|x_t))$. Once the diffusion model $p_{\\theta}$ is well trained, new data can be sampled by simulating $x_T \\sim \\mathcal{N}(0, I)$ and sampling iteratively from $x_{t-1} \\sim p_{\\theta}(x_{t-1}|x_t)$ for $t = T, . . . , 1$.\nDiffusion Conditional Generation Diffusion model is known to be a powerful conditional distribution learner (Rombach et al., 2022; Saharia et al., 2022; Peebles & Xie, 2023; Chen et al., 2023) and is capable of modeling distributions of the form $P_{data} (x|y)$, where $y$ is additional information such as class label, text, or histology images in our considered problem setting. This is enabled by allowing the neural network to take the condition $y$ as an additional input: $\\epsilon_{\\theta}(x_t,t,y)$. Notably, the condition $y$ often enters the model through its latent vector representation $h_y$. Therefore, diffusion models implicitly learn a mapping $h_y \\rightarrow P_{data}(x|y)$ and often succeed in extrapolating to novel unseen conditions at inference time."}, {"title": "4 DIFFUSION GENERATIVE MODELING OF SPATIAL GENE EXPRESSION", "content": "Problem Set-up Let $V \\in \\mathbb{R}^{L\\times L\\times 3}$ be a image patch of the H&E stained image, and $X \\in \\mathbb{R}^{C}$ be the associated gene expression profile, where L is the image patch size and C is the gene set size. We aim to infer X when only the image patch V is given. Existing methodologies treat the task as a regression problem and attempt to learn a deterministic function $f_{expr}$ such that $X \\approx f_{expr}(V)$. However, while one histology image contains extensive information about the corresponding gene profile, it is unlikely to uniquely determine the gene expression vector due to tissue heterogeneity and uncertainty in the cellular microenvironment. This renders the mapping $f_{expr}$ between image patches V and gene expressions X non-injective. To address this potential issue, we treat the spatial gene expression prediction as a generative modeling task and aim to learn the conditional distribution of gene expression given the histology image $X \\sim p_{gene}(X|V)$ from data sample pairs. This framework generalizes over the deterministic regression approach by potentially allowing one-to-many relationships between some image patches V and gene expression X. Note that when the learned $p_{gene}(X|V)$ is a degenerate delta distribution, i.e., $p_{gene}(X|V) = \\delta_{f_{expr}(V)}(x)$, we recover the deterministic regression setting. From now on, we focus on modeling the distribution of gene expression vectors conditioned on the associated histology image.\nDiffusion Generative Modeling of Gene Expressions We perform generative modeling of the conditional distribution $p_{gene}(X|V)$ with denoising diffusion model (DDPM). We choose the forward process $q(X_t|X_0, V) = \\mathcal{N}(X_t; \\sqrt{\\bar{a}_t} X_o, (1-\\bar{a}_t)I)$, where $\\bar{a}_t$ are constant computed from the noise schedule hyperparameter $\\beta_t$ as $a_t = 1 - \\beta_t,\\bar{a}_t = \\prod_{i=1}^{t} a_i$. We choose a linear noise schedule $\\beta_t = \\beta_{max} + (\\frac{t}{T} - 1) \\beta_{min}$. We write the backward process as $p_{\\theta}(X_{t-1}|X_t, V) = \\mathcal{N}(\\mu_{\\theta} (X, V, t), \\sigma I)$, where we fix the variance of the backward process to be untrained time de-"}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In this section, we present numerical results on Kidney Visium dataset, HER2ST dataset, and ablation studies on model hyperparameters and algorithm design choices. For additional results on other datasets, please refer to Appendix D.\nEvaluation metrics Our evaluation metrics include top k mean Pearson Correlation Coefficient (PCC) (denoted as PCC-k, calculated in the log-transformed space), mean absolute error (MAE), mean square error (MSE), which have been widely used in evaluating the accuracy of gene expression prediction in the existing literature. MAE and MSE are calculated respectively using all genes in the selected gene set in log-transformed space.\nAs is pointed out in Xie et al. (2024), one pitfall in prediction tasks is to output the mean value with minimal variation or without meaningful variation that is faithful to the ground truth biological heterogeneity. Also, in our exploration, we discovered that high PCC does not necessarily guarantee meaningful and faithful predictions that align well with ground truth expression. In fact, we discovered that PCC in log-transformed space would be surprisingly high if the prediction is simply the mean expression across all genes in this spot. This encourages us to propose a new evaluation metric that can better reflect how well a prediction model captures the heterogeneity within the data. We consider the following relative variation distance (RVD), calculated through:\n$RVD = \\frac{1}{C} \\sum_{i=1}^{C} \\frac{(\\sigma_{i_{pred}}^2 - \\sigma_{i_{gt}}^2)^2}{(\\sigma_{i_{gt}}^2)^2}$\nwhere $\\sigma_{i_{pred}}^2$ is the variance of the i-th gene expression prediction across spots (predicted gene variation) and $\\sigma_{i_{gt}}^2$ is the variance of the true i-th gene expression across spots (true gene variation). RVD represents a weighted average of the magnitude of deviation of the predicted gene variation from the true gene variation. RVD serves as a complementary metric to the current existing evaluation system that can better filter out false positive predictions created by solely focusing on PCC values. Additionally, we plot the gene variation curve against the ground truth, see Appendix A for more details."}, {"title": "5.1 KIDNEY VISIUM DATASET", "content": "Dataset and Preprocessing We applied Stem to a dataset that contains 23 kidney tissue sections from 22 individuals (Lake et al., 2023) covering three different health conditions (healthy reference (Ref), Chronic Kidney Disease (CKD), and Acute Kidney Injury (AKI)) and two different tissue types (cortex and medulla). Number of ST spots ranges from 315 to 4159 per slide. The gene profiling is performed using 10x Genomics Visium platform, which is a mainstream spatial transcriptomic sequencing platform that provides genomics profiling for a grid of spots with a diameter ~ 55\u03bcm along the tissue slide. We log-transformed the gene expression following Jaume et al. (2024).\nExperiment Setup An image patch of 224 \u00d7 224 pixels is cropped centered around each spot. Following a similar gene selection protocol in (He et al., 2020), we selected two gene sets, top 200 genes from the intersection of highly expressed (high mean) and highly variant (high variance) genes (denoted as HMHVG) and top 200 genes from all highly variable genes ordered in mean (denoted as HVG). Training, inference, and evaluation are performed in the log-transformed gene"}, {"title": "5.2 HER2ST DATASET", "content": "Dataset and Preprocessing We also applied Stem to one breast cancer dataset, HER2ST (Andersson et al., 2021), which is sequenced by SpatialTranscriptomics\u00b9 platform. This dataset includes 32 slices from 8 patients. From patient A-D, six tissue sections were collected with a distance of 32\u00b5m in between. From patient E-H, three consecutive tissue sections were taken for each patient. Since intuitively it would be easier to infer gene expressions for consecutive slides if their neighbors are included in the training data, we make the task more challenging by holding out B1 (which does not have any neighboring consecutive slide), for test evaluation. Each slide contains normal tissue regions and some of the slides contain in situ cancer or invasive cancer. The spot size for this dataset is 100\u03bcm in diameter and the total number of spots ranges from 176 to 712 per slide. We log-transformed the gene expression following Jaume et al. (2024).\nExperiment Setup An image patch of 224 \u00d7 224 is cropped around each spot. For the gene sets, we select top 300 HMHVG and 296 differentially expressed genes (DEGs), respectively, to perform training and evaluation. In HER2ST, the first slide in every patient is manually annotated by pathologists into 4 normal regions and 2 tumor regions. DEGs are selected following the standard preprocessing pipeline using Scanpy (Wolf et al., 2018) and the union of DEGs across all 6 regions from training slides are selected as features."}, {"title": "5.3 ABLATION STUDY", "content": "In this section, we perform an ablation study on the model hyperparameters and algorithm design choices. We examine the following three factors in order: choices of pathology foundation model, image augmentation ratio, and test slide health condition. In the following, we perform all the experiments on the Kidney Visium dataset and use the HMHVG gene set. We set the default setting to be: CONCH + UNI for the pathology foundation model, 1: 4 for image augmentation ratio, and 20-0038 (AKI) for the hold-out test slide. Unless further notice is given, we will keep the setting the same as the default and only vary the ablated parameter for each ablation study. The best values are marked in bold. We also perform additional ablation experiments on the scalability of Stem to large gene sets, effects of generated samples and sample statistics, influence of pathology foundation model size, and the representation power of histology image patch encoder. For more details on the extra ablation experiments, please refer to Appendix B.\nChoice of Foundation Models In this ablation experiment, we specifically choose not to augment the training dataset with image augmentation techniques. We seek to compare the effects of foundation models on algorithm performance by removing other potential influencing factors. We consider the possible combination of CONCH (Lu et al., 2024) and UNI (Chen et al., 2024), which produces embedding vectors of dimension 512 and 1024 for histology image patch of size 224 \u00d7 224 respectively. We evaluate the following three sets of choices: 1) CONCH only 2) UNI only 3) CONCH + UNI, and the evaluation result is in Table 3. Here, CONCH + UNI stands for using combined features extracted by both UNI and CONCH through simple concatenation. The results suggest that it works best to input the model combined histology image information of both foundation models. This is reasonable and accords with our intuition since UNI and CONCH are trained with distinct"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this work, we propose Stem, a novel generative modeling algorithm for spatially resolved gene expression prediction based on H&E stained histology images using conditional diffusion models. Stem generates highly accurate and biologically faithful predictions for unseen histology images at test time and achieves SOTA performance on multiple evaluation metrics across different datasets. For future work, it would be exciting to explore more conditioning mechanism, neural network architecture, and their influence on task performance. How to better use embedding generated by pathology foundation models on diffusion generative modeling is also an intriguing question for future explorations."}]}