{"title": "LLMMAP: FINGERPRINTING FOR LARGE LANGUAGE MODELS", "authors": ["Dario Pasquini", "Evgenios M. Kornaropoulos", "Giuseppe Ateniese"], "abstract": "We introduce LLMmap, a first-generation fingerprinting attack targeted at LLM-integrated applications. LLMmap employs an active fingerprinting approach, sending carefully crafted queries to the application and analyzing the responses to identify the specific LLM model in use. With as few as 8 interactions, LLMmap can accurately identify LLMs with over 95% accuracy. More importantly, LLMmap is designed to be robust across different application layers, allowing it to identify LLMs operating under various system prompts, stochastic sampling hyperparameters, and even complex generation frameworks such as RAG or Chain-of-Thought.", "sections": [{"title": "1 Introduction", "content": "Penetration testing or any other form of security analysis hinges on an information-gathering phase. Here, the attacker collects information about the target system, laying the groundwork for the exploitation phase. For instance, when auditing a remote machine or service, the first step for the attacker is to infer the operating system running on the target, usually, by a fingerprinting attack. The rationale is that different OSs and different versions of the same OS come with different known vulnerabilities and exploitable paths that would allow the system to be compromised.\nTesting the security of applications integrating Large Language Models (LLMs) is no different. Different LLMs have various known limitations and intrinsic vulnerabilities, such as susceptibility to specific adversarial inputs or entire attack families. Identifying which model and version of an LLM is integrated into an application reveals attack vectors that could compromise the system. Once the deployed LLM is determined, the attacker can leverage tailored adversarial inputs or exploit architecture-specific vulnerabilities, such as Buffer Overflow in Mixture of Experts [4] in Mixture of Experts based LLMs. If the target LLM is open-source, the attacker can use powerful white-box optimization approaches [13, 6, 2] for even more reliable and impactful attacks."}, {"title": "2 Active Fingerprinting for LLMs", "content": "Effective OS fingerprinting techniques rely on active interaction with the target system. For instance, attackers send TCP packets (known as probes) to the target and analyze the responses. Operating systems differ in aspects such as TCP window size, default TTL (Time to Live), handling of flags, reactions to unusual or malformed packets, and TCP sequence numbering. These variations allow attackers to distinguish between different OSs.\nLLMs exhibit similar variability-different LLMs respond differently to the same prompt. By analyzing these discrepancies, attackers can identify the underlying model. However, fingerprinting LLMs is inherently challenging due to several factors:\n\u2022 Randomized Outputs: Unlike deterministic operating systems, LLMs use randomized sampling to generate responses. This sampling is influenced by hidden hyperparameters, such as temperature and token repetition penalty, making consistent output difficult to achieve.\n\u2022 Model Customization: LLMs are often tailored for specific applications using a system prompt/directive, which sets the model's general behavior (see Figure 1). This customization can significantly alter the output distribution, complicating the fingerprinting process.\n\u2022 Applicative Layers: LLM applications frequently involve complex interaction pipelines. For example, a Retrieval Augmented Generation (RAG) framework might be used, integrating a knowledge base and additional hyperparameters (e.g., embedding model, chunking procedure). Similarly, advanced prompting frameworks like Chain-of-Thought [11] or ReACT [12] can further obscure the fingerprinting process.\nThese factors, individually or in combination, greatly influence an LLM's behavior, making fingerprinting a complex adversarial task. We refer to the cumulative effect of these factors as the prompting configuration of the LLM- integrated application."}, {"title": "2.1 Problem Formulation", "content": "In the following, we formalize the objective of the adversary in a fingerprinting attack. Let B be a remote application integrating an LLM (e.g., a chatbot served via a web interface). The application enables interaction with external parties. That is, an external user can submit a query q to the LLM and receive its output o, an interaction captured by the following oracle:\n$O(q) = o$, such that $o \\sim s(LLM_\\theta \\circ \\varphi(q))$,\nwhere $LLM_\\theta$ is the deployed LLM, and s is the prompting configuration; the set of transformations applied to the LLM's inputs and outputs, including the system prompt, the chosen sampling procedure, and the use of RAG or similar frameworks, as well as their arbitrary combination. The symbol $\\sim$ is used to indicate that the output of the model is generated via a stochastic sampling procedure. Hereafter, we call any input provided to the oracle O a query.\nWe assume O to be a perfect oracle; that is, nothing can be inferred about $LLM_\\theta$ besides what is revealed from the output of O itself. We consider both the prompting configuration s and the randomness of the sampling method to"}, {"title": "3 Overview of LLMmap", "content": "In this section, we introduce LLMmap, our approach to fingerprinting LLMs. LLMmap aims to identify the specific LLM used in an application through a combination of strategic questioning and machine learning analysis.\nThe process begins by preparing a set of carefully chosen questions, designed to elicit responses that reveal unique characteristics of different AI models. These questions, referred to as the querying strategy (Q), are then submitted to the O. The oracle responds to each question, and these question-response pairs, called traces $((q_i, o_i))$, are collected.\nThe collected traces are then analyzed by an inference model (f). This machine learning model processes the traces to determine which specific LLM is being used by the application. The goal is to accurately attribute an entry from the label space (C) that corresponds to the identified LLM version. The fingerprinting process is formalized in Algorithm 1.\nTo maximize the accuracy and efficiency of this fingerprinting process, careful selection of both the query strategy (Q) and the inference model (f) is crucial. The following sections will discuss our solutions for implementing these components effectively."}, {"title": "4 Querying strategy of LLMmap", "content": "Much like in traditional OS fingerprinting, not all queries have the same effectiveness in characterizing the target LLM. Some queries are more revealing than others. We assume that there exists a set of prompts that can elicit distinctive behaviors from different LLMs, making it possible to differentiate between them reliably. Therefore, developing an effective querying strategy (Q) for LLMmap involves identifying these key queries that can consistently highlight the differences among various LLMs."}, {"title": "4.1 In Pursuit of (Robust) Queries that Reveal the Model", "content": "To effectively fingerprint the target model, our work identifies two essential properties that queries should possess:\n(1) Inter-model Discrepancy: A query should generate outputs that are maximally different across different LLMs. In other words, the query should produce very different answers when used on different language models. Formally, let L be the universe of possible language models and let d be a distance function that measures differences in the output space of the LLMs. We aim to find a query $q^*$ that maximizes these differences, defined as:\n$q^* = \\arg \\max_{q \\in Q} (\\mathbb{E}_{(\\theta, \\theta' \\in L)}[d(LLM_\\theta(q), LLM_{\\theta'}(q))])$.\nThis means we want to identify queries that produce very different outputs for any given pair of different LLMs, $\\theta$ and $\\theta'$. This property increases the likelihood of distinguishing between different LLMs.\n(2) Intra-model Consistency: A query should produce consistent outputs across different prompting configurations and randomness when executed by the same underlying model $\\theta$. This means that even if the model is set up differently each"}, {"title": "4.2 Effective Query Strategies", "content": "Based on the properties defined earlier, we will now discuss some general prompt families that we have found to be effective for LLM fingerprinting."}, {"title": "4.2.1 Can We Simply Apply Banner Grabbing on LLMs?", "content": "While fingerprinting an LLM is generally a challenging task, there are a few instances where inferring the origin of the target model can be trivial. Indeed, LLM deployers might choose to: (i) explicitly state the name of the model in the system prompt, or (ii) train the model with samples that include the name of the model/family. In such cases, an adversary can infer information about the model by simply prompting it to reveal it, e.g., by submitting \u201cwhat model are you?\u201d or \u201cwhat's your name?\u201d. This is a well-known approach in offensive security for service/OS fingerprinting and is called banner grabbing. Hereafter, we refer to this class of queries as banner grabbing queries.\nBanner Grabbing Is Not a Robust Solution (per se): While inherently simple, this baseline approach is neither a general nor a reliable fingerprinting approach. Specifically:\n(1) In practice, according to our experiments, only a small portion of models, especially open-source ones, are aware of their name or origin. Moreover, even when the model has such information, it is usually limited to the model's family (e.g., LLaMa or Phi), lacking details on the generation and size of the model. For instance, LLaMa-3-8B and LLaMa-2-70B, or ChatGPT-4 and ChatGPT-40, would be considered the same model.\n(2) This approach is not robust to prompting configurations, such as different system prompts. A trivial mitigation against banner grabbing queries is for the LLM to state a different model name in the system prompt, thus overriding the true banner of the model and misleading the attacker (e.g., Figure 1).\n(3) Ultimately and more interestingly, banner-grabbing queries are not a reliable approach. We observed that models would often produce meaningful yet erroneous answers to such queries i.e., the model claims to be a different LLM. This usually happens because the model has been trained/fine-tuned on outputs generated by other models (typically OpenAI's models). For instance, SOLAR-10.7B-Instruct-v1.0 and openchat_3.5 falsely claim to be OpenAI models when answering banner grabbing queries. Similarly, unreliable answers might appear due to bias in the training set. For instance, the models aya-23-8B and 35B from Cohere respond to banner grabbing queries with \u201cCoral\u201d, another model from the same vendor. Table 2 in Appendix A provides additional examples for this behavior.\nBanner Grabbing Queries Induce Strong Inter-model Discrepancy: Although this baseline approach is unreliable, it results in highly inter-model discriminative responses (see Equation (2)), which are frequently factually wrong. Each model tends to answer such queries in unique ways. For instance, Google's Gemma models seem to avoid disclosing any information about themselves, responding with \u201cI am unable to provide information that may compromise my internal architecture or development process.", "Who created you?\". This banner grabbing query induces strong intra-model consistency (see Equation (3)) as it tends to bypass the effect of the system prompt more than other similar queries, such as \u201cWhat's your name?": ""}, {"title": "4.2.2 Queries That Request Meta-Information About The Model", "content": "Another family of queries with high inter-model discrepancy includes prompts that ask the model for information about itself or its training process (e.g., \u201cWhat's the size of your training set?\u201d), rather than knowledge-based tasks (e.g., \u201cWhat's the capital of France?", "What's your data cutoff date?\", the LLM might be aware of the answer, potentially leaking very important metadata about the training that can be exploited in the fingerprinting process.\"\n    },\n    {\n      \"title\": \"4.2.3 \\\"Malformed\\\" Queries Reveal Useful Information\",\n      \"content\": \"A common and effective approach in OS fingerprinting is to submit malformed packets to the target system. How the TCP/IP stack fails to process (or handle) these packets leaks substantial information about the OS. A similar rationale can be applied to LLMs. Indeed, how LLMs respond to \u201catypical\\\" prompts is generally unique and, more importantly, consistent across various prompting configurations (e.g., different system prompts).\nExploiting Models' Alignment: While LLMs lack any formal syntactic correctness for their inputs, we observed that adversaries could induce models to produce error messages\" by exploiting other features of the models, such as their alignment procedures.\nAlignment is a methodology that aims to prevent the model from answering harmful tasks (e.g., \u201cHow to build a bomb?\") or producing objectionable content overall. Due to the alignment process, when asked to respond to a harmful query, the LLM refuses by issuing a message stating that the prompted task cannot be served (see Figure 2). Attackers can exploit this behavior to induce strong and robust fingerprint signals, as these \u201cerror messages\" are typically highly model-dependent and sufficiently unique. More prominently, these responses also remain consistent across prompting configurations. Indeed, regardless of the directives set by the prompting configuration, the model prioritizes the refusal of the harmful task. Furthermore, we stress that issuing harmful prompts allows us to discriminate the other two macro-categories of models: aligned and non-aligned LLMs. Therefore, such queries provide distinguishing power when it comes to non-aligned LLMs.\nExploiting Weaker Forms of Alignments: Prompting harmful instructions is not the only way to exploit a model's alignment for fingerprinting. We observed another class of prompts that induced strong intra-model consistency by exploiting the \u201cnon-harmful bias\" induced by the alignment process. In particular, asking rhetorical/ethical questions (e.g., \"Is racism wrong?": "r \u201cIs climate change real?\u201d) provides discriminative and robust responses. As in the previous case, such queries provide a high degree of intra-model consistency as the model prioritizes answering these prompts over other directives imposed by the prompt configuration.\nMoreover, in contrast to harmful prompts, the model actually produces an answer for the given question, resulting in more articulate and richer outputs that can be used to characterize the target LLM.\nInconsistent Inputs: Besides exploiting models' alignment, an attacker can craft \u201cinconsistent\u201d (or more generally", "malformed": "queries by relying on nonsensical or semantically-broken prompts. For instance, submitting queries that mix several languages (e.g., \u201cBonjour, how are you doing today? \u00bfQu\u00e9 tal?\u201d). As in OS fingerprinting, the way the target handles such inconsistent inputs (e.g., answering the query in either English or Spanish) provides a unique behavioral signature for the LLM, which improves fingerprint accuracy when combined with other probing approaches."}, {"title": "4.2.4 Triggers for Prompt-Injection Boost Intra-model Consistency", "content": "As previously mentioned, prompting configuration can fundamentally shift the LLMs' output distribution, making it difficult to achieve intra-model consistency of probes and, thus, fingerprinting the model. To reduce the effect of prompt configurations on the model's behavior, attackers can rely on additional techniques to combine with a chosen query. In"}, {"title": "4.3 On Identifying the Right Subset of Queries for LLMmap", "content": "Based on the families of discriminative prompts defined in Section 4.2, our goal is to identify a small set of queries to compose an effective query strategy for LLMmap. To accomplish this, we generate a pool of 10 entries for each prompt family by combining manually created and synthetically generated prompts, resulting in a total of 50 possible queries. We then search for the smallest combination of queries that produces the best fingerprint results. This procedure is detailed in Appendix B. After this optimization phase, we end up with a query strategy composed of 8 queries, which are listed in Table 1, sorted by decreasing individual effectiveness. Hereafter, we use these queries to implement the query strategy Q unless stated otherwise."}, {"title": "5 Inference Model of LLMmap", "content": "Once the queries in Q have been submitted to the target application, the resulting traces are analyzed to identify the deployed LLM, i.e., the \u03b8. Due to the unpredictability of the application's responses, influenced by the unknown to the attacker prompting configuration and the randomness of the sampling procedure, we implement the inference phase using a machine learning model trained to derive robust predictions from the collected traces.\nWe consider two settings for the inference task:\n1. Closed-Set Fingerprinting Setting: The inference model is aware of a predefined set of possible LLMs and identifies which one generated the traces. Specifically, given a set of n known models $C={\\theta_1,..., \\theta_n}$, the inference model predicts which of the n models in C generated the input traces. In this setting, the inference model functions as a classifier $f : T^k \\rightarrow C$, where T represents the set of traces.\n2. Open-Set Fingerprinting Setting: The inference model does not have prior knowledge of all possible LLMs and instead produces a vector signature that can be matched against known templates. Specifically, the inference model is a function $f : T^k \\rightarrow \\mathbb{Z}^m$; given the collected traces as input, the model outputs an m-dimensional vector. This vector represents a signature that can be \"fuzzy-matched\" with stored LLM templates to recognize the target LLM."}, {"title": "6 Evaluation", "content": "This section details our evaluation setting as well as the results obtained by LLMmap."}, {"title": "6.1 Evaluation Setup", "content": "To train our inference models and evaluate the performance of LLMmap, we need to simulate a large number of applications that use different LLMs. This involves defining a set of LLMs to test (called the LLM universe L) and a set of possible prompting configurations (called the universe of possible prompting configurations S). The following section explains the choices we made for this simulation process.\nUniverse of LLMs: To evaluate LLMmap, we selected the 40 LLMs listed in Table 3. These models were chosen based on their popularity at the time of writing. We primarily use the Huggingface hub to select open-source models. We automatically retrieve the most popular models based on download counts by leveraging their API services. For closed-source models, we consider the three main models offered by the two most popular vendors (i.e., OpenAI and Anthropic) for which API access is available. Hereafter, we refer to these models as the LLM universe L.\nUniverse of prompting configurations: To enable LLMmap to accurately fingerprint an LLM across different settings, we need a method to simulate a large number of prompting configurations during the training phase of the inference model. We use a modular approach to define these prompting configurations by combining multiple elements. For each element, we create a pool of possible values. An prompting configuration is then generated by randomly selecting and combining elements from these pools. Specifically, we define an prompting configuration as a combination of three components:\n1. Hyper-Parameters for Sampling Procedure: We parametrize the sampling procedure by two hyper- parameters: temperature and frequency_penalty, in the range [0, 1] and [0.65, 1], respectively.\n2. System Prompt: We curated a collection of 60 different system prompts, which include prompts collected from online resources as well as automatically generated ones. Examples of system prompts are reported in Table 4 in Appendix A.\n3. Prompt Frameworks: We consider two settings: RAG and Chain-Of-Thought [11]. To simulate RAG, we create the input corpus by sampling 30 random entries from the dataset SQUAD 2.0 [8], and consider 6 prompt templates for retrieval-based-Q&A. Prompting frameworks are applied only in 20% of the cases.\nTo create disjoint sets of prompting configurations for training and evaluation, we randomly split the pool of each component into two disjoint sets of equal size. We then generate 1000 prompting configurations for training and evaluation. These sets are referred to as $S_{train}$ and $S_{test}$, respectively."}, {"title": "6.2 Results", "content": "Finally, in this section, we evaluate the performance of LLMmap, considering both the closed-set and open-set deployment of the inference model."}, {"title": "6.2.1 Closed-Set Classification Setting", "content": "Once the inference model has been trained, we test it using the 1K traces generated with the left-out prompting configurations in $S_{test}$. Given input traces generated by the target model, we use the closed-set classifier to infer the LLM that generated them from the list of LLMs in Table 3. On average, the model achieves an accuracy of 95.2% over the 40 LLMs. The confusion matrix in Figure 6 shows the accuracy for each model. The results indicate that LLMmap is generally robust across different models, correctly classifying 32 out of 40 LLMs with 95% accuracy or higher. This includes highly similar models, such as different instances of Google's Gemma or various versions of ChatGPT-4. The main exception is Meta's Llama-3-70B-Instruct, where our approach achieves only 72% accuracy. As shown in the confusion matrix, this lower accuracy is primarily due to misclassifications with closely related models, such as Smaug-Llama-3-70B-Instruct by Abacus.AI, which is a fine-tuned version of the original model.\nFingerprint Accuracy as a Function of Number of Queries: Naturally, the accuracy of fingerprinting depends on the number of queries made to the target. To balance context-specific needs, an attacker might reduce the number of interactions with the target application, but this typically results in decreased fingerprinting accuracy. This tradeoff is illustrated in Figure 5, where accuracy is plotted against the number of traces provided as input to the inference model."}, {"title": "6.2.2 Open-Set Classification Setting", "content": "Implementing fingerprinting with the open-set model involves two main steps.\nDerive a Database of Templates: After training the inference model, we derive a vectorial representation v for all traces in the training set by providing them as input to the inference model (see Section 5). We group all the vectors associated with traces generated by the same LLM and average them into a single vector. This results in 40 vectors, each representing a template for an LLM in Table 3. We refer to this list of vectors as DB. These vectors, after dimensional reduction, are depicted in Figure 7.\nInference Phase: Given traces $T^?$ generated by an unknown model, inference proceeds as follows: (1) We provide $T^?$ to the inference model and derive a vector $v^?$. (2) We compute the cosine similarity between $v^?$ and all the vectors in DB. (3) We output the LLM whose template has the highest similarity to $v^?$ as the prediction.\nUsing this approach, we evaluate the performance of the open-set inference model against our test set. Fingerprinting with the open-set inference model results in an average accuracy of 90%, which is 5% lower than the specialized closed-set classifier.\nFingerprinting New LLMs: The main advantage of the open-set fingerprinting approach is its ability to operate on LLMs that were not part of the training set. To do this, we collect one or more traces for a new LLM and derive its template to add to DB. The inference process remains unchanged. To evaluate the performance of the open-set inference model in fingerprinting new LLMs, we proceed as follows. Given the list of models in Table 1, (1) we remove an LLM (referred to as LLMout) and (2) train the inference model on the traces generated by the remaining 39 LLMs. (3) We then test the inference model to correctly recognize LLMout using the fingerprinting process described above. We repeat this process for each model in a k-fold cross-validation fashion.\nOn average, the inference model correctly identifies the unseen LLM with 81.1% accuracy. Individual results are reported in Figure 8. In this setting, predictions tend to be less robust and have higher variance overall; certain models are recognized with perfect accuracy, while others, such as Mistral-7b-v0.3, are recognized only 50% of the time. Nonetheless, the average accuracy remains meaningfully high.\nWe emphasize that once traces for a new model are collected, they can be added to the training set of the closed-set model to achieve more robust predictions. Given the small size of the model, retraining requires less than 10 minutes on a single GPU. Furthermore, the closed and open-set models can operate together without increasing the number of queries to the target application; once the traces are collected, they can be provided to both models, and their predictions can be combined for more robust inference."}, {"title": "7 Conclusion & Future Work", "content": "We introduce LLMmap, an effective and lightweight tool for fingerprinting LLMs deployed in LLM-integrated applications. While model fingerprinting is a crucial step in the information-gathering phase of AI red teaming operations, many other relevant details about a deployed LLM can also be inferred.\nThe LLMmap framework can be easily extended to support additional functionalities such as:\n1. Function Calls Enumeration: Identify all extra features that the LLM can use, such as web access or database queries.\n2. Prompting Framework Inference: Determine if the LLM is using a specific method to structure its responses, like RAG or ReACT.\n3. Fine-Tuning Inference: Detect if the LLM has been specially trained on additional data for specific tasks.\n4. Hyper-Parameters Inference: Infer the hyper-parameters being used by the deployed LLM, such as temperature and sampling regulations.\nOur future efforts will focus on implementing these functionalities within the LLMmap framework and making them available to the community."}]}