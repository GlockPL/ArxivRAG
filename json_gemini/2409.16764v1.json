{"title": "Offline and Distributional Reinforcement Learning for Radio Resource Management", "authors": ["Eslam Eldeeb", "Hirley Alves"], "abstract": "Reinforcement learning (RL) has proved to have a promising role in future intelligent wireless networks. Online RL has been adopted for radio resource management (RRM), taking over traditional schemes. However, due to its reliance on online interaction with the environment, its role becomes limited in practical, real-world problems where online interaction is not feasible. In addition, traditional RL stands short in front of the uncertainties and risks in real-world stochastic environments. In this manner, we propose an offline and distributional RL scheme for the RRM problem, enabling offline training using a static dataset without any interaction with the environment and considering the sources of uncertainties using the distributions of the return. Simulation results demonstrate that the proposed scheme outperforms conventional resource management models. In addition, it is the only scheme that surpasses online RL and achieves a 16% gain over online RL.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances towards 6G networks include complex real-world wireless environments requiring continuous monitoring and control. Such advancements urge the need for new learning-based methods for modeling and control. Recently, reinforcement learning (RL) has become attractive to the wireless domain due to its outstanding ability to provide model-free decision-making [1]. In online RL, an agent observes the current state of the environment, takes a decision (action), transits to a new state, receives a feedback reward evaluating the decision, and improves its policy\u00b9. To this end, deploying RL to radio resource management (RRM) problems is appealing due to their complex optimization objectives and the settled monitoring of their environments through feedback signals [2].\nSeveral works have exploited RL techniques in the RRM problem [3]\u2013[7]. For instance, the work in [3] proposes a power allocation and resource management scheme using deep RL, achieving a high average sum rate in different user densities. The authors in [4] leverage RL to perform a distributed resource scheduling while minimizing the on-grid energy consumption. In [5], a multi-agent RL (MARL) solution to the RRM is presented, whereas a graph neural network (GNN) architecture is proposed to solve the RRM problem in [6]. In [7], the authors propose a resource management algorithm in network slicing using a combination of generative adversarial networks (GANS) and deep RL. All in all, the majority of the literature, if not all, focused on online RL while solving the RRM problem.\nAlthough online RL contributes greatly to solving RRM, it faces serious challenges when transferred to real-world problems. Online RL heavily relies on exploring the environment, which is a random exploration at the beginning of the optimization, through online interaction with the environments. Traditional online RL faces four main obstacles:\n1) random exploration at the beginning of the optimization introduces poor service to the users;\n2) random exploration wastes time and resources. In addition, it might not be safe to interact randomly with the environment;\n3) in complex applications, such as RRM, online RL needs a huge amount of interaction and takes long periods to converge, which might not be practically feasible; and\n4) traditional RL only considers the average performance, neglecting sources of uncertainties and risks.\nThese challenges motivate the importance of offline RL and distributional RL. The former suggests optimizing the optimum policy using a static dataset without any interaction with the environment [8]. In contrast, the latter considers the environment uncertainties relying on the return distribution instead of the average return [9].\nThis work proposes a novel offline and distributional RL algorithm for the RRM problem. In contrast, to [10], which applies the offline RL scheme to the RRM, comparing a mixture of datasets, we rely on combining both offline and distributional RL to overcome the stochastic behavior of the environment. To our knowledge, this is the first work to combine offline RL and distributional RL for the RRM problem. The contributions of this paper are summarized as follows:\n\u2022 We propose an offline and distributional RL solution for the RRM problem. The proposed model maximizes the combination of the weighted sum and tail rates.\n\u2022 We demonstrate the theoretical details of the proposed algorithm as well as practical implementation and dataset collection.\n\u2022 We compared the proposed model to multiple benchmark schemes and an online RL algorithm.\n\u2022 Numerical results show that the proposed offline and distributional RL scheme outperforms the state-of-the-"}, {"title": "II. SYSTEM MODEL", "content": "Consider a wireless network of area L \u00d7 L square meters that comprises N randomly deployed access points (APs) and M randomly deployed user equipment (UEs) as shown in Fig. 1. To ensure practical scenarios, the random deployment of APs and UEs are controlled by minimum AP-UE distance \\(d_0\\) and minimum AP-AP distance \\(d_1\\). Our simulator considers an episodic environment consisting of time slots \\(t \\in \\{1, 2, ..., T\\}\\), where T is the total number of slots in one episode. At each time slot t, each UE moves randomly in the network with a speed v(t). The APs and UEs locations are fixed during each episode.\nAt the beginning of each episode, user association occurs, where each UE is associated with one of the APs. User-association is performed according to the reference signal received power (RSRP) of each user, where UE m is associated to AP n, such that \\(n = \\arg \\max_{i} RSRP_{mi}\\), where \\(i \\in \\{1, 2, ..., N\\}\\). The RSRP is affected by the transmit power pt and the channel. The channel between UE n and AP m is characterized by indoor path loss, log-normal shadowing with standard deviation \\(\\sigma_{sh}\\), and short-term frequency-flat Rayleigh fading. According to 3GPP [11], the indoor path loss between UE m and AP n is calculated as\n\\(PL_{mn} = 15.3 + 37.6\\log(d_{mn}) + PL_o,\\)\n(1)\nwhere \\(d_{mn}\\) is the euclidean distance between UE m and AP n, \\(d_{mn} > d_0\\) and \\(PL_o\\) is a reference path loss. The total power loss is the addition of the path loss, shadowing, and Rayleigh fading.\nAt time t, the received signal of UE m that is associated with AP n is\n\\(Y_m(t) = h_{mn}(t)x_n(t) + \\sum_{i\\neq n}h_{mi}(t)x_i(t) + n_m(t),\\)\n(2)\nwhere \\(h_{mn}\\) is the channel between UE m and AP n and \\(n_m(t) \\sim \\mathcal{CN}(0, \\sigma^2)\\) is the additive white Gaussian noise (AWGN) with variance \\(\\sigma^2\\). At time t, the instantaneous rate (Shannon capacity) of UE m that is associated with AP n is\n\\(R_m(t) = \\log_2(1 + \\frac{|h_{mn}(t)|^2 P_t}{\\sum_{i\\neq n} |h_{mi}(t)|^2 P_t + \\sigma^2}).\\)\n(3)\nwhere the term \\(\\frac{|h_{mn}(t)|^2 P_t}{\\sum_{i\\neq n} |h_{mi}(t)|^2 P_t + \\sigma^2}\\) represents the signal-to-interference-plus-noise ratio (SINR) of UE m at time t. The average rate (throughput) of UE m in an episode is\n\\(\\bar{R}_m = \\frac{1}{T} \\sum_{t=1}^{T} R_m(t).\\)\n(4)\nRRM problems aim to maximize the average data rates across all users. However, this objective is trivial to be solved in a way that each AP always schedule the user with the best SINR. Therefore, fairness across users must be considered in the RRM objective. In this work, we consider joint optimization of the sum-rate and the 5-percentile rate. The former is calculated as\n\\(R_{sum} = \\sum_{m=1}^{M} R_m,\\)\n(5)\nwhere the latter is the average rate achieved by 95% of the UEs. The 5-percentile rate is calculated as\n\\(R_{5\\%} = \\max R\\)\ns.t. \\(P[R_m \\geq R] \\geq 0.95\\), \\(\\forall m \\in \\{1, 2, ..., M\\}.\\)\n(6)\nTo this end, the objective is a weighted sum (joint combination) of the sum rate and the 5-percentile rate\n\\(R_{score} = \\mu_1 R_{sum} + \\mu_2 R_{5\\%},\\)\n(7)\nwhere \\(\\mu_1\\) and \\(\\mu_2\\) are user-chosen weights."}, {"title": "A. Problem Definition", "content": "The objective in this work is user scheduling, i.e., which APs serve which users, to maximize the sum-rate and the 5-percentile rate combination (\\(R_{score}\\))\n\\(P1: \\max_{A(t)} \\sum_{t=1}^{T} R_{score}(t),\\)\n(8)\nwhere A(t) is the joint action of all APs, i.e., the scheduling policy of all APs. However, optimizing the \\(R_{score}\\) using (7) in the objective directly is hard to optimize and shows instability convergence [5]. Therefore, we adjust the objective using the proportional fairness (PF) factor [5]. The PF factor describes the priority of each UE and is calculated as\n\\(PF_m(t) = w_m(t) R_m(t),\\)\n(9)"}, {"title": "", "content": "where \\(w_m(t)\\) is a weighting factor calculated recursively as\n\\(w_m(t) = \\frac{1}{\\bar{R}_m(t)}\\)\n(10)\n\\(\\bar{R}_m(t) = \\eta R_m(t) + (1-\\eta)\\bar{R}_m(t-1),\\)\n(11)\nwhere \\(\\eta\\) is a step parameter and \\(\\bar{R}_m(0) = R_m(0)\\). The PF factor is inversely proportional to the long-term rate of the user, reflecting that the higher the PF factor for a user indicates its need to be served. Therefore, the objective in (8) is simplified as\n\\(P1: \\max_{A(t)} \\sum_{t=1}^{T} \\sum_{m=1}^{M} (\\lambda \\frac{w_m(t)}{\\bar{W}}). R_m(t),\\)\n(12)\nwhere \\(\\lambda \\in [0, 1]\\) controls the trade-off between the sum-rate and the 5-percentile rate."}, {"title": "III. REINFORCEMENT LEARNING", "content": "In this section, we formulate and solve the objective in (12) using online reinforcement learning."}, {"title": "A. Markov Decision Process", "content": "The RRM problem can be formulated as a Markov decision process (MDP). An MDP is characterized by the tuple \\((s_t, a_t, r_t, s_{t+1}, \\gamma)\\), where \\(s_t\\) is the current state, \\(a_t\\) is the current action, \\(r_t\\) is the reward received from taking action \\(a_t\\) at state \\(s_t\\) and transiting to the next state \\(s_{t+1}\\), and \\(\\gamma \\in [0, 1]\\) is the discount factor that controls how much future rewards are considered in the RL problem. For a more practical and general RL formulation, we limit the number of UEs each AP can select to K UEs. At the beginning of each episode, user association occurs for each AP, then, among the associated users, only the best K users (the highest K users in terms of the weighting factor \\(W_{mn}(0)\\) calculated from (10)) are included for selection. The detailed MDP in the RRM problem is as follows\n1) State: each AP can observe two components related to each device among the selected top K devices, the SINR measurement \\(SINR_{kn}(t)\\) and the weighting factor \\(w_{kn}(t)\\). For N available APs, the overall state of the system is\n\\(s_t = (SINR_{11}(t), w_{11}(t), ..., SINR_{K1}(t), w_{k1}(t), ...,\\)\n(13)\n\\(SINR_{1N}(t), w_{1N}(t), ..., SINR_{KN}(t), w_{k\\nu}(t)).\\)\nThe state space size is 2NK.\n2) Action: each AP schedules a resource to one device only among the top K devices at each time slot (or chooses to be silent and serve no UEs). The overall action space is the scheduled devices chosen by each AP, and its size is \\((K + 1)^N\\).\n3) Reward: since the objective in (8) is hard to optimize, the reward function is formulated using the objective in (12)\n\\(r_t = \\sum_{m=1}^{M} (\\frac{w_m(t)}{\\bar{W}}) R_m(t),\\)\n(14)"}, {"title": "B. Online RL", "content": "RL frameworks aim to find the optimum policy \\(\\pi^*\\) that maximizes the accumulative rewards. Recently, deep neural networks provide power RL algorithms, such as deep Q-network (DQN) [12], proximal policy optimization (PPO) [13], and soft actor-critic (SAC) [14], that can solve large dimension problems. We choose DQN as our online RL algorithm in this work due to its simplicity and stability [15]. In addition, it is straightforward to introduce our algorithm in the next section on top of a DQN algorithm. DQN is a model-free online RL algorithm that uses a neural network to estimate the Q-function. In addition, it is an off-policy algorithm, where previous experiences saved (in a buffer called replay memory) from previous policies are sampled to update the current Q-function.\nTo find the optimum Q-function, DQN updates the Bellman optimality equation by minimizing the loss\n\\(L_{DQN} = \\mathbb{E} [(r + \\gamma \\max_{a'} Q_{\\theta}(s', a') - Q_{\\theta}(s, a))^2]\\)\n(15)\nwhere E is the average taken over the sampled experiences from the replay memory, \\(\\gamma\\) is the discount factor, \\(g\\) is the gradient step (iteration), s' is the next state and a' is the action to be chosen at s'. The Q-function \\(Q(s, a)\\) is modeled using a neural network with parameters \\(\\theta\\). The main challenge in online RL is the need for continuous online interaction with the environment, which might not be feasible or safe. Next, we present offline and distributional RL as practical alternatives."}, {"title": "IV. OFFLINE AND DISTRIBUTIONAL RL", "content": "This section presents the proposed offline and distributional RL algorithm to solve the objective in (12) offline."}, {"title": "A. Offline RL", "content": "Offline RL resorts to a static offline dataset without any online interaction with the environment. The dataset is collected using a behavioral policy, an online learning agent, a baseline policy, or even a random one. Note that, for the RRM problem, it has been proved in [10] that the dataset's quality heavily affects the convergence of offline RL algorithms. Deploying traditional deep RL algorithms offline fails to converge due to the distributional shift between the actions seen in the dataset and the learned actions [8]. Conservative Q-learning (CQL) [16] is a well-known offline RL algorithm that adds a regularization term (conservative parameter) to the Bellman update, overcoming the overestimation problem from the out-of-distribution (OOD) actions learned compared to those in the dataset.\nBuilding the CQL algorithm on top of DQN architecture is straightforward, where the CQL loss is calculated as\n\\(L_{CQL} = L_{DQN} + \\alpha \\mathbb{E} [\\frac{1}{\\mathcal{A}} \\log \\sum_{\\tilde{a}} \\exp (Q(s, \\tilde{a})) - Q(s, a)]\\)\n(16)"}, {"title": "B. Distributional RL", "content": "Distributional RL is a variant of RL that uses the distribution over return instead of the average return while optimizing the optimum policy [9]. Quantile-regression DQN (QR-DQN) [17] is a distributional RL algorithm that estimates the return distributions Z(s, a) using I fixed dirac delta functions. In QR-DQN, the output layer of the neural network has a size of the number of actions times the number of quantiles I. The distributional Bellman loss is calculated as\n\\(L_{QR-DQN} = \\frac{1}{I^2} \\sum_{j=1}^{I} \\sum_{j'=1}^{I} \\rho_{\\tau_j}(\\mathcal{T}^\\pi \\theta_{j'}(s', a') - \\theta_j(s, a)),\\)\n(17)\nwhere \\(\\theta_j(s, a)\\) is an estimate of the quantile inverse PDF of the return distribution and \\(\\rho_\\tau\\) is the quantile regression Huber loss [17]."}, {"title": "C. Conservative Quantile Regression", "content": "Conservative quantile regression (CQR) [18] is a variant of RL algorithms that combines CQL with QR-DQN, where the optimum Q-function is optimized offline using distributional RL. The CQR loss function is formulated as follows\n\\(L_{CQR} = L_{QR-DQN} \\+\n\\alpha \\mathbb{E} [\\frac{1}{I} \\sum_{j=1}^{I} \\log \\sum_{\\tilde{a}} \\exp (\\theta_j (s, \\tilde{a})) - \\theta_j(s, a)\\)\n(18)"}, {"title": "Algorithm 1: Conservative quantile regression algorithm for the RRM problem.", "content": "1 Define number of APs N, number of UEs M, number of best weighting factor users K, discount factor \\(\\gamma\\), learning rate \\(\\varsigma\\), number of quantiles I, conservative penalty constant \\(\\alpha\\), number of training epochs E, number of gradient steps G, offline dataset D, input layer size 2NK, and output layer size \\((K + 1)^N I\\)\n2 Initialize network parameters\n3 for epoch e in \\(\\{1, ..., E\\}\\) do\n4  for gradient step g in \\(\\{1, ..., G\\}\\) do\n5   Sample a batch B from the dataset D\n6   Estimate the CQR loss \\(L_{CQR}\\) in (18)\n7   Perform a stochastic gradient step based on the estimated loss with a learning rate \\(\\varsigma\\)\n8  end\n9 end\n10 Return \\(\\{\\theta_j(s, a)\\}_{j=1}^I\\)"}, {"title": "A. Baseline Schemes", "content": "We compare the proposed algorithm to some of the state-of-the-art baseline methods:\n\u2022 Random-walk: each AP randomly chooses one of the top K UEs to serve at each time slot.\n\u2022 Full-reuse: each AP chooses the user with the highest PF ratio among the top K UEs at each time slot.\n\u2022 Time-division multiplexing: each AP serves the top K UEs in a round-robin fashion. This scheme prioritizes fairness among users.\n\u2022 Information-theoretic link scheduling (ITLinQ): each AP prioritizes its associated UEs according to their PF ratios. Afterward, each AP goes through an interference tolerance check for each UE to make sure the interference level is lower than a certain threshold MSNR. If no UEs associated with the AP passes the interference tolerance check, this AP is turned off. This method proved to reach sub-optimal performance in the literature [19]."}, {"title": "B. Simulation Parameters and Dataset", "content": "We consider an environment of size 50 m \u00d7 50 m, N = 3 APs and M = 15 UEs. For the online RL, we build a DQN, where the neural network has 2 hidden layers with 256 neurons each. Each episode consists of T = 2000 time steps. We collect the offline dataset using a behavioral policy from an online DQN agent. In other words, we use the last 20% of the transitions of training an online DQN agent. We simulate a single NVIDIA Tesla V100 GPU using Pytorch framework. All the simulation parameters are shown in Table I."}, {"title": "C. Simulation Results", "content": "Fig. 3 demonstrates the convergence of online RL (DQN) as a function of training episodes. We first observe that the random scheme has the worst \\(R_{score}\\), while TDM and full-reuse show close \\(R_{score}\\) values. The sub-optimal scheme ITLinQ has the highest \\(R_{score}\\) among all the baseline methods. Moreover, DQN reaches convergence (\\(R_{score}\\) = 2.6) after around 150 episodes. It outperforms all the baseline schemes, including the sub-optimal scheme ITLinQ, by 20%.\nIn Fig. 4, we report the convergence of the proposed offline and distributional CQR algorithm compared to multiple offline/distributional RL algorithms, namely, CQL, DQN (in an offline manner), and QR-DQN. This figure also illustrates the online scheme (after convergence) and the baseline methods. Although DQN and QR-DQN achieve higher \\(R_{score}\\) than TDM and full-reuse, they fail to converge. In contrast, CQL surpasses the sub-optimal ITLinQ; however, it fails to reach the \\(R_{score}\\) as the online RL after convergence. Finally, the proposed CQR algorithm is the only one that can reach convergence offline and outperform the online RL.\nFig. 5 shows the performance of the proposed CQR algorithm compared to other baseline offline RL methods during testing after full training. It reports the sum-rate, 5-percentile rate, and \\(R_{score}\\) using an offline dataset contains 20% of the experience of an online DQN (Fig.5a-5c) and using an offline dataset contains 10% of the experience of an online DQN (Fig.5d-5f), respectively. Regardless of the size of the offline dataset, the CQR algorithm outperforms other offline RL schemes. In addition, the size of the dataset slightly affects the performance of CQR as the rates are higher with a larger dataset, which is often better in quality because it comes from the last experiences seen by a DQN agent, usually good experiences. In addition, a significant gap is recorded between the proposed CQR algorithm and other offline schemes with smaller datasets. This highlights that the proposed model requires less data to achieve reasonable rates than other offline RL methods."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we developed a novel offline and distributional RL algorithm for RRM. First, we formulated the problem as MDP and then introduced the practical limitations of online RL. Afterward, we introduced the proposed model using a combination of CQL and QR-DQN. Simulation results show that the proposed model achieved a higher \\(R_{score}\\) than all the baseline schemes. In addition, it is the only scheme to surpass online RL with a 20% gain in terms of the \\(R_{score}\\). Investigating the RRM problem using offline and distributional multi-agent RL is left for future work."}]}