{"title": "SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS", "authors": ["Jianwei Li", "Jung-Eun Kim"], "abstract": "As large language models (LLMs) are overwhelmingly more and more integrated\ninto various applications, ensuring they generate safe and aligned responses is a\npressing need. Previous research on alignment has largely focused on general\ninstruction-following but has often overlooked the unique properties and chal-\nlenges of safety alignment, such as the brittleness of safety mechanisms. To bridge\nthe gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which\nposits that safety alignment should teach an otherwise unsafe model to choose the\ncorrect reasoning direction - interpreted as a specialized binary classification task\n- and incorporate a refusal mechanism with multiple reserved fallback options.\nFurthermore, through SSAH, we hypothesize that safety guardrails in LLMs can\nbe established by just a small number of essential components. To verify this,\nwe conduct an ablation study and successfully identify four types of attribute-\ncritical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Ex-\nclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our\nfindings show that freezing certain safety-critical components (7.5%) during fine-\ntuning allows the model to retain its safety attributes while adapting to new tasks.\nAdditionally, we show that leveraging redundant units (20%) in the pre-trained\nmodel as an \"alignment budget\" can effectively minimize the alignment tax while\nachieving the alignment goal. All considered, this paper concludes that the atomic\nfunctional unit for safety in LLMs is at the neuron level and underscores that\nsafety alignment should not be complicated. We believe this work contributes to\nthe foundation of efficient and scalable safety alignment for future LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are demonstrating remarkable capabilities across a broad spectrum\nof natural language tasks, ranging from text generation to answering complex questions (Achiam\net al., 2023; Touvron et al., 2023a;b; Dubey et al., 2024). However, as these models are increasingly\nintegrated into real-world applications, concerns about the risk of generating harmful, unsafe, or\nunethical content have grown Askell et al. (2021); Bai et al. (2022); Zeng et al. (2024). This has led\nto a pressing need for safety alignment, which ensures that LLM outputs are not only coherent and\ninformative but also aligned with human values, ethical standards, and safety considerations.\nPrevious research on alignment has primarily focused on enhancing LLMs' ability to follow general\ninstructions without enough attention to model safety. This trend of treating safety alignment as a\nsubset of general alignment has obscured its distinct challenges (Ouyang et al., 2022; Rafailov et al.,\n2024; Zhou et al., 2024; Liu et al., 2023a; Yuan et al., 2023; Liu et al., 2023b). One major issue is\nthe brittleness of current safety mechanisms. Despite using benign data during model fine-tuning,"}, {"title": "2 RELATED WORK", "content": "Alignment and Safety Alignment: Alignment research in LLMs aims to ensure that models fol-\nlow human instructions and align with human preferences across various tasks. Early work, such\nas Askell et al. (2021); Bai et al. (2022), focused on enabling LLMs to \"Follow instructions and\nbe helpful, truthful, and harmless\" throughout the alignment process. Various alignment strategies\nhave since been explored (Wang et al., 2024), including Supervised Fine-Tuning (SFT) (Taori et al.,\n2023; Zhou et al., 2024), Reinforcement Learning with Human Feedback (RLHF) or AI Feedback\n(RLAIF) (Ouyang et al., 2022; Lee et al.), Instruction Tuning (Wei et al., 2021), Contrastive Learn-\ning (Rafailov et al., 2024; Xu et al., 2024), and Conditional Learning (Korbak et al., 2023). How-\never, researchers have realized that achieving helpfulness, truthfulness, and harmlessness presents\ndistinct challenges. More recent work has, therefore, shifted focus specifically toward the challenge\nof harmlessness, leading to an increasing emphasis on safety alignment-ensuring models avoid\nharmful outputs while maintaining utility Wei et al. (2024b); Qi et al. (2023).\nAlignment Tax and Fine-tuning Attack: The process of aligning LLMs with human preferences\noften incurs an \u201calignment tax\u201d, where models experience degraded performance on downstream\ntasks due to the trade-offs required to maintain alignment (Bai et al., 2022; Ouyang et al., 2022; Lin\net al., 2024; Wang et al., 2024). Additionally, fine-tuning attacks present another challenge: Yang\net al. (2023); Qi et al. (2023)have shown that fine-tuning LLMs, even with benign data, can weaken\nsafety measures. These findings highlight the inherent tension between safety alignment and utility,\nwhere improvements in one area often come at the expense of the other.\nModel Pruning: Model pruning is a technique that reduces model size by removing redun-\ndant parameters, neurons, channels, layers, etc., which decreases storage needs and computa-\ntional complexity without substantially impacting performance, namely magnitude pruning, unstruc-\ntured/structured pruning, etc Frantar & Alistarh (2022); Frankle et al. (2020); Anwar et al. (2017);\nAn et al. (2024); Li et al. (2024); Molchanov et al. (2019); Han et al. (2015); Lee et al. (2019);\nRenda et al. (2020). This method identifies and removes parts of the model that contribute least to\nits function, such as model weights with small magnitudes. In doing so, pruning extracts an efficient\nsub-model that runs faster on resource-constrained devices. We employ pruning as a tool to find out\nand delineate components to contribute to safety, utility, and both, respectively."}, {"title": "3 SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS (SSAH)", "content": "Previous research proposed Superficial Alignment Hypothesis (SAH): A model's knowledge and\ncapabilities are learned almost entirely during pretraining, while alignment teaches the model which\nsubdistribution of formats should be used when interacting with users (Zhou et al., 2024).\nHowever, this claim is centered on general alignment, and directly validating the hypothesis is chal-\nlenging due to the complex interplays between pretraining and alignment. When a model fails to\nfulfill a user's request, it can be difficult to determine whether the issue stems from the pretraining\nstage (due to lack of sufficient knowledge) or from the alignment process (due to misalignment in the\noutput format). For example, when a model struggles with solving a math problem, it could either\nbe a lack of relevant mathematical knowledge or the inability to structure its reasoning effectively.\nIn such cases, good instruction techniques like the Chain-of-Thought approach can significantly\nenhance the quality of the model's responses (Wei et al., 2022).\nSuperficial Safety Alignment Hypothesis (SSAH). Since our focus is specifically on safety align-\nment, which has distinct properties compared to general alignment, we carefully define the scope of\nour hypothesis. A key observation here is that, for a model to be able to fulfill a malicious request, it\nmust already possess the necessary knowledge and reasoning ability to carry out that harmful action.\nBased on this observation, we propose Superficial Safety Alignment Hypothesis (SSAH):"}, {"title": "4 LESS IS MORE FOR SAFETY ALIGNMENT", "content": "Based on the Superficial Safety Alignment Hypothesis (SSAH), we posit that safety alignment only\nneeds to teach the model the correct reasoning direction - either fulfilling or refusing a request - and\nto equip it with a standard refusal mechanism. This leads to the insight that safety alignment can\nbe achieved using only a small subset of critical computing units, as the task can be interpreted\nas a binary classification combined with a multi-selection task.\n4.1 IDENTIFYING SAFETY-CRITICAL COMPUTING UNITS\nTo verify this corollary, we designed experiments to determine the minimally essential subset of\ncomputing units in a large language model that is critical in establishing a safety guardrail. Follow-\ning this line of reasoning, we hypothesize that specific attributes of LLMs can be explicitly linked\nto certain computing units within the model. Our experiments are designed as follows:\nDefinition of Attribute Groups. This paper categorizes the attributes of LLMs into two main\nproperties: utility and safety. Following the above hypothesis, we first exclusively link safety or util-\nity attributes to specific computing units. We also speculate that some units may contribute to both\nattributes simultaneously. Moreover, considering that many components in LLMs are redundant,\nwe also hypothesize that certain computing units do not correlate with any attribute. Therefore, we\ndivide the computing units of LLMs into four groups: Exclusive Safety Units (ESU) and Exclusive\nUtility Units (EUU), which are linked exclusively to either safety and utility, respectively; Complex\nUnits (CU), which concurrently contribute to both safety and utility attributes; and Redundant Units\n(RU), which are not associated with any attribute.\nVerfication of Attribute Group. To verify our hypothesis that different groups of computing\nunits contribute exclusively, collectively, or neither to safety and utility attributes, we use a model\npruning mechanism. The rationale behind pruning is that removing components most closely linked\nto a specific attribute would significantly impact the model's performance in that area - it is a sort\nof ablation study. As pruning reduces the model's capacity, the most affected attributes reveal the\ncritical components for that function.\nFollowing Wei et al. (2024b), we construct two datasets to separately evaluate the model's perfor-\nmance on utility and safety. The utility dataset measures the model's functional capabilities (e.g.,\ngeneral reasoning, language understanding), while the safety dataset evaluates its ability to reject\nharmful or unethical queries. This allows us to identify the computing units most closely associated\nwith utility and safety, respectively. Unlike previous approaches that identify safety-critical com-\nponents at the weight level, we identify them at the neuron level, focusing on individual neurons"}, {"title": "4.2 WHY IS SAFETY BRITTLE?", "content": "Attribute Transfer Analysis in the Fine-Tuning Process. Previous research has shown that\nadapting safety-aligned LLMs to new tasks can often hurt their safety performance. Therefore, it is"}, {"title": "5 DISCUSSION, LIMITATION, AND CONCLUSION", "content": "Discussion. While our SSAH offers valuable insights into adversarial scenarios, such as jailbreak\nattacks, we do not propose a specific solution to address these issues in this work. If these issues\ncould be resolved within the framework of our theory, the term \u201cSuperficial\u201d in \u201cSuperficial Safety\nAlignment Hypothesis\u201d may no longer be necessary. Interestingly, recent research provides some\nsupporting evidence in this direction (Qi et al., 2024). However, it is also highly likely that ad-\nvanced attacks may not be fully mitigated by relying solely on the model's internal mechanisms. A\nsystematic, multi-layered approach, extending beyond the model itself, may be required to effec-\ntively defend against sophisticated adversarial threats.\nLimitation. When reallocating redundant units for safety purposes, we only explored the impact\nof the alignment method SFT. Due to resource limitations, we have not yet tested this approach on\nother alignment methods like PPO or DPO.\nConclusion. This paper distinguishes safety alignment from the general alignment in LLMs and\naddresses the three key questions: How does safety alignment affect model behavior? Why are\nsafety mechanisms brittle? and How to mitigate the safety alignment tax? By answering these\nquestions, we were able to demonstrate that safety alignment can be a straightforward process,\nrather than a myth."}, {"title": "A APPENDIX: SUPERFICIAL SAFETY ALIGNMENT HYPOTHESIS", "content": "In this section, we provide additional technical details and clarifications to supplement the experi-\nments and findings presented in the Superficial Safety Alignment Hypothesis (SSAH) section. These\ndetails help ensure reproducibility and offer deeper insights into how the Superficial Alignment Hy-\npothesis (SAH) was adapted to focus on safety-specific concerns. We also explain the methodology\nbehind model configuration, fine-tuning, and evaluation. This appendix includes further discus-\nsion on how general instruction-following models and safety-aligned models were fine-tuned and\nassessed to probe their reasoning directions when faced with malicious queries, and the results of\nthese assessments are presented in detail.\nA.1 SUPERFICIAL ALIGNMENT HYPOTHESIS IN LIMA\nThe Superficial Alignment Hypothesis (SAH), as proposed to Zhou et al. (2024), fundamentally\nchallenges the traditional assumption that a language model requires extensive fine-tuning on\ninstruction-following on preference data to align its responses with human expectation. Instead,\nSAH posits that the majority of a model's knowledge and capabilities are acquired during the pre-\ntraining phase, while the subsequent alignment phase primarily functions to guide the model's output\nformat when interacting with users. This hypothesis implies that, for many tasks, fine-tuning on a\nsmall, carefully selected set of aligned data is sufficient to achieve strong performance as long as the\npretraining stage has effectively captured the necessary underlying knowledge. The key assertion of\nSAH is that alignment is superficial, in the sense that:\n(1) Capabilities are Learned in Pretraining: During pretraining, the model acquires a vast\namount of general-purpose knowledge from diverse datasets. These datasets contain im-\nplicit structures and information about language, reasoning, factual knowledge, and even\nethical guidelines.\n(2) Alignment Guides Output Behavior: The alignment process is not responsible for teaching\nthe model new knowledge or capabilities. Rather, it acts as a filter that directs the model\nto produce acceptable formats or styles of responses based on user queries, reflecting the\ncorrect subset of its vast pretrained knowledge.\n(3) For instance, when tasked with generating an informative response, the model must select\na format that aligns with user expectations, such as providing clear instructions or expla-\nnations. However, the actual content of the response, e.g., factual knowledge, reasoning,\nand domain-specific expertise, stems from pretraining. The alignment stage merely teaches\nthe model how to express that knowledge or when to refrain from providing information in\ninappropriate contexts.\nChallenges and Motivations Behind SAH. One of the primary motivations for introducing SAH\nwas the observation that models tend to be capable of performing certain tasks after alignment fine-\ntuning on a minimal dataset. This observation challenges the need for extensive fine-tuning using\nreinforcement learning (e.g., RLHF) or large-scale human feedback, which can be computationally\ntoo expensive and time-consuming. The authors of LiMA argue that most of the functional capa-\nbilities of a language model are already present after pretraining, and that alignment is more about\nconditioning the model to apply these capabilities in a user-friendly way.\nThe Superficial Alignment Hypothesis can also help explain phenomena where models exhibit brit-\ntleness - for example, where an LLM generates inappropriate or harmful responses in new domains\nor under adversarial conditions. This brittleness is attributed to the fact that alignment does not\ndeeply alter the underlying decision-making processes of the model, but only skims the surface to\nadjust output behavior in specific contexts. Therefore, if an adversary finds a way to bypass these\nsuperficial alignments (e.g., via jailbreaking), the model's underlying pretrained knowledge and ca-\npabilities may still enable it to produce harmful or misaligned responses."}, {"title": "A.2 MODEL CONFIGURATION AND TRAINING DETAILS", "content": "In our probe experiments, we explore the reasoning direction differences between unsafety-aligned\nmodels and safety-aligned models across several popular LLaMA families, including LLaMA2,\nLLaMA3, and LLaMA3.1 (Fig. 7 describes more probing results on the HEx-PHI dataset). These\nmodels offer diverse pretrained knowledge and capabilities, allowing us to investigate how safety\nalignment affects model behavior when responding to malicious queries. To isolate the impact\nof reasoning direction when facing unsafe inputs, it is crucial to control for other confounding\nfactors. Existing open-source instruction-following models are typically both helpful and safe, while\npretrained open-source models without safety alignment are neither helpful nor safe. This dichotomy\npresents a challenge in disentangling the effect of general instruction-following capabilities from\nsafety-specific behaviors.\nThus, for each LLaMA variant (LLaMA2, LLaMA3, LLaMA3.1), we fine-tuned two separate mod-\nels using Supervised Fine-Tuning (SFT):\n(1) A General Instruction-Following Model that is trained to follow human instructions but\nwithout any explicit safety mechanisms. This model helps us evaluate how a model with\ninstruction-following capabilities but without safety guardrails reacts to malicious queries."}, {"title": "Supervised Fine-Tuning Process and Configuration.", "content": "We follow the alignment method outlined\nin the Zhou et al. (2024), which uses Supervised Fine-Tuning (SFT). For the general instruction-\nfollowing models, we employed the LIMA dataset, which includes over 1000 instruction-following\nexamples. However, we removed 13 safety-related examples to avoid conflating safety concerns with\ngeneral instruction-following abilities. The filtering process was assisted by GPT-4, following a set\nof instructions specifically designed to identify and exclude safety-related tasks. For the safety-\naligned models, we used the Alert dataset, which contains a variety of safety-critical instructions\nto teach the model how to respond safely to malicious queries.\nFor all models, we followed a consistent training configuration across the LLaMA2, LLaMA3, and\nLLaMA3.1 versions to ensure comparable results. The general instruction-following models were\ntrained on the LIMA dataset (with safety-related data removed). The fine-tuning was performed\nusing the following key parameters:\n\u2022 Batch size: We set the batch size per device to 4, with gradient accumulation steps of 6 on 3\nNVIDIA A6000 GPU, which gave us an effective batch size of 72.\n\u2022 Learning rate: The learning rate was set to 1.0e-5.\n\u2022 Epochs: The fine-tuning was conducted for 15 epochs.\n\u2022 Precision: BF16 precision was used to optimize memory usage.\n\u2022 Optimization: AdamW optimizer with B\u2081 = 0.9, \u03b22 = 0.95, and a weight decay of 0.1.\n\u2022 Learning rate scheduler: We employed a linear scheduler with no warm-up steps.\n\u2022 Seed: A random seed of 42 was used for reproducibility.\nFor the safety-aligned models, we fine-tuned the instruction-following models (trained on the LIMA\ndataset) further using the Alert dataset. The key fine-tuning parameters for this stage were as\nfollows:\n\u2022 Model initialization: We initialized the model from the previously fine-tuned general\ninstruction-following model.\n\u2022 Batch size: As with the general instruction model, we used a batch size of 4 with gradient\naccumulation steps of 6 on 3 NVIDIA A6000 GPU."}, {"title": "Model Evaluation and Validation.", "content": "To ensure that the two types of models we trained (i.e., gen-\neral instruction-following models and safety-aligned models) meet the desired criteria, we conducted\nthorough evaluations of both their instruction-following abilities (helpfulness) and their safety per-\nformance (harmfulness).\nFor the instruction-following ability, we evaluated the helpfulness of the model using the MT-Bench\nbenchmark (Zheng et al., 2023), which assesses the general utility and coherence of model responses\nacross a wide range of tasks. Importantly, we use GPT-4 as a judge to evaluate the helpfulness of the\nmodel's generated responses. Specifically, GPT-4 was used to compare the outputs of our trained\nmodels against standard task prompts in MT-Bench and assign scores based on response quality,\nrelevance, and overall helpfulness. To evaluate the safety performance of the models, we employed\ntwo complementary benchmarks: Adv-bench and HEx-PHI. These benchmarks were chosen to\ncomprehensively assess the models' ability to handle malicious or unsafe queries. To save space,\nplease refer to Sec. B.5 for more details about these two datasets and the corresponding metrics."}, {"title": "Results and Analysis", "content": "Table 5 presents the evaluation results for the different models across help-\nfulness and safety dimensions. As expected, the general instruction-following model performed\nwell in terms of helpfulness, as assessed by MT-Bench. However, it exhibited a significantly higher\nAttack Success Rate (ASR) in AdvBench and received high danger scores in the HEx-PHI bench-\nmark. These findings confirm that while general instruction-following models can accurately follow\nuser instructions, they fail to reject malicious or harmful requests, highlighting the absence of ro-\nbust safety mechanisms. In contrast, the safety-aligned model maintained comparable performance\nin helpfulness while demonstrating significantly better safety performance. These models showed\na much lower ASR in AdvBench and a lower danger score in HEx-PHI, reflecting their enhanced\nability to reject adversarial and harmful inputs."}, {"title": "B APPENDIX: LESS IS MORE FOR SAFETY ALIGNMENT", "content": "In this section, we provide additional technical details and clarifications to supplement the experi-\nments and findings presented in the Less is More for Safety Alignment section. These details will\nhelp ensure reproducibility and offer a deeper understanding of the methodology behind identify-\ning safety-critical units, attribute transfer analysis, and the use of redundant units as an alignment\nbudget.\nB.1 DEFINITION OF ATTRIBUTE GROUPS AND CATEGORIZATION PROCESS\nAs detailed in Section 4.1, we categorize the computational units (neurons and channels) of LLMs\ninto four distinct groups: Exclusive Safety Units (ESU), Exclusive Utility Units (EUU), Complex\nUnits (CU), and Redundant Units (RU). Exclusive Safety Units are primarily responsible for safety-\nrelated behavior, such as refusal mechanisms and detecting unsafe requests. Exclusive Utility Units\nare dedicated to general task performance, including natural language understanding, reasoning, and\ntask-specific knowledge retrieval. Complex Units contribute to both safety and utility, as these at-\ntributes are intertwined at a higher level of abstraction. Finally, Redundant Units are not significantly\ninvolved in either safety or utility and are often characterized by low activation variance across tasks.\nTo systematically assign computing units to these groups, we employ a structured pruning strategy\nbased on the variance of activation values. Specifically, we calculate the variance of activations\nacross a target dataset for each neuron or channel. Neurons with higher variance contribute more\nsignificantly to the model's performance on a given task, while neurons with low activation vari-\nance are considered redundant and can be pruned. We define two separate importance scores for\neach unit-Iu for utility-related tasks and Is for safety-related tasks. Units with extreme values in\neither dimension are considered Exclusive Units (either ESU or EUU), while units with significant\ncontributions to both dimensions are classified as Complex Units (CU).\nDatasets Used for Computing Iu and Is. To identify safety-critical regions in the model, we fol-\nlow Wei et al. (2024b) to prepare two types of datasets: safety dataset, for attributing safety-related\nbehaviors, and utility dataset, for attributing utility-related behaviors. Each dataset is structured in\na (prompt, response) format. Specifically, the safety dataset is compiled using harmful instructions\nfrom AdvBench (Zou et al., 2023a). We also divide AdvBench into AdvBencheval (100 instructions\nfor evaluation) and AdvBenchattr (420 instructions for attribution). We prompt Llama2-7B-chat with\nAdvBenchattr, collecting responses that refrain from following harmful instructions. For the utility\ndataset, we filter out safety-related (prompt, response) pairs using sensitive phrase matching (Qi\net al., 2023) from Alpaca-Cleaned, a refined version of the Alpaca dataset (Taori et al., 2023).\nBy performing structured pruning at various ratios and evaluating the impact on both utility and\nsafety performance, we can accurately categorize the model's computing units. The pruning process\ninvolves removing the least critical units and measuring performance degradation, ensuring that our\nattribution of units is aligned with their actual contribution to model behavior."}, {"title": "B.2 EVALUATING THE IMPACT ON BOTH UTILITY AND SAFETY PERFORMANCE", "content": "To evaluate the impact of pruning on both utility and safety performance, we measure the model's\nperformance using established benchmarks for both attributes. Our approach closely follows the\nmethods used by Sun et al. (2023); Wei et al. (2024b); Zou et al. (2023b), with adaptations to focus\non the specific aspects of utility and safety in the context of safety alignment."}, {"title": "Measuring Utility.", "content": "We evaluate the model's utility by measuring its average zero-shot accuracy\nacross six common tasks from EleutherAI's LM Harness (Gao et al., 2021): BoolQ (Clark et al.,\n2019), RTE (Wang, 2018), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2019),\nARC Challenge (Clark et al., 2018), and OpenBookQA (Mihaylov et al., 2018). These tasks were\nchosen to reflect a broad range of general reasoning and language understanding capabilities."}, {"title": "Measuring Safety.", "content": "We measure the model's safety by evaluating its attack success rate (ASR) in\nresponse to harmful instructions. Specifically, we prompt the model using AdvBencheval, which con-\nsists of 100 harmful prompts, and collect the model's responses. We consider an attack as successful\nif the model's response lacks key patterns indicative of instruction rejection. The ASR is then com-\nputed as the ratio of successfully attacked prompts to the total number of prompts evaluated. Our\nsafety evaluation includes two use cases:\n1. ASRvanilla: This metric reflects the model's response to harmful instructions under standard,\nnon-malicious conditions. We calculate this both with and without the system-level instructions\nthat define the model's behavior and constraints.\n2. ASRAdv-Suffix: In this setting, the attacker optimizes adversarial suffixes to bypass the model's\nsafety guardrails (Zou et al., 2023b). This setting allows us to test the model's resilience to\nmanipulative inputs that are designed to mislead the model into following harmful instructions."}, {"title": "B.3 MODEL PRUNING DETAILS AND STRUCTURED COMPONENTS", "content": "To implement structured pruning, we follow the method proposed by Li et al. (2024); An et al.\n(2024). The pruning process targets specific structured components (neurons or channels) within\nthe depth-2 modules of the transformer architecture, which includes both attention and feedforward\nlayers. A depth-2 module is represented as f(X) = \u0392\u03c3(AX), where A and B are weight matrices.\nThis paper focuses on the inner channel pruning (please refer to Fig. 1 in Li et al. (2024)): pruning\nthe input channels of matrix B and the output neurons of matrix A. This allows us to directly\nreduce the number of active channels and neurons in both the feedforward and attention mechanisms,\nensuring that less important components (those with low variance) are removed.\nWe calculate the importance score (I) for each channel or neuron by measuring the activation\nvariance across a target dataset, which is described in equation 1. For each module, channels and\nneurons with the least activation variance are pruned, as they are considered less critical for either\nutility or safety-related tasks.\nIn addition, to ensure consistency across layers and modules with differing scales, we apply a stan-\ndardization process to the computed importance scores. Following the methodology outlined in An\net al. (2024), the importance score for each channel or neuron is normalized to account for the vari-\nation in metrics across different layers and modules. The standardized importance score \u00ce, for a\ngiven layer l and channel/neuron j is computed as follows:\n\u00ce^l_{:,j} =  \\frac{I - E[I]}{\\sqrt{E[(I - E[I])^2]}}\nHere, I^l_{I.;} represents the raw importance score for the j-th channel or neuron in layer l, while E[Il;]\nrepresents the expected value (or mean) of the importance scores in that layer. The standard deviation\nis given by the square root of the variance of these scores. This standardization ensures that the"}, {"title": "B.4 ATTRIBUTE TRANSFER DURING FINE-TUNING", "content": "In our fine-tuning experiments described in Sec. 4.2, we track the attribute transfer of individual\nunits during the adaptation of safety-aligned models to new tasks. The process involves categorizing\nthe computing units into ESU, EUU, CU, and RU based on their behavior in the original, safety-\naligned model before and after fine-tuning. As fine-tuning progresses, we measure how many units\ninitially classified as ESU or CU are converted into EUU or RU. This is done by re-evaluating the\nimportance scores Is and Iu for each unit after every few epochs of training.\nThe key insight is that when units critical to safety (ESU or CU) are re-purposed for utility tasks\n(becoming EUU), the model's safety performance degrades. This transformation is tracked in the\nattribute transfer statistics, which are visualized in Fig. 5 of the main text. The attribute transfer\nanalysis highlights the brittleness of current safety mechanisms: when safety-aligned models are\nfine-tuned on new tasks, many safety-critical components lose their original function, compromising\nthe safety guardrails of the model."}, {"title": "B.5 EXPERIMENTAL SETUP FOR FREEZING SAFETY-CRITICAL COMPONENTS", "content": "To mitigate the safety performance degradation caused by fine-tuning, we experiment with freezing\nthe safety-critical components identified through pruning. After categorizing the units into ESU,\nEUU, CU, and RU, we freeze the Exclusive Safety Units (ESU) and the top 6% of Complex Units\n(CU) during the fine-tuning process. This ensures that these units retain a large part of their original\nfunction and are not re-purposed for utility tasks. The rest of the model is fine-tuned as usual on\nnew tasks, allowing the non-safety-critical components to adapt to the task while keeping the safety-\ncritical components unchanged.\nFine-Tuning Attack Datasets. For fine-tuning attack experiments, we use two popular instruction-\nfollowing datasets: Alpaca and Dolly. The Alpaca dataset (Taori et al., 2023) is a publicly available\ndataset created using GPT-3.5, and it contains 52,000 instruction-following samples across a variety\nof tasks. It has been widely used for instruction tuning due to its diversity in queries. The Dolly\ndataset (Conover et al., 2023) is another widely adopted dataset for instruction-tuning, created by\nDatabricks, which contains high-quality examples designed to improve the model's capability to\nfollow instructions, based on their open-source Dolly model. Both datasets allow us to effectively\nassess how fine-tuning for general instruction-following can impact the model's safety guardrails\nwhen safety-critical components are or are not frozen.\nSafety Evaluation Datasets. To evaluate safety performance, we use two distinct datasets: Ad-vBench and HEX-PHI."}, {"title": "B.6 DETAILS ON REDUNDANT UNITS AND ALIGNMENT BUDGET", "content": "In Section 4.3, we explore the possibility of repurposing redundant units (RU) as part of an align-\nment budget to minimize the alignment tax. The core idea is that pre-trained LLMs contain a large\npercentage of parameters that do not contribute significantly to task performance, as noted by Sun\net al. (2023) and Ma et al. (2023). These redundant units can be re-purposed to improve safety\nalignment without sacrificing utility performance.\nWe identify redundant units using the same variance-based pruning method described in Section 4.1.\nSpecifically, we compute an importance score for each neuron and channel based on the variance of\nactivations across the Alpaca dataset (We remove safety-related samples from the original version).\nOnce the redundant units are identified, we freeze the remaining parts of the model and fine-tune\nonly these redundant units during the alignment process. By carefully adjusting the proportion of\nredundant units re-purposed, we aim to achieve alignment without incurring the alignment tax-i.\u0435.,\nwithout sacrificing utility performance. This selective fine-tuning approach significantly reduces the\ncomputational burden compared to full model fine-tuning while maintaining high task performance."}, {"title": "Evaluation Benchmarks.", "content": "To evaluate the effectiveness of repurposing redundant units, we assess\nthe model's performance on both helpfulness (MT-bench) and accuracy (downstream tasks). Our\nevaluations consist of two main benchmarks:\nDownstream Tasks. As shown in Table 4, we evaluate the model's performance across a variety of\ntasks, including:\n\u2022 ARC-Challenge (ARC-C) and ARC-Easy (ARC-E) (Clark et al., 2018): These tasks test the\nmodel's ability to answer science questions, which require a combination of factual knowledge\nand reasoning.\n\u2022 HellaSwag (Zellers et al., 2019): A commonsense reasoning task requiring the model to predict\nthe next logical action in a situation."}, {"title": "B.7 PARAMETER-EFFICIENT FINE-TUNING (PEFT) COMPARISONS", "content": "In addition to full-model fine-tuning and freezing safety-critical components", "approach)": "n\u2022 LORA (Low-Rank Adaptation) (Hu et al.", "2021)": "LoRA introduces low-rank matrices into the\nattention mechanism, which are updated during fine-tuning while the original weights remain\nfrozen. For our experiments, we used a learning"}]}