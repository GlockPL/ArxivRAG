{"title": "Learning on Graphs with Large Language Models\n(LLMs): A Deep Dive into Model Robustness", "authors": ["Kai Guo", "Zewen Liu", "Zhikai Chen", "Hongzhi Wen", "Wei Jin", "Jiliang Tang", "Yi Chang"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance\nacross various natural language processing tasks. Recently, several LLMs-based\npipelines have been developed to enhance learning on graphs with text attributes,\nshowcasing promising performance. However, graphs are well-known to be suscep-\ntible to adversarial attacks and it remains unclear whether LLMs exhibit robustness\nin learning on graphs. To address this gap, our work aims to explore the potential of\nLLMs in the context of adversarial attacks on graphs. Specifically, we investigate\nthe robustness against graph structural and textual perturbations in terms of two\ndimensions: LLMs-as-Enhancers and LLMs-as-Predictors. Through extensive\nexperiments, we find that, compared to shallow models, both LLMs-as-Enhancers\nand LLMs-as-Predictors offer superior robustness against structural and textual\nattacks. Based on these findings, we carried out additional analyses to investi-\ngate the underlying causes. Furthermore, we have made our benchmark library\nopenly available to facilitate quick and fair evaluations, and to encourage ongoing\ninnovative research in this field.", "sections": [{"title": "1 Introduction", "content": "In recent years, significant progress has been made in the development of Large Language Models\n(LLMs) like Sentence-BERT [1], GPT [2], LLaMA [3], etc. These variants showcase exceptional\nperformance across a range of natural language processing tasks, such as sentiment analysis [4, 5],\nmachine translation [6, 7], and text classification [8, 9]. While LLMs are widely employed for\nhandling plain text, there is an increasing trend of applications where text data is linked with structured\ninformation represented as text-attributed graphs (TAGs) [10, 11]. Recently, solely utilizing LLMs\nfor graph data has proven effective in various downstream graph-related tasks, and integrating LLMs\nwith Graph Neural Networks (GNNs) [12, 13] can further enhance graph learning capabilities [10].\nAlthough graph machine learning methods with LLMs (Graph-LLMs) have reported promising\nperformance [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], their robustness to adversarial attacks\nremains unknown. Robustness has always been a crucial aspect of model performance, especially in\nhigh-risk tasks like medical diagnosis [25], autonomous driving [26], epidemic modeling [27], where\nfailures can have severe consequences. It is well-known that graph learning models such as GNNs\nare vulnerable to adversarial attacks, where adversaries manipulate the graph structure to produce\ninaccurate predictions [28, 29]. Moreover, text attributes in TAGs are also at risk of manipulation\nby attackers, which further raises concerns about the reliability of graph learning algorithms in\nsafety-critical applications. In the era of LLMs embracing graphs, a critical question arises: Are\nGraph-LLMs robust against graph adversarial attacks?"}, {"title": "2 Formulations and Background", "content": "We begin by providing preliminaries on graph neural networks, and then formalize the graph adver-\nsarial attacks. Finally, we briefly introduce the developments in large language models on graphs.\nMore details are shown in Appendix A.\nNotations. We define a graph as G = (V, E), where V denotes the node set and E represents the\nedge set. We employ X \u2208 \\mathbb{R}^{N\\times d} to denote the node feature matrix, where N is the number of nodes\nand d is the dimension of the node features. Furthermore, we use the matrix A \u2208 \\mathbb{R}^{N\\times N} to signify\nthe adjacency matrix of G. Finally, the graph data can be denoted as G = (A, X).\nGraph Neural Networks. GCN [12] is one of the most representative models of GNNs, utilizing\naggregation and transformation operations to model graph data. Unlike GCN, which treats all\nneighbors equally, GAT [13] assigns different weights to different nodes within a neighborhood\nduring aggregation.\nGraph Adversarial Attacks. In the context of G = (A, X) and a subset Vm \u2286 V comprising victim\nnodes, where yi signifies the label for node i, the attacker's objective is to discern a perturbed graph\ndenoted as G = (A, X). The primary goal is to minimize the attack objective \\mathcal{L}_{attack}.\nmin \\mathcal{L}_{attack} \\left( f_{\\theta} (\\tilde{G}) \\right) = \\sum_{i \\in V_m} l\\left(f_{\\theta} (\\tilde{G})_i, y_i\\right), \\quad \\tilde{G}^* = \\arg \\min_{\\tilde{G} \\in \\Psi \\left( G \\right)} \\mathcal{L}_{train} \\left( f_{\\theta} (\\tilde{G}) \\right), \\quad 0 \\leq \\left\\| \\tilde{G} \\right\\| \\leq \\theta. \t\text{\n}\\qquad (1)\nwhere f\u03b8 indicates the model function of GNN, \\mathcal{L}_{attack} represents the loss function for attacks, one\noption is to set \\mathcal{L}_{attack} = \u2212\\mathcal{L}, and \\tilde{G} can be either G or \\hat{G}. Here, \\hat{G} is chosen from a constrained\ndomain \\Psi(G). Given a fixed perturbation budget D, a typical constraint for \u03a8(G) can be expressed\nas ||A - \\tilde{A}||_p + ||X \u2013 \\tilde{X}||_q < D. This constraint implies that the perturbations introduced in the\nadjacency matrix A and node feature matrix X should be limited, and their combined L_0 should not\nexceed the specified budget D.\nWhile graph adversarial attacks can perturb either node features or graph structures, the complexity\nof structural information has led the majority of existing adversarial attacks on graph data to focus\non modifying graph structure, particularly through actions such as adding, deleting, or rewiring\nedges [35, 36, 37, 38, 39, 40, 41, 42, 43]. For example, the PGD attack [36] uses edge perturbation\nto overcome the challenge of attacking discrete graph structures via first-order optimization. In\ncontrast, the PRBCD attack [37] addresses the high cost of adversarial attacks on large graphs with a\nsparsity-aware optimization approach.\nOn one hand, we explore the robustness against structural attacks. On the other hand, instead of\ntargeting continuous features as in existing feature attack works, we adopt a direct approach by\nemploying textual attacks to evaluate robustness, which remains a relatively unexplored direction.\nTextual Attack. For tasks on TAGs, the raw inputs are in text format and it can be hard for attackers\nto manipulate the encoded features directly, which makes the traditional feature attacking on graphs\nless practical. Therefore, the textual attack is used in this study to evaluate the robustness of LLMs\nenhanced graph features. Textual attacks can be performed on different levels like character level or\nsentence level according to the target to be perturbed. In this work, we focus on word-level attacks,\nwhich can be defined as follows.\nWord-level attacks. Given a classifier f that predicts labels y \u2208 Y, the input X is defined in a\ncategorical space and each input is a sequence of n words x1,x2,\n, xn. Each word xi has a limited\namount of substitution candidates, denoted as S(xi). To keep the perturbation as unnoticeable as\npossible, a constraint, usually L\u2081 or L2 distance is applied. Finally, to fool classifier f to the largest\nextent, the following objective is maximized:\n\\arg \\max_{x' \\in S(x)} \\mathcal{L}(f(x'), y) \\\\ s.t., ||x'-x||_2 < \\epsilon,\n(2)\nwhere \\mathcal{L} is the loss function between original prediction y and prediction f(x') after perturbation.\n\u03f5 is the budget of the perturbation, which keeps the original and the perturbed sample as close as\npossible. As the loss function above is maximized, we will get a perturbed sample x' that makes\nthe classifier f generate a prediction far from the original output. For example, SemAttack [44]\ngenerates natural adversarial text by employing various semantic perturbation functions.\nLarge Language Models on Graphs. In recent years, remarkable progress has been achieved in the\nfield of Large Language Models (LLMs), with notable contributions from transformative architectures"}, {"title": "3 Benchmark Design", "content": "To deepen our understanding of the potential of Graph-LLMs in the context of robustness on\ngraph learning, we need to design diverse pipelines to systematically assess the robustness of LLM\napproaches against adversarial attacks on graphs. Our benchmark evaluation encompasses two pivotal\ndimensions: LLMs-as-Enhancers and LLMs-as-Predictors. In this section, we will introduce the\nbenchmark design. Details about the benchmark datasets are provided in Appendix B."}, {"title": "3.1 Threat Model", "content": "We describe the characteristics of the graph adversarial attacks we developed, including both structural\nand textual attacks, from the following aspects. (1) Adversary's Goal: The primary objective is\nfocused on evasion attacks, where the adversary seeks to manipulate the input graph data at inference\ntime to cause the model to make incorrect predictions. In this scenario, the adversaries do not have\nthe authority to change the classifier or its parameters. (2) Victim Models: The targets of these\nattacks include LLMs-as-Predictors and LLMs-as-Enhancers, of which the details will be thoroughly\nelaborated in Sections 3.2 and 3.3. (3) Adversary's Knowledge: The attacks are designed under\nwhite-box and grey-box frameworks, meaning that the adversary either possesses complete knowledge\nof the model architecture and parameters or not respectively. White-box attacks are employed during\nthe LLM-as-Enhancers experiments to evaluate robustness in the worst-case scenarios. However, for\nLLM-as-Predictors, it becomes impractical to perform white-box attacks due to the significant time\ncosts associated with these large, complex models, and thus we adopt a grey-box setting."}, {"title": "3.2 LLMs-as-Enhancers", "content": "For LLMs-as-Enhancers, our benchmark provides a fair and comprehensive comparison of existing\nrepresentative methods from two perspectives: structural attack and textual attack.\nStructural attack: We have discussed two commonly used methods for structural attacks, PGD [38]\nand PRBCD [37], in Section 1. Currently, existing frameworks rely on shallow features such as\nBag of Words (BOW) and TF-IDF. In the era of LLMs, it's imperative to examine the impact of\nLLM features on structural attacks. Therefore, we designed a pipeline for structural attacks using\nLLMs-as-Enhancers. Specifically, we first generate diverse feature types derived from various LLMs,\nincluding SBert, E5, LLaMA, and Angle-LLaMA [48], as well as LLaMA fine-tuned (LLaMA-FT)\nwith LoRa [49], etc. We then utilize these features to evaluate the performance of structural attacks.\nThe pipeline is visualized in Figure 2(a).\nTextual attack: We conduct an evaluation on text attacks to verify whether LLMs-as-Enhancers can\nwithstand textual attacks compared to traditional text preprocessing techniques. Specifically, in the\nwhite-box setting, we first conduct text attacks by using SemAttack [44] on the texts of text-attributed\ngraphs. Then, we encode the texts using different methods such as traditional techniques and LLMs.\nFinally, we assess their performance on GCN and MLP. Incidentally, to enhance efficiency on LLMs,\nwe modified SemAttack for batch-wise operation instead of word-level processing. The pipeline is\nillustrated in Figure 2(b)."}, {"title": "3.3 LLMs-as-Predictors", "content": "For LLMs-as-Predictors, we also perform structural and textual attacks on pre-trained and fine-\ntuned LLMs, respectively. Different from the attacks on LLMs-as-Enhancers, white-box attacks\non the predictors can bring enormous computational costs due to the complexity of the models.\nTherefore, this study adopts the grey-box setting, choosing LLMs-as-Enhancers as the victim model\nand transferring the attacked texts or graphs to the LLMs-as-Predictors. For pre-trained models, we\nutilize the same pipeline in [10], which describes the graph structure in text and inputs it along with\ntext features directly into GPT-3.5 for prediction. For fine-tuned models, we perform attacks on\nInstructGLM [33], which uses LLaMA [3] as the backbone and is fine-tuned on different benchmark\ndatasets.\nStructural attack: Although the LLMs-as-Predictors in this study flatten graph structure into\ntexts and only incorporate a small number of neighbors when predicting a node, it is possible that\nintroducing irrelevant or false neighbors can influence the prediction results. During the structure\nattack, we use PRBCD as the attack algorithm and choose SBert and GCN as the surrogate model.\nThe perturbed graph then serves as the input of GPT-3.5 or InstructGLM. The pipeline is depicted in\nFigure 2(c).\nTextual attack: The LLMs-as-Predictors directly utilize texts as inputs. Thus, perturbing the input\ntexts is also likely to have impacts to the prediction results. In this study, we first perform SemAttack\non the selected nodes with SBert and GCN as the surrogate model. After that, the perturbed texts are\nused to evaluate GPT-3.5 and InstructGLM. The pipeline is shown in Figure 2(d)."}, {"title": "4 Experiments", "content": "In this section, we assess the robustness of LLMs against graph adversarial attacks in their two roles:\nLLMs-as-Enhancers and LLMs-as-Predictors. Specifically, we aim to answer the following questions:\nQ1: How effective are the LLMs-as-Enhancers on structural attack? Q2: What is the effectiveness of\nLLMs-as-Enhancers on textual attacks? Q3: How effective are the LLMs-as-Predictors on structural\nattack? Q4: How do LLMs-as-Predictors perform on textual attacks?"}, {"title": "4.1 Structural Attack for LLMs-as-Enhancers", "content": "Experiment Design. To tackle the research question Q1, we enhance text attributes using LLMs\nand generate new features. These enriched features are then used to train a GCN as the victim\nmodel. Specifically, we use PGD to conduct white-box evasion attacks on the structures of small\ngraphs such as Cora, Citeseer, Pubmed, and Wikics, while we employ PRBCD to conduct white-box\nevasion attacks on the structures of large graphs Arxiv and History. We vary the perturbation rates\nat 0% (clean graphs), 5%, and 25%, which represent the ratio of perturbed edges to original edges.\nSubsequently, we feed the perturbed graphs into different LLMs-as-Enhancers architectures and\ncompare the LLM features with shallow features. To quantify model robustness, we report the test\naccuracy (ACC) and the percentage accuracy degradation after attacks compared to the original\naccuracy (GAP).\nResults. The results are reported in Table 1. Further details, including standard deviations, can be\nfound in Appendix D. From these results, we have the following observations.\nPerformance comparison on clean graphs. The features generated by pre-trained language models\nexhibit better performance on most clean datasets. For instance, SBert and e5-large (E5) show\nimprovements of 2.4% and 6.5% respectively on clean Pubmed datasets compared to TF-IDF.\nRobustness against structural attacks. For 25% evasion attacks, almost all language models exhibit\ngreater robustness compared to traditional BOW and TF-IDF approaches. For example, in the case of\na 25% evasion attack on Pubmed, while TF-IDF experiences a decrease of 13.9%, LLAMA only drops\nby 3.9%. Moreover, we observe that fine-tuning enhances the robustness of LLMs, as demonstrated by\nthe fact that fine-tuned LLaMA exhibits greater robustness compared to its unfine-tuned counterpart.\nFurther, by comparing the results on 5% and 25% attacks, we find that LLMs-as-Enhancers are more\nhelpful at higher perturbation rates. For example, with a 5% perturbation budget on Cora, only 4\nlanguage models show a lower GAP compared to the shallow BOW features, while this number\nincreases to 7 at the perturbation rate of 25%. However, by examining the accuracy on clean graphs,\nwe note that the performance of LLM features still declines considerably on certain datasets, such as\nArxiv, under high perturbation rates, indicating that graph LLMs remain vulnerable to attacks."}, {"title": "4.2 Textual Attack for LLMs-as-Enhancers", "content": "Experiment Design. To answer Q2, we conduct white-box evasion attacks on LLMs-as-Enhancers,\ntargeting textual attributes. We first utilize diverse LLMs-as-Enhancers to transform the text into\nnode embeddings and train a GCN and an MLP. Then we perform the evasion attack at the model\ninference stage by perturbing text using SemAttack. Then, LLMs are used to transform the perturbed\ntext into node embeddings, which will then be fed into the trained GCN or MLP for inference. For all\nmodels and datasets, we randomly sample 200 nodes as target nodes and utilize the Attack Success\nRate (ASR) [44] as the evaluation metric.\nResults. The results are reported in Table 2. Additional details, including standard deviations of\nperformance, are provided in Appendix E. From these results, we can draw the following observations.\nLLaMA performs well against textual attack when MLP is used as the victim model. When using\nMLP as the victim model, E5 and LLaMA demonstrate greater resilience against SemAttack, with a\nnoticeable downward trend in ASR for SBert, E5, and LLaMA models. For example, on the WikiCS\ndataset, SBert has an ASR of 62.45%, while the performance of LLaMA dropped to 22.17%. Another\ninteresting observation is that BOW shows better robustness than SBert on Cora, Pubmed, and Arxiv.\nGiven that BOW has a limited input of words, the robustness of BOW is likely to come from filtering\nthe perturbed words, whose frequencies are often low.\nFor textual attack, GCN as the victim model is more robust compared to MLP as the victim model. In\nterms of GCN as the victim model, fine-tuned LLaMA achieves the lowest ASR among all datasets,\nranging from 2.98% to 6.91%. Also, compared to MLP, the ASR for GCN decreases significantly\nand remains below 20% across all datasets.\nKey Takeaways 3: Among all models and settings, the fine-tuned model, LLaMA-FT, generally\nexhibits the best robustness against the textual attack on most datasets.\nKey Takeaways 4: In the LLMs-as-Enhancers framework, GCN greatly improves the robustness\nagainst textual attacks compared to MLP as the victim model."}, {"title": "4.3 Structual Attack for LLMs-as-Predictors", "content": "Experiment Design. To answer Q3, we explore the robustness of LLM-as-Predictors against graph\nstructural attacks, using GPT-3.5 and InstructGLM as the selected predictors. Given the difficulty\nin directly attacking these predictors due to their large number of parameters or lack of model\naccess, we adopt a grey-box setting. In this setting, LLMs-as-Enhancers are used as surrogate\nmodels during adversarial attacks, and the resulting attacked graph structures are then fed into the\nLLMs-as-Predictors. When employing GPT-3.5 in the graph domain, we follow the approach in [10]"}, {"title": "4.4 Textual Attack for LLMs-as-Predictors", "content": "Experiment Design. To answer Q4, we explore the performance of GPT-3.5 and InstructGLM with\nperturbed texts as inputs. The perturbed texts are generated by SemAttack and used as adversarial\ninputs for GPT-3.5 and InstructGLM to evaluate the robustness of LLMs-as-Predictors against textual\nattacks. Following the experiment design in Q3, the experiment is conducted in a grey-box setting.\nthis experiment is conducted in a grey-box setting. We use GCN with SBert embeddings as the\nsurrogate model, randomly sampling 200 target nodes for attack and evaluation.\nResults. The results of InstructGLM are presented in Table 4. More results about GPT-3.5 are\npresented in Table 11 in Appendix G. These results lead us to the following observations.\nGPT-3.5 shows stronger robustness compared to MLP but failed to exceed GCN. As shown in Table 11,\nour experiment reveals that GPT-3.5 has strong robustness compared to MLP but fails to exceed\nGCN and GAT in all few shot settings. On the other hand, GAT exhibits the strongest robustness,\nfollowed by GCN. However, on Cora and Pubmed, the Attack Success rate of GPT-3.5 is close to\nGCN, ranging from 0.55% to 1.70%.\nInstructGLM shows the strongest robustness compared to both MLP and GCN. Different from the\nabove observations of GPT-3.5, the fine-tuned InstructGLM shows stronger robustness on the three\ndatasets compared to GCN. While there is an ASR of 30.37% for GCN on Cora, InstructGLM can\nresist the textual perturbation and achieves an ASR of 1.21% with structural information incorporated.\nThe 0% ASR result of MLP on the Arxiv dataset is due to the low ACC, as we can not find a sample\nthat is both predicted correctly before the attack and predicted wrong after the attack. Also, this low\naccuracy is likely to be caused by the dataset used by InstructGLM, which only utilizes titles of a few\nwords as the node attributes.\nKey Takeaways 6: LLMs-as-Predictors exhibit stronger robustness against textual attacks compared\nto MLP. However, GPT-3.5, which is not fine-tuned, shows poorer robustness compared to GCN."}, {"title": "4.5 Analysis for Structural Attack", "content": "Since Sections 4.1 and 4.3 have shown the robustness of Graph-LLMs against the structure attack,\nwe conduct analysis from the following perspectives to explore the reasons behind such robustness.\nt-SNE visualization. First, we examine t-SNE visualizations of various initial features and find that\ninitial features generated by language models are more distinguishable in categories compared to\ntraditional features as shown in Fig. 3.\nDBI. To further evaluate the separability of input features, we use the Davies-Bouldin Index\n(DBI) [50], as shown in Table 5. The DBI score represents the average similarity measure be-\ntween each cluster and its most similar cluster, with lower scores indicating better clustering quality.\nA score of zero is ideal, signifying optimal clustering. We find that initial LLM features have lower\ninitial DBI scores. We further examine the DBI of embeddings both before and after the attack, and\nthe differences therein. Notably, embeddings from larger models exhibit a smaller decrease in DBI\nscores post-attack, as indicated by the DBI Diff in the Table 5.\nHomophily. Additionally, we analyze the homophily of the Cora dataset before and after attacks, as\nshown in Table 5. It discovers that the pre-attack Cora dataset has a homophily of 0.81, and after the\nattack, Cora with shallow features exhibits lower homophily.\nBased on the above results, we find that robustness is strongly positively correlated with the quality\nof features, indicating that higher distinguishability of features leads to stronger robustness and\nhigher homophily after attacks. This could be attributed to the richer information present in features\ngenerated by pre-trained language models, resulting in higher distinguishability in clustering. The\nhigher the quality of the features, the less the model depends on the structure. Therefore, high-quality\nfeatures of LLMs can be more robust against structural attacks."}, {"title": "4.6 Analysis for Textual Attack", "content": "From the experiments of the LLMs-as-Enhancers against textual attack in Section 4.2, we observe\nthat LLaMA performs much better than other smaller models and the fine-tuned LLaMA exhibits\nthe best robustness. In addition, GCN demonstrates much stronger robustness compared to MLP. To\nexplore the reasons behind this, we perform analysis from feature and structure perspectives.\nFeature Perspective. From the feature perspective, we assume that text attributes can have an impact\non the attack success rate. Specifically, we first explore some basic indicators like text entropy, text\nlength, and the number of words among successfully attacked nodes and failed attacked nodes, as\nshown in Table 6. Based on the observation with LLaMA and GCN as the victim model, it is obvious\nthat the successfully attacked nodes tend to have smaller entropy (less richness of texts), shorter texts,\nand smaller amounts of words. By comparing the DBI of the fine-tuned and not fine-tuned model, we\nalso observe that the fine-tuned model always has a smaller DBI compared to the fine-tuned model."}, {"title": "5 Conclusion and Future Directions", "content": "This work introduces a comprehensive benchmark for exploring\nthe potential of LLMs in context of adversarial attacks on graphs.\nSpecifically, we investigate the robustness against graph struc-\ntural and textual attacks in two dimensions: LLMs-as-Enhancers\nand LLMs-as-Predictors. Through extensive experiments, we find\nthat, compared to shallow models, both LLMs-as-Enhancers and\nLLMs-as-Predictors offer superior robustness against structural\nand textual attacks. Despite these promising results, several crit-\nical challenges and research directions remain worthy of future\ninvestigation.\nRethinking Textual Attack. Based on the observations above, we\nrealize that textual attacks can significantly affect the prediction of individual samples. However,\nwhen GCN serves as the victim model, the incorporation of neighbor information helps mitigate these\nperturbations, significantly reducing the attack's effectiveness. From an attack perspective, how the\nresistance capability of the GCN-based victim models can be weakened deserves attention, especially\nfor attacking stronger Graph-LLMs.\nCombining Textual and Structural Attack on Graphs. To enhance attack capabilities, a combined\nframework that perturbs both text attributes and graph structure is needed. However, challenges such\nas integrating textual and structural attacks to improve attack efficiency remain unsolved. In this\nstudy, we provide preliminary results in the Appendix I. Our experiment shows that adding additional\ntextual perturbations on top of structural perturbations can further degrade model performance.\nRethinking Graph-LLMs. From the results in Table 1, we can conclude that the angle-optimized\nAngle-LLaMA, which is more suitable for text encoding, exhibits better robustness against adversarial\nattacks compared to LLaMA. This phenomenon may inspire us to design better Graph-LLMs for text\nattribute encoding. Finally, we can use LLMs to perform attacks on graphs by generating harmful\nstructures and text attributes."}, {"title": "I Combining Textual and Structural Attack", "content": "To enhance attack capabilities, maybe a combined framework that perturbs both text attributes and\ngraph structure is needed. In this study, we provide some preliminary results. First of all, we design\na simple strategy that combines the structural attack and textual attack directly, further improving\nthe attack ability. The results are reported in Table 12. In addition, we design various combination\nstrategies for modifying the textual attack. These include prioritizing the attack on text attributes\nof small-degree nodes, targeting large-degree nodes first, and attacking text attributes within the\nsame cluster. Based on the results in Table 13, we find that prioritizing attacks on the text attributes\nof small-degree nodes is more effective. This is merely a preliminary attempt, and we hope it will\ninspire more in-depth research."}, {"title": "J Broader Impact", "content": "Graph machine learning methods with Large Language Models (Graph-LLMs) have reported promis-\ning performance, covering a wide range of applications. We are initiating this benchmark to call\nmore researchers' attention to whether Graph-LLMs are robust against graph adversarial attacks.\nThe proposed benchmark can significantly promote the development of robustness in Graph-LLMs.\nOur benchmark provides a comprehensive evaluation, including LLM-as-Enhancers and LLM-as-\nPredictors, and consider both structural attacks and textual attacks. However, there are still many\nresearch gaps to be filled. Firstly, we have only explored node classification tasks and have yet to\nexplore link prediction and graph classification tasks. Secondly, we have focused only on evasion\nattacks and have not addressed poisoning attacks. These limitations will be addressed in our future\nwork."}]}