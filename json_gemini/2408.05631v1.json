{"title": "PRTGaussian: Efficient Relighting Using 3D Gaussians with Precomputed Radiance Transfer", "authors": ["Libo Zhang", "Yuxuan Han", "Wenbin Lin", "Jingwang Ling", "Feng Xu"], "abstract": "We present PRTGaussian, a realtime relightable novel-view synthesis method made possible by combining 3D Gaussians and Precomputed Radiance Transfer (PRT). By fitting relightable Gaussians to multi-view OLAT data, our method enables real-time, free-viewpoint relighting. By estimating the radiance transfer based on high-order spherical harmonics, we achieve a balance between capturing detailed relighting effects and maintaining computational efficiency. We utilize a two-stage process: in the first stage, we reconstruct a coarse geometry of the object from multi-view images. In the second stage, we initialize 3D Gaussians with the obtained point cloud, then simultaneously refine the coarse geometry and learn the light transport for each Gaussian. Extensive experiments on synthetic datasets show that our approach can achieve fast and high-quality relighting for general objects. Code and data are available at https://github.com/zhanglbthu/PRTGaussian.", "sections": [{"title": "I. INTRODUCTION", "content": "Relightable view synthesis has played a crucial role in computer graphics and computer vision for a long time [1], [2], [3], [4]. It has many applications such as augmented reality and virtual object insertion. However, decoupling the lighting and reflectance information from the visual inputs and performing high-quality relighting is still slow, ill-posed and challenging. Recently, some studies [5], [2], [6] try to employ inverse rendering techniques to explicitly estimate objects' intrinsic properties (i.e. geometry and material) and scene lighting from images. However, these methods struggle to model complex light transport, such as subsurface scattering and indirect illumination. To tackle this problem, some methods [7], [1], [8] have attempted to directly model the light transport of objects using multi-view one-light-at-a-time (OLAT) datasets. Nevertheless, such approaches often require cumbersome representation and intensive sampling, resulting in slow training and rendering speeds, severely limiting their application scenarios. Realtime relightable view-synthesis can improve interactive applications like VR and gaming, enabling dynamic, user-responsive lighting.\nIn this paper, we propose PRTGaussian, a novel framework for fast training and realtime relighting. Given multi-view OLAT images of general objects, real-time view synthesis and relighting can be realized after efficient training. Specifically, we use 3D Gaussians [9] with high-order spherical harmonics as the scene representations. Compared to implicit representations like NeRF [10], this explicit representation allows for more efficient forward rendering, resulting in faster training and inference speeds. The computation of object's appearance is based on the idea of precomputed radiance transfer. To be specific, for each Gaussian, we take its encoded position as input to regress its transport coefficients via a neural network, the final color is then obtained by combining this information with its albedo and lighting approximated by spherical harmonics. However, jointly optimizing the geometry and the appearance of objects is challenging due to the high ambiguity and complexity involved in accurately capturing both aspects simultaneously. To mitigate this problem, we propose a two-stage training strategy. In the first stage, we construct a multi-view images of fixed illuminations from the OLAT dataset and obtain the position initialization of Gaussians from it using vanilla 3D Gaussian Splatting. In the second stage, we initialize this point cloud as 3D Gaussians and proceed with further training, which refines the geometry of 3DGS and learns the light transport for each Gaussian.\nIn summary, the contributions of this paper can be outlined as follows:\n\u2022\nWe propose an efficient relighting framework that utilizes 3D Gaussians for scene representation and precomputed radiance transfer for appearance modeling.\n\u2022\nWe introduce a two-stage training process, beginning with coarse point cloud reconstruction and 3DGS initialization from multi-view images, followed by the refinement of 3D Gaussians, along with the learning of light transport.\n\u2022\nWe conduct extensive experiments to demonstrate that our method achieves high-quality relighting effects that are comparable with SOTA while maintaining fast rendering speeds."}, {"title": "II. RELATED WORK", "content": "Traditional scene representations [11], [12], including mesh-based, voxel-based, and point cloud methods, have been widely used due to their simplicity and efficiency. However, these approaches often struggle with complex geometries and detailed appearances. To address these challenges, implicit representations [10], [13] such as Neural Radiance Fields (NeRF) [10] have emerged, providing continuous and high-resolution modeling capabilities that significantly enhance the quality and realism of 3D scene representations. Specifically, NeRF represents a continuous scene as a 5D vector-valued function, with the 3D spatial position and 2D viewing direction as inputs, and the corresponding color and volume density as outputs. The image can then be rendered using volumetric rendering techniques. However, the sampling of rays and the use of a large Multi-Layer Perceptron significantly reduce its speed. Subsequent works [14], [15] have improved the quality and speed of NeRF, but their training and rendering time remain extremely high.\nRecently, 3D Gaussians Splatting [9] has been introduced as an explicit scene representation. It uses 3D ellipsoids to represent scenes and employs a tile-based rasterization approach to render images, significantly improving training and rendering speeds while achieving high-quality novel view synthesis. The explicit nature of this representation also makes it more suitable for tasks such as scene editing and dynamic reconstruction. In this work, we use 3D Gaussians (3DGS) to represent objects.\nThe goal of inverse rendering is to decompose the scene's geometry, material, and lighting components from images. Under fixed lighting conditions, some methods [16], [2] use differentiable rendering to estimate an object's depth, normal and material properties. Since the forward process of these methods is often based on simple physical rendering models such as the Cook-Torrance microfacet model [17], they can handle only limited geometries and materials, such as subsurface scattering, transparency, and anisotropic materials.\nOther methods perform inverse rendering based on known multi-illumination conditions. NRHints [1] uses unstructured multi-view data with moving point light sources, employing two MLPs to separately represent the object's geometry and appearance. Alternative approaches [7], [18] learn appearance representations based on one-light-at-a-time (OLAT) data, but they typically focus on specific objects, such as human bodies and faces. We also utilize the OLAT dataset, but our method achieve lighting decoupling for general objects without relying on any priors.\nIn traditional computer graphics, rendering global illumi-nation effects for a scene is costly due to the extensive ray tracing and numerous light bounce computations required. To achieve real-time rendering with global illumination, Sloan et al. [19] introduced the method of precomputed radiance transfer (PRT), which precomputes the coefficients of incident light projected onto spherical harmonics bases and the transfer vector at each point on the surface. The transfer vector, also represented by spherical harmonics coefficients, describes how each point on the surface interacts with incident light, including global transport effects such as reflections and shadows. At runtime, the light vector and transfer vector are integrated to obtain the radiance at each point. To address the low-frequency limitations of spherical harmonics, [20][4] introduced new rep-resentations such as nonlinear Gaussian functions and spherical Gaussians based on this foundation. However, these methods rely on known geometry and material properties.\nWe aim to model the radiance transfer at each point solely from images. Specifically, we use two networks to separately encode and decode the radiance transfer vector."}, {"title": "III. METHOD", "content": "Given the multi-view OLAT data, our goal is to achieve free-viewpoint relighting of the object. An overview of our method is shown in Fig.1. In the first stage, we reconstruct the initial positions for the 3D Gaussians. In the second stage, we refine the properties of 3DGS and learn the radiance transfer. In this section, we provide details on data acquisition (Sec.III-A), initial geometry reconstruction (Sec.III-B) and radiance transfer learning (Sec.III-C).\nWe use a one-light-at-a-time (OLAT) dataset similar to the light-stage [21] setup, where camera and light information are known. We synthesize a multi-view OLAT dataset for general objects in Blender. Specifically, we uniformly sample 25 camera positions and 200 lighting positions on the upper hemisphere of the object. The lighting is considered as directional light, with the detailed setup shown in Fig.3.\nWe use a set of 3D Gaussians (3DGS) to represent the object's geometry. Each Gaussian can be defined as\n$$g_k = \\{\\mu, R, s, \\sigma, \\rho\\}$$\nwhere $\\mu \\in \\mathbb{R}^3$ is the 3D position, $R \\in SO(3)$ is the rotation matrix, $s \\in \\mathbb{R}^3$ is the per-axis scale factors, $\\sigma \\in \\mathbb{R}$ is the opacity value and $\\rho \\in \\mathbb{R}^3$ is the albedo. The covariance matrix of each Gaussian can be represented as:\n$$\\Sigma = Rdiag(s)diag(s)^TR^T$$\nIn the splatting process, 3D Gaussians are first projected onto the 2D plane,\n$$\\Sigma' = J V \\Sigma V^T J^T$$\nwhere $J \\in \\mathbb{R}^{2\\times3}$ is the Jacobian of the projective transformation, $V \\in \\mathbb{R}^{3\\times3}$ is the viewing transformation, and $\\Sigma' \\in \\mathbb{R}^{2\\times2}$ is the covariance matrix of the projected 2D Gaussians. The color $C_p$ of each pixel can be computed using accumulated volumetric rendering as follows:\n$$C_p = \\sum_{k \\in N} C_k \\alpha_k \\prod_{j=1}^{k-1} (1 - \\alpha_j)$$\nwhere $k$ is the index of $N$ ordered Gaussians, $\\alpha_k \\in \\mathbb{R}$ is estimated from the 2D covariance matrix and the opacity of each Gaussian, and $c_k \\in \\mathbb{R}^3$ is the color, which is determined by the albedo, radiance transfer, and incident light.\nDue to the high ambiguity in jointly optimizing geometry and appearance, we first reconstruct a coarse geometry of the object from multi-view images under uniform lighting. The uniformly illuminated multi-view data is obtained by summing and averaging the illumination for each viewpoint. Then, We use the vanilla 3D Gaussian Splatting[9] method for coarse geometry reconstruction from these images. The generated point cloud is applied for Gaussian initialization in the next stage.\nIn the second stage, we learn the radiance transfer for each 3D Gaussian. The basic rendering equation can be written as:\n$$L(o) = \\int_{\\Omega} L(i) \\rho(i, o) V(i) max(0, n \\cdot i) di$$\nwhere $i$ and $o$ represent the incident and outgoing directions, while $L$, $V$, $\\rho$, $n$ represent the lighting, visibility, albedo, and normal properties, respectively.\nIn this work, we primarily focus on diffuse materials where each Gaussian has a specific albedo $\\rho(i, o) = \\rho$. Based on the concept of Precomputed Radiance Transfer (PRT), the light and radiance transfer can be precomputed as follows:\n$$L(i) = \\sum_{j=1}^{n^2} l_j B_j(i)$$\n$$T_j = \\int_{\\Omega} B_j(i) V(i) max(0, n \\cdot i) di$$\nwhere $n$ is the order of the spherical harmonics, $B_j$ is the spherical harmonics basis, $l_j \\in \\mathbb{R}^{n^2}$ is the coefficient of the light source projected onto SH space, and $T_j \\in \\mathbb{R}^{n^2}$ is the radiance transfer term. Therefore, equation 5 can be rewritten as:\n$$L(0) = \\sum_{j=1}^{n^2} l_j T_j$$\nThe light coefficient $l_j$ can be obtained from the input lighting by direct projection. The radiance transfer term $T_j$ is encoded with a neural network. Specifically, we first encode the position $\\mu$ of each Gaussian into a higher-dimensional vector [22] $\\varphi(\\mu)$, and then decode it using an MLP to obtain the corresponding $T_j$.\nDuring the training process, the rendered image is compared with the ground truth image to calculate the loss:\n$$\\mathcal{L} = (1 - \\lambda) \\mathcal{L}_{1} + \\lambda \\mathcal{L}_{D-SSIM}$$\nwe use the gradient of this loss to update the parameters of the encoder and decoder, while simultaneously refining all the properties of each Gaussian."}, {"title": "IV. EXPERIMENTS", "content": "We implemented our method in Python using the PyTorch framework. Our encoder uses hash-grid encoding [22] with the number of levels, number of features per level, and hash table size set to 32, 8, and 220, respectively. Our decoder employs a four-layer fully connected MLP, with each layer consisting of 256 neurons. To capture higher frequency information, we set the order of the spherical harmonics basis to n = 9. During training, we used the Adam optimizer and set $\\lambda$ to 0.2. The entire training process was conducted on an NVIDIA RTX 4090, with a total training time of approximately 15 minutes per scene.\nWe conducted a comprehensive comparison against the state-of-the-art field-based method NRHints [1], which uses volume rendering and represents geometry in SDF."}, {"title": "V. LIMITATIONS", "content": "Although we have accelerated the training and inference speeds of existing relighting methods and achieved decent rendering results, our approach still has some limitations and shortcomings.\nOne major drawback is that we currently only consider diffuse materials and do not consider specular highlights and other view-dependent effects. Incorporating the view direction as a condition in the pipeline holds promise for addressing this issue in the future.\nAnother drawback is that although we use high-order SH to approximate the information of light and radiance transfer, our method still struggles to reproduce high-frequency rendering effects such as the sharp edges in hard shadows (as shown in Fig.7). This is due to the inherently low-frequency nature of spherical harmonics. Explicitly considering the visibility of 3DGS in future work will help alleviate this situation."}, {"title": "VI. CONCLUSIONS", "content": "In this study, we present PRTGaussian, a novel relighting framework that combines 3D Gaussian Splatting and Precom-puted Radiance Transfer to achieve efficient and high-quality relighting. Our approach demonstrates significant improve-ments in training and rendering speeds compared to existing techniques and efficiently utilizes multi-view OLAT data for realistic relighting. While our method excels in these areas, it is currently limited to diffuse materials and has difficulty capturing high-frequency details. Future work could address these limitations by integrating view-dependent effects and enhancing the representation of high-frequency components."}]}