{"title": "Summary of Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images", "authors": ["Kamorudeen A. Amuda", "Almustapha A. Wakili"], "abstract": "This study introduces a federated learning-based approach to predict HER2 status from hematoxylin and eosin (HE)-stained whole slide images (WSIs), reducing costs and speeding up treatment decisions. To address label imbalance and feature representation challenges in multisite datasets, a point transformer is proposed, incorporating dynamic label distribution, an auxiliary classifier, and farthest cosine sampling. Extensive experiments demonstrate state-of-the-art performance across four sites (2687 WSIs) and strong generalization to two unseen sites (229 WSIs).", "sections": [{"title": "Introduction", "content": "Hematoxylin and eosin (HE)-stained whole slide images (WSIs) are increasingly used beyond traditional tasks, applying deep learning to detect subtle molecular characteristics [1], [2]. In breast cancer, accurately predicting the status of HER2 is critical to guide treatment decisions [3]. Typically, HER2 status determination is based on immunohistochemistry (IHC) or in situ hybridization (ISH) [4], but deep learning enables prediction directly from HE-stained WSIs, reducing the need for these costly techniques.\nFederated learning (FL) [5] has shown promise in WSI analysis by enabling collaboration between multiple sites without the need to transfer large datasets, minimizing concerns about data privacy. However, challenges such as label imbalance and variations in specimen preparation across sites can impact model performance. While existing methods address these issues in natural scenes, applying them to WSI datasets presents additional challenges compared to centralized learning.\nThe proposed Point TransformerDDA+ approach addresses these challenges by capturing both local context and longrange dependencies using a point transformer block enriched with positional information. The proposed Farthest Cosine Sampling (FCS) method identifies the most distinctive features to capture long-range dependencies. Additionally, the proposed Dynamic Distribution Adjustment (DDA) method tackles label imbalance across sites, improving model initialization and generalization."}, {"title": "Related Work", "content": "Recent works on WSI classification, including HER2 status prediction, have primarily used MIL-based or graph-based methods. MIL-based approaches often utilize attention mechanisms or transformers to capture long-range dependencies but may neglect local spatial information. DSMIL combines features from different scales, while TransMIL models spatial relationships using transformers with position encoding not based on Euclidean space. Graph-based models, like PatchGCN and SlideGraph+, capture local information via graph structures but may struggle with long-term dependencies. Point neural networks, which capture both local context and longrange dependencies, have seen limited application in WSI classification [1], [2].\nFederated Learning in WSI Analysis\nFederated learning [5], [6] enables multi-site WSI model training but faces challenges like skewed label distribution. HistFL and TNBC-FL apply federated learning for cancer predictions, while approaches like FedProx and FedMGDA+ improve robustness on non-i.i.d. data. Despite progress, federated learning still underperforms compared to centralized learning for WSIs, needing further enhancement [6], [7]."}, {"title": "Methodology", "content": "This section introduces the problem of HER2 status prediction using a point transformer with federated learning."}, {"title": "", "content": "We describe the main components of the framework, including point feature extraction, the point transformer block, the point abstraction block, and federated learning with dynamic distribution adjustment. Figure 1 provides an overview of the proposed framework, highlighting the repeated blocks and key components such as the feature pyramid network (FPN), global average pooling (GAP), and multilayer perceptron (MLP).\nPoint Feature Extraction\nTo extract the features of points from a WSI, the CLAM method [1] is used to pre-process and patch the WSIs, where each patch is treated as a point. The coordinates of each patch are represented as (px,py,1), where the z-coordinate is fixed. These patches are then inputted into a nuclei segmentation network, pre-trained using a Swin-Transformer with a fourlevel feature pyramid network (FPN). The outputs from the four FPN layers are averaged and concatenated with the patch coordinates, resulting in a patch-level feature $X_n \\in R^{3+d}$, where d = 256. For each WSI with label y, the point set X = {X1,X2,...,X\\x\\} is formed. To optimize memory and computation, 1024 patches are randomly selected from each WSI.\nPoint Transformer Block\nIn our approach, we use the original point transformer block [8] to capture and aggregate local context information for each point via an attention mechanism. For each point Xi with position pi, we consider its k-nearest neighbors (where k = 16) as a subset $X^{(1)} \\subset X$. The attention score a(xi,xj) between xi and its neighboring points xj \u2208 X(1) is calculated as:\n$a(x_i,x_j) = W_q(W_ix_i \u2013 W_jx_j) + PE(p_i,p_j)$\nwhere PE(pi,pj) is the relative position encoding:\n$PE(p_i,p_j) = MLP(p_i - p_j)$\nAfter computing the attention scores, the localized features around xi are aggregated using a softmax function, and the final output yi is obtained by applying a residual connection:\n$z_i = \\sum_{x_j \\in X^{(i)}} S(a(x_i,x_j)) \\cdot (W_vx_j + PE(p_i,p_j))$\n$y_i = x_i + W_zzi$\nThis approach effectively aggregates local point features using attention and relative positional encoding.\nPoint Abstraction Block\nTo reduce the cardinality of a point set and capture longrange dependencies, a novel strategy called farthest cosine sampling (FCS) is proposed, replacing farthest point sampling (FPS). Unlike FPS, which samples on the basis of position, FCS operates in the feature space, using the cosine"}, {"title": "", "content": "distance as the metric between points. This method prevents missing important positive patches, especially in cases where WSIs contain few clustered positive patches. The cosine distance is defined as:\n$Dist (x_i, x_j) = 1 - \\frac{x_i x_j}{max(||x_i||_2 ||x_j||_2, 1e - 8)}$\nFCS iteratively selects the M/4 farthest points, ensuring that important patches are retained. After FCS, a k-nearest neighbor grouping is applied, followed by a max pooling of the neighborhood features for each sampled point:\n$y_i = MaxPooling_{x_j \\in X^{(i)}}(MLP(x_j))$\nThis approach improves HER2 status prediction by covering key points and capturing long-range dependencies in the feature space.\nPoint Classifier Block After performing four attention and abstraction operations, four abstract points are obtained, represented as Fg \u2208 R4\u00d7512. By averaging these grouped features, the final WSI-level feature Fh \u2208 R64 is derived:\n$F_h = MLP(GAP(F_g)) = f_h(X)$\nThe MLP has two layers, each with a linear transformation and ReLU activation. A linear layer with a softmax function, denoted as fc, outputs the final HER2 status probabilities p. The loss is computed using the cross-entropy (CE) loss function:\n$l(p,y) = - \\frac{1}{P} \\sum_{i=1}^{P} \\sum_{c=0}^{1} Y_{ic} log(P_{ic})$\nThis equation calculates the prediction error based on the HER2 status.\nFederated Learning with Dynamic Distribution Adjustment\nTo address the non-i.i.d. label distribution issue in WSIs from different sites, a dynamic distribution adjustment strategy is introduced. This strategy subsamples the majority of HER2WSIs to balance the label distribution in the early stages of federated learning. Over time, the balanced distribution gradually adjusts to match the real distribution. Specifically, all HER2+ WSIs are kept, and a Bernoulli mask M(k) is applied to the HER2- WSIs, where the subsampling probability bk evolves with training iterations. The loss function is formulated as:\n$L_{cls} = M \\cdot l(f_c(F_h),y)$\nTo prevent potential information loss from subsampling, an auxiliary classifier is added, which incorporates the real label distribution of each site.\n$L_{aux} (P_y) = -[ \\sum log(p_i^y) + \\sum log(p_i^y)]$\nThe total loss is then computed as"}, {"title": "", "content": "$L_{total} = L_{cls} + L_{aux}$"}, {"title": "Experiments", "content": "Dataset and Experimental Settings\nThe point transformer model for HER2 status prediction was tested on 2,916 WSIs from six sites. Four sites were used for federated learning, while two served as unseen data for external testing. Data from the federated sites were split into training (60%), validation (10%), and test (30%) sets. The evaluation was based on the mean AUC from the test set, using five repeated splits for model selection. Three model variants were tested: PointTransformer+ (with FCS), PointTransformerDDA (with DDA), and PointTransformerDDA+ (combining FCS and DDA).\nComparison with WSI Classification Methods\nThe evaluation includes models such as PointNet++, MILbased models (CLAM-SB, DSMIL, TransMIL, HistFL), and graph-based models (GraphSAGE, Patch-GCN, SlideGraph+) for comparison, all using federated learning. Table 1 shows that point-based models perform competitively by integrating local"}, {"title": "Conclusion", "content": "This study presents a novel approach for Whole Slide Image (WSI) analysis by treating a WSI as a point cloud with position data, rather than using traditional MIL or graphbased methods."}]}