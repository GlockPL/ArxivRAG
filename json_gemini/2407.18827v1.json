{"title": "HUMAN-ARTIFICIAL INTELLIGENCE TEAMING FOR SCIENTIFIC INFORMATION\nEXTRACTION FROM DATA-DRIVEN ADDITIVE MANUFACTURING RESEARCH USING\nLARGE LANGUAGE MODELS", "authors": ["Mutahar Safdart", "Jiarui Xiet", "Andrei Mirceat", "Yaoyao Fiona Zhao*"], "abstract": "Data-driven research in Additive Manufacturing (AM) has\ngained significant success in recent years. This has led to a\nplethora of scientific literature to emerge. The knowledge in\nthese works consists of AM and Artificial Intelligence (AI),\ncontexts that haven't been mined and formalized in an integrated\nway. It requires substantial effort and time to extract scientific\ninformation from these works. AM domain experts have\ncontributed over two dozen review papers to summarize these\nworks. However, information specific to AM and AI contexts still\nrequires manual effort to extract. The recent success of\nfoundation models such as BERT (Bidirectional Encoder\nRepresentations for Transformers) or GPT (Generative Pre-\ntrained Transformers) on textual data has opened the possibility\nof expediting scientific information extraction. We propose a\nframework that enables collaboration between AM and A\u0399\nexperts to continuously extract scientific information from data-\ndriven AM literature. A demonstration tool is implemented based\non the proposed framework and a case study is conducted to\nextract information relevant to the datasets, modeling, sensing,\nand AM system categories. We show the ability of LLMs (Large\nLanguage Models) to expedite the extraction of relevant\ninformation from data-driven AM literature. In the future, the\nframework can be used to extract information from the broader\ndesign and manufacturing literature in the engineering\ndiscipline.", "sections": [{"title": "1. INTRODUCTION", "content": "Additive manufacturing (AM), commonly known as 3D\nprinting, fabricates parts layer-by-layer [1]. Offering unique\nbenefits, the process can rival conventional manufacturing\ntechniques. This has inspired significant research efforts into the\ntechnology aimed at enhancing its maturity for industrial\nadoption. A major portion of the research from recent years has\nrelied on machine learning (ML) or deep learning (DL)-based\napproaches following the success of advanced data analytics\ntechniques [2, 3]. The scientific works at the intersection of two\ngrowing disciplines are extremely information-rich. It is critical\nto extract relevant information from the incoming literature flux\nin order to reproduce and adapt these solutions for real-world\napplications.\nThe plethora of emerging literature has led to several state-\nof-the-art reviews to summarize the development and highlight\nthe future of technology [1-3]. These reviews are divided across\nprocess technologies, applications, and solution types with\nvarying scopes. This leads to subjective and time-irrelevant\ninformation being captured. The efforts to summarize data-\ndriven AM research are limited in scope due to several reasons\nand fail to provide an all-encompassing reusable approach to\ninformation retrieval. Solutions capable of extracting the most\nrelevant information from data-driven AM research are needed\nthat can be re-used across a range of topics (e.g., technologies,\napplications, and data analytics solutions) in the field.\nThe challenge of extracting relevant information from\nscience and engineering publications is not new and dates back\nto the 1960s [4]. Many scientific disciplines are faced with the\nhigh flux of newly published literature limiting access to relevant\ninformation [5]. As a result, Scientific Information Extraction, or\nSciIE is an established field though its maturity varies across\ndisciplines. In general, Information Extraction or IE refers to a\nset of techniques in Natural Language Processing (NLP) that\nenable automated retrieval of structured information from text\n[6]. The solution to extract information can take many forms\nonce the raw data is processed and cleaned. In their review on\nSciIE, Hong et al. identified vocabulary generation, text\nclassification, named entity recognition, and relationship\nextraction as some of the steps in the information retrieval\npipeline [7]."}, {"title": "2. INFORMATION EXTRACTION FRAMEWORK", "content": "The proposed IE framework is divided into three major\ncomponents namely the base IE system, paragraph classification\ntier, and the query tier. The base IE system acts as the engine of\nthe IE framework and allows AM and AI researchers to interact\nthrough a GUI to iteratively improve the answers coming from\nthe query tier. Figure 1 presents an overview of the framework\nand outlines the key steps of each component. These components\nare explained in detail below."}, {"title": "2.1 Base IE System", "content": "The base IE system acts as the engine of our IE framework\nallowing users to upload scientific articles as PDF files, parsing\nthe PDF files into paragraphs and equations for downstream\nretrieval, allowing AM experts to search relevant paragraphs\nthrough regular or semantic search, and customizing retrievals to\neffectively shortlist relevant paragraphs.\nBased on the methodology of Lo et al., the base IE system\nprocesses user-provided PDFs of scientific AM articles with\nGROBID [10]. Metadata such as publication title, abstract,\nauthors, date and DOI are first extracted. Then GROBID parses\na paper's paragraphs and equations organized by section; as well\nas figures, tables and references (with corresponding links in the\ntext). Lastly, the different parsing outputs are saved to the user's\nlibrary for use in the IE system, along with the original PDFs for\nuser reference. Despite its widespread use, GROBID still\nstruggles with certain PDFs, displaying parsing errors like\nmissing or duplicate paragraphs, invalid organization of sections,\nincorrect or missing metadata, and illegible equations. In\nalignment with our human-centered considerations, we ensure\ntransparency and iterative improvement by letting users view\nand correct parsed results with the original PDFs for reference.\nIE systems typically involve an information retrieval (IR)\nstep, where relevant documents are first retrieved [11]. Because\nour use case involves extracting information from a given paper,\nwe retrieve relevant paragraphs instead. Specifically, we\ngenerate embeddings for paragraphs and for queries associated\nwith four categories of interest defined by the AM experts (e.g.,\ndata, modeling, sensing, and AM systems) We use cosine\nsimilarity to retrieve the paragraphs that are most similar to a\ngiven query. Because of computational constraints and\nmaintainability concerns associated with hosting the IE system,\nwe decided to use the OpenAI embeddings API instead of our\nown models.\nWhile we could perform IE on the full text of a paper, this\nhas significant drawbacks that motivate paragraph retrieval.\nFirst, for a given information item, typically only a small portion\nof the full text contains the desired information. Conversely,\nfeeding full-texts into our LLM-based IE model is infeasible or\ntoo costly to be viable. Second, limiting our LLM-based IE\nmodel to retrieved paragraphs which are typically quickly\nreadable and self-coherent- enables rapid and easy user-\nverification of our IE system's output by cross-referencing with\nretrieved passages. As errors are inevitable in any IE system, and\npernicious with LLMs, this human-centered consideration helps\nbuild trust through transparency (Explained in Section 3.2).\nSimilar to Dunn et al., we use LLMs to perform IE on\nscientific texts with a sequence-to-sequence formulation [12].\nHowever, in the process of participatory design (Explained in\nSection 3.4), we found that rigidly structured outputs did not lend\nthemselves well to extracting specific information items across\nmultiple categories. Instead, we prompt the LLM to synthesize\nthe retrieved passages, extracting the information that is relevant\nto the query of a given information item, or clearly indicating\ncases where no such information is present. As mentioned\nearlier, we use the OpenAI chat API instead of our own models\ndue to constraints on computational resources and\nmaintainability."}, {"title": "2.2 Paragraph Classification Tier", "content": "We expect to find the relevant information in specific\nparagraphs and hence there is value in training paragraph\nclassifiers to further expedite the IE process. In the long term,\nclassifiers specific to a certain domain can quickly filter the\nrelevant paragraph from the whole article. These relevant\nparagraphs can then be used in the query tier to provide specific\nanswers to the readers. It is also important to mention that\ncurrently the paragraph classification is done at an abstract level\n(e.g., data as compared to specific data characteristics or\nmodeling as compared to specific modeling details) to use\nshallow and light-weight classifiers that simplify the training\nprocess.\nOne of the key goals at this stage is to prepare global\nclassifier(s) for each field to quickly filter the relevant paragraph\nfrom irrelevant. The accuracy of the classifiers is expected to\ngrow gradually as the domain experts read through the papers\nand label the paragraphs in the base IE system. As a result, this\nwill incrementally decrease the effort of manually going through\neach article in the base IE system. Nonetheless, the long-term\neffectiveness of these classification models will depend on\nregular fine-tuning with newly labeled paragraphs. This could be\ninspired by the poor performance of the trained classifiers on\ncertain components of scientific information.\n$\\lij = \\sigma(w pi + bj)$"}, {"title": "2.3 Query Tier", "content": "The question or query tier represents the last component of\nthe proposed IE framework. It simply allows the users to ask a\nspecific question and expect a well-formulated answer. For this\npurpose, the filtered paragraphs are fed into a GPT model along\nwith the user query to output the answer. Currently, a query\nfunction is available in the base IE system to directly filter the\ninformation. In the future, we expect query functions to also\ninteract with the outputs of classification models once these are\nintegrated into the base IE system. Equation 3 represents the\nstructure of the prompt where both query (Q) and paragraphs (Pi)\nare fed to the model while being kept apart through a separator\n(SEP).\n$GPT\u2081 = Q + |SEP| + P\u2081 ... + |SEP|+ Pn$"}, {"title": "3. HUMAN-AI TEAMING FEATURES", "content": "The Human-AI teaming is central to the proposed IE\nframework to keep both AM and AI researchers in the loop as\nthe information retrieval pipelines specific to a certain domain\nare optimized. These are oftentimes referred to as human-\ncentered considerations in the broader NLP literature. In the\ncontext of data discovery, a problem similar to IE, Gregory and\nKoesten refine the notion of \"human-centered\" as thinking from\nthe perspective of the person(s) engaging in the activity; with a\nfocus on the interaction process and the \"user\" experience, taking\ninto account different contexts and needs [13]. In a different vein,\nEgan et al. present \u201cuser-centered\u201d NLP systems as a human-\ncomputer collaboration where computers do what they do well\n(process large amounts of information, filter, sort and prioritize)\nand humans do what they do well (assess, select, and refine with\ndomain expertise) [14]. And lastly, on a more abstract level,\nKotnis et al. define \"human-centric\u201d NLP research as a process\nwhere human stakeholders actively participate in the research\n[15]. In this section, we discuss several human-centered\nconsiderations which relate to these formulations and have\ninfluenced the development of our IE system."}, {"title": "3.1 User Interface", "content": "A prototype tool is implemented that reflects the base IE\nsystem explained in the previous section. Figure 2 shows an\noverview of the tool. The tool provides several functionalities to\nthe users including the option to create libraries to group PDF\nfiles from similar domains. Each library provides a list of papers\nand highlights the author and publication data. The uploaded\nPDF files can be viewed as-is. In addition, a simple text search\nor semantic search can be performed on the parsed PDF files."}, {"title": "3.2 Transparency and Trust", "content": "IE systems ideally improve the efficiency of their users,\nhowever, Schleith et al. suggest that a lack of transparency can\nlead to a lack of trust [16]. This lack of trust can in turn undo\nefficiency gains as users spend more time carefully reviewing\nsystem outputs they do not trust. Similarly, but in the context of\nLLM-generated summaries, Cheng et al. present appropriate\ntrust as enabling users to decide whether or not to rely on a given\nsystem output; which in turn requires transparency [17]. These\nchallenges are all-the-more important for LLMs, which are\nknown to generate convincing but erroneous confabulations.\nWe build our IE system around this human-centered\nconsideration of trust by transparency in a variety of ways. First,\nas noted in the base IE system, errors are introduced as early as\nthe data parsing stage. By overlaying interactions with the IE\nsystem on top of this raw data, we enable users to more easily\ncatch and correct errors related to parsing, building appropriate\ntrust. Additionally, transparency on the data level is essential for\nmitigating potential issues of data cascades [18].\nAs mentioned in Section 2.1, we also introduce an\nintermediate retrieval step before performing LLM-based IE.\nWhile this can improve factual consistency, it does not\ncompletely prevent confabulation [19]. However, adapting the\ninterface to enable streamlined cross-referencing of system\noutputs with retrieved passages (specifically paragraphs, to\nfacilitate quick verification) builds appropriate trust where\neliminating errors is otherwise infeasible."}, {"title": "3.3 Iterative Improvement", "content": "While transparency builds appropriate trust by supporting\nusers in deciding whether to rely on system outputs or not,\niterative improvements in IE systems can minimize the rate at\nwhich users should decide not to rely on a given output,\nimproving their experience. More specifically, human feedback\nwith human-in-the-loop approaches can be leveraged to improve\nthe reliability of IE system outputs [20]. We adapt this human-\ncentered consideration by enabling users to create custom\nretrievals where they can provide feedback on retrieved passages\nand iteratively improve the reliability of the custom retrieval.\nMore generally, Rahman and Kandogan find that human-in-the-\nloop IE workflows are typically iterative in nature, characterized\nby information foraging and sensemaking loops as users\niteratively improve their understanding of the task and the data\n[21]. We try to support this consideration and give users the\nflexibility required for this kind of iteration. Notably, we give\nusers fine-grained control over the underlying retrieval and\nextraction systems so they can experiment with different\napproaches."}, {"title": "3.4 Participatory Design", "content": "An important consideration throughout this work has been\nparticipatory design: ensuring a relationship of meaningful co-\ncreation and mutual learning between users and researchers [22,\n23]. This collaborative approach between developers (machine\nlearning researchers) and users (mechanical engineering\nresearchers) enabled iterative refinements throughout the\ndevelopment of the IE system prototype; from initial\nbrainstorming and problem formulation to interface adaptations\nthat address user-identified limitations of the underlying\nmachine learning models. Crucially, our approach is\nfundamentally human-in-the-loop rather than human-on-the-\nloop. In other words, users and researchers actively participate in\na task rather than passively supervising or validating its\nautomated completion. We found this dynamic played a\nsignificant role in fostering participatory design throughout\ndevelopment."}, {"title": "4. CASE STUDY", "content": "Inspired by the increasing scientific publications as shown\nin Figure 3, a case study was conducted using literature at the\nintersection of AM and ML. It is particularly challenging to find\nkey components of information quickly and effectively from\nliterature at the intersection of two growing fields. The tool\nenables an interactive way to query the information required and\nhence provides an opportunity to go through the literature\nquickly as compared to relying on existing reviews. The reviews\nbecome outdated with time and are limited in the way\ninformation can be represented. In addition to providing a faster\nand effective way to retrieve key information components, the\ntool can be used for other domains and applications so as to\nprovide a reproducible pipeline for SciIE."}, {"title": "4.1 Defining Relevant AM+Al Information", "content": "We categorized the information contained within the data-\ndriven AM literature into four categories which jointly represent\nmost of the key information required to understand and evaluate\nthe presented research. These categories are listed below:\nData Relevant: Information representing data for ML\napplications such as data characteristics, experimental settings,\ndata preparation, data processing, and data availability [24, 25].\nModel Relevant: Information related to ML-based modeling\nsuch as the algorithm, training process, the compute hardware &\nsoftware, and model availability.\nSensing Relevant: Information relevant to sensing technique\nand equipment such as the physical phenomenon, sensor type,\nsensor specifications, sensor settings, and sensor deployment.\nSystem Relevant: Information representing manufacturing\ntechnology, hardware, and materials used"}, {"title": "4.2 Collecting Research Articles", "content": "In order to conduct the case study, we retrieved 100 research\narticles representing ML-based research on in-process\nmonitoring and quality prediction challenges in AM. The latest\npublication year among the articles is 2023 whereas no limit was\nset on the starting year. These papers represent a diverse and\ncomprehensive body of research in ML-driven AM research. All\narticles were collected from Scopus. The decision to use 100\nresearch articles to validate the IE pipeline was made to get a\nrepresentative dataset spanning various subdomains in AM and\nML. The PDF files of all articles were downloaded and grouped\ninto an \"IE Validation\" library inside the prototype tool. As soon\nas the PDF of an article is uploaded, it is parsed at the backend\nto support subsequent search, labeling, and retrieval."}, {"title": "4.3 Searching, Labeling and Retrieval Customization", "content": "Once the PDF files were added in the prototype tool, these\nwere parsed to act as the input of the embedding model. The\ncurrent version of the prototype tool used the OpenAI text-\nembedding-ada-002 model to provide a paragraph-level vector\nrepresentation of the parsed PDF files. This set the stage to find\nrelevant information through semantic search where input query\nwas also featurized using the same model and the two were\ncompared using cosine similarity. However, in the case study,\nwe relied primarily on the retrieval functionality (Figure 4) and\niteratively updated it by selecting positive and negative\nparagraphs to compute the retrieval embedding of Equation 1.\nAs will be shown in the results, the ranking results and score\ngradually improved as we went through the papers labeling the\nparagraphs. This reflected the effectiveness of customizing the\nretrievals specific to each relevant information category.\nWhere specific information was not found in the top\nhighlighted paragraphs from customized retrievals, we used\nsearch functionality to find it. If the relevant paragraphs were\nfound through the search functionality, they were labeled as\npositive to include them in the retrieval embedding. Similarly,\nwhere irrelevant paragraphs were found in the top results of a\nspecific retrieval, these were marked as negative to be excluded\nfrom future results. Figure 5 shows the relevant and irrelevant\nparagraphs highlighted as positive and negative to account their\nrespective embeddings in the overall retrieval embedding. The\nlabeling process led to a multi-label text dataset for ML-driven\nAM literature to be used in the next step. To the best of the\nauthor's knowledge, this is the first NLP dataset in AM [24, 26]."}, {"title": "4.4 Classifying Paragraphs", "content": "The resulting multi-label paragraph dataset was downloaded\nfrom the prototype tool and used to develop paragraph classifiers\nas global domain models to rapidly filter relevant paragraphs for\ndownstream IE. The raw dataset represented 5039 paragraphs\nlabeled into four relevant categories namely data, model,\nsensing, and system. We introduced a fifth category for\nparagraphs that didn't belong to any of the above-mentioned\ncategories as \"irrelevant.\" This was done to evaluate the effect\nof including these paragraphs in the learning process. However,\ntheir inclusion introduced data imbalance, and these were\nsubsequently removed as a redundant category label.\nThe dataset was processed using the OpenAI embedding\nmodel to generate feature vectors for each paragraph. The\naugmented dataset was used to train a Random Forest classifier\nfrom the Scikit-learn library. Since the classifier doesn't natively\nsupport multi-target classification, we used the built-in\nMultiOutputClassifier strategy. We trained a simple multi-label\nmodel as a global classifier for ML-based AM literature. The\nclassifier can categorize the paragraph into four categories of\nrelevant information. However, irrelevant paragraphs will\nrequire to be filtered and out-of-balanced classes should be\ndown-sampled for future training of the model. The results are\npresented in the next Section."}, {"title": "4.5 Query and Response", "content": "During the case study, we used a query function that\nprompts an LLM to extract the relevant information by providing\nboth a user query and the relevant paragraph(s) to generate the\nanswer. This functionality can be used both during the labeling\nprocess to find missing information as well as after the classifiers\nhave been trained to filter the relevant paragraphs. Table 1 shows\nthe function used to prompt."}, {"title": "5. RESULTS AND DISCUSSIONS", "content": "The results from the case study are divided into two\ncategories. Table 2 shows the improvement in the ranking of"}, {"title": "6. CONCLUSIONS AND FUTURE WORKS", "content": "Inspired by the increasing frequency of research into data-\ndriven solutions of AM challenges, we propose an information\nextraction framework powered by LLMs and built around\nhuman-centered considerations. The framework is divided into\nthree components namely base IE system, classification tier, and\nthe query tier whereas the query functionality is also integrated\ninto the base system. The tool enables continuous update of the\ndatabase representing a specific scientific domain while allowing\ndomain experts to iteratively customize retrieval for LLM-based\nIE. The tool is deployed on the web and has restricted\ndevelopment access at the moment. We carried out a case study\nby building a library of 100 ML-based AM research articles and\ngoing through them to manually label and validate paragraph\ncontaining key information belonging to four categories namely\ndata, model, sensing and system. We confirm the gradual\neffectiveness of customizing retrievals as we progress through\nthe database. Moreover, the relevant paragraphs labeled as a\nresult of retrieval customization were downloaded and used to\ntrain a shallow multi-label classifier. The results of classification\non the test set indicate that it is possible to develop a global\nclassifier for a given domain thereby significantly expediting the\ninformation filtering step.\nThe future works include:\nValidating the prototype tool and proposed framework\nin another design and manufacturing subdomain.\nDefining methods and metrics to benchmark IE\nefficiency and effectiveness as compared to existing\ntools and approaches.\nIntroducing a notion of similarity threshold for relevant\nparagraphs in design and manufacturing scientific\nliterature.\nOpening the tool to the broader design and\nmanufacturing community to gather feedback."}]}