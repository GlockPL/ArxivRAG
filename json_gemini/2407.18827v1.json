{"title": "HUMAN-ARTIFICIAL INTELLIGENCE TEAMING FOR SCIENTIFIC INFORMATION\nEXTRACTION FROM DATA-DRIVEN ADDITIVE MANUFACTURING RESEARCH USING\nLARGE LANGUAGE MODELS", "authors": ["Mutahar Safdar", "Jiarui Xie", "Andrei Mircea", "Yaoyao Fiona Zhao"], "abstract": "Data-driven research in Additive Manufacturing (AM) has\ngained significant success in recent years. This has led to a\nplethora of scientific literature to emerge. The knowledge in\nthese works consists of AM and Artificial Intelligence (AI)\ncontexts that haven't been mined and formalized in an integrated\nway. It requires substantial effort and time to extract scientific\ninformation from these works. AM domain experts have\ncontributed over two dozen review papers to summarize these\nworks. However, information specific to AM and AI contexts still\nrequires manual effort to extract. The recent success of\nfoundation models such as BERT (Bidirectional Encoder\nRepresentations for Transformers) or GPT (Generative Pre-\ntrained Transformers) on textual data has opened the possibility\nof expediting scientific information extraction. We propose a\nframework that enables collaboration between AM and AI\nexperts to continuously extract scientific information from data-\ndriven AM literature. A demonstration tool is implemented based\non the proposed framework and a case study is conducted to\nextract information relevant to the datasets, modeling, sensing,\nand AM system categories. We show the ability of LLMs (Large\nLanguage Models) to expedite the extraction of relevant\ninformation from data-driven AM literature. In the future, the\nframework can be used to extract information from the broader\ndesign and manufacturing literature in the engineering\ndiscipline.", "sections": [{"title": "1. INTRODUCTION", "content": "Additive manufacturing (AM), commonly known as 3D\nprinting, fabricates parts layer-by-layer [1]. Offering unique\nbenefits, the process can rival conventional manufacturing\ntechniques. This has inspired significant research efforts into the\ntechnology aimed at enhancing its maturity for industrial\nadoption. A major portion of the research from recent years has\nrelied on machine learning (ML) or deep learning (DL)-based\napproaches following the success of advanced data analytics\ntechniques [2, 3]. The scientific works at the intersection of two\ngrowing disciplines are extremely information-rich. It is critical\nto extract relevant information from the incoming literature flux\nin order to reproduce and adapt these solutions for real-world\napplications.\nThe plethora of emerging literature has led to several state-\nof-the-art reviews to summarize the development and highlight\nthe future of technology [1-3]. These reviews are divided across\nprocess technologies, applications, and solution types with\nvarying scopes. This leads to subjective and time-irrelevant\ninformation being captured. The efforts to summarize data-\ndriven AM research are limited in scope due to several reasons\nand fail to provide an all-encompassing reusable approach to\ninformation retrieval. Solutions capable of extracting the most\nrelevant information from data-driven AM research are needed\nthat can be re-used across a range of topics (e.g., technologies,\napplications, and data analytics solutions) in the field.\nThe challenge of extracting relevant information from\nscience and engineering publications is not new and dates back\nto the 1960s [4]. Many scientific disciplines are faced with the\nhigh flux of newly published literature limiting access to relevant\ninformation [5]. As a result, Scientific Information Extraction, or\nSciIE is an established field though its maturity varies across\ndisciplines. In general, Information Extraction or IE refers to a\nset of techniques in Natural Language Processing (NLP) that\nenable automated retrieval of structured information from text\n[6]. The solution to extract information can take many forms\nonce the raw data is processed and cleaned. In their review on\nSciIE, Hong et al. identified vocabulary generation, text\nclassification, named entity recognition, and relationship\nextraction as some of the steps in the information retrieval\npipeline [7]."}, {"title": "2. INFORMATION EXTRACTION FRAMEWORK", "content": "The proposed IE framework is divided into three major\ncomponents namely the base IE system, paragraph classification\ntier, and the query tier. The base IE system acts as the engine of\nthe IE framework and allows AM and AI researchers to interact\nthrough a GUI to iteratively improve the answers coming from\nthe query tier. Figure 1 presents an overview of the framework\nand outlines the key steps of each component. These components\nare explained in detail below.\n2.1 Base IE System\nThe base IE system acts as the engine of our IE framework\nallowing users to upload scientific articles as PDF files, parsing"}, {"title": "3. HUMAN-AI TEAMING FEATURES", "content": "The Human-AI teaming is central to the proposed IE\nframework to keep both AM and AI researchers in the loop as\nthe information retrieval pipelines specific to a certain domain\nare optimized. These are oftentimes referred to as human-\ncentered considerations in the broader NLP literature. In the\ncontext of data discovery, a problem similar to IE, Gregory and\nKoesten refine the notion of \"human-centered\" as thinking from\nthe perspective of the person(s) engaging in the activity; with a\nfocus on the interaction process and the \"user\" experience, taking\ninto account different contexts and needs [13]. In a different vein,\nEgan et al. present \u201cuser-centered\u201d NLP systems as a human-\ncomputer collaboration where computers do what they do well\n(process large amounts of information, filter, sort and prioritize)\nand humans do what they do well (assess, select, and refine with\ndomain expertise) [14]. And lastly, on a more abstract level,\nKotnis et al. define \"human-centric\u201d NLP research as a process\nwhere human stakeholders actively participate in the research\n[15]. In this section, we discuss several human-centered\nconsiderations which relate to these formulations and have\ninfluenced the development of our IE system.\n3.1 User Interface\nA prototype tool is implemented that reflects the base IE\nsystem explained in the previous section. Figure 2 shows an\noverview of the tool. The tool provides several functionalities to\nthe users including the option to create libraries to group PDF\nfiles from similar domains. Each library provides a list of papers\nand highlights the author and publication data. The uploaded\nPDF files can be viewed as-is. In addition, a simple text search\nor semantic search can be performed on the parsed PDF files."}, {"title": "3.4 Participatory Design", "content": "An important consideration throughout this work has been\nparticipatory design: ensuring a relationship of meaningful co-\ncreation and mutual learning between users and researchers [22,\n23]. This collaborative approach between developers (machine\nlearning researchers) and users (mechanical engineering\nresearchers) enabled iterative refinements throughout the\ndevelopment of the IE system prototype; from initial\nbrainstorming and problem formulation to interface adaptations\nthat address user-identified limitations of the underlying\nmachine learning models. Crucially, our approach is\nfundamentally human-in-the-loop rather than human-on-the-\nloop. In other words, users and researchers actively participate in\na task rather than passively supervising or validating its\nautomated completion. We found this dynamic played a\nsignificant role in fostering participatory design throughout\ndevelopment."}, {"title": "4. CASE STUDY", "content": "Inspired by the increasing scientific publications as shown\nin Figure 3, a case study was conducted using literature at the\nintersection of AM and ML. It is particularly challenging to find\nkey components of information quickly and effectively from\nliterature at the intersection of two growing fields. The tool\nenables an interactive way to query the information required and\nhence provides an opportunity to go through the literature\nquickly as compared to relying on existing reviews. The reviews\nbecome outdated with time and are limited in the way\ninformation can be represented. In addition to providing a faster\nand effective way to retrieve key information components, the\ntool can be used for other domains and applications so as to\nprovide a reproducible pipeline for SciIE.\n4.1 Defining Relevant AM+Al Information\nWe categorized the information contained within the data-\ndriven AM literature into four categories which jointly represent\nmost of the key information required to understand and evaluate\nthe presented research. These categories are listed below:\n- Data Relevant: Information representing data for ML\napplications such as data characteristics, experimental settings,\ndata preparation, data processing, and data availability [24, 25].\n- Model Relevant: Information related to ML-based modeling\nsuch as the algorithm, training process, the compute hardware &\nsoftware, and model availability.\n- Sensing Relevant: Information relevant to sensing technique\nand equipment such as the physical phenomenon, sensor type,\nsensor specifications, sensor settings, and sensor deployment.\n- System Relevant: Information representing manufacturing\ntechnology, hardware, and materials used\n4.2 Collecting Research Articles\nIn order to conduct the case study, we retrieved 100 research\narticles representing ML-based research on in-process\nmonitoring and quality prediction challenges in AM. The latest\npublication year among the articles is 2023 whereas no limit was\nset on the starting year. These papers represent a diverse and\ncomprehensive body of research in ML-driven AM research. All\narticles were collected from Scopus. The decision to use 100\nresearch articles to validate the IE pipeline was made to get a\nrepresentative dataset spanning various subdomains in AM and\nML. The PDF files of all articles were downloaded and grouped\ninto an \"IE Validation\" library inside the prototype tool. As soon\nas the PDF of an article is uploaded, it is parsed at the backend\nto support subsequent search, labeling, and retrieval.\n4.3 Searching, Labeling and Retrieval Customization\nOnce the PDF files were added in the prototype tool, these\nwere parsed to act as the input of the embedding model. The\ncurrent version of the prototype tool used the OpenAI text-\nembedding-ada-002 model to provide a paragraph-level vector\nrepresentation of the parsed PDF files. This set the stage to find\nrelevant information through semantic search where input query\nwas also featurized using the same model and the two were\ncompared using cosine similarity. However, in the case study,\nwe relied primarily on the retrieval functionality (Figure 4) and\niteratively updated it by selecting positive and negative\nparagraphs to compute the retrieval embedding of Equation 1.\nAs will be shown in the results, the ranking results and score\ngradually improved as we went through the papers labeling the\nparagraphs. This reflected the effectiveness of customizing the\nretrievals specific to each relevant information category.\nWhere specific information was not found in the top\nhighlighted paragraphs from customized retrievals, we used\nsearch functionality to find it. If the relevant paragraphs were\nfound through the search functionality, they were labeled as\npositive to include them in the retrieval embedding. Similarly,\nwhere irrelevant paragraphs were found in the top results of a\nspecific retrieval, these were marked as negative to be excluded\nfrom future results. Figure 5 shows the relevant and irrelevant\nparagraphs highlighted as positive and negative to account their\nrespective embeddings in the overall retrieval embedding. The\nlabeling process led to a multi-label text dataset for ML-driven\nAM literature to be used in the next step. To the best of the\nauthor's knowledge, this is the first NLP dataset in AM [24, 26]."}, {"title": "4.4 Classifying Paragraphs", "content": "The resulting multi-label paragraph dataset was downloaded\nfrom the prototype tool and used to develop paragraph classifiers\nas global domain models to rapidly filter relevant paragraphs for\ndownstream IE. The raw dataset represented 5039 paragraphs\nlabeled into four relevant categories namely data, model,\nsensing, and system. We introduced a fifth category for\nparagraphs that didn't belong to any of the above-mentioned\ncategories as \"irrelevant.\" This was done to evaluate the effect\nof including these paragraphs in the learning process. However,\ntheir inclusion introduced data imbalance, and these were\nsubsequently removed as a redundant category label.\nThe dataset was processed using the OpenAI embedding\nmodel to generate feature vectors for each paragraph. The\naugmented dataset was used to train a Random Forest classifier\nfrom the Scikit-learn library. Since the classifier doesn't natively\nsupport multi-target classification, we used the built-in\nMultiOutputClassifier strategy. We trained a simple multi-label\nmodel as a global classifier for ML-based AM literature. The\nclassifier can categorize the paragraph into four categories of\nrelevant information. However, irrelevant paragraphs will\nrequire to be filtered and out-of-balanced classes should be\ndown-sampled for future training of the model. The results are\npresented in the next Section."}, {"title": "4.5 Query and Response", "content": "During the case study, we used a query function that\nprompts an LLM to extract the relevant information by providing\nboth a user query and the relevant paragraph(s) to generate the\nanswer. This functionality can be used both during the labeling\nprocess to find missing information as well as after the classifiers\nhave been trained to filter the relevant paragraphs."}, {"title": "5. RESULTS AND DISCUSSIONS", "content": "The results from the case study are divided into two\ncategories. Table 2 shows the improvement in the ranking of"}, {"title": "6. CONCLUSIONS AND FUTURE WORKS", "content": "Inspired by the increasing frequency of research into data-\ndriven solutions of AM challenges, we propose an information\nextraction framework powered by LLMs and built around\nhuman-centered considerations. The framework is divided into\nthree components namely base IE system, classification tier, and\nthe query tier whereas the query functionality is also integrated\ninto the base system. The tool enables continuous update of the\ndatabase representing a specific scientific domain while allowing\ndomain experts to iteratively customize retrieval for LLM-based\nIE. The tool is deployed on the web and has restricted\ndevelopment access at the moment. We carried out a case study\nby building a library of 100 ML-based AM research articles and\ngoing through them to manually label and validate paragraph\ncontaining key information belonging to four categories namely\ndata, model, sensing and system. We confirm the gradual\neffectiveness of customizing retrievals as we progress through\nthe database. Moreover, the relevant paragraphs labeled as a\nresult of retrieval customization were downloaded and used to\ntrain a shallow multi-label classifier. The results of classification\non the test set indicate that it is possible to develop a global\nclassifier for a given domain thereby significantly expediting the\ninformation filtering step.\nThe future works include:\n- Validating the prototype tool and proposed framework\nin another design and manufacturing subdomain.\n- Defining methods and metrics to benchmark IE\nefficiency and effectiveness as compared to existing\ntools and approaches.\n- Introducing a notion of similarity threshold for relevant\nparagraphs in design and manufacturing scientific\nliterature.\n- Opening the tool to the broader design and\nmanufacturing community to gather feedback."}]}