{"title": "A Contextual Combinatorial Bandit Approach to Negotiation", "authors": ["Yexin Li", "Zhancun Mu", "Siyuan Qi"], "abstract": "Learning effective negotiation strategies poses two key challenges: the exploration-exploitation dilemma and dealing with large action spaces. However, there is an absence of learning-based approaches that effectively address these challenges in negotiation. This paper introduces a comprehensive formulation to tackle various negotiation problems. Our approach leverages contextual combinatorial multi-armed bandits, with the bandits resolving the exploration-exploitation dilemma, and the combinatorial nature handles large action spaces. Building upon this formulation, we introduce NegUCB, a novel method that also handles common issues such as partial observations and complex reward functions in negotiation. NegUCB is contextual and tailored for full-bandit feedback without constraints on the reward functions. Under mild assumptions, it ensures a sub-linear regret upper bound. Experiments conducted on three negotiation tasks demonstrate the superiority of our approach.", "sections": [{"title": "1. Introduction", "content": "Negotiation serves as a fundamental process that underpins interaction among diverse agents across a wide spectrum of domains, ranging from diplomacy (Paquette et al., 2019; FAIR et al., 2022) and resource allocation (Lewis et al., 2017; Cao et al., 2018) to trading (Bagga et al., 2020). In these scenarios, an agent, represented as negotiator a, engages in negotiation with various counterparts g, with its state evolving. At each time step, negotiator a proposes a bid and receives feedback indicating whether the counterpart g accepts or rejects the proposal. Successful acceptance leads to a deal, while rejection leads to termination or further negotiation, possibly with counter-proposals from the counterpart. These negotiations can vary in form, and Figure 1 illustrates three representative negotiation problems: trading, resource allocation, and multi-issue negotiation. As negotiation experiences accumulate, an agent should continuously improve its negotiation ability.\nHowever, effectively exploiting past experiences in subsequent negotiations is challenging in the following aspects. Exploration-exploitation dilemma: As counterparts vary and the agent's state evolves, over-exploiting historical data may result in sub-optimal performance, while excessive exploration may make the counterpart lose patience. Existing works on negotiation (Lewis et al., 2017; Liu & Zheng, 2020; Sengupta et al., 2022) tend to neglect exploration, primarily focusing on exploitation, or simply explore by UCT (Buron et al., 2019), without considering observable contexts. Large action spaces: Consider a trading task in which our negotiator possesses items \\(V_1\\) while the counterpart holds items \\(V_2\\). The potential bid can be any subset of the union \\(V = V_1 \\cup V_2\\), resulting in \\(2^{|V|}\\) possible choices. Some studies (Cao et al., 2018; Bakker et al., 2019; Bagga et al., 2020) employ reinforcement learning to acquire negotiation strategies, but they primarily focus on tasks involving action spaces limited to a few hundred discrete actions or low-dimensional continuous action spaces. Partial observations: The profiles of counterparts, including their preferences and desires, cannot be fully observed. Relying solely on observable contexts for negotiation can be ineffective. Complicated acceptance functions: Inferring the likelihood of the counterpart accepting a bid remains challenging, even when their hidden states are known.\nIn this paper, we formulate negotiation problems using contextual combinatorial multi-armed bandits (Li et al., 2010; Chen et al., 2013; Qin et al., 2014; Wen et al., 2015; Chen et al., 2018; Agarwal et al., 2021; Nie et al., 2022) to address the exploration-exploitation dilemma and handle the large action spaces of combinatorial cardinality. Although negotiation involves a series of actions, unlike in reinforcement learning, where actions may lead to state transitions, bid actions in negotiation do not inherently trigger such transitions. Agents accumulate knowledge about their counterparts through interactions. Consequently, the bandit-based formulation is well-suited for negotiation problems.\nIn our formulation, an arm denotes an item involved in the negotiation, while a super arm signifies a bid composed of multiple items. The term acceptance is specifically designated to represent the reward, with a value of 1 assigned when the counterpart accepts the bid and 0 assigned in the case of rejection. Consequently, our primary objective is the systematic selection of super arms to gain a comprehensive understanding of the expected acceptance of each super arm while ensuring a substantial cumulative benefit in the long run. This formulation involves full-bandit feedback, where information regarding the acceptance of individual items within the bid remains inaccessible, and only an aggregate acceptance value for the entire bid is available. Otherwise, the feedback is referred to as semi-bandit. Presently, most works (Qin et al., 2014; Wen et al., 2015; Chen et al., 2018; Hwang et al., 2023) on combinatorial bandits rely on semi-bandit feedback. Although there are works (Rejwan & Mansour, 2020; Agarwal et al., 2021; Nie et al., 2022; Fourati et al., 2023) that consider full-bandit feedback, they are non-contextual and often subject to specific constraints.\nBuilding upon the above formulation, we propose a contextual algorithm for full-bandit feedback, named Negotiation UCB (NegUCB), to learn negotiation strategies and adeptly address the exploitation-exploration dilemma and the challenge of large action spaces. Moreover, NegUCB incorporates hidden states to tackle the issue of partial observations and handles diverse acceptance functions through kernel regression (Schulz et al., 2018; Vakili et al., 2023). Under mild assumptions, NegUCB's regret upper bound is guaranteed to be sub-linear with respect to the number of negotiation steps and independent of the bid cardinality, distinguishing itself from existing works on either semi-bandit or full-bandit feedback.\nIn summary, this paper makes three major contributions. First, we provide a comprehensive formulation for diverse types of negotiation problems in \u00a7 3.1. Second, we propose NegUCB to learn negotiation strategies, effectively addressing the prevalent challenges in negotiation in \u00a7 3.2. Lastly, we provide theoretical insights in \u00a7 3.3 and conduct experiments on representative negotiation tasks in \u00a7 4, highlighting the advantages and effectiveness of our method."}, {"title": "2. Related Work", "content": "Deep reinforcement learning has been applied to learning negotiation strategies. For instance, Rodriguez-Fernandez et al. (Rodriguez-Fernandez et al., 2019) adopt a DQN-based model (Mnih et al., 2015) to solve the contract negotiation problem characterized by discrete state and action spaces. Lewis et al. (Lewis et al., 2017) combine supervised learning with reinforcement learning to acquire negotiation strategies in a resource allocation task. RLBOA (Bakker et al., 2019) discretizes continuous action and state spaces and employs tabular Q-learning to learn bidding strategies, although it may encounter issues related to the curse of dimensionality. ANEGMA (Bagga et al., 2020) uses actor-critic (Bhatnagar et al., 2009) to mitigate the dimensionality challenge. Cao et al. (Cao et al., 2018) design two communication protocols to explore the emergence of communication when two agents negotiate. However, these approaches struggle to handle large discrete action spaces (Dulac-Arnold et al., 2016) and often give minimal consideration to exploration.\nSome studies investigate alternative approaches to negotiation. For instance, Buron et al. learn bidding strategies relying on Monte Carlo tree search (Buron et al., 2019). A decision tree-based negotiation assistant (Liu & Zheng, 2020) is specifically designed to predict prices in a car trading platform. Sengupta et al. (Sengupta et al., 2022) demonstrate a transfer learning-based solution to adapt base negotiation strategies to new counterparts rapidly. Cicero (FAIR et al., 2022) achieves mastery in the game of Diplomacy by integrating reinforcement learning with a language model. Nevertheless, these approaches deal with highly specific problems or issues in negotiation, yet they have not effectively tackled the prevalent challenges discussed above."}, {"title": "2.2. Multi-Armed Bandits", "content": "LinUCB (Li et al., 2010) has been introduced to formulate recommendation as a contextual bandit problem, assuming linearity in the reward concerning user and item contexts. It has demonstrated effective performance in recommendation and guarantees a sub-linear regret bound (Chu et al., 2011). FactorUCB (Wang et al., 2017) also makes a linearity assumption but considers hidden features alongside the observable contexts, leading to an improved click rate in recommendation. To overcome the linearity assumption in contextual bandits, KernelUCB (Valko et al., 2013; Chowdhury & Gopalan, 2017) transforms contexts into a high-dimensional space and applies LinUCB in this new space. Neural-UCB (Zhou et al., 2020) attempts to leverage deep neural networks to capture the relationship between contexts and rewards. However, its computational complexity makes it challenging to generalize to real tasks.\nCUCB (Chen et al., 2013) establishes a general framework for combinatorial multi-armed bandits. C2UCB (Qin et al., 2014) and ComLinUCB (Wen et al., 2015) incorporate contexts into combinatorial bandits based on the same linearity assumption as LinUCB. CC-MAB (Chen et al., 2018) focuses on problems with volatile arms and submodular reward functions. CN-UCB (Hwang et al., 2023) employs neural networks to address contextual combinatorial bandit problems, facing the computational limitation as Neural-UCB. However, these algorithms operate within semi-bandit feedback. Another relevant setting is the full-bandit feedback, in which rewards for individual arms are inaccessible. Algorithms designed for full-bandit feedback include CSAR (Rejwan & Mansour, 2020), DART (Agarwal et al., 2021), ETCG (Nie et al., 2022), and RGL (Fourati et al., 2023). However, their reward functions adhere to linearity or sub-modularity, and none of them consider contexts. In contrast, NegUCB is contextual, combinatorial, and tailored for full-bandit feedback without constraints on the reward functions. A comparative analysis is presented in Table 1."}, {"title": "3. Methodology", "content": "Unless otherwise specified, uppercase symbols represent sets, bold uppercase symbols denote matrices, bold lowercase symbols represent vectors, and lowercase symbols denote scalars or functions. \\(I_d\\) refers to an identity matrix with dimensions \\(d \\times d\\), and \\(O_d\\) represents a zero vector of size \\(d \\times 1\\). Kronecker product is denoted as \\(\\otimes\\). Frobenius norm of a matrix and the \\(l_2\\) norm of a vector are respectively denoted as \\(|| X ||_F\\) and \\(||x||\\). Mahalanobis norm of a column vector \\(x\\) based on matrix \\(A\\) is denoted as \\(||x||_A = \\sqrt{x^T A x}\\). \\(vec(A)\\) is the vectorization operator of matrix \\(A\\)."}, {"title": "3.1. Negotiation Formulation", "content": "In this section, we provide a comprehensive formulation that applies to various types of negotiation problems. First, we outline the negotiation framework, detailing the strategy for making proposals when it is our turn to bid and the criteria for deciding whether to accept or reject a bid from the counterpart. Next, we formulate the critical component within this framework."}, {"title": "3.1.1. NEGOTIATION FRAMEWORK", "content": "Denote the pool of the counterpart negotiators as \\(U\\), with a cardinality of \\(|U| = m\\). The item pool is represented as \\(V\\), where \\(|V| = n\\). It is essential to acknowledge that negotiation with a new counterpart may occur at any time, leading to an increase in \\(m\\) over time. Additionally, new items may be added to \\(V\\). Without loss of generality, we assume these two pools \\(U\\) and \\(V\\) to be constant. At time step \\(\\tau\\), our negotiator has a valid bid set \\(B\\), encompassing all feasible bids it can propose at this time. For example, in a trading task, a bid specifies which items our negotiator proposes to give to the counterpart and which items it requests in return. The validity of a bid is determined by whether our negotiator possesses the items it proposes to give. More designation of bids under various negotiation scenarios will be elaborated later.\nFor a valid bid \\(b \\in B\\) of our negotiator at time \\(\\tau\\), the acceptance function \\(r_\\tau\\) assesses whether the counterpart may accept or reject the bid, denoted by \\(r_\\tau(b) = 1\\) or \\(r_\\tau(b) = 0\\). This function is unknown and needs to be learned. Additionally, there exists a benefit function \\(f_\\tau\\), that measures the potential benefit of the bid to our negotiator, a metric highly dependent on the specific problem. Consequently, the optimal bid for our negotiator to propose at time step \\(\\tau\\) is determined by Equation 1, aiming to maximize its expected benefit. During negotiation, if our negotiator intends to propose a bid, it chooses a valid bid using Equation 1. Supposing our negotiator receives a bid \\(b\\) from the counterpart, it is evident that \\(r_\\tau(b) = 1\\); thus, our negotiator decides whether to accept the bid by evaluating if the bid is valid and optimal after setting \\(r_\\tau(b) = 1\\).\n\\[b_\\tau = \\arg \\max_b r_\\tau(b) \\times f_\\tau(b)\\]"}, {"title": "3.1.2. ACCEPTANCE FUNCTION", "content": "As the benefit function \\(f_\\tau\\) is dependent on the specific problem and crafted manually, it is not the primary focus of this work. Instead, we focus on learning the acceptance function \\(r_\\tau\\) using contextual combinatorial multi-armed bandits (Chen et al., 2013; Qin et al., 2014; Wen et al., 2015; Chen et al., 2018; Nie et al., 2022), where arms represent items in \\(V\\), super arms denote bids, and rewards are the acceptance labels. Consequently, the objective is to iteratively put forth beneficial bids to understand the expected acceptance of each bid by various counterparts while ensuring a substantial cumulative benefit in the long run. In the following, we review the details based on Figure 2.\nItems in pool \\(V\\) have contexts denoted as row vectors \\({y_w | w = 1, 2, ..., n}\\), collectively forming an item context matrix \\(Y = [y_1; y_2; ...; y_n]\\), as depicted in Figure 2 (a). At time step \\(\\tau\\), our negotiator and the counterpart form a negotiator pair, characterized by contexts denoted as a row vector \\(x_\\tau\\), as depicted in Figure 2 (b). It is worth noting that \\(x_\\tau\\) corresponds to one of the \\(m\\) counterparts in \\(U\\). In other words, it is a row of the negotiator context matrix \\(X = [x_1; x_2; ...; x_m]\\). In this work, we use \\(x_\\tau\\) or \\(x_w, w = 1, 2, ..., m\\), interchangeably to either highlight the time step or the counterpart index. Addressing the partial observation issue, we assume hidden states \\(U = [u_1; u_2; ...; u_m]\\) for the \\(m\\) negotiator pairs. Then the acceptance function \\(r_\\tau\\) at time step \\(\\tau\\) is estimated through Equation 2, where \\(\\Theta\\) in the first term represents the function parameters, \\(u\\) in the second term signifies the hidden state of the current negotiator pair. Specifically, the first term estimates the partial acceptance of the bid based on observed contexts, while the second term evaluates the partial acceptance of the bid based on hidden states.\n\\[r_\\tau(b_\\tau) = \\langle \\phi(x_\\tau), \\Theta \\psi(Y, b_\\tau) \\rangle + u_\\tau^T \\psi(Y, b_\\tau)\\]\n\\[\\psi(Y, b_\\tau) = \\phi(\\psi(Y, b_\\tau))\\]\nIn the first term of Equation 2, function \\(\\phi\\) transforms context \\(x\\) into a \\(h\\)-dimensional space \\(H\\) where \\(h\\) can be infinite. \\(\\psi(Y, b)\\) is expressed in Equation 3, where function \\(\\psi\\) extracts the context of bid \\(b\\) from the item context matrix \\(Y\\). A possible example of \\(\\psi\\) is provided in Figure 2 (c). Following this, function \\(\\phi\\) further transforms the bid context into a high-dimensional representation within space \\(H\\). Specifically, function \\(\\phi\\) transforms contexts into high-dimensional representations, allowing the acceptance function to operate non-linearly concerning the observed contexts.\nGiven historical negotiation data from step \\(1, 2, ..., \\tau\\), we aim to optimize the following objective function to derive functions \\(\\phi\\) and \\(\\psi\\), parameters \\(\\Theta\\) and hidden states \\(U\\), then use them for the subsequent time step \\(\\tau + 1\\). \\(\\lambda_1\\) and \\(\\lambda_2\\) are hyper-parameters for the regularization terms, \\(r_t\\) represents the actual acceptance at time \\(t\\), as depicted in Figure 2 (d), and \\(\\hat{r}_t\\) denotes the acceptance estimated by Equation 2.\n\\[\\min_{\\psi, \\phi, \\Theta, U} C = \\sum_{t=1}^T |r_t - \\hat{r}_t|^2 + \\lambda_1 ||\\Theta||^2 + \\lambda_2 ||U||^2\\]"}, {"title": "3.2. Negotiation UCB", "content": "In this subsection, building upon the above formulation, we introduce the NegUCB algorithm, a simple yet effective approach. In this algorithm, bids are represented as indicator vectors indicating the items involved in each bid. Please refer to \u00a7 B.2 for detailed examples.\nSince simultaneously deriving the functions \\(\\phi\\) and \\(\\psi\\), as well as the parameters \\(\\Theta\\) and \\(U\\) is challenging, we assume the format of the function \\(\\psi\\) in Assumption 3.1. In \u00a7 3.2.1, we provide the closed-form solutions for \\(\\Theta\\) and \\(U\\), which depend on function \\(\\phi\\). In \u00a7 3.2.2, we use kernel regression (Schulz et al., 2018) to eliminate the dependency on function \\(\\phi\\). At last, we summarize the NegUCB algorithm in \u00a7 3.2.3.\nIf the contexts of items are characterized by their basic features, the extraction function \\(\\psi\\) in Equation 5 can accurately capture the context of bid \\(b\\). In other words, it encompasses substantial information about the items included in the bid.\n\\[\\psi(Y, b) = Y^T b\\]\nDespite the linearity assumption on \\(\\psi\\), the acceptance function is non-linear because of the transforming function \\(\\phi\\)."}, {"title": "3.2.1. PARAMETERS", "content": "Obviously, the objective function \\(C\\) is not jointly convex concerning both \\(\\Theta\\) and \\(U\\). However, it is convex concerning one parameter if the other one is fixed. Therefore, we employ an alternative least square optimization approach, iterating the calculation of one parameter with a closed-form solution while keeping the other parameter fixed. Based on Assumption 3.1, the closed-form solution for \\(\\Theta\\) is as Equation 6, while that for \\(U\\) is as Equation 7.\n\\[vec(\\Theta) = (A_\\tau A_\\tau^T + \\lambda_1 I_{h^2})^{-1}A_\\tau^T(r_\\tau - D_\\tau vec(U))\\]\n\\[vec(U) = (D_\\tau D_\\tau^T + \\lambda_2 I_{mh})^{-1}D_\\tau^T(r_\\tau - A_\\tau vec(\\Theta))\\]\nRows of matrices \\(A_\\tau\\) and \\(D_\\tau\\) are samples as \\(\\phi(b_t^T Y) \\otimes \\phi(x_t)\\) and \\(\\phi(b_t^T Y) \\otimes p_t\\) where \\(p_t \\in \\mathbb{R}^{1 \\times m}\\) is a one-hot vector representing the counterpart index at time step \\(t = 1, 2, ..., T\\). It is evident that the solutions for parameters \\(\\Theta\\) and \\(U\\) are contingent on the transformation function \\(\\phi\\), which can take various forms, such as polynomial functions, neural networks, etc., and thus needs to be learned."}, {"title": "3.2.2. TRANSFORMATION FUNCTION", "content": "Given the limited amount of negotiation data with various counterparts, learning \\(\\phi\\) becomes intractable if it involves many parameters, such as in the case of neural networks. In NegUCB, we utilize Reproducing Kernel Hilbert Spaces within kernel functions to avoid the need for learning \\(\\phi\\), enhancing efficiency. Moreover, since iterating among three components, i.e., learning \\(\\phi\\), \\(U\\), and \\(\\Theta\\), is highly unstable, NegUCB iterates between learning \\(U\\) and \\(\\Theta\\), significantly improving the learning stability.\nCorresponding to matrices \\(A_\\tau\\) and \\(D_\\tau\\) dependent on function \\(\\phi\\), we define matrices \\(K_\\tau\\) and \\(Z_\\tau\\). Each entry \\((K_\\tau)_{t,j}\\) and \\((Z_\\tau)_{t,j}\\) are the dot product of the \\(t\\)-th and \\(j\\)-th samples of \\(A_\\tau\\) and \\(D_\\tau\\), respectively. By Assumption 3.2, we can calculate \\(K_\\tau\\) and \\(Z_\\tau\\) without knowing \\(\\phi\\).\nEach entry of \\(K\\) and \\(Z\\) can be calculated by Equation 8 and Equation 9 respectively, where \\(t, j = 1, 2, ..., T\\), and \\(K_1\\) and \\(K_2\\) are two kernel functions.\n\\[(K_\\tau)_{t,j} = K_1(x_t, x_j) \\times K_1(b_t^T Y, b_j^T Y)\\]\n\\[(Z_\\tau)_{t,j} = \\begin{cases}K_2(b_t^T Y, b_j^T Y) & p_t = p_j \\\\ 0 & p_t \\neq p_j\\end{cases}\\]\nDenoting the above entry values as \\(k_{tj}\\) and \\(z_{tj}\\), then the kernel vectors at time step \\(\\tau\\) are \\(k_\\tau = (k_{1\\tau}, k_{2\\tau}, ..., k_{\\tau,\\tau})\\) and \\(z_\\tau = (z_{1\\tau}, z_{2\\tau}, ..., z_{\\tau,\\tau})\\), and \\(K_\\tau\\) and \\(Z_\\tau\\) are the kernel matrices. Based on Assumption 3.2, we have Lemma 3.3 to approximate the acceptance function.\nInstead of learning transformation function \\(\\phi\\), parameters \\(\\Theta\\) and \\(U\\), and then estimating \\(r_{\\tau+1}(b)\\) by Equation 2, it is equivalent to iterate Equation 10 and Equation 11, then estimate \\(r_{\\tau+1}(b)\\) using Equation 12. Specifically, \\(k_{\\tau+1} = k_{\\tau+1}[1 : T]\\) and \\(z_{\\tau+1} = z_{\\tau+1}[1 : T]\\), which are \\(k_\\tau\\) and \\(z_\\tau\\) without their last entries.\n\\[A_\\tau vec(\\Theta) = K_\\tau(K_\\tau + \\lambda_1 I_\\tau)^{-1}(r_\\tau - D_\\tau vec(U))\\]\n\\[D_\\tau vec(U) = Z_\\tau(Z_\\tau + \\lambda_2 I_\\tau)^{-1}(r_\\tau - A_\\tau vec(\\Theta))\\]\n\\[\\hat{r}_{\\tau+1}(b) = k_{\\tau+1}(K_\\tau + \\lambda_1 I_\\tau)^{-1}(r_\\tau - D_\\tau vec(U)) + z_{\\tau+1}(Z_\\tau + \\lambda_2 I_\\tau)^{-1}(r_\\tau - A_\\tau vec(\\Theta))\\]\nConsidering the definitions of \\(A_\\tau\\) and \\(D_\\tau\\), it is evident that \\(A_\\tau vec(\\Theta)\\) and \\(D_\\tau vec(U)\\) are partial acceptances corresponding to the two terms in Equation 2 for historical time steps \\(t = 1,2,..., \\tau\\). From the iteration results, the two terms in Equation 12 estimate the respective terms in Equation 2 for the subsequent time step \\(\\tau + 1\\)."}, {"title": "3.2.3. NEGUCB ALGORITHM", "content": "For the subsequent time step \\(\\tau + 1\\), we can estimate \\(r_{\\tau+1}(b)\\) for each bid \\(b \\in B_{\\tau+1}\\) using Equation 12, then choose a bid to put forth or decide to accept or reject the bid from the counterpart by Equation 1. However, this approach relies solely on exploiting historical data, which may lead to sub-optimal choices. Hence, we explore the estimation uncertainty based on the Upper Confidence Bound principle (Li et al., 2010; Valko et al., 2013; Liu et al., 2018).\nInstead of Equation 1, we make decisions by Equation 13, where \\(e_{\\tau+1}\\) measures the estimation variance and is expressed in Equation 14. Parameters \\(\\alpha_{\\Theta}\\) and \\(\\alpha_U\\) are elaborated in Lemma 3.4. For notation conciseness, we use \\(k_{\\tau+1}\\) and \\(z_{\\tau+1}\\) to denote \\(k_{\\tau+1,\\tau+1}\\) and \\(z_{\\tau+1,\\tau+1}\\).\n\\[b_{\\tau+1} = \\arg \\max_b {\\hat{r}_{\\tau+1}(b) + e_{\\tau+1}(b)} \\times f_{\\tau+1}(b)\\]\n\\[e_{\\tau+1}(b) = \\alpha_{\\Theta} \\sqrt{k_{\\tau+1} - k_{\\tau+1}^T(K_\\tau + \\lambda_1 I_\\tau)^{-1}k_{\\tau+1}} + \\alpha_U \\sqrt{z_{\\tau+1} - z_{\\tau+1}^T(Z_\\tau + \\lambda_2 I_\\tau)^{-1}z_{\\tau+1}}\\]\nIntegrating exploitation and exploration, NegUCB is implemented online as Algorithm 1, where we use \\(a_\\tau\\) and \\(d_\\tau\\) to respectively denote \\(A_\\tau vec(\\Theta)\\) and \\(D_\\tau vec(U)\\) for notation conciseness. Online means the parameters are incrementally updated each time new negotiation data is generated. NegUCB essentially iterates between Step 1. Estimating the second term in Equation 2, then calculating the first term; Step 2. Estimating the first term in Equation 2, then calculating the second term."}, {"title": "3.3. Theoretical Analysis to NegUCB", "content": "If the true parameters satisfy \\(||\\Theta^*|| \\le \\beta_\\Theta\\) and \\(||U^*|| \\le \\beta_U\\), the samples satisfy \\(||\\phi(b_t^T Y) \\otimes \\phi(x_t)|| \\le 1\\) and \\(||\\phi(b_t^T Y) \\otimes p_t || \\le 1\\) for \\(t = 1,2, ..., \\tau\\), then with probability at least \\(1 - \\sqrt{\\delta}\\), the two terms in Equation 12 have estimation error bounds \\(\\alpha_{\\Theta}\\) and \\(\\alpha_U\\) as follows. Here \\(h^*\\) and \\(m^*\\) are the effective dimensions of \\(A_\\tau = A_\\tau A_\\tau^T + \\lambda_1 I_{h^2}\\) and \\(D_\\tau = D_\\tau D_\\tau^T + \\lambda_2 I_{mh}\\), \\(p, q \\in (0,1)\\) are constants.\n\\[\\alpha_{\\Theta} = ||vec(\\Theta_\\tau) - vec(\\Theta^*)||_{A_\\tau}\\]\n\\[< \\lambda_1 \\beta_{\\Theta} + \\sqrt{\\frac{h^*}{\\lambda_1}log(1 + \\frac{T}{\\lambda_1}) - log \\delta + \\frac{2\\beta_U}{\\sqrt{\\lambda_1}} p}\\]\n\\[\\alpha_U = ||vec(U_\\tau) - vec(U^*)||_{D_\\tau}\\]\n\\[< \\lambda_2 \\beta_U + \\sqrt{\\frac{m^*}{\\lambda_2}log(1 + \\frac{T}{\\lambda_2}) - log \\delta + \\frac{2\\beta_{\\Theta}}{\\sqrt{\\lambda_2}} q}\\]\nIn Lemma 3.4, \\(A_\\tau\\), and \\(D_\\tau\\) correspond to the first item of Equation 6 and Equation 7, respectively. Effective dimension (Valko et al., 2013; Vakili et al., 2021) is a commonly used concept in kernel regression and can be considered as the number of principal dimensions. They contract the bounds as \\(h^* <<< h^2\\) and \\(m^* < mh\\), where \\(h\\) is the dimension of \\(H\\). Bounds of each sample \\(\\phi(b_t^T Y) \\otimes \\phi(x_t)\\) and \\(\\phi(b_t^T Y) \\otimes p_t \\) are set as 1 for description convenience. They correlate with the number of items in the bid, referred to as the bid cardinality and denoted as \\(\\gamma \\le n \\in \\mathbb{Z}^+\\). We can guarantee the bounds of samples by normalizing the contexts \\(X\\) and \\(Y\\). Based on Lemma 3.4, we guarantee the performance of NegUCB by the following theorem.\nUnder the same assumptions as Lemma 3.4, with probability at least \\(1 - \\sqrt{\\delta}\\), the cumulative regret of Algorithm 1 has the following upper bound, where \\(r_t(b^*)\\) and \\(r_t(b_t)\\) are respectively the true acceptance of the optimal bid \\(b^*\\) and the bid chosen by Algorithm 1 at time step \\(t\\). \\(a_f\\) is the union bound of the benefit functions, i.e., \\(|f_t(b)| \\le a_f\\) for \\(b \\in B_t\\) and \\(\\forall t \\in {1, 2, ..., T}\\).\n\\[\\sum_{t=0}^T r_t(b^*) \\times f_t(b^*) - r_t(b_t) \\times f_t(b_t)\\]\n\\[< 2 \\alpha_{\\Theta} a_f \\sqrt{\\frac{2h^*}{\\lambda_1}log(1 + \\frac{T}{\\lambda_1})}\\]\n\\[+ 2 \\alpha_U a_f \\sqrt{\\frac{2m^*}{\\lambda_2}log(1 + \\frac{T}{\\lambda_2})}\\]\nIndeed, the cumulative regret is sub-linear concerning the number of time steps \\(T\\). This implies that as the number of negotiation steps increases, the cumulative regret grows at a slower rate, indicating improved negotiation capability. Besides, the bound is independent of the bid cardinality \\(\\gamma\\), distinguishing NegUCB from existing works (Qin et al., 2014; Wen et al., 2015; Chen et al., 2018; Nie et al., 2022; Fourati et al., 2023). It is a result of the full-bandit feedback and Assumption 3.1. The effect from bid cardinality to the cumulative regret bound is further discussed in \u00a7 A.4.1."}, {"title": "4. Experiments", "content": "In this section, we evaluate NegUCB across the three representative negotiation tasks depicted in Figure 1, comparing it with five representative baselines: ANAC agent, LinUCB, FactorUCB, KernelUCB, and a reinforcement learning-based negotiation method (Cao et al., 2018; Bagga et al., 2020). It is important to note that we extend the original UCB-based baselines to handle combinatorial bandits and full-bandit feedback effectively. Further analysis regarding the rationale behind baseline selection is in \u00a7 B.1.\nAs mentioned, the benefit function \\(f_\\tau\\) is problem-specific. In our experiments, we set \\(f_\\tau(b_\\tau) = 1\\) if \\(b_\\tau \\in C \\cap B_\\tau\\), where \\(C\\) consists of bids satisfying certain beneficial constraints, otherwise, \\(f_\\tau(b_\\tau) = 0\\). It implies that we encourage bids that are advantageous to us. This simple configuration lets us concentrate on the acceptance function \\(r_\\tau\\), rather than the handcrafted \\(f_\\tau\\). The subsections of specific tasks will further define the set \\(C\\) constraining bids."}, {"title": "4.1. Multi-issue Negotiation", "content": "ANAC (Automated Negotiating Agents Competition) is an international tournament that has been held since 2010, providing 50 negotiation domains. However, compared to the settings of NegUCB, ANAC tasks are relatively simple. For instance, negotiators and items lack contexts, and there is only one negotiator pair for each domain. Consequently, some components of NegUCB are not necessary for these tasks. In this subsection, we modify NegUCB for compatibility with ANAC tasks, showing the adaptability of NegUCB to diverse negotiation problems. In ANAC experiment"}]}