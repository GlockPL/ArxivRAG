{"title": "Deep Convolutional Neural Networks on Multiclass\nClassification of Three-Dimensional Brain Images for\nParkinson's Disease Stage Prediction", "authors": ["Guan-Hua Huang", "Wan-Chen Lai", "Tai-Been Chen", "Chien-Chin Hsu", "Huei-Yung Chen", "Yi-Chen Wu", "Li-Ren Yeh"], "abstract": "Parkinson's disease (PD), a degenerative disorder of the central nervous system, is\ncommonly diagnosed using functional medical imaging techniques such as single-\nphoton emission computed tomography (SPECT). In this study, we utilized two\nSPECT data sets (n = 634 and n = 202) from different hospitals to develop a model\ncapable of accurately predicting PD stages, a multiclass classification task. We used\nthe entire three-dimensional (3D) brain images as input and experimented with\nvarious model architectures. Initially, we treated the 3D images as sequences of two-\ndimensional (2D) slices and fed them sequentially into 2D convolutional neural\nnetwork (CNN) models pretrained on ImageNet, averaging the outputs to obtain the\nfinal predicted stage. We also applied 3D CNN models pretrained on Kinetics-400.\nAdditionally, we incorporated an attention mechanism to account for the varying\nimportance of different slices in the prediction process. To further enhance model\nefficacy and robustness, we simultaneously trained the two data sets using weight\nsharing, a technique known as cotraining. Our results demonstrated that 2D models\npretrained on ImageNet outperformed 3D models pretrained on Kinetics-400, and\nmodels utilizing the attention mechanism outperformed both 2D and 3D models. The\ncotraining technique proved effective in improving model performance when the\ncotraining data sets were sufficiently large.", "sections": [{"title": "Introduction", "content": "Parkinson's disease (PD) is a neurodegenerative disease with a typical onset age of 65\nto 70 years; PD is characterized by its main motor symptoms, namely bradykinesia,\nrigidity, and tremor, but the disease also has additional motor and nonmotor\ncharacteristics [1]. PD occurs when nerve cells, or neurons, in a brain area that\ncontrols movement become impaired or die. In 2011, the incidence rate of PD per\n100,000 in Taiwan was 85.1 and 364.1 for those aged 60\u201369 and \u226580 years,\nrespectively [2].\nAccording to the level of clinical disabilities or symptoms, PD can be divided\ninto several stages [3, 4]. However, individuals with PD may exhibit differences in the\ntype and severity of symptoms despite being at the same stage. Anatomic-pathologic\nstudies have highlighted the high misdiagnosis rate for PD [5]; thus, functional\nimaging techniques are increasingly being used to understand the pathophysiology\nand evolution of PD. Common functional imaging techniques used for this purpose\ninclude positron emission tomography, single-photon emission computed tomography\n(SPECT), and functional magnetic resonance imaging. The images we used in this\nstudy were obtained using SPE\u0421\u0422.\nIn clinical practice, SPECT images are typically evaluated visually or through\nregion-of-interest (ROI) analysis. The ROI approach involves marking out or\npositioning ROIs over the striatum (target) and the occipital cortex (reference) and\nsubsequently applying the background subtracted striatal uptake ratio as the\nevaluation metric [6]. Another approach involves conducting shape and intensity\ndistribution analysis, with pattern recognition techniques being used for\ndifferentiation [7, 8].\nComputer-aided diagnosis of PD has been researched from various perspectives,\nbut most studies have focused on the pattern recognition approach. Generally, the\noverall process involves two parts: feature extraction and classification. Feature\nextraction can be achieved (1) by applying dimension reduction approaches such as\nprincipal component analysis or singular value decomposition on the voxels of the\nwhole brain [9, 10], or (2) by using the voxels of the striatum or the striatal binding\nratio values as features [11-14]. After feature extraction, machine learning classifiers\nincluding support vector machine, linear/quadratic discriminant analysis, or na\u00efve\nBayes can be applied [9, 10, 12, 14-16].\nIn traditional machine learning classifiers, features are typically extracted using\nhandcrafted rules. In contrast, deep learning methods such as convolutional neural\nnetworks (CNNs) can automatically extract sufficient and representative features from\nimages, enabling automation in the computer-aided diagnosis process. CNNs are\nincreasingly being adopted for predicting Parkinson's Disease (PD) [17-19]. While\nseveral approaches have been developed for PD diagnosis, most focus on binary\nclassification-merely indicating whether an image shows signs of PD. Although PD\nis typically classified into several stages [3], multiclass classification approaches are\nrare [20]. Additionally, many studies on PD classification have used two-dimensional\n(2D) images despite the availability of three-dimensional (3D) information in SPECT\ndata [20]. When 2D models are applied to 3D SPECT data, 2D image slices with the\nclearest striatum shape are selected for analysis. However, this selection process must\nbe performed by physicians or experts, and it fails to capture the temporal or\npositional information that 3D SPECT images provide. To reduce the labor involved\nin manual selection and to utilize all available 3D information, models that take\nwhole-brain images as inputs are urgently needed. Therefore, we developed a fully\nautomated process that uses the entire 3D brain image as input for the multiclass\nclassification of PD stages.\nThree-dimensional images can be viewed as a series of 2D image slices,\nallowing them to be analyzed by 2D CNN models. In this approach, all slices pass\nthrough shared 2D convolutional layers to generate image representations, which are\nthen combined for final PD stage prediction [21, 22]. Given that we are working with\n3D medical images, using 3D CNN models is also appropriate to fully leverage the\navailable information [23, 24]. There has been considerable debate about the\neffectiveness of 2D versus 3D representation learning for 3D medical images. While\n2D methods benefit from extensive pretraining, they cannot fully capture the 3D\ncontext. Conversely, 3D methods excel at understanding 3D contexts but lack large,\ndiverse datasets for effective pretraining [25]. This paper introduces an architecture\nthat considers the relationships among 2D slices, enabling models to learn the\ndifferent contributions of each slice. Additionally, we co-train two SPECT datasets\nfrom different hospitals using weight sharing to improve the model's effectiveness and\nrobustness. We also provide an extensive experimental evaluation of multiple\napproaches for 3D medical image classification."}, {"title": "Material", "content": "We used two data sets provided by two different hospitals: (1) the Kaohsiung Chang\nGung Memorial Hospital (hereinafter called Chang Gung) and (2) the E-Da Hospital\n(hereinafter called E-Da). Two data sets saved in the DICOM (digital imaging and\ncommunications in medicine) files contained data on SPECT imaging using 99mTc-\nTRODAT-1 with three dimensions $(T, H, W)$, where $T$ represents the number of\nslices for each person, and $H$ and $W$ represent the height and width of each slice,\nrespectively. The three views (axial, coronal, and sagittal) of our data are presented in"}, {"title": "Methods", "content": "Preprocessing\nNormalization\nBecause the ranges of the pixel intensity values for each patient differed, potentially\naffecting the efficiency of machine learning algorithms, we scaled them by using\nmin-max normalization individually:\n$X_{i(norm)} = \\frac{X_i - min(X_i)}{max(X_i) \u2013 min(X_i)}$\nwhere $X_i$ represents the pixel values in all slices of the $i$th patient, and $min(X_i)$\nand $max(X_i)$ are the smallest and largest pixel value of $X_i$, respectively. After\nrescaling was conducted, the values all ranged from 0 to 1, and the relative magnitude\namong patients could also be maintained.\nSlice Selection\nFor the Chang Gung data set, during diagnosis, physicians had selected the slices\nwhose striatum can be recognized most clearly. The data set contained the indices\nof the selected slices that contained the clearest striatum shape were provided. Other\ninformation such as age and sex was recorded in the DICOM file."}, {"title": "Other Information", "content": "Epidemiological studies have indicated the existence of sex differences in PD: the\nincidence and prevalence of PD in men are higher than those in women [27]. PD is\nalso related to age: the incidence rates rise rapidly after the age of 60 years [28].\nMoreover, the male:female incidence rate ratio increases with age in general. For the\naforementioned reasons, age and sex were considered during the training process.\nBecause ages in this data set were positive numbers in years, they needed to be\nnormalized to range [0,1] to ensure that the scale would be the same as that of the\npixel values of slices. The normalized age is expressed as\n$a' = a_i/100, \\qquad a_i \\in [0, 100]$,\nwhere $a_i$ represents the age in years of the $i$th patient.\nSex only took two values, male or female; thus, it was represented by a dummy\nvariable defined as\n$\\text{G} = \\begin{cases} 1 & \\text{if gender is male} \\\\ 0 & \\text{if gender is female} \\end{cases}$\nAugmentation\nBecause the two data sets were both imbalanced and too small for deep learning\nmodels, overfitting was a concern; that is, the model may fit the training data\nexcessively, consequently leading to poor performance when predicting new data. To\navoid this problem, the most common approach for image classification tasks is to\nperform data augmentation, which provides new data by making minor alterations to\nthe existing dataset. The augmented data were generated using video transform and\ndepth transform, as described in the following section. We used online augmentation;\nthat is, image augmentation was conducted in mini-batches before feeding data to the\nmodel. The model with online augmentation was presented with different images at\neach epoch; this aided the generalization ability of the model, and because the model\ndid not need to save the augmented images on the disk, the computational burden was\nreduced.\nVideo Transform\nBecause our data were 3D SPECT images, which can be regarded as a series of slices\n(2D images), we adopted video transforms to ensure that the same random parameters\n(e.g., crop size, rotation angle, flip or not) would be applied to all the slices of each\npatient:\n\u2022 Random rotation (degree = 5): rotate the image by an angle ranging from\n-5\u00b0 to 5\u00b0"}, {"title": "Depth Transform", "content": "Video transform yielded images with a size of 72 \u00d7 72; however, the number of\nslices for each patient was different. We applied the trilinear interpolation approach\n[29] to construct new slices to unify the depth dimension of all patients' images.\nTrilinear interpolation is a multivariate interpolation method applied to 3D inputs (i.e.,\nvolumetric data sets). The value is approximated linearly by the intermediate point\nwithin the nearest cubic lattice, which is identical to two bilinear interpolations\ncombined with a linear interpolation.\nNotably, the result of trilinear interpolation is unrelated to the order of the steps\napplied along the three axes. We set the target slice number to 32 because most of the\nsamples in our data sets had fewer than 32 slices, and thus we would not lose too\nmuch information. After we applied trilinear interpolation, the data size of each\npatient was unified as 32 \u00d7 72 \u00d7 72.\nMulticlass Classification\nThe main objective of this study was to develop a valid model to yield accurate\nprediction of PD illness stages. Because each sample can only belong to one of C PD\nstages (including healthy) (C = 4 for the Chang Gung data set and C = 6 for the E-\nDa data set), this was a multiclass classification task. We conducted a deep learning\nmodel that mapped inputs of the ith image X\u012f to a C-dimentional label vector yi =\n(y\u04561, \u0443\u04562,, y\u0456c) with \u2200yic \u2208 {0,1}, yi1 + \u2026 + yic = 1. Therefore, the categorical\ncross-entropy can be used as the loss function:\n$\u00a3_{ce} = \\sum_{i}\\sum_{c=1}^{C} {[-y_{ic} ln (s(f_c(x_i)))]}$,\nwhere $f_c(X_i)$ is $X_i$'s cth input for the final fully connected layer and $s(z_c) =$\n$e^{z_c}/\\sum_{j=1}^{C} e^{z_j}$ is the softmax function.\nOur datasets were highly imbalanced. Most machine learning algorithms for\nclassification problems operate under the assumption of an equal number of samples\nin each class. In an unbalanced data set, the learning can be biased toward the\nmajority classes and fail to catch the patterns of the minority classes. A popular\nmethod to address imbalanced data is to set the class weight in the loss function. We\ndefined our class weights as\n$W_c = \\frac{N_c}{\\sum_{j=1}^{c} N_j}$ with $N_c = \\frac{n}{n_c}$, c = 1, \u2026, \u0421,\nwhere n represents the total number of samples, and ne is the number of samples in\nthe cth class (PD stage). The loss function with class weights thus becomes\n$L_{WCE} = \\sum_{i}\\sum_{c=1}^{C} {[-w_c y_{ic} ln (s(f_c(x_i)))]}$\nDeep CNN Models\nImage classification using CNN has demonstrated outstanding performances\ncompared with traditional machine learning approaches. CNN is a deep learning\nmethod designed to process structured arrays, and it has become dominant in various\ncomputer vision tasks. In addition to their application to classification, segmentation,\nand recognition problems related to image or video, CNNs have also been applied to\nnatural language processing. CNNs are designed to automatically and adaptively learn\nspatial hierarchies of features through back-propagation over multiple stacked\nbuilding blocks, such as convolution layers, non-linear layers (activations), pooling\nlayers, and fully connected layers. Generally, CNNs are composed of multiple layers\nof artificial neurons that are mathematical functions for calculating the weighted sum\nof multiple inputs and then yielding an activation value. Each neuron behaves\naccording to its weights. The operation of multiplying pixel values by weights and\nsumming them is called convolution. When a CNN is fed the pixel values of an\nimage, each layer generates several activation maps that highlight the relevant\nfeatures of the input image. The early few (or bottom) layers usually detect more basic\nfeatures such as edges and corners, with deeper layers extracting higher-level features\nsuch as objects. Lastly, based on the output of the final convolution layer, the fully\nconnected layers output a set of scores ranging from 0 to 1 that indicate how likely it\nis that the input belongs to each class."}, {"title": "2D Models", "content": "This study focuses on multiclass classification of 3D SPECT images.\nTraditionally, 3D images are analyzed using either 2D CNNs on individual slices (2D\nmodels) or 3D CNNs on the entire spatiotemporal volume (3D models). We introduce\nan architecture that uses an attention mechanism to consider the relationships among\nslices, allowing each slice to contribute differently to the prediction (slice-relation-\nbased models). Besides training the Chang Gung and E-Da data sets separately with\nvarious model architectures, we propose cotraining the two datasets simultaneously\nusing weight sharing to enhance model effectiveness and robustness.\nTransfer learning is the process of creating new models by fine-tuning previously\ntrained networks or a machine learning technique where a model trained for one task\nis utilized as the initial value of the model for another related task. That is, the\nknowledge gained from previous tasks is transferred to related ones. Transfer learning\nhas been widely applied to diverse tasks. One of its strengths is particularly helpful\nfor our task: transfer learning reduces the need and effort to recollect a large amount\nof training data, thus addressing the challenge of training a full-scale model from\nscratch or with little data. In our transfer learning training process, we used 2D and\n3D model architectures trained on ImageNet [30] and Kinetics-400 [31], respectively,\nas the pretrained model, froze the weights of the first few layers, set the remaining\nlayers as trainable, and then replaced the final few layers with customized layers.\nGiven that ImageNet and Kinetics-400 were trained on natural RGB images, some\nstudies demonstrated that, in different medical imaging applications, CNNs pretrained\non these large-scale natural image data sets performed better and were more robust to\nthe size of the target training data than the CNNs trained from scratch [32, 33].\n3D SPECT data can be regarded as a series of 2D images and thus can be analyzed by\n2D model architectures. We selected the VGG-16 architecture pretrained on ImageNet\nas our pretrained model, and we used its convolution layers connected with some\ncustomized layers to form our training model.\nBecause the images in ImageNet were RGB images with three channels, we\ncopied each slice three times to generate the three-channel input from our gray-scale\nmedical images, where each input was formed by a set of three repeated slices. All\nslices passed through shared VGG-16 convolutional layers to obtain image\nrepresentations; these outputs were then \u201csummarized\u201d through some customized\nlayers before proceeding to the final fully connected (FC) layers. We discarded the\nlast max-pooling layer in the convolution layers of the original VGG-16 architecture\nto ensure that the output shape would not be too small. The different customized\nlayers we used are described in the following sections."}, {"title": "Depth Transform", "content": "VGG plus linear (Linear): The first model was the simplest one and also the\nbaseline model we used for comparison. We added an adaptive average pooling layer\n(with target output size 1 \u00d7 1) to features with size (512,4,4) and obtained outputs\nof size (512, 1, 1). After the outputs from all slices were averaged, a multilayer\nperceptron (MLP) with rectified linear unit (ReLU) activation was used to reduce the\noutput dimension from 512 to 16. Age and sex were concatenated with the output\nfeatures if necessary and then fed into the FC layers. In this manner, the effects of age\nand sex would be appropriately considered. The model architecture is presented in\nFig. 4 (a).\nVGG plus Conv2D (Conv2D): For more advanced processing, we replaced the\naverage pooling layer in the aforementioned Linear model architecture with a 2D\nconvolutional layer. This enabled us to extract more realistic features. This model\narchitecture is presented in Fig. 4 (b).\nAxial-coronal\u2013sagittal convolutions (ACS): Because any of the three views\n(axial, coronal, and sagittal) of our 3D data can be regarded as 2D images, ACS\nconvolutions [25] were utilized in this model. In ACS convolutions, 2D kernels are\nsplit by the channel into three parts and convoluted separately on the axial, coronal,\nand sagittal views of 3D inputs; as a result, the weights pretrained on large 2D data\nsets could still be used. Suppose we wished to obtain a 3D output Xi \u2208\n$\\mathbb{R}^{C_o \\times T_o \\times H_o \\times W_o}$ with pretrained 2D kernels $W \\in \\mathbb{R}^{C_o \\times C_i \\times K \\times K}$, given a 3D input $X_i \\in$\n$\\mathbb{R}^{C_i \\times T_i \\times H_i \\times W_i}$, where $C_i$ and $C_o$ were the input and output channels, respectively,\n$T_i \\times H_i \\times W_i$ and $T_o \\times H_o \\times W_o$ denoted the number of slices, height, and width of\nthe input and output shapes, respectively, and $K$ represented the kernel size. To\ntransfer the 2D kernels to 3D kernels, we unsqueezed the 2D kernels into pseudo 3D\nkernels on an axis [34]. The ACS convolutions functioned by splitting and\nunsqueezing the 2D kernels into three parts that aimed for representations of three\nviews: $W_a \\in \\mathbb{R}^{C^{(a)} \\times C_i \\times K \\times K \\times 1}, W_c \\in \\mathbb{R}^{C^{(c)} \\times C_i \\times 1 \\times K \\times K},$ and $W_s \\in \\mathbb{R}^{C^{(s)} \\times C_i \\times K \\times 1 \\times K}$,\nwhere $C^{(a)} + C^{(c)} + C^{(s)} = C_o$. The output feature of each view then became\n$X_o^{(v)} = Conv3D(X_i, W_v) \\in \\mathbb{R}^{C_o \\times T_o \\times H_o \\times W_o}$, v \u2208 V = {a, c, s}.\n$X_o^{(a)}, X_o^{(c)},$ and $X_o^{(s)}$\noverall output feature, and finally went through the FC layers (Fig. 4 (c))."}, {"title": "3D Models", "content": "Because we were working with 3D medical images, the application of 3D models was\nalso appropriate. Here we employed the 3D models for action recognition and video\nclassification, as in Tran et al. [23], based on the ResNet-18 architecture. These\nmodels were all pretrained on the Kinetics-400 dataset [31], containing 306245 short-\ntrimmed, realistic action clips from 400 action categories.\nNotably, the input shape for these 3D models becomes (C,T,H,W), where C is\nthe channel and T is the number of video frames in a clip. All of our slices were\ngray-scale with one channel, whereas each frame of a clip in Kinetics-400 was an\nRGB image with three channels. For data set compatibility, each of our slices was\nrepeated three times, and then video transform and depth transform were applied to\nensure that all the inputs had the same shape.\nThe overall architectures of the following 3D models were similar: a 3D\npretrained model was followed by an average pooling layer and some MLPs.\nHowever, different pretrained models were used for each 3D model.\n3D ResNet (R3D): Unlike 2D CNNs, 3D CNNs can preserve and capture\ntemporal, or positional in our case, information because the filters are convoluted over\nboth time and space dimensions. In this model, the 2D convolutions were merely\nreplaced with 3D convolutions, with an extra temporal extent on the filter. The overall\nmodel architecture is illustrated in Fig. 5 (a).\nMixed convolutions ResNet (MC3): On the basis of the hypothesis that temporal\nmodeling with 3D convolutions would be useful only in the early layers and would be\nunnecessary in the late layers because the higher-level features are abstract [23], the\nmixed 2D-3D convolutions were used. Considering that the R3D model had five\ngroups of convolutions, we applied the MC3 model where the 3D convolutions in\ngroups 3, 4, and 5 were replaced with 2D convolutions. The architecture is presented\nin Fig. 5 (b).\n(2+1)D ResNet (R(2+1)D): A 3D convolution can be approximated by a spatial\n2D convolution followed by a temporal one-dimensional (1D) convolution, where the\nmodel captures spatial and temporal information from two separate steps. This\nprocess facilitates the optimization and doubles the number of nonlinear\ntransformations caused by the additional activations between 2D and 1D convolutions\nwithout increasing the number of parameters. In this model, the (2+1)D convolutions\nare substituted for 3D convolutions, and the architecture is illustrated in Fig. 5 (c)."}, {"title": "Slice-Relation-Based Models", "content": "We initially assumed that all the slices from one patient contributed equally in the\ntraining process of the 2D models because we merely used the average of their\nconvoluted features. However, during diagnosis, physicians only focus on slices\nwhose striata are clear enough to be recognized; thus, treating all the slices of one\npatient equally is not ideal. We therefore also considered the relation among slices to\nassist models to learn the differences among slice contributions.\nIndex embedding (IdxEmb): First, the slices of one patient were sequentially\nnumbered according to the order of their entry into the model, called slice index. The\nslice index was regarded as a categorical variable x and then mapped to a vector x\nwith a predefined dimension through entity embedding [35]. Entity embedding was\nused to operate a linear layer on the one-hot encoding of x as follows:\nx = W\u03b4x\nwhere m is the number of possible values for the categorical variable x, \u03b4x is a\nvector of length m with the ith element being 1 if i = x and 0 otherwise, i =\n1,\u2026, m, W = {Wij} \u2208 \u211dk\u00d7m is the weight matrix that connects the one-hot encoding\nwith the entity embedding, and k is the dimension of the entity embedding. W can\nbe regarded as the weights of the linear layer, which were trainable by using a\nstandard backpropagation method. The embedded index x was reshaped and added\nto the convoluted features of the corresponding slice and went through a 2D\nconvolution together. The output features of these slices were then gathered and\naveraged and finally fed into a classifier. The whole model architecture is presented in\nFig. 6 (a).\nAttention (Attn): Adding the information about the position of slices in the slice\nindex embedding model seemed appropriate, but the image registration problem\nproved challenging: we should have the absolute position for slices. Unfortunately,\nour data were functional images whose positional information was inaccurate; for\nexample, slice index 1 might correspond to different brain areas for different patients.\nTo avoid the image registration problem, we only considered the relative importance\nof all slices in one patient, an approach motivated by the attention mechanism [36,\n37]. Because our attention mechanism only considered the input images of a patient\nwith no extra output information and aimed for capturing the internal correlation of\nslices within a patient, we were in fact implementing self-attention [38, 39].\nOverall, we adopted the 2D model but, instead of taking the average, we used the\nweighted sum (Fig. 6 (b)). Details for the weighted sum are as follows. For one\npatient, each VGG-16 convolution output feature gi went through a convolution\nlayer with ReLU activation and yielded another output hi of size (1,1,1). We\napplied the softmax function to outputs h1,\u2026, hm to form the weight for each output\nfeature gi: wi = ehi/\u2211j=1mehj, where m is the number of slice sets. Subsequently,\nthe input of the following MLP (with the ReLU activation) can be represented as\nfollows: u = \u2211i=1m wigi. In this manner, the model allowed each patient to have a\ndifferent weighting scheme (i.e., different w1,\u2026, wm) with unique important slices.\nMultihead Attention (MH-Attn): The aforementioned attention mechanism\nfocuses on a specific aspect that the image slices of a patient reflected. However,\nmultiple aspects in these image slices together form the overall structure of the 3D\nimage. Multihead attention allows the model to jointly attend to information from\ndifferent aspect subspaces for different slices. We here adopted the multihead\nattention formulation used in Vaswani et al. [39]."}, {"title": "Cotraining Models", "content": "We initially trained our two datasets (Chang Gung and E-Da data sets) separately with\ndifferent model architectures. Although we obtained some favorable results, we still\ndid not leverage the high similarity between them: they were both SPECT brain\nimages and thus some shared characteristics should be present. Thus, we subsequently\ntrained these two data sets jointly by using the models introduced in the preceding\nsections. The schematic is illustrated in Fig. 7. The 2D and 3D pretrained models were\nstill used, but the inputs were evenly composed of the two data sets in each batch. We\napplied cotraining to the shared pretrained weights (i.e., the yellow block in Fig. 7),\nwhere the number of cotraining layers was a hyperparameter to be tuned, and the\nremaining parts were trained separately. The loss L can be expressed as\nL = LCG + LED\nwhere LCG and LED refer to the weighted categorical cross-entropy loss of the\nChang Gung and E-Da data sets, respectively. In this manner, the two data sets would\nshare the weights of the cotraining part, which increased the robustness of the model\nbecause of the increased variation and number of training samples. The model could\nalso retain the differentiation between the two data sets through the parts trained\nseparately.\nEvaluation\nK-Fold Cross-Validation\nTo assess the generalization ability of the models and to avoid overfitting and\nselection bias, we adopted stratified cross-validation. The number of folds was set to\nbe five, which meant the original samples were randomly partitioned into five groups,\nwith each group containing approximately the same proportions of class labels. In\neach cross-validation round, one group was for testing (called the test set), and the\nother four groups were for training (called the training set); 20% of the training set\nserved as the validation set.\nMetrics\nWe used two metrics, namely accuracy and F1 score, to evaluate our model\nperformance. Accuracy is defined as the proportion of samples that are correctly\nclassified. This evaluation metric is commonly used in classification tasks. However,\na problem with imbalanced data sets is that model accuracy can be judged as high\neven if it only correctly predicts the majority class. Because our data sets were\nimbalanced, we employed another metric to evaluate model performance: F1 score.\nFor each class, we are interested in the fraction of relevant samples among the\nretrieved samples (i.e., precision) and the fraction of retrieved samples among the\nrelevant samples (i.e., recall). The F1 score is the harmonic mean of the precision and\nrecall, with its optimal and poorest values being 1 and 0, respectively. In our task, we\nfirst calculated the F1 score for each class and then took the average of these F1\nscores, called macro F1 score, as our alternative evaluation metric."}, {"title": "Results", "content": "This section contains the results of all the experiments performed in the current study.\nWe first conducted experiments with a different number of selected slices for the\nanalysis. Different 2D models were then applied to our two data sets separately and\njointly.\nNumber of Slices Used\nThe results are illustrated in Table 2. For the Chang Gung data set where the indices\nof the mine slices selected by the physicians for each patient were available", "accuracy": 0.6798, "score": 0.587}, {"accuracy": 0.5248, "score": 0.2902, "set": "The 2D models' experiment results are presented in the top part\nof Table 3. In general"}, {"set": "For the E-Da data set"}, {"set": "For the Chang Gung data set, age and sex helped improve the\nmodel efficacy (higher accuracy and F1 score) and stability (lower standard deviation)\n(the middle part of Table"}]}