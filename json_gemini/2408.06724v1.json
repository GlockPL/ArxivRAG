{"title": "Adaptive Data Quality Scoring Operations Framework using Drift-Aware Mechanism for Industrial Applications", "authors": ["Firas Bayram", "Bestoun S. Ahmed", "Erik Hallin"], "abstract": "Within data-driven artificial intelligence (AI) systems for industrial applications, ensuring the relia-\nbility of the incoming data streams is an integral part of trustworthy decision-making. An approach\nto assess data validity is data quality scoring, which assigns a score to each data point or stream\nbased on various quality dimensions. However, certain dimensions exhibit dynamic qualities, which\nrequire adaptation on the basis of the system's current conditions. Existing methods often overlook this\naspect, making them inefficient in dynamic production environments. In this paper, we introduce the\nAdaptive Data Quality Scoring Operations Framework, a novel framework developed to address the\nchallenges posed by dynamic quality dimensions in industrial data streams. The framework introduces\nan innovative approach by integrating a dynamic change detector mechanism that actively monitors\nand adapts to changes in data quality, ensuring the relevance of quality scores. We evaluate the\nproposed framework performance in a real-world industrial use case. The experimental results reveal\nhigh predictive performance and efficient processing time, highlighting its effectiveness in practical\nquality-driven AI applications.", "sections": [{"title": "1. Introduction", "content": "Industries and businesses are actively accumulating data\nin unprecedented volumes, marking a definitive shift towards\na data-centric paradigms [1]. Within this landscape, data has\ntranscended its conventional role to become the cornerstone\nof success for artificial intelligence (AI) software solutions.\nData holds an intrinsic value due to its inseparable link to the\nlife cycle of machine learning (ML), which constitutes the\nprimary type of AI. Therefore, assessing the quality of the\ncollected data becomes an essential and imperative aspect in\nbuilding robust and reliable ML solutions [2]. Remarkably,\na prevalent concern in the industry is the disproportionate\nallocation of efforts in ML projects in research. According\nto a recent study conducted by MIT scientists [3], research\ninstitutions often allocate 90% of their efforts to improve ML\nalgorithms and only 10% to data preparation and validation.\nThe authors suggested that these numbers should be reversed\nfor better overall outcomes. In light of this, the significance\nof data quality (DQ) assurance extends beyond mere pro-\ncedural correctness. It plays a pivotal role in influencing\nthe cost-effectiveness and operational efficiency of industrial\nprocesses and operations.\nThere are two main approaches to data quality assess-\nment: quantitative assessments that involve scales and met-\nrics to quantify aspects of data quality, and qualitative as-\nsessments that focus on inherent characteristics and sub-\njective evaluations [4]. Quantitative assessments provide a\nmeasurement, offering more detailed information about the\nobjective estimates of data quality. This approach involves\nassigning specific numerical values to various aspects, called\na quality score or index [5], which enables a quantifiable\nunderstanding of data quality. In contrast, qualitative assess-\nments contribute to a more holistic perspective by exploring\nthe intrinsic qualities and subjective aspects of the data,\nreferred to as data profiling [6]. Through the use of scales\nand metrics, our study focuses on quantitative data scoring,\nas it provides valuable insights for industries.\nData quality scoring is a methodological approach that\ninvolves evaluating and assigning scores to data records\nbased on predefined criteria across several data quality di-\nmensions [7]. Each data quality dimension captures a unique\naspect, collectively contributing to a thorough assessment of\nthe overall data quality. The scores indicate the acceptance\nlevel of the addressed data quality dimensions, distinguish-\ning between high- and poor-quality data. Furthermore, data\nquality scores are interpreted as the degree to which data\nquality conforms to specified aspects of data quality [8].\nIn the context of large-scale systems, the importance of\nthese data quality scores extends beyond the evaluation of\ndata quality and can be exploited to enhance the overall\nperformance of the ML system [9].\nFrom a data-centric AI perspective, assessing data qual-\nity is crucial; nevertheless, it adds computational costs to\nsystems, especially in real-time production environments.\nThe practicality of these assessments in operational contexts\nposes a notable challenge due to inherent complexities and\nresource-intensive nature in industrial use cases. Therefore,\nstreamlining these processes becomes a vital requirement to\nensure their effectiveness and success in practical applica-\ntions [10]. Two primary approaches exist for data quality\nscoring: a traditional standard approach and an automated\napproach utilizing an ML-based system [11]. The standard"}, {"title": "2. Conceptual Background", "content": "Continuous monitoring and assessment of the incoming\ndata streams are crucial to ensure that dynamic changes are\ncaptured and that the prevalent condition (concept) remains\nconsistent. In data-centric applications, it is essential to ver-\nify that data quality remains high. Data quality is important\nin ensuring the data remains useful, as low-quality data can\nlead to physical failures or inaccuracies in sensor readings\n[15]. This process is illustrated in Figure 1, where time-\nseries data collected from the data source undergoes both\ndrift detection and data quality assessment. Consequently,\nthe AI system benefits from valuable meta-information ob-\ntained from these stages. Continuous drift monitoring en-\nsures that the incoming data remain contextually similar to\nthe training data [16], allowing intelligent systems to make\ninformed decisions based on high-quality information [17].\nWithin the context of data-driven applications, these\nchanges can have implications for the performance of both\ndata quality and application-level ML systems. They signify\nthe need for adaptation to maintain system reliability. In\npractice, application-level ML systems utilize drift detection\nto sustain performance in dynamic environments, while\ndata-quality scoring ML systems use it to ensure the quality\nand reliability of the generated data. Despite their different\nobjectives, both application-level and data quality scoring\nML systems rely on the same core principle: monitoring\nchanges in data characteristics. This common foundation\nallows for the successful application of identical drift de-\ntection mechanisms across both model types. This paper\nfocuses primarily on adapting data quality scoring ML sys-\ntems within the holistic AI application. This section provides\nessential background knowledge on these core concepts,\nlaying the foundation for a comprehensive understanding of\nthe fundamental elements within the scope of this research."}, {"title": "2.1. Drift Monitoring", "content": "Drift in data streams refers to the temporal evolution or\nevolving changes in the underlying statistical properties of\nthe data [18]. Detecting drift involves employing a method-\nology to determine and identify significant changes in data\nstreams at a specific time point t. In industrial settings,\ndetecting drift is a fundamental task that serves to indicate\nand diagnose the status of the ingested data, prompting\nappropriate actions in response to identified changes [19].\nTypically, drift is detected by quantifying the dissimilarity\nbetween the data probability distributions of two timestamps\nusing the divergence metric. If the divergence metric exceeds\na certain threshold \u03b6, it signifies a drift occurred in the data\n[20]. This metric can be calculated as follows:\n$D_t = \\delta(P || Q)$.\nThe decision-making logic is defined as follows: If $D_t >\\zeta$, a drift in the data is detected; otherwise, no detection\noccurs. Where $D_t$ represents the divergence metric recorded\nat timestamp t between probability distributions P and Q,\nand \u03b4 is a function measuring this divergence.\nIn the context of nonstationary applications, determining\na suitable threshold value (for detecting drift poses a\nsignificant challenge [21]. Industrial data streams are usually\ndynamic and complex, resulting in constantly changing sta-\ntistical properties. This variability makes establishing a sin-\ngle, universal threshold impractical. Moreover, effectively\ndetecting drift requires a subtle understanding of the specific\nindustrial processes and the context of the application [22].\nAs a result, recent research concludes that the drift detection\nthreshold should not be fixed. Instead, it must be adaptive,\nadjusting dynamically to reflect the evolving nature of the\nsystem and its underlying processes [23].\nTo overcome these challenges in developing our pro-\nposed adaptive data quality scoring framework, we have in-\ntegrated a dynamic method from our prior research, designed\nto efficiently detect changes in time-series data distributions\n[24]. This method has been Incorporated into our proposed\nframework to monitor the distribution of incoming applica-\ntion data. Specifically, it actively observes the divergence\nvalues computed for each time window frame using a sliding\nwindow mechanism. The divergence value is calculated us-\ning the Jensen-Shannon divergence, defined as follows [25]:\n$JSD(P_{his} || P_{cur}) := \\frac{1}{2} (H(\\frac{P_{his} + P_{cur}}{2}) - \\frac{H(P_{his}) + H(p_{cur})}{2})$\nwhere $P_{his}$ and $P_{cur}$ represent the probability distributions for\nthe historical and current application data samples, respec-\ntively, and the function H is the Shannon's entropy given by:\n$H(p) = - \\int p(Y) \\log p(Y)dY$.\nSubsequently, the algorithm makes decisions on drift\ndetection by performing hypothesis testing on the p-value of\nthe observed divergence value, indicating the extremeness\nof the current change magnitude with respect to historical\nmagnitudes. If the p-value falls below the defined signifi-\ncance threshold \u03c4, a drift detection signal is triggered. An\ninherent advantage of this method lies in its avoidance of the\nrequirement of a predefined threshold for drift magnitude.\nFurthermore, its dynamic nature ensures robustness and\nadaptability as the divergence distribution evolves with the\naccumulation of more data, making it a viable solution for\nthe evolving nature of real-world industrial applications."}, {"title": "2.2. Data Quality Assurance", "content": "In the industrial applications of ML systems, the as-\nsurance of data quality is fundamental to developing high-\nperformance decision-making support [26]. In particular,\nrigorous data quality practices are essential to ensure that\nthe data used for both training and inference are of optimal\nquality and represent the underlying processes in a timely\nmanner [27]. Low-quality data in industrial processes that\nare driven by data analytics could be caused by machine\nerrors, inconsistent sensor measurements, or abnormal pat-\nterns, among other interpretations. Therefore, data quality\nassurance is recognized as a critical factor that significantly\nimpacts the cost-effectiveness and operational efficiency of\nindustrial processes, addressing potential faults and improv-\ning overall performance.\nData quality assurance is a multi-dimensional concept\nthat spans various attributes to collectively assess the valid-\nity of collected data [28]. Each dimension offers a unique\nperspective on the specific characteristics of the data. Nu-\nmerous studies have compiled extensive lists, some iden-\ntifying up to 179 distinct quality dimensions [29]. These\ndimensions are further grouped into intrinsic, contextual,\naccessibility, and representational categories [30]. The se-\nlection of data quality dimensions is not uniform, as there\nis no universally accepted definition. Rather, they are often\ndefined based on the specific goals and requirements of a\ngiven task and application [31].\nBased on the data context of our use case, we have\nidentified and selected specific data quality dimensions that\nare highly relevant and applicable to the characteristics of\nour industrial application. We summarize the definitions of\ndata quality dimensions as follows:\n1. Accuracy: This dimension evaluates how well the\nrecorded data aligns with the actual values it is meant\nto represent. It evaluates precision and correctness,\nwhich are essential for detecting anomalies that could\nlead to product defects.\n2. Completeness: This dimension assesses the thor-\noughness of the observed data by checking for missing\nvalues that are collected from the data source. Ensur-\ning that all relevant sensor data is captured and no\nsensor failures occur.\n3. Consistency: This dimension examines whether the\nobserved values conform to the integrity constraints\nof the domain, ensuring that collected data records\nfall within the expected value ranges. This contributes\nto effective product quality monitoring and prevents\nerrors due to incorrect sensor readings.\n4. Timeliness: Describing the relevance of data for spe-\ncific tasks, timeliness assesses whether the observed\ndata is current. In dynamic industrial settings, where\nongoing tasks often demand up-to-date information,\nensuring the currency of collected data becomes of\nhigh importance. This ensures that data is still repre-\nsenting the system and can be trusted for the task.\n5. Skewness: This dimension goes beyond traditional\nmeasures, computing the distribution deviation of ob-\nserved data from a reference distribution. Addressing\nskewness is important for optimized performance, es-\npecially for ML systems, as skewed data distributions\ncan affect model accuracy and generalization. It ver-\nifies that no unexpected changes or unusual patterns\nexist.\nObserving the defined data quality dimensions, a key\npoint of consideration emerges within the context of the\nML systems: a subset of these dimensions, including time-\nliness and skewness, exhibit dynamic characteristics. This\nimplies that the quality aspects of these dimensions may\nvary according to the prevailing conditions. To illustrate,\nfor the timeliness dimension, what is deemed timely and\nwell-fitted in the present may not remain true in the fu-\nture. Similarly, the skewness of the data may vary with\ndifferent seasons, introducing variables such as drift and\nseasonality. Therefore, data demonstrating drift during a\nspecific season, resulting in a low skewness score for the\ncurrent season, might conversely indicate a high score in\na different season. On the other hand, certain dimensions,\nsuch as accuracy, completeness, and consistency, exhibit\nconstant characteristics, maintaining their relevance irre-\nspective of changing conditions. These constant dimensions\nfocus on the precision and validity of data representation\nrather than relying on transient factors and settings within\nthe problem domain. Therefore, the integration of the ML-\nbased scoring framework with an adaptation methodology\nspecifically addresses the adaptive nature of data quality in\ndynamic dimensions within data-centric ML applications.\nThis integration specifically addresses the adaptive nature of\ndata quality in data-centric ML applications."}, {"title": "3. Related Work", "content": "The pivotal role of data quality assessment in ensuring\nthe reliability and effectiveness of data-driven processes has\nbeen examined in diverse applications and domains, high-\nlighting its growing recognition in the research community.\nIn the field of healthcare, such as electronic health records\n(EHRs), researchers have explored methodologies to assess\nand improve the quality of patient data, acknowledging its\ncritical impact on medical decision-making and patient care\n[32]. Within the financial sector, studies have focused on as-\nsessing the quality of financial data to maintain the integrity\nof analytical models and regulatory compliance [33, 34].\nAssessment of spatial data quality has been studied by\nintroducing a comprehensive quality assessment framework\nfor linear features of Volunteered Geographic Information\n(VGI) by integrating novel quality metrics with those com-\nmonly used through factor analysis [35]. The common thread\nbetween these diverse applications is the recognition of data\nquality as a foundational element for robust and trustworthy\nresults [36].\nThe use of IoT technologies in industries to gather and\ngenerate data from IoT sensors, often in real-time, requires\na rigorous evaluation of data quality [37]. Taleb et al. [8]\npresented the Big Data Quality Management Framework\n(BDQMF) as an exhaustive strategy aimed at addressing\ndata quality challenges inherent in large-scale data systems.\nThe framework defines various dimensions of data quality\nand incorporates multiple components dedicated to man-\naging, validating, and monitoring data quality. It includes\na scoring mechanism to quantify different aspects of data\nquality. The authors explored issues related to data quality\nat both the individual cell instance and the schema levels\nwithin datasets. Another recent approach in this context is\nthe big data quality assessment framework (BIGQA) [38].\nThe framework provides a declarative solution specifically\ndesigned for non-expert users, featuring reporting function-\nality to visualize outcomes or scores indicating the quality\nof input datasets. In a different approach, the Data Quality\nAnomaly Detection Framework focuses on anomaly detec-\ntion [11], based on an extended isolation forest model. It\nintroduces the Quality Anomaly Score metric to evaluate the\ndegree of anomalousness in six dimensions of quality.\nIn addition to these studies, Chug et al. [39] have de-\nveloped a method to assess dataset quality using nine di-\nmensions, yielding a comprehensive score, report, and label.\nThe study introduced data quality ingredients as seman-\ntic indicators, identifying nine crucial aspects, including\nprovenance, characteristics, uniformity, metadata coupling,\nmissing cells, duplicate rows, data skewness, inconsistency\nratio in categorical columns, and attribute correlation. These\ningredients contribute to the calculation of the final score. In\na previous study, Ardagna et al. [40] proposed a method to\nenhance computational efficiency by focusing on a specific\ndata subset, reducing both time and resource requirements\nusing parallelization. To convey the reliability of data qual-\nity values, they introduced a confidence metric tied to the\nconsidered data volume and influenced by time constraints"}, {"title": "4. The Adaptive Data Quality Scoring\nOperations Framework", "content": "The proposed adaptive data quality scoring framework\nemploys an ML-based approach to score the data quality in\nindustrial applications. The primary objective of the frame-\nwork is to label the incoming data windows with a score\nthat quantifies the quality of the data window based on\nvarious pre-defined data quality dimensions. This innova-\ntive framework has been designed to address the intrinsic\nlimitations of the non-adaptive static data quality scoring\nframework, especially in industrial contexts characterized by\ndynamic data environments. Specifically, the novel adaptive\nframework effectively handles the dynamic aspects of data\nquality by re-assessing the data quality scores based on\nthe prevalent conditions observed in the application system.\nThis adaptation mechanism, which is based on the drift de-\ntection method, allows the dynamic data quality dimensions\nto conform to the evolving characteristics of the underlying\ndata and ML processes. Furthermore, the drift detection\nmethod eliminates the need to define a fixed window size\nw to retrain the ML model, since the retraining signal is\nactivated only when a drift is detected. The workflow of\nthe proposed adaptive data quality scoring framework is\nillustrated in Figure 2.\nFirst, we provide an overview of the underlying ML-\nbased scoring framework to establish a foundation and en-\nhance understanding. This overview summarizes the frame-\nwork's core components, specifically highlighting its im-\nplementation of MLOps practices to manage the ML re-\ngressor for data quality scoring. This context is crucial to\nunderstanding the operational environment of the proposed\nframework. Subsequently, we dissect the two integral phases\nto achieve the proposed framework: development and de-\nployment phases. The development phase focuses on initial-\nizing the framework and generating the necessary artifacts.\nThese artifacts are then streamlined in the deployment phase,\nenabling dynamic scoring of incoming data windows based\non their quality."}, {"title": "4.1. An Overview of ML-Based Data Quality\nScoring", "content": "The ML-based data quality scoring approach aims to\ndetermine the data quality of the collected data window to\ngenerate a unified score using an ML model [43]. This score\nreflects the overall adequacy of the data windows, consid-\nering multiple dimensions of data quality. Specifically, the\nML-based scoring framework is designed to address the\ncomplexity of scoring data quality in data-driven applica-\ntions [44]. The main innovation in the framework is stream-\nlining the scoring process by using an ML predictor instead\nof traditional standard scoring methods following MLOps\nprinciples, incorporating continuous monitoring and valida-\ntion practices. This integration provides significant speedup\nrates while maintaining high predictive performance levels.\nAdditionally, the framework's runtime remains unaffected\nby the number of quality dimensions, offering practical\nscalability in real-world applications with high sampling\nrates.\nTo accomplish ML-based scoring operations, the frame-\nwork workflow begins by initializing the ML predictor in\na warm-start mode. In particular, the ML predictor is ini-\ntiated using training data that contain ground-truth qual-\nity scores obtained by the standard-based approach. The"}, {"title": "4.2. Development Phase", "content": "Similar to any supervised ML system that requires a\nwarm start for effective initialization and optimal perfor-\nmance [47], the development phase of our proposed frame-\nwork starts with the initiation of crucial system artifacts\nessential for streamlining the solution in production. This\nphase includes tasks such as system setup, parameter con-\nfiguration, establishing the anomaly detector, defining data\ndistribution building parameters, managing metadata to cal-\nculate data quality dimension scores, ML model develop-\nment, and preparing all necessary components for subse-\nquent deployment. Additionally, certain meta-information\nhelps reduce the number of calculations in the deployment\nphase. Table 1 summarizes the overall artifacts produced\nin the development phase. The overall workflow of this\nphase is illustrated in Figure 2. The process begins with\ndata collection from the data source (Step 1), which is then\nsegmented into data windows (Step 2). These data windows\nare processed through various data quality dimensions (Step\n3), and the necessary auxiliary information for calculating\nthese dimensions, such as integrity constraints for consis-\ntency score and file paths, is loaded from a configuration file.\nThis information is used to calculate the individual ground-\ntruth quality scores (Step 4).\nThe core outcome of this phase is the creation of the ML\npredictor for data quality (Step 5). This predictor is trained\non historical data labeled with ground-truth quality scores\n(Step 6). These scores are calculated using the standard\napproach detailed in Section 4.1. To enhance the predictor's\nlearning capabilities and accelerate training, a data mutant\nsimulator is employed. This component introduces potential\ndata quality issues that may occur in real-world scenarios,\nenhancing the predictor's ability to learn data quality issues\nand potentially reducing the amount of real-world data re-\nquired. The mutation parameters are stored in configuration\nfiles, which include settings like the percentage of faults to\nbe simulated. Continuous performance monitoring is imple-\nmented using a test oracle to systematically evaluate the\nperformance of the ML predictor (Step 7), with a prede-\nfined threshold for a selected performance metric. Consistent\nachievement of this threshold indicates sufficient learning\nfor deployment to the production environment (Step 8),\nmarking the completion of the development phase."}, {"title": "4.3. Deployment Phase", "content": "Moving from the development environment to the pro-\nduction environment, the deployment phase of our adaptive\nframework involves incorporating the developed and related\ncomponents of the ML predictor and integrating them into\nthe operational system. To manage the ML system in this\nphase, the principles of MLOps are followed, including\nCI/CD. The adoption of MLOps deployment strategies re-\nsults in a systematic and efficient process for deploying and\nmaintaining the ML model in a live operational context [48].\nThe MLOps ecosystem encompasses a broad set of practices,\ntools, and techniques designed to automate the ML life cycle.\nThis includes version control, automated testing, continuous\nmonitoring, and continuous model updates, which makes it\nvalid for industrial applications with minimal intervention\n[49].\nAs illustrated in Figure 2, the real-time data collected\nfrom the source (Step 1) is segmented into data windows\n(Step 2) during the deployment phase. Subsequently, a\nchange detector is applied to the data window to assess\nthe occurrence of drift (Step 3). The change detector per-\nforms hypothesis testing to determine the significance of\nthe drift magnitude, as explained in detail in Section 2.1.\nThe change detector leverages meta-information collected\nduring the development phase, such as historical PDF for\ndata and divergence values, both of which are used in making\nthe calculations in this step. The result is then forwarded\nto the method activator component, which in turn makes\ndecisions about which flow to proceed with (Step 4). The\nflow chart diagram of the activator component of the method\nis depicted in Figure 3.\nIn the absence of detected drift, the method activator\ncomponent continues using the current ML model (Step\n5a) without initiating adaptation to make the DQ scoring\npredictions (Step 6a). However, in the case of drift detection,\nthe activator component of the method initiates a retraining\nsignal (Step 5b). This signal triggers the adaptation process\nflow, which simulates the re-scoring of the historical data\nbased on the prevailing conditions and updates the scores\nof the dynamic data quality dimensions, and scoring the\nnewly collected data (Step 6b). This mechanism ensures\nthat the previous ground-truth labels are properly updated to\nreflect the changes in the dynamic data quality dimensions.\nSubsequently, the ML model is retrained from scratch using\nboth the updated development-time training data and the\nnewly scored data points (Step 7b). This approach ensures\nthat the model fully integrates the updated ground-truth\nlabels for historical data, reflecting the changes in dynamic\ndata quality dimensions. Additionally, a version control sys-\ntem is employed to manage and track updates to both the\ndata quality scores and the ML model (Step 8), ensuring\nthat frequent updates are systematically documented and\nmanaged."}, {"title": "5. Experimental Results", "content": "To evaluate our proposed framework, practical exper-\niments were carried out in collaboration with Uddeholms\nAB, a leading steel manufacturer\u00b9. These experiments were\ncarried out to assess the efficiency of the framework from\nvarious perspectives, including the accuracy of predictive\nperformance, execution time, and resource consumption\nover time. Furthermore, we conducted a comparative anal-\nysis between our adaptive approach, the static data scoring\napproach, and the standard scoring approach. This analysis\nis presented to provide detailed insight into the merits\nand limitations of each approach. The following subsection\npresents a detailed description of the industrial use case\nand implementation, followed by a thorough analysis and\ndiscussion of the experimental results."}, {"title": "5.1. Implementation and Use Case Details", "content": "Our adaptive approach is designed to be streamlined\ninto a broader software system specifically developed to\nassist decision-making in industrial processes. In this set\nof experiments, the proposed framework was implemented\nand evaluated in the industrial application use case of the\nElectroslag Remelting (ESR) vacuum pumping process at\nthe Uddeholm steel manufacturer in Sweden. The overall AI-\ndriven application aims to sustain the production of high-\nquality steel by monitoring the pressure values. A represen-\ntation view of our studied application is shown in Figure 4.\nStarting with industrial machinery, the figure shows the\nkey ingredients of the application, including the furnace\nwhere a sensor collects pressure data within the vacuum\nchamber. Each time the vacuum pump is activated, which\ncan take up to 20 minutes, pressure values are continuously\nmonitored in this use case. The sensor records values ev-\nery millisecond and through the Apache Kafka streaming"}, {"title": "5.2. Drift Detection Sensitivity Analysis", "content": "In a real-world production environment, the effective-\nness of the adaptive data quality scoring approach is highly\ndependent on the change detection component to initiate the\nadaptation signal, a mechanism detailed in Section 4.3. A\nsensitivity analysis of the drift detection mechanism in the\napproach was conducted to understand the behavior of the\nchange detector. This analysis involved varying the signifi-\ncance thresholds represented by the p-values. The results, vi-\nsually presented in Figure 5, illustrate the number of changes\ndetected at different p-values, with the values tested includ-\ning \u03c4 = [0.03, 0.035, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1].\nThe analysis reveals a gradual increase in the number\nof detections from a significance level \u03c4 of 0.03 to 0.09,\nranging from 8 detections to 24, respectively, demonstrating\na steady response to subtle variations. However, a significant\nspike is observed at a p-value of 0.1, reaching 578 detections,\nindicating a potential higher sensitivity that could lead to\nfalse alarms. Therefore, to maintain a diverse set of detection\nnumbers in our experiments, we selected p-values of 0.03,\n0.04, 0.06, 0.08, and 0.09, excluding 0.1 due to its observed\nhigh sensitivity."}, {"title": "5.3. Performance Analysis of DQ Scoring\nPredictions", "content": "The predictive performance of our data quality scoring\nML model in the adaptive framework through a series of\nexperiments using the defined levels of significance thresh-\nold \u03c4. The evaluation involves calculating the errors of the\nXGBoost regressor results, measured by two metrics: Mean\nAbsolute Error (MAE) and R-squared (R2), which are given\nby:\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$.\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$.\nWhere n is the number of data points, $y_i$ is the actual value, $\\hat{y}_i$\nis the predicted value, and $\\bar{y}$ is the mean of the actual values.\nThe predictive performance of the framework was evalu-\nated by examining the observed errors over time, specifically\nMAE, presented in Figure 6, and R2 in Figure 7. Each\nsubfigure displays the error metric along with the corre-\nsponding detected drifts for each significance level \u03c4, anno-\ntated with shaded areas representing the highest and lowest\nperformances among the remaining levels. The results show\nthat drifts are detected; hence, adaptation is executed more\nfrequently in the initial stages of deployment than in later\nstages. This observation suggests that the drift detection\nmechanism becomes more robust over time, resulting in\nfewer false alarms as more divergence values are collected.\nFor prediction errors, the results show that after the\nexecution of adaptation mechanisms prompted by drift de-\ntection, the predictive performance often improves. This\nimprovement is particularly reflected in the decline in MAE\nand an increase in R2 metrics after adaptation. Specifically,\nat the end of the experimental duration, the MAE metric\nvalues are 0.136 for experiments with \u03c4 = 0.03 and 0.112\nfor experiments with \u03c4 = 0.09. The values for the R2\nmetric are 0.949 for experiments with \u03c4 = 0.03 and 0.978\nfor experiments with \u03c4 = 0.09. These variations in results\nare attributed to the fact that a lower t leads to a reduced\nsensitivity, resulting in fewer triggered adaptations that may\nnot be sufficient to update the predictor, while a higher \u03c4\nleads to more frequent adaptations."}, {"title": "5.4. Time required analysis", "content": "The time required for various configurations was ana-\nlyzed to understand their computational efficiency in the\ncontext of industrial tasks. Specifically, a comparison was\nperformed between different scoring methodologies, includ-\ning the adaptive, static, and the standard scoring approaches,\nto assess the time overhead in managing the data streams.\nThe results of the time required to process the scoring task\nfor the different approaches are summarized in Figure 10.\nOf all the approaches tested, we can see that the standard\nscoring methodology was the most time-consuming during\nthe experimental period, requiring a total processing time\nof 850.45 seconds. However, the static approach with a\nwindow size of 200 achieved the shortest processing time"}, {"title": "5.5. Analysis of Dynamic Data Quality Dimension\nScores", "content": "In this section, we analyze the evolution of dynamic data\nquality dimensions, specifically timeliness and skewness,\nunder various drift occurrences. The analysis focuses on ten\ndata points to highlight the dynamic nature of these data\nquality dimensions over time. The results of the discrepancy\nbetween each data point's score before and after each drift\noccurrence (for t = 0.03 across 8 drift occurrences), indicat-\ning how much the score changed, are summarized in Figures\n11a and 11b, respectively. The numbers presented show the\ndifference in the skewness and timeliness scores, highlight-\ning how adaptation affects the score of the respective data\nquality dimension.\nThe findings reveal a substantial difference in magnitude\nafter each adaptation, up to 0.3 for skewness and 0.4 for\ntimeliness. The most significant change occurs immediately\nafter the first occurrence after production, indicating that the\nsystem is more sensitive to change during this period. These\nexperiments indicate a significant scale of change in the data\nquality dimension scores, signifying the impact of evolving\npatterns of the underlying data distribution. Furthermore,\nthis shows the important role of adaptation in rescaling the\ndata based on the prevalent situation, ensuring that the data\nquality dimensions accurately reflect the evolving character-\nistics of the dataset as their scores evolve over time."}, {"title": "5.6. Resource Consumption", "content": "The analysis of resource consumption explores the per-\ncentage of CPU and memory usage for each scoring ap-\nproach, providing information on their respective resource\ndemands in terms of computational and memory require-\nments. This analysis directly impacts operational costs and\nsystem efficiency, which are crucial factors in determining\nthe most suitable approach to employ in production by\nindustries. The percentage of CPU usage of the different\napproaches is summarized in Figure 12, while memory\nusage is summarized in Figure 13.\nThe results indicate that the adaptive approach consumes\nslightly more CPU compared to the static, while the memory\nconsumption between the two approaches is very similar.\nThis difference in resource consumption can be attributed\nto the dynamic change detector mechanism employed by the\nadaptive approach, which actively monitors and updates the\nML predictor based on detected drifts in the data streams,\nresulting in slightly higher computational overheads. On the\ncontrary, the standard approach showed the lowest CPU and\nmemory usage across the board due to its lack of ML involve-\nment, resulting in a more straightforward and less resource-\nintensive process. Moreover, the analysis also reveals that\nvariations in parameters within each approach do not sig-\nnificantly impact the memory consumption percentage, and\nthe difference in the boxes is negligible. However, the dis-\ntinction is more apparent when considering CPU usage,\nprimarily due to the ML operations involved. Additionally,\nthe adaptive approach tends to exhibit wider ranges between\nthe CPU and memory usage whiskers compared to the\nstatic approach. This characteristic arises from the detection\nmechanism, which occasionally triggers adaptations in the\nML predictor. These adaptations lead to higher fluctuations\nin resource usage over time."}, {"title": "5.7. Operational Insights and Key Takeaways", "content": "In this set of real-world industrial experiments, the be-\\"}]}