{"title": "Adaptive Data Quality Scoring Operations Framework using Drift-Aware Mechanism for Industrial Applications", "authors": ["Firas Bayrama", "Bestoun S. Ahmedab", "Erik Hallin"], "abstract": "Within data-driven artificial intelligence (AI) systems for industrial applications, ensuring the reliability of the incoming data streams is an integral part of trustworthy decision-making. An approach to assess data validity is data quality scoring, which assigns a score to each data point or stream based on various quality dimensions. However, certain dimensions exhibit dynamic qualities, which require adaptation on the basis of the system's current conditions. Existing methods often overlook this aspect, making them inefficient in dynamic production environments. In this paper, we introduce the Adaptive Data Quality Scoring Operations Framework, a novel framework developed to address the challenges posed by dynamic quality dimensions in industrial data streams. The framework introduces an innovative approach by integrating a dynamic change detector mechanism that actively monitors and adapts to changes in data quality, ensuring the relevance of quality scores. We evaluate the proposed framework performance in a real-world industrial use case. The experimental results reveal high predictive performance and efficient processing time, highlighting its effectiveness in practical quality-driven AI applications.", "sections": [{"title": "1. Introduction", "content": "Industries and businesses are actively accumulating data in unprecedented volumes, marking a definitive shift towards a data-centric paradigms [1]. Within this landscape, data has transcended its conventional role to become the cornerstone of success for artificial intelligence (AI) software solutions. Data holds an intrinsic value due to its inseparable link to the life cycle of machine learning (ML), which constitutes the primary type of AI. Therefore, assessing the quality of the collected data becomes an essential and imperative aspect in building robust and reliable ML solutions [2]. Remarkably, a prevalent concern in the industry is the disproportionate allocation of efforts in ML projects in research. According to a recent study conducted by MIT scientists [3], research institutions often allocate 90% of their efforts to improve ML algorithms and only 10% to data preparation and validation. The authors suggested that these numbers should be reversed for better overall outcomes. In light of this, the significance of data quality (DQ) assurance extends beyond mere procedural correctness. It plays a pivotal role in influencing the cost-effectiveness and operational efficiency of industrial processes and operations.\nThere are two main approaches to data quality assessment: quantitative assessments that involve scales and metrics to quantify aspects of data quality, and qualitative assessments that focus on inherent characteristics and subjective evaluations [4]. Quantitative assessments provide a measurement, offering more detailed information about the objective estimates of data quality. This approach involves assigning specific numerical values to various aspects, called a quality score or index [5], which enables a quantifiable understanding of data quality. In contrast, qualitative assessments contribute to a more holistic perspective by exploring the intrinsic qualities and subjective aspects of the data, referred to as data profiling [6]. Through the use of scales and metrics, our study focuses on quantitative data scoring, as it provides valuable insights for industries.\nData quality scoring is a methodological approach that involves evaluating and assigning scores to data records based on predefined criteria across several data quality dimensions [7]. Each data quality dimension captures a unique aspect, collectively contributing to a thorough assessment of the overall data quality. The scores indicate the acceptance level of the addressed data quality dimensions, distinguishing between high- and poor-quality data. Furthermore, data quality scores are interpreted as the degree to which data quality conforms to specified aspects of data quality [8]. In the context of large-scale systems, the importance of these data quality scores extends beyond the evaluation of data quality and can be exploited to enhance the overall performance of the ML system [9].\nFrom a data-centric AI perspective, assessing data quality is crucial; nevertheless, it adds computational costs to systems, especially in real-time production environments. The practicality of these assessments in operational contexts poses a notable challenge due to inherent complexities and resource-intensive nature in industrial use cases. Therefore, streamlining these processes becomes a vital requirement to ensure their effectiveness and success in practical applications [10]. Two primary approaches exist for data quality scoring: a traditional standard approach and an automated approach utilizing an ML-based system [11]. The standard approach involves checking a predefined set of data quality dimensions and assigning scores to each dimension. In contrast, ML-based methods employ an ML regressor to predict the score of the processed data window instead of the manual scoring procedure.\nIn the domain of industrial processes where real-time data analysis is crucial, ML-based methods present benefits in terms of efficiency and speed [12]. Using ML algorithms can significantly reduce the time and resources required for data quality assessment, especially in large-scale environments [13]. The choice of an ML-based approach over traditional methods is driven by the need for a more scalable, efficient, and adaptive solution. ML-based methods can process large volumes of data quickly in real-time, providing a more timely evaluation of data quality. The predictive capabilities of ML models enable faster analysis and decision-making, enhancing the overall effectiveness of data quality evaluation processes. This capability is particularly beneficial in industries such as manufacturing, where early identification and resolution of data quality issues can optimize production processes and minimize downtime.\nML-based approaches can be categorized into adaptive and non-adaptive methods [14]. Regarding data quality assessment, adaptivity is not only about ML model adjustments. Rather, it refers to the ability to respond effectively to dynamic changes in data quality dimensions. These adaptive methods continuously monitor and update their quality assessment criteria based on incoming data, allowing them to respond effectively to evolving conditions. In contrast, non-adaptive methods rely on static quality assessment criteria throughout the analysis, potentially overlooking changes in data quality over time.\nExisting data quality scoring frameworks often neglect the crucial aspect of adaptivity, presenting a major obstacle to effective management of dynamic data quality. Specifically, non-adaptive ML frameworks have two inherent limitations that need to be addressed. Firstly, determining the optimal size of the optimal time checkpoint to retrain the ML model in production can be challenging and may lead to inefficiencies. Secondly, certain data quality dimensions exhibit a dynamic nature, reflecting the fluctuating conditions of industrial systems. For example, highly relevant data in one phase may be regarded as of a lower quality under different circumstances. The limitations of non-adaptive frameworks highlight the importance of retraining the ML model to capture these dynamic changes. ML models are trained on historical data to learn the characteristics of high- and low-quality data according to current conditions. When data characteristics change, quality scores also change, and an adaptation signal should be triggered to adapt the ML model to the evolving data quality characteristics, ensuring constant learning for more accurate assessments.\nThis paper introduces an innovative approach to tackle the dynamic challenges of data quality assessment by proposing a novel framework that integrates adaptivity into ML-based data quality scoring methodologies. Our main contribution lies in addressing the evolving nature of data quality and incorporating drift detection mechanisms to enhance data quality scoring accuracy. The proposed framework dynamically adjusts the retraining process based on evolving data patterns, enabling a more precise and adaptable data quality assessment over time. In addition, we introduce adaptive mechanisms to facilitate dynamic responses and recalibration of data quality scores according to the prevailing system conditions. This approach effectively meets the demands of large-scale industrial processes where data quality requirements evolve continuously.\nThe remainder of the paper is structured as follows. Section 2 outlines the conceptual background necessary to understand our proposed framework. In Section 3, we provide an overview of the existing literature related to data quality assessments. Following this, Section 4 introduces the framework with its development and deployment phases. Section 5 presents implementation details along with an analysis of its predictive performance and processing time efficiency. Finally, in Section 6, we summarize the key contributions of the paper and discuss potential opportunities for future research."}, {"title": "2. Conceptual Background", "content": "Continuous monitoring and assessment of the incoming data streams are crucial to ensure that dynamic changes are captured and that the prevalent condition (concept) remains consistent. In data-centric applications, it is essential to verify that data quality remains high. Data quality is important in ensuring the data remains useful, as low-quality data can lead to physical failures or inaccuracies in sensor readings [15]. This process is illustrated in Figure 1, where time-series data collected from the data source undergoes both drift detection and data quality assessment. Consequently, the AI system benefits from valuable meta-information obtained from these stages. Continuous drift monitoring ensures that the incoming data remain contextually similar to the training data [16], allowing intelligent systems to make informed decisions based on high-quality information [17]. Within the context of data-driven applications, these changes can have implications for the performance of both data quality and application-level ML systems. They signify the need for adaptation to maintain system reliability. In practice, application-level ML systems utilize drift detection to sustain performance in dynamic environments, while data-quality scoring ML systems use it to ensure the quality and reliability of the generated data. Despite their different objectives, both application-level and data quality scoring ML systems rely on the same core principle: monitoring changes in data characteristics. This common foundation allows for the successful application of identical drift detection mechanisms across both model types. This paper focuses primarily on adapting data quality scoring ML systems within the holistic AI application. This section provides essential background knowledge on these core concepts, laying the foundation for a comprehensive understanding of the fundamental elements within the scope of this research."}, {"title": "2.1. Drift Monitoring", "content": "Drift in data streams refers to the temporal evolution or evolving changes in the underlying statistical properties of the data [18]. Detecting drift involves employing a methodology to determine and identify significant changes in data streams at a specific time point t. In industrial settings, detecting drift is a fundamental task that serves to indicate and diagnose the status of the ingested data, prompting appropriate actions in response to identified changes [19]. Typically, drift is detected by quantifying the dissimilarity between the data probability distributions of two timestamps using the divergence metric. If the divergence metric exceeds a certain threshold \u03b6, it signifies a drift occurred in the data [20]. This metric can be calculated as follows:\nD\u2081 = \u03b4(P || Q).\n\nThe decision-making logic is defined as follows: If D\u2081 > \u03b6, a drift in the data is detected; otherwise, no detection occurs. Where D, represents the divergence metric recorded at timestamp t between probability distributions P and Q, and 8 is a function measuring this divergence.\nIn the context of nonstationary applications, determining a suitable threshold value (for detecting drift poses a significant challenge [21]. Industrial data streams are usually dynamic and complex, resulting in constantly changing statistical properties. This variability makes establishing a single, universal threshold impractical. Moreover, effectively detecting drift requires a subtle understanding of the specific industrial processes and the context of the application [22]. As a result, recent research concludes that the drift detection threshold should not be fixed. Instead, it must be adaptive, adjusting dynamically to reflect the evolving nature of the system and its underlying processes [23].\nTo overcome these challenges in developing our proposed adaptive data quality scoring framework, we have integrated a dynamic method from our prior research, designed to efficiently detect changes in time-series data distributions [24]. This method has been Incorporated into our proposed framework to monitor the distribution of incoming application data. Specifically, it actively observes the divergence values computed for each time window frame using a sliding window mechanism. The divergence value is calculated using the Jensen-Shannon divergence, defined as follows [25]:\n$JSD(P_{his} || P_{cur}) := \\frac{1}{2} (H(\\frac{P_{his} + P_{cur}}{2}) - \\frac{H(P_{his}) + H(P_{cur})}{2})$\nwhere $P_{his}$ and $P_{cur}$ represent the probability distributions for the historical and current application data samples, respectively, and the function H is the Shannon's entropy given by:\n$H(p) = - \\int p(Y) log p(Y)dY$.\nSubsequently, the algorithm makes decisions on drift detection by performing hypothesis testing on the p-value of the observed divergence value, indicating the extremeness of the current change magnitude with respect to historical magnitudes. If the p-value falls below the defined significance threshold \u03c4, a drift detection signal is triggered. An inherent advantage of this method lies in its avoidance of the requirement of a predefined threshold for drift magnitude. Furthermore, its dynamic nature ensures robustness and adaptability as the divergence distribution evolves with the accumulation of more data, making it a viable solution for the evolving nature of real-world industrial applications."}, {"title": "2.2. Data Quality Assurance", "content": "In the industrial applications of ML systems, the assurance of data quality is fundamental to developing high-performance decision-making support [26]. In particular, rigorous data quality practices are essential to ensure that the data used for both training and inference are of optimal quality and represent the underlying processes in a timely manner [27]. Low-quality data in industrial processes that are driven by data analytics could be caused by machine errors, inconsistent sensor measurements, or abnormal patterns, among other interpretations. Therefore, data quality assurance is recognized as a critical factor that significantly impacts the cost-effectiveness and operational efficiency of industrial processes, addressing potential faults and improving overall performance.\nData quality assurance is a multi-dimensional concept that spans various attributes to collectively assess the validity of collected data [28]. Each dimension offers a unique perspective on the specific characteristics of the data. Numerous studies have compiled extensive lists, some identifying up to 179 distinct quality dimensions [29]. These dimensions are further grouped into intrinsic, contextual, accessibility, and representational categories [30]. The selection of data quality dimensions is not uniform, as there is no universally accepted definition. Rather, they are often defined based on the specific goals and requirements of a given task and application [31].\nBased on the data context of our use case, we have identified and selected specific data quality dimensions that are highly relevant and applicable to the characteristics of our industrial application. We summarize the definitions of data quality dimensions as follows:"}, {"title": "3. Related Work", "content": "The pivotal role of data quality assessment in ensuring the reliability and effectiveness of data-driven processes has been examined in diverse applications and domains, highlighting its growing recognition in the research community. In the field of healthcare, such as electronic health records (EHRs), researchers have explored methodologies to assess and improve the quality of patient data, acknowledging its critical impact on medical decision-making and patient care [32]. Within the financial sector, studies have focused on assessing the quality of financial data to maintain the integrity of analytical models and regulatory compliance [33, 34]. Assessment of spatial data quality has been studied by introducing a comprehensive quality assessment framework for linear features of Volunteered Geographic Information (VGI) by integrating novel quality metrics with those commonly used through factor analysis [35]. The common thread between these diverse applications is the recognition of data quality as a foundational element for robust and trustworthy results [36].\nThe use of IoT technologies in industries to gather and generate data from IoT sensors, often in real-time, requires a rigorous evaluation of data quality [37]. Taleb et al. [8] presented the Big Data Quality Management Framework (BDQMF) as an exhaustive strategy aimed at addressing data quality challenges inherent in large-scale data systems. The framework defines various dimensions of data quality and incorporates multiple components dedicated to managing, validating, and monitoring data quality. It includes a scoring mechanism to quantify different aspects of data quality. The authors explored issues related to data quality at both the individual cell instance and the schema levels within datasets. Another recent approach in this context is the big data quality assessment framework (BIGQA) [38]. The framework provides a declarative solution specifically designed for non-expert users, featuring reporting functionality to visualize outcomes or scores indicating the quality of input datasets. In a different approach, the Data Quality Anomaly Detection Framework focuses on anomaly detection [11], based on an extended isolation forest model. It introduces the Quality Anomaly Score metric to evaluate the degree of anomalousness in six dimensions of quality.\nIn addition to these studies, Chug et al. [39] have developed a method to assess dataset quality using nine dimensions, yielding a comprehensive score, report, and label. The study introduced data quality ingredients as semantic indicators, identifying nine crucial aspects, including provenance, characteristics, uniformity, metadata coupling, missing cells, duplicate rows, data skewness, inconsistency ratio in categorical columns, and attribute correlation. These ingredients contribute to the calculation of the final score. In a previous study, Ardagna et al. [40] proposed a method to enhance computational efficiency by focusing on a specific data subset, reducing both time and resource requirements using parallelization. To convey the reliability of data quality values, they introduced a confidence metric tied to the considered data volume and influenced by time constraints"}, {"title": "4. The Adaptive Data Quality Scoring Operations Framework", "content": "The proposed adaptive data quality scoring framework employs an ML-based approach to score the data quality in industrial applications. The primary objective of the framework is to label the incoming data windows with a score that quantifies the quality of the data window based on various pre-defined data quality dimensions. This innovative framework has been designed to address the intrinsic limitations of the non-adaptive static data quality scoring framework, especially in industrial contexts characterized by dynamic data environments. Specifically, the novel adaptive framework effectively handles the dynamic aspects of data quality by re-assessing the data quality scores based on the prevalent conditions observed in the application system. This adaptation mechanism, which is based on the drift detection method, allows the dynamic data quality dimensions to conform to the evolving characteristics of the underlying data and ML processes. Furthermore, the drift detection method eliminates the need to define a fixed window size w to retrain the ML model, since the retraining signal is activated only when a drift is detected. The workflow of the proposed adaptive data quality scoring framework is illustrated in Figure 2.\nFirst, we provide an overview of the underlying ML-based scoring framework to establish a foundation and enhance understanding. This overview summarizes the framework's core components, specifically highlighting its implementation of MLOps practices to manage the ML regressor for data quality scoring. This context is crucial to understanding the operational environment of the proposed framework. Subsequently, we dissect the two integral phases to achieve the proposed framework: development and deployment phases. The development phase focuses on initializing the framework and generating the necessary artifacts. These artifacts are then streamlined in the deployment phase, enabling dynamic scoring of incoming data windows based on their quality."}, {"title": "4.1. An Overview of ML-Based Data Quality Scoring", "content": "The ML-based data quality scoring approach aims to determine the data quality of the collected data window to generate a unified score using an ML model [43]. This score reflects the overall adequacy of the data windows, considering multiple dimensions of data quality. Specifically, the ML-based scoring framework is designed to address the complexity of scoring data quality in data-driven applications [44]. The main innovation in the framework is streamlining the scoring process by using an ML predictor instead of traditional standard scoring methods following MLOps principles, incorporating continuous monitoring and validation practices. This integration provides significant speedup rates while maintaining high predictive performance levels. Additionally, the framework's runtime remains unaffected by the number of quality dimensions, offering practical scalability in real-world applications with high sampling rates.\nTo accomplish ML-based scoring operations, the framework workflow begins by initializing the ML predictor in a warm-start mode. In particular, the ML predictor is initiated using training data that contain ground-truth quality scores obtained by the standard-based approach. The model's prediction accuracy is monitored using a test oracle until it reaches a predetermined threshold T based on a performance metric. Once this threshold is met, the ML predictor is eventually deployed in the real-world problem. Meta-information files, such as the anomaly detection model and reference data distribution, are also prepared to calculate certain data quality dimension scores. Furthermore, as part of this phase, a mutant simulator is integrated to improve the model's learning by introducing a variety of data quality issues that may arise in practical situations, thus accelerating the learning process.\nOnce the framework is deployed in a production environment, the method activator becomes a key component in managing the pipeline flow. It employs predetermined criteria to select the appropriate approach for data quality scoring. Specifically, the activator chooses between the ML-based approach and the standard-based approach based on the collected data window and chunk size. The activator repeatedly executes the ML model to obtain data quality scores until the chunk size reaches a preconfigured threshold \u03b2. At this point, an evaluation is initiated using the standard-based approach to ensure continuous monitoring of the ML model's accuracy. This evaluation employs a specified test oracle to compare predicted quality scores with ground-truth scores, and if the model's performance falls below a pre-defined tolerance level, a retrain signal is activated.\nThe process of finding the consolidated data quality score involves aggregating the calculated values of the data quality dimensions. Traditional methods like the arithmetic mean may not be suitable due to their sensitivity to variable scales. To address this, the quality scores are standardized using z-scores, ensuring uniform integration of different data quality metrics [45]. This standardization involves calculating the z-score for each element in the quality score matrix."}, {"title": "4.2. Development Phase", "content": "Similar to any supervised ML system that requires a warm start for effective initialization and optimal performance [47], the development phase of our proposed framework starts with the initiation of crucial system artifacts essential for streamlining the solution in production. This phase includes tasks such as system setup, parameter configuration, establishing the anomaly detector, defining data distribution building parameters, managing metadata to calculate data quality dimension scores, ML model development, and preparing all necessary components for subsequent deployment. Additionally, certain meta-information helps reduce the number of calculations in the deployment phase. The overall workflow of this phase is illustrated in Figure 2. The process begins with data collection from the data source (Step 1), which is then segmented into data windows (Step 2). These data windows are processed through various data quality dimensions (Step 3), and the necessary auxiliary information for calculating these dimensions, such as integrity constraints for consistency score and file paths, is loaded from a configuration file. This information is used to calculate the individual ground-truth quality scores (Step 4).\nThe core outcome of this phase is the creation of the ML predictor for data quality (Step 5). This predictor is trained on historical data labeled with ground-truth quality scores (Step 6). These scores are calculated using the standard approach detailed in Section 4.1. To enhance the predictor's learning capabilities and accelerate training, a data mutant simulator is employed. This component introduces potential data quality issues that may occur in real-world scenarios, enhancing the predictor's ability to learn data quality issues and potentially reducing the amount of real-world data required. The mutation parameters are stored in configuration files, which include settings like the percentage of faults to be simulated. Continuous performance monitoring is implemented using a test oracle to systematically evaluate the performance of the ML predictor (Step 7), with a predefined threshold for a selected performance metric. Consistent achievement of this threshold indicates sufficient learning for deployment to the production environment (Step 8), marking the completion of the development phase."}, {"title": "4.3. Deployment Phase", "content": "Moving from the development environment to the production environment, the deployment phase of our adaptive framework involves incorporating the developed and related components of the ML predictor and integrating them into the operational system. To manage the ML system in this phase, the principles of MLOps are followed, including CI/CD. The adoption of MLOps deployment strategies results in a systematic and efficient process for deploying and maintaining the ML model in a live operational context [48]. The MLOps ecosystem encompasses a broad set of practices, tools, and techniques designed to automate the ML life cycle. This includes version control, automated testing, continuous monitoring, and continuous model updates, which makes it valid for industrial applications with minimal intervention [49].\nAs illustrated in Figure 2, the real-time data collected from the source (Step 1) is segmented into data windows (Step 2) during the deployment phase. Subsequently, a change detector is applied to the data window to assess the occurrence of drift (Step 3). The change detector performs hypothesis testing to determine the significance of the drift magnitude, as explained in detail in Section 2.1. The change detector leverages meta-information collected during the development phase, such as historical PDF for data and divergence values, both of which are used in making the calculations in this step. The result is then forwarded to the method activator component, which in turn makes decisions about which flow to proceed with (Step 4). The flow chart diagram of the activator component of the method is depicted in Figure 3.\nIn the absence of detected drift, the method activator component continues using the current ML model (Step 5a) without initiating adaptation to make the DQ scoring predictions (Step 6a). However, in the case of drift detection, the activator component of the method initiates a retraining signal (Step 5b). This signal triggers the adaptation process flow, which simulates the re-scoring of the historical data based on the prevailing conditions and updates the scores of the dynamic data quality dimensions, and scoring the newly collected data (Step 6b). This mechanism ensures that the previous ground-truth labels are properly updated to reflect the changes in the dynamic data quality dimensions. Subsequently, the ML model is retrained from scratch using both the updated development-time training data and the newly scored data points (Step 7b). This approach ensures that the model fully integrates the updated ground-truth labels for historical data, reflecting the changes in dynamic data quality dimensions. Additionally, a version control system is employed to manage and track updates to both the data quality scores and the ML model (Step 8), ensuring that frequent updates are systematically documented and managed."}, {"title": "5. Experimental Results", "content": "To evaluate our proposed framework, practical experiments were carried out in collaboration with Uddeholms AB, a leading steel manufacturer\u00b9. These experiments were carried out to assess the efficiency of the framework from various perspectives, including the accuracy of predictive performance, execution time, and resource consumption over time. Furthermore, we conducted a comparative analysis between our adaptive approach, the static data scoring approach, and the standard scoring approach. This analysis is presented to provide detailed insight into the merits and limitations of each approach. The following subsection presents a detailed description of the industrial use case and implementation, followed by a thorough analysis and discussion of the experimental results."}, {"title": "5.1. Implementation and Use Case Details", "content": "Our adaptive approach is designed to be streamlined into a broader software system specifically developed to assist decision-making in industrial processes. In this set of experiments, the proposed framework was implemented and evaluated in the industrial application use case of the Electroslag Remelting (ESR) vacuum pumping process at the Uddeholm steel manufacturer in Sweden. The overall AI-driven application aims to sustain the production of high-quality steel by monitoring the pressure values. A representation view of our studied application is shown in Figure 4.\nStarting with industrial machinery, the figure shows the key ingredients of the application, including the furnace where a sensor collects pressure data within the vacuum chamber. Each time the vacuum pump is activated, which can take up to 20 minutes, pressure values are continuously monitored in this use case. The sensor records values every millisecond and through the Apache Kafka streaming platform\u00b2, data windows are transmitted every second for real-time analysis. Subsequently, each data window proceeds to data streaming and validation services, passing through a data quality scoring framework. Subsequently, the scoring framework validates the collected data, assesses its quality, and generates a score. Following this, in the AI-driven decision-making process, alarms are triggered for improper pump events. These alerts are communicated to the maintenance team of the operational unit, which takes appropriate actions, such as stopping the pump event to eliminate its costs. The primary purpose of the application is to achieve a gradual decrease in pressure during appropriate pump events, reaching the desired minimum value within the allocated time while promptly identifying and addressing improper pump events to prevent interruptions and uphold optimal furnace operation.\nIn terms of implementation details, our proposed framework was built with the Python programming language, taking advantage of its multipurpose capabilities and wide range of libraries. The implementation also incorporates the YAML format for defining configuration files. These YAML-formatted configuration files allow for the clear and organized specification of different parameters and settings, ultimately enabling the flexibility and customization of the framework to meet specific requirements of the use case. For the ML models, we employ the widely-used extreme gradient boosting (XGBoost) model [50]. This decision tree-based ensemble technique has proven effective across various applications, demonstrating its suitability for industrial contexts and its capability to provide strong and easily understandable results [51]."}, {"title": "5.2. Drift Detection Sensitivity Analysis", "content": "In a real-world production environment, the effectiveness of the adaptive data quality scoring approach is highly dependent on the change detection component to initiate the adaptation signal, a mechanism detailed in Section 4.3. A sensitivity analysis of the drift detection mechanism in the approach was conducted to understand the behavior of the change detector. This analysis involved varying the significance thresholds represented by the p-values. The results, visually presented in Figure 5, illustrate the number of changes detected at different p-values, with the values tested including \u03c4 = [0.03, 0.035, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1].\nThe analysis reveals a gradual increase in the number of detections from a significance level \u03c4 of 0.03 to 0.09, ranging from 8 detections to 24, respectively, demonstrating a steady response to subtle variations. However, a significant spike is observed at a p-value of 0.1, reaching 578 detections, indicating a potential higher sensitivity that could lead to false alarms. Therefore, to maintain a diverse set of detection numbers in our experiments, we selected p-values of 0.03, 0.04, 0.06, 0.08, and 0.09, excluding 0.1 due to its observed high sensitivity."}, {"title": "5.3. Performance Analysis of DQ Scoring Predictions", "content": "The predictive performance of our data quality scoring ML model in the adaptive framework through a series of experiments using the defined levels of significance threshold \u03c4. The evaluation involves calculating the errors of the XGBoost regressor results, measured by two metrics: Mean Absolute Error (MAE) and R-squared (R2), which are given by:\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\nWhere n is the number of data points, y\u1d62 is the actual value, \u0177\u1d62 is the predicted value, and \u0233 is the mean of the actual values.\nThe predictive performance of the framework was evaluated by examining the observed errors over time, specifically MAE, presented in Figure 6, and R2 in Figure 7. Each subfigure displays the error metric along with the corresponding detected drifts for each significance level \u03c4, annotated with shaded areas representing the highest and lowest performances among the remaining levels. The results show that drifts are detected; hence, adaptation is executed more frequently in the initial stages of deployment than in later stages. This observation suggests that the drift detection mechanism becomes more robust over time, resulting in fewer false alarms as more divergence values are collected.\nFor prediction errors, the results show that after the execution of adaptation mechanisms prompted by drift detection, the predictive performance often improves. This improvement is particularly reflected in the decline in MAE and an increase in R2 metrics after adaptation. Specifically, at the end of the experimental duration, the MAE metric values are 0.136 for experiments with \u03c4 = 0.03 and 0.112 for experiments with \u03c4 = 0.09. The values for the R2 metric are 0.949 for experiments with \u03c4 = 0.03 and 0.978 for experiments with \u03c4 = 0.09. These variations in results are attributed to the fact that a lower t leads to a reduced sensitivity, resulting in fewer triggered adaptations that may not be sufficient to update the predictor, while a higher \u03c4 leads to more frequent adaptations.\nWe extend the analysis to evaluate the performance of the non-adaptive static approach, a framework dependent on a fixed window size w for adaptation. Unlike the adaptive approach, where adaptation is determined by significance level \u03c4, the static approach triggers adaptation if the window size has been reached and the ML performance falls below a defined level. The experiments conducted with the static approach involve varying window sizes for adaptation triggers, with four different window sizes w: 25, 50, 100, and 200. Smaller window sizes represent a more frequent assessment for adaptations, while larger window sizes offer a wider temporal range. The corresponding MAE and R2 metrics for the static approach are presented in Figures 8 and 9, respectively. Each subfigure within these figures illustrates the cumulative error metrics over time for different window sizes. As with our adaptive approach, the shaded area represents the range of performance for static approaches with other window sizes.\nThe results show that smaller window sizes show superior performance in both the MAE and R2 metrics. Specifically, for a window size of 25, the MAE at the end of the experimental period is 0.11, while it is 0.257 for a window size of 200. Furthermore, the metric R\u00b2 for a window size of 25 is 0.968, while it is 0.901 for a window size of 200. Similarly to our adaptive approach, there is a noticeable improvement following the adaptation process. This improvement is particularly evident in the plot for window size 200, as depicted in Figures 8d and 9d. Specifically, these figures illustrate a significant performance drop before reaching the window size of 200, indicating the need to update the predictor. In contrast, the window size of 25 follows a more frequent adaptation pattern, resulting in a faster recovery from performance drops, as shown in Figures 8a and 9a.\nWhen comparing our adaptive approach with the static approach, we can observe that both frameworks follow similar performance at the end of the experimental period. For the adaptive approach, optimal performance is observed at a significance threshold of 0.03, which aligns the performance of the static approach with a window size of 25. However, as we deviate from the optimal adaptation parameter of each methodology, performance starts to degrade gradually. Additionally, during the initial phases of the experiments, the performance of both adaptive and static approaches is more volatile, which can be explained by the models' sensitivity to changes in input, but as time progresses, the predictor increasingly matures and its performance becomes more consistent and stable. Meanwhile, the static approach displays a wider range of performance than the adaptive, reflecting the cumulative effect of errors across various window sizes."}, {"title": "5.4. Time required analysis", "content": "The time required for various configurations was analyzed to understand their computational efficiency in the context of industrial tasks. Specifically, a comparison was performed between different scoring methodologies, including the adaptive, static, and the standard scoring approaches, to assess the time overhead in managing the data streams. The results of the time required to process the scoring task for the different approaches are summarized in Figure 10.\nOf all the approaches tested, we can see that the standard scoring methodology was the most time-consuming during the experimental period, requiring a total processing time of 850.45 seconds. However, the static approach with a window size of 200 achieved the shortest processing time among all methods due to the lower frequency of adaptation triggers associated with larger window sizes. We can also observe that both the standard scoring and static approaches exhibited linear trends, with processing times accelerating rapidly over time. In contrast, the adaptive approach showed a more conservative trend, with processing times increasing at a slower pace. The discrepancy is the result of the periodic adaptation mechanism inherent in the static approach, whereas the adaptive approach only adapts when drift is detected, leading to a more regulated processing speed.\nFurthermore, the static approach demonstrated higher sensitivity to variations in the adaptation parameter, particularly the window size w, leading to increased dispersion in processing times. In particular, as we move from a larger w of 200 to smaller sizes like 25, there is a substantial increase in processing time, from 62.15 seconds to 515.30 seconds. This sensitivity is also observed to a lesser extent in the adaptive approach. As indicated by the results, the adaptive approach with \u03c4 = 0.03 required a processing time of 102.05 seconds and reached 203.91 seconds for t = 0.09.\nOverall, the time analysis reveals significant improvements in both processing efficiency and scalability with the adaptive approach. Specifically, the adaptive approach with \u03c4 = 0.03 achieved a processing time of 102.05 seconds an 88% reduction compared to the 850.45 seconds required by the standard scoring approach, representing a speedup factor of approximately 8.3x. In terms of scalability, Figure 10 illustrates that the processing time for the standard approach grows linearly with data volume, while our adaptive approach shows a more restrained growth rate. These trends suggest that the adaptive approach would maintain its efficiency advantage even as data volume scales."}, {"title": "5.5. Analysis of Dynamic Data Quality Dimension Scores", "content": "In this section, we analyze the evolution of dynamic data quality dimensions, specifically timeliness and skewness, under various drift occurrences. The analysis focuses on ten data points"}]}