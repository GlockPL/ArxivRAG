{"title": "DOA-Aware Audio-Visual Self-Supervised Learning for Sound Event Localization and Detection", "authors": ["Yoto Fujita", "Yoshiaki Bando", "Keisuke Imoto", "Masaki Onishi", "Kazuyoshi Yoshii"], "abstract": "This paper describes sound event localization and detection (SELD) for spatial audio recordings captured by first-order ambisonics (FOA) microphones. In this task, one may train a deep neural network (DNN) using FOA data annotated with the classes and directions of arrival (DOAs) of sound events. However, the performance of this approach is severely bounded by the amount of annotated data. To overcome this limitation, we propose a novel method of pretraining the feature extraction part of the DNN in a self-supervised manner. We use spatial audio-visual recordings abundantly available as virtual reality contents. Assuming that sound objects are concurrently observed by the FOA microphones and the omni-directional camera, we jointly train audio and visual encoders with contrastive learning such that the audio and visual embeddings of the same recording and DOA are made close. A key feature of our method is that the DOA-wise audio embeddings are jointly extracted from the raw audio data, while the DOA-wise visual embeddings are separately extracted from the local visual crops centered on the corresponding DOA. This encourages the latent features of the audio encoder to represent both the classes and DOAs of sound events. The experiment using the DCASE2022 Task 3 dataset of 20 hours shows non-annotated audio-visual recordings of 100 hours reduced the error score of SELD from 36.4 pts to 34.9 pts.", "sections": [{"title": "I. INTRODUCTION", "content": "Sound event localization and detection (SELD) is a task that aims to estimate the activations, classes, and directions of arrival (DOAs) of sound events from multichannel audio recordings [1]. It is one of the foundations of computational intelligence for understanding acoustic environments. The current standard approach to this task is to train a deep neural network (DNN) in a supervised manner using pairs of audio recordings with ground-truth annotations [2]-[4]. In general, it is difficult to collect a sufficient amount of annotated audio data covering a wide range of acoustic environments.\nA popular solution to this problem is data augmentation [5], [6]. Specifically, one can synthesize multichannel audio data by convolving source signals of various classes with room impulse responses (RIRs) that simulate various acoustic environments and DOA conditions. This approach is known to effectively improve the performance of SELD for many common sound events (e.g., music and speech). However, SELD for some complex sound events (e.g., wildlife sound) remains an open problem because it is difficult to isolate and capture their individual sound source signals.\nAnother solution is to pretrain an audio feature extractor that constitutes the front end of a DNN used for SELD with audio-visual self-supervised learning (AV-SSL) (Fig. 1) [7]. This approach can make effective use of abundant virtual reality (VR) contents, each of which consists of multichannel audio data recorded by first-order ambisonics (FOA) microphones and 360\u00b0 equirectangular visual data recorded by an omni-directional camera. Considering the cross-modal co-occurrence between the sounds and appearances of the same objects, one can jointly train audio and visual encoders in a contrastive fashion such that the audio and visual embeddings are made close to each other if they correspond to the same DOA and recording (positive sample) and far apart otherwise (negative sample). The front end of the trained audio encoder is then used for initializing the audio feature extractor for SELD.\nAudio-visual spatial alignment (AVSA) [7] is one of the latest AV-SSL methods. It takes advantage of the FOA format, in which the single-channel audio signal with an arbitrary DOA can be computed from the observed FOA data. The audio embedding extracted from this enhanced signal are made close to the visual embedding extracted from the visual crop centered on the same DOA in the equirectangular visual data. However, the audio feature extractor trained in this way is insufficient for SELD because the DOA features of sound events cannot be extracted from the enhanced signal. On the other hand, the features useful for DOA estimation of sound events are not learnable with such non-DOA-aware contrastive learning equivalent in principle to AVC [8].\nIn this paper, we propose a DOA-aware extension of AVSA. Our method differs from the conventional AVSA [7] in that it jointly extracts audio embeddings over a DOA grid from raw FOA audio data without DOA-wise signal enhancement. This encourages the latent features of the audio encoder to represent both the classes and DOAs of sound events. In addition, this paper also tackles one of the remaining challenges in AVSA that the cross-modal co-occurrence between sound and appearance does not hold for visible silent objects and occluded sound objects. In general, meaningful sound events exist only in a small number of DOAs, which limits this local contrastive learning based on the DOA-wise similarity between audio and visual embeddings.\nTo mitigate this problem, we also propose global contrastive learning based on recording-wise audio-visual similarity obtained by averaging the DOA-wise similarities over all the DOAs. Our goal is to maximize the similarity when the audio and visual embeddings correspond to the same recording (positive) and minimize it otherwise (negative). To encourage the audio encoder to extract DOA information as the latent features, we introduce a data augmentation technique that randomly spatially rotates only the equirectangular visual data to generate negative samples from the same recording."}, {"title": "II. RELATED WORK", "content": "This section reviews existing SELD and AV-SSL methods."}, {"title": "A. Sound event localization and detection (SELD)", "content": "The modern approach to SELD is to train a DNN in a supervised manner. For instance, the activations of sound events are estimated with a DNN, while the DOAs are estimated geometrically [2]. Both the activations and DOAs can be estimated using a DNN [3]. Recently, an end-to-end approach to SELD has been proposed for directly estimating a DOA vector whose length corresponds to the duration of sound events in the Cartesian coordinate system [4].\nTo improve the robustness of SELD with a limited amount of annotated audio data, data augmentation tequniques such as SpecAugment [9] and Mixup [10] can be employed. Particularly for audio data provided in the FOA format, rotation-based augmentation is known to be effective [11]. Multichannel audio data with ground-truth annotations can be synthesized by convolving audio samples of various classes with arbitrary transfer functions [5], [12]. Alternatively, a general-purpose audio model [13] pretrained with a large dataset of audio signals called AudioSet [14] in an unsupervised manner can be used for sound event detection [15]."}, {"title": "B. Audio-visual self-supervised learning (AV-SSL)", "content": "At the heart of AV-SSL is contrastive learning, which aims to train audio and visual encoders based on the cross-modal co-occurrence between the sounds and appearances of objects. The information encoded in the audio and visual embeddings may vary depending on the design of positive and negative samples. AVC [8], for example, uses standard video recordings and considers a pair of audio and visual data from the same recording as a positive sample and a pair of those from different recordings as a negative pair. The audio and visual embeddings are thus encouraged to represent the features of sound event classes. AVSA [7] uses spatial video recordings, which are originally made for VR application that requires DOA-based audio-visual rendering. AVSA, however, is essentially identical to AVC because it can be regarded as AVC for non-spatial video standard recordings obtained by decomposing spatial video recordings in a DOA-wise manner.\nAudio-visual temporal synchronization (AVTS) [16] is another AVC-like self-supervised method. A key feature of AVTS is that it incorporates audio and visual data in the same recording, but at different moments in time, to negative pairs. The embeddings are thus encouraged to represent both the classes and activations of sound events."}, {"title": "III. PROPOSED METHOD", "content": "This section describes two variants of DOA-aware AV-SSL to improve SELD. The first variant employs DOA-wise contrastive learning, and the other employs recording-wise contrastive learning (Fig. 2). In both variants, an audio feature extractor \\( A \\) is used to transform raw FOA data into the latent audio features that represent sound event classes and DOAs. In this study, \\( K \\) discrete points on the Fibonacci lattice [17] are considered as potential DOAs. Each DOA \\( k \\in [1, K] \\) is defined by an azimuth angle \\( \\theta_k \\) and an elevation angle \\( \\Theta_k \\). A projection head \\( H \\) is then used to jointly convert the latent features to the audio embeddings over the DOA grid. A visual encoder \\( V \\), in contrast, is used to separately convert visual crops of a 360\u00b0 equirectangular visual data over the DOA grid to the visual embeddings. Let \\( I \\) be the total number of recordings.\nIn DOA-wise contrastive learning, we maximize the local similarity between DOA-wise audio and visual embeddings when they correspond to the same DOA (positive sample), and minimize it otherwise (negative sample). In recording-wise contrastive learning, in contrast, we maximize global similarity obtained by averaging the local similarities over the DOA grid when the audio and visual embeddings correspond to the same recording (positive), and minimize it otherwise (negative).\nOnce \\( A \\), \\( H \\), and \\( V \\) are trained jointly in a self-supervised manner, \\( A \\) is connected to another head \\( H' \\) for SELD and the entire network is fine-tuned in a supervised manner."}, {"title": "A. Audio encoding", "content": "Let \\( X^{(a)}_i \\in \\mathbb{R}^{(4+3) \\times F \\times T_a} \\) be the multichannel audio spectrogram of spatial recording \\( i \\in [1, I] \\) obtained by stacking the mel spectrograms of the four-channel audio signals of the FOA format and the three intensity spectrograms on the orthogonal axes, where \\( F \\) is the number of mel frequency bins, and \\( T_a \\) is the number of frames. A series of frame-wise audio latent features denoted by \\( Y^{(a)}_i = \\{y^{(a)}_{it}\\}_{t=1}^{T_a} \\) is obtained with the audio feature extractor \\( A \\) as follows:\n\\[Y^{(a)}_i = A(X^{(a)}_i).\\]"}, {"title": "B. Visual encoding", "content": "Let \\( X^{(v)}_i = \\{x^{(v)}_{ikt}\\}_{k=1,t=1}^{K,T_v} \\) be the series of DOA-wise local images cropped from the 360\u00b0 equirectangular visual data of recording \\( i \\in [1, I] \\), where \\( x^{(v)}_{ikt} \\in \\mathbb{R}^{H \\times W} \\) is a local image that corresponds to DOA \\( k \\in [1, K] \\) at time \\( t \\in [1, T] \\), \\( H \\) and \\( W \\) are the height and width of each image (cropping size), and \\( T \\) is the number of frames. Note that \\( x^{(v)}_{ikt} \\) is centered on DOA \\( k \\) on the Gnomonic projection [18] of the original spatial visual data. A series of DOA- and frame-wise visual embeddings \\( Z^{(v)}_i = \\{z^{(v)}_{ikt}\\}_{k=1,t=1}^{K,T_v} = \\{Z^{(v)}_{ik}\\}_{k=1}^{K} \\), is obtained with the visual encoder \\( V \\) as follows:\n\\[Z^{(v)}_{ik} = V(X^{(v)}_{ik}),\\]\nwhere the same visual encoding is independently applied to each DOA \\( k \\) to obtain the embeddings that represent the classes of visible sound objects."}, {"title": "C. Self-supervised learning (pretraining)", "content": "We describe the two variants of contrastive learning.\n1) Similarity measures: Since the audio and visual data of the same spatial recording usually have the DOA-wise correspondence, we define the similarity between recordings \\( i \\) and \\( j \\) for each DOA \\( k \\) as the cosine similarity as follows:\n\\[S_{DOA}(Z^{(a)}_{ik}, Z^{(v)}_{jk}) = \\frac{1}{T_v} \\sum_{t=1}^{T_v} \\frac{z^{(a)}_{ikt} \\cdot z^{(v)}_{jkt}}{\\|z^{(a)}_{ikt}\\| \\|z^{(v)}_{jkt}\\|},\\]"}, {"title": "D. Supervised learning (fine-tuning)", "content": "Using annotated data, the audio feature extractor \\( A \\) is fine-tuned for SELD based on activity-coupled Cartesian DOA representation (ACCDOA) [20]. Specifically, multiple sound events of the same class can be separately assigned to different tracks. Let \\( P \\in \\mathbb{R}^{T' \\times C \\times N \\times 3} \\) be a multi-ACCDOA vector of \\( T' \\) frames, \\( C \\) classes, \\( N \\) tracks given by\n\\[P = H'(A(X)),\\]\nwhere \\( X \\in \\mathbb{R}^{(4+3) \\times F \\times T} \\) is the multichannel spectrogram computed in the same way as \\( X^{(a)}_i \\), where \\( T_a \\) is the number of frames. The projection head \\( H' \\) consists of several fully-connected layers, followed by an adaptive average pooling to suit the target time resolution \\( T' \\). Since the SELD label for frame \\( t \\), class \\( c \\), and track \\( n \\) is given as a pair of the activity \\( a_{tcn} \\in \\{0, 1\\} \\) and the Cartesian DOA vector \\( R_{tcn} \\), the target ACCDOA vector is given by\n\\[P_{tcn} = a_{tcn} R_{tcn}.\\]\nThe overall network is trained to minimize:\n\\[\\underset{A,H'}{\\text{min}} L_{ACCDOA},\\]\n\\[L_{ACCDOA} = \\frac{1}{T',C} \\sum_{t,c=1} \\text{min}_{a \\in \\text{Perm}(t)} \\frac{1}{N} \\sum_{n=1} MSE(P_{a,tcn}, P_{tcn}),\\]\nwhere \\( a \\in \\text{Perm}(t) \\) is a possible frame-level permutation of \\( M \\) tracks at the frame \\( t \\) and \\( \\text{Perm}(t) \\) is a set of all possible permutations. \\( MSE(\\cdot,\\cdot) \\) is the mean square error function."}, {"title": "IV. EVALUATION", "content": "This section reports a comparative experiment conducted for evaluating the proposed AV-SSL methods with the two variants of contrastive learning."}, {"title": "A. Network configuration", "content": "The STFT spectrograms with a shifting interval of 480 samples and a window size of 1024 were converted to the mel spectrograms with \\( F = 64 \\) mel bins. The number of DOAS \\( K \\) was 220.\nWe used a ResNet-Conformer [12] for the audio feature extractor \\( A \\) as shown in Figs. 3 (a)-(c). The architecture of the projection head \\( H \\) consisted of two linear+SiLU layers followed a linear layer which output dimension was 220 \\( \\times \\) 128 as shown in Figs. 3 (d). The FOA audio data were resampled at a sampling rate of 24 kHz and split into 2-second clips in the training.\nThe visual encoder \\( V \\) consisted of 9-layer R(2+1)D convolution layers [21] as shown in Fig. 4. The input equirectangular visual data were extracted at the time resolution \\( T \\) of 16 (8 Hz). The field of view \\( \\theta \\) of visual crops for each direction was 40\\( ^{\\circ} \\), and the resolution \\( H \\times W \\) was 16 \\( \\times \\) 16. The horizontal and vertical flipping was applied to the visual crops as data augmentation."}, {"title": "B. Pretraining", "content": "We used the YT-360 dataset [7] for pretraining. It contains VR contents collected from YouTube, each of which consists of a synchronized pair of FOA audio data and equirectangular visual data, including 246 hours of 10-second-long recordings on diverse genres including music and sports. After removing recordings with some missing channels, 104 hours of the training data and 20 hours of the validation data were used.\nBy the proposed AV-SSL methods, the ResNet-Conformer was trained for 100 epochs using AdamW optimizer [22] with a batch size of 4, a learning rate of 10\\( ^{-4} \\), a weight decay of 10\\( ^{-5} \\). Rotated recordings for the negative sample were obtained by randomly rotating the equirectangular visual data around the z-axis. The temperature hyper-parameter \\( \\tau \\) was 0.1. The SpecAugment [9] and the dropout with a rate of 0.1 were used. The model having the lowest validation loss was used for the downstream SELD task.\nThe conventional AV-SSL method called AVC [8] was also evaluated as a baseline for comparison. Specifically, the loss \\( L_{AVC} \\) was calculated with the recording-wise similarity \\( H(\\left\\{z_{jk}^{(v)}\\right\\}_{k=1}^{K}, \\left\\{z_{ik}^{(a)}\\right\\}_{k=1}^{K}) \\), where \\( z_i^{(a)} \\in \\mathbb{R}^{128} \\) was obtained by max-pooling the DOA-wise visual embeddings \\( \\left\\{z_{jk}^{(v)}\\right\\}_{k=1}^{K} \\) and \\( z_i^{(a)} \\in \\mathbb{R}^{128} \\) was directly obtained from the output layer H with output dimension 128.\nAs in [7], curriculum learning was introduced for the proposed methods, where two audio feature extractors \\( A \\) were first trained with AVC for 50 epochs, then trained with the two different proposed AV-SSLs for 50 epochs."}, {"title": "C. Fine-tuning", "content": "The SELD performance was evaluated on the STARSS22 and Synth1 datasets [5]. The STARSS22 is a dataset of actual recorded FOA data annotated with the activations, classes, and DOAs of 13 sound events every 0.1 seconds, where the class labels follow the Audioset ontology [14]. This dataset consists of 2.9 hours of training data and 2 hours of validation data. Note that some classes have very low frame coverages in STARSS22 as shown in Table I. The Synth1 is a dataset of synthesized FOAs annotated in the same way as STARSS22, obtained by remixing source signals from FSD50K [23] with room RIRs from TAU-SRIR database [24]. Two datasets of different sizes constructed from these datasets were used for fine-tuning. The first one was a dataset made up of all training data from STARSS22 and Synth1 (STARSS22+Synth1). The second one consisted only of all training data from STARSS22 (STARSS22). A dataset that consists of all validation data from STARSS22 and Synth1 was used as the validation dataset in both conditions. The original FOA data were split every 5 seconds, and they were sampled uniformly over all 13 classes to deal with the class imbalance in STARSS22.\nEach pretrained model was fine-tuned with a head H' to output 3-track multi-ACCDOAs [20]. The architecture of H' was the same as the head H shown in the Sec. IV-A except that the output dimension was changed to 13 \\( \\times \\) 3 \\( \\times \\) 3. AdamW with a learning rate of 10\\( ^{-4} \\) and a weight decay of 10\\( ^{-5} \\) was used for training. The models were trained for 1000 epochs. For data augmentation, the input FOA was randomly rotated. The dropout rate was 0.1 for ResNet-Conformer and 0.05 for H'."}, {"title": "D. Experimental results", "content": "Table II shows the overall SELD performances in terms of the five metrics, i.e., \\( ER_{<20^{\\circ}} \\), \\( F_{<20^{\\circ}} \\), \\( LE \\), \\( LR \\), and SELD score for each dataset and each pretraining method. In the STARSS22+Synth1 dataset, both the AV-SSL with DOA-wise contrastive learning and the AV-SSL with recording-wise contrastive learning improved the SELD scores by about 1.5 pts, while the conventional AVC improved only about 0.5 pts. This result indicates that our proposed approach is more suitable for the pretraining of SELD than AVC.\nTable III shows the performance gaps in the class-wise metrics, \\( F_{c,<20^{\\circ}} \\), \\( LE \\) and \\( LR \\), between the non-pretrained model and each pretrained model, where all the metrics were transformed to the range of [0, 1] for readability, and only some of the classes required for the following discussion were picked up due to the page limitation. The proposed AV-SSLs degraded the SELD scores more than AVC when only a few hours of imbalanced data were used for training (STARSS22). This would be partly due to the domain mismatch between YT-360 and STARSS22 and the class imbalance of STARSS22. As for the knock class, which was not included in the pretraining dataset (YT-360), for example, the detection performance \\( LR_c \\) got particularly worse. While in the classes related to home sounds (e.g., domestic sounds and door), which frequently appear in YT-360, the detection performance was improved.\nAnother reason for the degradation would be that a large amount of background music data independent from the visual data are included in YT-360. Background music was considered to have a negative impact on the localization of sound events related to music because it did not spatially correspond to the paired visual data. In fact, the localization performance \\( LE \\) was significantly degraded by the pretraining for the musical instruments and bell classes."}, {"title": "V. CONCLUSION", "content": "We proposed two variants of pretraining an audio feature extractor useful for SELD using spatial audio-visual recordings. To obtain the latent audio features representing not only the classes but also the DOAs of sound events, the audio encoder takes the FOA data as input, and outputs the audio embeddings over the DOA grid. The transfer learning with a sufficient amount of data showed the effectiveness of the proposed AV-SSLs as pretraining for SELD. For future work, one should deal with the deterioration in the SELD performance when sufficient labeled data is unavailable. One of the promising approaches would be to prepare the dataset of spatial audio-visual recordings covering various domains with good correspondence between audio and visual data."}]}