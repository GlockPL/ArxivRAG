{"title": "Getting in the Door: Streamlining Intake\nin Civil Legal Services with Large\nLanguage Models", "authors": ["Quinten STEENHUIS", "Hannes WESTERMANN"], "abstract": "Legal intake, the process of finding out if an applicant is eligible for help\nfrom a free legal aid program, takes significant time and resources. In part this is\nbecause eligibility criteria are nuanced, open-textured, and require frequent revi-\nsion as grants start and end. In this paper, we investigate the use of large language\nmodels (LLMs) to reduce this burden. We describe a digital intake platform that\ncombines logical rules with LLMs to offer eligibility recommendations, and we\nevaluate the ability of 8 different LLMs to perform this task. We find promising\nresults for this approach to help close the access to justice gap, with the best model\nreaching an F1 score of .82, while minimizing false negatives.", "sections": [{"title": "1. Introduction", "content": "Around the world, accessing meaningful help for civil legal problems means hitting\nclosed door after closed door. In the U.S., the first door-reaching a nonprofit legal aid\nlawyer-usually only opens after a poor person waits in a telephone intake queue for\nhours.\nThe intake screening challenge is an important piece of the \"access to justice\" gap,\nwhich affects up to 92% of the poor worldwide.[28,52,14,34,47], Unresolved legal is-\nsues, like housing, custody, guardianship and domestic violence claims, can harm indi-\nviduals and burden society. [39,34,52,18] Legal aid programs, an important part of the\nsolution, have funding to offer assistance only to a limited number of even the eligible\napplicants. [40,22,5]\nTo be eligible for legal help, individuals needs to comply with certain formal re-\nquirements, such as household income and citizenship requirements. The individual also\nneeds to comply with a set of substantive requirements, relating to the specific legal prob-\nlem they face. Because of high demand, an applicant who calls for intake screening may\nwait on hold for several hours."}, {"title": "1.1. Research Questions", "content": "In order to understand how LLMs can help with intake of legal aid applicants, we will\ninvestigate the following research questions:\n\u2022 RQ1 - How accurately can large language models apply intake rules to legal sce-\nnarios described by laypersons?\n\u2022 RQ2 - What types of errors do LLMs make, and how do they compare to human\nintake staff?\n\u2022 RQ3 - Can LLMs elicit missing information through high-quality follow-up inter-\nactions?"}, {"title": "2. Prior Work", "content": "This work builds on a number of important prior contributions."}, {"title": "2.1. Al for access to justice", "content": "Using AI for access to justice is a growing field with applications in legal information[8,\n31,49], form-filling[43,48], and dispute resolution [16,38,42,47,41,45,33,3,7,51,54,29].\nPrior work in the area of eligibility determinations includes the GetAid system [53],\nwhich uses decision trees and Toulmin argument structures to help lawyers determining\nlegal aid eligibility. Atkinson et. al. [1] apply formal methods to make eligibility recom-\nmendations for asylum claims in the European Court of Human Rights. Here, we inves-\ntigate whether LLMs can determine eligibility upon reading verbatim intake guidelines,\nwhich would be easier and faster to deploy than these models."}, {"title": "2.2. LLMs for Legal Reasoning", "content": "Recent years have shown a wide range of legal reasoning capabilities for LLMs, includ-\ning passing the bar exam[25], drafting contracts [26,46], decision making[15,12,21], an-\nnotating legal documents [36,35], explaining legal concepts [37], performing statutory\nreasoning [4,30] and providing legal information [44,13]. The LegalBench study mea-\nsures 20 models on 161 different legal tasks [20]. [32] discusses the LLMs ability to per-\nform legal reasoning. Here, we investigate whether they can apply complex legal intake\nrules (provided as text) to factual scenarios to determine legal intake eligibility."}, {"title": "2.3. Spotting legal issues and eliciting facts", "content": "Previous work has used machine learning to spot legal issues and direct users to deci-\nsion support tools. [10,50] Our approach differs by relying on zero-shot LLMs to assess\neligibility based on textual rules.\nSometimes, an initial user scenario description may not be sufficient to determine\nrule applicability. A number of projects have explored the use of AI and LLMs to elicit\nfacts from users. [6] uses schemas induced from cases to identify key facts for elicitation.\n[48] uses LLMs and logical rules to perform step-by-step assessment of whether relevant\nfacts are present in user descriptions. [19] use LLMs to ask follow-up questions in legal\naid advice scenarios in order to maximize the understanding of the clients intention and\nsituation. Here, we use LLMs to elicit information relevant to determine the eligibility of\nintake rules, and evaluate the capability with intake workers."}, {"title": "3. Proposed framework", "content": "We used the free and open source Docassemble framework to build the user-facing hous-\ning intake application. [24,27,41] The intake application can be accessed on a tenant's\nmobile phone and is both embedded in a legal help website (MOTenantHelp.org) and\nreferred to in the on-hold message for callers to the phone intake system.\nA typical interaction with the application is available on a smart phone, takes about\n5 minutes and asks questions on 5 separate screens."}, {"title": "3.1. Formal eligibility criteria", "content": "We used formal rules, encoded in Python, to determine eligibility based on location and\nstatutory requirements of 42 U.S.C. 2996g(e)."}, {"title": "3.2. Program-specific substantive criteria", "content": "Program-specific rules were stored as plain text inside the Docassemble application for\nlater retrieval. The rules, which we left unmodified, showed diversity in length, format-\nting, and substance. For example: one program uniquely accepted security deposit dis-\nputes, and two others prioritized discrimination cases. Housing subsidies (income-based\ngovernment support), a common priority, were described using varying key words across\nthe programs. A fourth program, which we did not study, accepted \"all landlord tenant\ndisputes.\"\nAlthough the platform handles all four programs, we focused on three in our study:"}, {"title": "3.3. Screening with LLM assistance", "content": "We ask the applicant to describe their legal problem in their own words, without any\npersonally identifiable information. The full set of rules and the problem description\nare then provided in a prompt to GPT-4-turbo with the temperature set to 0. The LLM\nis instructed to respond with one of three statuses: qualifies if the applicant appears to\nmeet the criteria, does not qualify if they clearly do not, or a follow-up question if more\ninformation is needed. Up to 10 follow-up questions are asked.\nFinally, the applicant is told the LLM's determination of their eligibility. We add\na disclaimer that the AI tool can get the decision wrong, and we provide a link to the\nprogram's website and phone number to complete a full intake."}, {"title": "4. Experimental design", "content": "To test the LLM-based intake process, we developed two datasets, D1 and D2."}, {"title": "4.1. Dataset", "content": "To assess how well LLMs apply intake rules (RQ1) and identify error types (RQ2), we\ndeveloped a dataset of 16 scenarios with correct initial responses from the LLM. Using"}, {"title": "4.1.1. D1 - Initial Response Dataset", "content": "D1 only captures the initial response by the model. In order to get an understanding of\nwhether the model is able to accurately elicit information from users (RQ3) and obtain a\nbetter qualitative understanding of the quality of the interactions, we manually generated\n11 additional diverse scenarios and simulated a tenant giving out just small bits of infor-\nmation at a time to better capture how well the LLM did at generating helpful follow-up\nquestions. We generated transcripts for these 11 scenarios."}, {"title": "4.1.2. D2 - Follow-up Response Dataset", "content": "We tested 8 well-known large language models, including 2 open source models\n(Llama 3.1 70b Instruct & 405B Instruct) and 6 commercial models from An-\nthropic (claude-3-5-sonnet-20240620), Google (Gemini 1.5 Pro), and OpenAI\n(gpt-40-2024-08-06, gpt-4-0613, gpt-4-turbo-2024-04-09, gpt-40-mini-2024-07-18).\nWe chose these models to represent a wide variety of popular commercial models at\ndifferent price points and open-source models, which could be important for legal aid\nproviders due to confidentiality concerns."}, {"title": "4.2. Models", "content": "The eligibility screening prompt was designed to save time for tenants and legal aid\nstaff, with the main goal of avoiding false negatives (i.e., ensuring qualified applicants\naren't mistakenly rejected). It was developed iteratively through tests in both the Chat-\nGPT interface and embedded in the Docassemble application. To prevent overfitting, the\niteration phase did not include any scenarios from dataset D1.\nInitially, the model would give inappropriate advice (e.g., suggesting the tenant work\nthings out with their landlord), which we addressed by clarifying that its task was to\ndetermine whether the user met the minimum intake criteria, not provide legal advice.\nWe also found that giving examples led to hallucinations, so the prompt omits example\nreplies. The full code and prompt are available on GitHub in two repositories."}, {"title": "4.3. Prompt design", "content": "We evaluated LLMs' ability to apply intake rules through 3 experiments."}, {"title": "4.4. Experiments", "content": "To examine how well LLMs are able to apply the intake rules to new scenarios (RQ1),\nwe set up a pipeline that would assemble a prompt based on our developed instructions\n(see section 4.3) as \"system message\", together with the intake rules from one of the\njurisdictions (see section 3.2) and one of the scenarios (D1, see section 4.1.1) as \"user\nmessage.\" This pipeline was then run for all of the scenario-jurisdiction pairs (48), for\neach of the selected models (8), yielding a total of 384 results. For each of the results, we\ncaptured the prediction (i.e. accept, deny, question) as well as the narrative explanation\nprovided by the LLM. The dataset and code to reproduce this experiment is available\nonline."}, {"title": "4.4.1. El - Prediction of the correct initial response", "content": "In order to understand the types of errors (RQ2), we manually analyzed the instances\nwhere our best-performing model (in our case, GPT-4-turbo) had a prediction that dis-\nagreed with our manually assigned results. We investigated both the type of error and the\nLLM-provided explanation."}, {"title": "4.4.2. E2 - Analysis of LLM errors", "content": "To understand the quality of overall conversations (RQ3), we had one expert rater em-\nployed by Legal Services of Eastern Missouri review the 11 transcripts (D2) of manu-\nally created conversations with the Docassemble application that used GPT-4-turbo. We\nasked the rater to assess the transcripts' accuracy and overall quality. The rater was also\nasked to provide comments on each transcript."}, {"title": "4.4.3. E3 - Qualitative Analysis of full transcripts", "content": "Table 3 shows the prediction performance for all models on the scenario-jurisdiction\npairs, for the possible answers (Accept, Deny, Question) and the Overall average\n(weighted average, by proportion of class). Overall, models have a weighted F1-score of\nbetween 0.56 and 0.82, with the best model being GPT-4-Turbo. We can further see that\nprecision for the \"Deny\" class is very high, which we will come back to in the discussion.\nTable 4 shows the performance of the best-performing model (GPT-4-Turbo) on the\nvarious jurisdiction intake rules. We can see that the model struggles the most with the\nMid-Missouri intake rules, while the Eastern Missouri rules seem the easiest to apply,\nwith a weighted average F1-score of 0.94."}, {"title": "5. Results", "content": "We analyzed the errors made by the model, both quantitatively and qualitatively. Figure\n2a shows a confusion matrix for all models, while figure 2b shows the confusion matrix\nfor GPT-4-Turbo, our best-performing model. We can see that the most common error\nis for the model to predict a situation that we classified as \"Deny\" as \"Question\". This\noccured 76 times for all models, and 4 times for GPT-4-turbo. It is very rare for a model\nto misclassify an accept or question scenario as deny.\nWe further analyzed these LLM errors and found 2 cases where the LLM revealed\nannotation mistakes, 1 case of ambiguous rules, and 6 actual LLM errors. We examine\nthese results in-depth in 6.2."}, {"title": "5.1. El - Prediction of correct initial response", "content": "In 73% of cases, the expert rater marked the overall result as correct. In 63% of cases,\nfollow-up questions were missed that would have improved the intake. However, the\nhuman rater gave perfect scores (5 out of 5) for understandability and overall satisfac-\ntion with the tool. The human rater identified specific domain knowledge gaps, such as\nrecognizing the severity of conditions like flimsy doors or locks."}, {"title": "5.2. E2 - Error Analysis", "content": "Details\nI have a support animal, but my landlord says I cannot have it and wrote me an angry\nletter threatening to throw me out.\nWhat city do you live in? Also, has your landlord filed an eviction against you in court?\nDeny\nThis was a human error. The LLM correctly caught that in Kansas City, tenants at risk\nof eviction, even if pre-court, qualify for assistance."}, {"title": "5.3. E3 - Qualitative analysis", "content": "ranking between the jurisdictions is: Mid-Missouri (Avg F1-Score 0.69), Western Mis-\nsouri (0.84) and Eastern Missouri (0.94). The form of the intake rules may contribute\nhere - Eastern Missouri has the longest and most detailed rules, while Mid-Missouri\nfeatures a long list of cases that are covered, which may have confused the model.\nOverall, we believe that the results are strong, showing that LLMs are able to identify\nrelevant intake rules from a set of many complex rules and apply these rules to often\nambiguous scenarios with high performance, in a zero-shot setting. Next, let us take a\nlook at the types of errors made by the LLMs."}, {"title": "6. Discussion", "content": "We investigated whether large language models can correctly determine eligibility of sce-\nnarios under complex, jurisdiction-specific intake rules. Let us explore what the results\ncan tell us regarding our research questions."}, {"title": "6.1. RQ1 - How accurately can large language models apply intake rules to legal\nscenarios described by laypersons?", "content": "In E1, we compared the performance of 8 different LLMs on the task of deciding whether\na scenario is eligible, not eligible, or more information is required to make this assess-\nment. A number of interesting points are revealed by this analysis.\nOn a per-model basis, GPT-4-turbo performs the best, with an F1-score of .13 over\nthe next best option. This is somewhat surprising, as we expected that more recent mod-\nels such as GPT-40 and Claude 3.5 sonnet would perform better, given that they are\nhigher on e.g. general chat leaderboards. We believe the performance discrepancy can\nbe explained by the fact that we used GPT-4-Turbo to refine the prompt. This shows the\nimportance of refining the prompt for specific models.\nWith regards to the jurisdictions, we see that there is a large performance difference\nbetween the intake rules of the various jurisdictions (Table 4). The overall \"difficulty\""}, {"title": "6.2. RQ2 - What types of errors do LLMs make, and how do they compare to human\nintake staff?", "content": "To better understand the results of the analysis, we quantitatively and qualitatively ex-\nplored the type of errors made by the model.\nThe most common error was unnecessary follow-up questions (83 cases), while in-\ncorrect denials (3) and incorrect acceptances (49) were less frequent (see Figure 2a). This\nindicates that even the worst-performing models could be seen as safe for screening, as\nthey prefer clarifying details or accepting cases, rather than rejecting cases.\nWe paid close attention to the errors made by our best performing model, GPT-4-\nturbo. In two cases, the LLM helped us spot an error in our own reasoning, namely the\nfact that for one city in Western Missouri, \u201crisk of eviction\" is sufficient for acceptance.\nOur human error appears to have artificially lowered the performance of the GPT-4-turbo\nmodel and shows the potential for large language models to help even experts in the do-\nmain to more closely read and understand similar rules (c.f. [23]). In one case, we rated\nthe answer as ambiguous - the rules themselves did not contain enough information to\nascertain that risk of eviction does not encompass post-eviction cases. In 5 cases, the\nLLM made an understanding mistake, for example failing to grasp that \"facing eviction\"\nrequires some imminence, failing to spot a requirement that security deposit claims re-\nquire a value of over $300 USD to qualify in Mid-Missouri, failing to make the link\nbetween having a support animal and disability discrimination, or failing to catch hints\n(\"I need help fighting this in court\") that a trial date may have been set. Overall, these\nerrors are relatively rare, and may be addressed in the future by further clarifications and\nstructuring of the rules. Further, the models always seem to err on the side of caution."}, {"title": "6.3. RQ3 - Can LLMs effectively elicit missing information through follow-up\nquestions?", "content": "The expert attorney's review of the dataset in D2 is encouraging. In many cases, the find-\ning was that additional questions could have been asked by the AI model, but overall,"}, {"title": "6.4. Limitations", "content": "While our results are promising, we recognize that they may also have limitations.\n1. Content censorship may limit applicability to other legal topics, including\ntopics involving violence, abuse, or unethical behavior. Google Gemini censored\na scenario that stated \u201cI'm a DV survivor, and my landlord won't renew my lease\nbecause they called the cops on my boyfriend.\"\n2. Our prompt was iteratively refined on GPT-4-turbo, but adjusting our prompt\ncould improve performance on other generally high performing models.[9]\n3. Biased LLM training data may expose applicants to risks. It is known that\nLLMs can reproduce biased outputs, which may be a special concern with the\nvulnerable legal aid population.[2,11,17] We mitigate this risk by keeping a hu-\nman in the loop, focusing the LLM on only the minimum qualification criteria,\nas well as prompting the LLM to explain the basis for its recommendation."}, {"title": "7. Conclusion and future work", "content": "Our results show that large language models can open doors in the legal intake process,\nreducing time spent by both applicants and staff. The errors observed were minor, similar\nto those a human intake worker might make, and sometimes even helped us catch our\nown mistakes. A qualitative review showed the models asked thoughtful questions and\nmade sound recommendations, while preferring clarification or accepting the scenarios\nin case of doubt, a highly desirable property for pre-screening tools.\nThough our approach is promising, legal intake is a rich area for further improve-\nments. Next steps include integrating the tool with a seamless online intake experience,\nimproving user analytics, and simplifying rule updates by allowing staff to upload doc-\numents directly. We also see potential in using semi-structured reasoning (c.f. [48]) and\nrefining prompts and intake rules to improve performance. Additionally, we plan to eval-\nuate human intake staff performance to better understand how LLMs compare and to\nfurther explore potential biases in the LLM's recommendations. Finally, we see poten-\ntial for this system to help with best match eligibility recommendations across multiple\nproviders rather than single-provider eligibility, further expanding access to the appro-\npriate help.\nBy improving eligibility information and reducing barriers, LLMs can help more\npeople find the legal assistance they need, representing a crucial step toward a more\naccessible and efficient legal system."}]}