{"title": "Demographic User Modeling for Social Robotics with Multimodal Pre-trained Models", "authors": ["Hamed Rahimi", "Mouad Abrini", "Mahdi Khoramshahi", "Mohamed Chetouani"], "abstract": "This paper investigates the performance of multimodal pre-trained models in user profiling tasks based on visual-linguistic demographic data. These models are critical for adapting to the needs and preferences of human users in social robotics, thereby providing personalized responses and enhancing interaction quality. First, we introduce two datasets specifically curated to represent demographic characteristics derived from user facial images. Next, we evaluate the performance of a prominent contrastive multimodal pre-trained model, CLIP, on these datasets, both in its out-of-the-box state and after fine-tuning. Initial results indicate that CLIP performs suboptimal in matching images to demographic descriptions without fine-tuning. Although fine-tuning significantly enhances its predictive capacity, the model continues to exhibit limitations in effectively generalizing subtle demographic nuances. To address this, we propose adopting a masked image modeling strategy to improve generalization and better capture subtle demographic attributes. This approach offers a pathway for enhancing demographic sensitivity in multimodal user modeling tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Developing intuitive and adaptive social robotics requires AI systems that effectively perceive individual needs, behaviors, and preferences [1]. These systems can dynamically tailor their responses to enhance user engagement and satisfaction by leveraging visual-linguistic features such as demographic information [2]. This perception-driven approach not only improves interaction quality and personalization but also addresses privacy concerns by focusing on context-sensitive observations. This is particularly important in delicate applications such as healthcare [3], [4] and education [5], where comprehending user behavior is key to ensuring safety and enhancing user experience. Analyzing user profiles, preferences, and behaviors is an integral component of the broader domain of user modeling tasks [6]. This task includes methodologies that capture and represent user features and personal characteristics to create accurate user profiles. The ultimate goal of user modeling is to adapt interactions to meet users' cognitive and physical needs. Traditionally, user modeling has relied on collecting and analyzing sensitive user information, raising significant privacy concerns [7]. Early approaches primarily focused on constructing personalized user profiles through historical data and expert-validated behavioral rules [8]. These methods often required extensive manual input, which posed challenges in terms of scalability and adaptability to dynamic user behaviors. Recent advances have shifted towards statistical and machine learning-based approaches to overcome these limitations [9]. While these approaches improved the handling of diverse real-world data, they struggled with adapting to new user types and unrepresented behaviors. Applications of these methods in social robotics further demonstrated the potential of neural architectures to enable memory-dependent tasks [10], although issues of scalability, flexibility, and parallelization persisted.\nMultimodal pre-training research [11] has demonstrated significant success across a spectrum of downstream tasks in recent years. This success has sparked remarkable interest in their potential to empower the next generation of personalized AI agents [12] that can learn individual needs, behaviors, and preferences. More recently, pre-trained models such as UserBERT [13] have gained prominence for their ability to capture interaction patterns without extensive private data storage. Building on this, innovations like User-LLM [14] have integrated user modeling with generative language models, offering a promising direction for scalable, flexible, and accurate personalized systems that also prioritize user privacy. However, these advancements are predominantly monomodal and often language-oriented, leaving the integration of multimodal data critical for holistic user modeling an open research challenge. Multimodal pre-trained models such as CLIP [15] offer a promising foundation for extending this research to visual-linguistic user modeling, enabling richer and more comprehensive interaction paradigms.\nHowever, the sparse and sensitive nature of visual-linguistic demographic data, including attributes such as gender, age, and ethnicity, poses significant challenges for multimodal pre-trained models like CLIP, which rely on contrastive training, to perform user modeling and profile representation tasks. To address these limitations, we propose integrating contrastive learning with masked image modeling (MIM), as shown in Figure 1, where multimodal pre-trained models are encouraged to predict occluded regions in images during training. This approach could enhance the model's ability to capture nuanced demographic patterns while mitigating biases.\nThis paper contributes to the literature on multimodal user modeling by (1) introducing two novel datasets designed to capture diverse visual-linguistic demographic traits for user profiling; (2) evaluating the performance of state-of-the-art multimodal models both in their out-of-the-box configuration and after fine-tuning, on these datasets; and (3) present an approach to improve generalization by leveraging masked image modeling techniques, which enhances the model's ability to capture subtle demographic attributes effectively."}, {"title": "II. BACKGROUND", "content": "Pre-trained models have been widely explored for user modeling, with notable advancements such as Pre-training User Model (PTUM) [16] and UserBERT [13]. Both models leverage large-scale sequential user behavior to represent user models in a high-dimensional space. Despite the significant progress in pre-trained behavior-based user models, there has been limited exploration of multimodal pre-trained models specifically designed for demographic user modeling based on extracting facial features. However, recent advancements in multimodal contrastive pre-trained models offer new potential for user modeling by integrating diverse data types, such as visual and textual cues, enabling more nuanced analysis of facial features alongside behavioral data [17].\nMultimodal contrastive pre-trained models such as CLIP [15] are designed to learn joint representations from various data modalities, primarily visual and textual information. They achieve this by ensuring that similar instances are represented closely in the space while dissimilar instances are kept far apart [18]. As shown in Figure 1(a), this process operates on a dataset $\\mathcal{D} = \\{(I_i, T_i)\\}_{i=1}^{N}$ comprising $N$ paired instances of images and text, where each image $I_i \\in \\mathbb{R}^{d_I \\times N}$ and text $T_i \\in \\mathbb{R}^{d_T \\times M}$ are represented as sequences of tokens. The indices $N$ and $M$ denote the sequential lengths of image and text tokens, respectively, while $d_I$ and $d_T$ represent their corresponding feature dimensions. The framework employs two primary encoder functions: an image encoder $E_I : \\mathbb{R}^{d_I \\times N} \\rightarrow \\mathbb{R}^{d_h \\times N}$ and a text encoder $E_T : \\mathbb{R}^{d_T \\times M} \\rightarrow \\mathbb{R}^{d_h \\times M}$, where $d_h$ denotes the hidden dimension. These Transformer-based encoders process their respective modalities to produce sequences of feature vectors $E_I(I) = \\{f_1^I, f_2^I, ..., f_N^I\\}$ and $E_T(T) = \\{f_1^T, f_2^T, ..., f_M^T\\}$. Special tokens, namely cls for images and eos for text, are incorporated into these sequences to capture global representations. A projection head $P : \\mathbb{R}^{d_h} \\rightarrow \\mathbb{R}^{d_e}$, implemented as a multilayer perceptron, maps the features associated with these special tokens to a shared embedding space, yielding $e_I = P(f_{cls}^I)$ for images and $e_T = P(f_{eos}^T)$ for text. The similarity between image and text embeddings is computed using a similarity function $s_{i,j} = \\text{sim}(e_i, e_j)$, typically implemented as cosine similarity. The learning objective is formalized through a bidirectional contrastive loss function $\\mathcal{L}_c$, defined as:\n$\\mathcal{L}_c = \\frac{1}{2N} \\sum_{i=1}^{N} (-\\log p(f_{\\text{img}}(I_i) | f_{\\text{txt}}(T_i)) - \\log p(f_{\\text{txt}}(T_i) | f_{\\text{img}}(I_i)))$ (1)\nwhere the matching probability for any image-text pair is computed using a softmax function with temperature parameter $\\tau$:\n$p(f_{\\text{txt}}(T_i) | f_{\\text{img}}(I_i)) = \\frac{\\exp(\\frac{s_{i,j}}{\\tau})}{\\sum_{k=1}^{N} \\exp(\\frac{s_{i,k}}{\\tau})}$ (2)\n$p(f_{\\text{img}}(I_i) | f_{\\text{txt}}(T_i)) = \\frac{\\exp(\\frac{s_{i,j}}{\\tau})}{\\sum_{k=1}^{N} \\exp(\\frac{s_{k,i}}{\\tau})}$ (3)\nThis formulation ensures bidirectional alignment between modalities by simultaneously optimizing the probability of matching images to their corresponding texts and vice versa. The temperature parameter $\\tau$ modulates the distribution of similarities during training. Through minimization of this contrastive loss, the framework learns to embed both visual and textual content in a shared semantic space where matched pairs are proximal while unmatched pairs are distant.\nRecent studies by AdaptSSR [19] and FairLISA [20] have highlighted inherent limitations of these models in user modeling with sensitive demographic data such as gender, which underscore gaps in fairness and adaptability within such models. AdaptSSR [19] addresses data sparsity issues by introducing Augmentation-Adaptive Self-Supervised Ranking, while FairLISA [20] tackles bias arising from sensitive attributes using an adversarial framework. In this paper, we build on a general facial Representation model (FaRL [21]) that is trained on LAION-FACE (a subset of the LAION dataset [22]) using both visual and textual modalities to develop robust multimodal facial representations. We apply this technique to our problem and investigate the impact of integrating the masked image modeling (MIM) objective with a multimodal contrastive pre-trained model for demographic user modeling tasks."}, {"title": "III. METHODS", "content": "We introduce two datasets designed to capture diverse visual-linguistic demographic traits for user profiling.\n1) GenUser: It includes 10K synthetic image-text pairs, featuring human faces alongside user profile information from diverse demographic backgrounds. The dataset is generated by \"generated.photos\" platform to ensure privacy and avoid using real personal data. To promote fairness, as shown in Figure 2, the entries are intentionally designed to represent a broad range of demographic groups, capturing diversity across key characteristics such as age, gender, and ethnicity. Each entry is accompanied by a JSON file integrating over 10 visual attributes that support a wide range of information about user profiles. As shown in Figure 3, these features, alongside the images, are processed using a VLM (\u201cGPT-4o\u201d) to generate a one-paragraph user profile, providing a concise yet detailed description based on the inferred demographic and emotional attributes. The 10K entries in the dataset are split into three parts: 1K for validation, 1K for testing, and 8K for training, ensuring a balanced distribution across the dataset for training and model evaluation.\n2) FairUser: It consists of 100K real-world text-image pairs derived from the FairFace dataset [23]. The dataset entries are carefully curated to ensure balance, diversity, and accurate labeling across race, gender, and age categories. Based on this dataset, we designed a user profile feature using the following template: \"The person appears to be race class gender class, approximately age class years old\". This template facilitates a structured and interpretable representation of demographic attributes for profiling tasks. The 100K entries in the dataset are split into three parts: 10K for validation, 10K for testing, and 80K for training, ensuring a balanced distribution across the dataset for training and model evaluation."}, {"title": "B. Demographic User Modeling", "content": "As illustrated in Figure 1, let $\\mathcal{D} = \\{(I_i, T_i)\\}_{i=1}^{N}$ be the training dataset of facial images and corresponding demographic descriptions. The proposed model employs two key components similar to FaRL [21]: an image-text encoder with contrastive loss, and a masked image modeling task with cross-entropy loss. The contrastive loss is as explained in Section II, while for masked image modeling we mask some image patches in the input and predict the visual tokens corresponding to the masked patches using the same image encoder.\nFormally, let $\\tilde{I}$ be the masked image, where some image patches are randomly masked. We extract the features from the image encoder, denoted as $\\{f_1^I, f_2^I, ..., f_N^I\\} = E_I(\\tilde{I})$. These features are then fed into a small Transformer encoder, which outputs the final hidden vectors, represented as $\\{h_1, h_2, ..., h_N\\} = E_{\\text{MIM}}(f_1^I, f_2^I, ..., f_N^I)$. The objective is to predict the masked regions using the corresponding hidden vectors $\\{h_k: k \\in M\\}$. Instead of directly predicting pixel values, which would require substantial memory consumption, a discrete variational autoencoder is used to first encode each image patch to one of $V$ possible values, where $V$ is the vocabulary of the autoencoder. A classification layer is then attached to the hidden vector $h_k$ to predict the index of the corresponding masked patch among $\\{1, ..., |V|\\}$. The masked image modeling loss, denoted as $\\mathcal{L}_{\\text{MIM}}$, can be expressed mathematically as:\n$\\mathcal{L}_{\\text{MIM}} = \\sum_{k \\in M} \\log p(q_\\phi(I) | \\tilde{I}),$ (4)\nwhere the conditional probability $p(q_\\phi(I) | \\tilde{I})$ represents the model's ability to predict the tokenized masked patch $q_\\phi(I)$ based on the corrupted input image $\\tilde{I}$. By incorporating this loss term, the model enhances its capacity to extract fine-grained facial characteristics while maintaining the broader semantic understanding achieved through contrastive learning. Therefore, the final loss function can be formulated as:\n$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_c + \\mathcal{L}_{\\text{MIM}},$ (5)\ncombining the contrastive loss $\\mathcal{L}_c$ in Equation (1) with the masked image modeling loss $\\mathcal{L}_{\\text{MIM}}$ in Equation (4)."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "Our baseline multimodal contrastive pre-trained model is CLIP (\"CLIP-ViT-Base-Patch16\"), known for its robust image-text alignment capability, achieved through a contrastive loss. For our proposed model, we built on FaRL (\"FaRL-Base-Patch16-ep64\") as it combines multimodal contrastive pre-training with a self-supervised learning component to enhance feature extraction from sparse or unstructured data. We compare the performance of these models both out-of-the-box and after fine-tuning to assess their inherent abilities and adaptability for user modeling tasks."}, {"title": "B. Finetuning Strategy", "content": "The models are fine-tuned using a contrastive fine-tuning strategy called FLYP [24]. This approach aligns the fine-tuning objective with pre-training by casting downstream class labels as text prompts and optimizing the contrastive loss between image embeddings and class-descriptive prompts. This method allows our models-CLIP and FaRL-to retain consistency with their original pre-training objectives. This strategy has shown substantial gains in few-shot learning, distribution shifts, and transfer learning benchmarks [24], making it suitable for our user profile modeling task. We set a training schedule of 10 epochs, with continuous validation to monitor performance. A stopping criterion is implemented, allowing training to halt early if validation performance does not improve for three consecutive evaluations. This early stopping threshold of three epochs helps to retain the most generalizable model parameters. The optimizer used in this study is AdamW, a variant of the Adam optimizer that improves weight decay implementation by decoupling it from the gradient update. The learning rate is set to $10^{-5}$. We run the models five times with different seeds on the datasets, and after fine-tuning and cross-validation, we evaluate their performance on the held-out test set to ensure exposure to previously unseen data for an unbiased assessment."}, {"title": "C. Metrics", "content": "To assess the performance of these models in capturing visual attributes of user profiles, we employed four key metrics within a user retrieval task. Specifically, for each text query $T$ in the test set, the model computes similarity scores with all images in the test set and identifies the image with the highest similarity. Accuracy is then determined by checking if this retrieved image matches the ground truth image paired with the query $T$.\n\\begin{itemize}\n    \\item Accuracy: Evaluates the overall success rate of retrieving the exact correct images based on text and image embeddings.\n    \\item Recall: Evaluates the model's ability to retrieve relevant images by calculating the proportion of true positive matches out of all relevant images.\n    \\item Precision: Assesses the proportion of relevant images retrieved among all retrieved images.\n    \\item F1 Score: Provides a balanced metric that combines precision and recall, particularly useful for evaluating the retrieval task when precision and recall are of equal importance.\n\\end{itemize}\nThese metrics allow us to comprehensively evaluate each model's effectiveness in multimodal user profile retrieval, particularly in how well they capture and reflect key visual attributes in a diverse set of user images."}, {"title": "V. RESULTS", "content": "As shown in Table I, the performance of CLIP and FaRL models is relatively low in the user retrieval task across both datasets. Specifically, CLIP demonstrates a slight edge over FaRL, achieving 16% accuracy on the GenUser dataset, which is only 3% higher than FaRL. On the FairUser dataset, CLIP attains 34% accuracy, outperforming FaRL by 11%. These results do not suggest that CLIP and FaRL models are inherently incapable of distinguishing between demographic user data. Instead, they highlight the models' challenges in accurately associating images with user profiles when processing complex queries involving sensitive and sparse features such as age, gender, ethnicity, facial attributes, and expressions."}, {"title": "B. Fine-tuned Performance", "content": "As shown in Figure 4, during fine-tuning on the GenUser dataset, the CLIP model begins to overfit after 5 epochs, while FaRL shows signs of overfitting after 4 epochs. In response, the cross-validation and thresholding mechanism halts the fine-tuning process until three additional epochs have been completed, preventing further overfitting. Despite this limitation, the average performance of both models over 5 runs improves, with CLIP achieving an F1 accuracy of 66% and FaRL reaching 65%. Similarly, on the FairUser dataset, CLIP begins overfitting after just 1 epoch, and FaRL after 2 epochs; here again, the cross-validation and threshold mechanism halts the training process for an additional three epochs to control overfitting. Under these conditions, the models attain F1 scores of 49% and 51% for CLIP and FaRL, respectively. These results demonstrate incremental performance improvements, although both models exhibit susceptibility to overfitting early in the fine-tuning process.\nIt is significant to emphasize that fine-tuning FaRL on one dataset results in a performance improvement on the other dataset. This cross-dataset enhancement indicates that our assumption on the integration of masked image modeling with contrastive learning leads to a degree of generalizability, potentially benefiting from the knowledge transfer acquired during fine-tuning. In contrast, CLIP exhibits the opposite behavior: its performance on the alternative dataset declines when fine-tuned on a specific dataset. This behavior might stem from the significant distribution shift between the datasets used in this evaluation and those CLIP was originally trained on. As a result, CLIP essentially behaves as if it is being trained from scratch, suffering from catastrophic forgetting. The model is prone to overfitting due to the relatively small size of the datasets compared to its massive parameter count (200M), making it less adaptable to diverse or unseen data distributions.\nTable II demonstrates a comparative example for the user retrieval task between CLIP and FaRL when processing a complex demographic query prompt. While CLIP successfully captures broad demographic attributes, it struggles to combine multiple specific demographic descriptors effectively. In contrast, the FaRL performs better in retrieving images that match the detailed demographic query, despite these exact attribute combinations not being present in its training data. This example highlights how general-purpose vision-language models like CLIP, though competent at basic demographic information retrieval, may have limitations when handling highly specific, multi-attribute demographic queries that require a more nuanced understanding of facial features."}, {"title": "VI. CONCLUSION", "content": "This paper explored the effectiveness of multimodal pre-trained models in capturing demographic representations from facial features for user profiling applications. Our empirical results demonstrate that while these models perform sub-optimally out-of-the-box, fine-tuning significantly enhances their predictive capacity. However, they still exhibit limitations in effectively generalizing demographic nuances. We suggest integrating a masked image modeling strategy to improve the model's generalizability and capture subtle demographic attributes. Future work will focus on refining these models and exploring additional strategies to enhance their performance in diverse and sensitive applications, such as healthcare, where understanding user behavior and preferences is critical."}]}