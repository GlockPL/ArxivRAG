{"title": "Jakiro: Boosting Speculative Decoding with Decoupled Multi-Head via MoE", "authors": ["Haiduo Huang", "Fuwei Yang", "Zhenhua Liu", "Yixing Xu", "Jinze Li", "Yang Liu", "Xuanwu Yin", "Dong Li", "Pengju Ren", "Emad Barsoum"], "abstract": "Speculative decoding (SD) accelerates large language model inference by using a smaller draft model to predict multiple tokens, which are then verified in parallel by the larger target model. However, the limited capacity of the draft model often necessitates tree-based sampling to improve prediction accuracy, where multiple candidates are generated at each step. We identify a key limitation in this approach: the candidates at the same step are derived from the same representation, limiting diversity and reducing overall effectiveness. To address this, we propose Jakiro, leveraging Mixture of Experts (MoE), where independent experts generate diverse predictions, effectively decoupling correlations among candidates. Furthermore, we introduce a hybrid inference strategy, combining autoregressive decoding for initial tokens with parallel decoding for subsequent stages, and enhance the latter with contrastive mechanism in features to improve accuracy. Our method significantly boosts prediction accuracy and achieves higher inference speedups. Extensive experiments across diverse models validate the effectiveness and robustness of our approach, establishing a new SOTA in speculative decoding. Our codes are available at https://github.com/haiduo/Jakiro.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs), such as GPT-40 (Jaech et al., 2024), LLaMA3 (Dubey et al., 2024) and Deepseek-r1 (Guo et al., 2025), have demonstrated remarkable capabilities across a wide range of applications, including question-answering, code synthesis, and machine translation. However, their token-by-token decoding process, combined with the growing size of the models, leads to significant inference latency, posing challenges for real-world deployment. Recently, speculative decoding (Leviathan et al., 2023; Chen et al., 2023) has emerged as an effective technique to accelerate LLM inference. This approach utilizes an efficient but weak draft model to predict multiple tokens in sequence, which are then verified in parallel by a more powerful but expensive target model. Since LLMs inference is often memory access bound (Shazeer, 2019), the verification stage can efficiently leverage hardware parallelism, achieving substantial and lossless speedups.\nRecent works, such as Medusa (Cai et al., 2024) and Hydra (Ankner et al., 2024), leverage multiple independent heads to predict the next N tokens, with all heads relying on the same final-layer features of the target model. This shared dependency limits the decoupling of predictions for different tokens, thereby constraining the overall prediction accuracy. In contrast, Eagle (Li et al., 2024a) and Eagle2 (Li et al., 2024b) adopts an autoregressive approach, where predictions at different time steps are based on the features from the previous step. Although the Eagle-style approaches successfully decouples draft tokens across different time steps,"}, {"title": "2. Preliminaries", "content": "Speculative decoding (Leviathan et al., 2023) is designed to accelerate inference from autoregressive LLMs without altering the output distribution. A smaller, computationally efficient draft model is employed to generate candidate tokens in the drafting phase, while a larger, high-quality target model verifies and refines these candidates. The key idea behind speculative decoding is to parallelize the token generation process, allowing multiple tokens to be proposed at once. This is achieved by observing that complex language-modeling tasks often contain subtasks that can be approximated by smaller models with adequate accuracy. By using these approximations, speculative decoding achieves significant speedup while maintaining the statistical properties of the target model's output.\nSuppose $t_i$ denotes the i-th token, and let $T_{x:y}$ represent the token sequence $t_x, t_{x+1}, ..., t_y$. The speculative decoding process involves three steps:\n1. Drafting: Given a prefix $T_{1:j}$, the draft model $q(.|.)$ autoregressively generates a sequence of candidate tokens $T_{j+1:j+y}$, each accompanied by a probability distribution $q_{j+1:j+y}$, where y is the number of inference steps per round for the draft model.\n2. Verification: The target model $p(.|.)$ computes the conditional probabilities $p_{j+1:j+y}$ for the same sequence in a single forward pass. Each candidate token $t_{j+i}$ is sequentially evaluated based on an acceptance criterion, such as $min(1, \\frac{p_{j+i}(t_{j+i})}{q_{j+i}(t_{j+i})})$.\n3. Refinement: Accepted tokens are appended to the output sequence, while rejected tokens are re-sampled using a normalized probability distribution $norm(max(0, p_{j+i} - q_{j+i}))$ to replace $t_{j+i}$ and discards the remaining tokens in the drafting.\nThe method ensures that the output distribution remains consistent with vanilla autoregressive decoding for both greedy and non-greedy sampling strategies, as proven in prior works (Leviathan et al., 2023; Chen et al., 2023)."}, {"title": "2.1. Speculative Decoding", "content": null}, {"title": "2.2. Medusa-style Decoding", "content": "Medusa-style (Cai et al., 2024; Ankner et al., 2024) decoding extends speculative decoding by employing multiple lightweight draft heads, typically implemented as small multi-layer perceptrons (MLPs), atop the last hidden state of the target LLM. These heads independently predict tokens based on the last hidden state of the target LLM, and their predictions are combined into candidate continuations. However, the shared use of the last hidden state across all heads results in incomplete decoupling, limiting the ability of each head to make diverse and independent predictions."}, {"title": "2.3. Eagle-style Decoding", "content": "In Eagle-style (Li et al., 2024a;b) decoding, the drafting stage decouples draft tokens at different time steps by performing autoregression at the feature level (before the LM head). The LM head of the original LLM is then used to convert features into draft tokens. This approach reduces uncertainty in the feature sequence, resulting in improved draft predictions. However, the top-k tokens within the same layer of the draft tree remain coupled, which limits the diversity and potential of the predictions.\nEaglel adopts a static tree-structured draft (Miao et al., 2023) in the verification stage, allowing branching paths to explore alternative continuations when draft tokens are rejected. This structure increases robustness by avoiding the full discard of draft sequences when individual tokens fail to meet the target model's criteria. Eagle2 improves upon Eagle1 by introducing dynamically adjustable draft trees. This allows the tree to adapt based on runtime conditions, optimizing token prediction and verification. Despite this enhancement, the lack of intra-layer decoupling in draft trees still limits the diversity of generated tokens, particularly in non-greedy sampling modes."}, {"title": "3. Jakiro", "content": null}, {"title": "3.1. Dynamic Decoupling and MoE Tree Construction", "content": null}, {"title": "3.1.1. DYNAMIC DECOUPLING WITH MOE HEADS", "content": "Unlike Medusa, which relies on the last hidden state for all draft heads, Jakiro employs multiple MoE heads that dynamically allocate expert modules to predict tokens. This mechanism accounts for inherent differences between tokens and ensures the decoupling of token predictions across heads. As a result, prediction confidence is improved while maintaining computational efficiency.\nThe structure of our draft model follows the design principles of the Eagle-style, utilizing a single decoder layer that includes a reduction dimension layer, an LLM attention layer, and parallel expert heads (lightweight MLP layers). And, the embedding layer and head layer remain consistent with the target model without introducing additional parameters. This ensures that inference efficiency is not compromised by the increase in the number of parameters."}, {"title": "3.1.2. MOE TREE CONSTRUCTION", "content": "Jakiro introduces a novel tree construction method that decouples the intra-layer dependencies of traditional draft trees while retaining the independence between inter-layers. This design ensures that draft tokens across different layers are generated independently, preserving the consistency of the auto-regressive decoding draft model with the target model and significantly enhancing the accuracy and factuality of predictions. At the same time, the decoupling of intra-layer tokens enables Jakiro to explore a broader range of candidate continuations or diversity, especially in non-greedy modes, without compromising the integrity of the output distribution.\nSpecifically, the original Eagle inference process can be summarized as follows: when a new token $t_i$ is passed through the embedding layer, a token embedding is obtained. This embedding is then concatenated with the hidden states (denoted as feature $f_{i\u22121}$) from the previous step. The concatenated result is processed through the Reduction layer for dimensionality reduction, and the output hidden state $h_i$ subsequently through the LLM decoder layer (including the attention layer Attn and MLP layer) to produce the hidden states $f_i$ for the current step. The next token $t_{i+1}$ is sampled via the Head layer and used for the next step of inference. This process iterates until an end-of-sequence token is encountered or the maximum token length is reached."}, {"title": "3.2. Efficient Integration of Contrastive Mechanism", "content": "Although Jakiro significantly enhances performance in non-greedy modes through its decoupled tree structure, it does not show a substantial advantage in strictly top-1 accuracy tasks, such as HumanEval and GSM8K, which are optimized for greedy decoding. However, by incorporating the Medusa parallel decoding mechanism, we reduce the number of forward passes during inference without compromising the final speedup, enabling Jakiro to achieve performance on par with Eagle2.\nMoreover, LLMs are prone to hallucinations (Tonmoy et al.,"}, {"title": "3.3. Training of the Draft Models", "content": "Similar to Eagle-style, in order to reduce training costs, we use a preprocessed fixed dataset for training the draft models. Other data augmentation hyperparameters are kept consistent with Eagle. Additionally, we adopt the Smooth L1 loss for predicting the next feature as a regression task and use cross-entropy to ensure the accuracy of tokens sequence. However, considering that the entire process of the draft model is autoregressive and computationally expensive, we use a parallel decoding mechanism during the penultimate step. Thus, the prediction of the next token is achieved through contrast between the two expert heads of the MoE, introducing an additional loss (similar to the Medusa implementation). The optimize objective is as follows:\n$L_{me}^{reg} = Smooth\\ L1(f_{i+1}^{top1}, f_{i}^{moe}),$ (9)\n$L_{const}^{reg} = Smooth\\ L1(f_{i+2}^{top1}, f_{i}^{const}),$ (10)\n$q_{i+2}, q_{i+3} = Softmax(logits_{i+1}^{moe}, logits_{i}^{const}),$ (11)\n$L_{moe}^{cls} = Cross\\ Entropy(p_{i+2}, q_{i+2}),$ (12)\n$L_{const}^{cls} = Cross\\ Entropy(p_{i+3}, q_{i+3}),$ (13)\n$L = L_{me}^{reg} + L_{const}^{reg} + w_{moe}L_{moe}^{cls} + w_{const}L_{const}^{cls}$ (14)\nWe use a combined loss function L to train the autoregressive draft model. Given that the classification loss is an order of magnitude larger than the regression loss numerically, and the importance of const is lower compared to moe, we set $w_{moe}$ and $w_{const}$ to 0.1 and 0.05, respectively."}, {"title": "4. Experiments", "content": "Models and tasks. Following the current mainstream works, we conduct experiments on Vicuna models (7B, 13B, 33B) (Chiang et al., 2023), LLaMA2-chat models (7B, 13B, 70B) (Touvron et al., 2023), and LLaMA3-Instruct (8B, 70B) (Meta, 2024), encompassing the common sizes of current mainstream LLMs. To evaluate the generality and robustness of our method, our Jakiro is evaluated across multiple tasks including multi-turn dialogue, code generation, mathematical reasoning, and instruction following, employing the MT-bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), GSM8K (Cobbe et al., 2021), Alpaca (Taori et al., 2023), CNN/Daily Mail (Nallapati et al., 2016), and Natural Questions (Kwiatkowski et al., 2019) datasets, respectively. Because Speculative decoding (Leviathan et al., 2023) conduct experiments with a batch size of 1, a setting subsequently adopted by other works such as Medusa-style (Cai et al., 2024; Ankner et al., 2024) and Eagle-style (Li et al., 2024a;b). Similarly, the majority of our experiments also adopt this setting.\nMetrics. Similar to prior speculative sampling approaches, our Jakiro method primarily focuses on reducing latency rather than optimizing throughput. To measure its acceleration effects, we utilize the following metrics:\n\u2022 Walltime speedup ratio: Compared to traditional autoregressive decoding, the ratio of speedup achieved in actual tests of end-to-end.\n\u2022 Average acceptance length 7: The average number of tokens accepted from speculative decoding per forward pass of the target LLM.\nSimilar to methods that use a strict speculative sampling mechanism, the acceleration achieved by Jakiro ensures that the output distribution of the target LLMs is maintained. Therefore, evaluating the quality of the generated results is both unnecessary and irrelevant, as the focus is on efficiency rather than output fidelity.\nTraining. We keep the target LLMs fixed throughout the training process. Our proposed Jakiro model is trained on the ShareGPT\u00b9 dataset using 68,000 dialogue iterations without needing extensive retraining or additional data beyond the pre-trained models, and with a learning rate set at 9e-5. The AdamW optimizer is employed with beta values (\u03b2\u03b9, \u03b22) = (0.9, 0.95), and gradient clipping is applied with a threshold of 0.5. The number of trainable parameters for Jakiro varies across model sizes: 0.35B for the 7B model, 0.56B for the 8B model, 0.88B for the 13B model, 1.42B for the 33B model, and 1.87B for the 70B model. For example, training the Jakiro draft model for the 70B model takes approximately 2-3 days on 4\u00d7A100 40GB GPU.\nTesting. Our experiments are conducted on different devices (AMD InstinctTM MI250-64G, NVIDIA A40-45G, and NVIDIA A100-40G) and the limitation of hardware GPU memory. For models of size (7B, 8B, 13B), (33B), and (77B), we perform single-GPU, two GPUs, and four GPUs, respectively."}, {"title": "4.1. Effectiveness", "content": "As shown in Table 1 and Table 2, our proposed Jakiro consistently outperforms existing methods in terms of speedup ratios across various datasets and models on M1250. The speedup ratios achieved by Jakiro, such as 2.99x on the MT-bench and 3.43x on the HumanEval task for the Vicuna 7B model under greedy mode, significantly surpass those of other methods, including Eagle2, which achieved a maximum speedup of 2.88x on average. The results across different tasks further highlight the effectiveness of Jakiro. For example, on the GSM8K task, Jakiro achieves a 3.11x speedup, maintaining a high average acceptance length of 4.95 tokens. This demonstrates that Jakiro not only accelerates inference but also retains high-quality output, maintaining the target LLM's performance without compromising accuracy.\nSince our Jakiro involves one less drafting step than Eagle2, there is no significant improvement in the average acceptance length of tokens. However, considering that the ultimate goal of speculative decoding is to improve speedup, our method is still effective. For instance, in the greedy mode for LLaMA2-Chat 7B, although the average acceptance length of our method is slightly lower than that of Eagle2 (4.51 vs. 4.63), the final speedup of our method is notably improved (2.83x vs. 2.66x). Furthermore, we observe that Jakiro excels across both code generation and natural language tasks. This is particularly notable because code generation tasks often benefit from greedy sampling, where the inherent structure of code makes it easier to predict subsequent tokens efficiently. Similarly, on Natural Questions and summarization tasks, Jakiro maintains superior performance.\nAdditionally, in non-greedy mode, the comparison methods with speculative sampling also reveal that Jakiro achieves significantly higher speedup ratios while also maintaining a higher average acceptance length. Such as Vicuna 7B, compared to Eagle2, our method achieves a 15.4% improvement in the final average speedup across all benchmark test sets (2.84 vs. 2.46). This suggests that Jakiro benefits from a more efficient drafting process that allows for longer and more stable sequences of tokens to be accepted, reducing the need for frequent re-sampling and minimizing the risk of errors during the inference process."}, {"title": "4.2. Ablation Study", "content": "N-k Setting of MoE. The number of candidate experts N and activated experts K involved in the computation for each token in a typical MoE architecture is 8 and 2, respectively. If speculative decoding uses the original chain-based construction of draft tokens, the number of experts activated each time is fixed and equal to 2. However, current mainstream speculative decoding methods use a tree structure to construct draft tokens. Although the number of candidate experts activated for each token is fixed, the process of constructing the tree results in the top-k candidate tokens being selected at each layer, which leads to a number of experts activated $K\u2265 2$ during each inference. Additionally, the more candidate experts there are, the better the model's predictive ability, but with an increased computational cost."}, {"title": "5. Related Work", "content": "Speculative decoding: Speculative decoding (SD)has emerged as a powerful technique to accelerate LLMs inference by reducing memory bandwidth bottlenecks. Early speculative decoding methods, such as those by (Stern et al., 2018) and (Sun et al., 2021), focused on greedy decoding strategies, while (Leviathan et al., 2023) and (Chen et al., 2023) expanded speculative sampling to non-greedy decoding. Recent works in SD have enhanced draft model efficiency, with methods like SpecInfer (Miao et al., 2023) employing tree attention to verify multiple draft tokens in parallel, and Medusa (Cai et al., 2024) using extra MLP heads to generate token drafts. Although these methods have achieved notable acceleration, they face limitations in token diversity and decoupling between draft and target models. Eagle (Li et al., 2024a) introduces a more dynamic approach by decoupling the draft tokens across different time steps. However, Eagle still maintains coupling between the Top-k tokens at the same layer in the draft tree, which restricts the diversity and specialization of tokens.\nOur approach builds upon these limitations by introducing a dynamic decoupling mechanism with Mixture of Experts (MoE) heads, which enables draft tokens to consider inherent differences between them, leading to more diversity and confident predictions.\nMixture of Experts: MoE has been explored extensively for improving the specialization of models. MoE techniques were originally proposed by (Jacobs et al., 1991; Jordan & Jacobs, 1994) and later adapted to language models, such as in GShard (Lepikhin et al., 2021) and Switch Transformer (Fedus et al., 2021), which scale MoE to large models with top-k routing strategies. More recently, the integration of MoE in Transformer-based architectures has garnered significant interest, with methods like StableMoE (Dai et al., 2022) exploring fixed routing strategies for more stable training. MoE heads have also been used in multi-modal settings (Du et al., 2022; Xue et al., 2023), where they enable specialization across different modalities. Furthermore, in the context of speculative decoding, our method combines MoE's dual-branch heads with contrastive decoding techniques, a strategy inspired by recent works (Shazeer et al., 2017; Dai et al., 2022) to improve the usefulness of draft token predictions, especially in greedy modes. By integrating these strategies, we can achieve more reliable predictions with faster inference, as demonstrated through our experiments.\nParallel decoding: Parallel decoding is known for its efficiency in machine translation (Ghazvininejad et al., 2019) and code generation (Gloeckle et al., 2024), has also been integrated into SD frameworks to further enhance efficiency. Although the use of parallel decoding in speculative frameworks has been under-explored, works like (Monea et al., 2023) and (Yi et al., 2024) have pioneered its application. These methods, however, still face challenges in achieving a perfect alignment between draft distributions and target models, which can limit their effectiveness in lossless acceleration. Our approach addresses these challenges by improving the decoupling mechanism within the MoE heads, ensuring better alignment and more diverse token predictions in both greedy and non-greedy modes."}, {"title": "Combine with Parallel Decoding and Contrastive Mechanism", "content": "The contrastive mechanism (perhaps similar to a form of contrastive learning) and its implementation combined with parallel decoding is discussed in Section 3.2. This approach aims to reduce one draft inference step compared to conventional autoregressive decoding LLMs (e.g., Eagle2) without significant loss in prediction performance, thereby achieving optimal speedup. However, the essence of these two mechanisms is different. The contrastive mechanism focuses on better predicting the next token in the sequence, while parallel decoding aims to output multiple future tokens in one step. Therefore, it is necessary to conduct further experiments to evaluate their respective impacts on final performance, with the results shown in Table 4.\nAs can be seen from the above results, using parallel decoding or the contrastive mechanism alone does lead to some improvement compared to using MoE alone, which demonstrates that the two proposed mechanisms can effectively complement Jakiro. However, combining both mechanisms further unleashes the potential of Jakiro, maintaining a comparable acceptance rate (similar to average accpet length) while further improving its speedup."}, {"title": "6. Conclusion", "content": "In summary, while speculative decoding has proven effective for accelerating LLM inference, challenges remain in improving token diversity and maintaining distribution alignment between draft and target models. Our work makes significant strides in addressing these challenges by introducing a dynamic decoupling mechanism with MoE heads, which improves prediction confidence, diversity, and factuality. Additionally, by combining contrastive mechanism with MoE's dual-branch heads, we further enhance the utility of our predictions, demonstrating the potential of this approach in both greedy and non-greedy generation tasks. Our proposed Jakiro demonstrates superior speedup ratios, high acceptance lengths, and overall efficiency across a variety of tasks and model sizes, making it a compelling solution for improving inference speed without sacrificing model output quality."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Supplementary experiments", "content": null}, {"title": "A.1. Average Acceptance Length.", "content": null}, {"title": "A.2. Results of Other Devices.", "content": null}]}