{"title": "FGR-Net: Interpretable fundus image gradeability classification based on deep reconstruction learning", "authors": ["Saif Khalid", "Hatem A. Rashwan", "Saddam Abdulwahab", "Mohamed Abdel-Nasser", "Facundo Manuel Quiroga", "Domenec Puig"], "abstract": "The performance of diagnostic Computer-Aided Design (CAD) systems for retinal diseases depends on the quality of the retinal images being screened. Thus, many studies have been developed to evaluate and assess the quality of such retinal images. However, most of them did not investigate the relationship between the accuracy of the developed models and the quality of the visualization of interpretability methods for distinguishing between gradable and non-gradable retinal images. Consequently, this paper presents a novel framework called \"FGR-Net\" to automatically assess and interpret underlying fundus image quality by merging an autoencoder network with a classifier network. The FGR-Net model also provides an interpretable quality assessment through visualizations. In particular, FGR-Net uses a deep autoencoder to reconstruct the input image in order to extract the visual characteristics of the input fundus images based on self-supervised learning. The extracted features by the autoencoder are then fed into a deep classifier network to distinguish between gradable and ungradable fundus images. FGR-Net is evaluated with different interpretability methods, which indicates that the autoencoder is a key factor in forcing the classifier to focus on the relevant structures of the fundus images, such as the fovea, optic disk, and prominent blood vessels. Additionally, the interpretability methods can provide visual feedback for ophthalmologists to understand how our model evaluates the quality of fundus images. The experimental results showed the superiority of FGR-Net over the state-of-the-art quality assessment methods, with an accuracy of > 89% and an F1-score of > 87%.", "sections": [{"title": "1. Introduction", "content": "Fundus retinal photography uses a fundus camera to record color images of the eye's internal surface condition to screen for eye disorders and track their progression. Various eye disorders, such as diabetic retinopathy (DR), cataract, age-related macular degeneration (AMD), and glaucoma are diagnosed using fundus imaging. These diseases affect a considerable percentage of the world's population. However, while ophthalmologists strive to provide appropriate medical care to many patients, the number of eye specialists available to satisfy the current demand is insufficient. Artificial Intelligence (AI) has recently played a significant role in capturing, evaluating, and analyzing fundus images. AI-based fundus image analysis systems help reduce the shortage of ophthalmologists by providing accurate and quick diagnoses of thousands of fundus images. Many Al-powered approaches for screening and diagnosing various eye diseases have been proposed in the literature. However, low-quality fundus images degrade the performance of AI-based fundus image analysis systems. Ophthalmologists have criteria to grade the quality of retinal images before treatment and diagnosis. In the case of fundus images, this process is called Image Quality Assessment (IQA) in general or Image Gradability Classification. The process determines whether an image can be used for diagnosis."}, {"title": "2. Related work", "content": "2.1. Retinal image gradability assessment\nMany fundus image gradability methods are based on two-class quality labels (i.e., 'Accept' and 'Reject'). Others are based on a three-class quality grading system (i.e., 'Good', 'Usable', and 'Reject'). For instance, proposed a method for automatically classifying the quality of retinal images based on the RGB color space. Their system uses vessel density, textural features, global histogram features, and a metric known as non-reference perceptual sharpness. They also concentrated on three Regions of Interest (ROI); lower retinal hemispheres, upper retinal hemispheres, and optic disk regions. introduced a retinal image assessment algorithm that selects images of acceptable generic quality. The algorithm uses three human visual system characteristics: multi-channel sensation, noticeable blur, and contrast sensitivity functions to detect illumination and color distortion, blur, and low contrast distortion. They used a total of 536 retinal images, 280 from proprietary datasets, and 256 from public databases. Then, they employed Support Vector Machines (SVMs)and a Decision Tree for binary classification. They achieved a sensitivity of 87.45% and a specificity of 91.66%. A recent work, proposed a method that automatically grades image quality on a continuous scale. The technique utilizes random forest regression models trained on image features discovered automatically using Fourier transform. The method was tested on DRIMDB, a publicly available dataset with binary"}, {"title": "2.2. Interpretability for fundus analysis models", "content": "Explainable machine/deep learning models help clinicians interpret black-box models and their decision-making process to verify why these models took a particular decision, which is extremely important in the medical field. Explainability is a desired feature in models that allows both users and researchers to understand relationships between model inputs, outputs and domain concepts. These can be classified into three broad groups:\nRule-extraction methods, which infer high-level rules from the relationship between inputs and outputs of a network.\nAttribution methods, which measure the importance of a component by changing to the input or internal components and recording how much the changes affect model performance. Attribution methods are often visualized and sometimes referred to as visualization or saliency methods.\nIntrinsic methods, which aim to improve the interpretability of internal representations with methods that are part of the model architecture. Intrinsic methods increase fidelity, clarity, and parsimony in attribution methods.\nThis work focuses on interpretability methods for deep learning models in computer vision. In particular, for computer vision techniques based on deep learning models, most interpretability methods visualize the information obtained by attribution methods. Visualization methods were popularized and provide various ways to visualize important features of a model. These are intuitive methods to gain various insights about a deep neural networks (DNN) decision process on many levels, including architecture assessment, model quality assessment and even user feedback integration."}, {"title": "2.3. Contributions", "content": "Previous and recent studies in the field of fundus image classification are an important topic because of their relationship to health care in today's society and the field of medical image processing research. Traditional machine learning methods generally produce more interpretable classification models. but they only perform better on small sample datasets. On the other hand, deep learning methods can utilize large datasets and models to achieve state-of-the-art performance. Furthermore, deep learning methods have achieved state of the art results in ophthalmology-related tasks.\nIn our work, we focus on solving the fundus image quality problem. We achieve better results than traditional deep learning networks by using an auxiliary autoencoder network to reconstruct the input image. The autoencoder helps to improve the intermediate representations and focus on relevant features to grade the quality of the fundus images. To validate this claim, we supplemented our model with interpretability methods to understand which features are taken into account. Additionally, the same interpretability techniques can help ophthalmologists and experts distinguish between gradable and ungradable images for timely recapture."}, {"title": "3. Methodology", "content": "This section explains the FGR-Net model for retinal image gradability classification. Besides, we demonstrate interpretability techniques to provide visual feedback for doctors to understand which landmarks FGR-Net looks for when classifying the gradability of input images."}, {"title": "3.1. FGR-Net model", "content": "Fig. 3 depicts the FGR-Net model for fundus image gradability assessment. The first (top) part of the network is an autoencoder trained to learn robust feature representations of fundus images. The intermediate representations learned by the autoencoder are fed into a classifier to predict the gradability of the input fundus images as three labels: Good, Usable, or Reject.\nIn FGR-Net, we present a self-supervised approach for image-image translation. To formulate the reconstruction for the fundus image, let $A \\in A$ be a fundus image. The problem of generating a reconstructed image, $B\\in A$, can be formally defined as a function: $f : A\\rightarrow A$, that maps elements from a domain A to the same domain A, under a constraint of that the representation of the input image must be encoded into a lower-dimensional manifold to force the compression of the input features. To optimize the autoencoder network, it is trained via backpropagation using as a loss function minimizing the distance between the reconstructed image and the input image (i.e., target).\nThe autoencoder network helps to learn the fundus image-relevant features of the input fundus image, including the visible quality features. Thus, the input to our autoencoder is an RGB fundus image, and the target is the same as the input fundus image. We propose that if the autoencoder network succeeds in reconstructing the same input image, the network succeeds in learning the input image's key features, including visual quality features. In this way, we can ensure that the intermediate representations preserve the information required for the gradability classification task. The experiments support our hypothesis. The proposed autoencoder network contains two sub-networks: encoder and decoder.\nEncoder: The encoder that obtains different levels of abstraction of fundus image features is continuously sampled through five blocks. Each block consists of a convolutional layer with a kernel size of 3 \u00d7 3 and an activation function of ReLU followed by a max-pooling of 2 x 2. The input image size to the encoder is 480 \u00d7 480 \u00d7 3, and the output is a feature map of 15 x 15 x 512.\nDecoder: The decoder consists of five deconvolution layers (i.e., up-sampling using bilinear interpolation and a convolutional layer with a kernel of 3 x 3) on the top-level feature map extracted from the encoder network to combine different features in the downsampling process and restore the input fundus image. Skip connections were used to connect the corresponding layers between the encoder and decoder networks to preserve the spatial information and the anatomical structures in fundus images. At the top of the decoder, a convolutional layer with"}, {"title": "3.2. Training", "content": "In this work, we tested the performance of the model with three different reconstruction loss functions, $L_{rec}$, used to compare the input images to the reconstructed images under self-supervision learning. Notably, we tested simple and standard loss functions as a reconstruction loss to support our idea that the autoencoder itself, not the loss function, helps the network find relevant patterns related to the characteristics of the image's quality.\nThe first tested reconstruction loss function $L_{rec}$ is to compute the mean square error (MSE) between the actual input image of input A and the reconstructed image of A resulting from the Autoencoder network. MSE is the average of squared differences between the actual and expected values that can be defined as:\n$L_{ree} (\\hat{A}, A) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{A}_i - A_i)^2$,\nwhere A(i) is the input image of pixel i, $\\hat{A}_i$ is the reconstructed image, and the n is the number of pixels in the image A.\nThe second tested reconstruction loss function, $L_{rec}$, is the Mean absolute error (MAE). MAE is the mean of the absolute differences between actual and predicted values that can be defined as:\n$L_{rec}(\\hat{A}, A) = \\frac{1}{n} \\sum_{i=1}^{n} |A - \\hat{A}|$.\nThe third reconstruction loss function, $L_{rec}$, is a structural similarity index measure (SSIM), a method for predicting the perceived quality of digital images. SSIM is used for measuring the similarity between two images. The SSIM index is a complete reference metric for measuring the quality of reconstructed images compared to input images. Contrary to the L1 and L2 losses, the SSIM metric measures the similarity by"}, {"title": "4. Interpretability of deep learning models", "content": "Fig. 4 represents the summary of the applied explainability process of FGR-Net in the testing phase. The first part represents the encoder network, the second part represents the classification network, and the third part the module for interpreting the results of the classifier using the Gradient, GradCAM, and Occlusion algorithms, respectively.\nWe briefly describe three well-known feature attribution methods: Gradient, GradCAM and Occlusion.  shows the result of each of these methods on a typical fundus image. For the occlusion method, green values (positive) indicate regions where the occlusion increases the class score and vice versa for red values. For the Gradient method, green values indicate that increasing the pixel's brightness causes the score to increase. For the GradCAM, the interpretation is more complex since it depends on the representation of the input by the encoder. Therefore, the absolute value of GradCAM's output is usually interpreted as the importance of each region for a specific class regardless of sign.\nGradient. The simplest feature attribution method computes the Gradient of an output score concerning the input. In the case of images, let $f: R^{H \\times W \\times C} \\rightarrow R$ be the function that represents a neural network and let L be a loss or score function. Then we can compute the Gradient of L concerning an input image x simply by using back-propagation, obtaining:\n$L'(x) = \\frac{dL(f(x))}{dx}$\n$L': R^{H \\times W \\times C} \\rightarrow R^{H \\times W \\times C}$\nTherefore L' can indicate how to change an input image to optimize for a score locally. A typical choice for L can be the score for a particular class so that L' indicates how to modify x to improve the score for that class.\nWhile simplest to compute and interpret, the Gradient method suffers from some disadvantages, namely saturation of the activation functions, gradient discontinuities, and thresholding artifacts.\nGradCAM. GradCAM follows the Gradient's method basic idea but adds a projection step so that the gradient can be computed concerning any intermediate layer and then projected back into the input to obtain importance scores. GradCAM been widely used to analyze deep learning models' representations. Since most researchers have used this method to understand fundus image analysis models, we briefly describe GradCAM's algorithm.\nLet $f: R^{H \\times W \\times C} \\Rightarrow R$ be a network composed of a feature network g that outputs a feature map and an output network h such that:\n$f(x) = h(g(x))$,\nwhere x is the input to the whole network. Let $L : R^n \\rightarrow R$ be a loss function for f. GradCAM computes a saliency score $s(x): R^{H \\times W}$ for each pixel in the input x. Note that this score is a function of the input image x, so different images produce a different GradCAM score."}, {"title": "5. Experimental results", "content": "This section introduces the datasets, evaluation metrics, and experiments performed to evaluate the FGR-Net model and the interpretation of the FGR-Net features."}, {"title": "5.1. Datasets", "content": "Three fundus image gradability datasets were used in our experiments: EyePACS, Eye-Quality (EyeQ), and an in-house dataset collected in the Hospital Universitari Sant Joan de Reus (HUSJR), Spain.\nEyePACS is a publicly available dataset containing 31,031 fundus images categorized into two classes: gradable and ungradable. EyePACS is divided into a training set of 29,033 images and a test set of 1999.\nEyeQ has 28,792 fundus images derived from the EyePACS dataset. Unlike EyePACS, EyeQ categorizes fundus images into three gradability classes: Good, Usable, and Reject. EyeQ is divided into a training set of 12,543 images and a test set of 16, 249images.\nThe in-house dataset was collected from HUSJR. It categorizes fundus images into two gradability classes: gradable and ungradable. In our experiments, we used this dataset as a test set of 1127 images."}, {"title": "5.2. Data augmentation", "content": "In this work, we applied different data augmentation techniques suggested in to increase the number of training samples by applying different transformations to each fundus image to diversify the training data, such as random rotations and flipping. For EyePACS, the dataset consists of two classes: (1) gradable with 21,812 images and (0) ungradable classes containing 7218 images. To construct a balanced dataset, after argumentation, class 0 has 28,872 images, and class 1 has 29,083 images. Total training data (i.e. gradable and ungradable) has 57,957 images. In turn, the EyeQ dataset, after augmentation, has 65,876 retinal images balanced split into a three-level quality grading system ('Good', 'Usable', and 'Reject')."}, {"title": "5.3. Parameter settings", "content": "We used the Adam optimizer with y = 0.1 and an initial learning rate of 0.001. A batch size of 2 and 50 epochs yielded the best combination. All experiments ran on a 64-bit Core I7-6700,"}, {"title": "5.4. Evaluation metrics", "content": "In this work, we used four metrics to evaluate the performance of classification fundamentals as an image gradability-based deep learning model: Accuracy, Precision, Recall, and F1 score.\nAccuracy that calculates the number of correct predictions divided by the number of predictions. Precision refers to the ratio of true positives to the sum of false positives and true positives. The third measure is Recall referred to as sensitivity, and defined as the ratio of true positives to the sum of positives. In turn, the fourth measure represents the F1 score as the weighted harmonic average of accuracy and recall , and it is considered one of the most critical performance measures of the classification model used for medical applications. The closer the value of the F1 score to 1.0, the better the expected performance of the model.\n$Accuracy = \\frac{(TP+TN)}{(TP+TN+FP + FN)}$\n$Precision = \\frac{TP}{(TP + FP)}$\n$Recall(Sensitivity) = \\frac{TP}{(TP + FN)}$\n$F1Score = \\frac{2T P}{(2TP+FP+ FN)}$\nwhere TP is the number of the true positive samples, TN is the number of the true negative samples, FP is the number of the false positive samples and FN is the number of the false negative samples."}, {"title": "5.5. Ablation study", "content": "In this section, we study the performance of various CNNs as backbone for the autoencoder network of the FGR-Net model. Particularly, we employed CoAtNets , ResNet50, Resnet101, Resnet152 , wide-resnet , DenseNet121, DenseNet201, DenseNet169 , SE-Net154,SE-ResNet-101, SE-ResNet-152, SE-ResNet-50, SE-ResNeXT101 , and VGG16 ."}, {"title": "5.6. Comparisons with state of the arts", "content": "5.6.1. Two-class model evaluation\nTo our knowledge, there are no deep learning-based models related to the two-class problem (gradable and ungradable). Most proposals focused on the level of image quality consisting of a three-class problem (good, usable and reject). Thus, in order to evaluate our two-class FGR-Net model merging the autoencoder and classifier networks, we updated the MCF-Net model proposed in to work with two classes of gradability. Thus, we retrained the MCF-Net model from scratch with the EyePACS dataset by changing the last layer's structure to work with two classification levels. As shown in , we find the FGR-Net model based on VGG16 as an encoder network with MSE as a reconstruction loss function outperforms the adapted MCF-Net model with two classes and the two variations of FGR-Net with MAE and SSIM loss functions in terms of the four measures. For instance, the F1-score with FGR-Net yielded a significant improvement of 6% more than the adapted MCF-Net model. Besides, our model combining the autoencoder and classifier networks achieved an accuracy of \u226589% with an improvement of 8% compared to the adapted MCF-Net model. A significant improvement of 25% appeared with the Precision measure compared to the MCF-Net. In addition, the FGR-Net model with the two other loss functions outperforms the adapted MCF-Net in terms of the four measures (Accuracy, Precision, Recall and F1-score) with significant improvements of 7%, 24%, 4%, and 5%, respectively."}, {"title": "5.6.2. Three-class evaluation", "content": "In order to compare the FGR-Net model with modern gradability assessment methods, we compared FGR-Net to the state-of-art of three-class (good, usable, Reject) gradability assessment on the public Eye-Quality (EyeQ) dataset. summarizes the results of the three variations of FGR-Net with eight methods: two methods are based on hand-crafted features; BRISQUE and NBIQA , and six methods based on deep learning designed for retinal fundus image quality assessment; TS-CNN , HVS-based method, MCF-Net , multivariate regression CNN (MR-CNN) , the Double branch network, SalStructIQA and multi-level quality assessment network . A three variations, with the three loss functions, of FGR-Net combining the autoencoder with classifier networks provided the best results in terms of Accuracy, Precision, Recall and F1-score. Among the three variations, the model with MSE as a loss function yields the best results in terms of Accuracy, Recall and F1-score. Thus, we showed only the results of FGR-Net with MSE as a loss function in . In turn, The model with MAE achieved the best Precision value. As shown in , our model significantly improved the F1-score by 14% compared to the handcrafted methods. Our model achieved a small improvement with the four measures compared to SalStructIQA and CNN-combined . However, the method proposed in combines deep and handcrafted features for assessing the fundus image quality. In turn, the method proposed in segments two salient features before classifying the quality of fundus images that adds more complexity to their model. In contrast, FGR-Net is a straightforward model. Since, we only used the encoder network and the classifier in the testing stage without extracting prior information from the input fundus images.\nIn order to check the scalability and upgradability of the FGR-Net model on the EyeQ dataset with three-classes ((0) Good, (1) Usable, (2) Rejected), we also computed the confusion matrix with three loss functions (MSE, MAE, SSIM) and overall classification accuracy in the test set. As explained above in the subsection, this allows for a more detailed analysis than just a high rating ratio. As shown in 9, TPs and TNs of FGR-Net with a test set of 16,255 images with three loss functions. The model was able to classify the fundus images into three classes with a small number of mispredictions. For instance, the model with MSE and the first class \"Good\" classified only five \"Reject\" images as good images and 387 \"Usable\" images as \"Good\" images. This result is intuitive since both \"Usable\" and \"Good\" images have similar characteristics. Among of the three reconstruction losses, MSE yields the highest TP and TN on the test set of the EyeQ dataset. FGR-Net achieved an AUC of 0.94 with class 0, 0.88 with class 1, and 0.91 with class 2. The results confirm the results shown in the confusion matrix, see the supplementary materials for more results."}, {"title": "5.7. Limitations and robustness", "content": "For the image gradability classification into three classes (i.e., Good, Usable and Reject), our model, FGR-Net, has one limitation related to the Usable class. Due to the underrepresentation in the data, the model may struggle to learn its characteristics and classify them accurately in some cases. To assess that, we have included , which shows 4 misclassified samples. These qualitative results support the quantitative results of the confusion matrix shown in Fig. 9. However, the good thing is that most misclassified Usable images are recognized as Good. This is acceptable from the clinical point of view since ophthalmologists can use Usable images for the diagnosis and screening, like Good images. Additionally, the Usable images are sometimes unclear to ophthalmologists; thus, the model may classify them as a Reject class. Thus, the FGR-Net model mimics human beings with the images of the Usable class.\nTo assess the robustness of the FGR-Net model and to show how the model behaves when presented with noisy or perturbed images, we systematically tested our model with six types of common perturbation methods:\nGaussian Blur (\u03c3 = [0.5,1.5])\nAdditive Gaussian Noise (scale = [0.5, 0.04*255])\nGamma Contrast (scale = [0.5, 1.5])\nAdditive Poisson Noise (\u03bb = 10.0)\nAffine Transformation (scale = [0.5, 1.5])"}, {"title": "5.8. Interpretation of model features", "content": "We used various interpretability methods to understand the Normal model based on a traditional classification network and our model, which combines an autoencoder and a classifier, to compare their internal representations. We focus on the two-class problem (gradable vs ungradable) since we are interested in determining which features the model uses to determine whether a fundus image is of good quality. Our approach employs:\nSaliency map methods such as Gradient and GradCAM visualizations to understand the relevance of the input regions.\nUnsupervised learning methods to identify common patterns in activations .\nGenerative and Adversarial techniques to understand model robustness and class definitions .\nOur experiments use PyTorch models via the Captum interpretability package ."}, {"title": "5.8.1. Saliency maps", "content": "We measured the importance of the input regions for the gradability classification task via Saliency maps. These allow us to understand which features the model expects to perform and assign a class to an input image.\nWe use two saliency map methods that yield different insights into the model. First, the classical Gradient method computes the output gradient concerning the input pixels , focusing more on border-like information such as blood vessels . Second, we employed GradCAM to understand the focus of the representations in the last feature map of the encoder part of the model, given that GradCAM provides a more holistic set of regions of interest . We do not use Occlusion methods since these are very computationally intensive .\n shows a few input images and the corresponding saliency maps obtained with Gradient and GradCAM, for both models and with samples of the Gradable class. The Gradient method focuses on smaller blood vessels of the fundus image for both models. This is expected since this method typically focuses on low-level features . However, in the FGR-Net case, the saliency map better highlights these blood vessels more consistently.\nOn the other hand, GradCAM focuses on more medically relevant structures, such as the optic disk, as well as the main blood vessel region. However, in the case of the Normal model based on the classification network, there is an unexpected and hard-to-explain focus on the upper-right border of the fundus disk, which is not a medically relevant region. shows the model focuses on the same border highlighted in  but now for samples of the Ungradable class. Again, the Normal model focuses on more irrelevant areas, especially the top-right border. On the other hand, the autoencoder model's GradCAM saliency map shows it focuses on relevant eye regions.\nWhile these examples illustrate, they focus on single samples and do not represent the whole complexity of the model's encoding. The supplementary materials (Appendix 2) contain a detailed analysis of saliency maps averaged over a large set of examples and also through unsupervised learning, which supports the previous findings."}, {"title": "5.8.2. Adversarial examples", "content": "Adversarial examples can be used to understand the robustness and vulnerability of a model against attacks. In general, attacks are perturbations of the input that are considered intentional and can fool models. However, in this case it would be helpful to consider inadvertent transformations that may perturb the fundus images and thus affect model performance, such as new equipment, calibration, or capture technique.\n shows how two typical fundus images of a class can be transformed into an image of another class via the white-box gradient-based adversarial attack Projected Gradient Descent (PGD) . We used a targeted PGD with 20 steps, a step size of 0.01, and a radius of 0.13 for all images and targets. In this case, we only focus on the FGR-Net model since both models have similar results with these techniques. Finally, we chose images representative of each class but not ideally classified by the model (around 0.95 model probability). In this way, the images are representative of their class. Still, at the same time, PGD can yield a perturbation that can make a gradable image more so in a way that is significant (i.e., can be visualized), and the same for the ungradable class.\nIn all cases, the figure shows the general tendency of the model to prefer backgrounds that are not as black as the inputs (blue values around the borders). This behavior is interesting since the dataset's images had their background segmented out and are, therefore, purely black at the edges, so the data distribution should not be centered on bright backgrounds. This suggests that the encoding of the classes is relative in terms of brightness since the brightness of the fundus is also increased in general. Besides, PGD tends to slightly blur the image while increasing its brightness, washing out the colors, and lowering the contrast. Still, at the same time, it adds additional blood vessels that were non-existent in the original image.\nWe can see that to transform images into the Ungradable class (rows 2 and 4), PGD tends to add small blood vessels, both for the image that was originally gradable as for the Ungradable case. In turn, to convert images to the gradable class, the model through PGD adds wider blood vessels in more specific locations and is more selective concerning smaller blood vessels (rows 1 and 3).\nThis strategy corresponds with the previous saliency analysis using the Gradient method, indicating that blood vessels are a vital factor in the classification. However, surprisingly, the smaller vessels' location is not essential to the recognition.\nOn the other hand, the figures show that for Ungradable images (rows 3 and 4), PGD almost completely ignores the optic disk region. This is not the case for the gradable images (rows 1 and 2), where PGD preserves (white color) the original image, although it produces a slight blurring effect.\nFinally, it is also interesting that for the Ungradable image, in both cases (rows 3 and 4) the PGD's strategy includes darkening the"}, {"title": "5.8.3. Performance evaluation for real-time feedback", "content": "Visualizations of the features on which the model focuses can help medical practitioners and technicians validate the quality in acquiring fundus images. With the advent of mobile fundus photography, real-time feedback on the quality of the fundus images can enable the acquisition of high-quality images with low-cost devices. Therefore, we evaluated the performance of each of the visualization methods on the proposed model. Since the decoder part is not used in the prediction, both the FGR-NET and the Normal model based on a classifier network have the same performance.\nWe tested both models on GPUs, NVIDIA GeForce 1050TI with 4 GB of RAM and NVIDIA Pascal X Titan with 12 GB of RAM, on an Intel i7 CPU with 16 GB of RAM. We measured the mean computation time over 50 runs. Each run consisted of a batch with a single sample to reflect a real-time setting.\n shows the computation time for the various methods. The latency for image prediction (row 1) is included as a baseline. In all cases, the coefficient of variation was lower than 0.004 (i.e., unbiased estimator). As  shows, the Pascal X Titan is capable of real-time performance, achieving 38 fps (26 ms) for the Gradient method and 25 fps (39 ms) for the GradCAM. The 1050ti achieves 9 fps (115 ms) and 6 fps (171 ms) for those methods. Given that these timings were obtained and measured using a stock PyTorch implementation, with single precision and without any optimizations, this performance can be improved with Captum's stock implementation of the interpretability measures.\nAdditionally, we included the performance of the Occlusion method, whose visualizations were not considered in the analysis because it is very computationally intensive."}, {"title": "6. Conclusions", "content": "This work proposed a deep learning model, FGR-Net, combining autoencoder and multi-layer classifier networks for predicting the gradability of retinal fundus images. The autoencoder consists of two networks: encoder and decoder. The autoencoder network is used to reconstruct the input fundus image. Our model also includes a multi-layer classifier fed by features extracted from the encoder network to rank the gradability of the fundus image as gradable or ungradable. FGR-Net's learning approach combines the cross-entropy loss function based on supervised learning and self-supervised learning by comparing the reconstructed image to the target image (i.e., the input image). The FGR-Net model based on the VGG16 backbone as the base of the encoder network and using the MSE as a reconstruction loss function achieved an overall accuracy of 0.8947, precision of 0.8800, recall of 0.8765, and F1-score of 0.8782. Our model outperformed the state-of-the-art retinal gradability assessment in the two-class (gradable and ungradable) and three-class (Good, Usable and Reject) tasks. The FGR-Net model can correctly identify the visual features of eye image gradability for a more precise grading system.\nIn addition, based on interpretability analysis, we show that the FGR-Net model mainly focuses on the presence and type of blood vessels in the fundus images via the use of three interpretability methods. FGR-Net showed that other vital structures, such as the optic disk and macula, play a lesser role than expected in the gradability of fundus image. The interpretability analysis also found that the addition of the decoder and reconstruction loss helps the FGR-Net model focus more on relevant structures of the fundus image. We also evaluate the computational cost of each interpretability method to determine their ability to run in a real-time context to improve feedback during image acquisition. Our results showed that inexpensive consumer-grade GPUs could provide acceptable performance for real-time computation, even with un-optimized model and interpretability implementations. FGR-Net can generate more interest in the biomedical community to improve the performance of retinal image gradability assessment tasks, which play an essential role in applications such as retinal image segmentation and automatic disease diagnosis.\nFuture work aims to use the developed assessment model to improve the accuracy of classifying and grading eye diseases (e.g., Diabetic Retinopathy) based on interpretation models."}, {"title": "Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "Data availability", "content": "Data will be made available on request."}, {"title": "Acknowledgment", "content": "This publication has received support from the research project RetinaReadRisk, with EIT Health and Horizon Europe funding under grant agreement 220718."}]}