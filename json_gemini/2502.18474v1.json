{"title": "A Contemporary Survey of Large Language Model Assisted Program Analysis", "authors": ["Jiayimei Wang", "Tao Ni", "Wei-Bin Lee", "Qingchuan Zhao"], "abstract": "The increasing complexity of software systems has driven significant advancements in program analysis, as traditional methods unable to meet the demands of modern software development. To address these limitations, deep learning techniques, particularly Large Language Models (LLMs), have gained attention due to their context-aware capabilities in code comprehension. Recognizing the potential of LLMs, researchers have extensively explored their application in program analysis since their introduction. Despite existing surveys on LLM applications in cybersecurity, comprehensive reviews specifically addressing their role in program analysis remain scarce. In this survey, we systematically review the application of LLMs in program analysis, categorizing the existing work into static analysis, dynamic analysis, and hybrid approaches. Moreover, by examining and synthesizing recent studies, we identify future directions and challenges in the field. This survey aims to demonstrate the potential of LLMs in advancing program analysis practices and offer actionable insights for security researchers seeking to enhance detection frameworks or develop domain-specific models.", "sections": [{"title": "I. INTRODUCTION", "content": "With the continuous advancement of information technology, software plays an increasingly significant role in daily life, making its quality and reliability a critical concern for both academia and industry [1]. This is because software vulnerabilities in domains such as finance, healthcare, critical infrastructure, aerospace, and cybersecurity [2] can lead to considerable financial losses or even societal harm [3]. Examples include data breaches in financial systems, malfunctioning medical devices, disruptions to power grids, failures in aviation control systems, and exploitation of security loopholes in sensitive government networks. Accordingly, many techniques have been proposed to detect such vulnerabilities that compromise software quality and reliability, and program analysis has been proven effective in such tasks. It aims to examine compter programs to identify or verify their properties to detect vulnerabilities through abstract interpretation, constraint solving, and automated reasoning [4].\nHowever, as software complexity and scale increase, traditional program analysis methods encounter challenges in meeting the demands of contemporary development. Specifically, these traditional methods face substantial challenges in handling dynamic behaviors, cross-language interactions, and large-scale codebases [5], [6]. Fortunately, recent advancements in machine learning have initiated a shift in program analysis [7] and shed light on a promising research direction to address the limitations of traditional program analysis methods.\nIn particular, the literature has attempted to combine deep learning with program analysis, applying it to strengthen the detection of vulnerabilities and achieve automated code fixes, thereby minimizing human intervention and increasing precision [8]. However, deep learning models lack the ability to effectively integrate contextual information over long sequences, limiting their performance in tasks requiring deep reasoning or multi-turn understanding [9], [10]. Consequently, these models struggle to handle complex software and large codebases and lack the capability for cross-project analysis.\nFortunately, the most recent advancement, i.e., large language model (LLM), has been found promising in addressing the limitations of early deep learning models, such as constrained contextual understanding and generalization, enabling them to handle tasks across multiple domains with greater versatility [11]\u2013[15]. Particularly, as for program analysis, LLMs surpass traditional deep learning methods and have been applied to various tasks [16]\u2013[20], including automated vulnerability and malware detection, code generation and repair, and providing scalable solutions that integrate static and dynamic analysis methods. Moreover, it also shows a great potential to cope with the growing difficulty of analyzing modern software systems.\nThough promising, the literature lacks a comprehensive and systematica view of LLM-assisted program analysis given the presence of numerous related attempts and applications. Therefore, this work aims to systematically review the state-of-the-art of LLM-assisted program analysis applications and specify its role in the development of program analysis. To this end, we systematically review the use of LLMs in program analysis and organized them into a structured taxonomy. Unlike previous surveys that broadly examined the applications of LLMs in cybersecurity, our work narrows its focus to program analysis, delivering a more detailed and domain-specific exploration. In addition, we collect the limitations mentioned in selected studies and analyze the improvements brought by the integration of LLMs, and specify the potential challenges and future research directions of LLMs in this domain.\nThe survey is organized as follows. We first introduce the background of program analysis and large language model in \u00a7 II. We then examine the application of LLMs in static analysis in \u00a7 III and discusses the use of LLMs in dynamic analysis in \u00a7 IV. We next explore how LLMs assist hybrid approaches that combine static and dynamic analysis in \u00a7 V. We finally address the challenges of applying LLMs to program analysis and outline potential future research directions in \u00a7 VI and conclude the survey in \u00a7 VII."}, {"title": "II. BACKGROUND", "content": "In this section, we first introduce prior knowledge about program analysis (\u00a7 II-A), including static analysis and dynamic analysis and the limitations in existing approaches, and then present the concepts of LLMs as well as the necesseity of leveraging LLMs for advancing program analysis (\u00a7 II-B).\nA. Program Analysis\nProgram analysis is the process of analyzing the behavior of computer programs to learn about their properties [21]. Program analysis can find bugs or security vulnerabilities, such as null pointer dereferences or array index out-of-bounds errors. It is also used to generate software test cases, automate software patching and improve program execution speed through compiler optimization. Specifically, program analysis can be categorized into two main types: static analysis and dynamic analysis [22]. Static analysis examines a program's code without execution, dynamic analysis collects runtime information through execution, and hybrid analysis combines both approaches for comprehensive results.\nStatic Analysis. Static analysis (a.k.a. compile-time analysis) is a program analysis approach that identifies program properties by examining its source code without executing the program. The pipeline for static analysis consists of key stages. The process begins with parsing the code to extract essential structures and relationships, which are transformed into intermediate representations (IRs) such as symbol tables, abstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs (DFGs). These IRs are then analyzed to detect issues such as unreachable code, data dependencies, and syntactic errors. These series of processes ultimately enhance code quality and reliability.\nDynamic Analysis. Dynamic analysis (a.k.a. runtime analysis) is a program analysis approach that uncovers program properties by repetitively executing programs in one or more runs [23]. The stages involved in dynamic analysis are depicted. These stages include instrumenting the source code to enable runtime tracking, compiling the instrumented code into a binary, and executing it with test suites. After completing the above steps, program traces such as function calls, memory accesses and system calls are captured.\nB. Large Language Models\nLarge Language Models (LLMs) are large-scale neural networks built on deep learning techniques, primarily utilizing the Transformer architecture [24]. Transformer models utilize self-attention mechanism to identify relationships between elements within a sequence, which enables them to outperform other machine learning models in understanding contextual relationships. Trained on vast datasets, LLMs learn syntax, semantics, context, and relationships within language, enabling them to generate and comprehend natural language [25]. Furthermore, LLMs possess knowledge reasoning capabilities, allowing them to retrieve and synthesize information from large datasets to answer questions involving common sense and factual knowledge.\nThe architecture and configuration features of LLMs (e.g., model families, parameter size, and context window length) collectively determine their capabilities, performance and applicability. The studies selected in this survey involve LLM model families such as LLaMA [26], CodeLLaMA [27] and GPT [28], [29]. The parameter size of a large model typically refers to the number of variables used for learning and storing knowledge. The parameter size represents a model's learning capacity, indicating its ability to capture complexity and detail from data. Generally, larger parameter sizes enhance the model's expressive power, enabling it to learn more intricate patterns and finer details. The context window refers to the range of text fragments a model uses when generating each output. It determines the amount of contextual information the model can reference during generation. Selecting appropriate architectures and configurations for LLMs in different scenarios is crucial for optimizing their performance."}, {"title": "III. LLM FOR STATIC ANALYSIS", "content": "Static analysis examines various objects, such as analyzing vulnerabilities and detecting malware in source code binary executables. Analyzing vulnerabilities in source code requires techniques like dependency analysis and taint tracking to trace the flow of sensitive data. On the other hand, Detecting malware focuses on control flow examination and behavior modeling to identify malicious patterns. Consequently, LLM assistance differs by program type and analysis purpose, which will be discussed in this section across four directions: (i) vulnerability detection (\u00a7 III-A), (ii) malware detection (\u00a7 III-B), (iii) program verification (\u00a7 III-C), and (iv) static analysis enhancement (\u00a7 III-D).\nA. LLM for Vulnerability Detection\nVulnerability detection focuses on identifying potential security risks or weaknesses in software through automated tools and techniques, which demand precise code analysis and a deep understanding of program behavior [47], [48]. Leveraging their advanced contextual comprehension, LLMs can analyze both semantic and syntactic patterns in source code, providing actionable suggestions and remediation strategies for addressing vulnerabilities. As a result, integrating LLMs into vulnerability detection has become a prominent application in program analysis.\nTo provide a clearer understanding of LLM applications in vulnerability detection, summarizes the intermediate representations (IRs) utilized and the specific vulnerability types addressed in selected studies. Figure 4 offers a visual overview of LLM integration at various stages, highlighting their roles in contextual understanding, feature extraction, enhanced detection accuracy, and remediation strategies. These capabilities enable efficient and precise identification of OS-level and application-level vulnerabilities. Additionally, a detailed comparison of the best-performing LLMs in the reviewed studies reveals key factors influencing their effectiveness and adoption.\nOS-level Vulnerability. OS-level vulnerabilities refer to security flaws within critical components of an operating system, such as the kernel, system libraries, or device drivers. These vulnerabilities can compromise the stability and security of the entire system, allowing attackers to gain unauthorized access, disrupt operations, or cause system-wide failures affecting all running applications. Common examples include memory management errors, privilege escalation, and resource misuse. Leveraging LLMs, tools like the LLift framework [30] address challenges such as path sensitivity and scalability in detecting OS-level vulnerabilities. By combining constraint-guided path analysis with task decomposition, LLift improves the detection of issues like use-before-initialization (UBI) in large-scale codebases. Ye et al. [31] developed SLFHunter, which integrates static taint analysis with LLMs to identify command injection vulnerabilities in Linux-based embedded firmware. The LLMs are utilized to analyze custom dynamically linked library functions and enhance the capabilities of traditional analysis tools. Furthermore, Liu et al. [32] proposed a system called LATTE, which combines LLMs with binary taint analysis. The code slicing and prompt construction modules serve as the core of LATTE, where dangerous data flows are isolated for analysis. These modules reduce the complexity for LLMs by providing context-specific input, allowing improved efficiency and precision in vulnerability detection through tailored prompt sequences that guide the LLM in the analysis process. In addition, Liu et al. [33] proposed a system for detecting kernel memory bugs using a novel heuristic called Inconsistent Memory Management Intentions (IMMI). The system detects kernel memory bugs by summarizing memory operations and slicing code related to memory objects. It uses static analysis to infer inconsistencies in memory management responsibilities between caller and callee functions. LLMs assist in interpreting complex memory management mechanisms and enable the identification of bugs such as memory leaks and use-after-free errors with improved precision.\nApplication-level Vulnerability. Application-level vulnerabilities are security weaknesses found within individual software programs. These vulnerabilities can compromise the application's performance, data integrity, or user privacy. However, they typically do not affect the overall stability of the operating system. Common examples include input validation issues, logic errors, and misconfigurations. These vulnerabilities can result in unauthorized access or data breaches, as well as application-specific security incidents [49]\u2013[55].\nTo address the challenges in application-level vulnerability detection, Wang et al. [34] introduced the Conformer mechanism, which integrates self-attention and convolutional networks to capture both local and global feature patterns. To further refine the detection process, they optimize the attention mechanism to reduce noise in multi-head attention and improve model stability. By combining structural information processing, pre-trained models, and the Conformer mechanism in a multi-layered framework, the approach improves detection accuracy and efficiency. Building on these advancements, IRIS [35] proposes a neuro-symbolic approach that combines LLMs with static analysis to support reasoning across entire projects. The static analysis is responsible for extracting candidate sources and sinks, while the LLM infers taint specifications for specific CWE categories. Similarly, Cheng et al. [36] combined semantic-level code clone detection with LLM-based vulnerability feature extraction. By integrating program slicing techniques with the LLM's semantic understanding, they refined vulnerability feature detection. This approach addresses the limitations of traditional syntactic-based analysis.\nMao et al. [37] implemented a multi-role approach where LLMs act as different roles, such as testers and developers, simulating interactions in a real-life code review process. This strategy fosters discussions between these roles, enabling each LLM to provide distinct insights on potential vulnerabilities. MSIVD [38] introduces a multi-task self-instructed fine-tuning technique that combines vulnerability detection, explanation, and repair, improving the LLM's ability to understand and reason about code through multi-turn dialogues. Additionally, the system integrates LLMs with a data flow analysis-based GNN, which models the program's control flow graph to capture variable definitions and data propagation paths. This enables the model to rely not only on the literal information in the code but also on the program's graph structure for more precise detection. Similarly, GPTScan [39] demonstrates how GPT can be applied to code understanding and matching scenarios, reducing false positives and uncovering new vulnerabilities previously missed by human auditors.\nIn the domain of IoT software, Yang et al. [40] explored the application of LLMs combined with static code analysis for detecting vulnerabilities. By leveraging prompt engineering, LLMs enhance the efficiency of vulnerability detection and reduce costs, ultimately improving scalability and feasibility in large IoT systems [57]\u2013[59]. Meanwhile, Xiang et al. [46] proposed LuaTaint, a static analysis framework designed to detect vulnerabilities in the web configuration interfaces of IoT devices. LuaTaint integrates flow-, context-, and field-sensitive static taint analysis with key features such as framework-specific adaptations for the LuCI web interface and pruning capabilities powered by GPT-4. By converting Lua code into ASTs and CFGs, the framework performs precise taint analysis to identify vulnerabilities like command injection and path traversal. The system uses dispatching rules and LLM-powered alarm pruning to improve detection precision, reduce false positives, and efficiently analyze firmware across large-scale datasets.\nMohajer et al. [42] presented SkipAnalyzer, a tool that employs LLMs for bug detection, false positive filtering, and patch generation. By improving the precision of existing bug detectors and automating patching, this approach significantly reduces false positives and ensures accurate bug repair. Meanwhile, Zhang et al. [44] introduced tailored prompt engineering techniques with GPT-4 [29], leveraging auxiliary information such as API call sequences and data flow graphs to provide structural and sequential context. This approach also employs chain-of-thought prompting to enhance reasoning capabilities, demonstrating improved accuracy in detecting vulnerabilities across Java and C/C++ datasets. Extending the application of LLMs in decentralized applications and smart contract analysis, Yang et al. [43] developed HYPERION, which combines LLM-based natural language analysis with symbolic execution to address inconsistencies between DApp descriptions and smart contracts. The system integrates a fine-tuned LLM to analyze front-end descriptions, while symbolic execution processes contract bytecode to recover program states, effectively identifying discrepancies that may undermine user trust.\nFor smart contract vulnerability detection, Hu et al. [45] introduced GPTLENS, a two-stage adversarial framework leveraging LLMs. GPTLENS assigns two synergistic roles to LLMs: an auditor generates a diverse set of vulnerabilities with associated reasoning, while a critic evaluates and ranks these vulnerabilities based on correctness, severity, and profitability. This open-ended prompting approach facilitates the identification of a broader range of vulnerabilities, including those that are uncategorized or previously unknown. Experimental results on real-world smart contracts show that GPTLENS outperforms traditional one-stage detection methods while maintaining low false positive rates. Focusing on Android security and software bug detection, Mathews et al. [41] introduced LLbezpeky, an AI-driven workflow that assists developers in detecting and rectifying vulnerabilities. Their approach analyzed Android applications, achieving over 90% success in identifying vulnerabilities in the Ghera benchmark."}, {"title": "B. LLM for Malware Detection", "content": "Malware detection determines whether a program has malicious intent and is an essential aspect of program analysis research. Initially, signature-based detection methods were predominantly used. As malware evolved, new detection techniques emerged, including behavior-based detection, heuristic detection, and model checking approaches. Data mining and machine learning algorithms soon followed, further enhancing detection capabilities [62]\u2013[67].\nTraditional malware detection methods struggle with challenges like obfuscation and polymorphic malware. LLMs offer a new approach to enhance detection accuracy and adapt to evolving threats by analyzing code semantics and patterns. Fujii et al. [68] utilized decompiled and disassembled outputs of the Babuk ransomware as inputs to the LLM to generate function descriptions through carefully designed prompts. The generated descriptions were evaluated using BLEU [69] and ROUGE [70] metrics to measure functional coverage and agreement with analysis articles. Additionally, Simion et al. [71] evaluated the feasibility of using out-of-the-box open-source LLMs for malware detection by analyzing API call sequences extracted from binary files. The study benchmarked four open-source LLMs (Llama2-13B, Mistral [72], Mixtral, and Mixtral-FP16 [73]) using API call sequences extracted from 20,000 malware and benign files. The results showed that the models, without fine-tuning, achieved low accuracy and were unsuitable for real-time detection. These findings highlight the need for fine-tuning and integration with traditional security tools.\nAnalyzing malicious behaviors to detect malware is another approach. Zahan et al. [74] employed a static analysis tool named CodeQL [75] to pre-screen npm packages. This step filtered out benign files, thereby reducing the number of packages requiring further investigation. Following this step, they utilized GPT-3 and GPT-4 models to analyze the remaining JavaScript code for detecting complex or subtle malicious behaviors. The outputs from the LLMs were refined iteratively. Accuracy improved through continuous adjustments to the model's focus based on feedback and re-evaluation.\nOther studies focus on applying LLMs specifically to Android malware detection. Khan et al. [76] extracted Android APKs to obtain source code and opcode sequences, constructing call graphs to represent the structural relationships between functions. Models such as CodeBERT [77] and GPT were employed to generate semantic feature representations, which were used to annotate the nodes in the call graphs. The graphs were enriched with structural and semantic information. These enriched graphs were then processed through a graph-based neural network to detect malware in Android applications. Zhao et al. [78] first extracted features from Android APK files using static analysis, categorizing them into permission view, API view, and URL & uses-feature view. A multi-view prompt engineering approach was applied to guide the LLM in generating textual descriptions and summaries for each feature category. The generated descriptions were transformed into vector representations, which served as inputs for a deep neural network (DNN)-based classifier to determine whether the APK was malicious or benign. Finally, the LLM produced a diagnostic report summarizing the potential risks and detection results."}, {"title": "C. LLM for Program Verification", "content": "Automated program verification employs tools and algorithms to ensure that a program's behavior aligns with predefined specifications, enhancing both software reliability and security. Traditional verification methods often require substantial manual effort, particularly for writing specifications and selecting strategies. These processes are often complex and prone to errors, especially in large-scale systems. In contrast, automated verification generates key elements such as invariants, preconditions, and postconditions, using techniques like static analysis and model checking to ensure correctness. The integration of LLMs further enhances this process by enabling the automatic analysis of code features and the efficient selection of verification strategies. This reduces manual intervention and significantly accelerates verification. Consequently, automated program verification has evolved into a more efficient and reliable method for ensuring software quality. This subsection introduces diverse applications of LLMs in program verification, highlighting their role in automating and enhancing critical tasks.\nAutomated Program Verification. The inputs in these studies can be categorized into four types: (i) Code, which includes program implementations or snippets used for analysis or synthesis. (ii) Specifications, referring to formal descriptions of program behavior, such as preconditions, postconditions, or logical formulas. (iii) Formal methods, encompassing mathematical constructs like theorems, proofs, and loop invariants for ensuring correctness. (iv) Error and debugging information, such as counterexamples, type hints, or failed code generation cases that aid in resolving programming issues.\nProof Generation. Proof generation in program verification automates the creation of formal proofs to ensure program correctness, logical consistency, and compliance with specifications. This process reduces the need for manual effort and enhances verification efficiency by streamlining complex proof tasks. Kozyrev et al. [79] developed CoqPilot, a VSCode plugin that integrates LLMs such as GPT-4, GPT-3.5, LLaMA-2 [26], and Anthropic Claude [93] with Coq-specific tools like CoqHammer [94] and Tactician [95] to automate proof generation in the Coq theorem prover. The authors implemented premise selection for better LLM prompting and created an LLM-guided mechanism that attempted fixing failing proofs with the help of the Coq's error messages. Additionally, Zhang et al. [80] developed the Selene framework to automate proof generation in software verification using LLMs. The framework is built on the industrial-level operating system microkernel [96], seL4 [97], and introduces the technique of lemma isolation to reduce verification time. Its key contributions include efficient proof validation, dependency augmentation, and showcasing the potential of LLMs in automating complex verification tasks.\nInvariant Generation. Invariant generation identifies properties that remain true during program execution, providing a logical foundation for verifying correctness and analyzing complex iterative structures like loops and recursion.\nSome studies have explored various ways to leverage LLMs for generating and ranking loop invariants. Jan\u00dfen et al. [82] investigated the utility of ChatGPT in generating loop invariants. The authors used ChatGPT to annotate 106 C programs from the SV-COMP Loops category [98] with loop invariants written in ACSL [99], evaluating the validity and usefulness of these invariants. They integrated ChatGPT with the Frama-C [100] interactive verifier and the CPAchecker [101] automatic verifier to assess how well the generated invariants enable these tools to solve verification tasks. Results showed that ChatGPT can produce valid and useful invariants for many cases, facilitating software verification by augmenting traditional methods with insights provided by LLMs. Additionally, Chakraborty et al. [81] observed that employing LLMs in a zero-shot setting to generate loop invariants often led to numerous attempts before producing correct invariants, resulting in a high number of calls to the program verifier. To mitigate this issue, they introduced iRank, a re-ranking mechanism based on contrastive learning, which effectively distinguishes correct from incorrect invariants. This method significantly reduces the verification calls required, improving efficiency in invariant generation.\nBesides, Pei et al. [85] explored using LLMs to predict program invariants that were traditionally generated through dynamic analysis. By fine-tuning LLMs on a dataset of Java programs annotated with invariants from the Daikon [102] dynamic analyzer, they developed a static analysis-based method using a scratchpad approach. This technique incrementally generates invariants and achieves performance comparable to Daikon without requiring code execution. It also provides a static and cost-effective alternative to dynamic analysis.\nIntegrating LLMs with Bounded Model Checking (BMC) has shown potential in enhancing loop invariant generation. Pirzada et al. [83] proposed a modification to the classical BMC procedure that avoids the computationally expensive process of loop unrolling by transforming the CFG. Instead of unrolling loops, the framework replaces loop segments in the CFG with nodes that assert the invariants of the loop. These invariants are generated using LLMs and validated for correctness using a first-order theorem prover. This transformation produces loop-free program variants in a sound manner, enabling efficient verification of programs with unbounded loops. Their experimental results demonstrate that the resulting tool, ESBMCibmc, significantly improves the capability of the industrial-strength software verifier ESBMC [103], verifying more programs compared to state-of-the-art tools such as SeaHorn [104] and VeriAbs [105], including cases these tools could not handle.\nWu et al. [84] proposed LaM4Inv, a framework that integrates LLMs with BMC to improve this process. The framework employs a 'query-filter-reassemble' pipeline. LLMs generate candidate invariants, BMC filters out incorrect predicates, and valid predicates are iteratively refined and reassembled into invariants.\nAutomated Program Verification. Automating program specification presents challenges such as handling programs with complex data types and code structures. To address these issues, Wen et al. [86] introduced an approach called AutoSpec. Driven by static analysis and program verification, AutoSpec uses LLMs to generate candidate specifications. Programs are decomposed into smaller components to help LLMs focus on specific sections. The generated specifications are iteratively validated to minimize error accumulation. This process enables AutoSpec to handle complex code structures, such as nested loops and pointers, making it more versatile than traditional specification synthesis techniques. Wu et al. [87] introduced the LEMUR framework. In this hybrid system, LLMs generate program properties like invariants as sub-goals, which are then verified and refined by reasoners such as CBMC [106], ESBMC [103] or UAUTOMIZER [107]. The framework is based on a sound proof system, thus ensuring correctness when LLMs propose incorrect properties. An oracle-based refinement mechanism improves these properties, enabling LEMUR to enhance efficiency in verification and handle complex programs more effectively than traditional tools. Additionally, Mukherjee et al. [88] introduced SynVer, a framework that integrates LLMs with formal verification tools for automating the synthesis and verification of C programs. SynVer takes specifications in Separation Logic, function signatures, and input-output examples as input. It leverages LLMs to generate candidate programs and uses SepAuto, a verification backend, to validate these programs against the specifications. The framework prioritizes recursive program generation, reducing the dependency on manual loop invariants and improving verification success rates.\nOthers. Other applications of LLMs in program verification include smart contract verification, symbolic execution, strategy selection and error specification inference. For instance, Liu et al. [89] developed a novel framework named PropertyGPT, leveraging GPT-4 to automate the generation of formal properties such as invariants, pre-/post-conditions, and rules for smart contract verification. The framework embeds human-written properties into a vector database and retrieves reference properties for customized property generation, ensuring their compilation, appropriateness, and runtime verifiability through iterative feedback and ranking. Similarly, Wang et al. [90] introduced an iterative framework named LLM-Sym. This tool leverages LLMs to bridge the gap between program constraints and SMT solvers. The process begins by extracting control flow paths, performing type inference, and iteratively generating Z3 [108] code to solve path constraints. A notable feature of LLM-Sym is its self-refinement mechanism, which utilizes error messages to debug and enhance the generated Z3 code. If the code generation process fails, the system directly employs LLMs to solve the constraints. Once constraints are resolved, Python test cases are automatically generated from Z3's outputs.\nAnother approach [91] automates the selection of verification strategies to overcome limitations of traditional tools like CPAchecker [101]. These tools often require users to manually select strategies, making the process more complex and time-consuming. LLMs analyze code features to identify suitable strategies, streamlining the verification process and minimizing user input. This automation not only improves efficiency but also minimizes reliance on expert knowledge. Additionally, Chapman et al. [92] proposed a method that combines static analysis with LLM prompting to infer error specifications in C programs. Their system queries the LLM when static analysis encounters incomplete information, enhancing the accuracy of error specification inference. This approch is effective for third-party functions and complex error-handling paths."}, {"title": "D. LLM for Static Analysis Enhancement", "content": "Beyond the previously mentioned applications of LLMs, other studies focus on leveraging LLMs to assist in certain processes of static analysis.\nCode Review Automation. Lu et al. [109] proposed LLaMA-Reviewer, a model that leverages LLMs to automate code review. It incorporates instruction-tuning of a pre-trained model and employs Parameter-Efficient Fine-Tuning techniques to minimize resource requirements. The system automates essential code review tasks, including predicting review necessity, generating comments, and refining code.\nCode Coverage Prediction. Dhulipala et al. [110] introduced CodePilot, a system that integrates planning strategies and LLMs to predict code coverage by analyzing program control flow. CodePilot first generates a plan by analyzing program semantics, dividing the code into steps derived from control flow structures, such as loops and branches. Subsequently, CodePilot adopts either a single-prompt approach (Plan+Predict in one step) or a two-prompt approach (planning first, followed by coverage prediction). These approaches guide LLMs to predict which parts of the code are likely to be executed based on the formulated plan.\nDecompiler Optimization. Hu et al. [111] proposed DeGPT, a framework designed to enhance the clarity and usability of decompiler outputs for reverse engineering tasks. DeGPT begins by analyzing the raw output of decompilers, identifying issues such as ambiguous variable names, missing comments, and poorly structured code. The framework leverages LLMs in three distinct roles:Referee, Advisor, and Operator to propose and implement optimizations while preserving semantic correctness.\nExplainable Fault Localization. Yan et al. [112] proposed CrashTracker, a hybrid framework that combines static analysis with LLMs. This approach improves the accuracy and explainability of crashing fault localization in framework-based applications. CrashTracker introduces Exception-Thrown Summaries (ETS) to represent fault-inducing elements in the framework. It also uses Candidate Information Summaries (CIS) to extract relevant contextual information for identifying buggy methods. ETS models are employed to identify potential buggy methods. LLMs then generate natural language fault reports based on CIS data, enhancing the clarity of fault explanations. CrashTracker demonstrates state-of-the-art performance in precision and explainability when applied to Android applications.\nExtract Method Refactoring. Pomian et al. [113] introduced EM-Assist, a tool that combines LLMs and static analysis to enhance Extract Method (EM) refactoring in Java and Kotlin projects. EM-Assist uses LLMs to generate EM refactoring suggestions and applies static analysis to discard irrelevant or impractical options. To improve the quality of suggestions, the tool employs program slicing and ranking mechanisms to prioritize refactorings aligned with developer preferences. EM-Assist automates the entire refactoring process by leveraging the IntelliJ IDEA platform to safely implement changes.\nObfuscated Code Disassembly. Rong et al. [114] introduced DISASLLM, a framework that combines traditional disassembly techniques with LLMs. The LLM component validates disassembly results and repairs errors in obfuscated binaries, enhancing the quality of the output. Through batch processing and GPU parallelization, DISASLLM achieves substantial improvements in both the accuracy and speed of decoding obfuscated code, outperforming state-of-the-art methods\nPrivilege Variable Detection. Wang et al. [115] presented a hybrid workflow that combines LLMs with static analysis to detect user privilege-related variables in programs. The program is first analyzed to identify relevant variables and their data flows, which provides an initial set of potential user privilege-related variables. The LLM is used to evaluate these variables by understanding their context and scoring them based on their relationship to user privileges.\nStatic Bug Warning Inspection. Wen et al. [116] proposed LLM4SA, a framework that integrates LLMs with static analysis tools to automatically inspect large volumes of static bug warnings. LLM4SA first extracts bug-relevant code snippets using program dependence traversal. It then formulates customized prompts with techniques such as Chain-of-Thought reasoning and few-shot learning. To ensure precision, the framework applies pre- and post-processing steps to validate the results. This approach tackles challenges like token limitations by optimizing input size, reduces inconsistencies in LLM responses through structured prompt engineering, and mitigates false positives via comprehensive validation.\nStatic Analysis Alert Adjudication. Flynn et al. [117] proposed using LLMs to automatically adjudicate static analysis alerts. The system generates prompts with relevant code and alert details, enabling the LLM to classify alerts as true or false positives. To address context window limitations, the system summarizes relevant code and provides mechanisms for the LLM to request additional details or verify its classifications.\nStatic Analysis Enhancement by Pseudo-code Execution. Hao et al. [118] presented E&V, a system designed to enhance static analysis using LLMs by simulating the execution of pseudo-code and verifying the results without needing external validation. It validates the results of the analysis through an automatic verification process that checks for errors and inconsistencies in the pseudo-code execution. This system is particularly useful for tasks like crash triaging and backward taint analysis in large codebases like the Linux kernel."}, {"title": "IV. LLM FOR DYNAMIC ANALYSIS", "content": "Dynamic analysis encompasses profiling and testing. Profiling focuses on understanding program performance by analyzing execution, such as counting statement or procedure executions through instrumentation. Testing aims to make sure the test suites can cover a program. Statement coverage verifies that every statement in the code is executed at least once during testing. Branch, condition, and path coverage evaluate how thoroughly all branches, conditions, and execution paths are tested [23]. This section examines how LLMs enhance dynamic analysis, focusing on (i) malware detection (\u00a7 IV-A) under profiling, (ii) fuzzing (\u00a7 IV-B) and (iii) penetration testing (\u00a7 IV-C) under testing.\nA. LLM for Malware Detection\nAs discussed in \u00a7 III-B, the definition of malware detection is provided. This subsection focuses on using LLMs to analyze runtime data for malware detection. The distinction between static and dynamic analysis depends primarily on the input source. For instance, if API call sequences are captured during program runtime, such as through sandboxes, debuggers, or runtime analysis frameworks, they are classified as dynamic analysis. Conversely, API call sequences extracted through methods like decompilation or disassembly from static files are classified as static analysis.\nYan et al. [119] proposed a dynamic malware detection method that utilizes GPT-4 to generate text representations for API calls, which are an essential feature in dynamic malware analysis. Their method incorporates the innovative use of prompt engineering, allowing GPT-4 to generate highly detailed, context-rich descriptions for each API call in a sequence. These descriptions go beyond simple API names and delve into the specifics of how each API call behaves within the context of the malware's execution. This provides a much deeper understanding of the malware's actions, as opposed to traditional approaches that primarily rely on raw, unprocessed sequences of API calls. After generating these descriptions, the next step in the pipeline involves using BERT to convert the textual descriptions into embeddings. These embeddings encapsulate the semantic information of the API calls and their interactions, thereby forming a high-quality representation of the entire API sequence. These representations are then passed through a CNN, which performs feature extraction and classification. This comprehensive approach addresses several major challenges faced by traditional API-based models.\nSimilarly, Sun et al. [120] developed a framework that uses dynamic analysis and LLMs to generate detailed cyber threat intelligence (CTI) reports. The framework captures syscall execution traces of malware and converts them into natural language descriptions using a Linux syscall transformer. These descriptions are organized into an Attack Scenario Graph (ASG) to preserve essential details and reduce redundancy. Sanchez et al. [121] applied pre-trained LLMs with transfer learning for malware detection. They fine-tuned the models with a classification layer on a dataset of benign and malicious system calls. This approach allows the model to distinguish between normal and malicious behavior while avoiding the need for training from scratch by leveraging pre-trained LLMs."}, {"title": "B. LLM for Fuzzing", "content": "Fuzzing is a technique for automated software testing that inputs randomized data into a program to detect vulnerabilities like crashes", "dimensions": "test case generation, input structure, and program structure. Test case generation can be mutation-based which alters existing inputs, or generation-based which creates new inputs from scratch. Input structure distinguishes smart fuzzing which utilizes input format knowledge, from dumb fuzzing which generates inputs blindly. Program structure analysis classifies fuzzing as black-box, grey-box, or white-box, based on the tester's level of program insight.\nThe use of LLMs for fuzzing is summarized, which highlights the strategies, program structures, LLMs employed, and applications in the studies. Most research utilizing LLMs for fuzzing focuses on greybox fuzzing.\nQiu et al. [123"}]}