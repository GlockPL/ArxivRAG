{"title": "Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs", "authors": ["Xin Zhou", "Ping Nie", "Yiwen Guo", "Haojie Wei", "Zhanqiu Zhang", "Pasquale Minervini", "Ruotian Ma", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": "Retrieval-Augmented Generation (RAG) significantly improved the ability of Large Language Models (LLMs) to solve knowledge-intensive tasks. While existing research seeks to enhance RAG performance by retrieving higher-quality documents or designing RAG-specific LLMs, the internal mechanisms within LLMs that contribute to the effectiveness of RAG systems remain underexplored. In this paper, we aim to investigate these internal mechanisms within the popular Mixture-of-Expert (MoE)-based LLMs and demonstrate how to improve RAG by examining expert activations in these LLMs. Our controlled experiments reveal that several core groups of experts are primarily responsible for RAG-related behaviors. The activation of these core experts can signify the model's inclination towards external/internal knowledge and adjust its behavior. For instance, we identify core experts that can (1) indicate the sufficiency of the model's internal knowledge, (2) assess the quality of retrieved documents, and (3) enhance the model's ability to utilize context. Based on these findings, we propose several strategies to enhance RAG's efficiency and effectiveness through expert activation. Experimental results across various datasets and MoE-based LLMs show the effectiveness of our method.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG, Lewis et al., 2020; Gao et al., 2024; Ding et al., 2024) has shown significant achievements in enhancing Large Language Models (LLMs, Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023). By retrieving relevant documents from external knowledge bases and incorporating them into the context, RAG allows LLMs to access query-relevant and up-to-date information, thereby improving their performance on a variety of knowledge-intensive NLP tasks (Lozano et al., 2023; Kang and Liu, 2023).\nDespite these achievements, RAG faces many challenges (Chen et al., 2024). For instance, long retrieved documents introduce additional inference cost (Xu et al., 2023), irrelevant or erroneous retrieved documents may lead to increased hallucinations (Shi et al., 2023a; Mallen et al., 2023), and LLMs might not effectively utilize related information from context (Xie et al., 2023). Although significant efforts have been made to improve the quality of retrieved documents (Xie et al., 2023; Wang et al., 2023) and train specialized models for RAG (Asai et al., 2024; Lin et al., 2023), there is limited research examining RAG from the perspective of LLMs' internal mechanisms.\nIn this paper, we aim at paying more attention to the internal states of retrieval-augmented LLMs, focusing on Mixture-of-Expert (MoE)-based LLMs (Du et al., 2022; Jiang et al., 2024) whose inner expert activations naturally reveal their internal states. Specifically, MoE-based LLMs comprise a set of experts that are often activated differently depending on the input context. We believe that certain core experts within the models play a vital role in managing specific types of contexts and regulating model behaviors. As a consequence, examining the function of these experts in conjunction with RAG can enhance our understanding of RAG's benefits and provide insights into how it can be further improved to address the aforementioned challenges.\nWe present Contrastive Expert Activation Inspection (CEAI), a simple but effective method for inspecting internal mechanisms of MoE-based LLMs. CEAI works by comparing the activation of experts given contrastive contexts, which are designed to induce opposite model behaviors.\nAs illustrated in Figure 1, CEAI helps discover three types of RAG-related experts that exhibit unique activation patterns, namely cognizant experts, quality experts, and in-context experts. Identified by inspecting activations when the MoE-based LLMs generate correct versus incorrect answers, cognizant experts determine the sufficiency of the model's internal knowledge to response to user queries on its own. Quality experts and in-context experts, on the other hand, assess the quality of retrieved documents and adjust the model's information utilization abilities from the context, respectively.\nBased on these findings, we propose a training-free adaptive RAG method by investigating and manipulating the activation of experts in MoE-based LLMs. Specifically, we let the activation of cognizant experts and quality experts serve as indicators of unnecessary retrieval and low-quality documents. By avoiding unnecessary retrieval and filtering out low-quality documents, the efficiency of RAG can be improved. We additionally enhance the model's ability to utilize contextual information by adjusting the activation of in-context experts. Moreover, we design data recipes and metrics to enhance the comprehensiveness of adaptive RAG evaluation. Experimental results across various datasets show the advantages of our method.\nOur contribution can be summarized as follows:\n\u2022 We propose CEAI, a method for detecting core experts that manage specific context types and model behaviors in MoE-based LLM.\n\u2022 We explore the impact of specific experts to the RAG process, discovering three types of RAG-related experts. These experts help determine knowledge sufficiency, assess the quality of retrieved documents, and enhance the model's ability to utilize context, showing potential for improving RAG.\n\u2022 Based on our findings, we take explicit advantage of these core experts to enhance the effectiveness and adaptivity of RAG with MoE-based LLMs. We verify the effectiveness of our method with comprehensive evaluation."}, {"title": "2 Method", "content": "2.1 Expert Activation in MoE\nThe MoE architecture replaces the Feed-Forward Network (FFN) module with the MoE module in each transformer layer. The MoE module typically consists of a routing network and multiple FFNs, each FFN module acting as an expert. During LLM's forward phase, the routing network selectively feeds each token to the most appropriate experts, and only the selected experts are activated and contribute to the computation. The output of the MoE module is the weighted sum of the activated experts. Formally, given an MoE module with N experts and an input token representation h, the output of the MoE module in i-th layer is:\n$MoE(h) = \\sum_{j=1}^{N} g_{i,j} (h)e_{i,j} (h), \\quad (1)$\nwhere $e_{i,j} (h)$ is the output of the j-th expert in i-th layer, and $g_{i,j} (h)$ is the gating value computed by the routing network. Typically, each token activates the top-k experts per layer, making the remaining $g_{i,j}(h)$ zero. This indicates that those experts are not suitable for the current context.\n2.2 Contrastive Expert Activation Inspection\nSince experts within the MoE are dynamically activated according to the demand of the context, we hypothesize that there are some core experts primarily responsible for specific contexts and model behaviors. To identify these core experts, we propose a straightforward yet effective method, Contrastive Expert Activation Inspection (CEAI), which detects core experts by comparing the expert activation frequency across contrastive scenarios.\nAn overview of CEAI is shown in Figure 2. We define a scenario as a set of input prompts that induce consistent model behavior. Contrastive scenarios include a positive scenario and a negative scenario, representing two types of input prompts designed to elicit opposite model behaviors. For example, a positive scenario can be that input prompt includes external documents, while a negative scenario excludes external documents. MOE LLMs require different abilities to handle these two contrastive scenarios, thus activating different experts to exhibit opposite behaviors. By comparing the activation frequencies of experts, we can exclude general experts that are activated in both scenarios, thereby highlighting the core experts more likely to be activated in their respective scenarios.\nGiven two dataset $D_{pos}$ and $D_{neg}$ representing data from contrastive scenarios, let $h_i = f(X)$ denote the input representation to the i-th layer MoE module for input prompt X. We introduce the concept of activation probability of the j-th expert on the i-th layer for scenario pos as:\n$p_{e_{i,j}}^{pos} = \\frac{1}{|D_{pos}|} \\sum_{X \\in D_{pos}} \\Pi(g_{i,j}(h_i)), \\quad (2)$\nwhere $\\Pi(g_{i,j}(h_i)) \\rightarrow {0,1}$ indicates whether expert $e_{i,j}$ is activated for the $h_i$. We take the last position of input prompt to calculate $h_i$ for all experiments and show explanation in Appendix A. We then introduce the contrastive activation probabilities, which is the difference between activation probabilities in two contrastive scenarios:\n$\\Delta P_{i,j} = P_{e_{i,j}}^{pos} - p_{e_{i,j}}^{neg} \\quad (3)$\nwhere $\\Delta P_{i,j} > 0$ indicates a higher activation probability for expert $e_{i,j}$ in the pos scenario compared to the neg scenario, suggesting that this expert is more responsible for the pos scenario. A negative $\\Delta P_{i,j}$ suggests the opposite.\nExpert Activation Pattern for Classification\nWe can use expert activation as a classifier to predict the type of scenarios, thereby determining the act of model for improving RAG. For example, if the expert activation can predict the internal knowledge of the model is sufficient for the current query, we can avoid unnecessary retrieval and enhance the efficiency of RAG. For this aim, we introduce a Scenario Score for classifying the scenarios based on the activation of core experts. Given contrastive activation probabilities obtained from Equation 3, the scenario score of any input is calculated as:\n$Scenario Score = \\sum_{i=1}^{L} \\sum_{j=1}^{N} \\Delta P_{i,j} \\cdot \\Pi(g_{i,j} (h_i)), \\quad (4)$\nwhere L is the number of layers, N is the number of experts per layer, and $\\Pi(g_{i,j}(h_i))$ is the indicator function same to Equation 2. A positive scenario score indicates a higher inclination towards the positive scenario, while a negative score indicates a tendency towards the negative scenario. This method has several variations. For instance, we can limit the calculation to the top and bottom items in $\\Delta P$, or perform a weighted summation using the values of $e_{i,j}$ and $g_{i,j} (h_i)$. These variations allow for more flexible and refined scenario predictions."}, {"title": "3 Inspecting Core Experts for RAG", "content": "In this section, we employ CEAI to inspect expert activation and discover three types of core experts to address RAG challenges. In \u00a73.2, we discover cognizant experts that indicate the sufficiency of the model's internal knowledge, which can enhance RAG by avoiding unnecessary retrievals. \u00a73.3 covers quality experts that can filter low-quality retrieved documents; In \u00a73.4, we show in-context experts that enhance the model's ability to utilize contextual information from retrieved documents.\n3.1 Experimental Settings\nWe use Mixtral-8x7B-instruct-v0.1 and Mixtral-8x22B-instruct-v0.1 (Jiang et al., 2024) in our experiments, as they stand out as widely used open-source MoE-based LLMs (Xue et al., 2024; Bai et al., 2024). Our investigation is mainly conducted on question-answering datasets including PopQA (Mallen et al., 2023) and RGBqa (Chen et al., 2024), which are commonly utilized for RAG analysis. We randomly select 1,000 samples from the PopQA and use the entire English subset of RGBqa with 300 samples. For retrievers, both PopQA and RGBqa have released their retrieved question-related documents, thus we directly use these officially retrieved documents in our experiments. We instruct the model to directly generate answers and apply CEAI at the first generated token. Greedy decoding is used for all experiments for reproducibility. Due to space constraints, we only introduce the main experimental results, and additional experimental results (e.g., other MoE-based LLMs) and more experimental details are provided in Appendix B.\n3.2 Cognizant Experts\nAlways retrieving external documents is not the optimal solution for RAG (Chen et al., 2024). The retrieved documents introduce additional inference cost, and low-quality retrieved documents can even mislead the LLMs (Shi et al., 2023a). A more reasonable strategy is to retrieve only when the internal knowledge LLM is insufficient to answer the given question (Asai et al., 2024). We hypothesize that expert activation can indicate whether LLM's internal knowledge is sufficient. In this subsection, we discover these cognizant experts with CEAI.\nSetup. We start by defining the contrastive scenarios for knowledge sufficiency. For each sample in the RAG dataset, we input only the question to the LLM and get a response. A positive scenario is that response contains the correct answer, which indicates LLM's knowledge is sufficient for this question. $D_{pos}$ consists of these answerable data. Response without the correct answer is regarded as the negative scenario. $D_{neg}$ consists of unanswerable data. The intuition behind our method is that certain experts specialize in specific types of knowledge and are frequently activated given knowledge-related questions. If the model lacks such knowledge and often answers incorrectly, the frequently activated experts in this scenario indicate model's knowledge is insufficient. We show the additional experimental details of cognizant experts in Appendix C.\nEmpirical Findings. By applying CEAI to the $D_{pos}$ (model answers correctly) and $D_{neg}$ (model answers incorrectly), we get the contrastive activation probability for the cognizant expert and visualize it in Figure 3. We can observe that: (1) there exists a clear distinction between the expert activation probabilities in both answerable and unanswerable scenarios. Such experts are widely present in all data and models, demonstrating the existence of cognizant experts. (2) The cognizant experts differ across datasets. We speculate that each dataset requires different types of knowledge, which are possessed by different experts, resulting in diverse activation of cognizant experts. This may help uncover the knowledge distribution patterns hidden in various tasks and scenarios. (3) For the larger model Mixtral-8x22B, the value of contrastive activation probability is generally higher compared to Mixtral-8x7B. One possible explanation is that the limited number of experts in smaller models forces each expert to share a wider range of knowledge and abilities during pre-training. Conversely, larger models have more experts, allowing for more distinct specialization.\nAnalyses. If our identified cognizant experts are indeed responsible for knowledge sufficiency, their activation should be able to predict whether the model can answer a question correctly. To test this hypothesis, we calculate a scenario score using the identified cognizant experts, which can predict if the model's knowledge is sufficient to answer the given question. We utilize the full set and randomly selected 50-shot subset to identify and search for the best cognizant experts, comparing them with a random guess baseline. Due to the imbalance between knowledge insufficiency and sufficiency data, we choose the F1-Score as the evaluation metric. The results in Table 1 show that, across all settings, cognizant experts outperform random guessing in predicting scenarios. Using the full set to identify cognizant experts leads to the best performance, while 50-shot also achieves impressive performance despite being a small fraction of the full set. With only the 50-shot, the Mixtral-8x7B achieves an absolute improvement of 13.86 percent over the random guessing baseline on the PopQA dataset. This highlights the strong generalization capability of cognizant experts. Overall, these findings support the existence of the cognizant expert and show its potential to avoid unnecessary retrieval to enhance the efficiency of RAG."}, {"title": "3.3 Quality Experts", "content": "A reason why retrieved documents are not always beneficial is that low-quality retrieved documents can mislead LLMs (Shi et al., 2023a). This motivates us to explore whether the expert activation can evaluate the quality of documents.\nSetup. We define the positive scenario as contexts containing high-quality documents and the negative scenario as contexts containing low-quality documents. Following Chen et al. (2024), high-quality documents are those that contain the correct answer, while low-quality documents do not. Low-quality documents are further divided into two categories: Distracting documents, which are related to the question but lack the correct answer, and Unrelated documents, which are not related to the question at all. We use RGBqa to construct the contrastive dataset as it offers retrieved documents with different qualities that satisfy our requirements. The construction process and additional details are presented in Appendix D.\nEmpirical Findings. Based on the results visualized in Figure 4, we observe distinct differences in expert activation between contexts containing high-quality versus low-quality documents. Such differences increase as document quality decreases, further confirming the impact of retrieved document quality on expert activation. Moreover, the activation differences become more evident as the model scale increases, a trend similar to that observed with cognizant experts. These findings validate the existence of quality experts.\nAnalyses. To further investigate the effectiveness of quality experts, we use scenario scores to predict the quality of retrieved documents. By combining $D_{pos}$ and $D_{neg}$, we construct the verification dataset that includes both high-quality and low-quality retrieved documents. Two datasets, Unrelated and Distracting, are created with different levels of low-quality documents. The experimental results in Table 2 demonstrate the effectiveness of quality experts in distinguishing the quality of retrieved documents. We observe that the quality expert consistently outperforms random guessing across both model scales and datasets. On Mixtral-8x22B, full-set quality expert achieves a remarkable 93% accuracy on the Unrelated dataset and a respectable 75.33% on the Distracting dataset. The 50-shot also achieves impressive performance, only slightly lower than the full-set. Similar to the observations made with cognizant experts, we find that the larger the difference in expert activation, the better the performance of the quality expert."}, {"title": "3.4 In-context Experts", "content": "The ability to leverage contextual information is crucial for RAG (Shi et al., 2023b). Yet, LLMs may struggle to effectively extract information from context, even though the context includes high-quality retrieved documents. (Xie et al., 2023).\nThis motivates us to identify in-context experts that can enhance the LLM's ability to utilize context.\nSetup. Intuitively, contrastive scenarios for using contextual information can be constructed by including or excluding the retrieved documents in context. We view the data that consists of a question paired with retrieved documents as $D_{pos}$, whereas data that includes only the question without retrieved documents as $D_{neg}$. To avoid potential bias from text length, we pad the texts in $D_{neg}$ to match the length of those in $D_{pos}$. We show in-context experts' detailed experimental setups for empirical findings and analyses in Appendix E.\nEmpirical Findings. Figure 5 reveals distinct differences in expert activation between scenarios where the context includes or excludes retrieved documents, validating the existence of in-context experts. Additionally, we find a subset of experts is frequently activated across all datasets, which suggests presence of universal in-context experts.\nAnalyses. We validate the effectiveness of in-context experts on RAG tasks. Guided by the contrastive activation probability AP of in-context experts, we control the activation of experts during the model's forward pass, adjusting its ability to use the information in context. To enhance this ability, we enforce the activation of experts with high AP values and increase their weight, while preventing the activation of experts with low values. Importantly, we do not increase the number of activated experts, each MoE module still activates the default number of experts. From the results in Table 3, we can observe that enhancing in-context experts improves the performance of RAG while inhibiting them leads to performance degradation. Given the complexity of the LLM's internal mechanisms, the activation of in-context experts can only play a limited role and cannot substantially control the model's behavior, which is within our expectations. Interestingly, random experts seem to be more effective in inhibiting model abilities compared to in-context experts, resulting in lower task performance. However, enhancing random experts does not lead to performance improvement, while enhancing in-context experts does. We hypothesize that this is because random experts do not intentionally control the target ability, but instead inhibit some general experts that are crucial to the general ability. In Appendix E, we show the experimental details and further demonstrate that inhibiting general experts that highly activated in any scenario (but ignored by in-context experts due to contrast operation) causes more severe performance degradation than random experts."}, {"title": "4 Application on Adaptive RAG", "content": "Recent research has increasingly focused on Adaptive RAG (ARAG), which reduces retrieval cost and enhances RAG performance by enabling retrieval only when necessary (Wang et al., 2023; Asai et al., 2024). Given the considerable role our identified experts fulfill in such scenarios, it is intuitive to use them to improve RAG. In this section, we first introduce our method in \u00a74.1, followed by the experimental setup in \u00a74.2. Finally, we show the effectiveness of our method in \u00a74.3.\n4.1 Enhancing RAG via Expert Activation\nWe introduce Expert-RAG, which utilizes three discovered experts to improve the effectiveness and adaptivity of RAG. Expert-RAG involves the following steps: (1) Knowledge Judgment: Given a question, we first input it to LLM and collect the activation of experts during the forward phase. Then we utilize cognizant experts to calculate the scenario score to predict whether knowledge is sufficient based on the expert activation of the given question. Retrieval is only enabled when the cognizant experts predict that the model's internal knowledge is insufficient. (2) Quality Filter: Once documents are retrieved, we input the question with retrieved documents to LLM and use the quality expert to predict the quality of documents. Only high-quality documents are used for further processing. (3) Retrieval Enhancement: With high-quality retrieved documents in context, we adjust the in-context experts to boost the model's ability to use context. Our method only requires a few data to identify the above experts, which is training-free and easy to implement.\n4.2 Experimental Setup\nWe first highlight the limitations of the current ARAG evaluation and then describe our main experimental setup. Additional details about data composition, metrics, prompts and hyperparameters are provided in Appendix F.\nRAG Evaluation. Most RAG datasets fail to effectively reflect the advantages of ARAG, as always retrieving documents often yields the best task performance on these data. Using only task performance as the evaluation metric cannot reflect the efficiency advantages of ARAG, and the retrieved documents can also be harmful in real-world applications. To address these issues, we propose evaluation metrics and data recipes to make ARAG evaluation more comprehensive.\nMetric. For task performance, we follow previous work (Shi et al., 2023b; Asai et al., 2024) and use Accuracy (Acc) as the task performance metric for all datasets. We mark a prediction as correct if any substring of the prediction exactly matches any of the gold answers. Additionally, we use Retrieval Score (R-Score) to evaluate the necessity of retrieval and R-Token to evaluate the additional inference cost introduced by RAG. We consider the R-Score measures the accuracy between the model's actual retrieval requirements and the ARAG method's retrieval predictions. R-Token represents the token length of the retrieved documents used for the generation, which relies solely on the retrieved documents and the tokenizer, allowing comparisons across devices and platforms. These two metrics allow us to evaluate the efficiency and effectiveness of ARAG comprehensively.\nDataset. We follow previous RAG work (Chen et al., 2024; Asai et al., 2024) and select commonly used QA datasets for evaluation, including PopQA (Mallen et al., 2023), RGBqa (Chen et al., 2024), and PubHealth (Asai et al., 2024). To better demonstrate the effectiveness of ARAG, we de-"}, {"title": "4.3 Results", "content": "Table 4 presents the main results of our experiments. We can observe that (1) Always RAG consistently improves task performance across nearly all datasets, except BalanceQA. However, the R-Score and R-Token reveal that there are many unnecessary retrievals required by Always RAG, which introduce additional costs. In contrast, our proposed Expert-RAG consistently achieves competitive performance with reduced retrieved tokens compared to the Always RAG across all datasets. We even outperform Always RAG on RGBqa, PubHealth, and BalanceQA. This is because we cannot guarantee that the retrieved documents always contain information beneficial to the current question. Always using retrieved documents may interfere LLM's internal knowledge, leading to performance degradation. (2) BalanceQA introduces \u201crisk\u201d into retrieval, which prevents heuristic methods like No RAG and Always RAG from achieving optimal performance, thereby highlighting the advantages of ARAG. Our method achieves a 9% improvement on the Acc score and a 14.7% improvement on R-Score on BalanceQA, demonstrating its effectiveness as an ARAG method. (3) As the core of ARAG, the cognizant expert brings the most improvements across all datasets. The quality expert can mitigate negative impact of frequently retrieved low-quality documents, while in-context experts are beneficial when contextual information is reliable. Overall, these results confirm the effectiveness of our metric and data recipe and show that it is practical to enhance RAG with expert activation."}, {"title": "5 Related Work", "content": "Mixture-of-Experts By replacing the dense FFN layer with dynamically activated experts (Jacobs et al., 1991), MoE greatly enhances model performance without increasing the number of activated parameters and thus is widely used in LLMs (Shazeer et al., 2016; Du et al., 2022; Jiang et al., 2024; DeepSeek-AI et al., 2024). As core mechanisms of MoE, the routing network activates the appropriate experts based on different input representations and scenario demands, thereby influencing model behavior (Zhou et al., 2022; Chi et al., 2022). This inspire us that expert activation can indicate scenario and adjust model behavior.\nRetrieval-augmented LLM In complex real-world applications, knowledge within LLMs' parameters (Brown et al., 2020; Touvron et al., 2023; Jiang et al., 2024) is usually insufficient or out-of-date, leading to hallucinations (Cao et al., 2020; Ji et al., 2023; Xu et al., 2024). To mitigate this issue, retrieval-augmented generation (RAG) enhances the LLM's input by retrieving query-relevant documents, offering external knowledge to improve the reliability of responses (Guu et al., 2020; Borgeaud et al., 2022; Ren et al., 2023). However, RAG still faces many challenges (Gao et al., 2024; Chen et al., 2024), such as the inference costs due to the lengthy retrieved documents (Xu et al., 2023), information interference from low-quality retrieved documents (Shi et al., 2023a), and the model's mistrust of the retrieved documents (Xie et al., 2023; Yu et al., 2023). To address these problems, some works focus on improving retriever quality (Pan et al., 2024; Ke et al., 2024) or refining the retrieved documents (Xie et al., 2023; Wang et al., 2023). Other works focus on training RAG-specific models (Lin et al., 2023), which require high training costs. Unlike these methods that improve RAG from an external perspective, we investigate the impact of internal expert activation within MoE-based LLM in various RAG scenarios and provide a low-cost adaptive RAG solution."}, {"title": "6 Conclusion", "content": "In this paper, we explore the impact of expert activation within MoE-based LLMs in the context of RAG. We introduce CEAI, a method that compares differences of expert activation frequency in contrasting scenarios to identify core experts responsible for specific scenarios. We identify three types of core experts for RAG: cognizant experts, quality experts, and in-context experts. We further demonstrate how the activation of core experts can predict scenarios and enhance model behaviors. Building on these insights, we propose an expert-based adaptive RAG method and several methods for comprehensive ARAG evaluation. Our experiments across multiple datasets confirmed the effectiveness of enhancing RAG via expert activation."}, {"title": "Limitation", "content": "The limitations of this work are: (1) The focus of this work is the impact of expert activation in MoE-based LLM on RAG. Given that all activated experts can be regarded as a natural subnetwork, there may also be such a subnetwork in a dense network. However, unlike MoE models, dense models do not have naturally activated experts that allow us to find such subnetworks directly. Since we focus on the expert activation in MoE-based LLM, the dense model is beyond the scope of this paper, and we leave this direction for future work. (2) Our experiments are conducted on instruction-tuned MoE-based LLMs. We did not evaluate the base model without instruction fine-tuning or models specifically designed for RAG. Despite this, our experiments included models of various scales, and we identified the same core experts in Qwen models with different MoE architectures and training methods. This finding suggests that the three types of experts we discovered are universally present and that our approach is generalizable."}, {"title": "A Additional Details of CEAI", "content": "In our experiments, CEAI is applied to the last token of the input sequence. This position is also the position for generating the first token. Implementing CEAI in this position avoids the computationally intensive generation process, thereby enhancing the efficiency of RAG systems. Our method leverages expert activations to make critical decisions during the retrieval process, such as whether to retrieve documents and how to utilize the retrieved information. Therefore, applying CEAI before the generating phase can avoid generating all the responses and improve efficiency."}, {"title": "B Additional Details of Detecting Core Experts", "content": "B.1 General Experimental Settings\nThis section details the experimental settings for identifying core experts on different scenarios.\nModels We use Mixtral-8x7B-instruct-v0.1, Mixtral-8x22B-instruct-v0.1 (Jiang et al., 2024), and Qwen1.5-MoE-A2.7B-Chat (Bai et al., 2023) in our experiments. Mixtral-8x7B consists of 32 layers; each layer contains 8 experts, and by default, only two experts are activated per layer. Mixtral-8x22B consists of 56 layers, maintaining the same configuration of 8 experts per layer and activating two experts by default. QWEN1.5-A2.7B comprises 24 layers, with each layer having 4 always-activated shared experts and 60 dynamic experts, out of which 4 are activated by default. For all experiments, we utilized the default configurations of the above models. During generation, we employed greedy decoding to improve reproducibility. These models were implemented using the Huggingface framework. All experiments were conducted on 8xNVIDIA-A100-80GB.\nRetrieved Documents For PopQA and RGBqa, we use the officially provided retrieved documents. In PopQA, each question was accompanied by five retrieved documents in the context. In RGBqa, the type and quantity of documents vary according to the specific experimental conditions we detailed in relevant sections. For PubHealth, following (Asai et al., 2024), we use Contriever-MS MARCO to retrieve the top five documents from Wikipedia, utilizing the official Wikipedia embeddings based on the 2018 English Wikipedia.\nPrompts Following previous work (Asai et al., 2024), we instruct the LLMs to directly generate answers. This also benefits CEAI, as CEAI is applied at the position of the first generated token. The prompts for different tasks are shown in Table 6 and are used consistently across all experiments. In scenarios where contexts do not contain retrieved documents, such as identifying cognizant experts, we filled the {retrieved document} placeholder with \"No paragraph available.\u201d Additionally, after applying the chat template to the prompt, we appended \"The answer is: \" to further ensure that the first generated token is the answer.\nEvaluation Metric For the RAG task, we use Exact Match as the performance metric. We lowercase both the LLM outputs and the answers and check if the correct answer exactly matches any part of the model's output. We use the accuracy between the core expert's prediction and the real scenario to measure the scenario prediction.\nVariant of Scenario Score As discussed in \u00a72.2, we explore various methods to compute scenario scores, treating these methods as hyperparameters for enhanced scenario prediction accuracy. We search these hyperparameters for all experiments and report the best performance. For each dataset, we select and retain only the Top-k and Bottom-k experts based on contrastive activation probability AP. These experts are identified as core experts for specific scenarios. Furthermore, we can use the contrastive probability of core expert Pei; and the expert activation probability of the current input $g_{i,j}$ as the weights to calculate the score. We can also utilize the activation without any weights to cacluate score, such as -1 -1 11 (Pei,j) \u2022 $I_2(g_{i,j} (x))$, where $I_1(P_{ei,j}) \\rightarrow {0,1}$ means $e_{i,j}$ is the selected core experts and $I_2(g_{i,j}(x)) \\rightarrow {0,1}$ means $e_{i,j}$ is activated for current input.\nB.2 Results of Qwen\nAs mentioned in \u00a73, we also conduct experiments on Qwen1.5-MoE-A2.7B-Chat model. This model has fewer parameters but more experts than the Mixtral series, featuring 60 dynamically activated experts. Given the extensive number of experts, comprehensive visualization of the results within the main text is impractical. Therefore, we present the visualization of Qwen's core experts here. The visualization of cognizant expert activations is shown in Figure 6 and Figure 7. Visualizations for quality experts are displayed in Figure 8 and Figure 9. The in-context expert visualizations are also shown in Figure 10 and Figure 11.\nTo assess the effectiveness of the various expert groups within the Qwen model, we also conduct experiments to evaluate whether these experts can be used to predict scenarios. The results are shown in Table 5. Despite the large number of experts and distinct MoE architecture, the expert activation patterns in QWEN are also similar to the Mixtral series. Specifically, the following observations were made: there are different expert activations in contrastive scenarios, activation of core experts can be used to predict scenarios, and more evident difference leads to higher prediction accuracy. The phenomena described above have also been similarly observed among various core experts in the Mixtral series models. This observation underscores the ubiquity of core experts within models structured around MoE-based LLM, irrespective of their specific architectures. Furthermore, these findings imply a robust generalizability of our proposed method."}, {"title": "C Additional Details of Cognizant Experts", "content": "In this subsection, we provide a detailed description of using cognizant experts to predict whether the model's internal knowledge is sufficient.\nDataset Given a RAG dataset D, we first divide it into $D_{pos}$ containing data points where the model answers correctly, and $D_{neg}$ consisting of data points where the model answers incorrectly. In our experiments, Full-Set is the combination of $D_{pos}$ and $D_{neg}$ and we use Full-Set as the evaluation data. To show the generalizability of cognizant experts, we also propose using 50-Shot, which consists of 50 samples randomly selected from the full set.\nMethod Cognizant experts are identified through the Contrastive Expert Activation Inspection (CEAI). First, based on Equation"}]}