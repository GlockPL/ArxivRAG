{"title": "A Few-Shot Approach for Relation Extraction Domain Adaptation using Large Language Models", "authors": ["Vanni Zavarella", "Juan Carlos Gamero-Salinas", "Sergio Consoli"], "abstract": "Knowledge graphs (KGs) have been successfully applied to the analysis of complex scientific and technological domains, with automatic KG generation methods typically building upon relation extraction models capturing fine-grained relations between domain entities in text. While these relations are fully applicable across scientific areas, existing models are trained on few domain-specific datasets such as SciERC and do not perform well on new target domains. In this paper, we experiment with leveraging in-context learning capabilities of Large Language Models to perform schema-constrained data annotation, collecting in-domain training instances for a Transformer-based relation extraction model deployed on titles and abstracts of research papers in the Architecture, Construction, Engineering and Operations (AECO) domain. By assessing the performance gain with respect to a baseline Deep Learning architecture trained on off-domain data, we show that by using a few-shot learning strategy with structured prompts and only minimal expert annotation the presented approach can potentially support domain adaptation of a science KG generation model.", "sections": [{"title": "1. Introduction", "content": "Knowledge graphs (KGs) [1] have proved effective for representing research knowledge dis- cussed in scientific papers and patents across several different domains [2, 3, 4]. New generation \"scientific KGs\" have moved from representing purely bibliographic information of research publications to support the construction of extensive networks of machine-readable information about entities and relationships pertaining to a certain domain, enabling fine-grained semantic queries over large scientific text collections such as: \u201cretrieve all methods that are used for Indoor Air Remediation in the time range T\u201d.\nTherefore, they can support downstream analytical services like technology trend analysis. For example, [5] uses the statistics of relation triples of type <Method;Used-for;Task> automat- ically extracted from paper abstracts to reconstruct historical trends of the top applications of target methods such as \u201cneural networks\" in different areas like speech recognition and computer vision.\nTherefore, we experiment on an empirical solution leveraging in-context learning capabilities of Large Language Models (LLMs) [10, 11] to perform schema-constrained data annotation, generating in-domain training instances for a baseline Relation Extraction architecture from only"}, {"title": "2. Data", "content": "The source data used in this experiment comprise titles and abstracts from a large collection of around 476k research articles in the AECO area published in the time range 2010-2023, retrieved from the OpenAlex\u00b2 open scientific graph database [16] using a set of platform-specific topic filtering tags.\nWe sampled a test set of around 50 abstracts, pre-processed and sentence split them using Spacy's English transformer pipeline en_core_web_trf-3.6.1\u00b3 and finally had them independently annotated by two domain experts using the Brat annotation tool [17], resulting in a total of 314 sentences, 448 entities, 132 relations instances. The inter-annotator positive specific agreement on entity detection ([18]\u2074) reached a mean F1 score of 0.73, indicating an overall satisfying agreement between the human annotators, although some marginal ambiguity is encountered for such a complex task. We publicly share the current version of the test dataset (called SCIERC- AECO) in the github: https://github.com/zavavan/sperty/blob/main/datasets/scierc_aec/scierc_ aec_test.json and plan to release extended versions in the future.\nWe used two random samples of respectively 3 and 10 sentences as example annotations for the few-shot LLM prompts described below."}, {"title": "3. Experimental Setups", "content": "The full-stack Relation Extraction task consists of generating, for an input token sequence X = X0, ..., Xn: a) a set of tuples E = < (xi,..xj), Te > of typed token sub-sequences of X, with 0 \u2264 i, j \u2264 n and Te \u2208 TE being a label belonging to the set TE of entity labels; b) a set R of triples <h, t, Tr > where h, t \u2208 E are, respectively, the entity head and tail of the relation, and Tr \u2208 TR is a relation label.\nAs baseline for the RE task we use SpERT (Span-based Entity and Relation Transformer) [19], a span-based model for joint entity and relation extraction. SpERT is a relatively simple approach using the pre-trained BERT for input token representation that classifies any arbitrary candidate token span into entity types, filters non-entities (None entity class) and finally classifies all pairs of remaining entities into relations.\nBy using only sentence-level context representations for sampling positive and negative training examples, the architecture allows single-pass runs through BERT for each sentence, resulting in significant speeding up of training. Despite this sentence-level RE simplification though, SpERT significantly outperforms other joint entity/relation extraction models on SciERC dataset, reaching up to 70.33% micro-average F1 on entity extraction and up to 50.84% micro- average F1 on relation extraction (around 2.5% improvement on both tasks).\nWe re-trained SpERT on SciERC training set (1861 sentences) using SciBERT (cased) embed- dings [20]. When tested over out-of-domain SCIERC-AECO data though, SpERT performance degrades drastically. First row in Table 1 shows Micro-average F1 scores on SCIERC-AECO for entity extraction (NER), relation detection without argument entity classification (RE) and relation detection considering entity classification (RE_w/NEC), respectively.\nIn order to test few-shot learning capability of LLMs for training data generation, we ex- periment on schema-constrained instruction prompts sent to the Chat Completion endpoint of the OpenAI gpt-3.5-turbo-0125 (ChatGPT) API [21]. The context length of the model is approximately 4096 tokens."}, {"title": "4. Results and Discussion", "content": "Table 1 reports Micro-average F1 scores when training with ChatGPT-generated data using the basic instruction Prompt (Schema Prompt) and the prompt enriched with schema description (Schema/Descr Prompt), for different values of K prompt examples. The last two rows report performance when training on merged ChatGPT-generated and SciERC data.\nAt a first glance the LLM seems to fully comply with the structural requirements of the annotation task, consistently generating schema-based output. In some cases, it \"semantically\u201d manipulates the input text (average 2% occurrence) so as to make the output annotations not directly usable for sequence labeling. For example, from the sentence \u201cThe carbon emissions throughout the entire life cycle of the building have been reduced by 20.99%.\u201d it generates an entity not anchored in text(T1;Task;Carbon emissions reduction).\nOverall, the performance level is not outstanding across all configurations, considering that the same model architecture is achieving a F1 measure of 2-3 factors higher when trained on in-domain manually curated data (SciERC) of comparable size. This may be due to the model degrading its generalization performance by learning from noisy labels [22], which is confirmed by observing that the best results are obtained by adding ChatGPT generated labels to curated out-of-domain SciERC labels. By considering only LLM-generated data, most configurations slightly outperform the baseline for NER while only one does it for RE, indicating that this is an harder task for LLM few-shot learning with respect to NER.\nAdding explicit Task definitions and increasing the number of few-shot examples both consistently raise the performance with respect to all metrics, with the second finding seemingly in contrast with what reported in [15]."}, {"title": "5. Conclusions", "content": "This contribution presents our currently-ongoing work on the potentials of Large Language Models (LLMs), specifically ChatGPT, for few-shot learning in the context of relation extraction domain adaptation. In particular the study aimed to generate in-domain training data for a Transformer-based relation extraction model within the Architecture, Construction, Engineer- ing, and Operations (AECO) domain by leveraging the in-context learning capabilities of LLMs. The experiments involved using structured prompts and minimal expert annotation to collect training instances from AECO research paper titles and abstracts.\nThe results indicate that the quality of the LLM-generated annotations may not be sufficient to support domain customization of a RE model from ground up. However, when combined with curated out-of-domain labels it can boost the performance on the new domain significantly.\nOverall, the research highlights the potential of using ChaptGPT for optimizing local, lower- sized models, which can be a more cost-effective strategy than relying on direct use of LLMs for inference in production settings.\nFuture work might include expanding the test set and conducting more extensive tests to further validate the approach, also considering other domains than AECO. In addition, in the future we plan to experiment with GPT-4 for data generation rather that ChatGPT, leveraging its powerful capabilities to improve the quality of synthetic data, such as in the powerful LLaVA multi-modal model [23].\nObject of further investigation and experiments will involve also the use of the latest advances in open-source LLMs, such as by employing open-source models like Mistral-7B-OpenOrca, Nous Hermes Mixtral, and Llama-3-70B, to explore their potential in relation extraction tasks. This could involve comparing the performance of these models directly with the current"}]}