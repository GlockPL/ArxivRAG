{"title": "Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum", "authors": ["Lanpei Li", "Jack Bell", "Massimo Coppola", "Vincenzo Lomonaco"], "abstract": "The increasing complexity of application requirements and the dynamic nature of the Cloud-Edge Continuum present significant challenges for efficient resource management. These challenges stem from the ever-changing infrastructure\u2014characterized by additions, removals, and reconfigurations of nodes and links\u2014and the variability of application workloads. Traditional centralized approaches struggle to adapt to these changes due to their static nature, while decentralized solutions face challenges such as limited global visibility and coordination overhead. This paper proposes a hybrid decentralized framework for dynamic application placement and resource management. The framework utilizes Graph Neural Networks (GNNs) to embed resource and application states, enabling comprehensive representation and efficient decision-making. It employs a collaborative multi-agent reinforcement learning (MARL) approach, where local agents optimize resource management in their neighborhoods and a global orchestrator ensures system-wide coordination. By combining decentralized application placement with centralized oversight, our framework addresses the scalability, adaptability, and accuracy challenges inherent in the Cloud-Edge Continuum. This work contributes to the development of decentralized application placement strategies, the integration of GNN embeddings, and collaborative MARL systems, providing a foundation for efficient, adaptive and scalable resource management.", "sections": [{"title": "I. INTRODUCTION", "content": "The Cloud-Edge Continuum has become a cornerstone of modern distributed computing, combining centralized cloud resources with edge and IoT devices to support diverse application requirements[1]. Applications in this Continuum range from real-time IoT services to computationally intensive data processing tasks, requiring resource management strategies that can balance latency, scalability, and efficiency[2]. How-ever, the dynamic nature of the Continuum-marked by fluctuating workloads, evolving network topologies, and heterogeneous resource capabilities-introduces significant challenges for effective resource allocation[21].\nTraditional resource management strategies typically rely on centralized architectures, which provide a global perspective but face challenges in adapting to dynamic changes due to high communication overhead and latency [16]. Decentralized"}, {"title": "II. PROBLEM SETTINGS", "content": "Traditional Cloud computing struggles to meet the quality-of-service (QoS) demands of IoT due to the scale, mobility, and geographical distribution of devices, as well as the need for low latency and localized processing[28]. The Cloud-Edge Continuum represents an evolution beyond traditional Cloud computing to address challenges arising from the rapid expansion of IoT devices and the vast amounts of data they generate. Unlike the cloud-centric model, the Continuum integrates a wide range of distributed computing resources across multiple layers, including cloud data centers, fog nodes, edge devices, and IoT endpoints[3]. This architecture aims to support diverse application requirements, including ultra-low latency for real-time interactions, high bandwidth for handling data-intensive tasks, distributed computing to enable parallel processing across multiple nodes, localized processing for data privacy and reduced network dependency, scalability to accommodate fluctuating workloads, and energy efficiency for sustainable operation in resource-constrained environments. However, the Continuum introduces several challenges, including the need for efficient resource discovery, monitoring, and management, as well as the coordination of application execution across heterogeneous resources and environments[5].\nResource management in the Cloud-Edge Continuum is crucial for optimizing performance and ensuring seamless service delivery across distributed environments. The goals of resource management in the Cloud-Edge Continuum focus on optimizing resource allocation, load balancing, resource provisioning, task scheduling, and maintaining QoS[20].\nTo evaluate resource management policies within the Cloud-Edge Continuum, three distinct models and a specified objective function are typically used:\n\u2022 Resource Model: represent the computational, storage, and network resources available across the Continuum, They capture device capabilities, network topology, resource heterogeneity, and dynamic changes such as variations in resource availability due to device mobility, workload fluctuations, or failures, providing a comprehensive view of the distributed environment.\n\u2022 Application Models: defines the structure and interactions within multi-services applications, including the topology of their components and communication paths. They capture the specific requirements of tasks or communication, such as latency sensitivity, bandwidth demands, and computational complexity. This ensures application constraints and performance metrics are accurately incorporated, enabling precise evaluation of how resource management policies impact application performance within the Continuum.\n\u2022 Cost Model: evaluate resource management expenses in the Continuum, considering computation and communication costs related to latency. Computation costs encompass the time required for processing tasks, while communication costs include the delay involved in transferring data, influenced by data size, bandwidth usage, and network conditions. These models provide a clear framework to balance performance and responsiveness in resource management policies.\n\u2022 Objective Function: focus on optimizing specific goals, such as minimizing latency, energy consumption, or operational costs while ensuring QoS and scalability. These models help identify trade-offs and balance competing objectives.\nThe dynamic nature of the Cloud-Edge Continuum, characterized by high variability, heterogeneity, and unpredictability, necessitates the use of autonomic control loop such as MAPE (Monitoring, Analysis, Planning, Execution) for effective resource management[19]. Models like resource, application, and cost models are integrated into the MAPE loop to guide decision-making in environments where resource states, and application demands fluctuate frequently.\nWhile MAPE offers an automated approach for managing the Cloud-Edge Continuum, addressing the dynamic nature of the Continuum remains challenging. One major issue is accuracy, as static approaches often struggle to account for real-time changes in resource availability, network conditions, or application demands, leading to suboptimal decisions. Optimization overhead is another challenge, as dynamic environments necessitate frequent updates and recalculations, increasing computational costs and decision-making delays[31]. Centralized systems face scalability issues, often becoming bottlenecks when managing large, heterogeneous resources across multiple providers. Additionally, there are practical trade-offs between system responsiveness and long-term optimization, making it difficult to achieve a balance that satisfies all objectives. A decentralized, local-based approach could address these challenges by leveraging localized insights to improve accuracy, distributing computational tasks to reduce overhead, and enhancing scalability through collaborative management across multiple resource providers. However, decentralization introduces its own challenges, such as the need for effective synchronization to achieve global objectives and system-wide efficiency, making it essential to carefully design communication mechanisms between local orchestrators such as the use of a global coordinator."}, {"title": "III. RELATED WORK", "content": "A. Optimization based approaches\nDirectly solving the optimization problem through Mixed Integer Linear Programming (MILP) [13] is a well researched problem area that has theoretical guarantees to find the optimal solution, but is only suitable for small scale resource management problems. Heuristic-based algorithms instead pose the task as a multi-dimensional bin-packing problem such as first-fit [12], bestfit [4], worstfit[7] and round-robin placement[23]"}, {"title": "B. AI approaches", "content": "The dynamic and complex nature of the Cloud-Edge Continuum has sparked interest in leveraging AI approaches for resource management. As opposed to the aforementioned optimization-based approaches which often struggle to adapt to rapid changes in network topology and fluctuating application workloads, AI-based approaches such as DRL and GNNS have emerged as promising alternatives for their ability to approximate optimal solutions with reduced computational cost and enhanced adaptability.\nDRL in Dynamic Resource Management: DRL has been widely explored for dynamic resource allocation, task scheduling and autoscaling in Cloud-Edge Continuum environments. In these settings, DRL agents learn policies by interacting with the environment, aiming to maximize cumulative rewards that are composed of performance metrics such as latency, throughput or cost. [17] proposed a DRL-based scheduler which adapts to workload variations in real-time, outperforming traditional heuristics-based methods. Whilst DRL-based methods excel with high-dimensional data, it can be beneficial to use compact representations of the data through embeddings.\nWhen considering the centralized control approach, a single reinforcement learning agent with global view may often suffice, but in the instance of decentralized control, multi-agent reinforcement learning (MARL) must instead be considered. In [22], they use multi-agent DRL in wireless networks to enable simultaneous decisions to be made in a distributed manner. The framework is based on deep independent Q-Learning (IQL), but also accounts for a variable number of users and network densities whilst using a fixed-size neural network. However, a drawback of this approach is that agents learn in a non-stationary environments despite the effects of other agents actions on the environment and reward locally [14]. In their paper, they propose to avoid the issues of IQL through using a shared reward function to align agents and reduce learning instability.\nTo improve the representation of data for a DRL agent to be able to effectively learn at scale, embeddings such as those created with GNNs (e.g. Graph Convolutional Networks(GCNs)[32] and GraphSAGE[9]) can be used to manage variable size graphs as input. In [27], they leverage GNNs in a decentralized setting to better model the function of expected rewards in a larger DRL framework. A GNN can portray a very complex representation for a system comprising of connected nodes whilst allowing for a distributed implementation. However, a drawback of GNNs is the requirement to rebuild the network when a node is added or removed, thereby adding large computational and time cost. In our approach, we instead mask unused nodes in the DRL agent state space, thereby reducing the need to retrain the GNN."}, {"title": "C. Decentralized Approaches", "content": "Traditionally in centralized approaches, there is a single global orchestrator with a comprehensive view of resources, making decisions to optimize resource allocation and performance across the Cloud-Edge Continuum[10]. In this setting, it is possible to achieve a globally optimal solution, however, this comes at the cost of scalability, increased latency and lack of resilience. Due to single controller, it can become a bottleneck for large scale systems which can slow the velocity at which decisions can be made.\nIn contrast, decentralized approaches distribute decision-making across multiple local agents, each with local information, that interact with neighboring nodes and agents. In [11], they use GNNs and MARL to enable decision making that incorporates local agent interactions. Each agent in the Vehicle-2-Anything (V2X) setting operates autonomously based on locally observed environment states, enabling efficient learning with reduced complexity. In this instance, they employ a weighted global reward to align local decisions to maximize their global objective function. However, the decentralization comes at a cost of no consistent view of the global state which can make lead to convergence at individual local states which may not align with the global optimum. Through leveraging GNNs and MARL pretraining at the local coordinator level along with the use of a global reward, we aim to offset the negative impacts of a fully decentralized approach."}, {"title": "D. Dynamic management approaches", "content": "Dynamic resource management in the Cloud-Edge Continuum has garnered significant attention due to fluctuating application workloads and infrastructure dynamics. Research has explored adaptive resource allocation strategies to respond to changes in application demands, network conditions, and resource mobility. For instance, [29] introduced a dynamic resource provisioning algorithm that utilizes both edge and cloud resources with a latency-aware packet forwarding method for latency-sensitive applications. Many approaches integrate feedback mechanisms to monitor system states and enable real-time adjustments, optimizing resource utilization and reducing costs while meeting application-level QoS requirements. For example, [8] applies the MAPE control loop concept to design a resource provisioning framework for cloud environments. RL techniques have also been employed to address dynamism, such as the DRL-based dynamic resource management algorithm proposed by [6], which optimizes transmission power and computing resource allocation, effectively reducing task delay compared to conventional methods. Additionally, [24] employs invalid action masking to temporarily disable unavailable actions, effectively addressing the challenges of a variable action space. In this work, we also leverage RL and invalid action masking to address dynamic changes in the Cloud-Edge Continuum environment, but in a decentralized manner"}, {"title": "IV. GENERIC MODEL", "content": "Our proposed model begins with a centralized use case, where GNNs are employed during the Analysis phase of the MAPE autonomic control loop. The GNN serves as a feature extractor, capturing the state of individual nodes while accounting for their logical inter-dependencies. These extracted features are then utilized by a centralized DRL agent, which has full observability of the entire system. The centralized agent focuses on optimizing global objectives, such as maximizing resource utilization and minimizing costs.\nTo efficiently address the inherent dynamism of the Cloud-Edge Continuum, this framework is extended to a decentralized approach by decomposing the problem into localized components. This decentralized design leverages a MARL Model, where multiple agents operate independently at the neighborhood level. These agents use localized insights to make decisions and collaborate to optimize resource allocation and system performance across the Continuum. This transition enhances scalability, adaptability, and responsiveness to dynamic changes in infrastructure and workloads."}, {"title": "A. System Model", "content": "The Cloud-Edge Continuum system comprises diverse resources distributed across multiple layers, including IoT devices, edge/fog nodes, and central clouds. These resources differ in computational power, memory, and network capabilities. Managing resources in this environment involves allocating application components to the available resources to optimize performance while meeting QoS requirements.\n1) Application Model: The application is modeled as a undirected graph $G_A = (C, E)$, where:\n\u2022 C denotes the set of vertices, expressed as $C = {c_1, c_2,..., c_N}$, where each vertex represents a component of the application. Following the microservice architecture, each component provides a unique service with specific requirements, including the number of CPU cores, GPU cores, amount of RAM, storage capacity, processing time for a specific device and the processing deadline. These requirements for each component $c_i$ are defined as $c_i = {cpu_i, gpu_i, ram_i, stor_i, P_i, ddl_i}$.\n\u2022 E represents the set of communication requirements ${e_1, e_2,..., e_n}$, defined as $e_i = {lat(i, j), bw(i,j)}$, between components. These requirements specify the communication link from $c_i$ to $c_j$. The interactions among services are further characterized by the maximum allowable message travel time (latency) and the minimum required message size for transmission.\n2) Resource Model: The resource model is defined as an undirected graph $G_R = (V, L)$, where:\n\u2022 $V = {v_1,..., v_M}$ where each item represents individual computational edge nodes or a central cloud data center. Each resource node $v_i = {cpui, gpui, rami, stori, pti, avali}$ is characterized by its available capacity including resources such as CPU cores, GPU cores, amount of RAM, storage capacity, device response time, and its availability indicator.\n\u2022 $L ={l_{i,j}}$ represents the link between resource $v_i$ and resource $v_j$, with network capacity $l_{i,j} = {lat(i, j), bw(i, j)}$ comprised of packet delivery time along with the maximum bandwidth.\n3) Cost Model: The cost model comprises the following elements:\n\u2022 Completion Time: for a given application component, the upper bound is defined as the sum of computation and communication time:\n$CT(C_i) = T_{comp} (C_i, v_k) + T_{comm}(C_i, v_k, C_j, v_m)$ (1)\nWhere:\n$T_comp(c_i, v_k) = pt_{vk} + pt_{c_i} (v_k)$\n$T_comm(c_i, v_k, c_j, v_m) = lat_{i,j}(msgsize_{i,j}, bw_{i,j})$"}, {"title": "V. APPLYING THE MODEL", "content": "A. Agents\nThe resource management framework incorporates two levels of agents: initially pretrained local agents and a global agent with aggregated local information, which are then jointly trained together to optimize application placement in the Cloud-Edge Continuum. These agents operate with the following characteristics:\n1) Local Agents: Local agents act as coordinators for specific subgraph at the neighborhood level, managing multiple nodes within their designated areas. They have the following features:\n\u2022 Partial Observability: Local agents have visibility only over the nodes in their neighborhood and the local environment, enabling them to respond quickly to dynamic changes (see Sect.V-B).\n\u2022 Pretrained Behavior: Local agents are pretrained using local reward functions that emphasize efficiency within their region, such as maximizing local resource utilization, minimizing latency, and reducing SLA violations.\nLocal Reward Function: Each local agent optimizes a reward function defined as:\n$R_{local} = \\alpha \\frac{1}{RU_{local}} + \\beta \\frac{1}{CT_{local_i}} + \\gamma \\frac{1}{SVR_{local_i}}$ (6)\nWhere:\n\u2022 $RU_{local_i}$: Local resource utilization.\n\u2022 $CT_{local_i}$: Completion time for tasks within the neighborhood.\n\u2022 $SVR_{local_i}$: SLA violation rate for local tasks.\n\u2022 $\\alpha, \\beta, \\gamma$: Weight parameters for balancing reward terms.\n2) Global Agent: The global agent coordinates with pretrained local agents to achieve system-wide optimization by considering global objectives. Its features include:\n\u2022 Aggregate Observability: The global agent has an abstract view of the platform provided by local agents, that we detail in Sect.V-B.\n\u2022 Joint Training: The global agent is trained jointly with local agents to align local and global objectives, ensuring both neighborhood-level efficiency and global consistency.\n\u2022 Non-Local Reward The non-local reward function, $R_{non-local}$, may include terms such as:\n$R_{non-local} = \\delta_1 \\cdot \\frac{1}{SVR} + \\delta_2 \\cdot \\frac{1}{CT_{app}}$ (7)\nWhere SVR is overall SLA violation rate, $CT_{app}$ is the overall completion time, and $\\delta_1, \\delta_2$ are the weighting parameters.\nGlobal Reward The global agent optimizes a reward function that considers both local and non-local scope:\n$R_{global} = \\lambda \\cdot R_{non-local} + \\sum_{i} \\mu_i \\cdot R_{local_i}$ (8)\nWhere:\n\u2022 $R_{non-local}$: Global objective based on system-wide metrics (e.g. overall resource utilization, global SLA adherence, and minimized network congestion).\n\u2022 $R_{local_i}$: Local rewards from individual local agents.\n\u2022 $\\lambda, \\mu_i$: Weight parameters for combining global and local objectives (deciding between linear or nonlinear combinations is part of the design choice).\nIn future work, we might consider the integration of local and global rewards can be achieved through either linear or nonlinear combinations. The choice of combination strategy will depend on the specific application requirements, such as the need for rapid local responsiveness versus long-term global optimization. This hybrid approach ensures scalability while maintaining overall system efficiency."}, {"title": "B. Observation Space", "content": "The observation space for agents in the Cloud-Edge Continuum consists of two key components: the state of the"}, {"title": "C. Action Space", "content": "The action space defines the possible decisions that local and global RL agents can make in the Cloud-Edge Continuum. It is designed to reflect the decentralized structure and enable scalability for dynamic environments.\n1) Action Space for Local Agents: Each local agent is responsible for deciding the placement of application components onto available resource nodes within its local network. The action space for a local agent is represented as:\n$A_{local} = {a_{ij} | a_{ij} \\in {0,1}, c_i \\in C, v_j \\in V_{local}}$ (13)\nWhere:\n\u2022 $a_{ij}$ = 1 indicates that application component $c_i$ is placed on resource node $v_j$.\n\u2022 $a_{ij}$ = 0 indicates that $c_i$ is not placed on $v_j$.\n\u2022 $V_{local}$ is the set of resource nodes within the local agent's neighborhood.\nTo accommodate dynamic environments and support scalability, a masked action space is employed: a certain percentage of additional resource nodes are initialized but marked as unavailable for current decision-making, reserving them for potential future dynamic resource addition. Resource availability is dynamically updated over time dependent on changing application and environmental factors.\n2) Action Space for Global Agent: The global agent focuses on high-level decisions by selecting local agents to execute resource management tasks. Its action space is represented as:\n$A_{global} = {a_i | a_i \\in {0,1}, i = 1,..., N_{local}}$ (14)\nWhere:\n\u2022 $a_i$ = 1 indicates that the global agent delegates resource management tasks to the i-th local agent.\n\u2022 $a_i$ = 0 indicates that the global agent does not delegate tasks to the i-th local agent.\n\u2022 $N_{local}$ is the total number of local agents in the system.\nThe proposed strategy offers decentralized management, where local agents handle granular task placement within their neighborhoods while the global agent manages high-level coordination. To meet large-scale requirements, future work will explore local agents collaboration by integrating the action and state spaces of multiple agents, enabling more coordinated and efficient decision-making across the system."}, {"title": "D. Optimization Techniques", "content": "This section outlines potential optimization techniques that may be employed during the implementation phase to enhance resource management in the Cloud-Edge Continuum.\n1) GNN Embeddings: GCN or GraphSAGE mentioned in III-B may be used to encode the graph states of the resource network and application components. These embeddings provide a compact representation of the system, aiding decision-making.\n2) Multi-Agent DRL Optimization: MARL methods may be explored to optimize application placement and resource allocation in a distributed setting. Algorithms such as A3C (Asynchronous Advantage Actor-Critic and PPO (Proximal Policy Optimization) may be implemented from Stable-Baselines3[26] library to streamline development and evaluation.\n3) Customized Experience Replay: A customized experience replay strategy may be utilized during the joint training phase to improve training stability and performance for local agents:\n\u2022 Replay Buffer: Stores experiences from the pretraining phase of local agents, including state-action-reward tuples.\n\u2022 Stabilization Mechanism: Experience replay helps balance updates between pretrained phase and joint phase, preventing instability and promoting faster convergence."}, {"title": "E. Training Algorithm", "content": "The training process consists of two phases: pretraining the local agents and jointly training the global agent alongside the local agents, as detailed in V-E. Below, we highlight the key features of the training algorithm.\n\u2022 Sequential Placement Strategy: At each time step, the local agent assigns the current application component to a resource node. Communication requirements such as latency and bandwidth are validated using a shortest path algorithm to ensure placement compliance.\n\u2022 Integration of GNN Embeddings: GNN embeddings are employed by both local and global agents to represent their observation spaces, providing a deeper understanding of the resource and application states.\n\u2022 MARL Training: Pretrained local agents collaborate with a global agent to jointly optimize application placement and resource allocation in the Cloud-Edge Continuum."}, {"title": "VI. EVALUATION PLAN", "content": "To validate our approach, we plan to evaluate it in a simulated environment to analyze the system behavior in controlled settings before moving to real-world deployments.\nA. Simulation Environment\nWe propose using a simulation framework to verify the approach. A suitable simulator should support:\n\u2022 Modeling of the Cloud-Edge Continuum, including resource nodes (e.g., cloud servers, edge devices, IoT nodes) and their communication links.\n\u2022 Configurable network topologies to evaluate performance under various scenarios.\n\u2022 Support for multi-agent reinforcement learning (MARL) with customizable policies for local and global agents.\n\u2022 Scalability to simulate large-scale systems with dynamic task arrivals and resource variability.\nWe are considering extending an existing simulator described in III-E to create a tailored platform for implementing and testing MARL in distributed environments. This enables fine-grained control over simulation parameters and integration of specific features required for evaluating the proposed approach.\nB. Evaluation Metrics\nThe following metrics will be used to assess the performance of the proposed system:\n\u2022 SLA Violation Rate: The percentage of application tasks that meet their specified deadlines.\n\u2022 Global Resource Utilization: The average utilization of computational and communication resources across all nodes and edges.\n\u2022 Completion Time: The total time taken to complete application placement and execution.\n\u2022 Training Time and Efficiency: The time required to train local and global agents, as well as the efficiency of the training process.\n\u2022 Memory Usage for Training: The memory consumption during the training phase, highlighting the feasibility of scaling to larger systems.\nC. Comparison with Centralized Approaches\nThe performance of the proposed decentralized multi-agent framework will be compared against traditional centralized resource management approaches. Key comparison points include: scalability to large-scale environments; adaptability to dynamic changes in network topology and resource availability; overall performance improvements in SLA adherence, resource utilization, and task completion times."}, {"title": "VII. CONCLUSIONS", "content": "We presented our viewpoint on exploiting distributed local and global learning for scalable resource management in Continuum platform, presenting a hierarchical system organization and a training algorithm for the local and global agents. The work to validate our approach via implementation and simulation of a Continuum platform is still ongoing. Building on the findings of the practical evaluation, we plan to extend and validate our approach further through the following steps:\n\u2022 Different Network Topologies: Investigate performance across various network configurations, including hierarchical, mesh, and hybrid topologies.\n\u2022 Scaling Up: Develop hierarchical coordination and validate system scalability through simulations in larger-scale environments with more nodes, edges, and application components.\n\u2022 Improving cost functions: The completion time cost model from section V assumes that application components run sequentially as an upper bound. Literature results about behavioral models of parallel and distributed patterns will be exploited to improve the completion time models.\n\u2022 Real-World Testing: Use real world testbeds to validate the practicality and robustness of the proposed system.\n\u2022 Dynamic Agent Creation: Explore mechanisms for dynamically creating and adapting agents to accommodate changes in network scale and task demands.\n\u2022 Economic Costs Inclusion: Incorporate economic considerations into the optimization framework, comparing cost-effectiveness of centralized and decentralized approaches."}]}