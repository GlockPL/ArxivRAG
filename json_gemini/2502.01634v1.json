{"title": "Online Gradient Boosting Decision Tree: In-Place Updates for Efficient Adding/Deleting Data", "authors": ["Huawei Lin", "Jun Woo Chung", "Yingjie Lao", "Weijie Zhao"], "abstract": "Gradient Boosting Decision Tree (GBDT) is one of the most popular machine learning models in various applications. However, in the traditional settings, all data should be simultaneously accessed in the training procedure: it does not allow to add or delete any data instances after training. In this paper, we propose an efficient online learning framework for GBDT supporting both incremental and decremental learning. To the best of our knowledge, this is the first work that considers an in-place unified incremental and decremental learning on GBDT. To reduce the learning cost, we present a collection of optimizations for our framework, so that it can add or delete a small fraction of data on the fly. We theoretically show the relationship between the hyper-parameters of the proposed optimizations, which enables trading off accuracy and cost on incremental and decremental learning. The backdoor attack results show that our framework can successfully inject and remove backdoor in a well-trained model using incremental and decremental learning, and the empirical results on public datasets confirm the effectiveness and efficiency of our proposed online learning framework and optimizations.", "sections": [{"title": "1. Introduction", "content": "Gradient Boosting Decision Tree (GBDT) has demonstrated outstanding performance across a wide range of applications (Biau et al., 2019; Rao et al., 2019; Liu & Yu, 2007; Lin et al., 2024a). It outperforms deep learning models on many datasets in accuracy and provides interpretability for the trained models (Sudakov et al., 2019; Qi et al., 2021a; Wen et al., 2020; Dorogush et al., 2018). However, in traditional setting, all data is simultaneously accessed in training procedure, which makes its application scenarios limited.\nOnline Learning. Online learning is a machine learning approach where data is sequentially available and used to update the predictor for the latest data (Bertsekas, 2015; Parisi et al., 2019; Hazan, 2016; Oza, 2005). Generally, online learning is expected to possess the capabilities of both incremental learning (adding training data) and decremental learning (removing a subset of training data). This allows the model to dynamically adapt to the latest data while removing outdated data. For example, recommender system can incrementally learn latest user behaviors and remove outdated behaviors without training from scratch (Wang et al., 2023; Shi et al., 2024).\nIncremental Learning. There are some challenges for incremental learning in GBDT due to its natural properties (Friedman et al., 2000; Li & Zhao, 2022a;b). Traditional GBDT trains over an entire dataset, and each node is trained on the data reaching it to achieve the best split for optimal accuracy. Adding unseen data may affect node splitting results, leading to catastrophic performance changes.\nMoreover, training gradient boosting models involves creating trees for each iteration, with tree fitting based on the residual of previous iterations. More iterations create more trees, increasing model sizes and hurting inference throughput. This also prohibits tasks like fine-tuning or transfer learning without substantially increasing model sizes.\nRecent studies have explored incremental learning on classic machine learning, such as support vector machine (SVM), random forest (RF), and gradient boosting (GB). Shilton et al. (2005); Laskov et al. (2006); Fine & Scheinberg (2001) proposed methods to maintain SVM optimality after adding a few training vectors. Wang et al. (2009) presented an incremental random forest for online learning with small streaming data. Beygelzimer et al. (2015a) extended gradient boosting theory for regression to online learning. Zhang et al. (2019) proposed iGBDT for incremental learning by \"lazily\" updating, but it may require retraining many trees when the new data size is large. It is important to note that prior studies on online gradient boosting (Beygelzimer et al., 2015a; Chen et al., 2012; Beygelzimer et al., 2015b) and incremental gradient boosting (Zhang et al., 2019; Hu et al., 2017) do not support decremental learning.\nDecremental Learning. Decremental learning is more complex and less studied than incremental learning. Cauwenberghs & Poggio (2000) presented an online recursive algorithm for training SVM with an efficient decremental learning method. Chen et al. (2019) proposed online incremental and decremental learning algorithms based on variable SVM, leveraging pre-calculated results. Brophy & Lowd (2021) and Brophy & Lowd (2020) provided methods for data addition and removal in random forests. Schelter et al. (2021) proposed robust tree node split criteria and alternative splits for low-latency unlearning. Many works have also studied decremental learning in deep neural networks (DNN). Bourtoule et al. (2021) introduced a framework that accelerates decremental learning by constraining individual data points' impact during training.\nWhile online learning has emerged as a popular topic recently, it has been barely investigated on GBDT. Wu et al. (2023); Lin et al. (2023a) are among the latest studies in decremental learning for GBDT. Wu et al. (2023) presented DeltaBoost, a GBDT-like model enabling data deletion. DeltaBoost divides the training dataset into several disjoint sub-datasets, training each iteration's tree on a different sub-dataset, reducing the inter-dependency of trees. However, this simplification may impact model performance. Lin et al. (2023a) proposed an unlearning framework in GBDT without simplification, unlearning specific data using recorded auxiliary information from training. It optimizes to reduce unlearning time, making it faster than retraining from scratch, but introduces many hyper-parameters and performs poorly on extremely large datasets.\nIn this paper, we propose an efficient incremental and decremental learning framework for GBDT. To the best of our knowledge, this is the first work that considers in-place incremental and decremental learning at the same time on GBDT. Additionally, our incremental and decremental learning applies a unified notion, enabling convenient implementation.\nChallenges. We identify three major challenges of in-place online learning for GBDT: (1) Unlike batch training of deep neural networks (DNN), more iterations in GBDT create more trees and parameters, leading to unbounded memory and computation costs in online learning. In-place learning on originally constructed trees is necessary for practicality. (2) Gradient-based methods in DNN add/subtract gradients for incremental and decremental learning, but GBDT is not differentiable. (3) GBDT depends on the residual of the previous tree, unlike independent iterations in random forests. Changing one tree requires modifying all subsequent trees, complicating incremental and decremental learning.\nContributions. (1) We introduce an efficient in-place online learning framework for gradient boosting models supporting incremental and decremental learning, extensible to fine-tuning and transfer learning. (2) We present optimizations to reduce the cost of incremental and decremental learning, making adding or deleting a small data fraction substantially faster than retraining. (3) We theoretically show the relationship among optimization hyper-parameters, enabling trade-offs between accuracy and cost. (4) We experimentally evaluate our framework on public datasets, confirming its effectiveness and efficiency. (5) We release an open-source implementation of our framework."}, {"title": "2. Online GBDT Framework", "content": "2.1. GBDT Preliminary\nGradient Boosting Decision Tree (GBDT) is an powerful ensemble technique that combines multiple decision tree to produce an accurate predictive model (Friedman et al., 2000; Friedman, 2001). Given a dataset $D_{tr} = \\{y_i, x_i\\}_{i=1}^{N}$, where N is the size of training dataset, and $x_i$ indicates the $i^{th}$ data vector and $y_i \\in \\{0, 1, ..., K - 1\\}$ denotes the label for the $i^{th}$ data point. For a GBDT model with M iteration, the probability $P_{i,k}$ for $i^{th}$ data and class k is:\n$P_{i,k} = Pr (Y_i = k|x_i) = \\frac{e^{F_{i,k}(x_i)}}{\\sum_{s=1}^{K} e^{F_{i,s}(x_i)}}, i = 1, 2, ..., N$ (1)\nwhere F is a combination of M terms:\n$F^{(M)}(x) = \\sum_{m=0}^{M-1} \\rho_m h(x; a_m)$ (2)"}, {"title": "2.2. Problem Setting", "content": "For classic GBDT, all training data must be loaded during training, and adding/deleting instances is not allowed afterwards. This work proposes an online GBDT framework that enables in-place addition/deletion of specific data instances to/from a well-trained model through incremental and decremental learning.\nProblem Statement. Given a trained gradient boosting model $T(\\theta)$ on training dataset $D_{tr}$, where $\\theta$ indicates the parameters of model T, an incremental learning dataset $D_{in}$, and/or a decremental learning dataset $D_{de}$ ($D_{de} \\subseteq D_{tr}$), our goal is to find a tree model $T(\\theta')$ that fits dataset $D_{tr} \\cup D_{in} \\backslash D_{de}$, where $|\\theta| = |\\theta'|$ (the parameter size and the number of trees stay unchanged).\nThe most obvious way is to retrain the model from scratch on dataset $D_{tr} \\cup D_{in} \\backslash D_{de}$. However, retraining is time-consuming and resource-intensive. Especially for online learning applications, rapid retraining is not practical. The key question of this problem is: Can we obtain the model $T(\\theta')$ based on the learned knowledge of the original model $T(\\theta)$ without retraining the entire model?\nThe proposed framework aims to find a tree model $T(\\theta')$ as close to the model retraining from scratch as possible based"}, {"title": "2.3. Framework Overview", "content": "The goal of this work is to propose an online learning framework for GBDT that supports incremental learning and decremental learning for any collection of data.\nOnline Learning in GBDT. The Algorithm 2 shows the procedure of online learning in GBDT. At first, the GBDT model is a well-trained model on the training dataset $D_{tr}$. Recall that the GBDT model is frozen and can not be changed after training - no training data modification. In this proposed framework, the user can do (1) incremental learning: update a new dataset $D_{in}$ to the model, and (2) decremental learning: remove a learned dataset $D_{de} \\subseteq D_{tr}$ and its effect on the model.\nAs shown in Algorithm 2, it is similar to the learning process, but it only needs to compute $r_{i,k}$ and $p_{i,k}(1 - p_{i,k})$ for online dataset $D'$ without touching the training dataset $D_{tr}$. Then, it will call the function of incremental learning or decremental learning to obtain $\\{R_{j,k,m}\\}_{j=1}^{J}$. Finally, we update $F_{i,k}$ with new $\\{R_{j,k,m}\\}_{j=1}^{J}$. Here we use the same notion to design the function of incremental learning and decremental learning - decremental learning is the inverse process of incremental learning for dataset $D'$. Therefore, we describe them in the Algorithm 3 at the same time.\nIncremental & Decremental Learning on One Tree. Algorithm 3 describes the detailed process for incremental and decremental learning, which are almost the same as decremental learning is the inverse of incremental learning for dataset $D'$. The main difference is at Line 3. First, we traverse all non-terminal nodes layer by layer from root to leaves. For each node, let s denote the current split. We re-compute the new best gain value with $r_{i,k}$ and $p_{i,k}(1 - p_{i,k})$ after adding D' for incremental learning or removing D' for decremental learning. If the current split s matches the new best split s' (after adding/removing D'), we keep the current split (Figure 1(a)). Otherwise, if the current best split has changed (s \\neq s', Figure 1(c)), we retrain the sub-tree rooted on this node and replace it with the new sub-tree. After testing all nodes, node splits remain on the best split. Finally, we recompute the prediction value on all terminal nodes. Appendix C provides a detailed explanation of Figure 1."}, {"title": "3. Optimizing Learning Time", "content": "In this section, we introduce optimizations for our online learning framework to reduce computation overhead and costs. The key step is deciding whether a node should be kept or replaced: Can we design an algorithm to quickly test whether the node should be retained or retrained without touching the training data? Our most important optimization is to avoid touching the full training dataset. We apply incremental update and split candidates sampling concepts from (Lin et al., 2023a), extend them to support online learning, and provide evidence of the relationship between hyper-parameters of different optimizations, enabling trade-offs between accuracy and cost. Additionally, we design optimizations specific to online learning: 1) adaptive lazy update for residuals and hessians to substantially decrease online learning time; 2) adaptive split robustness tolerance to significantly reduce the number of retrained nodes."}, {"title": "3.1. Update without Touching Training Data", "content": "To reduce computation overhead and online learning time, we target to avoid touching the original training dataset D, and only focus on the online learning dataset D'. Following the study (Lin et al., 2023a), we extend the optimization of updating statistical information to the scenarios of online learning: (1) Maintain Best Split; (2) Recomputing Prediction Value; (3) Incremental Update for Derivatives, and the computation cost is reduced from O(D + D') to O(D') by these optimizations. The implementation of these optimizations are included in Appendix E."}, {"title": "3.2. Adaptive Lazy Update for Derivatives", "content": "Although incremental update can substantially reduce online learning time, we can take it a step further: if no retraining occurs, the changes to the derivatives will be very small. How can we effectively utilize the parameters already learned to reduce online learning time?\nGradient Accumulation (Li et al., 2014; Goyal et al., 2017; Ruder, 2016; Lin et al., 2023b; Qi et al., 2021b) is widely used in DNN training. After computing the loss and gradients for each mini-batch, the system accumulates these gradients over multiple batches instead of updating the model parameters immediately. Inspired by Gradient Accumulation techniques, we introduce an adaptive lazy update for our online learning framework. Unlike Lin et al. (2023a), which perform updates after a fixed number of batches, we update the derivatives only when retraining occurs. This approach uses more outdated derivatives for gain computation but significantly reduces the cost of derivative updates."}, {"title": "3.3. Split Candidates Sampling", "content": "From the above optimizations, if retraining is not required, we can keep the current best split. In this case, we only need to iterate over the online learning dataset D' and update the prediction values to accomplish online learning, whether it involves adding or removing data. However, if the sub-tree rooted in this node requires retraining, it is necessary to train the new sub-tree on the data from the dataset $D_{tr} \\pm D'$ that reaches this node. It is clear that retraining incurs more resource consumption and takes a longer execution time. In the worst case, if retraining is required in the root node, it has to retrain the entire new tree on full dataset $D_{tr} \\pm D'$.\nTo reduce time and resource consumption of online learning, a straightforward approach is to minimize retraining frequency. Therefore, we introduce split candidate sampling to reduce frequent retraining by limiting the number of splits, benefiting both training and online learning. All features are discretized into integers in 0, 1, 2,\uff65\uff65\uff65, B \u2013 1, as shown in Appendix A. The original training procedure enumerates all B potential splits, then obtains the best split with the greatest gain value. In split candidates sampling, we randomly select $\\lfloor \\alpha B \\rfloor$ potential splits as candidates and only perform gain computing on these candidates. As $\\alpha$ decreases, the number of split candidates decreases, resulting in larger distances between split candidates. Consequently, the best split is less likely to change frequently.\nDefinition 1 (Distance Robust) Let s be the best split, and $\\frac{|D'|}{|D_{tr}|} = \\lambda$. $N_\\Delta$ is the distance between s and its nearest split t with same feature, $N_\\Delta = ||t - s||$. s is distance robust if\n$N_\\Delta > \\frac{1}{\\alpha}\\frac{Gain(s)}{\\frac{(\\sum_{x_i \\in l} g_{i,k})^2}{N_{is} \\sum_{x_i \\in l} h_{i,k}} + \\frac{(\\sum_{x_i \\in r} g_{i,k})^2}{N_{rs} \\sum_{x_i \\in r} h_{i,k}}}$ (6)\nwhere l represents the left child of split s, and it contains the samples belonging to this node, while r represent the right child, $N_{is}$ denotes $|l_s|$, and $N_{rs}$ denotes $|r_s|$. In this definition, $E(N_\\Delta) = 1/\\alpha$, where $\\alpha$ denotes the split sampling rate, we can observe that a smaller sampling rate will result in a more robust split, so we can reduce the number of retrain operations by reducing the sampling rate. Similarly, incremental learning can get the same result.\nDefinition 2 (Robustness Split) For a best split s and an split t with the same feature, t \u2260 s, and online learning data rate $\\frac{|D'|}{|D_{tr}|} = \\lambda$, the best split s is robust split if\n$Gain(s) > \\frac{1}{1-\\lambda}Gain(t)$ (7)\nRobustness split shows that, as $\\lambda = \\frac{|D'|}{|D_{tr}|}$ decreases, the splits are more robust, decreasing the frequency of retraining. In conclusion, decreasing either $\\alpha$ or $\\lambda$ makes the split more robust, reducing the change occurrence in the best split, and it can significantly reduce the online learning time."}, {"title": "3.4. Adaptive Split Robustness Tolerance", "content": "Recall the retraining condition for a node that we mentioned previously: we retrain the sub-tree rooted at a node if the best split changes. Although the best split may have changed to another one, the gain value might only be slightly different from the original best split. We show the observation of the distance of best split changes (the changes in the ranking of the best split) in Figure 2. The top row illustrates the distance of best split changes observed in the Adult and Covtype datasets for incremental learning, while the bottom row depicts same in Letter and SUSY datasets for decremental learning. Similar patterns are observed across various other datasets. For adding or deleting a single data point, the best split does not change in most cases. As the |D'| increases to 0.1%, 0.5%, and 1%, the best split in most cases switch to the second best. If we only apply the optimal split, it will lead to frequent retraining during online learning.\nThe distance of the best split changes is usually small. Tolerating its variation within a certain range and continuing to use the original split significantly accelerates online learning. We propose adaptive split robustness tolerance: for a node with $\\lfloor \\alpha B \\rfloor$ potential splits, if the current split is among the top $\\lfloor \\sigma \\alpha B \\rfloor$, we continue using it, where $\\sigma$ (0 \u2264 $\\sigma$ \u2264 1) is the robustness tolerance. $\\sigma$ = 0 selects only the best split, while $\\sigma$ = 1 avoids retraining. Higher $\\sigma$ indicates greater tolerance, making the split more robust and less likely to retrain. We recommend setting $\\sigma$ to approximately 0.1."}, {"title": "4. Experimental Evaluation", "content": "In this section, we compare 1) our incremental learning with OnlineGB2 (Leistner et al., 2009) and iGBDT (Zhang et al., 2019); 2) decremental learning with DeltaBoost (Wu et al., 2023) and MUinGBDT (Lin et al., 2023a); 3) training cost with popular GBDT libraries XGBoost (Chen & Guestrin, 2016), LightGBM (Ke et al., 2017), CatBoost (Dorogush et al., 2018) and ThunderGBM (Wen et al., 2020).\nImplementation Details. The details of environments and settings are included in Appendix B. We employ one thread for all experiments to have a fair comparison, and run ThunderGBM on a NVIDIA A100 40GB GPU, since it does not support only CPU3. Unless explicitly stated otherwise, our default parameter settings are as follows: $\\nu$ = 1, M = 100, J = 20, B = 1024, |D'| = 0.1% \u00d7 |Dtr|, \u03b1 = 0.1, and \u03c3 = 0.1.\nDatasets. We utilize 10 public datasets in the experiments. The specifications of these datasets are presented in Table 1. The smallest dataset, Optdigits, consists of 3,822 training instances, while the largest dataset, HIGGS, contains a total of 11 million instances. The number of dimensions or features varies between 8 and 87 across the datasets."}, {"title": "4.1. Training Time and Memory Overhead", "content": "Since the proposed online learning framework stores statistical information during training, this may impact both the training time and memory usage. Table 2 presents a detailed report of the total training time and memory overhead.\nTraining Time. Table 2 shows the total training time of our framework and baselines. Our online learning framework is substantially faster than OnlineGB, DeltaBoost, and XGBoost, and slightly slower than iGBDT. While slower on smaller datasets compared to LightGBM, it outperforms on larger datasets like SUSY and HIGGS, with training times similar to MUinGBDT. Overall, our framework offers significantly faster training than existing incr./decr. methods and is comparable to popular GBDT libraries.\nMemory Overhead. Memory usage is crucial for practical applications. Most incremental and decremental learning methods store auxiliary information or learned knowledge during training, potentially occupying significant memory."}, {"title": "4.2. Online Learning Time", "content": "Retraining from scratch can be time-consuming, but in some cases, the cost of online learning outweighs the benefits compared to retraining from scratch, making online learning unnecessary or unjustified. Hence, evaluating the cost of online learning is crucial for practical applications. Table 3 shows the total online learning time (seconds) and speedup v.s. baselines, comparing OnlineGB and iGBDT for incremental learning, and DeltaBoost and MUinGBDT for decremental learning.\nIn incremental learning, compared to OnlineGB and iGBDT, which also support incremental learning, adding a single data instance can be up to 254.4x and 17x faster, respectively. Furthermore, compared to retraining from scratch on XGBoost, LightGBM, CatBoost, and ThunderGBM (GPU), it can achieve speedups of up to 974.3x, 58.2x, 64.9x, and 27.6x, respectively. In decremental learning, when deleting a data instance, our method offers a speedup of 1,619.9x and 6.4x over DeltaBoost and MUinGBDT, respectively, and is 1,254.7x, 74.9x, 90.2x, and 29.6x faster than XGBoost, LightGBM, CatBoost, and ThunderGBM (GPU).\nOur method is substantially faster than other methods both in incremental and decremental learning, especially on large datasets. For example, in HIGGS dataset, the largest dataset in experiments, on removing (adding) 1% data, we are 3.1x faster than MUinGBDT (2.6x faster than iGBDT), while OnlineGB and DeltaBoost encounter out of memory (OOM)."}, {"title": "4.3. Batch Addition & Removal", "content": "In the traditional setting, GBDT models must be trained in one step with access to all training data, and they cannot be modified after training - data cannot be added or removed. In our proposed online learning framework, GBDT models support both incremental and decremental learning, allowing continual batch learning (data addition) and batch removal, similar to mini-batch learning in DNNs.\nWe conducted experiments on continual batch addition and removal by dividing the data into 20 equal parts, each with 5%|Dtr|."}, {"title": "4.4. Verifying by Backdoor Attacking", "content": "Backdoor attacks in machine learning refers to a type of malicious manipulation of a trained model, which is designed to modify the model's behavior or output when it encounters a specific, predefined trigger input pattern (Salem et al., 2022; Saha et al., 2020; Lin et al., 2024b). In this evaluation, we shows that our framework can successfully inject and remove backdoor in a well-trained, clean GBDT model using incremental learning and decremental learning. The details of backdoor attack experiments are provided in Appendix J.\nIn this evaluation, we randomly selected a subset of the training dataset and injected triggers into it to create a backdoor training dataset, leaving the rest as the clean training dataset. The test dataset was similarly divided into backdoor and clean subsets. We report the accuracy for clean test dataset and attack successful rate (ASR) for backdoor test dataset in Table 4. Initially, we trained a model on the clean training data (\"Train Clean\u201d), which achieved high accuracy on the clean test dataset but low ASR on the backdoor test dataset. We then incrementally add the backdoor training data with triggers in to the model (\u201cAdd Backdoor\"). After incremental learning, the model attained 100% ASR on the backdoor test dataset, demonstrating effective learning of the backdoor data. For comparison, training a model on the combined clean and backdoor training datasets (\"Train Backdoor\") yielded similar results to \"Add Backdoor\". Finally, we removed the backdoor data using decremental learning (\u201cRemove Backdoor\"), reducing the ASR to the level of the clean model and confirming the successful removal of backdoor data."}, {"title": "4.5. Additional Evaluations", "content": "To further validate our method's effectiveness and efficiency, we have included comprehensive additional evaluations in the Appendix due to page limitations:\n\u2022 Time Complexity Analysis: We analyze the computational complexity of our proposed framework compared to retraining from scratch in Appendix F.\n\u2022 Test Error Rate: We compare the test error rate between our method and baselines in Appendix G.\n\u2022 Real-world Time Series Evaluation: To confirm the performance of our methods on real-world datasets with varying data distributions, we conducted experiments on two time series datasets as included in Appendix H.\n\u2022 Extremely High-dimensional Datasets: To confirm the scalability of our framework, we report the experiments for two extremely high-dimensional datasets, RCV1 and News20, in Appendix L.\n\u2022 Model Functional Similarity: We evaluate the similarity between the model learned by online learning and the one retrained from scratch in Appendix I.\n\u2022 Approximation Error of Leaf Scores: Since our framework might use the outdated derivatives in the gain computation, to assess the effect of these outdated derivatives, we report the approximation error of leaf scores between the model after addition/deletion and the one retrained from scratch in Appendix N.\n\u2022 Data Addition with More Classes: Our framework supports incremental learning for previously unseen classes. Detailed results and analysis are provided in Appendix M.\n\u2022 Membership Inference Attack: Additional to backdoor attack, we also confirm the effectiveness of our method on adding/deleting data by membership inference attack (MIA) in Appendix K.\n\u2022 Ablation Study: We report the ablation study for different hyper-parameter settings in Appendix O."}, {"title": "5. Related Work", "content": "Incremental Learning is a technique in machine learning that involves the gradual integration of new data into an existing model, continuously learning from the latest data to ensure performance on new data (van de Ven et al., 2022). It has been a open problem in machine learning, and has been studied in convolutional neural network (CNN) (Polikar et al., 2001; Kuzborskij et al., 2013; Zhou et al., 2022), DNN (Hussain et al., 2023; Dekhovich et al., 2023), SVM (Chen et al., 2019; Cauwenberghs & Poggio, 2000) and RF (Wang et al., 2009; Brophy & Lowd, 2020). In gradient boosting, iGBDT offers incremental updates (Zhang et al., 2019), while other methods (Beygelzimer et al., 2015a; Babenko et al., 2009) extend GB to online learning. However, these methods do not support removing data.\nDecremental Learning allows for the removal of trained data and eliminates their influence on the model, which can be used to delete outdated or privacy-sensitive data (Bourtoule et al., 2021; Nguyen et al., 2022; Sekhari et al., 2021; Xu et al., 2024). It has been researched in various models, including CNN (Poppi et al., 2023; Tarun et al., 2021), DNN (Chen et al., 2023; Thudi et al., 2022), SVM (Karasuyama & Takeuchi, 2009; Cauwenberghs & Poggio, 2000), Naive Bayes (Cao & Yang, 2015), K-means (Ginart et al., 2019), RF (Schelter et al., 2021; Brophy & Lowd, 2021), and GB (Wu et al., 2023; Zhang et al., 2023). In random forests, DaRE (Brophy & Lowd, 2021) and a decremental learning algorithm (Schelter et al., 2021) are proposed for data removal with minimal retraining and latency.\nHowever, in GBDT, trees in subsequent iterations rely on residuals from previous iterations, making decremental learning more complicated. DeltaBoost Wu et al. (2023) simplified the dependency for data deletion by dividing the dataset into disjoint sub-datasets, while a recent study Lin et al. (2023a) proposed an efficient unlearning framework without simplification, utilizing auxiliary information to reduce unlearning time. Although effective, its performance on large datasets remains unsatisfactory."}, {"title": "6. Conclusion", "content": "In this paper, we propose an efficient in-place online learning framework for GBDT that support incremental and decremental learning: it enables us to dynamically add a new dataset to the model and delete a learned dataset from the model. It support continual batch addition/removal, and data additional with unseen classes. We present a collection of optimizations on our framework to reduce the cost of online learning. Adding or deleting a small fraction of data is substantially faster than retraining from scratch. Our extensive experimental results confirm the effectiveness and efficiency of our framework and optimizations - successfully adding or deleting data while maintaining accuracy."}, {"title": "Impact Statement", "content": "This paper introduces an online learning framework for GBDTsthat enables both incremental and decremental learning.While the framework offers significant potential benefits,such as adapting models to the latest data and supportingdata deletion requests, it also comes with some limitationsand risks that would need to be carefully considered in practical applications. The ability to manipulate models throughtargeted addition or deletion of data introduces new security vulnerabilities. More analysis is needed to understandscalability limitations and to explore the practical implications, especially for sensitive use cases. Future work shouldfurther discuss the deployment considerations and developstrategies to mitigate the risks while realizing the benefitsof online learning capabilities."}, {"title": "A. Feature Discretization.", "content": "The preprocessing step of feature discretization plays a crucial role in simplifying the implementation of Eq. (5) and reducing the number of splits that need to be evaluated. This process involves sorting the data points based on their feature values and assigning them to bins, taking into account the distribution of the data, as shown in Figure 4 and Algorithm 4. By starting with a small bin-width (e.g., 10-8) and a predetermined maximum number of bins B (e.g., 1024). It assigns bin numbers to the data points from the smallest to the largest, carefully considering the presence of data points in each bin. This iterative process continues until the number of bins exceeds the specified maximum.\nIn cases where the number of required bins surpasses the maximum limit, the bin-width is doubled, and the entire process is repeated. This adaptive discretization approach proves particularly effective for boosted tree methods, ensuring that feature values are mapped to integers within a specific range. Consequently, after the discretization mapping is established, each feature value is assigned to the nearest bin. After this discretization preprocessing, all feature values are integers within {0, 1, 2, \u2026\u2026\u2026, B \u2013 1}.\nThe advantage of this discretization technique becomes evident during the gain searching step. Instead of iterating over all N feature values, the algorithm only needs to consider a maximum of B splits for each feature. This substantial reduction in the number of splits to evaluate leads to a significant decrease in the computational cost, transforming it from being dependent on the dataset size N to a manageable constant B."}, {"title": "B. Experiment Setting", "content": "The experiments are performed on a Linux computing node running Red Hat Enterprise Linux 7, utilizing kernel version 5.10.155-1.el7.x86_64. The CPU employed was an Intel(R) Xeon(R) Gold 6150 CPU operating at a clock speed of 2.70GHz, featuring 18 cores and 36 threads. The system was equipped with a total memory capacity of 376 GB. We have built a prototype of our online learning framework using C++11. The code is compiled with g++-11.2.0, utilizing the \"O3\u201d optimization. Unless explicitly stated otherwise, our default parameter settings are as follows: J = 20, \u0412 = 1024, |D'| = 0.1% \u00d7 |Dtr|, \u03b1 = 0.1, and \u03c3 = 0.1. We report the ablation study for different settings in Appendix O."}, {"title": "C. Framework Overview", "content": "Figure 1 is a visual example of incremental and decremental learning of our proposed framework. Figure 1(b) is one tree of the GBDT model and has been well-trained on dataset Dtr = {0, 1, 2, 3..., 19}. Every rectangle in the tree represents a node, and the labels inside indicate the splitting criteria. For instance, if the condition Age < 42 is met, the left-child node is followed; otherwise, the right-child node is chosen. The numbers within the rectangles represent the prediction value of the terminal nodes. Please note that here the feature 42 is a discretized value, instead of the raw feature. Our online learning framework has the capability to not only incrementally learn"}]}