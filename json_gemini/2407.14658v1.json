{"title": "A New Lightweight Hybrid Graph Convolutional\nNeural Network - CNN Scheme for Scene\nClassification using Object Detection Inference", "authors": ["Ayman Beghdadi", "Azeddine Beghdadi", "Mohib Ullah", "Faouzi Alaya Cheikh", "Malik Mallem"], "abstract": "Scene understanding plays an important role in sev-\neral high-level computer vision applications, such as autonomous\nvehicles, intelligent video surveillance, or robotics. However, too\nfew solutions have been proposed for indoor/outdoor scene classi-\nfication to ensure scene context adaptability for computer vision\nframeworks. We propose the first Lightweight Hybrid Graph\nConvolutional Neural Network (LH-GCNN)-CNN framework as\nan add-on to object detection models. The proposed approach\nuses the output of the CNN object detection model to predict the\nobserved scene type by generating a coherent GCNN representing\nthe semantic and geometric content of the observed scene. This\nnew method, applied to natural scenes, achieves an efficiency\nof over 90% for scene classification in a COCO-derived dataset\ncontaining a large number of different scenes, while requiring\nfewer parameters than traditional CNN methods. For the benefit\nof the scientific community, we will make the source code publicly\navailable: https://github.com/Aymanbegh/Hybrid-GCNN-CNN.\nIndex Terms-Deep learning, Graph neural network, In-\ndoor/outdoor, Scene classification, Scene understanding", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, mobile and robotic systems have achieved a\nconsiderable level of autonomy thanks to the development of\nnew computer vision methods based on deep learning. Indeed,\nover the last decade, the evolution of deep learning techniques\nhas enabled us to perform increasingly complex and diversified\ntasks such as object detection and segmentation [1], object\nand action recognition [2]. Autonomous systems are capable\nof performing complex tasks in unfamiliar environments with\ngood reliability. This has enabled us to offer a solution for a\nwide range of fields.\nIn the context of autonomous navigation based on computer\nvision, i.e. visual SLAM (vSLAM), most existing methods\n[3], [4] are dedicated to a specific type of environment,\nindoor or outdoor. Most visual SLAM frameworks incorporate\nobject detection and/or segmentation models, enabling scene\ncontent analysis for complex tasks. In particular, the need for\nvSLAM methods to be robust to real environments containing\ndynamic objects has led to a rethinking of these methods.\nIndeed, the complexity of the tasks to be performed and of\nthe environments in which autonomous systems operate has\nled us to impose strong assumptions on the system.\nIn general, dynamic VSLAM methods [5], [6] include as-\nsumptions linked to the application framework to optimize\nthe efficiency of the complex tasks to be carried out. These\nmethods consider assumptions about the dynamic state of\ncertain objects as a function of the type of environment\n(indoor/outdoor) to not consider features from dynamic areas.\nIn most cases, these assumptions are immutable and for a\nsingle specific setting, which is particularly restrictive.\nConsequently, we propose a solution for dynamically\nadjusting these assumptions according to the type of scene\n(indoor/outdoor) by using an hybrid GCNN-CNN framework\nas shown in Fig.1. To this end, we have developed a GCNN\nmodel that performs scene classification (indoor/outdoor) by\nexploiting spatial and semantic information obtained from\nCNN object detection/segmentation models. This approach\noffers an efficient solution for extracting scene features and\ncontext relationships by exploiting the advantages of graphs\nas an efficient tool for representing complex data. To the best\nof our knowledge, this work is the first to address this issue\nby combining CNN and GCNN models for predicting scene\ntype of natural scenes.\nThe proposed model can be integrated as a framework\nwithin the well-known YOLACT [7] object detection or\nsegmentation model. It can also be extended to any model\nthat outputs labels and bounding boxes for detected objects.\nFurthermore, the introduced GCNN model can be added as\na network header to any object detection/segmentation model,\nmaking it easier to use in various computer vision applications.\nThe main contributions of this work are:\n\u2022 A Lightweight Hybrid GCNN-CNN framework for scene\nclassification is proposed, with predictive accuracy of\nover 90% and a much lower number of parameters than\nCNN models.\n\u2022 A GCNN-based solution for non-satellite image scene\nclassification is introduced for the first time and validated\nthrough extensive experiments on real data.\n\u2022 An efficient graph construction strategy exploiting both\nsemantic and spatial information of objects, and their\ndistribution in the scene is proposed.\n\u2022 A new GCNN-based framework incorporating the seman-\ntic contents and the spatial distribution of objects for\nbetter extraction of scene features and visual information\ncontext, i.e. objects relationship, in order to learn intrinsic\nattributes of the observed scenes is proposed.\n\u2022 A complete and flexible open-source code that can be\neasily integrated with any object detection/segmentation\nmodel for solving various computer vision problems is\nprovided.\nThe paper is organized as follows. Section II summarizes the\nrelated literature. Section III is devoted to detail our method.\nResults are presented and discussed in section IV. Conclusions\nand perspectives are provided in section V."}, {"title": "II. RELATED WORKS", "content": "The classification of scenes is a hot research topic, dating\nback to at least the 90s [8]. Despite the number of works\ndedicated to this subject, whether for indoor or outdoor scene\nclassification or other contexts, it remains an open research\nproblem. In what follows, we will limit ourselves to a few\npublished works that we feel are representative of the three\ndecades of research on indoor-outdoor scene classification.\nA. Indoor/outdoor scene classification\nWith the advent of deep learning techniques, great progress\nhas been made in the field of image classification, and several\nsurvey papers have highlighted this [9], [10]. In the case of\nindoor/outdoor scene classification (binary decision), although\nthis appears simpler than image classification in the broad\nsense, there are cases where the classification error is not\nnegligible. Indeed, recognizing and classifying objects in an\nimage may be less difficult than classifying indoor and outdoor\nscenes because of the difficulty in discerning between percep-\ntually and physically similar visual information that may exist\nin both indoor and outdoor scenes. Given the large amount of\nwork in the field and the trend over the last ten years, it seems\nmore appropriate to classify the methods into two categories:\ntraditional and deep learning-based.\nTraditional approaches This category includes Bayesian\nframework approaches [11], clustering methods based on low-\nlevel features of bag of words [12], conventional learning-\nbased methods such as SVM-based approaches [13], random\nForest classifiers [14] and other approaches such as those\nbased on the interesting concept of semantic typicality intro-\nduced in by Vogel and Schiele in [15].\nDeep learning based methods With the introduction of\ndeep neural learning approaches, real progress has been made\nin recent years in solving scene classification problems, in\ngeneral, [16] and especially in the case of Remote-Sensing\n[17]. But in the case of indoor/outdoor scene classification,\ndespite the progress made, the problem still remains open\ndue to its complexity [16]. The other difficulty is the fact\nthat the performance of deep learning-based methods depends\non the quality of the databases used in training. It is also\nworth noting the lack of sufficient databases of labelled indoor-\noutdoor scenes available to the public, as highlighted in [9].\nB. GCNN-based scene classification\nIn this work, we focus on a few recent and representative\nstate-of-the-art (SOTA) methods based on GCN architecture.\nLiang et al. [18] propose a method that learns discrimina-\ntive features of the scene by using the GCNN-CNN model.\nThis approach extracts global-based visual features of scenes\nthrough a CNN architecture and then constructs graphs to learn\nthe object-based location features. This method seems efficient\nfor aerial and satellite views but unsuitable for common natural\nscenes due to its inability to exploit the spatial distribution of\nobjects and their characteristic information.\nLi et al. [19] construct a new remote sensing knowledge\ngraph (RSKG) to recognize unseen remote sensing image\nscenes. It introduces a generation of Semantic Representation\nof scene categories used by a deep alignment network (DAN)\nto exploit cross-modal matching between visual features and\nsemantic representations. However, this approach does not take\ninto account the spatial information of objects or their dimen-\nsions when creating graphs, which also makes it unsuitable\nfor natural scenes.\nIn [20], the proposed method exploits the benefits of GCN\nto better extract potential context relationships between objects\nin the observed scene. The proposed framework exploits only\nsemantic scene information and spatial object distribution.\nHowever, this approach does not consider the object's intrinsic\nspatial information, making it ineffective for terrestrial images.\nIt should be noted that most indoor/outdoor scene classification\nmethods do not jointly incorporate information on the size of\nobjects, as well as proximity characteristics and links between\nthe various attributes of the image components observed in the\nlearning process design.\nMoreover, these methods are based on a graph construction\nstrategy that does not incorporate the most relevant intrinsic\nscene attributes, such as spatial and context relationships"}, {"title": "III. METHOD", "content": "The overall architecture of our new Lightweight Hybrid\nGraph Convolutional Neural Network - CNN Scheme (LH-\nG2CNN) is illustrated in Fig.2. The proposed model consists\nof a CNN object detection model that conveys semantic and\nspatial information to a GCNN model for boosting the scene\nclassification process.\nA. Problem formulation\nThe proposed method of scene classification by means of\nobject detection inference aims to produce a learning-based\nclassifier by using semantic information and spatial consis-\ntency of the scene. First, the output prediction of the object\ndetection inference provides a set of n labels L and bounding\nboxes B describing the contents of the scene. Then, we use\nthese characterizing features to produce a scene description\nexploiting the semantic and spatial scene contents and the\nspatial relation between the bounding boxes. Each object\nprediction provides a vector\n$P_i = {l_i, X_i, Y_i, W_i, h_i}^T$\nwhere $l_i$ is the predicted object label, $x_i$ and $y_i$ the position\nof the top left corner of the predicted bounding boxes, and $w_i$\nand $h_i$ its width and height. The spatial information contained\nwithin B consists of the information on scene geometry and\nobjects' size. First, we use these predicted features to compute\nthe Euclidean distances $A_{i,j}$ between bounding boxes. This\nEuclidean distance provides information on the geometric\nand spatial relationships between objects in the scene. This\nextracted information enables us to characterize the spatial\narrangement between objects in the scene through geometric\nreasoning. The object size, expressed as the bounding boxes\ndiagonals $d_i$ given by $d_i = \\sqrt{w_i^2 + h_i^2}$, represents an addi-\ntional geometric attribute for the graph construction process.\nThis information, combined with the semantic information,\nprovides a priori knowledge about the nature of the scene\nthrough the relationship between size and object type. Indeed,\na significant fluctuation in the size of objects of the same\nlabel contained in a scene induces a significant scene depth,\nincreasing the probability of corresponding to an outdoor\nscene conversely. Therefore, our approach attempts to combine\nsemantic and spatial reasoning into a coherent graph neural\nnetwork.\nB. Construction of the Space-Semantic Graph\nThe spatial semantic consistency of the detected object is\nintegrated in a graph G composed of edges E and nodes\nN that represent the main scene information. The strength\nof our approach is in attributing both spatial and semantic\ninformation to nodes and spatial information to edges, which\nbetter describes the scene contents.\nFrom a semantic point of view, only predicted labels L are\nconsidered to describe the node's attributes. The graph can\nbe defined as G = {V,E}, where nodes are expressed as\nV = {Vi : {li, di}|i \u2208 {1, ..., n}} integrating information from\nlabels and bounding boxes. Edges E describe the spatial distri-.\nbution of objects through their Euclidean distance. However,\nunlike most other approaches, not all nodes are connected by\nedges to maintain the coherence of the spatial distribution of\nobjects.\nWe determine the nearest neighbours of a node i by com-\nparing its distance $A_{i,j}$ for each node j \u2260 i. The distance with\nits nearest node $A_{imin}$ is expressed as follows:\n$A_{imin} = min A_{i,j}$\n                                                                                                                                  (1)\n$j<n$"}, {"title": "Algorithm 1 Space-semantic Graph construction algorithm", "content": "Algorithm 1 Space-semantic Graph construction algorithm\nInput: n predictions { Labels L, Bounding boxes B}, distance\nratio: \u03b2\nOutput: Graph G\nCompute each object diagonal d\nCompute each distance Ai,j between objects\nDetermine distance Aimin of nearest object\nfor i = 1 to n do\nCreate Node Vi\nAssign to node Vi\u2190 {li, di}\nfor j = 1 to n do\nif j\u2260 i then\nif \u2206i,j < \u2206imin + \u03b2\u00b7 \u2206imin then\nCreate edge Ei,j\nAssign to edge Ei,j\u2190 Ai,j\nend if\nend if\nend for\nend for\nG\u2190 {V, E}\nThe minimum distance is used to define the node connections\nof the graph. Only nodes at a distance close to Aimin up to a\ncertain distance ratio \u1e9e are considered in the process of graph\nedges' construction. The construction of a space-semantic\nGraph corresponding to an observed scene is described in\nAlgorithm 1. As a result, nodes are only connected by edges\nto their nearest neighbours with V = {Vi,j : {\u2206i,j}|i,j\u2208\n{1, ..., n}}. This nodes connectivity through specific edges is\nillustrated in Fig. 3. The nodes connectivity is represented\nby an adjacency matrix A representing existing edges. A is\nsymmetric, but only some edges are created and have attributed\nvalues as shown in Tab.I."}, {"title": "C. Graph Convolutional Neural Network models", "content": "Graph Convolutional Neural Networks (GCNN) models\nhave been introduced to solve various problems in computer\nvision, especially for visual data classification, human action\nrecognition, and face recognition, to name a few. In the\nfollowing section, the most common GCNN models are briefly\ndescribed.\n1) Graph Convolutional Network model: First, we propose\nthe Graph Convolutional Network (GCN) model [21] as a\ngraph-based neural network model for our scene classification\ntask. GCN is a multi-layer network that takes graphs G\ncomposed of nodes V and edges E. A diagonal degree matrix\n\u010e is expressed from the adjacency matrix A as follows:\n$D_{ii} = \\sum_j A_{ij}$                                                                                                                     (2)\nWhere A is the adjacency matrix of the graph G. The update\nstate in the GCN layer is expressed as follows:\n$h^{(k+1)} = o(\u010e^{-\\frac{1}{2}}\u00c3\u010e^{-\\frac{1}{2}}h^{(k)}W^{(k)})$                                                                                                                                                                                                                                 (3)\nWhere o is an activation layer, $h^{(k)}$ denotes activation of the\nkth layer, $W^{(k)}$ denotes the learnable weights in the layer k,\nand the first layer $h^{(0)} = V$.\n2) Graph Isomorphism Network model: Given that CNN\narchitectures do not allow contextual relations to be extracted\nfrom a scene, we have used the Graph Isomorphism Network\n(GIN) model [22]. The GIN model is a variant of the GCN\nmodel [21], which enables the differentiation of graphs that\nare not isomorphic to each other. The computational model\nof the GIN update nodes in the graph convolutional layer is\nexpressed as follows:\n$h_v^{(k)} = MLP^{(k)}((1+e^{(k)}) .h_v^{(k-1)} + \\sum_{u\\in N(v)}h_u^{(k-1)})$                                                                                                                                                                                                                                 (4)\nWhere $h_v^{(k)}$ denotes the feature representation of node v of\nthe kth hidden layer, $h_u^{(k-1)} = A_v$, MLP denotes Multilayer\nPerceptron, and e(k) is a learnable parameter. Here, A is the\nattribute matrix of the vth node.\nIn addition, GIN has the advantage of concatenating infor-\nmation nodes across all layers of the model. This readout func-\ntion enables the graph classification by using individual node\ninformation. The extracted intrinsic attributes of the scene\nare fed into the LogSoftmax layer to obtain the probability\nof the scene type for each class (indoor/outdoor), with the\nLogSoftmax function expressed as follows:\n$LogSoftmax(x_i) = log(\\frac{exp(x_i)}{\\sum_j (exp(x_i))})$                                                                                                                                                                                                                                 (5)\n3) LAF Aggregation Module: Pellegrini et al. [23] propose\na learnable aggregation function (LAF) corresponding to a\ngeneralized form of the $L_p$-norms, expressed as follows:\n$L_{a,b}(x) := (\\sum_i x_i^a)^{\\frac{b}{a}} (a, b \\geq 0)$                                                                                                                                                                                                                                 (6)\nWhere x = {x1,...,xN} denotes a finite multi-set of real\nnumbers, $L_{a,b}$ is invariant under the addition of zeros such\nas $L_{a,b}(x) = L_{a,b}(x\\cup 0)$, with 0 being a multi-set of zero\ncardinality. The LAF layer can be defined as a restricted LAF\nfunction for sets x \u2208 [0,1]:\n$LAF(x) := \\frac{\\alpha L_{a,b}(x) + \\beta L_{c,d}(1 \u2013 x)}{\\gamma L_{e,f}(x) + \\delta L_{g,h}(1 \u2212 x)}$                                                                                                                                                                                                                                 (7)\nWhere a, b, ..., h \u2265 0 are tunable parameters, and \u03b1, \u03b2, \u03b3, \u03b4\u03b5\n\u2208 R. Based on the Learnable Aggregation Function layer [23],\nwe use the GINLAF model."}, {"title": "IV. EXPERIMENTS", "content": "In this section, all the experiments carried out to train,\noptimize, and evaluate the proposed method are provided and\ndiscussed. The experiments were conducted on a computer\nwith an Intel Xeon CPU and a NVIDIA Tesla T4 GPU.\nA. Dataset\nTraining and evaluation were done on an MS-COCO [24]\nderived dataset providing scene type annotations, namely CD-\nCOCO [25]. This dataset offers a major contribution by\nexploiting the advantages of the famous MS-COCO dataset,\nnamely the very large number of annotated images containing\nobject labels, bounding boxes and masks. In addition, the CD-\nCOCO dataset provides the scene type, indoor/outdoor, of each\nimages. Consequently, it is not necessary to use an object\ndetection model to provide inputs to train our GCNN scene\nclassification model.\nThis database is divided into subsets of 72K, 9K, and 9K\nimages for the training, validation, and test sets, respectively. It\nis important to note that the images contained in the database\nare distorted, which reduces the effectiveness of the YOLACT\nmodel for object detection.\nB. Training\nThe training was carried out on several GCN variant models\nto fully investigate the capabilities of our method. In our case\nstudy, we trained the GCN, GIN, and GINLAF models on\nmultiple dimensions, which allowed us to conduct a compre-\nhensive comparative study. All models have been trained using\nthe cross-entropy loss function to learn the parameter values.\nThe learning rate Lr, the weight decay dd and the number\nof epochs are set to 0.001, 5e-4 and 10, respectively. For the\nGCN and GIN models, the dimension of the hidden layers is\nset to 1024, while that of the GINLAF model is 32. Finally,\nthe batch size is set to 64 for models.\nC. Proposed GCNN-based model results\nFirst, tests were carried out using the database groundtruth\n(labels and bounding boxes) directly as input to our GCNN\nmodel. A detailed study was then carried out to assess the\nimpact of the \u1e9e distance ratio on the performance of each\nmodel, as shown in Fig.4. The optimal \u1e9e value allows an\nincrease of 1.22%, 0.75%, and 0.88% for the GCN, GIN, and\nGINLAF models, respectively. This empirical study highlights\nthe optimal performance for \u03b2 = 0.1, which will be used for\nfurther experimentation.\nSecondly, the effect of the number of object classes present\nin the observed scene on performance was evaluated. Indeed,\nthe semantic content of a scene indirectly informs its intrinsic\nattributes. A scene composed of a large number of classes\nallows better extraction of the semantic relationships between\nobjects, leading to a better classification, as shown in Tab.II.\nHowever, this ablation study does not consider the number of\nobjects detected. The GIN model seems to be the most robust\nto variations in the semantic content of the observed scene,\nwhile the GINLAF model offers the best performance. A\ncomplete benchmark of various CNN and Vision Transformer\n(ViT) models has been performed to highlight the effectiveness\nand contribution of our method (see Tab.III).\nThe accuracy, number of parameters and running time of\neach model have been determined to provide a complete and\nconsistent comparison between the methods. This evaluation\nhighlights the low computational cost of generating predictions\nusing our method compared to CNN and ViT approaches for\nsimilar results (see Table.III). Indeed, our approach requires\n100 times fewer parameters and inference speed is 66 times\nfaster. These performances justify the contribution of the\napproach, which guarantees good accuracy for a computational\ntime suitable for its integration in an object detection model.\nD. YOLACT-GCNN framework results\nOur YOLACT-GCNN framework has been implemented\nand tested for the optimal \u1e9e parameter. In contrast to the\nprevious study, using the outputs of the object detection\nmodel as inputs to the GCNN model induces an effect of the\nYOLACT [7] model performance on the classification perfor-\nmance. It should be noted that the object detection accuracy\nof the YOLACT model is reduced due to disturbances in\nthe images contained in the CD-COCO database. In addition,\nincreasing the number of object classes detected does not\nresult in a proportional increase in the number of objects\ndetected. The GCN model gains accuracy as the semantic\ncontent increases in terms of the number of different classes\npresent in the observed scene. As shown in Table IV, the GIN\nmodel behaves in the opposite way, regressing as the semantic\ncontent increases."}, {"title": "V. CONCLUSION", "content": "Through this study, we have proposed a new lightweight\nGCNN-CNN models that allows us to overcome the limitations\nof CNN-based methods by incorporating semantic informa-\ntion, object proximity, and spatial information in the graph\ndesign. In addition, the proposed method is the first GCN\nmodel that can be used as an add-on to any object detection\nmodel. The experiment carried out on a dedicated dataset\nclearly demonstrates the efficacy of the proposed approach in\nindoor/outdoor scene classification.\nOur method achieves results that are very slightly lower\nthan traditional CNN methods with a much lighter and easier-\nto-deploy network. One limitation of the proposed solution is\nthat it is sensitive to the reliability of the object detection\nmodule and the semantic nature of the scene complexity.\nMeaning that the more objects the scene contains the higher\nthe accuracy achieved. It is worth noticing that the study could\nhelp design GCNN-based models for solving various computer\nvision problems. Finally, this new proposed solution could be\nconsidered as a starting point for GCNN-based non-satellite\nscene classification."}]}