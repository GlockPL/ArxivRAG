{"title": "A New Lightweight Hybrid Graph Convolutional Neural Network - CNN Scheme for Scene Classification using Object Detection Inference", "authors": ["Ayman Beghdadi", "Azeddine Beghdadi", "Mohib Ullah", "Faouzi Alaya Cheikh", "Malik Mallem"], "abstract": "Scene understanding plays an important role in several high-level computer vision applications, such as autonomous vehicles, intelligent video surveillance, or robotics. However, too few solutions have been proposed for indoor/outdoor scene classification to ensure scene context adaptability for computer vision frameworks. We propose the first Lightweight Hybrid Graph Convolutional Neural Network (LH-GCNN)-CNN framework as an add-on to object detection models. The proposed approach uses the output of the CNN object detection model to predict the observed scene type by generating a coherent GCNN representing the semantic and geometric content of the observed scene. This new method, applied to natural scenes, achieves an efficiency of over 90% for scene classification in a COCO-derived dataset containing a large number of different scenes, while requiring fewer parameters than traditional CNN methods. For the benefit of the scientific community, we will make the source code publicly available: https://github.com/Aymanbegh/Hybrid-GCNN-CNN.", "sections": [{"title": "I. INTRODUCTION", "content": "Nowadays, mobile and robotic systems have achieved a considerable level of autonomy thanks to the development of new computer vision methods based on deep learning. Indeed, over the last decade, the evolution of deep learning techniques has enabled us to perform increasingly complex and diversified tasks such as object detection and segmentation [1], object and action recognition [2]. Autonomous systems are capable of performing complex tasks in unfamiliar environments with good reliability. This has enabled us to offer a solution for a wide range of fields.\nIn the context of autonomous navigation based on computer vision, i.e. visual SLAM (vSLAM), most existing methods [3], [4] are dedicated to a specific type of environment, indoor or outdoor. Most visual SLAM frameworks incorporate object detection and/or segmentation models, enabling scene content analysis for complex tasks. In particular, the need for vSLAM methods to be robust to real environments containing dynamic objects has led to a rethinking of these methods. Indeed, the complexity of the tasks to be performed and of the environments in which autonomous systems operate has led us to impose strong assumptions on the system.\nIn general, dynamic VSLAM methods [5], [6] include assumptions linked to the application framework to optimize the efficiency of the complex tasks to be carried out. These methods consider assumptions about the dynamic state of certain objects as a function of the type of environment (indoor/outdoor) to not consider features from dynamic areas. In most cases, these assumptions are immutable and for a single specific setting, which is particularly restrictive.\nConsequently, we propose a solution for dynamically adjusting these assumptions according to the type of scene (indoor/outdoor) by using an hybrid GCNN-CNN framework as shown in Fig.1. To this end, we have developed a GCNN model that performs scene classification (indoor/outdoor) by exploiting spatial and semantic information obtained from CNN object detection/segmentation models. This approach offers an efficient solution for extracting scene features and context relationships by exploiting the advantages of graphs as an efficient tool for representing complex data. To the best of our knowledge, this work is the first to address this issue by combining CNN and GCNN models for predicting scene type of natural scenes.\nThe proposed model can be integrated as a framework within the well-known YOLACT [7] object detection or segmentation model. It can also be extended to any model that outputs labels and bounding boxes for detected objects. Furthermore, the introduced GCNN model can be added as a network header to any object detection/segmentation model, making it easier to use in various computer vision applications. The main contributions of this work are:\n\u2022 A Lightweight Hybrid GCNN-CNN framework for scene classification is proposed, with predictive accuracy of over 90% and a much lower number of parameters than CNN models.\n\u2022 A GCNN-based solution for non-satellite image scene classification is introduced for the first time and validated through extensive experiments on real data.\n\u2022 An efficient graph construction strategy exploiting both semantic and spatial information of objects, and their distribution in the scene is proposed.\n\u2022 A new GCNN-based framework incorporating the semantic contents and the spatial distribution of objects for better extraction of scene features and visual information context, i.e. objects relationship, in order to learn intrinsic attributes of the observed scenes is proposed.\n\u2022 A complete and flexible open-source code that can be easily integrated with any object detection/segmentation model for solving various computer vision problems is provided.\nThe paper is organized as follows. Section II summarizes the related literature. Section III is devoted to detail our method. Results are presented and discussed in section IV. Conclusions and perspectives are provided in section V."}, {"title": "II. RELATED WORKS", "content": "The classification of scenes is a hot research topic, dating back to at least the 90s [8]. Despite the number of works dedicated to this subject, whether for indoor or outdoor scene classification or other contexts, it remains an open research problem. In what follows, we will limit ourselves to a few published works that we feel are representative of the three decades of research on indoor-outdoor scene classification.\nWith the advent of deep learning techniques, great progress has been made in the field of image classification, and several survey papers have highlighted this [9], [10]. In the case of indoor/outdoor scene classification (binary decision), although this appears simpler than image classification in the broad sense, there are cases where the classification error is not negligible. Indeed, recognizing and classifying objects in an image may be less difficult than classifying indoor and outdoor scenes because of the difficulty in discerning between perceptually and physically similar visual information that may exist in both indoor and outdoor scenes. Given the large amount of work in the field and the trend over the last ten years, it seems more appropriate to classify the methods into two categories: traditional and deep learning-based.\nThis category includes Bayesian framework approaches [11], clustering methods based on low-level features of bag of words [12], conventional learning-based methods such as SVM-based approaches [13], random Forest classifiers [14] and other approaches such as those based on the interesting concept of semantic typicality introduced in by Vogel and Schiele in [15].\nWith the introduction of deep neural learning approaches, real progress has been made in recent years in solving scene classification problems, in general, [16] and especially in the case of Remote-Sensing [17]. But in the case of indoor/outdoor scene classification, despite the progress made, the problem still remains open due to its complexity [16]. The other difficulty is the fact that the performance of deep learning-based methods depends on the quality of the databases used in training. It is also worth noting the lack of sufficient databases of labelled indoor-outdoor scenes available to the public, as highlighted in [9].\nIn this work, we focus on a few recent and representative state-of-the-art (SOTA) methods based on GCN architecture. Liang et al. [18] propose a method that learns discrimina-tive features of the scene by using the GCNN-CNN model. This approach extracts global-based visual features of scenes through a CNN architecture and then constructs graphs to learn the object-based location features. This method seems efficient for aerial and satellite views but unsuitable for common natural scenes due to its inability to exploit the spatial distribution of objects and their characteristic information.\nLi et al. [19] construct a new remote sensing knowledge graph (RSKG) to recognize unseen remote sensing image scenes. It introduces a generation of Semantic Representation of scene categories used by a deep alignment network (DAN) to exploit cross-modal matching between visual features and semantic representations. However, this approach does not take into account the spatial information of objects or their dimensions when creating graphs, which also makes it unsuitable for natural scenes.\nIn [20], the proposed method exploits the benefits of GCN to better extract potential context relationships between objects in the observed scene. The proposed framework exploits only semantic scene information and spatial object distribution. However, this approach does not consider the object's intrinsic spatial information, making it ineffective for terrestrial images. It should be noted that most indoor/outdoor scene classification methods do not jointly incorporate information on the size of objects, as well as proximity characteristics and links between the various attributes of the image components observed in the learning process design.\nMoreover, these methods are based on a graph construction strategy that does not incorporate the most relevant intrinsic scene attributes, such as spatial and context relationships"}, {"title": "III. METHOD", "content": "The overall architecture of our new Lightweight Hybrid Graph Convolutional Neural Network - CNN Scheme (LH-G2CNN) is illustrated in Fig.2. The proposed model consists of a CNN object detection model that conveys semantic and spatial information to a GCNN model for boosting the scene classification process.\nThe proposed method of scene classification by means of object detection inference aims to produce a learning-based classifier by using semantic information and spatial consistency of the scene. First, the output prediction of the object detection inference provides a set of n labels L and bounding boxes B describing the contents of the scene. Then, we use these characterizing features to produce a scene description exploiting the semantic and spatial scene contents and the spatial relation between the bounding boxes. Each object prediction provides a vector\n\\(P_i = \\{l_i, X_i, Y_i, W_i, h_i\\}^T\\)\nwhere \\(l_i\\) is the predicted object label, \\(x_i\\) and \\(y_i\\) the position of the top left corner of the predicted bounding boxes, and \\(w_i\\) and \\(h_i\\) its width and height. The spatial information contained within B consists of the information on scene geometry and objects' size. First, we use these predicted features to compute the Euclidean distances \\(A_{i,j}\\) between bounding boxes. This Euclidean distance provides information on the geometric and spatial relationships between objects in the scene. This extracted information enables us to characterize the spatial arrangement between objects in the scene through geometric reasoning. The object size, expressed as the bounding boxes diagonals \\(d_i\\) given by \\(d_i = \\sqrt{w_i^2 + h_i^2}\\), represents an additional geometric attribute for the graph construction process. This information, combined with the semantic information, provides a priori knowledge about the nature of the scene through the relationship between size and object type. Indeed, a significant fluctuation in the size of objects of the same label contained in a scene induces a significant scene depth, increasing the probability of corresponding to an outdoor scene conversely. Therefore, our approach attempts to combine semantic and spatial reasoning into a coherent graph neural network.\nThe spatial semantic consistency of the detected object is integrated in a graph G composed of edges E and nodes N that represent the main scene information. The strength of our approach is in attributing both spatial and semantic information to nodes and spatial information to edges, which better describes the scene contents.\nFrom a semantic point of view, only predicted labels L are considered to describe the node's attributes. The graph can be defined as G = {V,E}, where nodes are expressed as \\(V = \\{V_i : \\{l_i, d_i\\}|i \\in \\{1, ..., n\\}\\}\\) integrating information from labels and bounding boxes. Edges E describe the spatial distribution of objects through their Euclidean distance. However, unlike most other approaches, not all nodes are connected by edges to maintain the coherence of the spatial distribution of objects.\nWe determine the nearest neighbours of a node i by com-paring its distance \\(A_{i,j}\\) for each node j \u2260 i. The distance with its nearest node \\(A_{i_{min}}\\) is expressed as follows:\n\\(A_{i_{min}} = min_{j<n} A_{i,j}\\)\nThis minimum distance is used to define the node connections of the graph. Only nodes at a distance close to \\(A_{i_{min}}\\) up to a certain distance ratio \u03b2 are considered in the process of graph edges' construction. The construction of a space-semantic Graph corresponding to an observed scene is described in Algorithm 1. As a result, nodes are only connected by edges to their nearest neighbours with \\(V = \\{V_{i,j} : \\{\\Delta_{i,j}\\}|i,j \\in \\{1, ..., n\\}\\}\\). This nodes connectivity is represented by an adjacency matrix A representing existing edges. A is symmetric, but only some edges are created and have attributed values as shown in Tab.I."}, {"title": "C. Graph Convolutional Neural Network models", "content": "Graph Convolutional Neural Networks (GCNN) models have been introduced to solve various problems in computer vision, especially for visual data classification, human action recognition, and face recognition, to name a few. In the following section, the most common GCNN models are briefly described.\nFirst, we propose the Graph Convolutional Network (GCN) model [21] as a graph-based neural network model for our scene classification task. GCN is a multi-layer network that takes graphs G composed of nodes V and edges E. A diagonal degree matrix \u010e is expressed from the adjacency matrix A as follows:\n\\(D_{ii} = \\sum_{j} A_{ij}\\)\nWhere A is the adjacency matrix of the graph G. The update state in the GCN layer is expressed as follows:\n\\(h^{(k+1)} = o(\u010e^{-\\frac{1}{2}}\u00c3\u010e^{-\\frac{1}{2}}h^{(k)}W^{(k)})\\)\nWhere o is an activation layer, \\(h^{(k)}\\) denotes activation of the \\(k^{th}\\) layer, \\(W^{(k)}\\) denotes the learnable weights in the layer k, and the first layer \\(h^{(0)} = V\\).\nGiven that CNN architectures do not allow contextual relations to be extracted from a scene, we have used the Graph Isomorphism Network (GIN) model [22]. The GIN model is a variant of the GCN model [21], which enables the differentiation of graphs that are not isomorphic to each other. The computational model of the GIN update nodes in the graph convolutional layer is expressed as follows:\n\\(h_v^{(k)} = MLP^{(k)} ((1+e^{(k)}) \\cdot h_v^{(k-1)} + \\sum_{u \\in N(v)}h_u^{(k-1)})\\)\nWhere \\(h_v^{(k)}\\) denotes the feature representation of node v of the kth hidden layer, \\(h_v^{(k)}\\) = \\(A_v\\), MLP denotes Multilayer Perceptron, and \\(e^{(k)}\\) is a learnable parameter. Here, \\(A_v\\) is the attribute matrix of the vth node.\nIn addition, GIN has the advantage of concatenating information nodes across all layers of the model. This readout function enables the graph classification by using individual node information. The extracted intrinsic attributes of the scene are fed into the LogSoftmax layer to obtain the probability of the scene type for each class (indoor/outdoor), with the LogSoftmax function expressed as follows:\n\\(LogSoftmax(x_i) = log(\\frac{exp(x_i)}{\\sum_j exp(x_i)})\\)\nPellegrini et al. [23] propose a learnable aggregation function (LAF) corresponding to a generalized form of the Lp-norms, expressed as follows:\n\\(L_{a,b}(x) := (\\sum_{i} x_i^a)^{\\frac{b}{a}} (a, b \\geq 0)\\)\nWhere x = {\\(x_1\\),...,\\(x_N\\)} denotes a finite multi-set of real numbers, \\(L_{a,b}\\) is invariant under the addition of zeros such as \\(L_{a,b}(x)\\) = \\(L_{a,b}(x\\cup0)\\), with 0 being a multi-set of zero cardinality. The LAF layer can be defined as a restricted LAF function for sets x \u2208 [0,1]:\n\\(LAF(x) :=\\frac{\\alpha L_{a,b}(x) + \\beta L_{c,d}(1 - x)}{\\gamma L_{e,f}(x) + \\delta L_{g,h}(1 - x)}\\)\nWhere a, b, ..., h \u2265 0 are tunable parameters, and \u03b1, \u03b2, \u03b3, \u03b4 \u2208 R. Based on the Learnable Aggregation Function layer [23], we use the GINLAF model."}, {"title": "IV. EXPERIMENTS", "content": "In this section, all the experiments carried out to train, optimize, and evaluate the proposed method are provided and discussed. The experiments were conducted on a computer with an Intel Xeon CPU and a NVIDIA Tesla T4 GPU.\nTraining and evaluation were done on an MS-COCO [24] derived dataset providing scene type annotations, namely CD-COCO [25]. This dataset offers a major contribution by exploiting the advantages of the famous MS-COCO dataset, namely the very large number of annotated images containing object labels, bounding boxes and masks. In addition, the CD-COCO dataset provides the scene type, indoor/outdoor, of each images. Consequently, it is not necessary to use an object detection model to provide inputs to train our GCNN scene classification model.\nThis database is divided into subsets of 72K, 9K, and 9K images for the training, validation, and test sets, respectively. It is important to note that the images contained in the database are distorted, which reduces the effectiveness of the YOLACT model for object detection.\nThe training was carried out on several GCN variant models to fully investigate the capabilities of our method. In our case study, we trained the GCN, GIN, and GINLAF models on multiple dimensions, which allowed us to conduct a comprehensive comparative study. All models have been trained using the cross-entropy loss function to learn the parameter values. The learning rate Lr, the weight decay dd and the number of epochs are set to 0.001, 5e-4 and 10, respectively. For the GCN and GIN models, the dimension of the hidden layers is set to 1024, while that of the GINLAF model is 32. Finally, the batch size is set to 64 for models.\nFirst, tests were carried out using the database groundtruth (labels and bounding boxes) directly as input to our GCNN model. A detailed study was then carried out to assess the impact of the \u03b2 distance ratio on the performance of each model, as shown in Fig.4. The optimal \u03b2 value allows an increase of 1.22%, 0.75%, and 0.88% for the GCN, GIN, and GINLAF models, respectively. This empirical study highlights the optimal performance for \u03b2 = 0.1, which will be used for further experimentation.\nSecondly, the effect of the number of object classes present in the observed scene on performance was evaluated. Indeed, the semantic content of a scene indirectly informs its intrinsic attributes. A scene composed of a large number of classes allows better extraction of the semantic relationships between objects, leading to a better classification, as shown in Tab.II. However, this ablation study does not consider the number of objects detected. The GIN model seems to be the most robust to variations in the semantic content of the observed scene, while the GINLAF model offers the best performance. A complete benchmark of various CNN and Vision Transformer (ViT) models has been performed to highlight the effectiveness and contribution of our method (see Tab.III).\nThe accuracy, number of parameters and running time of each model have been determined to provide a complete and consistent comparison between the methods. This evaluation highlights the low computational cost of generating predictions using our method compared to CNN and ViT approaches for similar results (see Table.III). Indeed, our approach requires 100 times fewer parameters and inference speed is 66 times faster. These performances justify the contribution of the approach, which guarantees good accuracy for a computational time suitable for its integration in an object detection model.\nOur YOLACT-GCNN framework has been implemented and tested for the optimal \u03b2 parameter. In contrast to the previous study, using the outputs of the object detection model as inputs to the GCNN model induces an effect of the YOLACT [7] model performance on the classification perfor-mance. It should be noted that the object detection accuracy of the YOLACT model is reduced due to disturbances in the images contained in the CD-COCO database. In addition, increasing the number of object classes detected does not result in a proportional increase in the number of objects detected. The GCN model gains accuracy as the semantic content increases in terms of the number of different classes present in the observed scene. As shown in Table IV, the GIN model behaves in the opposite way, regressing as the semantic content increases."}, {"title": "V. CONCLUSION", "content": "Through this study, we have proposed a new lightweight GCNN-CNN models that allows us to overcome the limitations of CNN-based methods by incorporating semantic information, object proximity, and spatial information in the graph design. In addition, the proposed method is the first GCN model that can be used as an add-on to any object detection model. The experiment carried out on a dedicated dataset clearly demonstrates the efficacy of the proposed approach in indoor/outdoor scene classification.\nOur method achieves results that are very slightly lower than traditional CNN methods with a much lighter and easier-to-deploy network. One limitation of the proposed solution is that it is sensitive to the reliability of the object detection module and the semantic nature of the scene complexity. Meaning that the more objects the scene contains the higher the accuracy achieved. It is worth noticing that the study could help design GCNN-based models for solving various computer vision problems. Finally, this new proposed solution could be considered as a starting point for GCNN-based non-satellite scene classification."}]}