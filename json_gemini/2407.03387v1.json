{"title": "ConCodeEval: Evaluating Large Language Models for Code Constraints in Domain-Specific Languages", "authors": ["Mehant Kammakomati", "Sameer Pimparkhede", "Srikanth G. Tamilselvam", "Prince Kumar", "Pushpak Bhattacharyya"], "abstract": "Recent work shows Large Language Models (LLMs) struggle to understand natural language constraints for various text generation tasks in zero- and few-shot settings. While, in the code domain, there is wide usage of constraints in code format to maintain the integrity of code written in Domain-Specific Languages (DSLs), yet there has been no work evaluating LLMs with these constraints. We propose two novel tasks to assess the controllability of LLMs using hard and soft constraints represented as code across five representations. Our findings suggest that LLMs struggle to comprehend constraints in all representations irrespective of their portions in the pre-training data. While models are better at comprehending constraints in JSON, YAML and natural language representations, they struggle with constraints represented in XML and resource-rich language Python.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown promising results (Brown et al., 2020) in comprehending and subsequently generating coherent text and code in zero and few-shot settings, especially for resource-rich languages. However, their practical utility depends on their ability to follow constraints at various granularity, encompassing user and system requirements. Recent work (Sun et al., 2023) shows LLMs have difficulty in understanding fine-grained hard constraints represented as a natural language in zero and few shot settings for tasks like paraphrase generation and numerical planning while performing decently well for coarse-grained constraints such as generating text on a given topic. While in the code domain, encoding use-case-specific constraints in code format is prevalent and crucial for enterprises to maintain the integrity of the code typically written in DSLs such as JSON, YAML, and XML that are often in structured format. Such DSLs are widely adopted for data exchange and configuration of systems such as Kubernetes, and Ansible and there is growing interest (Nair and Modani, 2023) in employing LLMs for DSLs. Depending on the use case and system, the constraints that are often fine-grained are articulated as schemas in various code languages like JSON schema, YAML, or Python Pydantic to validate DSL code. A typical schema holds fine-grained hard and soft constraints such as data types, required and optional fields, default values, numerical constraints, etc. LLMs must be cognizant of such constraints for practical use in producing reliable code. Therefore, we aim to study and understand the controllability of LLMs when fine-grained constraints are represented in code format."}, {"title": "2 Data as Code Generation in DSL", "content": "Task description: Given the schema, the generation task (see Listing 1) aims to produce a compliant data sample in code format in a DSL of interest. We draw inspiration from use cases such as synthesizing schema-compliant data from LLMs to train and evaluate smaller-sized models (Song et al., 2020) and generating diverse sets of samples to be used in product test pipelines. Since data represented in DSL is structured, LLMs need to be schema-aware during generation.\nDataset: We synthetically prepare 602 schemas across 5 representations having combinations of hard and soft constraints. First, we prepare JSON schemas using our combinatorial tool to generate a good mix of constraints. We then convert each JSON schema to XML and YAML schemas using automated tools to ensure equivalence across representations. Further, we include Python representation using the Pydantic library as a resource-rich general-purpose language in our evaluation generated using the Gemini-1.0-pro (Team et al., 2024) model as a code translation task. We extend our evaluation to natural language representation generated using rule-based templates over the JSON schema. We ensure equivalence of the generated schemas across languages by manually eyeballing the samples.\nEvaluation metric: Each instance of schema-compliant code that LLM generates is awarded one point when these code samples are validated using a schema validator tool. We then utilize the accuracy metric over all samples to benchmark performance across various models. Along with the accuracy evaluation metric, we also report invalid data-type samples (IS%), and samples generated with the invalid root data type (RTV%). The root data type is the data type of the whole JSON sample. For example, the root data type of sample represented in listing 1 is array. For IS and RTV metrics, the lesser the number better the performance.\nExperimental setup: We experiment with greedy decoding and beam search decoding using a beam width of 3. Both decoding techniques often perform similarly, yet greedy decoding consistently shows a slight edge; therefore, we present results using greedy decoding.\nPrompts: We experiment with zero-shot and 3-shot prompting for each model. For 3-shot prompting, we identify errors from the zero-shot setting, then select and use samples similar to the most frequent errors. Using entirely different schema representations as few-shot samples does not improve model performance. Examples of prompts can be found in Appendix 1\nResults: We observe that among the six schema representations studied, the natural language is best understood by the model for all output representations. For constraints in code format, JSON and YAML schema perform the best oweing to their wide usage in this use case though not being one of the major languages in the pre-training data. Additionally, Python though being one of the major portions of the code pre-training data, models struggle to understand the constraints represented in Python and subsequently failing to generate valid data samples across all output languages. Similarly, models struggle to comprehend XML schema and generate in XML across all schema representations. Among the Codellama and Granite families, we observe that Granite 20B and 34B are"}, {"title": "3 DSL Validation", "content": "Task description: There is a growing body of work (Chiang and Lee, 2023; Hada et al., 2024) on showing promising usage of LLMs as evaluators as an alternative to human evaluators in many tasks. While LLMs may not be deployed to validate DSL samples when schemas are in machine-readable formats, constraints can often be articulated in natural language format where automated validation is infeasible. Further, the ability to validate against the given schema throws light on LLMs' understanding of the relation between requirements and output. Given the DSL sample to validate and the schema, the task aims to answer the validity of the provided sample against the constraints. Through this boolean question task (see Listing 2), we study the following questions: (i) How LLMs perform on varying lengths of the schemas? (ii) How well do the LLMs perform with schema in natural language representation compared to code counterpart representations under similar constraints?"}, {"title": "5 Conclusion", "content": "We introduce two novel tasks to test the controllability of LLMs when constraints are in code format that are profound in the use of DSLs for data exchange and configuration of systems. We evaluate LLMs over 5 schema representations that include YAML, JSON, Python, XML, and natural language and 3 output representations that include YAML, JSON, and XML. Through both the tasks, we conclude that there is substantial scope of improvement for LLMs to comprehend constraints in code format to subsequently generate compliant sample or validate the given sample. Further, findings show that LLMs do not exhibit a direct correlation with the size of the language in the pre-training data used. Since LLMs struggled to comprehend constraints represented in resource-rich language Python rather showed better performance for JSON and natural language that are minor portions of the code pre-training data. We hope that our work provides some guidance on when to employ LLMs given the languages of interest and as well research in the direction to improve LLMs towards better controllability over code constraints."}, {"title": "Limitations", "content": null}, {"title": "Ethics Statement", "content": null}, {"title": "A Appendix", "content": "A.1 Prompts\nThis section defines the prompts which are used for models. We report different prompts for every"}, {"title": "A.1.1 Common prompt", "content": "For zero shot inference, we use a common prompt as it is for all the models irrespective of the model's prompt format and we observe best results for Task-1 with this prompt. The prompt is as follows."}, {"title": "A.1.2 Granite model family", "content": "The granite model generally follows the question-answering format. Task-1 prompts for granite family models are as follows.\nSystem prompt:\nSystem:\nYou are an intelligent AI programming assistant, utilizing a Granite code language model developed by IBM. Your primary function is to assist users in code explanation, code generation and other software engineering tasks. You MUST follow these guidelines: - Your responses must be factual. Do not assume the answer is yes when you do not know, and DO NOT SHARE FALSE INFORMATION. - You should give concise answers. You should follow the instruction and provide the answer in the specified format and DO NOT SHARE FALSE INFORMATION."}, {"title": "A.1.3 Llama family", "content": "For codellama 34B model we wrap the common prompt in [INST] and [/INST] tags. For the llama3-8B model, we use the System prompt along with user tags 6.\nSystem prompt: You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive. If a question does not make any sense or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\nOther than this, similar to the granite family we try Question answering format and instruction to wrap the output in quotes (\"\")."}]}