{"title": "Feasible Learning", "authors": ["Juan Ramirez", "Ignacio Hounie", "Meraj Hashemizadeh", "Juan Elenter", "Alejandro Ribeiro", "Jose Gallego-Posada", "Simon Lacoste-Julien"], "abstract": "We introduce Feasible Learning (FL), a sample-centric learning paradigm where models are trained by solving a feasibility problem that bounds the loss for each training sample. In contrast to the ubiquitous Empirical Risk Minimization (ERM) framework, which optimizes for average performance, FL demands satisfactory performance on every individual data point. Since any model that meets the prescribed performance threshold is a valid FL solution, the choice of optimization algorithm and its dynamics play a crucial role in shaping the properties of the resulting solutions. In particular, we study a primal-dual approach which dynamically re-weights the importance of each sample during training. To address the challenge of setting a meaningful threshold in practice, we introduce a relaxation of FL that incorporates slack variables of minimal norm. Our empirical analysis, spanning image classification, age regression, and preference optimization in large language models, demonstrates that models trained via FL can learn from data while displaying improved tail behavior compared to ERM, with only a marginal impact on average performance.", "sections": [{"title": "INTRODUCTION", "content": "Deep learning trends are shifting toward larger model architectures, as evidenced by GPT-4 (OpenAI, 2023), DALLE-3 (Betker et al., 2023), and Llama-3 (Llama Team, 2024). Larger models are capable of perfectly fitting increasingly large datasets, memorizing the data by achieving near-zero loss on all samples (Arpit et al., 2017; Zhang et al., 2017a). In this context, the Empirical Risk Minimization (ERM) framework does not specify a preference among the many interpolating solutions. Consequently, the solution recovered in practice depends not only on the learning framework but also on the inductive biases of the chosen optimization algorithm. For instance, Soudry et al. (2018) highlight the role of stochastic gradient descent dynamics in guiding ERM toward well-generalizing models.\nWhile research has extensively focused on developing optimization algorithms suited for learning via ERM (Kingma & Ba, 2015; Gupta et al., 2018), exploring alternative learning frameworks has received comparatively little attention. Alternatives to ERM could be better suited for specific learning scenarios, particularly when it is important to optimize for something other than average performance. Such alternatives may exhibit distinct properties, such as improved uncertainty quantification (Balasubramanian et al., 2014), fairness (Lahoti et al., 2020), or robustness (M\u0105dry et al., 2017).\nGiven the abundance of interpolating, well-generalizing solutions in modern machine learning problems, why would we limit ourselves to those derived from ERM?\nIn this paper, we introduce a novel learning framework called Feasible Learning (FL, \u00a72). FL formulates learning as a feasibility problem, where we seek a predictor that meets a bounded loss constraint for all training samples. Unlike the ubiquitous ERM framework, which optimizes for average performance, FL demands a minimum performance level for each data point.\nConcretely, FL formulates an optimization problem with a trivial (constant) objective function, while imposing a constraint on the loss of the predictor h : X \u2192 Y for each training sample $(x_i, y_i)_{i=1}^n$:\n$\\min_{h\\in H} 0 \\quad s.t \\quad l(y_i, h(x_i)) < \\epsilon \\quad \\text{for } i = 1,...,n,$\\ (FL)\nwhere $\\epsilon \\geq 0$ is the maximum allowed per-sample loss."}, {"title": "FEASIBLE LEARNING", "content": "We consider the problem of learning a predictor $h_\\theta : \\mathcal{X} \\to \\mathcal{Y}$ with parameters $\\theta \\in \\Theta$ on a labeled dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n$. The quality of the predictions is measured by a differentiable (surrogate) loss function $l : \\mathcal{Y}^2 \\to \\mathbb{R}_{\\geq 0}$. To simplify the notation, we denote the loss incurred on the i-th data point as $g_i(\\theta) \\coloneqq l(y_i, h_\\theta(x_i))$, and the vectorized version of these losses as $g(\\theta) = [g_1(\\theta), ..., g_n(\\theta)]^\\top$.\nThe Feasible Learning (FL) paradigm advocates for learning through solving a feasibility problem. Specifically, FL considers an optimization problem with a trivial, constant objective while enforcing a loss constraint for each training sample:\n$\\min_{\\theta\\in \\Theta} 0 \\quad s.t \\quad g(\\theta) \\leq \\epsilon, \\tag{FL($\\epsilon$)}$\nwhere $\\epsilon = [\\epsilon, ..., \\epsilon]^\\top \\in \\mathbb{R}^n$ is the constraint level\u00b3. In the FL framework, a model is acceptable if and only if it achieves a sufficiently small loss (given $\\epsilon$) on every training point. Choosing $l$ as the squared error imposes a bound on the maximum error per sample, while upper-bounding the per-sample cross-entropy sets a lower bound on the probability the model assigns to the correct label.\nWhen the model class can interpolate the training data, setting $\\epsilon = 0$ results in the set of solutions for FL matching that of ERM consisting of those that achieve zero training loss on all samples. When $\\epsilon > 0$, FL admits additional solutions those that satisfy the constraints but do not necessarily interpolate the data.\nIn the non-interpolation case, we expect FL to outperform ERM in terms of the maximum loss over the dataset potentially at the expense of a heightened average loss. However, this trend may not always manifest in practice due to the inherent challenges of solving non-convex (constrained) optimization problems.\nFunctional regularization. FL does not inherently favor interpolating or non-interpolating solutions, as long as both satisfy the constraints. This introduces"}, {"title": "Solving FL Problems", "content": "Even if the loss function l is convex in its inputs $(y_i, h_\\theta(x_i))$, it may not be convex in \u03b8. Therefore, FL(\u20ac) is typically a non-convex constrained optimization problem with no closed-form solution. Furthermore, the lack of an objective function and the non-convexity of the feasible set preclude the use of standard constrained optimization techniques such as projected gradient descent (Goldstein, 1964) or Frank-Wolfe (Frank & Wolfe, 1956). Instead, we leverage Lagrangian duality. The min-max Lagrangian game associated with FL(\u20ac) is:\n$\\min_{\\theta\\in \\Theta} \\max_{\\lambda \\geq 0} L_{FL}(\\theta, \\lambda) \\equiv \\lambda^\\top (g(\\theta) - \\epsilon),$\\tag{1}\nwhere $\\lambda \\geq 0$ is the vector of Lagrange multipliers associated with the constraints. We refer to $\\theta$ as the primal variables, and $\\lambda$ as the dual variables. FL yields a Lagrangian with one multiplier $x_i$ per datapoint $x_i$.\nA simple algorithm for finding min-max points of $L_{FL}$ is to perform gradient descent steps on $\\theta$ and projected gradient ascent steps on $\\lambda$ (Arrow et al., 1958, GDA). Alternating GDA updates (Zhang et al., 2022) yield:\n\\begin{align}\n\\lambda^{t+1} &\\leftarrow [\\lambda^t + \\eta_\\lambda (g(\\theta^t) - \\epsilon)]_+\\\\\n\\theta^{t+1} &\\leftarrow \\theta^t - \\eta_\\theta \\nabla_\\theta L_{FL}(\\theta^t,\\lambda^{t+1}) \\\\\n&\\coloneqq \\theta^t - \\eta_\\theta \\left( \\sum_{i=1}^n \\lambda_i^{t+1} \\nabla_\\theta g_i(\\theta^t) \\right),\n\\tag{2}\n\\end{align}\nwhere $[\\cdot]_+$ denotes a projection onto $\\mathbb{R}_{\\geq 0}$ to enforce $\\lambda \\geq 0$, and $\\{\\eta_\\theta, \\eta_\\lambda\\}$ are step sizes. We initialize $\\lambda_0 = 0$.\nThe primal updates in Eq. (2) resemble gradient descent on the ERM objective by following the gradients of the per-sample losses. However, unlike ERM, where these gradients are weighted equally, FL uses the Lagrange multipliers as weights. Since these multipliers are optimized, the algorithm dynamically re-weights the importance of each data point throughout training.\nThe re-weighting works as follows: if $g_i(\\theta) > \\epsilon$, the corresponding multipler increases; if $g_i(\\theta) < \\epsilon$, the multiplier decreases, potentially reaching zero. Consequently, data points with consistently high losses result in large multipliers, causing the primal updates to focus on reducing their loss. Conversely, data points with consistently small losses have small or even zero multipliers, allowing them to be largely ignored during optimization. Thus, a given primal update is influenced by the \"instantaneous\" incentive to satisfy a constraint, reflected in the loss gradient, and the \"historical difficulty\" of satisfying the constraint, captured in the multiplier. In \u00a75.3, we show how hard samples such as mislabeled ones tend to yield large multipliers.\nFunctional regularization. These optimization dynamics do not aim to satisfy the constraints beyond the prescribed level e. Once a constraint is strictly satisfied, the dual updates reduce the corresponding multiplier, discouraging the primal updates from further minimizing the loss for the corresponding sample.\nHowever, satisfying the constraint for some samples may require achieving a loss tighter than e on others. Additionally, primal-dual methods can overshoot into the interior of the feasible set due to a \"delay\" between initially meeting the constraint and sufficiently reducing the multiplier to relieve pressure on loss reduction. This overshoot can be mitigated by using PI controllers to update the multipliers instead of relying on gradient ascent (Stooke et al., 2020; Sohrabi et al., 2024).\nInfeasible problems. When applying GDA to infeasible problems, the multipliers associated with unsatisfiable constraints will increase indefinitely. This can potentially lead to numerical instability, disrupting optimization. However, this issue does not arise in our proposed method for solving RFL problems (\u00a73.1).\nThe cost of GDA on FL. Since the primal update direction $\\nabla_\\theta L_{FL}$ is a linear combination of the per-datapoint loss gradients $\\nabla_\\theta g_i(\\theta)$, it can be computed efficiently using automatic differentiation, without needing to store each gradient individually. This makes its computation as efficient as that of the ERM loss gradient. Therefore, applying (mini-batch) gradient descent-ascent on $L_{FL}$ is as efficient as performing (mini-batch) gradient descent on the ERM loss up to the cost of storing and updating the multipliers. This overhead is negligible when the dim(\u03b8) is much larger than n."}, {"title": "RESILIENT FEASIBLE LEARNING", "content": "The potential misspecification of FL(e) problems can be addressed by relaxing the constraints using slack variables, denoted by $u$. Given $\u03b1 > 0$, we consider the following constrained optimization problem:\n$\\min_{\\theta\\in \\Theta, u\\geq 0} \\frac{\\alpha}{2} ||u||_2^2 \\quad s.t \\quad g(\\theta) \\leq \\epsilon + u. \\tag{RFL($\\epsilon, \\alpha$)}$\nWe call this approach Resilient Feasible Learning (RFL) due to its robustness to problem misspecification. If the original FL problem is feasible, the corresponding RFL problem is equivalent, as the optimal relaxation $u$ will be zero. Crucially, RFL guarantees the existence of a feasible solution, even when the original FL problem is infeasible. We generally favor RFL over FL in practice since it alleviates the challenge of setting e.\nIn RFL(\u20ac, a), $u_i > 0$ represents a strict relaxation of the i-th constraint, while u\u2081 = 0 indicates that it remains unchanged. The cost of relaxing constraints depends on the norm of the slack variables. Although other norms could be used, we focus on the L2-norm due to its algorithmic advantages, as demonstrated in Prop. 1.\nWhile one might consider setting e = 0 thus allowing RFL to determine the tightest possible loss requirements through the slacks maintaining a positive e enables regularization, as in FL. This prevents the model from overly minimizing per-sample losses, even if some data"}, {"title": "Solving RFL Problems", "content": "As with FL problems, we use the Lagrangian approach to solve RFL problems. The min-max Lagrangian game associated with $RFL(\\epsilon, \\alpha)$ is given by:\n$\\min_{\\theta\\in \\Theta, u\\geq 0} \\max_{\\lambda\\geq 0} L_{RFL}(\\theta, u, \\lambda) \\coloneqq \\frac{\\alpha}{2} ||u||_2^2 + \\lambda^\\top (g(\\theta) - \\epsilon - u)\\tag{3}$\nWe now transform Eq. (3) to a problem without slacks.\nProposition 1. [Proof] For every \u03b8 \u2208 \u0398, the following strong duality condition holds:\n\\begin{equation}\n\\min_{u\\geq 0} \\max_{\\lambda\\geq 0} L_{RFL}(\\theta, u, \\lambda) = \\max_{\\lambda\\geq 0} \\min_{u\\geq 0} L_{RFL}(\\theta, u, \\lambda)\\tag{4}\n\\end{equation}\n\\begin{equation}\n= \\max_{\\lambda>0} L_{FL} (\\theta, \\lambda) - \\frac{\\alpha}{2} ||\\frac{\\lambda}{\\alpha}||_2^2\n\\tag{5}\n\\end{equation}\nAs a consequence of Proposition 1, the Lagrangian problem for RFL in Eq. (3) can be solved via a quadratically-regularized version of the FL Lagrangian:\n$\\min_{\\theta\\in \\Theta} \\max_{\\lambda>0} \\mathcal{L_\\alpha}(\\theta, \\lambda) \\coloneqq L_{FL}(\\theta, \\lambda) - \\frac{1}{2\\alpha} ||\\lambda||^2. \\tag{6}$\n$\\mathcal{L_\\alpha}$ is strongly concave on $\\lambda$, implying that for a fixed $\\theta$, the inner maximization has a unique solution $\\lambda^*$(whereas FL may yield an unbounded inner problem).\nThis formulation is advantageous, as gradient descent-ascent offers convergence guarantees for non-convex, strongly-concave min-max problems (Lin et al., 2020). In particular, strong convexity of the function g yields a linear convergence rate (Chen & Rockafellar, 1997).\nAlternating GDA updates on $L_\\alpha$ yield similar updates to those of primal-dual FL (Eq. (2)). The primal update direction remains the same: a linear combination of the per-sample loss gradients, weighted by the multipliers. The dual update includes a weight decay of 1/\u03b1, which \"discounts\" historical violations, resulting in different dynamics. For example, this prevents the multipliers for unsatisfiable constraints from growing indefinitely.\nBy analytically solving the inner maximization problem in Eq. (6), we recover the following result:\nProposition 2. [Proof] For every \u03b8 \u2208 \u0398, we have:\n$\\min_{\\theta\\in \\Theta} \\max_{\\lambda\\geq 0} L_{RFL}(\\theta, u, \\lambda) = \\min_{\\Theta} \\frac{\\alpha}{2} ||[g(\\theta)-\\epsilon]_+||^2\\tag{7}$"}, {"title": "RELATED WORK", "content": "Learning paradigms. FL stands in contrast to the standard Empirical Risk Minimization (ERM):\n$\\min_{\\theta\\in \\Theta} L_{ERM}(\\theta) \\coloneqq \\frac{1}{n} \\sum_{i=1}^n g_i(\\theta),\\tag{ERM}$\nwhich views the learning problem as \"choosing from the given set of functions the one which approximates best the supervisor's response\" (Vapnik, 1991, p.2). ERM operationalizes the notion of \"best approximation\" through the average loss across the training set.\nThere is a fundamental difference between the goals of the FL and ERM problems. To illustrate this, consider the case of recommender systems used by streaming or social media platforms. Service providers often prioritize metrics like average click-through rates or watch-time to measure overall system success and user engagement. However, individual users are primarily concerned with how well the recommendations align with their personal tastes and preferences. A system that performs well on average might still fail individual users by consistently suggesting irrelevant or inadequate content. ERM inherently allows for trade-offs between training samples, allowing models to perform poorly on certain samples as long as they compensate by performing exceptionally well on others.\nIn contrast to robust (Rawlsian) approaches (Lahoti et al., 2020), which minimize the worst-case risk:\n$\\min_{\\theta\\in \\Theta} \\max_{i\\in \\{1,...,n\\}} g_i(\\theta),\\tag{Rawlsian}$\nFL only requires that the upper bound in the per-sample loss is satisfied. Thus, FL does not prefer one model over another as long as both satisfy the constraints.\nThe pursuit of minimizing the average loss in ERM or the maximum loss in the Rawlsian approach can lead to models that overfit the training data or become overly confident. This excessive reduction in losses can harm generalization, motivating the use of regularization techniques. Unlike traditional methods, which promote parsimony using surrogate criteria like $L_p$-norms, FL explicitly establishes an upper bound on the loss itself through the constraint level e."}, {"title": "EXPERIMENTS", "content": "In this section, we empirically evaluate the Feasible Learning framework, demonstrating that FL and RFL present a compelling alternative to the widely used ERM framework. We demonstrate that models trained via FL can learn (\u00a75.1). We also explore the advantages of RFL over FL (\u00a75.2) and analyze their loss distribution profiles (\u00a75.3). See Appendix B for details on our experimental setup. For comprehensive results, see Appendix C.\nTasks. We train ResNet-18 models (He et al., 2016) for CIFAR10 (Krizhevsky, 2009) classification and for UTKFace (Zhang et al., 2017b) age regression. We also fine-tune an 8 billion parameter Llama-3.1 model (Llama Team, 2024) on a cleaned version of Intel Orca DPO pairs dataset. We use Direct Preference Optimization (DPO) (Rafailov et al., 2024). Finally, we train a Multi-Layer Perceptron for Two-Moons classification. Table 3 in App. B lists each task's training set size, which corresponds to the number of constraints.\nAs the constraint level e is expressed in terms of the loss, it can be interpreted for each task. Classification: we bound the cross-entropy loss, which translates into a lower bound on the predicted probability for the true class. Regression: we bound the Squared Error (SE), which corresponds to the difference in years between the predicted and true ages. We normalize the ages to have zero mean and unit variance. Preference Alignment (DPO): The DPO loss constraint is expressed as $\u03c3(r(y^+) \u2013 r(y^\u2212)) \u2265 exp(\u2212\u03b5)$, where $y^+$ and $y^-$ represent a pair of preferred and dispreferred completions, respectively, r is an implicit reward model defined via log-likelihood ratios, and is a sigmoid function.\nMethods. We train models via \u2460 ERM, \u2461 CSERM: Clamped-and-Squared ERM (Eq. (7)), \u2462 FL: Feasible Learning, and \u2463 RFL: Resilient Feasible Learning.\nExperimental uncertainty. Unless stated otherwise, all reported metrics are averaged over 5 seeds.\nSoftware & Hardware. Our implementations use PyTorch (Paszke et al., 2019) and the Cooper library for constrained optimization (Gallego-Posada et al., 2024). Experiments are run on NVIDIA L40S GPUs."}, {"title": "Can We Learn with Feasible Learning?", "content": "We begin by evaluating models trained with FL using ERM's primary success criterion: average performance. Despite FL tackling a different problem and irrespective of its effectiveness in solving it we assess whether FL still succeeds in the standard learning task.\nTable 1 presents results for a CIFAR10 classification task. We include FL and RFL under two requirements: \u20ac = 0, where the model is required to assign a probability of 1 to the correct label, matching ERM's solution set assuming interpolation is possible; and e = 0.51, where a true class probability of 0.6 is required, ensuring correct classification with a small margin.\nThese results demonstrate that FL and RFL can effectively learn classifiers with only a slight degradation in average performance compared to ERM on both the training and test sets. However, FL and RFL may offer advantages in tail behavior and robustness (\u00a75.3), making this trade-off appealing for certain applications. We observe this trend across all tasks (see Appendix C).\nOptimization budget. Notably, FL and RFL achieve comparable performance to ERM within the same training budget of 200 epochs. However, it is important to note that poor choices of the dual step size can cause FL and RFL to converge more slowly if chosen too small, or experience degraded performance if set too high.\nRobustness. We found that, despite introducing a new hyper-parameter with the dual step size, FL and RFL are \u2460 similarly robust to the choice of the primal step size as ERM, and \u2461 fairly robust to the choice of the dual step size, achieving good performance across multiple orders of magnitude (see Appendix C)."}, {"title": "How Does Resilience Help?", "content": "Table 2 presents an ablation study on the choice of RFL's a for the UTKFace age regression task. We select \u20ac = 0.0-which is unattainable due to the presence of duplicated samples in the dataset with different labels to emphasize the benefits of resilience in providing flexibility to satisfy the constraints.\nFL's constraints are too restrictive, leading to poor average and maximum performance, significantly worse than ERM. We attribute this to its optimization dynamics, which cause some multipliers to grow indefinitely, destabilizing the optimization process. In contrast, RFL can relax these requirements and achieve performance comparable to ERM. In particular, RFL (\u03b1 = 10-3) outperforms ERM in average train and test errors. Moreover, although RFL relaxes the constraints, potentially allowing for larger maximum errors than FL, it achieves smaller Max SE's, further indicating a failure of FL.\nMoreover, we observe that while certain values of a may yield better performance, a wide range of values spanning multiple orders of magnitude can still result in strong performance. In other words, RFL demonstrates relatively low sensitivity to a.\nA trade-off in using RFL is that, even though the choice of e becomes less critical, we now need to select an appropriate a. Our findings across various choices of e and tasks indicate that finding suitable a values may require extensive tuning (see Appendix C)."}, {"title": "CONCLUSION", "content": "In this work, we introduce Feasible Learning, a novel learning paradigm that frames learning as a constraint satisfaction problem. We show that FL problems can be solved using a primal-dual approach, which is as computationally efficient as ERM with gradient descent and offers comparable hyperparameter robustness.\nFL aligns with the growing demand for user-specific performance as machine learning models are increasingly applied in personalized areas like recommender systems and healthcare. Unlike ERM, FL directly supports meeting potential regulatory or industry standards that demand a certain level of performance for all users.\nWe demonstrate that models can learn through FL, even when using tools originally developed for ERM, such as modern deep learning architectures and mini-batch optimization techniques. Developing algorithmic tools specifically tailored to learning via FL is an important direction of future research."}]}