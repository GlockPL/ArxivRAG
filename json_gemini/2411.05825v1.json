{"title": "SurfGNN: A robust surface-based prediction model with\ninterpretability for coactivation maps of spatial and cortical features", "authors": ["Zhuoshuo Lia", "Jiong Zhangb,*", "Youbing Zenga", "Jiaying Lina", "Dan Zhang", "Jianjia Zhanga", "Duan Xud", "Hosung Kim", "Bingguang Liuf*", "Mengting Liua,*"], "abstract": "Current brain surface-based prediction models often overlook the variability of regional attributes at\nthe cortical feature level. While graph neural networks (GNNs) excel at capturing regional differences,\nthey encounter challenges when dealing with complex, high-density graph structures. In this work,\nwe consider the cortical surface mesh as a sparse graph and propose an interpretable prediction\nmodel-Surface Graph Neural Network (SurfGNN). SurfGNN employs topology-sampling learning\n(TSL) and region-specific learning (RSL) structures to manage individual cortical features at both\nlower and higher scales of the surface mesh, effectively tackling the challenges posed by the overly\nabundant mesh nodes and addressing the issue of heterogeneity in cortical regions. Building on this, a\nnovel score-weighted fusion (SWF) method is implemented to merge nodal representations associated\nwith each cortical feature for prediction. We apply our model to a neonatal brain age prediction task\nusing a dataset of harmonized MR images from 481 subjects (503 scans). SurfGNN outperforms\nall existing state-of-the-art methods, demonstrating an improvement of at least 9.0% and achieving\na mean absolute error (MAE) of 0.827\u00b10.056 in postmenstrual weeks. Furthermore, it generates\nfeature-level activation maps, indicating its capability to identify robust regional variations in different\nmorphometric contributions for prediction.", "sections": [{"title": "1. Introduction", "content": "There is growing evidence that brain development/aging\ntrajectories and developments of brain disorders both could\nbe traced on the cerebral cortex [1]. A prevalent approach for\ncharacterizing cerebral cortex is to reconstruct the cortical\nsurface and measure the morphological features, such as\ncortical thickness, surface area, sulcal depth, myelin content,\netc. [2] Currently, an important application of cortical fea-\ntures is to predict phenotypes, such as age [3, 4], sex[5], and\nbrain disease states[6] using machine learning methods, and\nit could help explore important biomarkers about the cortex\nevolutional process and diagnose brain disorders.\nFor surface-based analysis, early approaches solely fo-\ncused on vertex features without considering the topological\nstructure of the surface mesh[8]. More recent techniques uti-\nlized GNN-based networks to collectively examine the node\nfeatures and topological architecture of the cortical surface,\ndue to the graph-like characteristics of the surface mesh[9].\nHowever, managing cortical surface meshes that contain a\nvast number of vertices presents substantial computational\ndifficulties in graph analysis. A common approach to address\nthis is down-sampling the surface mesh, significantly reduc-\ning the vertex count before model learning [3, 4, 5]. Nev-\nertheless, this could either diminish the prediction accuracy\nof the model or reduce its capacity to yield meaningful and\ninterpretable results.\nAnother commonly employed approach for cortical sur-\nface manipulation works within a spherical framework [10,\n11, 12, 13]. It collects node features from the brain's surface,\nsequentially down-samples them, adhering to the hierar-\nchical spherical architecture of the cortical surface, and\neventually combines them for prediction. These models are\nefficient in managing large scale graphs with a high density\nof nodes. However, they couldn't flexibly identify the best\nsub-graph structures or important nodes that contribute best\nto the prediction task. This might be important because dif-\nferent regions exhibit diverse responses to various predictive\ndemands[17].\nIn addition to exploring the spatial heterogeneity at the\nglobal level, researches to date have seldom focused specif-\nically on the feature-level heterogeneity. That is, different\nfeatures may exhibit different spatial patterns in prediction\ntasks. Every cortical feature corresponds to a distinct macro-\nor micro- structure of the cerebral cortex. Observations dur-\ning rapid developmental stages [17], the aging process[20],\nor under pathological conditions [21] have demonstrated that\nthe regional variation in different cortical features could\nserve as distinct biomarkers for varying brain conditions.\nEnabling the separate manipulation of each cortical feature\nwithin the model, before integrating them for prediction\nanalysis, might significantly boost the model's performance.\nWe hypothesize that by allowing the autonomous expression\nof each feature, the model could capture more nuanced and\nimpactful information for prediction tasks. It could help\ndisaggregate the contribution of different cortical features to\nthe prediction, enabling spatial-feature-level interpretations\nof the model for each subject.\nA further challenge faced by surface-based models is\ntheir interpretability. It involves exploring the diverse char-\nacteristics of the cortical surface and identifying biomark-\ners associated with specific phenotypes. Post-hoc saliency-\nbased methods are widely used, which aim to pinpoint the\nmost impactful input features that contribute to a prediction\ntask by examining the gradients or activations within the\nnetwork in relation to a specific input[22]. Nevertheless,\nthese methods have limitations[23, 24] and may not always\nhold for neuroimaging and neuroscience research, where\navailable data are typically small-sized and much more com-\nplex [22, 25]. Another strategy is to develop a deep learning\nmodel with inherent self-interpretability[26]. This entails\ncreating an end-to-end framework that facilitates the identifi-\ncation of detailed explanatory factors, thereby improving the\nextraction of discriminative representations and leading to\nmore accurate outcomes. Previous research in this area, such\nas SiT[18] and NeuroExplainer[25], has shown promising\nresults. Motivated by these findings, creating a surface-based\nprediction model with built-in interpretability emerges as a\nsignificant and promising area of research.\nTo fulfill the outlined requirements and tackle the previ-\nously mentioned challenges, inspired by the GNN-based net-\nworks and the spherical frameworks, we propose a Surface\nGraph Neural Network (SurfGNN) as a self-interpretable\nprediction model. The entire framework of SurfGNN con-\nsists of topology-sampling learning (TSL) and region-specific\nlearning (RSL) structures for each cortical feature, and a\nscore-weighted fusion (SWF) structure across all features\nfor prediction. We assess our model within the context\nof a brain age prediction task. Predicting brain age from\nstructural brain neuroimaging data poses challenges akin to\nthose faced in various neuroimaging applications, serving\nas a foundation for developing and testing deep learning\nalgorithms. Furthermore, this task has gained attention due\nto its potential clinical and biological significance [57].\nTo summarize our contributions as follows:\n1. We formulate a graph analysis process comprising\nTSL and RSL structures, extending from low-level\nto high-level surface meshes characterized by higher\nand fewer numbers of vertices, respectively. The TSL\nefficiently performs sampling on sparse graphs, pre-\nserving the overall brain topological shape, while\nthe RSL effectively conducts in-depth graph analysis,\ndistinguishing the varied impacts of different brain\nregions on prediction.\n2. We propose a novel score-weighted fusion mecha-\nnism to amalgamate node information derived from"}, {"title": "2. Related works", "content": "2.1. Deep Learning Models on Cortical Surfaces\nSeveral strategies exist for applying deep learning mod-\nels to the cortical surface in non-Euclidean spaces for predic-\ntive tasks. The first strategy involves obtaining the spectral\ndomain and applying conventional convolutional neural net-\nworks (CNNs) on it [47, 8, 48]. It often performs poorly be-\ncause models based on spectral features capture only global\ninformation and miss local details[49]. The second strategy\ninvolves projecting the original surface onto an intrinsic\nspace, such as the tangent space, and then using 2D CNN\nmodels[50, 7]. This projection strategy, inescapably, intro-\nduces feature distortion and necessitates re-interpolation,\ncomplicating the network, which in turn elevates the com-\nputational burden and diminishes overall accuracy[10].\nOver recent years, the spherical space has usually been\napplied to surface-based models in several studies. Zhao\net al. [10] proposed the SphericalUNet with the Residual\nHexagonal Convolution, while Monti et al.[14] used Gaus-\nsian MM Conv in their model MoNet. Jiang et al.[15]\ndirectly applied the MeshConv on the sphere with their\nmodel UG-SCNN. These models well preserve the com-\nplete surface structure during learning, but couldn't flexibly\nidentify the best subgraph structures or important nodes that\ncontribute best to the prediction task. Additionally, Dahan et\nal.[18, 19] developed the application of vision transformers\nto spherical grid surfaces, implementing the self-attention\nmechanism. Nonetheless, the prediction accuracy of these\nmodels significantly relies on the selection of certain hyper-\nparameters, like the shape and size of patches, which could\nbe challenging to determine in practice.\nAccordingly, in our work SurfGNN, we implement a\ngraph neural network to formulate the feature learning pro-\ncess, efficiently managing complex non-Euclidean surfaces\nwith a high density of nodes and exploring the regional\nheterogeneity of cortical surface for prediction.\n2.2. Interpretable Methods\nPost-hoc methods are widely used in deep learning over\ngridded data, with some extensions to cortical surface cases.\nBesson et al. [5] employed the CAM method to map dis-\ncriminative brain regions for both sex and age prediction\ntasks. Liu et al. [4] applied perturbation methods with the\nsurface-based GCN model for brain age prediction. An-\nother approach involves building a deep learning model"}, {"title": "3. Method", "content": "Fig. 1 illustrates the complete architecture of SurfGNN.\nWe first outline the approach for extracting cortical surfaces\nand morphological features from brain MR images. Then we\nprovide a comprehensive exposition of each module within\nSurfGNN. This includes specific graph convolutional layers\nand graph pooling layers, utilized in the topology-sampling\nlearning and the region-specific learning structures, along\nwith the score-weighted fusion structure incorporating a\nread-out layer to predict phenotypes. Additionally, we de-\nscribe the employed loss functions during the network train-\ning process.\n3.1. Surface Reconstruction and Morphological\nFeatures Extraction\nBrain cortical features input to the model are retrieved\nfrom T1-weighted MR images, through NEOCIVET[27, 28,\n29], a pipeline for neonatal brain MRI processing. Begin-\nning with general MR image preprocessing-comprising\ndenoising, intensity nonuniformity correction, and brain ex-\ntraction-the process includes brain tissue segmentation to\ndelineate distinct regions such as white matter (WM), graph\nmatter (GM) and CSF. A marching-cube-based framework\ngenerates a triangulated mesh representing the WM surface\nalong the GM-WM boundary. The surface mesh is resampled\nto a fixed number of 81,924 vertices using the icosahedron\nspherical fitting, then refined to obtain the CSF surface along\nthe sharp edge of the GM-pial interface. Cortical features,\nincluding cortical thickness, sulcal depth, and GM/WM\nintensity ratio, are measured at each vertex on the surface.\nCortical surfaces with these features are later transformed\ninto a consistent template using the transformation obtained\nin the surface registration, enabling inter-subject compar-\nisons.\nFollowing the pipeline, a surface mesh, covering both\nWM and CSF surfaces for each brain, is generated, con-\ntaining 81,924 vertices and cortical feature measured at\neach vertex. Each surface mesh, with its 81,924 vertices,\nthen undergoes down-sampling using the icosahedron down-\nsampling approach [30] to four additional levels of resolu-\ntion, specifically 20,484, 5,124, 1,284, and 324 vertices,\nas Fig.2 shows. Notably, at lower resolutions, such as 84\nvertices, the mesh struggles to delineate a distinct brain\nstructure. Thus, a mesh with 324 vertices is typically consid-\nered the minimum resolution with the NEOCIVET pipeline.\nAbove all, the surface mesh of the brain is modeled\nas a sparse graph, where surface vertices serve as nodes,"}, {"title": "3.2. Topology-sampling Learning Structure", "content": "The model processes complex sparse graphs derived\nfrom high-node-count surface meshes. To manage this com-\nplexity while preserving the original brain shape, we intro-\nduce the topology-sampling learning (TSL) structure, which\ninspired by the previous spherical framework [7, 10]. It is\nutilized to integrate node context and to systematically sam-\nple sparse graphs according to the surface order illustrated\nin Fig. 2.\n3.2.1. Global Graph Convolutional Layer\nThe purpose of the graph convolutional layers here is\nto extract low-dimensional surface information from the\ngraph. A common approach models is the 1-ring hexagonal\nconvolution applied in spherical models [10]. However, un-\nlike in spherical meshes, our irregular surface meshes face\ninconsistencies in adjacent node counts. To address this, our\nimprovement strategy based on the hexagonal convolution,\ninvolves utilizing maximum number of adjacent nodes and\nrepresenting the features of absent nodes using zero, giving\nrise to a novel graph convolutional layer-global graph\nconvolutional layer.\nFor a sparse graph with $N^{(1)}$ nodes in the $l$th layer, $v_i^{(l)}$\nrepresents the $i$th node and $x_i^{(l)} \\in \\mathbb{R}^{d_l}$ as its feature. $E_i^{(1)}$ is\ndefined as the number of $v_i^{(l)}$'s 1-hop neighbors plus one (the\none represents the node $v_i^{(l)}$ itself). This value represents the\nnumber of nodes within a one-edge distance from $v_i^{(l)}$ and\nis used to determine the filter size for convolutional layers.\nSince different nodes may have varying numbers of neigh-\nbors, $E_i^{(1)}$ is not consistent across all nodes. To maintain\nefficiency, we use the maximum value of $E_i^{(1)}$, denoted as\n$E^{(1)}$, as the size of the convolutional filter. Moreover, $x_i^{(1)} =$\n$[x_1, ..., x_{E_i^{(l)}}]^{(l)}$ represents the feature of $v_i^{(l)}$ to learn. In cases\nwhere $E_i^{(l)} < E^{(1)}$, we employ zeros to complete the features\nof absent neighboring nodes and it must be $x_i^{(1)} \\in [\\mathbb{R}^{1\\times E^{(1)}d^{(1)}}$.\nNotably, the order of nodes is based on the angle between\nthe vector of center vertex to neighboring vertex and the x-\naxis in the tangent plane. If there are not enough neighboring\nnodes, the zero-padding is applied at the end.\nUltimately, we obtain the feature matrix. $X^{(1)} =$\n$[x_1, ..., x_{N^{(1)}}]^T$ as the input for the convolutional layer\nacross the entire graph. The filter $W \\in [\\mathbb{R}^{E^{(1)}d \\times d(1+1)}$ is\nestablished to learn it, and the output graphs with the same\n$N^{(1)}$ nodes and $d^{(1+1)}$ feature channels can be obtained.\nThe zero-padding strategy is employed to maintain con-\nsistency in global node information transmission, ensuring\nthat each node fully and uniquely integrates the information\nfrom its adjacent nodes. Notably, since the number of nodes"}, {"title": "3.2.2. Topology-preserved Pooling Layer", "content": "The pooling layer in this instance is designed to merge\nlocal features and transmit them to upper levers. Whereas\nit is similar with the node clustering pooling method[31] in\ntypical GNN models, a key distinction lies in the uniform and\nconsistent pattern of node clustering across all nodes from\ndifferent subjects for topology preservation, which is based\non the organization of vertices on each scale of meshes, as\nshown in Fig. 2. This pooling strategy is widely employed in\nthe spherical framework [7, 10].\nSpecifically, the pooling operation for node $u$ involves\nemploying the average pooling operation to combine its\nadjacent nodes feature, and then, nodes and features cor-\nresponding to a sparse graph at the next resolution are\npreserved. That is, assuming the graph before pooling with a\ncount $N^{(1)}$ of 81,924 nodes, after a single round of pooling,\nthe node count $N^{(1+1)}$ reduces to $\\frac{(N^{(\u00b9) + 12)}{4}$ of the output graph,\nfollowing the formula $N^{(1+1)} = \\frac{(N^{(\u00b9) + 12)}{4}$. This formula\nis derived from the process of triangularizing the surface\nmesh during the construction of brain surfaces."}, {"title": "3.3. Region-specific Learning Structure", "content": "Following the previously TSL structure", "calculated\nas": "n$x_i^{(l+1)} = relu(w_i^{(l)}x_i^{(l)} + \\sum_{j \\in N_i^{(l)}(i)}w_i^{(l)} x_j^{(l)})$\nwhere $w_i^{(l)}$ is the embedding kernel to be learned for the $i$th\nnode in the $l$th layer, $N_i^{(l)}(i)$ denotes the set of indices of\nneighboring nodes of node $v_i^{(l)}$. Eq. 1 illustrates the message\npassing mechanism [56"}]}