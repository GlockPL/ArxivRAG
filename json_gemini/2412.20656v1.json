{"title": "Overcoming Class Imbalance: Unified GNN Learning with Structural and Semantic Connectivity Representations", "authors": ["Abdullah Alchihabi", "Hao Yan", "Yuhong Guo"], "abstract": "Class imbalance is pervasive in real-world graph datasets, where the majority of annotated nodes belong to a small set of classes (majority classes), leaving many other classes (minority classes) with only a handful of labeled nodes. Graph Neural Networks (GNNs) suffer from significant perfor- mance degradation in the presence of class imbal- ance, exhibiting bias towards majority classes and struggling to generalize effectively on minority classes. This limitation stems, in part, from the message passing process, leading GNNs to overfit to the limited neighborhood of annotated nodes from minority classes and impeding the propa- gation of discriminative information throughout the entire graph. In this paper, we introduce a novel Unified Graph Neural Network Learning (Uni-GNN) framework to tackle class-imbalanced node classification. The proposed framework seamlessly integrates both structural and seman- tic connectivity representations through seman- tic and structural node encoders. By combin- ing these connectivity types, Uni-GNN extends the propagation of node embeddings beyond im- mediate neighbors, encompassing non-adjacent structural nodes and semantically similar nodes, enabling efficient diffusion of discriminative in- formation throughout the graph. Moreover, to harness the potential of unlabeled nodes within the graph, we employ a balanced pseudo-label generation mechanism that augments the pool of available labeled nodes from minority classes in the training set. Experimental results under- score the superior performance of our proposed Uni-GNN framework compared to state-of-the-art class-imbalanced graph learning baselines across multiple benchmark datasets.", "sections": [{"title": "1. Introduction", "content": "Graph Neural Networks (GNNs) have exhibited significant success in addressing the node classification task (Kipf & Welling, 2017; Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2018) across diverse application domains from molecular biology (Hao et al., 2020) to fraud detection (Zhang et al., 2021). The efficacy of GNNs has been particularly notable when applied to balanced annotated datasets, where all classes have a similar number of labeled training instances. The performance of GNNs experiences a notable degradation when confronted with an increasingly imbalanced class dis- tribution in the available training instances (Yun et al., 2022). This decline in performance materializes as a bias towards the majority classes, which possess a considerable number of labeled instances, resulting in a challenge to generalize effectively over minority classes that have fewer labeled instances (Park et al., 2021; Yan et al., 2023). The root of this issue lies in GNNs' reliance on message passing to dis- seminate information across the graph. Specifically, when the number of labeled nodes for a particular class is limited, GNNs struggle to propagate discriminative information re- lated to that class throughout the entire graph. This tendency leads to GNNs' overfitting to the confined neighborhood of labeled nodes belonging to minority classes (Tang et al., 2020; Yun et al., 2022; Li et al., 2023). This is commonly denoted as the 'under-reaching problem' (Sun et al., 2022) or 'neighborhood memorization' (Park et al., 2021).\nClass-imbalanced real-world graph data are widespread, spanning various application domains such as the Internet of Things (Wang et al., 2022), Fraud Detection (Zhang et al., 2021), and Cognitive Diagnosis (Wang et al., 2023). Con- sequently, there is a critical need to develop GNN models that demonstrate robustness to class imbalance, avoiding biases towards majority classes while maintaining the abil- ity to generalize effectively over minority classes. Tradi- tional methods addressing class imbalance, such as over- sampling (Chawla et al., 2002) or re-weighting (Yuan & Ma, 2012), face limitations in the context of graph-structured data as they do not account for the inherent graph struc- ture. Consequently, several approaches have been pro- posed to specifically tackle class imbalance within the realm of semi-supervised node classification. Topology-aware"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Class Imbalanced Learning", "content": "Class-imbalanced learning methods generally fall into three categories: re-sampling, re-weighting, and data augmen-"}, {"title": "2.2. Class Imbalanced Graph Learning", "content": "Traditional class imbalance methods assume independently distributed labeled instances, a condition not met in graph data where nodes influence each other through the graph structure. Existing class imbalanced graph learning can be categorized into three groups: re-weighting, over-sampling, and ensemble methods. Re-weighting methods assign im- portance weights to labeled nodes based on their class labels while considering the graph structure. Topology-Aware Margin (TAM) adjusts node logits by incorporating local connectivity (Song et al., 2022). However, re-weighting methods have limitations in addressing the under-reaching or neighborhood memorization problems (Park et al., 2021; Sun et al., 2022). Graph over-sampling methods gener- ate new nodes and connect them to the existing graph. GraphSMOTE uses SMOTE to synthesize minority nodes in the learned embedding space and connects them using an edge generator module (Zhao et al., 2021). ImGAGN employs a generative adversarial approach to balance the input graph by generating synthetic nodes with semantic and topological features (Qu et al., 2021). GraphENS generates new nodes by mixing minority class nodes with others in the graph, using saliency-based node mixing for features and KL divergence for structure information (Park et al., 2021). However, these methods increase the graph size, leading to higher computational costs. Long-Tail Experts for Graphs (LTE4G) is an ensemble method that divides training nodes based on class and degree into subsets, trains expert teacher models, and uses knowledge distillation to obtain student models (Yun et al., 2022). It however comes with a computational cost due to training multiple models."}, {"title": "3. Method", "content": "In the context of semi-supervised node classification with class imbalance, we consider a graph G = (V, E), where V represents the set of N = |V nodes, and E denotes the set of edges within the graph. E is commonly represented by an adjacency matrix A of size N \u00d7 N. This matrix is assumed to be symmetric (i.e. for undirected graphs), and it may contain either weighted or binary values. Each node in the graph is associated with a feature vector of size D, while the feature vectors for all the N nodes are organized into an input feature matrix X \u2208 RN\u00d7D. The set of nodes V is partitioned into two distinct subsets: V\u2081, comprising labeled nodes, and Vu, encompassing unlabeled nodes. The labeled nodes in Vi are paired with class labels, and this information is encapsulated in a label indicator matrix Y\u00b9 \u2208 {0,1}Nixc. Here, C signifies the number of classes, and Ni is the num- ber of labeled nodes. Further, Vi can be subdivided into C non-overlapping subsets, denoted as {V\u00bf\u00b9,\u2026\u2026,VC}, where each subset Vii corresponds to the labeled nodes belonging to class i. It is noteworthy that Vi exhibits class imbalance, characterized by an imbalance ratio p. This ratio, defined as \u03c1 = \\frac{min (|V^i|)}{max (|V^i|)}, is considerably less than 1. Such class imbalance problem introduces challenges and necessitates specialized techniques in the development of effective node classification models.\nThis section introduces the proposed Unified Graph Neural Network Learning (Uni-GNN) framework designed specif- ically for addressing class-imbalanced semi-supervised node classification tasks. The Uni-GNN framework in- tegrates both structural and semantic connectivity to fa- cilitate the learning of discriminative, unbiased node em- beddings. Comprising two node encoders-structural and semantic-Uni-GNN ensures joint utilization of these two facets. The collaborative efforts of the structural and seman- tic encoders converge in the balanced node classifier, which effectively utilizes the merged output from both encoders to categorize nodes within the graph. Notably, a weighted loss function is employed to address the challenge of class-imbalanced nodes in the graph, ensuring that the classifier is robust and capable of handling imbalanced class distri- butions. To harness the potential of unlabeled nodes, we introduce a balanced pseudo-label generation strategy. This strategy generates class-balanced confident pseudo-labels for the unlabeled nodes, contributing to the overall robust- ness and effectiveness of the Uni-GNN framework. In the rest of this section, we delve into the details of both the Uni- GNN framework and the balanced pseudo-label generation strategy, as well as the synergies between them."}, {"title": "3.1. Unified GNN Learning Framework", "content": "The Unified GNN Learning (Uni-GNN) framework com- prises three crucial components: the structural node encoder,"}, {"title": "3.1.1. STRUCTURAL NODE ENCODER", "content": "The objective of the structural encoder is to learn an embed- ding of the nodes based on structural connectivity. Instead of directly using the input adjacency matrix A, we construct a new structural connectivity-based graph adjacency ma- trix Astruct that extends connections beyond the immediate neighbors in the input graph. This matrix is determined by the distances between pairs of nodes, measured in terms of the number of edges along the shortest path connecting the respective nodes, such that\n$\\text{Astruct} [i, j] = \\begin{cases} \\frac{1}{\\text{SPD}(i, j)} & \\text{if } \\text{SPD}(i, j) \\leq a \\\\ 0 & \\text{otherwise} \\end{cases}$ (1)\nwhere SPD(., .) is the shortest path distance function that measures the distance between pairs of input nodes in terms of the number of edges along the shortest path in the input"}, {"title": "3.1.2. SEMANTIC NODE ENCODER", "content": "The objective of the semantic node encoder is to learn node embeddings based on the semantic connectivity. The se- mantic node encoder, denoted as fsem, comprises L message passing layers. In each layer l of the semantic node encoder, represented by fem, a semantic-based graph adjacency ma- trix Acem is constructed based on the similarity between the embeddings of nodes from the previous layer of the semantic and structural node encoders, measured in terms of clustering assignments. For each layer l, the following"}, {"title": "3.1.3. BALANCED NODE CLASSIFIER", "content": "We define a balanced node classification function 6, which classifies the nodes in the graph based on their structural and semantic embeddings learned by the Structural Encoder and Semantic Encoder respectively. In particular, the balanced node classification function takes as input the output of the L-th layers of the structural and semantic node encoders, denoted as HL Istruct and Hen Isem, respectively:\nP = \u03a6(HL Istruct||HL Isem) (7)\nwhere P \u2208 RN\u00d7C is the predicted class probability matrix of all the nodes in the graph. Given the class imbalance in the set of labeled nodes Vi, the node classification func- tion is trained to minimize the following weighted node classification loss on the labeled nodes:\nL = \u2211iev Weilce (Pi, Y).\niEVI (8)\nHere, lce denotes the standard cross-entropy loss function. For a given node i, P\u2081 represents its predicted class proba- bility vector, and Y is the true label indicator vector if i is a labeled node. The weight we associated with each node i is introduced to balance the contribution of data from different classes in the supervised training loss. It gives different weights to nodes from different classes. In particular, the class balance weight we is calculated as follows:\nwci = \\frac{1}{|V^{ci}|} (9)\nwhere ci denotes the class index of node i, such that Y\u00b9[i, ci] = 1; and |V| is the size of class ci in the la- beled nodes- i.e., the number of labeled nodes Ve from class ci. Since we\u2081 is inversely proportional to the corre- sponding class size, it enforces that larger weights are as- signed to nodes from minority classes with fewer labeled"}, {"title": "3.2. Balanced Pseudo-Label Generation", "content": "To leverage the unlabeled nodes in the graph, a balanced pseudo-label generation mechanism is proposed. The objec- tive is to increase the number of available labeled nodes in the graph while considering the class imbalance in the set of labeled nodes. The goal is to generate more pseudo-labels from minority classes and fewer pseudo-labels from major- ity classes, thus balancing the class label distribution of the training data. In particular, for each class c, the number of nodes to pseudo-label, denoted as \u00d1e, is set as the difference between the largest labeled class size and the size of class c, aiming to balance the class label distribution over the union of labeled nodes and pseudo-labeled nodes:\n\u00d1c = maxi\u2208 {1,...,c}(|V|) \u2013 |Vi| (10)\nThe set of unlabeled nodes that can be confidently pseudo- labeled to class c can be determined as:\nV = {i | max(Pi) > \u03f5, argmax(Pi) = c, i \u2208 Vu} (11)\nwhere e is a hyperparameter determining the confidence prediction threshold. Balanced sampling is then performed on each set Ve by selecting the top Ne nodes, denoted as Top (V), with the most confident pseudo-labels based on the predicted probability P\u2081[c]. This results in a total set of pseudo-labeled nodes, denoted as Vu, from all classes:\nVu = {Top\u2081 (V),\u2026, Top\u00d1c (V)}. (12)\nThe Unified GNN Learning framework is trained to mini- mize the following node classification loss over this set of pseudo-labeled nodes Vu:\n\u2211Pi , Largmax(P\u2081)EVi\u2208Vlee.\n$\\text{Lps} = \\frac{1}{|V_u|}$ (13)\nwhere lce again is the standard cross-entropy loss function, Pi is the predicted class probability vector with classifier 4, and 1argmax(P\u2081) is a one-hot pseudo-label vector with a single 1 at the predicted class entry argmax(Pi). This pseudo- labeling mechanism aims to augment the labeled node set, particularly focusing on addressing class imbalances by generating more pseudo-labels for minority classes."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup", "content": "Datasets & Baselines We conduct experiments on three datasets (Cora, CiteSeer and PubMed) (Sen et al., 2008). To ensure a fair comparison, we adhere to the evaluation protocol used in previous studies (Zhao et al., 2021; Yun et al., 2022). The datasets undergo manual pre-processing to achieve the desired imbalance ratio (p). Specifically, each majority class is allocated 20 labeled training nodes, while each minority class is assigned 20 \u00d7 p labeled training nodes. For validation and test sets, each class is assigned 25 and 55 nodes, respectively. Following the protocol of (Yun et al., 2022), we consider two imbalance ratios (p = 10%, 5%), and two different numbers of minority classes: 3 and 5 on Cora and CiteSeer, and 1 and 2 on the PubMed dataset. Ad-"}, {"title": "4.2. Comparison Results", "content": "We evaluate the performance of Uni-GNN framework on the semi-supervised node classification task under class im- balance. Across the three datasets, we explore four distinct evaluation setups by manipulating the number of minority classes and the imbalance ratio (p). Additionally, we ex- plore the long-tail class label distribution setting for Cora and CiteSeer with an imbalance ratio of p = 1%. We as- sess the performance of Uni-GNN using balanced Accuracy (bAcc), Macro-F1, and Geometric Means (G-Means), re- porting the mean and standard deviation of each metric over 3 runs. Table 1 summarizes the results for different num- bers of minority classes and imbalance ratios on all three datasets, while Table 2 showcases the results under long-tail class label distribution on Cora and CiteSeer.\nTable 1 illustrates that the performance of all methods di- minishes with decreasing imbalance ratio (p) and increasing numbers of minority classes. Our proposed framework con- sistently outperforms the underlying GCN baseline and all other methods across all three datasets and various evalua- tion setups. The performance gains over the GCN baseline are substantial, exceeding 10% in most cases for Cora and CiteSeer datasets and 13% for most instances of the PubMed dataset. Moreover, Uni-GNN consistently demonstrates su- perior performance compared to all other comparison meth- ods, achieving notable improvements over the second-best method (LTE4G) by around 3%, 5%, and 11% on Cora, Cite- Seer with 3 minority classes and p = 10%, and PubMed with 1 minority class and p = 10%, respectively. Similarly, Table 2 highlights that Uni-GNN consistently enhances the performance of the underlying GCN baseline, achieving"}, {"title": "4.3. Ablation Study", "content": "We conducted an ablation study to discern the individual contributions of each component in our proposed frame- work. In addition to the underlying GCN baseline, eight variants were considered: (1) Independent Node Encoders (Ind. Enc.): each node encoder exclusively propagates its own node embeddings, instead of propagating the concate- nated semantic and structural embeddings. Specifically, fsem solely propagates Hem\u00b9 while fstruct solely propagates Hstruct (2) Drop Lps: excluding the balanced pseudo-label generation. (3) Astruct = A: structural connectivity con- siders only immediate neighbors of nodes (a = 1). (4) Semantic Encoder only (Semantic Enc.): it discards the structural encoder. (5) Structural Encoder only (Structural Enc.): it discards the semantic encoder. (6) Imbalanced Pseudo-Labeling: it generates pseudo-labels for all unla- beled nodes with confident predictions without considering class imbalance. (7) Fixed {Aem}l_2: it does not update the semantic connectivity during training. (8) Wc\u2081 = 1, Vi \u2208 Vi: it assigns equal weights to all labeled nodes in the train- ing set. Evaluation was performed on Cora and CiteSeer datasets, each with 3 minority classes and imbalance ratio (p) of 10%, and with long-tail class label distribution and imbalance ratio (p) of 1%. The results are reported in Table 3.\nTable 3 illustrates performance degradation in all variants compared to the full proposed framework. The observed performance decline in the Independent Node Encoders (Ind. Enc.) variant underscores the importance of simultaneously propagating semantic and structural embeddings across both the semantic and structural connectivity. This emphasizes the need for incorporating both aspects to effectively learn more discriminative node embeddings. The performance drop observed in the Semantic Enc. and Structural Enc. vari- ants underscores the significance and individual contribution of each connectivity type to the proposed Uni-GNN frame- work. This highlights the critical role that each type of con- nectivity plays in the overall performance of the proposed framework. The Astruct = A variant's performance drop em- phasizes the importance of connecting nodes beyond their immediate structural neighbors, enabling the propagation of messages across a larger portion of the graph and learning more discriminative embeddings. The performance degra- dation in Drop Lps and Imbalanced Pseudo-Labeling vari-"}, {"title": "4.4. Hyper-Parameter Sensitivity", "content": "To investigate the impact of the hyper-parameters in Uni- GNN framework, we present the results of several sensitivity analyses on the Cora dataset with 3 minority classes and an imbalance ratio of 10% in Figure 2. Figures 2a, 2b, 2c, and 2d depict the accuracy of Uni-GNN as we independently vary the max SPD distance in Astruct (a), the number of clusters (K), the pseudo-label confidence threshold (\u20ac) and the rate of updating Asem}l-2 (\u03b2), respectively. Larger values for a result in performance degradation due to over- smoothing as the graph becomes more densely connected. Optimal performance is achieved with a = 2. Uni-GNN exhibits robustness to variations in the hyperparameters K, e, and \u1e9e within a broad range. It consistently outperforms state-of-the-art methods across diverse settings of these hy- perparameters, as depicted by the corresponding results presented in Table 1. Small K values lead to noisy clus- ters with mixed-class nodes, while large K values result in over-segmented clusters with sparse semantic connectiv- ity. Optimal performance is achieved when K falls within"}, {"title": "5. Conclusion", "content": "In this paper, we introduced a novel Uni-GNN framework for class-imbalanced node classification. The proposed framework harnesses the combined strength of structural and semantic connectivity through dedicated structural and semantic node encoders, enabling the learning of a uni- fied node representation. By utilizing these encoders, the structural and semantic connectivity ensures effective prop- agation of messages well beyond the structural immediate neighbors of nodes, thereby addressing the under-reaching and neighborhood memorization problems. Moreover, we proposed a balanced pseudo-label generation mechanism to incorporate confident pseudo-label predictions from minor- ity unlabeled nodes into the training set. Our experimental evaluations on three benchmark datasets for node classifi- cation affirm the efficacy of our proposed framework. The results demonstrate that Uni-GNN adeptly mitigates class imbalance bias, surpassing existing state-of-the-art methods"}, {"title": "A. Training Procedure", "content": "The details of the training procedure of the Unified GNN Learning framework are presented in algorithm 2."}, {"title": "B. Datasets", "content": "The specifics of the benchmark datasets are detailed in Table 4. Detailed class label distribution information for the training sets in all evaluation setups on all datasets are provided in Table 5."}]}