{"title": "Towards General Purpose Robots at Scale:\nLifelong Learning and Learning to Use Memory", "authors": ["William H Yue"], "abstract": "The widespread success of artificial intelligence in fields like natural language processing\nand computer vision has not yet fully transferred to robotics, where progress is hindered\nby the lack of large-scale training data and the complexity of real-world tasks. To address\nthis, many robot learning researchers are pushing to get robots deployed at scale in\neveryday unstructured environments like our homes to initiate a data flywheel. While\ncurrent robot learning systems are effective for certain short-horizon tasks, they are not\ndesigned to autonomously operate over long time horizons in unstructured environments.\nThis thesis focuses on addressing two key challenges for robots operating over long time\nhorizons: memory and lifelong learning.\nWe propose two novel methods to advance these capabilities. First, we introduce\nt-DGR, a trajectory-based deep generative replay method that achieves state-of-the-art\nperformance on Continual World benchmarks, advancing lifelong learning. Second, we\ndevelop a framework that leverages human demonstrations to teach agents effective\nmemory utilization, improving learning efficiency and success rates on Memory Gym\ntasks. Finally, we discuss future directions for achieving the lifelong learning and memory\ncapabilities necessary for robots to function at scale in real-world settings.", "sections": [{"title": "1 Introduction", "content": "Fields like language and vision have achieved remarkable progress in advancing artificial\nintelligence, largely through training massive neural networks on extensive and diverse\ndatasets [43, 52, 15, 83]. However, these successes have not fully translated to robotics,\nwhere robots are often limited to impressive demos but in controlled environments. This\ncontrasts sharply with the widespread deployment seen in natural language processing\nand computer vision [1, 12]. A significant bottleneck in robotics progress is the lack of\nlarge-scale training data. While language and vision data are abundant online, robot data\ndoes not naturally exist at such scale [55, 79].\nA long-standing goal for robot learning researchers is to ignite a data flywheel (Fig-\nure 1.2) through large-scale deployment of general-purpose robots [63, 123]. With more\nusers, robots would collect more diverse data, enabling better models and more capable\nrobots, which would attract even more users [25, 30]. But what does it take to achieve this\ndata engine? To get general-purpose robots into our homes and workplaces, they must\noperate effectively over long-horizon timescales, far beyond the short-horizon tasks featured\nin today's demos. State-of-the-art robot learning algorithms are not yet designed to handle\nthe complexity and long time-horizons required for real-world, large-scale deployment.\nTake the example of a Roomba vacuum robot (Figure 1.3). It may perform well in\na controlled demo, cleaning a small, obstacle-free room. But in a real-world home, it\nrequires constant user intervention\u2014moving obstacles that get it stuck, redirecting it to\nmissed areas\u2014making it hardly more efficient than simply vacuuming yourself. People\nwant robots that don't need such babysitting. For instance, rather than setting up a robot\nin front of a washing machine with pre-set laundry and controls, we want robots that\nautonomously locate the laundry basket in a closet, carry it to the laundry room, operate\nthe washing machine, and return clothes to their proper place.\nThis demands robots capable of long-horizon operation, on the scale of hours, days, or\neven weeks, rather than just minutes. Long-horizon functionality requires sophisticated\nmemory to recall tasks, locations, and routines, as well as the ability to continually learn.\nFor example, memory is crucial to remember where clothes are stored, navigate routes,\nand recall how items are organized. However, current methods typically rely only on\nimmediate observations or those from a few seconds earlier, neglecting the broader context\n[13, 22, 121, 31]. Additionally, as environments evolve, robots must acquire new skills such\nas adapting to changes in household layouts or factory workflows. Unfortunately, most\nexisting approaches involve fixed models that cannot adapt post-deployment [13].\nCrucially, achieving the memory and lifelong learning that enables long-horizon op-\neration requires more than scaling up current methods. We need fundamentally new\napproaches that enable robots to develop robust memory and continuously learn through-\nout their deployment, addressing the complexities of real-world tasks over extended\ntimescales. This thesis presents my efforts over the past two years to advance adaptive\nlifelong learning algorithms and develop capable memory mechanisms for robotic agents.\nIn Chapter 2, we present t-DGR, a novel lifelong learning algorithm that achieves\nstate-of-the-art performance on the Continual World benchmark. Chapter 3 introduces a\nframework for teaching agents to effectively utilize memory through human demonstrations.\nThese chapters are primarily based on my papers [118] and [117], respectively. Finally,\nChapter 4 outlines potential future directions inspired by this work."}, {"title": "2 Lifelong Learning", "content": "In this chapter, we introduce a method that enables robots to learn continuously throughout\ntheir lifetime, rather than just once. Deep generative replay has emerged as a promising\napproach for continual learning in decision-making tasks. This approach addresses the\nproblem of catastrophic forgetting by leveraging the generation of trajectories from\npreviously encountered tasks to augment the current dataset. However, existing deep\ngenerative replay methods for continual learning rely on autoregressive models, which\nsuffer from compounding errors in the generated trajectories. We propose a simple,\nscalable, and non-autoregressive method for continual learning in decision-making tasks\nusing a generative model that generates task samples conditioned on the trajectory\ntimestep. We evaluate our method on Continual World benchmarks and find that\nour approach achieves state-of-the-art performance in terms of average success rate metric\namong continual learning methods. The code used in these experiments is available at\nhttps://github.com/WilliamYue37/t-DGR."}, {"title": "2.1 Introduction", "content": "Continual learning, also known as lifelong learning, is a critical challenge in the advance-\nment of general artificial intelligence, as it enables models to learn from a continuous\nstream of data encompassing various tasks, rather than having access to all data at once\n[84]. However, a major challenge in continual learning is the phenomenon of catastrophic\nforgetting, where previously learned skills are lost when attempting to learn new tasks\n[71].\nTo mitigate catastrophic forgetting, replay methods have been proposed, which involve\nsaving data from previous tasks and replaying it to the learner during the learning of\nfuture tasks. This approach mimics how humans actively prevent forgetting by reviewing\nmaterial for tests and replaying memories in dreams. However, storing data from previous\ntasks requires significant storage space and becomes computationally infeasible as the\nnumber of tasks increases.\nIn the field of cognitive neuroscience, the Complementary Learning Systems theory\noffers insights into how the human brain manages memory. This theory suggests that\nthe brain employs two complementary learning systems: a fast-learning episodic system\nand a slow-learning semantic system [70, 59, 69]. The hippocampus serves as the episodic\nsystem, responsible for storing specific memories of unique events, while the neocortex\nfunctions as the semantic system, extracting general knowledge from episodic memories\nand organizing it into abstract representations [80].\nDrawing inspiration from the human brain, deep generative replay (DGR) addresses\nthe catastrophic forgetting issue in decision-making tasks by using a generative model as\nthe hippocampus to generate trajectories from past tasks and replay them to the learner\nwhich acts as the neocortex (Figure 2.2) [93]. The time-series nature of trajectories in\ndecision-making tasks sets it apart from continual supervised learning, as each timestep of\nthe trajectory requires sufficient replay. In supervised learning, the learner's performance\nis not significantly affected if it performs poorly on a small subset of the data. However,\nin decision-making tasks, poor performance on any part of the trajectory can severely\nimpact the overall performance. Therefore, it is crucial to generate state-action pairs\nthat accurately represent the distribution found in trajectories. Furthermore, the high-\ndimensional distribution space of trajectories makes it computationally infeasible to\ngenerate complete trajectories all at once."}, {"title": "2.2 Related Work", "content": "This section provides an overview of existing continual learning methods, with a particular\nfocus on pseudo-rehearsal methods."}, {"title": "2.2.1 Continual Learning in the Real World", "content": "As the field of continual learning continues to grow, there is an increasing emphasis on\ndeveloping methods that can be effectively applied in real-world scenarios [107, 5, 8, 44, 105].\nThe concept of \u201cGeneral Continual Learning\" was introduced by Buzzega et al. [17] to\naddress certain properties of the real world that are often overlooked or ignored by existing\ncontinual learning methods. Specifically, two important properties, bounded memory\nand blurry task boundaries, are emphasized in this work. Bounded memory refers to the\nrequirement that the memory footprint of a continual learning method should remain\nbounded throughout the entire lifespan of the learning agent. This property is crucial\nto ensure practicality and efficiency in real-world scenarios. Additionally, blurry task\nboundaries highlight the challenge of training on tasks that are intertwined, without clear\ndelineation of when one task ends and another begins. Many existing methods fail to\naccount for this characteristic, which is common in real-world learning scenarios. While\nthere are other significant properties associated with continual learning in the real world,\nthis study focuses on the often-neglected aspects of bounded memory and blurry task\nboundaries. By addressing these properties, we aim to develop methods that are more\nrobust and applicable in practical settings."}, {"title": "2.2.2 Continual Learning Methods", "content": "Continual learning methods for decision-making tasks can be categorized into three main\ncategories.\nRegularization Regularization methods in continual learning focus on incorporating\nconstraints during model training to promote the retention of past knowledge. One simple\napproach is to include an L2 penalty in the loss function. Elastic Weight Consolidation\n(EWC) builds upon this idea by assigning weights to parameters based on their importance\nfor previous tasks using the Fisher information matrix [57]. MAS measures the sensitivity\nof parameter changes on the model's output, prioritizing the retention of parameters with\na larger effect [4]. VCL leverages variational inference to minimize the Kullback-Leibler\ndivergence between the current and prior parameter distributions [76]. Progress and\nCompress learns new tasks using a separate model and subsequently distills this knowledge\ninto the main model while safeguarding the previously acquired knowledge [92]. However,\nregularization methods may struggle with blurry task boundaries as they rely on knowledge\nof task endpoints to apply regularization techniques effectively. In our experiments, EWC\nwas chosen as the representative regularization method based on its performance in the\noriginal Continual World experiments [113].\nArchitecture-based Methods Architecture-based methods aim to maintain distinct\nsets of parameters for each task, ensuring that future learning does not interfere with\nthe knowledge acquired from previous tasks. Packnet [67], UCL [2], and AGS-CL [50]\nall safeguard previous task information in a neural network by identifying important\nparameters and freeing up less important parameters for future learning. Identification\nof important parameters can be done through iterative pruning (Packnet), parameter\nuncertainty (UCL), and activation value (AGS-CL). However, a drawback of parameter\nisolation methods is that each task requires its own set of parameters, which may eventually\nexhaust the available parameters for new tasks and necessitate a dynamically expanding\nnetwork without bounded memory [115]. Additionally, parameter isolation methods\nrequire training on a single task at a time to prune and isolate parameters, preventing\nconcurrent learning from multiple interwoven tasks. In our experiments, PackNet was\nselected as the representative architecture-based method based on its performance in the\noriginal Continual World experiments [113].\nPseudo-rehearsal Methods Pseudo-rehearsal methods mitigate the forgetting of\nprevious tasks by generating synthetic samples from past tasks and replaying them to\nthe learner. Deep generative replay (DGR) (Figure 2.2) utilizes a generative model,\nsuch as generative adversarial networks [33], variational autoencoders [56], or diffusion\nmodels [41, 95], to generate the synthetic samples. Originally, deep generative replay\nwas proposed to address continual supervised learning problems, where the generator\nonly needed to generate single data point samples [93]. However, in decision-making\ntasks, expert demonstrations consist of trajectories (time-series) with a significantly\nhigher-dimensional distribution space.\nOne existing DGR method generates individual state observations i.i.d. instead of\nentire trajectories. However, this approach leads to a higher sample complexity compared\nto generating entire trajectories. The sample complexity of generating enough individual\nstate observations i.i.d. to cover every portion of the trajectory m times can be described\nusing the Double Dixie Cup problem [74]. For trajectories of length n, it takes an\naverage of $\\mathcal{O}(n \\log n + mn \\log \\log n)$ i.i.d. samples to ensure at least m samples for each\ntimestep. In scenarios with limited replay time (small m) and long trajectories (large n)\nthe sample complexity can be approximated as $\\mathcal{O}(n \\log n)$ using the Coupon Collector's\nproblem [73]. The additional $\\mathcal{O}(\\log n)$ factor reduces the likelihood of achieving complete\nsample coverage of the trajectory when the number of replays or replay time is limited,\nespecially considering the computationally expensive nature of current generative methods.\nFurthermore, there is a risk that the generator assigns different probabilities to each\ntimestep of the trajectory, leading to a selective focus on certain timesteps rather than\nequal representation across the trajectory.\nAnother existing DGR method is autoregressive trajectory generation. In the existing\nautoregressive method, CRIL, a generator is used to generate samples of the initial state,"}, {"title": "2.3 Background", "content": "This section introduces notation and the formulation of the continual imitation learning\nproblem that we use in this chapter. Additionally, we provide a concise overview of\ndiffusion probabilistic models used in our generative model implementation."}, {"title": "2.3.1 Imitation Learning", "content": "Imitation learning algorithms aim to learn a policy $\\pi_\\theta$ parameterized by $\\theta$ by imitating a\nset of expert demonstrations $\\mathcal{D} = {\\tau_i}_{i=1...M}$. Each demonstration trajectory $\\tau_i$ is a sequence of\nstate-action pairs $\\{(s_j, a_j)\\}_{j=1...|\\tau_i|}$ where $|\\tau_i|$ is the length of the trajectory. Each trajectory\ncomes from a task $\\mathcal{T}$ which is a Markov decision process that can be represented as a tuple\n$\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{T}, \\rho_0 \\rangle$ with state space $\\mathcal{S}$, action space $\\mathcal{A}$, transition dynamics $\\mathcal{T} : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$,\nand initial state distribution $\\rho_0$. Various algorithms exist for imitation learning, including"}, {"title": "2.3.2 Continual Imitation Learning", "content": "In the basic formulation most common in the field today, continual imitation learning\ninvolves sequentially solving multiple tasks $\\mathcal{T}_1, \\mathcal{T}_2, ..., \\mathcal{T}_N$. When solving for task $\\mathcal{T}_i$, the\nlearner only gets data from task $\\mathcal{T}_i$ and can not access data for any other task. In a more\ngeneral scenario, certain tasks may have overlapping boundaries, allowing the learner to\nencounter training data from multiple tasks during certain phases of training. The learner\nreceives a continuous stream of training data in the form of trajectories $\\tau_1, \\tau_2, \\tau_3, ...$ from\nthe environment, where each trajectory $\\tau$ corresponds to one of the $N$ tasks. However,\nthe learner can only access a limited contiguous portion of this stream at any given time.\nLet $s_i$ be the success rate of task $\\mathcal{T}_i$ after training on all $N$ tasks. The continual\nimitation learning objective is defined as maximizing the average success rate over all\ntasks:\n$S = \\frac{1}{N} \\sum_{i=1}^{N} s_i$                                                                      (2.2)\nThe primary issue that arises from the continual learning problem formulation is the\nproblem of catastrophic forgetting where previously learned skills are forgotten when\ntraining on a new task."}, {"title": "2.3.3 Diffusion Probabilistic Models", "content": "Diffusion probabilistic models [41, 95] generate data through a learned reverse denoising\ndiffusion process $p_\\theta(x_{t-1} | x_t)$. The forward diffusion process $q(x_t | x_{t-1})$ gradually adds\nGaussian noise to an input $x_0$ at each time step $t$, ultimately resulting in pure noise $x_T$ at\n$t = T$. The forward diffusion process is defined as:\n$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t \\textbf{I})$                                                                      (2.3)"}, {"title": "2.3.4 Notation", "content": "Deep generative replay involves training two models: a generator $G_\\gamma$ parameterized by\n$\\gamma$ and a learner $\\pi_\\theta$ parameterized by $\\theta$. We define $G_\\gamma^{(i)}$ as the generator trained on\ntasks $\\mathcal{T}_1 ... \\mathcal{T}_i$ and capable of generating data samples from tasks $\\mathcal{T}_1 ... \\mathcal{T}_i$. Similarly, $\\pi_\\theta^{(i)}$\nrepresents the learner trained on tasks $\\mathcal{T}_1 ... \\mathcal{T}_i$ and able to solve tasks $\\mathcal{T}_1 ... \\mathcal{T}_i$.\nA sequence of state observations $(s_1, s_2, ..., s_{n-1}, s_n)$ is temporally coherent if\n$\\forall 1 < i < n, \\exists a \\in \\mathcal{A} : \\mathcal{T}(s_i, a, s_{i+1}) > \\epsilon$, where $0 < \\epsilon < 1$ is a small constant representing a\nthreshold for negligible probabilities."}, {"title": "2.4 Method", "content": "Our proposed method, t-DGR, tackles the challenge of generating long trajectories by\ntraining a generator, denoted as $G_\\gamma(j)$, which is conditioned on the trajectory timestep $j$\nto generate state observations. Pseudocode for t-DGR is provided as Algorithm 1. The\nalgorithm begins by initializing the task index, replay ratio, generator model, learner\nmodel, and learning rates (Line 1). The replay ratio, denoted as $0 < r < 1$, determines\nthe percentage of training samples seen by the learner that are generated. Upon receiving\ntraining data from the environment, t-DGR calculates the number of trajectories to\ngenerate based on the replay ratio r (Lines 4-5). The variable L (Line 7) represents the\nmaximum length of trajectories observed so far.\nTo generate a trajectory $\\tau$ of length L, t-DGR iterates over each timestep $1 \\leq j \\leq L$\n(Line 9). At each timestep, t-DGR generates the j-th state observation of the trajectory\nusing the previous generator $G_\\gamma^{(t-1)}$ conditioned on timestep $j$ (Line 10), and then labels\nit with an action using the previous policy $\\pi_\\theta^{(t-1)}$ (Line 11). After generating all timesteps\nin the trajectory $\\tau$, t-DGR adds it to the existing training dataset (Line 14). Note that\nthe generated state observations within a trajectory do not have temporal coherence, as\neach state observation is generated independently of other timesteps. This approach is\nacceptable since our learner is trained on state-action pairs rather than full trajectories.\nHowever, unlike generating state observations i.i.d., our method ensures equal coverage of\nevery timestep during the generative process, significantly reducing sample complexity.\nOnce t-DGR has augmented the training samples from the environment with our\ngenerated training samples, t-DGR employs backpropagation to update both the generator\nand learner using the augmented dataset (Lines 16-18). The t-DGR algorithm continues\nthis process of generative replay throughout the agent's lifetime, which can be infinite\n(Line 2). Although we perform the generative process of t-DGR at task boundaries for\nease of understanding, no part of t-DGR is dependent on clear task boundaries.\nArchitecture We employ a U-net [85] trained on the loss specified in Equation 2.6 to\nimplement the generative diffusion model $G_\\gamma$. Since we utilize proprioceptive observations\nin our experiments, $\\pi_\\theta$ is implemented with a multi-layer perceptron trained on the loss\nspecified in Equation 2.1."}, {"title": "2.5 Experiments", "content": "In this section, we outline the experimental setup and performance metrics employed to\ncompare t-DGR with representative methods, followed by an analysis of experimental\nresults across different benchmarks and performance metrics."}, {"title": "2.5.1 Experimental Setup", "content": "We evaluate our method on the Continual World benchmarks CW10 and CW20 [113],\nalong with our own variant of CW10 called BB10 that evaluates the ability of methods to\nhandle blurry task boundaries. CW10 consists of a sequence of 10 Meta-World [116] tasks,\nwhere each task involves a Sawyer arm manipulating one or two objects in the Mujoco\nphysics simulator. For computational efficiency, we provide the agents with proprioceptive\nobservations. Notably, the observation and action spaces are continuous and remain\nconsistent across all tasks. CW20 is an extension of CW10 with the tasks repeated twice.\nTo our knowledge, Continual World is the only standard continual learning benchmark for\ndecision-making tasks. BB10 gives data to the learner in 10 sequential buckets $B_1, ..., B_{10}$.\nData from task $\\mathcal{T}_i$ from CW10 is split evenly between buckets $B_{i-1}, B_i$, and $B_{i+1}$, except\nfor the first and last task. Task $\\mathcal{T}_1$ is evenly split between buckets $B_0$ and $B_1$, and task\n$\\mathcal{T}_{10}$ is evenly split between buckets $B_9$ and $B_{10}$.\nIn order to bound memory usage, task conditioning should utilize fixed-size embeddings\nlike natural language task embeddings, rather than maintaining a separate final neural\nnetwork layer for each individual task. For simplicity, we condition the model on the task\nwith a one-hot vector as a proxy for natural language prompt embeddings. For BB10, the\nmodel is still conditioned on the underlying task rather than the bucket. Additionally,\nwe do not allow separate biases for each task, as originally done in EWC [57]. Expert\ndemonstrations for training are acquired by gathering 100 trajectories per task using\nhand-designed policies from Meta-World, with each trajectory limited to a maximum of\n200 steps. Importantly, the learner model remains consistent across different methods\nand benchmark evaluations. Moreover, we maintain a consistent replay ratio of $r = 0.9$\nacross all pseudo-rehearsal methods.\nWe estimated the success rate S of a model by running each task 100 times. The\nmetrics for each method were computed using 5 seeds to create a 90% confidence interval.\nFurther experimental details, such as hyperparameters, model architecture, random seeds,\nand computational resources, are included in Appendix A.3, A.1, A.2. This standardization\nenables a fair and comprehensive comparison of our proposed approach with other existing\nmethods."}, {"title": "2.5.2 Metrics", "content": "We evaluate our models using three metrics proposed by the Continual World benchmark\n[113], with the average success rate being the primary metric. Although the forward\ntransfer and forgetting metrics are not well-defined in a setting with blurry task boundaries,\nthey are informative within the context of Continual World benchmarks. As a reminder\nfrom Section 2.3.2, let $N$ denote the number of tasks, and $s_i$ represent the success rate of\nthe learner on task $\\mathcal{T}_i$. Additionally, let $s_i(t)$ denote the success rate of the learner on\ntask $\\mathcal{T}_i$ after training on tasks $\\mathcal{T}_1$ to $\\mathcal{T}_t$.\nAverage Success Rate The average success rate, as given by Equation 2.2, serves as\nthe primary evaluation metric for continual learning methods."}, {"title": "Average Forward Transfer", "content": "We introduce a slightly modified metric for forward\ntransfer that applies to a broader range of continual learning problems beyond just\ncontinual reinforcement learning in the Continual World benchmark. Let $s_i^{ref}$ represent\nthe reference performance of a single-task experiment on task $\\mathcal{T}_i$. The forward transfer\nmetric $FT_i$ is computed as follows:\n$FT_i = \\frac{D_i - D_i^{ref}}{1 - e^{(-D_i^{ref})}}, D_i = \\frac{s_i(i) + s_i(i - 1)}{2}, D_i^{ref} = \\frac{s_i^{ref}}{2}$\nThe expressions for $D_i$ and $D_i^{ref}$ serve as approximations of the integral of task $\\mathcal{T}_i$\nperformance with respect to the training duration for task $\\mathcal{T}_i$. The average forward\ntransfer $FT$ is then defined as the mean forward transfer over all tasks, calculated as\n$FT = \\frac{1}{N} \\sum_i^N FT_i$.\nAverage Forgetting We measure forgetting using the metric $F_i$, which represents\nthe amount of forgetting for task i after all training has concluded. $F_i$ is defined as the\ndifference between the success rate on task $\\mathcal{T}_i$ immediately after training and the success\nrate on task $\\mathcal{T}_i$ at the end of training.\n$F_i = s_i(i) - s_i(N)$\nThe average forgetting $F$ is then computed as the mean forgetting over all tasks, given by\n$F = \\frac{1}{N} \\sum_i^N F_i$"}, {"title": "2.5.3 Baselines", "content": "We compare the following methods on the Continual World benchmark using average\nsuccess rate as the primary evaluation metric. Representative methods were chosen\nbased on their success in the original Continual World experiments, while DGR-based\nmethods were selected to evaluate whether t-DGR addresses the limitations of existing\npseudo-rehearsal methods.\n\u2022 Finetune: The policy is trained only on data from the current task.\n\u2022 Multitask: The policy is trained on data from all tasks simultaneously.\n\u2022 OEWC [92]: A variation of EWC known as online Elastic Weight Consolidation\n(OEWC) bounds the memory of EWC by employing a single penalty term for the\nprevious model instead of individual penalty terms for each task. This baseline is\nthe representative regularization-based method.\n\u2022 PackNet [67]: This baseline is the representative parameter isolation method.\nPacknet safeguards previous task information in a neural network by iteratively\npruning, freezing, and retraining parts of the network.\n\u2022 DGR [93]: This baseline is a deep generative replay method that only generates\nindividual state observations i.i.d. and not entire trajectories.\n\u2022 CRIL [32]: This baseline is a deep generative replay method that trains a policy\nalong with a start state generator and a dynamics model that predicts the next\nstate given the current state and action. Trajectories are generated by using the\ndynamics model and policy to autoregressively generate next states from a start\nstate.\n\u2022 t-DGR: Our proposed method.\nDue to the inability of oEWC and PackNet to handle blurry task boundaries, we made\nseveral adjustments for CW20 and BB10. Since PackNet cannot continue training\nparameters for a task once they have been fixed, we treated the second repetition of tasks\nin CW20 as distinct from the first iteration, resulting in PackNet being evaluated with\nN = 20, while the other methods were evaluated with N = 10. As for BB10 and its\nblurry task boundaries, the best approach we could adopt with oEWC and PackNet was\nto apply their regularization techniques at regular training intervals rather than strictly\nat task boundaries. During evaluation, all tasks were assessed using the last fixed set of\nparameters in the case of PackNet."}, {"title": "2.5.4 Ablations", "content": "To evaluate the effect of generator architecture on pseudo-rehearsal methods, we included\nan ablation where the diffusion models in pseudo-rehearsal methods are replaced with\nWasserstein generative adversarial networks (GAN) with gradient penalty [37]. To evaluate\nthe effectiveness of diffusion models at generative replay, we evaluated the quality of\ngenerated samples for past tasks at all stages of continual learning. Since we utilize\nproprioceptive observations rather than RGB camera observations, there is no practical\nway to qualitatively evaluate the generated samples. The proprioceptive observations\nare 39-dimensional, comprising 3D positions and quaternions of objects, and the degree\nto which the robot gripper is open [116]. These raw numbers cannot be qualitatively\nevaluated like RGB images. Instead, we use the average diffusion loss as a quantitative\nproxy metric for generation quality. When the generator model is learning task i, we\ncompute the average diffusion loss for data samples from tasks 1 to i \u2212 1. We then\ncompare this average diffusion loss to the average diffusion loss when the generator model\nfirst learned task i. The difference between the two provides an estimate of how much\ngenerative replay has degraded the generation quality of previous tasks."}, {"title": "2.5.5 Discussion", "content": "t-DGR emerges as the leading method, demonstrating the highest success rate on CW10\n(Table 2.1a), CW20 (Table 2.1c), and BB10 (Table 2.1b). Notably, PackNet's performance\non the second iteration of tasks in CW20 diminishes, highlighting its limited capacity for\ncontinually accommodating new tasks. This limitation underscores the fact that PackNet\nfalls short of being a true lifelong learner, as it necessitates prior knowledge of the task\ncount for appropriate parameter capacity allocation. On the contrary, pseudo-rehearsal\nmethods, such as t-DGR, exhibit improved performance with the second iteration of tasks\nin CW20 due to an increased replay time. These findings emphasize the ability of DGR\nmethods to effectively leverage past knowledge, as evidenced by their superior forward\ntransfer in both CW10 and CW20.\nBB10 (Table 2.1b) demonstrates that pseudo-rehearsal methods are mostly unaffected\nby blurry task boundaries, whereas PackNet's success rate experiences a significant drop-\noff. This discrepancy arises from the fact that PackNet's regularization technique does\nnot work effectively with less clearly defined task boundaries.\nIn our experiments across CW10, CW20, and BB10, we observed that diffusion models\noutperform GANs as the generator for pseudo-rehearsal methods. We hypothesize that the\ndistributional shifts in continual learning exacerbate instability issues with GAN training\n[88, 14, 72", "32": "is to alleviate the\nburden of trajectory generation from the generator by transferring some of the generation\ncomplexity to a dynamics model. Our findings support this motivation when working\nwith less capable generators [27", "27": "reveal a reversal in performance ranking among these methods,\nsuggesting that more capable generators diminish the need to offload generation complexity\nfrom the main generator.\nThe diminishing performance gap between DGR and t-DGR as the replay ratio\nincreases in Table 2.1d indicates that a higher replay ratio reduces the likelihood of any\nportion of the trajectory being insufficiently covered when sampling individual state\nobservations i.i.d., thereby contributing to improved performance. This trend supports the\ntheoretical sample complexity of DGR derived in Section 2.2.2, as $\\mathcal{O}(n \\log n + mn \\log \\log n)$\nclosely approximates the sample complexity of t-DGR, $\\Theta(mn)$, when the replay amount\n$m \\rightarrow \\infty$. However, while DGR can achieve comparable performance to t-DGR with a high\nreplay ratio, the availability of extensive replay time is often limited in many real-world\napplications.\nRecent studies have indicated that applying generative replay to diffusion models\nfor image data leads to a collapse of generation quality due to compounding errors in\nthe generated synthetic data [119, 68, 94"}]}