{"title": "Visual Verity in Al-Generated Imagery: Computational Metrics and Human-Centric Analysis", "authors": ["MEMOONA AZIZ", "UMAIR REHMAN", "SYED ALI SAFI", "AMIR ZAIB ABBASI"], "abstract": "The rapid advancements in Al technologies have revolutionized the production of graphical content across various sectors, including entertainment, advertising, and e-commerce. These developments have spurred the need for robust evaluation methods to assess the quality and realism of AI-generated images. To address this growth, we conducted three studies. First, we introduced and validated a questionnaire namely Visual Verity, measuring photorealism, image quality, and text-image alignment. Second, we applied this questionnaire to assess images from AI models (DALL-E2, DALL-E3, GLIDE, Stable Diffusion) and camera-generated images, revealing that camera-generated images excelled in photorealism and text-image alignment, while AI models led in image quality. We also analyzed statistical properties, finding camera-generated images scored lower in hue, saturation, and brightness. Third, we evaluated Computational metrics' alignment with human judgments, identifying MS-SSIM and CLIP as the most consistent with human assessments. Additionally, we proposed the Neural Feature Similarity Score (NFSS) for assessing image quality. Our findings highlight the need for refining Computational metrics to better capture human visual perception, enhancing Al-generated content evaluation.", "sections": [{"title": "1 INTRODUCTION", "content": "The impressive scale and rapid evolution of AI-generated images (AGIs) have transformed the digital visual landscape, impacting industries such as entertainment, advertising, and e-commerce. In 2023 alone, the generation of AI-created images surpassed 15 billion, a volume that took traditional photography 150 years to accumulate. The daily production rate has reached approximately 34 million images, with platforms like Adobe Firefly contributing significantly by boasting one billion images generated by users in just three months since its launch [Everypixel Journal 2024]. The growing reliance on Al for image generation not only emphasizes the technological strides in the field but also highlights the critical need for reliable evaluation metrics that align with human perceptual standards. The image-generating segment of the generative AI market, valued at approximately 299.2 million USD in 2023, is expected to grow to 917.4 million USD by 2030, reflecting a compound annual growth rate of 17.4% [Tech Report 2024]. This rapid market expansion and the vast production of AI images underscore the importance of developing robust methods to assess the quality and photorealism of AI-generated content effectively [Caramiaux and Fdili Alaoui 2022; Talebi and Milanfar 2018b].\nGiven this dramatic increase in AI-generated content, there is a need to evaluate these images across key dimensions: photorealism, image quality, and text-image alignment. Photorealism ensures that images convincingly mimic real-world scenes, which is vital for applications such as virtual reality, gaming, and digital marketing. A survey revealed that 85% of users find photorealistic images more engaging, leading to increased interaction and satisfaction [Li et al. 2019; Shi et al. 2023]. Equally important is image quality, which includes attributes like sharpness, color fidelity, and the absence of artifacts. High image quality significantly influences user engagement and behavior in areas such as e-commerce and social media. Recent studies show that higher quality images see a 25% increase in user engagement and a 20% higher likelihood of purchase in online shopping scenarios [Russakovsky et al. 2014]. Text-image alignment is also important in contexts where images are paired with captions, such as educational materials, e-commerce, and social media. Effective alignment ensures that visual content complements and enhances textual information, improving comprehension and retention. A study indicates that properly aligned text and images can enhance user comprehension by up to 40% and increase user retention by 25% [Hinz et al. 2020].\nIn measuring these key dimensions using computational image metrics, previous research has categorized these metrics into pixel-based and model-based metrics. Pixel-based metrics, such as the Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) [Hore and Ziou 2010], have traditionally been used to assess the structural similarity and noise variations between AI-generated images and reference images [Talebi and Milanfar 2018a;"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "We begin by providing background and related works on AI generative image models, computational image metrics, subjective evaluation of human study and scaling strategies."}, {"title": "2.1 Al Generative Image Models", "content": "The field of AI-generated images has seen significant advancements, evolving from simple neural networks to sophisticated models leveraging transformers and diffusion techniques. This progress has been marked by the development of notable models like DALL-E [Ramesh et al. 2021], DALL-E 2 [Ramesh et al. 2022], DALL-E 3 [Betker et al. 2023], GLIDE [Nichol et al. 2021], and Stable Diffusion [Rombach et al. 2022], each contributing uniquely to the landscape of AI image generation.\nThe OpenAI's introduced its commercial DALL-E model in January 2021, which marked a significant milestone in Al image generation. They are utilizing transformer architectures, DALL-E demonstrated the capability to generate novel images from textual descriptions by blending various concepts and styles creatively. Despite its innovative approach, DALL-E's outputs were often limited in resolution and detail, reflecting the early stages of integrating language and image generation technologies [Ramesh et al. 2021].\nGLIDE is another notable model developed by Nichol et al. [Nichol et al. 2021], which extends the principles of diffusion models by conditioning the image generation process on textual information. It is known for its ability to upscale images while maintaining high photorealism and caption accuracy, GLIDE has been considered a robust alternative to GANs and early transformer-based models. Its integration in models like DALL-E 2 highlights its importance in enhancing text-conditional image generation.\nIn April 2022, OpenAI released DALL-E 2. It is bringing substantial advancements over its predecessor DALL-E by incorporating a modified GLIDE model and CLIP embeddings. These enhancements allowed DALL-E 2 to generate higher resolution images, four times greater than DALL-E, with improved realism and accuracy. The model excelled in tasks like outpainting, inpainting, and creating variations from prompts, representing a substantial leap in generating photorealistic and contextually coherent images [Ramesh et al. 2022]. According to Ramesh et al., [Ramesh et al. 2022] the integration of CLIP embeddings significantly enhanced the model's ability to understand and accurately depict complex scenes.\nThe DALL-E 3 is the latest iteration introduced by OpenAI in September 2023. The DALL-E 3 further refined the integration of text and image generation. By leveraging more advanced techniques, DALL-E 3 aimed to improve the fidelity of generated images, especially in complex scenes with multiple objects. It addressed previous models' limitations by enhancing consistency across different parts of an image and aligning more closely with human language understanding. The improvements made DALL-E 3's text-to-image generation process more intuitive and effective, further pushing the boundaries of what these models can achieve [Betker et al. 2023].\nStable Diffusion is another advanced image generation model, which was developed by Rombach et al., [Rombach et al. 2022] at Stability AI. It has become notable for its high-resolution image generation capabilities. This model operates by iteratively refining Gaussian noise into detailed images, which makes it particularly effective for generating high-quality visuals. The open-source nature of Stable Diffusion has led to widespread adoption and application in various fields, including digital art and media. Scientific reviews have praised its flexibility and high-quality outputs, positioning it as a leading tool in the domain of AI-generated imagery [Rombach et al. 2022]. Assessing the quality, photorealism, and text-image alignment of images generated by these models is critical for various applications in fields such as digital marketing, virtual reality, entertainment, and education. Quality ensures that the images meet high standards of detail and clarity, which is essential for applications like e-commerce, where clear and detailed images can significantly influence purchasing decisions. For instance, higher-quality images have been shown to increase user engagement and conversion rates in online shopping platforms, as consumers and consumers are more likely to trust and purchase products that are visually appealing and accurately represented [Russakovsky et al. 2014]."}, {"title": "2.2 Computational Image Metrics", "content": "In measuring image quality, photorealism, text-image alignment using computational image metrics, previous research has categorized these metrics into pixel-based and model-based metrics. Pixel-based metrics, such as the Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) [Hore and Ziou 2010], have traditionally been used to assess the structural similarity and noise variations between AI-generated images and reference images [Talebi and Milanfar 2018a; Wang et al. 2023]. However, the availability of a reference image is not always guaranteed. Although some datasets include similar camera-captured images that can be used as references [Lin et al. 2014b]. These metrics also require the images being compared to have the same structural composition, which can be challenging for Al-generated images with varying structures, object positions, and lighting conditions. As a result, when the outcomes of these metrics are compared to human evaluations, there is often a large discrepancy.\nPixel-based metrics directly take images as input and compute similarity scores. The Peak Signal-to-Noise Ratio (PSNR) is defined as:\n```latex\nPSNR = 10 log10\\left(\\frac{MAX^2}{MSE}\\right)\n```\nwhere MAX is the maximum possible pixel value of the image and MSE is the Mean Squared Error between the reference and the generated image. PSNR is effective for measuring the overall noise level but is less sensitive to perceptual differences [Hore and Ziou 2010].\nThe Structural Similarity Index Measure (SSIM) improves upon PSNR by considering perceptual factors. SSIM is calculated as:\n```latex\nSSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1) (2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1) (\\sigma_x^2 + \\sigma_y^2 + C_2)}\n```\nwhere \u00b5x and \u00b5y are the mean intensities, \u03c3x and \u03c3y are the variances, and \u03c3xy is the covariance of the images x and y. Constants"}, {"title": "2.3 Subjective Evaluations of Generative Images", "content": "Human visual perception plays a important role in image comprehension, particularly in evaluating image quality and realism. Li et al. [Li et al. 2023] introduced a comprehensive questionnaire designed to assess the general quality and fidelity of AI-generated images. The study focused on gathering human feedback on various aspects of image quality, such as sharpness, color accuracy, and overall aesthetic appeal. This questionnaire aimed to standardize the subjective evaluation process, making it easier to compare human assessments with computational metrics. The findings from their study underscored the importance of human input in validating the perceived quality of generative images, highlighting discrepancies between human judgments and algorithmic assessments [Li et al. 2023].\nLu et al. [Lu et al. 2024] advanced the field by proposing HPBench, a benchmarking framework specifically designed for the subjective evaluation of photorealism in AI-generated images. Their research provided a structured approach to quantify photorealism, involving participants in a series of controlled experiments where they rated the realism of images generated by various Al models. The HPBench framework emphasized the need for high-fidelity images in applications like virtual reality and film production, where visual realism is paramount. By establishing benchmarks for photorealism, this study contributed significantly to the development of more sophisticated and realistic generative models [Lu et al. 2024].\nRagot et al. [Ragot et al. 2020] conducted a pivotal study that explored the alignment of painting captions with their corresponding visuals. Their research aimed to understand how well Al-generated captions matched the content and context of the images they described. By involving human participants in evaluating the accuracy and relevance of these captions, the study provided valuable insights into the capabilities and limitations of current text-to-image generation models. This work highlighted the importance of semantic coherence in generative models, especially in applications like digital storytelling and educational tools where accurate descriptions are important [Ragot et al. 2020].\nQiao et al. [Qiao and Eglin 2011] study realistic behavior in computer-generated humans images. The research particularly focuses on the effects of facial behavior display, including facial expressions, head movements, and eye movements, on audience perception. Using a CG animated human head, the study examines how these behaviors influence believability, eeriness, and accurate behavior recognition among participants. The findings indicate that facial movements impact the perceived eeriness of CG humans, while the combination of facial expressions and head movements enhances believability. Additionally, the accuracy of behavior recognition improves with the inclusion of head movements. This study underscores the importance of detailed and realistic facial behaviors in achieving lifelike and believable CG human characters, which is important for applications in virtual reality, gaming, and animation [Qiao and Eglin 2011].\nZhou et al. [Zhou and Kawabata 2023] conducted study that explored into the subjective realms of beauty, liking, valence, and arousal by offering insights into emotional responses to AI-generated images. Their study utilized a diverse set of images and involved participants in rating their emotional reactions to each image. The findings revealed the complex interplay between visual aesthetics and emotional responses, providing a deeper understanding of how generative models can be tuned to evoke specific emotions. This research is particularly relevant for applications in advertising, social media, and entertainment, where emotional engagement is a key factor [Zhou and Kawabata 2023].\nTreder et al. [Treder et al. 2022] focused on the subjective evaluation of image quality through a human-centered study. Their research prioritized human ratings to gauge various dimensions of image quality, including sharpness, color fidelity, and the presence of artifacts. By comparing human assessments with computational metrics, the study highlighted the discrepancies that often arise between human perception and algorithmic evaluations. This work underscored the need for more reliable and perceptually aligned metrics to accurately assess the quality of AI-generated images, which is critical for applications in fields such as digital art and photography [Treder et al. 2022]."}, {"title": "2.4 Scaling Strategies", "content": "Scaling strategies play a critical role in image quality assessment by scaling metric scores with human perceptual judgments for comparison. Traditional methods such as Z-standardization and Min-Max scaling can introduce biases, often disproportionately emphasizing certain scores over others, such as giving undue weight to Peak Signal-to-Noise Ratio (PSNR) at the expense of Structural Similarity Index Measure (SSIM). To mitigate these biases, advanced non-linear methods have been developed to align metric scores more closely with mean opinion scores (MOS) from human evaluators, providing a more accurate evaluation of image quality [Rehman et al. 2015].\nAdvanced models, such as the Non-linear Receptive Field model dynamically optimizing parameters to enhance correlation with human ratings, tailored to specific image datasets [Luna et al. 2023]. This model adjusts its parameters based on the visual complexity of the dataset, thus providing a more precise mapping between computed metric values and human perceptual scores. The Non-linear Receptive Field model has demonstrated improved performance in aligning with human judgments by particularly in datasets featuring complex visual features.\nMoreover, the Saliency-Guided Local Full-Reference Image Quality Assessment leverages visual saliency to weight local image quality scores, prioritizing regions likely to capture human attention. This approach is based on the premise that not all image regions contribute equally to perceived quality; therefore, it focuses on areas that are more salient to the human visual system [Varga 2022]. By integrating saliency maps into the assessment process, this method aims to provide a more accurate reflection of human visual perception, improving the correlation with subjective quality ratings.\nAnother significant scaling strategy is the use of Multi-Task Learning (MTL) frameworks, which can predict human perceptual scores by training on multiple related tasks simultaneously. MTL frameworks help leverage shared representations across different tasks, thereby improving the generalization and robustness of the quality assessment models [Huang et al. 2022]. These frameworks have been particularly effective in handling diverse and heterogeneous datasets, offering a scalable solution to image quality assessment.\nDeep learning-based models for scaling, such as those incorporating convolutional neural networks (CNNs) and transformer architectures, have gained prominence for their ability to learn feature representations from large datasets and align them with human perceptual scores through end-to-end training [Zhang et al. 2018]. By utilizing large-scale annotated datasets, these models can capture complex patterns and nuances in visual data, making them highly effective for quality assessment.\nDespite the advancements brought by these methods, their complexity and black-box nature often render them less transparent and harder to trust in practical applications. This underscores a critical gap in the field: the need for an interpretable and fair scaling strategy that practitioners can readily trust. The development of transparent scaling methods addresses this gap by providing robust, interpretable, and fair solutions for scaling image quality metrics, thereby enhancing their practical applicability and acceptance.\nIn response to these challenges, we already proposed the Interpolative Binning Scale (IBS) [Aziz et al. 2024], which is an innovative scaling strategy designed to improve the interpretability and fairness of quality assessments. Unlike traditional methods, IBS employs a binning approach to segment metric scores into discrete intervals, which are then interpolated to provide a continuous and smooth scaling. This method ensures that the scaling is not biased by extreme values and provides a more balanced representation of the metric scores across the entire range. By aligning the interpolated scores with human perceptual data, IBS offers a transparent and easily interpretable scaling method that can be trusted by practitioners. In this paper, we present an expert evaluation of IBS to demonstrate its effectiveness in aligning computational metric scores with human visual perception [Aziz et al. 2024]."}, {"title": "3 STUDY 1: DEVELOPMENT AND VALIDATION OF A QUESTIONNAIRE ON PHOTOREALISM, IMAGE QUALITY, AND TEXT-IMAGE ALIGNMENT", "content": "This section provides detailed information on the development and statistical validation of a questionnaire designed to evaluate photorealism, image quality, and the alignment of text with images. The proposed questionnaire, named Visual Verity, is structured into four components: Demography, Photorealism, Image Quality, and Text-Image Alignment. Each component is designed with a specific rationale to ensure comprehensive evaluation and accurate data collection.\nVisual Verity received ethics approval from the Non-Medical Research Ethics Board (NMREB) at the University of Western Ontario (UWO), Canada, under project ID: 124753. The official approval is publicly accessible at: https://github.com/udanish50/VisualVerity/."}, {"title": "3.1 Visual Verity: Demography Questionnaire", "content": "The Demography Questionnaire is designed to collect essential demographic data from participants, which is important for understanding the diversity within our sample. We collect limited demographic information to ensure participant privacy, as our questionnaire is partially anonymous."}, {"title": "3.2 Visual Verity: Photorealism Questionnaire", "content": "The Photorealism Questionnaire is designed to evaluate the perceived realism of AI-generated images. It will help to understand how photorealism is important for applications ranging from virtual reality to digital art, where the believability of an image can significantly impact user experience. This section provides the rationale behind each question included in the questionnaire. The Likert scale with the following options: strongly agree, somewhat agree, neutral, somewhat disagree, and strongly disagree is used in all questions.\nThe questionnaire begins with PQ1R, which asks if the image looks like a photograph of a real scene. This question is fundamental as it directly probes the participant's immediate perception of realism, which is a primary indicator of photorealism in AI-generated images. The second question PQ2R follows by asking whether participants can easily imagine seeing the image in the real world. This question addresses the contextual plausibility of the image, an important factor in photorealism [Shadiev et al. 2020].\nThe third question, PQ3R, focuses on the visual details that contribute to the image's realism. High levels of detail, such as sharpness and clarity, are key components of photorealistic images. The detailed images are more likely to be perceived as realistic because they mimic the high-resolution perception of human vision. PQ4R asks about the natural appearance of textures in the image. Texture realism is a critical aspect of photorealism, as realistic textures can significantly enhance the believability of an image. The accurate texture representation is important for the realistic depiction of surfaces and materials in synthetic images.\nFinally, PQ5R evaluates the contribution of lighting and shadows to the image's realism. Proper lighting and shadowing are essential for creating depth and dimensionality, which are pivotal for photorealism. The lighting effects can dramatically influence the perception of realism in digital images. These questions collectively aim to provide a comprehensive assessment of the photorealism of Al-generated images, allowing for nuanced insights into how different aspects of an image contribute to its overall realism."}, {"title": "3.3 Visual Verity: Image Quality Questionnaire", "content": "The Image Quality Questionnaire is designed to evaluate the perceived quality of AI-generated images. Assessing image quality is important for numerous applications, including digital media, virtual reality, and e-commerce, where clarity, color accuracy, and the absence of distortions significantly impact user experience and satisfaction. This section details the rationale behind each question included in the questionnaire. The Likert scale with the following options: strongly agree, somewhat agree, neutral, somewhat disagree, and strongly disagree, is utilized in all questions.\nThe questionnaire begins with IQ1G, which asks if the image is clear and sharp. Image clarity and sharpness are fundamental aspects of quality, directly influencing how detailed and lifelike an image appears. The second question IQ2G is asking about the vibrancy and lifelikeness of colors in the image. The color accuracy and vibrancy are critical for creating visually appealing images that accurately represent the intended scene or subject. The third question, IQ3G, seeks participants' overall satisfaction with the image quality. This question provides a holistic measure of perceived image quality by capturing participants' general impressions and satisfaction levels. IQ4G asks about the presence of visible artifacts or distortions in the image. Artifacts and distortions can significantly degrade the perceived quality of an image, affecting its realism and usability. Finally, IQ5G evaluates whether the resolution of the image meets participants' expectations. These questions collectively aim to provide a comprehensive assessment of the image quality of AI-generated images."}, {"title": "3.4 Visual Verity: Text-Image Alignment Questionnaire", "content": "The Text-Image Alignment Questionnaire is designed to evaluate the degree to which Al-generated images align with their corresponding textual descriptions. It has applications in automated content creation, digital marketing, and educational tools, where the coherence between text and visuals significantly impacts user engagement and comprehension. Each question in the questionnaire is designed to assess different facets of alignment by ensuring a comprehensive evaluation of this important aspect. The Likert scale with the following options: strongly agree, somewhat agree, neutral, somewhat disagree, and strongly disagree is used in all questions.\nThe questionnaire begins with CQ1M, which asks whether the image perfectly aligns with the given caption. This question addresses the overall coherence between the text and the visual content. Perfect alignment is essential for ensuring that the generated image accurately represents the described scene, which is particularly important in fields like advertising and educational content. CQ2M follows, asking if the elements in the image correspond to the described scene in the caption. This question focuses on the specific components and details within the image, ensuring that all elements mentioned in the caption are accurately depicted. The third question, CQ3M, seeks to understand if participants would describe the image with a caption that closely matches the provided one. This question evaluates the naturalness and intuitiveness of the text-image pair, ensuring that the generated captions feel authentic and accurate to human observers.\nCQ4M asks if the image misses some details mentioned in the caption. This question is critical for identifying gaps or omissions in the generated image, which can detract from the perceived accuracy and completeness of the representation. Ensuring that all details are correctly depicted is important for maintaining the credibility and reliability of AI-generated content. The last question CQ5M, evaluates whether the image is perceived as a true representation of the given caption. This question provides a holistic measure of the text-image alignment, capturing the overall impression of accuracy and coherence. A strong alignment between text and image is vital for applications where users rely on visual content to understand or complement textual information.\nThese questions collectively aim to provide a thorough assessment of the alignment between text and AI-generated images, enabling a nuanced analysis of how well these images meet the expectations set by their descriptions."}, {"title": "3.5 Questionnaire Validation", "content": "The subsection will provide details on scale refinement and validation by exploratory factor analysis, confirmatory factor analysis using partial least squares and structural equation modelling.\n3.5.1 Scale Refinement and Validation by Exploratory Factor Analysis on, N=200). We carried out an Exploratory Factor Analysis (EFA) on the first group of 200 participants. This method helped us uncover the basic factor pattern of the questions in our survey and to check that each question accurately matched the specific category it was meant to measure. We found that the data was a good fit for this kind of analysis because the Kaiser-Meyer-Olkin Measure of Sampling Adequacy was 0.906. This high number means our data was suitable for the analysis. Additionally, Bartlett's Test of Sphericity gave us a Chi-Square value of around 2141, which confirmed that our questions were interrelated enough to proceed with a valid factor analysis.\nThe results of the EFA clearly showed three separate categories, which we had intended to measure: Photorealism, Image Quality, and text-image alignment. The analysis told us how many underlying factors there were, how strongly each survey question was connected to its factor, and how much of the overall variation was explained by these factors. This confirmed that our questionnaire was well-constructed and could effectively measure the different aspects we were interested in, as shown in the referenced table 6.\n3.5.2 Confirmatory Factor Analysis (CFA) using Partial Least Squares (PLS) Structural Equation Modeling (SEM) (N=300). WarpPLS 8.0 was employed to perform scale validation for the constructs of Photorealism, Image Quality, and Text-Image Alignment using the study's two Paper (i.e., 300). CFA was conducted to confirm the factor structure identified in the EFA by providing a more rigorous validation of the survey's construct validity. This approach is suited for the assessment of the measurement model (i.e., based on reflective models comprising Photorealism, Image Quality, and Text-Image Alignment) as it can estimate the scale validation without developing the structural model [Abdul-Latif and Abdul-Talib 2017].\nSince the study's scale is based on reflective constructs, the estimation of reflective constructs includes the assessment of outer loadings, convergent validity (AVE), and reliabilities. Hair et al. [Hair and Alamer 2022] suggested that the outer loading should be 0.60 or greater, convergent validity (i.e., Average Variance Extracted AVE) ought to be 0.50 or above, and reliabilities should exceed the value 0.70. Table 2 shows that each item's loading on its respective factor was critically evaluated. Item loadings for each scale, ranging from 0.833 to 0.944, which indicate strong individual contributions to their respective constructs. Cronbach's Alpha for Photorealism, Image Quality, and Text-Image Alignment were 0.931, 0.952, and 0.94, respectively, suggesting high internal consistency. Composite Reliability ranged from 0.948 to 0.957, and AVE values from 0.784 to 0.847, affirming the convergent validity of the scales, as shown in Table: 6.\nThis step was important to ensure that each of the three scales measured distinct constructs and was not overly inter-correlated. Discriminant validity was assessed using the HTMT ratio [Henseler et al. 2015]. This method involved comparing the correlations between constructs against the correlations within constructs. The"}, {"title": "3.6 Discussion and Implications", "content": "The process of developing and validating the Visual Verity questionnaire highlights several key considerations and implications for the evaluation of AI-generated images. The demography ensures that participant diversity is well-documented by providing critical insights into how demographic factors like gender, age, educational background, and cultural context impact the evaluation of Al-generated images. This demographic profiling is essential for identifying biases and variations in perception, which can inform more equitable and inclusive AI development practices.\nThe photorealism component of the questionnaire addresses the immediate perceptual aspects of AI-generated images, such as their resemblance to real-world scenes and the natural appearance of textures and lighting. By focusing on these elements, the questionnaire ensures that the generated images meet high standards of realism, which is crucial for applications in virtual reality, digital art, and gaming. In assessing image quality, the questionnaire evaluates clarity, color vibrancy, and the absence of distortions. High image quality is critical for user satisfaction in digital media, e-commerce, and other visual-centric applications. By asking participants to rate these aspects, the questionnaire helps identify areas where AI-generated images may fall short. The text-image alignment section focuses on the coherence between textual descriptions and generated images, which is vital for content creation, marketing, and educational tools. Ensuring that images accurately reflect their descriptions enhances the utility and effectiveness of AI systems in these domains. The detailed questions about alignment help pinpoint discrepancies between text and image.\nThe validation processes, including Exploratory Factor Analysis (EFA) and Confirmatory Factor Analysis (CFA), confirm that the Visual Verity questionnaire is a robust tool for evaluating AI-generated images. These statistical validations ensure that each section of the questionnaire accurately measures its intended constructs by providing reliable data for analysis. The rigorous validation process enhances the credibility and applicability of the questionnaire in diverse research and development settings.\nThe findings from the validation and application of the Visual Verity questionnaire have several implications. First, they highlight the need for comprehensive evaluation tools that consider multiple dimensions of image perception. As result of validation we excluded one item from image quality and text-image alignment. Second, they underscore the importance of demographic diversity in understanding how different groups perceive AI-generated images. They point to the potential for using detailed feedback to improve the realism, quality, and contextual relevance of AI-generated images, thereby enhancing their applicability across various domains."}, {"title": "4 STUDY 2: BENCHMARKING HUMAN PERCEPTION FOR PHOTOREALISM, IMAGE QUALITY, AND TEXT-IMAGE ALIGNMENT AGAINST CAMERA-GENERATED AND AI-GENERATED IMAGES", "content": "This section outlines the details about datasets, preprocessing and presentation of images, analytical and statistical analysis, and statistical validation of each category and associated discussion."}, {"title": "4.1 Materials", "content": "Our study utilized the MS COCO (Microsoft Common Objects in Context) dataset, a foundational resource and large-scale dataset consisting of 328K images [Lin et al. 2014b]. The MS COCO dataset is renowned for its extensive collection of images, each annotated with distinct object categories and accompanied by human-written captions. These captions provide a diverse array of descriptions, capturing the essence and context of the scenes depicted. To ensure a manageable and focused study, we selected a subset of 20 images from the MS COCO dataset. This selection process was driven by practical considerations, as conducting a survey on images with human participants typically requires 20 to 30 minutes per participant. By choosing a diverse range of categories within this subset, we ensured that our study covered a broad spectrum of scenes and contexts. Each image in this subset was paired with its corresponding human-written caption.\nWe generated corresponding images using four distinct AI generative models: DALL-E2, DALL-E3, Stable Diffusion, and Glide. Each model received the same human-written caption associated with the selected images by allowing us to compare the performance of different AI technologies and understand the architectural differences among these models [Aziz et al. 2024]. The image generation process was carried out using the NVIDIA Corporation GA102GL [RTX A6000] (rev a1).\nThe architecture of each model utilizes different technologies. DALL-E2 and DALL-E3 leverage a combination of transformer architectures, with DALL-E2 incorporating CLIP embeddings to enhance its ability to generate high-resolution, photorealistic images with improved contextual understanding. Stable Diffusion operates by iteratively refining Gaussian noise into detailed images, which is notable for its high-resolution output and flexibility. GLIDE uses the principles of diffusion models by conditioning the image generation process on textual information and is known for upscale images while maintaining high photorealism. Due to the industry-oriented nature of both DALL E and Stable Difusion, only high-level architectural detail is available, which allows us to conduct on a diverse range of models. By employing these diverse AI generative models and utilizing state-of-the-art hardware, our study aims to benchmark human perception of photorealism, image quality, and text-image alignment.\nTo ensure a fair comparison, we reshaped all images to a uniform size of 299x299 pixels. This resizing was accomplished using linear interpolation [Lin et al. 2014a]. Linear interpolation is a technique known for preserving the original image quality while adjusting dimensions [Aziz et al. 2024]. This reshaping of images is important to ensure that all images, irrespective of their source or original dimensions were presented for evaluation in a fair environment. This is also important and required by computational image metrics assessments because these metrics require images should be of the same shape."}, {"title": "4.2 Recruitment of Study Participants", "content": "We recruited 350 study participants from Prolific [Prolific 2024], an online platform recognized for its diverse and highly relevant participant pool. This platform ensured that our study engaged individuals capable of providing valuable insights on the images evaluated. To deliver our survey, we utilized the Qualtrics platform, known for its robust and user-friendly interface [Qualtrics 2024].\nTo ensure thoughtful completion of the survey, we compensated participants at a rate of $10.28 CAD per hour through the Prolific platform. In accordance with guidelines provided by the Non-Medical Research Ethics Board (NMREB), participants were compensated even if they chose to leave the study before completion, emphasizing our commitment to ethical standards.\nParticipants were presented with camera and AI-generated images accompanied by a caption, with the images displayed in a randomized order to prevent any order bias. The origin of the image was not disclosed. This was important to ensure unbiased assessments based solely on the visual and contextual content of the images, free from any preconceived notions about the AI models involved.\nTo encourage thorough evaluation, participants were required to spend a minimum of 120 seconds assessing each image. Each participant evaluated one camera-generated image and four AI-generated images, ensuring a comprehensive assessment across different types of imagery. According to Prolific's statistics, participants spent an average of 16 minutes and 5 seconds completing the survey, which indicates substantial engagement with the task. For the image assessments, we employed a Likert scale ranging from 'Strongly Disagree' to 'Strongly Agree', allowing us to capture nuanced evaluations of the images' quality, realism, and text-image alignment."}, {"title": "4.3 Objective Properties of Images", "content": "We analyzed the objective properties of camera-generated and AI-generated images to identify significant differences in their key characteristics (see Table: 9). This analysis involved calculating metrics such as hue, saturation, brightness, color vibrancy, and entropy, which provide a quantitative basis for distinguishing between the Camera and AI-generated images [Zhou and Kawabata 2023"}]}