{"title": "Quantum Powered Credit Risk Assessment: A Novel Approach using hybrid Quantum-Classical Deep Neural Network for Row-Type Dependent Predictive Analysis", "authors": ["Minati Rath", "Hema Date"], "abstract": "The integration of Quantum Deep Learning (QDL) techniques into the landscape of financial risk analysis presents a promising avenue for innovation. This study introduces a framework for credit risk assessment in the banking sector, combining quantum deep learning techniques with adaptive modeling for Row-Type Dependent Predictive Analysis (RTDPA). By leveraging RTDPA, the proposed approach tailors predictive models to different loan categories, aiming to enhance the accuracy and efficiency of credit risk evaluation. While this work explores the potential of integrating quantum methods with classical deep learning for risk assessment, it focuses on the feasibility and performance of this hybrid framework rather than claiming transformative industry-wide impacts. The findings offer insights into how quantum techniques can complement traditional financial analysis, paving the way for further advancements in predictive modeling for credit risk.", "sections": [{"title": "1 Introduction", "content": "In the dynamic landscape of finance, risk assessment is a cornerstone of decision-making, especially in credit lending. While traditional credit risk assessment methods have been somewhat reliable, they often fail to capture the complexities and rapid changes of modern financial markets. This creates a pressing need for advanced tools that improve the speed, accuracy, and efficiency of credit risk analysis [1].\nQuantum computing, a transformative technology, has demonstrated immense potential across various industries, including finance. Its unparalleled computational capabilities open new avenues for enhancing credit risk assessment processes [2]. This paper introduces a novel framework that in-tegrates quantum computing with classical deep neural networks, referred to as 'Quantum-Powered Credit Risk Assessment,' to address these challenges.\"\nConventional credit risk assessment techniques rely on historical data, static models, and pre-defined risk categories, often proving inadequate in adapting to the volatile nature of financial markets and leading to less precise risk predictions. The core objective of this paper is to in-troduce a more flexible and adaptive methodology through the integration of quantum computing and classical deep neural networks. This approach combines the strengths of quantum computing's computational efficiency with the learning capabilities of deep neural networks, providing a robust solution to the problem of traditional risk model limitations.\nTo further enhance the accuracy of credit risk models, this study proposes the use of Row-Type Dependent Predictive Analysis (RTDPA). RTDPA represents a shift in credit risk assessment by recognizing that different loan types exhibit distinct characteristics and risk profiles. By analyzing loan subcategories individually, predictive models can be tailored to the specific attributes of each loan type. This results in more precise risk assessments and a deeper understanding of potential vulnerabilities within a credit portfolio [3].\nThe framework for Quantum-Powered Credit Risk Assessment is presented in this paper, and the key research objectives are as follows:\n1. To develop a hybrid quantum-classical model that enhances the predictive power of credit risk assessments.\n2. To explore how quantum computing can be effectively integrated with classical deep neural networks to improve the analysis of financial data.\n3. To apply the RTDPA technique in credit risk models, improving their adaptability to diverse loan types and facilitating a more granular risk assessment.\nAdditionally, the implementation of RTDPA and its benefits for identifying and managing credit risks are discussed, alongside the integration of quantum computing to enhance the performance of deep neural networks. This research also touches upon the potential challenges associated with quantum models, particularly in optimization processes, and introduces alternative optimization techniques like Beetle Antennae Search (BAS), which may offer further improvements for hybrid quantum-classical models. While BAS is not directly applied in this study, it is recognized as a promising avenue for future research.\nThis research contributes to the intersection of quantum computing, deep learning, and finan-cial risk assessment. By exploring the synergies between these fields, we aim to better understand the potential benefits and challenges of Quantum-Powered Credit Risk Assessment. Ultimately, this approach seeks to offer a more adaptive, precise, and effective framework for credit risk analysis in an increasingly dynamic financial landscape."}, {"title": "2 Literature Review", "content": "Credit risk assessment is a critical aspect of financial decision-making, central to the lending practices of financial institutions. Traditional methods, including logistic regression and decision trees, have long served as the foundation for credit risk assessment models. These models depend heavily on historical data and predefined risk categories for classifying borrowers into specific risk groups. While these traditional methods have been somewhat effective, they struggle to adapt to the dynamic and complex nature of modern financial markets. This limitation often results in inaccuracies in risk predictions [4].\nIn recent years, there has been a notable shift toward exploring more advanced techniques for credit"}, {"title": "3 Deep Learning(DL)", "content": "Deep Learning is a sub field of machine learning specialising in the study of neural networks with trainable parameters weight(W) and bias(b). They are composed of layers of interconnected nodes (neurons). These networks can be deep, meaning they can have multiple hidden layers [Figure 1].\nDL model = $f_{DL}(x,\\theta) = \\sigma(Wx + b), \\theta = (W,b)$,\nwith cost function\nC = $\\sum_{data,x,y} |f(x,\\theta) \u2013 y|^2$,\nUsing Gradient Descent '\u2207' of cost 'C' with respect to parameters '\u03b8' with respect to time 't':\n$\\theta^{(t+1)} = \\theta^{(t)} \u2013 \\eta \\nabla_{\\theta} C$.\nThe parameters are updated iteratively in the direction of the gradient until convergence is achieved. After training, the model can be applied to various tasks, such as classification, re-gression, and image processing.\nDeep learning models are particularly powerful because they can autonomously extract hierar-chical data representations, identifying pertinent features directly from raw data. This makes them ideal for tasks where traditional feature engineering is either complex or infeasible. However, the effectiveness of these models often depends on the availability of large amounts of labeled data, which can be a significant challenge in certain research domains\nThe versatility and adaptability of deep learning algorithms make them applicable across diverse fields. For example, in medical diagnostics, DL models have revolutionized image-based disease de-tection. Similarly, in the financial domain, deep learning has been leveraged for algorithmic trading and fraud detection. Despite these successes, challenges like interpretability, computational cost, and scalability persist. Understanding the architecture of neural networks is essential to designing efficient and accurate models. The structure of a basic feed-forward neural network is illustrated below, highlighting the flow of data, the role of weights, and the application of biases at various layers.\nThe versatility and adaptability of deep learning algorithms make them applicable across diverse fields. For example, in medical diagnostics, DL models have revolutionized image-based disease de-tection. Similarly, in the financial domain, deep learning has been leveraged for algorithmic trading and fraud detection. Despite these successes, challenges like interpretability, computational cost, and scalability persist. Understanding the architecture of neural networks is essential to designing efficient and accurate models. The structure of a basic feed-forward neural network is illustrated"}, {"title": "4 Quantum Computing", "content": "Quantum computing is an advanced computational paradigm that leverages the principles of quan-tum mechanics to process information. Unlike classical bits, which represent data as either 0 or 1, quantum bits (qubits) can exist in superposition, enabling them to represent multiple states simul-taneously. The power of quantum systems arises from the ability to measure complex amplitudes, enabling the manipulation of quantum states in ways that classical systems cannot achieve in a very high dimensional vector space. This property allows quantum computers to perform complex cal-culations at unprecedented speeds, making them particularly well-suited for algorithms involving large datasets or intricate mathematical operations [20]. In the context of hybrid quantum-classical algorithms, such as those involving Row-Type Dependent Predictive Analysis (RTDPA), quantum computing can enhance feature encoding and model training efficiency [21]. The integration of quantum layers can facilitate improved representation of data patterns, leading to better predic-tive performance. Furthermore, quantum entanglement allows qubits to be correlated, providing an additional layer of computational advantage by enabling parallel processing of information. As such, quantum computing has the potential to revolutionize machine learning applications, particularly in areas demanding high accuracy and rapid processing capabilities."}, {"title": "4.1 Quantum Embedding", "content": "Quantum embedding refers to the process of mapping classical data onto a quantum system, enabling quantum algorithms to leverage quantum computing's advantages. This mapping allows classical data to be represented in quantum states, typically through a process called quantum data encoding. Various techniques exist for quantum embedding, including amplitude encoding, basis encoding, and quantum feature maps, each with its unique advantages and trade-offs. In amplitude encoding, classical data is mapped to the amplitudes of a quantum state, allowing high-dimensional information to be represented efficiently within a smaller number of qubits. The ability to encode classical data in quantum states allows quantum computers to process and manipulate the information in ways that classical systems cannot, making it a valuable technique in the development of hybrid quantum-classical algorithms, such as those used in predictive modeling and machine learning. Quantum embedding, therefore, is an essential component for integrating quantum systems into machine learning workflows, improving both the accuracy and speed of models by exploiting quantum advantages in handling complex and high-dimensional data."}, {"title": "4.2 Quantum Entanglement", "content": "Quantum entanglement is a fundamental phenomenon in quantum mechanics where qubits become correlated in such a way that the state of one qubit cannot be described independently of the state of the other, even if they are spatially separated. This property allows quantum computers to per-form computations that would be impossible for classical computers. When qubits are entangled, they can be used to process information in parallel, dramatically speeding up computations. The entangled qubits share information instantaneously, which can enable faster problem-solving in computationally intensive tasks such as optimization and large-scale data analysis. In the context of quantum machine learning, entanglement plays a critical role by enabling quantum algorithms to efficiently explore and process large search spaces. By leveraging entanglement, quantum al-gorithms can achieve exponential speedups over classical counterparts in specific tasks, such as solving optimization problems or training complex models. The power of quantum entanglement lies in its ability to create correlations between quantum states that are not possible in classical systems, providing a significant advantage in the processing of large datasets or the solving of otherwise intractable problems."}, {"title": "5 Quantum Deep Learning", "content": "Quantum Deep Learning combines the properties of quantum computing and deep learning neural networks[15]. Quantum properties like superposition and entanglement, to process information can potentially enhance the capabilities of deep networks for faster convergence of algorithms. These networks are constructed using quantum gates and qubits instead of classical bits. QDL can take advantage of quantum data encoding, which is the process of encoding classical data in quantum form leading to more efficient processing for certain tasks. The quantum data can be mapped to a quantum feature space using techniques like the quantum kernel[23]. Quantum Neural Networks can be used for tasks like classification, regression, and optimization.\nMoreover, the integration of quantum mechanics into machine learning paradigms opens up new avenues for solving complex problems that are intractable for classical methods. For instance, quantum algorithms can significantly speed up matrix inversion and linear algebra operations[24], foundational tasks in training deep learning models enabling models to process larger datasets more efficiently. This enhancement in computational power is particularly beneficial in high-dimensional feature spaces, facilitating improved feature extraction, representation learning, and optimization processes[25]. QDL frameworks can enhance generalization capabilities by utilizing quantum noise and uncertainty, which act as a form of regularization, thereby reducing overfitting in models.\nRecent advances have also demonstrated the potential of quantum neural networks in tasks such as image recognition, where their ability to represent complex patterns offers significant advantages over classical approaches. Furthermore, hybrid architectures that combine classical and quantum methodologies are paving the way for innovative solutions across various domains, including fi-nance where they assist in credit risk assessment [26] [27] [28] [29]\u2014and healthcare, where they help in predicting patient outcomes and detecting anomalies in medical imaging."}, {"title": "6 Quantum Powered Credit Risk Assessment", "content": "This study presents a comprehensive framework for a hybrid quantum-classical deep learning model designed to enhance predictive analysis capabilities, incorporating Row-Type Dependent Predic-tive Analysis (RTDPA) and Synthetic Minority Over-sampling Technique Synthetic Minority Over-sampling Technique (SMOTE). The framework illustrates the integration of quantum computing elements with classical deep learning algorithms, highlighting the synergistic benefits of both ap-proaches [fig 2]. A detailed algorithm is outlined to guide the implementation of this hybrid model, ensuring efficient data processing and analysis while addressing class imbalance through SMOTE. Additionally, a circuit model is provided, showcasing the quantum and classical components uti-lized within the framework [fig 3]. Together, these elements form a robust foundation for advancing hybrid quantum-classical deep learning methodologies in various applications."}, {"title": "7 Experimental Results", "content": "In this section, we present description of data, algorithm execution and the results of our analysis."}, {"title": "7.1 Data", "content": "Our dataset comprises over 25,000 samples sourced from xxx Bank, encompassing 81 attributes detailed in Table5. Within this dataset, loan types can be distinctly grouped into two categories: Agriculture loans and Personal loans. Each loan entry falls under one of four classifications: standard, substandard, doubtful, or loss, as outlined in Table1. Notably, for personal loans, there are only three samples categorized as \"loss,\" prompting us to consolidate them into the \"doubtful\" category. To implement hybrid quantum classical deep neural network for Row-Type Dependent Predictive Analysis (HyQuC-DeepNN-RTDPA), we have segregated rows associated with each subcategory of loans and pre-processed them individually."}, {"title": "7.2 Data PreProcessing", "content": "Features dryland and wetland are not applicable to personal loan, so we removed them from analysis of personal loan. We also analysed missing values for each category and dropped features having missing values more then 70% as listed in table2 below."}, {"title": "7.3 Feature Selection", "content": "After preprocessing the data, we removed features with unique values across categories and handled missing values. To address the challenge of high dimensionality, we employed Principal Component Analysis (Principal Component Analysis (PCA))[30], a statistical technique often used to examine relationships between variables and reduce dimensionality. PCA projects the data onto a new vec-tor space determined by the eigenvectors of the original dataset, allowing us to evaluate variable importance while reducing redundancy and dimensionality. By generating linear combinations of the original variables, PCA yields a new set of variables that expose clearer underlying patterns within the data.\nTo identify the optimal number of principal components capturing most of the data's variance, we created a scree plot. This plot reveals the \"elbow\" or \"knee\" point where the explained vari-ance or eigenvalues begin to level off, indicating the number of components that should be retained to avoid diminishing returns in capturing additional variance. For our model, we selected 43 prin-cipal components for personal data 4a and 38 for agricultural data 4b.\nHowever, due to limitations in quantum simulators, which are currently unable to process data with up to 38 and 43 principal components, we tested our algorithm using only the first 5 princi-pal components. This reduction was necessary to accommodate the computational constraints of quantum simulation while still allowing us to evaluate the algorithm's effectiveness on a manage-able subset of the data. By focusing on the most significant components, we aimed to preserve the essential structure and variance within the data, ensuring meaningful insights despite the reduced"}, {"title": "7.4 Data Augmentation", "content": "Data augmentation is crucial in machine learning, especially for imbalanced datasets. In the bank-ing sector, where data often reflects significant class imbalance, it helps improve model performance and robustness. This technique is vital for enhancing predictive accuracy in loan assessments[31].\nIn particular, loan types such as Agriculture and Personal Loans exhibit imbalanced data dis-tributions. For instance, in Agriculture Loans, we observe 17,496 Standard cases versus only 294 Substandard and 210 Loss cases. Similarly, Personal Loans show 4,398 Standard against just 126 Substandard and 3 Loss cases.\nData augmentation generates synthetic samples for minority classes, allowing models to learn effectively from these underrepresented categories. This leads to better generalization, reduces the risk of over fitting, and ensures compliance with fair lending regulations. By employing data augmentation, banks can improve risk assessment models, thereby enhancing overall loan portfolio performance and making more informed lending decisions [32]."}, {"title": "8 Result Analysis", "content": "The training and validation loss curves for personal loans in fig5 indicate that the training loss fluctuates between 0.657 and 0.659, indicating that the model is learning. Towards the end of the epochs, it shows a slight downward trend but stabilizes, suggesting a plateau. The validation loss ranges from 0.3818 (epoch 191) to 0.4977 (epoch 180), with a noticeable downward trend, particularly from epochs 191 to 194. The validation loss generally remains lower than the training loss, which is a positive indicator of the model's generalization ability. This trend suggests that the model can adapt well to unseen data. Overall, while the model is improving, there may still be opportunities for further optimization to enhance its performance. The observation of stabilization in training loss suggests that additional tuning may be required to achieve better results.\nThe performance evaluation of the personal loan model in table3 reveals strong performance for the Standard Class, with a precision of 0.9959, indicating high confidence in predictions, although the recall of 0.8517 suggests some misclassification. The Sub Standard Class struggles significantly,"}, {"title": "8.2 Agriculture Loan Performance", "content": "The training and validation loss curves for agriculture loans in fig 6 indicate that the training loss is consistently around 0.75, indicating some stagnation in learning. The accuracy is slightly improving, but the changes are minimal between epochs (moving from 0.6911 to 0.6954). The validation loss fluctuates between epochs, indicating possible over fitting or instability in learning.\nNotably, some epochs show an increase in validation loss (from 0.5719 to 0.7138), which can be a concern. There is an overall trend of improvement in accuracy, the fluctuations in validation met-rics suggest the model may not be generalizing well to unseen data. Some epochs show a notable drop in validation accuracy (from 0.7952 to 0.7281), which could signal over fitting, particularly if the training accuracy continues to improve.\nThe Agricultural loan model achieves a reasonably high overall accuracy of 0.81, although this metric can be misleading due to class imbalance, with the majority of predictions dominated by the \"Standard\" class. Examining the macro average metrics (Precision 0.52, Recall 0.69, F1-Score 0.51) provides a more balanced view across classes, revealing that while the model has a decent recall (sensitivity to detecting classes), it struggles with precision, particularly for the minority \"Sub Standard\" class. The weighted average indicates better overall performance, heavily influenced by"}, {"title": "8.3 Economic Impact", "content": "Implications of HyQuC-DeepNN-RTDPA in the Banking Sector:\nThe HyQuC-DeepNN-RTDPA model has been specifically tested for credit risk analysis, demon-strating its effectiveness in identifying complex patterns in loan applicant profiles and predicting creditworthiness. The model's integration of quantum and classical deep learning layers allows it to manage diverse row types, such as various loan products (e.g., personal loans, mortgages, business loans), each with distinct risk profiles and data characteristics.\n1. Credit Risk Analysis The model is particularly suited for credit risk prediction due to its row-type-dependent structure. It can adapt to the unique characteristics of different customer groups, allowing for more granular and accurate risk assessments. Quantum layers, through oper-ations like entanglement and quantum measurement, enhance the model's ability to detect subtle correlations and interactions between features, such as income, credit history, and loan size. This leads to better risk discrimination, potentially reducing default rates and improving lending deci-sions.\n2. Fraud Detection Although initially applied to credit risk, the HyQuC-DeepNN-RTDPA model's architecture is equally effective for other predictive tasks, such as fraud detection. By leveraging quantum-inspired entangling layers, the model can efficiently process high-dimensional transactional data to identify anomalous patterns indicative of fraudulent behavior. The ability to handle row-type-specific pre processing also allows the model to differentiate between transaction types (e.g., high-frequency vs. low-frequency) and apply custom detection techniques.\n3. Predictive Maintenance and Forecasting In addition to credit risk and fraud detection, the model is also well-suited for other predictive analysis purposes in the banking sector, such as: Cus-tomer churn prediction: By modeling different customer segments (e.g., high-value vs. low-value customers), the model can identify those at risk of leaving and suggest retention strategies. Port-folio management: The quantum layers can capture the intricate relationships between financial assets, making the model useful for portfolio optimization and risk forecasting. Operational risk management: By handling heterogeneous datasets, the model can help banks predict and mitigate operational risks related to internal processes and regulatory compliance.\n4. Class Imbalance Handling Another notable feature of the model is its ability to address class imbalance across different row types. This is particularly valuable in scenarios such as minor-ity class loan applicants (e.g., applicants with non-traditional credit histories), where traditional models struggle. The HyQuC-DeepNN-RTDPA model effectively augments such datasets with quantum-inspired data augmentation techniques, improving model robustness and fairness.\nBroader Applicability in Predictive Analysis While this model has been applied to credit risk analysis, its hybrid architecture allows it to be extended to a wide range of predictive tasks across various domains. The ability to handle row-type-dependent processing makes it versatile for ap-plications such as marketing analytics, customer behavior prediction, risk forecasting in other industries, and time-series analysis. Future work can explore these applications by adapting the row-type-specific pre processing and hybrid quantum-classical architecture to new data domains and problem contexts."}, {"title": "9 Computational Constraints", "content": "The experiments in this study were conducted under significant hardware limitations, which im-pacted certain aspects of the methodology and analysis. The experiments were conducted on a system with 16 GB RAM, 8-core CPU specifically. The lack of access to high-performance com-puting infrastructure restricted our ability to perform computationally intensive operations such as the calculation of confidence intervals for key performance metrics (e.g., precision, recall, F1-score, accuracy, and ROC AUC). While these metrics are vital for providing a more robust and sta-tistically sound evaluation, the hardware constraints necessitated a trade-off in prioritizing other critical analyses.\nThese limitations also influenced the scale of hyper-parameter tuning and the level of complexity"}, {"title": "10 Limitations of the Study", "content": "Dimensionality Reduction Constraints: The use of Principal Component Analysis (PCA) for dimensionality reduction reduced the original feature set to only five components. While PCA aims to retain the most significant variance, this reduction may have resulted in the loss of critical information. As a result, the model's ability to accurately differentiate be-tween various loan statuses may have been compromised, particularly when subtle patterns essential for distinguishing between certain categories are lost. The limitations of current quantum simulators further restricted the depth of experimentation. Specifically, due to con-straints in handling higher-dimensional data, which are currently unable to process datasets with up to 38 or 43 principal components, we were limited to testing our algorithm using only the first 5 principal components.\nQuantum Model Sensitivity: Quantum models are inherently sensitive to the structure of the input data and the methods employed in data preprocessing. In this study, the re-liance on PCA as the sole dimensionality reduction technique may not have fully captured the complexities of the dataset. Other dimensionality reduction methods or a more diversi-fied preprocessing strategy could potentially improve the model's performance by preserving more nuanced features of the data. Additionally, the results presented in this study were based on angle embedding. However, other embedding techniques, such as Amplitude Em-bedding,superposition encoding could be explored in future research. These techniques offer different ways of encoding classical data into quantum states and may provide a more effective representation for certain types of datasets. Furthermore, due to computational constraints, we were unable to conduct extensive parameter variations or perform a detailed sensitivity analysis, both of which could provide deeper insights into the model's behavior and help optimize its performance.\nImbalanced Class Distribution: The dataset used in this study exhibited significant class imbalance, particularly in the 'Doubtful' and 'Loss' categories for both personal and agri-cultural loans. This imbalance can introduce bias in the model's predictions, resulting in a performance metric that might favor the majority class. While accuracy may appear high, the model could under perform on minority classes, affecting the overall robustness and fair-ness of predictions. To address class imbalance, SMOTE (Synthetic Minority Over-sampling Technique) and its variants can be explored. These include Borderline-SMOTE, which focuses on generating synthetic samples near the decision boundary, and its further variants, Borderline-SMOTE1 and Borderline-SMOTE2, which specifically target borderline or misclassified samples. SMOTE-ENN and SMOTE-Tomek Links combine SMOTE with data cleaning techniques like Edited Nearest Neighbor (ENN) and Tomek Links to refine the dataset after oversampling. ADASYN (Adaptive Synthetic Sampling) adjusts the number of synthetic samples based on the difficulty of minority class instances. KMeans-SMOTE uses clustering techniques to group similar minority class instances before applying SMOTE, ensuring that synthetic data respects the data structure. SVMSMOTE leverages Support Vector Machines (SVM) to create synthetic samples near the decision boundary, focusing on support vectors. Other variants, such as Random-SMOTE, generate synthetic samples by randomly interpolating between neighbors, while Cluster-SMOTE uses clus-tering algorithms to group similar data points before oversampling. These variants offer various strategies to improve the quality and relevance of the synthetic samples generated, depending on the specific characteristics of the dataset and problem at hand. However, due to computational constraints and the specific focus of this study, these techniques were not incorporated into our analysis. Future studies could investigate the impact of these balancing techniques on model performance, which may further enhance the model's ability to handle imbalanced datasets and improve predictions for the minority classes.\nLimited Hyperparameter Tuning: Due to computational constraints, the hyperparame-ters of the quantum model may not have been thoroughly optimized. Hyperparameter tuning is a crucial step in improving the performance of machine learning models, and inadequate tuning may have limited the model's potential to achieve its optimal performance. While a more extensive hyperparameter search could have potentially enhanced the model's ability to capture underlying patterns more effectively, we did perform a structured hyperparameter tuning process as outlined previously.\nBeyond the hyperparameters we tuned, additional adjustments could be made in future work to further optimize the model's performance. For instance:\n\u2022 Quantum Circuit Design: Experimenting with different quantum circuit architec-tures, such as varying the number of layers, gates, and types of quantum gates, could influence model performance.\n\u2022 Learning Rate Schedules: Implementing learning rate schedules (e.g., exponential decay, step decay) could improve convergence during training.\n\u2022 Optimizer Choices: Testing different optimization algorithms (e.g., Adam, SGD, Adagrad) could result in more efficient training and potentially better performance.\n\u2022 Dropout Rates: Implementing dropout or regularization techniques could help im-prove generalization by preventing overfitting.\n\u2022 Batch Normalization: Incorporating batch normalization could help with stabilizing training by normalizing activations between layers.\n\u2022 Early Stopping: Implementing early stopping criteria could prevent overfitting and save computational resources by halting training once performance plateaus.\nExploring these additional hyperparameters, along with more comprehensive searches, could enhance the robustness and effectiveness of the quantum model.\nInterpretability of Results: One of the significant challenges with quantum models is that their results are often difficult to interpret when compared to classical machine learning approaches. This lack of interpretability can pose a major limitation in practical applications, especially in sectors like banking, where understanding the reasoning behind model decisions is essential for ensuring trust and transparency. To address this, further research is needed to enhance the interpretability of quantum models, making them more suitable for real-world decision-making processes. Developing techniques that attribute quantum model predictions"}, {"title": "11 Conclusion", "content": "This framework presents a systematic approach for integrating QDL techniques into credit risk analysis, specifically designed for row-type dependent predictive analysis. It addresses essential theoretical foundations, practical implementation strategies, and explores the potential to enhance predictive capabilities within the banking sector. However, certain challenges remain, including the high computational costs associated with quantum processing, scalability limitations, and the current need for specialized hardware. Addressing these shortcomings will be essential for broader adoption and practical application in industry settings."}, {"title": "12 Model Functionality", "content": "To provide a clearer understanding of the interactions between the quantum and classical layers, a simple example is explored."}, {"title": "12.1 Step 1: Quantum Encoding", "content": "Normalization:\nNormalized $x_1 = \\frac{20}{\\sqrt{20^2 + 10^2}} = \\frac{20}{\\sqrt{500}} \u2248 0.8944$ and $x_2 = \\frac{10}{\\sqrt{20^2 + 10^2}} = \\frac{10}{\\sqrt{500}} \u2248 0.4472$\nThe quantum states for each data point are represented as:\nFor Data Point 1 (20 True): $|\\psi_1\\rangle = \\begin{bmatrix}0.8944\\\\0\\end{bmatrix}$\nFor Data Point 2 (10 False): $|\\psi_2\\rangle = \\begin{bmatrix}0\\\\0.4472\\end{bmatrix}$"}, {"title": "12.1.1 Quantum Processing", "content": "Assuming we have a simple quantum circuit that applies a rotation gate $R(\\theta) = \\begin{bmatrix}\\cos(\\frac{\\theta}{2}) & -\\sin(\\frac{\\theta}{2}) \\\\\\sin(\\frac{\\theta}{2}) & \\cos(\\frac{\\theta}{2})\\end{bmatrix}$ to the state, with \u03b8 = 0.325 radians."}, {"title": "12.2 Step 2: Classical Processing", "content": "Feeding into Classical Layers:\nAssuming we have a simple dense layer with: Weights: w = [0.5, 0.5] (for two inputs), Bias: b = 0.1 and Sigmoid: \u03c3(x) = $\\frac{1}{1+e^{-x}}$\nFeature Selection: In many machine learning models, specific features are selected to repre-sent the data. The quantum state is represented as a vector with multiple components, such as $|\\psi\\rangle = \\begin{bmatrix}0.8845\\\\0.1446\\end{bmatrix}$. Here, the dominant amplitude (in this case, 0.8845) is often chosen as it may represent the most significant aspect of the quantum state. This is because the first amplitude can be linked to the probability of measuring a certain outcome, thus influencing the subsequent classical processing."}, {"title": "12.3 Step 3: Loss Calculation", "content": "Using Binary Cross-Entropy: Loss = $-(y \\cdot log(\\hat{y}) + (1 \u2212 y) \\cdot log(1 \u2013 \\hat{y}))$\nFor the first data point: Assuming y = 1 (True):\nLoss = $-(1 \\cdot log(0.6321) + 0 \\cdot log(1-0.6321)) \u2248 \u2013 log(0.6321) \u2248 0.4587$\nFor the second data point: Assuming y = 0 (False):\nLoss = $-(0 \\cdot log(0.5796) + 1 \\cdot log(1-0.5796)) \u2248 - log(0.4204) \u2248 0.8665$\nTotal Loss = $\\frac{0.4587+0.8665}{2} = \\frac{1.3252}{2} \u2248 0.6626$"}, {"title": "12.4 Backward Pass (Gradient Calculation)", "content": "12.4.1 Gradients for Classical Layers:\nTo find the gradients with respect to the weights w:"}, {"title": "12.4.2 Gradients for Quantum Layers:", "content": "To calculate the gradient of the output probabilities with respect to \u03b8, we apply the parameter shift rule: $f(\\theta) = \\frac{[f(\\theta+\\frac{\\pi}{2})\u2013f(\\theta-\\frac{\\pi}{2})]}{2}$. Here, f(\u03b8) is the probability of measuring the state corresponding to |s\u27e9.[33] [34]\nEvaluate f (\u03b8+$\\frac{\\pi}{2}$): First, we calculate the new output probabilities when \u03b8 is increased by $\\frac{\\pi}{2}$:\nFirst, we calculate the new output probabilities when \u03b8 is increased by $\\frac{\\pi}{2}$:\nStep 1: Define the Parameter Shift Rotation Matrices For a rotation by \u03b8+ $\\frac{\\pi}{2}$:\nFor f (\u03b8+$\\frac{\\pi}{2}$):\n$R(\\theta+\\frac{\\pi}{2}) = \\begin{bmatrix}0 & -1\\\\1 & 0\\end{bmatrix}$"}, {"title": "Author Information", "content": "Authors and Affiliations:\nPhD, Analytics and Decision Science, IIM Mumbai, India. minati.rath.2019@iimmumbai.ac.in.\nProfessor, Analytics and Decision Science, IIM Mumbai, India. hemadate@iimmumbai.ac.in."}, {"title": "Corresponding Authors", "content": "Correspondence to minati06@gmail.com, Minati Rath"}]}