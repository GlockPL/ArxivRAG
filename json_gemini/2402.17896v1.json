{"title": "Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents", "authors": ["Corby Rosset", "Ho-Lam Chung", "Guanghui Qin", "Ethan C. Chau", "Zhuo Feng", "Ahmed Awadallah", "Jennifer Neville", "Nikhil Rao"], "abstract": "Existing question answering (QA) datasets are no longer challenging to most powerful Large Language Models (LLMs). Traditional QA benchmarks like TriviaQA, NaturalQuestions, ELI5 and HotpotQA mainly study \"known unknowns\" with clear indications of both what information is missing, and how to find it to answer the question. Hence, good performance on these benchmarks provides a false sense of security. A yet unmet need of the NLP community is a bank of non-factoid, multi-perspective questions involving a great deal of unclear information needs, i.e. \"unknown unknowns\". We claim we can find such questions in search engine logs, which is surprising because most question-intent queries are indeed factoid. We present Researchy Questions, a dataset of search engine queries tediously filtered to be non-factoid, \u201cdecompositional\" and multi-perspective. We show that users spend a lot of \"effort\" on these questions in terms of signals like clicks and session length, and that they are also challenging for GPT-4. We also show that \u201cslow thinking\u201d answering techniques, like decomposition into sub-questions shows benefit over answering directly. We release ~ 100k Researchy Questions, along with the Clueweb22 URLs that were clicked.", "sections": [{"title": "1 Introduction", "content": "The advent of Large Language Models (LLMs) has ushered in a new era for the field natural language processing, with short- and long- form question- answering (QA) at the forefront of recent achieve- ments (OpenAI et al., 2023). Historically, QA benchmarks served as the crucible for evaluating a model's grasp of natural language understanding. However, LLMs have nearly perfected many QA datasets, particularly those involving answering short, factual questions like \u201cWhat is the capital of Brunei?"}, {"title": "QA systems to logically bridge information across multiple documents or paragraphs. While these datasets have made strides in increasing question complexity, the answers are still ultimately factoid, and it's clear what sub-questions ought to be asked to recall the missing information. Furthermore, the construction of these datasets (e.g. synthetically generated from paths of wikipedia links) leads to a distribution mismatch w.r.t questions humans ask.\nThere exist several sources of long-form, non- factoid QA datasets like ELI5 (Fan et al., 2019), Stack Exchange, Yahoo Answers (Zhang et al., 2016), and WikiHowQA (Bolotova-Baranova et al., 2023). While answers to these questions are more involved than factoid questions, ELI5 and Wiki- HowQA elicit more expository answers than analyt- ical ones. The Oxford Allsouls dataset (Liu et al., 2023b) contains 1k college-level essay prompts which are multi-perspective, but are designed to evaluate persuasive writing skills and do not have associated documents to ground responses. AQua- MuSe (Kulkarni et al., 2020) is an excellent at- tempt at filtering Natural Questions (NQ) for multi- faceted queries, but their method was constrained by relying on the relatively short paragraph-length answers already present in NQ.\nThe rise of \"LLM Agents\" e.g. (Wu et al., 2023b) have opened the door for even deeper col- laboration between users, LLMs, and tools. In response, more recent datasets have focused on completing challenging tasks in open-ended envi-", "content": "QA systems to logically bridge information across multiple documents or paragraphs. While these datasets have made strides in increasing question complexity, the answers are still ultimately factoid, and it's clear what sub-questions ought to be asked to recall the missing information. Furthermore, the construction of these datasets (e.g. synthetically generated from paths of wikipedia links) leads to a distribution mismatch w.r.t questions humans ask.\nThere exist several sources of long-form, non- factoid QA datasets like ELI5 (Fan et al., 2019), Stack Exchange, Yahoo Answers (Zhang et al., 2016), and WikiHowQA (Bolotova-Baranova et al., 2023). While answers to these questions are more involved than factoid questions, ELI5 and Wiki- HowQA elicit more expository answers than analyt- ical ones. The Oxford Allsouls dataset (Liu et al., 2023b) contains 1k college-level essay prompts which are multi-perspective, but are designed to evaluate persuasive writing skills and do not have associated documents to ground responses. AQua- MuSe (Kulkarni et al., 2020) is an excellent at- tempt at filtering Natural Questions (NQ) for multi- faceted queries, but their method was constrained by relying on the relatively short paragraph-length answers already present in NQ.\nThe rise of \"LLM Agents\" e.g. (Wu et al., 2023b) have opened the door for even deeper col- laboration between users, LLMs, and tools. In response, more recent datasets have focused on completing challenging tasks in open-ended envi-"}, {"title": "ronments with tools like a web browser, file system, database, etc.\nIn particular, Gaia (Mialon et al., 2023) tests understanding of multi-modal inputs (images and text), as well as complex reasoning across levels of difficulty to solve. AgentBench (Liu et al., 2023c) provides closed environments for an LLM to in- teract with APIs in various scenarios, including coding (interacting with file systems or databases), games/puzzles, and web browsing/shopping. While these datasets advance the field of metrics for LLM agents, they are small, consisting of only 466 and 1,091 questions respectively that were hand-curated by the authors.\nThe need for more challenging QA datasets also stems from some alarming trends: while there are hundreds of public LLMs, they are pretrained on only a handful existing corpora (Gao et al., 2020; Raffel et al., 2023), or distilled from one of a few teacher LLMs (Peng et al., 2023). Further- more, much more data scraped from the internet for training is itself going to be AI-generated con- tent, leading to an echo-chamber effect (Dohma- tob et al., 2024; Wu et al., 2023a). Hence, the convergent evolution (Stayton, 2015) of LLMs increases the risk that they will not recognize that they don't know something, e.g. see GPT- 4 and Mixtral 8x7b in Table 10. This is espe- cially true when LLMs act in the capacity of LLM-as-a-judge (Zheng et al., 2023a; Yuan et al., 2024), or when faced with very multi-faceted/multi-", "content": "ronments with tools like a web browser, file system, database, etc.\nIn particular, Gaia (Mialon et al., 2023) tests understanding of multi-modal inputs (images and text), as well as complex reasoning across levels of difficulty to solve. AgentBench (Liu et al., 2023c) provides closed environments for an LLM to in- teract with APIs in various scenarios, including coding (interacting with file systems or databases), games/puzzles, and web browsing/shopping. While these datasets advance the field of metrics for LLM agents, they are small, consisting of only 466 and 1,091 questions respectively that were hand-curated by the authors.\nThe need for more challenging QA datasets also stems from some alarming trends: while there are hundreds of public LLMs, they are pretrained on only a handful existing corpora (Gao et al., 2020; Raffel et al., 2023), or distilled from one of a few teacher LLMs (Peng et al., 2023). Further- more, much more data scraped from the internet for training is itself going to be AI-generated con- tent, leading to an echo-chamber effect (Dohma- tob et al., 2024; Wu et al., 2023a). Hence, the convergent evolution (Stayton, 2015) of LLMs increases the risk that they will not recognize that they don't know something, e.g. see GPT- 4 and Mixtral 8x7b in Table 10. This is espe- cially true when LLMs act in the capacity of LLM-as-a-judge (Zheng et al., 2023a; Yuan et al., 2024), or when faced with very multi-faceted/multi-"}, {"title": "perspective questions, where the consequences could be that users \u201cmiss the whole picture\" or worse, are misled (Zheng et al., 2023b; Liu et al., 2023b). While retrieval-augmentation (Lewis et al., 2021; Borgeaud et al., 2022; Guu et al., 2020) can help complement LLM agents, the risk is merely shifted to whether a sub-system retrieves the right information and uses it correctly (Liu et al., 2023a).\nWe believe the well-studied phenomenon of \"unknown unknowns\u201d (United States Congress et al., 1981) applies to LLM Agents in scenarios ad- dressing complex questions requiring \u201cslow think- ing\" (Kahneman, 2011). Simply put, one strategy is to iteratively re-frame or decompose the problem into a set of \"known unknowns\u201d (which charac- terize most of the aforementioned QA datasets). For these sub-problems, it should be clearer what information is missing, how to find it, and once found, how the \u201cknown known", "open-domain": "content", "perspective questions, where the consequences could be that users \u201cmiss the whole picture\" or worse, are misled (Zheng et al., 2023b; Liu et al., 2023b). While retrieval-augmentation (Lewis et al., 2021; Borgeaud et al., 2022; Guu et al., 2020) can help complement LLM agents, the risk is merely shifted to whether a sub-system retrieves the right information and uses it correctly (Liu et al., 2023a).\nWe believe the well-studied phenomenon of \"unknown unknowns\u201d (United States Congress et al., 1981) applies to LLM Agents in scenarios ad- dressing complex questions requiring \u201cslow think- ing\" (Kahneman, 2011). Simply put, one strategy is to iteratively re-frame or decompose the problem into a set of \"known unknowns\u201d (which charac- terize most of the aforementioned QA datasets). For these sub-problems, it should be clearer what information is missing, how to find it, and once found, how the \u201cknown known": "ontributes to the final answer. Several techniques such as chain- of-thought question decomposition (Radhakrish- nan et al., 2023) and tree-of-thought (Yao et al., 2023a) prompting take a similar approach to plan long-horizon solutions to complex problems. How- ever, those studies still operate over traditional QA benchmarks like HotpotQA, or over simple games like crossword puzzles. Hence, the right bench- mark of questions for these advanced decomposi- tion techniques still does not exist for open-domain"}, {"title": "web scenarios (Krishna et al., 2021).\nWe present Researchy Questions to study the dynamics of how LLM agents handle unclear infor- mation needs associated with very complex ques- tions. We define a Researchy Question as a non- factoid question that expects a long-form answer (longer than a paragraph!) entailing substantial research or effort to synthesize. A Researchy Question can be instantiated as a complex search task (Aula and Russell, 2008) with unclear infor- mation needs that requires analyzing multiple docu- ments or pieces of evidence. A Researchy Question does not have a single correct answer, but rather multiple perspectives allowing a dense manifold of answers over which varying criteria can determine which is better. In practice, the act of answering a Researchy Question probably involves dec\u043e\u0442\u0440\u043e- sition into sub-questions that aid the retrieval of comprehensive information, reducing the risk of missing unknown unknowns. Lastly, a Researchy Question represents a genuine information need that real people asked. Figure 1 qualitatively com- pares other canonical QA datasets.\nResearchy Questions is primarily a QA dataset to evaluate question answering systems or LLM Agents with the ultimate goal of achieving ever higher-quality answers using any tools neces- sary. Notwithstanding, it is also a search/retrieval dataset in the sense that finding and correctly incor-", "content": "web scenarios (Krishna et al., 2021).\nWe present Researchy Questions to study the dynamics of how LLM agents handle unclear infor- mation needs associated with very complex ques- tions. We define a Researchy Question as a non- factoid question that expects a long-form answer (longer than a paragraph!) entailing substantial research or effort to synthesize. A Researchy Question can be instantiated as a complex search task (Aula and Russell, 2008) with unclear infor- mation needs that requires analyzing multiple docu- ments or pieces of evidence. A Researchy Question does not have a single correct answer, but rather multiple perspectives allowing a dense manifold of answers over which varying criteria can determine which is better. In practice, the act of answering a Researchy Question probably involves dec\u043e\u0442\u0440\u043e- sition into sub-questions that aid the retrieval of comprehensive information, reducing the risk of missing unknown unknowns. Lastly, a Researchy Question represents a genuine information need that real people asked. Figure 1 qualitatively com- pares other canonical QA datasets.\nResearchy Questions is primarily a QA dataset to evaluate question answering systems or LLM Agents with the ultimate goal of achieving ever higher-quality answers using any tools neces- sary. Notwithstanding, it is also a search/retrieval dataset in the sense that finding and correctly incor-"}, {"title": "porating the right evidence is a critical sub-system to meet expectations of trustworthiness and ground- edness (Zheng et al., 2023b; Liu et al., 2023b). While we believe that question decomposition is a critical piece of solving Researchy Questions, it is unclear how to define or measure the quality of sub-questions. To aid this endeavor, we reveal what URLs end users found useful, with the hope that good sub-questions will at least lead to the information found in those clicked documents.\nWe release about 96K Researchy Questions consisting of real users' queries to a commercial search engine, and additionally:\n1. The decomposition of the question into a 2- level hierarchical plan (See Table 2 Left).\n2. For each question, the user-aggregated click distribution over URLs in a publicly available web corpus, ClueWeb22.\n3. Ordered list of sub-queries corresponding roughly to the sub-questions that can be read- ily issued to a search engine\nIn Section 2 we describe how Researchy Questions were obtained and then characterize them in Section 3. In Section 4 we verify that web users expend more effort on Researchy Questions over other queries. In Section 5 we evaluate and compare decompositional answering techniques from (Radhakrishnan et al., 2023).", "content": "porating the right evidence is a critical sub-system to meet expectations of trustworthiness and ground- edness (Zheng et al., 2023b; Liu et al., 2023b). While we believe that question decomposition is a critical piece of solving Researchy Questions, it is unclear how to define or measure the quality of sub-questions. To aid this endeavor, we reveal what URLs end users found useful, with the hope that good sub-questions will at least lead to the information found in those clicked documents.\nWe release about 96K Researchy Questions consisting of real users' queries to a commercial search engine, and additionally:\n1. The decomposition of the question into a 2- level hierarchical plan (See Table 2 Left).\n2. For each question, the user-aggregated click distribution over URLs in a publicly available web corpus, ClueWeb22.\n3. Ordered list of sub-queries corresponding roughly to the sub-questions that can be read- ily issued to a search engine\nIn Section 2 we describe how Researchy Questions were obtained and then characterize them in Section 3. In Section 4 we verify that web users expend more effort on Researchy Questions over other queries. In Section 5 we evaluate and compare decompositional answering techniques from (Radhakrishnan et al., 2023)."}, {"title": "2 Researchy Questions Construction", "content": "Researchy Questions are real user queries from search logs. While search logs have a rich diversity of query types and intents (Bolotova et al., 2022; Bu et al., 2010), they mostly contain factoid or navigational queries, which need to be filtered."}, {"title": "2.1 Stage 1: Mining Search Logs", "content": "We obtained a set of query-url click pairs from a commercial search engine that were logged be- tween July 2021 and August 2022, which maxi- mally overlaps with the creation of the Clueweb22 snapshot of web documents (Overwijk et al., 2022). This way, we can simply indicate which Researchy Questions clicked on which documents. We be- gan with a large sample of english, non-adult queries that had least one click. We denote these as \"General Queries\u201d, which were further filtered.\nAn important filtering criteria is frequency: we kept queries if they were issued at least 50 times in the logs. This criteria is simple but powerful: it helps denoise the dataset (reduces spelling errors), and also allows us to focus on questions that are not \"one-offs\". This helps us glean insights into repeated user behaviors as they interact with the search engine.\nIn order to select queries which are answer- seeking intent (i.e. actual \u201cquestions\u201d as distinct from navigational queries like \u201cfacebook login", "fast running shoes": "or local intent like", "classifiers": "n\u2022 Query Language: English\n\u2022 Adult intent: False\n\u2022 Number of Distinct Occurrences: \u2265 50\n\u2022 3 < Number of Query Words: < 15\n\u2022 Number of Distinct URLs Clicked On: \u2265 2\n\u2022 Question Intent Classifier: True\n\u2022 Navigational Intent: False\n\u2022 Local / Real Estate / Map Intent: False\n\u2022 Retail/Shopping Intent: False\n\u2022 Coding/Technical Intent: False\n\u2022 Health/Medical Intent: False\n\u2022 Triggered Possible Answer Cards: \u2265 1\n\u2022 Triggered lots of Ads: False\nTo explain some of the points above: an Answer Card is a high-precision feature in a search engine where a paragraph containing the answer is shown at the top of the results page, distinct from the \u201cten blue links", "a lot of Ads\" is determined by summing the total number of ads shown for the query over the full year and dividing by the number of times the query was issued, and then picking a threshold above which queries seemed \u201cshopping intent": "The Ads"}, {"title": "requirement also helped catch any shopping intent queries missed by the Retail intent classifier.\nWe wanted to remove Coding/Technical queries because such questions are often motivated by very specific problems that are often solved by one piece of documentation after lots of clicking around, which is not the behavior we wish to target in this dataset. Health and medical questions are avoided largely because they often overlap too much with those that ought to be addressed by a licensed med- ical professional. Many shopping / retail queries could be construed as \u201cresearchy", "what are the best headphones\", but we avoid them in this dataset because it is hard to distinguish whether a URL was clicked due to aggressive advertising or a real information need.\nAfter this stage of filtering, we arrived at 15.7M \"QnA Queries\u201d which are largely recognizable as natural language questions that deal with open- domain knowledge. This size was manageable enough to efficiently run our own bert-large scale classifiers we use in the next stage of filtering.": "content", "requirement also helped catch any shopping intent queries missed by the Retail intent classifier.\nWe wanted to remove Coding/Technical queries because such questions are often motivated by very specific problems that are often solved by one piece of documentation after lots of clicking around, which is not the behavior we wish to target in this dataset. Health and medical questions are avoided largely because they often overlap too much with those that ought to be addressed by a licensed med- ical professional. Many shopping / retail queries could be construed as \u201cresearchy": "."}, {}, {"title": "2.2 Stage 2: Factoid Classifier", "content": "We needed a way to distinguish which QnA Queries were factoid vs. non-factoid; we trained a bi- nary classifier on auto-labeled data for this pur- pose. The training data was a uniform sample of 200k questions from the 15.7M QnA Queries. La- bels for the questions were collected from gpt3 (text-davinci-003) prompted with few-shot ex- amples as shown in Figure 4. The labels were then used to train a bert-large non-factoid question classifier, which was then inferenced on the full set of 15.7M queries. By manual inspection, a thresh- old was chosen above which we were satisfied that the questions were meaningfully non-factoid. The resulting 1.0M met the non-factoid threshold of 0.75 as shown on the left hand side of Figure 3, which we denote as \"Non-factoid QnA Queries\u201d."}, {"title": "2.3 Stage 3: Decompositional Classifier", "content": "Not all the resulting non-factoid QnA queries ex- hibited the \"decompositional\u201d information needs. Namely, they often looked expository or \u201chow-to\" with generally one correct answer which did not have many perspectives. We trained a second classi- fier to score the extent to which a question requires asking sub-questions. The specific definition of \u201cre- quiring sub-questions\u201d is described in a prompt in Figure 5 given to ChatGPT (gpt-35-turbo) to col- lect labels. We used ChatGPT because we figured this was a relatively intense cognitive task. We inferenced ChatGPT on about 40k outputs from the non-factoid classifier that met the non-factoid threshold of 0.75. We used the labels to train a separate bert-large \u201cdecompositional\u201d classifier.\nAgain, we chose a threshold by manual inspec- tion to indicate which of the 1.0M Non-factoid QnA Queries were also decompositional, which happened to be 0.6 as shown on the right-hand side of Figure 3. Out of the 1.0M queries that met the non-factoid threshold, 146k also met the de- compositional threshold. These 146k became the Researchy Questions candidates before dedupli- cation. This procedure is not perfect; in Table 6 we show examples of some non-factoid questions that did not meet the decompositional threshold.\""}, {"title": "2.4 Stage 4: Deduplication", "content": "The final step of filtering is deduplication. We take an agglomerative clustering approach (Everitt, 1974), where the only parameter is a distance threshold \u03f5 below which two queries are consid- ered \"duplicate intent\".\nWe represent the semantic intent of a query by an ANCE-based (Xiong et al., 2020) vector encoder $\\vec{v_q} \\leftarrow encoder(q_i)$. We instantiate a metric space defined by 1 - $cosine(\\cdot, \\cdot)$ of the vector encod- ings using the faiss implementation of an approxi- mate nearest neighbor (ANN) index (Johnson et al., 2019). For each question in the index, we search"}, {"title": "for the nearest neighbors {$q_j$ ~ ANN($q_i$) s.t. 1.0 - $\\vec{v_q}\\cdot \\vec{v_q}$ < \u03f5}. For agglomerative clustering, we define a \"group\" as a set of queries in which all pair- wise distances are within \u03f5. We found that about 63% of queries were singletons (did not belong to a group of size greater than one), and the average group size was 3.8. For example, the queries \u201cwhat were tanks used for in wwl\u201d, \u201chow were the tanks used in wwl\u201d and \u201cwhy were tanks needed in wwl\" were all part of the same group. For all groups of size greater than one, we selected the query that was issued most often in the logs as the representa- tive \"head\" of the group. After combining the heads of groups and singletons, about 70% of queries re- mained, yielding 102k Researchy Questions Although we did our best to de-duplicate question intent, some clusters of topics remain, e.g. a quick keyword count shows about 600 contain the string \"ww2", "supreme court": "", "content": "for the nearest neighbors {$q_j$ ~ ANN($q_i$) s.t. 1.0 - $\\vec{v_q}\\cdot \\vec{v_q}$ < \u03f5}. For agglomerative clustering, we define a \"group\" as a set of queries in which all pair- wise distances are within \u03f5. We found that about 63% of queries were singletons (did not belong to a group of size greater than one), and the average group size was 3.8. For example, the queries \u201cwhat were tanks used for in wwl\u201d, \u201chow were the tanks used in wwl\u201d and \u201cwhy were tanks needed in wwl\" were all part of the same group. For all groups of size greater than one, we selected the query that was issued most often in the logs as the representa- tive \"head\" of the group. After combining the heads of groups and singletons, about 70% of queries re- mained, yielding 102k Researchy Questions Although we did our best to de-duplicate question intent, some clusters of topics remain, e.g. a quick keyword count shows about 600 contain the string \"ww2"}, {"supreme court": ""}, {"title": "2.5 Stage 5: Final GPT-4 Filtering", "content": "As a final quality control step after deduplication, we had all 102k questions labeled by GPT-4 for intrinsic attributes of the question like how multi- faceted it is, how reasoning-intensive it is likely to be, etc. The full set of eight attributes is defined in Figure 7, along with histograms of those scores in Figure 6 for both Researchy Questions and Natural Questions. All eight are scored on a scale of 1-10. About 3% of 102k questions were re- moved based on the attributes \u201cambiguous\" and \u201cincomplete\" which indicate defective questions too difficult to answer; some examples are shown in Table 7. Another 2% were removed for being too \u201cassumptive\u201d, meaning the question was phrased in a presumptuous way that was likely to bias the answer as shown in Table 8. Another 2% was removed for safety reasons as shown in Table 9, where we deem risk of harm too high by attempt- ing to answer the question. Not all \"assumptive\" questions are harmful per se. Finally, the remaining 96k queries are what we release.\""}, {"title": "3 Characterizing Researchy Questions", "content": "Across the 96k Researchy Questions (split into 90k train, 6.4k test), there are in aggregate 350k unique documents clicked, 48% of which can be found in the english subset of Clueweb22 Set B (Overwijk et al., 2022); the rest are in Sets A or L. For each question we release, there are on av- erage 4.9 +/- 3.5 clicked documents (See Figure 2 Right), indicating a good diversity of information needs and much higher than the average query over- all. Conversely, for each document there is only 1.4 +/- 2.3 associated Researchy Questions (See Fig- ure 2 Left) indicating good query de-duplication.\nTo get a sense of how intrinsically difficult Researchy Questions are compared to other datasets, we asked GPT-4 how many sub-questions or search engine queries would be necessary to ask/issue in order to fully answer each question. An example decomposition in shown in Table 2, and the aggregate results are shown in Table 1. Clearly, GPT-4 thinks that most factoid QA datasets (top of the table) require the fewest sub-questions to answer, whereas Researchy Questions require the most even among longer-form QA datasets.\nWe also compared Researchy Questions to another search-log based QA dataset \u2013 Natural Questions (Kwiatkowski et al., 2019) \u2013 along the 8 quality dimensions descried in Section 2.5, such as how reasoning- and knowledge-intense they are. The comparative histograms are shown in Figure 6, Clearly, GPT-4 thinks that Researchy Questions require much more knowledge, reason- ing, and are inherently more multi-faceted.\nTable 3 shows the distribution of first words across Researchy Questions (as well as inter- mediate datasets used in the filtering funnel de- scribed in the next section). For comparison, MS Marco queries (also from web search logs) are much more factoid e.g. only 1.64% of them start with \"why", "should the death penalty be legal- ized\", the fact that in the US, \u201cthe cost of enforcing the death penalty cost millions more than life im- prisonment": "would have a large impact on the economic arguments in that answer. We define a Pivotal Fact as a piece of information that is so surprising and consequential that, once known, drastically changes how an LLM Agent would an- swer the question (and the answer wouldn't be as good without it); but it wouldn't know about it un- less it asked the right sub-question to retrieve it, e.g.", "onment": "Hence, a pivotal fact is the information analogue of a black swan, a critical event that is hard to predict but highly impactful (Taleb, 2008);"}, {"title": "4 Agreement with User Search Behavior", "content": "More complicated questions ought to require more effort to answer (Kelly et al., 2015). We can approx- imate the amount of effort users expend in terms of behavior signals like clicks and session length.\nIn Table 4 (Left) we show aggregate click statis- tics for each subset of queries. The results show that Researchy Questions, which are both non- factoid and decompositional, lead to more in-depth consumption (clicks and sat-clicks) of more di- verse information (unique urls), agreeing with prior work (Hassan et al., 2014).\nIn Table 4 (Right) we show behavioral signals exhibited by users at the session level rather than individual click level. For instance, if a QnA-type query appeared at any time in any session in the date range, the whole session is included in the \"QnA Sessions\" row. The results clearly show users were twice as engaged answering non-factoid ques- tions than sessions devoted to factoid-y intents, and six times longer than the average session overall."}, {"title": "5 Evaluating Answer Techniques to Researchy Questions", "content": "Since there is no one \"correct\" answer to a Re- searchy Question, we contend that they ought be evaluated in a relative, side-by-side fashion a la Alpaca-Eval syle (Li et al., 2023), with e.g. the \"closed-book\" answer as the reference.\nSince Researchy Questions are intended to be answered by decomposing them into sub-questions, we evaluate two decompositional question answer- ing answer techniques \u2013 chain-of-thought decom- position, and factored decomposition \u2013 against the direct answering baseline. Factored decomposition makes a separate call to an LLM for each sub- question independently, and then a final \u201crecom- position", "Accuracy\u201d as a binary score of whether the candidate answer was consistent with the gold. The": "core"}]}