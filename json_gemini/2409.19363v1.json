{"title": "Learning Strategy Representation for Imitation Learning in Multi-Agent Games", "authors": ["Shiqi Lei", "Kanghon Lee", "Linjing Li", "Jinkyoo Park"], "abstract": "The offline datasets for imitation learning (IL) in multi-agent games typically contain player trajectories exhibiting diverse strategies, which necessitate measures to prevent learning algorithms from acquiring undesirable behaviors. Learning representations for these trajectories is an effective approach to depicting the strategies employed by each demonstrator. However, existing learning strategies often require player identification or rely on strong assumptions, which are not appropriate for multi-agent games. Therefore, in this paper, we introduce the Strategy Representation for Imitation Learning (STRIL) framework, which (1) effectively learns strategy representations in multi-agent games, (2) estimates proposed indicators based on these representations, and (3) filters out sub-optimal data using the indicators. STRIL is a plug-in method that can be integrated into existing IL algorithms. We demonstrate the effectiveness of STRIL across competitive multi-agent scenarios, including Two-player Pong, Limit Texas Hold'em, and Connect Four. Our approach successfully acquires strategy representations and indicators, thereby identifying dominant trajectories and significantly enhancing existing IL performance across these environments.", "sections": [{"title": "Introduction", "content": "Although reinforcement learning has become a powerful technique for sequential decision-making in various domains such as robotic manipulation [1], autonomous driving [7], and game playing [32], conventional reinforcement learning demands substantial online interactions with the environment, which can be costly and sample inefficient while potentially leading to safety risks [4, 5]. To address these issues, many methods have emerged to enable efficient learning using offline datasets generated by demonstrators. For example, imitation learning (IL) [26] replicates actions from the offline dataset without reward information, while offline reinforcement learning [13, 20] is provided access to reward signals. Offline learning datasets are usually collected from multiple demonstrators to enlarge dataset scale and diversity [30, 23, 24], which leads to a dataset of behaviors with various characteristics. However, standard IL algorithms treat all data samples in the dataset as homogeneous, potentially learning undesired behaviors from sub-optimal trajectories.\nTo address the above issue, the key insight in our proposed method is to assign each trajectory in the offline dataset with a unique learned attribute, i.e., strategy representation, so that we can further analyze each trajectory considering its specificity and filter out sub-optimal data. With a precise depiction of each trajectory and their distribution on the representation space, we can judge the performance of each trajectory by only collecting a few (less than 5%) data with trajectory rewards or even without any reward information. In this work, we introduce Strategy Representation for Imitation Learning (STRIL), an efficient and interpretable approach designed to improve IL by filtering sub-optimal demonstrations from offline datasets.\nFigure 1 illustrates an overview of STRIL. Note that STRIL is a plug-in method compatible with existing IL algorithms. It consists of three components: strategy representation learning using a Partially-trainable-conditioned Variational Recurrent Neural Network (P-VRNN), indicator estimation, and filtered IL. The detailed steps and corresponding contributions are outlined as follows:\n\u2022 We propose an unsupervised framework with P-VRNN to efficiently extract strategy representations from multi-agent game trajectories. Strategy representation for each trajectory is customized as a network condition.\n\u2022 We define the Randomness Indicator (RI) and Exploited Level (EL), which utilize strategy representation to effectively evaluate offline trajectories in a zero-sum game. EL can be precisely estimated even with limited reward data, while RI requires no reward data.\n\u2022 We enhance existing IL methods by filtering out sub-optimal trajectories according to the RI and EL indicators, ensuring that IL is trained exclusively on the dominant trajectory.\n\u2022 We demonstrate that STRIL can provide effective strategy learning without player identification and significantly enhance the performance of various IL algorithms in competitive zero-sum games, including Two-player Pong, Limit Texas Hold'em, and Connect Four."}, {"title": "Related Works", "content": "Imitation Learning. In the conventional IL settings, the expert trajectories only have information of state-action pairs without reward information [26, 27, 18, 10, 14], and it is assumed that the demonstrations are homogeneous oracle. However, realistic crowd-sourced datasets are usually multi-modal and include sub-optimal demonstrations. Some IL methods are proposed for multi-modal offline datasets, such as [16] and [11]. As for sub-optimal data, there are plenty of approaches to alleviate the negative influence [6, 8, 35], but all these methods require environment dynamics or the rankings over the demonstrations. In contrast, our method does not require such information. Sasaki and Yamashina [28] enhances behavior cloning (BC) with noisy demonstrations, but their method does not deal with general sub-optimal trajectories. TRAIL [33] achieves sample-efficient IL via a learned latent action space and a factored transition model. We would like to additionally mention the work by Franzmeyer et al. [12], which adopts a similar framework of filtering the offline dataset and uses an IL algorithm. Nevertheless, their method is studied in a cooperative setting and requires reward information.\nIL with Representation Learning. The work by Beliaev et al. [3] closely aligns with our research, sharing the primary goal of extracting expertise levels of trajectories. They assume that the demonstrator has a vector indicating the expertise of latent skills, with each skill requiring a different level at a specific state. These elements jointly derive the expertise level. The method also considers the policy worse when it is closer to uniformly random distribution. However, this assumption cannot be satisfied even in simple games like RPS, where a uniformly random strategy constitutes a Nash equilibrium. Play-LMP [22] leverages unsupervised representation learning in a latent plan space for improved task generalization. However, employing a variational auto-encoder (VAE) with the encoder outputting latent plans is unsuitable for multi-player games, potentially leaking opponent information from the observations and disrupting the evaluation of the demonstrator. Grover et al. [15] also studies learning policy representations from offline trajectories. They use the information of agent identification during training, which enables them to add a loss to distinguish one agent from others. However, this information may not be provided in the offline datasets."}, {"title": "Preliminaries", "content": "Markov Games. A Markov game [21] is a partially observable Markov decision process [19] (POMDP) adapted to a multi-agent setting, where each agent has its own reward function. In a Markov game, there is a state space S and n agents, with each agent i having a corresponding action space A\u1d62 and observation space O\u1d62. When an agent is not required to take action at a certain state, its action space contains only one action, referred to as a 'null' action. At each time step t: (1) each agent i obtains an observation o \u2208 O\u1d62 and selects an action a \u2208 A\u1d62 based on the policy of agent i, \u03c0\u1d62 : O\u1d62 \u00d7 A\u1d62 \u2192 [0,1]; (2) the agent receives a reward r : S \u00d7 A\u1d62 \u2192 R based on the state and the action; (3) the state is changed according to the transition function T : S \u00d7 A\u2081 \u00d7 ... \u00d7 A\u2099 \u2192 S. For a complete trajectory \u03c4 = ((o\u2080, a\u2080), ..., (o\u209c, a\u209c)) of agent i, there is a reward r received at each time step t. We define the trajectory reward \u03c4 as r\u0302\u1d62(\u03c4) = \u03a3\u209cr. The expected trajectory reward of player i with strategy \u03c0\u1d62 and opponents with strategy \u03c0\u208b\u1d62 is defined as r\u1d62(\u03c0\u208b\u1d62, \u03c0\u1d62) = E\u03c4~(\u03c0\u208b\u1d62,\u03c0\u1d62) [r\u0302\u1d62(\u03c4)], in which \u03c4 ~ (\u03c0\u208b\u1d62, \u03c0\u1d62) denote that \u03c4 is generated with agents using strategy (\u03c0\u208b\u1d62, \u03c0\u1d62).\nBest Response, Exploitability, and Nash Equilibrium. We use r\u1d62(\u03c0\u208b\u1d62, \u03c0\u1d62) to specify the player strategy \u03c0\u1d62 and opponent strategy \u03c0\u208b\u1d62. The best response of opponent strategy \u03c0\u208b\u1d62 is defined as BR(\u03c0\u208b\u1d62) = argmax\u03c0\u1d62 r\u1d62(\u03c0\u208b\u1d62, \u03c0\u1d62), which refers to the strategy of player i that maximizes player i's reward. We additionally define the best response of strategy \u03c0\u1d62 as BR(\u03c0\u1d62) = argmax\u03c0'\u208b\u1d62 \u03a3j\u2208P,j\u2260i rj(\u03c0'\u208b\u1d62, \u03c0\u1d62), which equals to argmin\u03c0'\u208b\u1d62 r\u1d62(\u03c0'\u208b\u1d62, \u03c0\u1d62) in the zero-sum case.\nBR(\u03c0\u1d62) refers to the strategy of all the other players except player i that maximizes their trajectory reward, which is equivalent to minimizing player i's payoff in zero-sum games. Let P be the set of all the players in the game, and the strategy \u03c0 = (\u03c0\u1d62)\u1d62\u2208P be the strategy of all the players. We define the exploitability of strategy \u03c0 as E(\u03c0) = \u03a3\u1d62\u2208P (r\u1d62(\u03c0\u208b\u1d62, BR(\u03c0\u208b\u1d62)) \u2212 r\u1d62(\u03c0)), which reflects the extent to which the strategy can be exploited. In zero-sum symmetric cases, we define the exploitability of a player strategy \u03c0\u1d62 as E(\u03c0\u1d62) = \u2212r\u1d62(BR(\u03c0\u1d62), \u03c0\u1d62) = \u03a3j\u2208P,j\u2260i rj(BR(\u03c0\u1d62), \u03c0\u1d62). A strategy \u03c0\u1d62 is \u025b-Nash equilibrium if E(\u03c0\u1d62) < \u03b5."}, {"title": "Problem Formulation", "content": "Consider a multi-player competitive zero-sum game, and we have a dataset of game histories that include the trajectories of each player. The trajectories are generated by diverse players, ranging from high-level experts to amateurs. We aim to extract strategy representations from trajectories, distinguish the players with different levels, and learn an expert policy from the dataset via imitation learning. We assume that we do not have the identifications of the players. In our problem, we collect a set \u0393 of trajectories \u03c4 = ((o\u2080, a\u2080), ..., (o\u209c, a\u209c)) from different games and demonstrators. The trajectory reward for a subset \u0393'\u2286\u0393 is available for exploited level estimation. We assume that the strategy of a player is consistent within a single trajectory."}, {"title": "Learning Strategy Representation", "content": "Identifying the strategy of a player is essential to evaluating their skill level. However, this becomes challenging when player identification is unavailable in the dataset because the strategy of the player changes according to their opponent within each episode. Therefore, we propose a Partially-trainable-conditioned Variational Recurrent Neural Network (P-VRNN) featuring a strategy representation that is learnable and remains constant throughout the trajectory. Strategy representation becomes the optimal representation for each trajectory by training it to minimize the P-VRNN loss.\nThe P-VRNN models the player's decision-making process and includes four major components similar to the original VRNN, as shown in Figure 2. To disentangle the strategy of the opponent player from the strategy representation, we consider the observation as a conditional variable. We define p as the generative model and q as the inference model."}, {"title": "Generation", "content": "Based on the dependency of our P-VRNN model, We can model the decision-making process as follows:\n$$p(a_{\\leq T}, z_{\\leq T}|o_{\\leq t},l) = \\prod_{t=1}^{T} p(a_t|z_{\\leq t}, a_{<t}, o_{<t}, l) p(z_t|a_{<t}, z_{<t}, o_{<t}, l)$$\nWithout knowing the action $a_t$, the prior distribution of latent variable $z_t$ can be derived from the past actions $a_{<t}$, past latent variables $z_{<t}$, observations $o_{<t}$, and strategy representation $l$. The computation graph of the P-VRNN shows that the recurrent variable $h_{t-1}$ integrates the past actions $a_{<t}$, latent variables $z_{<t}$, and observations $o_{<t}$. Therefore, with the assumption of a Gaussian distribution for the prior, the sampling process of $z_t$ is influenced by $h_{t-1}$, the current observation $o_t$, and the strategy representation $l$ as follows:\n$$z_t | h_{t-1}, o_t, l \\sim N(\\mu_{pri,t}, diag(\\sigma_{pri,t}^2)), [\\mu_{pri,t}, \\sigma_{pri,t}] = pri(h_{t-1}, o_t, l),$$\nwhere $pri$ is a prior network. We also follow the convention in VAE and assume that the latent variable has a diagonal covariance matrix.\nThe generation process is obtaining action $a_t$ from the latent variables $z_{<t}$, past actions $a_{<t}$, observations $o_{<t}$, and strategy representation $l$ just same as to the decision-making process of the player. By substituting the past information using recurrent variable $h_{t-1}$, action generation is defined as follows:\n$$a_t | h_{t-1}, z_t, o_t, l \\sim Cat(\\mu_{dec,t}), \\mu_{dec, t} = dec(h_{t-1}, z_t, o_t, l),$$\nwhere $Cat$ stands for categorical distribution and $dec$ is a decoder network.\nThe recurrent unit takes in all the variables of the current step and the recurrent variable of the previous step, which includes all the past information. At each time step, $h_t$ is updated as follows:\n$$h_t = rec(h_{t-1}, a_t, z_t, o_t, l),$$\nwhere $rec$ is a recurrent network."}, {"title": "Inference", "content": "Approximate posterior inference is modeled as follows:\n$$q(z_{\\leq T}|a_{\\leq T}, o_{\\leq T}, l) = \\prod_{t=1}^{T} q(z_t|a_{\\leq t}, z_{<t}, o_{\\leq t}, l).$$\nThe latent variable $z_t$ is obtained from the actions $a_{<t}$, past latent variables $z_{<t}$, observations $o_{<t}$, and strategy representation $l$. Like the prior and action generation, we replace the past information with a recurrent variable of the previous step, $h_{t-1}$. Therefore, the approximate posterior distribution is defined as follows:\n$$z_t | h_{t-1}, a_t, o_t,l \\sim N(\\mu_{enc,t}, diag((\\sigma_{enc,t})^2)), [\\mu_{enc,t}, \\sigma_{enc,t}] = \\enc(h_{t-1}, a_t, o_t, l),$$\nwhere $enc$ is a encoder network."}, {"title": "Learning", "content": "Similar to [9], the loss function of P-VRNN is a negative of the variational lower bound, using Equations (1) and (5), as follows:\n$$L=E_{q(z_{\\leq T}/a_{\\leq t},o_{\\leq t},l)} [\\sum_{t=1}^{T} L_{Recon,t} + L_{Reg,t}]$$\nThe reconstruction loss for each timestep, which evaluates how well the generated action aligns with the original action, is formulated as follows:\n$$L_{Recon,t} = -log p_o(a_t|z_{<t}, a_{<t}, o_{<t}, l).$$\nThe regularization loss for each timestep, which measures the divergence between the posterior and prior distributions, is formulated as follows:\n$$L_{Reg,t} = KL(q(z_t|a_{<t}, z_{<t}, o_{<t}, l) || p_o (z_t|a_{<t}, z_{<t}, o_{<t}, l))$$\nAt the beginning of the training, the strategy representation $l$, which is a trainable variable, is randomly initialized for each trajectory T. The condition part of P-VRNN consists of an observation o\u209c that changes over time and the strategy representation $l$, which is consistent during the whole trajectory and trainable. During the training, all the $l$ are optimized together with the parameters of $\u03c6_{pri}$, $\u03c6_{enc}$, $\u03c6_{dec}$, and $\u03c6_{rec}$ to minimize the loss function described in Equation (7). While the networks are trained to minimize the loss across all trajectories, the strategy representations are individually optimized for each trajectory to provide customized guidance and insights. Consequently, the strategy representation $l$ should be adjusted to more effectively capture and express the unique strategies of each trajectory. It is important to note that the process of deriving $l$ is conducted unsupervised, without needing player identification, ensuring privacy and generalizability."}, {"title": "Indicators for Imitation Learning", "content": "Utilizing the learned P-VRNN and the strategy representation for each trajectory, we propose the RI and EL indicators."}, {"title": "Randomness Indicator (RI)", "content": "Given the well-trained P-VRNN and the strategy representation dataset, we can evaluate the reconstruction loss and regularization loss for each trajectory. The regularization loss shows the capability of the posterior to approximate the prior, which reflects the performance of extracting the information of the next action from the past information, observation, and strategy representation. In the process of P-VRNN training, the regularization loss of each trajectory is gradually optimized to a very small value close to 0. However, the reconstruction loss typically cannot be so small since the action decoder gives a probability distribution over actions, and players usually do not act deterministically. For a well-trained P-VRNN, the predicted action distribution closely matches the true probability distribution of the corresponding strategy of the trajectory. So if there are $n$ possible actions $a_{t,1}, a_{t, 2}, ..., a_{t,n_t}$ for $a_t$, we can approximately calculate the expectation of the one-step reconstruction loss as\n$$E_{p_o(a_t|z_{\\leq t},a_{<t},o_{<t},l)} [L_{Recon,t}] = \\sum_{i=1}^{n_t} -p_o(a_{t,i}|z_{<t}, a_{<t}, o_{<t}, l) log p_o(a_{t,i}|z_{<t}, a_{<t}, o_{<t}, l) = H (p_o(a_t | z_{<t}, a_{<t}, o_{<t}, l)) .$$\nIt is the entropy of $p_o(a_t | z_{<t}, a_{<t}, o_{<t},l)$, which reflects the randomness of the player with strategy representation $l$, given $z_{<t}, a_{<t}$, and $o_{<t}$. Following the hypothesis of Beliaev et al. [3], a strategy with more randomness is considered worse. Since we have the whole trajectory with a unified strategy representation $l$, we can define the RI of a trajectory as its cumulative reconstruction loss:\n$$RI(T) = \\sum_{t=1}^{T}H (p_o(a_t|z_{\\leq t}, a_{<t}, o_{\\leq t}, l)) .$$\nWe highlight that the RI does not require any reward information, and the whole procedure is fully unsupervised."}, {"title": "Exploited Level (EL)", "content": "If we can access the trajectory rewards of select trajectories, we can determine to what extent the strategy of each trajectory in the offline dataset is exploited by utilizing the geometric structure of the strategy representation space. The key insight of this approach is that the trajectories with similar strategy representations tend to exhibit similar strategies. We define measure $d_\u03c0$ on the strategy space \u03a0, and assume that the strategy of agents generating dataset \u0393 and its subset \u0393' are both sampled according to $d\u03c0$. Denote a trajectory as \u03c4 and the representation function mapping trajectories to learned representations as $f(\u03c4)$. We remark that a trajectory \u03c4 should be mapped to a probability distribution of strategies such that \u222b\u03c0\u2208\u03a0 \u03c4(\u03c0)d\u03c0 = 1, where \u03c4(\u03c0) is the probability of using strategy \u03c0 when having trajectory \u03c4, instead of a single strategy. But we can view the mixture of \u03c0 with probability \u03c4(\u03c0) as a single mixed strategy \u222b\u03c0\u2208\u03a0 \u03c4(\u03c0)d\u03c0, so we can still use notation \u03c0(\u03c4) to represent the strategy of \u03c4. We define the EL as follows:\n$$EL(T) = E_\u03c0 [-r(\u03c0, \u03c0(\u03c4)) | r(\u03c0, \u03c0(\u03c4)) \\leq 0]$$\n$$\\frac{\\int_{\\pi \\in \\Pi} (-r(\\pi, \\pi(\\tau)))_+ d \\pi}{\\int_{\\pi \\in \\Pi} 1_{r(\\pi,\\pi(\\tau))\\leq 0}d \\pi},$$\nwhere $r(\u03c0, \u03c0(\u03c4))$ returns the expected trajectory reward of a player with strategy \u03c0(\u03c4) by default, $(x)_+ = max \\{x, 0\\}$, and $1_c = 1$ if and only if condition c is satisfied, otherwise $1_c = 0$. EL(\u03c4) is the negative of the expectation of the trajectory reward less than 0 when played with the demonstrators who generate the offline dataset. This value can reflect the extent to which the demonstrators exploit the strategy of \u03c4. The reason why EL is chosen as an indicator is discussed in detail in the Appendix. To estimate EL with latent representation space structure, we provide an alternative definition of EL\u03b4:\n$$EL_\\delta(T) = \\frac{\\sum_{d(f(T),f(T'))<\\delta}(-r(\\pi, \\pi(\\tau')))_+}{\\sum_{d(f(T), f(T'))<\\delta}1_{r(\\pi, \\pi(\\tau'))\\leq 0}},$$"}, {"title": "Filtered Imitation Learning", "content": "The last step of the STRIL is to filter the offline dataset with a chosen percentile $p$ of an indicator. The indicator $I$ can be any mapping from the trajectories to real numbers such as RI or EL. Specifically, for an indicator $I(\u03c4)$, the offline dataset \u0393 is filtered into \u0393\u209a = {\u03c4 \u2208 \u0393 | I(\u03c4) < I\u209a}, where I\u209a satisfies that P\u1d63[I(\u03c4) < I\u209a] = p. After filtering the dataset, the original IL algorithm is employed. For IL algorithms that directly define loss function over target function and trajectories, the new loss function can be explicitly written as\n$$L_p(\\pi) = E_{\u03c4} [1_{I(\u03c4)<I_p} \\cdot L_{IL}(\\pi,\u03c4)],$$\nwhere $L_{IL}(\u03c0, \u03c4)$ is the loss function of the IL algorithm. As an example, $L_{IL}(\u03c0, \u03c4) = \\sum_{t=0}^{T} log \u03c0(a_t | o_t)$ in vanilla BC algorithm. As the value of $p$ closer to 0, more data is filtered out; conversely, setting $p$ to 1 filters none of the data, reverting STRIL to the original IL algorithm."}, {"title": "Experiments", "content": "Experiment Settings\nWe validate the effectiveness of our approach using two-player zero-sum games, including Two-player Pong, Limit Texas Hold'em [34], and Connect Four [31]. Details of each environment are provided in the Appendix.\nDataset generation. We employ different methods to create training datasets with diverse demonstrators for the environments. For Two-player Pong and Connect Four, we use self-play with opponent sampling [2] with the Proximal Policy Optimization (PPO) algorithm [29]. For Limit Texas Hold'em, we use neural fictitious self-play [17] with Deep Q-network (DQN) algorithm [25] to generate expert policies, given its complexity and the need to adapt to various opponents. Behavior models are then selected from multiple intermediate checkpoints to generate the offline data. We assume that only 5% of the dataset is reward-labeled for EL estimation. Details on the generation of demonstrators, their performance, and the dataset are provided in the Appendix.\nEvaluation metrics. We evaluate our method across three environments to demonstrate the effectiveness of the learned strategy representation in STRIL using estimated indicators. In a zero-sum game, evaluating policy performance involves ensuring the policy is not vulnerable to exploitation by a specific strategy. In order to capture the worst-case scenario against opponent strategies in the dataset, we evaluate the performance of the imitative model, \u03c0\u1d62, using the Worst Score (WS) over the demonstrator set, I:\n$$WS(I, \\pi_i) = \\min_{j\\in I}r_i(\\pi_j, \\pi_i),$$\nwhere $r_i(\\pi_j, \\pi_i)$ represents the trajectory reward of i against j. For Two-player Pong and Connect Four, we calculate the reward using the formula $(N_{win} - N_{lose})/N_{game}$, where $N_{win}$, $N_{lose}$, and $N_{game}$ represent the number of wins, losses, and total games, respectively. For Limit Texas Hold'em, where a player can win by varying margins depending on the game, we determine the reward as the average difference between the total chips won and lost. We set $N_{game}$ to 2,000."}, {"title": "Strategy Representation with Indicators", "content": "In this subsection, we visualize the learned strategy representations using multiple labels. If the latent space exceeds two dimensions, it is initially reduced to two dimensions using PCA. These reduced representations are then color-coded based on different labels: player ID, RI, EL, and trajectory reward. Note that player ID and trajectory reward serve as ground truth references while RI and EL are estimated. Instead of using the exact values, we color the percentiles of RI, EL, and trajectory reward. The implementation details for P-VRNN are provided in the Appendix.\nTwo-player Pong. As shown in Figure 3a, strategy representations of each demonstrator naturally cluster together in the Two-player Pong environment. Figure 3d demonstrates that trajectory rewards only partially align with player strategies due to performance variability depending on the opponent's strategy, a common characteristic of competitive games. Players 1 and 8, exhibiting the worst and best performances, respectively, form clusters with consistent values independent of the opponent. Figure 3b illustrates that the RI highlights players 5, 6, and 8 as excelling in reconstruction tasks. Additionally, Figure 3c shows that the dominant players, 4 and 8, have strategies that are least susceptible to exploitation, signifying more robust performance. Additionally, it is observed that the most expansive cluster with the lowest density has the highest EL, suggesting that the least trained strategy exhibits unstable behavior and is the most vulnerable one to exploitation. These indicators establish a strong standard for data filtering in subsequent IL applications from two different perspectives, both differentiating between dominant and dominated strategies.\nLimit Texas Hold'em & Connect Four. In Limit Texas Hold'em, there are seven players: two experts, three mid-level players, and two novice players. Figure 3e demonstrates that the learned representations are well separated and ordered according to their expertise levels. Figure 3h illustrates that while trajectory rewards can effectively identify very poor strategies, they fail to consistently differentiate among more effective strategies, as the rewards vary across different opponents. However, Figures 3f and 3g show that our proposed indicators not only perfectly distinguish the dominant strategies but also rank them accurately. In Connect Four, Figure 3i shows dominant player strategies on the right and dominated player strategies on the left. In contrast to the trajectory rewards which are inconsistent within the same strategy, our RI and EL patterns show a strong capability to extract characteristics and assess the performance of these strategies."}, {"title": "Learning from Offline Dataset", "content": "To evaluate the STRIL, we considered three IL algorithms. First, we employed Behavior Cloning (BC), a basic IL algorithm. Next, we used IQ-Learn [14], an advanced imitative algorithm. Finally, we implemented ILEED [3], a state-of-the-art method capable of handling a diverse range of demonstrator data. In our evaluation, we excluded methods that rely on online interactions (e.g., GAIL [18]) or necessitate interactions with experts (e.g., DAgger [27]) in offline learning approaches. We applied STRIL to each algorithm to evaluate its performance enhancement. Note that all the experiments were repeated three times.\nGeneral results. In Table 1, we compared the WS of four types of data filtering methods. A hyperparameter search was conducted to identify the appropriate percentile, p, of indicators for each model and environment. Note that all experiments were repeated three times, and the results are reported with error bars. The original ILEED, which considers the expertise level of the data, generally performs better than other original algorithms on average. For the filtering method, the RI and EL enhance the performance of the original methods in most cases. In some instances, their performance is even comparable to the Best method. In the case of Two-player Pong, the EL method outperforms the RI method because EL more accurately distinguishes the dominant strategy. Additionally, in Limit Texas Hold'em, both RI and EL show similar performance, which aligns with the similar qualitative results observed in the strategy space. However, the RI and EL methods did not improve ILEED on Connect Four because the dataset aligns well with the assumption of ILEED. Consequently, filtering the data according to randomness is equivalent to reducing valid data, which results in worse performance. A detailed analysis of the dataset leading to this result is provided in the Appendix.\nSensitivity analysis. Figure 4 shows the performance for each IL algorithm across different percentile values for each indicator. For the BC and IQ-Learn algorithms, the RI and EL methods provide improved performance in all the cases. Although ILEED is designed to learn from diverse demonstrators, the RI and EL methods can be effectively used in some environments because ILEED struggles to distinguish the dominant policy in a multi-agent environment. For the EL method, due to the significant decrease in the size of the filtered dataset, a drop in performance from p = 0.1 to p = 0.05 is commonly observed. In contrast, in the range of p \u2265 0.1, the overall performance is enhanced as p decreases. EL is a reliable indicator since it has a few reward-labeled data as anchors, while RI solely takes estimated randomness as evaluation metrics. The result of the RI method across different p's shows less stable behavior, as the optimal results are achieved on p = 0.4 or p = 0.1 in different game scenarios. However, choosing a relatively small p for an unknown dataset is a preferred option since the most proficient demonstrators usually have the most stable strategies."}, {"title": "Conclusion", "content": "In this work, we proposed an effective framework, STRIL, to extract the representations of the offline trajectories and enhance imitation learning methods in multi-agent games. We designed a P-VRNN network, which shows extraordinary results in learning the strategy representations of trajectories without requiring player identification. We then defined two indicators, RI and EL, for imitation learning. We can estimate RI and EL by utilizing the strategy representation and subsequently filter the offline dataset with the indicators. The imitation learning algorithms show significant performance improvements with the filtered datasets. The limitations and broader impacts are provided in the Appendix.\nIn future work, we plan to utilize the P-VRNN as a customized behavior prediction model and explore the geometry of the strategy representation space. Additionally, we aim to develop IL methods that integrate the indicators beyond simply filtering the dataset."}, {"title": "Two-player Pong", "content": "Each player controls a paddle on one side of the screen. The goal is to keep the ball in play by moving the paddles up or down to hit it. If a player misses hitting the ball with their paddle, it loses the game. The observation of players includes ball and paddle positions across two consecutive time steps and potential actions include moving up or down."}, {"title": "Limit Texas Hold'em", "content": "Players start with two private hole cards, and five community cards are revealed in each stage (the flop, turn, and river). Each player has to create the best five-card hand using a combination of their hole and the community cards. During the four rounds, players can select call, check, raise, or fold. The players aim to win the game by accumulating chips through strategic betting and building strong poker hands. The observation of players is a 72-element vector, with the first 52 elements representing cards (hole cards and community cards) and the last 20 elements tracking the betting history in four rounds."}, {"title": "Randomness Indicator (RI)", "content": "Given the well-trained P-VRNN and the strategy representation dataset, we can evaluate the reconstruction loss and regularization loss for each trajectory. The regularization loss shows the capability of the posterior to approximate the prior, which reflects the performance of extracting the information of the next action from the past information, observation, and strategy representation. In the process of P-VRNN training, the regularization loss of each trajectory is gradually optimized to a very small value close to 0. However, the reconstruction loss typically cannot be so small since the action decoder gives a probability distribution over actions, and players usually do not act deterministically. For a well-trained P-VRNN, the predicted action distribution closely matches the true probability distribution of the corresponding strategy of the trajectory. So if there are $n$ possible actions $a_{t,1}, a_{t, 2}, ..., a_{t,n_t}$ for $a_t$, we can approximately calculate the expectation of the one-step reconstruction loss as\n$$E_{p_o(a_t|z_{\\leq t},a_{<t},o_{<t},l)} [L_{Recon,t}] = \\sum_{i=1}^{n_t} -p_o(a_{t,i}|z_{<t}, a_{<t}, o_{<t}, l) log p_o(a_{t,i}|z_{<t}, a_{<t}, o_{<t}, l) = H (p_o(a_t | z_{<t}, a_{<t}, o_{<t}, l)) .$$\nIt is the entropy of $p_o(a_t | z_{<t}, a_{<t}, o_{<t},l)$, which reflects the randomness of the player with strategy representation $l$, given $z_{<t}, a_{<t}$, and $o_{<t}$. Following the hypothesis of Beliaev et al. [3], a strategy with more randomness is considered worse. Since we have the whole trajectory with a unified strategy representation $l$, we can define the RI of a trajectory as its cumulative reconstruction loss:\n$$RI(T) = \\sum_{t=1}^{T}H (p_o(a_t|z_{\\leq t}, a_{<t}, o_{\\leq t}, l)) .$$\nWe highlight that the RI does not require any reward information, and the whole procedure is fully unsupervised."}, {"title": "Exploited Level (EL)", "content": "If we can access the trajectory rewards of select trajectories", "follows": "n$$EL(T) = E_\u03c0 [-r(\u03c0, \u03c0(\u03c4)) | r(\u03c0, \u03c0(\u03c4)) \\leq 0"}]}