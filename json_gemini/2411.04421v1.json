{"title": "Variational Low-Rank Adaptation Using IVON", "authors": ["Bai Cong", "Nico Daheim", "Yuesong Shen", "Daniel Cremers", "Rio Yokota", "Mohammad Emtiyaz Khan", "Thomas M\u00f6llenhoff"], "abstract": "We show that variational learning can significantly improve the accuracy and\ncalibration of Low-Rank Adaptation (LoRA) without a substantial increase in the\ncost. We replace AdamW by the Improved Variational Online Newton (IVON)\nalgorithm to finetune large language models. For Llama-2 with 7 billion parameters,\nIVON improves the accuracy over AdamW by 2.8% and expected calibration error\nby 4.6%. The accuracy is also better than the other Bayesian alternatives, yet\nthe cost is lower and the implementation is easier. Our work provides additional\nevidence for the effectiveness of IVON for large language models. The code is\navailable at https://github.com/team-approx-bayes/ivon-lora.", "sections": [{"title": "Introduction", "content": "Bayesian methods are expected to improve the accuracy and calibration performance of Large\nLanguage Models (LLMs) on downstream tasks, but they rarely succeed at such massive scale and, even when they do, often there is a substantial cost to pay. This is certainly true for finetuning with\nLow-Rank Adaptation [10], where many Bayesian variants have recently been proposed but they\nall require additional computations compared to standard finetuning practices. For example, the\nSWAG-LORA method [17] needs additional computation to obtain an estimation of the posterior.\nLORA ensemble [22] requires multiple LoRA checkpoints to be trained. Methods such as Laplace-LORA [24] require an additional pass through the data to compute a Hessian or Fisher approximation.\nIt is then natural to ask whether it is ever possible to use Bayes to improve LoRA without such overheads and increase in the costs.\nHere, we show that the variational (Bayesian) learning can significantly improve both the accuracy and calibration of LoRA finetuning without a substantial increase in the cost. Our proposal is to simply replace the standard optimizers like AdamW by a variational learning algorithm called the Improved\nVariational Online Newton (IVON) algorithm [19]. IVON uses a nearly identical implementation\nas AdamW and the swap requires just a few lines of code change. The main advantage of IVON\nis that its scale vector, used for the learning rate adaptation, also yields an estimate of posterior variance for free. The only minor overhead is due to sampling from the posterior but we show that\nthis cost is negligible in practice (approximately 1% of the total training time). We achieve significant\nimprovements in performance when finetuning the Llama-2 model with 7 billion parameters on a\nrange of commonsense reasoning tasks: accuracy increases by 2.8% while expected calibration error\n(ECE) decreases by 4.6%. The accuracy is also better than the other Bayesian alternatives, yet the\ncost is much lower and the implementation is easier. Our work provides additional evidence for the\nwork of Shen et al. [19], showing effectiveness of IVON for large deep networks."}, {"title": "Variational low-rank adaptation using IVON", "content": "We will introduce our approach that we call IVON-LORA. The idea is simple: we replace the standard\nAdamW optimizer by IVON which optimizes a variational-Bayesian objective. In other words, we\nswitch the standard objective used by AdamW to a variational one. More formally, let us denote\nthe AdamW objective by $l(\\theta)$ where $\\theta$ is the vector containing all the entries of LoRA's low-rank\nparameters (often denoted by A and B). The variational learning minimizes a different objective\nwhere an expectation of $l(\\theta)$ over a distribution $q(\\theta)$ is used (shown on the right),\n$\\min_\\theta l(\\theta) \\quad \\text{versus} \\quad \\min_{q(\\theta)} E_{q(\\theta)} [l(\\theta)] + \\frac{\\lambda}{N} D_{KL}[q(\\theta) ||p(\\theta)].$ (1)\nIVON uses a Gaussian $q(\\theta) = \\mathcal{N}(m, \\text{diag}(v))$. The mean m plays a similar role to $\\theta$ obtained by\nAdamW, while the posterior variance vector v is an additional quantity. The prior $p(\\theta) = \\mathcal{N}(0, v_0 \\mathbf{I})$\nis a zero mean isotropic Gaussian with a scalar variance $v_0$. A scalar weighting parameter $\\lambda$ is used\nto take care of the data size N. This is because $l(\\theta)$ is often an average over the whole dataset.\nTherefore, when using $\\lambda = N$, we target the posterior distribution while with larger values we go\ntowards a \u201ccolder\" posterior [26, 8].\nDespite such differences in the objectives, the implementation of IVON is nearly identical to AdamW\nwhich makes the replacement easy and can be done by just a few lines of code change. The key\npoint is that estimation of v is automatically done through the scale vector that adapts the learning\nrate. Therefore, posterior variances are obtained for free. The only additional step is to sample\n$\\theta \\sim \\mathcal{N}(m, \\text{diag}(v))$ to evaluate the expectation in Eq. 1, but its overhead can be reduced by using\none Monte-Carlo sample per iteration. For the details, we refer to the original IVON paper by Shen\net al. [19]. Overall, IVON is a promising alternative to the existing Bayesian approaches that require\nadditional overheads due to either post-processing or extra training runs."}, {"title": "Experiments", "content": "To evaluate the effectiveness of the proposed method, we use IVON to finetune a pretrained Llama-2\nmodel with 7 billion parameters [20] on six datasets with commonsense reasoning multiple-choice\nor true/false questions. These six datasets are WinoGrande-Small (WG-S), WinoGrande-Medium\n(WG-M) [18], ARC-Challenge (ARC-C), ARC-Easy (ARC-E) [4], OpenBookQA (OBQA) [15],\nand BoolQ [3]. We evaluate the performance of the trained LoRA adapters by calculating the\naccuracy and Expected Calibration Error (ECE) on the test set. We also use test Negative Log-Likelihood (NLL) and Brier score because ECE can be sometimes unreliable [1]. As for the baseline\nmethods, we compare the performance of IVON-LORA adapters with LoRA adapters trained using\nAdamW. We also consider other Bayesian alternatives, including Monte Carlo Dropout (MC Dropout)\n[6], Laplace Approximation (LA) [24], Stochastic Weight Averaging (SWA) [11, 17], and SWA-Gaussian (SWAG) [13, 17].\nIVON is evaluated in two ways at test time: first, by using the prediction just at the mean m, and\nsecond, by using an averaged prediction over 10 samples from the posterior distribution. The two\nmethods are referred to as \u2018IVON@mean' and 'IVON', respectively. For a fair comparison, we use\nthe same number of samples for MC Dropout and SWAG.\nThe results are summarized in Table 1. First, we observe that IVON, as an alternative to AdamW,\nsignificantly improves the generalization of LoRA finetuning. When evaluated at the mean, IVON\noutperforms AdamW finetuning and other Bayesian adaptations of LoRA on all datasets in terms\nof accuracy, often by a large margin. We also observe that IVON exhibits improved calibration\ncompared to AdamW and MC Dropout, as indicated by the lower ECE, NLL and Brier values.\nNext, we observe that ensembling with samples from IVON's posterior distribution further improves\ncalibration. When evaluate at an ensemble of 10 samples drawn from the posterior distribution,\nIVON outperforms all other methods and is comparable to the best-performing LA (with a Kronecker-factored Hessian) and SWAG on ECE, NLL and Brier. Notably, IVON achieves this despite using\na diagonal Hessian and without an additional pass through the data for computing Hessians at the\nconverged point as in Laplace methods. With this improvement in calibration, IVON still maintains\ncomparable or better accuracy over other methods."}, {"title": "Discussion", "content": "Our direct variational learning approach using IVON effectively improves calibration and accuracy in\nLORA finetuning. Given the strong results, we hope that this work invigorates research in variational\nmethods for LLMs. Reasons for IVON's success are not fully understood, but one hypothesis is the\nprevention of overfitting as the finetuning datasets are often comparably small. This may be attributed\nto the preference for simpler solutions (flatter minima) which is inherent in variational learning [9, 7].\nIn a broader context, several recent works consider related approaches to improve language model\nfinetuning. Following a PAC-Bayesian framework, Liu et al. [12] proposes to finetune the full model\nusing perturbed gradient descent. Chen and Garner [2] uses variational learning to estimate parameter"}]}