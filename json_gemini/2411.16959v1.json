{"title": "RoCoDA: Counterfactual Data Augmentation for Data-Efficient Robot Learning from Demonstrations", "authors": ["Ezra Ameperosa", "Jeremy A. Collins", "Mrinal Jain", "Animesh Garg"], "abstract": "Imitation learning in robotics faces significant challenges in generalization due to the complexity of robotic environments and the high cost of data collection. We introduce RoCoDA, a novel method that unifies the concepts of invariance, equivariance, and causality within a single framework to enhance data augmentation for imitation learning. RoCoDA leverages causal invariance by modifying task-irrelevant subsets of the environment state without affecting the policy's output. Simultaneously, we exploit SE(3) equivariance by applying rigid body transformations to object poses and adjusting corresponding actions to generate synthetic demonstrations. We validate RoCoDA through extensive experiments on five robotic manipulation tasks, demonstrating improvements in policy performance, generalization, and sample efficiency compared to state-of-the-art data augmentation methods. Our policies exhibit robust generalization to unseen object poses, textures, and the presence of distractors. Furthermore, we observe emergent behavior such as re-grasping, indicating policies trained with RoCoDA possess a deeper understanding of task dynamics. By leveraging invariance, equivariance, and causality, RoCoDA provides a principled approach to data augmentation in imitation learning, bridging the gap between geometric symmetries and causal reasoning.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in imitation learning [1-5] have demonstrated the ability to teach robots complex behaviors by behavior cloning. These methods have shown remarkable success in controlled environments where the training data closely matches the test scenarios. Despite this, current approaches exhibit limited generalization capabilities when deployed in novel environments or on new tasks.\nIn contrast, fields like natural language processing, computer vision, and audio processing have experienced gains in performance and generalization due to the availability of vast quantities of diverse data scraped from the internet.\nRobotics, however, lacks a comparable large-scale, diverse dataset to drive similar breakthroughs. Several attempts have been made to collect large-scale robotics datasets [6-9], but these efforts remain relatively small in comparison to other subfields of AI. An additional challenge is that robotics data does not simply contain static observations, but captures a complex dynamic system where states and actions are causally connected. However, this additional structure also presents an opportunity. The inherent symmetries and causal relationships between robot actions and environment states offer a rich source of information that can be exploited. Recognizing this allows us to focus the model's attention on task-relevant subsets of the state space and generalize across irrelevant factors.\nOur key insight is that the causal relationships inherent in robotics tasks can be harnessed to create more robust, scalable policies. By understanding which components of the task are causally dependent, invariant, or equivariant to the robot's actions, we develop a framework for augmenting training data to enable policy generalization and robustness.\nWe present Counterfactual Data Augmentation for Robot Learning (RoCoDA). Our contributions are as follows:\n1) Unified Framework for Data Augmentation: We formalize the relationship between invariance, equivariance, and causality using group theory and probabilistic graphical models. This framework allows RoCoDA to exploit geometric symmetries and causal structures to produce robust and generalizable policies for complex robotic tasks.\n2) RoCoDA: We introduce a novel data augmentation method that leverages causal invariance, SE(3) equivariance, and visual invariance to improve the robustness, generalization, and performance of behavior cloning policies."}, {"title": "II. RELATED WORK", "content": "Behavior Cloning \u2013 Behavior Cloning (BC) is a fundamental approach in imitation learning where a policy is trained to mimic expert demonstrations by directly mapping states to actions. Several recent BC methods have enabled robots to execute complex tasks by mimicking teleoperation data. [1, 10] classify 3D subgoals for an end-effector to achieve, and [2, 4, 5] predict latent actions using a CVAE, VQ-VAE, and causal convolution, respectively. These methods have proven extremely effective at fitting fine-grained, multi-modal action distributions in their training data.\nDespite their effectiveness on data similar to that in their datasets, BC methods are typically sensitive to distributional shifts, resulting in decreased performance when encountering unseen states or variations in the environment. Our approach addresses this limitation by incorporating causal, invariant, and equivariant augmentations, improving the policy's robustness and generalization capabilities.\nData Augmentation for Robot Policy Learning Data augmentation improves how well robot policies perform, especially when there is limited data or challenges like moving from simulation to the real world. Past work shows that techniques like random crops on images or changing simulation settings help improve policy performance. Random cropping in image observations has shown improvements for both RL and BC methods [3, 11-15]. Domain randomization has been employed to bridge the sim-to-real gap by varying simulation parameters (e.g., texture, friction, lighting) for reinforcement learning algorithms [16]. State-based augmentations, such as adding Gaussian noise [17] improve policy resilience to environmental changes by smoothing the learned state-action space. [13] applied 3D transformations to target object poses and corresponding actions, thereby ensuring that the policy is equivariant to these transformations. Recently, generative models have been used for synthetic data generation as well [18-20].\nCausal Data Augmentation \u2013 Causal data augmentation has been well explored in the context of reinforcement learning [21-23]. CoDA [21] introduces counterfactual data augmentation for reinforcement learning by resampling subsets of the state space while preserving causal dependencies. MoCoDA [22] extends CoDA by incorporating a learned dynamics model, allowing for the handling of overlapping parent sets and enhancing the generation of causally consistent data for RL in more complex environments. Both of these methods focused on sample efficiency in low-dimensional tasks for Offline RL. In contrast, our work enables robustness and generalization for vision-based behavior cloning algorithms on a set of complex, multi-step tasks.\nOur method, RoCoDA, builds upon these techniques by combining geometric equivariance with causal invariance to generate more diverse and causally consistent training data, thereby improving efficiency and generalization."}, {"title": "III. A UNIFIED PERSPECTIVE ON DATA GENERATION", "content": "In this section, we present a unified framework that connects the concepts of invariance, equivariance, and causality, which are central to our method, RoCoDA.\nThe notions of invariance and equivariance are fundamental in understanding the relationship between observation and action in robotics. These concepts are rooted in group theory, which provides a mathematical framework for modeling symmetries and transformations.\nGroups and Symmetries A group G is a set equipped with an operation that satisfies four properties: closure, associativity, identity, and invertibility. Groups are essential for modeling symmetries and transformations in various domains, including robotics.\nInvariance and Equivariance \u2013 f is G-equivariant if, $\\forall x \\in X$ and $g \\in G$, $f(g.x) = g. f(x)$, where denotes the group action of G on the respective spaces. Equivariance ensures that applying a group transformation before or after f yields consistent results under the group action on Y. f is G-invariant if, $\\forall x \\in X$ and $g \\in G$, $f(g. x) = f(x)$. Invariance is a special case of equivariance where the group action on Y is trivial, i.e. $g \\cdot y = y \\forall y \\in Y$ and $g \\in G.\nPolicies as Group Actions \u2013 A policy $\\pi : S \\rightarrow A$ represents a group action when the following conditions hold:\n1) There exists a group G that acts on the state space S via a group action $\\\u00a2_s : G \\times S \\rightarrow S$. This means that for any group element $g \\in G$ and state $s \\in S$, the transformation $g\\\u00a2_s$ is a valid operation in S.\n2) There is a corresponding action of G on the action space A, denoted $\\A : G \\times A \\rightarrow A$. This ensures that applying a group transformation to the state will induce a transformation on the action.\n3) The policy $\\pi$ is equivariant with respect to the group action, meaning that applying a transformation $g \\in G$ to the state induces the same transformation in the action:\n$\\pi(g\\\u00a2_s) = g\\cdot\\pi(s)$, $\\forall g\\in G$, $\\forall s \\in S$\nThis relationship implies that the policy respects the group structure and that actions taken in transformed states are consistent with the transformations applied to the original states. We leverage geometric equivariance by applying random SE(3) (rigid body) transformations to object poses and transforming the corresponding actions, thus generating diverse yet consistent training examples that respect the task's spatial symmetries."}, {"title": "A. Causality: Data Generation with Counterfactuals", "content": "The state input s to a robot policy $\\pi(s)$ can often be decomposed into partitions $s = (s_C,s_I)$, where $s_c$ contains causally relevant variables, and $s_I$ contains causally irrelevant variables with respect to the action a. We can then consider the policy as a function of $s_c$ alone: $\\pi(s) = \\pi(s_C,s_I) = \\pi(s_c)$, implying that the action is invariant to changes in $s_I$. Notably, this independence of $s_C, s_I$ allows us to manipulate $s_I$ without affecting the action $\\pi(s)$. Subsequently, we can generate counterfactual data points, by choosing different $s_I$, without affecting $\\pi(s)$, as long as $s_c$ remains unchanged.\nWe can say that the policy $\\pi(s)$ is causally invariant to $s_c$. In this context, the policy represents a function with some group action G consisting of transformations that only affect causally irrelevant components of the state, while leaving causally relevant components unchanged:\n$\\pi (g\\cdot (s_c, s_I)) = \\pi(s_c)$, $\\forall g\\in G$\nFactored MDPs - We model the environment as a Markov Decision Process (MDP) defined by the tuple (S, A, P), where S is the state space, A is the action space, and P is the transition function. In complex environments with multiple objects, the number of states can be exponential in the number of objects, making learning challenging [22]. To manage this complexity, we consider Factored MDPs (FMDPs), where the state and action spaces are described by sets of variables {$X^i$}, such that $S \\times A = X^1 \\times X^2 \\times \\dots \\times X^n$. Each state variable $X^i$ depends on a subset of variables $Pa(X^i)$ at the previous time step, known as its parents.\nLocal Causal Models In practice, the global factorization assumed by FMDPs is rare. We instead leverage Local Causal Models (LCMs), where the state-action space decomposes into disjoint local neighborhoods {$L_k$}. Each neighborhood $L_k$ is associated with its own transition function $P_{L_k}$ and causal graph $G_{L_k}$. In this framework, if $(s_t, a_t) \\in L_k$, each state variable $X^{i}_{t+1}$ depends on its parents $Pa_{L_k}(X^{i}_{t+1})$ at the previous time step: $X^{i}_{t+1} \\sim P^k_{L_k}(Pa_{L_k}(X^{i}_{t+1}))$. We refer to the tuple $(X^i, Pa_{L_k}(X^{i}), P^k_{L_k})$ as a causal mechanism."}, {"title": "IV. ROCODA: GENERALIZED DATA AUGMENTATION", "content": "RoCoDA aims to improve the generalization and robustness of imitation learning policies by leveraging both the causal structure inherent to robotic tasks and the equivariance of robotic actions to task-relevant objects. A graphical model of our method vs other data augmentation methods is shown in Figure 2. Notably, previous works only perform a single type of augmentation, while we can perform fundamentals-guided simultaneous augmentations across multiple independently varying quantities.\nWe achieve this by augmenting a small dataset in the following manner:\n1) Counterfactual Data Augmentation: We create a causal graph for each subtask within the demonstrations. This allows us to resample and mix subsets of the state space from different trajectories, ensuring causal consistency.\n2) Equivariant Data Augmentation via SE(3) Transformations: We expand the dataset by exploiting the special Euclidean group (SE(3)) equivariance of actions with respect to object poses. This involves applying random transformations (translation + rotation) to object poses and adjusting the corresponding actions accordingly.\n3) Standard Augmentation: We also apply standard data augmentations, which we refer to as Camera Translation Invariance (random crop), Hue Invariance (color jitter), and Observation Perturbation Invariance (state noise), to enhance the diversity of the dataset."}, {"title": "A. Counterfactual Data Augmentation", "content": "We leverage the causal relationships between different entities in the environment to augment our dataset. This involves generating causal graphs for each causal phase (subtask) and resampling subsets of the state space while maintaining causal consistency.\nFor a particular demonstration, let the state of the i-th entity be denoted as $X_i$. The state for the entire system can be represented as $X = (X_1, X_2, ..., X_n)$ where n is the number of entities in the environment. If the entities are causally independent, we can assume that the joint distribution of their states can be factored into independent marginal distributions:\n$P(X) = P(X_1,..., X_n) = \\prod_{i=1}^{n} P(X_i)$\nSimilarly, if causal dependencies exist between $X_1$ and $X_2$ and are independent of all other entities, the joint distribution can be factorized as independent subsets of states:\n$P(X) = P(X_1, X_2) \\prod_{i=3}^{n} P(X_i)$\nNotably, the state can be factored into independent subsets, the data can be augmented by sampling in the product space of individual subsets [21].\nMoreover, we define causal phases in a task trajectory with a heuristic based on invariant transitions. For manipulator arms, changes in the gripper state (i.e., when the gripper opens or closes) result in non-interacting causal phases in the task. This segmentation allows us to construct causal graphs for each subtask. We can define causal graphs for each causal phase, by identifying causally independent entities during each phase. For tasks where multiple agents are present, it is often not practical to define a causal graph for all combinations of causal phases for all agents. Hence, we generate individual causal graphs for each agent, for each causal phase.\nFor each causal phase, we construct an adjacency matrix using the dependencies of state variables across time. We construct a joint causal graph over all state variables as a joint block-diagonal adjacency matrix. Let $A_1$ and $A_2$ be the adjacency matrices for two agents. We compute this joint adjacency matrix as:\n$A = (A_1 \\vee A_2) \\vee (A_1 \\vee A_2)^T$,\nThe aforementioned process partitions the state space into subsets given the causal graph(s) associated with a causal phase. Entities that are causally dependent are placed in the same partition of the state space. For each partition, we sample states from a different trajectory in the dataset in the same causal phase, ensuring that causally dependent entities remain consistent. Independently sampling states from each group creates new states that respect the causal structure of the causal phase. By maintaining causal dependencies, these new states result in valid trajectories that the policy can learn from. In practice, we also use a simulator to sample object states from partitions of the state space, and then render the resulting augmented state before inputting it into an image-conditioned behavior cloning algorithm.\nCounterfactual data augmentation creates synthetic datapoints which are out-of-distribution but follow the same causal structure as data generating distribution."}, {"title": "B. SE(3)-Equivariant Data Augmentation", "content": "We exploit the SE (3) equivariance of robotic actions to object poses. This means that applying a transformation to the object poses can be compensated by a corresponding transformation to the actions, preserving the structure of the task. Formally, let $T_{obj}(s_t)$ be the homogeneous transformation between the pose of a target object in the source dataset and the current pose of the target object, and let $T_{act}(a_t)$ be the corresponding transformation applied to the current action. Then we have $s' = T_{obj}(s_t)$, and $a' = T_{act}(a_t)$.\nLet D be a limited dataset of task demonstrations $D = {T_i}_{i=1}^{n}$, where each trajectory $T_i = {(s_t, a_t)}^{T_i}_{t=1}$ consists of states $s_t$ and actions $a_t$. These trajectories are split into subtasks to generate sub-trajectories, with each subtask being associated with a target object. Sub-trajectories are augmented by randomly generating an unseen pose for the target object, sampling a sub-trajectory from the original dataset, and transforming this sub-trajectory using the homogeneous transformation between the target object and the same object's pose in the original dataset. This preserves the gripper pose from the original dataset with respect to the target object at every time step. Following [13], we add a linear interpolation segment at the start of the augmented sub-trajectory to move the robot into a path that overlaps with the corresponding gripper pose in the sampled original segment. We additionally save states and actions from augmented sub-trajectories that successfully completed the subtask, and discard those that did not succeed.\nWe assume that we can determine whether an augmented trajectory successfully completes a subtask, in line with prior work [13]. Trajectories that do not result in successful completion are discarded, ensuring that the policy learns from valid demonstrations."}, {"title": "C. Standard Data Augmentation", "content": "Following prior work, we apply traditional data augmentation techniques to the observations to enhance robustness:\nCamera Translation Invariance Random cropping has found success in training image-based methods [3, 11-15]. We use random resizing and cropping to encourage translation and scale invariance.\nLighting and Hue Invariance \u2013 We use color jitter to make the model robust to lighting conditions and object colors. Note that this cannot be used when colors are relevant to the task, such as in the Stack Three task.\nObservation Noise Invariance - Inspired by [17], we find that adding Gaussian noise to observation enhances performance and improves model robustness."}, {"title": "V. RESULTS", "content": "Through our experiments, we aim to answer the following questions:\nRQ1: How does encoding causal dependencies between states and actions enhance policy performance compared to models that assume i.i.d. data?\nRQ2: What is the impact of counterfactual data augmentation on the generalization capabilities of behavior cloning algorithms?\nRQ3: Does counterfactual data augmentation improve sample efficiency in policy learning compared to existing approaches?\nRQ4: What emergent behaviors arise when policies are constrained to focus solely on task-relevant information?"}, {"title": "A. Experimental Setup", "content": "We evaluate our method, RoCoDA, on a variety of multi-step robotic tasks using Action Chunking Transformer (ACT) [2] as our base imitation learning algorithm. ACT is an encoder-decoder transformer model that learns to autoregressively output latent actions generated by a conditional variational autoencoder (CVAE).\nRemark. Although prior work has shown that multi-task imitation learning can be achieved by scaling data and selecting appropriate algorithms [6, 7, 24, 25], we focus on the single-task setting for each model in this work. This enables us to isolate the effects of our data augmentation algorithm on individual tasks without confounding factors introduced by multi-task learning."}, {"title": "1. Tasks", "content": "We evaluate our method on the following tasks:\nThree Block Stack: The robot must stack three distinct colored blocks in a predefined sequence based on their color.\nThree-Piece Assembly: The robot assembles three pieces by stacking them in a specific orientation.\nCoffee Task: The task involves placing a coffee pod into a coffee machine and closing the lid.\nTransport: This task involves two robotic arms. The left arm removes a lid and picks up a hammer, while the right arm moves a cube from a back bin to a front bin, takes the hammer from the left arm, and places it in another bin.\nLibero-Object: We use a task from the Libero-Object dataset where the robot must pick up a specific object (e.g., tomato sauce) and place it in a basket."}, {"title": "2. Baselines", "content": "We use ACT, a popular BC baseline [2]. We denote ACT vanilla as trained on unaugmented datasets. Further, we use a state-of-the-art augmentation schema MimicGen [13], that uses SE(3) transformations for data augmentation. We further augment the data with random cropping, as was done in the original work."}, {"title": "B. Experiments", "content": "1. Policy Performance -To address RQ1, we compare the performance of RoCoDA to baselines on the Three Block Stack, Three-Piece Assembly, and Coffee Preparation tasks. Additionally, we perform ablations by removing components of RoCoDA to assess their individual contributions.\nRoCoDA consistently outperforms baseline methods across all three tasks. The performance gap is largest for Three-Piece Assembly, where RoCoDA nearly doubles the performance of MimicGen. In the Three Block Stack task, RoCoDA benefits from equivariant, causal, and visual augmentation in a compositional manner. Note that we omit color jitter from RoCoDA for this experiment since the task is causally dependent on the color of the blocks.\nThis suggests that counterfactual augmentation benefits most from complex tasks consisting of multiple substeps. This is consistent with the fact that in the Coffee task, performance is roughly even amongst all baselines and method variants utilizing equivariant data augmentation. RoCoDA does not seem to benefit from visual augmentation for these tasks, however. This may be because this task demands precision, and there is a trade-off between robustness and accuracy, as demonstrated in [26]. In essence, because visual augmentation demands that the BC policy be robust to a span of translations in the camera plane, image scales, and color variations, the model has less capacity to fit the action distribution, and thus the policy loses fidelity.\nWe further investigate the performance of individual augmentations on the transport environment (Table II). We show that counterfactual data augmentation and random resize and crop achieve 71% for single camera view and and 94% accuracy for multi-view cameras, outperforming other augmentations. Notably, in simpler tasks, visual augmentations like resize and crop may yield high success rates because they provide sufficient variability for training robust policies. We find, however, that counterfactual data augmentation performs best in complex, long-horizon tasks due to its ability to maintain causal relationships between states and actions, and marginalize over subsets of the state space that are causally invariant to actions."}, {"title": "2. Generalization", "content": "To address RQ2, we evaluate the generalization ability of RoCoDA on a Libero-Object task [27]. In particular, we test the policy's ability to generalize to:\n\u2022 Unseen Distractors: Training with a subset (2/5) of distractor objects and testing with the whole set to assess robustness to irrelevant objects in the environment.\n\u2022 Unseen Textures: Applying unseen textures to objects and backgrounds at test time. Random colors are applied over objects while preserving details such as opacity, text, etc.\nWe test several models in these evaluation settings (Table III). We select one task from the Libero Object dataset ('pick up the tomato sauce and place it in the basket"}, {"title": "3. Scaling Synthetic Data", "content": "To address RQ3, we evaluate the performance of RoCoDA when trained on varying amounts of demonstrations. We scale the number of demonstrations using SE(3) equivariance and quantify the effect of diversity and complexity in the dataset (Table IV).\nWe compare the performance of models trained with and without causal-based data augmentation across the Coffee and Three Block Stack tasks with varying number of demonstrations (10, 50, 100, 200, and 1000). For the Three Block Stack task, counterfactual data augmentation on 200 demonstrations improves performance to 30% success compared to a lesser number of demonstrations, suggesting augmentation performance scales and continues to improve at 1000 demonstrations, but at a lesser rate.\nIn contrast, the Coffee task, while also requiring precise actions, may have a simpler causal complexity. The number of possible combinations of independent state partitions and causal phases is smaller in comparison to the Three Block Stack task. This suggests that in simpler tasks, causal augmentation is less advantageous but still offers benefits by introducing diversity."}, {"title": "4. Qualitative Analysis", "content": "To explore RQ4, we identify emergent behaviors that the policy was not explicitly trained to exhibit. One such behavior is re-grasping, where the policy recovers from a failed grasp attempt by reopening the gripper and attempting to grasp again (Figure 4). We observed in rollouts that once the gripper is fully closed, the robot fails to continue the task as this action is not in the training distribution (grippers are only closed partially when picking up objects). During sequences where the robot is in a transition phase and not interacting with other entities, local factorization of the gripper position and augmenting the states of the gripper positions can be performed. This learned behavior indicates a level of adaptability and error recovery not present in the training data."}, {"title": "VI. CONCLUSION", "content": "ROCODA provides a principled and effective framework for data augmentation in imitation learning that leverages causal invariance, geometric equivariance, and visual symmetries. This unified perspective advances the theoretical understanding of policy learning and offers practical benefits for developing robust and generalizable robotic systems. Policies trained with RoCoDA generally exhibited higher task success rates, robust generalization to unseen object poses, textures, and distractors, and improved sample efficiency, requiring fewer demonstrations to achieve comparable or superior performance."}, {"title": "APPENDIX", "content": "A. Assumptions & Limitations\n1) Generating demonstrations from human source. We use the same assumptions as [13] to generate new demonstrations.\n2) Access to simulation states. We use information about simulation states to create new demonstrations and to create causally augmented data.\n3) Subtask boundaries are determined by changes in gripper state. Defining the subtask boundaries by changes in the gripper state limits the the environments we experiment on. Environments that involve skills such as pushing or pulling objects would require using a different heuristic to divide subtasks.\n4) Causal Relationships are known. The causal relationships between entities are assumed to be known by a domain expert. For each subtask, each timestep is labeled with its corresponding causal graph."}, {"title": "B. Additional Related Works", "content": "Data Augmentation Data augmentation has played a critical role in the success of machine learning models by artificially expanding the size of training datasets and improving model generalization. Early work in this area applied basic transformations such as rotation, scaling, translation, and distortion to image datasets like MNIST [28], laying the groundwork for modern augmentation methods. The success of convolutional neural networks in image classification tasks further highlighted the importance of data augmentation. AlexNet [29] employed random cropping and horizontal flipping to improve image classification performance. Subsequent methods like Cutout [30] and Mixup [31] introduced techniques for robustness to occlusion and linear interpolation between samples, respectively.\nGenerative Data Synthesis As generative models continue to gain popularity [32, 33], researchers have found multiple ways to use them to generate synthetic data for robotics. GenAug [18] Utilizes generative models like Stable Diffusion to alter textures and generate diverse visual samples, improving model robustness to appearance variations. ROSIE [19] Performs infilling using generative models to complete partially observed states and generate data with diverse backgrounds. SynthER [20] trains diffusion models on existing datasets to produce synthetic demonstrations, thus increasing the diversity and size of the training data. While some approaches [20, 34-37] train diffusion models on existing datasets to produce synthetic demonstrations, increasing the diversity and size of the training data, these methods often focus on high-level semantic variations and may not adequately address low-level spurious correlations in state-action relationships. In this work, we establish a framework that leverages explicit causal and geometric augmentations. Exploring generative models as a complementary strategy is left for future work."}, {"title": "C. Task Descriptions", "content": "In this section we provide more detail of tasks used in our experiments (Figure 5). For our experiments we use a panda arm for data collection and policy rollout. We use source data in collected in D variations of Three Block Stack, Three Piece Assembly and Coffee.\nThree Block Stack: The robot must stack three distinct colored blocks in order of green, red, and then the blue cube on top. We generate trajectories for the $D_1$ [13] variation of a larger spatial distribution of the blocks on the table.\nThree Piece Assembly: This task requires precision and correct orientation, making it more challenging than simple block stacking due to the importance of part alignment. This task involves building a structure by assembling three pieces in a precise sequence and orientation. We generate trajectories for the $D_2$ [13] variation of the environment where all three pieces have a top-down rotation variation and all pieces can be initialized in different positions. We note that using MimicGen yielded a low success rate of generating trajectories. As MimicGen can generate biased data [13] low performance may be due to biased object configurations in the generated dataset.\nCoffee Task: This task tests interaction with articulated objects and sequential dependencies, emphasizing the handling"}, {"title": "D. Causal Relationships for Tasks", "content": "In our experiments, we use spatial information of objects as states to be augmented. In tasks such as block stacking, where the stacking order depends on block color, we could further factorize object states (e.g. color, texture, etc) to enhance augmentation. However, we simplify the causal structure by not factoring object states in this way. We specify causal relationships for each subtask and provide causal graphs for Three Block Stack and Coffee in Figure 7. Subtasks are defined based on changes in the robot's gripper state as mentioned in Section III. When sampling data, we use the corresponding causal graph to sample new states. For instance, the third phase of Three Block Stack has two independent partitions: (1) Robot and Cube C and (2) Cube A and Cube B. As the states of Cube A and B are irrelevant to the robot's movement to Cube C, the states of Cube A and B can be sampled from another trajectory of the same subtask phase without affecting the causal relationship. We additionally factorize the robot's gripper position and augment the gripper position randomly while the robot transits to pick up objects."}, {"title": "E. Training Details", "content": "Across all experiments, we keep training hyperparameters unchanged. We use hyperparameters similar to [2], but decrease the image resolution, feedforward dimensions, and hidden dimensions by a factor of two. We additionally double the batch size and use a chunk size of 15 and do not use temporal ensembling (Table V). We train Three Block Stack, Three Piece Assembly, and Coffee datasets using the agentview and roboto eye in hand camera views. As for the Libero Object dataset, we train on the agentview camera view. While for the Transport we experiment with single agentview and multicamera view with robot0\u02d9eye in hand, robotl eye in hand, shouldercamera0 and shouldercameral (Table II). Similar to [3], actions are positional control and use a 6D rotation representation."}, {"title": "F. Scaling Synthetic Dataset Size", "content": "We experiment with scaling the synthetic dataset size to understand the impact of data quantity on generalization. While data augmentation effectively increases the dataset and improves a model's generalization, there are diminishing returns. Similar to [22] we explore increasing the dataset size and assess the ratio of demonstration data to causally augmented data. Separate from Section IV-A where data is sampled with some probability of applying causal augmentation, we augment offline as described in Algorithm 1.\nThe performance of scaling the dataset is evaluated on Three Block Stack and Coffee using the same hyperparameters. We produce 500 demonstrations for each environment using SE(3) equivariant data augmentation, expand the data up to 10x through causal augmentation, and train using the number of gradient steps fixed at one million. We report our performance as the maximum success rate for all policy evaluations on 3 separate seeds as done in [15]. In the Three Block Stack environment, performance improves by approximately 20% when adding synthetic data up to a ratio of 1:1 (Figure 6). However, as the dataset is expanded beyond the 1:1 ratio, the performance gains diminish with success rates falling within the margin of error. This indicates that adding more synthetic data beyond a 1:1 ratio does not result in substantial improvement and suggests some flexibility in the synthetic-to-real ratio. In certain tasks, we observed that counterfactual data augmentation sometimes led to out-of-distribution samples that reduced performance, especially at higher synthetic ratios (e.g., 1:5 in Coffee). This may be attributed to the model's sensitivity to shifts in the distribution when synthetic data dominates. Similarly, this aligns with discussion of RQ3 in Section V-B as Three Block Stack benefits largely from augmentation while Coffee has marginal performance gains. We again attribute this to the complexity of each task notice that there are a total of 8 causally independent state partitions throughout the execution of Three Block Stack, compared to only 2 for Coffee."}]}