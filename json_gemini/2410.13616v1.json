{"title": "Spatiotemporal Object Detection for Improved\nAerial Vehicle Detection in Traffic Monitoring", "authors": ["Kristina Telegraph", "Christos Kyrkou"], "abstract": "This work presents advancements in multi-class\nvehicle detection using UAV cameras through the development of\nspatiotemporal object detection models. The study introduces a\nSpatio-Temporal Vehicle Detection Dataset (STVD) containing\n6,600 annotated sequential frame images captured by UAVs,\nenabling comprehensive training and evaluation of algorithms\nfor holistic spatiotemporal perception. A YOLO-based object de-\ntection algorithm is enhanced to incorporate temporal dynamics,\nresulting in improved performance over single frame models. The\nintegration of attention mechanisms into spatiotemporal models is\nshown to further enhance performance. Experimental validation\ndemonstrates significant progress, with the best spatiotemporal\nmodel exhibiting a 16.22% improvement over single frame\nmodels, while it is demonstrated that attention mechanisms hold\nthe potential for additional performance gains.\n\nImpact Statement-Transportation systems have significant\nimpacts on economic growth, social development and the envi-\nronment. This article demonstrates how to better monitor road\ntraffic using UAVs through a spatiotemporal deep learning model.\nThis can provide the basis for the development of real-time\ntraffic monitoring and control strategies, that when deployed\ncan help to: 1) Perform real-time analysis of traffic patterns,\ncongestion points, and bottlenecks at any location, and use this\ndata to optimize signal timings, reroute traffic, and alleviate\ncongestion. 2) Provide effective monitoring that helps detect\nincidents, such as accidents or stalled vehicles promptly, and help\nprevent secondary accidents. 3) Gain insights into usage pattern\nthrough long-term traffic monitoring data analysis, helping urban\nplanners make informed decisions about road expansions, new\ninfrastructure projects, and public transportation development.\nIndex Terms-spatiotemporal data, video analysis, deep learn-\ning, object detection, convolutional neural networks", "sections": [{"title": "I. INTRODUCTION", "content": "ISUAL object detection has taken enormous strides\nover the past decade thanks to advancements in deep\nlearning architectures and availability of large datasets. These\napproaches have mainly focused on single image processing.\nSeminal works in this area include state-of-the-art image\nobject detection methods such as the Region-based CNN series\n(RCNN) [1]\u2013[3], and the You Only Look Once series (YOLO)\n[4]\u2013[11], and more recently, transformer-based models such\nas the Detection Transformer (DETR) [12], [13] and the You\nOnly Look at One Sequence (YOLOS) model [14], where\nby processing single images they are tasked with regressing\nbounding box information and associated classes. Hence,\nsince videos are ultimately a sequence of image frames, they\nare also applicable to video processing by processing the\nincoming frame in isolation. The most popular techniques to\nincorporate temporal information and reasoning include post-\nprocessing methods such as tracking to build associations [15].\nAs such, image-based detection on video-based data com-\nmonly encounters difficulties when dealing with phenomena\nsuch as occlusion, motion-blur, and variations in illumination\nconditions. Hence, it does not always produce reliable results,\nas each image will be perceived independently and it does\nnot account for the aforementioned phenomena and valuable\ninformation present in the temporal domain. These drawbacks\nstem from the fact that the machine learning models cannot\nexplicitly utilize the temporal information found in video\nstreams. Hence, while generic image object detection has\nwitnessed many achievements, there is still a gap in the\nutilization of both spatial and temporal information, which also\nrequires appropriate datasets.\nPromising results in generic image object detection led to\nthe introduction of algorithms to extract motion information\nfrom videos. Initial approaches post-processed image detection\nalgorithm results on video frames to enhance spatiotemporal\ncues [16]\u2013[18]. In later works, researchers integrated these\npost-processing modules into the model itself to create end-to-\nend models for video object detection. These models employed\ndifferent methods to capture the motion information, such as\noptical flow [19], [20], and memory modules [21]\u2013[23]. Such\ntechniques have not yet been investigated in the context of\nUAV-based sensing. UAVs are increasingly being used in trans-\nportation monitoring applications to provide on-demand and\ninfrastructure-free information-rich (vehicle types, trajectories,\ncounts, queue lengths) road traffic monitoring [24]\u2013[26]. Ex-\nisting single frame models face challenges such as occlusions\nin dense regions, maintaining consistency of detection across\nframes, and have access to limited features of a single frame.\nThis work aims to enhance multi-class vehicle detection\nfrom UAV cameras through the development of spatiotemporal\nobject detection models trained on 6-channel and 7-channel\ninput data rather than conventional 3-channel RGB images\n(as shown in Fig. 1). By leveraging the temporal information\nthrough architectural modifications with different backbone\nand input options, we perform detection efficiently and in an\nend-to-end fashion that does not use optical flow or memory\nmodules, and is suitable for real-time application. In particular,\nthe main contributions of this article are summarized as\nfollows:\n\u2022\nA Spatio-Temporal Vehicle Detection Dataset (STVD)\ncomprising 6,600 sequential frame images, meticulously\nannotated with categorizations encompassing three dis-\ntinct vehicle classes: 'car', 'truck', and 'bus'. This dataset\nwas created employing aerial footage acquired by Un-\nmanned Aerial Vehicles (UAVs) across diverse segments\nof the road network situated within the geographical\nbounds of the Republic of Cyprus.\n\u2022 Investigate how to extend YOLOv5 object detection\nframework for processing of spatiotemporal data to en-\ncompass temporal dynamics through architectural en-\nhancements and variations in input representations. Lead-\ning to improved performance over single frame models.\n\u2022 We demonstrate that the introduction of attention mecha-\nnisms into the spatiotemporal models can lead to further\nperformance improvements.\n\u2022\nThrough a series of experiments we investigate different\nmodels, through both quantitative and qualitative analyses\nand examine class-specific performance. The results of the\nspatiotemporal models show significant progress with the best\nspatiotemporal model having 16.22% improvement over the\nsingle frame model. Experiments also showed that incorpo-\nrating attention-mechanisms into the spatiotemporal model\narchitecture has potential to boost results even further.\nThe remainder of the paper is organized as follows. Sec-\ntion II provides a summary of background information and\nrelated works in image and video object detection. Section\nIII discusses the proposed approach and the custom dataset.\nSection IV describes the performance metrics considered and\nthe setup of model training and inference experiments. Section\nV presents the experimental results, both quantitative and\nqualitative, while Section VI provides a discussion on the\nimplications of the results. Finally, Section VII presents the\nmain conclusions, as well as areas of improvement and future\nwork."}, {"title": "II. BACKGROUND AND RELEVANT WORK", "content": "Image object detectors based on deep learning can be\ndivided into two categories: two-stage detectors and one-stage\ndetectors. An exemplary two-stage detector is the Faster R-\nCNN [3], where candidate object bounding boxes are proposed\nin the first stage, and features are extracted from each candi-\ndate box in the second stage to carry out the bounding-box\nregression and classification tasks. These kinds of detectors\npossess very high accuracy at the expense of high inference\nspeeds. One-stage detectors, on the other hand, such as SSD\n[27] and the YOLO family of detectors [4]\u2013[11], do not have\nan initial region proposal step, they propose regressing bound-\ning boxes and class probabilities from the images directly in\none stage, making them more efficient and suitable for real-\ntime applications due to their high inference speeds.\nYOLO [4] divides the input image into grid cells, where\neach grid cell is responsible for predicting the bounding box\nand confidence scores of objects centered within it. Confidence\nscores indicate how likely an object exists according to the\nmodel. YOLOv2 [5] further built on top of the original YOLO,\nadopting several novel concepts, such as anchor boxes and\nremoval of fully connected layers to improve its speed and\nprecision. The third generation of YOLO [6] proposed a more\nrobust feature extractor for its backbone called Darknet-53.\nIt allowed adaptation to more complex datasets containing\nmultiple overlapping labels. It also utilized three different\nfeature map scales to predict bounding boxes, increasing its\nperformance on smaller sized objects. YOLOv4 [7] further\nadded techniques to achieve the best speed-accuracy trade-off,\nthrough an improved loss function, Complete-IoU, and experi-\nmented with additional augmentation techniques. YOLOv5 [8]\nfocused on faster training and ease of use and demonstrated\nwide applicability across applications with a similar accuracy\nto YOLOv4.\nIn more recent literature, transformer architectures have\nbeen explored in computer vision applications after achieving\nbreakthroughs in areas of Natural Language Processing (NLP).\nThe transformer architecture, which employs self-attention\nmechanisms, has proved to be effective in capturing long-\nrange dependencies, prompting researchers to venture beyond\njust convolutions for image object detection and other vi-\nsion tasks [28]. Various studies have shown that employing\nboth convolutions and self-attention in so-called hybrid CNN-\ntransformer architectures can leverage both the local nature of\nconvolutional layers and the global context that self-attention\nprovides to achieve comparable and even better outcomes in\npractice [29], [30]. Overall, image object detection models lay\nthe foundation for the needed development of video object\ndetection networks [31]."}, {"title": "B. Video Object Detection", "content": "Video detection, or Spatiotemporal detection is a more\nchallenging task, as it aims to detect patterns in both space\nand time. The earliest attempts to detect objects in video\nincluded using a state-of-the-art image detector on the video\nframes, extracting the spatiotemporal information, and using\nit to improve the image detector's preliminary results in a\npost-processing fashion. Sequence Non-Maximum Suppres-\nsion (Seq-NMS) [16], Tubelet Proposal Networks (TPN) [17]\nand Tubelets with CNNs (T-CNN) [18] all had a main strategy\nof mapping the results of image detectors across adjacent\nvideo frames, with the main difference between the post-\nprocessing methods being the mapping strategy used. While\nthese methods are straightforward, post-processing techniques\nare usually undesirable and do not meet requirements for\nmodern real-time applications. These methods are more time-\nconsuming and usually not as effective due to their sequential\nnature. Hence, during inference, bounding boxes are processed\nand refined sequentially, rather than in one pass.\nSeveral methods for spatiotemporal understanding were\nlater integrated into the single image detectors, allowing them\nto learn motion information directly during training in an end-\nto-end manner. Feature level methods that use optical flow\nsuch as Flow-Guided Feature Aggregation (FGFA) [19] and\nDeep Feature Flow (DFF) [20], acquire temporal information\nfrom pixel-to-pixel correspondence between adjacent frames,\nusing a key-frame to supplement features of other frames.\nHowever, adding optical flow to a network significantly in-\ncreases model parameters, making these methods slow [31].\nSubnetworks based on context were also integrated into\nsingle image detectors for the aim of spatiotemporal under-\nstanding. A variant of the traditional long short-term memory\n(LSTM) model [32], that achieved very good results in many\nfields in the past, is the convolutional LSTM model [21]. It\nuses different 'gate' operations to extract and propagate fea-\ntures, thus it is able to establish context and long-term object\nassociations between consecutive frames. The Association-\nLSTM [22], was proposed to improve video object detection.\nIt consists mainly of the SSD [27] image object detector, and\na convolutional LSTM [21]. SSD performs the detection on\neach frame of the video, extracting individual frame features,\nwhich are then stacked and fed to the LSTM. Zhu and\nLiu [33] introduced a lightweight model that also leveraged\nthe combination of the SSD single image detector [27] and\nconvolutional-LSTM layers [21]. They inject the LSTM layers\ndirectly into a modified SSD detector to refine the input\nframe at each timestep and extract additional temporal cues.\nZhang and Kim [34] proposed a temporal convolutional-\nLSTM approach that utilizes two types of temporal context\ninformation. Short-term context from adjusting the feature\nmap from the directly preceding frame using optical flow,\nand long-term context from distant preceding frames through\nthe convolutional LSTM. Comparisons with post-processing\nmethods and flow-based methods showed improved results.\nOverall, while LSTMs require less computation than optical\nflow, they require more memory.\nDeng et al. [35] attempted to tackle the storing of redundant\ninformation by exploiting external memory that comprises\nof addressable matrices, through attentional read and write\nprocesses. This allowed them to store information over a\nlonger period, to be retrieved and aggregated through an\nattention-based global search. Beery et al. [36] also utilized\nmemory banks, both long-term and short-term, to store con-\ntextual information. Their proposed model, Context R-CNN,\nuses the two-stage Faster R-CNN architecture as the base\nmodel, wherein the first stage of detection, the box proposals\nof the network are routed through attention-based modules\nto incorporate temporal features from frames from the past.\nAnother approach to extending convolutional neural networks\ninto the time dimension is to employ 3-dimensional CNNs for\nspatiotemporal feature learning. Ji et al. [37] developed a 3D\nCNN model to capture motion information encoded in multiple\nadjacent frames for the task of action recognition in airport\nsurveillance videos. Tran et al. [38] proposed 3D CNNs in the\ncontext of large-scale supervised learning tasks. They showed\nthat 3D CNNs can outperform 2D CNNs on various video\nanalysis applications. While 3D CNN based methods achieve\ngood performance, their deployment is expensive as they have\nhigher computational complexity than conventional 2D CNNs.\nDue to the robustness and efficiency of conventional CNNs,\nresearchers were inclined to build onto the existing architecture\nmerely by introducing a special module that can extract and\nlearn temporal representations. Lin et al. [39] proposed the\nTemporal Shift Module (TSM), an approach that was able\nto achieve the performance of 3D CNNs without the added\ncomplexity and cost for action recognition. Passos et al. [40]\nproposed a spatial-temporal consistency module that estimates\nthe displacement of detected objects of interest from frame\nto frame. They extend the 2D spatial bounding boxes into\nthe 3D space-time dimension by estimating an object's spatial\ndisplacement from one frame to another, aligning them space-\nwise and computing their pairwise intersection over union\n(IoU).\nSpatiotemporal information was also utilized for the task\nof tiny object detection (TOD) in wide area motion imagery\n(WAMI). Lalonde et al. [41] proposed a two-stage spatiotem-\nporal CNN that exploits both appearance and motion informa-\ntion using an input of five stacked grayscale frames utilizing\na Faster-R-CNN-like region proposal network exceededing\nstate-of-the-art results on the WPAFB 2009 dataset. Corsel\net al. [42] later outperformed this work on the same dataset\nusing a spatiotemporal model based on the YOLOv5 object\ndetection framework. They proposed two approaches, the first\napproach exploits temporal context by sampling every three\nconsecutive frames from video sequences of the greyscale\nWPAFB 2009 WAMI dataset, where with each frame $f_t$, $f_{t-1}$\nand $f_{t+1}$ were sampled with it, thus requiring a future frame\nto process the current frame. Their second approach was to\nuse a two-stream architecture with the first stream handling\nthe three frame representations from the first approach, and\nwith the second stream handling exclusive motion information\nobtained from the absolute difference of the three frames\nused. They applied their models to single-class detection of\ntiny objects. Nevertheless, the necessity to wait for future\nframes to arrive makes this approach not suitable for real-\ntime applications. Our work attempts to leverage temporal\ninformation on a smaller network, without sampling future\nframes, making it more suitable for real-time applications.\nOur work also applies the two-stream architecture approach\nusing the absolute greyscale frame difference of two RGB\nframes, $f_t$, and $f_{t-1}$ along with the two frames themselves.\nWe target multi-label, multi-class detection and classification\non a custom aerial RGB dataset of road networks in complex\nsettings captured from a UAV rather than from a single area\nin low resolution satelite images."}, {"title": "C. Attention Mechanisms", "content": "Attention in deep learning is a concept inspired by cognitive\nfunctions of humans, which is the natural tendency to selec-\ntively focus on parts of information deemed more important.\nIt has been brought to the area of deep learning and computer\nvision with great success [43]. The progress of attention-based\nmodels in computer vision in the deep learning era can be\ndivided into four phases [44]. The first phase is characterized\nby work to combine deep neural networks with attention\nmechanisms, such as the RAM network [45], that recurrently\npredicts important features while updating the network con-\ncurrently. The second phase begins with the introduction of\nspatial attention, a mechanism that learns positions of interest\nin a spatial map. Jaderberg et al. [46] introduced a differ-\nentiable spatial transformer (STN) that finds these positions\nof interest through different transformations such as cropping,\nrotation, scaling and skew, adaptively according to the input\nfeature map. The third phase began with a novel-attention\nmechanism called Squeeze-and-Excitation networks (SENet)\n[47]. SENets adaptively recalibrated features channel-wise by\nexplicitly modelling their interdependencies, to focus on the\nmost important channels. More channel attention mechanisms\nfollowed, like Efficient Channel Attention networks (ECA-\nNet) [48] and Convolutional Block Attention module (CBAM)\n[49]. The last and current phase of attention in computer\nvision is the self-attention era that was first introduced by\nVaswani et al. [50] in transformers and rapidly revolutionized\nthe field of natural language processing. The first to introduce\nself-attention to computer vision was Wang et al. [51] who\nproposed non-local neural networks that showed superiority\nas they captured longer-range dependencies. The Detection\nTransformer (DETR) [12] employed a CNN backbone to\nextract visual features and combined it with a Transformer-\nbased decoder to perform object detection. It successfully\ncombined the two in an end-to-end trainable pipeline. The\nVision Transformer (ViT) [52] employed a multi-head self-\nattention architecture, and was able to achieve performance\ncomparable to modern CNNs. A new branch of vision trans-\nformers called multi-scale vision transformers emerged where\nthese models gradually reduced the number of tokens while\nincreasing the number of token feature dimensions in a multi-\nstage hierarchical design, such as the Pyramid ViT [53], and\nthe Swin Transformer [54]. Multi-scale vision transformers\nwere also extended and adapted for video understanding [55],\n[56], by perceiving videos as sequences of images that are\nsimilarly flattened into patches."}, {"title": "III. PROPOSED APPROACH", "content": "1) Dataset Contribution: In terms of datasets there has\nbeen considerable progression in static datasets [57], multi-\nmodal datasets [58], and altitude-aware datasets [59] for aerial\nobject detection. However, these datasets are not suitable to\ninvestigate spatiotemporal detection models, since they do\nnot have information from adjacent time instances. STVD is\nspecifically designed to encapsulate both spatial and temporal\ninformation, as it consists of consecutive frames extracted from\nvideo clips. This temporal continuity offers a richer context for\nvehicle detection tasks, enabling algorithms to leverage motion\ndynamics and temporal dependencies.\n2) Dataset Description: A dataset suitable for spatiotem-\nporal object detection (summarized in Table I and visualized\nin Fig. 2) is constructed using several aerial video clips of\ntraffic in different road segments in Nicosia, Cyprus, captured\nusing UAVs, rather than single areas in low resolution satelite\nimages as other datasets. A drone hovers statically over an\narea and captures the clips from a bird's eye view (top-down),\ndirectly on top of the street segment of interest, therefore there\nis no drone movement involved. The exact height of each video\nvaries between ~ 80 \u2013 110 metres. Consequently, the object\nsizes also varied.\nBy compiling multiple sequences of images extracted from\nthese videos, the dataset accumulates a substantial corpus\nof 6,600 frames. The dataset encapsulates 3 classes: 'car',\n'truck' and 'bus' with a distribution of 81165, 1541, and\n1625 respectively in the case that we only use the even frame\nannotations, which approximately doubles when considering\nthe entire dataset. An additional challenge of the dataset that\nmirrors real world application is the fact that the classes are\nnot balanced, as there is a significantly larger number of cars\ncompared to trucks and buses, as in a regular transportation\nnetwork. The images have Full-HD resolution, with object\nsizes approximately between 20 x 20 to 150 \u00d7 150 pixels. The\ndataset was prepared in the YOLO format. The dataset was\nsplit into 80% for training and the remaining 20% for valida-\ntion. The importance of such a dataset lies in its capability to\nencapsulate both spatial and temporal nuances. We note the\nframes belonging in the same continuous sequence as such\nthe dataset can potentially be used to develop approaches that\noperate on multiple sequential frames for object detection by\nsampling a number of frames from the same sequence. While\nthe dataset might seem small in comparison to other datasets\nevery frame has a large number of vehicle instances which\nstill enables learning rich representations. The naturally lower\nnumber of instances of 'truck' and 'bus' in a typical road\nnetwork is also a challenge that our dataset replicates and is\nan important problem for the community to tackle.\n3) Dataset Refinement: After the initial data collection\nphase, the collected images undergo a refinement to remove\npotential duplicate frames and bad-quality images. This refers\nto segments of clips where there was traffic at a standstill\nfor example at a red traffic light, which results in a lot of\nframes that are almost the same, since cars do not move.\nIn addition, frames depicting the same scenery over a few\nframes are removed to not bias the dataset. Footage that was\nnot appropriate for our purpose i.e., during setup where the\ndrone is moving and camera is not looking downwards was\nalso removed.\n4) Data Annotation: The annotation process requires the\nrigorous labeling of images based on the different classes.\nSince a frame can contain hundreds of vehicles and annotation\ncan be very time consuming we employ an accelerated auto-\nlabeling process. First, we annotate part of the dataset and\ntrain an object detection model. This model is then used to\nprovisionally predict bounding boxes of the remaining frames.\nThen all the frames were manually processed to account for\nmisclassifications, missed detections, and double detections.\nThis proved efficient, as they accurately placed bounding\nboxes on a significant portion of vehicle instances, requiring\nonly minor adjustments. This enabled us to annotate a much\nlarger set than previous works. To guarantee integrity of\nthe results we employ a two-step approach, where a second\nindependent researcher verified the annotated frames, ensuring\na thorough validation procedure. It is worth noting that these\nsingle-image models were not employed further and did not\ninfluence any other processes or results. Finally, we take the\nfirst 80% from every clip for the training set, and the rest\nfor the validation set. This method of splitting mitigates this\ncloseness in frames as the validation frames would be extracted\nfrom the end of the clips."}, {"title": "B. Detection Models", "content": "To develop spatiotemporal detection models we investigate\nhow to better incorporate concepts and enhancements such\nas multi-frame processing and attention mechanisms to a\nbaseline single frame detection network, namely YOLOv5 [8].\nThis model also serves as a reference point for evaluating\nthe effectiveness of the enhancements done to incorporate\nthe temporal dimension. The YOLOv5 framework [8], was\nchosen for this work for its fast inference speeds, high level\nof accuracy and scalability. It is a member of the YOLO\n(You Only Look Once) family of single-stage regression object\ndetectors, hence it uses a CNN architecture that is trained\non single images and in one forward pass predicts object\nbounding boxes and their class probabilities.\nThe model architecture of the YOLOv5 model comprises\nof a backbone network, responsible for extracting deep-level\nfeatures, and a head, which acts as a feature aggregator,\ncombining features from the backbone from different scales,\nand a detection head which is responsible for making the\npredictions. The simplified architecture of YOLOv5 can be\nvisualized in Fig. 3. The architecture incorporated the C3\nmodule, as seen in Fig. 3, in both the backbone and head\narchitecture, which is a simplified variant of the Cross Stage\nPartial network (CSP) block [60]. YOLOv5 has multiple\nmodels of different sizes ranging from the smallest YOLOv5n,\nto the largest YOLOv5x, each designed for a specific use\ncase of required speed-accuracy trade-off [8]. In this work, the\nYOLOv5s architecture is adapted to create the models which\nprovides a very good trade-off for real-time performance as\nwell as accuracy and room for adding new features."}, {"title": "1) Single Frame Model:", "content": "The single frame model is essen-\ntially the base YOLOv5s model that is trained on single frames\nto act as a benchmark for comparison with the spatiotemporal\nmodels and evaluate any improvements resulting from the\nadded features. Fig. 4 provides an illustration of how this\nparticular model processes examples independently of each\nother, as well as showing the input shapes corresponding to\nstandard 3-channel images."}, {"title": "2) Spatiotemporal Models:", "content": "A neural network exploiting\nboth temporal and spatial relations of the input can be con-\nstructed in multiple ways. Most commonly, methods process\nspatial information first and fuse the output to add the temporal\ndimension. In addition, we also explore spatiotemporal models\nthat employ additional temporal information which in this\ncontext is in the form of one previous frame that is processed\nby the same network from the beginning. The spatiotemporal\nmodels are trained on a sequence of paired images, utilizing\nthe ground truth labels of the most recent frame for training\nand evaluation. Hence, since a pair acts as an evaluation\ninstance, essentially, half the annotations are used. Thus for\nfairness, the same frames (half of the dataset) are used for\ntraining and evaluation of the single frame model as well.\nNext, we describe the different spatiotemporal models that are\ninvestigated."}, {"title": "a) Frame Pair Model:", "content": "This model is a natural extension\nof a single frame model where the input is a pair of two\nconsecutive frames at a time, concatenated channel-wise to\nresult in a tensor of 6 channels. Fig. 5 illustrates how the\nexamples are combined and fed to the model. In this setting\nonly the first convolutional layer is changed compared to the\nsingle frame to account for the larger channel size."}, {"title": "b) Frame Pair and Difference Model:", "content": "This model natu-\nrally extends the pair model by adding an additional single-\nchannel tensor which is the pixel-wise absolute greyscale\nframe difference of the two input images as shown in Fig.\n6, resulting in a 7-channel input. The main motivation behind\nthis model being that we provide an additional signal that\nthe model can use, that highlights areas of change and can\nthus guide the model to focus more on those regions that\nmight either provide new information or be more challenging\nto detect (i.e., in the case of moving vehicles)."}, {"title": "c) Two-Stream Model:", "content": "This model essentially uses the\nsame input as the Frame Pair and Difference Model, however,\nthe input is split in two as the frame pair and the frame\ndifference channels are fed into two separate backbones, after\nwhich the outputs of both backbones are concatenated and\npassed to the model head, where the extracted features are\nconcatenated at three detection scales. Both backbones are\nidentical to that of the base YOLOv5 backbone shown in Fig.\n3, except for their input layer channels. The head architecture\nis also identical to that of base YOLOv5. Fig. 7 illustrates the\nproposed two-stream model architecture."}, {"title": "3) Attention-Spatiotemporal Models:", "content": "The spatiotemporal\nmodels are further enhanced with attention mechanisms that\nprovide the model with the flexibility of learning to which\nspatiotemporal information to attend to. The attention mech-\nanisms would be applied to the spatiotemporal feature maps,\nallowing the model to refine them by focusing on the important\ninformation and ignoring the less important information. Two\nattention mechanisms were explored, Squeeze-and-Excitation\nnetworks (SENet) [47], as well as Efficient Channel Attention\nnetworks (ECA-Net) [48]. Their addition was investigated in\nboth the Two-Stream Model as well as the Frame Pair and\nDifference Model.\nSENet [47] is a CNN architecture that employs Squeeze-\nand-Excitation blocks. These blocks weigh their input chan-\nnels adaptively according to their relevance, as opposed to\nconvolutional layers in a CNN which give equal weights\nto each channel [47]. ECA-Net [48] is a CNN architecture\nthat employs Efficient Channel Attention blocks. As with\nSENets, ECA-Nets also provide channel attention but at a\nlower complexity trade-off and thus computational cost. It\nreduces each channel in the input tensor to a single pixel in\nthe same way as in SENets, this vector is then subjected to a\n1-D striding convolution. This makes ECA-Net more efficient\nas the total number of parameters added is just the size of\nthe convolutional kernel, as opposed to SENets which employ\na feed-forward network. By design, this also eliminates the\ndimensionality reduction present in SENets hidden layer.\nThe addition of these blocks to the spatiotemporal models\nwas explored in two different ways in the model architecture,\nadding them as a layer at a single level at the end of\nthe backbone, right before the final spatial pyramid pooling\nlayer, as well as embedding them into the C3 modules at\nfour different levels in the head architecture. In the attention\nembedded two-stream model, the attention layers are added in\nboth backbones 1 and 2, at the same position. As a result,\nthe two-stream-SE model has two SE layers, one in each\nbackbone. Similarly, the two-stream-ECA model has two ECA\nlayers, one in each backbone. Another approach that was\nexplored was embedding the SE and ECA mechanisms into\nthe C3 modules in the head architecture to apply attention\nduring feature aggregation at different scales. The C3 module\ndiscussed earlier, is responsible for extracting feature maps\nat different scales and resolutions. Therefore, adding attention\nmechanisms in the C3 blocks could help the network amplify\nthe more important features at different input scales and\nresolutions. The resulting modules of embedding SE and ECA\ninto the C3 modules were called the C3SE and C3ECA\nmodules respectively. Fig. 8 illustrates the two methods of\nadding attention mechanisms to the architecture. The backbone\ncolumn (right) shows where the SE or ECA layers are added in\nthe first method, and the head column (left) shows how these\nmodified C3SE modules are substituted in place of the original\nC3 modules in the head architecture of the spatiotemporal\nmodels. In the attention-spatiotemporal models that utilize\nC3ECA modules, they are also implemented in the same way."}, {"title": "IV. TRAINING AND EXPERIMENTS SETUP", "content": "1) Training: Training spatiotemporal models can pose sev-\neral challenges compared to standard single image models\nsince we no longer can retrieve single images completely at\nrandom. Thus, for training the model we implement appro-\npriate data loading mechanisms that retrieve only valid pairs\nof frames for training to respect the temporal frame ordering.\nIn addition, frames corresponding to different video locations\nare also not mixed together. Compared to default training\nstrategies used in YOLO models the following adjustments\nwere made. 1) Mosaic augmentation is disabled since they\nresult in drastic reduction in the size of the objects which\nare already small, but more importantly, they would also\nmix frames from different times and locations. 2) For the\nspatiotemporal models any augmentation that randomizes the\nframe order was also disabled since the ordering needs to be\npreserved so that the temporal domain information remains\nvalid. Instead, images are shuffled during training in the single\nframe model, and are shuffled in pairs before training in the\nspatiotemporal models. Traing hyperparameters are shown in\nTable II.\n2) Validation and Inference: We derive the best model\nfrom training by picking the one that performed best on\na weighted combination of the 10% mAP@0.5 and 90%\nmAP@0.5:0.95, on the validation set and then used it for\ninference testing. The image size used was 640 as with\ntraining, and the batch size was 32. The results also contained\nthe class-specific metrics. Inference tests were carried out to\nperform a qualitative analysis of the spatiotemporal as well as\nthe attention-spatiotemporal models' results. The confidence\nthreshold for the inference tests was 0.25. The tests were done\non several video frames from the validation set of different\nroad segments and environments to also measure inference\nspeed on a NVIDIA Tesla V100 GPU."}, {"title": "B. Performance Metrics", "content": "We monitor different metrics", "Recall": "Precision is the percentage of\ncorrect positive predictions. It is the model's ability to identify\nand detect only"}]}