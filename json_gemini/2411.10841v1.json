{"title": "Adaptive Learning of Design Strategies over Non-Hierarchical Multi-Fidelity Models via Policy Alignment", "authors": ["Akash Agrawal", "Christopher McComb"], "abstract": "Multi-fidelity Reinforcement Learning (RL) frameworks significantly enhance the efficiency of engineering design by leveraging analysis models with varying levels of accuracy and computational costs. The prevailing methodologies, characterized by transfer learning, human-inspired strategies, control variate techniques, and adaptive sampling, predominantly depend on a structured hierarchy of models. However, this reliance on a model hierarchy overlooks the heterogeneous error distributions of models across the design space, extending beyond mere fidelity levels. This work proposes ALPHA (Adaptively Learned Policy with Heterogeneous Analyses), a novel multi-fidelity RL framework to efficiently learn a high-fidelity policy by adaptively leveraging an arbitrary set of non-hierarchical, heterogeneous, low-fidelity models alongside a high-fidelity model. Specifically, low-fidelity policies and their experience data are dynamically used for efficient targeted learning, guided by their alignment with the high-fidelity policy. The effectiveness of ALPHA is demonstrated in analytical test optimization and octocopter design problems, utilizing two low-fidelity models alongside a high-fidelity one. The results highlight ALPHA's adaptive capability to dynamically utilize models across time and design space, eliminating the need for scheduling models as required in a hierarchical framework. Furthermore, the adaptive agents find more direct paths to high-performance solutions, showing superior convergence behavior compared to hierarchical agents.", "sections": [{"title": "INTRODUCTION", "content": "Multi-fidelity Reinforcement Learning (RL) frameworks [1\u20135] enhance design efficiency by leveraging high-fidelity simulations and low-fidelity alternatives like simplified physics simulations, reduced-order models, surrogate models, and partially converged results [6\u201310]. These frameworks, which employ strategies such as transfer learning [1,2], human-inspired approaches [3], control variate techniques [4], and adaptive sampling [5], rely on a hierarchical structure of models. This structure entails discrete levels of model fidelity, from low to high, with each level representing a distinct trade-off between computational cost and accuracy. However, this approach often overlooks the nuanced error distributions across the design space that transcend these discrete fidelity levels. These error distributions stem from diverse sources inherent to different types of low-fidelity models. They can vary widely due to factors such as model-specific assumptions and simplifications [6\u20139], discrepancies in model complexity versus the complexity of different regions within the design space [11\u201313], the variability in data availability and distribution across models [14\u201317] and challenges posed by hybrid modeling approaches [18,19]. To address this challenge, this work proposes an adaptive multi-fidelity RL framework, ALPHA (Adaptively Learned Policy with Heterogeneous Analyses), that leverages heterogeneous low-fidelity models alongside a high-fidelity model to learn a high-fidelity policy.\nDeep RL [20\u201323] is adept at exploring complex, non-differentiable, multi-dimensional designs spaces involving various objectives and constraints [3,24\u201333]. It excels in sequential decision-making, incrementally navigating through complex spaces for optimal long-term outcomes. This navigation becomes more efficient when the design task is part of a higher-level iterative design procedure [34]. Moreover, the adaptivity and noise tolerance of Deep RL make it a robust choice for dynamic design environments [35,36]. While RL offers numerous possibilities for advancing engineering design, its practical implementation is limited by the computational resources required for high-fidelity simulations (like computational fluid dynamics or finite element analysis). These simulations, indispensable for the evaluation of design alternatives, serve as the reward function in RL algorithms, often causing computational bottlenecks. This can delay project timelines and raise concerns over economic and environmental sustainability [37,38], limiting their applicability in engineering design.\nIn the pursuit of advancing computational efficiency of RL in design, multi-fidelity RL is emerging as a promising approach [1\u20135]. This approach strategically integrates both high-fidelity simulations, which provide accurate analysis of engineered systems at higher computational costs, and low-fidelity models, which offer computationally economical but less precise alternatives [6\u201310]. By leveraging the strengths of both high and low-fidelity models, multi-fidelity RL facilitates more efficient design exploration, especially when high-fidelity data is limited or expensive to obtain. The state-of-the-art methods including transfer learning [1,2], human-inspired approaches [3], control variate approaches [4], and adaptive sampling [5] have shown significant promise in diminishing computational demands. These methods leverage prior knowledge and/or implicit information acquired during the learning process regarding model accuracy. This insight, combined with key aspects of the multi-fidelity RL methodology, enables strategic decisions on model usage, balancing computational cost and solution quality of designs. However, these approaches predominantly rely on a rigid hierarchical structure of models. This hierarchical view assumes a monotonic relationship between model fidelity and accuracy, with each step up in fidelity offering superior performance across the entire design space. While this hierarchy simplifies model management, it oversimplifies the nuanced differences that can exist between models. In this work, we examine these models from the perspective of the error of the model over the design space. These errors are not uniformly distributed; instead, they can exhibit heterogeneity, varying significantly across different regions of the design space [10,39,40].\nThe heterogeneity of errors in low-fidelity models stems from diverse sources inherent to different types of low-fidelity models. For simplified physics models, the underlying assumptions may ignore or oversimplify certain phenomena that become critical in some regions of the design space [11\u201313]. For instance, the geometry of a design significantly influences the dominance of laminar or turbulent flow. This variation in flow type across different regions affects model accuracy because different models are typically calibrated for specific flow conditions. A model that performs well under laminar flow might not accurately capture the complexities of turbulent flow. Therefore, using a model that does not account for the dominant flow type in a particular region of the design space can lead to errors. Moreover, different low-fidelity models might adopt varying assumptions about the flow type and employ distinct methods to model that flow type, impacting model accuracy [11-13].\nSurrogate model frameworks for multi-fidelity RL alleviate some of the issues noted above, but are severely limited by the availability and distribution of data [14\u201317], especially in engineering design applications. The surrogate models available for exploration may have been prepared using distinct datasets, some of which may be more densely populated in certain regions of the design space. This disparity in the underlying datasets across the design space can lead to diverse error distributions of models. Furthermore, datasets with partially converged results of high-fidelity simulations, may also lead to heterogeneity in model errors. Hybrid models, which combine different modeling approaches, are typically used to reduce errors by leveraging the complementary strengths of each approach. For example, integrating empirical data with physics-based simulations [19] can enhance model accuracy by combining data-driven insights with the robustness of physical laws. However, when the complementarity between these approaches is not perfect, the hybrid model may still exhibit varying levels of accuracy across different regions of the design space. In such cases, the model might introduce additional complexity, as the different components of the hybrid model may have their own unique error characteristics that can interact in unpredictable ways [18]. To address such challenges, this work introduces an adaptive multi-fidelity RL framework, ALPHA (Adaptively Learned Policy with Heterogeneous Analyses), that leverages heterogeneous low-fidelity models alongside a high-fidelity model to learn a high-fidelity policy.\nThe rest of the paper is organized as follows. Section 2 introduces multi-fidelity RL and discusses several prevailing frameworks. In Section 3, we propose an adaptive multi-fidelity"}, {"title": "BACKGROUND", "content": "Multi-fidelity reinforcement learning is an approach that leverages data from multiple models of varying fidelity to enhance computational efficiency, particularly in contexts where high-fidelity data is scarce or costly to obtain. The term fidelity [41] denotes the accuracy or realism of a model, where high-fidelity models provide more accurate or realistic data at the expense of higher computational costs or time consumption. On the other hand, low-fidelity models, though less accurate, are computationally cheaper to interact with, offering a simplified representation of reality.\nIn the realm of engineering design, simulators are often used for training RL agents. Here, the notion of simulator fidelity is pivotal in characterizing the accuracy of the relationship between an engineered system and its performance attributes. High-fidelity simulators, often employing computationally intensive numerical methods (like computational fluid dynamics or finite element analysis), accurately capture the underlying relationships of interest, mirroring the behavior of real-world systems closely.\nConversely, low-fidelity simulators, which could entail simplified physics or partially converged results, offer a less accurate but computationally economical alternative. Low-fidelity models, such as reduced-order models and surrogate models, also provide a computationally efficient means to approximate the behavior of more complex systems with less accuracy [6-10].\nMulti-fidelity model-based and model-free reinforcement learning techniques have emerged as paradigms to harness varying fidelity levels in several fields including engineering design, robotics, and optimization. While all techniques rely on being aware of the highest fidelity level, several utilize additional insights about the hierarchy of simulators, including bounds on reward errors across fidelity levels [42]. Several methods leverage transfer learning approaches which can be further classified into unidirectional and bidirectional transfer. The former uses some heuristic to control the switch to higher levels, however it is typically not informed by knowledge across fidelity levels. For instance, Bhola et al. [1] utilized a reward convergence metric as the control criteria to switch to a higher fidelity level in an airfoil design problem tackled with Proximal Policy Optimization (PPO). In another work by Geng et al. [2], both the design configuration and policies were transferred from low to high fidelity in a Deep Deterministic Policy Gradient solver for a propeller design problem.\nOn the other hand, bidirectional transfer involves more explicit learning across fidelities by intricately tying fidelity switching to the learning of the transition and reward models across the levels in model-based approaches. For instance, Cutler et al. [42] employed a greedy tabular Q learning approach, wherein the transfer of Q values from low- to high-fidelity happens once a predefined number of steps exhibiting sufficient model learning of the former is encountered. Furthermore, while operating at a higher-fidelity level, the algorithm also informs updates to the models at the lower level. Concurrently, at each step, the algorithm checks the certainty of the models at this level. If uncertainty is encountered, it steps down to the lower level to enable continual learning including that of previously unnoticed dynamics. While using a greedy algorithm, the overall operation across fidelity levels still ensures a degree of exploration. Another work [43] extends this by employing Gaussian Processes (GPs) to learn the transition function, whereby the underlying variance, and the sum of variances of past experiences, informs the fidelity switching decisions. Moreover, they also investigated a model-free variant that learns optimal Q values using GPs.\nDiverging from transfer-based methods, Felipe Leno da Silva et al. [44] employed elitist heuristics in symbolic optimization problems, wherein better performers are biased to operate at high fidelity levels. Further, they utilize modified policy gradient objectives that are biased towards high-fidelity returns or conditioned with respect to a top-quantile Q measure. This technique showcases superior performance than transfer-based approaches. In another work, a multi-fidelity action value function estimator that is unbiased with respect to the highest fidelity is designed [4]. Specifically, it is based on a control variate approach that utilizes additional low-fidelity experience for the true low-fidelity estimate. It taps into the correlation between low and high-fidelity returns, leading to a reduced variance in Q estimates and a better resultant policy. In another line of work by Qiu et al. [45], a multi-agent study employs a depth-first search strategy to unearth local feasible policies on a low-fidelity simulator, which is further leveraged to formulate mixed policies alongside a baseline soft actor-critic policy for enhanced policy rollout.\nIn contrast to previous approaches that independently leveraged models of varying fidelity, Li et al. [5] utilized a low-fidelity and a multi-fidelity surrogate for training a DQN agent on a design problem. Initially, the agent learns with a low-fidelity model that is pretrained with low-fidelity data. This provides a broad understanding of the design space. Further, this trained agent guides the selective sampling of high-fidelity data, focusing on the most promising regions identified during the initial exploration. The combined low- and high-fidelity data are subsequently used to train a multi-fidelity surrogate. This multi-fidelity surrogate is employed to continue the training of the agent, ultimately leading to high-quality solutions at a reduced computational expense.\nThis work specifically builds on an earlier hierarchical framework [3] that progressively utilizes models from low- to high-fidelity within episodic design tasks to achieve a high solution quality at a reduced computational cost. In the current work, we extend the framework to adaptively handle heterogeneous low-fidelity models alongside a high-fidelity one to learn a high-fidelity policy."}, {"title": "METHODOLOGY", "content": "This work proposes a multi-fidelity RL framework, ALPHA (Adaptively Learned Policy with Heterogeneous Analyses), for design space exploration that can adaptively handle heterogeneous low-fidelity models alongside a high-fidelity. This section outlines the methodology used to construct and evaluate the framework. In Section 3.1, we propose the adaptive multi-fidelity framework based on the alignment of low-fidelity policies with the high-fidelity policy. Section 3.2 details the methodology for training and evaluating RL agents to assess their ability to adaptively utilize models across time and design space. Further, we describe a comparative study that evaluates the performance of ALPHA against a hierarchical framework."}, {"title": "ALPHA, An Adaptive Multi-fidelity Reinforcement Learning Framework", "content": "The adaptive multi-fidelity RL framework proposed in this work, referred to as ALPHA (Adaptively Learned Policy with Heterogeneous Analyses), builds upon a prior multi-fidelity framework [3]. Like that work, ALPHA aims to solve a skeletal design problem by starting with seed designs and iteratively tuning the continuous and discrete variables to minimize an objective, $f'$ while satisfying the constraints $G'$ and $H'$.\nFurthermore, similar to the previous framework and the techniques discussed in Section 2, we assume awareness of the highest fidelity level. However, unlike the previous framework in which a single agent controlled the entire process, ALPHA uses one agent for every analysis model, catering to the heterogeneity of models rather than using a hierarchical structure. Each design agent learns to take actions ($a_t$) in a design state ($s_t$) based on the feedback received in the form of scalar rewards (r). Specifically, each agent learns a policy ($\\pi$) to maximize the sum of rewards derived from the corresponding analysis model. The agent reward ($r_{t+1}$) measures the quality of the action $a_t$ that transitions the design from state $s_t$ to $s_{t+1}$, with objective and constraint components like the prior work [3]. The adaptive interaction of the agents with the design space is illustrated in Figure 1 and detailed in Algorithm 1.\nALPHA comprises one high-fidelity (HF) agent and multiple low-fidelity (LF) agents, with the case of two LF agents (LF\u2081and LF2) illustrated in Figure 1 and discussed in detail here. These agents interact with the design environment, collecting experience data used to update their respective policies. This experience data contains states, actions, and rewards, observed during interactions with the design space. During training, the degree of alignment between different LF policies and the HF policy will vary based on the underlying heterogeneity of the error distributions of the LF models. The proposed approach aims to prioritize using the HF agent in regions where all LF policies diverge significantly from the HF policy, ensuring targeted learning in critical areas. Conversely, regions where one or more of the LF policies align well with the HF policy can continue to learn using the aligned LF agents, thereby reducing computational costs.\nFor the case of two low-fidelity models, the alignment of both the low-fidelity policies $\\pi_{LF_1}$ and $\\pi_{LF_2}$ are measured with respect to the high-fidelity policy $\\pi_{HF}$. Specifically, cosine similarity between the mean of the action distributions of the policies $(\\pi_{LF_1}, \\pi_{LF_2}, \\pi_{HF})$ serves as the alignment metric. Further, an alignment threshold determines whether a particular LF model is aligned to the HF model or not. For instance, for the iteration illustrated in Figure 1, the alignment threshold with respect to the HF policy corresponds to an angle of 45 degrees. For this case of two low-fidelity models, there are four possible scenarios based on the alignment threshold: both LF\u2081 and LF2 align with HF; LF\u2081 aligns, but LF2 does not; LF2 aligns, but LF\u2081 does not; neither LF1 nor LF2 align with HF."}, {"title": "Analytical test optimization problem", "content": "The analytical test optimization problem in this work is adapted from the two-dimensional Ackley function [50], a standard benchmark in optimization. Specifically, we introduce two parameters corresponding to the shifted position of the global minimum and a scaling factor for the minimum function value at this point. These modifications offer the flexibility to prepare high- and low-fidelity models as discussed further below.\nThe modified Ackley function (f) is mathematically defined as follows:\n$f_{x_c,a}(x_1,x_2) = g_{x_c,a}(x_1,x_2) + h_{x_c}(x_1,x_2)$     (1)\n$g_{x_c,a}(x_1,x_2) = 20 - 20\\alpha e^{-0.2\\sqrt{0.5((x_1-x_{c_1})^2 + (x_2-x_{c_2})^2)}}$ (2)\n$h_{x_c}(x_1,x_2) = e - e^{0.5(cos 2\\pi(x_1-x_{c_1})+cos 2\\pi(x_2-x_{c_2}))}$      (3)\nwhere, $x_1$ and $x_2$ belong to the two-dimensional real plane ($R^2$) with bounds \u2013d \u2264 $x_i$ \u2264 d, for i = 1,2, d = 32.768, $x_c$ is the shifted location of the minima and \u03b1 is the scaling factor for the minimum value.\nWe utilize the above formulation to further prepare a high-fidelity (HF) model and two low-fidelity (LF\u2081, LF2) models. The contour plots of these models in the scaled domain (-1 \u2264 $x_i$ \u2264 1 for i = 1, 2) are shown in Figure 2. The high-fidelity model ($f_{HF}$) is prepared by the superposition of two Ackley functions, each having distinct locations and scaling factors:\n$f_{HF} = f_{(-0.5d,-0.5d),1} + f_{(0.5d,0.5d),1.5}$ (4)\nThe locations of the global minima of the individual Ackley functions are labelled in Figure 2(a) as A and B. The mean cost of this model on an Intel Xeon CPU used in this work is 275 \u03bcs. The low-fidelity models are prepared by varying the parameters $x_c$ and \u03b1 in $g_{x_c,a}$, while only utilizing the midrange value for $h_{x_c}$. Specifically, by setting \u03b1 as 0 for different $g_{x_c,a}$ components in the low-fidelity models, each model is tailored for different regions of the search space, without following a strict fidelity hierarchy. Furthermore, the locations of the minima are shifted by a different amount than the high-fidelity model to reflect the inaccuracies that are typical of low-fidelity models. These locations are labelled as C and D in Figures 2(b, c). Lastly, simplifying $h_{x_c}$ to a midrange value reduces the computational expense while still providing a reasonable estimate on average. The two low-fidelity models are mathematically defined as follows:\n$f_{LF_1} = g_{(-0.3d,-0.3d),1} + h_m + g_{(0.3d,0.3d),0} + h_m$ (5)\n$f_{LF_2} = g_{(-0.3d,-0.3d),0} + h_m + g_{(0.3d,0.3d),1.5} + h_m$ (6)\n$h_m = 0.5 \\times (e - e^{-1})$  (7)\nThe mean cost of these models on an Intel Xeon CPU used in this work is 32 \u03bcs.\nThe architecture of the policy neural network used in this case study is defined as follows:\n$I_2 - D_{1024,R} - D_{1024,R} - \\{ \\frac{(O_1)_{2,T}}{(O_2)_{2,SP}} \\}$\nwhere $I_{N_i}$ represents the input layer of size $N_i$, $D_{j,k}$ represents a dense (hidden) layer of size j with an activation denoted by k, R represents the ReLU activation, T represents the hyperbolic tangent activation, SP denotes the soft plus activation and $O_{N_o,k}$ represents the output layers of size $N_o$ with activation k. Specifically, the branches $O_1$ and $O_2$ correspond to the mean and standard deviation of the action distributions that are output by the policy. Similarly, the architecture of the value function neural network used in this case study is defined as follows:"}, {"title": "Octocopter design problem", "content": "The octocopter design problem involves a corpus of components and a flight dynamics simulator [51,52]. The components include batteries, motors, and propellers. The design space of the problem comprises a continuous variable for arm length and three ordinal variables for the choice of batteries, motors, and propellers from an ordered set of the components. Figure 3 illustrates the design artifact of an octocopter generated by assigning random values to the design variables. The reader is referred to prior work [52] for details on the corpus of components used in this problem. By considering all possible discrete values and merely 10 values for the continuous variable, the size of the combinatorial space is of the order of 106.\nThe design objective is based on a maneuvering task along a trajectory defined by a set of waypoints. Specifically, we define a maximization objective as follows:\n$Q = \\frac{d}{D} \\times \\bar{v} \\times (1-\\bar{e})$   (8)\nwhere d is the distance covered along the trajectory, D is total distance to be covered to complete the trajectory, $\\bar{v}$ is the normalized average speed of the maneuver, and $\\bar{e}$ is the normalized average error (deviation) from the specified trajectory. To emphasize, this objective aims at developing long-range, fast and stable quadcopters.\nThe flight dynamics simulator serves as high-fidelity (HF) model for this case study. The median cost of this model on an Intel Xeon CPU used in this work is 1.78 s. Additionally, the cost at the 25th percentile is 0 s, while at the 75th percentile it is 22.04 s, indicating significant variability in the range and speed of the octocopters. Notably, the zero second values represent octocopters that are not flyable due to interferences or incompatibility of the components. The low-fidelity models (LF\u2081, LF2) are prepared by training two neural networks on distinct datasets obtained from a partially converged optimization run for designing the octocopter. This optimization data was scaled using a min-max normalization technique. Figure 8 showcases the reduced dimensional Principal Component Analysis (PCA) space of this partially converged scaled data with the highlighted subsets reflecting the datasets used to train the low-fidelity models. The tailoring of each model to different regions of the search space leads to heterogenous models, contrasting with a rigid hierarchy and providing an effective testbed for the proposed framework.\nThe architecture of both the LF\u2081 and LF2 neural networks is defined as follows:\n$I_4 - D_{64,R} - D_{32,R} - O_{1,L}$\nwhere $I_{N_i}$ represents the input layer of size Ni, $D_{j,k}$ represents a dense (hidden) layer of size j with an activation denoted by k, R represents the ReLU activation, L represents linear activation, and $O_{N_o,k}$ represents the output layer of size $N_o$ with activation k. The size of the datasets used to train the LF\u2081 and LF2 models are 680 and 1358 respectively. The prediction accuracies of the LF\u2081 and LF2 models on the validation subsets of their respective datasets are 0.51 and 0.56, respectively. Furthermore, their prediction accuracies on the entire dataset shown in Figure 4 are 0.23 and 0.38, respectively. This implies that the low-fidelity models are indeed tailored to different regions of the space. Lastly, the mean cost of evaluation for both these models on an Intel Xeon CPU used in this work is 208 \u03bcs.\nThe architecture of the policy neural network used in this case study is like the previous case and is defined as follows:\n$I_4 - D_{1024,R} - D_{1024,R} - \\{\\frac{(O_1)_{4,T}}{(O_2)_{4,SP}}\\}$\nFurther, the architecture of the value function neural network used in this case study is like the previous case and is defined as follows:\n$I_4 - D_{1024,R} - D_{1024,R} - O_{L,1}$"}, {"title": "RESULTS AND DISCUSSION", "content": "Analytical test optimization problem\nThe RL policies were trained and evaluated with the proposed adaptive multi-fidelity RL framework (ALPHA), a hierarchical multi-fidelity RL framework from prior work [3], and with each of the low-fidelity 1 (LF\u2081), low-fidelity 2 (LF\u2082) and high-fidelity (HF) models individually. For the hierarchical framework, two configurations were used for training: 35% steps with LF\u2081, 35% steps with LF2, followed by 30% HF, and 35% steps with LF2, 35% steps with LF\u2081, followed by 30% HF. Specifically, 300 seed points were randomly sampled and utilized for training and evaluating all the policies. The results of the evaluation for the six cases (ALPHA, Hierarchical MFRL 1, Hierarchical MFRL 2, High-fidelity RL, Low-fidelity 1 RL, and Low-fidelity 2 RL) as per the high-fidelity model (QHF) are shown in Figure 9. Furthermore, Figure 10 depicts representative evaluation trajectories for all cases in the scaled problem domain, with the model contours overlaid on these trajectories.\nThe quality of the solutions for all the cases is better than the seed designs, indicated by the upward trend in all plots. However, the final solution qualities are significantly higher for the ALPHA case and Hierarchical MFRL 1 case as compared to all other cases. Further, the nature of the quality-iteration plots is drastically different between these two high performance cases. For the ALPHA case (Fig. 9(a)), there is a high dispersion in initial iterations followed by convergence to a high-quality solution. For the Hierarchical MFRL 1 case (Fig. 9(b)), the quality rises slowly when the low-fidelity models are operational. Further, a steep rise in quality is observed when the agent switches to the high-fidelity model. This difference between the agents is also reflected in the representative trajectories in Figure 10. Specifically, the adaptive agent follows a more direct path from the seed to the global optimum at A (0.5, 0.5). This contrasts with the Hierarchical MFRL 1 case wherein agent first moves towards the point C (-0.3, -0.3) as per the LF\u2081 model followed by a drastic change in direction as guided by the LF2 model for the next few iterations. Further, it changes direction again as per the HF model to converge near its global optimum at A (0.5,0.5). For the Hierarchical MFRL 2 case (Fig. 9(c)), the quality again rises slowly when the low-fidelity models are operational. Further, after switching to the high-fidelity model, a moderate increase in quality is observed. In Figure 10, this agent first moves towards the point D (0.3,0.3) as per the LF2 model followed by a drastic change in direction as guided by the LF\u2081 model for the next few iterations. Further, it continues in that direction to converge to a nearby high-quality local optimum at B (-0.5, -0.5) as guided by the HF model. These trends of the hierarchical cases indicate that the performance of the underlying framework is sensitive to the ordering as well as the proportions of model usage. This contrasts with the adaptive framework which is not restricted by a predefined schedule of model usage. For the High-fidelity RL case (Fig. 9(d)), we observe a steady increase up to a moderate solution quality, albeit with high variance. For the illustrated representative trajectory, we observe convergence to a local optimum for this case. This is potentially because the agent has not fully learned the underlying complexities of the high-fidelity model within the 300 episodes. Lastly, for the cases that just utilize one of the low-fidelity models (Figs. 9(e) and 9(f)), the agents converge to low quality solutions based on the operational model. The trajectories corresponding to these cases converge to the optima of the low-fidelity models at approximately C (-0.3, -0.3) and D (0.3,0.3), respectively.\nTo understand the adaptive learning process of the ALPHA framework, the proportion of usage of models is evaluated across time. Figure 11 shows the evolution of model usage across the training of the agent with four distinct regimes (R1, R2, R3, R4) in the trends. Figure 12 shows the policy maps corresponding to these regimes.\nIn the first regime R\u2081, the alignment threshold for the policies is the most relaxed. Further, the representative policy map at episode 40 in Figure 12 reveals that the agents have only learned some broad patterns, often leading to the e-greedy case where both LF policies are aligned. Accordingly, the proportion of model usage reflects the corresponding probability values of model selection on average. In the next regime R2, we observe an increase in the usage of the LF2 model and a complementary decrease in the usage of the LF\u2081 model at tighter thresholds values. This is in correspondence with the policy map at episode 120 in Figure 12. Specifically, we observe that LF2 policy is more aligned to the HF policy than the LF\u2081 policy in the first, second and fourth quadrants of the problem domain. In the regime R3, the usage proportion of both low-fidelity models drops with a complementary increase in the high-fidelity model usage. This is because of the misalignment of both LF policies at even tighter threshold values, as depicted in the policy map at episode 220 in Figure 12. The last regime R4 serves to increasingly refine the learning of the HF policy with negligible influence of the LF models. The final HF policy as depicted in the policy map at 300 episodes converges to the global optimum at (0.5, 0.5).\nTo further understand the adaptive nature of model selection across search space, we employ Moran's I to analyze the spatial autocorrelation of model usage proportions within a grid space of the scaled problem domain. Figure 13 shows the model usage proportions in this grid space. For the LF\u2081 model grid, Moran's I is 0.4463 with a p-value of less than 0.0001, indicating a moderate and statistically significant spatial clustering of model usage. This corroborates the observation that the LF\u2081 model is most frequently used in the third quadrant. Further, this aligns with the policy maps in Figure 12 and can be attributed to the proximity of the LF\u2081 model optimum at (-0.3, -0.3) to a high-quality local optimum of the HF model at (-0.5, -0.5). For the LF2 model grid, Moran's I is 0.1161 with a p-value of 0.0739, suggesting weak and statistically insignificant clustering of model usage. This corroborates with the observation that the LF2 model is widely used in first, second and fourth quadrants except in the proximity of (0.5, 0.5). This aligns with the policy maps in Figure 12 and can be attributed to the proximity of the LF2 model optimum at (0.3, 0.3) to the global optimum of the HF model at (0.5, 0.5). For the HF model, Moran's I is 0.3556 with a p-value of 0.0002, revealing moderate and statistically significant clustering of the HF model. This corroborates with the observation that the HF model is dominantly used in the region surrounding this global optimum at (0.5, 0.5). This can be attributed to the increased accuracy required near the global optimum, necessitating the high-fidelity model's precision. To summarize, low-fidelity models are used to explore broader regions, while the high-fidelity model focuses on critical areas, enhancing the efficiency and accuracy of the learning process. These trends showcase the ALPHA framework's adaptive use of multiple low-fidelity models alongside a high-fidelity model to achieve targeted learning across the search space.\nTo contextualize the quality-efficiency tradeoff of the ALPHA framework, the total time for evaluating the objective, and solution quality as measured by the high-fidelity model (QHF) was computed for all the agents. Figure 14 illustrates this tradeoff. Specifically, Fig. 14(a) shows the distribution of the quality of seeds and the solutions obtained using all the trained agents. Fig. 14(b) shows the total time required to evaluate the objective values during training. For the low-fidelity cases, we observe a low-quality of solutions obtained at a low computational cost. For the hierarchical cases, while both have a moderate computational expense, the solution quality is sensitive to the ordering of models. For the ALPHA case, we observe a high solution quality at a computational expense greater than the individual hierarchical cases. However, it is important to note that exploring different hierarchical schedules could incur additional time. Lastly, the high-fidelity case has the highest computational expense with a high variance in solution quality. This is potentially because the agent has not fully learned the underlying complexities of the high-fidelity model within the limited number of episodes. While training for longer may result in convergence to the global optimum, such extensive training will lead to even higher computational expense. These results showcase the capability of the proposed adaptive multi-fidelity framework to effectively balance solution quality with computational expense without the need for explicitly scheduling models."}, {"title": "Octocopter design problem", "content": "The RL policies were trained and evaluated with the proposed adaptive multi-fidelity RL framework (ALPHA), a hierarchical multi-fidelity RL framework, and with each of the models individually, similar to the previous problem. However, 1200 seed points and episodes were utilized for training and evaluating all the policies. A higher number was used to account for the high-dimensionality and complexity of the design problem.\nThe results of the evaluation for the six cases (ALPHA, Hierarchical MFRL 1, Hierarchical MFRL 2, High-fidelity RL, Low-fidelity 1 RL, and Low-fidelity 2 RL) as per the high-fidelity model are shown in Figure 15. Furthermore, Figure 16 depicts representative evaluation trajectories for all cases in a reduced dimensional space of the scaled problem domain. Specifically, this reduced space was prepared using PCA on all the evaluation trajectories obtained from all the agents and the search data of the adaptive agent. The latter is integrated into the evaluation data to better understand the adaptive use of different models in the search space as discussed further in this section.\nThe quality of the solutions for all the cases is better than the seed designs, albeit with drastically different trends across the agents. For the ALPHA case (Fig. 16(a)), there is a high dispersion in initial iterations followed by convergence to a high quality with low variance. This indicates that the model can consistently find high-quality solutions. For the Hierarchical MFRL 1 case (Fig. 16(b)), the quality does not improve when the LF\u2081 model is operational. Further, the quality rises slowly when the LF2 model is operational. Lastly, a steep rise to a moderate quality is observed when the agent switches to the high-fidelity model. However, this rise is not consistent across the iterations of the episode. For the Hierarchical MFRL 2 case (Fig. 15(c)), the quality rises significantly with a high dispersion in initial iterations when the LF2 model is operational. Further, after switching to the LF\u2081 model, the quality drops to a poor value. Lastly, the quality rises to a high value with the use of the HF model similar to the adaptive case (Fig. 15(a)), albeit with a higher variance. These trends of the hierarchical cases indicate that the performance of the underlying framework is sensitive to the ordering as well as the proportions of model usage. This contrasts with the adaptive framework which is not restricted by a predefined schedule of model usage. For the High-fidelity RL case (Fig."}]}