{"title": "AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual Alignment", "authors": ["Yuxin Zuo", "Wenxuan Jiang", "Wenxuan Liu", "Zixuan Li", "Long Bai", "Hanbin Wang", "Yutao Zeng", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "abstract": "Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual alignment. Our findings suggest that although LLMs also demonstrate promising cross-lingual alignment in Information Extraction, there remains significant imbalance across languages, revealing an underlying deficiency in the IE alignment. To address this issue, we propose AlignXIE, a powerful code-based LLM that significantly enhances cross-lingual IE alignment through two strategies. Firstly, AlignXIE formulates IE across different languages, especially non-English ones, as code generation tasks, standardizing the representation of various schemas using Python classes to ensure consistency of the same ontology in different languages and align the schema. Secondly, it incorporates an IE cross-lingual alignment phase through a translated instance prediction task proposed in this paper to align the extraction process, utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples, generated by our proposed LLM-based automatic pipeline for IE parallel data construction, with manual annotation to ensure quality. Ultimately, we obtain AlignXIE through multilingual IE instruction tuning. Although without training in 9 unseen languages, AlignXIE surpasses ChatGPT by 30.17% and SoTA by 20.03%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 63 IE benchmarks in Chinese and English under various settings, demonstrate that AlignXIE significantly enhances cross-lingual and multilingual IE through boosting the IE alignment.", "sections": [{"title": "1 Introduction", "content": "Multilingual Information Extraction (Multilingual IE) focuses on automatically extracting structured knowledge from unstructured or semi-structured texts across various languages, following manually designed schemas. Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023a,b), trained on massive multilingual corpora, have significantly advanced multilingual language processing. Previous studies (Zhang et al., 2023; Qi et al., 2023; Wang et al., 2024; Gao et al., 2024) have shown that LLMs exhibit spontaneous cross-lingual alignment to facilitate the transfer of abilities and knowledge across languages. Our findings suggest the presence of this alignment in IE, indicating a strong potential for improving the IE cross-lingual transfer. We first define IE parallel data, which is across various languages that share the same schema, sentences, and extracted instances. Previous studies proposed label projection to generate such data to alleviate the challenges of IE in low-resource (Kolluru et al., 2022; Hennig et al., 2023; Rios et al., 2024). By translation and manual annotation, we constructed an IE parallel dataset in Chinese based on CoNLL 2003 (Sang and Meulder, 2003), to evaluate the InstructUIE (Wang et al., 2023a). Table 1 shows the results. After training Flan-T5 (Chung et al., 2022) on English IE to obtain InstructUIE, there was a notable enhancement in Chinese IE (2.13\u219220.39), strongly supporting the presence of IE cross-lingual alignment. However, the significant performance gaps between languages (92.94 vs 20.39) indicate that the alignment remains weak. To address this issue, we propose two strategies to enhance IE cross-lingual alignment by aligning the schema and extraction process in different languages, thereby improving the cross-lingual and multilingual IE."}, {"title": "2 Code-Based Multilingual IE", "content": "AlignXIE formulates the IE tasks across various languages using a unified code generation framework. In this section, we introduce the instruction and completion of the proposed code-based multilingual IE. The instruction consists of two parts (\u00a72.1): (1) representation code of schema across different languages (\u00a72.1.1); (2) task prompts for completion (\u00a72.1.2). Subsequently, we present the completion format (\u00a72.2)."}, {"title": "2.1 Instruction Format", "content": null}, {"title": "2.1.1 Schema Representation", "content": "Class Definition The IE schema comprises mul-"}, {"title": "2.1.2 Task Prompt", "content": "The task prompt contains a docstring that defines the task and incorporates a string variable sentence in Python, which contains the text to be extracted. Then, the task prompt is concatenated to the schema, resulting in the instruction."}, {"title": "2.2 Completion Format", "content": "The completion represents the final output generated by the model, which begins after the assignment operation (i.e., results =). Each completion is a list, where each element is an instantiated object, representing the extracted structured knowledge instance. Additionally, we utilize the exec() function in Python to execute the instruction and completion of all code-based data in this work to ensure quality. We also use this function to execute the generated code to extract the results."}, {"title": "3 Training Framework", "content": "AlignXIE adopts a two-phase training framework based on instruction tuning. The first phase is the IE cross-lingual alignment phase, where we propose the task and data (\u00a73.1). Subsequently, we conduct the multilingual IE instruction tuning to obtain AlignXIE (\u00a73.2). Appendix D shows examples of instruction tuning data of the two phases."}, {"title": "3.1 IE Cross-Lingual Alignment Phase", "content": "First, we introduce the instruction and completion of the translated spans prediction task (\u00a73.1.1). To"}, {"title": "3.1.1 Translated Instances Prediction Task", "content": "The translated instances prediction task is the training objective of this phase, aiming to perform IE in the target language. The instruction starts with a task description, prompting the LLM to predict translated instances by concatenating the input-output pairs of source-language IE data with the input of target-language IE data to form the instruction. The completion contains the output of the target language data. Figure 1 shows an example."}, {"title": "3.1.2 IE Parallel Data Construction Pipeline", "content": "The construction of IE parallel data involves two critical components: span translation and schema translation. Spans refer to the desirable spans annotated in the text of the IE task (Kripke and Munitz, 1971; Chen and Yuille, 2004). Our pipeline mainly performs span translation, followed by a post-processing step that translates the schema and associates it with the corresponding spans (Onyshkevych, 1994; Milward and Thomas, 2000; Lu et al., 2022). Schema translation can be accomplished using LLMs and manual annotation. The primary challenge lies in the span translation, which involves two key issues: 1) Inaccurate translation: Either the spans or the sentences are inaccurately translated; 2) Missing Spans: The translated spans may be missing from the translated sentence. To address these issues, we propose a three-stage pipeline implementing contextual translation and multi-grained rephrasing, which is capable of generating parallel data for any IE task across any languages, as shown in Algorithm 1. The detailed prompts are shown in Appendix C."}, {"title": "3.1.3 ParallelNER", "content": "Since alignment training requires including bilingual IE samples within a single prompt, the excessive concepts in RE and EE result in insufficient context length of LLMs. Thus, we construct"}, {"title": "3.2 Multilingual IE Training Phase", "content": "Following the previous phase, we conducted multilingual IE instruction tuning with 46 IE datasets in English and Chinese. The detailed dataset statistics are shown in Appendix B. We follow the instruction and completion format introduced in Section 2."}, {"title": "4 Experimental Settings", "content": "AlignXIE enhances IE cross-lingual transfer by aligning the schema and extraction process. Our extensive evaluation of IE cross-lingual transfer can be divided into two main dimensions: external-language transfer (cross-lingual IE), and internal-language transfer (multilingual IE)."}, {"title": "4.1 Metrics", "content": "We use span-based offset Micro-F1 to evaluate the methods. For NER, an entity is considered correct if the span and type match a golden annotation. For RE, a relation is considered correct if its type, subject entity, and object entity match a golden annotation. For ED, an event is valid if its trigger and type match a golden annotation. For EAE, given event type and trigger, an argument is valid if the span and its role match a golden annotation."}, {"title": "4.2 Implementation Details", "content": "AlignXIE is fine-tuned based on Baichuan2-7B-Base (Baichuan, 2023). We utilize the LLaMA-Factory (Zheng et al., 2024) for instruction tuning. We use LoRA (Hu et al., 2022) for parameter-efficient fine-tuning and set the LoRA rank to 32. The warmup ratio is set to 0.01. The learning rates for these two phases are set to 3 \u00d7 10\u22124. We limit the sequence length to 4096 and set the batch size to 256. During the inference phase, we set the temperature to 0. All experiments are conducted on 8 x NVIDIA-A800 80G."}, {"title": "4.3 Datasets", "content": "We followed the IEPILE (Gui et al., 2024), YAYI-UIE (Xiao et al., 2023), and B2NER (Yang et al., 2024) to conduct a comprehensive evaluation. Overall, for English IE, we evaluate performance across 28 benchmarks for NER, 10 benchmarks for RE, 6 benchmarks for ED, and 3 benchmarks for EAE. For Chinese IE, we evaluate performance on 4 benchmarks for NER, 5 benchmarks for RE, 4 benchmarks for ED, and 2 benchmarks for EAE. For cross-lingual IE, we evaluate performance on the multilingual NER benchmark Multiconer22 (Malmasi et al., 2022b)."}, {"title": "5 Results", "content": null}, {"title": "5.1 Cross-Lingual Evaluation", "content": "To evaluate whether AlignXIE improves IE cross-lingual alignment, we conducted the cross-lingual evaluation on 9 unseen languages of Multiconer22. We have also supplemented the evaluation in Chinese and English. Table 2 shows results. AlignXIE significantly outperforms ChatGPT and GLINER in the cross-lingual setting, achieving a 30.17% and 20.03% improvement, respectively. For 4 unseen languages used in B2NER that constitute more than 0.1% of the general LLM pretraining corpus (Touvron et al., 2023c), AlignXIE outperforms B2NER by 19.36%, with results comparable to B\u00b2NER in both Chinese and English, demonstrating a significant improvement in cross-lingual transfer despite lacking supervised training data as described in \u00a74.4.\nSurprisingly, despite not having any training data in 9 unseen languages, AlignXIE has achieved performance comparable to the supervised baseline in Spanish and Turkish. The results demonstrate that AlignXIE exhibits excellent cross-lingual generalization for IE, which can be attributed to the enhancement of IE cross-lingual alignment."}, {"title": "5.2 Supervised Evaluation", "content": "The results for NER, RE, EE (including ED and EAE) tasks are shown in Tables 3, 4, and 5 respectively. AlignXIE outperforms the multilingual SOTA baselines on most benchmarks for four tasks and ranks within the top-2 results across RE and ED benchmarks. In the English IE, AlignXIE has achieved significant average improvements of 3.03 and 2.92 F1 points on the RE and ED tasks, with an improvement of 7.85 points on the kbp37 of RE. In the Chinese IE, AlignXIE has consistently achieved SoTA, with 4.12 points average improvement over the SoTA baseline on the EAE task. Moreover, it can be observed that the model not only achieves significant improvements in NER but also exhibits even greater enhancements in RE, ED, and EAE. This indicates that the alignment phase enhances the multilingual NER capability through cross-lingual alignment, which improves the performance of all tasks by leveraging the foundational spotting capabilities of NER.\nTo further demonstrate that AlignXIE improves multilingual IE across all languages through IE cross-lingual alignment, we also compared it comprehensively with SoTA monolingual (English) IE systems, which are fine-tuning on LLMs including InstructUIE (Wang et al., 2023a), UniversalNER (Zhou et al., 2024), GoLLIE (Sainz et al., 2024), KnowCoder (Li et al., 2024), GLINER (Zaratiana et al., 2024), and GNER (Ding et al., 2024). Appendix A.2 shows a detailed comparison with all baselines. Compared to 10 SOTA methods in the supervised setting, it achieves top-2 results on 14 (of 23) English NER benchmarks, indicating substantial enhancements. Specifically, when compared to other code-based IE systems,"}, {"title": "5.3 Zero-Shot Evaluation", "content": "Tables 6 and 7 show the zero-shot performance in English and Chinese, respectively. Since our method uses the data and setups of IEPILE, which differ from B2NER in data pruning, exclusion of dataset names, etc., the comparison with B2NER in zero-shot evaluation is unfair. These differences, as shown by Yang et al. 2024, significantly impact zero-shot performance, with the average gaps in English tasks reaching up to 9.3 points.\nAlignXIE achieved SoTA performance across all benchmarks in Chinese tasks, demonstrating substantial and consistent improvements in generalization ability. In English tasks, AlignXIE exhibited a noteworthy average increase of 4.24 points on the NER task. Similarly, in Chinese tasks, AlignXIE surpassed existing SOTA"}, {"title": "5.4 Ablation Study", "content": "We conduct comprehensive ablation experiments to investigate whether the code-based multilingual IE and IE cross-lingual alignment phase of AlignXIE contribute to the performance.\nCode-Based Multilingual IE We first evaluate the text-based (Wang et al., 2023a) and code-based multilingual IE. Table 8 shows results. To ensure a fair comparison, we remove all class comments in our code-based instructions. We use 6 English NER datasets, ACE 2005, AnatEM, BC2GM, BC5CDR, CONLL 2003, WNUT 2017, and 2 Chinese NER datasets, MSRA, ResumeNER to conduct instruction tuning on the Baichuan2-7B for supervised evaluation. The code-based method surpasses the text-based method by 0.59 points, with increases of over 1.00 points in 3 datasets, especially the Chinese datasets MSRA and ResumeNER. The results show that the code can mitigate schema differences between languages, thereby enhancing the cross-lingual alignment to improve the performance of multilingual IE.\nIE Alignment Training To show whether the IE cross-lingual alignment phase contributes to the performance, we further conduct an ablation study by removing the cross-lingual alignment phase of AlignXIE and denoting it as AlignXIE (w/o align phase). Table 9 shows the average performance of supervised evaluation on 4 tasks across 45 benchmarks. The results indicate that, compared to AlignXIE (w/o align phase), AlignXIE"}, {"title": "6 Related Work", "content": "Transfer Learning for cross-lingual IE Huang et al. (2017) introduced a zero-shot learning method for EE, facilitating generalization across unseen languages. Building on zero-shot approaches, Huang et al. (2022) utilized generative language models for cross-lingual EAE. Hsu et al. (2023) enhanced cross-lingual RE through prompt-based learning, reducing reliance on large annotated datasets. Zubillaga et al. (2024) emphasized the significance of typology in cross-lingual EE, particularly for low-resource languages like Basque. In contrast, AlignXIE focuses on leveraging the spontaneous alignment of LLMs to facilitate cross-lingual IE transfer on all IE tasks.\nLLMs for Multilingual IE YAYI-UIE (Xiao et al., 2023) proposes a chat-enhanced instruction tuning framework for UIE. IEPILE (Gui et al., 2024) collects a comprehensive bilingual IE instruction corpus, and B2NER (Yang et al., 2024) further designs a universal entity taxonomy. However, previous works overlook the mutual influence mechanisms between schemas in different languages and lack a unified representation.\nCode-Based IE Code-based IE aims to formulate the IE as a code generation task. CodeIE (Li et al., 2023) first leverages the Code-LLM and recasts IE tasks into Python function completion tasks. Code4Struct (Wang et al., 2023b) represents schema in Python classes for the EAE task. GoLLIE (Sainz et al., 2024) incorporates guidelines within class comments. Besides, KnowCoder (Li et al., 2024) utilizes class inheritance, class methods, and type hints to further enrich the schema representation. However, these works focus on the English IE task, and we are dedicated to exploring the significant role of code in multilingual IE."}, {"title": "7 Conclusion", "content": "In conclusion, this paper introduces AlignXIE, a code-based multilingual IE model that significantly enhances cross-lingual transfer through unified schema representation and an alignment training phase. AlignXIE utilizes Python classes to represent schemas uniformly across languages, ensuring semantic consistency in multilingual IE tasks. Additionally, our proposed cross-lingual alignment phase leverages high-quality IE parallel datasets ParallelNER, which are generated by our proposed LLM-based pipeline, to boost multilingual generalization. Comprehensive experiments demonstrate that AlignXIE achieves SoTA in Chinese and English IE under various settings, with remarkable cross-lingual capability."}, {"title": "Limitation", "content": "Extending multilingual IE training to languages beyond English and Chinese remains an exploration for future work. Meanwhile, the inclusion of additional languages would provide deeper insights into the cross-lingual alignment mechanisms within multilingual IE systems. Introducing parallel data from other IE tasks during the cross-lingual alignment phase also presents promising opportunities for further exploration. Furthermore, due to various limitations, we were unable to retrain the model or conduct a comprehensive evaluation using the B2NER framework. Addressing this issue will be a priority for future work."}, {"title": "A Experiments", "content": null}, {"title": "A.1 Pipeline Evaluation", "content": "We evaluate our pipeline on the label projection benchmark WikiANN (Pan et al., 2017), which is a multilingual NER dataset. We use 10 languages, including Bengali (bn), German (de), Spanish (es), Persian (fa), Hindi (hi), Korean (ko), Dutch (nl), Russian (ru), Turkish (tr), and Chinese (zh), and randomly sample 1,000 samples for each language. We compare our pipeline with CLaP (Parekh et al., 2024),, Awesome-Align (Dou and Neubig, 2021), and EasyProject (Chen et al., 2023). We mainly evaluate faithfulness following the setting of CLaP, measured as the percentage of instances where translated labels appear in the translated sentence, as accuracy evaluation requires costly native speaker rankings across methods, incurring high costs.\nTable 10 shows the result. We achieve 99% in faithfulness on average, outperforming the current SoTA Awesome-Align by 3% points on average. We attribute the achievement to the latter two stages, which rephrase the span with minimal disturbance to the sentence. Besides, other baselines experience significant performance drops in certain languages, such as EasyProject in Russia. In contrast, our pipeline achieves scores exceeding 93% across all languages, which strongly demonstrates the robustness and reliability of our pipeline. The results further substantiate the stability of our pipeline, demonstrating its capability to adapt effectively across different linguistic contexts."}, {"title": "A.2 Full Comparison of Evaluation", "content": "We conduct a comprehensive comparison in English between AlignXIE and all current SoTA methods including monolingual IE methods and multilingual IE methods under the supervised setting in Tables 11, 12, 13 and zero-shot setting in Table 14. In the NER task, we demonstrate top-2 results across 14 (of 23) benchmarks; in the RE task, we achieve top-2 results across 5 (of 8) benchmarks. Moreover, particularly in the ED and EAE tasks, we rank among the top-2 results across all benchmarks, further substantiating the significant efficacy of our method in multilingual IE.\nAdditionally, in the comparison of three benchmarks DIANN (Zavala et al., 2018), Ontonotes 5 (Hovy et al., 2006), and WNUT 2017 (Derczynski et al., 2017), utilized by other English code-based IE work (Sainz et al., 2024; Li et al., 2024), AlignXIE outperforms the other baselines, which further demonstrated that cross-lingual alignment can enhance monolingual IE.\nIn the zero-shot setting, we mainly compare our method with multilingual IE systems including IEPILE, and YAYI-UIE. Due to numerous different settings with B\u00b2NER, it is unfair to compare our method with B\u00b2NER. Our method achieves superior performance compared to other baseline approaches in the AI subset."}, {"title": "B Data Statistics", "content": null}, {"title": "B.1 Dataset Statistics", "content": "In this work, we conduct evaluations on 63 datasets, comprising 34 datasets for the NER task, 15 datasets for the RE task, 10 datasets for the ED task, and 5 datasets for the EAE task. To fairly compare with code-based methods Knowcoder and GoLLIE in demonstrating the effectiveness of our cross-language alignment, we also used PileNER (Zhou et al., 2024) for training. The detailed statics are shown in Tables 15, 16, and 17, respectively."}, {"title": "B.2 ParallelNER", "content": "In this section, we mainly introduce the statistics of ParallelNER, which is constructed from two datasets WikiNeural (Tedeschi et al., 2021) and CLUENER2020 (Xu et al., 2020). The detailed statistics are shown in Table 18."}, {"title": "C Prompts", "content": "Description Generation Using entity description generation as an example, we introduce the prompts we used including Description Initialization and Description Polish in Tables 19 and 20."}, {"title": "D Examples", "content": "We present the examples of instruction-tuning data for the IE cross-lingual alignment phase and the multi-lingual IE training phase in Figure 2 and Figure 3, respectively."}]}