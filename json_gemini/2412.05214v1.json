{"title": "AI's assigned gender affects human-AI cooperation", "authors": ["Sepideh Bazazi", "Jurgis Karpus", "Taha Yasseri"], "abstract": "Cooperation between humans and machines is increasingly vital as artificial intelligence (AI) becomes more integrated into daily life. Research indicates that people are often less willing to cooperate with Al agents than with humans, more readily exploiting AI for personal gain. While prior studies have shown that giving AI agents human-like features influences people's cooperation with them, the impact of AI's assigned gender remains underexplored. This study investigates how human cooperation varies based on gender labels assigned to AI agents with which they interact. In the Prisoner's Dilemma game, 402 participants interacted with partners labelled as AI (bot) or humans. The partners were also labelled male, female, non-binary, or gender-neutral. Results revealed that participants tended to exploit female-labelled and distrust male-labelled Al agents more than their human counterparts, reflecting gender biases similar to those in human-human interactions. These findings highlight the significance of gender biases in human-AI interactions that must be considered in future policy, design of interactive Al systems, and regulation of their use.", "sections": [{"title": "Introduction", "content": "From small-scale interactions in daily traffic to large-scale coordinated actions to tackle global warming and pandemics, cooperation between people is crucial at all scales of human social affairs. However, people's individual and collective interests are not always perfectly aligned. We often have to sacrifice some of our personal interests for the collective good, and we have to trust that others will not simply take advantage of our willingness to do so\u00b9. Many factors, including one's selfish pursuit of personal interests with disregard for others and in-group favouritism at the expense of out-groups, can hinder cooperation between individuals and groups2-4. And yet, despite these hurdles, we often opt to cooperate with others to attain mutually beneficial outcomes for all parties involved5\u20139.\nThis rise of artificial intelligence (AI) is reshaping our world. We may soon share roads with fully automated (self-driving) vehicles and work alongside robots and AI-powered software systems to pursue joint endeavours10. It is, therefore, crucial to investigate whether human willingness to cooperate with others\u2014especially when it is required to sacrifice some of one's personal interests\u2014will extend to human interactions with AI. While that is likely to vary across cultures and depend on people's general attitudes toward accepting new technologies11-\n16, recent studies showed that people cooperate significantly less with Al agents than with humans under similar conditions17\u201320. One reason for this reduced cooperation with Al is people's greater willingness to exploit cooperative AI agents for selfish gain compared to their desire to exploit cooperative humans21,22.\nA way to change people's perception of AI agents and, consequently, willingness to cooperate with them is to provide Al agents with human-like features23\u201325. For example, engaging in human-like discussion with a computer can increase people's willingness to cooperate with it18. However, the overall effects of human-like features of Al agents on human desire to cooperate with them, such as the display of human-like emotions, voice or looks, are mixed and vary across cultures24,26\u201328\nOne understudied anthropomorphic feature of AI agents\u2014yet perfectly familiar to anyone who has used a voiced GPS navigation guide or smart home assistant device is gender. There is some evidence that AI's assigned gender can influence people's behavioural dispositions, for example, willingness to donate money29, and that existing gender stereotypes affecting human-human interactions extend to human interactions with \"gendered\" voice computers30. Additionally, people have been found to perceive female bots as more human-like than male bots\u00b3\u00b9 and to assign human-like attributes, including gender, to AI-powered systems such as ChatGPT, the default perception of which as a male can be reversed when the chatbot's \u201cfeminine\u201d abilities (e.g., providing emotional support for a user) are highlighted32. However, how Al's assigned gender affects people's willingness to cooperate with interactive AI agents in mixed-motive \u201cwhat's-best-for-me-is-not-what's-best-for-us\u201d settings is largely unknown.\nThe Prisoner's Dilemma is a well-known economic game in which two players simultaneously make binary decisions, generating four possible outcomes of their independently made choices. The best outcome for both players collectively is when they both cooperate. However, each player has an incentive to defect and gain more individually at the expense of the cooperating player. These incentives might lead to a scenario where both players defect\u2014an outcome worse"}, {"title": null, "content": "for both compared to mutual cooperation1,2. The game has been extensively used to study people's willingness to cooperate with fellow humans and, more recently, the determinants of cooperation in self-learning algorithms and human willingness to cooperate with them17,21,33.\nThe Prisoner's Dilemma has also been used to study gender biases in people's willingness to cooperate with others. A meta-analysis of 272 studies covering 50 years of empirical work on this topic until 2011 found that there was no significant difference in cooperation rates across genders overall. However, men tended to cooperate with men more than women did with women. In contrast, in interactions between men and women, women tended to cooperate more than men34. This rather surprising finding was also replicated more recently35.\nWhile overall cooperation and defection rates were similar across genders, the reasons why men and women defect can vary. One popular hypothesis is that men defect primarily because of their selfish motives. At the same time, women do so because they fear that others will not cooperate with them (cooperation in the Prisoner's Dilemma pays only when one's co-player cooperates as well)34. Some empirical data supports this: women have been found to cooperate more than men in a variant of the Prisoner's Dilemma game, in which defection was associated with selfishness, but not with the fear that one's co-player may fail to cooperate36. In most previous studies, participants did not know their co-player's gender. In the few studies that investigated the effect (of the knowledge) of one's co-player's gender on one's willingness to cooperate, people of both genders cooperated more with women than with men, and women cooperated more than men across the board34,37.\nHowever, how Al's assigned gender may affect people's willingness to cooperate with it remains largely unknown. On the one hand, providing AI agents with gender may nudge people into treating them similarly to how they treat fellow humans, increasing people's willingness to cooperate with them. On the other hand, human interaction with gendered Al agents may produce unwelcome side effects, such as the reinforcement of gender stereotypes and the spillover of the impact of those stereotypes on human behaviour from human-human to human-AI interactions, or vice versa. Therefore, our two primary research questions are: 1) Will human willingness to cooperate with gendered AI agents in mixed-motive settings be similar to their willingness to cooperate with fellow humans? 2) Will any existing gender biases in human-human interactions in these settings extend to people's interactions with AI?\nTo find answers to these questions, we recruited participants to interact with gendered partners labelled either humans or AI-powered bots in an online Prisoner's Dilemma game (Figure 1). As the literature suggests, this game was particularly well suited for our purpose, given its prevalence in prior works investigating both human cooperation with AI agents and the effects of gender on people's willingness to cooperate with others."}, {"title": "Results", "content": "Do people cooperate similarly with AI agents as they do with humans?\nWhen comparing overall cooperation rates, participants cooperate slightly more with humans than with Al agents (52% vs 49%; Chi-squared test: $p = 0.138$; the sum of MC and IC behaviours in Figure 3). While this difference is not statistically significant, we see significant differences in participants' motives underlying their decision to defect. When participants defect against a human, 70.5% of the time this is due to a lack of trust that their partner would cooperate with them, MD (they predict their partner to defect), and only 29.5% of the time due to willingness to exploit their partner, E (they predict their partner to cooperate). This ratio, however, changes to 59.4% vs 40.6% when participants defect against an AI agent. Therefore, when participants defect against their partner, the motive to exploit their partner is more prevalent in participants' interactions with Al compared to interactions with humans (Chi- squared test, $p < 0.0001$; E in Figure 3)."}, {"title": null, "content": "Do people cooperate differently based on their partner's gender?\nParticipants generally cooperate with partners labelled as female more than with any other gender (regardless of the type of partner, human or AI). They cooperate the least with males (percentage of cooperators with females = 58.6%, males = 39.7%, non-binary = 50.0%, not identifying with a gender = 48.6%; MC and IC behaviours in Figure 4). Based on Monte Carlo simulations, these results were significantly different from the benchmark ($p < 0.001$; see Supplementary Information for full results).\nThe high cooperation rate with females is largely due to participants' high motivation and optimism about achieving mutually beneficial cooperation with them (MC against females =\n90.2%, males = 73.0%, non-binary = 87.6%, not identifying with a gender = 86.2%; Figure 4).\nThe low cooperation rate with males, on the other hand, appears to be largely driven by participants' lack of optimism about their (male) partners' cooperation (MD). Compared to all other genders of one's partner, the overwhelming majority of participants who defected against males did not trust that their (male) partner would cooperate with them (MD against females =\n51.4%, males =81.6%, non-binary = 55.2%, not identifying with a gender = 63.3%; Figure 4).\nWhen participants defect against a female partner, however, they are much more likely to exploit their partner for selfish gain (E). Compared to all other genders, people's motive to exploit is most prevalent in their interactions with females (E against females = 48.6%, males = 18.4%, non-binary = 44.8%, not identifying with a gender = 36.7%; Figure 4).\nGenerally, when examining people's behaviour towards different genders here, the pattern of behaviour exhibited towards males differs from that shown towards all other genders, whereas females, non-binary and those who do not identify with gender are treated similarly (one- sample t-test, separates males from the other three groups with $p = 0.05$ for overall cooperation rate, $p = 0.006$ for MC, $p = 0.02$ for IC, $p = 0.05$ for MD, and $p = 0.02$ for E. No other group is separated from the rest at a $p < 0.05$ significance level).\nIs people's willingness to cooperate with gendered AI agents similar to their willingness to cooperate with gendered humans?\nThe patterns in cooperation rates reported above remain the same when we separately analyse the data for the Human and AI partners. In both human-human and human-AI interactions, people cooperate with females more than they do with all other genders and cooperate the least with males. Comparing the prevalence of MC and IC behaviours across the two rows in each column of Figure 5, reported in Table 1, does not reject the null hypothesis that the cooperation rates across genders do not differ for human or AI partners; paired t-test, $p = 0.11$)."}, {"title": "Discussion", "content": "Our experimental study showed that people cooperate with gendered AI agents almost as much as they cooperate with humans. This differs from results reported in previous studies that showed people to cooperate significantly less with (genderless) AI agents than with humans17\u2013\n22. Thus, the answer to our first research question is affirmative: providing Al agents with human-like gender can increase people's willingness to cooperate with them. However, our results also showed that, despite the increase in cooperation, when people defect, their motive to exploit their partner is more prevalent in their interactions with Al compared to their interactions with humans, which aligns with previous results21,22.\nWe also found that people cooperate more with female partners than partners of any other gender, and they cooperate the least with males. This is consistent with other studies34,37. Our examination of the behavioural motives underlying these behavioural dispositions revealed that high cooperation with females was largely due to people's high motivation and expectation to achieve mutually beneficial cooperation with them. Low cooperation with males was largely due to a lack of optimism about one's male partner's cooperation. These differences in people's expectations about their female and male partner's cooperation are justified since women tend to cooperate more than men across the board. We also found that the pattern of people's"}, {"title": null, "content": "behaviour towards males was starkly different from that towards all other genders and that females, non-binary, and not identifying with gender were generally treated quite similarly.\nCrucially, we found the same results\u2014behavioural dispositions and motives\u2014in human interactions with gendered AI agents. Therefore, the answer to our second research question is also affirmative: existing gender biases in human-human interactions extend to people's interactions with AI. As such, the potential increase in human cooperation with Al agents thanks to Al's assigned human-like gender comes at a cost: unwelcome gender-specific exploitative behaviours found in human-human interactions will manifest themselves in human interactions with gendered AI agents too.\nConsistent with previous studies, we found that female participants are more cooperative than male participants34. We also found that among participants who defect, the motive to exploit their partner is more prevalent among male participants than it is among female participants. Additionally, we observed homophily in female participants: compared to baseline cooperation, female participants cooperated more with (human and AI) females and less with (human and AI) males. We did not observe that among male participants, which makes sense because participants of both genders cooperated less with males, largely due to a lack of trust that their male partners would cooperate with them.\nThis study used a one-shot Prisoner's Dilemma game. Our goal was not to investigate how cooperation may evolve in repeated interactions between the same two players over time. Repeated interactions expand the set of strategies available to players, for example, tit-for-tat reciprocation of cooperative and uncooperative actions, that can help bring about and sustain mutually beneficial cooperation. While people have been found to cooperate less with Al agents than with humans in repeated interactions too17, it would be fruitful in future research to investigate repeated interactions between humans and gendered AI agents.\nTo control country-level variability that may affect cooperation rates, we recruited all participants from a single country\u2014in this case, the United Kingdom. However, some cultural differences have been found in people's willingness to cooperate with others39, and there may be cross-cultural variability in gender-specific biases in human cooperation. Future work should also address these questions from the point of view of human interaction with gendered Al agents.\nIn addition, when we consider discrimination by humans against other humans, race and ethnicity are other important attributes which so often strongly influence the levels of observed discrimination41. Machines do not embody race, but their country of origin could be the basis of human discrimination against them. It may be for this very reason we can, in some cases, customise the accents of voiced AI systems according to our preferences. Future research in this domain would be highly welcome.\nObserved biases in human interactions with AI agents are likely to impact their design, for example, to maximise people's engagement and build trust in their interactions with automated systems. Designers of these systems need to be aware of unwelcome biases in human interactions and actively work towards mitigating them in the design of interactive AI agents."}, {"title": null, "content": "While displaying discriminatory attitudes towards gendered Al agents may not represent a major ethical challenge in and of itself, it could foster harmful habits and exacerbate existing gender-based discrimination within our societies. By understanding the underlying patterns of bias and user perceptions, designers can work towards creating effective, trustworthy AI systems capable of meeting their users' needs while promoting and preserving positive societal values such as fairness and justice."}, {"title": "Methods", "content": "Experimental design\nOur experimental method consists of a series of one-shot online Prisoner's Dilemma games. Here, we describe the study design (pre-registered: https://osf.io/38esk) in detail. This research complies with University College Dublin (UCD) Human Research Ethics Regulations and Human Research Ethics Committee (HREC) guidelines for research involving human participants. The research study protocol has been approved by UCD Human Research Ethics (HS-E-22-45-Yasseri). Informed consent was obtained from all human participants prior to the experiment.\nParticipants and payment\nIn July 2023, participants were recruited from the UK through a global crowdsourcing platform (Prolific). All participants were 18+ years old. Once recruited, they were redirected to an external website to participate in our experiment. All participants were anonymised, and no personal information (name, date of birth, etc.) was collected. Participants were assured that any information they provided in the study could not lead to their identification.\nParticipants (n=402; 223 female and 179 male) received a payment (flat rate calculated based on the duration of the experiment, approximately \u00a310.87 per hour on average) for their participation, as well as a payment based on their performance in the experiment (bonus reward, \u00a33.91 on average).\nPre-experiment Survey\nBefore the start of each experimental session, participants were asked to complete a survey (see Supplementary Information) that collected their demographic information.\nExperimental Trials\nIn a series of experimental trials, human participants played a well-known mixed-motive game: Prisoner's Dilemma. This game is well-established for evaluating cooperative dispositions17,\n21. Each participant was shown the game instructions, which explained the rules and how it is played, provided an example of the game, and explained how they would be rewarded (see Supplementary Information for a preview of the experiment)."}, {"title": null, "content": "In each round, participants then chose whether to cooperate (\u201cgo team\u201d) or to defect (\u201cgo solo\u201d) with their partner, resulting in them scoring points (that translates to a monetary reward). The cooperative choice was to \"go team\" (\u25a0) because mutual cooperation was better for both players (70 points each) compared to mutual defection (30 points each). However, each participant had a personal incentive to defect when expecting their partner to cooperate (scoring 100 instead of 70 points) - see Fig. 1 for the game's scoring table.\nTo ensure that each participant understood the game, we asked them to take a short quiz in which they had to indicate their score for a set of hypothetical combinations of their and their co-player's choices in the game. We did this to check their understanding of how to play and to filter out participants who did not understand the game properly. Participants who answered correctly were allowed to continue to play. A total of 402 participants passed this comprehension test and participated in subsequent experimental trials (55 participants failed the comprehension tests).\nEach participant played ten rounds of the Prisoner's Dilemma game, with a different partner in each round. As for the partner's decision, we randomly selected a decision from a uniform distribution. In this way, the level of cooperation/defection that we measure is purely a result of the participant's decisions and not the performance of their virtual partners.\nParticipants first played one round of the game with a partner labelled as a human and another with a partner labelled a \u201cbot\u201d (AI agent). The order of these two rounds was randomised. This was done to draw their attention to their partners' changing types and familiarise them further with the game.\nNext, participants played the game with eight differently labelled partners. The label specified whether the partner was a human or an AI bot and the gender with which that partner identified. Specifically, the gender labels were \u201cmale\u201d, \u201cfemale\u201d, \u201cnon-binary/fluid\u201d, or \u201cdoes not identify with a gender\" (see Fig. 1 for two examples). Participants were informed in advance that the gender their partner identifies with will be displayed to them. In the case of a AI partner, participants were told that \u201cartificial intelligent bots have learnt how to play by observing humans play the game, and by playing the game among themselves. At the end of their training, the bot is then required to identify as one of the following genders, based on their experience with the game with other humans or bots: Male, Female, Non-binary / Fluid, Does not identify with any gender\".\nThe partner's label (human/AI and gender) was visible on the screen throughout the trial. The order in which participants faced partners with different labels was randomised for each participant. In each round of the game, we recorded the participant's decision to cooperate (\u201cgo team\u201d) or not (\u201cgo solo\u201d) and their prediction about the partner's choice (cooperate or not)\u2014see Fig. 1.\nMeeting so many different partners in a fast sequence online might indicate to participants that their partners who were labelled as humans weren't real people. Although this was not a major concern to us, since we were primarily interested in comparing participants' decisions across the differently labelled partners that they faced, we simulated randomised waiting times (1 to"}, {"title": null, "content": "5 seconds) for getting a participant to play with a new partner between any two successive rounds of the game.\nPost-experiment Survey\nAfter completing all rounds, participants completed a second survey (see Supplementary Information) to examine their attitudes and motivations towards artificial intelligence."}, {"title": "Analysis and statistical benchmarking", "content": "In each experimental trial, we recorded a participant's decision, i.e., to cooperate with their partner (\u201cgo team\u201d) or defect (\u201cgo solo\u201d) and their prediction about their partner's choice (cooperate or defect). Based on the combination of the participant's responses, we determined the behavioural motive for each participant using the behavioural matrix in Fig. 2) and aggregated across treatments to obtain the counts of participants exhibiting the four possible behavioural motives: mutually beneficial cooperation (MC), exploitation (E), mutual defection (MD), unconditional or irrational mutual cooperation (IC), for each treatment combination (partner type and gender).\nHowever, the prevalence of any pair of a specific decision and the prediction about the partner's decision could be an artefact of an unbalanced number of choices made for decisions and predictions. For example, imagine a player who always predicts their partner will defect regardless of their type and gender. In this case, the correlation between their decisions and their prediction should not contribute to our calculation of the prevalence of any of the four types of behaviour (MC, E, MD, and IC) for any treatment group.\nTo determine whether the observed differences in counts of each behaviour in each experimental treatment were significant, we conducted Monte Carlo simulations to do the same counts, assuming there is no relationship between the participant's choice towards their partner and their prediction about their partner's decision. We generated a synthetic series of participants' decisions and synthetic series of participants' predictions about the partners' decisions by shuffling the order of decisions and predictions (preserving the overall counts of choices despite the reordering of each choice in the sequence), eliminating any causal relation between the pair of variables (participants' decisions and predictions). We conducted 100 iterations of the simulations and calculated descriptive statistics for the prevalence of each of the four behaviours in the simulated results (mean, standard deviation, confidence intervals). We compared the statistics of the four conditional observations (MC, E, MD, and IC) in simulated results to the actual observations found in our experiments (using a one-sample T- test). A statistically significant difference between the simulated and observed counts indicates the observed results are significantly different from what would be expected if we assume no causal relationship between a participant's decisions and their prediction of the partner's decision.\nIn our analysis of the post-questionnaire results, each individual response to each question was scored based on how positive the response towards AI was. For example, \u201cStrongly agree\" in response to the statement \u201cI am impressed by what Artificial Intelligence can do.\u201d received a"}, {"title": null, "content": "score of 2, whereas \u201cStrongly disagree\" received a score of -2. We reduced the dimensionality of all the responses to all questions using principal component analysis to produce a single overall principal component (pc1) score across all questions for each participant - this measures each participant's overall attitude towards AI. A positive/negative principal component score indicates a positive/negative attitude towards AI.\nFurthermore, we conducted a binomial logistic regression analysis to examine the effects of the independent variables-participant gender, their partner's gender, pc1 and the interactive effects between them\u2013on the binary dependent variable\u2013participant's choice to cooperate or defect-for human and AI partner treatments."}, {"title": "Analysing participant's gender and partner's gender interaction", "content": "Baseline Calculation\nTo calculate the influence of the interaction between the participant's gender and their partner's genders on the rate of a specific behaviour B, we calculated adjusted baseline rates for each pair type (Female-Female, Male-Male, Female-Male, Male-Female) based on the observed participant and partner-specific rates observed in the experiment.\nFor each interaction group, the adjusted baseline rate was calculated as the weighted average of the participant's baseline rate ($P_{participant}$) and the partner's baseline rate ($P_{partner}$).", "equations": ["Padjusted,Female-Female = (Nfemale_participants \u00d7 Pfemale_participant + Nfemale_partner \u00d7 Pfemale_partner) / (Nfemale_participant+Nfemale_partner)"]}, {"title": null, "content": "This approach assumes an equal contribution of participant and partner effects to the expected cooperation rate.", "equations": []}, {"title": "Odds and Odds Ratios", "content": "The odds of behaviour B were calculated for both observed rates and the adjusted baseline rates as $Odds = P(B) / (1 - P(B))$. For each interaction group, the odds ratio was calculated as:\n$Odds Ratio = Odds(observed) / Odds(baseline)$. The log-odds ratio for each interaction group was then computed as the natural logarithm of the odds ratio. Log-odds ratios indicate whether observed cooperation rates are higher or lower than the adjusted baseline, positive log-odds ratios indicate greater-than-expected prevalence of B and negative log-odds ratios indicate lower-than-expected prevalence.", "equations": []}, {"title": "Binomial test", "content": "We conducted binomial tests for each interaction group to assess the significance of deviations from the adjusted baseline. The null hypothesis (Ho) assumed that observed rates were equal to the adjusted baseline rates. For a sample size N and observed successes k, the p-value was calculated as $p = P(X >= k | n = N, p = Pbaseline)$, where X follows a binomial distribution.", "equations": []}]}