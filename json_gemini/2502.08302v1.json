{"title": "HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting", "authors": ["Shibo Feng", "Peilin Zhao", "Liu Liu", "Pengcheng Wu", "Zhiqi Shen"], "abstract": "Generative models have gained significant attention in multivariate time series forecasting (MTS), particularly due to their ability to generate high-fidelity samples. Forecasting the probability distribution of multivariate time series is a challenging yet practical task. Although some recent attempts have been made to handle this task, two major challenges persist: 1) some existing generative methods underperform in high-dimensional multivariate time series forecasting, which is hard to scale to higher dimensions; 2) the inherent high-dimensional multivariate attributes constrain the forecasting lengths of existing generative models. In this paper, we point out that discrete token representations can model high-dimensional MTS with faster inference time, and forecasting the target with long-term trends of itself can extend the forecasting length with high accuracy. Motivated by this, we propose a vector quantized framework called Hierarchical Discrete Transformer (HDT) that models time series into discrete token representations with l2 normalization enhanced vector quantized strategy, in which we transform the MTS forecasting into discrete tokens generation. To address the limitations of generative models in long-term forecasting, we propose a hierarchical discrete Transformer. This model captures the discrete long-term trend of the target at the low level and leverages this trend as a condition to generate the discrete representation of the target at the high level that introduces the features of the target itself to extend the forecasting length in high-dimensional MTS. Extensive experiments on five popular MTS datasets verify the effectiveness of our proposed method.", "sections": [{"title": "Introduction", "content": "Multivariate time series forecasting task has been applied to many real-world applications, such as economics, traffic, energy and weather. As a generative task, MTS forecasting presents challenges in two key aspects: the inherent high-dimensionality of the data distribution, and the long-term forecasting. To model the complex distributions of high-dimensional data, previous studies have established deep generative models in both autoregressive and non-autoregressive ways. To our knowledge, most of the work in the context of high-dimensional MTS has focused on short-term forecasting (predicted length: 24, 48).To improve long-term forecasting, various Transformer architectures have been proposed, but most are focused on low-dimensional scenarios. Effectively modeling high-dimensional distributions with longer forecasting lengths remains a challenge. A key issue is integrating deep generative models with sequence modeling frameworks to handle both high-dimensional data and long-term forecasting tasks.\nExisting works have several attempts to utilize various forms of deep generative models, such as Normalizing flows, Variational Auto-Encoder (VAEs), Diffusion models to model high-dimensional MTS. They apply deep generative models to the high-dimensional distributions over time, learning the patterns of distribution changes along the temporal dimension for precise prediction. Due to complex patterns and long temporal dependencies of MTS, directly modeling high-dimensional MTS distributions in the time domain can lead to issues of distribution drift and overlook the correlations between variables, limited to short-term forecasting settings.\nRecently, several attention-variant Transformer frameworks and LLM-based structures have been applied to long-term forecasting of MTS, showing excellent performance on MTS datasets. Building on the success of these methods, we identified two key modules: the series decomposition block, which uses moving averages to smooth periodic fluctuations and highlight long-term trends, and the discrete Transformer for MTS modeling. Inspired by these approaches, we first learn the discrete representations of the MTS and then incorporate the long-term trends of the forecasting target into our model. This allows us to enhance forecasting length capability with high accuracy.\nAs a discrete framework, Vector Quantized"}, {"title": "Methods", "content": "Our model comprises several key components. In this section, we present an overview of these components, which are divided into two stages. The training and inference details are shown in Algorithm 1, 2 and 3. Figure 1 provides an overview of the model architecture. In the stage 1, we have two types of VQGAN structures (Encoder, Quantization, Decoder): one is based on the discrete representation learning of the downsampled time series, and the other is based on the discrete representation learning structure corresponding to the prediction targets. Since the VQ strategy is operated on the channel dimension, the inter-variate correlations are captured in stage 1. In stage 2, a context encoder and a base Transformer decoder perform temporal cross-attention to generate discrete downsampled targets. The output from these low-level modules is then fed into a self-conditioned Transformer decoder to autoregressively predict discrete target tokens. This two-stage approach captures inter- and intra-correlations with discrete tokens, enhancing the accuracy of time series forecasting."}, {"title": "Stage 1: Modulating Quantized Vector", "content": "Series Downsample Module. According to the Autoformer , the moving average operation of non-stationary time series can smooth out periodic fluctuations and highlight long-term trends. As the objective of our work is to address the challenge of long-term forecasting in high-dimensional MTS, it is crucial for us to retain long-term patterns with the downsampled time series. For length-$\\tau$ input series $X_{pred} \\in \\mathbb{R}^{T\\times D}$, the process is:\n\n$X_{down} = AvgPool(Padding(X_{pred})),$ (1)\n\nwhere $X_{down} \\in \\mathbb{R}^{T\\times D}$ denotes the long-term pattern representations. Here, we introduce the $AvgPool(.)$ for moving average with the $Padding(.)$ to keep the series length unchanged. $X_{down}$ is the self-condition of targets, which consists of long-term patterns for the following future targets forecasting.\nDiscrete Tokenization using VQGAN. In the discrete representation learning of stage 1, the discrete learning modules of targets and downsampled targets show the same structure, which consists of an encoder and a decoder, with a quantization layer that maps a time series input into a sequence of tokens from a learned codebook. The details of these modules are provided in the Appendix C. Specifically, given any time series $X_{pred} \\in \\mathbb{R}^{T\\times D}$ can be represented by a spatial collection of codebook entries $z_{q_1} \\in \\mathbb{R}^{s\\times n_z}$, where $n_z$ is the dimensionality of quantized vectors in the codebook and $s$ is the length of the discrete token sequence. In this way, each time series can be equivalently represented as a compact sequence with $s$ indices of the code vectors. The quantization operates on the channel dimension, capturing inter-variate correlations. Formally, the observed target $X_{pred}$ and down-"}, {"title": "Stage 2: Modelling Prior Distribution with HDT", "content": "In this section, we introduce the details of the hierarchical discrete transformer. In stage 2, we establish a framework to estimate the underlying prior distribution over the discrete space for generating discrete time series tokens. This allows the post-quantization layers and the decoder from stage 1 to reconstruct the continuous targets. First, we present the overall generation process for the discrete tokens, as illustrated in Figure 1. Then, we detail the specific implementation procedures for both the low-level and high-level generation separately.\nLow-level Token Generation. This process can be considered a preliminary process of target token generation of high-level. Specifically, we now have the context data $X_{p} \\in \\mathbb{R}^{h\\times D}$ and the discrete representation of the downsampled target $S_{down} = \\{ z_{s_1},z_{s_2},...,z_{s_d}\\} \\in \\mathbb{R}^{s_d\\times n_z}$, where $h$ is the look-back window length and $D$ is the number of variates, $s_d$ is the length of discrete downsampled target sequence and $n_z$ is the feature dimension of the discrete representation. We formulate the training process by:\n\n$H_{p} = \\mathbb{E}_{T}(X_{p}),$ (9)\n\n$P(S_{down}|C) = \\prod_{i} P(z_{s_i}|S_{<i}^{s_d},c=H_{p}),$ (10)\n\n$L_{base} = \\mathbb{E}_{p(x)}[-log p(s_{down})],$ (11)\n\nwhere $\\mathbb{E}_{T}$ is the contextual encoder that is the Transformer encoder in our experiment. $H_{p} \\in \\mathbb{R}^{h\\times n_z}$ is the output of the context encoder and $L_{base}$ is the loss function of base Transformer decoder at the low-level framework. $p(z_{s_i}|S_{<i}^{s_d},c=H_{p})$ is to compute the likelihood of the full representation $p(S_{down}|C)$.\nWe then obtain the trained context embedding $H_{p}$ and the downsampled tokens $S_{down}$. Moreover, the discrete downsampled results directly impact the generation of high-level discrete targets, we explored three different methods for obtaining $H_{p}$. These methods are explained in detail in the subsequent experimental section.\nHigh-level Token Generation. After training the context encoder and base Transformer decoder in the low-level framework, we not only capture the content features of the context but also ensure that the discrete downsampled sequences retain long-term patterns. This provides additional conditions related to the target's own features in the high-level framework, thereby enhancing the accuracy of long-term forecasting. We have the discrete target $S_{pred} = \\{ z_{a_1},z_{a_2},...,z_{a_p}\\} \\in \\mathbb{R}^{s_p\\times n_z}$, $S_{down}$ and $H_{p}$, where the $s_p$ is the length of discrete target sequence. The process of autoregressively generating $S_{pred}$ can be described as follows:\n\n$p(S_{pred} |C) = \\prod_i p(\\frac{S_i}{\\tau_{t}}|z_{s_i}, c= \\{S_{down}, H_{p}\\} ),$ (12)\n\n$L_{self-cond} = \\mathbb{E}_{p(x)}[-log p(S_{pred})],$ (13)\n\nwhere the $S_{down}$ and $HP$ are fixed, the cross-attention of self-conditioned Transformer decoder is operating between the $S_{down}$ and $S_{pred}$, the temporal cross-attention is introduced to the $HP$ and $S_{pred}$, as shown in Figure 1. After completing the high-level training, we can input the discrete form of the target into the stage 1 decoder $G_{\\theta_t}$, to reconstruct the predicted target. Notably, unlike the popular diffusion models, the VQ discretization strategy effectively avoids the efficiency issues associated with iterative diffusion structures and autoregressive prediction methods."}, {"title": "Experiments", "content": "We conducted experiments to evaluate the performance and efficiency of HDT, covering short-term and long-term forecasting as well as robustness to missing values. The evaluation includes 5 real-world benchmarks and 12 baselines. Detailed model and experiment configurations are summarized in Appendix C.\nDatasets. We extensively evaluate the proposed HDT on five real-world benchmarks, covering the mainstream high-dimensional MTS probabilistic forecasting applications, Solar, Electricity , Traffic, Taxi and Wikipedia. These data are recorded at intervals of 30 minutes, 1 hour, and 1 day frequencies, more details refer to Appendix B.\nBaselines. We include several competitive multivariate time series baselines to verify the effectiveness of HDT. Previous work DeepAR, GP-Copula and Transformer-MANF .Then, we compare HDT against the diffusion-based methods, TimeGrad, MG_TSD, D\u00b3VAE, CSDI, SSSD, TSDiff with additional Transformer layers followed by S4 layer and TimeDiff. Among the MTS forecasting with VQ-Transformer, we introduce and VQ-TR for comparisons. The details of baselines are shown in Appendix F.\nEvaluation Metrics. For probabilistic estimates, we report the continuously ranked probability score across summed time series ($CRPS_{sum}$) , a widely used metric for probabilistic time series forecasting, as well as a deterministic estimation metric $NRMSE_{sum}$ (Normalized Root Mean Squared Error). For detailed de-"}, {"title": "Ablation studies", "content": "Effect of Discrete Representation $z_q$ in Eqn. (4). To verify the effectiveness of discrete representations in MTS, we conducted an experiment by bypassing the discretization of the intermediate variable $z$ in stage 1, directly inputting it into stage 2 for autoregressive generation via cross-attention with the context encoder. We tested this on three datasets (Electricity, Traffic, Taxi) with two prediction lengths (48 and 96), covering dimensions from 370 to 1214. As shown in Table 2, the continuous structure (C-Transformer) performed poorly in both probabilistic and deterministic scenarios. We believe that without discretization, $z$ acts as an infinitely large codebook, making it difficult for the stage 2 Transformer to fit properly. This highlights the effectiveness of our discrete Transformer structure.\nEffect of Historical Condition $H_P$ in Eqn. (13). To verify the applicability of discrete representations in multivariate time series, we set four different forms of historical sequences during the second stage of training: (i) HDT-hc: the continuous features from the stage1, not transformed into discrete form; (ii) HDT-hd: transformed into the corresponding discrete form in the stage 1; (iii) HDT-hd*: the discrete features, without entering the Encoder of stage 2 (iv) HDT-hdc: concatenation of discrete and continuous representations from the stage 1. We test on two high-dimensional and distinct types of multivariate time series and the results are shown in Table 3, the relatively stable and periodic Traffic, and the Taxi series, which is of higher frequency of fluctuations and more outliers. We observe that in the Traffic and Taxi of all prediction settings, HDT-hc performs obviously lower than HDT-hd and HDT-hd*, while HDT-hdc is com-"}, {"title": "Conclusion", "content": "In this paper, we propose a hierarchical self-conditioned discrete method HDT to enhance high-dimensional multivariate time series (MTS) forecasting. Our novel two-stage vector quantized generative framework maps targets into discrete token representations, capturing target trends for long-term forecasting. To the best of our knowledge, this is the first discrete Transformer architecture applied to high-dimensional, long-term forecasting tasks. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach. Future research will explore integrating multimodal data into MTS forecasting."}, {"title": "Appendix", "content": "In the supplementary, we provide more implementation details, more experimental results, and visualization of test samples of our HDT. We organize our supplementary as follows\n\u2022 In Section A, we give the Related Work of HDT, including vector quantization-based frameworks and deep generative model-based MTS two parts.\n\u2022 In Section B, we provide more details of used datasets and metrics in our experiment.\n\u2022 In Section C, we provide the experiment setup, including the hyperparameters and detailed structures of stage 1, 2 frameworks.\n\u2022 In Section D, we draw the comparison of the memory usage and inference time between HDT and other strong baselines, highlighting the source-efficient and efficiency.\n\u2022 In Section E, we show more experimental results wtih obvious non-stationary datasets Hospital and COVID Deaths with different prediction lengths {24, 48, 96} to compare the prediction performance with state-of-the-art generative methods.\n\u2022 In Section F, we provide the details of baselines in our main experiment.\n\u2022 In Section G, we summarize the limitations and showcase more test samples on seven MTS datasets."}, {"title": "A. Related work", "content": "Vector quantization-based frameworks\nUnlike many deep learning methods directly focusing on the continuous data domains, Vector Quantization-based frameworks map complex continuous domains into finite discrete domains. VQVAE decomposes the image generation process into two parts: initially, it trains a vector quantized autoencoder aimed at image reconstruction, transforming images into a compressed sequence of discrete tokens. Then the second stage learns an autoregressive model, e.g., PixelSNAIL , to model the underlying distribution of token sequences. Driven by the effectiveness of VQVAE and progress in sequence modeling, many approaches follow the two-stage paradigm. DALL-E improves token prediction in the second stage by using Transformers, resulting in a strong text-to-image synthesis model. VQGAN employs adversarial loss during its first stage, training a more efficient autoencoder, which allows for the synthesis of images with greater details.\nDeep generative model-based MTS\nTo improve the reliability and performance of high-dimensional MTS, instead of modeling the raw data, there exist works inferring the underlying distribution of the time series data with deep generative models"}, {"title": "B. Dataset and Metric Details", "content": "Dataset. We summarize the dataset details of our MTS long-term forecasting. As shown in Table 5, Solar is the hourly photo-voltaic production of 137 stations in Alabama State; Electricity is the hourly time series of the electricity consumption of 370 customers; Traffic is the hourly occupancy rate, between 0 and 1, of 963 San Francisco car lanes; Taxi is the spatio-temporal half hourly traffic time series of New York taxi rides taken at 1214 locations; Wikipedia is the daily page views of 2000 Wikipedia pages. Among them, the Solar shows certain periodic patterns, whereas the others predominantly display non-stationary characteristics.\nMetric. CRPS measures the compatibility of a cumulative distribution function $P$ with an observation $x$ as: $CRPS(F,x) = \\int(P(y) - I\\{x \\le y\\})^2dy$, where $I\\{x \\le y\\}$is the indicator function which is one if $x < y$ and zero otherwise. The empirical CDF of $P$, i.e., $P(y) = \\frac{1}{N}\\sum_{i=1}^{N}I\\{X_i \\le y\\}$ with $n$ samples $X_i \\sim P$ as the approximation of the predictive CDF. It utilizes $N$ samples to estimate the empirical CDF and take the CRPS-sum in the"}, {"title": "C. Detailed Experiment Setup andArchitectures", "content": "In this section, we summarize the detailed experiment setup of our HDT. Table 7 shows the hyperparameters of our overall structure in stages 1 and 2. {*} represents the hyperparameters used in our experiments. Table 8, 9 and 10 show the detailed modules of our HDT, among these components, Convld refers to the 1-d convolution operation, while the Self-Attn Block and Cross-Attn Block represent the standard multi-head self-attention and cross-attention of TransformerDecoderLayer, respectively."}, {"title": "D. Memory Usage and Model Efficiency", "content": "We comprehensively compare the performance of inferencetime and memory usage of the following models: TimeGrad,SSSD, TimeDiff, TSDiff and MG_TSD with our efficientdiscrete framework. The results are recorded with the officialmodel configuration and the same samples numbers=100. InFigure 4, we compare the efficiency under two representativetive datasets (963 variates in Traffic and 1214 in Taxi) withdifferent forecasting length (Traffic:96, Taxi:48) and same96 time steps for lookback.From Figure 4, we observe that HDT demonstrates a sig-nificant advantage in memory usage and inference time com-pared to diffusion models on high-dimensional multivariatetime series. It's evident that diffusion-based models tend toincrease diffusion steps significantly to enhance predictionperformance, especially in high-dimensional data. Forexample, MG_TSD introduces the concept of multiple gran-ularities, which not only suffers from the inherent limita-tions of autoregressive structures but also adds additionalinference results from different granularities, thereby furtherslowing down the inference speed and increasing model size.Non-autoregressive forms like TimeDiff sacrifice some in-ference accuracy to expedite inference speed. HDT, on theother hand, leverages a compressed discrete structure without relying on a large diffusion framework, which enhancesprediction accuracy by utilizing the target itself while ensuring the model is both source-efficient and time-efficient."}, {"title": "E. More Experiment Results", "content": "To validate the advantages of HDT in high-dimensional,non-stationary datasets, we added COVID Deaths and Hospital, as shown in Table 11. From Table 11, we demonstratesignificant performance improvements in the complex Hospital dataset. Notably, HDT-var.T, a discrete Transformervar.T, a discrete Transformerwithout a self-condition strategy, outperforms diffusion-based methods in short-term forecasting settings. However,it struggles to adapt to increased forecast lengths. HDT addresses this issue by leveraging its own trend, confirming itseffectiveness in long-term predictions for high-dimensionalsettings.Furthermore, to further demonstrate the effectiveness ofHDT in probabilistic forecasting of high-dimensional MTS,we introduce two metrics Prediction Interval Coverage Probability (PICP) and Quantile Interval Coverage Error (QICE)from TMDM and CARD, which are defined as follows:\n\n$PICP: = \\frac{1}{N} \\sum_{n=1}^{N}I \\{y_n \\le y_n^{high} \\}$ (16)\n$QICE : = \\frac{1}{M} \\sum_{m=1}^M \\frac{1}{N}\\sum_{n=1}^NI\\{y_{n}^{low_m} > y_n \\}$  (17)\n\nwhere ym = \\sum Iynylowm \u2022 In Ynyhighm, Yn low and ymhigh represent the low and high percentiles, respectively. In oursettting, we choose the 2.5th and 97.5th percentile, thus anideal PICP value for the learned model should be 95% andwe set M = 10, and obtain the following 10 quantile in-tervals (QIs) of the generated samples (generated sample\u0177 \u2208 RSXTXD, target y \u2208 RXD, S is the sample size=100of our setting): below the 10th percentile, between the 10th"}, {"title": "F. Details of baselines", "content": "In our experiments, we compared HDT against 5 types of models, which are shown as follows.\n1. Gaussian process based model\n\u2022 GP-Copula: It employs a separate LSTM unrolling for each time series, and models the joint emission distribution using a Gaussian copula with a low-rank plus diagonal covariance structure.\n2. Probabilistic Deep Learning model\n\u2022 DeepAR :A probabilistic model based on RNNs that learns the distribution parameters for predicting the next time point.\n3. Normalizing flow based models\n\u2022 Transformer-MAF : Replace the LSTM of the LSTM-MAF with the Transformer.\n4. Diffusion based models\n\u2022 TimeGrad : An auto-regressive model based on the diffusion model, which is used for generating each timestamp value autoregressively.\n\u2022 CSDI : A two types of Transformer based non-autoregressive diffusion model for generating multivariate time series.\n\u2022 D\u00b3VAE: A coupled diffusion probabilistic model with bidirectional variational auto-encoder (BVAE) for time series generation.\n\u2022 SSSD : Replaces the transformers in CSDI by a structured state space model to avoid the quadratic complexity issue with non-autoregressive way.\n\u2022 TimeDiff : A non-autoregressive diffusion model with future mixup and autoregressive initialization strategies for multivariate time series forecasting.\n\u2022 TSDiff : An unconditional diffusion model with self-guidance strategy for probabilistic time series forecasting\n5. Discrete vector quantization models\n\u2022 VQ-TR : Map large sequences to a discrete set of latent representations as part of the Attention module for time series forecasting."}, {"title": "G. Limitations and Visualizations", "content": "In the section, we summarize the limitation of this work and showcase ground-truths and generations on the five datasets of our main experiment, as shown in Fig. 6 to Fig. 11.\nLimitations: In HDT, we need to train a separate codebook for each dataset, as we have not yet achieved the discretization of all datasets under a unified codebook setup. It is important to note that due to significant distribution differences between various time series, discretizing all datasets with a single codebook is highly challenging. In the future, we plan to draw on the approach of MOIRAI to construct a unified discretized representation based on the unified time series modeling approach.\nVisualization: We showcase the visualized results cross five datasets with the corresponding prediction length used in our main experiment, which are shown in Fig. 6 to Fig. 11."}]}