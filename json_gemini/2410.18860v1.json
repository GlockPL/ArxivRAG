{"title": "DECORE: DECODING BY CONTRASTING RETRIEVAL HEADS TO MITIGATE HALLUCINATIONS", "authors": ["Aryo Pradipta Gema", "Chen Jin", "Ahmed Abdulaal", "Tom Diethe", "Philip Teare", "Beatrice Alex", "Pasquale Minervini", "Amrutha Saseendran"], "abstract": "Large Language Models (LLMs) often hallucinate, producing unfaithful or factually incorrect outputs by misrepresenting the provided context or incorrectly recalling internal knowledge. Recent studies have identified specific attention heads within the Transformer architecture, known as retrieval heads, responsible for extracting relevant contextual information. We hypothesise that masking these retrieval heads can induce hallucinations and that contrasting the outputs of the base LLM and the masked LLM can reduce hallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads (DeCoRe), a novel training-free decoding strategy that amplifies information found in the context and model parameters. DeCoRe mitigates potentially hallucinated responses by dynamically contrasting the outputs of the base LLM and the masked LLM, using conditional entropy as a guide. Our extensive experiments confirm that DeCoRe significantly improves performance on tasks requiring high contextual faithfulness, such as summarisation (XSum by 18.6%), instruction following (MemoTrap by 10.9%), and open-book question answering (NQ-Open by 2.4% and NQ-Swap by 5.5%).", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have emerged as powerful natural language generators, demonstrating remarkable capabilities across a range of tasks. However, LLMs are prone to hallucinations, where the model generates content that is not grounded in reality or misrepresents the facts. The tendency of LLMs to hallucinate undermines their reliability, especially when applied in high-stakes domains such as clinical decision-making or legal reasoning.\nUnderstanding the underlying mechanisms responsible for hallucinations in LLMs remains challenging. Wu et al. (2024) found that there are special attention heads responsible for retrieving relevant information from a given context, which they called \"retrieval heads\". While identifying these mechanisms is key to understanding LLMs, little research has explored how to use these insights to effectively mitigate hallucinations, which is the focus of our work.\nWe propose a novel decoding method termed Decoding by Contrasting Retrieval Heads (DeCoRe), as illustrated in Figure 1. This method builds on the assumption that masking retrieval heads can induce hallucination by impairing the ability of the model to retrieve relevant information from the context. DeCoRe leverages Contrastive Decoding to amplify the differences between the original and the hallucinating outputs, leading to more accurate final responses. Furthermore, we propose using the conditional entropy of the model's next-token distribution to control the contrastive decoding mechanism.\nOur findings show that DeCoRe significantly improves accuracy in tasks that require contextual faithfulness, such as XSum, MemoTrap, Open Book Natu-"}, {"title": "2 DECORE: DECODING BY CONTRASTING RETRIEVAL HEADS", "content": "DeCoRe operates by masking specific retrieval heads to trigger hallucinations and then employs a contrastive mechanism that penalises outputs resembling those from the hallucinating model, thereby amplifying the more accurate predictions of the base model. We further enhance this approach with a dynamic entropy-controlled mechanism to adjust the contrastive effect based on the entropy of the next token distribution of the model. Figure 1 illustrates this process over time."}, {"title": "2.1 MASKING RETRIEVAL HEADS", "content": "In this section, we describe how we mask retrieval heads in our base LLM to induce hallucinations, following the notation from Vaswani et al. (2017).\nGiven a base LLM $f_{base}$, let $x_{<t} = (x_1, x_2, ..., x_{t-1})$ be a sequence of previous tokens, where $x_i \\in X$ and $X$ denotes the vocabulary of the model. The logits for the next token distribution at time step t are given by $f_{base}(x_{<t}) \\in R^{|X|}$, and the probability of the next token $x_t$ is:\n\\begin{equation}\\label{}\nP_{base} (x_t | x_{<t}) \\propto exp (f_{base} (x_{<t}))\n\\end{equation}\nIn our approach, we derive a variant of the base LLM by masking a set of retrieval heads. We identify these heads using the method proposed by Wu et al. (2024), which involves analysing attention patterns on the Needle-in-a-Haystack (NitH; Kamradt, 2023) dataset. The NitH dataset is"}, {"title": "2.2 CONTRASTING BASE AND MASKED LLMS", "content": "Given the base and masked LLMs from Section 2.1, our goal is to improve the faithfulness of the generated output. To achieve this, we propose contrasting the next-token distributions of the base and masked models, effectively increasing the likelihood of the tokens selected by the former while decreasing the likelihood of the tokens selected by the latter. More formally, DeCoRe uses the following next-token distribution $p(x_t | x_{<t})$:\n\\begin{equation}\\label{}\np(x_t|x_{<t}) \\propto exp [(1 + \\alpha) log P_{base} (x_t | x_{<t}) - \\alpha log P_{masked} (x_t | x_{<t})].\n\\end{equation}\nIn Equation (7), the new next-token distribution $p(x_t | x_{<t})$ is defined by contrasting the next-token distributions of the base model $P_{base} (x_t | x_{<t})$ and the masked model $P_{masked} (x_t | x_{<t})$, introduced in Section 2.1; and $\\alpha \\in R$ is a scaling factor that controls the weight of the next-token distribution induced by the base model $P_{base} (x_t | x_{<t})$ and the one induced by the masked model $P_{masked} (x_t | x_{<t})$. The term $(1 + \\alpha) log P_{base} (x_t | x_{<t})$ in Equation (7) encourages the base LLM to predict a highly probably token under its distribution, while -$\\alpha log P_{masked} (x_t | x_{<t})$ penalises predictions that are also likely under the masked model's distribution."}, {"title": "2.3 DYNAMIC CONTRASTIVE DECODING", "content": "We propose a method to dynamically select the hyper-parameter $\\alpha$ using an uncertainty quantification approach, namely the conditional entropy, which is considered a reliable predictor for whether"}, {"title": "3 EXPERIMENT SETUP", "content": "Hallucinations in LLMs can generally be categorised into two types - factuality and faithfulness hallucinations. Factuality hallucinations refer to instances where the generated content is factually incorrect with respect to world knowledge. Faithfulness hallucinations refer to instances where the generated content fails to accurately adhere to the given source of information. Moreover, hallucinations tend to \"snowball\" in longer generation tasks such as multi-hop reasoning, compounding errors across multiple generation steps due to the inherently sequential behaviour of auto-regressive decoding in LLMs.\nIn this section, we describe our experimental setup to evaluate DeCoRe. We employ a diverse set of benchmarks to assess contextual faithfulness, factual accuracy, and multi-hop reasoning capability. Given that retrieval heads are important in correctly retrieving contextual information (Wu et al., 2024) and looking back over long reasoning processes , while attention heads play a significant role in information transfer between tokens, our experimental setup is designed to answer the following key research questions: 1) Can DeCoRe improve contextual faithfulness? 2) Can DeCoRe maintain or enhance the factual recall capabilities of LLMs? 3) Does coupling DeCoRe with CoT improve the multi-hop reasoning capability of the LLM?"}, {"title": "3.1 DATASETS AND EVALUATION METRICS", "content": "Faithfulness. We evaluate faithfulness on summarisation, instruction-following, and reading comprehension datasets. XSum is an abstractive summarisation dataset developed from BBC articles. We sub-sample 1,000 examples, following Chuang et al. (2024), and evaluate summaries using ROUGE-L, BERTScore, and factKB for factual consistency. MemoTrap tests whether models can avoid memorisation traps and adhere to the given instructions, with performance reported using macro- and micro-averaged accuracy. Instruction-Following Eval (IFEval) evaluates the ability of the models to follow instructions on a set of verifiable instructions such as \u201cwrite in more than 400 words\". The performance is reported using Prompt-level and Instruction-level strict accuracies, which measure the percentage of prompts where all verifiable instructions are followed, and the percentage of verifiable instructions followed overall. Open-Domain Natural Questions (NQ-Open) a QA dataset where we use an open-book configuration with one supporting document per question as described by Liu et al. (2024). NQ-Swap is a version of NQ where the answer entity in the context was replaced with another entity and is used to evaluate the faithfulness of the model to the modified context. We evaluate the models with the Exact Match (EM) metric and, following Kandpal et al. (2023) and Liu et al. (2024), we consider a prediction as correct if any sub-string of the prediction exactly matches any of the ground truth answers.\nFactuality. For factuality evaluation, we use four datasets\u2014TruthfulQA, TriviaQA, PopQA, and NQ-Open. TruthfulQA (Lin et al., 2022) (MC1, MC2, MC3, and Gen) is used to evaluate whether models can avoid common human falsehoods; MC1, MC2, and MC3 are multi-label classification tasks, and Gen is a generation task where evaluations use fine-tuned GPT models to assess the correctness and informativeness of the generated outputs. TriviaQA, PopQA"}, {"title": "3.2 MODELS", "content": "We evaluate two models from the Llama3 family Dubey et al. (2024), namely Llama3-8B-Instruct and Llama3-70B-Instruct, to analyse the influence of model size on the downstream results. In Appendix H, we also report results from other model families, such as Mistral (Jiang et al., 2023) and Qwen2 (Yang et al., 2024)."}, {"title": "3.3 BASELINES", "content": "We compare DeCoRe against six baselines: 1) Greedy decoding; 2) Contrastive Decoding (CD; Li et al., 2023), where LLaMA3-8B-Instruct serves as the amateur model and LLaMA3-70B-Instruct act as the expert model; 3) Context-Aware Decoding (CAD; Shi et al., 2024), a variant of CD where the amateur model is the same as the expert model but is not presented with the additional context; 4) Decoding by Contrasting Layers (DoLa; Chuang et al., 2023) that subtracts the logits in early layers to calibrate the final-layer logits. We evaluate two versions: DoLa-low (i.e., contrasting the first half of the layers with the final layer) and DoLa-high (i.e., contrasting the second half with the final layer); 5) Activation Decoding (AD; Chen et al., 2024) which uses the sharpness of context activations within intermediate layers to calibrate the next token prediction; 6) ITI that trains linear classifiers on TruthfulQA data to obtain \"factual\" heads and layers with corresponding \"factual\" direction vectors and then apply intervention during the decoding process. Note that ITI requires a training process on labelled data, whereas other baselines and DeCoRe are training-free. Also note that CAD is only applicable on tasks with additional context (i.e., XSum, open book NQ-Open, NQ-Swap, and open book MuSiQue). All implementation details are available in Appendix J."}, {"title": "3.4 DECORE VARIANTS", "content": "We evaluate three variants of DeCoRe: 1) DeCoRestatic, which employs a static scaling factor $\\alpha$ throughout generation; 2) DeCoReentropy, which entropy to dynamically adjust the strength of the contrastive decoding; 3) DeCoReentropy-lite, which is similar to DeCoReentropy, except that it employs a smaller LLM with the same vocabulary space as the masked LLM."}, {"title": "4 RESULTS", "content": "In the following, we present the evaluation results of DeCoRe across faithfulness, factuality, and multi-hop reasoning tasks. We show that DeCoRe mitigates faithfulness and factuality hallucinations, and improves the accuracy of the model when combined with CoT prompting. These effectively answer our research questions stated in Section 3. Additionally, we examine the impact of the number of masked retrieval heads on task performance. Finally, we demonstrate that DeCoRe reduces conditional entropy over time in long-generation tasks, contributing to more accurate outputs. These results highlight DeCoRe's broad effectiveness in enhancing LLM performance."}, {"title": "5 RELATED WORKS", "content": "Internal Mechanism of LLMs. Studies have attempted to dissect the inner workings of LLMs by focusing on layers, neurons, and attention heads. A seminal discovery in this area is the identification of induction heads, the attention heads that perform an induction algorithm by looking back over the context to predict a similar completion. Similarly, Wu et al. (2024) identified retrieval heads, a specific set of attention heads responsible for maintaining long-context factuality. These insights into the internal workings of LLMs is instrumental to our work, which focuses on these mechanisms to reduce hallucination. Our work leverages the idea that the masking of retrieval heads leads to hallucination.\nConstrained Decoding. Constrained decoding focuses on intervening during the generation process to reduce hallucinations. One example of constrained decoding is Inference-Time Intervention (ITI) which intervenes by probing and modifying attention heads or layers associated with model correctness. Contrastive Decoding (CD; Li et al., 2023) improves fluency and coherence by contrasting outputs from stronger expert LMs with those from weaker, smaller LMs. Building on CD, Shi et al. (2024) propose Context Aware Decoding (CAD) to mitigate con-"}, {"title": "6 CONCLUSION", "content": "DeCoRe (Decoding by Contrasting Retrieval Heads) is a novel decoding strategy that aims to reduce faithfulness and factuality hallucinations in LLMs. DeCoRe is based on the assumption that masking retrieval heads can induce hallucinations by limiting the ability of the model to retrieve relevant information from the given context. Specifically, DeCoRe uses retrieval head masking to create a version of the model that is more likely to generate hallucinations and combines it with the original model via a contrasting decoding scheme (Section 2.2). Furthermore, we propose a simple approach to control the strength of the contrasting decoding scheme by using the conditional entropy of the next-token distribution of the model (Section 2.3). Our experimental results show that DeCoRe significantly improves the accuracy of the model in tasks requiring contextual faithfulness and in some factual recall and reasoning tasks.\nLimitations. While DeCoRe improves the performance of the base model across most tasks, there is no \"free lunch\u201d; existing baselines may still produce more accurate results than DeCoRe in specific tasks (e.g., ITI in TruthfulQA or CAD in NQ-Swap). However, these baselines often offer limited improvements or may even generate less accurate responses in other tasks. We also observed that DeCoRe offers only marginal enhancements in factual recall tasks, suggesting that retrieval heads may not play a primary role in factual recall except for information transfer. Finally, while we propose using the conditional entropy of the model's next-token distribution to control the contrastive decoding scheme in DeCoRe, more \u201csemantic\u201d methods of uncertainty quantification may also be used."}, {"title": "A REPRODUCIBILITY STATEMENT", "content": "1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We claim to propose a novel training-free decoding strategy that leverages retrieval head mechanism, which we present as DeCoRe (Decoding by Contrasting Retrieval Heads).\n(b) Did you describe the limitations of your work? [Yes] See Section 6.\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix B.\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Our code is available at https://github.com/aryopg/DeCoRe. See details in Appendix J for more details.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We mention the implementation details including the hardware, libraries, implementation of the baselines, as well as task-specific setups in Appendix J. We also provide a justification of the number of retrieval heads to be masked in Appendix C.2. Additionally, we provide the full ablation study results of different number of retrieval heads in Appendix G.\n(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] We reported error bars for experiments requiring multiple runs (i.e., masking random heads in Figure 6 and Figure 8, along with their accompanying tables).\n(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix J.1.\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] See Section 3\n(b) Did you mention the license of the assets? [N/A] All used assets are open-source.\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] Our code is available at https://github.com/aryopg/DeCoRe.\n(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A]\n(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]"}, {"title": "B ETHICS STATEMENT", "content": "Our proposed method, DeCoRe, aims to mitigate hallucinations in LLMs, particularly in tasks where contextual faithfulness are critical. By improving the reliability of LLMs, DeCoRe has the potential to reduce the risks associated with incorrect or misleading information generation."}, {"title": "C RETRIEVAL HEADS", "content": "We follow the procedure provided by Wu et al. (2024) which defines the retrieval score of attention heads as the ratio of successful copy-paste operations. They propose to calculate the retrieval score by compiling three sets of Needle-in-a-Haystack samples. Given a question q and its corresponding answer k (the needle), we insert k in a given context x (the haystack) at a random position index range $i_q$. The language model is then tasked with answering q based on the haystack with the inserted needle. We set q and k unique and irrelevant with the given long context, ensuring that if an answer is correctly generated, it is indeed copied from the context, not from the model's internal knowledge. Retrieval score of head h is defined as:\n\\begin{equation}\\label{}\nretrieval\\_score_h = \\frac{|G_h|}{|K|}\n\\end{equation}\nWhere Gh is the set of tokens copy-pasted by head h. Retrieval score signifies the attention head ability to recall tokens from the given context, and can be used as a metric to identify retrieval heads in transformer-based LLMs."}, {"title": "C.2 RETRIEVAL SCORES", "content": "As shown in Figure 5, the retrieval scores for each model follow a similar pattern across all examined LLM variants. According to Wu et al. (2024), an attention head can be considered a retrieval head if it performs a copy-paste operation at least 10% of the time, which corresponds to a retrieval score of 0.1. In all the models evaluated, the retrieval scores drop below 0.1 just before reaching the 50th retrieval head. This indicates that beyond this number, the attention heads may not be reliably"}, {"title": "D PERFORMANCE OF BASELINE MODEL WITH MASKED HEADS", "content": "DeCoRe operates under the assumption that masking retrieval heads would cause hallucinations in LLMs. Therefore, the expected behaviour is that the performance of the LLM would go down the more retrieval heads that are masked."}, {"title": "D.2 FAITHFULNESS", "content": "Figure 6a illustrates the contrasting effects of masking retrieval heads (blue) and random heads (orange) on faithfulness evaluation tasks across XSum, MemoTrap, open-book NQ, and NQ-Swap. In XSum, masking retrieval heads results in a sharp decline in factKB scores $(r_{ret} = -0.93)$, indicating the critical role of retrieval heads in maintaining factual consistency in summarisation."}, {"title": "D.3 FACTUALITY", "content": "Figure 6b shows the effect of masking retrieval heads (blue) and random heads (orange) on factual recall tasks across TruthfulQA, TriviaQA, PopQA, and NQ Closed.\nIn TruthfulQA, masking retrieval heads has a negligible effect on the MC2 score $(r_{ret} = -0.06)$, while masking random heads shows a moderate negative correlation $(r_{random} = -0.80)$. This suggests that retrieval heads do not play a major role in answering truthful questions, and the decline"}, {"title": "D.4 CHAIN-OF-THOUGHT", "content": "The performance of the Llama3-8B-Instruct model with different numbers of masked retrieval heads on the MuSiQue dataset, both with and without Chain-of-Thought (CoT) prompting, is shown in Figure 6c. The table compares the closed-book and open-book settings to assess the influence of CoT on model performance. In the closed-book setting without CoT prompting, masking retrieval heads leads to a gradual performance decline, with scores decreasing from 7.41 (baseline) to 5.63 (with 100 masked heads). This indicates that the model's ability to reason through multiple hops is compromised as retrieval heads are removed. The decline of performance in the open-book setting without CoT prompting further indicates the importance of retrieval heads in open-book QA tasks.\nThe inclusion of CoT prompts generally boosts performance in both closed-book and open-book settings. Similar to the setup without CoT prompting, masking retrieval heads in the CoT setup decreases the performance gradually. Interestingly, in the CoT + open-book setup, masking only"}, {"title": "E.1 EVALUATION OF NON-REJECTION RESPONSES", "content": "Table 11: TruthfulQA Generation Evaluation excluding the rejected instances. Notice the rate of rejection that is very high on the instruction-tuned Llama3-8b."}, {"title": "E.2 EVALUATION COST", "content": "The fine-tuning of two davinci-002 models (to measure truthfulness and informativeness) costs approximately $43. While each run of evaluation is approximately $0.8."}, {"title": "F CORRELATION BETWEEN LENGTH-NORMALISED ENTROPY AND CORRECTNESS", "content": "One motivation to use the length-normalised entropy as a measure of how much information to contrast relies heavily on the premise that length-normalised entropy is a reliable proxy of answer correctness. To verify this assumption, we conducted statistical tests (Student's T-test (Student, 1908) and a Mann-Whitney U-test ) and to determine whether the length-normalised entropy of correct answers tends to be lower than that of incorrect answers."}, {"title": "F.2 STATISTICAL TESTS", "content": "The results of these statistical tests, as presented in Table 13, show that the differences in entropy between correct and incorrect answers are statistically significant across all models, with low p-"}, {"title": "F.3 REGRESSION", "content": "To further quantify the relationship between length-normalised entropy and answer correctness, we calculated the McFadden's pseudo-R2 for the logistic regression models fitted across the different setups (DeCoRe, Baseline, and DoLa). As shown in the regression plots"}, {"title": "G.1 EFFECT OF RANDOM HEAD MASKING ON TASK PERFORMANCE OF DECORE", "content": "As shown in Figure 8, the performance of DeCoReentropy exhibits different patterns when masking random attention heads compared to the targeted masking of retrieval heads in Section 4. A key observation is that the standard deviation is much larger across most tasks, indicating higher variability in performance when random heads are masked. This variability indicates that DeCoReentropy cannot benefit only from masking any random attention heads.\nIn XSum, we still observe a positive correlation between the number of masked random heads and task performance, though the correlation (r = 0.89) is weaker than that seen when masking retrieval heads. This suggests that masking random heads can still improve contextual faithfulness in summarisation, though the impact is less pronounced especially when considering the highest possible performance achieved by masking random heads.\nMemoTrap, which exhibits a strong positive correlation when masking retrieval heads, now shows a weak negative correlation (r = -0.34). This shift implies that random masking does not improve the model's instruction-following capabilities, and further suggests that the improvements seen were due to the targeted masking of retrieval heads. This supports the idea that retrieval heads play a key role in tasks requiring the faithful execution of instructions."}, {"title": "G.2 FAITHFULNESS", "content": "Table 14 accompanies Figure 3 (top) and Table 15 accompanies Figure 8 (top).\nIn the case of masking retrieval heads in DeCoRe entropy (Table 14), the results show different trends depending on the type of the task. In summarisation (XSum) and instruction following (MemoTrap) tasks, we can observe an increase in performance the more retrieval heads are masked. This indicates the importance of retrieval heads in these tasks, similar to the findings mentioned in Appendix D.2.\nHowever, the results show a different trend in open-book QA tasks (Open Book NQ-Open and NQ-Swap). In both Open Book NQ-Open and NQ-Swap, we can observe an increase in performance starting from masking 10 retrieval heads, and gradually goes down. In the case of Open Book NQ-Open, the performance is above the baseline variant until it drops below it when we mask 60 retrieval heads. While in the case of NQ-Swap, the performance remains above the baseline model"}, {"title": "G.3 FACTUALITY", "content": "Table 16 accompanies Figure 3 (bottom) and Table 17 accompanies Figure 8 (bottom).\nAs shown in Table 16, the results in TruthfulQA shows less clear correlation compared to other factuality evaluation tasks. For closed-book QA tasks like TriviaQA, PopQA, and Closed Book NQ-Open, a negative correlation is observed between the number of masked retrieval heads and performance. Similar negative correlations are observed when random heads are masked as shown in Table 17. The similarity in the performance degradation across both retrieval and random heads indicates that other model mechanisms might be responsible for factual recall."}, {"title": "G.4 CHAIN OF THOUGHT", "content": "Table 18 accompanies Table 3 to show the performance of DeCoRe entropy when masking retrieval heads across different setups of MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting, in both closed-book and open-book settings."}, {"title": "H ABLATION WITH OTHER LLM FAMILIES", "content": "Table 20 shows the performance of other model families (i.e., Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct) evaluated across faithfulness tasks with different decoding strategies. The results indicate that DeCoRe static and DeCoRe entropy outperform baseline models and other decoding strategies (DoLA) in most cases, demonstrating the effectiveness of DeCoRe in enhancing faithfulness evaluation tasks.\nFor Mistral-7B-Instruct-v0.3, both DeCoRe static and DeCoRe entropy perform competitively. Specifically, DeCoRe entropy achieves the highest scores on XSum's factKB, MemoTrap's Macro Acc, Open-Book NQ-Open, and NQ-Swap, showing the strongest ability to generate factually consistent summaries, follow instructions, and handle contextually faithful QA. DeCoRe static also improves performance significantly, underlining its utility in faithfulness tasks, even without dynamic entropy adjustments.\nFor Qwen2-7B-Instruct, DeCoRe entropy also leads in most tasks. It shows top performance on XSum's factKB, MemoTrap and Open-Book NQ-Open, indicating that it excels in generating factually consistent summary, following instruction, and answering complex QA questions. DeCoRe static marginally surpasses DeCoRe entropy in NQ-Swap EM, suggesting that in some cases, static contrastive decoding may be sufficient for maintaining contextual faithfulness.\nOverall, the trend observed across both model families confirms that DeCoRe, whether in static or entropy-controlled mode, provides significant improvements in maintaining contextual faithfulness regardless of the base model family, outperforming traditional decoding strategies like DoLA across summarisation, instruction-following, and QA tasks."}, {"title": "H.2 FACTUALITY", "content": "Table 21 compares the performance of Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct on factuality evaluation tasks using different decoding strategies. For Mistral-7B-Instruct-v0.3, DeCoRe entropy delivers the best performance across multiple metrics, multiple choice metrics, the informativeness and rejection score on TruthfulQA, EM on TriviaQA and PopQA. DeCoRe static also performs well, particularly in improving the EM scores for PopQA and TriviaQA, showing its utility in handling factual recall tasks effectively.\nQwen2-7B-Instruct shows a similar pattern. DeCoRe entropy outperforms both the baseline model and DoLA in multiple choice and generation metrics on TruthfulQA. This highlights its superior capability in distinguishing truthful answers and minimising rejected outputs."}, {"title": "H.3 CHAIN OF THOUGHT", "content": "Table 22 presents the performance of Mistral-7B-Instruct-v0.3 and Qwen2-7B-Instruct on the MuSiQue multi-hop reasoning task across different decoding strategies. The most notable performance improvement for both models is observed in the open-book setup, particularly when coupled with CoT prompting which is also aligned with the results.\nWithout CoT, the open-book setup already shows strong performance, with DeCoRe entropy outperforming both DoLA and the baseline model. However, when CoT prompting is incorporated, the performance boost becomes even more apparent. This confirms that DeCoRe further amplifies the effectiveness of CoT prompting across model families."}, {"title": "I ABLATION OF DECORESTATIC", "content": "DeCoRestatic uses a hyperparameter $\\alpha$ to control how much we want to contrast the prediction of the masked model from the base model, as shown in Equation (7). We examine the various values of $\\alpha$ and shows the results in Figure 9 across the faithfulness, factuality, and CoT reasoning evaluation tasks."}, {"title": "J IMPLEMENTATION DETAILS", "content": "We run all the experiments with NVIDIA A100 80GB GPUs. Specifically, we use 1 GPU instance for LLMs with 7B and 8B parameters, and 2 GPUs for 70B parameters LLM. We use the Hugging-face Transformers libraries (Wolf et al., 2020) and custom LLM model python classes from Wu et al. (2024) which contains the snippet to mask the attention heads. Our code is available at"}]}