{"title": "FAMMA: A BENCHMARK FOR MULTILINGUAL MULTIMODAL FINANCIAL QUESTION ANSWERING", "authors": ["Siqiao Xue", "Tingting Chen", "Fan Zhou", "Qingyang Dai", "Zhixuan Chu", "Hongyuan Mei"], "abstract": "In this paper, we introduce FAMMA, an open-source benchmark for financial multilingual multimodal question answering (QA). Our benchmark aims to evaluate the abilities of multimodal large language models (MLLMs) in answering questions that require advanced financial knowledge and sophisticated reasoning. It includes 1,758 meticulously collected question-answer pairs from university textbooks and exams, spanning 8 major subfields in finance including corporate finance, asset management, and financial engineering. Some of the QA pairs are written in Chinese or French, while a majority of them are in English. These questions are presented in a mixed format combining text and heterogeneous image types, such as charts, tables, and diagrams. We evaluate a range of state-of-the-art MLLMs on our benchmark, and our analysis shows that FAMMA poses a significant challenge for these models. Even advanced systems like GPT-40 and Claude-35-Sonnet achieve only 42% accuracy. Additionally, the open-source Qwen2-VL lags notably behind its proprietary counterparts. Lastly, we explore GPT 01-style reasoning chains to enhance the models' reasoning capabilities, which significantly improve error correction. Our FAMMA benchmark will facilitate future research to develop expert systems in financial QA. The leaderboard is available at https://famma-bench.github.io/famma/.", "sections": [{"title": "INTRODUCTION", "content": "Benchmarks have played a pivotal role in advancing AI research, particularly in the realm of large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023; Jiang et al., 2023; 2024; Meta, 2024). Benchmarks have been helping researchers track the advancement of LLMs in a variety of capabilities, including general language understanding and knowledge acquisition (Wang et al., 2019; Hendrycks et al., 2021a; Zhou et al., 2023; Wang et al., 2024b), code generation (Chen et al., 2021a; Liu et al., 2023; Jimenez et al., 2024), mathematical reasoning (Cobbe et al., 2021; Hendrycks et al., 2021b), tool use (Yan et al., 2024; Srinivasan et al., 2023; Trivedi et al., 2024), and legal reasoning (Guha et al., 2023). Meanwhile, we have seen a scarcity of high-quality benchmarks in financial reasoning, an area where practitioners are eager to benefit from LLMs.\nWe envision that LLMs will have a broad and significant impact in the finance industry, enabling intelligent systems that can assist human experts in various tasks such as risk management and predictive analytics. Towards this goal, high-quality benchmarks are needed to track the capabilities of LLMs in understanding financial knowledge and answering complex financial questions. Unfortunately, existing benchmarks in this domain cannot fully reflect the nature of daily work of financial"}, {"title": "RELATED WORK", "content": "The application of natural language technologies in finance dates back to the early 2000s, when sentiment analysis was used to analyze how media would impact stock market movements (Tetlock, 2007; Pang et al., 2008). Over recent years, the emergence of LLMs has inspired research in advancing financial industry with LLMs, including pretraining and fine-tuning LLMs with finance-related text (Wu et al., 2023; Yang et al., 2023), improving sentiment analysis with LLMs (Konstantinidis et al., 2024; Inserte et al., 2024; Cao et al., 2024), and building chatbots that specialize in finance knowledge (Chase, 2022; Stratosphere-Technology, 2023; Xue et al., 2023b; 2024).\nSeveral existing benchmarks can be used to evaluate these modern models and systems, including FiQA (Maia et al., 2018), FinQA (Chen et al., 2021b), ConvFinQA (Chen et al., 2022), FinanceBench (Islam et al., 2023), and FinBen (Xie et al., 2024). However, these benchmarks cannot reflect the nature of real problems that financial practitioners have to deal with on a daily basis. In particular, their data only has text but not data of other modalities; their data is only in English; their questions only test knowledge at a rudimentary to intermediate level. The finance-related questions in MMMU (Yue et al., 2024) involve data of other modalities like tables and charts. However, this general-purpose benchmark covers multiple disciplines (e.g., art, business, science, humanities, etc), and thus has a very limited coverage on finance-related questions. Our FAMMA benchmark makes a unique and focused contribution to the community on top of existing benchmarks: it has a much broader coverage on subfields of finance; its data is in multiple languages and of multiple modalities; its questions test advanced knowledge."}, {"title": "THE FAMMA BENCHMARK", "content": "FAMMA provides comprehensive coverage across eight key subfields: alternative investments, corporate finance, derivatives, economics, equity, financial statement analysis, fixed income, and portfolio management. These topics closely align with those taught in elite academic programs, such as Princeton's Master in Finance, as well as professional certifications like the CFA program. The dataset consists of both multiple-choice (55.5%) and open questions (45.5%). Additionally, 70.4% of the questions feature single-image scenarios, while 29.6% involve multi-image scenarios. Notably, 99.5% of the questions are accompanied by explanations. The questions are distributed across three difficulty levels and three most widely used languages in the finance industry (eFinancialCareers, 2022): English (78.8%), Chinese (14.4%), and French (6.8%). FAMMA is divided into a validation set and a test set. The validation set, useful for hyperparameter selection, contains 120 questions, while the test set comprises 1638 questions"}, {"title": "DATASET CONSTRUCTION", "content": "Question collection. We assembled a team of seven volunteer STEM researchers to create a comprehensive set of multimodal questions. Five are co-authors of this paper, while the other two are graduates from a Chinese university. Two annotators hold finance degrees, and the others have completed relevant coursework. These annotators draw upon open-source textbooks, exams, and other study materials , and apply their expertise to rewrite or create new questions when needed. The new questions are either entirely original, not present in the data sources, or enhanced versions of existing questions.\nThe annotators are tasked with selecting questions that require advanced, master-level, or professional knowledge to answer. This selection process is guided by aligning the questions with a minimum of CFA Level 1 difficulty (CFA Institute, 2024b), ensuring they meet industry standards of complexity. Additionally, selected questions must incorporate multimodal information, such as tables, images, or other visual data, to enrich the input and challenge the model's ability to process diverse formats. By following these criteria, we have curated a diverse set of approximately 2,000 questions, drawn from a wide range of authoritative sources.\nData quality control. We follow a two-stage data cleaning process to ensure the data quality.\n\u2022 In the first stage, we conduct a thorough review to correct formatting errors, fix typos, remove duplicate questions, and verify the accuracy of explanations. Each question is cross-verified by 2-3 annotators to ensure consistency and accuracy. Formatting errors and typos arise due to variations in the original sources, such as UTF encoding issues in Chinese and French texts, and the explanations are either provided by the source materials or written by annotators.\n\u2022 In the second stage, we classify each question into one of three difficulty levels\u2014easy, medium, or hard\u2014and label it with the appropriate subfield.\nThe difficulty levels are aligned with the concept-specific standards of the CFA curriculum (CFA Institute, 2024a). In addition, questions that require processing more complex information, such as multiple tables and images, are considered more difficult. In cases where the difficulty is ambiguous, the annotators use their judgment. Additionally, questions deemed overly simplistic\u2014such as those based purely on memorization or with answers that are obvious from the context\u2014are removed to maintain the desired level of challenge and to ensure they test knowledge and reasoning. The subfield annotation is determined by the explicit topics provided in the data source. If the subfield is not clearly specified, the annotators use their discretion to assign the most appropriate category based on the content of the question.\nThe JSON formats of the multiple-choice and open questions are illustrated in Listing 5 and Listing 6 in Appendix A, respectively.\nTo conclude, the FAMMA dataset offers a diverse range of questions, enabling the evaluation of models across various scenarios and allowing for fine-grained analysis of their performance."}, {"title": "EXPERIMENTS", "content": "EXPERIMENTAL SETUP\nBenchmarked MLMMs. We evaluate three cutting-edge closed-source models that are ranked among the top 10 on the Multimodal Arena Leaderboard (Lmsys Org, 2024):\n\u2022 GPT-40 (OpenAI, 2024a): The latest iteration in the GPT series, GPT-40 features enhanced capabilities in language and vision understanding, as well as improved generation performance.\n\u2022 Claude-3.5-Sonnet (Anthropic, 2024): Developed by Anthropic, Claude Sonnet introduces architectural innovations that improve multimodal dialogue and reduce harmful outputs.\n\u2022 Claude-3-Sonnet (Anthropic, 2024): An earlier version of the Claude Sonnet model.\nAdditionally, we assess a leading open-source model: Qwen2-VL (Yang et al., 2024) that achieves state-of-the-art performance on visual understanding benchmarks, including MathVista (Lu et al., 2024) and DocVQA (Mathew et al., 2021).\nGeneration process. MLMMs are instructed to understand the format and the structure of the questions, and return the response, under a zero-shot setting on our benchmark. The instruction prompts are designed to be straightforward and consistent across all models. Please refer to Listing 7 and Listing 8 in Appendix B for the prompts used to to guide responses to multiple-choice and open questions, respectively. During the final stage of generation for multiple-choice questions, we utilize both regex and GPT-4o to extract the corresponding lettered option from the response. Any discrepancies between the two methods will be manually reviewed and validated by annotators.\nLM-powered evaluation. During the evaluation process, we use GPT-4o as an LM evaluator to assess the accuracy of responses generated by LLMs for each question. The reported score represents the accuracy of these responses. Each response is categorized as either correct or incorrect, and the reported score reflects the average accuracy across the entire set of questions.\nThe LM evaluator is instructed to understand the format and structure of the questions, as well as to consider the key points in the ground-truth answers for evaluating the responses. Please refer to Listing 9 in Appendix B for the instructions provided for evaluating the answers. Note that for open-ended questions, where both gold and generated answers are provided, there is a single correct answer, making the 1-0 correctness straightforward to determine. We set the temperature of the LM evaluator as 0 to keep the evaluation results deterministic."}, {"title": "RESULTS AND ANALYSIS", "content": "Main results. We repeat the generation and evaluation process three times, and report the average result along with the standard error across all experiments. See the overall scores, breakdown by difficulty levels and languages in Table 2. We summarize the key findings as follows.\n\u2022 FAMMA presents a comprehensive challenge. Human performance sets the highest benchmark with an overall score of 56.96, leading across all difficulty levels. Among the models, GPT-40 ranks first with a score of 42.11, followed closely by Claude-35-Sonnet at 41.87. Both models fall approximately 15 points short of human performance and, based on our estimates, about 20 points below CFA professional levels. This substantial gap underscores the significant challenges FAMMA poses for MLLMs.\n\u2022 The open-source Qwen2-VL significantly lags behind more advanced closed-source MLLMs. According to its technical report (Wang et al., 2024a), Qwen2-VL has not been explicitly optimized for financial corpora, whereas the Claude family models prioritize finance as a key domain for evaluation and improvement (Claude, 2024). Interestingly, Qwen2-VL performs better on hard questions than on medium ones. A possible explanation is that hard questions often require higher computational complexity and advanced mathematical reasoning, areas where Qwen2-VL excels. In fact, its technical report highlights superior performance on MathVista (Lu et al., 2024), outperforming other MLLMs, including GPT-40 and the Claude models.\nTo conclude, the main results highlight the progress in MLMM QA in finance but also underscores the challenge of surpassing human-level performance.\nAnalysis I: model performance across different subfields. As shown in Figure 5, GPT-40 demonstrates the largest margin in economics, a social science discipline that studies the behaviour and interactions of economic agents. The result indicates its rich knowledge in social domains in addition to mathematics reasoning. GPT-40 also excels at financial statement analysis, whose context usually contains long tables , indicating its superior ability in table understanding (Kim et al., 2024). This well-rounded performance suggests that GPT-40 possesses a broad understanding of diverse financial concepts, excelling in knowledge-based and applied assessments.\nAnalysis II: model performance across different languages. Seen from Table 2, a consistent observation is that all models perform best in English, with GPT-40 and Claude-35-Sonnet comparably surpass the other competitors with a score around 44. For both GPT-40 and Claude-35-Sonnet, Chinese performance falls noticeably behind English, especially in harder categories, suggesting that there are significant gaps in how well these models handle complex tasks in Chinese. The comparatively low scores for Chinese might reflect the challenges related to tokenization or potentially smaller and less diverse training corpora in Chinese relative to English. In addition, Qwen2-VL, perform poorly on Chinese test, indicating it falls short of training on Chinese financial corpus (zhu, 2024).\nClaude-35-Sonnet outperforms its competitors in French, likely due to the Claude family's focus on optimizing non-English languages, such as Spanish and French (Claude, 2023; 2024). Interestingly, it achieves a higher score on hard questions compared to medium ones. It's important to highlight that the number of hard questions in French is limited to just 32 (see Table 8 in Appendix A), primarily covering portfolio management, derivatives, and fixed income-three subfields where Claude-35-Sonnet consistently excels with high scores.\nOverall, GPT-40 and Claude-35-Sonnet demonstrate robust, well-rounded language skills, while Qwen2-VL and Claude-3 show areas for improvement, particularly in non-English contexts. This suggests that language support and training on diverse financial corpora play a key role in overall model performance in multilingual financial QA tasks.\nAnalysis III: error characterisation. To investigate the limitations of current MLLM capabilities on FAMMA, we conduct a comprehensive analysis of error types observed in our evaluation. We meticulously examined 100 randomly sampled error instances from GPT-40's generations in a single experiment run. The sample includes 76 responses in English, 14 in Chinese, and 10 in French, with a distribution across difficulty levels of 30 easy, 30 medium, and 40 hard.\nThrough our analysis, we identified five common error types and categorized all instances accordingly, with detailed examples provided below and in Appendix B.2:\n\u2022 Data misinterpretation (DM): errors where the model fails to correctly interpret the input data, whether textual, visual (e.g., charts, tables), or a combination of both. For instance, as shown in Listing 11 in Appendix B.2, GPT-40 incorrectly reads a number from a figure, where a small section of the image has low resolution due to the data source.\n\u2022 Incomplete context understanding (ICU): errors where the model fails to understand the full context of the question or misses critical details. See Listing 12 in Appendix B.2 for an example that GPT-40 overlooks the option and reaches to a wrong result.\n\u2022 Numerical inaccuracy (NI): errors involving incorrect calculations or misinterpretation of numerical data. Listing 13 in Appendix B.2 presents a response where GPT-40 produces incorrect decimal values during a square calculation.\n\u2022 Domain knowledge gaps (DKG): errors where the model lacks sufficient understanding of specific financial concepts or practices.\n\u2022 Ambiguous answer generation (AAG): errors where the model provides vague or incomplete responses that do not fully answer the question, or fails to clearly and correctly align its internal computation with the correct final choice. \nAnalysis IV: common error patterns. Seen from Figure 6, a significant portion, 42.5%, of errors is attributed to DKG, highlighting that GPT-40 struggles to handle financial domain knowledge, which may indicate limitations in its training on financial data. AAG accounts for 17.5% of the errors, indicating that the model sometimes generates unclear or imprecise responses despite correctly understanding the question. NI represents 17.5% of the errors, suggesting that GPT-40 still faces challenges with precise calculations or handling numerical values correctly. The remaining two error types are less frequent but still notable, showing some difficulty in interpreting context or visual data. These findings underscore the need for improvements in finance domain-specific training, and answer generation to enhance GPT-40's performance.\nAnalysis V: errors across languages and difficulties. In all languages, DKG consistently account for the highest proportion of errors, with French leading at 46.08%, indicating GPT-40 struggles with understanding finance domain-specific knowledge, particularly in non-English contexts. Interestingly, AAG error emerges as the second most common error type across all three languages, suggesting that despite differences in language complexity, GPT-40 often provides unclear or incomplete answers regardless of the language.\nWhen analyzing error types across difficulty levels, AAG dominates in both easy (34.33%) and medium (36.49%) categories, while DKG takes the lead in hard questions (57.08%). This shift suggests that for easier questions, the model tends to generate ambiguous answers, likely due to overgeneralization or incomplete interpretations. However, as the complexity increases, the GPT-40's lack of domain knowledge becomes more evident.\nAnalysis VI: can RAG or 01-reasoning chain help? We explore two independent methods for improving GPT-40's performance on FAMMA:\n\u2022 Retrieval augmented generation (RAG): we augment GPT-40 with external financial knowledge base by incorporating content from textbooks \u201cCFA Level III SchweserNotes, Books 1-5, 2023\", which comprehensively cover most of the topics included in FAMMA.\n\u2022 Dynamic Chain-of-Thought (COT) prompting: we implement 01-style reasoning chains (OpenAI, 2024b), where at each step, GPT-40 can either proceed to the next reasoning step (by trying multiple methods, exploring alternative answers, or questioning previous solutions) or provide a final answer. The process begins with a system prompt that includes instructions to guide a step-by-step reasoning approach. Once the problem is introduced as a user message, an assistant message is pre-loaded to establish a standardized starting point for the GPT-40's response generation.\""}, {"title": "CONCLUSION", "content": "In this paper, we introduced a comprehensive benchmark for multilingual multimodal QA within the financial domain, addressing the growing need for robust systems that can interpret and respond to queries in various languages while leveraging diverse data modalities."}, {"title": "DATASET DETAILS", "content": "Data source. The question-response pairs are primarily collected from free online resources, quizzes, textbooks, and other study materials.\nData format. Following data validation, we provide the following information for each question:\n\u2022 Question ID: a unique identifier for the question acroos the whole dataset.\n\u2022 Context: relevant background information related to the question.\n\u2022 Question: the specific query being asked.\n\u2022 Images: directories of images referenced in the context or question.\n\u2022 Options: a list of possible answers, applicable only to multiple-choice questions.\n\u2022 Question type: categorized as either multiple-choice or open-ended.\n\u2022 Main question ID: a unique identifier for the question within its context; questions with the same context share the same ID.\n\u2022 Sub question ID: a unique identifier for the question within its corresponding main question.\n\u2022 Answer: a concise and accurate response.\n\u2022 Explanation: a detailed justification for the answer.\n\u2022 Images for explanation: directories of images supporting the explanation.\n\u2022 Subfield: the specific area of expertise to which the question belongs, categorized into eight sub-fields.\n\u2022 Language: the language in which the question text is written.\n\u2022 Difficulty: a measure of the question's complexity based on the level of reasoning required."}, {"title": "HUMAN PERFORMANCE ESTIMATION", "content": "Based on the analysis of CFA exams (see https://300hours.com/cfa-passing-score/), the passing score is approximately 68% for all the three levels. During the annotation process, the difficulty levels of FAMMA's questions-easy, medium, and hard-closely correspond to those of CFA Levels I, II, and III. For the medium questions, we assume those who fails the Level I will have a score of 50% on Level II, therefore the corresponding score for Level II over the whole population is 68% * 68% + (1-68%) *50% = 62.24%. By similarly assuming those who are not qualified for Level III will have a score of 40% on Level III, the expected score of the whole population for Level III becomes 62.24%*68%+(1-62.24%)*30% = 57.26%. In this context, we set the human score for easy, medium, and hard questions to be equal to that of Level I, II, III\u201468%, 62.24%, 57.26%, respectively, which resulting a overall score of 59.86%."}, {"title": "CASE STUDIES", "content": "We present a few sample cases of error response from GPT-40.\n\u2022 Data misinterpretation: GPT-40 incorrectly reads a figure as 16% instead of the correct value of 18%, likely due to a small section of the figure having low resolution from the data source, which leads to inaccurate calculations\n\u2022 Incomplete context understanding: GPT-40 overlooks the option \"one of the options are correct\" when generating a response, resulting in an incorrect answer\n\u2022 Numerical inaccuracy: GPT-40 produces incorrect decimal values during a square calculation, leading to an erroneous output\n\u2022 Domain knowledge gaps: GPT-40 misunderstands the nature of the high-touch agency approach in financial markets, confusing its application in exchange-traded derivatives and large trades \n\u2022 Ambiguous answer generation: GPT-40 correctly performs the computation but arrives at an incorrect final result due to ambiguity in answer interpretation\nThese instances are firstly categorized by LM-evaluators, then validated by human expert based on their knowledge and the golden explanations if available."}, {"title": "DETAILS ON RAG AND 01-REASONING EXPERIMENTS", "content": "RAG setup. We utilize 5 CFA Level III curriculum textbooks\u2014\u201cCFA Level III SchweserNotes, Books 1-5, 2023\", which comprehensively cover most of the topics found in FAMMA- as the external knowledge source. The notes are in PDF format, each consisting of 200-300 pages with quizzes at the end of every chapter, though these quizzes are not included in FAMMA. We upload them to GPT-40 via the API for queries.\nDynamic COT setup. The implementation is based on the open source project which is originally built on Llama-3.1. We improve the project to be compatible with GPT-40. The process begins with a system prompt that includes instructions to guide a step-by-step reasoning approach. Once the problem is introduced as a user message, an assistant message is pre-loaded to establish a standardized starting point for the GPT-40's response generation."}, {"title": "FUTURE WORK", "content": "Looking ahead, future work can build on our findings and the established benchmark in several important ways:\n\u2022 Dataset enrichment: we propose further enhancement of the benchmark by incorporating a wider array of languages, and richer multimodal content. This approach will aim to capture the intricacies of real-world financial inquiries more effectively.\n\u2022 we encourage researchers to develop innovative models using refined RAG techniques or advanced prompting methods to compete with those featured on the leaderboard, promoting healthy competition and further advancements."}, {"title": "Multi-choice questions in JSON representation.", "content": null}]}