{"title": "Iterative Object Count Optimization for Text-to-image Diffusion Models", "authors": ["Oz Zafar", "Lior Wolf", "Idan Schwartz"], "abstract": "We address a persistent challenge in text-to-image models: accurately generating a specified number of objects. Current models, which learn from image-text pairs, inherently struggle with counting, as training data cannot depict every possible number of objects for any given object. To solve this, we propose optimizing the generated image based on a counting loss derived from a counting model that aggregates an object's potential. Employing an out-of-the-box counting model is challenging for two reasons: first, the model requires a scaling hyperparameter for the potential aggregation that varies depending on the viewpoint of the objects, and second, classifier guidance techniques require modified models that operate on noisy intermediate diffusion steps. To address these challenges, we propose an iterated online training mode that improves the accuracy of inferred images while altering the text conditioning embedding and dynamically adjusting hyperparameters. Our method offers three key advantages: (i) it can consider non-derivable counting techniques based on detection models; (ii) it is a zero-shot plug-and-play solution facilitating rapid changes to the counting techniques and image generation methods; and (iii) the optimized counting token can be reused to generate accurate images without additional optimization. We evaluate the generation of various objects and show significant improvements in accuracy. The project page is available at https://ozzafar.github.io/count_token.", "sections": [{"title": "1 Introduction", "content": "Image generation models have tremendously advanced in synthesizing high-quality, realistic images (Xia et al. 2021). The introduction of conditioning mechanisms (Tov et al. 2021; Rombach et al. 2022a) has made it a standard tool for content generation (Ramesh et al. 2022), 3D modeling (Liu et al. 2023), improving classifiers (Samuel et al. 2024), and more. However, depicting a specific number of objects in the image with text conditioning often fails to capture the exact quantity of details. For example, the model often depicts a significantly larger quantity when asked to generate an image of a specific number, such as \"ten oranges\".\nThis issue is not unique to text-to-image diffusion models. It has also been studied in visual question answering, where 'how many' questions failed with end-to-end training and required dedicated mechanisms (Zhang, Hare, and Pr\u00fcgel-Bennett 2018). Similarly, fine-tuning on a dedicated dataset was required in text-image matching to obtain satisfactory results (Paiss et al. 2023). This effort mirrors the acquisition of counting skills in humans (Starr, Libertus, and Brannon 2013). Therefore, the development of counting systems constitutes a distinct aspect of learning.\nWe derive inspiration from this learning process and address the ability to count in diffusion models by optimizing a distinct quantity alignment objective, ensuring a specific number is depicted. For this, we rely on a derivable counting mechanism, which approximates the number of objects through potentials (Jiang, Liu, and Chen 2023).\nThere are two limitations to directly employing a counting mechanism: First, gradient-based guidance techniques are often applied during the diffusion denoising process and require a model that can work on noisy images while ours works on images. Second, approximating object count with potentials is inaccurate, considering different viewpoints. Ideally, the optimization would rely on an accurate detection mechanism. However, this approach has a discrete nature and non-smooth transitions, which break the continuity required for differentiation.\nWe address these challenges by proposing a new technique involving iterative image generation. The method applies a counting loss on the generated image, allows the use of pre-trained models, and optimizes the embeddings of an added counting token with gradient descent. Further, we scale the counting loss potentials based on a detection model at each time step, allowing us to reinforce the correct number of objects according to any detection mechanisms. This process is repeated until we achieve satisfactory object count accuracy. Once convergence is reached, the token can be reused to generate new images, including different backgrounds and classes, without additional optimization steps.\nOur method offers several advantages. First, it combines derivable potential signals scaled with non-derivable scores, allowing optimization based on any detection mechanism. Second, it is zero-shot and plug-and-play; hence, our method can rapidly integrate improvements in the text-to-image generator or counting models. Lastly, the optimized token can be reused for additional generations without further optimization.\nWe evaluate our method using various classes with different numbers of required objects. We also demonstrate our ability to reuse the token for different classes and prompts without further optimizations. We compare our model to re-"}, {"title": "2 Related Work", "content": "The field of image generation has been studied extensively, both for GANS (Goodfellow et al. 2014; Li et al. 2019b; Qiao et al. 2019a,b; Tao et al. 2022; Li et al. 2019a; Zhang, Xie, and Yang 2018) and, more recently, with diffusion models (Ho, Jain, and Abbeel 2020; Crowson et al. 2022; Ramesh et al. 2021; Gafni et al. 2022; Jain et al. 2022). The use of diffusion models has, in particular, enabled an unprecedented capability in generating high-quality, diverse images from natural language input with models such as DALLE 2 (Ramesh et al. 2022), Imagen (Saharia et al. 2022), Parti (Yu et al. 2022), Make-A-Scene (Gafni et al. 2022), Stable Diffusion (SD) (Rombach et al. 2022b), and CogView2 (Ding et al. 2022).\nOur method is built on top of text conditioning image generation. Below, we briefly review the related areas of editing image generation models.\nPersonalized image generation. Personalization in image generation is often based on a small set of images representing a specific concept. Textual Inversion (TI) (Gal et al. 2023) learns a new token representing the concept, requiring 3-5 training images to learn the concept's identity. Dream-Booth (Ruiz et al. 2023) also fine-tunes the diffusion model parameters. Collecting data for abstract concepts, such as the number of objects, is challenging. This is because the number of objects is not a qualitative concept but rather a quantitative one.\nClass conditioned image generation. The classifier guidance method integrates the score of a gradient from a pre-trained classifier to condition the generation on a specific class (Dhariwal and Nichol 2021). This approach, however, requires a classifier capable of processing both real and noisy data. In contrast, Stable Diffusion conditions the diffusion process on class labels; however, training relies on curated small annotated datasets, which compromises the generation quality (Ho and Salimans 2021). Subsequent research proposed the application of a pre-trained classifier to the generated images through an iterative generation of images (Schwartz et al. 2023). Our work employs a counting network to manipulate images rather than using a classifier. Furthermore, our iterative mechanism allows for optimizing scores of non-derivable detection mechanisms, which is important for accurate counting.\nControlled image generation. ControlNet (Zhang, Rao, and Agrawala 2023) uses local controls, such as edge maps, depth maps, or segmentation masks, for precise diffusion control. GLIGEN (Li et al. 2023) is trained for open-world grounded text-to-image generation. Uni-Control (Zhao et al. 2023) allows using local and global controls, such as CLIP image embeddings. However, while local controls can add a specific number of objects, for instance, from segmentation maps, they often generate non-realistic images as the conditioning limits the natural placement of objects.\nOptimizing prompt correspondence. Several works have optimized the correspondence between the text prompt and the image. Prompt-to-prompt (Hertz et al. 2023) utilizes the"}, {"title": "3 Method", "content": "We now outline our strategy for steering the text-to-image model by modifying a representation associated with an added counting token. This process involves iterative optimization steps, updating the counting token embeddings until the desired quantity of objects is achieved.\n3.1 Text conditioned diffusion process\nThe diffusion process generates image $x \\sim p(x)$ from random noise via an iterative denoising process. Specifically, given an image $x_0 \\sim p(x)$, the training is done by generating noisy samples $x_t = \\sqrt{a_t}x_0 + \\sqrt{1 - a_t} \\cdot \\epsilon_t$, where $a_t$ are hyperparameters and $\\epsilon_t \\sim N(0, I)$. A neural network is then used to predict the added noise $\\epsilon_t$ using the objective function:\n$\\mathbb{E}_{x,\\epsilon_t,t}[||\\epsilon_{\\theta}(x_t, t) - \\epsilon_t||^2]$,\n(1)\nA similar process is defined for conditional denoising, where each step depends on a conditioning factor $y$:\n$\\mathbb{E}_{x,\\epsilon_t,t}[||\\epsilon_{\\theta}(x_t, t, y) - \\epsilon_t||^2]$.\n(2)\nIn text-to-image methods, the training reconstructs images with conditioned text, which describes the image during denoising. This requires extensive data to generalize mappings between any number and corresponding objects. Another approach might modify the noise prediction to align with a specific number of objects. However, this requires a counting technique that operates on noisy intermediate data. In our work, we propose a method that alters the embeddings of the text conditioning using signals from an off-the-shelf counting model. The process is repeated over generated images until the desired accuracy is achieved. We detail the process in the next section.\n3.2 Inference Time Optimization for Objects Count\nOur goal is to count the number of objects in an image accurately. However, a pre-trained model that counts objects receives an image as input, limiting the ability to intervene during the denoising diffusion process. Thus, we apply our optimization to the generated image.\nLoss. Our optimization relies on a differentiable counting function, $Count$, which estimates the number of objects of a given class in an image. The counting loss for class c is then defined as the distance from the desired number, i.e.,\n$L_{counting}(x, c, N) = || Count(x, c) - N||$,\n(3)\nwhere $Count(x, c)$ estimates the number of objects from class c in the image x.\nSpecifically, we employ CLIP-Count (Jiang, Liu, and Chen 2023), a differentiable method for object counting that can recognize various objects. The module is fine-tuned to match image patches with their corresponding classes by leveraging CLIP (Radford et al. 2021), thereby creating a potential map over the image $\\Phi(x, c) \\in \\mathbb{R}^{h \\times w \\times 1}$, where h and w are the image dimensions. This potential map provides a visual representation of the locations where objects of a particular class are likely to be found within the image. The method aggregates the potential map and normalizes the results with a scaling hyperparameter, i.e.,\n$Count(x, c) = \\sum_{i=1}^{(hw)} scale \\cdot \\Phi_i(x, c)$.\n(4)\nFinding a general scaling factor for any object is challenging, especially for complex objects with deformed potential maps (see Fig. 3). Instead, we control the generation process by optimizing the scaling hyperparameter through a detection-based counting score.\nDetection-based Dynamic Scale: Employing a detection model, such as YOLO (Hussain 2023), for counting could improve accuracy. This involves detecting objects of the desired class and then summing the detected objects. However, this discrete and non-smooth method disrupts the continuity required for differentiation. Nevertheless, we control our optimization process and align it with a detection-based score by dynamically changing the scaling factor at each iteration step, i.e.,\n$Count(x, c) = \\sum_{i=1}^{(hw)} \\Lambda_{scale, x, D} \\cdot \\Phi_i(x, c)$,\n(5)\nwhere $\\Lambda_{scale, x, D} = \\frac{Count_D(x,c)}{\\sum_{i=1}^{(hw)} \\Phi(x,c)}$ and $Count_D(x, c)$ is the number of times class c appears in image x according to detection model D.\nCLIP Matching Penalty. We also find that optimizing for object count only may fix the counting but adversarially corrupt the original semantics of the image. Therefore, a semantic loss, which ensures that the generated class remains as intended, is also added, i.e.,\n$L_{semantic}(x, c) = CLIP(x, \"A photo of c\"),$\n(6)\nwhere CLIP is a matching score function between an image and a text. Thus, we solve the following optimization problem:\n$L(x, c, N) = L_{counting}(x, c, N) + \\lambda L_{semantic}(x, c),$\n(7)\nwhere the hyper-parameter $\\lambda$ calibrates the trade-off between an accurate number of objects and the accuracy of the depicted classes. This loss term may be redundant, as recent diffusion models maintain image semantics strongly even when the encoded prompt is altered. Nevertheless, we add this term to our training for a general-purpose solution, as we find older versions of text-to-image models prone to corrupting the image semantics. We show examples in the supplementary material.\nOptimization. The process begins with an initial prompt, \"A photo of N c.\" To control the generation, we concatenate to the text embedding a newly added pseudo token embedding, denoted as $e_{count}$. The embeddings are randomly initialized and updated during training. Optimizing existing tokens leads to similar performance but corrupts existing tokens that may have multiple meanings. Formally, we optimize the counting token for each image generated by updating the embeddings, i.e.,\n$\\nabla_{e_{count}} \\leftarrow e_{count} + \\alpha \\frac{\\nabla_{e_{count}} L(x, c, N)}{||L(x, c, N)||^2},$\n(8)\nwhere $L(x, c, N)$ is the optimization loss we aim to minimize to achieve N objects from class c depicted in the generated image x and $\\alpha$ is the learning rate. After updating the counting token, a new picture is generated with the optimized token. We stop the process when a threshold of $L(x, c, N)$ is reached.\nPropagating Gradients Over the Diffusion Process. Calculating the gradients over many steps of the diffusion process may lead to a memory explosion. To address this, we employ a diffusion model that requires 1-4 steps (Sauer et al. 2023) and find that one step is sufficient for generating accurate images.\nThe recent introduction of a fast diffusion process has significantly enhanced the applicability of our iterative optimization, which can also be applied to other score-based edits. The benefits include the use of pre-trained models and non-differentiable scores. Additionally, the user can stop the process at any given iteration, providing better control."}, {"title": "4 Results", "content": "Benchmark. We have designed a benchmark to test various classes and quantities. We employ the same classes as the FSC-147 dataset, a common dataset for object counting comprising 147 categories, including common objects, vehicles, and animals (Ranjan et al. 2021). We generated 25 examples for each number and each class, ranging from one to 25, resulting in 3,674 samples."}, {"title": "4.1 Quantitative results", "content": "In Tab. 1, we evaluate the accuracy of object counts compared to the baselines. SD suffers from significant inaccu-"}, {"title": "4.2 Qualitative results", "content": "In Fig. 8, we present images of different classes. From left to right, we first demonstrate that even when the images differ by only one object, e.g., four birds instead of three or six bowls instead of five, our method can perform fine-grained changes and remove one object. Next, we show that SD may depict an excessive number of objects. For instance, an image of five chairs shows more than twenty. Moreover, a large number, such as 25, can lead to a massive object count. Our method accurately reduces the count to approximately 25. Interestingly, the generated image sometimes zooms in to allow an exact count.\nIn Fig. 9, we illustrate the optimization process over time. Our method gradually reduces the number of objects at each time step. Notably, the changes are minimal; i.e., the viewpoint, colors, patterns, and other features remain unchanged, making our method ideal for editing.\nIn Fig. 10, We show examples of generated images with different object potential scalings. Our method is more accurate than SD in both static and dynamic cases. Notably, the dynamic method yields more precise results. For instance, it accurately depicts eight bottles, unlike the static scaling. We believe this is because dynamic scaling better handles side-view images, ensuring accurate object counts.\nIn Fig. 11, we demonstrate the token reuse ability. For instance, our method trained a token for the prompt \"A photo of 10 oranges,\" which was later employed for \"A photo of"}, {"title": "5 Conclusion", "content": "This work shows that state-of-the-art methods often fail to depict exact numbers and frequently generate more objects than requested. Our method generates images iteratively, computing an object's potential at each step, estimating the number of objects, and optimizing text conditioning.\nOur optimized images improve object count accuracy while maintaining other image features. An optimized counting token can also be reused to generate new images with different conditions and classes without further optimization.\nAs training large generative models demands more datasets and computational resources, we hope our iterative approach, based on a one-step diffusion process, will advance plug-and-play methods that leverage existing models. Our method may also address other pitfalls of current text-to-image methods, such as spatial relations between objects and compositional issues."}]}