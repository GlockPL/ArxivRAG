{"title": "A Unified Learn-to-Distort-Data Framework for Privacy-Utility Trade-off in Trustworthy Federated Learning", "authors": ["Xiaojin Zhang", "Mingcong Xu", "Wei Chen"], "abstract": "In this paper, we first give an introduction to the theoretical basis of the privacy-utility equilibrium in federated learning based on Bayesian privacy definitions and total variation distance privacy definitions. We then present the Learn-to-Distort-Data framework, which provides a principled approach to navigate the privacy-utility equilibrium by explicitly modeling the distortion introduced by the privacy-preserving mechanism as a learnable variable and optimizing it jointly with the model parameters. We demonstrate the applicability of our framework to a variety of privacy-preserving mechanisms on the basis of data distortion and highlight its connections to related areas such as adversarial training, input robustness, and unlearnable examples. These connections enable leveraging techniques from these areas to design effective algorithms for privacy-utility equilibrium in federated learning under the Learn-to-Distort-Data framework.", "sections": [{"title": "Introduction", "content": "Federated learning has evolved as a hopeful paradigm for enabling collaborative learning among various parties while maintaining data privacy [14, 11]. In federated learning, numerous clients combine efforts to train a joint model without directly disclosing their private information. Instead, every client uses its personal data to train an independent model, communicating only the model modifications with the central server, which aggregates these updates to ameliorate the global model. This strategy allows clients to benefit from the collective information of the whole federation without compromising the confidentiality of their individual datasets.\nHowever, relevant studies have manifested that federated learning is susceptible to privacy attacks, where an adversary can infer important details about the parties' private data by analyzing the model modifications shared[15, 16]. To mitigate this vulnerability, various privacy-preserving mechanisms have been proposed, including differential privacy [6, 1], secure multiparty computation [2], and homomorphic encryption [9]. These mechanisms aim to preserve the confidentiality of the parties' data by mixing noise or encryption into the model updates prior to communicating them with the server.\nWhile these privacy-preserving mechanisms have shown promise in enhancing the confidentiality of federated learning systems, they often result in decreased model utility or usefulness. The inherent equilibrium between privacy and utility poses significant challenges in designing productive and efficient mechanisms for preserving privacy in federated learning. On one hand, strong privacy guarantees are essential to safeguard the confidence of the clients' details and maintain the trust of the participants. On the other hand, high model utility is crucial to make sure that the practicality and usefulness of the learned models in practical implementations.\nWithin this paper, we first manifest the theoretical basis of the privacy-utility equilibrium in federated learning based on Bayesian privacy definitions and total variation distance privacy definitions. We present quantitative relationships between privacy leakage and utility loss under these privacy definitions, which provide valuable insights into the inherent trade-offs in privacy-preserving federated learning. We then propose the Learn-to-Distort-Data framework, which provides a principled approach to navigate the privacy-utility equilibrium by explicitly modeling the distortion introduced by the privacy-preserving mechanism as a learnable variable and optimizing it jointly with the model parameters. The framework formulates the privacy-preserving federated learning issue as a constrained optimisation issue, with the target of minimizing the utility loss while ensuring that the privacy disclosure remains within an acceptable threshold.\nWe demonstrate the applicability of the Learn-to-Distort-Data framework to a wide range of privacy-preserving mechanisms based on data distortion, including differential privacy, secure multiparty computation, homomorphic encryption, and data compression. By properly designing the distortion variable and the loss function, the framework can capture the essential features of different mechanisms and optimize their performance in a unified manner.\nFurthermore, we highlight the connections between the Learn-to-Distort-Data formulation and related areas such as adversarial training, input robustness, and unlearnable examples. Adversarial training aims to improve the resistance of machine learning models against adversarial attacks by expanding the data with adversarially perturbed samples. Input robustness refers to a model's"}, {"title": "The Trade-off of Privacy-Utility in Machine Learning and Federated Learning", "content": "The trade-off of privacy-utility has been a fundamental challenge in various domains, including machine learning and federated learning. This section begins with a review of relevant research on the trade-off of privacy-utility about the general machine learning, and then focus on the specific setting of federated learning."}, {"title": "The Trade-off of Privacy-Utility in Machine Learning", "content": "In the machine learning literature, the trade-off of privacy-utility has been researched from diverse perspectives. For instance, in locally private contexts, [5] examined how to balance privacy and convergence rate. They showed the convergence rate of learning algorithms can be maintained while ensuring a certain level of privacy protection. [20] proposed an ideal trade-off of privacy-utility via linear programming. However, their closed-form solution is only suitable for a particular binary scenario.\nThe rate-distortion-uncertainty region, which describes the ideal trade-off among compression rate, distortion, and privacy leakage, has been explored in the context of information theory [21, 26, 23]. [23] quantified utility using privacy and accuracy using entropy, and defined a utility-privacy balance region for independent and identically distributed datasets by leveraging rate-distortion theory. However, extending these results to more general scenarios remains an open challenge.\n[25] proposed a united privacy-distortion framework, in which distortion was measured by computing the anticipated distance in Hamming units between the input and output. They assessed privacy leaks by differential privacy (DP), identifiability, and reciprocal information as separate measurements, and established the connection between these different privacy indicators."}, {"title": "Trade-off of Privacy-Utility", "content": "In light of federated learning, the trade-off of privacy-utility has obtained significant attention as a result of the dispersed character of the learning process and the need to defend the privacy of the clients' data. [30, 28, 32, 33, 31, 29, 34] have investigated the trade-off of privacy-utility about federated learning by considering distortion on the model parameters. They formulate this trade-off as a constrained optimisation issue, where the objective is to reduce the loss in utility while satisfying a predefined constraint on privacy leakage.\nIn contrast to these works, our learn-to-distort framework focuses on the trade-off of privacy-utility about federated learning by introducing distortion on the data itself. By explicitly modeling the distortion on the data as a learnable variable, our framework provides a principled approach to navigate the trade-off of privacy-utility in federated learning. This distinction highlights the novelty of our work compared to the existing literature on the trade-off of privacy-utility in the aspect of federated learning.\nOur learn-to-distort framework unifies various techniques for preserving the private, including differential privacy, secure multiparty computation, and homomorphic encryption, under a common"}, {"title": "Related Work", "content": "This section investigates relevant research on privacy-utility trade-off in the aspects of machine learning, federated learning, and then introduce adversarial training, which are closely related to our proposed Learn-to-Distort-Data framework."}, {"title": "Adversarial Training", "content": "A formula called adversarial training aims to strengthen the robustness of models used for machine learning against adversarial attacks. The central idea is to extend the training data with adversar-ially perturbed samples and train the model to accurately categorize these examples. One way to formula optimization problem for adversarial training is as a min-max issue:\n$\\min_{\\theta}\\max_{\\hat{x}} L(f(\\theta; \\hat{x}), y)$,\ns.t., $||\\hat{x} - x|| \\leq \\epsilon$.\nHere, \u03b8 represents the model parameters, \\( \\hat{x} \\) denotes the perturbed input samples, \\( x \\) means the initial input samples, y is the matching label, and L(\u00b7) denotes the loss function. The constraint \\( ||\\hat{x} - x|| \\leq \\epsilon \\) ensures that the perturbations are bounded within a certain range \u20ac to maintain the similarity between the perturbed and original samples.\nThe optimization problem can be further reformulated in terms of the perturbation variable \u03b4:\n$\\min_{\\theta}\\max_{\\delta} L(f(\\theta; x + \\delta), y)$,\ns.t., $||\\delta|| \\leq \\epsilon$.\nIn this formulation, \u03b4 represents the perturbation added to the original input x. The constraint \\( ||\\delta|| \\leq \\epsilon \\) limits the magnitude of the perturbation to ensure that the perturbed samples remain within a certain distance from the original samples.\nAdversarial training has been widely studied in the literature [7, 13] and is useful to boost the resilience of models used for machine learning to a number of adversarial assaults. The Learn-to-Distort-Data framework proposed in this paper shares some similarities with adversarial training, as both aim to find the optimal perturbation or distortion that optimizes a certain objective function while satisfying certain constraints. As such, the techniques and insights from adversarial training can be adapted to generate privacy-preserving distortions and optimize the trade-off of privacy-utility in federated learning systems."}, {"title": "Unlearnable example", "content": "Based on the research of adversarial training, other methods to enhance robustness have been put forward already. Including the study of unlearnable samples [10], by adding disturbance to the training data, in order to prevent the model from studying effective features, so as to defend the privacy of data; It is also possible to introduce data disturbance to make the model produce the wrong classification effect, and to boost the robustness of the neural network model using error minimization attacks [36].\nThe optimization objective of the error minimization attack problem is as follows:\n$\\min_{\\theta} \\max_{\\delta} E_{(x_i,y_i)\\sim D}[L(f(x_i + \\delta; \\theta), y_i)]$,\ns.t. $||\\delta|| \\leq \\epsilon$."}, {"title": "Preliminaries", "content": "This section presents the fundamental premises and notations that are essential for understanding our learn-to-distort framework. We first provide an overview of federated learning and its vulnerability to privacy attacks. We then formally define the semi-honest attacker in federated learning, and introduce the privacy leakage and utility loss metrics used throughout the paper. Finally, we give a detailed notation table in Section A"}, {"title": "Federated Learning", "content": "Federated learning is a collaborative learning paradigm that permits numerous clients to train a joint model without directly providing their private information [14]. In a typical federated learning setup, every client k \u2208 {1, . . ., K} has a personal dataset Dk = {(xi, Yi)}1, where xi \u2208 Rd represents the feature vector and yi \u2208 R denotes the corresponding label. The purpose is to acquire a global model fo : Rd \u2192 R parameterized by 9 that minimizes the following objective function:\n$\\min_{\\theta} \\frac{1}{K}\\sum_{k=1}^{K}\\frac{1}{n_k} \\sum_{i=1}^{n_k}L(f_{\\theta}(x_i), y_i)$,\nwhere the loss function is symbolized by L(\u00b7, \u00b7).\nThe federated learning procedure can be presented as follows:\n1. Every client k trains a model locally \u03b8k on their personal dataset Dk by minimizing the local objective function:\n$\\min_{\\theta_k} \\frac{1}{n_k}\\sum_{i=1}^{n_k}L(f_{\\theta_k}(x_i), y_i)$.\n2. Every client k shares the local model modifications \u0394\u03b8k = 0k - 0 with a central server, where 0 represents the current global model."}, {"title": "Semi-honest Attacker in Federated Learning", "content": "The semi-honest attacker of federated learning is an adversary who follows the regulations correctly but tries to infer sensitive info about the clients' private information by studying the model modifications shared. We imagine a scenario in which the attacker has entry to the global model parameter 0 and aims to conclude the initial dataset \u010f of a specific user.\nThe attacker can formulate an optimization problem to conclude the initial dataset on the basis of the uncovered model parameter. Let d denotes the inferred dataset, and let \\( g(\\hat{d}) = \\arg\\min_{\\theta}2L(\\theta, \\hat{d}) \\) be a function that maps the dataset d to the corresponding model parameter. The attacker's objective is to seek a dataset d that minimizes the deviation between the inferred parameter g(d) and the exposed model parameter 0:\n$\\min_{\\hat{d}} ||g(\\hat{d}) \u2013 \\theta||^2$."}, {"title": "Trade-off between Bayesian Privacy and Model Utility", "content": "This section provide a theoretical examination of the privacy-utility trade-off within federated learning when Bayesian privacy is measured using a-skewed Jensen-Shannon divergence and total variation distance. We first introduce the necessary notations and definitions.\nFA, FB, and F symbolize the attacker's faith distribution about the client's private data Dk upon viewing the safeguarded info, without observing any info, and viewing the unprotected content, respectively. The corresponding probability density functions are denoted as fA, fB, and f*.\nThe model parameters are represented by w, and the personal data from client k is denoted as Dk.\nDefinition 4.1 (Utility Loss). The definition of utility loss is\n$E_u = \\frac{1}{K}\\sum_{k=1}^{K}E_{u,k} = \\frac{1}{K}\\sum_{k=1}^{K}[U(P^*, D_k) \u2013 U(P^d, D_k)]$"}, {"title": "Measuring Privacy Leakage using a-skew Jensen-Shannon Divergence", "content": "We proceed to study a generalized Jensen-Shannon divergence using the skewness parameter a.\nDefinition 4.4 (Generalized Jensen-Shannon Divergence). [17] Let \\(f_{M^{\\alpha}}^{k}(d) = \\alpha f_{A}^{k}(d) + (1 - \\alpha)f_{B}^{k}(d)\\) represent the mixture density of distribution FM, we define\n$JS_{\\alpha}(F_A^k||F_B^k) = \\alpha KL(F_A^k||F_M^k) + (1 - \\alpha)KL(F_B^k||F_M^{\\alpha})$.\nwhere a represents the preference of smoothing between two distributions, and offers the ability to adjust the divergence skew. [4]."}, {"title": "Measuring Privacy Leakage using Total Variation Distance", "content": "In this scenario, the privacy disclosure of any information w is identified as \\( \\epsilon_{p,w} = | f_{D_k|W_k}(d|w) - f_{D_k}(d) | \\). The average privacy disclosure is then calculated by taking average over all possible assignments of d and w.\nDefinition 4.6 (Bayesian Privacy Leakage with Total Variation Distance). The average privacy leakage is defined as\n$\\epsilon_{p,k} = \\int_{d\\in D_k} |f_{D_k}^\\theta(d) - f_{D_k}(d)| d\\mu(d)$,\nwhere \\( f_{D_k}^\\theta(d) = \\int_{W_k} f_{D_k|W_k}(d|w) dP^\\theta(w) \\).\nThis definition quantifies the privacy leakage with the total variation distance across the attacker's faith distribution with and without viewing the safeguarded information, averaged over all possible private data samples."}, {"title": "Trade-off between Data Privacy and Model Utility", "content": "We adopt the definition of privacy leakage from [29], which quantifies the amount of private infor-mation exposed to the attacker. Let d(m) denote the original m-th data sample, and \\( \\hat{d}^{(m)} \\) represent the attacker's inferred m-th data sample at iteration i. The quantity of learning rounds is denoted by I. The privacy leakage ep is identified as:\n$\\epsilon_p = \\begin{cases} \\frac{1}{D} \\sum_{i=1}^{I} \\frac{1}{n_k} \\sum_{m=1}^{n_k} ||d^{(m)} \u2013 \\hat{d}^{(m)}|| & I > 0 \\\\ 0, & I = 0\\end{cases}$"}, {"title": "Utility Loss", "content": "The utility loss measures the degradation in model performance due to the privacy-preserving mechanism. Let fo(\u00b7) denote the model parameterized by 0, and signify the loss value with L(\u00b7, \u00b7). The utility loss eu is defined as:\n$E_u = E_{(x,y)\\sim D}[L(f_{\\theta^\\gamma}(x), y) \u2013 L(f_{\\theta^x}(x), y)]$,\nwhere x represents the distorted version of the input x obtained by applying the privacy-preserving mechanism. The utility loss quantifies the difference in the expected loss between the original and distorted inputs, with eu = 0 indicating no loss in utility, and larger values of eu indicating greater degradation in model performance.\nThe privacy leakage and utility loss metrics provide a quantitative way to measure the trade-off of privacy-utility within federated learning systems. The goal of privacy-preserving federated learning is to design mechanisms that minimize the privacy disclosure while keeping the utility cost within an tolerable range. In the following sections, we will introduce our learn-to-distort framework, which provides a principled approach for navigating the trade-off of privacy-utility by explicitly modeling the distortion as a learnable variable and optimizing it jointly with the model parameters."}, {"title": "Theoretical Connections between The Trade-off of Privacy-utility and Learn-to-Distort-Data Problem", "content": "In this section, we establish the theoretical links between the trade-off of privacy-utility problem and the Learn-to-Distort-Data problem. We first introduce the optimization problem for the trade-off of privacy-utility about federated learning and show how it can be reduced to the Learn-to-Distort-Data problem. We then deduce a fundamental relationship between the privacy leakage and the distortion extent, which allows us to transform the constraint on privacy leakage into a constraint on distortion extent."}, {"title": "The Trade-off of Privacy-utility Problem", "content": "The trade-off of privacy-utility problem about federated learning can be formulated as the following limited optimisation issue:\n$\\min_{\\theta} L(f(\\theta; x + \\delta), y)$\ns.t.\n$\\epsilon_p\\leq \\epsilon$.\nHere, x represents the input data, \u03b4 denotes the distortion applied to the data, 0 represents the model parameters, f(\u00b7) symbolizes the model function, y means the matching label, and L(\u00b7) denotes"}, {"title": "Reduction to Learn-to-Distort-Data Problem", "content": "To navigate the trade-off of privacy-utility, we propose to reduce the constrained optimization problem to a Learn-to-Distort-Data problem. The key idea is to explicitly model the distortion d as a learnable variable and optimize it jointly with the model parameters 0 to reach the ideal equilibrium across privacy and utility. The Learn-to-Distort-Data problem can be formulated as follows:\n$\\min_{\\theta} \\min_{\\delta} L(f(\\theta; x + \\delta), y)$,\ns.t., $||\\delta|| \\geq \\epsilon_1$.\nHere, 61 is a predefined threshold that determines the minimum magnitude of the distortion required to provide the required level of privacy protection. The constraint \\( ||\\delta|| \\geq \\epsilon_1 \\) ensures that the distortion is sufficiently large to achieve the desired level of privacy.\nThe optimization problem can be related to error-minimization attacking for unlearnable example. In the Learn-to-Distort-Data problem, the distortion is learnable and dynamically optimized, and the objective is to seek a harmonious equilibrium across safeguarding privacy and maximising the usefulness of the model while optimizing not only the privacy protection but also the model utility. In the error-minimization attacking for unlearnable example, the distortion is usually small and the objective of the attacker is to induce the model to provide incorrect forecasts.\nTo establish the connection between the trade-off of privacy-utility problem and the Learn-to-Distort-Data problem, we need to deduce a connection between the privacy leakage \\( \\epsilon_p \\) and the distortion extent \\( ||\\delta|| \\)."}, {"title": "The Upper Threshold for Privacy Disclosure", "content": "This section present a key theorem that establishes the correlation across the privacy leakage and the the level of distortion from the perspective of a semi-honest adversary in federated learning. The high-level thought of the proof is to first derive bounds on the cumulative regret of the adversary's optimization algorithm and the distance between data samples. Then, we use these bounds to establish a connection between the privacy leakage and the distortion extent. Finally, we prove the main theorem by leveraging the derived relationship."}, {"title": "The Learn-to-Distort-Data Framework for Privacy-Preserving Federated Learning", "content": "The Learn-to-Distort-Data framework provides a unified perspective on privacy-preserving mechanisms in federated learning that protect privacy by distorting the data. These mechanisms aim to optimize the model performance while ensuring that the privacy leakage remains within an acceptable threshold. The framework formulates the privacy-preserving federated learning problem as a constrained optimization problem:\n$\\min_{\\theta} \\min_{\\delta} L(f(\\theta; x + \\delta), y)$,\ns.t., $||\\delta|| \\geq \\epsilon_1$.\nHere, @ symbolizes the model parameters, a denotes the input data, & represents the distortion introduced by the privacy-preserving mechanism, y means the matching label, and L(\u00b7) denotes the loss. The constraint \\( ||\\delta|| \\geq \\epsilon_1 \\) ensures that the privacy disclosure does not goes beyond a certain limit e defined in advance. The Learn-to-Distort-Data framework captures a wide range of privacy-preserving mechanisms that distort the data to protect privacy in federated learning. By properly designing the distortion variable d and the loss function L(\u00b7), the framework can optimize the model performance while maintaining the desired level of privacy protection. Here are some examples of privacy-preserving mechanisms that can be represented within the Learn-to-Distort-Data framework:\nDifferential Privacy (DP): Differential privacy [6, 1] is a widely used privacy-preserving mechanism that introduces carefully calibrated noise to the data or model modifications to protect the confidentiality of personal data samples. Within the Learn-to-Distort-Data framework, the distortion variable d can be designed to represent the noise introduced by the DP mechanism. The optimization problem for DP-based federated learning can be formulated as:\n$\\min_{\\theta} \\min_{\\sigma^2} E_{s\\sim N(0,\\sigma^2)}L(f(\\theta; x + \\delta), y)$,\ns.t., $ \\sigma^2 \\in A$.\nHere, A represents the set of valid noise variance values that satisfy the privacy constraint. The optimization problem aims to find the ideal model parameters @ and the noise variance \u03c3\u00b2 that minimize the expected loss function over the noise distribution while preserving the data's confi-dentiality."}, {"title": "Connections to Related Areas", "content": "In this section, we show the connections between the Learn-to-Distort-Data formulation and related areas, and highlight how these connections can be leveraged to design effective algorithms for the trade-off of privacy-utility within federated learning."}, {"title": "Connection to Adversarial Training", "content": "Adversarial training ([7, 13]) is the technique that aims at enhancing the resilience of machine learning models against adversarial attacks. The central idea is to expand the training data with adversarially perturbed examples and train the model to accurately categorize these samples. The optimization problem in adversarial training is typically expressed as a min-max issue, where the inner maximization focuses on finding the perturbation that maximizes the loss, while the outer minimization seeks to update the model parameters to cut this adversarial loss.\nThe Learn-to-Distort-Data formulation of privacy-preserving federated learning shares some simi-larities with adversarial training. In both cases, the objective is to seek the ideal perturbation or distortion which optimizes a certain objective function (e.g., minimizing the loss or maximizing the privacy) while satisfying certain constraints. As such, the techniques and insights from adversarial training, such as generating adversarial examples and optimizing model parameters to minimize the adversarial loss, can be adapted to generate privacy-preserving distortions and optimize the trade-off of privacy-utility within federated learning systems."}, {"title": "Connection to Input-Robustness", "content": "Input-robustness ([35, 12, 27]) refers to a model's ability to keep up its performance when noise or input disturbances are present.. Input-robustness is a desirable property in many applications, such as image classification or speech recognition, in which the input could be subject to various types of distortions or corruptions.\nThe Learn-to-Distort-Data formulation of privacy-preserving federated learning is also related to the concept of input-robustness. By explicitly modeling the distortion as a learnable variable and optimizing it to minimize the influence on the model's performance, the Learn-to-Distort-Data formulation can be seen as a way to enforce input-robustness against the backdrop of privacy-preserving federated learning. The techniques and insights from input-robustness, such as data augmentation or stability training, can be adapted to design and analyze privacy-preserving mech-anisms that are robust to input distortions."}, {"title": "Connection to Unlearnable Examples", "content": "Unlearnable examples ([10, 19]) are input examples that are difficult or impossible for a model to learn from. Unlearnable examples can arise due to various factors, such as label noise, data corruption, or adversarial attacks.\nWithin the context of privacy-preserving federated learning, the distortion introduced to safeguard privacy can be seen as a form of data corruption that may render some examples unlearnable. The Learn-to-Distort-Data formulation provides a way to explicitly model and optimize the distortion to minimize the impact on the model's ability to study from the distorted examples. The techniques and insights from the study of unlearnable examples, such as curriculum learning or robust opti-mization, can be adapted to design and analyze privacy-preserving mechanisms that are resilient to the presence of unlearnable examples.\nBy establishing these connections to related areas, we highlight the potential of the Learn-to-Distort-Data framework in leveraging the techniques and insights from these areas to design effective algorithms for privacy-utility trade-off in federated learning. These connections also open up new possibilities for future research and development in the area of privacy-preserving machine learning."}, {"title": "Conclusion", "content": "This paper proposed a unified Learn-to-Distort-Data framework for privacy-preserving federated learning, which formulates the issue as a constrained optimisation issue and reduces it to a Learn-to-Distort-Data problem. We demonstrated that the Learn-to-Distort-Data framework can accom-modate diverse privacy-preserving mechanisms used in federated learning, including differential privacy, secure multiparty computation, and homomorphic encryption. We highlighted that the Learn-to-Distort-Data formulation implies connections to various related areas, such as adversarial training, input-robustness, and unlearnable examples, which can be leveraged to design effective algorithms for the trade-off of privacy-utility in federated learning. We provided a rigorous theoret-ical analysis of the trade-off of privacy-utility under our Learn-to-Distort-Data framework, which offers valuable insights for the design and evaluation of privacy-preserving mechanisms for federated learning environments.\nOur work makes several important contributions to the realm of privacy-preserving federated learn-ing. First, by formulating the problem as a Learn-to-Distort-Data problem, we provide a unified framework that can accommodate a wide range of privacy-preserving mechanisms and enable the design of optimal mechanisms that reach a equilibrium across privacy protection and model perfor-mance. Second, by establishing connections to related areas, we highlight the potential of leveraging techniques and insights from these areas to design effective algorithms for the trade-off of privacy-utility in federated learning. Third, our theoretical analysis provides a rigorous foundation for understanding the trade-off of privacy-utility in federated learning and offers valuable insights for the creation and evaluation of privacy-preserving mechanisms.\nThere are several promising directions for future research. One direction is to explore more sophis-ticated distortion mechanisms that can generate more realistic and informative distortions, such as generative models or autoencoder-based techniques. Another direction is to develop more ef-ficient algorithms for solving the Learn-to-Distort-Data optimization problem, which can scale to large-scale federated learning systems with millions of clients and high-dimensional data. The last direction is to investigate the robustness and stability of the Learn-to-Distort-Data framework un-der different types of attacks and noise models, and develop techniques for mitigating the influence of these considerations on the privacy and utility of the learned model.\nIn conclusion, we believe that our Learn-to-Distort-Data framework provides a principled and flexible approach for navigating the trade-off of privacy-utility in trustworthy federated learning systems. Our theoretical analysis and algorithmic design offer valuable insights and tools for the development of privacy-preserving federated learning systems that can enable secure and effective collaboration among numerous clients while defending the confidentiality of individual participants. We desire that our work will stimulate further research and development in this important area, and contribute to the realization of the whole potential of federated learning in real-world applications."}]}