{"title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model", "authors": ["Zhuan Shi", "Jing Yan", "Xiaoli Tang", "Lingjuan Lyu", "Boi Faltings"], "abstract": "The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection (RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.", "sections": [{"title": "1 Introduction", "content": "Recently, text-to-image diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023; Nichol et al. 2022; Rombach et al. 2022a; Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into highly accurate and visually coherent images. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022; Ho, Jain, and Abbeel 2020a; Kawar et al. 2023), image denoising (Ho, Jain, and Abbeel 2020a; Xie et al. 2023), and super-resolution (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020b). While the progress in text-to-image generative models has profoundly impacted different industries, it also presents significant challenges for copyright protection. These models utilize extensive training data that may include copyrighted works, which they are sometimes capable of memorizing(Carlini et al. 2023). This ability can result in the production of images that closely resemble protected content (See in Figure 1), posing significant challenges to copyright protection(Elkin-Koren et al. 2023). Recent legal cases, such as those involving Stable Diffusion(Rombach et al. 2022b) and Midjourney(Mansour 2023), highlight concerns over the use of copyrighted data in AI training, where the models potentially infringe on the rights of numerous artists. These cases highlight a growing concern: Could the high-quality content synthesized by these generative Als be excessively similar to copyrighted training data, potentially violating the rights of copyright holders? Various methods have been proposed for source data copyright protection. One approach involves using unrecognizable examples(Gandikota et al. 2023; Zhang et al. 2023) that prevent models from learning key features of protected images either during inference or training stages. However, this method is highly dependent on the specific image and model, and it lacks general reliability. Watermarking(Dogoulis et al. 2023; Epstein et al. 2023) inserts specific, unnoticeable patterns into protected images to detect copyright infringement, but further research is needed to improve its robustness. Machine unlearning(Bourtoule et al. 2020; Ginart et al. 2019; Huang et al. 2021; Gao et al. 2023; Nguyen et al. 2022) removes contributions of copyright data, aligning with the right to be forgotten, while dataset deduplication(Somepalli et al. 2022) helps reduce the risk of training sample memorization. Despite these efforts, existing copyright protection methods still have the following limitations: (1) They lack a standardized copyright metric that aligns with copyright laws and regulations, making it difficult to determine if generated"}, {"title": "2 Related Work", "content": "We briefly review related work about text-to-image diffusion models, copyright protection and Reinforcement Learning from Human Feedback. Text-to-Image Diffusion Models: Recently, text-to-image diffusion models have garnered significant attention in research. These advanced methods (Balaji et al. 2023; Nichol et al. 2022; Rombach et al. 2022a; Saharia et al. 2022) have demonstrated exceptional capabilities in converting textual descriptions into visually coherent and realistic images with high accuracy. The advancements in these techniques have unlocked numerous possibilities for various downstream tasks, including image editing (Avrahami, Lischinski, and Fried 2022; Ho, Jain, and Abbeel 2020a; Kawar et al. 2023), image denoising (Ho, Jain, and Abbeel 2020a; Xie et al. 2023), and super-resolution (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020b). Copyright Protection: Several studies in the legal literature have examined copyright issues in machine learning and data mining, focusing primarily on potential infringements during the training phase.: (1)Watermarking(Dogoulis et al. 2023; Epstein et al. 2023), which inserts specific, unnoticeable patterns into protected images to detect copyright infringement, has been explored, but further research is needed to improve its robustness. (2)Concept Removal: To remove explicit artwork from large models, (Gandikota et al. 2023) presents a fine-tuning method for concept removal from diffusion models. Additionally, (Zhang et al. 2023) presents the \"Forget-Me-Not\" method, which enables the targeted removal of specific objects and content from large models within 30 seconds while minimizing the impact on other content. (3) Dataset Deduplication: (Somepalli et al. 2022) explores whether diffusion models create unique artworks or directly replicate certain content from the training dataset during image generation. (4) Machine Unlearning: Numerous model unlearning methods have been developed in the context of image-related tasks (Bourtoule et al. 2020; Ginart et al. 2019; Huang et al. 2021; Gao et al. 2023; Nguyen et al. 2022), among others. While machine unlearning is designed to protect the privacy of target samples, (Chen et al. 2021) demonstrates that in the context of model classification tasks, machine unlearning might leave traces. Reinforcement Learning from Human Feedback: Numerous studies have explored using human feedback to optimize models in various settings, such as simulated robotic control (Christiano et al. 2017), game-playing (Bradley Knox and Stone 2008), machine translation (Nguyen, Daum\u00e9 III, and Boyd-Graber 2017), citation retrieval (Menick et al. 2022), browsing-based question-answering (Song, Meng, and Ermon 2021), summarization (Stiennon et al. 2020; Ziegler et al. 2020), instruction-following (Ouyang et al. 2022), and alignment with specifications (Bai et al. 2022). Recently, (Lee et al. 2023) studied the alignment of text-to-image diffusion models to human preferences using a method based on reward-weighted likelihood maximization. Their method corresponds to one iteration of the reward-weighted regression (RWR) method. Additionally, (Black et al. 2023a) proposed a class of policy gradient algorithms to perform reinforcement learning by posing denoising diffusion as a multi-step decision-making problem. Their findings show that DDPO significantly outperforms multiple iterations of weighted likelihood maximization (RWR-style) optimization. Therefore, we adopt the DDPO method for reinforcement learning in this context."}, {"title": "3 Problem Formulation", "content": "Considering a training image dataset D, composed of both copyrighted (Dc) and non-copyrighted (Dnc) images. The dataset proportions are represented by pc and Pnc, with Pc + Pnc = 1.\nFor each image xi in the dataset, we associate a feature vector fi and a corresponding text prompt ti. The diffusion model is trained to generate an image $\\hat{x}_i$ from the prompt ti,"}, {"title": "4 Main Approach", "content": "we propose a novel approach to minimize copyright infringement in text-to-image diffusion models by leveraging reinforcement learning (RL) and our proposed copyright metrics. We first define a copyright metric to measure how closely a generated image resembles copyrighted content. Then, we integrate this copyright metric into reward function and employ reinforcement learning techniques to fine-tune a pre-trained text-to-image diffusion model. Specifically, the model is trained to maximize the reward by iteratively adjusting its parameters to reduce the likelihood of producing copyright-infringing images. By doing so, we ensure that the model maintains high image quality while adhering to copyright constraints.\nAs shown in Figure 2, the training process of RLCP is as follows:\n\u2022 Gather Datasets: Compile datasets that include both original and copyright-infringing samples.\n\u2022 Prompts Generation: Fed these images into the CLIP interrogator, allowing us to obtain prompts that correspond to each anchor image. The CLIP Interrogator is utilized to convert copyrighted images into corresponding textual information. This text is subsequently refined and transformed into prompts, which are then inputted into a diffusion model to generate the corresponding infringing images.\n\u2022 Model Training: the prompts are used as input for the stable diffusion model, resulting in the generation of images by the stable diffusion model.\n\u2022 Discriminator-Based Scoring: Use a discriminator in the reward model to score generated samples based on the two metrics. Samples less similar to the copyrighted data receive higher rewards."}, {"title": "4.2 Copyright Metric", "content": "We begin by reviewing key aspects of copyright law and introducing the extrinsic and intrinsic legal tests. We then demonstrate how these tests can be analytically modeled using indicators of semantic and perceptual similarity. Finally, we provide a detailed explanation of the methods used to measure semantic and perceptual similarity.\nCopyright Law. In the U.S., proving copyright infringement with AI-generated outputs requires two criteria: the AI must have accessed the copyrighted works, and the outputs must be substantially similar to those works. Courts typically use a two-part test to evaluate substantial similarity: an extrinsic test, which objectively compares specific expressive elements, and an intrinsic test, which subjectively compares the overall impression based on the perception of an ordinary audience. The plaintiff must prove substantial similarity under both tests. Therefore, we aim to select metrics that allow to closely mimic such legal tests.\nSemantic metric. We leverage the CLIP model(Radford et al. 2021) to generate semantic embeddings for anchor and generated images and calculate the metrics by:\n$\\begin{cases} emb_{ori} = CLIP(Image_{ori}) \\\\ emb_{gen} = CLIP(Image_{gen}) \\end{cases}$ (1)\n$Loss_{sem} = MSE(emb_{ori}, emb_{gen})$, (2)\nwhere $Image_{ori}$ and $Image_{gen}$ denote the anchor image and generated image, respectively; CLIP denotes the CLIP's image encoder. Here we utilize the Mean Squared Error (MSE) between the embeddings as the evaluation metric."}, {"title": "Perceptual metric", "content": "Here we used the Learned Perceptual Image Patch Similarity (LPIPS) metric proposed by (Zhang et al. 2018). First, LPIPS normalises the feature dimension of all pixels and layers to unit length, scales each feature by a specific weight, and we then calculated squared 12 distance between these weighted activations. The squared distances are then averaged across the image dimensions (spatial averaging) and summed over the layers, resulting in the final perceptual distancemetric d as follows:\n$d(x, y) = \\sum_{l} \\frac{1}{H_lW_l} \\sum_{i,j} (w_l \\cdot (x_{l,ij} - y_{l,ij}))^2$ (3)\nwhere $x_{l,ij}$ and $y_{l,ij}$ denotes the normalized feature vectors at layer l at pixel (i, j). $H_l$ and $W_l$ denote the height and width of the feature map at layer l, respectively. The parameter $w_l$ represents the weight assigned to each feature at layer l.\nWe normalize the result and subtract it to 1 to obtain a metric for perceptual similarity $loss_{perc}$ as follows:\n$loss_{perc} = \\frac{1 - d(x, y)}{1 + d(x, y)}$ (4)\nFinally, we denote the total copyright loss as:\n$Loss_{CL} = \\alpha \\cdot loss_{sem} + \\beta \\cdot loss_{perc}$ (5)\nThe parameters $\\alpha$ and $\\beta$ control the trade-off between these two components."}, {"title": "4.3 The proposed method RLCP", "content": "RLCP aims to achieve the dual objectives of maintaining high-quality image generation and ensuring compliance with copyright laws, thereby addressing the pressing challenge of copyright infringement in text-to-image diffusion models. It consists of several key components:\n\u2022 Reward-Based Learning Framework: Our approach involves using a discriminator in the reward model to score generated samples. Images that are less similar to copyrighted data receive higher rewards. This reward system helps guide the model towards generating compliant images.\n\u2022 Fine-Tuning with Reinforcement Learning: Starting from a pre-trained text-to-image diffusion model, we fine-tune it using RL techniques to optimize the generation process according to the defined reward functions. This involves framing the denoising process as a multi-step decision-making problem and applying policy gradient algorithms to maximize the reward signal.\n\u2022 KL Regularization: To prevent the model from overfitting to the reward function, we introduce KL divergence as a regularization term. This helps maintain the generative capabilities of the original diffusion model while steering it towards producing non-infringing content.\nSpecific training process we describe how we fine-tune a pre-trained text-to-image diffusion model using reinforcement learning (RL) to minimize the risk of generating copyright-infringing content. Our approach is guided by a reward function that leverages the copyright loss (CL) metric defined in Eq. (5), and incorporates KL regularization to ensure the model maintains high-quality image generation.\nModel Initialization: We begin with a pre-trained diffusion model $M_{\\theta}$, where $\\theta$ denotes the model parameters. The model is initialized based on a large-scale text-to-image dataset that includes both copyrighted (Dc) and non-copyrighted (Dnc) data.\nReward Function Design: The reward function $r(x_0, c)$ is the key component guiding the model's training. It is defined as a weighted sum of the semantic similarity loss $L_{sem}$ and the perceptual similarity loss $L_{perc}$:\n$r(x_0, c) = \\alpha \\cdot CL_{sem}(x_0, c) + \\beta \\cdot CL_{perc} (x_0, c)$,\nwhere:\n\u2022 $CL_{sem} (x_0, c)$ represents the semantic similarity loss, calculated as the Mean Squared Error (MSE) between the generated image embeddings and the embeddings of the original copyright images.\n\u2022 $CL_{perc} (x_0, c)$ represents the perceptual similarity loss, measured using the LPIPS metric.\nThe parameters $\\alpha$ and $\\beta$ are the same as Equation.(5), allowing fine-tuning of the sensitivity to copyright similarity.\nTraining with DDPO: We utilize Denoising Diffusion Policy Optimization (DDPO) (Black et al. 2023b) to optimize the model. This approach treats the denoising process as a sequence of actions in a Markov Decision Process (MDP). The goal is to maximize the expected reward:\n$J(\\theta) = E_{c~p(c),x_0~M_{\\theta}(x_0|c)} [r(x_0, C)]$\nThe model iteratively updates the parameters $\\theta$ using gradient ascent to increase the reward, which in turn reduces the likelihood of generating infringing content.\nKL Regularization: To prevent the model from overfitting to the reward function, we introduce a KL divergence term that regularizes the training. This term penalizes significant deviations from the original model distribution:\n$L_{KL} = \\sum_{t=1}^{T} E_{p_{\\theta}(x_t|c)} [KL(p_{\\theta}(x_{t-1}|x_t, c) || p_{pre}(x_{t-1}|x_t, c))]$\nThe final objective function that the model optimizes combines the reward function with KL regularization:\n$L(\\theta) = -J(\\theta) + \\lambda \\cdot L_{KL}$,\nwhere $\\lambda$ is a regularization parameter that balances the trade-off between achieving high rewards (minimizing copyright loss) and maintaining the generative capabilities of the original diffusion model."}, {"title": "5 Experiment", "content": "In this section, we first evaluate the effectiveness of our proposed copyright loss metric. Then we evaluate RLCP on 3 real-world datasets. Furthermore, we explore the impact of the proportion of copyright images in the training set on the efficiency of RLCP."}, {"title": "5.1 Experiment Setup", "content": "Dataset. Our datasets contain copyright and non-copyright data. To enhance realism, our search is confined to famous artwork and creation figures, which we designate as our copyright dataset:\n1. Paintings: Painting artworks(WikiArt 2024) often embody the distinctive style of the artist, encompassing aspects such as brushstrokes, lines, colors, and compositions. We gathered over 1000 paintings from Vincent Van Gogh.\n2. Cartoon Figures: Cartoon figure images, including characters from animations and cartoons, are often protected by law. Similar to portraits, we have curated a dataset of around 1000 influential animated characters and figures by collecting information from reputable sources such as Kaggle\u00b2 and Wikipedia\u00b3.\n3. Portrait: The right of a portrait encompasses an individual's authority over their own image, including their facial features, likeness, and posture. In our research, we gathered over 500 portrait images from Wikipedia, an open-access, multilingual online encyclopedia that provides information on a multitude of subjects.\nFor the non-copyright dataset, we sourced images from ImageNet (Deng et al. 2009), selecting one image from each class, resulting in a total of 1,000 images.\nEvaluation Metric. We use our proposed metric Copyright Loss (CL) as well as CLIP and $l_2$ norm to measure the degree of copyright violation. We also use FID to measure the generative quality of text-to-image diffusion model.\n1. CLIP: We evaluate changes of CLIP scores, for text-image similarity.\n2. Copyright Loss (CL): We quantify the similarity between the original copyright images and their unlearned counterparts on the feature level, utilizing our proposed CL(copyright loss) metric.\n3. $l_2$ norm: We calculate the squared $l_2$ distance between generated and training images.\n4. Fr\u00e9chet Inception Distance (FID): We use FID to evaluate the generative image quality of text-to-image diffusion model. The formulation of the metric is as follows:\n$FID = ||\\mu_x \u2013 \\mu_y ||^2 + Tr(\\Sigma_x + \\Sigma_y \u2013 2(\\Sigma_x\\Sigma_y)^{1/2})$ (6)\nwhere:\n\u2022 $\\mu_x$ and $\\Sigma_x$ represent the mean and covariance matrix of the feature vectors from real images.\n\u2022 $\\mu_y$ and $\\Sigma_y$ represent the mean and covariance matrix of the feature vectors from generated images.\n\u2022 Tr() denotes the trace of a matrix.\nThis evaluation provides valuable insights into assessing copyright infringement while preserving the ability to generate non-infringing content.\nBaselines. Four baselines are listed as follows:\n\u2022 Stable diffusion (SDXL): Stable Diffusion(von Platen et al. 2022) is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n\u2022 Forget-Me-Not: Forget-Me-Not(Zhang et al. 2023) is an efficient and low-cost solution designed to safely remove specified IDs, objects, or styles from a well-configured text-to-image model in as little as 30 seconds, without impairing its ability to generate other content.\n\u2022 Concept Removal (CA): CA(Gandikota et al. 2023) An efficient method of ablating concepts in the pretrained model.\n\u2022 Unified Concept Editing(UCE): UCE(Kumari et al. 2023) edits the model without training using a closed-form solution, and scales seamlessly to concurrent edits on text-conditional diffusion models."}, {"title": "5.2 Experiment Results", "content": "In this section, we evaluate the effectiveness of our proposed RLCP method across several dimensions. We compare our results against baseline methods, measure the impact of different proportions of copyrighted data, and analyze the trade-offs between copyright protection and image quality.\nEffectiveness of copyright metric. Our first set of experiments aims to assess the reliability of our proposed copyright loss (CL) metric in distinguishing between copyrighted and non-copyrighted content.\nThe experimental results shown in Figure 3 were obtained using a subset of the paintings dataset(See in Figure 4), which includes works such as \"The Starry Night\" by Van Gogh. CLIP similarity was calculated by leveraging the CLIP model to generate semantic embeddings, followed by computing the Mean Squared Error (MSE). The L2-norm was determined by calculating the squared L2 distance between these weighted activations, while LPIPS normalized the feature dimensions of all pixels and layers to unit length, scaling each feature by a specific weight.\nIn Figure 3, the \u201cquery index\" refers to the position of the query image in the dataset, while the \u201cvalue index\" represents the corresponding value or similarity score for each generated image. The heatmaps illustrate good performance by showing high similarity scores for relevant image pairs, indicating that the model successfully retrieves or generates images that closely match the queries in terms of semantic or visual content.\nThe resulting heatmaps demonstrate that the combined copyright loss metric, which integrates both semantic and perceptual metrics, offers a more balanced and accurate assessment of copyright infringement risks. The copyright loss"}, {"title": "Experimental environment and hyperparameters", "content": "All the experiments were conducted on a cluster with 2 80Gb A100 GPUs.\nParameter configuration table. Parameter settings."}, {"title": "Performance of Copyright Protection", "content": "We have two primary foundational models in our repertoire. The first one is our finetuned model based on reinforcement learning, SD-finetuned, which is employed to evaluate and assess the performance of the reinforcement learning algorithm and finetuned the stable diffusion model with the use of similarity metrics. This evaluation includes both the model's ability to forget copyrighted images and generated data quality. The second foundational model is on the basic SD-XL 1.0. During the unlearning experiments conducted on these two foundational models, for each image to be forgotten, the learning rate for gradient ascent is set at 3e-4, detailed hyperparameters are listed in Table 1.\nThe result are listed on Table 2, Table 3, Table 4. While RLCP shows relatively weaker performance on the FID, it still performs exceptionally well on the L2-norm metric. This discrepancy can be explained by the nature of FID, which measures the distribution similarity between generated and target images. Since RLCP focuses on avoiding the generation of copyrighted data, it naturally alters the distribution, leading to a higher FED score. Despite this, the L2-norm results indicate that RLCP maintains strong performance in generating non-infringing content that aligns with the original image features.\nWe also give examples of fine-tuned results for copyright protection on three datasets (See in Figure 5, Figure 6, Figure 7)."}, {"title": "Impact of Copyright Data Proportion", "content": "We further explore how the proportion of copyrighted images in the training dataset affects the performance of the RLCP method. As illustrated in Figure 8, increasing the proportion of copyrighted data leads to an increase in copyright loss, while FID decreases slightly, reflecting a trade-off between protection and quality. One potential concern is whether RLCP can scale effectively when a large proportion of the training set consists of copyrighted images. The results address this concern by showing that RLCP continues to perform strongly, even with a high percentage of copyrighted images, outperforming all alternative methods.\nWe measured copyright loss and FID across various datasets with differing ratios of copyrighted to non-copyrighted images. As shown in Figure 8, the copyright loss also rises with more copyrighted data, yet the FID shows only a slight decrease.\nWhile RLCP demonstrates a clear ability to reduce copyright infringement, there is an inherent trade-off between minimizing copyright loss and maintaining image quality. The results suggest that RLCP provides a favorable balance, where the reduction in copyright loss is achieved with only a minimal impact on FID. This balance is particularly evident in the results for portrait images, where RLCP achieves both low copyright loss and high FID."}, {"title": "6 Conclusion and Future Work", "content": "In this paper, we presented a Reinforcement Learning-based Copyright Protection (RLCP) for copyright infringement in text-to-image diffusion model. RLCP proposes a copyright loss metric that mirrors legal tests used to assess substantial similarity, and then integrates this metric into a reinforcement learning framework for model fine-tuning, and the use of KL divergence to regularize and stabilize the model training process. Experiments conducted on three mixed datasets of copyright and non-copyright images show that RLCP significantly reduces the likelihood of generating infringing content while preserving the visual quality of the generated images. Our results demonstrate that balancing the proportion of copyrighted and non-copyrighted data in the training set is crucial for minimizing copyright infringement without compromising image quality. We also showed that the reward-driven RL framework effectively fine-tunes diffusion models by optimizing for both copyright compliance and data fidelity.\nWhile our approach demonstrates promising results, there are several areas for future work: (1) Broader Application Domains: Future work could extend RLCP to other domains beyond image generation, such as text or audio generation models, where copyright concerns are equally prevalent. (2) Dynamic Dataset Management: Investigating adaptive or dynamic dataset augmentation strategies could be beneficial. As models encounter more copyright-protected data, dynamically adjusting the training process may lead to more robust copyright protection without overfitting to specific datasets."}]}