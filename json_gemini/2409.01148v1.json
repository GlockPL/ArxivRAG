{"title": "FMRFT: Fusion Mamba and DETR for Query Time Sequence Intersection Fish Tracking", "authors": ["Mingyuan Yao", "Yukang Huo", "Qingbin Tian", "Jiayin Zhao", "Xiao Liu", "Ruifeng Wang", "Haihua Wang"], "abstract": "Growth, abnormal behavior, and diseases of fish can be early detected by monitoring fish tracking through the method of image processing, which is of great significance for factory aquaculture. However, underwater reflections and some reasons with fish, such as the high similarity, rapid swimming caused by stimuli and multi-object occlusion bring challenges to multi-target tracking of fish. To address these challenges, this paper establishes a complex multi-scene sturgeon tracking dataset and proposes a real-time end-to-end fish tracking model, FMRFT. In this model, the Mamba In Mamba (MIM) architecture with low memory consumption is introduced into the tracking algorithm to realize multi-frame video timing memory and fast feature extraction, which improves the efficiency of correlation analysis for contiguous frames in multi-fish video. Additionally, the superior feature interaction and a priori frame processing capabilities of RT-DETR are leveraged to provide an effective tracking algorithm. By incorporating the QTSI query interaction processing module, the model effectively handles occluded objects and redundant tracking frames, resulting in more accurate and stable fish tracking. Trained and tested on the dataset, the model achieves an IDF\u2081 score of 90.3% and a MOTA accuracy of 94.3%. Experimental results demonstrate that the proposed FMRFT model effectively addresses the challenges of high similarity and mutual occlusion in fish populations, enabling accurate tracking in factory farming environments.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of aquaculture, real-time detection and assessment of fish conditions play a crucial role in improving fish farming efficiency and enhancing management practices (Van der Schalie, Shedd, Knechtges and Widder, 2001). Monitoring fish condition allows for the timely detection of feeding issues during the aquaculture process, enabling precise feeding, that reduces feed waste, minimizes water pollution, and boosts yields (Niu, Chen, Hageman, McMullin, Wing and Ng, 2023). Fish exhibit various behavioral responses to external stimuli such as light, water quality, and breeding density. By tracking fish status, farmers can gain valuable insights into fish health, environmental adaptation, and more (Papadakis, Papadakis, Lamprianidou, Glaropoulos and Kentouri, 2012; Nema, Hasan, Bhargava and Bhargava, 2016). Compared to traditional methods based on sensors or manual observation, fish tracking using computer vision offers advantages such as real-time monitoring, non-contact observation, and non-interference, simulating human vision for target recognition and tracking.\nThis approach is an effective means of achieving intelligent management in large-scale aquaculture operations (Zhang Chongyang and Chen Ming, 2020).\nMulti-target tracking is a computer vision task focused on localizing and tracking multiple targets within a video sequence. A variety of algorithms have emerged in this field, with the two most dominant strategies being Tracking by Detection and Tracking by Query. The core idea of Tracking by Detection is to first use a target detection algorithm to identify targets within each frame of the video. Subsequently, matching algorithms such as DeepSort (Wojke, Bewley and Paulus, 2017a), ByteTrack (Zhang, Sun, Jiang, Yu, Weng, Yuan, Luo, Liu and Wang, 2022), or other related algorithms are employed to associate and match the detected targets across consecutive frames, thereby enabling the tracking of the target trajectory. The advantage of this method lies in its reliance on a powerful target detector that provides accurate target position information. However, it still encounters significant challenges in complex scenes involving occlusion, lighting changes, and fast target motion. In contrast, Tracking by Query is an emerging approach that represents each target as a query, typically a feature vector, to search and match targets throughout a video sequence. Algorithms such as TransMOT (Chu, Wang, You, Ling and Liu, 2023), TransCenter (Xu, Ban, Delorme, Gan, Rus and Alameda-Pineda, 2022), and MOTR (Zeng, Dong, Zhang, Wang, Zhang and Wei, 2022) have been developed under this paradigm. Query-based tracking methods are particularly adept at handling changes in target appearance and occlusion, exhibiting greater robustness in complex scenes due to their utilization of the continuity of the target's features.\nHowever, fish tracking is often more complex than the scenarios typically encountered in object tracking. Firstly, as shown in Figure 1(a), the morphological changes of individual fish are not pronounced at different growth stages or during the same period due to behaviors such as respiration and swimming, which increases the complexity of target identification. Secondly, as shown in Figure 1(b), variations in lighting conditions at different angles of the fish tank, along with the refraction, scattering, and absorption of light, result in low image contrast, low clarity, and image fading, all of which hinder accurate detection and tracking of the target. As shown in Figure 1(c, d), fish frequently cover each other while swimming, particularly in high-density aquaculture environments, where such occlusions pose significant challenges to continuous target tracking. Additionally, bubbles generated by oxygenators, feed residues in the water, and sensor equipment may have similar texture or brightness characteristics to the fish targets in the image, further complicating fish tracking.\nIn response to these challenges, this paper proposes a real-time fish tracking model, FMRFT, designed for tracking fish in complex factory farming scenarios. The main contributions of this paper are as follows:\n(1) Innovative Framework Fusion: We have innovatively fused Mamba In Mamba (MIM) and the RT-DETR within the existing MOTR framework to achieve accurate tracking of fish targets. This fusion strategy enhances the model's ability to effectively address occlusion and similarity challenges in complex environments.\n(2) Novel Query Time Sequence Intersection (QTSI): We propose a novel query interaction module (QTSI), which facilitates the interaction and fusion of information by calculating the intersection over union (IoU) between Tracking Query, Detect Query, and real frames during the training phase. This design significantly reduces reliance on a single Tracking Query and effectively prevents the generation of multiple redundant detection frames for a single target.\n(3) Enhanced Data Fusion Method (Fusion MIM): To further improve the depth and breadth of feature extraction, we designed a new data fusion method, Fusion MIM, which deeply fuses MIM feature information at different scales and strengthens the model's ability to extract features at multiple levels through feature interaction.\n(4) Innovative Temporal Tracking Query Interaction Module (MQIM): We introduced a Mamba Query Interaction Module (MQIM) that enables tracking queries to be learned through deeper interaction with Decoder layer outputs. This interaction mechanism enhances the model's adaptability to target changes in dynamic scenes and improves tracking stability.\n(5) New Multi-Fish Tracking Dataset: A new multi-objective fish tracking dataset has been established, covering sturgeon fish tracking data from various culture scenarios and containing a total of 8,000 high-quality sturgeon fish tracking images. This dataset provides a valuable visual resource for fish behavior analysis and health assessment.\nThe main contents of the remaining chapters are as follows: Section 2 reviews previous work in fish tracking, and provides a brief introduction to the Mamba and DETR modules. Section 3 details the proposed FMRFT method. Section 4 presents comparative and ablation experiments, along with visualizations of the experimental results. Finally, Section 5 concludes the paper and suggests directions for future research."}, {"title": "2. Related Work", "content": "Multi-Object-Tracking (MOT) is a key technology in the field of computer vision, widely applied in areas such as autonomous driving, intelligent surveillance, and behavior recognition. However, multi-target tracking presents challenges like occlusion, deformation, motion blur, crowded scenes, fast motion, illumination changes, and scale variations-issues that also arise in single-target tracking. Additionally, multi-target tracking involves complex challenges specific to this domain, such as trajectory initialization and termination, and mutual interference between similar targets. As a result, multi-target tracking remains a challenging area in image processing and continues to attract significant long-term research investment. Currently, there are two commonly used target-tracking strategies, which are introduced below:"}, {"title": "2.1. Tracking by Detection", "content": "Tracking by Detection is a widely used paradigm in multi-target tracking (MOT) (Benjdira, Koubaa, Azar, Khan, Ammar and Boulila, 2022), as illustrated in Figure 2. This approach is typically comprises two steps: target detection and target association (Jiao, Zhang, Liu, Yang, Hou, Li and Tang, 2021). Detection is performed using various deep-learning models. However, the primary challenge lies in target association, i.e., tracking the trajectory of the object of interest (Wojke, Bewley and Paulus, 2017b). Common methods for target association include linear regression (Seber and Lee, 1977), mean drift (Comaniciu, Ramesh and Meer, 2000), Hidden Markov Models (Chen, Fu and Huang, 2003), Kalman filters (Rodriguez, Sivic, Laptev and Audibert, 2011; Reid, 1979), Extended Kalman filters (Mitzel and Leibe, 2011), and Particle filters (Jin and Mokhtarian, 2007; Yang, Duraiswami and Davis, 2005a; Hess and Fern, 2009; Han, Joo and Davis, 2007; Hu, Li, Luo, Zhang, Maybank and Zhang, 2012; Liu, Li and Chen, 2012; Breitenstein, Reichlin, Leibe, Koller-Meier and Van Gool, 2009; Yang, Lv, Xu and Gong, 2009).\nAs the originator of detection-based target tracking, Bewley et al. (Wojke et al., 2017b) developed the first efficient online multi-target tracking method, SORT, which achieves fast and accurate tracking of multiple targets in a video by using a Kalman filter for motion prediction and a Hungarian algorithm for data association. Wojke et al. (Wojke et al., 2017a) proposed the DeepSORT algorithm, building upon SORT. By introducing appearance features obtained through deep learning feature extraction, DeepSORT addresses the identity-switching problem that SORT encounters when dealing with occlusion and long-term tracking, thereby enhancing the robustness of multi-target tracking. StrongSORT (Du, Zhao, Song, Zhao, Su, Gong and Meng, 2023) further improves multi-target tracking by enhancing object detection, feature embedding, and trajectory association, as well as introducing the AFLink and GSI algorithms. By associating nearly all detection frames instead of just high-scoring ones, StrongSORT improves tracking performance and accuracy. ByteTrack (Zhang et al., 2022) addresses the issues of occlusion and low-scoring detection frames, which may cause SORT to miss real objects and fragment trajectories, by associating almost all detection frames instead of just high-scoring ones. The OC-SORT (Cao, Pang, Weng, Khirodkar and Kitani, 2023) algorithm enhances the traditional SORT method by introducing the Observation Centre Re-update (ORU) and Observation Centre Momentum (OCM), which solve problems related to cumulative error and inaccurate direction estimation in cases of occlusion and nonlinear motion.\nIn detection-based fish tracking approaches, mainstream target detection algorithms are typically employed to detect fish. For instance, Martija et al. (Martija and Naval, 2021) implemented an end-to-end tracking and detection algorithm by redesigning the Deep Hungarian Network (DHN) to compute discriminative affinity scores for predictive detection between consecutive frames. This was combined with the Faster R-CNN model to detect fish in field-captured video sequences for multi-fish tracking. Sun et al. (Sun, Zhang, Shi, Tang, Chen, Xiong, Dai and Li, 2024) proposed a tracking technique based on the YOLOv7-DCN and SORT algorithms, which tracks the primary targets in fishing vessel operations by employing enhanced target detection and counting algorithms that integrate Kalman filters and Hungarian algorithms. Wang et al. (Wang, Xia and Lee, 2021) developed a parallel shape index feature-based fish tracking algorithm, which detects the head and center of the fish body and integrates the SORT framework and Kalman filter to accurately track the movement trajectories of large numbers of zebrafish. Gong et al. (Gong, Hu and Zhou, 2022) achieved efficient and accurate underwater fish tracking by incorporating the CBAM attention mechanism into the YOLOv4-tiny model to enhance feature learning, in combination with a SORT tracker."}, {"title": "2.2. Tracking by Query", "content": "With the widespread application of attention mechanisms, particularly Transformers, in the field of computer vision, query-guided target tracking methods have demonstrated significant advantages in tracking robustness, thereby offering new research perspectives and breakthroughs in multi-objective tracking (MOT) (Kugarajeevan, Kokul, Ramanan and Fernando, 2023). Currently, TransTrack (Sun, Cao, Jiang, Zhang, Xie, Yuan, Wang and Luo, 2020) and TrackFormer (Meinhardt, Kirillov, Leal-Taixe and Feichtenhofer, 2022) are two representative tracking frameworks that utilize the Transformer architecture to address MOT tasks. TransTrack integrates target detection and association into a unified framework by utilizing the Transformer's attention mechanism, as depicted in Figure 3. In contract, the TrackFormer algorithm introduces a Transformer-based encoder-decoder architecture and employs an autoregressive trajectory query mechanism, effectively addressing key challenges in multi-target tracking, including data association, identity preservation, and spatiotemporal trajectory prediction. Additionally, it also enables end-to-end trainable multi-target tracking and segmentation, as shown in Figure 4.\nAdditionally, Xu et al. (Xu et al., 2022) proposed a Transformer-based multi-object tracking architecture that, for the first time, addresses the challenges of under-detection and computational inefficiency in crowded scenarios by introducing dense image correlation detection queries and efficient sparse tracking queries. This innovation significantly enhances the accuracy and operational efficiency of multi-object tracking. The MOTR (Zeng et al., 2022) algorithm further enhances accuracy and efficiency in video sequence tracking by introducing a \"track query\" and iterative prediction mechanism. This approach enhances the post-processing stage, which traditionally relies on heuristic correlations based on motion and appearance similarity, and addresses the challenge of exploiting temporal variations in video sequences in an end-to-end manner. Furthermore, the MOTRv2 (Zhang, Wang and Zhang, 2023) algorithm enhances the detection performance of MOTR by integrating a pre-trained YOLOX object detector to generate proposals as anchors, significantly boosting both the accuracy and efficiency of multi-target tracking.\nGiven its flexible query-key mechanism, numerous scholars have recently adapted the Transformer architecture to the field of multi-fish tracking. Gupta et al. (Gupta, Mukherjee, Chaudhury, Lall and Sanisetty, 2021) proposed a deep fish tracking network named DFTNet, which combines a twin network for encoding appearance similarity and an Attention Long Short-Term Memory network (Attention LSTM) for capturing motion similarity between consecutive frames, thereby enabling efficient fish tracking. Li et al. (Li, Liu, Wang, Li and Yue, 2024) introduced a Transformer-based multi-fish tracking model (TFMFT), incorporating a Multiple Association (MA) approach to enhance tracking fault tolerance by integrating simple cross-linking matches in the Identification (ID) matching module. Liu et al. (Liu, Li, Zhou, Li and Duan, 2024a) developed FishTrack, a multi-fish online tracking model with three branches: target detection, trajectory prediction, and re-identification. This model simultaneously establishes a fish motion model and an appearance model to achieve multi-fish online tracking. Mei et al. (Mei, Yan, Qin, Yang and Chen, 2024) proposed a novel single-target fish tracking method, SiamFCA, which is based on a twin network and a coordinate attention mechanism. This method further enhances images using contrast-constrained adaptive histogram equalization (CLAHE) to improve the accuracy and robustness of the model in complex scenes.\nHowever, the aforementioned methods primarily address challengessuch as occlusion and complex environments, while often overlooking the strong correlation between different parts of fish bodies and the temporal continuity between consecutive frames. This oversight leads to phenomena such as parts of fish bodies with similar shapes being mistakenly recognized as the same fish, as well as redundancy in the detection frames."}, {"title": "2.3. Vision Mamba and RT-DETR", "content": "In the field of visual representation learning, the introduction of the Vision Mamba (Zhu, Liao, Zhang, Wang, Liu and Wang, 2024) model marks a significant advancement over traditional Transformer architectures. By leveraging the Bidirectional State Space Model (BSSM), Vision Mamba aims to overcome the scalability limitations of traditional self-attention mechanisms when handling long sequential data. Compared to the Vision Transformer (Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold and Gelly, 2020) model, Vision Mamba efficiently processes image sequences through BSSM, capturing both global and local visual information while achieving linear time complexity, thereby significantly enhancing computational efficiency.\nThe significant advantage of the Vision Mamba model lies in its ability to process long sequential data, enabling it to excel in vision tasks involving high-resolution images and video data. Building on this foundation, numerous researchers have further advanced the model by introducing methods such as Fusion Mamba (Dong, Zhu, Lin, Luo, Shen, Liu, Zhang, Guo and Zhang, 2024), Mamba in Mamba (Chen, Tan, Gong, Chu, Wu, Liu, Ye and Yu, 2024), and VMamba (Liu, Tian, Zhao, Yu, Xie, Wang, Ye and Liu, 2024b), thereby opening new research directions in the field of visual representation.\nDETR (Carion, Massa, Synnaeve, Usunier, Kirillov and Zagoruyko, 2020) transforms the target detection task into a multi-class classification problem by integrating the Transformer architecture, excelling in accurate object localization and relational modeling, making it particularly suitable for applications such as multi-object tracking. However, DETR is also associated with high computational complexity and substantial resource demands. In contrast, the RT-DETR (Zhao, Lv, Xu, Wei, Wang, Dang, Liu and Chen, 2024) model capitalizes on the strengths of the Transformer architecture to significantly enhance both inference speed and accuracy. By efficiently integrating encoder and IoU-aware query selection, while eliminating the need for traditional NMS post-processing, RT-DETR represents a significant breakthrough in real-time target detection.\nIn the specific domain of target tracking, existing Transformer-based frameworks often face challenges such as target loss and computational inefficiency, especially when tracking fast-moving targets over extended periods. To address these challenges and improve tracking stability and accuracy, this study integrates the Mamba in Mamba (MIM) framework with RT-DETR. This combination leverages the MIM framework's efficient long-sequence processing capabilities and memory mechanism for feature extraction, resulting in more effective fish target tracking. This approach not only enhances real-time tracking performance but also improves adaptability to target motion characteristics in complex environments, offering a novel technical solution in the field of target tracking."}, {"title": "3. Methods", "content": ""}, {"title": "3.1. Main Framework", "content": "This framework leverages the MIM architecture for feature extraction, the RT-Decoder architecture for decoding, and the QTSI and MQIM modules for post-processing the detection and tracking queries. The overall structure is illustrated in Figure 5. Initially, video sequences are sequentially input into the model, where each frame undergoes efficient feature extraction and encoding through the MIM architecture. To ensure the Query thoroughly learns the feature information, the model integrates the uncertainty minimal query selection unique to RT-DETR during the training phase. The initialized Detection Query is extracted from the encoder using RT-DETR's distinctive scheme and input into the RT-Decoder for decoding. For subsequent frames, the Track Query from the previous frame, combined with the decoded Detection Query, is used as the tracking query for the current frame after interaction with QTSI. Simultaneously, the Track Query and the newly initialized Detection Query are processed through MQIM to serve as the tracking query for the next frame."}, {"title": "3.2. Fusion MIM", "content": "To effectively extract fish boundary information in each frame, this paper incorporates the feature extraction mechanism of Mamba In Mamba (MIM) (Chen et al., 2024) into the feature extraction module. Although MIM offers the advantage of multi-scale feature representation, it faces challenges in enabling sufficient interaction between features at different scales. Deep features encapsulate rich detailed information, whereas shallow features are more adept at capturing global context. Existing feature fusion models, such as Feature Pyramid Network (FPN) (Lin, Doll\u00e1r, Girshick, He, Hariharan and Belongie, 2017), Debiased Single-Shot Multi-Frame Detector (DSSD) (Fu, Liu, Ranga, Tyagi and Berg, 2017), Differential Typical Correlation Analysis (Chaib, Liu, Gu and Yao, 2017), and Adaptive Spatial Feature Fusion (ASFF) (Liu, Huang and Wang, 2019), have made progress in inter-level feature correlation. However, these models still struggle with the detailed mapping of deeper features and the effective transfer of shallow global information.\nTo address this problem, this paper proposes an innovative feature extraction module, Fusion MIM, which combines the advantages of the Feature Fusion Single-Shot Multi-Frame Detector (FSSD) (Li, Yang and Zhou, 2017). This module is specifically designed to enhance the mapping of detailed information from deep features to shallow layers, while also improving the transfer of global information from shallow features. The Fusion MIM module is illustrated in Figure 6.\nThis module employs four layers of MIM feature extraction sub-modules at different scales to obtain the initial visual word dimension $W \\in \\mathbb{R}^{H \\times W \\times C}$ and visual sentence dimension $S \\in \\mathbb{R}^{H \\times W \\times D}$ of the image by passing the video frame image $X \\in \\mathbb{R}^{H \\times W \\times 3}$ of the fish through the Stem module, as shown in Equation (1):\n$W_0, S_0 = Stem(X)$ (1)\nAfterward, the initial visual word and visual sentence dimensions are passed through four MIM module stages of varying depths to obtain feature information at different scales $F$, as shown in Equations (2), (3), and (4):\n$W_{i}, S_{i} = MIM_{i} (W_{i-1}, S_{i-1}), i = 1, 2, ..., L_i$ (2)\n$f_i = \\text{Upsample Block} (S_{L_i})$ (3)\n$F = \\text{Cat} (f_i), i = 1,2,3,4$ (4)\nWhere i represents the feature extraction process corresponding to the ith layer, $L_i$ represents the depth of the MIM module for each layer, and the acquired features F are fed into the feature fusion module for interactive information fusion, as shown in Equations (5) and (6):\n$f_{i}^{enc} =  \\begin{cases}\nW_F(\\text{UBConv}(f_i), \\\\ \\text{USConv}(\\text{DCA}(f_{i+1}) \\\\ \\qquad + \\text{DCA}(f_i)\n\\text{DCA}(f_i), i = 4\n\\end{cases}, i = 1, 2, 3$ (5)\n$F^{enc} = \\text{Cat} (f_{i}^{enc}), i = 1,2,3,4$ (6)\nWhere $W_F$ denotes the feature mapping module, designed to map deep detail information to shallow features through weighted fusion, and DCA refers to the double cross-attention module, which captures long-range dependencies by sequentially addressing channel and spatial dependencies between multi-scale encoder features to bridge the semantic gap between encoder and decoder features. This well-designed feature fusion strategy effectively integrates various scale features in Mamba In Mamba, enhancing the richness and accuracy of feature representation and providing robust support for the precise extraction of the fish body boundary."}, {"title": "3.3. Query Time Sequence Intersection", "content": "Problems such as excessive similarity between individual sturgeon and severe occlusion among fish cause the original MOTR model overly rely on Track Query, leading to false tracking. To address this issue, this paper proposes a Query Temporal Interaction Module (QTSI) adapted from the MO-YOLO (Pan, Feng, Di, Bo and Xingle, 2023) model. The QTSI enables the model to evenly distribute Query detection while minimizing additional computational burden. This module is utilized exclusively during the training phase.\nSince both Detect Query and Track Query contain the object's bounding box (BBOX) information, new objects are typically predicted through Detect Query. This approach helps avoid the issue in the original model where Track Query might incorrectly carry over from frame t (where t > 2) to subsequent frames for prediction. The specific calculation process of QTSI is shown in Figure 8, Firstly, we define the following terms:\n\u2022 Detection Frame $BBox_{det}$: Corresponds to the detection query $w_{det}^{l}$, from the previous frame.\n\u2022 Tracking Frame $BBox_{tr}$: Corresponds to the tracking query $w_{tr}^{m}$ from the previous frame, excluding new matches.\n\u2022 Real Frame $w_{rew}^{new\\_o}$: The actual frame from the previous time step.\n\u2022 Tracking Query $w_{tr}^{t-1}$: The tracking query from the previous frame, including new matches.\nHere, l, m, n, and o represent the number of detection frames, tracking frames, real frames, and new matching queries, respectively, with m + o = n. Define MIOU as the maximum IOU, ERF as each real frame, ETF as each tracking frame, and EDF as each detection frame. Then, to determine the maximum Intersection over Union (IOU) score:\n1. Calculate the MIOU score of EDF with ERF, which is $[IOU_{1}^{det\\_g1}, IOU_{2}^{det\\_g1}, ..., IOU_{n}^{det\\_g1}]$.\n2. Calculate the MIOU score of ETF with ERF, which is $[IOU_{1}^{tr\\_gt}, IOU_{2}^{tr\\_gt}, ..., IOU_{m}^{tr\\_gt}]$.\n3. Calculate the MIOU score of the above two results.\nIf the maximum IOU score exceeds a predefined threshold $\\Phi_{iou}$, use the corresponding query $w_{tr}^{f\\_m}$ for further processing. Then, solve the concatenation result with $w_{tr}^{t-1^{new\\_o}}$ to determine the tracking query $w_{tr}^{new}$ for the subsequent frames, as shown in Equation (7).\n$w_{tr}^{new} = w_{tr}^{f\\_m} \\cup w_{tr}^{t-1^{new\\_o}}$ (7)"}, {"title": "3.4. Mamba Query Interaction Module", "content": "In this paper, we propose a new Mamba-based Query Fusion Interaction Module (MQIM), which transforms the original QIM module into a Mamba-based temporal feature interaction module. By leveraging the bidirectional temporal interaction mechanism of Vision Mamba (Zhu et al., 2024), MQIM facilitates feature association across multiple frames through long-term feature memory and feedback. This enhancement improves the tracking of fast-moving objects. The module diagram is shown in Figure 9.\nBy interacting with the tracking query processed by the QTSI module with the corresponding decoder output for temporal features, the initial tracking query of the next frame gains prior knowledge from the previous frame. This process enables better generalization of the previous frame's tracking results to the tracking of the next frame."}, {"title": "3.5. Joint Average Loss", "content": "In this paper, multiple loss modules are utilized for optimization, with the loss for a single frame image calculated as shown in Equation (8).\n$L(\\Upsilon_{t}, \\Upsilon_{t}^{'})= \\lambda_{c l s} L_{c l s}+\\lambda_{l_{1}} L_{l_{1}}+\\lambda_{g i o u} L_{g i o u}$ (8)\nwhere $L_{cls}$ is the focal loss (Li, Liu, Liu, Zhao and Liu, 2019), $L_{l_{1}}$ is the L1 loss, and $L_{giou}$ is the GIOU loss (Rezatofighi, Tsoi, Gwak, Sadeghian, Reid and Savarese, 2019). $\\lambda_{c l s}, \\lambda_{l_{1}}$ and $\\lambda_{g i o u}$ are the corresponding weight coefficients.\nSince the MOTR algorithm unifies the common loss of multi-frame images as the overall loss for the entire video sequence, effectively improving the tracking performance of time-sequenced video sequences, this paper also adopts this approach to optimize the loss calculation. The track loss and detector loss are calculated according to Equation (8), then summed and averaged as shown in Equation (9) and Equation (10):\n$M = \\sum_{i=1}^{N} (L_{i}^{tr}(\\Upsilon_{i}^{tr}, \\Upsilon_{i}^{'tr})+L_{i}^{det}(\\Upsilon_{i}^{det}, \\Upsilon_{i}^{'det}))$ (9)\n$L_o (\\Upsilon_{t}^{tr}, \\Upsilon_{t}^{det}) =  \\frac{M}{\\sum_{i=1}^{N} V_i}$ (10)\nWhere $V_i = V_i^{tr}+V_i^{det}$ represents the total number of ground truths in frame i, $L_{i}^{tr}(\\Upsilon_{i}^{tr}, \\Upsilon_{i}^{'tr})$ and $L_{i}^{det}(\\Upsilon_{i}^{det}, \\Upsilon_{i}^{'det})$ represent the tracking loss and detection loss of frame i, respectively."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets and Settings", "content": "In this paper, a new sturgeon fish tracking dataset was constructed. The video clips in this dataset were collected at the National Innovation Center for Digital Fishery of China Agricultural University. The camera resolution was set to 1920 x 1080, with a frame rate of 30 FPS. The video clips were captured in two different experimental scenarios, as shown in Figure 10.\nTo simulate a realistic aquaculture scenario (e.g., environments involving bubbles, water rotation, etc.), Scene 2 was enhanced with water circulation and oxygenation devices, as well as sensors of various morphologies. The video attributes are detailed in Table 1, with video clip lengths ranging from 10 to 20 seconds. A total of 11,000 labeled video frames were obtained through video frame-splitting and automatic labeling techniques. Specific dataset information, including the division of training and testing datasets, is presented in Table 1, where both cases are included in the dataset."}, {"title": "4.2. Evaluation Metrics", "content": "To demonstrate the superiority of the proposed FMRFT model, it is evaluated using several metrics, including Multi-Object Tracking Accuracy (MOTA), Multiple Object Tracking Precision (MOTP) (Dendorfer, Rezatofighi, Milan, Shi, Cremers, Reid, Roth, Schindler and Leal-Taix\u00e9, 2020; Ciaparrone, S\u00e1nchez, Tabik, Troiano, Tagliaferri and Herrera, 2020), Identification F1-score (IDF\u2081) (Ristani, Solera, Zou, Cucchiara and Tomasi, 2016), Identification Precision (IDP), Identification Recall (IDR), Frames Per Second (FPS), and Training Memory Allocation per GPU (TMA).\nMOTA measures the accuracy of single-camera multi-target tracking, expressed by the Equation (11):\n$MOTA = 1- \\frac{FN+FP+ \\Phi}{T}$ (11)\nWhere FN is the miss rate (i.e., positive samples predicted as negative by the model), FP is the false alarm rate (i.e., negative samples predicted as positive by the model), $\\Phi$ represents the sum of target jumps across all frames (i.e., changes in the tracking trajectory from 'tracking' to 'no-tracking'), and T is the total number of true targets in all frames. The closer MOTA is to 1, the better the performance of the tracker.\nMOTP is a measure of the accuracy of single-camera multi-target tracking matching, which refers to the distance between the predicted trajectory and the true trajectory, reflecting the accuracy of the tracking results, and is expressed by the Equation (12):\n$MOTP = \\frac{\\sum_{i, t} d_{i, t}}{C_t}$ (12)\nWhere $c_t$ denotes the number of matches in frame t. The matching error is computed for each pair of matches, and $d_{i, t}$ is the bounding box overlap between hypothesis i and its assigned ground truth object.\nIDF\u2081 refers to the F1 score for object ID identification in each object frame, calculated using the Equation (13) shown below:\n$IDF_{1} =  \\frac{2 \\times IDTP}{IDTP + IDFP + IDFN}$ (13)\nFurthermore, IDP and IDR re used to evaluate the detector's and tracker's performance in more detail. The formulas for calculating IDP and IDR are shown in Equations (14) and (15):\n$IDP = \\frac{IDTP}{IDTP + IDFP}$ (14)\n$IDR = \\frac{IDTP}{IDTP + IDFN}$ (15)\nWhere IDTP and IDFP represent the number of true positive IDs and false positive IDs, respectively, while IDFN represents the number of false negative IDs. IDTP is the sum of the weights of the edges selected as true positive ID matches, indicating the percentage of correctly assigned detections throughout the entire video. IDFN is the sum of the weights of the selected false negative ID edges, and IDFP is the sum of the weights of the selected false positive ID edges."}, {"title": "4.3. Experimental Evaluation", "content": "To highlight the superiority of the models proposed in this paper, we trained and tested the mainstream Tracking by Detection and Tracking by Query methods on the newly introduced sturgeon dataset using the FMRFT multi-target fish tracking algorithm, all under the same experimental conditions. The experimental results are presented in Table 3 (with\u2193 indicating lower values are better, \u2191 indicating higher values are better). In this table, OC-SORT (Cao et al., 2023) and FairMOT (Zhang, Wang, Wang, Zeng and Liu, 2021) are detection-based models, while the remaining models are query-based, which include TrackFormer (Meinhardt et al., 2022), TransCenter (Zeng et al., 2022), MOTR (Zeng et al., 2022), MOTIP (Yang, Duraiswami and Davis, 2005b), and FMRFT (proposed in this paper).\nFrom Table 3, it can be observed that FMRFT achieves the highest, IDR, and MOTA scores, with values of 90.3%, 90.4%, and 96.3%, respectively, while also maintaining a lower MOTP of 0.123. Additionally, FMRFT demonstrates a good FPS and low video memory usage during training. Compared to traditional detection-based models, FMRFT exhibits significant advantages in multi-target fish tracking tasks. Although detection-based models excel in detection speed, they fall short in tracking and recognition accuracy, particularly in complex scenes. In contrast, query-based multi-target tracking models show improvements in relevant metrics. Among these, TransCenter achieves the highest IDP, and MOTR achieves the lowest MOTP. However, FMRFT delivers the best overall performance, especially in IDF1 and MOTA metrics.\nTo further validate the effectiveness of the FMRFT model, we demonstrate its performance in the same experimental scenario by visualizing the tracking results. As shown in Figure 11, comparing the tracking results at"}]}