{"title": "HARNESSING WAVELET TRANSFORMATIONS FOR GENERALIZABLE DEEPFAKE FORGERY DETECTION", "authors": ["Lalith Bharadwaj Baru", "Shilhora Akshay Patel", "Rohit Boddeda"], "abstract": "The evolution of digital image manipulation, particularly with the advancement of deep generative models, significantly challenges existing deepfake detection methods, especially when the origin of the deepfake is obscure. To tackle the increasing complexity of these forgeries, we propose Wavelet-CLIP, a deepfake detection framework that integrates wavelet transforms with features derived from the ViT-L/14 architecture, pre-trained in the CLIP fashion. Wavelet-CLIP utilizes Wavelet Transforms to deeply analyze both spatial and frequency features from images, thus enhancing the model's capability to detect sophisticated deepfakes. To verify the effectiveness of our approach, we conducted extensive evaluations against existing state-of-the-art methods for cross-dataset generalization and detection of unseen images generated by standard diffusion models. Our method showcases outstanding performance, achieving an average AUC of 0.749 for cross-data generalization and 0.893 for robustness against unseen deepfakes, outperforming all compared methods. The code can be reproduced from the repo: https://github.com/lalithbharadwajbaru/\nWavelet-CLIP", "sections": [{"title": "1. INTRODUCTION", "content": "In today's digital landscape, we are witnessing an inundation of counterfeit images, arising from various sources. Some of these images are manipulated versions of authentic photos, altered using tools such as FaseShifter [1] and proprietor photoshop tools [2], while others are crafted through advanced machine learning algorithms. The advent and refinement of deep generative models [3, 4, 5] have particularly highlighted the latter category, drawing both admiration for the photo-realistic images they can produce and concern over their potential misuse. The challenge is compounded by the diverse origins of these fake images; they may manifest as real human faces created by generative adversarial networks or as intricate scenes synthesized by diffusion models [5]. This growing variety underscores the inevitability of encountering new forms of image forgery. Against this challenges, our research aims to devise a diverse generalizable fake detection framework capable of identifying any falsified image, even when training is confined to a single type of generative model.\nThere are numerous methods developed for deepfake generalization both within and cross-domain evaluation [6, 7, 8, 9, 10]. Of which, some works rely of basic encoders such as EfficentNet [9] and Xception [11]. Some models leverage frequency-based statistics for identfying some details which spatial domain can't capture [12, 13, 14]. Most of the existing deepfake detection models demonstrate significant results in scenarios where the training and testing data come from the same dataset. However, these detectors frequently face challenges in cross-domain or cross-dataset scenarios, where there is a significant discrepancy between the distribution of the training data and that of the testing data. Our method significantly advances the field of digital forensics by offering a robust model capable of countering the evolving threat of digital image forgery. It achieves this by effectively generalizing across different datasets and adeptly identifying deep-fakes produced by powerful, previously unseen generators.\nThe proposed approach offers distinct advantages over current methodologies in two key directions:\n1. We introduce an innovative Wavelet-based classifier designed specifically for deepfake detection, showcasing its applicability in identifying manipulated (deep-fake) content.\n2. Next, we highlight the capability of representations derived from CLIP to not only perform effectively across different unseen datasets but also to accurately identify images generated by models trained on previously unseen datasets."}, {"title": "2. METHODOLOGY", "content": "The main objective of this work is to devise a generalizable deepfake identification model which has two significant properties. First, the model is required to capture low-frequency features with detailed granular representations. Second, these representations should be adept at discerning forgery-specific characteristics, determining their authenticity or counterfeit nature.\nTherefore, our objective is to engineer a feature extractor (or encoder) which is capable of extracting granular features, alongside crafting a classifier that can effectively differentiate between deepfake and authentic camera-captured images. Thus, we partition the entire model into two primary components: a) the Encoder and b) the Classification Head."}, {"title": "2.1. Encoder", "content": "A good encoder has to understand the crucial features from the image distribution and map them to the latent space. This latent features should carry the prominent features of image. But, when it comes to generalization, the features have to be more relevant irrespective of trained or seen samples. In such scenarios, model that is trained on internet-scale data in a self-supervised fashion should provide fine-grained features irrespective of nature of data. Hence, we adopt pre-trained vision transformer [16] model that is trained via. CLIP fashion[15] pertaining strong one-shot transferable features. This encoder maps a image into a representation space of feature dimension $d$ where, $Enc_{\\theta}: \\mathbb{R}^{256 \\times 256 \\times 3} \\rightarrow \\mathbb{R}^{d}$ (we denote frozen encoder as $\\theta$). The latent features $Z$ captured for our study are using ViT-L/14 [15] and is represented as,\n$$Z = Enc_{ViT}(x), Z \\in \\mathbb{R}^{768}$$"}, {"title": "2.2. Classification Head", "content": "The classification head is tasked with categorizing the features generated by our encoder. Drawing inspiration from frequency-based techniques like Fourier Transforms [12], we focus on extracting subtle forgery indicators from images. We have developed a frequency-based Wavelet Classification Head that processes the features $Z$ derived from CLIP to determine their authenticity. In the following sections, we will provide a primer for the Discrete Wavelet Transforms and their inversions, and explain how certain design decisions can enhance the effectiveness of the classifier to identify deep-fakes.\nWavelet Transforms Wavelet Transforms are used to analyze various frequency components of a signal and is particularly useful representations that have hierarchical or multi-scale structure [27]. Applying a Discrete Wavelet Transform (DWT) the representation splits into low and high frequency components. Low-frequency components are responsible in capturing broad and nuanced features. Whereas high frequency components capture sharp features. The DWT decomposes as Low-pass filter ($l$), that captures the low-frequency component. Similarly, captures the high-frequency component, resulting in a down-sampled signal $d_1$. These are denoted as,\n$$s_{1k} = \\sum_{j} l(j-2k) s_j$$\n$$d_{1k} = \\sum_{j} h(j-2k) s_j$$\nThese operations can be expressed in matrix form, where $L$ and $H$ are matrices constructed from low-pass and high-pass filter coefficients, respectively, performing filtering and down-sampling in a single step through the operations $s_1 = Ls$ and $d_1 = Hs$. The Inverse Discrete Wavelet Transform IDWT reconstructs the original signal $s$ from its low-frequency ($s_1$) and high-frequency ($d_1$) components using:\n$$s_j = \\sum_{k} (l_{j-2k}s_{1k} + h_{j-2k}d_{1k})$$\nFor 2D signals like images, applying DWT along both dimensions results in four components- $X_{ll}$, (low-frequency in both dimensions), $X_{lh}$ (low-frequency row-wise, high-frequency column-wise), $X_{hl}$ (high-frequency row-wise, low-frequency column-wise), and $X_{hh}$ (high-frequency in both dimensions)-by applying matrices $L$ and $H$ across rows and columns, respectively. The DWT and IDWT can be expressed as,\n$$X_{ll} = LX L^T,$$\n$$X_{lh} = HX L^T,$$\n$$X_{hl} = LX H^T,$$\n$$X_{hh} = HX H^T,$$\n$$X = L^T X_{ll} L + H^T X_{hL} + L^T X_{lH} + H^T X_{hh} H.$$\nWavelet Classifier Now, we apply these transformations to the features derived from our encoder for effective classification. It is well-established that low-frequency components contain valuable information within the acquired representations. Therefore, to capture the most significant representations, we opt to transform the low-frequency features obtained from the DWT using an MLP layer (ref eq (11)) i.e., $X_{ll}$. This method facilitates the learning of broad and granular invariances. Subsequently, IDWT is employed to reconstruct these features into the spatial domain (ref eq (12)). The refined representations post-transformation are instrumental in discerning low-frequency components and spatial details, thereby strengthening our capability to differentiate between authentic and deepfake representations effectively (ref fig 1). The mathematical formulation can be described as,\n$$f_{low}^{(n)}, f_{high}^{(n)} = DWT(Z^{(n)})$$\n$$f_{low}^{'(n)} = MLP(f_{low}^{(n)})$$\n$$Z_{new}^{(n)} = IDWT([f_{low}^{'(n)}, f_{high}^{(n)}]).$$\nThe algorithm for our proposed model is delineated in Algorithm 1, positioning it as a versatile deepfake detection solution. Essentially, the model's efficacy lies in the robust learning capabilities of a strong encoder to classify representations accurately. We will next evaluate the effectiveness of this methodology by analyzing the performance of our proposed framework across diverse experimental conditions."}, {"title": "3. EXPERIMENTS", "content": "In this section, we will first detail the evaluation protocol, datasets used. Later we will discuss the performance of our approach with various state-of-the-art approaches.\nIn alignment with the training and evaluation protocol established by Yan et al. [6], the models undergo initial training on the FaceForensics++ c23 dataset [19]. Subsequent evaluations employ a cross-domain test using datasets such as Celeb-DF v1 (CDFv1) [28], Celeb-DF v2 (CDFv2) [28], and FaceShifter (Fsh) [1], thereby providing a robust framework to test the generalization capabilities of all the models. While the existing benchmarks do not encompass tests on emerging diffusion models, our research extends to examining general-izability with novel synthetic samples. Utilizing state-of-the-art diffusion models like DDPM [3], DDIM [4], and LDM [5] (wihtout text-guidance), we generate 10,000 images (from each model) from the CelebA dataset-none of which were included in training phase. This approach enables a comprehensive evaluation of model's adaptability to novel and unseen data, leveraging its potential for practical deployment in digital forensics. To assess the effectiveness of the results we use AUC (Area Under the Curve) and EER (Equal Error Rate) as fundamental metrics [6].\nAdditionally, we reproduce the method Ojha et al. [20] which was similar to our approach. Ours and Ojha et al. [20] uses a pre-trained self-supervised encoder and do not train or fine-tune it. Their major limitation is that they allow training a Linear Classifier for individual generative model (DDPM, DDIM and LDM) and the classification head lacks the generalizability. Thus, both these self-supervised encoders are not trained (frozen encoder) and only the classification heads are trained."}, {"title": "4. CONCLUSIONS", "content": "Thus, we anticipate a pivotal role for large transformer models, given their proficient ability to discern subtle distinctions by capturing specific nuances from forged features. Overall, Wavelet-CLIP secures state-of-the-art results in cross-data generalization and successfully identifies potential deepfakes originating from diffusion models. As a future direction, we plan to explore the capabilities of large pre-trained transformers on various text guidance-based [5], editing-based[29], and translation-based[30] diffusion models. Such research will establish a foundation for designing detection models capable of thwarting generated deepfakes, even when there is a slight shift in the distribution of the original source."}]}