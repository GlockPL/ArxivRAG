{"title": "Predicate Invention from Pixels via Pretrained Vision-Language Models", "authors": ["Ashay Athalye", "Nishanth Kumar", "Tom Silver", "Yichao Liang", "Tom\u00e1s Lozano-P\u00e9rez", "Leslie Pack Kaelbling"], "abstract": "Our aim is to learn to solve long-horizon decision-making problems in highly-variable, combinatorially-complex robotics domains given raw sensor input in the form of images. Previous work has shown that one way to achieve this aim is to learn a structured abstract transition model in the form of symbolic predicates and operators, and then plan within this model to solve novel tasks at test time. However, these learned models do not ground directly into pixels from just a handful of demonstrations. In this work, we propose to invent predicates that operate directly over input images by leveraging the capabilities of pretrained vision-language models (VLMs). Our key idea is that, given a set of demonstrations, a VLM can be used to propose a set of predicates that are potentially relevant for decision-making and then to determine the truth values of these predicates in both the given demonstrations and new image inputs. We build upon an existing framework for predicate invention, which generates feature-based predicates operating on object-centric states, to also generate visual predicates that operate on images. Experimentally, we show that our approach pix2pred is able to invent semantically meaningful predicates that enable generalization to novel, complex, and long-horizon tasks across two simulated robotic environments.", "sections": [{"title": "1 Introduction", "content": "A core research objective in robotics is to develop a robot that can make long-term decisions from low-level sensory inputs to accomplish a very broad range of tasks. Recent work has shown that model-free imitation learning can solve complex tasks directly from pixels (Chi et al. 2023; Zhao et al. 2023). However, sequential and compositional general-ization to novel tasks beyond the training distribution remain open challenges, especially when task horizons are long and demonstration datasets are small. In this work, we take an aggressively model-based approach to address these chal-lenges. In particular, we use sparse demonstration data to learn abstract world models (Silver et al. 2023; Liang et al. 2024) and plan in those world models to achieve strong com-positional and sequential generalization.\nAs a simple, pedagogical example, consider the \"Burger\" domain depicted in Figure 1. The agent is given demon-strations showing how to cook a patty, how to cut lettuce,"}, {"title": "2 Related Work", "content": "Decision-Making with Foundation Models. Our work is strongly inspired by impressive recently-demonstrated capa-bilities of Large (Vision) Language Models (LLMs, VLMs) across a variety of challenging text and image tasks (Ope-nAI 2023; Team et al. 2023; Dubey et al. 2024). We build on a large body of work that leverages foundation models for decision-making in robotics tasks (Ichter et al. 2023; Singh et al. 2023; Liang et al. 2022; Huang et al. 2022; Curtis et al. 2024; Huang et al. 2023, 2024; Hu et al. 2023a; Duan et al. 2024; Yang et al. 2024) (see Hu et al. (2023b) for a recent survey). However, many of these approaches operate under a number of different restrictive assumptions. Some are confined to a particular task distribution (e.g., only pick-and-place) or seek to synthesize new relatively short-horizon skills (e.g., pouring a liquid into a cup) (Huang et al. 2023, 2024; Duan et al. 2024). Others assume skills are provided, and attempt to use foundation models to compose them to solve longer-horizon tasks (Ichter et al. 2023; Singh et al. 2023; Liang et al. 2022; Huang et al. 2022; Curtis et al. 2024; Hu et al. 2023a; Quartey et al. 2024; Kumar et al. 2024a). These approaches all use a foundation model di-rectly for planning, which has been shown to perform sig-\nLearning Abstractions for Planning. We build on a long line of work that learns abstractions for efficient plan-ning (Bertsekas, Castanon et al. 1988; Jong and Stone 2005; Abel, Hershkowitz, and Littman 2016; Ugur and Piater 2015; Asai and Fukunaga 2018; Konidaris, Kaelbling, and Lozano-P\u00e9rez 2018; Silver et al. 2021; James, Rosman, and Konidaris 2022; Mao et al. 2022; Chitnis et al. 2022; Ku-mar et al. 2023; Silver et al. 2023; Han et al. 2024; Liu et al. 2024). Several of these works assume that predicates or operators are already given (Chitnis et al. 2021; Kumar et al. 2023; Mao et al. 2022; Siegel et al. 2024) or focus on the problem of learning predicates and other abstrac-tions from online interaction (James, Rosman, and Konidaris 2022; Liang et al. 2024), or assume access to dense nat-ural language descriptions with each demonstration (Liu et al. 2024; Han et al. 2024). By contrast, we aim to invent predicates and operators in an offline fashion from image-based demonstrations with no supervision on the kinds or number of predicates to learn. Like us, \u201cskills to symbols\" (Konidaris, Kaelbling, and Lozano-P\u00e9rez 2018) performs predicate invention given skills, but operates on point cloud data and uses a bisimulation objective to select predicates that is different from the fast-planning objective that we use. Our work builds directly on the method from Silver et al. (2023), which learns predicates and operators that operate over a handcrafted, low-dimensional feature space. We ex-tend this approach to learn predicates and operators that op-erate directly on images, and we demonstrate experimentally (Section 5) that this is necessary to solve tasks where at-tributes of objects important for decision-making cannot be easily extracted and encoded into a low-dimensional feature space."}, {"title": "3 Problem Setting and Background", "content": "We consider the problem of learning from demonstrations in deterministic, fully-observed environments. pix2pred takes as input a set of core object-types with meaningful natural-language names, an initial set of predicates, a set of parameterized controllers, and demonstrations of solv-ing tasks in the form of goal-directed observation-action se-quences, where goals are expressed in terms of the initial predicates and the sequences are segmented by controller. Each observation contains raw sensor data that includes one or more images characterizing the world state at that time, as well as data from sensors on the robot. We assume access to a perception function that can identify basic attributes of objects (e.g., position, dimensions, etc.) from the raw sensor data and can annotate objects of the given object-types on the images with unique names. Each controller execution in the demonstration is labeled with its name, the names of the objects it operates on in terms of names given by the percep-"}, {"title": "3.1 Demonstrations", "content": "Our demonstrations are provided within a set of training tasks drawn from some distribution T. We are ultimately interested in learning to solve any task T drawn from this distribution. All tasks occur within a common environment.\nEnvironments and Tasks We model an environment as a tuple (I, A, f, A, P), where I is the observation space, A is the action space, and f is an unknown transition function, denoted f: I \u00d7 A \u2192 I. An observation obs \u2208 I consists of a sequence of one or more images (obsimg) representing camera images from cameras operated by the robot, as well as a vector of continuous real-valued numbers (obsfeat.) representing raw sensor data associated with the robot, such as GPS and encoder data. The agent receives some obser-vation obst at some timestep t, and can execute an action a \u2208 A. The environment will then return a new observation obst+1 f(obst, a). We assume f is deterministic. A is a finite set of object types, where each type X \u2208 A has a name (e.g., robot, patty, etc.) and a tuple of real-valued fea-tures representing basic attributes of the object (e.g., position for most objects, position and joint angles for the robot). Within an environment, a task is a tuple (O, io, g). O is a set of objects, each with a type from A. The environment's perception function P : I \u00d7 O \u2192 X processes each obser-vation into a structured state used for downstream decision-making. These states have two parts: an image-based state, (ximg), and a feature-based state, (xfeat), concatenated to-gether to form a state x \u2208 X. The perception function gives a unique name to each object in the observations and annotates\nthe images with their names, as shown in Figure 2; this forms the image-based state. In practice this can be achieved by combining open-vocabulary object-detection and segmenta-tion models (Ren et al. (2024)). The perception function also identifies the basic attributes of each object according to its type; this forms the feature-based state. In practice, this can be implemented with custom detectors for each property of each object, and is simple when the attributes are basic prop-erties like pose and dimensions. We assume that objects are fully-observable to the perception function (i.e., there is no observation in which certain objects or types of objects are unobservable). io is the initial observation of the task, and 20 represents the initial state of a task produced by the per-ception function, where x\u3002\u2190P(io, O). We will henceforth refer to a task as involving this initial state: (0, xo, 9). We describe the goal g in a the next section.\nInitial Predicate Set and Goals In order to specify the goal g of a task, as well as provide a basis for decision-making and learning, we provide an initial set of predicates"}, {"title": "3.2 Operators and Planning", "content": "Predicates \u03a8 induce an abstract state space Sy of the task's underlying continuous state space X. Operators help specify an abstract transition model over S.\nMore specifically, given a particular state xt, as well as predicates \u03a8, we can compute the corresponding abstract state (denoted by st) by selecting all the ground atoms that are \u201ctrue\u201d in xt. Formally, st = ABSTRACT(xt, \u03a8) \u2261 {V: cy(xt) = true,\u2200\u03c8 \u2208 \u03a8}. Then, each operator w is a tuple \u03c9 = (\u03c4, \u03a1, \u0395+, E\u00af,C). Here, v are typed variables repre-senting the operator's arguments, P, E+ and E- are sets of predicates representing operator preconditions, add effects and delete effects respectively, and C is a controller asso-ciated with the operator. Note importantly that the discrete parameters of C must be a subset of v. Specifying objects of the correct type as arguments induces a ground operator 3 = (P,E+, E-,C). Ground operators define a (partial) abstract transition function: given w = <P, E+, E\u00af, C), if PC s, then the successor abstract state s' is (s \\ E\u00af) UE+ where is set difference.\nA ground operator specifies a corresponding ground con-troller C. To extract an action a from this ground controller, we must specify values for the parameters O associated with controller C. Following previous work (Silver et al. 2023; Kumar et al. 2023; Chitnis et al. 2022), we leverage contin-uous parameter samplers to do this.\nGiven these components, we can perform planning to solve test tasks. We assume access to a planner that takes a set of predicates, operators, and samplers, as well as a task T = (O, xo, g) associated with an environment, and outputs a plan that achieves the goal at the symbolic level, if such a plan exists. We implement such a planner via off-the-shelf classical planning algorithms (Helmert 2006), with details in Appendix A.1, though we note that our predicate invention framework is compatible with other planning strategies."}, {"title": "3.3 Predicate Invention Problem", "content": "Given demonstrations D structured as described in Sec-tion 3.1, our objective is to learn additional predicates (\u03a8), operators (\u03a9), and samplers (\u03a3) that enable aggressive gen-eralization to novel test tasks from the task distribution T by calling a planner with these learned components. We di-rectly leverage an existing approach to learn neural network samplers from our demonstration data (Appendix A.3). We are mainly interested in learning predicates and operators.\nPredicate Invention via Hill-Climbing We build on the framework introduced by Silver et al. (2023) for predicate invention. Their method first generates an initial pool of can-didate predicates pool from a grammar and then selects the subset of predicates from this pool that maximizes an objec-tive function designed to measure planning efficiency over the demonstrations, where the selection is done via a hill-climbing optimization. Predicates subselected from the pool pool are added to the initial set init to form a final set of predicates \u03a8. After, operators are learned in terms of the predicates \u03a8. Intuitively, optimizing their particular objec-tive fuction yields the predicate set (and corresponding op-"}, {"title": "4 Inventing Predicates from Pixels", "content": "Our main interest is to invent predicates from pixels (i.e., predicates that operate directly over one or more images in our image-based state ximg). We wish to do this from a hand-ful of structured demonstrations described in Section 3.1. These demonstrations do not possess any direct supervision for additional predicates (i.e., the demonstrations provide no indication of the number or structure of any predicates we should invent). The hill-climbing framework for predicate invention (Section 3.3) is able to learn predicates in such a setting, but requires a pool of initial candidate predicates to subselect from. To define such an initial pool, we must specify two things: what the predicates should be (i.e., what visual concepts are relevant for decision-making), and how should they be implemented (i.e., for every predicate & in the pool, how can we define a corresponding classifier func-tion cy).\nWe propose to use a VLM to both generate an initial pool of predicates that might be relevant to decision-making, and implement the classifier function for each predicate in the pool. Recent work has demonstrated that VLMs are able to answer a variety of common-sense natural language ques-tions given one or more images (Antol et al. 2015; Kim et al. 2024; Majumdar et al. 2024). The core intuition motivating our approach is that predicate generation and implementa-tion correspond to two different types of questions. Pred-icate generation can be accomplished by giving the VLM access to our demonstrations, and asking \u201cwhat visual con-cepts are changing between false and true after each action execution?"}, {"title": "4.1 Implementing Visual Predicates with a VLM", "content": "Recall that each state x \u2208 X is composed of two parts: a feature-based state xfeat and an image-based state ximg. Vi-sual predicates operate exclusively over ximg. Specifically, a visual predicate is a tuple of a name, a sequence of m types, and a classifier function: vis = (name, (X1,..., Am), C\u03c8). Here, the classifier function returns a boolean given an image-based state and object arguments cf : Ximg \u00d7 Om \u2192 {true, false}."}, {"title": "4.2 Proposing an Initial Pool of Visual Predicates", "content": "We generate a pool candidate of visual predicates by prompting a VLM on each demonstration d \u2208 D. For a demonstration d \u2208 D of length k = len(d), we extract the image-based state at each timestep xmg, t \u2208 [0, k \u2212 1] and add a text heading to each image corresponding to which timestep t in the demonstration it belongs to. We then pass all these images in sequence, along with the actions executed in between them (a0, a1,..., aka-1) directly to a VLM. We prompt the VLM to output a set of proposals for ground atoms based on these inputs.\nGiven a set of ground atoms proposed for each demon-stration, we then parse this set to discover atoms that are syntactically incorrect. In particular, we remove atoms that include object names that are not part of the demonstra-"}, {"title": "4.3 Predicate Invention via Subset Selection From Noisy Data", "content": "Given the procedure described in Section 4.2, we can gener-ate an initial pool of visual predicates . We combine these with feature-based predicates (generated via a pro-grammatic grammar as described in Silver et al. (2023)) to create an overall pool pool \u2190 isol U feat We wish to run hill-climbing optimization as described in Section 3.3 to subselect a small set of relevant predicates that are optimized for efficient and effective planning over our training tasks.\nUnfortunately, we cannot directly apply the particular hill-climbing optimization from previous work because our visual predicates are noisy. Visual predicates might not be consistent across states in the demonstrations due to occa-sional hallucination or mislabeling on the part of the VLM. This noise creates outliers in the abstract state space transi-tion data and causes the operator learning algorithm to over-fit and create a large number of unnecessary operators to model these outlier transitions. These extra operators make planning inefficient by increasing the branching factor of the planner's search algorithm. They also cause the hill-climbing predicate subselection \u2013 which interally performs operator learning on every subset of predicates it scores \u2013 to overfit and select a large number of unnecessary predi-cates. These overfit predicates can appear in the definitions of all operators \u2013 not just the operators created to model the outlier transition data \u2013 because operators are learned to be chainable (add effects of an earlier operator satisfy the pre-conditions of a later operator). The overfit predicates and overfit operators fail to generalize outside of the training distribution. We combat this problem by modifying the op-"}, {"title": "5 Experiments", "content": "Our experiments are designed to answer the following ques-tions.\nQ1. How well does pix2pred generalize to novel, more complex goals when compared to an imitation ap-proach that doesn't use a planner?\nQ2. How critical is it to perform explicit score-based optimization for subselection of predicates?\nQ3. How critical is it to invent both visual and feature-based predicates?\nEnvironments. We now describe our experimental envi-ronments and tasks, with details in Appendix A.6. We im-plement 4 task distributions across 2 environments. Each environment was introduced by previous work and lightly adapted for our setting.\n\u2022 Kitchen: A simulated robotic environment featuring a robotic arm positioned in front of a kitchen that includes a stove, a microwave, and controllable lights (Gupta et al. 2019). The task is to boil water in a kettle by turn-ing on a specific stove burner and pushing the kettle onto it. The task has a short horizon, requiring only two high-level skills in sequence to achieve the goal. Demonstra-tions show the robot solving the task using a particular burner from various kettle initial positions, and at test time the robot must use a different burner on the same stove.\n\u2022 Burger: A simulated 2D grid world environment (based on the Robotouille environment introduced by (Wang et al. 2023)) featuring a robot that must prepare and as-semble burgers from various components. Preparation includes chopping lettuce on a cutting board and cooking patties on a grill. We experiment with three task distri-butions within this environment, with each distribution featuring its own set of training and testing tasks: (1) Bigger Burger involves producing a burger with extra patties after being shown how to create a burger with a single patty, (2) More Burger Stacks involves producing multiple burgers after being shown how to create a single burger, and (3) Combo Burger involves producing burg-ers with multiple ingredients after being shown how to create burgers with a single ingredient. The tasks have a long horizon. For example, test tasks in Bigger Burger, More Burger Stacks, and Combo Burger can require a se-quence of 16, 29, and 30 controllers to achieve the goal, respectively.\nApproaches. We evaluate our method against several ab-lations and baselines from the literature, including those that attempt to solve test tasks directly without learning any ab-stractions.\n\u2022 Ours: Inventing visual and feature-based predicates with pix2pred, learning operators in terms of these predi-cates, and planning with these operators at test time."}, {"title": "6 Limitations and Conclusion", "content": "We proposed pix2pred, a method for inventing symbolic predicates that operate over raw images. We found that our approach, from just a handful of demonstrations (3-12), is able to invent semantically meaningful predicates that afford efficient planning and generalization to novel tasks across a range of domains and problems.\nThere are several noteworthy limitations of our present work. Firstly, VLM hallucinations adversely impact pred-icate proposal and atom labelling \u2013 especially with small amounts of demonstration data \u2013 which can inhibit our ap-proach's ability to learn useful predicates and operators. Sec-ondly, the hill-climbing framework we use for predicate in-vention can be extremely slow, especially as the number of demonstrations and size of the initial predicate pool grows. Finally, our approach assumes input demonstrations are seg-mented in terms of a provided set of parameterized con-trollers with names that correspond to their function.\nOne interesting future direction is to expand the struc-ture of predicates a VLM is allowed to produce, perhaps by allowing it to write arbitrarily-long text descriptions or even code to define new predicates (Liang et al. 2024). An-other is to improve the underlying hill-climbing optimiza-tion procedure to be significantly more efficient and toler-ant to noise, enabling pix2pred to scale to much larger domains that require potentially many tens or hundreds of predicates. Eventually we'd like to extend pix2pred to au-tomatically segment demonstrations and learn controllers so that we can learn to plan directly from the simplest low-level demonstration data."}, {"title": "A Appendix", "content": "A.1 Planner Implementation Details\nAlgorithm 1: Planning and Execution pseudocode\n1 Input: Task (O, xo, g), predicates \u03a8, operators \u03a9, samplers \u03a3.\n2 80 \u2190 ABSTRACT (x0, \u03a8)\n3 Call classical planner to generate an abstract plan: planabs = wo, W1, ..., Wm \u2190 Planner(so, \u039f, \u03a8, \u03a9, g)\n4 Extract controller sequence with discrete args filled in: skeleton = C0, C1, ..., Cm\n5 For i = 0,...,m:\nSample 0 ~ \u03c3\u03c9;, where \u03b8 represents the continuous parameters of C\u2081 and \u03c3\u03c9; is the sampler associated with wi\nUse 0 to fully ground controller C\u2081 into an action a\u015f. execute action a\u017c and obtain the following state Xi+1 \u2190\u2190 f(xi, ai)\nAlgorithm 1 shows the pseudocode for the planning and execution strategy we implement following recent work (Kumar et al. 2024b). Given an initial state xo for a task, we simply evaluate the classifiers of the given pred-icates \u03a8 to convert the state into an abstract state so. We then call a classical planner with the task object set O and operators \u03a9 to compute an abstract plan that achieves the goal (if one exists). We extract the ground controller se-quence for this abstract plan, and then simply greedily ex-ecute each controller sequentially by calling the sampler as-sociated with each operator.\nWe note that more sophisticated planning strategies are possible. In particular, provided a photorealistic simulation environment for the transition function f, we could leverage many task and motion planning (Garrett et al. 2021), such as bilevel planning (Silver et al. 2023; Kumar et al. 2023; Chitnis et al. 2022).\nA.2 Operator Learning\nWe adapt the \"cluster and intersect\" operator learning strat-egy from previous work (Chitnis et al. 2022) to handle noise in predicate values inherent to our setting. Specifically, we learn a set of operators \u03a9 from our demonstrations D and predicates I in four steps. Of these, the first three steps are largely taken directly from previous work: we introduce a modification to the third step, as well as the final step to handle noise.\n1. Partitioning: Each demonstration can be expressed as a sequence of transitions {(x, a, x')}, with x,x' \u2208 X and a \u2208 A. Recall that each action a is a controller with par-ticular discrete and continuous arguments specified; let C denote the corresponding controller with the same dis-crete object argument values, but continuous parameter values left unspecified. First, we use \u03a8 to ABSTRACT all states x, x' in the demonstrations D, creating a dataset of transitions {(s, a, s')} with s, s' \u2208 S\u0173. Next, we partition these transitions via the following equivalence relation:\n(81,01,81) = (82, a2, 82) if the effects and controllers unify, that is, if there exists a mapping between the ob-jects such that C1, (81 \u2013 s\u2081), and (s1 - 81) are equiva-lent to C2, (82 - 82), and (82 - 82) respectively. After this step, we have effectively 'clustered' all transitions in D together: we can associate each transition {(s, a, s')} with a particular equivalence class.\n2. Arguments and Effects induction: For each equivalence class created in the previous step, we create v by select-ing an arbitrary transition (s, a, s') and replacing each object that appears in the controller C or effects with a variable of the same type. This further induces a substitu-tion \u03b4 : v \u2192 O for the objects O in this transition. Given this, the E+, and E- can then be created by applying \u03b4 to (s' - s), and (s \u2013 s') respectively. By construction, for all other transitions 7 in the same equivalence class, there exists an injective substitution 8 under which the con-troller arguments and effects are equivalent to the newly created E+, and E\u00af.\n3. Precondition learning: The only remaining component required to turn each equivalence class into an opera-tor is the operator preconditions. For this, we perform an intersection over all abstract states in each equiva-lence class (Bonet and Geffner 2019; Curtis et al. 2021). Recall that an abstract state is simply the collection of ground atoms that are 'true', thus taking an intersection amounts to finding the set of atoms that are always true across every initial state of every transition (s, a, s') in the equivalence class. However, since some of our pred-icate classifiers might be noisy they might not always hold. Thus, we take a 'soft' intersection: we take any atom as a precondition that is true across more than a specific percentage (set by a hyperparameter hpre_frac) of transitions in the equivalence class. More specifically, \nD P \u2190 \u2229r=(s,.,.) 871(s) if |671(8)| \u2265 hpre frac, where 8-1(s) substitutes all occurrences of the objects in s with the parameters in v following an inversion of 87, and dis-cards any atoms involving objects that are not in the im-age of 67. |6-1(8)| denotes the number of transitions in D in which the lifted atom |8-1(8)| holds, and |D| denotes the total number of transitions in D. In our experiments, we set hpre frac to 0.8.\n4. Pruning low-data operators: Noise in the atom values of-ten causes there to be equivalence classes with very few data points, since transitions that have been affected by noise do not unify with other transitions in our dataset. These lead to operators that are overfit to those partic-ular noisy transitions, which are undesirable. We com-bat this via simply discarding learned operators that have data below a certain fraction (denoted by hyperparameter hdata frac) of the total transitions associated with a partic-ular controller C. In particular, let TC denote the num-ber of transitions in D where the action a involves a par-ticular controller C. For any operator w, let |\u03c4\u03c9| denote the number of datapoints associated with the equivalence class used to construct that operator. We only keep an operator w if Tw > hdata frac. In our experiments, we set\nTC"}, {"title": "A.3 Sampler Learning", "content": "To enable execution, we must also learn samplers for proposing continuous controller parameters (Algorithm 1). Adapting prior work (Silver et al. 2023; Kumar et al. 2023; Chitnis et al. 2022), we train one sampler per operator, de-fined as:\n\u03c3(x, 01,..., Ok) = $o(x[01] \u2295\uff65\uff65\uff65\u2295x[Ok]),\nwhere x[0] is the feature vector for o, \u2295 denotes concatena-tion, and so is a learned model. Treating this as supervised learning, we use operator-specific datasets \u03c4\u03c9, where each of these datasets is composed of all transitions from the equiv-alence class used to induce this operator (Appendix A.2). Each transition (Si, ai+1, Si+1) \u2208 \u03c4\u03c9 maps operator argu-ments to objects via \u03b4 : v \u2192 07. Using this mapping, the input for training is x[\u03b4(v1)] \u2295 \u00b7\u00b7\u00b7 \u2295 x[\u03b4(\u03c5\u03ba)], where (01,..., Uk) = v, and the output is the continuous param-eter vector 0, which are the continuous parameters used for Ai+1.\nEach sampler is parameterized by two neural networks."}, {"title": "A.4 More on Goals", "content": "The agent may sometimes invent visual predicates that can be interpreted as component of the task goal. For example, the goal of a task may be to make a burger with a cooked patty inside it. Here, the goal predicate would be an indicator for if this is achieved or not, perhaps called BurgerComplete (bottom_bun, patty, top bun), in which case Cooked (patty) is an implicit component of that goal, in that BurgerComplete's classifier implicitly checks that the patty is cooked. This is in contrast with a situation where the goal of a task is to fill a cup of water, and where the cup starts out upside down so that you can't pour into it. Here, Upright (cup) may not be a component of HasWater (cup), in that the classifier for HasWater may not be explicitly checking if the cup is upright. In the former case, one could argue"}, {"title": "A.5 Additional Prompting Details", "content": "Here, we provide additional details how we prompt a VLM for both labeling and proposal of atoms.\nAtom Labeling. Recall from Section 4.1 that the goal of atom labeling is to obtain the truth value of a particular ground atom (00,...,\u03bf\u03b9) given an image-based state at timestep t in a trajectory xmg. We do this by prompting a VLM with a text prompt, as well as a string representation of the atom (e.g. Cooked (patty1)) and asking it to out-put the truth value of the atom. Since we usually want to query for the values of many atoms at once, we provide all atoms in one single query and ask a VLM to label their truth values simultaneously.\nWe use a different prompt depending on whether timestep t = 0 ort > 0. For t = 0, we simply provide the ini-tial image-based state xo xing and a prompt (shown below) that asks the model to label all the values of atoms listed in the prompt with either \u201cTrue\u201d, \u201cFalse\u201d or \u201cUnknown\".\nYou are a vision system for a robot. Your job is to output the values of the following predicates based on the provided visual scene. For each predicate, output True, False, or Unknown if the relevant objects are not in the scene or the value of the predicate simply cannot be determined. Output each predicate value as a bul-leted list with each predicate and value on a different line. For each output value, provide an explanation as to why you labelled this predicate as having this particu-lar value. Use the format: <predicate>: <truth_value>. <explanation>.\nPredicates:\nWe then specify the ground atom strings whose values we'd like to query. For instance, in the \"Bigger Burger\" task, these might be:\nWe found two aspects of this were critical to labeling ac-curacy. Firstly, chain-of-thought prompting (Wei et al. 2022) was extremely useful: our prompt asks the VLM to explic-itly provide reasoning for its choices in addition to labeling the truth value. Secondly, recall that the image-based state ximg includes segmentation of object names rendered onto the image itself (as depicted in Figure 2). This is a form of set-of-marks prompting (Yang et al. 2023) and is critical to the VLM being able to identify objects being referred to in the text prompt.\nFor all states xt in a trajectory that are not the initial state (i.e., t > 0), we prompt the VLM with the following infor-mation: (1) the image-based state from the current and pre-vious timesteps (i.e. xmg ting and img), (2) the action executed to get to the current state (i.e. at-1), and (3) the response of the VLM from the previous timestep.We modify the prompt to the VLM to be as follows (here we show some of the predicates queried and a particular ac-tion for a certain timestep in a trajectory for a Burger task):\nYou are a vision system for a robot. You are provided with two images corresponding to the states before and after a particular skill is executed. You are given a list of predicates below, and you are given the values of these predicates in the image before the skill is executed. Your job is to output the values of the following predi-cates in the image after the skill is executed. Pay careful attention to the visual changes between the two images to figure out which predicates change and which predi-cates do not change. For the predicates that change, list these separately at the end of your response. Note that in some scenes, there might be no changes. First, out-put a description of what changes you expect to happen based on the skill that was just run, explicitly noting the skill that was run. Second, output a description of what visual changes you see happen between the before and after images, looking specifically at the objects involved in the skill's arguments, noting what objects these are. Next, output each predicate value in the after image as a bulleted list with each predicate and value on a different line. For each predicate value, provide an explanation as to why you labeled this predicate as having this particu-lar value. Use the format: <predicate>: <truth_value>.\nPredicates:\nYou\""}, {"title": "A.6 Additional Experimental Details", "content": "Additional Environment Details. Here, we describe in detail the initial predicates, training demonstrations, and test tasks for each task in each of our experimental"}]}