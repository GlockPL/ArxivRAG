{"title": "TQA-Bench: Evaluating LLMs for Multi-Table Question Answering with Scalable Context and Symbolic Extension", "authors": ["Zipeng Qiu", "You Peng", "Guangxin He", "Binhang Yuan", "Chen Wang"], "abstract": "The advent of large language models (LLMs) has unlocked great opportunities in complex data management tasks, particularly in question answering (QA) over complicated multi-table relational data. Despite significant progress, systematically evaluating LLMs on multi-table QA remains a critical challenge due to the inherent complexity of analyzing heterogeneous table structures and potential large scale of serialized relational data. Existing benchmarks primarily focus on single-table QA, failing to capture the intricacies of reasoning across multiple relational tables, as required in real-world domains such as finance, healthcare, and e-commerce. To address this gap, we present TQA-Bench, a new multi-table QA benchmark designed to evaluate the capabilities of LLMs in tackling complex QA tasks over relational data. Our benchmark incorporates diverse relational database instances sourced from real-world public datasets and introduces a flexible sampling mechanism to create tasks with varying multi-table context lengths, ranging from 8K to 64K tokens. To ensure robustness and reliability, we integrate symbolic extensions into the evaluation framework, enabling the assessment of LLM reasoning capabilities beyond simple data retrieval or probabilistic pattern matching. We systematically evaluate a range of LLMs, both open-source and closed-source, spanning model scales from 7 billion to 70 billion parameters. Our extensive experiments reveal critical insights into the performance of LLMs in multi-table QA, highlighting both challenges and opportunities for advancing their application in complex, data-driven environments. Our benchmark implementation and results are available at https://github.com/Relaxed-System-Lab/TQA-Bench.", "sections": [{"title": "1 Introduction", "content": "The rise of large language models (LLMs) [1] has unlocked unprecedented opportunities for tackling complex data management tasks [2, 3, 4, 5], particularly in question answering (QA) across intricate relational data [6, 7, 8, 9, 10]. Despite these advancements, systematically evaluating LLMs on multi-table QA remains a significant challenge due to the task's inherent complexity \u2014 Multi-table QA requires LLMs to extract and analyze information from multiple interconnected tables, often dealing with highly heterogeneous table structures and serialized lengths. As LLMs continue to demonstrate remarkable capabilities across various data management applications [4, 5, 11, 12], we believe there is an urgent need for a comprehensive understanding of LLM's performance in tackling the complexities of Multi-Table QA.\nSystematically evaluating and understanding the performance of LLMs on Multi-Table QA is a crucial step toward unlocking their full potential for data management and business intelligence [6, 13, 14]. Structured relational data is pervasive across domains such as finance [15], healthcare [16], and e-commerce [17]. Real-world tasks often require processing, retrieving, and reasoning over multiple tables to support data-driven analysis and decision-making. However, there is a significant gap between existing Table QA benchmarks [6, 13, 14] and the practical demands of applications that operate on real-world tabular data. We believe that addressing this disparity is essential to bridging the divide and advancing the utility of LLMs in complex, data-centric environments."}, {"title": "2 Benchmark Construction", "content": "The construction process of our Multi-Table QA benchmark is systematically divided into four key phases: data collection, relational data sampling, evaluation task definition, and question generation with symbolic extensions, where each phase can be summarized as:\n\u2022 Collect Multi-Table data (\u00a72.1). To ensure the diversity and representativeness of our benchmark, we collect a wide variety of large-scale relational databases. These databases serve as the foundation for generating multi-table QA tasks.\n\u2022 Relational data sampling (\u00a72.2). We design a sampling methodology to create subsets of each table with varying serialized lengths. This approach ensures that the sampled data maintains the structural integrity and heterogeneity of the original datasets to evaluate LLM's performance under different context lengths ranging from small (8K tokens) to large (64K tokens).\n\u2022 Define evaluation task categories (\u00a72.3). We define three primary question categories, further divided into seven subcategories, inspired by those commonly found in traditional Table QA datasets. These categories are designed to capture a broad spectrum of question types, reflecting the diverse requirements of real-world multi-table QA tasks.\n\u2022 Generate evaluation question with symbolic extension (\u00a72.4). For each question category, we develop structured question templates that are augmented with symbolic extensions to assess reasoning capabilities beyond simple retrieval. These templates are paired with Python-based answer generation, enabling the automated creation of benchmark questions and ensuring scalability and reliability in task evaluation.\nThis structured and systematic process enables the creation of a scalable, diverse, and effective benchmark for evaluating LLM performance on complex multi-table QA tasks."}, {"title": "2.1 Multi-Table Data Collection", "content": "Many existing Table QA datasets primarily collect their tables from Wikipedia [23, 24, 25, 26, 27, 28, 29], which is a source known for its high quality. However, the inclusion of Wikipedia in the training datasets of many LLMs introduces a potential bias and contamination in Table QA benchmarks, as these models are pre-acquainted with the content. Additionally, these datasets often feature relatively short tables, typically consisting of only tens of rows, which do not adequately challenge the models' capabilities in handling more comprehensive Table QA tasks. To address these shortcomings and enhance the robustness of our benchmarks, we consciously opted to exclude Wikipedia-derived tables from our dataset collection, which ensures a more rigorous evaluation by testing the models against unfamiliar and complex table structures. In our data collection procedure, we consider the following data sources:\nWorldBank. To address the limitations of Wikipedia-sourced data, we incorporated datasets from the World-Bank [30], a well-established repository of real-world tabular data. WorldBank datasets are particularly valuable for their extensive rows and columns, often exceeding the token processing limits of most LLMs. Additionally, these datasets frequently include simple yet meaningful foreign key relationships, which are crucial for generating actionable insights. Our analysis of WorldBank data revealed that real-world datasets often feature long-context, multi-table structures, characteristics that are generally absent from existing Table QA benchmarks. For our benchmark, we selected a dataset from WorldBank [31], ensuring its compatibility with our experimental setup and its ability to challenge LLMs in realistic, complex scenarios.\nDataGov. In addition to WorldBank, we also collect data from DataGov [32], another rich repository of real-world tabular information. The datasets in DataGov often comprise large tables with numerous rows and columns, providing a robust testbed for evaluating LLMs' capabilities in handling extensive tabular contexts. Many of these tables exhibit simple foreign key relationships, enabling multi-table reasoning and facilitating deeper insights. For our benchmark, we selected two datasets from DataGov: the Water Quality Data [33] and Food Facility Inspections [34]. These datasets were sampled and scaled to accommodate varying context lengths, allowing us to evaluate model performance across a range of experimental setups.\nBIRD. To complement the datasets from WorldBank and DataGov, we included seven databases from BIRD [35], a benchmark designed for testing Text2SQL tasks. BIRD databases closely resemble real-world datasets, featuring multiple tables and complex foreign key relationships. This makes them an excellent addition to our benchmark, as they reflect the intricacies of working with realistic Multi-Table environments. However, due to BIRD's primary focus on Text2SQL tasks, many of its databases lack referential integrity, such as missing or invalid foreign key relationships. Our sampling approach requires referential integrity and acyclic foreign"}, {"title": "2.2 Sampling to Variate Context-Length", "content": "A key observation from Table 1 is that the selected databases often contain tables with over 100,000 rows, which significantly exceeds the token processing capabilities of mainstream LLMs. As a result, directly constructing a Multi-Table QA dataset from such large-scale tables remains impractical due to context length constraints.\nTo address this challenge, we have developed a novel sampling method designed to generate databases of varying context lengths, thus enabling scalable benchmarking across diverse experimental setups. This approach allows us to adapt the benchmark to different computational constraints and evaluation needs. The process of sampling from the ten databases, referred to as the original databases, is detailed in Table 2. This sampling process involves two primary steps: first, determining the topological order of the tables based on their foreign key relationships to ensure referential integrity is maintained during sampling; second, performing the row sampling for each table to create smaller, manageable versions of the original databases. These steps ensure that the structural and relational properties of the databases are preserved, even at a reduced scale, allowing for effective benchmarking under various conditions."}, {"title": "2.3 Evaluation Task Categories", "content": "The landscape of table QA benchmarks has significantly evolved over the years, with various benchmarks emphasizing distinct tasks. Early benchmarks predominantly focused on straightforward tasks such as table lookup and aggregation. These tasks required language models to extract values or compute simple summaries directly from tabular data [23, 25]. As research in this area has progressed, however, the complexity of tasks in datasets has increased. Recent benchmarks have begun to incorporate tasks that require numerical reasoning, demanding computational abilities from language models. These tasks include performing arithmetic operations and understanding numerical relationships, which add a layer of complexity and sophistication to the benchmarks [36, 37]. This evolution reflects the growing capabilities of language models and the increasing sophistication of the queries they are expected to handle.\nDespite notable advancements, most existing datasets are constructed around single short-context tables that primarily aim to deepen the level of reasoning within a constrained context. For instance, these datasets frequently encompass tasks that require performing multiple steps of arithmetic operations such as addition, subtraction, multiplication, or division [36, 37]. However, they often do not reflect the complexities and challenges associated with long-context, multi-table scenarios. To bridge this gap, our benchmark has been strategically designed not merely to increase the depth of reasoning arbitrarily but to focus on three carefully delineated categories that align with distinct levels of difficulty: lookup, aggregation, and complex calculation. This structured approach allows us to assess the models' capabilities more comprehensively, covering a broader range of table QA complexities and scenarios. Each category encompasses specific subcategories, which are systematically illustrated in Figure 1 and enumerated below:\n\u2022 Lookup tasks are foundational in table-based reasoning. They require the model to locate and extract specific information from tables. We design two tasks in this category:\n\u2022 Entity lookup: this task involves retrieving a specific value in the table based on given conditions. For instance, in Figure 1, a question like \"What is the description of air carrier 20398?\" requires the LLM to locate the relevant row in the \u201cAir_carriers\u201d table and extract the corresponding \"Description\" value. In Multi-Table scenarios, such queries often require cross-referencing rows across different tables, adding complexity to the task.\n\u2022 Top selection: This task focuses on identifying key elements or the \u201ctop\u201d entities in a table based on a specific criterion. For example, in Figure 1, a question like \u201cWhich airport lands the most flights starting from Chicago, IL: Chicago O'Hare International?\" requires the model to perform several steps: first, identify the \"Code\" of the specified airport (\u201cORD\u201d). Next, count the flights landing at various airports originating from \u201cORD.\u201d Finally, the model must determine the \"Description\" of the airport(s) with the highest count. In cases of multi-tables, multiple elements may share the same \"top\" rank, requiring the model to apply precise filtering and selection techniques.\n\u2022 Aggregation tasks, though conceptually simpler, test an LLM's ability to filter and compute integrated information from the table or the join of multiple tables. We include three aggregation functions in categories:\n\u2022 Count: this task requires the model to determine the total number of rows or elements satisfying a specific condition. For instance, in Figure 1, a query like \u201cHow many airlines land in Flint, MI: Bishop International?\" involves identifying the \u201cCode\u201d of \u201cFlint, MI: Bishop International\u201d and counting all rows where \u201cDEST\u201d equals this \"Code.\"\n\u2022 Sum: this task requires the model to compute the total of a specific numerical attribute across rows that meet certain criteria. For example, in Figure 1, a question like \u201cWhat is the total flight delay (DEP_DELAY) for flights starting from Chicago, IL: Chicago O'Hare International?\" requires summing the values in the \u201cDEP_DELAY\u201d column for rows where \u201cORIGIN\u201d equals \"ORD.\u201d\n\u2022 Average: this task requires the model to calculate the mean of a numerical column for rows matching a condition. For example, in Figure 1, a question like \u201cWhat is the average flight delay (ARR_DELAY) for flights landing in Flint, MI: Bishop International?\u201d involves calculating the average value in the \"ARR_DELAY\" column for rows where \u201cDEST\u201d equals \u201cFNT.\u201d\n\u2022 Complex calculation tasks evaluate advanced reasoning capabilities, focusing on more intricate operations. We categorize these into two subcategories:"}, {"title": "2.4 Evaluation Question Generation with Symbolic Extension", "content": "Inspired by the GSM-Symbolic framework [18], we use the symbolic extension in our benchmark to increase the number of evaluation questions while keeping their quality high. Symbolic extension works effectively with sampling to create diverse and meaningful questions, making the benchmark more robust for evaluating LLMs.\nSymbolic Question Generation. Our symbolic extension is divided into two principal components: template question design, and the generation of questions and solutions, as depicted in Figure 1. Template questions are crafted with placeholder variables instead of fixed values, enabling dynamic content generation. These variables are subsequently instantiated, and the correct answers are computed using Python implementation. This methodology facilitates the creation of multiple question instances from a single template, thereby enhancing the benchmark's versatility and scalability. To enable a more effective evaluation, we employ multiple-choice questions (MCQs) rather than relying solely on traditional metrics such as exact match, BLEU, or F1 scores. These conventional metrics can fall short of accurately assessing the reasoning capabilities of large language models (LLMs). MCQs offer a more direct method to evaluate understanding and reasoning by providing discrete, comparable options [38]. For each database, we manually design two template questions per subcategory, each paired with a corresponding Python code solution. This approach yields a total of 140 template questions across all databases and subcategories. To populate the benchmark with diverse instances, we leverage the ten database instances created for each database and context length. Using the symbolic extension, we generate ten question instances for each template question. An overview of the total number of benchmark instances is provided in Table 4, illustrating the extensive scale and scope of our benchmark."}, {"title": "3 Evaluation Setup", "content": "Our benchmark is created using database sampling and symbolic extension, making it challenging to test all the generated instances. To address this, we design experiments across multiple dimensions to evaluate its performance effectively."}, {"title": "3.1 LLM Benchmark Scope", "content": "To ensure a comprehensive evaluation of LLM's performance on our benchmark, we select 22 LLMs from various companies or research organizations. The selected models cover the most advanced proprietary LLMs available such as GPT-4O [39], as well as other widely-recognized open-source models such as QWEN2.5 [40]. It is worth mentioning that we also choose a state-of-the-art model from DeepSeek that employs mixture-of-experts (MoE) architecture [41]. Moreover, we include two domain-specific LLMS, TABLELLAMA [42] and TABLEGPT2 [43], which are specifically fine-tuned for analyzing tabular data and accomplishing various table-based tasks. Meanwhile, the parameter scales of the models we choose range from 2B to 72B, which may provide insights into the relationship between model size and Multi-Table QA performance. Such diversity ensures the benchmark evaluates models of varying architectures, specializations, and computational complexities, providing valuable insights into the strengths and limitations of current LLMs in Table QA tasks. We enumerate the details of the selected LLMs below:\n\u2022 GPT-4O-MINI [44] and GPT-4O [39]: we evaluate GPT-4O-MINI and GPT-4O from OpenAI as the state-of-the-art close-source model in this benchmark. Both of them are proprietary instruct models that are tailored for conversational AI and reasoning tasks. All of them support long context up to 128K tokens.\n\u2022 QWEN2.5 [40]: we select the 3B, 7B, 14B, 72B versions of QWEN2.5 Instruct model and a 7B Coder model for evaluation. All of them support long context up to 128K tokens.\n\u2022 LLAMA3.1 [45]: we select the 8B and 70B LLAMA3.1 Instruct model for evaluation. Both of them support a context length up to 128K tokens.\n\u2022 BAICHUAN2 [46]: we select BAICHUAN2 7B chat model and 13B chat model for evaluation. Both of them support long context up to 192K tokens.\n\u2022 GEMMA2 [47]: we select the 2B, 9B, and 27B versions of GEMMA2 Instruct model for evaluation. All of them only support a context length up to 8K tokens.\n\u2022 GLM-4 [48]: we evaluate GLM-4-9B-CHAT on the benchmark. The model is a chat model that supports a context length of 128K tokens.\n\u2022 MISTRAL [49]: we evaluate MISTRAL-NEMO-INSTRUCT and MISTRAL-7B-INSTRUCT on the benchmark. Both of them are instruct models. MISTRAL-NEMO-INSTRUCT is trained with 12.2B parameters and supports up to 128K context window. MISTRAL-7B-INSTRUCT is trained with 7B parameters and supports up to 32k tokens.\n\u2022 VICUNA [50]: we select VICUNA-7B-V1.5-16K and VICUNA-13B-V1.5-16K to evaluate. As the name suggests, VICUNA-7B-V1.5-16K is a chat model trained with 7B parameters and supports up to 16k tokens. VICUNA-13B-V1.5-16K is a chat model trained with 13B parameters and supports up to 16k tokens.\n\u2022 DEEPSEEK-V2 [41]: we evaluate DEEPSEEK-V2-LITE-CHAT on the benchmark. Unlike dense architecture,"}, {"title": "3.2 Evaluation Design and Implementation", "content": "We designed our evaluation setup to be simple and consistent, ensuring it serves as a strong baseline. All benchmark tests follow the same standardized template shown in Figure 2. This uniform approach avoids unnecessary optimizations or tricks, making the results easy to compare across different experiments and models.\nDesign of the Experiment. We have meticulously designed a series of concrete experiments to test the following hypothesis: How the choice of serialization format, the scale of LLM, and the incorporation of symbolic extensions influence the performance of LLMs in handling long-context, Multi-Table QA tasks? Concretely, each experiment aims to evaluate different facets of this hypothesis as we enumerated below:\n\u2022 Experiment 1. Serialization format evaluation: given the importance of serialization in managing long-context, Multi-Table data, our first experiment compares two commonly used formats: Markdown and CSV. These formats are selected for their standardization and efficiency in handling long tables, as opposed to JSON. We conduct this comparison using a variety of models including GPT-4O, GPT-4O-MINI, QWEN2.5-7B-Instruct, Qwen2.5-CODER-7B-INSTRUCT, and LLAMA3.1-8B-INSTRUCT. The results from this experiment will determine the serialization method to be used in subsequent experiments to ensure consistency across all evaluations.\n\u2022 Experiment 2. Comprehensive evaluations of LLMs: following the selection of the serialization method,"}, {"title": "4 Results and Analysis", "content": null}, {"title": "4.1 Serialization Format Evaluation Results", "content": "In this study, we conducted an initial comparison between two serialization formats\u2014Markdown and CSV\u2014to evaluate their impact on LLM performance. The results, summarized in Table 5, reveal significant differences in how LLMs process and perform with these formats.\nOverall Results. The benchmark results indicate that Markdown consistently outperforms CSV across a majority of LLMs. While GPT-4O-MINI showed slightly better average accuracy with CSV in specific context lengths, the average accuracy for all other models was higher with Markdown. This suggests that even for long-context tables, Markdown remains the preferred format for LLMs. A plausible explanation for this preference is that Markdown tables are more common in the pre-training data corpus of LLMs, enabling better adaptation to this format during inference.\nDetailed Discussion. We further present some interesting observations. First, when analyzing results across various subcategories, the trend favoring Markdown persisted. Although some models demonstrated improved performance with CSV for certain subcategories and context lengths, the performance gap was generally small. In contrast, Markdown consistently provided a more substantial boost in accuracy for most models, reinforcing its advantage as a serialization format. Second, our analysis also uncovered that coder LLMs performed better with CSV than their original counterparts. This improvement can likely be attributed to the specific fine-tuning (SFT) processes for these models, which may include exposure to CSV tables as part of code-relevant tasks. However, even the Coder model showed a stronger overall preference for Markdown, underscoring its versatility and effectiveness across different use cases.\nConclusion. Given these results, we conclude that Markdown is the superior format for table serialization for LLMs in Multi-Table QA tasks. As a result, our subsequent experiments will use Markdown, ensuring optimal performance across a broad range of models and tasks."}, {"title": "4.2 Comprehensive LLM Evaluation Results", "content": "In this section, we present a broader evaluation results of LLMs using Markdown as the serialization format for tables. Results are shown in Table 6. he overall performance is illustrated in Figure 3. We enumerate our interesting observations below:\nChat LLM Performance. We find that chat-oriented LLMs generally perform poorly on our benchmark due to lack of instruction following ability. Chat Models except GLM4-9B-CHAT get lower than 25% overall accuracy. These LLMs struggle to properly follow the instructions provided in the prompt in Figure 2, despite the explicit clarification that there is only one correct choice and that the answer must adhere to a specific format. Instead, chat models often produce responses such as \u201cI don't know,\u201d \u201cNone of the above,\" or provide multiple answers in a format that deviates from the expected structure. This behavior likely stems from their design, which prioritizes conversational fluency over strict adherence to task-specific instructions. The GLM4-9B-CHAT performs slightly better since it can follow the instructions better. Interestingly, we also find that the overall accuracy of chat models tends to decline as their scale increases. A manual inspection using the \u201cairline\u201d database revealed distinct failure patterns for different models. For instance, larger versions of BAICHUAN2 frequently exhibited repetitive output (e.g., \u201c-338.166666666666 . . . \u201d repeated many times), whereas smaller versions, while often failing to follow instructions, still managed to produce readable and structured responses. For VICUNA series, smaller models occasionally generated multiple-choice answers (e.g., \u201cAnswer: C/D\u201d), where our regular expression matched one option"}, {"title": "4.3 Influence of Sampling and Symbolic Extension", "content": "To investigate the influence of symbolic extensions on model performance, we conducted a specific experiment using the \"airline\" database with a context length 8K. This experiment included all ten sampled database instances and 1400 generated questions. Each batch consisted of 14 questions, each generated from a unique question template. For each sampled database instance, we generated 10 such batches, allowing us to index each batch using a combination of the database instance index and question instance batch index. To analyze the results, we created a heatmap to illustrate the accuracy of each question instance batch for all models and a histogram to show the accuracy distribution across batches for each model. We also compared the average accuracy between three sets of batches: all batches from the \"airline\" database (all in airline), 5 batches previously used from \"airline,\" (5 in airline) and 5 batches previously used across all databases (5 in all). These visualizations are shown in Figure 5.\nDetailed Result Analysis. We summarize our detailed analysis of the influence of the symbolic extension. The heatmap results highlight that question difficulty varies across batches and models. For example, a batch in the lower-left corner of the heatmap appears easier for the QWEN model, which achieves high accuracy, while the same batch presents an average difficulty for other models, indicated by a consistent green color. This finding suggests that different models have distinct preferences or tendencies when handling certain question instances. Therefore, incorporating symbolic extensions enhances the stability of question instances, providing a more reliable assessment of model performance. Furthermore, the histogram shows a detailed distribution of the accuracy, which indicates that if the quantity of the generated question is small, we might evaluate model performance imprecisely. For instance, while GPT-4O-MINI performs well in most cases, it still struggles with certain batches. Conversely, although GLM-4 underperforms in most cases, it achieves high accuracy in some batches. These observations underscore the value of symbolic extensions in ensuring a more balanced and thorough evaluation of models. Lastly, we examined comparative lines across different test sets. All models, except GPT-4O-MINI, showed similar results between broad tests and sensitive tests in the \"airline\" database, indicating that using 5 batches in earlier studies was generally sufficient to represent the performance across all batches. Additionally, the comparison between all databases and the \u201cairline\u201d database revealed that the question templates in \"airline\" are generally more complex, posing greater challenges for models to handle effectively.\nConclusion. We believe that these experimental results suggest that the sampling mechanism combined with symbolic extension will generate reliable benchmark results. Notice that when the number of sampled database instances increases, it will lead to different symbolic extension executions in generating different questions. The alignment of the accuracy distribution illustrates the stable benchmark results when equipped with both our sampling mechanism and symbolic extensions. This further validates the significance of symbolic extensions in capturing diverse difficulty levels in model evaluations."}, {"title": "5 Related Work", "content": "Table QA. The task of question answering (QA) over relational databases has been a focal point in the fields of natural language processing and data management [1]. Given a user query, table QA aims to provide precise answers through table understanding and reasoning [8, 9, 10]. The technique to tackle table QA problems can be categorized into two classes based on whether the question in terms of natural language is transformed into a structured query (i.e., SQL). The first class leverages the technique of Text2SQL, which translates natural language queries into executable SQL statements, enabling users to interact with relational databases without manually programming [51, 52, 53, 54, 55, 9, 56]. The second approach uses an end-to-end machine learning model to process the question and serialized relational table to generate the answer [29, 37, 28, 27, 26]. For example, TABLE-BERT [6] utilizes a rule-based methodology to transform tabular data into coherent natural language sentences, thereby enabling the understanding of structured data in a format more amenable to human interpretation and further natural language processing tasks; TAPAS [57] enhances the original BERT architecture by incorporating a mechanism to encode tables as part of its input, allowing the model to directly interpret tabular data alongside textual queries, thereby improving its performance on table-based question answering tasks; PASTA [7] introduces a novel pre-training strategy involving six types of sentence-table cloze tasks, utilizing a synthesized corpus derived from 1.2 million entries in WikiTables to enhance the model's ability to integrate and reason with tabular data. MULTITABQA [8] employs a multi-table TQA approach, leveraging the single table TQA model tapex-base as its foundational model, which allows for the extension of table parsing capabilities to scenarios involving multiple related tables; recently, AutoTQA [10] uses multi-agent LLMs, involving five distinct roles: user, planner, engineer, executor, and critic to collaborate respond to user inquiries through conversations. Note that our proposed benchmark targets comprehensively evaluating the techniques falling into the second category with reliable results.\nAssessment of LLM over data management tasks. LLMs have demonstrated remarkable capabilities, offering an opportunity to build a new wave of innovative AI applications [58]. In the data management community, LLM has also reshaped the paradigm of design and implementation of many data management tasks [2, 3, 4, 5] including data integration [59, 60], database system tuning [61], query optimization [62], table summarization [63], table formatting [64] etc. Various domain-specific LLMs for tabular data have also been released. For example, TABLELLAMA [65] is fine-tuned on the TableInstruct dataset, which comprises 2.6 million table-based tasks to process 8 in-domain table-related tasks. TABLEGPT [66, 43] aims to unify tables, natural language, and commands into a single LLM to enable seamless interaction with tables, supporting functionalities such"}, {"title": "6 Conclusion", "content": "In this paper, we introduced TQA-Bench, a new Multi-Table QA benchmark specifically designed to rigorously evaluate the capabilities of LLMs in processing complex, relational data across multiple tables. Our benchmark applies diverse relational database instances drawn from real-world public datasets, a flexible sampling mechanism that allows for the creation of tasks with varying context lengths from 8K to 64K tokens, and the integration of symbolic extensions to test higher-order reasoning capabilities. These features ensure that TQA-Bench not only challenges the LLMs beyond simple data retrieval tasks but also provides a robust framework for assessing their ability to handle complex reasoning across multiple tables. Through systematic evaluations involving both open-source and closed-source LLMs, with scales ranging from 7 billion to 70 billion parameters, our findings highlight the variable performance of these models under complex Multi-Table QA scenarios. We expect that TQA-Bench can serve as a pivotal step toward realizing the full potential of LLMs in enhancing data-driven decision-making across various sectors."}]}