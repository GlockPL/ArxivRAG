{"title": "Frequency-Adaptive Low-Latency Object Detection Using Events and Frames", "authors": ["Haitian Zhang", "Xiangyuan Wang", "Chang Xu", "Xinya Wang", "Fang Xu", "Huai Yu", "Lei Yu", "Wen Yang"], "abstract": "Fusing Events and RGB images for object detection\nleverages the robustness of Event cameras in adverse en-\nvironments and the rich semantic information provided by\nRGB cameras. However, two critical mismatches: low-\nlatency Events vs. high-latency RGB frames; temporally\nsparse labels in training vs. continuous flow in infer-\nence, significantly hinder the high-frequency fusion-based\nobject detection. To address these challenges, we pro-\npose the Frequency-Adaptive Low-Latency Object Detector\n(FAOD). FAOD aligns low-frequency RGB frames with\nhigh-frequency Events through an Align Module, which re-\ninforces cross-modal style and spatial proximity to address\nthe Event-RGB Mismatch. We further propose a training\nstrategy, Time Shift, which enforces the module to align\nthe prediction from temporally shifted Event-RGB pairs and\ntheir original representation, that is, consistent with Event-\naligned annotations. This strategy enables the network to\nuse high-frequency Event data as the primary reference\nwhile treating low-frequency RGB images as supplementary\ninformation, retaining the low-latency nature of the Event\nstream toward high-frequency detection. Furthermore, we\nobserve that these corrected Event-RGB pairs demonstrate\nbetter generalization from low training frequency to higher\ninference frequencies compared to using Event data alone.\nExtensive experiments on the PKU-DAVIS-SOD and DSEC-\nDetection datasets demonstrate that our FAOD achieves\nSOTA performance. Specifically, in the PKU-DAVIS-SOD\nDataset, FAOD achieves 9.8 points improvement in terms of\nthe mAP in fully paired Event-RGB data with only a quarter\nof the parameters compared to SODFormer, and even main-\ntains robust performance (only a 3 points drop in mAP) un-\nder 80\u00d7 Event-RGB frequency mismatch.", "sections": [{"title": "1. Introduction", "content": "Event cameras [9], with their sub-millisecond temporal res-\nolution and high dynamic range (>120 dB), offer the po-\ntential to tackle challenges such as detecting fast-moving\nobjects and performing object detection in extreme light-\ning conditions [12, 28], i.e., overexposed and underexposed\nenvironments. However, compared to the RGB modality,\nEvent can only provide sparse pulse information and lacks\nabsolute intensity, resulting in inferior semantic informa-\ntion. Therefore, fusing both modalities provides the poten-\ntial to simultaneously leverage the merits from both sides\n[3, 19, 20, 42] to achieve higher accuracy and robustness.\nDespite this potential, the fusion of Event and RGB can\ncome at the sack of high temporal resolution [3, 19], a defin-\ning feature of Event stream [12, 27]. Two critical chal-\nlenges impeding the high-frequency fusion-based object de-\ntection can be summarized as Event-RGB Mismatch and\nTrain-Infer Mismatch, as explained in Figure 1. Event-RGB\nMismatch results from the different sampling frequencies of\nEvent and RGB cameras [19, 44], where the former can cap-\nture over 10,000 frames (dense Event representation) per\nsecond while the latter typically can only capture around\n60 frames [9] per second. On the other hand, the Train-\nInfer Mismatch, first highlighted in work [47], refers to the\ninference frequency exceeding the training frequency, lead-\ning to a severe performance degradation [12]. Because of\nthe substantial cost of acquiring high-frequency annotated\ndata, models are typically trained on low-frequency anno-\ntations. This necessitates that models generalize effectively\nfrom low-frequency training to high-frequency inference.\nTo address all the challenges mentioned above, while\nmaintaining high detection performance and fast inference\nspeed, we design a robust Event-RGB fusion detection\nframework FAOD. To address the Event-RGB Mismatch,\nwe propose an Align Module that rectifies the RGB frames,\nensuring the style and spatial proximity between RGB fea-\ntures and Event features. To activate the offset generator\nin the Align Module, we design a training strategy called\nTime Shift, which simulates the misalignment caused by\nfrequency discrepancies between the two modalities and en-\nforces consistency between the outputs with and without the\nintroduced offsets, to ensure that the module is adequately\ntrained. This training strategy introduces a shift between the\nRGB data and the Event data aligned with the annotations.\nAs a result, the model prioritizes high-frequency Event data\nwhile using the rectified low-frequency RGB data as a sup-\nplement, maximizing the low-latency advantage of Event\ndata for high-frequency detection. More importantly, we\nfind that these corrected Event-RGB data pairs, compared\nto standalone Event data, enable generalization from lower\ntraining frequency to higher inference frequencies, regard-\nless of whether RNNs [30] or SSMs [31] are used as mem-\nory networks. This benefit arises from the complementary"}, {"title": "2. Related Work", "content": "Existing Event-based object detection methods [8, 12, 23,\n25, 28, 29, 43] can be divided into two categories. The first\ndirectly inputs Event points, commonly employing Graph\nNeural Networks (GNN) [23, 29] or Spiking Neural Net-\nworks (SNN) [4, 8, 18]. The second approach involves\nconverting sparse Event points into dense Event frames"}, {"title": "2.1. Object Detection for Event Cameras", "content": "Existing Event-based object detection methods [8, 12, 23,\n25, 28, 29, 43] can be divided into two categories. The first\ndirectly inputs Event points, commonly employing Graph\nNeural Networks (GNN) [23, 29] or Spiking Neural Net-\nworks (SNN) [4, 8, 18]. The second approach involves\nconverting sparse Event points into dense Event frames\n[12, 25, 28, 47] before feeding them into dense neural\nnetworks. However, those methods based on GNNs or\nSNNs [4, 8, 23, 29] still lag behind CNN-based methods\n[12, 21, 27], and some of them rely on specific hardware\nsupport [8, 13, 34]. Therefore, compared to directly pro-\ncessing sparse Event points, most methods tend to convert\nEvent points into dense frames in object detection.\nTo address the specific challenges posed by Event char-\nacteristics, existing methods [12, 21, 27, 28, 47] have incor-\nporated tailored designs. To handle information loss caused\nby responding only to brightness changes of the Event cam-\nera, [12, 21, 28] integrate memory networks [15, 30] to pro-\nvide supplementary temporal information. To accommo-\ndate the high temporal resolution of Event cameras, [12, 25]\nemploy simplified architectures or linear-complexity trans-\nformers for efficient processing. To utilize the sparsity\nof the Events, [27] adopts adaptive token sampling during\nattention calculation to reduce computational complexity.\nMoreover, [47] introduces the SSM [31] to enhance the ro-\nbustness of models under varying input frequencies. These\nspecialized designs have significantly advanced the field of\nEvent-based object detection.\nHowever, due to the limited semantic information pro-\nvided by Event cameras, detection performance using\nEvent-based methods still significantly lags behind RGB-\nbased approaches [3, 19, 22], even with the strategies above.\nAdditionally, the finite memory length of memory networks\n[15, 30] introduces challenges such as memory decay dur-\ning prolonged static periods and performance degradation\nwhen the network works at a frequency higher than the\ntraining frequency. These issues hinder the capabilities of\nEvent cameras in high-speed scenarios, even when utilizing\nthe SSM [31, 47], a memory network with longer mem-\nory length. Therefore, a fusion-based approach [3, 19] that\ncombines Event and RGB data is the optimal choice since\nRGB data can compensate for the lack of semantic informa-\ntion in Events and alleviate the burden of memory networks\nto achieve frequency generalization."}, {"title": "2.2. Event-RGB Fusion for Object Detection", "content": "Most research in Event-RGB fusion for object detection\naims to develop effective fusion strategies for the Event and\nRGB features [3, 5, 17, 19, 35, 40, 45], motivated by the\ncomplementary nature of these data modalities, i.e., Event\ndata excels in extreme conditions and RGB data offers rich\nsemantic information in typical scenes. A common ap-\nproach in early studies is to adopt late fusion for combin-\ning Event and RGB data. Specifically, [5] employs non-\nmaximum suppression (NMS) to merge detection results,\nwhile [17] chooses to fuse confidence maps derived from\nthe two modalities. More recent works [3, 19, 35, 45] tend\nto explore the integration of features at the middle layers of\nthe network, motivated by the desire to better leverage the\ncomplementary nature of different modalities while also re-\nducing computational costs. For instance, [45] improves\nthe convolutional attention module proposed in [40] to en-\nhance the shared features between Event and RGB data,\nwhile [3, 19] utilize attention mechanisms to achieve more\neffective feature fusion.\nUnlike most previous studies [3, 19, 45], our FAOD fo-\ncuses on achieving fusion between Event and RGB data\nacross varying frequencies, tackling challenges arising from\nthe Event-RGB Mismatch and the Train-Infer Mismatch.\nThis approach, which combines Event and RGB data, al-\nlows the system to maintain high performance at very high\nframe rates, similar to Event-based systems, but with sig-\nnificantly better results than Event-based methods. Addi-\ntionally, it outperforms both Event-based and RGB-based\ndetection approaches by a considerable margin."}, {"title": "3. Method", "content": "3.1. Preliminary\nEvent Representation. Since Event cameras capture infor-\nmation asynchronously from individual pixels rather than\ncapturing the entire picture at a specific timestamp, it is nec-\nessary to convert the Event stream into dense Event frames\nbefore feeding them into networks [1, 2, 37, 46]. In this\nwork, we use a simple Event representation as used in [12]\nfor all of the comparison methods to ensure a fair evalua-\ntion. Specifically, given a set of Event points \u03a6 occurring\nwithin a time interval from ta to to, where each Event point\nek is represented by its coordinates (xk, Yk), timestamp tk,\nand polarity pk, i.e., ek = (Xk, Yk, Pk, tk), we can obtain an\nEvent representation E using the following formula,\n$E(x,y,p,t) = \\sum_{\\epsilon_k \\in \\Phi} \\delta(p_k - p)\\delta(x - x_k, y - Y_k)\\delta(t - T_k)$,\n$T_k = \\frac{t_k-t_a}{t_b-t_a}\u00b7T]$,\n(1)\nwhere \u03b4 indicates dirac function, p represents the polarity\n(positive or negative), and T denotes the number of discrete\nbins of E.\nSpatial Feature Extraction. We employ CSPDarkNet\n[38], a widely adopted backbone in the YOLO series [10],\nto extract spatial features from both Event and RGB data\nsince it offers a compelling balance between accuracy and\nspeed. We also do ablation studies on other spatial feature\nextractors such as ResNet [14], MaxVit [36], and Swins [26]\nfor comparison in the supplementary material."}, {"title": "3.2. Main Architecture", "content": "Fig. 3 illustrates the overall architecture of FAOD. The\nmodel processes Event frames at time step i and RGB im-\nages at time step j, with i \u2265 j to adapt the higher frame"}, {"title": "3.3. Feature Alignment for Event-RGB Mismatch", "content": "The higher frame rate of Event cameras compared to RGB\ncameras often leads to temporal and spatial misalignment\nwhen fusing, i.e., Event-RGB Mismatch. To maintain the\nhigh temporal resolution of Event cameras, RGB images\nmust be warped to align with the Event frames. We use\nthe deformable convolution [6] to address the modality mis-\nalignment problem, as the Align Module shown in Fig. 3(c).\nIt consists of two main components: an Adaptive Instance\nNormalization (AdaIN) module [16] for style transfer and a\ndeformable convolution for alignment. AdaIN is employed\nto transfer the style of RGB images to Event frames, miti-\ngating the impact of modality discrepancies on alignment.\nSpecifically, AdaIN achieves style transfer by modulating\nthe mean and variance of RGB image features as follows:\n$AdaIN(\u0399', \u0395') = \u03c3(\u0395')\\frac{\u0399_i \u2013 \u03bc(\u0399_i)}{\u03c3(\u0399_i)}+ \u03bc(\u0395')$, (2)\nwhere I and E represent the features of the RGB image\nand Event frame, respectively, while \u03c3 and \u03bc denote the\nstandard deviation and mean. Following AdaIN adjustment,\nthe features extracted from the Event and RGB images are\nconcatenated and fed into a convolution layer to produce\noffsets, which will guide the deformable convolution for\nrectifying the RGB images.\nRelying solely on the Align Module is insufficient for\nadaptively correcting RGB images, since the Event and\nRGB data in the dataset are at the same fixed frequency\nas the annotations, leading to a lack of supervision for the\ndeformable convolution in the Align Module. Instead of\ndesigning separate loss functions [39] or adding extra mod-\nules to guide the training of the Align Module, we propose\na training strategy to achieve that. Specifically, we shift\nthe RGB images in paired Event-RGB data during the in-\nput stage to simulate misalignment between the Event and\nRGB data. We randomly move the RGB frames forward by\nseveral time units, as Event frames typically fuse with for-\nward RGB frames due to their higher frequency. We refer\nto this training strategy as Time Shift which can be formu-\nlated as:\n$M(E(t), I(t \u2212 \u2206t)) = M(E(t), I(t))$,\n$At \u2208 [At_{min}, At_{max}]$,\n(3)\nwhere M() represents the output of the model, and\nAt denotes a random temporal offset within the range"}, {"title": "3.4. Frequency Adaptation for Train-Infer Mismatch", "content": "Event-based detection [12, 21, 27, 28, 47] systems expe-\nrience a significant performance drop when encountering\ndata with frequencies higher than those seen during train-\ning, known as Train-Infer Mismatch, primarily due to the\nlimited memory length and fixed-frequency adaptation of\nmemory networks. VIT-S5 [47] first highlighted this is-\nsue and proposed using SSMs [31, 47] instead of RNNs to\nenable generalization from low to high frequencies. How-\never, we observe that within the same frequency varia-\ntion from training to inference, the combination of Event\ninformation with corrected RGB data experiences signif-\nicantly less performance degradation compared to stan-\ndalone Event data. This suggests that FAOD facilitates gen-\neralization from lower training frequencies to higher infer-\nence frequencies. This is because, unlike pure Event-based\nmethods, fusion-based approaches leverage complementary\nRGB data, which can considerably alleviate the burden on\nmemory networks.\nSimply incorporating RGB data into the model is insuf-\nficient. Additional strategies are required to prioritize Event\ninformation over RGB since generalizing from low to high\nfrequencies primarily involves increasing the frequency of\nEvent data, rather than RGB data. However, during training,\nthe model tends to favor RGB data due to its richer semantic\ncontent, neglecting the high-frequency characteristic of the\nEvent. We also address this issue through the application\nof Time Shift. In the training, the strategy introduces off-\nsets between RGB images and annotations to encourage the\nnetwork to align RGB features to Event features which syn-\nchronizes with the annotations. Consequently, during in-\nference, the model can concentrate on utilizing both Events\nand aligned RGB features, rather than relying on RGB im-\nages from previous timesteps, which can maintain the low-\nlatency nature of the Event stream toward high-frequency\ndetection.\nIt is worth noting that while SSMs offer superior gen-\neralizability and longer memory compared to RNNs, their\nmemory capacity is still limited. This limitation becomes\napparent in pure Event-based detection tasks involving\nhigh-frequency Events. In contrast, Event-RGB fusion\nmethods can effectively overcome this challenge by utiliz-\ning complementary information from RGB data, maintain-\ning a better and consistent performance across varying fre-\nquencies. Therefore, we identify Event-RGB fusion as a\nmore robust approach for high-frequency object detection."}, {"title": "4. Experiments", "content": "Datasets We use PKU-DAVIS-SOD [19] and DSEC-\nDetection [11] as our experimental datasets. Considering\nthe larger dataset size, manual annotations, and higher an-\nnotation frequency of PKU-DAVIS-SOD, we choose it to\nperform experiments addressing the Event-RGB Mismatch,\nTrain-Inference Mismatch, and various ablation studies.\nPKU-DAVIS-SOD [19], captured by the Davis346 cam-\nera at a resolution of 346 \u00d7 260 pixels, offers perfectly\naligned Event and RGB data. The dataset contains 1080.1k\nmanually annotated bounding boxes at a rate of 25Hz, en-\ncompassing three object categories (cars, pedestrians, and\ntwo-wheelers) and Three different scene conditions (nor-\nmal, motion blur, and low-light).\nDSEC-Detection [11], acquired by the Gen3 Prophesee\nThe Event camera has a resolution of 640 x 480 and in-\ncludes 8 categories with 39.0k annotated bounding boxes.\nThere exist other versions of annotations for this dataset, but\nthey only cover a subset of sequences, e.g., 41 sequences in\n35 and 51 sequences in [22] with fewer bounding boxes.\nThis work utilizes the more comprehensive annotations pro-\nvided by [11].\nImplementation Details We train our models using 32-\nbit precision for 400k iterations, employing the ADAM op-\ntimizer [7] and a OneCycle learning rate schedule [32].\nTo enhance training efficiency, we adopt a mixed batch-\ning strategy, alternating between standard Backpropagation\nThrough Time (BPTT) and Truncated BPTT (TBPTT) as\nproposed in RVT [12]. We use a batch size of 4 and a maxi-\nmum learning rate of 1.5e-4. Both datasets are trained on\na 40G A100 GPU for approximately 3 days. Note that\nwe use the complete 3 categories on the PKU-DEVIS-\nSOD dataset and 8 categories on the PKU-DEVIS-SOD\ndataset during both training and test. To augment our\ndataset, we apply random horizontal flipping, and zooming,\nand use Time Shift for misaligned Event-RGB pairs testing.\nWe adopt mean Average Precision (mAP) [24] as our pri-\nmary evaluation metric. Additionally, to assess model effi-"}, {"title": "4.1. Setup", "content": "Datasets We use PKU-DAVIS-SOD [19] and DSEC-\nDetection [11] as our experimental datasets. Considering\nthe larger dataset size, manual annotations, and higher an-\nnotation frequency of PKU-DAVIS-SOD, we choose it to\nperform experiments addressing the Event-RGB Mismatch,\nTrain-Inference Mismatch, and various ablation studies.\nPKU-DAVIS-SOD [19], captured by the Davis346 cam-\nera at a resolution of 346 \u00d7 260 pixels, offers perfectly\naligned Event and RGB data. The dataset contains 1080.1k\nmanually annotated bounding boxes at a rate of 25Hz, en-\ncompassing three object categories (cars, pedestrians, and\ntwo-wheelers) and Three different scene conditions (nor-\nmal, motion blur, and low-light).\nDSEC-Detection [11], acquired by the Gen3 Prophesee\nThe Event camera has a resolution of 640 x 480 and in-\ncludes 8 categories with 39.0k annotated bounding boxes.\nThere exist other versions of annotations for this dataset, but\nthey only cover a subset of sequences, e.g., 41 sequences in\n35 and 51 sequences in [22] with fewer bounding boxes.\nThis work utilizes the more comprehensive annotations pro-\nvided by [11].\nImplementation Details We train our models using 32-\nbit precision for 400k iterations, employing the ADAM op-\ntimizer [7] and a OneCycle learning rate schedule [32].\nTo enhance training efficiency, we adopt a mixed batch-\ning strategy, alternating between standard Backpropagation\nThrough Time (BPTT) and Truncated BPTT (TBPTT) as\nproposed in RVT [12]. We use a batch size of 4 and a maxi-\nmum learning rate of 1.5e-4. Both datasets are trained on\na 40G A100 GPU for approximately 3 days. Note that\nwe use the complete 3 categories on the PKU-DEVIS-\nSOD dataset and 8 categories on the PKU-DEVIS-SOD\ndataset during both training and test. To augment our\ndataset, we apply random horizontal flipping, and zooming,\nand use Time Shift for misaligned Event-RGB pairs testing.\nWe adopt mean Average Precision (mAP) [24] as our pri-\nmary evaluation metric. Additionally, to assess model effi-"}, {"title": "4.2. Benchmark Comparisons", "content": "In this section, we comprehensively compare FAOD against\nSOTA methods [12, 19, 26] on the PKU-DAVIS-SOD and\nDSEC-Detection datasets. Our experiments are catego-\nrized into three parts: (1) experiments on perfectly aligned\nEvent-RGB data without Event-RGB Mismatch and Train-\nInfer Mismatch, (2) experiments under Event-RGB Mis-\nmatch conditions, and (3) experiments under Train-Infer\nMismatch conditions. To ensure a thorough comparison,\nwe include recent SOTA methods for Event-based object\ndetection (e.g., RVT [12]), RGB-based tasks (e.g., Swin\nTransformers [26], MaxViT [36]), and fusion-based object\ndetection (e.g., SODformer [19]). Furthermore, we intro-\nduce novel baselines by combining advanced feature extrac-\ntors from other domains using Event and RGB (e.g., EFNet\n[33] for deblurring, AFNet [44] for tracking, FF-KDT [39]\nfor feature point extraction) with lightweight YOLOX [10]\nFPN and Head. Comprehensive evaluations include mAP,\nAP50, inference time, and model parameters."}, {"title": "4.2.1 Experiments under Paired Event-RGB", "content": "This experimental paradigm is commonly used in existing\nstudies [3, 19, 45], serving as a benchmark for evaluat-\ning the efficacy of feature extraction and fusion strategies\nfor these two modalities. In this setting, Event and RGB\ndata are temporally and spatially aligned during training and"}, {"title": "4.2.2 Experiments under Event-RGB Mismatch", "content": "In this setting, the Event data has a higher frequency than\nthe RGB data, resulting in spatial and temporal misalign-\nment. For instance, if the Event frames are constructed at 25\nHz and the RGB camera at 5 Hz, each RGB frame will cor-\nrespond to the subsequent five timestamps of Event frames.\nThis experiment evaluates the model's ability to handle un-\npaired Event-RGB data."}, {"title": "4.2.3 Experiments under Train-Infer Mismatch", "content": "This setting provides a fixed frequency of RGB images (for\nfusion-based methods) and higher frequencies for Events\nthan the frequency in the training. To maintain perfor-\nmance under this scenario, the model needs the ability to\nrectify RGB images to Events, given that the frequency of\nthe Event frames is always higher than that of RGB images,\nbut also requires the memory network to have generalizabil-\nity from low to high frequencies. Notably, we do not use 25\nHz RGB for evaluation but use 5 Hz and 2.5 Hz instead\nto introduce a certain degree of offset between the RGB im-\nages and the labels, thus preventing methods from achieving\nhigh mAP by overfitting to the RGB images."}, {"title": "4.3. Ablation Study", "content": "This section analyzes the influence of the proposed mod-\nules and strategies on the model's performance under three\nscenarios: paired Event-RGB data condition, Event-RGB\nMismatch, and Train-Infer Mismatch. Specifically, we in-\nvestigate (1) the impact of fusion modules and memory\nnetworks on the model's performance in the paired Event-\nRGB scenario; (2) the effect of the Align Module and Time\nShift training strategy on the model's performance in the\nunpaired Event-RGB scenario; and (3) the impact of differ-\nent memory networks and Time Shift on the model's gen-\neralizability from low to high frequencies. We perform all\nthe ablation studies on PKU-DAVIS-SOD [19] dataset and\nusing the same setting in Section 4.2."}, {"title": "4.3.1 Fusion Module and Memory Network", "content": "EF Fusion Module By comparing the proposed EF Fusion\nModule with the direct concatenation of features, we find\nthat our module can achieve a 0.5 mAP and 1.7 AP50 im-\nprovement with only a negligible increase of 0.1M model\nparameters"}, {"title": "4.3.2 Ablation of RGB Alignment", "content": "Table 6 illustrates the impact of the Align Module and Time\nShift on achieving Event-RGB alignment. The results indi-\ncate that Time Shift is crucial for enabling the model to per-\nform RGB rectification, as it alone can sustain performance\nin unpaired Event-RGB scenarios. In contrast, the Align\nModule is ineffective when used independently due to the\nlack of supervision. However, when using the Align Mod-\nule combined with Time Shift, the model's performance can\nimprove further than when using Time Shift alone. This\ndemonstrates that, although the model can learn to use mis-\naligned RGB images through Time Shift without the Align\nModule, the incorporation of the Align Module allows for\nmore effective utilization of RGB information, leading to\nimproved performance."}, {"title": "4.3.3 Ablation of Memory Blocks", "content": "This ablation investigates the impact of different memory\nnetworks and Time Shift on the model's ability to gener-"}, {"title": "5. Conclusion", "content": "To enable high-speed object detection by integrating both\nEvents and RGB frames, we propose a novel fusion de-\ntection model named FAOD, to address two critical mis-\nmatches, i.e., Event-RGB Mismatch and Train-Infer Mis-\nmatch. By employing the Align Module and Time Shift\ntraining strategy, FAOD enables RGB rectification even\nwhen only paired Event-RGB data is available for train-\ning. Moreover, we find that the data composed of high-\nfrequency Events and wrapped low-frequency RGB im-\nages used in FAOD, demonstrates improved generalizability\nfrom low to high frequencies compared to the standalone\nEvent data. This suggests that the approaches based on\nfusion are better suited for high-speed detection scenarios\nthan those relying solely on Events. In addition, thanks to\nthe shallow feature fusion structure and effective modules,\nFAOD achieves low-latency inference while maintaining\nhigh detection performance. Experimental results demon-\nstrate that FAOD can not only achieve SOTA performance at\nfixed training frequency with effectively balanced accuracy,\nspeed, and parameter efficiency, but also can be utilized to\nfuse Event and RGB data at arbitrary frequencies."}, {"title": "6. Shallow Fusion vs. Deep Fusion", "content": "This section analyzes the advantages of shallow feature\nfusion over deep feature fusion architecture. We extend the\nshallow feature fusion architecture that applies to the first\nstage to a deep feature fusion architecture that applies to all\nstages,\nas shown in Figure 4. Unlike the shallow feature fu-\nsion architecture, the deep feature fusion architecture feeds\nboth the Event and RGB modalities, as well as the fused\nfeatures, into all three later stages. Specifically, in the deep\nfeature fusion architecture, the fused features are concate-\nnated with the features from the previous stage. To ensure\nconsistent scales, the features of the prior stage undergo a\nmax-pooling operation. Moreover, in the deep feature fu-\nsion architecture, the feature extractors for both the Event\nand RGB modalities are completely independent, with each\nstage including an Align Module and an EF Fusion mod-\nule, theoretically enabling more comprehensive utilization\nof both types of features.\nAs shown in Table 8, we compare the two fusion types\nin terms of mAP, AP50, AP75, model parameters, and in-\nference time on a single V100 GPU. The results indicate\nthe deep feature fusion architecture exhibits a noticeable de-\ncline in mAP, AP50, and AP75, with drops of 1.3, 3.0, and\n1.2 points, respectively. These results indicate that deep fea-\nture fusion does not offer advantages for accuracy. Further-\nmore, the deep feature fusion architecture has a significantly\nlarger model size of 55.4M parameters, more than double\nthat of the shallow feature fusion architecture, and its infer-\nence speed is halved. Considering these observations, we\nultimately opt for the shallow feature fusion architecture."}, {"title": "7. The Supervision of Align Module", "content": "This section investigates whether providing direct super-\nvision to the Align Module is necessary. Under the Time\nShift training strategy condition, the Event and RGB data\nare misaligned. However, since we have access to the en-\ntire sequence during training, we can obtain RGB data that\nis perfectly aligned with the Event data. Consequently,\nwe can directly supervise the Align Module by calculating\nthe Mean Squared Error (MSE) loss between the corrected\nRGB data and the perfectly aligned RGB data, as illustrated\nin Figure 5. The loss function of the Align Module is:\n$LAM = MSE(AM(Funpaired), Fpaired)$, (4)\nwhere AM indicates the Align Module, Funpaired refers\nto the RGB feature with a temporal offset relative to the\nEvent feature, while Fpaired denotes the RGB feature that\nis temporally aligned with the Event feature.\nAs shown in Table 9, the experimental results indicate\nthat adding Aligned Loss does not improve model perfor-\nmance but leads to a slight degradation. This suggests\nthat forcibly correcting misaligned images to match images"}, {"title": "8. Ablation of Spatial Feature Extractor", "content": "In this chapter, we evaluate the impact of various feature\nextractors on model performance. Specifically, we com-\npare CNN-based architectures such as CSPDarknet [38]\nand ResNet [14], as well as Transformer-based models like\nMaxViT [36] and Swins Transformer [26]. To ensure a fair"}, {"title": "9. Visualization Results", "content": "This section presents the visual results, including the per-\nformance of different methods under Event-RGB Mismatch\n(as shown in Figure 6) and the generalizability of pure\nEvent methods and fusion-based approaches under Train-\nInfer Mismatch (as shown in Figure 7).\nVisualization under Event-RGB Mismatch. Figure 7 vi-\nsualizes the localization results, revealing that AFNet [44],\nReNet [45], and FF-KDT [39] predominantly place bound-\ning boxes on lagged RGB frames rather than on the higher-\nfrequency Event data. In contrast, FAOD is primarily driven\nby Event data, with bounding boxes aligning well with the\ncorresponding Events. Furthermore, we observe that these\nfusion-based methods do not strictly adhere to RGB data but\ntend to select the higher-quality modality. While this mech-\nanism is effective in many scenarios, such as fusing RGB\nand infrared images [41] for day-night complementary, it is\nunsuitable for Event-RGB data with disparate frequencies.\nIn such cases, focusing primarily on Event data is essential\nfor achieving high-frequency object detection.\nVisualization under Train-Infer Mismatch. Figure 7\npresents the visualization of RVT [12], ViT-S5 [47], and\nFAOD, all equipped with memory networks, on Event data\nwith varying frequencies. It is evident that both RVT and\nViT-S5 experience increased false positives and missed de-\ntections as the Event frequency rises, with ViT-S5 showing\nrelatively better performance. In contrast, FAOD maintains\na stable output across different frequencies. This highlights\nthe superior generalizability of FAOD from low to high fre-\nquencies compared to Event-only methods, suggesting its\nsuitability for high-frequency object detection."}, {"title": "10. Questions & Answers", "content": "Q1: Why using a series of misaligned datasets created\nby Time shift for training allows the model to handle\nEvent-RGB Mismatch?\nA: The challenge of the Event-RGB Mismatch lies in the\ntemporal and spatial misalignment between Event and RGB\nframes. Event and RGB data are typically perfectly aligned\nin the previous training paradigm. As a result, the model\ndoes activate the Align Module to correct RGB data during\ntraining, which subsequently renders the Align Module inef-\nfective at rectifying misaligned RGB data during inference.\nThe essence of the proposed"}]}