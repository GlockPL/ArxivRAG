{"title": "Statistical Analysis of the Impact of Quaternion Components in Convolutional Neural Networks", "authors": ["Gerardo Altamirano-G\u00f3mez", "Carlos Gershenson"], "abstract": "In recent years, several models using Quaternion-Valued Convolutional Neural Networks (QCNNs) for different prob- lems have been proposed. Although the definition of the quaternion convolution layer is the same, there are different adaptations of other atomic components to the quaternion domain, e.g., pooling layers, activation functions, fully connected layers, etc. However, the effect of selecting a specific type of these components and the way in which their interactions affect the performance of the model still unclear. Understanding the impact of these choices on model performance is vital for effectively utilizing QCNNs. This paper presents a statistical analysis carried out on exper- imental data to compare the performance of existing components for the image classification problem. In addition, we introduce a novel Fully Quaternion ReLU activation function, which exploits the unique properties of quaternion algebra to improve model performance.", "sections": [{"title": "1. Introduction", "content": "Among different types of artificial neural networks, those using a combination of convolution and pooling layers have achieved the best performance for image classification tasks [1, 2, 3, 4, 5]. A convolution layer is a variation of a perceptron neuron, which applies a weight-sharing technique, similar to the receptive fields discovered by Hubel and Wiesel [6, 7]. This layer, in combination with a pooling layer, produces an in- variant signature to a group of geometric transforma- tions [8, 9, 10], e.g. small translations or rotation of the input images [11, 12]. Some of the main problems in designing deep mod- els of CNNs are: reducing the number of parame- ters without losing generalization, and the vanishing and exploding gradient problems when training the net- work [13, 14]. Fundamental research and experimental analysis have shown that some algebraic systems, such as complex and hyper-complex numbers, have the po- tential to solve these problems [15, 16, 17, 18]. Thus, in recent years, several convolutional neural network mod- els using a quaternion representation (QCNNs), instead of the real number representation, have been proposed, see for example [19, 20, 21, 22, 23, 24]. These models have shown that they can achieve sim- ilar or better results than their real-valued counterparts. However, the atomic components of each of the quater- nion models differ, e.g. activation functions, initializa- tion algorithm, pooling method, etc. This makes ex- perimental data from previous works unsuitable to draw conclusions about the effect of each individual compo- nent. In addition, the models are tested over problems of different domains, e.g. image classification [20, 22], ar- tificial image generation [23, 24], natural language pro- cessing [25], etc. and have used different datasets. The main contributions of this work are as follows: \u2022 We present an n-way ANOVA test carried out on experimental data for comparison of the different components of QCNNs for the image classification problem. We selected four factors to test: the type of activation function (Fully Quaternion ReLU or Split Quaternion ReLU function [20, 26, 27]), the type of fully connected layer (Quaternion Fully Connected layer [19] or Quaternion Inner Prod-"}, {"title": "2. Methods", "content": "2.1. Quaternion algebra In mid-XIX century, W.R. Hamilton (1805-1865) in- troduced the concept of quaternion, which he defined as the ratio between two vectors [31, 32, 33]. From this definition, he obtained the quadrinomial form of a quaternion [32]:\n$q = qr + q_i i + q_j j + q_k k,$\nwhere $qr, q_i, q_j, q_k$ are scalars, and $i, j, k$ are imaginary bases, i.e. $i^2 = j^2 = k^2 = -1$.\nIn terms of modern mathematics, the quaternion alge- bra, H, is: the 4-dimensional vector space over the field of the real numbers, generated by the basis {1, i, j, k}, and endowed with the following multiplication rules:\n(1)(1) = 1\n(1)(i) = jk = \u2212kj = i\n(1)(j) = ki = \u2212ik = j\n(1)(k) = ij = \u2212ji = k\n$i^2 = j^2 = k^2 = \u2212 1$\n(2)\nnon-\n(1)\nFinally, a useful operation when working with quater- nions is the conjugate. Let, $q = qr + q_i i + q_j j + q_k k$, be a quaternion, its conjugate, $q$, is defined as follows:\n$\\bar{q} = qr \u2013 q_i i \u2013 q_j j \u2013 q_k k.$\n2.2. QCNNs components The first step when working with QCCNs is to map the data to the quaternion domain. For a dataset contain- ing images, an RGB image is mapped to the quaternion domain by encoding the red, green, and blue channels into the imaginary parts of the quaternion:\n$q = 0 + R i + G j + B k,$\n(4)\n(3)\nIn contrast, for grayscale images, the grayscale values are mapped to the real part of the quaternion, and the imaginary components are set to zero.\nNext, we introduce the main atomic components for designing QCNNs architectures.\n2.2.1. Quaternion convolution layers Each sample, denoted by Q, is represented as an N \u00d7 M matrix where each element is a quaternion:\n$Q = [q(x, y)] \u2208 H^{N\u00d7M};$\n(5)\nin addition, Q can be decomposed in its real and imagi- nary components:\n$Q = QR + Q_i i + Q_j j + Q_k k$\n(6)\nwhere $QR, Q_i, Q_j, Q_k \u2208 R^{N\u00d7M}$, and i, j, k represent the complex basis of the quaternion algebra.\nIn the same way, a convolution kernel of size L \u00d7 L is represented by a quaternion matrix, as follows:\n$W = [w(x, y)] \u2208 H^{L\u00d7L}$\n(7)\nwhich can be decomposed as:\n$W = WR + W_i i + W_j j + W_k k,$\n(8)\nwhere $WR, W_i, W_j, W_k \u2208 R^{L\u00d7L}$, and i, j, k represent the basis of the quaternion algebra.\nUsing the left-sided definition of discrete quaternion convolution, the convolution layer is defined as follows:\n$F = W * Q,$\n(9)\nwhere F \u2208 $H^{(N-L+1)\u00d7(M-L+1)}$ represents the output of the layer, i.e. a quaternion feature map, and each element of the tensor is computed as follows [34, 35, 36]:\n$f(x, y) = (w * q)(x, y). = \\sum_{r=-}^{} \\sum_{s=-} [w(r, s)q(x - r, y \u2013 s)].$"}, {"title": "2.2.2. Pooling layers", "content": "In addition, for intermediate layers with more than four channels, the input data is divided into 4-channel sub-inputs. Each sub-input is convoluted with a differ- ent quaternion kernel to produce a partial output, and the final quaternion feature map, is computed by summing all the partial outputs. This is stated formally as follows: Let X \u2208 RN\u00d7M\u00d7C be an input data, N is the number of rows, M the number of columns, and C is the number of channels, where C%4 = 0, then X is partitioned as follows:\n$X = [Q0, Q1,..., Q_{(C/4)-1}]$\n(11)\nwhere each Qs \u2208 $H^{N\u00d7M}, 0 < s < (C/4) \u2013 1$ is a quater- nion input channel.\nLet V \u2208 RL\u00d7L\u00d7K, with K%4 = 0, be the convolution kernel, then:\n$V = [Wo, W1,..., W _{(K/4)-1}]$\n(12)\nwhere each Ws \u2208 HL\u00d7L, 0 < s < (K/4) \u2013 1 is a quater- nion kernel channel.\nThen, each partial output is computed as follows:\n$Fs = Ws * Qs,$\n(13)\nwhere 0 < s < (C/4)-1, and the final quaternion feature map, F \u2208 HN\u00d7M, is obtained by summing all outputs:\n$F = \\sum_{s=0}^{(C/4)-1} F_s.$\n(14)\nA pooling layer introduces a sort of invariance to ge- ometric transformations, such as small translations and rotations. In this work, it is applied a channel-wise aver- age layer [37, 38, 22] as follows: First, we select a win- dow from the input image, denoted by Q \u2208 $H^{L_1\u00d7L_2}$; then, the pooling procedure is applied on this sub-image:\n$S_{QAvePool}(Q) = \\frac{1}{L_1L_2}\\sum_{i=1}^{L_1} \\sum_{j=1}^{L_2} q(i, j)$\n(15)\nThis process is repeated over the whole image by mov- ing the window mask from point to point in the input image."}, {"title": "2.2.3. Activation functions", "content": "The role of an activation function is to simulate the triggering behavior of a biological neuron. From the computational perspective, non-linear activation func- tions are required for constructing a universal interpola- tor of a continuous quaternion valued function [39]. In addition, it is desirable that the activation function were analytic so that gradient descendant techniques can be applied in the training stage. However, the non-analytic condition can only be satisfied by some linear and con- stant functions [40, 41]. A typical way to circumvent this problem is the use of quaternion splits functions, i.e. a mapping f: H\u2192 H, such that:\n$f(q) = fr(q) + fi(q)i + fj(q)j + fk(q)k,$\n(16)\nwhere $q \u2208 H$, fr, fi, fj, and fk are mappings over the real numbers: f: R\u2192R. Thus, the Split Quater- nion ReLU activation function [20, 26, 27] is defined as follows:\n$S QReLU(q) = ReLU(qr) + ReLU(q_i)i +$\n$ReLU(q_j)j + ReLU(q_k)k,$\n(17)\nwhere ReLU: RR is the real-valued ReLU func- tion [42, 43]:\n(18)\n$ReLU(x) = max(0, x).$\n(19)\nA more general approach is the use of Fully Quater- nion Functions, i.e. a mapping f: H \u2192 H. Based on the Complex ReLU activation function [44, 45], we propose the Fully Quaternion ReLU activation function, defined as follows:\n$FQReLU(q) = \\begin{cases}\nq &\\text{if } \\theta \\in [0, \\pi/2] \\\\\n0 &\\text{otherwise} \\end{cases}$\n(20)\nwhere $q = qr + q_i i + q_j j + q_k k \u2208 H$, and 6 is the phase of the quaternion, computed as follows [46]:\n$\\theta = atan(\\frac{\\sqrt{q_i^2+q_j^2+q_k^2}}{q_R})$\n(21)\nFor these functions, the learning dynamics are built using partial derivatives on the quaternion domain."}, {"title": "2.2.4. Fully Connected Layers", "content": "Let Q be a N\u2081 \u00d7 N2 \u00d7 N3 tensor", "quaternion": "n$Q = [q(x", "z)": "H^{N_1\u00d7N_2\u00d7N_3"}, "n(22)\nwhere N1, N2, N3 are the height, width and number of channels of the input.\nNow, it is defined a quaternion kernel, W, of size N\u2081 X N2 \u00d7 N3, where each element is a quaternion:\n$W = [w(x, y, z)"], "19]": "n$f = \\sum_{r,s,t}^{N_1,N_2,N_3} [w(r, s, t)q(r, s, t)] .$\n(24)\nh"}