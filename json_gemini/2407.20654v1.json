{"title": "Prompting Encoder Models for Zero-Shot Classification: A Cross-Domain Study in Italian", "authors": ["Serena Auriemma", "Martina Miliani", "Mauro Madeddu", "Alessandro Bondielli", "Lucia Passaro", "Alessandro Lenci"], "abstract": "Addressing the challenge of limited annotated data in specialized fields and low- resource languages is crucial for the effective use of Language Models (LMs). While most Large Language Models (LLMs) are trained on general-purpose English corpora, there is a notable gap in models specifically tailored for Ital- ian, particularly for technical and bureaucratic jargon. This paper explores the feasibility of employing smaller, domain-specific encoder LMs alongside prompt- ing techniques to enhance performance in these specialized contexts. Our study concentrates on the Italian bureaucratic and legal language, experimenting with both general-purpose and further pre-trained encoder-only models. We evaluated the models on downstream tasks such as document classification and entity typ- ing and conducted intrinsic evaluations using Pseudo-Log-Likelihood. The results indicate that while further pre-trained models may show diminished robustness in general knowledge, they exhibit superior adaptability for domain-specific tasks, even in a zero-shot setting. Furthermore, the application of calibration techniques and in-domain verbalizers significantly enhances the efficacy of encoder models. These domain-specialized models prove to be particularly advantageous in scenar- ios where in-domain resources or expertise are scarce. In conclusion, our findings offer new insights into the use of Italian models in specialized contexts, which may have a significant impact on both research and industrial applications in the digital transformation era.", "sections": [{"title": "1 Introduction", "content": "Pre-trained LMs have had a significant impact on Natural Language Processing (NLP), with the \"pre-train and fine-tune\" paradigm rapidly becoming the predomi- nant approach to apply effective models on a wide variety of downstream tasks [1-3, inter alia]. However, one of the main concerns when working with LMs is the paucity of annotated data, especially for specific domains or low-resource languages, required to fine-tune the additional classification layer on top of these models for downstream tasks, such as classification. Recently, prompt-based tuning has started to affirm as a promising way to perform similar tasks, significantly reducing the need for annotated data. This approach has been proven to be very effective with Large Language Models (LLMs) [4]. However, it is often the case that LLMs are not available for low-resource languages, and that their performance drastically decreases when they are challenged on specific domains. Moreover, in the Digital Transformation era, businesses frequently need to integrate artificial intelligence systems into their application ecosystems. This requires them to utilize specialized, publicly available models while also employing effective methods to leverage these models in scenarios where annotated language resources are unavailable, thereby operating in a zero-shot mode.\nHence, we decided to evaluate two smaller domain-specific encoder models: BureauBERTO [5], a LM further pre-trained on Italian bureaucratic texts (i.e., administrative acts, banking and insurance documents), and Italian Legal BERT [6] (henceforth referred to as Ita-Legal-BERT), a LM adapted to the Italian legal domain, on various classification tasks on domain-specific data exploiting a prompt-based tech- nique in a zero-shot scenario. Additionally, we compared the performance of both models with that of a generic Italian model, UmBERTO.\nSince BureauBERTO has proved to be particularly accurate in a fill-mask task [5], in which the model had to predict both random and in-domain masked words, we sought to further inspect the domain lexical knowledge acquired by this model during the domain adaptation. We aimed at leveraging this knowledge to imple- ment two classification tasks in the Public Administration (PA) domain, modeled as prompt-based classification. Specifically, we challenged the model to predict both the topics of PA texts, and the type of generic and PA-related named entities occur- ring in sentences extracted from administrative documents. Additionally, we aimed to evaluate BureauBERTo on a language variety kindred to its domain of specializa- tion, namely Italian legal language. Furthermore, we wanted to investigate whether there are differences in the performance of specialized models obtained through addi- tional pre-training with and without a preliminary vocabulary expansion with domain words (see Section 3.1), particularly when the tasks are modeled as prompt based classification and heavily rely on the models' vocabulary. Therefore, we tested two"}, {"title": "2 Related work", "content": "Pre-trained LMs have proven to be effective in NLP tasks related to specific domains, whether they were trained from scratch [9, 10], or further pre-trained on domain data [6, 11, 12] with a Masked Language Modeling (MLM). More recently, the MLM"}, {"title": "2.1 Prompt-tuning", "content": "training objective has been leveraged to solve various NLP tasks reformulated as a sort of cloze task, allowing the LM to directly solve it without any or with very few labeled examples. One of the first works in this direction was proposed by [13], who performed zero-shot learning using pre-trained LMs without fine-tuning on a dataset of training examples. Within similar conditions, but using the larger GPT-3, near state-of-the-art results have been achieved [4] for some SuperGLUE tasks [14]. Competitive performance with GPT-3 has been attained using much smaller models like the 220M parameters ALBERT, by performing some gradient-based fine-tuning of the model using the labeled examples on a cloze task [15]. Since then, prompt-based learning has gained attention as a simple way to perform, among other tasks, zero-shot classification [16]. However, it is essential to note that the performance of prompt- based learning techniques scales with model size [17]. Consequently, general-purpose LLMs with billions of parameters are typically used in prompt-learning experiments, even for specialized domains such as the legal one [18]. In contrast, for the biomedical and clinical domains, smaller specialized models like BioBERT [11] and Clinical BERT [19] outperformed GPT-2 and T5 in a few-shot prompt based classification of medical texts [20]. The authors hypothesize that the advantage of the BERT-based models is possibly due both to their domain adaptation and to their bidirectional MLM training objective, which is more similar to the prompt template format than those of auto- regressive and sequence-to-sequence models like GPT-2 and T5. A similar finding was reported by [21] even for the much larger GPT-3 over BioBERT. Nevertheless, these approaches are constrained by the model input size, which limits the length of the conditioning input context and can significantly affect performance [21].\nAlthough prompt-based classification with specialized models has been explored for the medical and clinical domains, to the best of our knowledge, this is the first work that focuses on applying prompts to the Italian administrative and legal language in a zero-shot classification scenario."}, {"title": "2.2 Mapping labels through the verbalizer", "content": "To perform prompt-based classification with encoder-only LMs, a specific template is required to reformulate the original classification task as a cloze-task, where the text to be classified is fed to the model followed by a prompt sentence, such as \"This <text> is about [MASK]\". In this way, the model has to predict the probability that a certain word is filled in the \"[MASK]\" token. The mapping from the label candidate word to a specific class is gained through the verbalizer [15], which represents the original class names as a set of label words. The choice of the verbalizer greatly influences the model performance [22].\nUsually, verbalizers are manually constructed by associating each class with one or few word labels that capture the main content of the class texts and on which the model relies to perform the classification [15, 23]. However, selecting appropriate words is not trivial because the probability of the chosen label words being the \"correct\" token in the [MASK] position within the prompt determines the model's accuracy in the task. To reduce the human effort in handcrafting the verbalizer, another approach automatically finds the label mapping using discrete search on a small amount of"}, {"title": "2.3 Model stabilization through calibration", "content": "A notable challenge in prompt-based approaches lies in their sensitivity to variations in prompt templates and verbalizers [27-29]. This issue is particularly evident in the so-called \"in-context\" learning where the model learns how to solve a task given some human-designed instruction and a few demonstrations provided in the prompt, without requiring any parameters update. According to [7], the instability of results in few-shot learning may be attributed to three types of biases resulting from the prompt:\n\u2022 the recency bias, i.e., the model tends to repeat the answer that appears most recently, typically towards the end of the prompt;\n\u2022 the majority label bias, which occurs when there is a class imbalance in a text classification prompt, such as having more Positive than Negative sentiment examples;\n\u2022 the common token bias according to which the model is more prone to output tokens that are frequent in its pre-training distribution.\nThe last bias is particularly relevant for text classification tasks, even in zero- shot settings, because the model makes predictions by choosing label words associated with specific classes. As certain label words appear more frequently in its pre-training corpus, the model is inherently biased toward predicting certain classes. To mitigate these biases, various calibration approaches have been proposed.\nThe contextual calibration (CC) was first introduced by [7]. It relies on the assumption that a model's bias towards a certain class can be estimated by feeding the model a content-free input string, such as \"N/A\", an empty string, or gibberish tokens. Ideally, given a content-free string, the model would assign an equal score to"}, {"title": "3 Experimental settings", "content": "To conduct our experiments we used OpenPrompt, an open-source Python frame- work for prompt-learning. This framework also provides methods to experiment with different types of verbalizers, such as manual and knowledgeable verbalizers, and to calibrate models. We conducted our experiments on an NVIDIA A100 GPU."}, {"title": "3.1 Models", "content": "For our experiments, we compared the performance of three encoder-only models, namely UmBERTO, Ita-Legal-BERT and BureauBERTO."}, {"title": "3.2 Datasets", "content": "We tested the models on two different domains, PA and legal, and we used four different datasets, two for each classification task."}, {"title": "3.2.1 PA", "content": "For the prompt document classification, we used a subset of the ATTO corpus [33], which is a collection of short administrative documents annotated with labels denoting topics. We filtered this dataset keeping only those instances (2,812) that were annotated with a single topic label (see Table A1 in Appendix A for the distribution of the labels in the dataset). We split the dataset, using 90% for testing the models and 10% for performing batch calibration and creating the knowledgeable verbalizer.\nFor the prompt entity typing task, we used the PA-corpus of [34], a collection of 460 PA documents with token-level annotations of Named Entities denoting both general entities, such as persons (PER; 3,706), locations (LOC; 4,498), organizations (ORG; 3,594), and domain-specific entities, like legislative norms (LAW; 5,217), acts (\u0410\u0421\u0422; 2,240), and PA-related organizations (\u041e\u0420\u0410; 2,074). As a test set, we adopted the same subset as the one reported by [33] corresponding to the 10% of the dataset. To build the knowledgable verbalizer we used the training set, i.e., a subsample cor- responding to the 70% of the dataset. We extract 180 sentences from this subset to apply batch calibration."}, {"title": "3.2.2 Legal", "content": "For the prompt-document classification in the legal domain, we used a dataset comprising 3,190 Italian civil judgments annotated with various topics (see Table A2 in Appendix A for the complete list). Italian civil judgments follow a quasi-standardized"}, {"title": "3.3 Verbalizers", "content": "We conducted our prompt-based classification experiments in the PA domain using three types of verbalizers, namely a base, a manual and a knowledgeable one. For the legal domain, we focused only on the base and the knowledgeable verbalizers, as we lacked the domain expertise necessary to depict civil judgements topics at a finer- grained level. The aim of this comparison was to better understand the correlation between the lexical knowledge of LMs and the use of domain-related terms as the set of word labels encompassed within the verbalizers.\nThe Base-verbalizer simply uses the Italian name of the classification classes reported in the datasets as label words (e.g., Ambiente - \u201cEnvironment\" is the label word for the class AMBIENTE \"ENVIRONMENT\" in the PA document classification task.)\nThe Manual-verbalizer is a verbalizer that we constructed by adding some syn- onyms of the class name and some related PA terms as label words for each class, to better depict the documents content and the entity types (in this case the label words for the class AMBIENTE - \"ENVIRONMENT\" are: ambiente - \u201cenvironment\", natura \u201cnature\u201d, territorio - \u201cterritory\u201d, flora - \u201cflora\", etc.).\nFinally, the Knowledgeable verbalizer was created following the Label-Name Only Text Classification (LOTClass) method by [26] to build the external knowledge- base, which was then refined using the approach of [25]. The LOTClass method enables constructing a category vocabulary for each class that contains semantically related words to the label name of the class by leveraging the model vocabulary via MLM. We applied this method to additional samples from each dataset not used for testing the models. We can summarize the applied methodology in the four following steps."}, {"title": "3.4 Calibration", "content": "To stabilize the models' results, we calibrated them for both entity typing and doc- ument classification tasks in the PA and legal domains. Calibration was performed twice for each task using the Contextual Calibration (CC) and Batch Calibration (BC) approaches to evaluate how different calibration methods affect the performance of generic vs. specialized models, and with different verbalizers. In the CC settings, we used an empty string as content-free input. In the BC approach, we calibrated the models using a stratified sample of unlabeled texts extracted from the dataset that"}, {"title": "3.5 Prompt-based classification", "content": "We performed the prompt-based classification across different settings. For each domain, we performed CC and BC on all three models, and compared the performance with that obtained without any calibration.\nWe tested the models using: a base verbalizer, where each class is linked to one or few label words that correspond to the names of the classes in the original dataset annotations described in Section 3.2 (e.g., the one from the ATTO corpus and the InformedPA corpus for the PA domain); a manual verbalizer, enriched by a collection of domain terms manually selected as representative for the classification labels; and a knowledgeable verbalizer, specific to each model. For the knowledgeable verbalizer (See Section 3.3), we created three different versions for each domain and task, using"}, {"title": "3.5.1 Entity Typing", "content": "We modeled the NER task introduced by [34] as an entity typing task. Entity typing can be considered a subtask of NER and focuses on entity classification. In other words, systems assign a label to an already extracted entity. This task is often formulated to challenge systems at retrieving sub-categories organized in a hierarchical structure (e.g., an entity corresponding to a person may be specified as director, major, lawyer, etc.) As in [34], we asked models to identify only coarse-grained entities: generic ones, such as persons (PER), locations (LOC), and organizations (ORG); and related to the administrative domain: law references (LAW), administrative acts (ACT), and PA organizations (OPA).\nWe prompted the models by feeding them with a sentence and an entity occurring in it, asking to predict the entity type in place of a masking token. The resulting template is: <text>. In questa frase, <entity> \u00e8 un esempio di <mask>.\nAs anticipated, we verbalized the entities in three ways. In the first experiment, we provided an Italian translation of the entity or a single word representing the entity class. In the second experiment, we expanded most of the label words by including synonyms and other terms related to the various classes (See verbalizers for entity typing in Table 2). Finally, we used the knowledgeable verbalizer, by exploiting an external knowledge base automatically created as described in Section 3.3."}, {"title": "3.5.2 Document Classification", "content": "For the recognition of the topics in PA and legal documents, we designed the following template to model the document classification task as a masked language modeling problem: <text>.Questo documento parla di <mask>.\nThus, LMs are challenged to infer the topic of the document by predicting the most appropriate label word to represent the masked token in the prompt, following the document text. Since the ATTO corpus contains only short documents of a maximum of 600 tokens, by setting the tokenizer's truncation at 512 tokens, we were able to feed the models the entire document in almost all cases. To classify Italian civil judgements, we addressed the input length limit by feeding the models only with the factual background section of the judgements, which provides a general overview of the topic."}, {"title": "3.6 Evaluation metrics", "content": "We evaluated the performance of the models with common classification metrics, such as Precision, Recall, and F1-Score."}, {"title": "3.7 Pseudo Log-Likelihood", "content": "We employed Pseudo Log-Likelihood scores (PPLs) to evaluate the linguistic compe- tence of models on domain texts with an intrinsic metric, and to determine whether this competence influenced their performance in domain-specific tasks modeled via prompting. Working with domain data necessitates understanding how well models can comprehend texts specific to that domain, especially when tasks are carried out without fine-tuning, so models can solely rely on the linguistic competence gained dur- ing pre-training to solve them. By incorporating PLLs computation into our analysis, we can gain insight into the capacity of our three models to represent and process domain specific terminology and linguistic structures.\nThe two specialized models, BureauBERTo and Ita-legal-BERT, are not only based on different encoder architecture (ROBERTa vs. BERT, respectively), but also diverge in terms of approaches used for their domain-adaptation: BureauBERTO vocabulary was expanded with domain-related tokens before additional pre-training, while no vocabulary expansion was performed for Ita-legal-BERT. Additionally, UmBERTo rep- resents an instance of Italian models trained once and directly on a composite corpus comprising, among varoius genres, administrative and legal texts, which contributed to its effectiveness in tasks within these domains. Accounting for variability in mod- els overall capacity to handle administrative and legal data, also in relation to the way they acquire their generic vs. domain-specific linguistic competence, can provide deeper insights into their behaviour when challenged on domain text classification tasks.\nThus, we aim to measure the probability given by the models to the tokens extracted from a sample of the considered datasets, or in other words, how much the models are surprised by analyzing the in-domain texts of our legal and PA datasets. Since we are dealing with encoder models, PLL is a valid alternative to Perplexity, which applies only to auto-regressive models. PLL [35] is computed by summing the conditional log probabilities $log PMLM (Wt | W\\t)$ of each token t in the sentence W. These are induced in BERT-like models by replacing wt with [MASK]. Let $\u0398$ indicate the model's parameter, then the PLL function is defined as:\n$PLL(W) := \u2211log PMLM(Wt | W\\t; \u0398)$\nTo compute PLLs, we collected eight documents per class from the ATTO corpus and the judgements dataset, and 90 and 135 sentences per class from the InformedPA and the legal entity typing dataset, respectively (see Section 3.2). The dataset size was chosen based on the number of elements in the least numerous class of the dataset and was stratified across all other classes. Additionally, we computed PLLs on a collection of 482 sentences from various genres, corresponding to the test set of the Italian Stanford Dependency Treebank (ISDT) [36], that we used as baseline to identify any significant difficulties in modeling domain-specific texts.\nSince PLLs are comparable under the same vocabulary dimension, which varied among our models, we normalized the scores using sentence length (i.e., the number of tokens) to mitigate this effect."}, {"title": "4 Results and discussion", "content": null}, {"title": "4.1 Entity typing", "content": null}, {"title": "4.1.1 PA domain", "content": "In this paragraph, we analyze the results of the entity typing task when applied to the PA domain. Table 5 shows the Macro Average F1-score obtained by the three examined models in each setting, using the three verbalizers described earlier. In Table 6 the Fl-scores for each class are reported.\nNo-calibration. By employing a base verbalizer, where a single label is associated with each class, UmBERTo almost doubled the results obtained by BureauBERTO (0.36 vs. 0.20), whereas Ita-Legal-BERT obtained very low results (0.08). Surpris- ingly, for a domain entity like ACT, BureauBERTo missed all the entities, whereas UmBERTo obtained a low but higher score (0.14). For the LAW entity, UmBERTO outperforms both Ita-Legal-BERT and BureauBERTo, as well. We may suppose that this is because UmBERTo was trained on Common-Crawl, which also contains legal and administrative texts in its Italian section. Very high results are obtained by UmBERTO for PER entities, reaching 0.83 in our zero-shot scenario. Even though on OPA Ita-Legal-BERT overpasses BureauBERTo and UmBERTo (0.20 vs 0.05 and 0.19), the three models obtain very low results for this class and for LOC, and ORG. These two latter classes are very similar to each other: ORG refers to organizations in general, comprising firms and associations, whereas OPA can be considered as a subclass of ORG, and refers to organizations within the Public Administration, such as municipal departments. Such overlapping may impact classification.\nThen, we added to the prompt highly distinctive words for each class and inte- grated them into a manual verbalizer. In this case, we notice a better ability of both BureauBERTO and Ita-Legal-BERT to recognize domain-specific entities such as ACT, LAW, and OPA. However, despite the general improvement in recognizing such classes, we noticed that they perform worse than UmBERTO for traditional enti- ties. Nonetheless, whereas Ita-Legal-BERT still obtained very low scores (F1 Marco AVG 0.15), BureauBERTO (0.52) outperforms the general-purpose model, UMBERTO (0.37), by exhibiting an improvement more than twofold (0.52 vs 0.20). This signif- icant boost in performance suggests that the domain-adapted model is likely to be more attuned and proficient in leveraging domain-specific terminology.\nNevertheless, it is important to acknowledge that domain-specific terms may wield less influence over generic entities such as PER. With the manual verbalizer, UmBERTO fails to recognize any PER entity. We observed that the model identi- fies almost all the people's names as ORG entities. Thus, we carried out an ablation study by considering only the in-domain model BureauBERTO and the general-purpose model UmBERTO. The ablation study consisted in deleting the in-domain terms added for the PER entity class, i.e. generalit\u00e0 - \u201cparticulars\u201d and nominativo \"name\".\nThe results are summarized in Table C8, Appendix C. They show that the perfor- mance of UmBERTo increases not only for the PER entities but that the ablation improves the Fl-score of the ORG class as well. Whereas UmBERTo reaches the high- est performances for overall F1 Micro Avg, the adapted model still obtained higher"}, {"title": "4.1.2 Legal domain", "content": "In this paragraph, we analyze the results of entity typing modeled as a prompting task within the legal domain. Table 7 presents the Macro Average Fl-score achieved by the three models under examination across different settings, utilizing the three verbalizers previously described. Table 8 details the F1-scores for each class."}, {"title": "4.2 Document classification", "content": null}, {"title": "4.2.1 PA domain", "content": "Regarding the prompt document classification experiments in the PA domain, the results summarized in Table 9 reveal a trend similar to that observed in entity typing."}, {"title": "4.2.2 Legal Domain", "content": "Regarding the prompt-document classification in the legal domain, we only tested models using the base and the knowledgeable verbalizers, as we lacked the domain expertise necessary to select the most suitable labels to best depict the different topics in the civil judgments dataset for the manual verbalizer.\nAs shown in Table 11, with the base verbalizer, the best-performing model was con- sistently BureauBERTo, followed by UmBERTO and Ita-legal-BERT, across all three settings: no-calibration, CC and BC. Despite being the domain-specialized model, Ita- legal-BERT struggles to reach the accuracy levels of the other two models when tasked via prompting. This may be because Ita-legal-BERT is based on the BERT architec- ture, while the other two models can rely on the robustly optimized architecture of ROBERTa [32]. The advantage of BureauBERTo over UmBERTo in classifying legal texts may stem from its additional pre-training on administrative texts, which in the Italian language share some common linguistic features with legal language.\nOn the other hand, with the knowledgeable verbalizer, the domain-specialized model outperforms the other two in the CC setting, namely when the word labels are selected by the model and calibration is performed on content-free input strings. In contrast, when calibrating on data extracted from the dataset (BC), the best per- forming model was UmBERTO. The class-wise comparison reported in Table 12 shows that BC actually improved UmBERTo performance by enhancing its ability to rec- ognize classes such as EDILIZIA E URBANISTICA-'CONSTRUCTIONS AND URBAN PLAN- NING', SERVIZI DEMOGRAFICI-'DEMOGRAPHICS' or PERSONALE-'PERSONNEL' which the model completely fails to categorize in the CC setting. Similarly, for BureauBERTO and Ita-Legal-BERT, we observed a better ability to recognize most of the clas- sification classes in the BC setting compared to their perfomance with CC. The least recognizable class for all three models was AMMINISTRAZIONE E SEGRETERIA GENERALE-'ADMINISTRATION AND GENERAL SECRETARIAT', likely because the texts belonging to this class cover a wide range of diverse and generic topics."}, {"title": "5 Conclusion and future work", "content": "In this paper, we propose a zero-shot prompt tuning classification approach for solving two tasks related to the Italian PA and legal domains: the classification of docu- ments according to their topic and the recognition of the entity types occurring in administrative sentences.\nWe compared the perfomance of the PA-specialized model BureauBERTo with that of the legal-specialized Ita-Legal-BERT and of the domain-agnostic model UmBERTO across these tasks. Our findings indicate that, by performing calibration with both Contextual Calibration (CC) and Batch Calibration (BC) approaches, all models demonstrated enhanced performance in almost all settings. However, we observed an interplay between the type of calibration performed and the adopted verbalizer.\nIn the PA domain, when the task is performed by the specialized model with the knowledgeable verbalizer, calibrating the model on empty input strings (CC), yields satisfactory results, meaning that in this case the verbalizer can be built by leveraging self-supervision techniques, without human intervention. Similarly, for the classifica- tion of legal documents, the in-domain model Ita-Legal-BERT outperforms the other two only when the knowledgeable verbalizer is combined with the CC approach.\nThe knowledgeable verbalizer also allows BureauBERTo to achieve the highest results in the administrative text classification in both BC and CC settings, and even without calibration. Hence, another advantage of using a specialized model is the possibility of leveraging the model itself to construct an effective task verbalizer, thanks to the model ability to find appropriate in-domain word labels.\nIn contrast, with the manual verbalizer, which we only tested in the PA domain, the best-performing model was the generic UmBERTo in both BC and CC settings. This demonstrates that domain-agnostic models can be effectively employed for solv- ing domain-specific tasks when empowered by human domain expertise encompassed within the verbalizer and stabilized through calibration techniques. Adopting a man- ual verbalizer in combination with the CC approach can be particularly beneficial for specific domains where neither specialized models nor domain data are available for calibration or to construct an external knowledge base from which to derive a knowledgeable verbalizer.\nAmong all, the BC approach enables the best overall performance for all three models across different tasks and domains. This supports previous findings indicating that relying on content-based strings instead of content-free input can help to better mitigate the model contextual biases in prompt-based tasks [31], particularly in zero- shot scenarios. However, this approach requires additional batches of class examples, which may not always be available in specific domains.\nFurthermore, with the BC, we observed competitive performance in the entity typing tasks for both specialized models, BureauBERTo and Ita-Legal-BERT, in their respective domains, even when the tasks are performed using the base verbalizer. This suggests that for tasks like entity typing, where entities can be described using a small set of domain words, the domain-specialized models represent a valuable choice since they do not require additional human effort to construct the verbalizers.\nIn conclusion, our results underscore the possibility of effectively using encoder- only models for performing domain-related tasks modeled via prompting in a zero-shot setting, especially when stabilized through calibration techniques. Specialized encoders demonstrate competitive abilities in handling both generic and domain-specific data, as evidenced by their PPLs (see Table 13), and obtain surprisingly high results also in supervised tasks, such as text classification. Notably, their acquired domain knowledge also enables these models to perform well in entity typing tasks without requiring addi- tional effort to customize the verbalizers. These models are indeed accustomed to the distinctive way in which entity names and domain-specific terminology are employed in PA and legal texts. This capability makes them considerably helpful for automati- cally finding finer-grained word labels to perform text classification without relying on annotated data. This opens up the possibility of employing smaller specialized mod- els to automate the verbalizer construction, which might also be used to perform the task with larger models.\nAs a future direction for our work, we aim to explore additional approaches to leverage specialized models for prompt-based tasks, fully exploiting their potential in contexts where annotated data are limited or unavailable. Additionally, we plan to test larger language models on these tasks to assess the impact of model size on domain-specific performance."}, {"title": "Appendix A Datasets", "content": "In this section, we report the topic distribution of the document classification datasets for the administrative and legal domains, listed in Table A1 and Table A2, respectively."}, {"title": "Appendix B Label words", "content": "This section shows the word labels encompassed within the different verbalizers we tested. Table B3 lists the words used in the base verbalizer for the legal entity typing experiment, while Table B4 reports those used for the document classification task.\nTable B5 and Table B6 contain the English translation of some of the label words used in the models' knowledgeable verbalizers for the document classification and entity typing tasks in both PA and legal domains. They are the English translation of Table 4 and Table 3."}, {"title": "Appendix C Ablation study", "content": "In this section, we report in Table C8 the results obtained in the ablation study on the entity typing task for the administrative domain."}, {"title": "Appendix D Fill-mask results", "content": "Preliminary experiments on a fill-mask task (Fig.D1) showed that BureauBERTO outperformed UmBERTo when predicting masked words on Public Administra- tion documents [5]. This motivated us to evaluate BureauBERTo domain-specific knowledge in an unsupervised setting in prompt-based zero-shot classification tasks."}]}