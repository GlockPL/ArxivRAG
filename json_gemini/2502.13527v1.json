{"title": "Exploiting Prefix-Tree in Structured Output Interfaces for Enhancing\nJailbreak Attacking", "authors": ["Yanzeng Li", "Yunfan Xiong", "Jialun Zhong", "Jinchao Zhang", "Jie Zhou", "Lei Zou"], "abstract": "The rise of Large Language Models (LLMs)\nhas led to significant applications but also intro-\nduced serious security threats, particularly from\njailbreak attacks that manipulate output genera-\ntion. These attacks utilize prompt engineering\nand logit manipulation to steer models toward\nharmful content, prompting LLM providers to\nimplement filtering and safety alignment strate-\ngies. We investigate LLMs' safety mechanisms\nand their recent applications, revealing a new\nthreat model targeting structured output inter-\nfaces, which enable attackers to manipulate\nthe inner logit during LLM generation, requir-\ning only API access permissions. To demon-\nstrate this threat model, we introduce a black-\nbox attack framework called AttackPrefix Tree\n(APT). APT exploits structured output inter-\nfaces to dynamically construct attack patterns.\nBy leveraging prefixes of models' safety re-\nfusal response and latent harmful outputs, APT\neffectively bypasses safety measures. Experi-\nments on benchmark datasets indicate that this\napproach achieves higher attack success rate\nthan existing methods. This work highlights the\nurgent need for LLM providers to enhance secu-\nrity protocols to address vulnerabilities arising\nfrom the interaction between safety patterns\nand structured outputs.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of Large Lan-\nguage Models (LLMs) in recent years and their\nwidespread application in various domains, the emergence of attacks targeting these\nmodels, particularly jailbreak attacks aimed at ma-\nnipulating output generation, poses significant se-\ncurity threats. Such attacks primarily leverage\ntechniques like prompt engineering to inject harm-\nful inputs prior to generation, guiding the model\ntoward producing harmful, false, or unethical out-\nputs. In response, LLM researchers and vendors\nemploy various safety mechanisms to mitigate the\nrisk of generating harmful content, including out-\nput filtering, and constrained decoding. Specifically, post-\ntraining methods, such as Safety Alignment, involve constructing safety-\naligned datasets to guide the model in avoiding\npotential harmful patterns, prioritizing safe and be-\nnign responses over harmful ones in the output\nrankings.\nAs LLMs continue to evolve in domain-specific\napplications, structured output has emerged as a\ncritical feature. By fine-tuning the capacity for struc-\ntured output, developing structured generation tem-\nplates, and controlling model logit masks during\ngeneration through automated mechanisms, LLMs\ncan produce outputs that conform to various struc-\ntured formats such as JSON and YAML. However, these capabilities also in-\ntroduce new potential security vulnerabilities, as\nattackers may gain direct or indirect control over\nthe decoding process, thereby influencing model\noutputs.\nIn this paper, we conduct a comprehensive anal-\nysis of the current state of research on safety fine-\ntuning and structured output. We find that while\nexisting protective measures can resist straightfor-\nward attacks to some extent, the continual evolution\nof attack techniques poses a growing challenge to\ntheir effectiveness. Our motivations can be summa-\nrized as following observations:\nObservation 1: During safety alignment, LLMs sig-"}, {"title": null, "content": "nificantly reduce the probability of predicting and\nsampling harmful content through post-training;\nhowever, this does not lower the probability of such\nharmful outputs below the average token predic-\ntion probability. This implies that by expanding the\nsampling and output range, these harmful outputs\nare still obtainable.\nObservation 2: During post-training, the safe re-\nsponses of the model, such as generating phrases\nsuch as \"I am sorry, I cannot produce unethical con-\ntent\", often depend on the safety alignment dataset\nused. To ensure the stability of the model's train-\ning and user experience, these safety patterns are\ntypically at sentence level and are drawn from a\nlimited set.\nObservation 3: In model inference, the token-by-\ntoken prediction process of LLMs strives to main-\ntain the coherence and completeness of sentences.\nThis means that once the prefix tokens of a gen-\nerated sentence are determined, it is challenging\nfor the model to alter the overall semantics of the\nsentence through safety measures.\nThus, the token-by-token inference mode of\nLLMs conflicts with the sentence-level safety pat-\nterns learned during post-training, creating opportu-\nnities for attackers to manipulate subsequent output\nby intervening in the prefix of the model's output.\nMoreover, structured output provides attackers with\nmechanisms to intervene in the model's logit out-\nput, allowing for potential control over the prefix\nsampling results to generate harmful and unethi-\ncal responses. This attack paradigm inspires the\nproposal of a novel framework, named AttackPre-\nfixTree (APT). By utilizing service interfaces that\nprovide structured outputs, we online construct a\ntree structure that includes model safety pattern pre-\nfixes and harmful outputs. Through iterative calling\nmodel and sampling, the APT continually expands\nnodes representing safe prefixes and harmful out-\nputs, ultimately guiding the formation of harmful\noutputs and the potential security measure prefix\ntree of the model. Our investigation reveals that\nexisting models struggle to address this dynamic\nform of attack under the proposed framework, indi-\ncating that model service providers must carefully\nreconsider and enhance security measures when\noffering structured output interfaces. The contribu-\ntions of our proposed method can be summarized\nas follows:"}, {"title": null, "content": "1. We propose a new threat model for jailbreak\nattacks that exploits structured output inter-\nfaces, demonstrating how adversaries can by-\npass sentence-level refusal patterns through\ntoken-level manipulation of safety prefixes.\nOur analysis reveals fundamental vulnerabil-\nities in current safety alignment approaches\nwhen confronted with structured output con-\nstraints.\n2. We implement the proposed threat model by\ndesigning AttackPrefixTree (APT), a black-\nbox attack framework that systematically com-\nbines online tree-based exploration of safety\nprefixes with constrained decoding to dynami-\ncally suppress refusal patterns. By leveraging\nstructured output interfaces, our approach en-\nables token-level sampling manipulation with-\nout requiring model weight access while auto-\nmatically retrieving refusal patterns.\n3. Extensive evaluations across JailBreakBench,\nAdvBench, and HarmBench demonstrate\nstate-of-the-art attack success rates comparing\nwith existing methods, suggesting that model\nproviders reconsider the security threats as-\nsociated with open access to direct logit ma-\nnipulation interfaces. We further discover the\nimplications of this threat model on the rea-\nsoning models, revealing new potential risks\nin multi-stage reasoning model generation pro-\ncessing."}, {"title": "2 Preliminary & Related Works", "content": ""}, {"title": "2.1 Structured Output", "content": "OpenAI introduced structured output in the\nAPI model, which made the outputs adhere reliably\nto the developer-supplied JSON schemas. A widely\nused method is constrained decoding. Outlines\nand LM Format Enforcer\nare famous libraries that enforce the output format\n(JSON Schema, Regex, etc.) of a language model,\nused by vLLM as the structure\noutput backend.\nDuring the LLM generation phase, constrained\ndecoding dynamically maintains a valid token mask\ncorresponding to predefined restrictions. This mask\nis applied to modify the output logits of the LLM,\nthereby generating a constrained probability dis-\ntribution from which the next token is sampled,"}, {"title": "2.2 LLM Jailbreak", "content": "Jailbreak attacks aim to surpass LLMs' safety\nmechanisms, coercing models to generate harm-\nful content like \u201cHow to damage a traffic light\".\nWei et al. introduces the concept of jail-\nbreak attack that is an attempt to elicit a direct\nresponse to a harmful prompt for by prompt modi-\nfication, a problem which has received widespread\nattention. The possibility of jailbreak comes from\nmodels' Competing Objectives, i.e., inconsistencies\nobjective of helpfulness and harmlessness during\nalignment and mismached objective of pre-training\nand safe-alignment training. The success of a jail-\nbreak attack often hinges on initial response tokens,"}, {"title": "2.2.1 Auto-method", "content": "The white-box methods need the access permis-\nsion to model's weight and usually need lots of\ncomputation and times. The black-box are ad-hoc\nmethods, depend on human's carefully design and\nmany tires to find a new pattern. In many situations,\nwe do not have access to the model's weight, and\nspending time designing a jailbreak attack prompt\nis unaffordable. Methods for automatically generat-\ning jailbreaks are proposed. Most method-\nologies employ various strategies to generate a sub-\nstantial pool of candidate prompts (predefined cate-\ngories, via iterative model self-improvement, etc.).\nThese approaches are complemented by a discrim-\ninator that assesses the effectiveness of each can-\ndidate prompt in successfully executing an attack.\nBy navigating through a defined search space and\nemploying a search-and-validate mechanism, these\nmethods autonomously identify prompts capable\nof achieving successful attacks.\nfound that open source\nsafety-aligned LLMs are still vulnerable to jail-\nbreak attack. The alignment procedures are based"}, {"title": null, "content": "on a subset of decoding settings while remaining\nvulnerable to attacks under other decoding settings,\nsuch as different temperature, decoding penalty,\nand constraint decoding.\ninvestigates the impact of\nconstrained decoding on jailbreak attacks and pro-\nposes a novel attack method termed EnDec. EnDec\ninvolves training an auxiliary model designed to\ndetect whether the current output contains negative\nphrases such as \"Sorry\" or \"illegal\". Upon detec-\ntion of such terms, the method forcibly replaces the\ntoken with its antonym, thereby circumventing the\nconstraints imposed by the decoding process."}, {"title": "2.2.2 Anti-Jailbreak Techniques", "content": "Significant research efforts have been dedicated to\ndeveloping anti-jailbreak strategies to counteract\nmalicious attacks on LLMs. These methods typ-\nically target specific jailbreak vulnerabilities and\ncan be categorized into four primary approaches:\n1. Safety Alignment: These techniques focus\non identifying harmful examples in training\ndata or model outputs and refining the model's\nsafety through alignment processes. Represen-\ntative work includes post-training alignment\noptimizations and safety-enhanced fine-tuning.\n2. Prompt Perturbation: This strategy involves\nsystematically modifying user prompts to neu-\ntralize potential risks, such as adding semantic\nconstraints or rephrasing queries to prevent\nunsafe responses.\n3. Input/Output Detection: Researchers have\ndeveloped auxiliary detection models that\nscreen both user inputs and LLM outputs for\nharmful content, enabling real-time blocking\nof unsafe interactions.\n4. Parameter Intervention: Advanced meth-\nods directly analyze neural weights to iden-\ntify safety-critical parameters, then enhance\nrobustness through selective parameter mod-\nification or activation steering."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Threat Model", "content": "We consider a black-box attack scenario where an\nattacker exploits the LLM's structured output func-"}, {"title": null, "content": "tionality (e.g., regex constraints, JSON/XML for-\nmatting) to bypass safety guardrails. The attacker\ninteracts solely via the LLM's public API and has\nno access to model parameters, gradients, or inter-\nnal safety mechanisms. The attack framework is\ndefined as follows:\nAttack Strategy: The attacker crafts (1) a mali-\ncious query $P_q$ (e.g., \u201cHow to damage traffic light\u201d),\nand (2) a structured output pattern $P_p$ (e.g., regex\n\"(?!Sorry I cannot assist)\"). These components are\ncombined into a jailbreak template $P_J$:\n$P_J = P_q \\boxtimes P_p$ (1)\nwhere $\\boxtimes$ denotes the orthogonal concatenation of\n$P_q$ and $P_p$ to form a structured input prompt. The\nLLM's inference engine processes $P_J$ under the\nconstraints imposed by $P_p$, which restricts outputs\nto match predefined formats to suppress safety disclaimers.\nAttack Goal: the attacker's goal is to manipulate\nthe inputs to maximize the likelihood of generat-\ning a harmful output $R$. The attacker can obtain\nthe LLMs' jailbreaked responses by querying the\nAPI (with the structured output function), without\nany other prior knowledge of the model's internal\nparameters or constraints."}, {"title": "3.2 Implementation Details", "content": "The attack consists of two phases, aimed at con-\nstructing an AttackPrefixTree that leverages the\nstructured output capabilities of the LLM to gener-\nate harmful content.\nAttackPrefixTree is a hierarchical tree struc-\nture where each node represents a generated text\nsegment during the exploration of attack pattern.\nNodes are categorized into two types:\n\u2022 Positive Node: Contains generated harmful\ncontent (e.g., \"Step 1: You could damage a\ntraffic light by ...\"). The \u201charmful\" is defined\nby the harmfulness criteria. These nodes will\nbe expanded recursively util reaching token\nlimits.\n\u2022 Negative Node: Stores prefixes of safety\nresponses (e.g., \u201cSorry, I cannot assist...\").\nThese nodes terminate further expansion but\nupdate the suppression set S to refine Pp, sup-\npressing safety disclaimers in subsequent iter-\nations.\""}, {"title": null, "content": "unsafe content and mining the pattern of safety re-\nsponses. And our attack goal and strategy can be\ninstanced as the construction of AttackPrefixTree,\nwhich contains 2 pharses:\nPhase 1: AttackPrefixTree Construction. The\nconstruction process begins with the user query Pq\nand pre-defined pattern Pp (can be empty $P_p \\leftarrow \\varnothing$).\nFrom the initial query, the attacker employs a\nDepth-First Search (DFS) approach to systemat-\nically explore each level of the tree and each node\nunder the constraints of the \u201cstructured output\" pat-\nterns. At each node, the attacker attempts to decode\nthe output based on the structured output pattern\nPp. As the attacker travels through the tree, a dis-\ncriminator model evaluates the outputs generated\nat each node. We employ LLama-Guard-2 as a binary classifier to assess text's\nharmfulness, which is a powerful reward model\nto guarantee the safeness of LLMs in the safety\nalignment training.\nDepending on the discriminator's outcome, the\nnode is labeled as either positive (represented as a\nred node) or negative (represented as\na green node). For positive nodes, the\nattacker continues expanding the tree by generat-\ning subsequent tokens under the structured output\nconstraints, recursively probing for harmful con-\ntent. Negative nodes trigger backtracking, and their\nprefixes are added to a suppression set S. This set\ndynamically updates the structured output pattern\nPp to exclude these unsafe prefixes in subsequent\nqueries (achieved through constrained decoding\ntechniques such as Outlines), effectively \u201cpruning\"\nundesirable response pathways.\nThis iterative process continues until the AP\u0422\nis fully constructed, i.e. all the leaves are reached\nthe max_token limitation. The detail of this step is\npresented in Algorithm 1.\nFallback. When the accumulated number of\nnegative children nodes of a node exceeds the hy-\nperparameter threshold, we simply deprecate that\nnode. If all branches (reaching the maximum beam\nsize) are marked as deprecated during tree construc-\ntion starting from the root node, the framework will\nclears all existing nodes and initiates APT recon-\nstruction from scratch."}, {"title": null, "content": "Algorithm 1: Construction APT\nInput: Initial query Pq, Initial Pattern Pp,\nMax tokens $T_{max}$\nOutput: AttackPrefixTree T\n1 Initialize root node $v_0 \\leftarrow (P_q, P_p)$\n2 T \u2190 {$v_0$}\n3 DFSstack \u2190 [$v_0$]\n4 while stack \u2260 0 do\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nend\n20\n21 end\nv' \u2190 DFSstack.pop()\npath \u2190 getPath(T, v')\nif path.length > $T_{max}$ then\ncontinue\nQ' \u2190 path + v'\nP' \u2190 v'.getNegChildren()\nR \u2190 LLM(Q', P')\nif Discriminator(Pq, R) = harmful then\nv\" \u2190 extractSentence(R)\nT.addPosChild(v\" \u2192 v')\nDFSstack.push(v\")\nelse\nr' \u2190 extractPrefix(R)\nT.addNegChild(r' \u2192 v')\n22 return T\nPhase 2: Path Reranking. Upon constructing\nthe APT in Phase 1, we evaluate all root-to-leaf"}, {"title": null, "content": "paths to identify the optimal jailbreak response. Let\n$\\mathcal{P}$ = {$r_1$, ..., $r_n$} denote the set of complete paths\nof positive nodes in T. The harmfulness score $s_i$\nfor path $r_i$ is computed using the discriminator's\nconfidence:\n$s_i = Discriminator(P_q, r_i)$ (2)\nGiven our use of Llama-Guard-2 as the discrim-\ninator, the harmfulness score for candidate path is\nderived from the model's confidence in determining\nthe content is safe (i.e. the probability of generat-\ning the \"safe\" token, denoted as $\\Psi$): $s_i$ = 1 - $\\Psi$.\nWe select the top-k (k=1) path $p^*$ with maximal\nharmfulness score:\n$p^* = \\arg\\underset{p_i \\in P}{\\text{max}}s_i$ (3)\nThe final output $P_{final}$ is then generated by\ntraversing $p^*$ and concatenating its node contents.\nThe phase 2 can be summurized as Algorithm 2.\nAlgorithm 2: Path Reranking\nInput: APT T\nOutput: Response $P_{final}$\n1 L \u2190 Leaf nodes of T\n2 S \u2190 \\{\\}\n3 foreach l \u2208 L do\n4\n5\n6 end\nP \u2190 TravelPath(T, l)\nS.add((P, s))\n7 Sort S descending by s\n8 $P_{final}$ \u2190 S[0].P\n9 return $P_{final}$"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Setting", "content": "Evaluation. To evaluate the effective-\nness of our attack framework, we employ\nHarmBench-LLaMA-2-13B-CLS as the evaluator.\nHarmBench provides a\nwell-trained LLM-based classifier for assessing\nwhether the generated outputs constitute successful\nattacks, i.e. whether the attacker bypasses the tar-\nget LLM's safety mechanisms and obtains harmful\ncontent. The evaluation process aligns with the\nHarmBench framework, ensuring reproducibility\nand comparability with related research. We\npresent the implementation details in Appendix A."}, {"title": null, "content": "Metrics. We employ the Attack Success Rate\n(ASR) as the primary metric, which is common-\nused in jailbreak evaluation. It measures the pro-\nportion of generated outputs that are classified as\nharmful by the evaluator. Formally, for an attack\nquery set Q and the corresponding generated con-\ntent G, the ASR is computed as:\n$ASR = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}(Evaluator(Q_i, G_i) = Harmful)$, (4)\nwhere \"Evaluator\" is the discriminator, and $\\mathbb{I}(\u00b7)$ is the in-\ndicator function.\nDatasets. To evaluate our proposed method, we\nadopt the experimental setup of and leveraging three pub-\nlicly available datasets for comparative analysis:\n1) Harmbench, which con-\ntains 510 harmful behaviors annotated with fine-\ngrained labels. We use the \u201cstandard behavior\"\nsubset in the test volumn of Harmbench, which\ncontains 159 samples; 2) advBench comprising 520 adversarial attack prompts;\n3) JailbreakBench, featuring\n100 labeled harmful queries designed to test model\nrobustness.\nBaselines and LLMs. To demonstrate the per-\nformance of our proposed framework, we compare\nAPT with three previous representative jailbreak\napproaches: GCG generates\ntoken-optimized adversarial suffixes to elicit harm-\nful outputs; PAIR refines at-\ntack prompts iteratively through attacker-simulator\ndialogue; JailMine automates\nmalicious response mining via iterative rejection\nsuppression. We evaluate these methods on five\nwidely-used and open-sourced LLMs: Llama2-7B-\nChat and Llama2-13B-Chat, and Qwen-\n7B-Chat/Qwen-14B-Chat."}, {"title": "4.2 Experimental Results", "content": "Our evaluation across three benchmark datasets\nreveals that APT achieves state-of-the-art ASR\nscore compared to baseline methods. As shown\nin Table 1, Table 2 and Table 3, APT demonstrates\nconsistent improvements over existing approaches,\nwith average ASR gains of 2%~3% across all\nbenchmarks compared to the strongest baseline\n(JailMine). Notably, APT achieves near-perfect\nsuccess rates (97%~99% ASR) on Llama, Mistral"}, {"title": null, "content": "and Qwen series models for AdvBench queries,\nsuggesting these models remain highly vulnera-\nble to structured output manipulation despite their\nsafety alignment. This persistent vulnerability\nacross model architectures, parameter scales, and\nsafety training paradigms underscores the funda-\nmental limitations of current refusal pattern imple-\nmentations.\nNotably, we observe that attack approaches with\nexplicit criteria from judge models (e.g., JailMine\nand APT) exhibit substantially higher ASR on Ad-\nvBench and JailbreakBench compared to Harm-\nBench (20%~22%'s gap). This gap may arise from\nthe complexity of defining harmful behavior. Specifically, HarmBench contains\nmore diverse examples, including scenarios that are\nnot strictly illegal or contextually harmful. Con-\nsequently, the Llama2-Guard judge model fails to\nflag certain responses as unsafe under its predefined\ncriteria, preventing their incorporation as positive\nnodes in APT. This phenomena emphasizes future\nresearch to investigate jailbreak mechanisms across\nbroader and more diverse context and develop adap-\ntive safety assessment mechanisms."}, {"title": "4.3 Parameter Analysis", "content": "There are a few hyper-parameters in our frame-\nwork. We conduct an analysis experiment to study\nthe impact of the beam size setting. We employ\nJailbreakBench and a series of LLMs (including"}, {"title": "4.4 Attack on Reasoning Models", "content": "Recent advances in test-time reasoning models\ndemonstrate enhanced problem solving capabil-\nities through very long thought processing. To\nassess their vulnerability in context of structured\noutput, we evaluate DeepSeek-R1 under\nour APT framework. We conduct experiments\non AdvBench and JailbreakBench under two at-\ntack settings: (1) direct manipulation of reasoning\ntraces, i.e. the content inside <think> tags; (2)\nsuppression of refusal patterns in final outputs af-"}, {"title": null, "content": "ter thinking processing. Table 4 summarizes the\nresults."}, {"title": "5 Conclusion and Discussion", "content": "In this paper, we reveal a new threat model that\nstructured output interfaces in LLMs could intro-\nduce critical vulnerabilities to jailbreak attacks.\nOur proposed AttackPrefixTree (APT) framework\ndynamically exploits safety pattern prefixes and\nconstrained decoding to bypass safeguards, achiev-\ning higher ASR on HarmBench, advBench, and\nJailbreakBench than existing methods. This ex-\nposes a fundamental conflict between token-level\ninference and sentence-level safety alignment, indi-\ncating that LLM providers and developers should\nconsider enhancing security protocols for struc-\ntured outputs to balance utility with adversarial\nresilience.\nTo counter structured output-based jailbreak at-\ntacks, the service vendors could implement real-\ntime constrained decoding monitors to detect ad-\nversarial pattern manipulation, and using dynamic\nrefusal template diversification to prevent attackers\nfrom reliably suppressing patterns of safety refusal\nresponses. Hybrid strategies that integrate input-\noutput consistency verification with adaptive logit\nmasking during constrained decoding could further\nenhance model robustness without compromising"}, {"title": null, "content": "the utility of structured generation. These proposed\ndefenses offer a promising solution to the chal-\nlenges outlined in this paper, ensuring the preser-\nvation of functional structured output capabilities\nwhile addressing the identified vulnerabilities."}, {"title": "6 Limitations", "content": "Our proposed method does not consider and opti-\nmize for efficiency optimization in structured de-\ncoding. Constructing a full APT with big beam\nsize is leading to significant time and token over-\nhead. Meanwhile, during the online construction\nof the FSM to suppress negative safety patterns,\nthe processing time increases with the complex-\nity and length of the patterns. Specifically, when\nthe number of negative pattern prefixes exceeds\n30, the construction time becomes intolerable long\nand may even result in service timeouts. Since\nthis paper primarily focuses on potential security\nvulnerabilities arising from new popular model ser-\nvice features (i.e., structured output interfaces), we\nleave further research on efficiency improvements\nfor constrained decoding to future work.\nIn this work, we measure the success of at-\ntacking through the referee model's evaluation\n(HarmBench-CLS), without validating the content\nof jailbroken responses, which may contain hallu-\ncinations. However, from the perspective of the\ndefenders, both hallucinated content and de facto\nharmful outputs should be rejected. Our findings\nreveal potential vulnerabilities in LLM service that\nexpose logit manipulation interface like structured\noutput can be exploited by adversaries for jailbreak-\ning.\nAnother limitation is that our approach achieves\na limited improvement in ASR compared to previ-\nous SOTA methods. In fact, existing approaches\nthat use judge models to detect refusal behaviors\nand then musk those patterns show similar perfor-\nmance under the identical benchmarks and config-\nurations. The primary contribution of our work\nis introducing a new threat model that leverages\nthe recent fashion feature, i.e. structured output,\nand building APT framework to demonstrate it.\nRather than solely aiming for higher ASR, our goal\nis to reveal hidden risks in current safety-aligned\nLLMs and provide practical advice for developers\nto strengthen defenses against evolving jailbreak\nattacks."}, {"title": "Ethics Statement", "content": "This study is dedicated to investigating potential\nLLMs' jailbreak vulnerabilities in structured out-\nput scenarios, while providing protective strategies\nfor LLM vendors and developers (a.k.a. red team-,\ning). Our objective is to mitigate the risks of LLMs\nbeing compromised by jailbreak attacks across vari-\nous application scenarios, thereby enhancing model\nsecurity and reliability to make positive contribu-\ntions to both the LLM research community and\nsociety. It should be noted that our research, in-\ncluding the evaluated benchmark datasets, contains\nsamples that may potentially be harmful, offen-\nsive, or inappropriate. These samples are strictly\nused for research purposes to expose and assess\nidentified vulnerabilities in LLM structured output\nframeworks, and inform the development of secu-\nrity solutions for LLM practitioners. They are not\nintended for malicious citation or descriptive pur-\nposes. The authors have conducted rigorous ethical\ndeliberations throughout this research process, in-\ncluding consultations with ethics committees. All\npotentially harmful content samples have under-\ngone strict censor procedures, and very sensitive\nmaterials were properly redacted or removed prior\nto publication."}, {"title": "A Implementation Details", "content": ""}, {"title": "A.1 Infrastructure Configuration", "content": "Our experiments were conducted on 2 compute\nnodes, each equipped with 4\u00d7NVIDIA A100\n40GB GPUs and 256GB system RAM. We de-\nployed vLLM 1.0 as the infer-\nence server, leveraging its continuous batching and\nPagedAttention optimizations to accelerate the gen-\neration. Structured output backend is enforced via"}, {"title": "A.2 Model Configuration", "content": "All tested LLMs used their default or recommended\ncontext window sizes. The temperature in all mod-\nels are setting as 0.8 across experiments to balance\noutput diversity and coherence. For safety eval-\nuation, judge model Llama-Guard-2 and referee\nmodel HarmBench-CLS ran on the same infrastruc-\nture with FP16 quantization to minimize latency\nduring harmfulness classification. The beam size of\nAPT is set as 5, max negative pattern size is set as\n30, and max_token in APT (i.e. the max sampling\nlength along each branches) is set as 200."}]}