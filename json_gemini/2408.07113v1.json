{"title": "A Theory-Based Explainable Deep Learning Architecture for Music Emotion", "authors": ["Hortense Fong", "Vineet Kumar, K. Sudhir"], "abstract": "This paper develops a theory-based, explainable deep learning convolutional neural network (CNN) classifier to predict the time-varying emotional response to music. We design novel CNN filters that leverage the frequency harmonics structure from acoustic physics known to impact the perception of musical features. Our theory-based model is more parsimonious, but provides comparable predictive performance to atheoretical deep learning models, while performing better than models using handcrafted features. Our model can be complemented with handcrafted features, but the performance improvement is marginal. Importantly, the harmonics-based structure placed on the CNN filters provides better explainability for how the model predicts emotional response (valence and arousal), because emotion is closely related to consonance\u2014a perceptual feature defined by the alignment of harmonics. Finally, we illustrate the utility of our model with an application involving digital advertising. Motivated by YouTube's mid-roll ads, we conduct a lab experiment in which we exogenously insert ads at different times within videos. We find that ads placed in emotionally similar contexts increase ad engagement (lower skip rates, higher brand recall rates). Ad insertion based on emotional similarity metrics predicted by our theory-based, explainable model produces comparable or better engagement relative to atheoretical models.", "sections": [{"title": "Introduction", "content": "Music is widely regarded as among the most effective and efficient of channels to influ-ence emotion; it is often called the language of emotion (Corrigall and Schellenberg 2013).As emotions play a central role in many elements of marketing and consumer behavior,marketers routinely use music to evoke emotion along the customer journey, from needrecognition to purchase, in advertising, content marketing, and stores (Gorn 1982, Krishna2012). In particular, music is almost universally used in advertising and advertisers spendconsiderable effort crafting it to elicit desired emotions and consequent marketing out-comes.\u00b9 As such, a tool that maps music (or an ad with music) to its evoked emotioncan be valuable to marketers. For example, marketers can use such a method to automateemotion-based contextual matching of ads and content at scale to increase ad engagement.On music and video platforms, it can be valuable in automating creation of mood-basedplaylists and \"next song\" recommendations for users from their large collections.\u00b2In this paper, we develop a theory-based, explainable deep learning convolutional neuralnetwork (CNN) classifier that takes music audio as input and predicts the sequence ofemotions it evokes in a listener. We characterize emotion using the well-established valence-arousal framework developed by Russell (1980). Valence measures how positive or negativea listener feels and higher valence maps to more positive feeling. Arousal measures howenergetic a listener feels and higher arousal maps to greater excitement and energy.\u00b3Existing music emotion classifiers can be grouped into two types based on explainabilityand accuracy, and there is often a tradeoff between these. Classifiers using handcraftedfeatures and traditional machine learning methods are typically more explainable, butoften less accurate whereas classifiers using data-driven variables and \u201catheoretical\" deeplearning methods are typically less explainable but more accurate. By incorporating theoryinto the model design, our deep learning model seeks to be both accurate and explainable.We highlight and explain the two key distinguishing features in the modeling contribu-tion. First, and most important, we incorporate music theory based on harmonics in themodel. When a musical note is generated, its pitch is characterized by its fundamentalfrequency (i.e., lowest frequency) and harmonics refer to the whole number multiples ofthe fundamental frequency. We design harmonics filters to capture theoretically knownmusical features that link to emotion.\u2074 Such filter construction is challenging because cur-rent CNN implementations for music are based on adaptations of models developed forcomputer vision where spatial contiguity is meaningful. In music, however, many important"}, {"title": "Related Literature", "content": "Our paper builds upon several distinct streams of literature across fields, as detailed below.\nMusic induces emotion, as shown by a wide literature using methods ranging from surveys\nto brain scans (Johnson-Laird and Oatley 2016). Since our focus is on the background\nmusic of content and ads, which typically falls under Western tonal music (Stoppe 2014,\nNelson et al. 2013), our focus in this paper is on the musical associations between Western\ntonal music and emotion.12 In particular, consonance and dissonance play a major role in\ncreating emotion in background music. According to Nelson et al. (2013), the manipulation\nof consonant and dissonant harmonies \u201cis the most immediately effective way a composer\ncan alter the mood of a scene.\" In Western tonal music, consonance is associated with\npositive valence and low arousal emotion, while dissonance is associated with negative\nvalence and high arousal emotion (Gabrielsson and Lindstr\u00f6m 2010, Thompson et al. 2010).\nAppendix C discusses how timbre and pitch also impact emotional response.\nEmotional responses to music have marketing implications, and a substream of literature\nfocuses on the relationship between music features and marketing outcomes. Bruner (1990)\noverviews how music elicits different moods, which in turn impact ad outcomes. Yang\net al. (2022) use low-level acoustic features to predict ad audio energy levels and find that\nenergetic commercials are more likely to be watched for longer. Boughanmi and Ansari"}, {"title": "Machine Learning with Unstructured Audio Data", "content": "While audio includes both speech and music, our focus in this paper is on music. Traditional\nmachine learning methods, like SVM, previously produced good classification performance\nin many settings by using handcrafted features, such as mel frequency cepstral coefficients\n(MFCCs). However, the performance of deep learning algorithms has overtaken almost all\nother methods in audio applications, similar to vision applications (Hinton et al. 2012).\nThe crucial advantage that deep learning has is that features are automatically learned\nfrom data, rather than predefined (Choi et al. 2017a).\nWith deep learning, music is typically converted to a spectrogram, which is used as\nthe input to the learning algorithm. A few researchers have attempted to build music-\nspecific deep learning models rather than use CNN models designed for image recognition.\nFor example, to predict ballroom music genre, Pons et al. (2016) suggest using musically-\nmotivated CNN filters to capture low-level timbral and temporal elements of music. This\ntranslates to using various rectangular convolutional filters\u2014tall and skinny filters for tim-\nbral elements and short and wide filters for temporal elements. Using data with labeled\nemotion and mid-level features (e.g., melodiousness, articulation), Chowdhury et al. (2019)\nbuild a deep learning model that includes an interpretable mid-level feature layer to predict\nemotion. We contribute to the audio machine learning literature by designing and devel-\noping novel CNN filters based on harmonics, and demonstrate how the filters are useful\nfor prediction as well as explanation."}, {"title": "Ad Insertion in Videos", "content": "An evolving literature has examined video ads within streaming videos to understand what\nad, content video, and user characteristics impact ad performance. For a survey of this\nliterature, see Frade et al. (2021).\u00b9\u00b3 The typical outcomes examined in studies of video ads"}, {"title": "Deep Learning Model for Music Emotion", "content": "We develop a CNN for emotion classification that incorporates theory relating to the\nphysics of sound waves and the perception of Western tonal music by listeners.\nFigure 1 overviews the steps of our deep learning model that maps music to emotion. Step\nS1 takes six seconds of raw audio sound wave data as input.\u00b9\u2074 In Step S2, the music\nclip is converted to a short-time Fourier transform (STFT) spectrogram. In Step S3, we\ntransform the STFT spectrogram to a mel spectrogram, which characterizes how the sound\nis perceived by the human ear. In Step S4, the mel spectrogram is used as a visual input"}, {"title": "Physical Properties of Sound Waves", "content": "Music (or any sound) is a pressure wave\nthat travels through the air until it reaches the listener's ear. The waveform illustrated in\n(S1) of Figure 1 graphs the change in air pressure at a certain location over six seconds\n(M\u00fcller 2015). Audio data can be represented in a number of ways. While a waveform is\none way to visually represent sound, it does not model how humans hear sound, which is\nbased on the underlying frequencies. \u00b9\u2075"}, {"title": "Short-Time Fourier Transform Spectrogram", "content": "A spectrogram visualizes frequency\nand time features of audio data (M\u00fcller 2015). The STFT spectrogram, which is produced\nby taking the Fourier transform of short overlapping time windows of the waveform to\ndecompose it into its individual frequencies and their respective magnitudes, maps the\nsquared magnitude of each frequency over time. The parameters that go into generating an\nSTFT spectrogram are the sampling rate, window type and size, and hop length.16 Let x\nrepresent the discrete-time signal of the audio signal, w the window function, which takes\nin N samples, and H the hop size. The window function specifies how we weight the audio\nsignal within each window of time and the hop size specifies how many samples we jump\nbetween each window. The discrete STFT X of signal x is:\n$$X(m,k) := \\sum_{n=0}^{N-1} x(n+mH)w(n)exp(-2\\pi i kn/N),$$\nwhere m is the time index, k \u2208 [0 : K] is the frequency index, and i := \u221a\u22121. A sampling rate\nof 44,100 Hz generates a spectrogram that extends up to 22,050 Hz. The STFT spectrogram\ncan then be written as:\n$$S(m,k) := |X(m,k)|^2.$$\nThe magnitude of the complex number X (m, k) captures the presence of each frequency\nat each time sample. Squaring the magnitude yields the power of each frequency at each\ntime sample. We generate an STFT spectrogram for each six-second clip of music. The\nresulting frequency \u00d7 time dimensions of the STFT spectrograms are 2049 \u00d7 517.\u00b9\u2077\nThe STFT spectrogram of Figure 2a is represented in Figure 3a, with the x-dimension\nrepresenting time, the y-dimension frequency, and color the power of each frequency bin at\neach time sample (red (blue) is high (low) power). The frequency and time dimensions are\ndiscretized since we are working with a digital signal. In the STFT spectrogram, frequency\nand time are shown on linear scales while power is shown on a log scale and measured in\ndecibels (dB) since humans perceive volume on a log scale.\u00b9\u2078 In Figure 3a, the large patch\nof blue before second three indicates a lack of high frequencies early in the music."}, {"title": "Mel Spectrogram based on Auditory Perception", "content": "Humans are better at perceiving\nfrequency differences at low pitches than at high pitches (M\u00fcller 2015).\u00b9\u2079 With equal sen-\nsitivity across the frequency spectrum, the STFT spectrogram does not represent human"}, {"title": "Incorporating Harmonics-based Structure into CNN Filter Design", "content": "Convolu-\ntion filters in CNNs are matrix operations learned from the data with the goal of correctly\nmaking predictions. In designing convolution filters to learn features to classify music emo-\ntion, we must account for how humans perceive sound and in particular the non-contiguous\nstructure of many musical features. We know from acoustic physics that harmonic fre-\nquencies underlie important mid-level musical features, like consonance, timbre, and pitch,\nwhich in turn relate to emotional response. We incorporate the harmonics-based structure\ninto the CNN filters with two goals in mind: 1) relevant mid-level features are parsimo-\nniously captured and 2) the model is more explainable relative to atheoretical deep learning\nmodels. We detail the concepts of harmonics and pitch class and how they connect to the\ndevelopment of theory-based filters next."}, {"title": "Harmonics", "content": "The STFT from (S2) decomposes the sound wave into its constituent sine\nwaves, known as partials. The harmonics of the sound wave are the partials that are"}, {"title": "Pitch Class", "content": "We organize the harmonics-based filters around the 12 pitch classes of\nWestern music\u2014\u0410, \u0410\u2021, B, C, C\u2021, D, D\u2021, E, F, F\u2021, G, G\u2021\u2014since instruments are tuned to\nthese pitch classes.\u00b2\u00b9 Table 1 lists the fundamental frequency of the lowest pitch in each\npitch class for nearly all instruments within human hearing, and we use these frequencies\nto construct the filters."}, {"title": "Harmonics Filter Design", "content": "We now detail the steps to develop the harmonics-based convo-\nlution filter for a given pitch class. The unique characteristic of the filter is that it focuses\non specific non-contiguous regions of the spectrogram to flexibly learn a wide variety of\nfeatures. The filter uses \u201cblinders\u201d to select frequencies input into the CNN, and a con-\nvolution size that considers a large range of frequencies at each time frame. The term\n\"blinders\" refers to the matrix operation that selects and weights the mel bands in the mel\nspectrogram prior to convolution, whereas \u201charmonics filter\" refers to the combination of\nthe blinders with convolution. Steps (i) - (v) below are done separately for each pitch class."}, {"title": "Calculate pitch class filter frequencies", "content": "We use the set of frequencies defined in eq. (3)\nto design blinders that retain only the frequencies of interest. Beginning with the lowest\nfundamental frequency of pitch class pin Table 1, we calculate the set of harmonic fre-\nquencies associated with p. For example, the lowest A has a fundamental frequency of"}, {"title": "Calculate frequency bands", "content": "We calculate frequency bands around each wn. The under-\nlying rationale is that the human ear cannot distinguish frequencies within a small band.\nThe exact size of the band depends on a number of factors, including duration, intensity,\nand frequency, so the width of the bands does not follow a rule. We calculate 1 Hz bands\ncentered around each frequency wn."}, {"title": "Calculate STFT indicator column", "content": "We identify the match between STFT bins and\nthe frequency bands corresponding to the pitch class. To match the STFT bins with the\nset of frequency bands, we create an STFT indicator column such that each element is\nequal to one when the center frequency of the STFT bin falls within one of the frequency\nbands. STFT bins that are not close to the frequency bands may not be chosen."}, {"title": "Construct mel blinders", "content": "We multiply the STFT indicator column by the mel filter\nbank to generate a mel weight column that has Nmel = 256 dimensions. Repeating the mel\nweight column over the time dimension generates the mel blinders. Finally, we multiply\nthe mel spectrogram by the mel blinders to generate the input to the CNN's convolution\nlayer. Appendix E details the steps."}, {"title": "Implement convolution filter", "content": "There are a few others choices to be made before we\ncan directly apply the convolution filter. For a spectrogram, the convolution filter height\ndetermines the number of frequency bins included and the width determines the number of\ntime frames. It is common practice to design the convolution filter to also have a depth (i.e.,\nmultiple channels) so that multiple features can be learned simultaneously.\u00b2\u00b2 The stride\nspecifies how much the filter slides over the image before performing another operation.\u00b2\u00b3\nSince the perception of harmonics is based on the interaction of all frequencies at any given\ntime, we set the filter height to the height of the spectrogram and both the filter width"}, {"title": "Convolutional Neural Network Architecture", "content": "Our CNN to predict music emotion\nconsists of multiple types of layers, including a convolutional layer, pooling layers, and a\nfully connected layer. Designing a neural network involves several architectural and hyper-\nparameter choices. We use standard architecture choices as appropriate, and describe the\nreasoning in Appendix F. Figure 4 summarizes the overall architecture. The operational-\nization of the model with harmonics filters is as follows:\n1. For each pitch class:\n (a) Apply the non-contiguous harmonics filters from Step (S4).\n (b) Batch normalize and take ReLU of the feature maps.\n (c) Average pool over time frames and apply dropout.\n2. Concatenate the hidden layers generated by each of the pitch classes.\n3. Max pool over the pitch classes and apply dropout.\n4. Use one fully connected layer and apply softmax to output a probability distribution\n over the four emotion quadrants.\n5. Output the quadrant Q1, Q2, Q3 or Q4 with max probability as the predicted label.\nThe emotion prediction problem aims to classify an input music clip into one of four\nvalence-arousal quadrants, i.e., a multiclass classification problem. The objective function\nof the model is to minimize the cross-entropy loss between the predicted and actual outputs.\nThe cross-entropy loss LCE over the set of music clips is:\n$$L_{CE} = -\\sum_{i=1}^{N}\\sum_{k=1}^{4} Y_{ik}log(p(\\hat{Y}_{ik}))$$"}, {"title": "Predicted Emotion", "content": "The model maps each six-second clip into a valence-arousal\nquadrant. Q1 captures positive valence-high arousal, Q2 captures negative valence-high\narousal, Q3 captures negative valence-low arousal, and Q4 captures positive valence-low\narousal. To provide a reference emotion for each quadrant, we borrow the notation from\nPanda et al. (2018): Q1\u2014exuberance, Q2\u2014anxiety, Q3\u2014sadness, Q4-contentment. The\npredictions over time characterize the dynamics of music emotion.\u00b2\u2077"}, {"title": "Explainability", "content": "To gain visibility into what the model learns and uses for emotion classification, we use\ngradient-weighted class activation mapping (Grad-CAM). Grad-CAM uses the gradients of\na target class (e.g., Q1) with respect to feature maps, which flow into a given convolutional\nlayer, to produce a heatmap that highlights the regions of the input image that positively\npredict the class (Selvaraju et al. 2017).\u00b2\u2078 Since there is a filter and therefore feature map\nfor each pitch class, we obtain a Grad-CAM heatmap for each pitch class, which are 1 \u00d7 517\nimages (517 representing the number of time samples). The heatmap brightness value for\nclip k and pitch class pat each time sample t is:\n$$b_{kpt}^j = ReLU\\Big(\\sum_f \\alpha_f^j A_{kpt}^f\\Big) \\text{ where } \\alpha_f^j = \\frac{1}{Z} \\sum_t \\frac{\\partial y^j}{\\partial A_{kpt}^f},$$\nIn this expression, j represents the class (quadrant), Af the feature map for channel f at\ntime t, y the score for class j, and Z the feature map size (517).\u00b2\u2079 The brightness value is\na linear combination of the feature maps Af and their importance \u03b1f in predicting class j.\nWe stack the heatmaps associated with each of the 12 pitch classes to obtain an overall\nheatmap in which the y-axis represents the 12 pitch classes and the x-axis represents time.\nColor denotes bjkpt , which captures the importance of the non-contiguous frequencies within\neach pitch class towards the classification into quadrant j, and brighter colors capture\nlarger values. Let B = pt bept represent the sum of the heatmap brightness values over\nall pitch classes and time samples with respect to quadrant j for clip k. Let Bj = \u2211k Bjk\nrepresent the average brightness of all clips with true quadrant label j.\u00b3\u2070\nCombining the structure of the harmonics filters with the Grad-CAM heatmaps enables"}, {"title": "Empirical Analysis", "content": "We begin by describing the datasets used to train the models. We then report the perfor-\nmance of our proposed architecture with harmonics filters and compare it against bench-\nmark models proposed in the literature. Finally, we show how our model is explainable\nusing gradient-based model visualizations and compare it to visualizations generated by\nother CNN models which use atheoretical and low-level filters."}, {"title": "Datasets", "content": "We combine two public datasets compiled by music emotion researchers, which serve com-\nplementary purposes in our analysis: Soundtracks (Eerola and Vuoskoski 2011) and the\nMediaEval Database for Emotional Analysis in Music (DEAM) (Aljanaki et al. 2017).\nThe Soundtracks dataset consists of 360 excerpts from movie soundtracks that range\nin duration from 10 to 30 seconds. One benefit of movie soundtracks is that they are\ncomposed to elicit emotion. The music clips are instrumental and do not contain any lyrics,\ndialogue, or sound effects. The clips were chosen to: 1) elicit either a discrete emotion\nor to be high or low on valence, arousal, or tension, 2) evoke only a single emotion over\nthe length of the clip, and 3) be unfamiliar to prevent song familiarity from impacting\nemotion tagging. University students and staff with musical expertise annotated the song\nemotions, and six annotators tagged each music excerpt.\u00b3\u00b9 Perceived discrete emotions and\nvalence and arousal were separately annotated on a scale of 1 to 7. Inter-rater consistency\n(Cronbach's alpha) was 0.92 for valence, 0.90 for arousal, and ranged from 0.66 to 0.93\nfor the discrete emotions. We split the excerpts into non-overlapping six-second segments.\nExcerpts selected for the discrete emotions happy, sad, tender, fear, and anger are mapped\nonto the valence-arousal quadrants such that happy maps to Q1, fear and anger to Q2, sad"}, {"title": "Model Performance", "content": "We use precision, recall, and F1-score, standard measures in the machine learning litera-\nture, to evaluate our model. We calculate these metrics for each class (quadrant) and a\nweighted average by the number of samples in each class determines each overall measure.\nTable 2 summarizes the performance of the models. The performance metrics are aver-\naged over each fold of the held-out test data from stratified 10-fold cross-validation.\u00b3\u00b3 The\nstandard deviations of the performance measures over the ten folds are in parentheses."}, {"title": "Explainability of Harmonics Filters", "content": "In \u00a73, we discussed the patterns to expect\nin terms of the brightness of Grad-CAM heatmaps based on theory related to harmonics,\nperception of consonance, and emotional response to consonance. We illustrate explain-\nability around the role of consonance in two ways. First, we compare the heatmaps with"}, {"title": "Limited Explainability of Atheoretical and Low-Level Filters", "content": "Grad-CAM\nvisualizations can also be produced for the other filter types. We find that given their low-\nlevel focus without specific theory to guide our expectations, it is more difficult to interpret\nwhat musical features are captured and connect how they contribute to the classification\nof a particular class. The Grad-CAM heatmaps for the square filter CNN in Figure 7 are\nequivalent to heatmaps produced for an image recognition model. They show which con-\ntiguous regions of the input image contribute to the classification of a particular class,\nproviding an idea of the range of frequencies and times contributing to the classification.\nHowever, it is unclear what human-understandable musical features are being learned.\nNote that in contrast to our harmonics filters, pitch classes are not used here.\nThe heatmaps based on separate time and frequency filters highlight which time periods\nor frequencies contribute most to a particular classification (see Appendix Figures Kla\nand K1b). Similar to square filters, it is less clear how to interpret the heatmaps, mak-\ning it challenging to understand what musical features the models learn. In the models\nusing atheoretical and low-level filters, it is more challenging to determine what musical\nfeature is learned and used for emotion classification. Thus, our contribution in incorporat-\ning the non-contiguous theory-based harmonics filters is to provide greater explainability,\nspecifically with consonance, while obtaining similar performance as atheoretical filters."}, {"title": "Model Capability to Learn Features", "content": "While our discussion of explainability has primarily focused on a specific and important\nfeature, i.e., consonance, harmonics filters can also learn other features, as foreshadowed\nin \u00a74.3. For instance, the combination of harmonic frequencies provides information about\npitch perception, timbre (or tone color), and spectral spread and complexity (McAdams\nand Giordano 2014).\nThe comparable classification accuracy between the theory-based CNN with harmonics\nfilters and the models with atheoretical filters suggests that the harmonics filters are learn-\ning multiple musical features relevant for emotion classification, since consonance alone\nwould not yield such high accuracy. To assess this systematically, we check whether clas-\nsification performance improves if we add handcrafted features found to predict music"}, {"title": "Application: Emotion-based Ad Insertion in Content Videos", "content": "Our proposed theory-based deep learning model can be used in a number of real-time\nemotion-based applications by predicting the valence and arousal of music clips. We demon-\nstrate the value with an illustrative application involving emotion-based contextual tar-\ngeting, where the algorithm determines the optimal emotion-based ad insertion point for a\nvideo ad within a content video (e.g., YouTube video) with time-varying emotional content.\nSuch emotion-based contextual targeting with automated content matching is gaining\nimportance. Increasing privacy restrictions limit person-specific targeting of advertising,\nmaking contextual and content-based targeting for ad placement more relevant and use-\nful.\u2074\u00b2 Further, given the vast amount of UGC available, non-algorithmic approaches to\nmatching ads with content is challenging, if not impossible, to implement at scale. The\nsize of the matching problem is very large, and a platform like YouTube needs to match\nbillions of ads and content videos daily.\nThroughout a content video, emotion often varies over time and so the various ad inser-\ntion slots differ in emotion. Since ads also often elicit emotion, we seek to understand how\nto match the ad to the insertion slot based on emotion. While marketing researchers have\nconsidered the overall emotion of content videos for ad matching (Coulter 1998, Kamins\net al. 1991, Puccinelli et al. 2015, Kapoor et al. 2022), our focus is on automatically iden-\ntifying the optimal ad insertion slot within videos that vary in emotion over time.\nDoes emotional congruence or contrast work better for ad insertion? It is an empirical\nquestion as to whether ads that are similar to the emotional context of the content video"}, {"title": "Experiment: Is Emotional Congruence or Contrast More Effective?", "content": "We discuss the experimental design and results below."}, {"title": "Experimental Setup", "content": "There are multiple ad insertion points, which vary in\nevoked emotion, within each content video. The outcome variables of interest are ad skip\n(as a proxy for attention and interest) and brand recall (as a proxy for memorability). We\nuse a full factorial design across four ads, four content videos, and six ad insertion points\nper content video, yielding a total of 96 experimental cells.\nWe develop a Qualtrics survey that shows an ad inserted partway through a content\nvideo, mimicking the concept of YouTube's mid-roll ads. Similar to YouTube, a \u201cSkip Ad\""}, {"title": "Content Videos and Ads", "content": "We select a diverse set of content videos and ads.\nThe content videos are selected to a) contain background music most of the time and b)\nbe long enough to vary in emotion over time and allow for multiple ad insertion points.\nThe videos range from 5.7 to 7.9 minutes in length, include both animated and live-action\nvideos, and include videos with and without speech. Appendix Table L1 provides details\non the four content videos. For each content video, we fix six ad insertion points (Time 1,\n..., 6) that are roughly one-minute apart and occur at natural changes in the audio and\nimages, similar to YouTube.\u2074\u2074 Appendix Table L2 specifies the insertion times.\nWe select four ads such that each of the four valence-arousal quadrants is represented\nby the primary emotion in the first six seconds of one of the ads. The emotion of the first\nsix seconds is important, since viewers on YouTube have the option to skip at six seconds.\nWe seek to understand the interaction effect of the content video emotion with the initial\nad emotion. The ads include background music, are 30 seconds long, and cover a range of\nindustries.\u2074\u2075 Appendix Table L3 provides details on the four ads."}, {"title": "Emotion Tagging", "content": "Since we seek to measure emotional distance, we must first character-\nize the emotion of the content videos and ads. Within the experiment, we use human-tagged\nemotion as the ground truth. To obtain the emotion tags, we recruit survey participants on\nProlific, an online survey platform. We show respondents either the content videos in seg-\nments (as defined by Appendix Table L2) or the first six seconds of the ads and ask them\nabout their valence and arousal levels after watching each clip.\u2074\u2076 With multiple tags per\nclip, each clip can then be characterized by the distribution of emotion over the quadrants."}, {"title": "Emotional Distance Measure", "content": "To measure emotional distance between an ad\nand an insertion point in the content video, we calculate the Jensen-Shannon (JS) distance.\nThe JS distance is computed between their probability distributions over the emotion\nquadrants. Let Pt represent the emotion probability distribution of the content video at\ntime t, and Q the emotion probability distribution of the ad. JS distance is defined as:\n$$JSD(P||Q) = \\sqrt{\\frac{1}{2}D(P||M)+\\frac{1}{2}D(Q||M)}$$\nwhere M = \\frac{1}{2}(P + Q) and D is the Kullback-Leibler (KL) divergence. The benefit of JS\ndistance over KL divergence is that it is symmetric between Pt and Q and always finite.\nThe larger the JS distance, the more dissimilar the ad emotion is from the content video\nemotion at time t."}, {"title": "Outcomes of Interest", "content": "Our dependent variables are \u201crevealed preference\u201d-type\nmetrics and of significant interest to advertisers. Ad skip captures whether someone skips\nor voluntarily continues to watch the ad. Unaided brand recall captures whether someone\npaid attention to the ad and its memorability.\nAn ad skip occurs if the participant presses the \u201cSkip Ad\u201d button within five seconds\nof its appearance.\u2074\u2077 This definition captures the emotion interaction of the content video\nat the time of ad insertion and the first six seconds of the ad. We capture recall in the\nset of questions asked to participants after watching the content video. In the survey, the\nbrand shows as video information when the ad begins and disappears after three seconds.\nTherefore, even if participants skip the ad, they are still exposed to the brand."}, {"title": "Experimental Results", "content": "Each participant is randomly assigned to one of the 96\ncells.\u2074\u2078 Across all content videos, ads, and insertion points, 1,413 participants on average\nskipped 42.0% of ads (viewed 58.0%) and correctly recalled 45.2% of brands. As expected,\nthe correlation between skip and recall is negative, with a value of -0.23 (p-value < 0.01)."}, {"title": "Ad Insertion Automation", "content": "Our theory-based and benchmark models can predict the emotion distributions of the ads\nand content to calculate emotional distances at scale. We use these to compare ad skip and\nrecall outcomes from showing each ad at the most emotionally similar ad insertion point\nbased on the predictions of the models."}, {"title": "Calculating Model-predicted Emotional Distances", "content": "We transform the first six\nseconds of audio of each ad and 30 seconds of audio before each ad insertion point in the\ncontent videos into mel spectrograms. For the content videos, we break the 30-second clips\ninto five six-second clips. We use the models to predict the emotion distribution of each\nsix-second clip.\u2075\u2070 For the 30-second content video clips, we average over the five predicted\nemotion distributions associated with each six-second clip. Using the 24 content emotion\ndistributions (four content videos \u00d7 six insertion points) and four ad emotion distributions,\nwe calculate the JS distances between the ads and the content at the six insertion points.\nFor each combination of model, content video, and ad, we determine which ad insertion\npoint is the most emotionally similar.\u2075\u00b9"}, {"title": "Skip and Recall Rates", "content": "From the experiment, we have the skip and recall rates\nfor the 96 experimental cells. For each model, we average the skip and recall rates as well as\nthe ground-truth JS distances (based on human-tagging) of the most emotionally similar\ninsertion point for each of the four ads in each of the four content videos as predicted by\neach model and show the results in Table 5. Note that these average skip and recall rates\nare based on a simulation of ad insertion automation using the experimental data.\nOur proposed theory-based model using harmonics filters selects insertion points that\nare relatively high in emotional similarity (i.e., low JS distance) compared to the other\ndeep learning models. These ad insertion points generate relatively favorable skip and recall\nrates, suggesting that our model can be useful in automated emotion-based ad insertion.\nThe models that includes tempo features, the best performing model from \u00a74.4, further\nimproves upon these results."}, {"title": "Incorporating Other Video Modalities", "content": "Our primary analysis has focused on\nusing emotion evoked from music. However, with videos, emotional content may be present\nacross multiple modalities (e.g., facial expressions, text of speech). Multimodal emotional\ncontent can also be used to predict emotional distance. When the videos have human faces,\nwe can use publicly available tools to estimate emotion from facial expressions. Similarly,\nemotional content can also be obtained from voice tonality and speech text.\nIn our application, we observe that speech or human faces might not be present in each\ncontent video or in the first six seconds of ads, implying using speech or facial emotion\\"}]}