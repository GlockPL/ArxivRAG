{"title": "A Theory-Based Explainable Deep Learning Architecture for Music Emotion", "authors": ["Hortense Fong", "Vineet Kumar, K. Sudhir"], "abstract": "This paper develops a theory-based, explainable deep learning convolutional neural network (CNN) classifier to predict the time-varying emotional response to music. We design novel CNN filters that leverage the frequency harmonics structure from acoustic physics known to impact the perception of musical features. Our theory-based model is more parsimonious, but provides comparable predictive performance to atheoretical deep learning models, while performing better than models using handcrafted features. Our model can be complemented with handcrafted features, but the performance improvement is marginal. Importantly, the harmonics-based structure placed on the CNN filters provides better explainability for how the model predicts emotional response (valence and arousal), because emotion is closely related to consonance\u2014a perceptual feature defined by the alignment of harmonics. Finally, we illustrate the utility of our model with an application involving digital advertising. Motivated by YouTube's mid-roll ads, we conduct a lab experiment in which we exogenously insert ads at different times within videos. We find that ads placed in emotionally similar contexts increase ad engagement (lower skip rates, higher brand recall rates). Ad insertion based on emotional similarity metrics predicted by our theory-based, explainable model produces comparable or better engagement relative to atheoretical models.", "sections": [{"title": "Introduction", "content": "Music is widely regarded as among the most effective and efficient of channels to influ-\nence emotion; it is often called the language of emotion (Corrigall and Schellenberg 2013).\nAs emotions play a central role in many elements of marketing and consumer behavior,\nmarketers routinely use music to evoke emotion along the customer journey, from need\nrecognition to purchase, in advertising, content marketing, and stores (Gorn 1982, Krishna\n2012). In particular, music is almost universally used in advertising and advertisers spend\nconsiderable effort crafting it to elicit desired emotions and consequent marketing out-\ncomes. As such, a tool that maps music (or an ad with music) to its evoked emotion\ncan be valuable to marketers. For example, marketers can use such a method to automate\nemotion-based contextual matching of ads and content at scale to increase ad engagement.\nOn music and video platforms, it can be valuable in automating creation of mood-based\nplaylists and \"next song\" recommendations for users from their large collections.\nIn this paper, we develop a theory-based, explainable deep learning convolutional neural\nnetwork (CNN) classifier that takes music audio as input and predicts the sequence of\nemotions it evokes in a listener. We characterize emotion using the well-established valence-\narousal framework developed by Russell (1980). Valence measures how positive or negative\na listener feels and higher valence maps to more positive feeling. Arousal measures how\nenergetic a listener feels and higher arousal maps to greater excitement and energy.\nExisting music emotion classifiers can be grouped into two types based on explainability\nand accuracy, and there is often a tradeoff between these. Classifiers using handcrafted\nfeatures and traditional machine learning methods are typically more explainable, but\noften less accurate whereas classifiers using data-driven variables and \u201catheoretical\" deep\nlearning methods are typically less explainable but more accurate. By incorporating theory\ninto the model design, our deep learning model seeks to be both accurate and explainable.\nWe highlight and explain the two key distinguishing features in the modeling contribu-\ntion. First, and most important, we incorporate music theory based on harmonics in the\nmodel. When a musical note is generated, its pitch is characterized by its fundamental\nfrequency (i.e., lowest frequency) and harmonics refer to the whole number multiples of\nthe fundamental frequency. We design harmonics filters to capture theoretically known\nmusical features that link to emotion. Such filter construction is challenging because cur-\nrent CNN implementations for music are based on adaptations of models developed for\ncomputer vision where spatial contiguity is meaningful. In music, however, many important"}, {"title": "Related Literature", "content": "Our paper builds upon several distinct streams of literature across fields, as detailed below."}, {"title": "Listener Response to Music", "content": "Music induces emotion, as shown by a wide literature using methods ranging from surveys\nto brain scans (Johnson-Laird and Oatley 2016). Since our focus is on the background\nmusic of content and ads, which typically falls under Western tonal music (Stoppe 2014,\nNelson et al. 2013), our focus in this paper is on the musical associations between Western\ntonal music and emotion. In particular, consonance and dissonance play a major role in\ncreating emotion in background music. According to Nelson et al. (2013), the manipulation\nof consonant and dissonant harmonies \u201cis the most immediately effective way a composer\ncan alter the mood of a scene.\" In Western tonal music, consonance is associated with\npositive valence and low arousal emotion, while dissonance is associated with negative\nvalence and high arousal emotion (Gabrielsson and Lindstr\u00f6m 2010, Thompson et al. 2010).\nAppendix C discusses how timbre and pitch also impact emotional response.\nEmotional responses to music have marketing implications, and a substream of literature\nfocuses on the relationship between music features and marketing outcomes. Bruner (1990)\noverviews how music elicits different moods, which in turn impact ad outcomes. Yang\net al. (2022) use low-level acoustic features to predict ad audio energy levels and find that\nenergetic commercials are more likely to be watched for longer. Boughanmi and Ansari"}, {"title": "Machine Learning with Unstructured Audio Data", "content": "While audio includes both speech and music, our focus in this paper is on music. Traditional\nmachine learning methods, like SVM, previously produced good classification performance\nin many settings by using handcrafted features, such as mel frequency cepstral coefficients\n(MFCCs). However, the performance of deep learning algorithms has overtaken almost all\nother methods in audio applications, similar to vision applications (Hinton et al. 2012).\nThe crucial advantage that deep learning has is that features are automatically learned\nfrom data, rather than predefined (Choi et al. 2017a).\nWith deep learning, music is typically converted to a spectrogram, which is used as\nthe input to the learning algorithm. A few researchers have attempted to build music-\nspecific deep learning models rather than use CNN models designed for image recognition.\nFor example, to predict ballroom music genre, Pons et al. (2016) suggest using musically-\nmotivated CNN filters to capture low-level timbral and temporal elements of music. This\ntranslates to using various rectangular convolutional filters\u2014tall and skinny filters for tim-\nbral elements and short and wide filters for temporal elements. Using data with labeled\nemotion and mid-level features (e.g., melodiousness, articulation), Chowdhury et al. (2019)\nbuild a deep learning model that includes an interpretable mid-level feature layer to predict\nemotion. We contribute to the audio machine learning literature by designing and devel-\noping novel CNN filters based on harmonics, and demonstrate how the filters are useful\nfor prediction as well as explanation."}, {"title": "Ad Insertion in Videos", "content": "An evolving literature has examined video ads within streaming videos to understand what\nad, content video, and user characteristics impact ad performance. For a survey of this\nliterature, see Frade et al. (2021). The typical outcomes examined in studies of video ads"}, {"title": "Deep Learning Model for Music Emotion", "content": "We develop a CNN for emotion classification that incorporates theory relating to the\nphysics of sound waves and the perception of Western tonal music by listeners."}, {"title": "Model Elements", "content": "Figure 1 overviews the steps of our deep learning model that maps music to emotion. Step\nS1 takes six seconds of raw audio sound wave data as input. In Step S2, the music\nclip is converted to a short-time Fourier transform (STFT) spectrogram. In Step S3, we\ntransform the STFT spectrogram to a mel spectrogram, which characterizes how the sound\nis perceived by the human ear. In Step S4, the mel spectrogram is used as a visual input"}, {"title": "Physical Properties of Sound Waves", "content": "Music (or any sound) is a pressure wave\nthat travels through the air until it reaches the listener's ear. The waveform illustrated in\n(S1) of Figure 1 graphs the change in air pressure at a certain location over six seconds\n(M\u00fcller 2015). Audio data can be represented in a number of ways. While a waveform is\none way to visually represent sound, it does not model how humans hear sound, which is\nbased on the underlying frequencies."}, {"title": "Short-Time Fourier Transform Spectrogram", "content": "A spectrogram visualizes frequency\nand time features of audio data (M\u00fcller 2015). The STFT spectrogram, which is produced\nby taking the Fourier transform of short overlapping time windows of the waveform to\ndecompose it into its individual frequencies and their respective magnitudes, maps the\nsquared magnitude of each frequency over time. The parameters that go into generating an\nSTFT spectrogram are the sampling rate, window type and size, and hop length. Let x\nrepresent the discrete-time signal of the audio signal, w the window function, which takes\nin N samples, and H the hop size. The window function specifies how we weight the audio\nsignal within each window of time and the hop size specifies how many samples we jump\nbetween each window. The discrete STFT X of signal x is:\n$$X(m,k) := \\sum_{n=0}^{N-1}x(n+mH)w(n)exp(-2\\pi ikn/N),$$\nwhere m is the time index, k \u2208 [0 : K] is the frequency index, and i := \u221a\u22121. A sampling rate\nof 44,100 Hz generates a spectrogram that extends up to 22,050 Hz. The STFT spectrogram\ncan then be written as:\n$$S(m,k) := |X(m,k)|^2.$$\nThe magnitude of the complex number X (m, k) captures the presence of each frequency\nat each time sample. Squaring the magnitude yields the power of each frequency at each\ntime sample. We generate an STFT spectrogram for each six-second clip of music. The\nresulting frequency \u00d7 time dimensions of the STFT spectrograms are 2049 \u00d7 517.\nThe STFT spectrogram of Figure 2a is represented in Figure 3a, with the x-dimension\nrepresenting time, the y-dimension frequency, and color the power of each frequency bin at\neach time sample (red (blue) is high (low) power). The frequency and time dimensions are\ndiscretized since we are working with a digital signal. In the STFT spectrogram, frequency\nand time are shown on linear scales while power is shown on a log scale and measured in\ndecibels (dB) since humans perceive volume on a log scale. In Figure 3a, the large patch\nof blue before second three indicates a lack of high frequencies early in the music."}, {"title": "Mel Spectrogram based on Auditory Perception", "content": "Humans are better at perceiving\nfrequency differences at low pitches than at high pitches (M\u00fcller 2015). With equal sen-\nsitivity across the frequency spectrum, the STFT spectrogram does not represent human"}, {"title": "Incorporating Harmonics-based Structure into CNN Filter Design", "content": "Convolu-\ntion filters in CNNs are matrix operations learned from the data with the goal of correctly\nmaking predictions. In designing convolution filters to learn features to classify music emo-\ntion, we must account for how humans perceive sound and in particular the non-contiguous\nstructure of many musical features. We know from acoustic physics that harmonic fre-\nquencies underlie important mid-level musical features, like consonance, timbre, and pitch,\nwhich in turn relate to emotional response. We incorporate the harmonics-based structure\ninto the CNN filters with two goals in mind: 1) relevant mid-level features are parsimo-\nniously captured and 2) the model is more explainable relative to atheoretical deep learning\nmodels. We detail the concepts of harmonics and pitch class and how they connect to the\ndevelopment of theory-based filters next.\nHarmonics. The STFT from (S2) decomposes the sound wave into its constituent sine\nwaves, known as partials. The harmonics of the sound wave are the partials that are"}, {"title": "Harmonics", "content": "The STFT from (S2) decomposes the sound wave into its constituent sine\nwaves, known as partials. The harmonics of the sound wave are the partials that are"}, {"title": "Harmonics Filter Design", "content": "We now detail the steps to develop the harmonics-based convo-\nlution filter for a given pitch class. The unique characteristic of the filter is that it focuses\non specific non-contiguous regions of the spectrogram to flexibly learn a wide variety of\nfeatures. The filter uses \u201cblinders\u201d to select frequencies input into the CNN, and a con-\nvolution size that considers a large range of frequencies at each time frame. The term\n\"blinders\" refers to the matrix operation that selects and weights the mel bands in the mel\nspectrogram prior to convolution, whereas \u201charmonics filter\" refers to the combination of\nthe blinders with convolution. Steps (i) - (v) below are done separately for each pitch class.\n(i) Calculate pitch class filter frequencies: We use the set of frequencies defined in eq. (3)\nto design blinders that retain only the frequencies of interest. Beginning with the lowest\nfundamental frequency of pitch class pin Table 1, we calculate the set of harmonic fre-\nquencies associated with p. For example, the lowest A has a fundamental frequency of\n$$w_n=n f_0  \\forall n \\in Z^+.$$"}, {"title": "Calculate frequency bands", "content": "We calculate frequency bands around each wn. The under-\nlying rationale is that the human ear cannot distinguish frequencies within a small band.\nThe exact size of the band depends on a number of factors, including duration, intensity,\nand frequency, so the width of the bands does not follow a rule. We calculate 1 Hz bands\ncentered around each frequency wn."}, {"title": "Calculate STFT indicator column", "content": "We identify the match between STFT bins and\nthe frequency bands corresponding to the pitch class. To match the STFT bins with the\nset of frequency bands, we create an STFT indicator column such that each element is\nequal to one when the center frequency of the STFT bin falls within one of the frequency\nbands. STFT bins that are not close to the frequency bands may not be chosen."}, {"title": "Construct mel blinders", "content": "We multiply the STFT indicator column by the mel filter\nbank to generate a mel weight column that has Nmel = 256 dimensions. Repeating the mel\nweight column over the time dimension generates the mel blinders. Finally, we multiply\nthe mel spectrogram by the mel blinders to generate the input to the CNN's convolution\nlayer. Appendix E details the steps."}, {"title": "Implement convolution filter", "content": "There are a few others choices to be made before we\ncan directly apply the convolution filter. For a spectrogram, the convolution filter height\ndetermines the number of frequency bins included and the width determines the number of\ntime frames. It is common practice to design the convolution filter to also have a depth (i.e.,\nmultiple channels) so that multiple features can be learned simultaneously. The stride\nspecifies how much the filter slides over the image before performing another operation.\nSince the perception of harmonics is based on the interaction of all frequencies at any given\ntime, we set the filter height to the height of the spectrogram and both the filter width"}, {"title": "Convolutional Neural Network Architecture", "content": "Our CNN to predict music emotion\nconsists of multiple types of layers, including a convolutional layer, pooling layers, and a\nfully connected layer. Designing a neural network involves several architectural and hyper-\nparameter choices. We use standard architecture choices as appropriate, and describe the\nreasoning in Appendix F. Figure 4 summarizes the overall architecture. The operational-\nization of the model with harmonics filters is as follows:\n1. For each pitch class:\n(a) Apply the non-contiguous harmonics filters from Step (S4).\n(b) Batch normalize and take ReLU of the feature maps.\n(c) Average pool over time frames and apply dropout.\n2. Concatenate the hidden layers generated by each of the pitch classes.\n3. Max pool over the pitch classes and apply dropout.\n4. Use one fully connected layer and apply softmax to output a probability distribution\nover the four emotion quadrants.\n5. Output the quadrant Q1, Q2, Q3 or Q4 with max probability as the predicted label.\nThe emotion prediction problem aims to classify an input music clip into one of four\nvalence-arousal quadrants, i.e., a multiclass classification problem. The objective function\nof the model is to minimize the cross-entropy loss between the predicted and actual outputs.\nThe cross-entropy loss LCE over the set of music clips is:\n$$L_{CE} = - \\sum_{i=1}^{N} \\sum_{k=1}^{4} Y_{ik}log(p(\\hat{Y}_{ik}) )$$"}, {"title": "Predicted Emotion", "content": "The model maps each six-second clip into a valence-arousal\nquadrant. Q1 captures positive valence-high arousal, Q2 captures negative valence-high\narousal, Q3 captures negative valence-low arousal, and Q4 captures positive valence-low\narousal. To provide a reference emotion for each quadrant, we borrow the notation from\nPanda et al. (2018): Q1\u2014exuberance, Q2\u2014anxiety, Q3\u2014sadness, Q4-contentment. The\npredictions over time characterize the dynamics of music emotion."}, {"title": "Explainability", "content": "To gain visibility into what the model learns and uses for emotion classification, we use\ngradient-weighted class activation mapping (Grad-CAM). Grad-CAM uses the gradients of\na target class (e.g., Q1) with respect to feature maps, which flow into a given convolutional\nlayer, to produce a heatmap that highlights the regions of the input image that positively\npredict the class (Selvaraju et al. 2017). Since there is a filter and therefore feature map\nfor each pitch class, we obtain a Grad-CAM heatmap for each pitch class, which are 1 \u00d7 517\nimages (517 representing the number of time samples). The heatmap brightness value for\nclip k and pitch class pat each time sample t is:\n$$b^{j}_{kpt} = ReLU ( \\sum_{f} \\alpha_{f}^{j} A^{f}_{kt} ) \\,\\, where \\,\\, \\alpha_{f}^{j} = \\frac{1}{Z} \\sum_{t} \\frac{\\partial y^j}{\\partial A^{f}_{kt}}.$$\nIn this expression, j represents the class (quadrant), A^{f} the feature map for channel f at\ntime t, y the score for class j, and Z the feature map size (517). The brightness value is\na linear combination of the feature maps Af and their importance a in predicting class j.\nWe stack the heatmaps associated with each of the 12 pitch classes to obtain an overall\nheatmap in which the y-axis represents the 12 pitch classes and the x-axis represents time.\nColor denotes b^j_{kpt}, which captures the importance of the non-contiguous frequencies within\neach pitch class towards the classification into quadrant j, and brighter colors capture\nlarger values. Let B_{j}^{k} = \\sum_{p,t} b^{j}_{kpt} represent the sum of the heatmap brightness values over\nall pitch classes and time samples with respect to quadrant j for clip k. Let Bj = \\frac{\\sum_{k} B^k_j}{n_j} represent the average brightness of all clips with true quadrant label j.\nCombining the structure of the harmonics filters with the Grad-CAM heatmaps enables\nus to make sense of what the model learns and uses for classification. The harmonics filters\nan RNN layer in place of average pooling to summarize the feature maps over time. We find that this model results in\nlower classification performance (F1 = 0.42) than our proposed model without positional encoding. This is consistent\nwith the fact that in Choi et al. (2017b) (Figure 3), they also find that RNN marginally reduces the classification\naccuracy for the two emotions they classify (happy, sad). Due to the short duration of six seconds, there is limited\npotential to improve performance and the overall performance from adding the RNN deteriorates due to overfitting\nsince the RNN introduces many more parameters. However, including RNN may improve performance with longer\nduration clips. Note also that in our setting, the emotion of a song may vary over time, so it might not be the case\nthat longer duration clips would always perform better, especially if we are making an assumption that a single clip\nhas only one dominant emotion."}, {"title": "Limited Explainability of Atheoretical and Low-Level Filters", "content": "Grad-CAM\nvisualizations can also be produced for the other filter types. We find that given their low-\nlevel focus without specific theory to guide our expectations, it is more difficult to interpret\nwhat musical features are captured and connect how they contribute to the classification\nof a particular class. The Grad-CAM heatmaps for the square filter CNN in Figure 7 are\nequivalent to heatmaps produced for an image recognition model. They show which con-\ntiguous regions of the input image contribute to the classification of a particular class,\nproviding an idea of the range of frequencies and times contributing to the classification.\nHowever, it is unclear what human-understandable musical features are being learned.\nNote that in contrast to our harmonics filters, pitch classes are not used here.\nThe heatmaps based on separate time and frequency filters highlight which time periods\nor frequencies contribute most to a particular classification (see Appendix Figures Kla\nand K1b). Similar to square filters, it is less clear how to interpret the heatmaps, mak-\ning it challenging to understand what musical features the models learn. In the models\nusing atheoretical and low-level filters, it is more challenging to determine what musical\nfeature is learned and used for emotion classification. Thus, our contribution in incorporat-\ning the non-contiguous theory-based harmonics filters is to provide greater explainability,\nspecifically with consonance, while obtaining similar performance as atheoretical filters."}, {"title": "Model Capability to Learn Features", "content": "While our discussion of explainability has primarily focused on a specific and important\nfeature, i.e., consonance, harmonics filters can also learn other features, as foreshadowed\nin \u00a74.3. For instance, the combination of harmonic frequencies provides information about\npitch perception, timbre (or tone color), and spectral spread and complexity (McAdams\nand Giordano 2014).\nThe comparable classification accuracy between the theory-based CNN with harmonics\nfilters and the models with atheoretical filters suggests that the harmonics filters are learn-\ning multiple musical features relevant for emotion classification, since consonance alone\nwould not yield such high accuracy. To assess this systematically, we check whether clas-\nsification performance improves if we add handcrafted features found to predict music"}, {"title": "Empirical Analysis", "content": "We begin by describing the datasets used to train the models. We then report the perfor-\nmance of our proposed architecture with harmonics filters and compare it against bench-\nmark models proposed in the literature. Finally, we show how our model is explainable\nusing gradient-based model visualizations and compare it to visualizations generated by\nother CNN models which use atheoretical and low-level filters."}, {"title": "Datasets", "content": "We combine two public datasets compiled by music emotion researchers, which serve com-\nplementary purposes in our analysis: Soundtracks (Eerola and Vuoskoski 2011) and the\nMediaEval Database for Emotional Analysis in Music (DEAM) (Aljanaki et al. 2017).\nThe Soundtracks dataset consists of 360 excerpts from movie soundtracks that range\nin duration from 10 to 30 seconds. One benefit of movie soundtracks is that they are\ncomposed to elicit emotion. The music clips are instrumental and do not contain any lyrics,\ndialogue, or sound effects. The clips were chosen to: 1) elicit either a discrete emotion\nor to be high or low on valence, arousal, or tension, 2) evoke only a single emotion over\nthe length of the clip, and 3) be unfamiliar to prevent song familiarity from impacting\nemotion tagging. University students and staff with musical expertise annotated the song\nemotions, and six annotators tagged each music excerpt. Perceived discrete emotions and\nvalence and arousal were separately annotated on a scale of 1 to 7. Inter-rater consistency\n(Cronbach's alpha) was 0.92 for valence, 0.90 for arousal, and ranged from 0.66 to 0.93\nfor the discrete emotions. We split the excerpts into non-overlapping six-second segments.\nExcerpts selected for the discrete emotions happy, sad, tender, fear, and anger are mapped\nonto the valence-arousal quadrants such that happy maps to Q1, fear and anger to Q2, sad"}, {"title": "Model Performance", "content": "We use precision, recall, and F1-score, standard measures in the machine learning litera-\nture, to evaluate our model. We calculate these metrics for each class (quadrant) and a\nweighted average by the number of samples in each class determines each overall measure."}, {"title": "Model Explainability", "content": "We now compare the explainability based on Grad-CAM for our CNN model with har-\nmonics filters and other benchmark CNN models."}, {"title": "Explainability of Harmonics Filters", "content": "In \u00a73, we discussed the patterns to expect\nin terms of the brightness of Grad-CAM heatmaps based on theory related to harmonics,\nperception of consonance, and emotional response to consonance. We illustrate explain-\nability around the role of consonance in two ways. First, we compare the heatmaps with"}, {"title": "Limited Explainability of Atheoretical and Low-Level Filters", "content": "Grad-CAM\nvisualizations can also be produced for the other filter types. We find that given their low-\nlevel focus without specific theory to guide our expectations, it is more difficult to interpret\nwhat musical features are captured and connect how they contribute to the classification\nof a particular class. The Grad-CAM heatmaps for the square filter CNN in Figure 7 are\nequivalent to heatmaps produced for an image recognition model. They show which con-\ntiguous regions of the input image contribute to the classification of a particular class,\nproviding an idea of the range of frequencies and times contributing to the classification.\nHowever, it is unclear what human-understandable musical features are being learned.\nNote that in contrast to our harmonics filters, pitch classes are not used here.\nThe heatmaps based on separate time and frequency filters highlight which time periods\nor frequencies contribute most to a particular classification (see Appendix Figures Kla\nand K1b). Similar to square filters, it is less clear how to interpret the heatmaps, mak-\ning it challenging to understand what musical features the models learn. In the models\nusing atheoretical and low-level filters, it is more challenging to determine what musical\nfeature is learned and used for emotion classification. Thus, our contribution in incorporat-\ning the non-contiguous theory-based harmonics filters is to provide greater explainability,\nspecifically with consonance, while obtaining similar performance as atheoretical filters."}, {"title": "Model Capability to Learn Features", "content": "While our discussion of explainability has primarily focused on a specific and important\nfeature, i.e., consonance, harmonics filters can also learn other features, as foreshadowed\nin \u00a74.3. For instance, the combination of harmonic frequencies provides information about\npitch perception, timbre (or tone color), and spectral spread and complexity (McAdams\nand Giordano 2014).\nThe comparable classification accuracy between the theory-based CNN with harmonics\nfilters and the models with atheoretical filters suggests that the harmonics filters are learn-\ning multiple musical features relevant for emotion classification, since consonance alone\nwould not yield such high accuracy. To assess this systematically, we check whether clas-\nsification performance improves if we add handcrafted features found to predict music"}, {"title": "Ad Insertion Automation", "content": "Our theory-based and benchmark models can predict the emotion distributions of the ads\nand content to calculate emotional distances at scale. We use these to compare ad skip and\nrecall outcomes from showing each ad at the most emotionally similar ad insertion point\nbased on the predictions of the models."}, {"title": "Calculating Model-predicted Emotional Distances", "content": "We transform the first six\nseconds of audio of each ad and 30 seconds of audio before each ad insertion point in the\ncontent videos into mel spectrograms. For the content videos, we break the 30-second clips\ninto five six-second clips. We use the models to predict the emotion distribution of each\nsix-second clip. For the 30-second content video clips, we average over the five predicted\nemotion distributions associated with each six-second clip. Using the 24 content emotion\ndistributions (four content videos \u00d7 six insertion points) and four ad emotion distributions,\nwe calculate the JS distances between the ads and the content at the six insertion points.\nFor each combination of model, content video, and ad, we determine which ad insertion\npoint is the most emotionally similar."}, {"title": "Skip and Recall Rates", "content": "From the experiment, we have the skip and recall rates\nfor the 96 experimental cells. For each model, we average the skip and recall rates as well as\nthe ground-truth JS distances (based on human-tagging) of the most emotionally similar\ninsertion point for each of the four ads in each of the four content videos as predicted by\neach model and show the results in Table 5. Note that these average skip and recall rates\nare based on a simulation of ad insertion automation using the experimental data.\nOur proposed theory-based model using harmonics filters selects insertion points that\nare relatively high in emotional similarity (i.e., low JS distance) compared to the other\ndeep learning models. These ad insertion points generate relatively favorable skip and recall\nrates, suggesting that our model can be useful in automated emotion-based ad insertion.\nThe models that includes tempo features, the best performing model from \u00a74.4, further\nimproves upon these results."}, {"title": "Incorporating Other Video Modalities", "content": "Our primary analysis has focused on\nusing emotion evoked from music. However, with videos, emotional content may be present\nacross multiple modalities (e.g., facial expressions, text of speech). Multimodal emotional\ncontent can also be used to predict emotional distance. When the videos have human faces,\nwe can use publicly available tools to estimate emotion from facial expressions. Similarly,\nemotional content can also be obtained from voice tonality and speech text.\nIn our application, we observe that speech or human faces might not be present in each\ncontent video or in the first six seconds of ads, implying using speech or facial emotion\nis not always feasible. For the videos with human faces, we assess the skip and recall\nrates when including face emotion alongside music emotion. We could not include speech\nemotion since there was not enough speech in the first six seconds of the ads. We find mixed\nevidence of the value of including face emotion. Including face emotion slightly improves\nthe recall rate but hurts the skip rate. Appendix L.1 details the analysis.\nOverall, we find that there is potential in incorporating emotion information from images\nand text, but the existing tools are limited in their ability to extract emotion information\nfrom short clips (i.e., first six seconds of ads) and animated videos. However, this is a\nmoving target and as these methods steadily improve, these findings could well change."}, {"title": "Managerial Implications", "content": "Past studies have provided evidence that emotional ads impact attention and memory\n(Cohen et al. 2018, Holbrook and Batra 1987). The results of this study support the theory\nthat emotional similarity decreases ad skipping and increases brand recall. Our method\ncan be used to determine time-varying emotion based on the background music of videos,\nand we show that it performs as well as atheoretical CNN models while being explainable."}, {"title": "Application: Emotion-based Ad Insertion in Content Videos", "content": "Our proposed theory-based deep learning model can be used in a number of real-time\nemotion-based applications by predicting the valence and arousal of music clips. We demon-\nstrate the value with an illustrative application involving emotion-based contextual tar-\ngeting, where the algorithm determines the optimal emotion-based ad insertion point for a\nvideo ad within a content video (e.g., YouTube video) with time-varying emotional content.\nSuch emotion-based contextual targeting with automated content matching is gaining\nimportance. Increasing privacy restrictions limit person-specific targeting of advertising,\nmaking contextual and content-based targeting for ad placement more relevant and use-\nful. Further, given the vast amount of UGC available, non-algorithmic approaches to\nmatching ads with content is challenging, if not impossible, to implement at scale. The\nsize of the matching problem is very large, and a platform like YouTube needs to match\nbillions of ads and content videos daily.\nThroughout a content video, emotion often varies over time and so the various ad inser-\ntion slots differ in emotion. Since ads also often elicit emotion, we seek to understand how\nto match the ad to the insertion slot based on emotion. While marketing researchers have\nconsidered the overall emotion of content videos for ad matching (Coulter 1998, Kamins\net al. 1991, Puccinelli et al. 2015, Kapoor et al. 2022), our focus is on automatically iden-\ntifying the optimal ad insertion slot within videos that vary in emotion over time.\nDoes emotional congruence or contrast work better for ad insertion? It is an empirical\nquestion as to whether ads that are similar to the emotional context of the content video"}, {"title": "Experiment: Is Emotional Congruence or Contrast More Effective?", "content": "We discuss the experimental design and results below."}, {"title": "Experimental Setup", "content": "There are multiple ad insertion points, which vary in\nevoked emotion, within each content video. The outcome variables of interest are ad skip\n(as a proxy for attention and interest) and brand recall (as a proxy for memorability). We\nuse a full factorial design across four ads, four content videos, and six ad insertion points"}]}