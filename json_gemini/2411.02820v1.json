{"title": "DroidSpeak: Enhancing cross-LLM communication", "authors": ["Yuhan Liu", "Esha Choukse", "Shan Lu", "Junchen Jiang", "Madan Musuvathi"], "abstract": "In multi-agent systems utilizing Large Language Models (LLMs), communication between agents traditionally relies on natural language. This communication often includes the full context of the query so far, which can introduce significant prefill-phase latency, especially with long contexts.\nWe introduce DroidSpeak, a novel framework to target this cross-LLM communication by leveraging the reuse of intermediate data, such as input embeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the need to reprocess entire contexts for fine-tuned versions of the same foundational model. This approach allows faster context integration while maintaining the quality of task performance. Experimental evaluations demonstrate DroidSpeak's ability to significantly accelerate inter-agent communication, achieving up to a 2.78\u00d7 speedup in prefill latency with negligible loss in accuracy. Our findings underscore the potential to create more efficient and scalable multi-agent systems.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have transformed the landscape of AI-driven applications, enabling a wide range of advanced capabilities, from natural language understanding to complex task automation [38, 50]. These LLMs are no longer restricted to single-agent operations; instead, they are increasingly utilized in multi-agent systems where specialized agents collaborate to achieve complex objectives [23, 41, 48, 54]. In such systems, agents often need to exchange information, align contexts, and refine shared goals, forming intricate workflows that span multiple rounds of communication. Examples include customer service automation, collaborative content creation, and even sophisticated simulations in social sciences. \nTraditional communication between LLM agents relies heavily on natural language exchanges, emulating human-like dialogue, as shown in Figure 1. While this approach is intuitive and ensures coherence, it also introduces significant overhead. LLM inference includes a prefill phase, to understand the input, and decode phase, to serially generate the output [2, 43]. Prefill phase tends to be much more compute-intensive than the decode phase [2, 43]. As agents collaborate over extensive interactions, especially in long-running or contextually dense dialogues, the prefill phase latency can dominate the overall communication time, hindering real-time responsiveness and scalability.\nTo address these challenges, we propose DroidSpeak, a novel communication protocol that streamlines LLM inter-agent exchanges among finetuned LLMs of the same foundational LLM. We note that the prefill phase bottleneck is caused due to each receiver-LLM needing to translate natural language back to the context the sender-LLM already had. On the other hand, we dive into the similarities and differences between finetuned LLMs based on a common foundational model. We show that each layer across these LLMs has a different degree of similarity and a different degree of impact on the final output accuracy. We exploit this by communicating across LLMs using the intermediate data selectively per layer. We utilize the high bandwidth interconnects across GPU nodes in cloud systems today [10, 11], to offset the precious computational time and resources of the GPUs themselves. This strategy circumvents the need for complete context regeneration, enabling faster integration without sacrificing the quality of outputs. By selectively reusing and recalculating only what is necessary, DroidSpeak significantly reduces prefill delays while maintaining high task performance standards.\nWe make the following contributions in this paper.\n\u2022 We identify a growing challenge in LLM agent workflows, of repeated context prefill delays due to communication.\n\u2022 We present an analysis of similarities and differences between two LLMs that share a common foundational model, and show that each layer has a different level of impact on accuracy.\n\u2022 We design DroidSpeak to selectively reuse and re-compute the intermediate data of sender-LLM at the receiver-LLM to minimize communication overheads.\n\u2022 Using a comprehensive methodology and dataset, we evaluate DroidSpeak on a variety of model-pairs and datasets to show that we can achieve up to a 2.78\u00d7 speedup in prefill latency with negligible impact on accuracy."}, {"title": "BACKGROUND & MOTIVATION", "content": "Transformers [4, 51] are the standard architecture for most generative language models, powering a broad range of applications in AI-driven text generation. A transformer model processes an input sequence of tokens\u2014units that can represent words, punctuation, or word segments\u2014and generates an output sequence in two primary phases: prefill and decoding.\nDuring the prefill phase, the model takes in the input tokens, where each layer in the model's attention mechanism processes an embedding (E) tensor to produce two three-dimensional tensors: key (K) and value (V) tensors. These K and V tensors contain critical context information for later use by the model and are collectively referred to as the KV cache.\nIn the decode phase, or token generation phase, the KV cache is used to calculate attention scores between token pairs, producing an attention matrix that supports autoregressive token generation.\nThe prefill phase is computationally more demanding than the decoding phase, with its complexity scaling super-linearly with input length, whereas decoding complexity grows linearly. This separation of computational demands underscores the need for optimized handling of each phase [2, 43]."}, {"title": "LLM Agents in Specialized Tasks", "content": "While traditional chatbots generate generic responses, LLM agents are specifically designed to tackle complex, specialized tasks. In multi-agent setups, each agent contributes to a subset of the overall task, resulting in a collaborative system of specialized agents that collectively achieve sophisticated goals."}, {"title": "Challenges in Inter-Agent Communication", "content": "In multi-agent LLM systems, agents often need to share intermediate outputs for the task to progress. For example, in MetaGPT [20], each task passes through multiple agents (e.g., Product Manager, Architect, Engineer), and each agent sends their output to the next, who processes it further to refine the final product.\nTraditionally, agents communicate through natural language. However, as shown in Figure 3, each time a sender agent completes its output, it passes this data and conversation history to the receiving agent. This requires the receiving agent to reprocess all prior context during its prefill phase. The super-linear scaling of prefill latency with input length can result in significant delays, particularly in conversations where agents interact iteratively, leading to repeated prefill overhead. For example, in the HotpotQA dataset [3], the average prefill delay for Llama-3-70B-Instruct is 2.16 seconds, compared to a relatively short 0.21 seconds for decoding. Such discrepancies highlight the prefill phase's high cost and illustrate the inefficiency of conventional inter-agent communication."}, {"title": "Case Studies on Long Outputs and Prefill Delays", "content": "In complex multi-agent systems, an agent's output can be substantial, leading to long prefill delays for the next agent.\nMapCoder [25]: In tackling competitive programming challenges, a retrieval agent identifies related problems, which are then passed to a planning agent to generate step-by-step solutions. Next, a coding agent translates these steps into executable code, which is tested and debugged iteratively. In such cascading systems, the previous agent's output can reach up to 38,000 tokens, causing substantial prefill delays.\nReflexion [48]: Here, an actor agent interacts with an environment to produce a trajectory, which is scored by an evaluator agent. A self-reflection agent then analyzes this trajectory and provides feedback to the actor. This sequence continues until the evaluator is satisfied, but each step's long outputs lead to significant prefill delays for subsequent agents.\nThe substantial delays in these scenarios stem from each agent repeatedly prefilling the same context. To address these challenges, we aim to explore a more efficient, context-preserving communication method that reduces redundant prefill costs, enabling faster and more streamlined inter-agent exchanges."}, {"title": "CHARACTERIZATION AND OPPORTUNITIES", "content": "Previous work has explored reducing a single model's prefill delay by reusing intermediate results generated during the prefill phase, such as the KV cache, across requests to the same model. Building on this idea, we envision an improved \u201clanguage\u201d for agent communication, where agents leverage these intermediate results. This could allow the receiving agent to utilize the sender's precomputed data, minimizing redundant computation and accelerating response times."}, {"title": "Methodology", "content": "To test the effectiveness of reusing the sender's intermediate results on the receiver agent, it is important to build a benchmark that includes LLM pairs and datasets for accuracy testing. To the best of our knowledge, no such benchmark currently exists. Without careful curation, a fine-tuned LLM may perform even worse on downstream tasks than the base model, making the use of the fine-tuned LLM unnecessary for these tasks."}, {"title": "Quality", "content": "The generation quality of fine-tuned LLMs can be significantly higher than that of the base models. We observe that even though the receiver LLMs consistently outperform the sender LLMs in terms of quality, their model weights are actually quite similar."}, {"title": "Similarity", "content": "Weights. In Figure 5, we plot the fraction of the difference between the sender and receiver LLM weights relative to the original sender LLM's weights. It is evident that this difference represents only a small fraction of the original weights.\nIntermediate Data. In Figure 6, we plot the fraction of the difference between the KV or E cache of the sender and receiver LLMs relative to the original sender LLM's KV or E cache. Although the difference is larger than the difference in weights, it is still a small portion compared to the original sender LLM's KV or E cache."}, {"title": "DESIGNING DROIDSPEAK", "content": ""}, {"title": "Naively reusing the KV cache", "content": "A naive way of eliminating prefill delay on the receiver agent's side is to reuse all of the sender agent's KV cache. Figure 7 shows the results of reusing Llama-3-8B's KV cache on Llama-3-8B-Instruct. We can see that naively reusing all the KV cache from the sender agent on the receiver agent greatly degrades its generation quality to almost the same as the base model before fine-tuning.\nTo understand this, we further examine the differences in intermediate data between the sender and receiver LLMs. In Figure 8, we plot the relative differences in KV and E cache between the two LLMs, normalized by the sender LLM's KV and E cache. The results indicate that these differences vary across layers, suggesting that layer-wise optimizations are required. Note that as shown in Figure 2, each layer's KV computation requires the E for that layer. However, reusing KV caches precludes access to E. Therefore, we look at reusing contiguous layers of KV cache until the last layer. Figure 9 shows the results from this approach. Although the impact on accuracy is not bad, the degree of freedom for optimization is low."}, {"title": "Reusing Embedding Cache", "content": "Following insights from prior work [24] that show that models are generally converging towards a common embedding space, we explored reusing the E cache from the base LLM on the fine-tuned LLM. Reusing the E cache also offers greater flexibility in terms of the layer-position of the data that can be reused, unlike reusing KV cache. For example, if we reuse the KV cache for the first three layers and then switch to recomputation, we cannot proceed because the input embedding (E) for the 4th layer is missing. This issue can be resolved by reusing the E cache instead. In this case, when we want to switch to recomputation at the 4th layer, the input embedding (E) for that layer is still available, allowing recomputation to begin at the 4th layer."}, {"title": "Overheads of reusing E cache vs reusing KV cache", "content": "Although reusing E cache provides great flexibility in terms of which layers to reuse, it costs overhead in GPU memory, transmission, and computation. Consider the scenario where the sender and receiver LLMs are placed on two GPU nodes, and they are interconnected with an Infiniband link. On the sender LLM's side, the E cache needs to be stored in GPU memory before sending it to the receiver LLM's side. Similarly, sending the E cache through the bandwidth link incurs extra transmission delay. Finally, after the E cache is sent to the receiver LLM's side, an extra QKV projection operation (\u00a73.1) is required to turn the E cache into KV cache, which incurs extra computation delay. These three types of delay grow linearly with respect to the number of reused layers.\nUnlike reusing E cache which incurs memory and delay overheads, reusing KV cache does not incur any of the GPU memory overhead or extra transmission and computation delay. However, as we have discussed in \u00a75.2, once we start reusing the KV cache for a layer, we will lose the opportunity to recompute for later layers due to the lack of embedding E."}, {"title": "Reusing KV+E cache", "content": "One solution to the lost E problem with KV cache reuse is: at the transition layer, where we want to switch from KV cache reusing to recomputation, we load the embedding E from the sender LLM, then we avoid the problem of missing E thus able to recompute after that. We refer to this as KV+E cache reuse.\nFigure 13 compares reusing the E cache alone versus reusing KV+E caches in terms of prefill delay and accuracy tradeoff. For the three model pairs, the tradeoff between delay and accuracy is similar whether reusing both caches or just the E cache.\nTherefore, reusing KV+E caches provides similar benefits in terms of delay and accuracy without adding GPU memory overhead to the sender LLM or computation overhead to the receiver LLM."}, {"title": "End-to-end flow with DroidSpeak", "content": "Putting together, the end-to-end system is shown in Figure 14, at the offline stage, DroidSpeak first profiles which layers to reuse for each pair of LLMs, referred to as reusing configuration, on an example profiling dataset. At the online stage when the sender communicates with the receiver LLM, the KV and E cache will be sent to the receiver LLM, based on the reusing configuration. The receiver LLM then partially re-computes the new KV cache for those layers that do not reuse them."}, {"title": "EVALUATION", "content": ""}, {"title": "Setup", "content": "We evaluate DroidSpeak on four pairs of LLMs, where the sender LLMs are base models that the receiver models fine-tuned upon.  Note that the receiver models are such that their accuracy on the datasets is better than the sender models.\nWe evaluate DroidSpeak on six different datasets with different tasks, including question answering, summarization, and math reasoning, listed in Table 1."}, {"title": "Quality Metrics", "content": "We measure generation quality using the standard metric of each dataset [3, 59].\n\u2022 F1 score: used to evaluate the model's response on HotpotQA, NarrativeQA, and MultifieldQA datasets. It measures the probability that the generated answer matches the ground-truth answer of the question-answering task.\n\u2022 Rouge score: used to evaluate the quality of the summarization dataset, Government Report, by comparing the overlap of n-grams, word sequences, or word pairs between the generated text and reference texts.\n\u2022 Accuracy: used to evaluate the model's output on the MMLU-Stem and GSM8K datasets. The accuracy is defined as whether the generated answer matches the ground-truth answer.\nWe evaluated the systems' performance metric by measuring the prefill delay, which is the time from the arrival of the user query to the generation of the first token. This includes the time to load KV and E cache from the remote GPU server, and the prefill computation time.\nWe compare DroidSpeak with the following baselines:\n\u2022 Full prefill of the receiver model: the receiver model prefills the text of the context, which represents the baseline of the highest computation overhead but the best quality we can get.\n\u2022 Reusing all the E cache of the sender model: the sender model sends its E cache for all the layers to the receiver model and the receiver model runs KV projection to get the KV cache. Then the receiver model runs decoding with the generated KV cache.\n\u2022 Reusing all the KV cache of the sender model: the sender model sends its KV cache for all the layers to the receiver model. Then the receiver model runs decoding with the transferred KV cache."}, {"title": "Hardware", "content": "We use two NVIDIA A100 GPU nodes, with 200Gbps Infiniband link connected, to benchmark our results. The sender LLM is hosted on one A100 GPU node, and the receiver LLM on another."}, {"title": "Overall Results", "content": "We show the improvement of DroidSpeak over the baselines, as described in \u00a76.1."}, {"title": "DISCUSSION", "content": "What is the pattern across model pairs? In this section, we explore the patterns of KV and E cache reusing across different model pairs.  We can see that clearly different model pairs show different patterns in reusing. Specifically, for the Mistrallite and Mistral-7B pair, up to 15 layers of KV and E cache can be reused without affecting accuracy. For the Llama-3-8B pair, up to 13 layers can be reused without loss of accuracy, and for the Llama-3-70B pair, up to 40 layers can be reused without compromising generation quality.\nWhat is the pattern across different datasets for a model pair? Next, we explored how cache reuse patterns change across different datasets while keeping the model pair fixed (Mistrallite and Mistral-7B). This suggests that the layer reuse configuration (i.e., how many layers to reuse) determined on one dataset can be generalized to others for the same model pair.\nWhat is the pattern across different tokens? Furthermore, we ask the question: does the difference in embedding or KV cache between the sender and receiver LLM differ similarly large for different tokens? We use Pearson's correlation coefficient as the metric to quantify the difference in the embeddings or KV cache. We can see that for different tokens in all the layers have very different correlation scores, leading to a large variance. This suggests that optimizations adaptive to different tokens could be developed. We do not cover this in the current version of the paper, and we leave this as a future work."}, {"title": "RELATED WORK", "content": "Recent studies propose multi-agent systems in fields such as coding assistance [6, 18, 20, 23, 25, 44, 49], gaming [1, 7, 17, 33, 37, 60, 61], and social simulation [17, 40, 42], among others. Recent research highlights that using fine-tuned LLMs as agents can improve generation quality in areas like general question answering [5, 9, 58, 64], tool learning [15, 45], personalization [31], and coding tasks [52]. DroidSpeak focuses on optimizing communication delay between two LLM agents when one is a fine-tuned version of the other.\nRecent LLM systems research focuses on improving the serving throughput or inference delay. One line of work design better scheduling policies of inference requests [2, 28, 32, 35, 43, 47, 63], one line of work propose efficient memory management for serving LoRA models [8, 30, 46, 53], and another line of work design smart caching policy for KV cache offloading [13, 21, 26, 29]. DroidSpeak reduces the inference latency of serving multi-agent system requests when one agent is a fine-tuned version of another.\nMany prior works focus on compressing the KV cache to make its GPU memory usage [29, 39, 55, 56, 62] or transmission delay smaller [34]. DroidSpeak is orthogonal to this line of work. Their proposed compression algorithms are applicable on DroidSpeak, reducing the size of the KV cache to reduce the communication delay.\nAnother line of research reduces the prefill delay when blending non-prefix KV caches from multiple different contexts [16, 57]. This approach could be applied in scenarios where the sender and receiver agents are instances of the same LLM with different system prompts. In contrast, DroidSpeak addresses the case where the sender and receiver agents share the same foundational model, but the receiver is fine-tuned based on the sender agent."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In this work, we introduced DroidSpeak, a novel framework designed to streamline inter-agent communication among LLMs that share a base model by leveraging intermediate data reusability, specifically focusing on KV and E caches. Our results demonstrate that DroidSpeak can significantly reduce prefill latency, achieving up to a 2.78\u00d7 improvement with minimal impact on output quality. By enabling selective reuse of intermediate representations, we address the prefill bottlenecks that slow inter-agent exchanges, without impacting the quality. The experiments conducted across various datasets and model pairs affirm DroidSpeak's efficacy and robustness in preserving generation quality while optimizing latency, underscoring its potential as a scalable solution for multi-agent LLM setups.\nBuilding on the promising outcomes of DroidSpeak, several avenues for future research remain open. First, while DroidSpeak has shown considerable efficiency improvements for LLMs sharing a base model (fine-tuned versions of a foundational model), there is need to expand this work to include models that differ from each other in shape and size too. Another prospective research direction involves integrating advanced compression techniques into the KV and E caches to minimize transmission delays further. There is also potential for further optimizations by incorporating adaptive caching mechanisms. These mechanisms could dynamically adjust reuse strategies based on real-time latency constraints and resource availability."}]}