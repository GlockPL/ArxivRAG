{"title": "Causal Abstraction in Model Interpretability: A Compact Survey", "authors": ["Yihao Zhang"], "abstract": "The pursuit of interpretable artificial intelligence has led to significant advance-ments in the development of methods that aim to explain the decision-making pro-cesses of complex models, such as deep learning systems. Among these methods,causal abstraction stands out as a theoretical framework that provides a princi-pled approach to understanding and explaining the causal mechanisms underlyingmodel behavior. This survey paper delves into the realm of causal abstraction,examining its theoretical foundations, practical applications, and implications forthe field of model interpretability.", "sections": [{"title": "Introduction", "content": "The increasing complexity of modern machine learning models, particularly deep learning systems,has raised concerns about their interpretability and transparency [3]. As these models are deployedin a wide range of applications, from healthcare to finance, it is crucial to understand how they makedecisions and to ensure that their outputs are reliable and trustworthy. The field of model inter-pretability has thus emerged as a critical area of research, with the goal of developing methods thatcan explain the decision-making processes of complex models in a human-understandable manner.\nRecent research on the interpretability of complex systems has increasingly focused on understand-ing what drives specific behaviors in machine learning models-particularly black-box models likeneural networks. These models, due to their vast number of parameters and complex architectures,often make it difficult for humans to discern how decisions are made. One promising approach totackling this challenge is causal abstraction [19, 2, 8], which provides a formal framework for trac-ing the specific causes of model behaviors. Rather than just simplifying or approximating a model,causal abstraction enables researchers to examine the internal variables and mechanisms responsiblefor generating certain outputs, offering a clearer window into how the model operates at a deeper,mechanistic level.\nThis concept has its origins in fields like program analysis, where abstraction is used to isolate thecritical components of complex systems [5]. However, causal abstraction goes beyond simplifyinga model-it systematically identifies and traces causal pathways within the model, allowing for pre-cise explanations of what caused certain behaviors. This approach aligns with the growing interestin mechanistic interpretability, where the goal is to understand the internal workings of a modelby isolating and examining the variables and mechanisms that directly influence its behavior. Byrevealing these causal links, causal abstraction improves our ability to interpret, trust, and explainmachine learning models in a more transparent and accountable way.\nIn this survey paper, we delve into the realm of causal abstraction, exploring its theoretical founda-tions, practical applications, and implications for the field of model interpretability. Based on two newest research on causal abstraction for model explanation [25, 8], we begin by introducing thedevelopment of the concept of causal abstraction and its possible relationship to problems in causalreasoning and causal inference. We then discuss how causal abstraction can be applied to interpret"}, {"title": "Development of Causal Abstraction Theory", "content": "The concept of causal abstraction draws foundational support from the work of Rubenstein et al.in their 2017 paper, Causal Consistency of Structural Equation Models [19]. In this paper, theyproposed a theoretical framework of exact transformations, which formalizes the conditions underwhich two models are equivalent in a causal sense. The framework focuses on ensuring that causalmechanisms are preserved between models, addressing the issue of causal consistency.\nTheir motivating example came from a model linking total cholesterol (TC) to heart disease (HD),where experimental results paradoxically indicated that higher TC could both increase and decreaseHD risk. Modern understanding, which differentiates between two types of cholesterol-HDL andLDL-each with distinct effects on HD, showed that the original model broke causal consistency.Rubenstein et al.'s \"exact transformations\" provided a formal method to determine when two modelsremain consistent in terms of their causal structure.\nWhile Rubenstein et al. did not introduce the idea of causal abstraction directly, their work laid thetheoretical groundwork by offering a formal mathematical basis for determining causal equivalencebetween models. This provides a critical foundation for later work in causal abstraction, which uses these principles to construct simplified models that preserve key causal relationships, enabling moreinterpretable model explanations.\nCausal abstraction was formally introduced by Beckers et al. in 2018 [2], building on Halpern'sfoundational discussions on causal models [11]. In this framework, structural equation models areformalized using a signature, which consists of a set of variables and their associated possible values.The causal relationships between these variables-referred to as mechanisms in later works\u2014arerepresented by a set of functions. Each function determines the value of an endogenous variablebased on the values of other variables. Interventions are formalized by replacing the mechanisms of certain variables with fixed values, a process analogous to do-calculus [17], and are referredto as hard interventions in subsequent literature. Building on these formalized structures, causalabstraction is initially defined as an equivalence relationship between a low-level model and a high-level model, where all interventions in the low-level model can be mapped to the high-level modelvia a function that satisfies specific properties. This framework provides a series of progressivelymore restrictive definitions of abstraction for causal models.\nIn their paper, Beckers et al. take an important step by demonstrating the possibility of abstractinga complex system of variables into a simpler one through the aggregation of variables into macro-variables, while preserving the underlying causal mechanisms. This abstraction is critical for the development of causal modeling, where simpler models can still capture the essential causal rela-tionships of more complex systems.\nThis theory has been further developed in subsequent works. In 2020, Beckers et al. extended theconcept of causal abstraction to account for more realistic scenarios where an abstract causal modelserves as an approximation of the underlying system [1]. In this extended framework, they address the discrepancies that may arise between low- and high-level causal models of the same system, andprovide a formal account of how one causal model can approximate another-a topic of independentinterest.\nThe framework introduces the notion of approximate casual abstraction, which is extended to prob-abilistic causal models, allowing for uncertainty in the abstracted model. To quantify the approxima-tion, a distance function is used on certain features or variables to measure the difference betweentwo models. This approach can be naturally extended to probabilistic causal models by consider-ing the expected value of the distance function, where the hidden exogenous variables follow someprobabilistic distribution. This work broadens the scope of causal abstraction, providing a structuredmethod to account for imperfections and uncertainty in causal modeling.\nBy 2022, the foundations of causal abstraction theory were further solidified. Otsuka et al. [16] ex-tended the theory by leveraging the mathematical framework of category theory. They developed acategory-theoretic criterion for determining the equivalence of causal models that possess different,"}, {"title": "Enhancement of Model Interpretability through Causal Abstraction", "content": "Causal abstraction provides a formal framework for improving the interpretability of complex ma-chine learning models by revealing the underlying causal mechanisms driving their behavior. Ratherthan merely simplifying a model, causal abstraction helps researchers pinpoint specific causes be-hind model outputs, offering insights that are more intuitive and aligned with human reasoning. Thissection highlights practical applications where causal abstraction has been employed to interpretmodels across various domains.\nThe first study to apply causal abstraction to artificial intelligence models for interpretability wasconducted by Geiger et al. [10], who laid the initial theoretical foundation for causal abstraction inneural networks. While neural networks are often treated as black boxes, they have the distinct ad-vantage of being highly observable, making them well-suited as a base model for causal abstraction.Geiger et al. introduced an initial method, causal abstraction analysis, to evaluate neural networksby first positing hypotheses in the form of a causal model. They then assessed the alignment ofneural representations with this hypothesized causal model using an algorithm called distributedalignment search.\nTo validate whether the neural representations align with high-level causal variables, they employedthe interchange intervention method. This approach exemplifies how causal abstraction can beextended beyond theoretical frameworks to practical applications in model interpretation. By usinginterchange interventions, they experimentally assessed whether the causal model holds within theneural network, wherein the high-level model serves as an abstraction of the neural network's mech-anisms. Specifically, alignment search calculates how closely the lower-level model aligns withthe high-level causal model. Here, interchange interventions are critical, experimentally connectinginternal representations to the causal model through counterfactual testing. Although abstraction it-self is challenging, this counterfactual approach allows researchers to verify causal effects, bridgingcausal abstraction with practical model interpretability.\nThe methodology of Distributed Alignment Search (DAS) was further developed by Wu et al. intheir 2023 work on interpretability at scale, particularly for large-scale language models [25]. In thisstudy, they introduced Boundless DAS, an enhanced version of DAS designed to efficiently explorecausal alignments in models with billions of parameters, such as the Alpaca model. Boundless DASimproves upon the original by replacing brute-force search steps with learnable parameters, thusenabling effective causal analysis across distributed neural representations.\nIn their work, Wu et al. applied Boundless DAS to the Alpaca model, investigating its causal mecha-nisms in a simple numerical reasoning task. By identifying causal alignments between interpretableBoolean variables and neural representations within Alpaca, they demonstrated that causal abstrac-tion could scale robustly, retaining faithful alignment even under variations in input and context.This adaptation marks a significant step forward in applying causal abstraction to large languagemodels, advancing model interpretability in a practical, scalable manner.\nThe other approach to causal abstraction in neural networks was proposed by Geiger et al. in their2024 paper [8]. They introduced a theoretical foundation for causal abstraction in neural networks,focusing on the interchange intervention method. This method allows for the calculation of potentialcausal relationships between internal vectors of the model and its output behavior, providing a meansto trace causality from within the model to its observable behavior. By defining a causal modelthat includes input and target variables, they formalized the process of analyzing causal effectswithin models, offering a new perspective on causal abstraction for models that explicitly depend onspecific input values.\nWe finally introduce the recent work by Geiger et al., which presents a unified framework for mech-anistic interpretability by revisiting the foundational principles of causal abstraction and extendingthem to formalize a variety of interpretability methods for neural networks [8]. This frameworkbuilds upon and enhances the concept of mechanistic interpretability [3], a crucial approach withinAI interpretability focused on reverse-engineering the computational mechanisms and representa-tions learned by neural networks. Mechanistic interpretability aims to translate these learned rep-resentations into human-understandable algorithms and concepts, providing a fine-grained, causalunderstanding that is essential for aligning AI systems with human values and ensuring safe opera-tion. This emphasis underscores the increasing relevance of causality-based techniques in advancingmechanistic insights."}, {"title": "Dissucssion and Conclusion", "content": "This compact survey highlights the latest advancements in causal abstraction theory and its appli-cations for model interpretability. Causal abstraction is increasingly important in addressing mech-anistic interpretability, especially for large-scale systems where understanding exact causal driversbehind behaviors like hallucination or alignment issues [12, 3] is critical. Previous methods, such asneuron- and representation-level analyses [7, 28], while effective, are largely experimental model-ing techniques. For complex models like modern large language models, a foundational theoreticalapproach is needed, much like automaton extraction [24, 23, 27], which offered robust theoreticalgrounding but not always applicable for nowadays models.\nCausal abstraction addresses this need by supporting counterfactual reasoning and causal analysis,as evidenced in scaled applications like Alpaca 7B [25]. However, the theory remains incomplete,largely centered on equivalence rather than true abstraction, and primarily formalizes existing meth-ods. Future research could focus on (1) developing refined abstraction methods, as seen in DAS; (2)establishing exact causal dependencies between internal and external model behaviors; and (3) creat-ing a streamlined theoretical framework tailored specifically to interpretability, minimizing broadercausal theory complexities."}]}