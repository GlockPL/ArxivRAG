{"title": "WALL-E: WORLD ALIGNMENT BY RULE LEARNING\nIMPROVES WORLD MODEL-BASED LLM AGENTS", "authors": ["Siyu Zhou", "Tianyi Zhou", "Yijun Yang", "Guodong Long", "Deheng Ye", "Jing Jiang", "Chengqi Zhang"], "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-\nbased agents? While the gaps between the prior knowledge of LLMs and the specified\nenvironment's dynamics do exist, our study reveals that the gaps can be bridged by align-\ning an LLM with its deployed environment and such \u201cworld alignment\" can be efficiently\nachieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few\nadditional rules suffice to align LLM predictions with the specified environment dynam-\nics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free\nthrough LLMs, by inducing, updating, and pruning rules based on comparisons of agent-\nexplored trajectories and world model predictions. The resulting world model is composed\nof the LLM and the learned rules. Our embodied LLM agent \u201cWALL-E\" is built upon", "sections": [{"title": "1 INTRODUCTION", "content": "While large language models (LLMs) have been successfully\napplied to complex reasoning, generation, and planning tasks,\nthey are not sufficiently reliable to be deployed as an agent in\nspecific open-world environments, e.g., games, VR/AR sys-\ntems, medical care, education, autonomous driving, etc (Ope-\nnAI, 2023; Wei et al., 2022; Liu et al., 2024). A primary\nreason for the failures is the gap between the commonsense\nreasoning with prior knowledge of pretrained LLMs and the\nspecified, hard-coded environment's dynamics, which leads\nto incorrect predictions of the future states, hallucinations, or\nviolation of basic laws in LLM agents' decision-making pro-\ncess (Mu et al., 2023b; Yang et al., 2024; Das et al., 2018; Wu\net al., 2024). Although the alignment of LLMs with human\npreferences has been widely studied as a major objective of\nLLM post-training, \u201cworld alignment\" with an environment's\ndynamics has not been adequately investigated in building\nLLM agents (Hao et al., 2023; Rafailov et al., 2024; Ge et al.,\n2024). Moreover, many existing LLM agents are model-free\nand their actions are directly executed in real environments\nwithout being verified or optimized in advance within a world\nmodel or simulator (Mu et al., 2023b; Yao et al., 2023; Shinn\net al., 2024; Zitkovich et al., 2023; Wu et al., 2023; Micheli &\nFleuret, 2021; Brohan et al., 2022). This leads to safety risks\nand suboptimality of generated trajectories.\nIn this paper, we show that aligning an LLM with environment\ndynamics is both necessary and crucial to make it a promis-\ning world model, which enables us to build more powerful embodied agents. In particular, we introduce a\nneurosymbolic world model that composites a pretrained LLM with a set of newly learned rules from the\ninteraction trajectories with the environment. This specific form of world model combines the strengths of\nboth in modeling the environment dynamics, i.e., (1) the rich prior knowledge, probabilistic, and deduc-\ntive reasoning capability of LLMs (Hu & Shu, 2023); and (2) the hard constraints and rigorous guarantees\nenforced by rules (Li et al., 2024a). While creating a rule-only world model for a complex environment is\nchallenging due to the massive amount of rules and uncertainty (Xiao et al., 2021), in our method, only a few\ncomplementary rules suffice to align a pretrained LLM to specific environment dynamics. This is achieved\nby simply including these rules in the LLM's prompt without tedious training or inference. In contrast,\nexisting LLM agents usually require expensive finetuning of LLMs via RL/imitation learning on trajectory\ndata, or memory-heavy inference with a long input context of buffered trajectories (Mu et al., 2023b; Gao\net al., 2023a; Yang et al., 2024; Shinn et al., 2024)."}, {"title": "2 RELATED WORK", "content": "Recent studies have integrated LLMs with rule learning to improve reasoning and generalization capabilities\nacross various tasks, including numerical reasoning, knowledge graph exploration, and adherence to prede-\nfined rules (Yang et al., 2023a; Zhu et al., 2023c; Mu et al., 2023a; Yang et al., 2023b; Luo et al., 2023).\nHowever, prior work has not focused on aligning LLM-based world models with dynamic environments.\nOur research addresses this gap by applying rule learning to enhance model-based agent performance in\nsuch contexts. Several works have also used LLMs to construct world models for task planning by translat-\ning natural language into representations or combining LLMs with task-specific modules (Wong et al., 2023;\nGuan et al., 2023; Tang et al., 2024). Unlike these approaches, we directly employ LLMs as world models,\nleveraging their inherent knowledge for greater flexibility and efficiency. While some works use LLMs as\nworld models, typically relying on fine-tuning or human defined prompts for alignment with environment\ndynamics (Xiang et al., 2024; Xie et al., 2024; Zhao et al., 2024; Hao et al., 2023; Liu et al., 2023). Our\nmethod advances this by automatically learning rules through exploration, reducing human intervention and\nimproving performance. For a more comprehensive discussion of related work, please refer to Appendix A."}, {"title": "3 METHOD", "content": "We consider a scenario where a LLM, denoted as $f_{wm}$, is deployed in a dynamic environment for agent inter-\naction over discrete time steps. At each time step t, the agent observes the current state $s_t$, selects an action"}, {"title": "3.1 MODEL-PREDICTIVE CONTROL (MPC) OF WORLD MODEL-BASED LLM AGENTS", "content": "at, and transitions to the next state $s_{t+1}$. This transition is represented as $d_t$ = ($s_t$, $a_t$, $s_{t+1}$). A trajectory\n$\\tau$ = ($d_0$, $d_1$,..., $d_{\\tau-1}$) comprises a sequence of such transitions, capturing the agent's behavior from the\ninitial to the terminal state within an episode.\nThe LLM-based world model $f_{wm}$ predicts the subsequent state $\\hat{s}_{t+1}$ based on the current state and action:\n$\\hat{s}_{t+1} = f_{wm}(s_t, a_t)$,\nModel Predictive Control (MPC) is a widely recognized framework for model-based control. In this context,\nwe integrate MPC with the LLM-based world model $f_{wm}$ to enhance agent planning and decision-making,\nthe whole framework is illustrated in Figure 2. The objective is to determine an optimal sequence of actions\n$a_{t:t+H}$ over a finite horizon H that maximizes the expected cumulative reward. At each time step t, the\noptimization problem is formulated as:\n$a_{t:t+H}^* = arg \\max_{a_{t:t+H}} E [\\sum_{i=0}^H \\gamma^i F(\\hat{s}_{t+i+1}) ]$,\nwhere $\\gamma$ is the discount factor, and $F(\\hat{s}_{t+i+1})$ denotes the reward function.\nHowever, if the LLM-based world model is misaligned with the actual environment dynamics, the predicted\nstate $\\hat{s}_{t+1}$ may not match the true state $s_{t+1}$. This misalignment leads to incorrect reward evaluations,\nresulting in inaccurate cumulative reward estimates. Consequently, the derived action sequence $a_{t:t+H}^*$ may\nbe suboptimal or erroneous, leading to ineffective control decisions by the agent. Therefore, addressing the\nmisalignment between the LLM world model and the environment's true dynamics is crucial for ensuring\noptimal performance within the MPC framework."}, {"title": "3.2 WORLD ALIGNMENT BY RULE LEARNING (WALL-E)", "content": "In complex environments, direct state prediction is challenging due to complexity and randomness. To\naddress this, our world model uses a two-stage approach: first, it assesses action_result (e.g., success or\nfailure), then generates the subsequent state_info (provides state details) based on the action success:\n$\\hat{s}_{t+1} = (action\\_result_{t+1}, state\\_info_{t+1}) = f_{wm}(s_t,a_t)$,\nTo address potential misalignment between the $f_{wm}$ and the real environment, we introduce a rule learning\nframework, illustrated in Figure 3 and detailed in the following sections. The learned rules align the $f_{wm}\nwith the environment, enhancing state prediction accuracy and improving agent performance within the\nMPC framework.\nComparing Predicted and Real Trajectories. To find misalignments between the LLM world model and\nthe real environment, we compare action outcomes in predicted and actual next state, focusing on the binary\naction_result rather than detailed state_info. This focus provides a reliable basis for identifying discrepancies.\nLet the predicted trajectories be $\\tau^{predicted}$ = {$d$ = ($s_t$, $a_t$, $\\hat{s}_{t+1}$)}$_{t=0}^T$. Then, we may divide $\\tau^{predicted}$ into\ncorrect and incorrect transition set, and correct the wrong $\\hat{s}_{t+1}$ (see Step 1 of rule learning in Figure 3):\n$\\begin{aligned}\nD^{correct} &= \\{&correct = (s_t, a_t, s_{t+1}) \\mid \\hat{s}_{t+1} = s_{t+1}\\},\\\\\nD^{incorrect} &= \\{&incorrect = (s_t, a_t, s_{t+1}) \\mid s_{t+1} \\neq s_{t+1}\\},\\\n\\end{aligned}$\nwhere $s_{t+1}$ is the true state given by environment. Then $\\tau^{predicted}$ = $\\D^{correct} \\cup \\D^{incorrect}$. By analyzing\n$\\D^{incorrect}$, we pinpoint where the model's predictions diverge from reality, highlighting areas needing correc-\ntion through additional rules."}, {"title": "Learning New Rules from Real Trajectories.", "content": "Before address these misalignments, we prompt the LLM\n$f_{gen}$ to generate new natural language rules from real trajectories $T_{real}$ (see Appendix B.1 for detailed\nprompt). The LLM is given the task setup and state-action structures to infer new natural language rules\n$R_{NL}^{new}$ that explain the observed dynamics, ensuring they are distinct from previous rules $R_{NL}^{previous}$:\n$R_{NL}^{new} = f_{gen} (T_{real}, R_{previous}^{new})$,"}, {"title": "Refining Learned Rules.", "content": "Then, we prompt the LLM to update existing rules based on the real trajectories\n$T_{real}$ (see Appendix B.2 for detailed prompt). Early-stage rules could be inaccurate due to data drift caused\nby the limited data, so the LLM identifies conflicting rules and modifies or discards them as needed. The set\nof all existing rules up to the current point is $R_{existing} = R_{previous}^{NL} \\cup R_{NL}^{new}$, where the LLM $f_{refine}$ refines these\nrules with the real trajectories:\n$R_{NL} = f_{refine} (T_{real}, R_{existing})$,"}, {"title": "Translating Natural Language Rules to Code.", "content": "The next step is translating the refined natural language\nrules $R_{NL}$ into executable code. We prompt the LLM $f_{code\\_gen}$ to produce the code-based rule set $R_{code}$ (see\nAppendix B.3 for detailed prompt):\n$R_{code} = f_{code-gen} (R_{NL})$,"}, {"title": "Rule Set Pruning via Maximum Coverage.", "content": "In the final step, to address the inherent uncertainty and\nvariability in the LLM-driven rule-learning process, we programmatically verify and refine the rule set to\nreduce dependence on the LLM. The code-based rules $R_{code}$ are executed and validated against the labeled\npredicted transitions $\\tau^{predicted}$. Any rule that fails to predict a transition correctly is discarded, ensuring that\nonly accurate and effective rules are retained."}, {"title": "3.3 INFERENCE ON LLM AGENTS WITH LEARNED RULES", "content": "After completing the rule learning process, we obtain rules in two distinct forms: natural language rules\n$R_{NL}$ and code-based rules $R_{code}$. Both types of rules enhance the LLM world model's ability to predict the\nnext state $\\hat{s}_{t+1}$ within the planning framework: For natural language rules, these can be embedded directly\ninto the LLM's input prompt to guide the model's predictions, e.g., $\\hat{s}_{t+1} = f_{wm}(s_t,a_t, R_{NL})$. For code-\nbased rules, these are applied programmatically after the LLM generates its initial prediction, e.g., $\\hat{s}_{t+1} =$\nApplyRules($f_{wm}(s_t, a_t)$, $R_{code})$. Here, the function ApplyRules serves as a verification layer, overriding the\nLLM's prediction if an active rule contradicts the generated outcome. For further details on rule activation,\nrefer to Appendix G.\nBy integrating learned rules, the aligned LLM world model enhances the agent's planning process signifi-\ncantly. This alignment allows the agent to more effectively obtain optimal action sequences $a_{t:t+H}$ through\ntwo key improvements: First, the alignment leads to more accurate reward evaluations $F(\\hat{s}_{t+1})$, increas-\ning the likelihood of selecting optimal action sequences $a_{t:t+H}$ within the MPC framework. Second, the\naligned world model, equipped with learned rules, provides high-quality feedback that helps the agent\nrefine $a_{t:t+H}$ effectively. Along with predicting action results and state information, it offers auxiliary infor-\nmation when an action is predicted to fail, including:\n\\begin{itemize}\n    \\item Feedback: A textual explanation of the failure based on violated rules.\n    \\item Suggestion: Recommendations for corrective actions or improvements based on the current state,\n    action taken, and violated rules.\n\\end{itemize}\nThis information is crucial when an action fails, guiding the agent in revising its strategy by exploring\nalternatives or adjusting its approach(see Appendix D.2 for examples).\nIn conclusion, integrating learned rules improves the LLM world model's prediction accuracy and provides\nactionable feedback, enabling more efficient and adaptive planning."}, {"title": "4 EXPERIMENTS", "content": "We evaluate the environment modeling and task-solving ca-\npabilities of WALL-E on open-world environments using the\nMinecraft (Fan et al., 2022) and ALFWorld (Shridhar et al.,\n2020b) benchmarks. Compared to state-of-the-art (SOTA)\nLLM/VLM agents, WALL-E achieves higher success rates\nwith lower costs in terms of replanning time and token us-\nage for reasoning. Notably, in Minecraft, WALL-E surpasses\nbaselines by 15-30% in success rate while costing 8-20 fewer\nreplanning rounds and only 60\u201380% of tokens. In ALFWorld,\nit achieves a record of 95% success rate only after 6 iterations,\nsignificantly exceeding other planning-based methods such as\nRAFA (Liu et al., 2023). Moreover, integrated with our pro-\nposed rule learning method, WALL-E achieves a 15% higher\nsuccess rate than methods relying on a long input context of\nbuffered trajectories. These highlights demonstrate WALL-\nE's superior efficiency and effectiveness in complex and open-\nworld environments."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Benchmarks. Minecraft is a popular open-world environment. We employ the standard evaluation\npipeline provided by MineDojo's TechTree tasks (Fan et al., 2022). These tasks can be categorized into\nsix levels of increasing difficulty: Wood, Stone, Iron, Gold, Diamond, and Redstone (see Appendix E.1 for\ndetails). ALFWorld is a virtual environment designed as a text-based simulation where agents perform\ntasks by interacting with a simulated household environment (Shridhar et al., 2020b). This benchmark\nincludes six distinct task types, each requiring the agent to accomplish a high-level objective, such as\nplacing a cooled lettuce on a countertop (see Appendix E.2 for details).\nMetrics. (1) Success rate (higher is better): the percentage of tasks the agent completes successfully. (2)\nReplanning rounds (lower is better): the number of times the agent revisits the same task to revise its\nplan for recovering from the failed task planning. (3) Token cost (lower is better): the number of tokens\nconsumed by LLM agent/world models during task completion. For Minecraft, we select four tasks from\neach level to serve as the testing set and the remaining tasks to construct the training set. All these three\nmetrics are employed in our experiment. The task will be marked incomplete if the agent either dies in\nthe environment (such as being killed by hostile mobs or falling into lava) or reaches one of the following\nmaximal budgets: 10-minute time limit and maximum replanning rounds. In these cases, the replanning\nrounds and token cost will be set to the maximal value. For ALFWorld, we train WALL-E on the designated\ntraining set and evaluate its performance on a set of 134 predefined testing tasks. The averaged success rate\nover several trials is used as the evaluation metric to measure the performance of all baselines."}, {"title": "4.2 MAIN RESULTS", "content": "We conduct a detailed comparison of WALL-E and existing baseline methods in Tables 1, 2 and 3, to\ndemonstrate its superior performance in terms of success rate, planning efficiency, and token cost consumed\nby LLMs across diverse tasks.\nWALL-E demonstrates superior planning and task-solving abilities. Tables 1 and 3 show that our\nmethod achieves the highest success rates across different environments. Specifically, in the Minecraft\nenvironment, WALL-E outperforms other baselines by an impressive margin of 15-30%. Figure 4 shows"}, {"title": "4.3 EFFECTIVENESS OF RULE LEARNING", "content": "In order to demonstrate the effectiveness of our proposed rule learning method, we conduct a comparative\nstudy against GITM (Zhu et al., 2023b)- a method employing buffered trajectories as in-context examples to"}, {"title": "4.4 ABLATION STUDY", "content": "We conduct a comprehensive ablation study to evaluate the importance of various components in WALL-\nE. Specifically, we separately remove the learned rules and the world model and check their effects on\nWALL-E's final performance. According to the results in Table 4, we give the following conclusions.\n(1) Regardless of whether the learned rules are applied within the agent or the world model, adding them\nsignificantly enhances the total performance. The success rate increases by 20% to 30% approximately.\nThis observation underscores the crucial role that rules play in improving the effectiveness of WALL-E. (2)\nWhen the learned rules are utilized within the world model, they contribute to nearly a 30% improvement in\nsuccess rate, whereas using rules within the agent result in about a 20% improvement. This disparity may\nbe primarily due to the fact that the learned rules are highly related to the state information (See Appendix\nD for more details). (3) MPC using a world model without applying any rules cannot significantly improve\nWALL-E's performance in terms of the success rate and the number of replanning times. This finding\nsuggests that the alignment between the world model and the environment dynamics by rule learning is\ncrucial to our appealing results."}, {"title": "5 CONCLUSION", "content": "We have shown that LLMs can effectively serve as world models for agents when aligned with environment\ndynamics through rule learning. Our neurosymbolic approach bridges the gap between LLMs' prior knowl-\nedge and specific environments without gradient updates. By integrating a rule-enhanced LLM-based world"}, {"title": "A DETAILED RELATED WORK", "content": "Recent studies have explored integrating LLMs with rule learning to enhance\nreasoning and model behavior. For instance, Yang et al. (2023a) introduced rule distillation, enabling LLMs\nto learn from predefined rules, which improved generalization with limited training data. Similarly, Zhu et al.\n(2023c) proposed the Hypotheses-to-Theories (HtT) framework, which enhanced numerical and relational\nreasoning by generating and validating rules from training data. In the same vein, Mu et al. (2023a) devel-\noped the RuLES framework to evaluate LLM adherence to developer-specified rules, addressing challenges\nlike rule evasion through adversarial inputs. Furthermore, Yang et al. (2023b) presented the Tuning-free Rule\nAccumulation (TRAN) framework, allowing LLMs to accumulate rules from incorrect cases to avoid repeat-\ning mistakes without additional tuning. Lastly, in knowledge graph reasoning, Luo et al. (2023) introduced\nChatRule, a framework that mines logical rules over knowledge graphs using LLMs.\nThese studies show the potential of combining LLMs with rule learning to improve reasoning and general-\nization. However, none have integrated rule learning with LLM-based world models, which is the focus of\nour work. We explore how rule learning can align LLM world models with specific environment dynamics,\nthereby improving the performance of model-based agents in dynamic environments."}, {"title": "Using LLMs to Build World Models.", "content": "Many studies have leveraged LLMs to construct world models for\nplanning. For example, Wong et al. (2023) proposed translating natural language instructions into adaptable\nplanning representations via LLMs, enabling flexible and context-aware world modeling. Similarly, Guan\net al. (2023) showed that combining pre-trained LLMs with task-specific planning modules improves task\nsuccess rates by providing a more detailed understanding of the environment. Another approach, World-\nCoder Tang et al. (2024), exemplified an LLM agent that constructs world models by generating and execut-\ning code to simulate various states and actions, refining its understanding iteratively.\nThese studies demonstrate the utility of LLMs in building world models to improve planning and reasoning\nin complex environments. However, unlike these works, our approach directly employs the LLM as the\nworld model, utilizing its inherent knowledge and reasoning abilities without an explicit model-building\nphase. This direct use of LLMs enhances adaptability and computational efficiency."}, {"title": "LLMs with Rule Learning.", "content": "Some methods rely on fine-tuning to align the LLM world model\nwith the environment. For example, Xiang et al. (2024) fine-tuned LLMs with embodied experiences in\na simulated world to enhance reasoning and planning abilities in embodied environments. Similarly, Xie\net al. (2024) transformed LLMs into world models by incorporating knowledge of action preconditions and\neffects, fine-tuning the models to reason about actions and predict their outcomes accurately.\nOther approaches align LLMs as world models through prompting. For instance, Zhao et al. (2024) intro-\nduced the LLM-MCTS algorithm, prompting LLMs to serve as both the policy and world model for large-\nscale task planning, integrating commonsense priors with guided search. In another approach, Hao et al.\n(2023) introduced Reasoning via Planning (RAP), where LLMs are prompted to act as reasoning agents and\nworld models by generating reasoning trees to explore solutions. Finally, (Liu et al., 2023) used a Bayesian\nadaptive Markov Decision Process to guide LLMs in planning future trajectories, prompting them to predict\nfuture states.\nWhile these approaches demonstrate the potential of using LLMs as world models, they often require exten-\nsive fine-tuning or rely heavily on human-crafted prompts, making them labor-intensive and inflexible. Our\nwork overcomes these limitations by automatically extracting rules from exploration experiences, reducing\nhuman effort and enhancing adaptability across different environments."}, {"title": "B DETAILED PROMPT", "content": null}, {"title": "B.1 LEARN NEW RULES FROM REAL TRAJECTORIES", "content": "Prompt for Learning New Rules from Real Trajectories\nYou are responsible for mining new rules from the given transitions, ensuring\nthat these rules differ from the ones already provided.\nFocus on generating general and universal rules that are not tied to any\nspecific item or tool.\nYour goal is to generalize across different objects, creating flexible rules\nthat can be applied broadly to diverse contexts and situations.\nI will give you an array of transitions:\n[\n{\n'state_0': {\n}\n},\n{\n},\n\"state feature 1\": {\"feature name\": value, ...},\n'action': {\n},\n\"name\": \"action name\",\n\"action feature 1\": {\"feature name\": value, ...},\n'action_result': {\n\"feedback\": \"the environment feedback\",\n\"success\": \"Whether the action is executed successfully, give 'True' or\n'False' only\",\n\"suggestion\": \"If the 'action' fails, 'suggestion' would be given based\non 'state 0' and 'action'\"\n'state_0':\n{\n\"state feature 1\": {\"feature name\": value, ...},\n},\n'action': {\n},\n\"name\": \"action name\",\n\"action feature 1\": {\"feature name\": value, ...},\n'action_result': {\n\"feedback\": \"the environment feedback\",\n\"success\": \"Whether the action is executed successfully, give 'True' or\n'False' only\",\n\"suggestion\": \"If the 'action' fails, 'suggestion' would be given based\non 'state 0' and 'action'\"\n}\n}\n]\nand an array of rules:\n["}, {"title": "RESPONSE FORMAT:", "content": "{\n] \n}\n\"new_rules\":[\n\"Rule ...: For action ...; Checking Method: ...\",\n\"Rule . For action .; Checking Method: ...\",\nInstructions:\n- Ensure the response can be parsed by Python 'json.loads', e.g.: no trailing\ncommas, **no single quotes**, etc.\n- Please use you knowledge in <ENV>, do inductive reasoning. You need to dig up\nas many rules as possible that satisfy all transitions.\nExtract and utilize only the features that influence the outcome of the\naction.\nPlease generate general and universal rules; the rules should not reference\nany specific item or tool! You need to generalize across various items or\ntools.\nGenerate only the rules under what conditions the action will fail.\nWhile generating a rule, you also need to state how to check if a transition\nsatisfies this rule. Please be specific as to which and how 'features' need\nto be checked"}, {"title": "B.2 REFINE LEARNED RULES", "content": "Prompt for Refining Learned Rules\nYou are responsible for improving the existing rules by verifying that they\nhold true for all transitions.\nThis involves identifying any conflicting rules, diagnosing potential issues,\nand making necessary modifications.\nEnsure that the refined rules are consistent and correctly align with the\ntransitions provided, avoiding any contradictions or overlaps.\nI will give you an array of transitions:\n[\n{\n'state_0': {\n}\n},\n\"state feature 1\": {\"feature name\": value, ...},\n'action': {\n}\n\"name\": \"action name\",\n\"action feature 1\": {\"feature name\": value, ...},"}, {"title": "RESPONSE FORMAT:", "content": "},\n{},\n'action_result': {\n\"feedback\": \"the environment feedback\",\n\"success\": \"Whether the action is executed successfully, give 'True' or\n'False' only\",\n\"suggestion\": \"If the 'action' fails, 'suggestion' would be given based\non 'state 0' and 'action'\"\n'state_0':\n{\n\"state feature 1\": {\"feature name\": value, ...},\n}\n},\n]\nand an array of rules:\n[\n]\n\"Rule 1: For action if..., the action will fail; Checking Method:\n..\",\n\"Rule 2: For action if..., the action will fail; Checking Method:\n...\nRESPONSE FORMAT:\n{\n\"verified_rules\":[\n\"Rule ...: For action ; Checking Method: ...\",\n\"Rule ... For action .; Checking Method: ...\",\n],\n\"conflicting_rules\":[\n\"Rule ...: For action ; Checking Method: ...\"\n\"Rule ... For action ; Checking Method:\n..\",\n],\n\"improved_rules\":[\n\"Rule ...: For action ; Checking Method: ...\",\n\"Rule ...: For action ; Checking Method:\n...\n],\n\"final_rules\":[\n\"Rule ..: For action ; Checking Method: ...\"\n\"Rule ... For action ; Checking Method:\n..\",\n]\n}\nwhere\nverified_rules: list rules that satisfy all the provided transitions.\nconflicting_rules: list rules that contradict any of the transitions. Modify\nthese rules if they can be modified correctly and put them in\n'improved_rules'.\nimproved_rules: show modified 'conflicting_rules'.\nfinal_rules: combine all the rules from 'verified_rules', 'new_rules'.\nInstructions:\nEnsure the response can be parsed by Python 'json.loads', e.g.: no trailing\ncommas, **no single quotes**, etc.\nPlease use you knowledge in , do inductive reasoning. You need to dig up\nas many rules as possible that satisfy all transitions.\nExtract and utilize only the features that influence the outcome of the\naction.\nPlease generate general and universal rules; the rules should not reference\nany specific item or tool! You need to generalize across various items or\ntools.\nGenerate only the rules under what conditions the action will fail.\nWhile generating a rule, you also need to state how to check if a transition\nsatisfies this rule. Please be specific as to which and how 'features' need\nto be checked"}, {"title": "B.3 TRANSLATE NATURAL LANGUAGE RULES TO CODE", "content": "Prompt for Translating Natural Language Rules to Code\nYou are responsible for generating code rules by implementing the learned rules\nin Python.\nYour task is to write a function that takes the current state and an action as\ninputs, evaluates these conditions, and returns a Boolean value based on\nthe specified rule.\nThis function should effectively mirror the logic of the rules, enabling\nprecise predictions for various state-action pairs.\nThe function should be defined as follows:\n```python\ndef expected_rule_code (state, action):\n# Your code here\n return feedback, success, suggestion\nwhere\nfeedback: a string, give the action feedback based on success or not."}, {"title": "Examples for Minecraft's State Space", "content": "state\n{\n\"equipment\": {\n\"dirt\": 60.0,"}, {"title": "Minecraft's Action Space", "content": "craft (obj, materials, platform): craft the object with the materials and\nplatform; used to craft new object that is not in the inventory or is not\nenough.\n- obj: a dict, whose key is the name of the object and value is the object\nquantity, like {\"crafting table\": 1} and {\"stone pickaxe\": 1}.\n- materials: a dict, whose keys are the names of the materials and values are\nthe quantities, like {\"planks\": 4} and {\"cobblestone\": 3, \"stick\": 2}.\n- platform: a string, the platform used for crafting the current 'object', like\n\"furnace\" and \"crafting table\". Set to null if without any platform.\nmine (obj, tool, y_level): dig down to the y-level and mine the specified object\nwith the tool. This action will go underground and continuously mine the\nobject until the desired quantity is obtained.\n- obj: a dict, whose key is the name of the object and value is the object\nquantity, like {\"stone\": 5} and {\"iron ore\": 1}.\n- tool (string): the tool used for mining, like \"wooden pickaxe\". Set to null\nif without any tool."}, {"title": "Examples for ALFWorld's State Space", "content": "state =\n{\n\"reachable_locations\": [\n\"cabinet 5\",\n\"cabinet 4\","}, {"title": "Action Space for Minecraft", "content": "go to [location/object]: Move to a specified location or object.\nopen [object]: Open a specified object like a cabinet or drawer.\nclose [object]: Close an opened object.\ntake [object] from [location]: Pick up an item from a specified location.\nput [object] in/on [location]: Place an item in or on a specified location.\nclean [object] with [location/tool]: Clean an object using a specific location\nor tool, like cleaning lettuce at the sink basin.\nheat [object] with [tool): Use an appliance, such as a microwave, to heat an\nitem.\ncool [object] with [tool): Use a cooling tool or appliance, such as a fridge,\nto cool an item."}, {"title": "D LEARNED RULES", "content": "There are two points to note about the numbering of the rules:\n\\begin{itemize"}, "n    \\item The reason for duplicates is that the numbering is based on actions, and different actions have their\n    own separate sequences. For example: Rules for Craft: [Rule 1, Rule 2,"]}