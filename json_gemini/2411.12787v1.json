{"title": "Visual Cue Enhancement and Dual Low-Rank Adaptation for Efficient Visual Instruction Fine-Tuning", "authors": ["Pengkun Jiao", "Bin Zhu", "Jingjing Chen", "Chong-Wah Ngo", "Yu-Gang Jiang"], "abstract": "Fine-tuning multimodal large language models (MLLMs) presents significant challenges, including a reliance on high-level visual features that limits fine-grained detail comprehension, and data conflicts that arise from task complexity. To address these issues, we propose an efficient fine-tuning framework with two novel approaches: Vision Cue Enhancement (VCE) and Dual Low-Rank Adaptation (Dual-LoRA). VCE enhances the vision projector by integrating multi-level visual cues, improving the model's ability to capture fine-grained visual features. Dual-LoRA introduces a dual low-rank structure for instruction tuning, decoupling learning into skill and task spaces to enable precise control and efficient adaptation across diverse tasks. Our method simplifies implementation, enhances visual comprehension, and improves adaptability. Experiments on both downstream tasks and general benchmarks demonstrate the effectiveness of our proposed approach.", "sections": [{"title": "1. Introduction", "content": "Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across a wide range of tasks, showcasing versatility and effectiveness in diverse applications. Existing MLLMs [1, 14, 15] typically integrate pretrained Large Language Models (LLMs) [5, 26] with pretrained vision encoders [16, 22], using a vision projector to connect visual features to the LLMs.\nThe standard training process for MLLMs generally involves two stages: (1) a vision projector pretraining stage, where only the vision projector is trained to align visual features for the LLMs, and (2) an instruction tuning stage, where both the vision projector and the LLM are trained to perform visual instruction tasks. To reduce the extensive number of parameters in LLM tuning, low-rank adaptation (LoRA) [8] was proposed by injecting lightweight adapters into pretrained LLMs for efficient fine-tuning. The vision projector pretraining aligns visual and language feature spaces, while instruction tuning equips MLLMs to generate responses aligned with human instructions. The goal of these two stages is to equip MLLMs with sufficient visual cues and to ensure accurate instruction-based responses.\nDespite the great success, existing MLLMs have two main limitations. On the one hand, in the vision projector pretraining stage, current methods primarily rely on high-level visual features, often overlooking low-level and fine-grained details, which limits visual comprehension. For example, most works [2, 15] use only the high-level semantic feature map for vision token projection, while some [24, 31] leverage multi-layer visual features yet still miss finer details. On the other hand, in the instruction-tuning stage, as downstream tasks grow in diversity and complexity, data conflicts in LoRA-based instruction tuning become more prominent [4, 9]. To mitigate this, recent studies [4, 18] incorporate the Mixture of Experts (MoE) paradigm into LORA modules, embedding multiple LoRA units within a single linear layer so that each unit can leverage its specific strengths. To better accommodate varied tasks, LoRA experts with different levels of granularity [] or customized routing strategies [18] have been introduced. However, this complex design not only increases implementation complexity but may also extend the time required for training and inference. We thus raise the question: Is there a simple and unified approach to flexibly represent any group of LORA experts?\nIn this paper, we address existing limitations by proposing Vision Cue Enhancement (VCE) and Dual Low-Rank Adaptation (Dual-LoRA) to improve the vision projector pretraining stage and the instruction finetuning stage respectively. An overview of our overall training pipeline is shown in Figure 1. For the VCE module, we extend feature capture to multi-level vision feature maps instead of relying solely on high-level semantic feature maps. Specifically, for each patch in the high-level feature map, we extract neighboring patches from the middle-layer feature maps and combine them with the original patch to create a locally enhanced feature patch. This approach enhances local visual cues while maintaining computational efficiency. To address data conflicts during MLLM instruction tuning, we introduce Dual-LoRA, a unified adaptation module that decouples learning into a skill low-rank space and a task-activation low-rank space. The skill space captures specific knowledge in downstream tasks, while the task space acti-"}, {"title": "2. Related Works", "content": "Multi-Modal Large Language Models (MLLMs) integrate pretrained vision encoders with pretrained large language models (LLMs), showcasing remarkable instruction-following and generalization capabilities. Early models like Flamingo and BLIP-2 successfully adapted LLMs for visual tasks, demonstrating strong zero-shot generalization and in-context learning abilities. Visual instruction tuning, a key technique in recent studies [14], has further enhanced MLLMs by using vision-language (VL) datasets and improving visual instruction-following data. Additionally, some research approaches vision as a language, employing textual annotations (e.g., location descriptions of bounding boxes) to train MLLMs effectively.\nEnhancing Vision Feature Representation in MLLMS primarily involves vision encoder ensembling, resolution enhancement, and multi-level feature map fusion. For vision encoder ensembling, MMVP [25] utilizes a Mixture of Features (MoF) strategy to integrate image features from CLIP-VIT [22] and DINOv2 [20]. Similarly, MouSi [7] uses an ensemble technique to leverage the strengths of individual vision encoders, introducing a fusion network to unify outputs from different encoders, such as CLIP, DINOv2, and SAM. DeepSeekVL [19] adopts a hybrid vision encoder design, encoding images by combining SigLIP-L for low-resolution inputs and SAM-B for high-resolution inputs. LLaVA-UHD [29] organizes input images for efficient and scalable encoding, utilizing a compression module to further condense image tokens from visual encoders. Some approaches focus on fusing multi-level feature maps. For example, Cambrian-1 [24] introduces the Spatial Vision Aggregator, a dynamic, spatially-aware connector that integrates high-resolution visual features with LLMs while minimizing token count. While most of these methods require full fine-tuning of the LLM, we propose a lightweight visual cue enhancer for more efficient fine-tuning.\nLow-Rank Adaptation (LoRA) [8] introduces a method that freezes pretrained model weights and injects trainable low-rank decomposition matrices into the linear layers of the Transformer architecture, significantly reducing the number of trainable parameters required for downstream tasks. Mixture of Experts in LoRA (MoE-LoRA) has gained popularity for addressing data conflicts in down-"}, {"title": "3. Method", "content": "3.1. Preliminaries\nMulti-Modal Large Language Models (MLLMs). A standard MLLM (e.g., LLaVA [14]) consists of an image feature encoder V(\u00b7), a vision feature projector P(\u00b7), and a large language model G(\u00b7). Given an image-question-answer (VQA) triplet (I, Tque, Tans), V extracts image features, which are then projected into vision tokens tvision through P. These vision tokens are concatenated with question text tokens tque, and the combined sequence is passed to G to generate predicted answer tokens tpred. Instruction tuning aims to optimize the parameters of P and G so that tpred closely matches the target answer tokens tans:\n\\{\n\"equation\": \"\\theta_{P}, \\theta_{G} = \\arg \\min_{\\theta_{P},\\theta_{G}} L(t_{\\text{pred}}, t_{\\text{ans}}),\"\n\\}\n\\{\n\"equation\": \"t_{\\text{pred}} = G(t_{\\text{vision}}, t_{\\text{que}}; \\theta_{G}),\"\n\\}\n\\{\n\"equation\": \"t_{\\text{vision}} = P(V(I); \\theta_{P}).\"\n\\}\nHere, \\(\\theta_{P}\\) and \\(\\theta_{G}\\) represent the trainable parameters of P and G, respectively, and L(\u00b7, \u00b7) denotes the loss function, typically the cross-entropy loss.\nLow-Rank Adaptation (LoRA). Due to the large number of parameters in G, optimizing \\(\\theta_{G}\\) can be highly resource-intensive. To address this, LoRA [8] has been proposed as a lightweight adapter injected into the linear layers of LLMs. LoRA significantly reduces the number of trainable parameters by decomposing the transformation of the input feature into a low-rank form. The LoRA module is represented by two parameter matrices, A and B with the same rank r. In the i-th injected layer, let \\(W_i\\) denote the pre-trained weights and \\(x_i\\) denote the input feature for this layer; then the output after applying the LoRA module can be expressed as:\n\\{\n\"equation\": \"z_{i} = W_{i}x + \\frac{\\alpha}{r}B_{i}A_{i}x,\"\n\\}\nwhere \\(z_i\\) is the output feature passed to the next layer, and \u03b1 is a scaling factor. For convenience, we omit the indices i in the following. With LoRA, the trainable parameters of F are reduced to \\{\\theta_{P}, A, B\\}.\n3.2. Framework Overview\nWe propose a two-stage approach for efficient Visual In-struction Tuning: (1) Enhanced Vision Projector Pretraining"}, {"title": "3.3. Multi-level Local Visual Cue Enhancement", "content": "Typical vision projectors primarily rely on high-level vision feature maps to align vision and language modalities. For example, LLaVA [14] and Qwen-VL [1] use the penultimate layer output of ViT, which captures high-level, language-aligned semantic features but may overlook finer, detail-oriented visual cues.\nTo address these limitations, we propose a lightweight Vision Cues Enhancement (VCE) module. This module uses the final vision feature map as an anchor feature and applies a single deformable cross-attention mechanism to neighboring patches across other layer feature maps for each patch of the anchor feature. An intuitive illustration is shown in Figure 3 (a).\nDenote F = [f] as the multi-level feature maps produced by the vision encoder V. We refer to the high-level feature map used for projection into vision tokens as the anchor feature fa, while several intermediate layers are designated as reference feature maps [fref]. The vision encoder processes the input image I through multiple blocks to extract these multi-level features F, formulated as:\n\\{\n\"equation\": \"[f^{\\text{ref}}], f^{a} = V(I; \\text{select-index}),\"\n\\}\nwhere select-index specifies the index of reference feature maps.\nTo enhance the vision cues, we integrate local visual details from [fref] into fa. We utilize deformable attention [28] to efficiently compute cross-attention across multi-level feature maps, applied between [fref] and fa:\n\\{\n\"equation\": \"f^{*} = \\text{deform-attn}(f^{a}, [f^{\\text{ref}}]; \\Theta_{\\text{deform-attn}}),\"\n\\}\nwhere \\(\\Theta_{\\text{deform-attn}}\\) represents the trainable parameters of the deformable attention module, deform-attn(\u00b7). The resulting f* is the enhanced feature map, combining high-level semantic information with local vision cues from multi-level feature maps.\nAfter obtaining the anchor feature fa and the enhanced visual cue f*, we add them up and apply normalization to produce the final enhanced vision feature map fvision. This feature map is subsequently projected into vision tokens tvision through the vision feature projector P:\n\\{\n\"equation\": \"f_{\\text{vision}} = \\text{norm}(f^{a} + f^{*}),\"\n\\}\n\\{\n\"equation\": \"t_{\\text{vision}} = P(f_{\\text{visual}}; \\theta_{P}).\"\n\\}\nThe enhanced vision token tvision is then concatenated with text tokens and fed into the multimodal model G for further processing."}, {"title": "3.4. Dual Low Rank Adaptation", "content": "LORA assumes that in large pre-trained models, the model parameters have a high degree of redundancy, with a low-rank structure that can be effectively represented through low-rank matrix approximation. However, as downstream tasks become more complex or diverse, data conflicts can arise due to the limited representational capacity of the low-rank structure [4, 9]. Some methods integrate Mixture of Experts into LoRA to better adapt to diverse downstream tasks. They propose expert modules with different granularities [10] and use various routing strategies [4, 10, 18, 27].\nIn contrast, we propose Dual Low-Rank Adaptation (Dual-LoRA), a unified approach that decouples adaptation into a task-skill dual low-rank structure, removing the need for complex expert splitting or intricate routing strategies. Figure 3 (b) and (c) illustrate the differences between our method and other strategies for mitigating data conflicts.\nProposition 1: For a set of LoRAs \\(\\{B_{k}A_{k}\\}k=1^{K}\\), each with rank 1, where K is the total number of LoRAs, the representation space of a single LoRA with rank K contains, and"}, {"title": "3.4.1. Skill Low-Rank Space", "content": "We propose the construction of a skill low-rank space to encapsulate specific knowledge for downstream tasks while ensuring domain knowledge consistency. According to Corollary 1, a single LoRA with equivalent parameters can theoretically represent any configuration of LoRA experts across various ranks without loss of expressive power. Let S represent the skill space of the unified LoRA, modeled by a parameter metric S.\nIn practice, however, a split LoRA expert group often outperforms a single, larger LoRA [10]. This can be attributed to variations in routing strategies and initialization across individual LoRAs. For example, LLaVA-MOLE [4] activates the top-k experts for targeted learning, while RODE [10] employs heterogeneous LoRA experts, each initialized with distinct distributions. The complex training process, particularly the routing score, leads to divergence in the representational spaces learned by different LoRAs over time. The complex training process, especially the routing score, leads to divergence in the representational spaces learned by different LoRAs over time. Intuitively, even for the same input, each expert may experience unique gradients due to its intrinsic numerical differences and routing scores.\nThis observation suggests the need for an additional space to mediate between the instruction task and the skill space S, allowing S to be allocated with numerical differences according to instruction variance."}, {"title": "3.4.2. Rectified Skill Low-Rank Space", "content": "Based on the skill low-rank space, we aim to activate prior knowledge in the task space guided by specific instructions. According to Corollary 2, additional space can be used to map the skill space S, enabling the simulation of sparsely activated experts that respond to varying instructional tasks. We use the parameter matrix T to modulate S, which is mapped through a non-linear activation \u03c3 to achieve the effect of rank rectification. In this paper, we use the ReLU [11] activation function due to its sparsity-inducing properties and ease of optimization. The rectified, decomposed feature space, mapped from the skill and task spaces, can then be expressed as:\n\\{\n\"equation\": \"D(x) = \\frac{\\alpha}{r} B(\\text{Sx} \\cdot \\sigma(\\text{Tx})),\"\n\\}\nwhere D denotes the Dual-LoRA module. To smooth the distribution of the skill space and improve stability in subsequent computations, we normalize the skill space using layer normalization, which can be easily implemented thanks to the unified single LoRA design. With normalization applied to the skill space, Equation 9 can be revised as:\n\\{\n\"equation\": \"D(x) = \\frac{\\alpha}{r} B(\\text{Norm}(\\text{Sx}) \\cdot \\sigma(\\text{Tx})).\"\n\\}\nFinally, we combine the adaptation feature generated by D with the feature generated from the pre-trained weight W to obtain the output feature z:\n\\{\n\"equation\": \"z = W(x) + \\frac{\\alpha}{r} B(\\text{Norm}(\\text{Sx}) \\cdot \\sigma(\\text{Tx})).\"\n\\}"}, {"title": "3.5. Training", "content": "Our method consists of two training stages, as shown in Figure 1. In the vision projector pretraining stage, we train the parameters of the vision projection modules, i.e., \\{\\theta_{P}, \\Theta_{\\text{deform-attn}}\\}, using descriptive image-caption pairs. In the subsequent visual instruction fine-tuning stage, we enable additional training for the Dual-LoRA parameters, where \\{\\theta_{P}, \\Theta_{\\text{deform-attn}}, S, T, B\\} are set to be trainable, leveraging the full range of downstream tasks."}, {"title": "4. Experiments", "content": "4.1. Model and Optimizer Configuration\nFor the base pre-trained multimodal large language model, we use the widely adopted open-source model LLAVA-1.5-7B [14]. LLAVA-1.5-7B integrates a CLIP ViT-L [22] with an image resolution of 336px and a patch size of 14, a two-layer MLP projector to map visual features into tokens, and Vicuna v1.5 [5] as the language model. In our setup, we omit any special tokens, such as image indicator tokens, to enclose visual tokens. For the VCE module configuration, we use the outputs from the 2nd-to-last, 8th-to-last, 14th-to-last, and 20th-to-last layers of CLIP VIT-L as reference feature maps (see Section 3.3 for details). In the Dual-LoRA setting, we set the LORA dropout rate to 0.05.\nOur VCE module requires only 5.52 MB of memory, compared to the 40.02 MB needed for the vision projector in LLAVA-1.5-7B.Furthermore, our Dual-LoRA task consumes approximately 1.5 MB per LoRA rank when injected solely into the query projection and value projection layers. For optimization, we use the Adam optimizer across all trainable parameters. During the pretraining stage, we set a global batch size of 256 and a learning rate of 0.001 with no weight decay. In the fine-tuning stage, we use a global batch size of 128, setting the learning rate to 0.0002 for the LMM adapter and 0.00002 for the vision feature projector, respectively."}, {"title": "4.2. Evaluation on Downstream Tasks", "content": "4.2.1. Experiment Setup\nDataset and Downstream Tasks. We begin by evaluating our proposed method on downstream fine-tuning tasks using the UniFood dataset [10], a multi-task dataset with 100,000 samples in the food domain. The tasks in UniFood include ingredient recognition, recipe generation, and nutrition estimation. For ingredient recognition, we use Intersection over Union (IoU) and F1-score as evaluation metrics. For recipe generation, we assess performance with Sacre-BLEU [21] and Rouge-L [13]. For nutrition estimation, we use the percentage of mean average error (pMAE) [32] as the metric.\nBaselines. We select several Mixture of Experts (MoE) methods within the LoRA framework: a sparse activation method, MoE-LoRA (top-2), which activates the top-2 experts; a dense activation method, MoE-LoRA (softmax), which uses a softmax router to engage all experts; and a rectified activation method, RoDE [10], with rectified diverse expert activation. Both MoE-LoRA (top-2) and MoE-LORA (softmax) are configured with ranks [16, 16, 16, 16], while RoDE is set with varying ranks [32, 16,8,8]. Additionally, we include a standard LoRA baseline with rank 64, and our Dual-LoRA is also set to rank 64.\nImplementation Details. We conduct our experiments on a setup with 8 RTX 4090 GPUs. The LLM adapters are integrated only within the linear layers of the query and value projectors in the transformer blocks, allowing us to train the model on a single RTX 4090 GPU if necessary.\n4.2.2. Performance Comparison\nTable 1 presents the results of our method compared to baseline methods on the UniFood dataset. As shown, our Dual-LoRA method consistently outperforms the sparse expert activation method (Top-2), the dense expert activation method (Softmax), and the rectified expert activation method (RODE) across all tasks. Additionally, incorpo-"}, {"title": "4.3. Evaluation on General MLLM Benchmarks", "content": "4.3.1. Experiment Setup\nMLLM Benchmarks. For the general benchmark evaluation, following LLaVA [14], we use a 558K subset of the LAION-CC-SBU dataset [23] and the llava-v1.5-mix665k dataset [14] for instruction fine-tuning. Four widely used MLLM benchmarks are used for evaluation, including MMBench [17], SEED-Bench [12], and LLaVA-Bench In-the-Wild (LLAVAW) [14]. The first three benchmarks assess various MLLM capabilities, such as perceptual understanding and visual reasoning, through binary yes/no questions (MME) or multiple-choice questions (MMBench and SEED-Bench). We use the image-only subset of SEED-Bench (SEED), a popular choice for many image-based MLLMs [2, 30].\nImplementation Details. Our experiments are conducted on a setup with 4 x A100 GPUs (80GB). The selected layers for LLM adapter injection match those used in LLAVA-1.5-7B-LORA [14], with the adapter rank r set to 128 and scaling factor \u03b1 set to 256 in Equation 11.\n4.3.2. Performance Comparison\nThe results of our proposed method, alongside state-of-the-art MLLMs, are presented in Table 2. The Dual-LoRA + VCE model consistently ranks among the top two across all benchmark tests. Compared to other methods, this model demonstrates robust performance across multiple benchmarks, indicating excellent generalization and strong adapt-"}, {"title": "4.4. Qualitative Results", "content": "Figure 4 presents a comparison between our proposed method and the vanilla LORA fine-tuned version of LLAVA-1.5-7B on food-related downstream tasks. The vanilla LoRA demonstrates knowledge inconsistency, with the ingredient list failing to align with the generated cooking instructions. In contrast, our method produces more accurate and knowledge-consistent answers, ensuring the ingredient list aligns with the cooking instructions. These results visually underscore the effectiveness of our approach."}, {"title": "4.5. Ablation Study", "content": "In this section, we conduct ablation studies to verify different components of our method on UniFood dataset. The setup is the same as Section 4.2.1.\nVCE and Dual-LoRA. We conducted an ablation study to analyze the impact of different projector and adapter configurations on the performance of our model in ingredient recognition and recipe generation. The results are shown in Table 3. This ablation study shows that both multi-level local visual feature enhancement (VCE) and Dual-LoRA positively impact performance. When both are enabled, the best results are achieved for ingredient recognition and"}, {"title": "4.6. Efficiency Analysis", "content": "As listed in Table 5, we present the results of the vanilla LoRA and Dual-LoRA across three configurations of total rank values (32, 64, and 128). The results indicate that Dual-LoRA consistently outperforms the vanilla LoRA counterparts across all rank settings (32, 64, and 128) with only a slightly higher number of parameters. Notably, Dual-LORA achieves superior performance even with fewer trainable parameters than LoRA with higher ranks, such as Var2-Dual-LoRA vs. Var3-LoRA and Var5-LoRA, as well as Var4-Dual-LoRA vs. Var5-LoRA, which is also visualized in Figure 5. We also observe that Dual-LoRA shows relatively larger improvements over vanilla LoRA when the rank size is small. We attribute this to data conflicts being more obvious with fewer trainable parameters, and the incorporation of task space in Dual-LoRA helps mitigate this issue. These findings highlight the effectiveness of Dual-LORA in addressing data conflicts when training multiple tasks in downstream applications."}, {"title": "5. Conclusion", "content": "In this paper, we explore efficient visual instruction fine-tuning for multi-modal large language models (MLLMs) by leveraging pre-trained models and utilizing only a few adapter parameters. Visual instruction fine-tuning of MLLMs faces two key challenges: insufficient capture of visual cues and conflicts among various instruction tasks due to the limited adapter parameters. To address these challenges, we propose a framework comprising Vision Cues Enhancement (VCE) and Dual Low-Rank Adaptation (Dual-LoRA). For Vision cue enhancement, we use multi-level feature maps to enrich the extracted vision cues. For Dual Low-Rank Adaptation, we introduce a rectified skill-space mapping technique, enabling the model to learn complex knowledge and respond specifically to instructions. Dual Low-Rank Adaptation possesses the expressive power of any ensemble of experts with a combined rank lower than its own. Evaluations on both general visual instruction benchmarks and domain-specific benchmarks demonstrate the effectiveness of our proposed method."}]}