{"title": "RevGNN: Negative Sampling Enhanced Contrastive Graph Learning for Academic Reviewer Recommendation", "authors": ["WEIBIN LIAO", "YIFAN ZHU", "YANYAN LI", "QI ZHANG", "ZHONGHONG OU", "XUESONG LI"], "abstract": "Acquiring reviewers for academic submissions is a challenging recommendation scenario. Recent graph learning-driven models have made remarkable progress in the field of recommendation, but their performance in the academic reviewer recommendation task may suffer from a significant false negative issue. This arises from the assumption that unobserved edges represent negative samples. In fact, the mechanism of anonymous review results in inadequate exposure of interactions between reviewers and submissions, leading to a higher number of unobserved interactions compared to those caused by reviewers declining to participate. Therefore, investigating how to better comprehend the negative labeling of unobserved interactions in academic reviewer recommendations is a significant challenge. This study aims to tackle the ambiguous nature of unobserved interactions in academic reviewer recommendations. Specifically, we propose an unsupervised Pseudo Neg-Label strategy to enhance graph contrastive learning (GCL) for recommending reviewers for academic submissions, which we call RevGNN. RevGNN utilizes a two-stage encoder structure that encodes both scientific knowledge and behavior using Pseudo Neg-Label to approximate review preference. Extensive experiments on three real-world datasets demonstrate that RevGNN outperforms all baselines across four metrics. Additionally, detailed further analyses confirm the effectiveness of each component in RevGNN.", "sections": [{"title": "1 INTRODUCTION", "content": "Academic reviewer recommendation is the process of identifying suitable reviewers for various submissions, such as funding proposals, research papers, technical plans, etc. The recommendation model for this task is designed to solve the serious problem of information overload faced by the review system [3, 35, 37]. When dealing with numerous submissions and a lengthy list of candidate reviewers, an efficient and accurate method is required to identify experts who possess the requisite skills and are willing to participate in the review process.\nPrevious reviewer recommendation studies have typically been based on the assumption that reviewers are more likely to review submissions that closely align with their research focus [46]. However, this assumption may not always hold due to the latent reviewing preference [17, 53]. For example, a reviewer's decision to review a submission is influenced not only by its direct relevance to their area of expertise but also by factors such as the reputation of the venue, the implied relevance of the field, and the reviewer's current workload. Therefore, capturing comprehensive contextual information requires a perspective that considers the interconnectivity among heterogeneous entities.\nTherefore, learning better review preferences from reviewers' past behaviors (i.e., reviewer-submission interaction graph) can significantly supplement the shortcomings brought by content-based recommendations. By leveraging powerful message-passing mechanisms to model users and items with their preference into low-dimensional feature representations within an end-to-end framework, the graph neural networks (GNN)-based approach has been applied to various recommendation tasks, including online shopping, e-learning, and entertainment [47, 57, 70, 71], and also has shown significant potential in academic reviewer recommendation."}, {"title": "Key Challenge", "content": "However, previous GNN models cannot be directly used in the academic reviewer recommendation due to their inherent graph sparseness. As shown in Figure 1, the majority of peer review records are kept confidential to ensure the objectivity of the review process. This policy makes information sharing among editorial offices and funding agencies nearly impossible [28]. Consequently, it is arbitrary to assume that an unobserved interaction between a submission and a reviewer represents a negative sample indicating that the reviewer would not review the submission. Therefore, the challenge namely \"Vague meaning of negative interactions\" lies in finding a way to integrate the dense knowledge information in submissions with the limited sparse behavior labels among reviewers [58, 61].\nIn light of the above discussion, this study proposes RevGNN, a novel approach for academic reviewer recommendation using graph contrastive learning (GCL). RevGNN employs a two-stage encoder structure to learn comprehensive embedding representations of reviewers and submissions. In the first stage, we use a decoupled GNN to encode the behavior preference of reviewers, while the prior scientific semantic knowledge of submissions is captured using a pre-trained scientific language model. In the second stage, we address the extreme sparsity in the scholar-submission bipartite graph by introducing the Pseudo Neg-Label strategy to enhance the performance of negative sampling during the graph contrastive learning process. The major contributions of this study are summarized as follows:\n\u2022 Facing the observation that recommendations based solely on topic similarity were far inefficient, we propose a graph learning-based recommendation framework namely RevGNN which comprehensively considers the reviewer's behavioral preferences, submission's implicit prior knowledge, and their contrastive features.\n\u2022 We argue that the role of unobserved interactions is not the same as general recommendation due to insufficient exposure, thereby causing false negative sampling. Thus we propose the Pseudo Neg-Label strategy to extract appropriate negative samples during the GCL training.\n\u2022 We perform extensive experiments on three public real-world datasets, and the results show that our proposed RevGNN outperforms all baselines over four metrics."}, {"title": "2 RELATED WORKS", "content": null}, {"title": "2.1 Academic Reviewer Recommendation", "content": "As formally introduced by [63]. the academic reviewer's recommendation refers to suggesting the editor/administrator with experts in one or several fields to jointly evaluate the scientific research proposals or achievements (such as funding proposal, paper, and technical reports). An efficient academic reviewer recommendation is urgently needed under the massive number of manuscripts, as a vivid exampple is that the number of submission received by NeurIPS (Annual Conference on Neural Information Processing Systems) and AAAI (The Association for the Advancement of Artificial Intelligence) in 2020 is 5.6 and 5.5 times respectively than that in 2014 [40]. Currently, the academic reviewer recommendation has become the cornerstone of submission management systems among academic journals, conferences and funding bodies, to reduce the unbearable workload of finding suitable reviewers who also agree to provide the review service [68]. There are usually two types of methodology for academic reviewer recommendation [46], which are retrieval-based and assignment-based approaches.\nThe retrieval-based approach focuses on improving the matching degree between the submission and reviewer candidates. This type of recommendation emphasizes the significance of relevance and coverage, and takes a loose basic hypothesis of the application scenario: Reviewers are free to choose whether or not to review the submission. Early retrieval-based approaches are mostly based on content matching with probabilistic topic models. [1] proposed"}, {"title": "2.2 Graph Learning-based Recommendation", "content": "The graph neural network, as a new type of neural network, has been proposed to extract features from non-Euclidean space data. GNNs capture the complex relationships between nodes and edges in graph-structured data, making them applicable to various scenarios such as knowledge graphs [32, 36], recommendation systems [15, 23, 62, 64], and time series prediction [56]. The work most relevant to ours involves user recommendation using GNNs, particularly those studies based on bipartite graphs of historical interactions between users and items. These studies employ graph-based learning methods to learn embedding representations and predict user preferences for unseen items[50]. Early matrix factorization methods, such as SVD++ [23], decompose the adjacency matrix of the graph to derive the embedding, which is useful but insufficient for encoding complex user behaviors. Then, the idea of neural collaborative filtering, such as NeuMF [16], CML [18] and NIRec [20], is proposed to replace the inner product of matrix factorization with parameter-learnable neural networks. Emerging Graph Convolution Networks (GCN) are also used to learn embeddings via the feature propagation of graph convolution, and achieve convincing performance in both theoretical and practical research, such as PinSage [57], NGCF [47], and LightGCN [15].\nRecently, the self-supervised contrastive learning studies have been reported to alleviate the sparse cold-start interactions in graph recommendation [61], and received massive attention. SGL [49] augmented node representations by dropping out elements on the manipulated graphs. SEPT [58] proposed a social recommender system that generates multiple positive samples for contrastive learning on the perturbed graph. SimGCL simplifies the contrastive learning paradigm in the recommendation by replacing the graph augmentations with uniform noises for contrastive views [59]. However, it should be noted that graph learning-based recommendation models have not been widely adopted into academic reviewer recommendations yet, due to the performance degradation caused by the false negative sampling on sparse graphs with low exposure rates."}, {"title": "3 PRELIMINARIES", "content": null}, {"title": "3.1 Problem Formulation", "content": "In this study, we define the academic reviewer recommendation as a link prediction task in a heterogeneous graph. That is, we define a heterogeneous academic graph \\(G = {N,E}\\) where the \\(N = {N_{scholar}, N_{submission}, ...}\\) denotes the set of different types of nodes, and \\(E = {E_{review}, E_{author}, ...}\\) denotes the set of different types of relationships. Let the number of nodes \\(|N| = n\\), and we have \\(E_i C N \\times N\\) where i denotes any type of possible relations. The objective of academic reviewer recommendation is to construct a function f that can predict if there exists a \"review\" relation by giving a submission-scholar pair:\n\n\\[\\begin{aligned}\n\\text{Find} \\quad & f(u, v) \\rightarrow {0,1},\\\\\n\\text{s.t.} \\quad & f(u, v) = 1 \\Leftrightarrow e_{u,v} \\in E_{review},\n\\end{aligned}\\]\n\nwhere \\(u \\in N_{scholar}, v \\in N_{submission}\\)\nIt should be noted that the submission and scholar nodes, alongside with review relationship between these two types of nodes are necessary for our task, and other types of nodes, as well as relations, are optional but useful in portraiting submissions and scholars in the body of knowledge."}, {"title": "3.2 GNN-based Recommendation", "content": "The GNN-based recommendation model usually works on a user-item bipartite graph, denoted as \\(G = {U, V, R}\\) which consists of users \\(U = {u_1, u_2, ..., u_m}\\), items \\(V = {v_1, v_2, . . ., v_n}\\), behavioral interactions \\(R \\in R^{m \\times n}\\), where \\(m = |U|\\) and \\(n = |V|\\). The objective of the GNN-based model is to predict whether there is an edge by giving a pair of specific users and items. Taking a similar foundation with other GNN models, GNN-based recommendation models conduct"}, {"title": "Message passing over the bipartite graph", "content": "Message passing over the bipartite graph to derive the semantic representations of users and items. Generally, this message passing has two processes, namely the aggregation and pooling [50].\nIn the aggregation process, the GNN model collects representation propagated by the neighbor for each node u at each layer, and this process at l-layer can be represented as:\n\\[h_u^{(l)} = \\mathcal{U}(h_u^{(l-1)}, AGG({h_v^{(l-1)}, v \\in N_u})),\\]\nwhere AGG and \\(\\mathcal{U}\\) stand for the specific aggregation and update strategies. For instance, GCN [21] aggregates each node's neighborhood nodes by using the following propagation rule:\n\\[h_u^{(l)} = \\sigma(\\mathbf{W}^{(l)} (h_u^{(l-1)} + \\sum_{v \\in N_u} \\bar{\\mathbf{A}}_{uv} h_v^{(l-1)})),\\]\nwhere \\(h_u^{(l)}\\) denotes the embedding of node u after l-layer's aggregation, \\(\\sigma(\\cdot)\\) denotes the non-linear activation function, \\(\\mathbf{W}\\) denotes a weight matrix, and \\(N_u\\) denotes the neighborhood of node u. \\(\\bar{\\mathbf{A}} = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{A} \\mathbf{D}^{-\\frac{1}{2}}\\) , where D is the diagonal node degree matrix with self loop \\(D_{ii} = \\sum_j A_{ij} + I\\) , note that I is the identity matrix.\nThen, during the Pooling operation, the representation learned by the GNN is utilized to perform prediction. Usually, these pooling functions combine the embeddings propagated at each layer to produce the final representation \\(h_u\\). Formally, for node u, the the pooling function POOL can be formulated as:\n\\[\\mathbf{h}_u = POOL(\\mathbf{h}_u^{(0)}, \\mathbf{h}_u^{(1)}, ..., \\mathbf{h}_u^{(L)}),\\]\nwhere L is the total number of propagation layers."}, {"title": "3.3 Graph Contrastive Learning for Recommendation", "content": "The basic idea of Contrastive Learning is to construct a particular set of representational invariances as data augmenta- tions. Moreover, GCL takes an additional hypothesis that changing embedding representation and partial structure perturbation are independent [59]. Thus, in addition to the original supervised recommendation loss, the GCL-based recommendation model introduces a supervised contrastive loss. Taking SGL [49] as example, it utilizes an InfoNCE [45] loss:\n\\[\\mathcal{L}_{CL} = \\sum_{i \\in B} -log\\frac{e^{\\frac{z_i \\cdot z_i'}{\\tau}}}{\\sum_{z_j \\in B} e^{\\frac{z_i \\cdot z_j'}{\\tau}}},\\]\nwhere \\(z_i, z_j\\) are users/items in the same sampled batch B, \\(z'\\) and \\(z\"\\) are augmented node representations learned from two dropout downsampled graphs, and \\(\\tau > 0\\) is the temperature. The CL loss tries to optimize the representation of nodes, making the distance of similar positive pairs (\\(z_i\\) and \\(z'_i\\)) closer, while the dissimilar negative pairs (\\(z_i\\) and \\(z'_j\\)) more orthogonal."}, {"title": "4 THE PROPOSED REVGNN", "content": null}, {"title": "4.1 Overview", "content": "We present the general framework of the proposed RevGNN in Figure 2. RevGNN has a two-stage encoder and one recommendation decoder. In the encoding process, the inputted heterogeneous graph is first detached into a bipartite to represent the submission-scholar behavior and a knowledge graph to describe the knowledge of nodes. We derive the basic embedding representation of nodes with two structure-specific encoders, namely preference encoder and"}, {"title": "4.2 Stage-1 Encoder: Behavior & Knowledge Representation Learning", "content": "The heterogeneous academic graph provides not only informative features for describing entities including submissions and scholars but also difficulty in modeling the multi-modal structure. Thus, we propose a \"divide and rule\"-based strategy to model different types of components in this graph.\nThe RevGNN first detaches the entire academic graph into two subgraphs:\n\u2022 Submission-scholar bipartite graph: This subgraph only contains scholar, submission, and review relations between them. Considering that review behavior is an action-based behavior, this bipartite graph provides an indication of the preferences and habits that scholars (i.e., potential reviewers) have maintained over the past review process.\n\u2022 Academic knowledge graph: This subgraph equals the complementary set of the submission-scholar bipartite graph. Most relations and attributes in this graph are fact-based behaviors, therefore it is considered to provide subject information and relevance in the current knowledge system.\nThen, these two subgraphs are encoded by two completely different encoders to extract corresponding features.\nPreference Encoder. For the submission-scholar bipartite graph, we utilize a decoupled graph convolution network (GCN) [48] to effectively learn the representation embedding of submission and scholar by aggregating embedding from their neighbor nodes. Formally, we denote this encoding process via a l-layer decoupled GCN as:\n\\[\\mathbf{h}_u^{(l)} = \\bar{\\mathbf{A}} \\mathbf{h}_u^{(l-1)}\\mathbf{W}^{(l)}.\\]\nwhere \\(\\mathbf{H}_D = [\\mathbf{h}_u^{(1)},...,\\mathbf{h}_u^{(l)}]^T \\in R^{n \\times d}\\) is the representation of the nodes derived at l-th layer, \\(\\bar{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}\\) denotes the normalized adjacent matrix given that A is the adjacent matrix with self-loops and D is the corresponding"}, {"title": "Probability of edge from submission u to scholar v", "content": "diagonal degree matrix. \\(\\mathbf{W}^{(l)}\\) is a learnable parameter matrix at l-th layer and d denotes the number of embedding dimensions. The reason for choosing this decoupled graph convolution is to avoid using the activation function, so as to enable the entire training process into one matrix multiplication process with better efficiency [7, 8]. The probability of edge from submission u to scholar v is predicted through a non-parametric residual dot production:\n\\[\\hat{y}_{u,v} = \\text{ReLU}( \\langle \\mathbf{h}_u, \\mathbf{h}_v \\rangle),\\]\nwhere\n\\[\\begin{aligned}\n\\mathbf{h}_u &= \\sum_{i=1}^L \\mathbf{h}_u^{(i)} \\\\\n\\mathbf{h}_v &= \\sum_{i=1}^L \\mathbf{h}_v^{(i)}\n\\end{aligned}\\]\nUnlike GNN-based recommendation models such as LightGCN [15] and NGCF [47], we do not use the Bayesian Personalized Ranking loss (BPR) [39] as the optimization loss function, and the reasons are twofold. First, the basic assumption of BPR is that the unobserved edge in a graph is very likely to be a negative sample, which does not meet the fact of the sparsity issue in the submission-scholar bipartite graph. Second, the sampling time brought by BPR is likely to be redundant, which is to be addressed in the Stage-2 Encoder of RevGNN. Thus, we select the binary cross-entropy loss (BCE) for the preference encoder on observed edges only:\n\\[\\mathcal{L}_{beh} = -\\sum_{(u,v) \\in E_{review}} \\hat{y}_{u,v} log \\hat{y}_{u,v} , + (1 - \\hat{y}_{u,v}) log(1 - \\hat{y}_{u,v}),\\]\nwhere \\(\\hat{y}_{u,v}\\) denotes the observed edges between u and v.\nKnowledge Encoder. The text title and abstract of a submission provide a refined theme description with scientific key points. However, the understanding of this theme, such as the location in the disciplinary taxonomy, exists in an implicit rather than an explicit form. Therefore, it is necessary to consider not only the title and abstract but also to combine the prior knowledge to learn the relevance of different kinds of relations for comprehensive representation learning.\nHere we utilize the OAG-LM (also known as OAG-BERT) [30], a scientific domain-specific language encoding model, to produce the embedding representation for each submission. Different from the traditional plain text scientific pre-training model (Such as SciBERT [2]), OAG-LM designs a two-dimensional location coding, so that we can input heterogeneous entity-relationship type information into the additional dimension, and the embedding of this information could also be inputted into the OAG-LM as prior knowledge. According to its design, in addition to the title, heterogeneous entities and relations including abstract, venue, and authoring scholar affiliations are able to be optional graphic knowledge to improve the representation performance. In other words, the full formulation of \\(N\\) and \\(E\\) in the problem formulation are \\(N = {N_{scholar}, N_{submission}, N_{venue}, N_{affiliation}}\\) and \\(E = {E_{review}, E_{author}, E_{af filied}, E_{published}}\\). The output of OAG-LM is the embedding of the [CLS] token, which is regarded as storing the information of all inputted data.\nHowever, as a pre-trained model, we do not finetune the OAG-LM with submission record in the experiment to avoid data leakage problems, and thus the loss would not be used for optimization during the training process.\nAggregation and Pooling. After knowledge and behavioral information are encoded separately, we use specific strategies to aggregate them together as the basis for the next stage of encoding. Also, for efficiency reasons, we need to keep only those elements that are directly related to the review purpose. Therefore, the encoding objective of this stage is to transfer heterogeneous node and relationship information to scholars and submissions, and we compress the"}, {"title": "4.3 Stage-2 Encoder: Contrastive Representation Learning with Pseudo Neg-Label", "content": "In this section, we propose a contrastive encoding stage for better relative embedding representation learning under a graph that is extremely sparse but has lots of unobserved false negative edges. We first construct the positive sample strategy by manipulating the embedding representation of the current node's neighbors. Furthermore, inspired by the idea of Debiased Contrastive Learning [66], we propose an unsupervised predecessor task during the negative sampling, namely Pseudo Neg-Label, to assign nodes into different classes, and sample negative nodes with different classes for the current node.\nRethinking the Sampling in GCL Strategy. The key challenge of academic reviewer recommendation is that the scholar-submission interaction graph is extremely sparse due to the review mechanism, and this sparsity is likely to come from unobserved edges. Although the recent development of self-supervised learning methods on graphs makes node representations more effective by performing relative embedding learning by constructing self-generated labels on graphs, most of them still assume that an unobserved edge tends to be a non-existent edge (i.e., a negative sample) [31, 52]. In fact, in our task, the reason why the reviewer node did not have a review edge with the submission node"}, {"title": "Contrastive Encoder with Pseudo Neg-Label Sampling", "content": "The graph contrastive learning's key idea is to perform relative representation learning on pairs of data points. It tries to optimize the representation of nodes, making the distance of similar (positive) pairs closer, while the dissimilar (negative) pairs more orthogonal. Therefore, the strategy of extracting positive and negative samples for the current node to establish similar and negative pairs is the key design in graph contrastive learning.\nMost of the existing recommendation models based on graph contrastive learning focus on how to construct positive samples. For the selection of negative samples, the existing methods often focus on how to improve the difficulty of distinguishing negative samples from positive samples, so that the model can learn more features from difficult negative samples [55]. However, in our academic reviewer recommendation task, we need to focus on avoiding potentially false negative samples. Therefore, we propose to construct mediation pseudo-tags according to data distribution and select only nodes inconsistent with the current node pseudo-tags as negative samples. The full pipeline of this process is shown in Figure 3.\nFormally, in RevGNN, we use a common approach to derive a positive sample by manipulating feature embedding on each node and using a GCN layer [21] to propagate embedding through the graph:\n\\[\\mathbf{h}_i' = \\sigma(\\bar{\\mathbf{A}} \\mathbf{h}_i \\mathbf{W}_c),\\]\nwhere \\(\\mathbf{h}_i'\\) is derived by a function that randomly masks a portion of embedding on the given node [12], \\(\\mathbf{W}_c\\) is the weighted matrix, and \\(\\sigma(\\cdot)\\) is the activation function.\nOn the other hand, to avoid false negative sampling issues, we design a clustering layer on the embedded representa- tion of nodes to jointly optimize with the graph contrastive learning framework. We first use the shared-weight GCN layer as used in positive sampling, which is denoted as:\n\\[\\mathbf{H}^2 = \\sigma(\\bar{\\mathbf{A}} \\mathbf{H}^1 \\mathbf{W}_c),\\]\nwhere \\(\\mathbf{H}^1\\) is the matrix of all nodes' embedding (\\(\\mathbf{h}_i \\in \\mathbf{H}^1\\)). The clustering layer provides each node with a cluster- ID which is able to be regarded as a discriminative self-supervised pseudo label. Suppose there are C clusters, and their cluster center {\\(\\mu_i\\)}_{i=1}^C are randomly initialized in the space of \\(R^d\\), the clustering is performed by minimzing the following KL-divergence-based cluster loss:\n\\[\\mathcal{L}_{clus} = \\sum_{u} \\sum_{i=1}^C P_{u, i} log \\frac{P_{u,i}}{q_{u,i}},\\]\nwhere i denotes the i-th cluster, and u \u2208 Nscholar \u222a Nsubmission. \\(P_{u,i}\\) is the target distribution which is calculated by:\n\\[P_{u,i} = \\frac{q_{u,i} / \\sum_u q_{u,i}}{\\sum_{v \\in N_{scholar} \\cup N_{submission}} q_{v,i} / \\sum_u q_{u,i}},\\]"}, {"title": "Deriving a positive sample", "content": "where \\(q_{u,i}\\) denotes the similarity between embedding representation \\(\\mathbf{h}_u\\) and cluster center \\(\\mu_i\\), which is measured by the t-distribution:\n\\[q_{u,i} = \\frac{\\sum_i (1 + \\frac{\\|\\mathbf{h}_u - \\mu_i\\|^2}{\\alpha})^{-1}}{\\sum_i (1 + \\|\\mathbf{h}_u - \\mu_i\\|^2)^{-1}}.\\]\nBy minimizing \\(\\mathcal{L}_{clus}\\), we can assign each node with a cluster ID. We regard this ID as a mediation pseudo-tag, and therefore in the negative sampling we only select nodes from clusters in which the current node is not located, thereby avoiding adding potential false negative nodes. We name this process the Pseudo Neg-Label sampling, and here we have the final negative sample set:\n\\[H_u = {\\mathbf{h}_v}_{c(u)\\neq c(v)},\\]\nwhere c(u) denotes the cluster ID of node u.\nBased on the derived positive sample \\(\\mathbf{h}_i'\\) (Eq. 11), negative sample set \\(H_u\\) (Eq. 16), and the original encoded embedding \\(\\mathbf{h}_u\\), we construct the InfoNCE-like contrastive loss:\n\\[\\mathcal{L}_{cl} = \\sum_{\\mathbf{h}_u \\in H_2} - log\\frac{e^{\\frac{\\mathbf{h}_u \\mathbf{h}_i'}{\\tau}}}{e^{\\frac{\\mathbf{h}_u \\mathbf{h}_i'}{\\tau}} + \\sum_{\\mathbf{h}_v \\in H_u} e^{\\frac{\\mathbf{h}_u \\mathbf{h}_v}{\\tau}}}.\\]\nTo summarize, in this stage-2 encoder, we jointly optimize \\(\\mathcal{L}_{cl}\\) and \\(\\mathcal{L}_{clus}\\) to tune the parameters in GCN layer \\(\\mathbf{W}_c\\), and thereby encoding the contrastive mutual information into node embedding representations."}, {"title": "4.4 Interaction-Oriented Recommendation Decoder", "content": "After the two-stage encoding, scholars and submissions are adequately represented by embedding vectors in a fixed dimension. Then, a decoder is required to resolve the embedding representation to predict whether there is an edge by giving any pair of scholars and submissions. Considering that embedding representations contain behavior, prior knowledge, and comparative information, we design a ranking-based recommendation network that can calculate the interactive attention between submissions that the current and reviewers have interacted with in the past.\nAs shown in Figure 4, the interaction attention is actually a variant of the activation unit in the design of Deep Interest Network (DIN) [69]. In the beginning, we provide the encoded embedding representations of a scholar (potential"}, {"title": "The final recommendation", "content": "reviewer), submissions the scholar has reviewed, and the submission candidate. We denote them as \\(\\mathbf{h}_u, \\mathbf{H}_p = {\\mathbf{h}_{p_1}, \\mathbf{h}_{p_2}, ...},\\) and \\(\\mathbf{h}_0\\), respectively. Note that the history of review could have multiple submissions, as we only draw the scenario where there is only one submission for the purpose of concision.\nIn the decoding stage, we first calculate the interaction attention between the candidate submission and the historical submissions. For each historical submission, the out product (Kronecker product) is performed, and then we concatenate it with the original \\(\\mathbf{h}_i\\) and \\(\\mathbf{h}_0\\) to a multilayer perceptron layer (MLP):\n\\[a_i = \\mathbf{W}_a^{(2)} \\sigma(\\mathbf{W}_a^{(1)} (\\mathbf{h}_{p_i} \\otimes \\mathbf{h}_0) + b_a^{(1)})+b_a^{(2)},\\]\nwhere \\(b_a^{(1)}\\) and \\(b_a^{(2)}\\) are bias matrix, \\(\\mathbf{W}_a^{(1)}\\) and \\(\\mathbf{W}_a^{(2)}\\) are weighted matrix with the size of \\((d^2, n)\\) and \\((n, 1)\\), and n is the size of hidden dimension. We also select the PReLU [14] as the activation by following the design of DIN. The output \\(a_i\\) is regarded as an attentive weight that scales the impact of historical review.\nIn the final recommendation, we use another MLP to predict the probability between scholar and submission with the concatenation of scholar, submission, and weighted historical submissions:\n\\[\\hat{y}_{u,v} = Sigmoid(\\sigma(\\mathbf{W}_r^{(2)} \\sigma(\\mathbf{W}_r^{(1)} (\\mathbf{h}_u || \\sum_i a_i \\mathbf{h}_{p_i} || \\mathbf{h}_0) + b_r^{(1)})) + b_r^{(2)})).\\]"}, {"title": "5 EXPERIMENTS", "content": null}, {"title": "5.1 Datasets", "content": "Due to the confidentiality of peer review, publicly available benchmarks are extremely scarce. Previous studies have mostly evaluated using simulated data, such as constructing datasets through expert self- or third-party annotation. Although this approach is reasonable for retrieval tasks, it may disrupt the density and other characteristics of the actual review behavior of reviewer recommendation scenarios. Therefore, we opted for the recently released Frontiers dataset [65], sourced from real peer review, as the primary dataset to validate our proposed RevGNN model. This dataset is a recently released public repository for evaluating the performance of finding reviewers for paper submissions.\nIt collects paper information from six Frontiers Press journals with high-impact factors, as well as reviewer information disclosed by their editorial office. Due to a lack of complete information and limited hardware capacity, we reshuffled the entire data records to produce smaller but high-quality subsets. We retain high-quality reviewers with at least 2 or 3 reviews and formed Frontiers-4k and Frontiers-8k. Frontiers-4k has 4,000 submissions and 6,711 reviewers, and each submission has at least two reviewers. Similarly, Frontiers-8k has 8,132 submissions and 9,560 reviewers where each submission has at least two reviews.\nTo demonstrate the generalizability of our proposed RevGNN across different scenarios, we also test the traditional NIPS dataset to validate the model's performance. The NIPS dataset is an earlier released, small-scale, manually annotated dataset [34]. The NIPS dataset [34] consists of rankings of reviewers and accepted papers from the NIPS 2006 conference, obtained through expert human judgments of relevance. These rankings are used to approximate the assignment of reviewers to the submitted articles. The dataset is widely utilized in research related to reviewer recommendations for peer review of research articles [44]."}, {"title": "5.2 Baselines", "content": "To verify the superiority of the proposed RevGNN model in academic reviewer recommendation, we compared the performance of the following baseline models:\n\u2022 TF-IDF Reviewer Rec [38]: TF-IDF is a classic statistical method used to assess the importance of a word to a set of documents. It is introduced into the expert matching problem by measuring the consine distance on TF-IDF between the reviewer and the submitting paper.\n\u2022 TPMS [4]: Toronto Paper Matching System (TPMS) algorithm is a tool commonly used for academic conference paper assignment. It calculates matching scores by analyzing the research interests and domain keywords of authors and reviewers, thereby optimizing the match between papers and reviewers to improve the quality and efficiency of the review process.\n\u2022 Wide&Deep [6]: Wide & Deep is another classic click-through rate (CTR)-based recommendation model which is composed of a Wide part of a single layer and a Deep part of a multi-layer. It enables a strong memory ability and generalization ability, and finally can quickly process and remember a large number of historical behavior features in the process of recommendation ranking.\n\u2022 DeepFM [13]: DeepFM is an improvement based on Wide&Deep. By combining the Deep part with the Factoriza- tion Machine (FM), the model replaces the Wide part with the FM layer, which improves the model's ability to extract information from the embedded representation.\n\u2022 ENMF [5]: ENMF is a matrix factorization-based recommendation model with a non-sampling strategy. It argues that a negative sampling strategy is not always robust, thus the performance of sampling-based methods could degrade in some cases.\n\u2022 RecVAE [42]: RecVAE is a variational autoencoder-based recommendation model by including composite prior distribution between the encoding and decoding.\n\u2022 LightGCN [15]: LightGCN is a representative GNN-based recommendation model that simplifies the graph convolution network by removing modules to establish matrix-based embedding propagation and update.\n\u2022 SGL [49]: SGL is a state-of-the-art self-supervised graph learning recommendation model. It introduces an auxiliary self-supervised task to pre-train the node representation via self-discrimination and then applies to the semi-supervised specific task.\n\u2022 SimGCL [60]: SimGCL is a CL-based model that learns unified user/item representations for recommendation via the contrastive learning paradigm, thereby implicitly alleviating popularity bias. Additionally, SimGCL discards the graph augmentation mechanism and instead adds uniform noise to the embedding space to create contrastive views."}, {"title": "Other evaluation metrics", "content": "\u2022 SHT [51", "64": "ApeGNN is a diffusion-based GNN approach that designs a node-wise adaptive mechanism for message aggregation so that each node can adaptively decide its weights based on the local degree structure. In the experiment, we select both local structure settings and denote them as HK (Heat kernel) as well as PPR (Personalized PageRank).\n5.3 Evaluation Metrics\nIn this study, we select Recall, Precision, Hit Rate, and Normalized Discounted Cumulative Gain to measure the performance of academic reviewer recommendation.\n\u2022 Recall evaluates the coverage of effective items in the prediction list of top-K recommendations.\n\\[Recall@K = \\frac{1}{|N_{submission}|} \\sum_{U_i \\in N"}]}