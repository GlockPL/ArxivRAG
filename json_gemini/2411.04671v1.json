{"title": "CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR", "authors": ["Kadir Burak Buldu", "S\u00fcleyman \u00d6zdel", "Ka Hei Carrie Lau", "Mengdi Wang", "Daniel Saad", "Sofie Sch\u00f6nborn", "Auxane Boch", "Enkelejda Kasneci", "Efe Bozkir"], "abstract": "Recent developments in computer graphics, machine learning, and sensor technologies enable numerous opportunities for extended reality (XR) setups for everyday life, from skills training to entertainment. With large corporations offering consumer-grade head-mounted displays (HMDs) in an affordable way, it is likely that XR will become pervasive, and HMDs will develop as personal devices like smartphones and tablets. However, having intelligent spaces and naturalistic interactions in XR is as important as technological advances so that users grow their engagement in virtual and augmented spaces. To this end, large language model (LLM)-powered non-player characters (NPCs) with speech-to-text (STT) and text-to-speech (TTS) models bring significant advantages over conventional or pre-scripted NPCs for facilitating more natural conversational user interfaces (CUIs) in XR. In this paper, we provide the community with an open-source, customizable, extensible, and privacy-aware Unity package, CUIfy, that facilitates speech-based NPC-user interaction with various LLMs, STT, and TTS models. Our package also supports multiple LLM-powered NPCs per environment and minimizes the latency between different computational models through streaming to achieve usable interactions between users and NPCs. We publish our source code in the following repository: https://gitlab.lrz.de/hctl/cuify", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in computer graphics, hardware, and artificial intelligence have led virtual and augmented reality (VR/AR) systems to become ubiquitous and head-mounted displays (HMDs) to be used more regularly. VR and AR have different use contexts and configurations, and each of them provides different advantages for users. For instance, VR is especially useful for generating simulations to train users in a fully immersive setting [1]\u2013[4]; AR is more convenient when context-aware visual support is overlaid on real-world content [5]\u2013[7]. While both VR and AR, which are part of the broader field of extended reality (XR), can be useful for everyday life, making such virtual and augmented spaces intelligent often requires significant engineering effort, especially for environmental and social interactions.\nConsidering intelligent XR spaces, practitioners often utilize non-player characters (NPCs), and these characters interact with users for different purposes [8], [9]. However, single-purpose NPCs may cause users to lose interest after a few interactions, as these characters tend to repeat the same or very similar content, eventually leading users to stop using the XR application. To this end, generative artificial intelligence (AI) and particularly large language models (LLMs) can provide numerous opportunities for XR due to their versatile computational capabilities, as they are trained with a significant portion of the Internet and can generate highly realistic synthetic data. By default, LLMs are utilized for the next-word prediction task; however, they can also be aligned for conversational purposes [10], and one of the most prevalent examples is ChatGPT, which became publicly available in 2022 [11]. Since then, the general public has been more heavily exposed to generative AI systems and models. At the same time, the use of LLMs has accelerated in various domains, including medicine [12], law [13], and education [14].\nWhile LLMs and generative AI models can be embedded into XR for different purposes, such as for creating 3D virtual content based on user preferences or for modifying interactive experiences, one of the most straightforward schemes is to embed LLMs into NPCs for speech-based interaction so that they can execute as conversational user interfaces (CUIs). In fact, prior research has utilized LLMs for speech-based NPC-user interactions with speech-to-text (STT) and text-to-speech (TTS) models [15]\u2013[17]. However, to the best of our knowledge, there is no open-source software that implements the pipeline consisting of STT, LLM, and TTS models for XR in a generic and extendable way. This means that for every XR application that includes LLM-based speech interaction, especially with NPCs, practitioners either implement the aforementioned pipeline from scratch or replicate it from previous projects, likely by also carrying out some modifications.\nConsidering the aforementioned issue, in this paper, we provide the community with an open-source Unity\u00b9 package that combines LLMs, STT, and TTS models into a pipeline to enable speech-based interaction. Our package minimizes the latency between the models by utilizing streaming, supports plugging different models and pipelines into multiple NPCs in a single environment, and can prompt the LLMs. Furthermore, it supports both accessing LLMs via application programming"}, {"title": "II. RELATED WORK", "content": "This section is divided into three subsections to cover the main aspects of interaction techniques in XR, the integration of LLMs in XR, and existing LLM-based open-source tools that support user interaction in XR."}, {"title": "A. Interaction in XR", "content": "Conventional interaction techniques, such as for desktop interfaces (e.g., mouse and physical keyboard), are unsuitable for the immersive nature of virtual interactions in XR [18]. Various methods, including controllers, hand gestures, gaze, speech, or a combination of these modalities, have been proposed to interact in 3D spaces [19], [20]. Each method offers certain advantages depending on the interaction context and desired level of immersion. Speech-based interaction complements and is often integrated into other techniques in multimodal interaction to enable intuitive interaction experiences in XR [21].\nOne of the most common interaction techniques in virtual spaces is controller-based interaction. These techniques provide users with input devices that are easy to adapt and familiarize [22]. Similarly, hand gesture-based interactions have gained importance [23], particularly those utilizing camera-based tracking systems [24], [25] or wearable technologies such as gloves [26]. These methods allow for natural and intuitive hand movements in virtual spaces and enhance immersion by mimicking real-world interactions.\nWhen considering hands-free interaction techniques, gaze-based interactions have been extensively researched, offering methods such as eye clicking [27]\u2013[29], eye dwelling [29], [30], and eye blinking [31] as mechanisms for object selection and control. In addition, gaze interactions are often combined with other input modalities to enhance precision and versatility. For instance, combining gaze with head movements [32] or hand gestures [33], [34] allows users to perform more complex actions while maintaining focus on virtual scenes.\nSpeech-based interaction is another method that provides a hands-free alternative, allowing users to interact naturally with the environment [35], [36], especially in scenarios where a physical input is impractical or when users are engaged in other tasks [37]. Those interactions offer higher adoption rates and better usability, particularly for novices. Recent developments in LLMs can transform speech-based interaction techniques in XR due to the versatile conversational capabilities of these models."}, {"title": "B. Large Language Models in XR", "content": "Interaction techniques in XR have rapidly evolved with different sensing modalities. Recently, with the spread of LLMs and their applications in various domains, LLM-powered NPCs and interactive objects in XR spaces have started to enable more immersive and intuitive experiences. These advances enhance user interaction through verbal inputs, offering a hands-free, conversational means of engaging with the presented content. To this end, Bozkir et al. [38] argued for integrating LLMs into XR, emphasizing their potential for enhancing inclusion and engagement while raising concerns about the privacy of voice-enabled interactions. In another work, Liu et al. [39] presented ClassMeta, LLM-driven interactive virtual classmates, in which the system uses voice commands to encourage student participation in virtual classrooms. The authors demonstrated the capabilities of LLMs in creating dynamic and interactive learning environments that simulate peer interactions. Additionally, Izquierdo-Domenech et al. [40] combined VR with voice-enabled LLMs to provide context-aware educational experiences, significantly improving learning outcomes through personalized and natural interactions.\nLau et al. [41] took an approach by leveraging VR and generative AI to revitalize oral traditions, using narrative personalization to reconnect youth with cultural folklore. The authors demonstrated that personalized storytelling in VR significantly boosts engagement and interest in cultural learning, increasing user engagement considerably compared to non-personalized settings. Similarly, Lau et al. [16] explored using LLM-powered chatbots in VR for heritage education, focusing on traditional Scottish curling in virtual settings. The authors found that LLM-powered chatbots, compared to pre-scripted ones, improved interactivity, engagement, and learning outcomes, highlighting their effectiveness in enhancing cultural heritage dissemination. Additionally, LLM-powered chatbots offered more dynamic learning experiences with higher usability than pre-scripted chatbots.\nBeyond speech-based interactions with LLMs, De et al. [42] introduced the LLM for Mixed Reality (LLMR) framework that is designed to create and modify interactive MR experiences in real time using LLMs. The framework incorporates computational techniques such as scene understanding, task planning, and self-debugging, and it demonstrated a four times lower error rate compared to GPT-4 and received positive usability feedback. These results highlight the effectiveness and usability of the LLMs in MR for different tasks."}, {"title": "C. LLM-based Interaction Tools for XR", "content": "Several open-source tools for XR have emerged to facilitate intuitive and complex interactions. To this end, Voice2Action [43], [44] was proposed to enable voice-driven object manipulation in Unity. It integrates LLMs to enable users to adjust 3D objects with voice commands, making it possible to resize and reposition buildings in virtual environments. Another tool, LLMUnity [45], supports the implementation of LLM-powered characters in Unity, enhancing real-time interaction by enabling dynamic conversational agents that respond to user input, supporting local LLMs. Furthermore, Talk-With-LLM-In-Unity [46] combines speech recognition with LLMUnity [45] and the Google Gemma 2 model [47], enabling natural language-driven navigation. This package was primarily designed to integrate voice-controlled"}, {"title": "III. SYSTEM DESCRIPTION", "content": "We propose CUIfy, an open-source package that combines a backend server with Unity clients to provide easy-to-use LLMs-powered conversational agents in Unity. Our package provides a user-friendly interface for different speech-to-text, text-to-speech, and large language models. It also provides options to select TTS voice types and allows users to utilize system prompts for LLMs. We discuss the technical specifications and system usage in the following subsections."}, {"title": "A. Technical Specifications", "content": "The CUIfy package consists of two parts: a Python server for processing voice input and a Unity client that creates requests. The Unity client includes a user-friendly configuration interface. Users can choose various local or online models (i.e., through APIs) with different configuration options, which are listed in Table I.\nThe Python server creates separate threads for each incoming socket connection and handles connections simultaneously. Each connection starts with incoming configuration messages, and the server can serve different APIs with different settings for each connection. These simultaneous connections enable the use of different NPCs with different configurations to work a unique voice, system prompt, and conversation history.\nThe server consists of state-of-the-art STT, TTS, and LLMs (either through APIs or local models) to offer a straightforward process for integrating both individual or publicly available models, whether they are local or online. In addition, using Docker containers ensures that the package provides cross-platform operation without compatibility problems with any local model.\nThe Unity client uses built-in Unity microphone API and .NET Socket Class [49] to ensure that it seamlessly works in every native platform. Each object with the client script creates a unique socket connection with the server, allowing for the creation of multiple configurable NPCs. Since the default microphone library only records audio with a specific duration, which is configurable under the Unity editor, the client script eliminates white noises if the input recording is shorter than the recording duration. If the input recording is longer than the recording duration, clients stream the recording to the server to ensure that the processing time latency is short. Furthermore, the server streams the LLM outputs to the client sentence by sentence if supported by the model (and API) and selected by the user.\nThe server can easily be configured to run on the cloud environment with the correct network setup. The Dockerization process ensures that it can be easily run on any cloud server without struggling with dependencies. The client works seamlessly on Android, Windows, and macOS platforms, with the host on the same or different machine. We tested the CUIfy with Varjo XR-3, Meta Quest 2, and Meta Quest 3 HMDs using Unity version 2022.3.26f1, which is a Long Term Support version."}, {"title": "B. System Usage and Guidelines", "content": "CUIfy's server can be easily deployed in a Docker container, which makes the setup process easy and straightforward. The source code is also well-designed and flexible, allowing users to add or edit models easily. The Unity package consists of a client script and additional libraries for working with audio"}, {"title": "IV. DISCUSSION AND FUTURE DIRECTIONS", "content": "The advancements in LLMs and speech models hold substantial potential for developing speech-based interaction techniques to control virtual environments. They also facilitate personalized, interactive NPCs and dynamic narrations within virtual spaces. These settings have great potential for enhancing collaboration and accessibility, providing features such as real-time translations and transcriptions. XR settings, with their inherently immersive nature, are especially well suited for verbal interactions with NPCs. CUIfy leverages this potential by providing a tool that simplifies the integration of speech-based NPCs into Unity environments, enabling natural and realistic interactions with minimal technical effort. This feature benefits users without coding expertise, allowing an easy-to-use implementation in new and existing projects.\nIn real-time conversational interactions, key challenges like latency and conversation quality often arise. To address the latency issue, CUIfy employs a streaming technique that allows NPCs to speak as soon as content generation starts without waiting for the entire output to be generated. This approach significantly reduces the latency and will most likely enhance the user experience. While APIs and large local models often offer the best conversation quality, using APIs follows a pay-as-you-go model, and large models require considerable processing power. Our package also supports lightweight models that can run on local devices, though they generally provide slightly lower quality than full-scale models. Currently, CUIfy supports eight LLMs, four STT, and three TTS models, as provided in Table I. As generative AI and LLMs continue to evolve and new models are expected to emerge, CUIfy's architecture allows the integration of those models and APIs easily, which remains the focus of future work."}, {"title": "V. CONCLUSION", "content": "We introduced a generic, easy-to-use, and extendable Unity package that enables interactive NPCs designed for XR settings. Our package supports several speech-to-text models, LLMs, and text-to-speech models, allowing users to select models based on their preferences and needs while addressing both quality and privacy requirements. The package is optimized to enhance conversation quality and facilitate integration into projects, supporting both APIs and local models. In future work, we plan to expand the number of supported models and conduct additional optimizations to enhance performance."}]}