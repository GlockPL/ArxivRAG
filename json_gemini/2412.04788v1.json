{"title": "GUIDE: A Global Unified Inference Engine for Deploying Large Language Models in Heterogeneous Environments", "authors": ["Yanyu Chen", "Ganhong Huang"], "abstract": "Efficiently deploying large language models (LLMs) in real-world scenarios remains a critical challenge, primarily due to hardware heterogeneity, inference framework limitations, and workload complexities. These challenges often lead to inefficiencies in memory utilization, latency, and throughput, hindering the effective deployment of LLMs, especially for non-experts. Through extensive experiments, we identify key performance bottlenecks, including sudden drops in memory utilization, latency fluctuations with varying batch sizes, and inefficiencies in multi-GPU configurations. These insights reveal a vast optimization space, shaped by the intricate interplay of hardware, frameworks, and workload parameters. This underscores the need for a systematic approach to optimize LLM inference, motivating the design of our framework, GUIDE. GUIDE leverages dynamic modeling and simulation-based optimization to address these issues, achieving prediction errors between 25% and 55% for key metrics such as batch latency, TTFT, and decode throughput. By effectively bridging the gap between theoretical performance and practical deployment, our framework empowers practitioners\u2014particularly non-specialists\u2014to make data-driven decisions and unlock the full potential of LLMs in heterogeneous environments cheaply.", "sections": [{"title": "Introduction", "content": "The deployment of Large Language Models (LLMs) has become a pressing challenge, driven by their transformative breakthroughs in natural language processing, computer vision, and multimodal tasks. Models such as GPT [1], OPT [2], LLaMA [3], and Qwen [4], equipped with billions or even trillions of parameters, demonstrate unparalleled capabilities in generating semantically coherent, contextually rich content and solving complex tasks. These advancements have enabled widespread adoption across diverse applications, including chatbots, content creation, and scientific research.\nHowever, efficiently deploying such LLMs in real-world scenarios remains a critical bottleneck due to the complexity of real-world constraints. For many enterprise users and individual practitioners, the deployment process is particularly challenging because it requires deep expertise in hardware configurations, inference frameworks, and optimization strategies. Without this expertise, they often struggle to fully leverage the capabilities of LLMs, leading to underutilized hardware, inefficient resource allocation, and suboptimal performance. This complexity underscores the urgent need for accessible and systematic tools that simplify the deployment process, enabling users with varying levels of expertise to achieve efficient and cost-effective inference.\nInference performance is heavily influenced by the heterogeneity of hardware platforms, inference frameworks, and deployment strategies. These disparities often result in underutilized computational resources, increased operational costs, and degraded user experiences, such as higher latency or reduced throughput. These challenges underscore the urgent need for systematic optimization in LLM inference, including hardware-aware tuning, inference framework enhancements, and intelligent scheduling, to bridge the gap between theoretical state-of-the-art model performance and practical deployment requirements, while improving resource utilization, reducing latency, and lowering deployment costs.\nDespite extensive efforts to optimize LLM inference, existing approaches often target isolated aspects, such as hardware acceleration [5], framework-specific tuning, or parallelization strategies [6].While these methods achieve localized improvements, they fall short in addressing the intricate dependencies"}, {"title": "Background & Motivation", "content": null}, {"title": "Transformer Models", "content": "Transformer models [9] have significantly advanced artificial intelligence by enabling breakthroughs in natural language processing (NLP), computer vision, and multimodal tasks. As shown in Figure 2, the architecture consists of stacked Transformer blocks, each composed of Multi-Head Attention (MHA), Feed-Forward Networks (FFN), and Normalization (Norm) layers. Residual connections facilitate gradient flow during training, while positional encodings provide sequence order information that the architecture itself lacks.\nThe Transformer architecture is divided into an encoder and a decoder. The encoder processes input sequences and builds contextual representations by capturing global dependencies through self-attention. The decoder generates output sequences by attending to both the encoder's output and its"}, {"title": "LLM Inference Challenges", "content": "Deploying Large Language Models (LLMs) presents several technical challenges, primarily arising from memory bottlenecks, computational complexity, and latency requirements. A critical issue is memory usage during inference, dominated by the storage of Key-Value (KV) pairs in self-attention mechanisms. These KV pairs are retained across all layers, with memory consumption scaling linearly with sequence length and model depth. For long input sequences or deep models, this can quickly exceed hardware capacities, especially on GPUs with limited memory. Techniques like memory-efficient attention and model quantization have been proposed to alleviate this constraint, but they often involve trade-offs in precision or computational overhead.\nAnother major challenge is the quadratic computational complexity of the self-attention mechanism with respect to sequence length. This limits throughput, particularly for tasks requiring long context windows or high concurrency. Underutilization of hardware resources is common in such scenarios, further exacerbated by mismatches between model architectures and hardware capabilities.\nLatency requirements add another layer of complexity, especially for real-time applications. Factors such as batch size, sequence length, and parallelism strategies heavily influence latencies. High-concurrency settings, small batch sizes, or"}, {"title": "LLM Inference Optimization", "content": "Optimizing the inference of LLMs requires addressing challenges such as memory bottlenecks, computational complexity, and latency constraints. Over the years, researchers have developed a variety of techniques, including algorithmic optimizations, deployment strategies, and specialized inference frameworks. This section provides an overview of these methods and their relevance to LLM inference."}, {"title": "Algorithmic Optimizations", "content": "Algorithmic techniques are fundamental to overcoming memory and computational challenges in LLM inference. Quantization methods, such as SmoothQuant [10] and LLM.int8 [11], reduce the precision of weights and activations, significantly lowering memory consumption while maintaining acceptable accuracy. Pruning approaches, such as SparseGPT [12], identify and remove redundant parameters, thereby reducing computational complexity and accelerating inference. FlashAttention optimizes the self-attention mechanism by minimizing memory access overheads, enabling efficient processing of long sequences. These techniques have demonstrated their effectiveness in addressing specific bottlenecks, making them widely adopted in real-world deployments."}, {"title": "Inference Frameworks", "content": "The growing demand for efficient LLM deployments has led to the emergence of numerous inference frameworks, each designed to address different aspects of the deployment process. Popular frameworks such as vLLM, DeepSpeed-FastGen [13], TGI (Text Generation Inference) [14], Faster-Transformer [15], and LLaMA.cpp [16] represent diverse strategies for optimizing latency, throughput, and resource utilization. These frameworks provide tailored solutions for various deployment scenarios, ranging from high-performance cloud systems to resource-constrained edge environments.\nVLLM introduces dynamic batching and parallelized token generation to improve computational efficiency and reduce latency. As shown in Figure 3, the figure illustrates the logical-to-physical block mapping framework used in VLLM. FastGen leverages scalability through dynamic splitting and fusion techniques integrated into its backend architecture. As illustrated in Figure 4, the DeepSpeed-FastGen backend consists of two main components: DeepSpeed-MII, which supports continuous batching and dynamic splitting, and DeepSpeed-Inference, which utilizes block KV-cache for efficient inference. TGI supports distributed inference with model sharding, enabling large-scale deployments across multi-GPU or multi-node clusters. FasterTransformer focuses on optimizing inference for NVIDIA hardware, leveraging advanced kernel-level optimizations. LLaMA.cpp, in contrast, caters to resource-constrained environments by employing aggressive quantization and memory management techniques.\nAlthough this work primarily evaluates vLLM and FastGen due to their unique optimization strategies, the methodologies discussed are broadly applicable to other frameworks, reflecting the diversity of approaches in this area."}, {"title": "Deployment Strategies", "content": "Deployment strategies, such as tensor parallelism, data parallelism, and pipeline parallelism, are critical for distributing computations across hardware resources. Tensor parallelism partitions model parameters across GPUs, enabling efficient handling of large-scale models. However, the approach introduces substantial communication overheads, as intermediate results must be synchronized during inference. Data parallelism, by splitting input data across devices, is well-suited for batch processing but requires careful synchronization to maintain consistency. Pipeline parallelism divides model layers across GPUs, allowing simultaneous execution of different stages of the model but introducing latency due to inter-stage dependencies."}, {"title": "Experimental Insights", "content": null}, {"title": "Memory Utilization Drop-Offs", "content": "Both vLLM and FastGen exhibit sharp memory utilization drop-offs at critical points across specific hardware and model combinations. For vLLM, dramatic declines are observed on GPUs such as RTX 4090 and V100, where memory utilization drops abruptly as batch sizes increase from smaller values (e.g., 8) to larger ones (e.g., 32) for models like Qwen, OPT, and LLaMA (Figure 5. On A6000 GPUs, similar drop-offs occur with LLaMA, while FastGen, despite maintaining relatively higher utilization levels under most conditions, also encounters significant declines, such as with the OPT model on RTX 4090 and the LLaMA model on A6000. A100 GPUs exhibit more stable performance, yet inefficiencies persist when handling complex models like Qwen and OPT.\nThese abrupt drop-offs stem from mismatches between KV-cache management strategies and hardware memory allocation mechanisms. vLLM's fixed-granularity KV-cache allocation strategy, while effective for small-batch tasks, leads to severe memory fragmentation and underutilization as batch sizes grow. On GPUs like A100, which allocate memory in large blocks (e.g., 2MB), these inefficiencies are magnified. FastGen, though employing a more adaptive caching mechanism, struggles with certain model and hardware combinations, where suboptimal allocation persists.\nMoreover, the heterogeneous characteristics of models, such as KV-cache size and access patterns, further exacerbate these issues. Uniform caching strategies fail to adapt"}, {"title": "Batch Size and Latency Divergence", "content": "Latency performance shows a striking divergence between VLLM and FastGen as batch size increases, with a clear inflection point observed. For small-batch tasks (Batch Size < 16), vLLM achieves significant latency advantages over FastGen, making it highly suitable for real-time applications. However, as batch size grows, vLLM's latency gradually deteriorates, and at Batch Size 32, FastGen surpasses vLLM, demonstrating stable and scalable performance. This divergence is particularly pronounced on high-end GPUs such as RTX 4090 and A6000, where vLLM's latency increases sharply, while FastGen maintains consistent performance over a wider range of batch sizes (Figure 6).\nThis divergence can be attributed to the fundamental differences in the resource allocation and task scheduling strategies employed by the two frameworks. vLLM prioritizes prompt processing by allocating substantial resources to KV-cache initialization and storage. While this approach performs well for small-batch tasks, it leads to significant resource contention as batch sizes grow. Specifically, vLLM's static KV-cache allocation strategy fails to scale with increasing workload complexity, causing delays in token generation and sharp latency spikes. In contrast, FastGen employs a dynamic splitting and fusion strategy that optimizes matrix operations by breaking them into smaller, parallelizable chunks. This approach minimizes contention, reuses intermediate results, and ensures balanced resource utilization, enabling FastGen to scale efficiently with larger batch sizes.\nThese findings highlight a significant optimization space for balancing latency performance across batch sizes. The trade-off between real-time performance for small batches and scalability for larger batches reveals opportunities to improve task scheduling and resource allocation strategies, particularly for high-end GPUs and diverse workload requirements."}, {"title": "Performance Under Parallel Environments", "content": "When tensor parallelism (TP) increased from TP=1 to TP=2, a divergence in first token latency (TTFT) was observed. For models like LLaMA, VLLM exhibited noticeable increases in TTFT, while FastGen maintained stable latency and even achieved slight reductions in some cases. As shown in Figure 7 and Figure 8, vLLM's TTFT rises with increasing parallelism, whereas FastGen demonstrates relatively consistent performance. Specifically, in the figures, the bottom-left corner represents the optimal performance, and the top-right corner indicates the worst performance. The green box highlights the performance with a single GPU, while the red line represents the performance with 2 GPUs. The blue arrow illustrates the trend from single GPU to multi-GPU, and the black line shows how this trend changes with increasing prompt length.\nThe observed divergence is primarily caused by the complex interaction between tensor parallelism (TP) and the computational and communication patterns of different phases. During the Prefill phase, input prompts are processed in a"}, {"title": "Dynamic Batch Size Optimization", "content": "Batch size selection plays a critical role in determining latency performance, as it directly affects how computational resources are utilized. Different tasks exhibit distinct optimal batch sizes due to their unique computational characteristics and resource demands. For tasks with high computational intensity, such as matrix multiplications with large dimensions, the GPU's computational units can quickly reach saturation even at smaller batch sizes. In contrast, tasks with lower computational demands scale efficiently to larger batch sizes before encountering hardware bottlenecks. Beyond these points, further increasing the batch size often leads to diminishing returns as resources such as compute units and memory bandwidth become fully utilized, limiting scalability and causing latency to increase.\nThis variation in optimal batch size highlights the intricate interaction between task characteristics and hardware constraints. Computationally intensive tasks saturate GPU resources at smaller batch sizes, leaving little room for further scalability, while less intensive tasks allow for greater scaling before bottlenecks arise. These differences make it clear that a one-size-fits-all batch size configuration is inherently suboptimal, as it fails to account for the varying demands of different workloads.\nThese observations reveal a significant optimization space for tailoring batch size configurations to specific task characteristics and hardware conditions. By analyzing the relationship between computational intensity, hardware utilization, and latency performance, it is possible to explore strategies that dynamically adjust batch size to better align with workload demands."}, {"title": "Motivation", "content": "The insights from our experiments highlight critical bottlenecks in LLM inference, revealing a vast optimization space shaped by the intricate interplay of multiple factors, including hardware configurations, model architectures, inference frameworks, deployment strategies, optimization methods, and workload parameters (e.g., batch size, sequence length). These factors, combined with the computational characteristics of the workloads, create a highly complex and sensitive environment where small changes in one dimension can significantly impact overall performance. Current methods, which often rely on static configurations or isolated tuning, fail to address these dynamic dependencies, leaving substantial optimization potential untapped in real-world deployments.\nTo address these challenges, we propose a systematic framework for modeling and simulating LLM inference. This framework aims to explore the multi-dimensional optimization space created by interactions among hardware, models, frameworks, and workload characteristics. By identifying the key factors that influence performance and understanding their interdependencies, our approach provides actionable insights for intelligent deployment decisions. It bridges the gap between theoretical advancements and practical requirements, paving the way for scalable, efficient, and deployable LLM inference across diverse configurations and scenarios."}, {"title": "Framework", "content": "To mitigate the performance bottlenecks identified in Section 2, we propose an intelligent deployment system designed to optimize inference performance for large-scale models. This system systematically models various configurations and simulates the performance across different inference setups, allowing for the identification of the optimal solution under complex inference configurations. It explores the multi-dimensional parameter space of model inference, including hardware platforms, inference frameworks, parallel strategies,"}, {"title": "Overview", "content": "GUIDE addresses these challenges by automating the exploration of the multi-dimensional parameter space involved in large-scale model inference. It dynamically adapts to different hardware configurations, inference frameworks, and parallel execution strategies, ensuring that it identifies the most efficient deployment configurations for a given task. At its core, GUIDE optimizes both memory and computational resource usage, enabling the deployment of large-scale models with minimal manual effort while achieving maximized performance.\nOne of the key capabilities of GUIDE lies in its intelligent hybrid parallel simulation. By combining data parallelism (DP) and tensor parallelism (TP), it simulates various GPU configurations to determine the optimal parallel strategy. This simulation process minimizes total execution time in order to find the best parallel config. The system models the computational load and memory overhead of various stages in the inference process, including the prefill and decode phases. It dynamically adjusts batch sizes and sequence lengths based on these models to fit within the constraints of available GPU memory. This adjustment abstracts the influence of inference frameworks such as vllm and fastgen on the inference process, and to model these frameworks, it simulates the dynamic batch processing and dynamic split flow.\nTo further enhance its predictive capabilities, GUIDE incorporates the Roofline model [17], similar to the approach used by LLM-Viewer [18], which models the computational and memory overheads of inference. This integration allows it to analyze task performance and identify bottlenecks in computation and memory bandwidth, providing insights that guide decisions about parallel strategies and resource allocation. Coupled with simulation-based optimization, the system evaluates task execution time and throughput for different parallel configurations. Based on these metrics, it selects the top-performing configurations, ensuring that performance goals are met under the given constraints.\nIn addition to these analytical capabilities, GUIDE automates the generation of deployment configurations. By analyzing hardware and workload characteristics, it produces a set of potential configurations that can handle diverse deployment scenarios without requiring manual fine-tuning. This automation streamlines the process of adapting to new environments, making the system highly versatile and efficient.\nThrough these techniques, GUIDE empowers users to achieve near-optimal inference performance for large-scale models. Its adaptability ensures that resource utilization is maximized and task execution is completed in the shortest possible time. Whether deployed in single-GPU setups or large multi-GPU clusters, the system's flexibility and robustness make it suitable for a wide range of deployment environments.\nAs shown in Figure 9, GUIDE considers various factors and simulates the performance characteristics of the actual inference process, modeling the multi-dimensional parameter space involved in large-scale model inference. It dynamically adapts to different hardware configurations, inference frameworks, and parallel execution strategies, ensuring that it identifies the most efficient deployment configurations for a given task."}, {"title": "Inference Engine", "content": "Step 1: Memory Usage and Maximum Parallelism Calculation\nTo evaluate the memory overhead, which primarily arises from the key-value (kv) storage and model weight storage, we first perform a mathematical analysis using the Model Analyzer component. This step does not require the inference framework itself such as vllm or fastgen since they do not affect the total amount of memory consumption during prefill and decode phases, but takes into account user inputs for optimization such as FlashAttention and H2O which reduce the memory consumption. The analysis focuses on calculating the kv overhead for a batch size of 1. The kv overhead is derived by averaging the memory usage for a single token during both the prefill and decode phases.\nThe available GPU memory is then calculated as:\nAvailable Memory = GPU Memory - Model Slice Memory\n(where Model Slice Memory is determined by TP splitting). Using this, the maximum parallelism can be derived as:\nMaximum Parallelism = DP $\\times (\\frac{Available\\ Memory}{kv\\ overhead\\ per\\ request})$, \nand the maximum allowed batch size (Max Batch Size) is:\nMax Batch Size = $\\frac{Maximum\\ Parallelism}{Data\\ Parallelism}$"}, {"title": "Task Scheduling and Simulation for Optimal Performance", "content": "Once the maximum batch size has been determined, the next step is to analyze the performance of the chosen configuration using the Model Analyzer. In this step, we analyze both vllm and FastGen inference models under the same configs as step 1. Specifically, the kv overhead for each token during the prefill and decode phases is calculated. The total memory required for processing a request is the sum of the kv overhead and the model weight size.\nThe Model Analyzer also calculates the computational overhead based on the number of tokens to be generated. This information is then used to generate a task list for GUIDE, which consists of tasks for the prefill and decode stages. Each task includes both memory and computation costs. The task list is passed to the Parallel Simulator, which calculates the data transfer and memory read/write times caused by different parallel strategies. The simulator outputs the optimal parallel configuration, recording the top three best configurations.\nGUIDE then evaluates the performance of different configurations based on inference time or throughput. For each configuration, the simulation results give the best parallel configuration and the corresponding inference time. And the final throughput is derived as follows:\nsingle_gpu_throughput = $\\frac{1}{TPOT}$\nand\nmulti_gpu_throughput = $\\frac{N \\cdot T_{single}}{1 + log_2(P_{TP})}$,\nwhere TPOT represents the token processing time per operation for a single GPU, N is the number of GPUs, Tsingle denotes the single-GPU throughput, and PTP stands for the parameter parallelism.\nTTFT for the prefill phase is simply the time per token for the prefill phase.\nThese results are used to identify the top three configurations with the best performance based on inference time or throughput, where the configuration with the lowest inference time and highest throughput is considered optimal."}, {"title": "Model Analyzer", "content": "In this section, we describe the second key component of GUIDE, which is the Model Analyzer. This technique focuses on optimizing memory utilization and computational efficiency by dynamically adjusting key parameters, such as batch size, sequence length, and KV cache size, based on the available GPU memory and model requirements. As shown in Figure 10, the Model Analyzer integrates dynamic batch"}, {"title": "Dynamic Batch Size Adjustment", "content": "The dynamic batch size adjustment mechanism is designed to optimize the batch size while ensuring it adheres to the constraints imposed by GPU memory limitations. This approach consists of two primary steps: calculating the memory required per request and dynamically adjusting the batch size. First, the memory required for the key-value (KV) storage per request is calculated as:\nkv_byte_per_request = 2 \u00d7 N \u00d7 H \u00d7 S\u00d7B\u00d7 kv,\nwhere: - N represents the number of hidden layers, - H is the hidden size of the model, - S is the sequence length, - B is the batch size, - kv denotes the memory in bytes required for each key-value pair.\nTo isolate the memory consumption independent of the batch size, the memory required for KV storage per request without the influence of batch size is expressed as:\nkv_byte_per_request_without_batchsize = 2 \u00d7 N \u00d7 H \u00d7 S \u00d7 kv.\nGiven the GPU's total memory capacity Cgpu and the memory occupied by the model Cmodel, the available memory for KV storage is:\nCavailable = Cgpu - Cmodel.\nThe maximum allowable batch size under these memory constraints is determined as:\nBmax = $\\frac{Cavailable}{kv\\_byte\\_per\\_request\\_without\\_batchsize}$\nIf the input batch size Binput exceeds this maximum allowable batch size, the batch size is adjusted to:\nB = min(Binput, Bmax).\nTo further optimize memory usage and enhance prompt processing performance, the adjusted batch size is divided into smaller sub-batches. The sub-batch splitting process is designed to ensure that each sub-batch fits within the available memory while maintaining computational efficiency.\nThe size of each sub-batch is determined by a dynamically chosen parameter, denoted as split_size, which balances memory constraints with the need for efficient computation. Specifically, the batch size B is divided into sub-batches of size split_size, ensuring that the number of sub-batches is maximized without exceeding memory limitations. The number of sub-batches is determined by the ratio of the batch size to the split size, with any remaining portion of the batch forming an additional sub-batch."}, {"title": "Dynamic Sequence Length Adjustment", "content": "The core of the dynamic sequence length adjustment is to optimize the sequence length based on the GPU's available memory, ensuring that memory constraints are respected while maximizing efficiency. The process consists of two main steps: calculating the memory required per request and adjusting the sequence length dynamically.\nFirst, the memory required for each request, without considering the sequence length, is calculated as:\nkv_byte_per_request_without_seqlen = 2 \u00d7 N \u00d7 H \u00d7 B \u00d7 kv,\nGiven the available memory, the maximum sequence length Smax that can fit within the memory constraints is calculated as:\nSmax = $\\frac{Cavailable}{kv\\_byte\\_per\\_request\\_without\\_seqlen}$\nIf the input sequence length exceeds the maximum sequence length allowed by the available GPU memory, the sequence length is adjusted to fit within the memory constraints.\nAdditionally, to simulate the impact of long prompts, the sequence is divided into smaller segments. The size of each segment is determined by a base value, which is then multiplied by a tuning factor to allow for fine-tuning. The total number of splits required is calculated by dividing the input sequence length by the split size and rounding up to the nearest integer.\nFinally, the sequence length is adjusted based on the number of splits, with the adjusted sequence length being the"}, {"title": "Parallel Simulator", "content": "In this section, we introduce the third key technique of GUIDE: the Parallel Simulator. This simulator focuses on optimizing task execution in multi-GPU environments by simulating hybrid parallelism, combining Data Parallelism (DP) and Tensor Parallelism (TP). The goal is to find the optimal parallel configuration that minimizes the total execution time of the tasks.\nAs shown in Figure 11, the Parallel Simulator handles the task execution flow by simulating both Data Parallelism and Tensor Parallelism to minimize the execution time across multiple GPUs."}, {"title": "Task and GPU Models", "content": "The core of the simulator consists of two main models: the Task class and the GPU class.\nTask Model: Each task in the simulator is represented by a Task object, which holds the data size and the compute load of the task. - GPU Model: The GPU class captures the hardware characteristics of a single GPU, including its compute performance, memory bandwidth, communication bandwidth, and latency."}, {"title": "Roofline Model for Performance Analysis", "content": "To analyze whether a task's performance is constrained by compute power or memory bandwidth, the simulator uses the Roofline model from LLM-Viewer. This model plots the performance ceiling of a task based on the available compute and memory resources. Specifically, the Roofline model can be expressed as:\nPerformance = min $\\left(\\frac{compute\\_load}{compute\\_time}, \\frac{memory\\_bandwidth}{data\\_size}\\right)$\nWhere: - compute_load is the number of floating point operations (FLOPs) required by the task. - compute_time is the time taken by the task on the GPU based on its compute performance (TFLOPS). - memory_bandwidth is the rate at which data can be transferred between memory and the processor. The Roofline model helps to identify the bottleneck for each task\u2014whether the task is memory-bound or compute-bound\u2014and informs the simulation of execution time."}, {"title": "Task Execution and Hybrid Parallelism Modeling", "content": null}, {"title": "Time Calculation", "content": "The task execution time can be broken down into three key components. The first component is the compute time, which is determined by the total computation load of the task divided by the computational power of the GPU. Mathematically, it is expressed as:\ncompute_time = $\\frac{compute\\_load}{gpu\\_compute\\_power}$\nThe second component is the data read/write overhead, which refers to the time required to read and write data to and from the GPU memory. This is calculated by dividing the total data size by the GPU memory read/write bandwidth:\ndata_read_write_time = $\\frac{total\\_data}{gpu\\_memory\\_bw}$\nThe third component is the data transfer overhead, which represents the time required to transfer data between system memory and the GPU memory. This is determined by the total amount of data transferred and the GPU memory communication bandwidth:\ndata_transfer_time = $\\frac{total\\_data\\_transferred}{gpu\\_memory\\_comm\\_bw}$\nThe total task execution time is then the sum of these three components."}, {"title": "Hybrid Parallelism (DP and TP) Modeling", "content": "Hybrid parallelism, which combines Data Parallelism (DP) and Tensor Parallelism (TP), plays a crucial role in optimizing task execution. In the context of hybrid parallelism, the task is divided into multiple groups, with each group of tasks processed in parallel across several GPUs. In Data Parallelism"}, {"title": "Simulating Hybrid Parallelism", "content": "A key feature of the simulator is its ability to simulate hybrid parallelism, which combines both Data Parallelism (DP) and Tensor Parallelism (TP). The simulate_task_execution function enables the simultaneous use of both parallel strategies. The function first generates all possible combinations of DP and TP configurations using the get_configurations method, ensuring that the number of GPUs used does not exceed the available resources.\nThe simulation process begins with configuration generation, where the get_configurations method creates all feasible DP and TP configurations by varying the number of GPUs allocated to the DP and TP tasks. The objective here is to explore the full range of possible configurations and identify the optimal balance between the two parallel strategies.\nOnce the configurations are generated, the simulator proceeds with the task execution simulation. The simulate_hybrid_parallel method is used to simulate the execution of tasks under each DP and TP configuration. This method considers important factors such as compute time, memory bandwidth, and communication delays. By accounting for these variables, the simulator provides a detailed assessment of how the tasks are executed in a hybrid parallel environment.\nFinally, after simulating the execution for all configurations, the simulator evaluates the total execution time for each configuration. The optimal configuration is the one that minimizes the total execution time across all tasks. This configuration is selected based on its ability to best balance the execution times of both DP and TP tasks, while adhering to the constraints of available GPU resources. The goal is to achieve the most efficient use of resources, ensuring that the task execution is optimized across the hybrid parallel environment."}, {"title": "Implementation", "content": "In this section, we describe the implementation of our system, which is built using Python and consists of a frontend and a backend. The frontend serves as the user interface, while the backend handles core functionalities, including user request processing and performance analysis."}, {"title": "Backend Implementation", "content": "The backend, implemented in Python, acts as the core component for processing user requests and executing logic. It is composed of three main modules: 'inference engine', 'model analyzer', and 'parallel simulator'. The \u2018inference engine' module serves as the interface, accepting user inputs such as budget, selected model, generated sequence length, and input sequence length. Based on these inputs, it triggers backend processes to provide outputs, including the hardware configuration with the highest throughput, the configuration with the minimum inference time, and the top three hybrid parallelism configurations optimized for performance and budget constraints. To achieve these outputs, 'inference engine\u02bb coordinates with 'model analyzer' and 'parallel simulator', calculates performance metrics, and presents the results to the user for informed decision-making.\nThe second module, 'model analyzer', built on the 'llm-viewer' framework, evaluates the performance of inference frameworks and optimizations. This module integrates frameworks such as vllm and fastgen for model assessment and considers key-value (KV) store optimizations, including h2o, to enhance data handling during inference. By analyzing the impact of these frameworks and optimizations on throughput and inference time, 'model analyzer' identifies the most suitable configurations for the user's specific requirements.\nThe third module is a hybrid parallelism simulator designed to test various parallelization strategies within the constraints of the user's budget. It evaluates combinations of data parallelism, pipeline parallelism, and model parallelism to determine configurations that minimize inference time and maximize throughput. By considering the effect of budget constraints on these strategies, 'parallel simulator' identifies optimal configurations that balance performance with resource availability, enabling users to achieve efficient model deployment under limited resources."}, {"title": "User Interface", "content": "A web-based user interface (UI) was developed to configure simulation parameters and display results. The UI allows users to select a model from a dropdown menu, input a budget, and configure parameters such as sequence length, throughput requirement, latency preference, and precision tolerance."}, {"title": "Evaluation", "content": null}, {"title": "Experimental Setup", "content": "We evaluate the accuracy and performance of our simulator by comparing simulated results with real experimental data under various configurations. The primary goal of the evaluation is to model and analyze the behavior of large language model (LLM) inference tasks in real-world environments, then compare it with the predictions of our simulator. The evaluation involves two parallel tracks: real-world execution on actual hardware and simulated execution within the modeling framework, with the results from both tracks used to compute prediction errors as key evaluation metrics.\nIn real-world experiments, tasks are designed to simulate common LLM inference scenarios. Each task involves a set of input prompts, where the prompt lengths are randomly generated with fixed values to represent realistic usage patterns. For the outputs, the simulated tasks generate fixed-length sequences, while in real-world tasks, we intentionally limit the maximum output length to align with the fixed output length in the simulation. This approach introduces some unavoidable discrepancies, as real-world model outputs are inherently variable and difficult to fully control.\nEach model was tested with varying configurations to evaluate the simulator's accuracy under diverse scenarios. The configurations included batch sizes of 4, 8, 16, 32, which represent different levels of parallel workloads, and prompt lengths of 128, 256, 512, 1024, which reflect diverse input complexities. The output length was fixed at 256 tokens in the simulation environment for standardization, while in real-world experiments, the maximum output length was constrained to 256 tokens to align with the simulation. These configurations were chosen to comprehensively cover real-world usage patterns and provide a robust evaluation of the simulator's performance.\nIn the simulator, the same configurations, including hardware settings, batch sizes, prompt lengths, and model architectures, are replicated. The simulator generates predictions for three key metrics: Time-to-First-Token (TTFT), which measures the time required to produce the first token; Decode Throughput, defined as the throughput during the decoding phase, measured in tokens processed per second; and Batch Latency, representing the total time required to complete the processing of an entire batch. These predictions are compared against real-world measurements to calculate the error values, which serve as indicators of the simulator's accuracy.\nThe experiments are conducted on diverse hardware, software, and model configurations. For single-GPU experiments, we tested VLLM on NVIDIA A100 80G, V100 32G, A6000, and RTX 4090 GPUs, while FastGen was evaluated on NVIDIA A100 80G, A6000, and RTX 4090. For multi-GPU experiments, we used NVIDIA A100 80G, A6000, and RTX 4090 under tensor parallelism for both frameworks. The large language models assessed include THUDM/chatglm3-6b, Qwen/Qwen2-7B, facebook/opt-6.7b, and meta-llama/Llama-2-7b-hf."}, {"title": "Results Analysis", "content": null}, {"title": "Single-GPU Evaluation", "content": "In our single-GPU experiments, we evaluated a variety of models to assess their error performance across different configurations. To simplify the data presentation, we report the mean error rates across all tested models, as summarized in Table 2. Specifically, the mean errors for Batch Latency, TTFT, and Decode Throughput are 33.04%, 33.31%, and 51.43%, respectively, for VLLM, and 32.74%, 41.43%, and 54.94% for FastGen. Figure 13 provides a detailed view of these errors across different GPU types, input lengths, and batch sizes. This aggregated presentation offers insight into the general trends and variations in prediction errors under diverse configurations.\nThe results presented in Table 2 and Figure 13 reveal the complexities in accurately predicting GPU performance across diverse configurations. While the overall trends show"}, {"title": "Multi-GPU Evaluation", "content": "In our multi-GPU experiments", "metrics": "Batch Lat"}]}