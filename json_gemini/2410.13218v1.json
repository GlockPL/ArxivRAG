{"title": "CBT-BENCH: Evaluating Large Language Models on Assisting\nCognitive Behavior Therapy", "authors": ["Mian Zhang", "Xianjun Yang", "Xinlu Zhang", "Travis Labrum", "Jamie C. Chiu", "Shaun M. Eack", "Fei Fang", "William Yang Wang", "Zhiyu Zoey Chen"], "abstract": "There is a significant gap between patient needs\nand available mental health support today. In\nthis paper, we aim to thoroughly examine the\npotential of using Large Language Models\n(LLMs) to assist professional psychotherapy.\nTo this end, we propose a new benchmark,\nCBT-BENCH, for the systematic evaluation\nof cognitive behavioral therapy (CBT) assis-\ntance. We include three levels of tasks in CBT-\nBENCH: I: Basic CBT knowledge acquisi-\ntion, with the task of multiple-choice questions;\nII: Cognitive model understanding, with the\ntasks of cognitive distortion classification, pri-\nmary core belief classification, and fine-grained\ncore belief classification; III: Therapeutic re-\nsponse generation, with the task of generating\nresponses to patient speech in CBT therapy\nsessions. These tasks encompass key aspects\nof CBT that could potentially be enhanced\nthrough AI assistance, while also outlining a\nhierarchy of capability requirements, ranging\nfrom basic knowledge recitation to engaging in\nreal therapeutic conversations. We evaluated\nrepresentative LLMs on our benchmark. Exper-\nimental results indicate that while LLMs per-\nform well in reciting CBT knowledge, they fall\nshort in complex real-world scenarios requiring\ndeep analysis of patients' cognitive structures\nand generating effective responses, suggesting\npotential future work.", "sections": [{"title": "1 Introduction", "content": "Mental health conditions have reached alarm-\ning levels globally, with one in eight people af-\nfected, according to the World Health Organization\n(2023). There is a severe shortage of mental health\nprofessionals. In the U.S., more than 160 million\npeople live in areas with insufficient mental health\nproviders, with rural regions being especially un-\nderserved\u00b3. This critical gap underscores the need\nfor AI-driven tools to support professionals and\nexpand access to care. Existing research has ex-\nplored mental health condition classifications (Gao\net al., 2018; Senn et al., 2022), empathetic con-\nversations (Sharma et al., 2021, 2023a; Adikari\net al., 2022), and chatbots designed for simple\ndiscourse structures (Hsu et al., 2023). However,\nwork on professional assistance in real therapy\nsettings remain limited. Some studies have ad-\ndressed specific tasks in cognitive behavioral ther-\napy (CBT) (Beck, 2020), such as cognitive distor-\ntion classification (Chen et al., 2023b). However,\nthere are many other critical stages in CBT that\ncould potentially be enhanced through automation.\nIn this work, we aim to thoroughly investigate\nthe proficiency and potential of LLMs in support-\ning various facets and stages of professional mental\nhealth care. To this end, we propose CBT-BENCH,\na systematic benchmark for evaluating CBT effi-\ncacy. CBT-BENCH is structured in three levels,\nfrom CBT knowledge recitation to the therapeutic\nresponses generation in CBT sessions, providing\na hierarchical assessment of CBT capabilities. To\nensure the professionalism and high quality of our\nbenchmark, we collaborate with domain experts\n(clinical psychologists, professors, and social work-\ners) throughout the construction of CBT-BENCH.\nIn level I, we aim to assess basic CBT knowl-\nedge acquisition. We propose CBT-QA, a new\ndataset of 220 multiple-choice questions. The QA\npairs are collected from CBT exam questions for\nMaster of Social Work (MSW) students as well as\ncompositions from CBT experts, covering a wide\nrange of CBT knowledge, including basic concepts,\npractical knowledge, and case studies.\nIn level II, we aim to assess cognitive model"}, {"title": "2 Related Work", "content": "Our work is the first to systematically evaluate\nLLMs' ability to assist professional human ther-\napists in the specialized field of CBT. The works\ndirectly related to our research include injecting\ndomain knowledge of mental health into the mod-\nels (Yang et al., 2024; Kim et al., 2024), cognitive\ndisorder detection (Shreevastava and Foltz, 2021a;\nWang et al., 2023; Chen et al., 2023c), negative\nthoughts recognition and reframing (Sharma et al.,\n2023b; Maddela et al., 2023; Sharma et al., 2024),\nand patient simulation (Chen et al., 2023a) or ther-\napist simulation (Liu et al., 2023) in therapeutic\nconversation. Wang et al. (2024) proved that with\nclear modeling of the cognitive model of patients,\nLLMs could act more like real patients. Also, we\nbelieve that enhancing the understanding of the\ncognitive models of patients could be beneficial to\ninterpretable medical decisions (Yang et al., 2023),\nwhich is a crucial step towards reliable and safe\nautomated mental healthcare (Ji et al., 2023; Grabb\net al., 2024). In the proposed therapeutic response\ngeneration task, we leverage the feedback from\nprofessional therapists to assess the potential of\nLLMs responding like a therapist in real therapy\nsessions, while Louie et al. (2024) explored how\nto make LLMs roleplay patients with the feedback\nfrom professional therapists."}, {"title": "3 CBT-BENCH", "content": "In this section, we elaborate on how CBT-BENCH\nis constructed. We discuss level I, II, and III tasks\nin \u00a73.1, \u00a73.2, and \u00a73.3, respectively. Note that apart\nfrom collaborating with social work professors and\nclinical psychologists, who are our co-authors, we\nrecruit CBT experts (clinical psychologists, social\nworkers, etc.) from UpWork for all additional data\nannotation tasks (See \u00a78 for recruiting details)."}, {"title": "3.1 Level I: Basic CBT Knowledge\nAcquisition", "content": "To evaluate the basic CBT knowledge acquired by\nLLMs, we propose a new dataset, CBT-QA, en-\ncompassing 220 multiple-choice questions. We\nfirst worked with two social work professors and\ncollected 92 multiple-choice questions from the ex-\nams they issued for graduate CBT courses. Each\nquestion has 2-5 answer choices. Then, we hired\nfour CBT experts to compose new QA pairs, using\nthe QA pairs from the exam source as guidance,\nand each expert was tasked with composing 50.\nTo ensure high quality, we required the experts\nto cross-verify the QA pairs and excluded those\ndeemed of low quality. We ended up with 178 high-\nquality QA pairs, which, when combined with the\n92 ones sourced from exams, resulted in our CBT-\nQA dataset of 270 QA pairs. We randomly selected\n50 ones for use such as in-context learning exam-\nples. The remaining 220 pairs were designated as\nthe final test set.\nTo get a better understanding of the fine-grained\nknowledge types involved in CBT-QA, we em-\nployed another two CBT experts to categorize the\ntest set into four categories. Table 1 shows example\nQA pairs from the four categories and correspond-\ning distributions in the test set. See Appendix B\nfor more examples. To assess human performance,\nwe employed another two CBT experts to solve the\ntest set and ended up with an average accuracy of\n90.7% with an agreement rate over 80%."}, {"title": "3.2 Level II: Cognitive Model Understanding", "content": "In the exam setting of CBT-QA, most questions\nassess the models' ability to recite knowledge, as\nLLMs may have seen this information during pre-\ntraining-especially when it comes to basic \u0421\u0412\u0422\nknowledge and concepts that are prevalent online.\nHowever, knowledge recitation alone is insufficient\nto support the effective use of LLMs in assisting\nreal practice. To address this, we propose the Level\nII task set: cognitive model understanding. In\nCBT, therapists develop a cognitive model to rep-\nresent a patient's unhealthy cognitive processes\nthat contribute to their mental health issues (Beck,\n2020). A cognitive model typically includes beliefs,\nthoughts, emotions, and other relevant elements.\nThe associations among these components provide\na clear representation of the patient's maladaptive\ncognitive process (See Appendix B for an example\ncognitive model). Constructing a patient's cogni-\ntive model is a crucial step in CBT (Beck, 2020;\nBeck and Haigh, 2014; Wang et al., 2024), as thera-\npists must first summarize these maladaptive struc-\ntures before working to correct them. We believe\nthat the process of cognitive model construction\ncan be effectively enhanced with the assistance of\nLLMs, which could help in automatically identify-\ning key components\u2014such as beliefs and thought\npatterns\u2014and their associations.\nExisting work mostly focuses on cognitive dis-\ntortion classification (Sharma et al., 2023c; Shree-\nvastava and Foltz, 2021b; Chen et al., 2023b)-\nclassifying the maladaptive thinking patterns in\na patient's speech, which can be viewed as one\ncomponent of cognitive modeling. However, ex-\nisting datasets are typically limited by poor qual-\nity (Shreevastava and Foltz, 2021b; Chen et al.,\n2023b) or simplicity of the input speech. In our\nLevel II tasks, we first propose a new dataset, CBT-CD, for cognitive distortion classification with en-"}, {"title": "3.3 Level III: Therapeutic Response\nGeneration", "content": "At this level, we take a further step to explore a\nlongstanding and challenging question: Do LLMs\nhave the ability to conduct effective therapeutic\nconversations? This ultimate capability assesses"}, {"title": "4 Level I and II Experiments", "content": "We evaluated six popular LLMs' performance\non basic CBT knowledge acquisition (level I)\nwith CBT-QA and cognitive model understand-\ning (level II) with CBT-CD, CBT-PC, and CBT-FC. The models include a close-sourced model\nGPT-40 and five open-sourced models of sizes\nranging from 7B to 405B: Mistral-v0.3-7B (Jiang\net al., 2023), Gemma-2-9B (Gemma Team, 2024),\nLlama-3.1-8B, Llama-3.1-70B, and Llama-3.1-405B (Llama Team, 2024). We refer to models\nwith sizes under 10B as small models while others\nas large models (including GPT-40).\nWe cast the classification tasks of level II into\nmultiple-choice questions, where each question can\nhave more than one option. Prompt examples for\neach task of level I and II can be found in Ap-\npendix C. The inferencing temperature is set as 0.0\nto eliminate the randomness of the model genera-\ntions for reproducibility and data type bfloat16 is\nused for the model weights and activations."}, {"title": "4.2 Results", "content": "Table 5 shows the performance of the LLMs. We re-\nport accuracy for CBT-QA and weighted precision,\nrecall, and F1 score for the other three datasets.\nHere are the findings:\nLarge LLMs are better at answering CBT\nknowledge questions. Large models could achieve\nhigher accuracies on CBT-QA than small models.\nThis could be attributed to larger models storing\nmore knowledge. However, Gemma-2-9B has a\nsimilar model size as Llama-3.1-8B and Mistral-v0.3-7B and it surpasses these two by a significant\nmargin, even comparable to Llama3.1-70B. This\nmay be caused by the distribution difference of the\ndata used for training.\nSimply making the models larger could not en-\nhance their understanding ability of the cogni-\ntive model. Mistral-v0.3-7B and Llama-3.1-7B\nthese two small models get the best F1 scores on\nCBT-CD and CBT-PC, outperforming models of\nlarge sizes. This gives us insight that when in-\ncreasing the model size to pursue better general\ncapabilities like reasoning, the expertise of mental\nhealth care should also receive attention.\nCurrent LLMs struggle with detecting fine-\ngrained cognitive disorders or core beliefs. CBT-CD and CBT-FC are very challenging for current\nLLMs. The models generally perform poorly on\nthese two datasets which are very difficult even\nfor professional therapists. Enhancing these abili-\nties of LLMs could make the therapy process more\nefficient and accurate, and our datasets serve as\nvaluable resources to propel potential advances.\nTo delve further, we examine the accuracies for\nquestions of different knowledge types in CBT-QA\nand detailed F1 scores on labels in CBT-CD, CBT-PC, and CBT-FC. The results for two fine-grained\nclassification tasks, CBT-CD and CBT-FC, are\nshown in Figure 17. We could see that for CBT-QA, questions of basic CBT knowledge and con-\ncepts and others are easier to answer than questions\nof case studies and practical CBT knowledge. This\ncould be the reason that questions of case studies\nand practical CBT knowledge require more under-\nstanding of how to use the corresponding knowl-\nedge, not just reciting them. For CBT-CD, the\nmodels all struggle to identify mental filter, manifi-\ncation, and overgeneralization disorders. And large\nmodels are not consistently outperforming small"}, {"title": "5 Level III Experiments", "content": "We evaluate three LLMs' performance for CBT-DP (level III): Llama-3.1-8B, Llama-3.1-405B,\nand GPT-40. For all generations, we keep the\ngeneration temperature at 0.7. For evaluation, as\nsuggested by domain experts (\u00a73.3), we conduct\npairwise comparisons between model generations\nand the reference responses in (Boswell and Con-stantino, 2022) under our proposed criteria set\n(\u00a73.3). To ensure a fair comparison, we prompt the\nLLMs to generate responses within similar lengths\nas the reference (see Appendix C)."}, {"title": "5.2 Expert Evaluation", "content": "To minimize bias in the annotation process, we\nmixed and randomly shuffled the comparison pairs\nacross all models. For each annotation pair, we\nrandomized the order in which the model-generated\nand reference responses appeared in the annotation\ninterface. This approach ensures that annotators\nremain unaware of the source of each response.\nWe instruct the experts to label each pair with one"}, {"title": "5.3 Overall Result and Analysis", "content": "Here we first report the overall pairwise compari-\nson results between each LLM vs. reference on 10\nexercise categories in Table 6. Notably, the average\nresults show that all three LLMs still lag behind hu-\nmans by a significant margin, with Llama-3.1-405B\nachieving the best result while GPT-40 is the worst.\nMoreover, Llama-3.1-8B and GPT-40 never sur-pass human-written references in every single class\nof exercise, while Llama-3.1-405B shows some\nsuperior results in half of the exercise categories.\nFurther insights reveal that Llama-3.1-405B is bet-"}, {"title": "6 Conclusion", "content": "In this paper, we propose a new benchmark, CBT-BENCH, to comprehensively evaluate the efficacy\nof assisting CBT with LLMs. CBT-BENCH con-\ntains three levels of tasks ranging from basic CBT\nknowledge acquisition to therapeutic response gen-\neration, totaling five new datasets constructed by\ndomain experts. We experiment with representa-"}, {"title": "7 Limitations", "content": "In this work, we propose a set of novel tasks aimed\nat addressing key stages of CBT where LLMs could\npotentially assist professionals. We collaborated\nclosely with domain experts to select tasks that\nare both important and challenging, and that are\nwell-suited for AI enhancement. However, annotat-\ning datasets in highly specialized fields like mental\nhealth is very costly, which has limited the size of\nour datasets and constrained our ability to scale to\nadditional tasks. There may be other stages in CBT\nthat could also benefit from AI assistance, which\nwe leave for future research. Additionally, for the\nlevel III task, we used deliberate practice as an ef-\nfective proxy to evaluate performance, given the\ndifficulty of acquiring real CBT session data due to\nprivacy concerns. Bridging this gap by approximat-\ning real CBT sessions, while respecting ethical and\nprivacy constraints, remains an important direction\nfor future work."}, {"title": "8 Ethics Statement", "content": "IRB (Institutional Review Board) Approval.\nThis project is approved by our Institutional Re-\nview Board (IRB). For all the data annotation and\nevaluation, we work with our co-authors (clinical\npsychologists, professors, and social workers) and\nhire experts from UpWork. Experts are defined\nas those with a graduate degree in clinical psychol-\nogy, social work, or other related majors and have\nreceived at least 5 hours of CBT training. We hired\na total of 8 US-based experts, each at an hourly\nrate of $60. For some experts, we agreed on a fixed\npayment based on an estimated completion time,\ncalculated using the same hourly rate.\nInformed Consent. All experts hired were 18\nor older and provided informed consent (Check\nAppendix H for the consent forms). We did not\nassess any clinical outcomes. All data collected\nfrom the experts were de-identified and consented\nto be released for research purposes.\nSystem and Data Usages. In this work, our pri-\nmary goal is to systematically evaluate the capabil-\nities of large language models (LLMs) in assisting\nCognitive Behavioral Therapy (CBT) in order to\nprovide insights for the future development of AI-based tools to support psychotherapy professionals.\nAll data and systems developed in this work are in-\ntended exclusively for academic research purposes.\nThey should not be used as real applications. We\nrecognize the importance of human expertise and\njudgment in therapeutic settings and affirm that\nAI should be used to augment, not substitute, the\nessential role of trained professionals. Future devel-\nopments of AI-based systems should not be used\ndirectly to interact with real patients without the\nsupervision of professionals. We strongly advocate\nfor rigorous ethical oversight in the deployment\nof AI-based systems in sensitive areas like mental\nhealth. Furthermore, all data used in this study\nwere either publicly available or collected with full\ntransparency and consent, ensuring the privacy and\nconfidentiality of individuals involved. All data\ncollected from the experts were de-identified and\nconsented to be released for research purposes."}]}