{"title": "Imitating Language via Scalable Inverse Reinforcement Learning", "authors": ["Markus Wulfmeier", "Michael Bloesch", "Nino Vieillard", "Arun Ahuja", "J\u00f6rg Bornschein", "Sandy Huang", "Artem Sokolov", "Matt Barnes", "Guillaume Desjardins", "Alex Bewley", "Sarah Maria Elisabeth Bechtle", "Jost Tobias Springenberg", "Nikola Momchev", "Olivier Bachem", "Martin Riedmiller", "Matthieu Geist"], "abstract": "The majority of language model training builds on imitation learning. It covers pretraining, supervised fine-tuning, and affects the starting conditions for reinforcement learning from human feedback (RLHF). The simplicity and scalability of maximum likelihood estimation (MLE) for next token prediction led to its role as predominant paradigm. However, the broader field of imitation learning can more effectively utilize the sequential structure underlying autoregressive generation. We focus on investigating the inverse reinforcement learning (IRL) perspective to imitation, extracting rewards and directly optimizing sequences instead of individual token likelihoods and evaluate its benefits for fine-tuning large language models. We provide a new angle, reformulating inverse soft-Q-learning as a temporal difference regularized extension of MLE. This creates a principled connection between MLE and IRL and allows trading off added complexity with increased performance and diversity of generations in the supervised fine-tuning (SFT) setting. We find clear advantages for IRL-based imitation, in particular for retaining diversity while maximizing task performance, rendering IRL a strong alternative on fixed SFT datasets even without online data generation. Our analysis of IRL-extracted reward functions further indicates benefits for more robust reward functions via tighter integration of supervised and preference-based LLM post-training.", "sections": [{"title": "1 Introduction", "content": "In recent years, the imitation of existing human knowledge via large datasets has become a key mechanism underlying increasingly capable and general artificial intelligence systems [17, 41, 9]. Pretraining and supervised fine-tuning phases for large language models (LLMs) predominantly rely on imitation learning, in particular next token prediction via maximum likelihood estimation (MLE). In addition, preference-based fine-tuning is affected by imitation via initial online data generation and optimization objectives such as regularization towards the previously fine-tuned LLM [42, 12].\nThe field of imitation learning for sequential decision making has a long-standing history for applications such as robotic control [4, 29]. Recently, perspectives to language modeling have shifted towards explicit treatment as a sequential decision making problem \u2013 in particular for later stages of model adaptation via reinforcement learning from human feedback (RLHF) [42, 13, 17, 62]. This vantage point opens up new opportunities for the effective use of different data sources and obtaining aligned models that better represent human intent. It includes a broader scope for which data contains information about rewards and preferences as well as the dynamics-aware optimization of each action based on its future impact \u2013 all while taking into account computational scalability.\nDue to the importance of imitation learning for language modelling, we believe detailed analysis of the underlying imitation problem for sequential decision making is warranted. Despite the widespread perspective of RL for aligning and fine-tuning language models (via RLHF), supervised learning via maximum likelihood estimation for next token prediction remains the dominant component of our imitation learning pipelines due to its simplicity and scalability. However, pure MLE for next token prediction (including \"teacher forcing\" [58]) can create challenges in autoregressive models, many of which being related to classic challenges with behavior cloning [8], its equivalent in the context of sequential decision making. Compounding errors can occur due to iterative model application creating data sequences which further shift from the model's training distribution [51, 30], growing increasingly likely for longer sequences [16]. In particular, a model's own samples can cause such distribution shifts and exposure bias [47, 6, 5]. Taking the RL perspective to imitation aims to mitigate these issues via dynamics-aware optimization, where each action is optimized for the impact on the whole future trajectory. It further enables a shift from passive to active learning, where the system actively generates data. Furthermore, best performance of the fine-tuned model is only one metric of importance. Indeed, continued alignment of language models to human preferences requires sampling for a given prompt, collecting human preferences over these completions, and finally aligning the model via preference fine-tuning. Improving the diversity of sampled completions via temperature sampling [7], or sampling from a mixture of past models [55] has been linked to improvements in downstream performance. Studying inverse RL, and potential divergences, and regularizations provides another angle to increasing diversity [23].\nIn this paper, we investigate RL-based optimization, in particular the distribution matching perspective to inverse reinforcement learning (IRL), for fine-tuning language models; which can be contrasted with standard MLE as depicted in Figure 1. Our goal is improved understanding of when, and how, IRL can be used as an effective alternative for supervised MLE in the fine-tuning pipeline. The evaluation covers both adversarial and non-adversarial, offline and online methods. We further extend inverse soft Q-learning [20] to explicitly provide a principled connection to classical behavior cloning or MLE. Our experiments range from 250M to 3B parameter models for the encoder-decoder T5 [46] and decoder-only PaLM2 [3] models. Throughout evaluation, we investigate both task performance and diversity of model generations illustrating clear benefits of inverse RL over behaviour cloning for imitation learning. A further, principal benefit of RL-centric imitation is the natural connection to later RLHF stages via rewards obtained from demonstration data and we take first steps to analyse the value of IRL-obtained reward functions.\nOur key contributions are:\n\u2022 We investigate the RL-centric perspective to imitation for LLMs, extracting rewards and directly optimizing actions for sequence generation instead of individual token likelihood."}, {"title": "2 Methods", "content": "Language generation can be modeled as sequential decision making problem. On a token level, it is the problem of generating the next token $x_i$ given the already generated sequence of tokens $(x_0,..., x_{i-1})$. We thus seek a distribution $\\pi(x_i|x_0,..., x_{i-1})$, which we will also refer to as a policy. For a given policy the likelihood of generating a sequence $x = (x_0,...,x_n)$ can be computed autoregressively:\n\n$p(x) = \\prod_{i=0}^{N} \\pi(x_i|x_0,..., x_{i-1}).$ \n\nThe classic maximum likelihood estimation based approach leverages this factorization in order to efficiently train the policy by maximizing the log-likelihood of the training sequences, $D = \\{x,...,x^M\\}$:\n\n$\\arg \\max_{\\pi} \\sum_{x \\in D} \\log p(x) = \\arg \\max_{\\pi} \\sum_{x \\in D} \\sum_{i=0}^{N} \\log \\pi(x_i|x_0, ..., x_{i-1}).$\n\nFor fine-tuning problems, where a pre-trained model is fine-tuned to a particular set of tasks, the problem can be formulated analogously but may have additional conditioning variables which we will leave out for the sake of clarity.\nDistribution matching. State-action distribution matching algorithms [25, 20], which are well-established in the field of imitation learning \u2013 and can be seen as solving an IRL problem see e.g. [25] \u2013 approach the problem in a different manner: They seek to minimize the divergence between the \\(\\gamma\\)-discounted state-action distribution of the policy $\\pi(a|s)$ and the discounted state-action distribution of the expert policy $\\pi_E(a|s)$. We define the discounted state distribution for a Markov Decision Process (MDP) as $p(s) = (1 - \\gamma) \\sum_{i=0}^{\\infty} P(s_i = s|\\pi)$ and the discounted state-action distribution is accordingly defined as $\\mu_{\\pi}(s, a) = \\pi(a|s)p(s)$; where $P(s_i = s|\\pi)$ is the probability of seeing state s when acting according to the policy $\\pi$. For our autoregressive generation MDP, the state corresponds to the concatenation of already generated tokens (commonly including the prefix or prompt), $s_i = (x_0,..., x_{i-1})$ and the action is the next token, $a_i = x_i$ thus yielding a problem with deterministic dynamics. We omit state and action arguments in the following discussion whenever it is clear from the context.\nIn order to enable more straightforward algebraic manipulation, the divergence is often combined with a weighted causal entropy term $H(\\pi) = -E_{\\mu_{\\pi}} \\log(\\pi(a|s))$. The goal is then to find a policy that minimizes the objective $J(\\pi)$:\n\n$J(\\pi) := D_f(\\mu_{\\pi}||\\mu_E) - \\lambda H(\\pi),$\n\nwhere $D_f = E_{\\mu_E}[f(\\frac{\\mu_{\\pi}(a, s)}{\\mu_E(a, s)})]$ is an f-divergence. Different f-divergences have been used in the literature ([25, 23]).\nWhen taking the example of the reverse KL divergence, with $f(t) = f_{RKL}(t) = - \\log(t)$, we can decompose the objective into a state distribution and an MLE term:\n\n$\\min_{\\pi} D_{f_{RKL}}(\\mu_{\\pi}||\\mu_E) = KL(\\rho_E||\\rho) + E_{\\rho_E}[KL(\\Pi_E||\\pi)];$"}, {"title": "3 Experiments", "content": "In this section, we evaluate the benefits of different inverse RL based methods in comparison to MLE for training large language models. We assess their impact on task performance, diversity of model generations, and computational requirements. Concretely, we compare MLE-based next token prediction and different IRL methods for fine-tuning LLMs on common benchmarks. In addition, we perform ablations on online\u00b3 vs offline versions of IQLearn, showing results across dataset and model sizes. We finally add analysis of the implicitly learned rewards extracted from SFT data via IRL methods (which bring the potential downstream use to aid RLHF/RLAIF [42, 36] training stages).\nThese experiments mainly aim to answer the following questions:\n\u2022 Do IRL methods provide a scalable, effective alternative to MLE for fine-tuning?\n\u2022 How are different algorithms placed on the Pareto front of task performance and diversity?\n\u2022 For which task and dataset properties is IRL particularly relevant?\n\u2022 What is the impact of online data for IRL training?\n\u2022 How informative are rewards extracted from SFT data?"}, {"title": "3.1 Algorithms, baselines, and datasets", "content": "In addition to naive maximum likelihood estimation for next token prediction, we evaluate the following IRL methods. Generative adversarial imitation learning (GAIL) [25] represents a common adversarial framework training a discriminator to distinguish transitions from agent and dataset and a separate policy. We heuristically adapt the algorithm to mitigate instability for adversarial training [63, 61] including the start for MLE-trained checkpoints and additional loss terms (including MLE) with further details in Appendix A.1.3. IQLearn [20] departs from adversarial learning and our reformulation from Eq. 14 enables us principled control of the temporal difference regularization component to retain stable training. We will use the reformulated offline variant of the algorithm in all experiments and further add an ablation to its online version in Section 3.3.1. Since we compare all methods with respect to task performance and diversity of generations, we additionally evaluate further entropy regularization terms for the MLE baseline; clearly denoted with 'ent-reg' in all plots, corresponding to the respective regularization parameter \u03bb in GAIL and MLE (see Appendix A.2 for a more detailed description). In line with previous work on inverse RL for language modelling,"}, {"title": "3.2 Quality-diversity evaluations", "content": "We evaluate both encoder-decoder and decoder-only model classes, respectively using the T5 [46] and PALM2 [3] models. Our main visualizations focus on task performance and diversity of model generations. For task performance, we use the standard metrics for the respective benchmarks (e.g. ROUGE-1 and accuracy percentage). To measure diversity of model generations we calculate self-similarity of generated examples as measured by Self-BLEU [67]. A high score denotes low diversity and vice versa. Using these different axes of evaluation allows us to visualize Pareto fronts between performance and diversity which we will use to assess algorithmic differences. We further add the evaluation via per-token-entropy in Appendix A.3.1."}, {"title": "3.2.1 T5 models", "content": "We perform experiments with the base, large, and xl T5 models [46] on the XSUM [39] and GSM8k [14] tasks. These models further serve as foundation for our later ablations. We are able to obtain small but notable gains in performance across all tasks, as shown in Figures 2 and 3, in particular math and reasoning tasks show clear improvements in accuracy when fine-tuning with IRL compared to MLE. We hypothesize that specific and shared structure of responses is better exploited via IRL methods. There is a more emphasized boost in the diversity of model generations for IQLearn over MLE. In comparison to prior work [59], we have been able to stabilize GAIL training, with minor changes described in Appendix A.1.3, but are required to start from a checkpoint previously trained via MLE to ensure strong similarity between dataset and model generations starting GAIL training. This only applies to the T5 model class and for PaLM models GAIL is highly challenging to stabilize with details in Appendix A.1.3. While entropy terms can further be added to MLE optimization, we hypothesize that better trade-offs between diversity and performance can be obtained via methods able to aggregate information across trajectories to optimize entropy over a different space, complete trajectories rather than just per step policy entropy."}, {"title": "3.2.2 PaLM2 models", "content": "We also perform experiments with PALM2 models [3], specifically fine-tuning from a pre-trained PALM2 'Gecko' model. We evaluate offline IQLearn on the summarization task TLDR [52], the mathematical reasoning task GSM8k [14], and the large (285M examples) English-to-German translation dataset WMT22 [33]. We limit these experiments to a single choice of the regularization parameter \u03bb per task to save computational costs and instead include an analysis of the effect of the sampling temperature parameter during sampling (noting that we similarly observed IQLearn outperforming MLE at varying temperatures for the T5 models). We selected the checkpoints with early stopping for WMT22 as BC was overfitting on the task, unlike TLDR and GSM8K for which we evaluated their latest checkpoints.\nSimilar to the previous section, we perceive improvements over MLE on all three benchmarks, though for lower accuracy values MLE covers a part of the front. Figure 4 summarizes these improvements, showing the performance of the trained models depending on the temperature used during sampling responses for the test set prompts. These results show a similar behavior between all three tasks, where IQLearn achieves higher performance in a low temperature regime."}, {"title": "3.3 Analysis and ablations", "content": "We perform additional experiments and post-training analyses to better understand the impact of dataset size, initial checkpoints and computational demands of offline and online algorithms."}, {"title": "3.3.1 Computational efficiency and accuracy for online & offline inverse RL", "content": "One of the key benefits of (offline) MLE compared with (online) IRL-based methods is its lower computational costs. These are principally related to online rollouts - slow, autoregressive sampling from the model. Our re-formulation of IQLearn results in an algorithm that can be applied offline on a fixed dataset, which underlies all IQLearn results presented, mitigating this limitation. In this section, we additionally present update and sampling times across algorithms and the comparison with the online application of IQLearn (see Appendix A.1.2). Figure 5 demonstrates minimal task performance gains (for the T5 base mode on GSM8k), though considerably improved diversity for model generations. At the same time, Table 1 visualizes the relative cost of sampling in comparison to different algorithm updates, excluding sampling. Note that MLE batch sizes are smaller than IRL ones as we add additional online samples to the batch. Experiment times for online IRL can be reduced, but at the cost of additional hardware, via distributed training with separate sampling infrastructure. The application choice of online or offline IRL finally lies with the practitioner trading of additional computational cost with diversity benefits.\nWe provide further intuition for the lower differences between offline and online Inverse RL via toy experiments in Appendix A.3.3. At its core, the specific structure of autoregressive language generation, in particular the concatenation underlying single-turn dynamics, prevents exact recovery after mistakes, which could be otherwise learned via IRL in the online setting. Therefore, extensions of the LLM action space to enable recovery can be a fruitful direction for increased benefits from online data with RL and IRL style methods [15]."}, {"title": "3.3.2 Dataset size ablations", "content": "Evaluating training on smaller subsets of GSM8k and XSUM with T5 base is respectively pictured in Figures 6 and 7. Performance improvements are consistent across dataset sizes. The analysis of subsets of XSUM further demonstrates increased robustness against overfitting with IQLearn not showing any of the performance loss over time that plagues MLE in particular with the smallest subsets which cannot be overcome with simple entropy regularization. In line with arguments around the compounding of errors in imitation learning [51], we find that both datasets with longer targets and smaller datasets show stronger task performance gains for IRL."}, {"title": "3.4 Reward analysis", "content": "In comparison to related work on applying IRL to classical control domains, there is no access to ground truth reward functions underlying the process of data generation. Instead, we measure the correlation between IRL extracted rewards and other task-specific performance metrics. High correlation here tells us how informative a reward function is w.r.t. task performance.\nIn particular, IQLearn represents learned rewards implicitly via the Q-function as $r_t = Q(s_t, a_t) - \\gamma V (s_{t+1})$, and its online version (i.e. with a non-zero mix-in ratio \u03b1 of on-policy examples, see Appendix A.1.2) additionally exposes the algorithm to (initially, low reward) policy rollouts to help discriminate between them and (high reward) SFT data. In Table 2, we report the Spearman's rank correlation coefficient between accumulated rewards (over complete sampled trajectories for the full validation sets) for online IQLearn (\u03b1 = 0.1) and task-specific metrics. Compared to MLE rewards, which, as expected, are not strongly correlated with any metric, we see a clear increase in correlation pointing to IQLearn incorporating the task-relevant quality into the extracted rewards. We find that using online data is important for consistent correlations across all tasks, in particular as we evaluate over agent generated rollouts. We hypothesize that the comparably lower correlations for GSM8k are likely to be explained by the task's idiosyncratic metric: only the correctness of the final generated numerical expression affects the accuracy calculation, effectively ignoring most of the trajectory (and so its transition's rewards) up to a specific answer segment. This highly targeted reward function becomes harder to learn. Finally, Figure 8 displays how the reward alignment with the task metrics increases with larger TD regularization \u03bb."}, {"title": "4 Related Work", "content": "General imitation learning. Imitation learning assumes a dataset of expert demonstrations, and the aim is to train a policy that matches the expert. There are two broad categories of imitation learning approaches: behavioral cloning (BC) and inverse reinforcement learning (IRL). In BC, a policy is trained using regression to directly mimic the expert demonstrations [45]. This is analogous to supervised fine-tuning of language models via MLE. BC requires sufficient data coverage to perform well, and suffers from compounding errors at evaluation time, as the policy deviates from the state distribution covered by the demonstrations. This can be alleviated by additionally querying the expert for the correct action in the states visited by the agent [51].\nInverse reinforcement learning. In contrast, IRL jointly infers the policy and reward function, such that the provided expert demonstrations are optimal under the reward function, and the learned policy maximizes this reward function [40]. By using additional environment interactions beyond the demonstrations, IRL can in theory overcome the compounding errors observed with BC [64]. Note that IRL is related to RL from human feedback (RLHF) but differs in key aspects: IRL also learns both a reward and policy, but extracts information from demonstration and agent data rather than paired preference data [31]. The game-theoretic approach to IRL treats the optimization problem as a zero-sum two-player game [54]. A subset of recent IRL methods can be seen as combining game-theoretic IRL with entropy regularization of the policy, where the doubly-nested optimization is implemented with a classifier (GAIL [25], DAC [34], AIRL [19]), implicit reward functions (ValueDICE [35], IQLearn [20]), or a Lagrangian dual objective (PPIL [56]). The stable, large-scale application of inverse RL methods has been a persistent goal throughout these developments. The classical requirement for complete RL optimization before updating the reward function has presented a limitation [69] which can be overcome via abstracted [60, 9] or linearized models [18], iterative adversarial training [25, 19] or lastly saddlepoint-based value function based formulations [20]. We expand on the insights of the latter to evaluate the competitive performance of computationally cheap offline IRL and emphasise the connection between MLE and IRL [44].\nImitation learning for language modeling. Understanding language modeling as an imitation problem has been previously explored. Indeed, the link can already be made from MLE, commonly referred to as Behavioral Cloning (BC) [8] from an imitation perspective. Although the link to imitation has been made explicit recently, either theoretically [53], or in the case of distillation [1, 28, 37, 26], some works had already tackled the issues of MLE with imitation-like techniques. For example, adversarial training of text generation an alternative to MLE was first proposed in SeqGAN [65], and followed-up by a series of work using GANs for text generation [32]. These methods have been shown to work only in the temperature 1 regime [11], a possible shortcoming that we address in Section 3. Then, leveraging the literature of imitation learning, GAIL was successfully adapted to language [59], showing an improvement over MLE. Closer to our contributions, IQLearn was also utilized for language in SequenceMatch [15]. Key differences to our work include the reformulation as temporal difference regularized MLE, comparison with other inverse RL methods and focus on computational costs via the application of offline IQLearn. Indeed, SequenceMatch requires the use of online data, via the introduction of the \"backward\" token, that allows the model to change a previously chosen token during sampling."}, {"title": "5 Discussions", "content": "Our investigation focuses on diversity measures such as Self-BLEU or model entropy which are easily calculable but limited with respect to their ability to describe the impact on later training stages. Future evaluation and practical application will demonstrate if the increased diversity is relevant to RLHF such as for human raters in preference data evaluation or improved exploration during subsequent RL optimization [48].\nThe field of imitation learning has led to a gamut of algorithms, many of which are intuitively simple to implement with existing RL or RLHF infrastructure (e.g. [49, 57, 24]). Ease of adaptation and hyperparameter tuning have principal impact on our practical algorithm choices and the methods and extensions discussed in this work enabled quick first results and iteration. Looking forward, the evaluation and adaptation of further imitation learning methods to LLMs is likely to lead to fruitful results in the coming years. While our analysis focuses on specific algorithms, our key arguments should be seen in the light of the benefits of underlying mechanisms rather than the specific methods."}, {"title": "6 Conclusions", "content": "This paper presents a detailed investigation of the potential of IRL algorithms for imitation in language model tuning focusing on performance, diversity, and computational requirements. We introduce a reformulation of IQLearn which enables principled interpolation between robust, standard supervised fine-tuning and more effective IRL algorithms. Our experiments demonstrate particularly strong gains for IRL on the Pareto front of task performance and diversity of model generations. While prior work primarily focused on online IRL, we demonstrate that computationally cheaper offline IRL, without the requirement of online sampling, already obtains crucial performance gains over MLE-based optimization. Additional correlation analysis between IRL-extracted rewards and performance metrics further emphasises the potential to obtain more accurate and robust reward function for language modelling. We hope this work will help to pave the way for better compromises between data and compute efficiency via RL-based algorithms across the complete LLM training pipeline."}, {"title": "A Appendix", "content": "A.1 Implementation Details\nA.1.1 IQLearn\nFor IQLearn, fine-tuning starts with a policy which is subsequently finetuned as a Q-function. In order to do so, we take the logits underlying the LLM softmax layer and continue training as Q-values. In entropy regularised RL, we can build on the equality between optimal policy \u03c0* and optimal Q-function Q* via \u03c0*(a|s) = 1/zs exp Q*(s, a) with normalization factor $Z = \\sum_{a'\\in \\mathcal{A}} exp Q^*(s, a')$.\nWe further obtain V via $V(s) = \\log \\sum_{a\\in \\mathcal{A}} exp Q(s, a)$.\nDue to the translation invariance of the softmax function, the there can be a considerable state-based offset between initial logits and final values. In other words, with $Q(s, a) = V(s) + A(s, a)$ only the advantage function A has to be accurately reflected by the initial policy logits, while the state-based value V can be arbitrarily inaccurate. In practice, we can add further KL regularization, or separate the representation of state and advantage function, to stabilize training during the additional identification of correct offsets but found it to be unnecessary for our experiments in comparison to recent work [15].\nWe handle terminal states by setting their values to zero in line with the original IQLearn paper [20]. Improvements such as learned values for the terminal states [2, 15] did not contribute to improved performance in this setting."}, {"title": "A.1.2 Online IQLearn", "content": "We also implemented an online version of IQLearn, which makes use of additional (non-expert) samples. In contrast to IQLearn [21] which makes use of these examples to estimate the initial value, $E_{\\rho_0}v(s)$ (which is equivalent between online and offline data when the prompt is the same as in our case), we integrate the additional samples to relax the distribution matching loss:\n\n$\\min_{\\pi} D_f ((1 - \\alpha) \\mu + \\alpha \\mu_{\\beta}||(1 - \\alpha) \\mu_E + \\alpha \\mu_{\\beta}) - \\lambda H(\\pi),$\n\nwhere $\u00b5B$ is the discounted state-action distribution of the additional samples and where \u03b1 \u2208 [0, 1) is the strength of the mix-in. The problem is thus relaxed and allows the policy to match the mix-in distribution $\u00b5\u03b2$ in case the expert is too difficult to match.\nFrom the adapted distribution matching loss the same steps can then be taken as in Section 2. This results in the generalised loss:\n\n$\\min_{v, \\pi^*} E_{(1-\\alpha)\\mu_E+\\alpha\\mu_{\\beta}}[f^*(-r(v^*, \\pi^*)) + r(v^*, \\pi^*)] - E_{\\mu_E} \\lambda \\log(\\pi^*)].$\n\nwith\n\nr(\u03c5", "\u03c0": "\u03c5", "pi": "E_{\\xi}\\gamma \u03c5"}, {"title": "A.1.3 GAIL", "content": "For GAIL, both the policy and discriminator are represented via separate networks initialized from the initial, pre-trained, LLM. Policy optimization is performed similar to PPO with an A2C update, with re-scaled advantage and KL constraint to the initial policy. The KL constraint has a weight hyperparameter that is annealed over 10,000 steps to a final cost weight displayed below. The value network is also initialized from the initial LLM. The discriminator is trained with a cross-entropy objective. The reward is re-shaped from the the discriminator output to be a positive reward:\nrt = log(1 + exp(D(st))).\nWe explore the heuristic combination of GAIL with standard MLE training by using a weighted combination of GAIL and MLE losses for the policy. While this is not required for GSM8k, it leads to considerable improvements for XSUM.\nPolicy, value function and discriminator are all updated with the Adam optimizer, with a constant learning rate and linear warm-up. The discriminator is updated after every step of policy optimization."}, {"title": "A.2 MLE & Entropy Regularization", "content": "As mentioned in the experiments section, we compare our methods to an entropy regularized version of MLE, to disentangle between the imitation contibution and the regularization. This algorithm simply follows the objective\n\n$\\min_{\\pi} E_{\\mu_E} [-\\log(\\pi) - \\lambda H(\\pi)],$\n\nwhere we compute the entropy at each token of the sequences form $\\mu_E$."}, {"title": "A.2.1 Computational Requirements", "content": "Our experiments with T5 models use TPU v3 infrastructure and are running between approximately 3 days and 2 weeks. Our experiments with PaLM2 models use TPU v4 infrastructure and are running under 1 week."}, {"title": "A.3 Additional Experiments", "content": "We include a set of further experiments to complement the results in the main paper."}, {"title": "A.3.1 Quality-Diversity Evaluation", "content": "In addition to the Self-BLEU metric, we further add plots for model entropy and add the ROUGE-LSUM results in Figure 9. Further performance metrics like MAUVE [43] and BertScore [66] can be of use in the future to further represent human judgement and preferences."}, {"title": "A.3.2 PALM2 Additional Results", "content": "We complement results on the temperature sweep over PALM2 models in Figure 10, with additional metrics for GSM8k and TLDR (accuracy with calculator and rouge scores). This confirms the results of the main paper experiment, showing that IQLearn can consistently outperform MLE in accuracy, even when sampling with a lower temperature than the training one."}, {"title": "A.3.3 Toy Experiments for Offline and Online IRL with Autoregressive Generation", "content": "The toy scenario displayed by the MDP in Figure 11 is used to represent a key difference between many classical control settings and autoregressive language generation. The agent starts in the left black state and has to reach the green on the right. In each of the bottom states the agent has 2 actions, move left or move up. The transition dynamics are noisy so that the agent executes the unwanted action 10% of the time. There are two variants of the MDP, one where the agent can return from the top states by learning to execute the right action and one where it cannot. The latter represents one aspect of concatenation dynamics, the agent cannot return to the exact same state after sampling the wrong action. In a way, it cannot correct its behaviour exactly.\nWhen training offline and online variants IQLearn in this setting with demonstrations without mistakes (and their correction), we clearly see that the online version of IQLearn outperforms offline learning by over 11% in success rates, while in the setting without recovery, the difference is considerably smaller and results are within each others confidence bounds with more variance for the offline agent."}, {"title": "A.3.4 Analyzing GAIL Stabilization", "content": "We empirically find GAIL overall more complex to tune and stabilize and aim to provide further insights here. Intuitively, control for language modelling differs from many classical applications via large discrete action space and terminations not being state but action conditioned. In other words, the agent can directly chose to terminate. Therefore the agent can learn to directly terminate if the discriminator provides negative rewards. If we provide positive rewards, tuning those rewards becomes a complex task as seen in Figure 12, where without additional MLE objective the GAIL agent often uses the maximum sample length with correlated loss of performance (visualized by the drop on ROUGE-1 values). Additional MLE training can help to both shape policy behaviour but further also provide more relevant data for the discriminator [50]. Especially in high dimensional action spaces it otherwise becomes challenging to obtain a useful reward signal from the discriminator as seen in prior work [59]."}, {"title": "A.4 Expanded Experiments on WMT22", "content": "For WMT22, we additionally evaluate IQLearn performance for a wide range of \u03bb values. Here, IQLearn consistently gains (up to +2.16 BLEU) over BC until very high values of \u03bb (Table 5). Note, that unlike the sampling performance curves in Figure 4, here beam search decoding (size 4) is used with a length penalty 0.6."}]}