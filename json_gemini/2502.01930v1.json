{"title": "Distributionally Robust Direct Preference Optimization", "authors": ["Zaiyan Xu", "Sushil Vemuri", "Kishan Panaganti", "Dileep Kalathil", "Rahul Jain", "Deepak Ramachandran"], "abstract": "A major challenge in aligning large language models (LLMs) with human preferences is the issue\nof distribution shift. LLM alignment algorithms rely on static preference datasets, assuming that they\naccurately represent real-world user preferences. However, user preferences vary significantly across\ngeographical regions, demographics, linguistic patterns, and evolving cultural trends. This preference\ndistribution shift leads to catastrophic alignment failures in many real-world applications. We address\nthis problem using the principled framework of distributionally robust optimization, and develop two\nnovel distributionally robust direct preference optimization (DPO) algorithms, namely, Wasserstein DPO\n(WDPO) and Kullback-Leibler DPO (KLDPO). We characterize the sample complexity of learning the\noptimal policy parameters for WDPO and KLDPO. Moreover, we propose scalable gradient descent-style\nlearning algorithms by developing suitable approximations for the challenging minimax loss functions\nof WDPO and KLDPO. Our empirical experiments demonstrate the superior performance of WDPO and\nKLDPO in substantially improving the alignment when there is a preference distribution shift.", "sections": [{"title": "Introduction", "content": "The alignment of large language models (LLMs) with human values and preferences is a central objective\nin machine learning, enabling these models to produce outputs that are useful, safe, and aligned with\nhuman intent. Since LLMs are trained on vast, diverse datasets using self-supervised learning, an additional\nalignment phase is often required to refine their behavior based on human feedback. A widely adopted\napproach for this is Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ziegler\net al., 2019; Ouyang et al., 2022), which involves training a reward model using human preference data and\noptimizing the LLM using reinforcement learning approaches, such as proximal policy optimization. More\nrecently, Direct Preference Optimization (DPO) has emerged as an alternative that simplifies the alignment\nprocess by directly optimizing model parameters based on human preferences without requiring an explicit\nreward model. These alignment techniques have played a crucial role in improving the ability of LLMs to\ngenerate responses that adhere to human expectations and societal norms, leading to today's powerful chat\nmodels (Achiam et al., 2023; Touvron et al., 2023).\nDespite the importance of the LLM alignment problem, RLHF and DPO remain fundamentally challenging\nand fragile, mainly due to three reasons. (i) Diversity of human preferences: Standard RLHF/DPO approaches\nimplicitly assume that human preferences can be accurately captured by a single reward function. In reality,\nhuman preferences are highly diverse, context-dependent, and distributional, making it infeasible to\nrepresent them with a one-size-fits-all optimization framework Zhao et al. (2024); Durmus et al. (2023)."}, {"title": "Related Work", "content": "Robust RLHF: Bai et al. (2022) proposed to adjust weights on the combination of loss functions based\non different topics (harmless vs. helpful) for robust reward learning. Chakraborty et al. (2024) proposed\nto learn multiple reward functions for different sub-populations through an expectation-maximization\napproach, and a robust policy based on these rewards via a max-min optimization, which is different from\nour distributional robust learning approach. Padmakumar et al. (2024) augmented the existing binary\npreference datasets with synthetic preference judgments to estimate the diversity of user preferences. Yan\net al. (2024) proposed a Bayesian reward model ensemble to quantify the uncertainty of reward estimation\nand used it to reduce reward overoptimization. Bukharin et al. (2024) proposed a robust RLHF approach for\naddressing the preference data corruption problem.\nRobust DPO: Huang et al. (2024) proposed XPO that implements the principle of pessimism in the face\nof uncertainty via regularization with the x\u00b2-divergence for avoiding reward hacking/overoptimization\nw.r.t. the estimated reward. Ramesh et al. (2024) proposed a group robust preference optimization (GRPO)\napproach for addressing the diverse preference problem. This approach considered the total DPO loss as the\nweighted sum of the individual DPO losses from individual preference data sets. They find the worst-case\nweights for the individual data set losses and the optimal parameter for the LLM against this worst-case\nloss, which is different from the distributional robust learning approach. Differently from this, our approach\ndoes not assume access to different data sets, and develops a direct distributionally robust learning variant\nof DPO. Chowdhury et al. (2024) considered the setting where e-fraction of the preference labels in the\ntraining dataset is corrupted and proposed a noise-robust algorithm to mitigate its effect assuming the\nknowledge of e. The work closest to ours is Wu et al. (2024) which used a distributionally robust approach\nto address a different problem of data corruption and noise in the preference data. Different from our work,\nthey neither consider the distribution shift problem nor provide any theoretical performance guarantees.\nHowever, in our empirical studies, we adapt this method as a baseline to compare our algorithms. We\nemphasize their work did not have similar experimental studies to address the preference distribution shift\nproblem.\nDistributionally Robust Learning: Distributionally robust learning is a statistical learning framework\ndesigned to enhance model performance under distributional shifts between training and test data Chen\nand Paschalidis (2018). It employs a minimax approach where an adversary maximizes the expected loss by\nshifting the test distribution within a specified uncertainty set, while the learner minimizes this adversarial\nloss. This approach using the f-divergence (Namkoong and Duchi, 2016; Duchi and Namkoong, 2021; Levy\net al., 2020) and the Wasserstein metric (Mohajerin Esfahani and Kuhn, 2018; Kuhn et al., 2019; Gao et al.,\n2022) have gained significant attention recently. Distributionally robust algorithms have been developed to\naddress problems in supervised learning Chen and Paschalidis (2018); Namkoong and Duchi (2016); Levy\net al. (2020), multi-armed bandits Si et al. (2020); Yang et al. (2023) and reinforcement learning Panaganti\net al. (2022); Zhou et al. (2024); Shi and Chi (2024); Yang et al. (2022)."}, {"title": "Preliminaries", "content": "Notations: We use calligraphic letters for sets, e.g., $S$ and $A$. For any vector $x$, $\\|\\cdot\\|$ denotes the Euclidean\nnorm. When $\\Sigma$ is some positive semi-definite matrix, we write $\\|x\\|_{\\Sigma} = \\sqrt{x^T\\Sigma x}$ as a semi-norm of $x$. For"}, {"title": "Distributionally Robust DPO", "content": "From the DPO objective (Eq. (5)), we define the pointwise DPO loss function as follows\n$\\ell(z; \\theta) = -y \\log \\sigma(\\beta h_\\theta(s, a^\\omega, a^\\ell)) - (1 - y) \\log \\sigma(\\beta h_\\theta(s, a^\\ell, a^\\omega)),$ \nwhere $h_\\theta(s, a^\\omega, a^\\ell) := \\log \\frac{\\pi_\\theta(a^\\omega | s)}{\\pi_{\\text{ref}}(a^\\omega | s)} - \\log \\frac{\\pi_\\theta(a^\\ell | s)}{\\pi_{\\text{ref}}(a^\\ell | s)}$ is the preference score of an answer $a^\\omega$ relative to another\none $a^\\ell$ (but parameterized in policy parameter $\\theta$). Let $\\mathcal{P}(\\rho; \\mathcal{P}^0)$ be a distributional uncertainty set centered\naround $\\mathcal{P}^0$ with radius $\\rho > 0$. Following the principles of distributionally robust optimization (DRO), we\nformulate the distributionally robust DPO objective as:\n$\\min_\\theta \\max_{\\mathcal{P} \\in \\mathcal{P}(\\rho;\\mathcal{P}^0)} \\mathbb{E}_{z \\sim \\mathcal{P}}[\\ell(z; \\theta)].$\nIntuitively, we aim to find the best policy under the worst-case data distribution.\nWhen we have a Wasserstein uncertainty set $\\mathcal{P}_{W_p}$, i.e., Eq. (6) equipped with the $p$-th order Wasserstein\ndistance, we define the Wasserstein DPO (WDPO) loss as follows\n$\\mathcal{L}^W(\\theta; \\rho) = \\sup_{\\mathcal{P} \\in \\mathcal{P}_{W_p}(\\rho;\\mathcal{P}^0)} \\mathbb{E}_{z \\sim \\mathcal{P}}[\\ell(\\theta; z)],$\nSimilarly, given a Kullback-Leibler uncertainty set $\\mathcal{P}_{KL}(\\rho; \\mathcal{P}^0)$, we define the KLDPO loss function as\nfollows\n$\\mathcal{L}^{KL}(\\theta; \\rho) = \\sup_{\\mathcal{P} \\in \\mathcal{P}_{KL}(\\rho;\\mathcal{P}^0)} \\mathbb{E}_{z \\sim \\mathcal{P}}[\\ell(\\theta; z)].$\nWhen the nominal distribution $\\mathcal{P}^0$ is replaced with its empirical counterpart, i.e., $\\mathcal{P}_n := (1/n) \\sum_{i=1}^n \\delta_{z_i}$,\nwhere $z_1, ..., z_n$ are $n$ i.i.d. samples from $\\mathcal{P}^0$, we use $\\mathcal{L}_n^W(\\theta;\\rho)$ and $\\mathcal{L}_n^{KL}(\\theta; \\rho)$ to denote the empirical\nWDPO and KLDPO losses incurred by the policy parameter $\\theta$, respectively."}, {"title": "Theoretical Analysis", "content": "In this section, we present the sample complexity guarantees for our WDPO and KLDPO algorithms. We\nmake the following assumptions for the rest of the papers.\nAssumption 1 (Log-linear policy class). Let $\\psi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}^d$ be a known $d$-dimensional feature mapping\nwith $\\text{max}_{s,a}|\\|\\psi(s, a)\\||_2 \\leq 1$. Assume a bounded policy parameter set $\\Theta := {\\theta \\in \\mathbb{R}^d: |\\|\\theta\\||_2 \\leq B}$. We\nconsider the following class of log-linear policies:\n$\\Pi = {\\pi_\\theta: \\pi_\\theta(a | s) = \\frac{\\exp (\\theta^T\\psi(s,a))}{\\sum_{a'\\in A} \\exp (\\theta^T\\psi(s,a'))}}$.\nRemark 1. This is a standard assumption in the theoretical analysis of the RL algorithms (Agarwal et al.,\n2021; Modi et al., 2020), RLHF (Zhu et al., 2023), and DPO (Nika et al., 2024; Chowdhury et al., 2024). Our\nanalysis can be extended to the neural policy class where $\\theta^T\\psi(s, a)$ is replaced $f_\\theta(s, a)$, where $f_\\theta$ is a neural\nnetwork with twice differentiability and smoothness assumptions."}, {"title": "Estimation Error for WDPO", "content": "Let $\\theta^* \\in \\text{argmin}_{\\theta \\in \\Theta} \\mathcal{L}^{DPO}(\\theta)$ be the ground-truth optimal policy parameter with respect to the true nominal\ndistribution and let its empirical counterpart be $\\theta_n \\in \\text{argmin}_{\\theta \\in \\Theta} \\mathcal{L}^{DPO}(\\theta)$. Now for the robust policy param-\nmeters, we let $\\theta_n^W \\in \\text{argmin}_{\\theta \\in \\Theta} \\mathcal{L}^W(\\theta; \\rho)$, and let its empirical counterpart be $\\theta_n^{W} \\in \\text{argmin}_{\\theta \\in \\Theta} \\mathcal{L}_n^W(\\theta; \\rho)$.\nWe first establish the strong convexity of $\\mathcal{L}^W$.\nLemma 1. Let $\\ell(z; \\theta)$ be the DPO loss function given in Eq. (7). The Wasserstein DPO loss function,\n$\\mathcal{L}^W(\\theta; \\rho) = \\sup_{\\mathcal{P}: W_p(\\mathcal{P},\\mathcal{P}^0)<\\rho} \\mathbb{E}_{z \\sim \\mathcal{P}}[\\ell(z; \\theta)],$\nis $\\gamma\\lambda$-strongly convex in $\\Theta$ with respect to $\\|\\cdot\\|_2$, where $\\lambda$ is the regularity condition number defined in Assump-\ntion 2, and $\\gamma = \\frac{\\beta^2 e^{4\\beta B}}{(1+e^{4\\beta B})^2}$.\nNow, present our main result on the sample complexity result for the convergence of the robust policy\nparameter.\nTheorem 1 (Estimation error of $\\theta_n^{W}$). Let $\\delta \\in (0, 1)$. With probability at least $1 - \\delta$, we have\n$\\|\\theta_n^{W} - \\theta_n^{W}\\|_2 \\leq \\sqrt{\\frac{8 K^2 \\log(2/\\delta)}{\\gamma \\lambda n}}$,\nwhere $\\gamma = \\frac{\\beta^2 e^{4\\beta B}}{(1+e^{4\\beta B})^2}$ and $K = |\\log \\sigma(-4\\beta B)|$, $\\lambda$ is the regularity condition number defined in Assumption 2."}, {"title": "Estimation Error for KLDPO", "content": "Let $\\theta_n^{KL} \\in \\text{argmin}_{\\theta \\in \\Theta} \\mathcal{L}^{KL} (\\theta; \\rho)$, and let its empirical counterpart be $\\theta_n^{KL} \\in \\text{argmin}_{\\theta \\in \\Theta} \\mathcal{L}_n^{KL} (\\theta; \\rho)$. The\nproofs for the convergence of KLDPO loss function and policy parameter closely mirror those for the\nWasserstein DPO. We defer the detailed proofs of KLDPO to Appendix C and only state the theorems in\nthis section.\nTheorem 2 (Estimation error of $\\theta_n^{KL}$). Let $\\delta \\in (0, 1)$. With probability at least $1 - \\delta$, we have\n$\\|\\theta_n^{KL} - \\theta_n^{KL}\\|_\\Sigma^2 \\leq \\sqrt{\\frac{8 \\Sigma^2 \\exp (L/\\Lambda)\\log(2/\\delta)}{\\gamma \\lambda n}}$,\nwhere $\\gamma = \\frac{\\beta^2 e^{4\\beta B}}{(1+e^{4\\beta B})^2}$, $\\lambda$ is the regularity condition number defined in Assumption 2, $0 < \\lambda \\leq \\lambda_{min}(\\Sigma_\\mathcal{P}^0)$.\n$\\Lambda, \\Sigma$ are some universal constants, and $L$ is an upper bound on the loss function $l$."}, {"title": "Tractable (Approximate) Algorithms", "content": "While our distributionally robust DPO formulations enjoy finite-sample guarantees, it is computationally\nchallenging to solve the min-max objective of Eq. (8) using stochastic gradient descent methods. Though\nmany min-max optimization problems can be solved by alternating gradient descent methods, our problem\nis not directly amenable to such an approach as we do not have direct control over the data distribution\n$\\mathcal{P} \\in \\mathcal{P}(\\rho; \\mathcal{P}^0)$ and they are not parameterized. Moreover, the preference data is generated according to the\nnominal distribution $\\mathcal{P}^0$ and we do not have data samples from any other distributions in the uncertainty\nset $\\mathcal{P}(\\rho; \\mathcal{P}^0)$. \u03a4\u03bf overcome this challenge, we introduce principled tractable algorithms to solve WDPO and\nKLDPO."}, {"title": "Tractable WDPO", "content": "The connection between Wasserstein distributionally robust optimization (DRO) and regularization has been\nestablished in various settings by many (Mohajerin Esfahani and Kuhn, 2018; Shafieezadeh-Abadeh et al.,\n2019; Chen and Paschalidis, 2018). We leverage the recent progress in Wasserstein theory on connecting\nWasserstein distributionally robust optimization to regularization. For $p$-Wasserstein DRO, $p \\in (1, \\infty]$, Gao\net al. (2022) shows that for a broad class of loss functions, possibly non-convex and non-smooth, with high\nprobability, the Wasserstein DRO is asymptotically equivalent to variation regularization. In particular, an\nimmediate consequence of Gao et al. (2022, Theorem 1) is that, when $p = 2,$\n$\\min_\\theta \\in \\Theta \\sup_{\\mathcal{P}: W_p(\\mathcal{P},\\mathcal{P})<\\rho_n} \\mathbb{E}_{z \\sim \\mathcal{P}}[\\ell(z; \\theta)] = \\min_{\\theta \\in \\Theta} {\\mathbb{E}_{z \\sim \\mathcal{P}^0} [\\ell(z;\\theta)] + \\rho_n \\sqrt{(1/n) \\sum_{i=1}^n ||\\nabla_z(\\ell(z_i;\\theta)||_2} + O_p(1/n),$\nwhere $\\rho_n = O(1/\\sqrt{n})$. That is, one can solve the Wasserstein DRO objective by adding a gradient\nregularization to the empirical risk minimization (ERM) loss, $\\mathbb{E}_{z \\sim \\mathcal{P}^0} [\\ell(z; \\theta)]$. Based on this, we propose a\ntractable WDPO algorithm in Algorithm 1."}, {"title": "Tractable KLDPO: Approximate Dual Solution", "content": "The following proposition shows that we can approximate the worst-case probability distribution in a KL\nuncertainty set w.r.t. a given loss function. Similar results can also be found in distributionally robust\nreinforcement learning literature (e.g., Gadot et al. (2024)).\nProposition 2 (Worst-case distribution (informal)). Let $\\mathcal{P} \\in \\mathbb{R}^n$ be the worst-case distribution w.r.t. a loss func-\ntion $\\ell$ and KL uncertainty around the empirical distribution $\\mathcal{P}_n^0$, defined as $\\mathcal{P} = \\text{supp}_{\\mathcal{P}: D_{KL} (\\mathcal{P} || \\mathcal{P}_n^0)<\\rho} \\mathbb{E}_{z \\sim \\mathcal{P}}[\\ell(z; \\theta)]$.\nThen,\n$\\mathcal{P}(i) \\propto \\mathcal{P}^0(i) \\cdot \\text{Exp} \\big((\\omega - l(z_i; \\theta))/\\tau\\big),$\nwhere $\\omega \\leq \\sum_{i=1}^n \\mathcal{P}_n^0(i)l(z_i; \\theta)$, and $\\tau > 0$ is some constant."}, {"title": "Experiments", "content": "We use the Emotion dataset (Saravia et al., 2018) which consists of English Twitter texts. Each text is\ncategorized into six emotions: sadness, joy, love, anger, fear, surprise. To ensure data quality, we excluded"}, {"title": "WDPO Training", "content": "In WDPO training, one of the main challenges is calculating the gradient penalty of\nthe DPO loss with respect to the input. However, since the input is tokenized as integers, gradient cannot\nbe directly calculated. To address this, gradient is calculated with respect to the first hidden state, which is\ntypically the output of the embedding layer, where gradients are tracked. The model was trained for 40\nepochs with an effective batch size of 64. We used RMSProp optimizer, with a learning rate of $5.0 \\times 10^{-7}$\nfollowing 12 warmup steps. A maximum gradient norm of 10 was applied to ensure stable training. The"}, {"title": "KLDPO Training", "content": "The model was trained for 40 epochs with an effective batch size of 64. We used\nRMSProp optimizer, with a learning rate of $5.0 \\times 10^{-7}$ following 12 warmup steps. A maximum gradient\nnorm of 10 was applied to ensure stable training. The DPO beta parameter was set to 0.1 for all training\nruns. Experiments were conducted on a single 40 GB A100 GPU and gradient was accumulated over two\nsteps to keep training consistent across all algorithms."}, {"title": "More Simulation Results", "content": "In this section, we include more simulation results where models are trained\non various nominal preference models. In Fig. 8, models are trained on the preference labels generated\naccording to $r_1^\\alpha(0.1), r_1^\\alpha(0.3), r_1^\\alpha(0.5), r_1^\\alpha(0.7), r_1^\\alpha(0.9)$, respectively. Starting from the third plot, we notice\nthat the robustness of our KLDPO and WDPO (along with the baseline robust policy Dr. DPO) reduces.\nThis is because when the training preference model is closer to the testing preference model, the preference\nshift diminishes. In such cases, non-robust algorithm such as DPO will not be affected much.\nIn Fig. 9, we provide additional simulation results for function class $r_2^\\alpha$. The models are trained on the\npreference labels generated according to $r_2^\\alpha(0.1), r_2^\\alpha(0.3), r_2^\\alpha(0.5), r_2^\\alpha(0.7), r_2^\\alpha(0.9)$, respectively. Similar\nto the $r_1^\\alpha$ function class, we also observe that when the training preference model is closer to the testing\npreference model, the performance of non-robust DPO and the robust models, WDPO and KLDPO, is more\nor less homogeneous.\nWe summarize the key implementation and hardware details of text generation tasks in Table 1."}, {"title": "Data Generation", "content": "A preference dataset was created, consisting of a chosen and a rejected completion\nfor each prompt in the dataset. The first four tokens from each text in the emotion dataset were used as\nprompts. Using the SFT model, two completions were generated for each prompt. These completions were\ngenerated with a top-k value of 0, top-p of 1, and up to 64 new tokens. The completions were then\nevaluated using the reward model, and the chosen and rejected completions were determined based on a\ncombined metric derived from the predicted rewards. In the first plot of Fig. 6, we show the correlation\namong $r_1, r_2$, and $r_1^\\alpha(0.1)$. We can see that, as designed, the training preference model is mostly influenced\nby $r_2$ (fear). Recall that $r_1^\\alpha(0.1)$ is by design 1/10 of $r_1$ (anger) and 9/10 of $r_2$ (fear). The correlation\nheatmap verifies that we indeed have an accurate mixture training preference. In the last plot of Fig. 7,\nwe show the correlation among $r_1, r_2, r_3, r_4, r_5, r^\\alpha$. Recall that $r^\\alpha$ is constructed under equally-weighted\ninfluence of all five standalone reward models."}, {"title": "Conclusions", "content": "In this paper, we proposed the formalism of distri-\nbutionally robust DPO, developed two novel algo-\nrithms using this framework, and established their\ntheoretical guarantees. We also developed efficient\napproximation techniques that enable scalable im-\nplementation of these algorithms as part of the exist-\ning LLM alignment pipeline. We showed extensive\nempirical evaluations that validate the effectiveness\nof our proposed algorithms in addressing preference distribution shifts in LLM alignment. In future works,\nwe plan to extend our distributionally robust DPO algorithms to address the challenges of reward hacking.\nWe also plan to develop distributionally robust algorithms for other RLHF approaches."}]}