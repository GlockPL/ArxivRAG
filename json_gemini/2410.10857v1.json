{"title": "Mirror-Consistency: Harnessing Inconsistency in Majority Voting", "authors": ["Siyuan Huang", "Zhiyuan Ma", "Jintao Du", "Changhua Meng", "Weiqiang Wang", "Zhouhan Lin"], "abstract": "Self-Consistency, a widely-used decoding strategy, significantly boosts the reasoning capabilities of Large Language Models (LLMs). However, it depends on the plurality voting rule, which focuses on the most frequent answer while overlooking all other minority responses. These inconsistent minority views often illuminate areas of uncertainty within the model's generation process. To address this limitation, we present Mirror-Consistency, an enhancement of the standard Self-Consistency approach. Our method incorporates a 'reflective mirror' into the self-ensemble decoding process and enables LLMs to critically examine inconsistencies among multiple generations. Additionally, just as humans use the mirror to better understand themselves, we propose using Mirror-Consistency to enhance the sample-based confidence calibration methods, which helps to mitigate issues of overconfidence. Our experimental results demonstrate that Mirror-Consistency yields superior performance in both reasoning accuracy and confidence calibration compared to Self-Consistency.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have substantially influenced diverse sectors with their advanced applications (Chowdhery et al., 2022; Schick et al., 2023; Wu et al., 2023; Shen et al., 2023; Zhang et al., 2023). To further bolster LLMs' reasoning ability, Self-Consistency (Wang et al., 2023a) initially generates a wide range of reasoning pathways, then performs a marginalization to determine the most consistent response. Although generally effective, this approach relies primarily on the plurality voting rule, which focuses only on the most frequent answer, thereby neglecting other minority responses. Consequently, the crucial inconsistencies in the resampled answers, which could reveal uncertainties and potential errors of LLMs, often do not receive the attention they merit.\nTo address this limitation, we introduce Mirror-Consistency, which enables the LLM to analyze and learn from discrepancies in its resampled responses. The complete process is illustrated in Figure 1. We sequentially resample new responses. Each new response is then compared to the previous majority voting result. If the new response deviates from the majority's decision (i.e., it represents a minority opinion), we position the LLM to act as an arbitrator. In this role, the LLM reflects on the differences between the new response and the previous majority opinion and offers a feedback, which is then utilized to guide the generation of subsequent responses. This process effectively equips the standard Self-Consistency method with a 'mirror', enabling the LLM to learn from inconsistencies observed during the sampling process.\nWe then delve into another application of Mirror-Consistency: the confidence calibration of LLMs (Desai and Durrett, 2020; Geng et al., 2023). Recent studies leverage the consistency of multiple generations to assess the LLM's confidence in its responses (Xiong et al., 2023; Wang et al., 2023a; Manakul et al., 2023; Xiong et al., 2023; Portillo Wightman et al., 2023). Following this idea, we find that Mirror-Consistency, which involves repeatedly comparing and reflecting on the discrepancies between the majority and the minority opinions, provides a more robust quantification of uncertainty, particularly in cases of overconfidence. Our motivation for using mirror consistency in confidence calibration stems from the idea that answers that consistently emerge as the majority after several rounds of comparisons with other candidate answers are likely more reliable.\nWe conduct experiments on four reasoning datasets with four different LLMs to compare the performance of Mirror-Consistency with Self-Consistency. The results confirm the efficacy of reflecting on the inconsistencies during the sampling process, showing improvements in both reasoning accuracy and confidence calibration."}, {"title": "Mirror-Consistency", "content": "This section details the implementation of Mirror-Consistency. Self-Consistency depends on the plurality voting rule, which tends to overlook minority responses that may be inconsistent yet informative. To address this gap, we introduce a reflective form of consistency, termed Mirror-Consistency.\nMirror-Consistency and Self-Consistency share procedural similarities, as both methods generate multiple responses and use majority voting to determine the final output. The key distinction is that Mirror-Consistency examines the inconsistencies within the sampling process, rather than merely repeat sampling independent answers. The Mirror-Consistency process begins with an initial sampling phase to produce the first response, denoted as $r_0$. This is followed by a series of iterations that alternate between Reflection on Inconsistency and Conditional Resampling, as shown in Algorithm 1. We next introduce these two steps in detail. The corresponding prompts are provided in Appendix D.\nReflection on Inconsistency During the kth Reflection on Inconsistency, the model compares the newly resampled response $r_k$ with the majority voting result $m_{k-1}$ from previous rounds. If the responses align, the process moves to the next resampling phase. Otherwise, the model analyzes the causes of the discrepancies and formulates a suggestion for additional checks, as the inconsistencies of multiple reasoning pathways often reveal uncertainty and potential mistakes within the generation. This suggestion is incorporated into the existing checklist $C_{k-1}$, updating it to $C_k$ for use in the subsequent Conditional Resampling.\nConditional Resampling In this phase, the feedback obtained from the previous round is utilized to guide the generation of subsequent responses. During the kth round of Conditional Resampling, the checklist $C_k$, derived from earlier rounds, is integrated into the prompt, facilitating the generation of a new response $r_{k+1}$. This method ensures that the LLM concentrates on areas of uncertainty exposed by inconsistencies noted during prior samplings.\nComparison with Self-Reflection Standard intrinsic self-correction methods directly generate feedback for the initial responses, which frequently exhibit overconfidence and difficulty in identifying errors (Huang et al., 2023; Tyen et al., 2023). In contrast, Mirror-Consistency identifies potential errors by critically analyzing the inconsistencies across multiple reasoning pathways. This contrastive strategy is generally more practical than directly generating feedback (Zhang et al., 2024)."}, {"title": "Calibration with Mirror-Consistency", "content": "In this section, we introduce the application of Mirror-Consistency to the calibration of LLMs. Recent studies have shown that consistency across multiple model generations serves as a reliable indicator of confidence (Portillo Wightman et al., 2023; Manakul et al., 2023). While effective, these methods rely on the Self-Consistency logic (i.e., 'independent' repeated sampling) and overlook the analysis of inconsistencies among responses. In contrast, Mirror-Consistency employs a reflective form of consistency: responses maintaining consistency after numerous comparisons and reflections are deemed more reliable; while responses that vary upon comparison and reflection highlight the LLM's inherent uncertainties. Hence, we propose using the distribution of responses generated by Mirror-Consistency as a measure of confidence.\nNext, we introduce two fundamental types of sample-based confidence metrics. A comprehensive analysis of five existing sample-based confidence metrics is provided in Appendix A. The most commonly employed strategy is the agreement-based metric (Lyu et al., 2024). For each input $x$, we generate $n$ candidate outputs, $\u0302r_1,..., \u0302r_n$. We then apply majority voting across the answers to determine the most-voted answer $r^* = \\text{argmax}_r \\sum_{i=1}^{n} 1(\u0302r_i = r)$. The agreement-based confidence score is then defined as the percentage of answers that agree with the most-voted answer:\n$Agree(r^*) = \\frac{1}{n} \\sum_{i=1}^n 1(\u0302r_i = r^*)$\nAgree solely relies on the most voted answer. First-Second-Distance (FSD) (Lyu et al., 2024) provides another option by considering the top two most-voted answers, denoted as $r_1$ and $r_2$. $FSD(r)$ is the difference in their agreement rates:\n$FSD(r) = Agree(r_1) - Agree(r_2)$\nFurther experiments, as detailed in subsection 4.2 and Appendix A, demonstrate that this reflective form of consistency provides more reliable calibration than standard sample-based methods."}, {"title": "Experiments", "content": "In this section, we assess the efficacy of Mirror-Consistency from two distinct angles: First, we evaluate its reasoning accuracy in comparison to Self-Consistency across four reasoning datasets. Second, we demonstrate how the reflective form of consistency embodied by Mirror-Consistency improves the sample-based calibration methods."}, {"title": "Reasoning Accuracy", "content": "Benchmarks We evaluate our models using four distinct datasets: GSM8K (Cobbe et al., 2021) and SVAMP (Patel et al., 2021), which focus on arithmetic reasoning; StrategyQA (Geva et al., 2021) and Date Understanding (BIG-Bench collaboration, 2021), which belong to Multi-hop QA. We follow the same split setting as Lyu et al. (2023).\nLLMS We utilize four different large language models: GPT-3.5-turbo-0613, Qwen-turbo, and the newly released Llama-3 family (8B/70B). More details can be found in subsection C.2."}, {"title": "Confidence Calibration", "content": "Experiment Setup We compare the performance of Mirror-Consistency and Self-Consistency, each using a comparable number of calls (19 vs. 20). We consider two LMs, GPT-3.5-turbo and Qwen-turbo, on two tasks, GSM8K and SVAMP, with the same settings in subsection 4.1. We use Expected Calibration Error (ECE) (Guo et al., 2017) as the evaluation metric. We first consider two confidence metrics, Agree and FSD, as introduced in section 3.\nMain Result Table 2 presents the ECE comparisons across two different confidence metrics."}, {"title": "Conclusion", "content": "Mirror-Consistency effectively remedies a crucial limitation in the standard Self-Consistency method: the disregard for minority viewpoints during sampling. Our approach examines the discrepancies across various responses to identify uncertainties within the generative process. The experimental results demonstrates that Mirror-Consistency not only achieves higher reasoning accuracy compared to Self-Consistency but also enhances model calibration, especially when standard sample-based calibration methods face issues of overconfidence."}, {"title": "Limitations", "content": "We acknowledge several limitations that invite further investigation: Firstly, while Self-Consistency can generate multiple answers in parallel, Mirror-Consistency, due to the fact that each generation of a new answer needs to be compared with the previous majority opinion, must generate answers sequentially. Secondly, our experimental setup solely utilizes Chain-of-Thought (Wei et al., 2022) as the prompt strategy for sampling. Exploring a broader array of prompt strategies could provide deeper insights into their interactions with the Mirror-Consistency approach. Thirdly, the central motivation behind Mirror-Consistency is to effectively harness the inconsistencies that emerge during repeated sampling processes. This requires the responses to exhibit enough diversity. To enhance this diversity, we could enable the large language model to automatically generate prompts that vary across different dimensions (Zhang et al., 2024). Fourthly, we fix the temperature parameter throughout our study. Future work could experiment with varying the temperature to assess the robustness of Mirror-Consistency under different settings. Lastly, since Mirror-Consistency relies on prompting to encourage reflective thinking, it necessitates strong instruction-following capabilities within the model. While the four models considered in our experiments demonstrate adequate ability in this regard, this strategy may not be as effective with smaller-scale or lower-performing models."}, {"title": "Ethical Considerations", "content": "We recognize several ethical Considerations. Our approach, Mirror-Consistency, improves LLM robustness by addressing inconsistencies but cannot fully negate the risks of errors or biases from the underlying data and model structures. Additionally, the method's iterative process, involving repeated resampling and reflection, presents concerns about computational costs and environmental impacts. Finally, it's crucial to clearly convey the limitations of Mirror-Consistency, informing users about possible failure scenarios and the potential repercussions. This transparency is vital for setting appropriate expectations for real-world applications."}, {"title": "Additional Calibration Results", "content": "In this section, we explore the impact of various metrics on calibration results based on Mirror-Consistency and Self-Consistency approaches. Recently, numerous studies have adopted sample-based methods to assess the confidence of Large Language Models (LLMs) (Wang et al., 2023a; Manakul et al., 2023; Xiong et al., 2023; Portillo Wightman et al., 2023). These studies have proposed different metrics to derive corresponding confidence scores from the varying resampled responses. Again, for each input x, we generate n candidate outputs, 1, ..., \u00cen. We then apply majority voting across the answers to determine the most-voted answer r = argmax, \u03a3=11(i = r). We consider the following five metrics:\n\u2022 Agreement-based (Lyu et al., 2024):\nThe agreement-based confidence score, which is the most common metric and has been introduced in section 3, is defined as the percentage of answers that agree with the most-voted answer:\n$Agree(r) = \\frac{1}{n} \\sum_{i=1}^n 1(\u0302r_i = r)$\n\u2022 Entropy-based (Lyu et al., 2024):\nWe first derive a set of unique answers from the model's output, denoted as r, by eliminating duplicates. The entropy-based consistency, Ent(r), is then defined as follows:\n$Ent(r) = 1 - \\frac{1}{\\log \\hat{r}} \\sum_{i=1}^{\\hat{r}} p_i \\log p_i$\nHere, \u00ee represents the number of unique answers, and pi is the normalized frequency of each unique answer i in the dataset. This formulation inversely relates the entropy measure to the consistency of the answer distribution, where a lower entropy implies a more uniform and certain response pattern.\n\u2022 FSD-based (Lyu et al., 2024):\nFSD stands for First-Second-Distance. To calculate the FSD-based consistency, we first identify the top two most-voted answers, denoted as \u2081 and 2. We then calculate the agreement rates for these two answers, denoted as Agree(r1) and Agree(r2), respectively. The FSD-based consistency, FSD(r), is computed as the difference in their agreement rates:\n$FSD(r) = Agree(r_1) - Agree(r_2)$\n\u2022 Answer-Number-based (Wang et al., 2024):\nAgain we derive a set of unique answers from the model's output, denoted as r. The answer-number-based, Ans-Num(r), is then simply defined as:\n$Ans-Num(r) = 1 - \\frac{\\hat{r}}{n}$\n\u2022 Pairwise-Comparison-based (Wang et al., 2024):\nWe count the number of times each different answer is repeated, denoted as n\u2081 . . . n|f|. Specially, we denote the index of the majority voting result as ip. Then the pairwise-comparison-based, Pairwise(r), metric is defined as:\n$Pairwise(r) = \\prod_{j\\neq ir} \\frac{N_i}{N_i + n_j}$\nWe evaluate the performance of Mirror-Consistency and Self-Consistency on the task of confidence calibration using the five distinct metrics described above. Our experiments span two datasets, GSM8K and SVAMP, and consider two models: GPT-3.5-turbo and Qwen-turbo.\nMain result As shown in Table 3, we find that Mirror-Consistency outperforms Self-Consistency most of the time. Additionally, we provide the corresponding calibration curves in Figure 3. We set the number of bins as 10. We do not plot bins"}, {"title": "Related Works", "content": "Recent advancements in large language models have shed light on their advanced cognitive intelligence, notably their ability for self-correction. This attribute enables LLMs to amend their initial responses by integrating both external and self-generated feedback, thus improving previous outputs (Xi et al., 2023; Pan et al., 2023; Nathani et al., 2023). The concept of self-correction encompasses a variety of techniques that have been extensively surveyed and categorized based on the source of feedback and the timing of correction (Pan et al., 2023). These techniques range from using explicit error messages in tasks, such as code execution, to human feedback and model-generated prompts (Miao et al., 2024; Chen et al., 2023a; Huang et al., 2023; Kim et al., 2023). While iterative prompting techniques have shown promise, recent research has raised concerns about LLMs' capacity for independent reflection, revealing limitations in modifying responses without external feedback (Huang et al., 2023; Stechly et al., 2023; Liang et al., 2023; Tyen et al., 2023)."}, {"title": "Confidence Calibration", "content": "In the realm of machine learning, uncertainty quantification methods are crucial for assessing the risk associated with model predictions (R\u00fcping, 2006; Desai and Durrett, 2020). Traditional calibration approaches, including probabilistic, ensemble-based, and density-based methods, although effective, require extensive computational resources and access to model internals, making them less viable for closed-source LLMs (Guo et al., 2017; Lakshminarayanan et al., 2017; Gal and Ghahramani, 2016; Lee et al., 2018; Yoo et al., 2022). Furthermore, some post-hoc strategies have emerged, focusing on eliciting model-estimated probabilities of correctness or directly verbalized confidence levels (Kadavath et al., 2022; Lin et al., 2022; Mielke et al., 2022). And recently, inspired by the sample-based methods, calibration through sample consistency, which relies solely on model input and output, has been explored. (Wang et al., 2023a; Manakul et al., 2023; Xiong et al., 2023; Portillo Wightman et al., 2023)."}, {"title": "Refined Problem-Solving Strategies", "content": "Existing prompting strategies engage in two forms of reasoning: exploring various perspectives (breadth) and refining ideas to reduce errors (depth). Self-consistency and related methods (Wang et al., 2023b; Huang et al., 2022; Yoran et al., 2023; Jain et al., 2023) promoting breadth through diverse reasoning sampling. In contrast, strategies like self-reflection and abstraction (Shinn et al., 2023; Madaan et al., 2023; Paul et al., 2023; Zheng et al., 2023) focus on the depth of reasoning by iteratively refining prompts."}, {"title": "More Implementation Details", "content": "Math Word Problems (MWP). This category includes challenges where the objective is to compute numeric solutions to problems framed in natural language. Our analysis incorporates several datasets, each selected to evaluate distinct aspects of mathematical reasoning. For GSM8K (Cobbe et al., 2021), participants engage with a diverse set of elementary math questions, testing their ability to apply basic arithmetic operations and logical reasoning. Meanwhile, SVAMP (Patel et al., 2021) is specifically designed to probe the robustness of models against changes in question phrasing and structural complexity, offering a stringent test of comprehension and adaptability.\nMulti-hop QA. Tasks in this section demand answers to intricate questions via a series of logical deductions, necessitating a nuanced understanding of the content. Answers may be in the form of Boolean values or specific textual responses. StrategyQA (Geva et al., 2021) dataset presents science questions that require an implicit strategy for multi-step reasoning, challenging the model's ability to form and execute complex inferential chains. Date Understanding (BIG-Bench collaboration, 2021) tests temporal reasoning by asking participants to"}, {"title": "Model Details", "content": "Closed-Source Language Models We utilize two advanced closed-source language models, GPT-3.5-turbo and Qwen-turbo. For GPT-3.5-turbo, the API is accessible at platform.openai.com. Qwen-turbo, another potent large language model, is available via the DashScope provided by Alibaba Cloud (alibabacloud.com). Detailed guidance about DashScope and its integration can be accessed here. Further information about utilizing Qwen-turbo through Alibaba Cloud, including API instructions can be found here."}, {"title": "Other Details", "content": "Evaluation Metrics We use Expected Calibration Error (ECE) (Guo et al., 2017) as the calibration metric. Predictions are first binned into M = 10 intervals based on model confidence. For each bin Bm, we calculate the average accuracy acc(Bm) and average confidence conf(Bm). ECE is defined as their weighted absolute differences:\n$ECE = \\sum_{m=1}^{M} \\frac{|B_m|}{N} |acc(B_m) - conf(B_m)|$"}, {"title": "Prompt Template", "content": "In this section, we outline the three crucial prompts required for the future realization of Mirror-Consistency: 1) Prompt for Simple Resampling, denoted as Psample, 2) Prompt for Reflection on Inconsistency, denoted as Pcontrast, 3) Prompt for Conditional Resampling, denoted as psample w/ fb.\nIt is important to note that our implementation adopts the Chain-of-Thought (COT) approach to resample multiple answers. However, it is feasible to explore a variety of other prompt methods, such as Least-to-Most (LtM) (Zhou et al., 2023), Program of Thoughts (PoT) (Chen et al., 2023b) and Faithful CoT (FCOT) (Lyu et al., 2023). Furthermore, in our experiments to validate the generality of Mirror-Consistency, we apply the same prompts across diverse datasets, including arithmetic, symbolic reasoning, and commonsense reasoning. Nonetheless, future work may tailor prompts specifically to the characteristics of different datasets, thus enhancing reflection and contrast, to potentially improve Mirror-Consistency."}, {"title": "Prompt for Simple Resampling Psample", "content": "The following template is employed in a simple resampling process. During its use, [QUESTION] must be substituted with the specific question at hand. This template is designed for initial sampling in Mirror-Consistency and for instances when the Checklist is empty, as well as serving as a baseline in our experiments with Self-Consistency."}, {"title": "Prompt for Contrast Pcontrast", "content": "The template for reflecting on inconsistencies is as follows. In practice, [QUESTION] should be replaced with the actual question, [PRE-MAJORITY-VOTE] should be replaced with the majority vote answer from all previous rounds (in cases of multiple identical responses, one is randomly selected as the majority vote answer), [CUR-RESPONSE] should be substituted with the most recently generated response of the current round, and finally [PRE-CHECKLIST] should be replaced by the checklist from the last round. This template facilitates the reflection of inconsistencies within the Mirror-Consistency approach."}, {"title": "Prompt for Conditional Resampling Psample w/ fb", "content": "In the conditional resampling process, the following template is utilized. It requires replacing [QUESTION] with the specific question and substituting [CHECKLIST] with the latest checklist derived from the previous contrast phase. This template is used in Mirror-Consistency for resampling new responses based on reflective feedback addressing inconsistencies."}]}