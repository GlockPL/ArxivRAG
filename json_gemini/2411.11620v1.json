{"title": "ST-Tree with Interpretability for Multivariate Time Series Classification", "authors": ["Mingsen Du", "Yanxuan Wei", "Yingxia Tang", "Xiangwei Zheng", "Shoushui Wei", "Cun Ji"], "abstract": "Multivariate time series classification is of great importance in practical applications and is a challenging task. However, deep neural network models such as Transformers exhibit high accuracy in multivariate time series classification but lack interpretability and fail to provide insights into the decision-making process. On the other hand, traditional approaches based on decision tree classifiers offer clear decision processes but relatively lower accuracy. Swin Transformer (ST) addresses these issues by leveraging self-attention mechanisms to capture both fine-grained local patterns and global patterns. It can also model multi-scale feature representation learning, thereby providing a more comprehensive representation of time series features. To tackle the aforementioned challenges, we propose ST-Tree with interpretability for multivariate time series classification. Specifically, the ST-Tree model combines ST as the backbone network with an additional neural tree model. This integration allows us to fully leverage the advantages of ST in learning time series context while providing interpretable decision processes through the neural tree. This enables researchers to gain clear insights into the model's decision-making process and extract meaningful interpretations. Through experimental evaluations on 10 UEA datasets, we demonstrate that the ST-Tree model improves accuracy in multivariate time series classification tasks and provides interpretability through visualizing the decision-making process across different datasets.", "sections": [{"title": "1. Introduction", "content": "Multivariate time series classification is an important and challenging task for time series, which are sequences of measurements collected from sensors. A time series is a set of data in chronological order and is widely used in a variety of domains and in a variety of real-world applications, including atrial fibrillation detection (Li et al. (2023)), error message detection (Yu et al. (2022)), intelligent device detection Du et al. (2023a), sleep record classification Chambon et al. (2017), drunk driving detection (Li et al. (2015)), human activity recognition (Ma et al. (2019)) and so on.\nTime series classification is one of the important tasks in time series analysis. Time series classification methods predict labels for unknown time series instances from the knowledge gained from existing labeled instances. In recent years, more and more researchers focus on the time series classification problem, which has important research value in the field of data mining (Du et al. (2024); Huang & Deng (2023); Chen & Shi (2021)). High accuracy is a concern for all researchers. However interpretability is still important in many tasks. While much work has been done on interpretability in computer vision, tabular data classification and natural language processing, research on time series has not received as much attention (Rojat et al. (2021)).\nMost methods are not directly applicable to time series due to the different structure and nature of time series data (Ismail et al. (2020)). In the early days of time series research, shapelets based methods were extensively studied (Ji et al. (2019); Tao et al. (2022)). Shapelets provide interpretability of time series data and can be used to characterize time series by identifying and extracting representative shape patterns (Ruiz et al. (2020)). These shape patterns can be used to interpret the predictions of the model and thus enhance the understanding of the time series. However long time series data can cause the shapelet selection and extraction process to become more difficult, leading to increased computational complexity (Ji et al. (2019)). There are also dictionary-based TSC methods (Middlehurst et al. (2019)), classification decisions can be explained by the distribution of symbolic words. However, this explanation is not intuitive enough because the distribution of symbolic words is difficult to judge directly. Some researchers can achieve high accuracy and interpretability through the combination of traditional shapelet and depth methods (Ji et al. (2022a,b)). However, these methods are not intuitive for the classification decision process.\nIn recent years, a large number of deep model-based methods have been proposed, and commonly used models such as onvolutional Neural Network (CNN), Recurrent Neural Network (RNN), Fully Convolutional Network (FCN), Long Short-Term Memory (LSTM) and Transformer and so on (Wang et al. (2016); Turb'e et al. (2022); Fu et al. (2024); Wan et al. (2024)). Among them, Transformer is a deep neural network model based on the self-attention mechanism with strong time series modeling capability. Through the self-attention mechanism, Transformer can capture global dependencies and local patterns in time series data and extract rich feature representations. Deep models are widely used for their end-to-end and classification high accuracy. However, in numerous scenarios are in high-risk environments such as disease prediction or autonomous driving, the black-box characterization of these models is a significant drawback. Numerous approaches (Du et al. (2024)) post-hoc interpretation such as gradcam, however, cannot reflect the decision-making process. Thus it becomes crucial to utilize some method to reflect the interpretability of the decision process of the deep model.\nIn multivariate time series classification, both deep neural network models and decision tree models have their advantages. However, two issues need to be addressed for these methods: (1) High accuracy. Deep neural network models learn complex feature representations by (Ismail et al. (2020)), resulting in high accuracy rates. Various methods based on deep models have emerged. The vast majority of deep models are black-box, post hoc explanatory models. (2) Interpretability. The decision tree model is interpretable and can clearly show the decision-making process, making the results easier to understand and interpret (Theissler et al. (2022); Kim et al. (2022)). Both deep neural network models and decision tree models have their own advantages and limitations. In order to balance accuracy and interpretability, researchers have proposed methods that combine both (Pagliarini et al. (2022)).\nIn response to the above challenges, we proposed ST-Tree with interpretability for multivariate time Series classification to deal with the problem of model interpretability while balancing high accuracy and an interpretable decision process. The method consists of two parts: the time patch module and the neural tree module. Specifically, the time patch module uses ST as a feature extractor and time patch generator. To improve the interpretability of the model and visualize the decision-making process, the generated time patches are fed into a tree model, the neural tree module, which hierarchically paths the multivariate time series through a binary tree. Finally, the leaf prediction module perform label prediction on the multivariate time series by calculating branch routing scores. The transparent decision process of ST-Tree is shown in Fig. 1, each node corresponds to a time series segment closest to the prototype. The main contributions of this paper are as follows:\n1.  We propose the ST-Tree model, which combined ST with an additional neural tree model, which can take full advantage of ST to learn the time series context, and can use neural tree to provide an interpretable decision process.\n2.  The proposed neural tree model decision-making itself is transparent, and each node judgment criterion can be clearly understood through the prototypical solution of the nodes.\n3.  The ST-Tree model is demonstrated to have improved accuracy on multivariate time series classification tasks through experimental evaluation on 10 UEA datasets.\n4.  We visualize the tree model on different types of datasets and illustrate the good interpretability of the model through the decision-making process.\nThe rest of this paper is organized as follows: Section 2 provides an overview of the related work in the field. In Section 3, we present our method in detail. Section 4 describes the sensitivity experiments conducted to validate our approach, comparing it with other state-of-the-art methods. Finally, Section 5 concludes the paper."}, {"title": "2. Related work", "content": "2.1. Transformer based methods\nZuo et al. (2023) introduced a variable position coding layer and a self-attention mechanism based on variable-position to utilize variable and positional information of shapes. Cheng et al. (2023) proposed FormerTime, which uses a hierarchical network with multiscale feature maps, an efficient temporally-approximate attention layer, and a contextual positional encoding strategy. Du et al. (2023b) proposed MF-Net, which acquires local features through an attention block, global features through a sparse self-attention block, and integrates them using a graph neural network (GNN) to capture spatial features. Yang et al. (2023) presented Dyformer, which decomposes time series into subsequences with different frequency components, employs adaptive learning strategies based on dynamic architecture, and introduces a feature map attention mechanism to capture multi-scale local dependencies. H\u00f6llig et al. (2022) introduced TSEvo, which combines different time series transformations to generate realistic counterfactuals using an integrated time series Transformer. Du et al. (2024) proposed MTSC_FF, which utilizes a continuous wavelet transform-based attention layer to extract frequency domain features. It also employs a sparse self-attention layer to extract long-range features and captures spatial correlation using Kendall coefficients.\n2.2. Methods with interpretability\n2.2.1. Traditional methods with interpretability\nChen & Wan (2023) propose an interpretable approach for selecting time shapelets that incorporates position and distance metrics to assess the discriminative ability of candidate shapes, enhancing interpretability through the shape transformation process. Manzella et al. (2021) proposed algorithms for extracting Interval Temporal Logic Decision Trees and extend the approach to Interval Temporal Random Forests, maintaining logical interpretability while modeling patterns at the propositional level. Nguyen et al. (2019) introduced an algorithm that combines symbolic representations at multiple resolutions and domains, leveraging extended symbol sequence classifiers to filter the best features using a greedy selection strategy. Hou et al. (2016) employed a generalized feature vector approach and a fused lasso regularizer to learn shape locations and obtain sparse solutions, outperforming previous shape-based techniques in terms of speed. Delaney et al. (2020) proposed Native-Guide for generating proximal and reasonable counterfactuals for instance-based time series classification tasks by leveraging local in-sample counterfactuals. Bahri et al. (2022) introduced rule transform (RT) for generating discriminative temporal rules. RT creates a new feature space representing the support for temporal rules using Allen interval algebra and shapelets as units.\n2.2.2. Deep methods with interpretability\nYounis et al. (2023) introduced FLAMES2Graph, which visualizes highly activated subsequences and captures temporal dependencies using an evolutionary graph to explain deep learning decisions. Lee et al. (2023) proposed Z-Time, utilizing temporal abstraction and relationships to create interpretable features across multiple time series dimensions. Tang et al. (2019) proposed DPSN, which trains a neural network-based model and interprets it using representative time-series samples and shapes. Dual-prototype shapes are generated, representing the overall shape and discriminative partial-length shapes of different classes. Pagliarini et al. (2022) presented a hybrid approach combining neural networks and temporal decision trees to capture temporal patterns and make decisions based on temporal features. Ma et al. Ma et al. (2020) introduced ADSN, which employs an adversarial training strategy to generate shapelets that closely resemble the actual subsequence. Shapelet transformation is then used to extract discriminative features. Wan et al. (2023) proposed an early classification memory shapelet learning framework that utilizes an in-memory distance matrix and optimizes for accuracy and earlyness to extract interpretable shapes. Zhu & Hill (2021) presented a networked time series shapelet learning approach that incorporates spatio-temporal correlations using a network impedance-based adjacency matrix. Ghods & Cook (2022) proposed PIP, an interpretable method that co-learns classification models and visual class prototypes, providing a higher combination of accuracy and interpretability. Fang et al. (2018) discovered shape candidates, measure their quality using coverage, and adjust them using a logistic regression classifier."}, {"title": "3. Method", "content": "ST (Liu et al. (2021)) brings certain advantages for processing multivariate time series. It employs a patch-based processing strategy, dividing the time series data into parallelizable sub-sequences known as time patches, similar to patches in images. Additionally, by incorporating self-attention mechanism, it becomes possible to model the contextual information between different time steps in the series, enabling the capturing of temporal relationships and dependencies.\nTo leverage these advantages, we propose the ST-Tree, illustrated in Fig. 2. It utilizes a time patch module (shown in Fig. 2 (a)) to extract time patches. These time patches are then fed into the prototype-based neural tree module (depicted in Fig. 2 (b)), which enhances the interpretability of the model.\n3.1. The pseudocode to train ST-Tree\nAlgorithms 1 and 2 describe the training process for ST-Tree.\n3.2. Time patch module\nWe optimize a ST-based architecture, specifically the time patch module, for efficient feature extraction and obtaining time patches. The structure of the time patch module is illustrated in Fig. 3. Time partition module is utilized as the first step to segment and obtain the initial time patch. The multivariate time series X of dimensions (T \u00d7 C) is reshaped using four non-overlapping neighboring timestamps to generate a time patch, which is embedded with dimensions $\\frac{1}{4}\\times 4nC$. The attention module (detailed in Section 3.4.2) is employed as the second step for feature refinement. Embedding the attention module into the self-attention structure helps it to obtain more differentiated time domain features and time patches. Finally, the features pass through a Layer Normalization (LN) layer and an Multi-Layer Perceptron (MLP) layer for further processing.\nThe self-attention calculation formula is shown in Eq. (1). The query (Q), key (K) and value (V) are the matrices by extracting features from attention module (detailed in Section 3.4.2).\n$X_{SA} = softmax \\bigg(\\frac{attention_{Q} X (attention_{K} X)^T}{\\sqrt{d_k}}\\bigg) attention_{V} X \\quad(1)$\nSHIFT module. It incorporates a shift-window mechanism to limit the self-attention computation to non-overlapping local temporal patch windows. This design choice enables the interaction of attention between individual time patches.\n3.3. Neural Tree module\n3.3.1. Overview\nAfter obtaining patches from the Time Patch Module in Section 3.2, the next steps involve initializing a tree.\n1.  Feature Transformation\nFirst, the input patches are transformed using a sigmoid activation as Eq. 2, where patches is the input feature tensor of ST model representation in Section 3.2.\n$patches' = sigmoid(patches) \\quad(2)$\n2.  Prototype Chunking\nThen, the prototype vector prototypes is chunked into multiple prototypes as Eq. 3, where prototypes is the prototype tensor, num_prototypes is the number of prototypes to chunk. These prototypes will correspond to each node in the tree.\n$chunked\\_prototypes = chunk(prototypes, num\\_prototypes) \\quad(3)$\n3.  Tree Traversal\nFinally, the transformed features and prototypes are processed through the tree structure. For each node in the tree, the output is computed as Eq. 4, where logits is the prediction of ST model. patches' is the transformed feature tensor obtained from feature_transformation. The out is the final prediction result.\n$out = TreeTraversal(logits, patches', chunked\\_prototypes) \\quad(4)$\n3.3.2. Recursive Tree\nLet T(i, d) be the tree initialization function where i is the index and d is the current depth. The formula for tree traversal is given in Eq. 5, with detailed steps outlined in Algorithm 2.\n$T(i, d) =\\begin{cases}\nLeaf(i, num\\_classes) & \\text{if } d = tree\\_depth\\\\\nBranch(i, T(i + 1, d + 1), & \\text{if } d < tree\\_depth\\\\\nT(i + left.size + 1, d + 1), prototypes)\n\\end{cases} \\quad(5)$\nwhere Leaf(i, num_classes) generates a leaf node with index i and number of classes when d reaches the maximum tree_depth. Branch creates a branch node when d < tree_depth. left.size is the size of the left subtree, used to determine the starting index of the right subtree. prototypes represents the current prototypes for the branch node.\n3.4. Neural tree module\nST-Tree uses prototypes to identify discriminative regions in time patches. The prototype feature vector serves as a representative of a time series category. When discriminative features are sampled, they are routed to child nodes based on a routing direction determined by a routing module. Specifically, each tree node corresponds to a trainable prototype $P_i$ for measuring the routing score $N_i \\in [0, 1]$ for the reshaped time patch $\\check{z}$, as shown in Fig. 2(b).\nWe introduce a module that uses perfect binary trees and a routing mechanism. The module consists of sets of nodes $N(\\cdot)$, leaf nodes $L(\\cdot)$, and edges $E_{ij}(\\cdot)$ connecting parent node i to child node j. This module represents a perfect binary tree, where each internal node has two children: $N_{2\\times i}$ and $N_{2\\times i+1}$. Given the encoder output $z_0=f(x;\\theta)$, the module predicts the final class label $\\hat{y}$ using a soft decision-making process.\n3.4.1. Branch routing module\n1.  Computing L2 Distance for Similarity\nST-Tree computes the squared Euclidean distance between the time patch and the prototype of each category (as described in Eq. 7-12). This distance is considered as the similarity score between the time patch and the category prototype, where a smaller distance value indicates a higher similarity score. To quantify the similarity between the prototype and the time patch in each node, the similarity score is determined as in (Chen et al. (2018)).\nThe distance is measured using a convolutional operation, where each prototype acts as a convolutional kernel on z. The distance between the prototype and z is computed, and we represent the similarity by using a logarithmic similarity measure as briefly defined in Eq. (6), detailed in 7-12.\n$N(z_i) = log \\bigg(1 + \\frac{\\max_{P_i \\in patches(z_i)} (\\frac{1}{\\mid\\mid \\check{z} - P \\mid\\mid_2 + \\epsilon})}{c}\\bigg) \\quad(6)$\nAssume that the prototype vector has dimensions of [1, 1, k], where k is less than the length L' of the time series. The time patch vector x has dimensions [B, C, L']. The detailed steps to compute the L2 distance are as follows:\n(a) Square the input time patch features x as Eq. 7.\n$x^2_{i,j} = (x_{i,j})^2 \\quad(7)$\n(b) Use convolution to compute the sum of squares within a local region as Eq. 8. In Eq. 8, k is the length of the convolution kernel. $x_{i,j+k}$ is element of the input feature x at position (i, j + k). k is length of the proto_vector.\n$x2\\_patch\\_sum_{i,j} = \\sum_{k=0}^{H-1} x^2_{i,j+k} \\quad(8)$\n(c) Compute the sum of squares of the prototype vector as Eq. 9. In Eq. 9, proto_vectork is element of the prototype vector at position k.\n$p2 = \\sum_{k=0}^{H-1} (proto\\_vector_k)^2 \\quad(9)$\n(d) Compute convolution similarity, use convolution to compute the dot product between the feature vector and the prototype vector as Eq. 10.\n$xpi_{i,j} = \\sum_{k=0}^{L'-1} x_{i,j+k} \\cdot proto\\_vector_k \\quad(10)$\n(e) Compute the L2 distance, combine the sum of squares and the dot product to get the final L2 distance as Eq. 11. In Eq. 11, x2_patch_sumij is sum of squared values of the input feature x over a local patch. p2 is sum of squared values of the proto_vector. xpi,j is dot product of the input feature x and the proto_vector.\n$distances_{i,j} = \\sqrt{\\max (0, x2\\_patch\\_sum_{i, j} + p2 - 2xpi_{i,j})} \\quad(11)$\n(f) Compute similarity as Eq. 12.\n$similarity_{i,j} = log \\bigg(\\max \\bigg(1 - \\frac{1}{distances_{i,j} + 1}\\bigg)\\bigg) \\quad(12)$\n2.  Routing Score\nThe routing score $N(z_i)$ of the i-th tree node represents the similarity between the nearest reshaped time patch $\\check{z}$ and the prototype $P_i$. The routing score determines the branching direction (left or right), as shown in Fig.2(b). We define the routing score for each child node as briefly described in Eq.(13), detailed in Eq. 14-19.\n$R_{i,j} (Z_i) =\\begin{cases}\n[N (z_i)] & \\text{if } j = 2 \\times i\\\\\n( 1 - [N (z_i)] ) & \\text{if } j = 2 \\times i + 1\n\\end{cases} \\quad(13)$\n(a) In Eq. 14, similarity is similarity scores computed from the distance.\n$maxim = maxpoolld(similarity) \\quad(14)$\n(b) In Eq. 15 nd 16, to left is probability of choosing the left branch, to left takes the values from the 1st column of all rows. maxim is maximum similarity values after maximum pooling.\n$to\\_left = maxim_{:,0} \\quad(15)$\n$to\\_right = 1 - to\\_left \\quad(16)$\n(c) In Eq. 17 and 18, logits is prediction from ST model. attention_l, attention_r are functions that applies the attention model (Section 3.4.2) on the input patches for the left branch, enhancing relevant features.\n$l\\_dists = left(logits, attention\\_l(patches)) \\quad(17)$\n$r\\_dists = left(logits, attention\\_r(patches)) \\quad(18)$\n(d) In Eq. 19, dists is the computed distance values in current node, combining distance information from both the left and right branches. to left is the probability of choosing the left branch, calculated from the maximum similarity values. The value typically ranges between [0, 1]. to right is the probability of choosing the right branch, computed as 1 \u2013 to_left. l_dists (r_dists) is distance from the left (right) branch, indicating the distance between the feature vector and the prototype vector of the left (right) branch.\n$dists = to\\_left \\times l\\_dists + to\\_right \\times r\\_dists \\quad(19)$\n3.4.2. Attention module\nWe propose an attention module that utilizes spatial and channel attention mechanisms for feature refinement and extraction. We introduce the context converter module to capture more distinct time patches. Finally, the context time patch $Z_i$ enhanced by attention module ($\\mathcal{V}_{i,j}$) to get $z_j$ using Eq. (22), and next $z_j$ will be fed into the corresponding child nodes. Assume that the dimension of patch is $B \\times C \\times L'$. The structure of attention module is shown in Fig. 2 (c).\nChannel Attention: the spatial information of the feature graph is first aggregated using average and maximum pooling operations to generate two different spatial context descriptors representing the average and maximum pooling features, respectively. Finally, the two are spliced. The channel attention is computed as Eq. (20). In Eq. (20), the dimension of $p$ is $B\\times C\\times 1$.\nSpatial Attention: the relationship between feature spaces is utilized to generate a spatial attention map, which is complementary to channel attention. The average pooling operation and the maximum pooling operation are first applied along the channel axis generate a valid feature descriptor. The spatial attention is calculated as Eq. (21). In Eq. (21), the dimension of $p_j$ is $B \\times 1 \\times L'$. In Eq. (20) and Eq. (21), $c^o$ is one-dimensional convolution operation with filter size $a$.\n$q = sigmoid (c^o [avgpoolld (q_i) ; maxpoolld (q_i)]) \\quad(20)$\n$q_j = sigmoid (c^o [avgpool1d(q); maxpoolld(q)]) \\quad(21)$\n$z_j = \\mathcal{V}_{i,j} (z_i) = attention (z_i) \\quad(22)$\n3.4.3. Prediction module\nRouting score. The leaf node in each tree module corresponds to the leaf prediction module $L(\\cdot)$ used to predict the category probabilities. Let $p(z_0)$ be the cumulative routing score of the output $z_0$ from the root node to the l-th leaf node on a set of edges of a particular path $p_i$. The cumulative routing score $p(z_i)$ is computed by using Eq. (23). In Eq. 23, $R_{i,j}$ indicates the routing from node i to child node j, detailed in Eq. 13. $E_{i,j} (z_i)$ represents the function that transforms the input feature $z_i$ for node i and child node j. children(i) denotes the set of all child nodes of node i.\n$\\rho (z_i) = Aggregate (\\{R_{i,j} (E_{i,j} (z_i)) \\mid j \\in children(i)\\}) \\quad(23)$\nFC. Each leaf prediction module passes through the time patch fully connected layer (timeFC), and tree fully connected layer (treeFC), as shown in Fig. 2 (d). The leaf prediction module is computed as Eq. (24). In Eq. (24), treeFC is obtained based on a certain path, and timeFC is obtained through time patch module. Thus, the module can combine local path and global contextual feature representations. The detailed steps are shown as Eq. 25 - 27.\n$\\mathcal{L} (z_l, x) = treeFC (z_l) + timeFC(z_0) \\quad(24)$\n1.  Use max pooling to process the features of the nodes in the layer preceding the leaf as Eq. 25. In Eq. 25, patches\" is the feature from the layer preceding the leaf.\n$x = maxpoolld(patches\") \\quad(25)$\n2.  Obtain the final prediction through path prediction and ST prediction as Eq. 25. In Eq. 25, pred(x) are predictions obtained from a MLP layer in the leaf node, logits are predictions of ST module.\n$dists' = pred(x) + logits \\quad(26)$\n3.  Use softmax function to obtain the final prediction as Eq. 25. In 25, pred(x) are predictions obtained from a MLP layer in the leaf node, logits are predictions of ST module.\n$dists\\_leaf = softmax(dists') \\quad(27)$\nLoss. The ultimate prediction $\\hat{y}$ is obtained through the multiplication of all leaf predictions $\\mathcal{L}$ with the sum of the accumulated routing scores $\\rho_l$, as illustrated in Eq. (28)-Eq. (31). The optimization of $\\hat{y}$ is achieved by utilizing the Cross-Entropy loss function with the true label y. The final prediction $\\hat{y}$ is computed by multiplying all leaf predictions $\\mathcal{L}$ by the sum of the accumulated routing scores $\\rho_l$, as shown in Eq. (28)-Eq. (31).\n$\\hat{y} = \\rho \\cdot g(x) \\quad(28)$\n$g_l(x) = \\sigma (\\mathcal{L}(x, z_l)) \\quad(29)$\n$\\rho = [\\rho_1(z_1), \\rho_2(z_2), ..., \\rho_n (z_n)] \\quad(30)$\n$g(x) = [g_1(x), g_2(x), ..., g_n(x)] \\quad(31)$\nIn Eq. 28, $\\rho$ is a vector of routing scores for each node l, representing the similarity between each time patch $z_l$ and its prototype. $g(x)$ is a vector of functions $g_l(x)$ that apply an activation function $\\sigma(\\cdot)$ to the leaf $\\mathcal{L}(x, z_l)$ for each node.\nThe loss function for leaf nodes to make predictions is shown in Eq. (32). In Eq. (32), N is the training sample set, M is the number of labels, y is the true label, and $\\hat{y}$ is the model prediction label.\n$Loss = \\frac{1}{N}\\sum_{i=1}^{N}\\sum_{j=1}^{M}  y_{i,j} \\cdot log \\hat{p} (\\hat{y}_{i,j}) \\quad(32)$"}, {"title": "4. Experiment", "content": "4.1. Experimental setting\n4.1.1. Datasets\nWe used 10 multivariate benchmark datasets from the UEA archive 3 for our comparison experiments. These datasets are mainly categorized as: Motion", "AWR)": "The AWR dataset comprises 12 sensors providing X", "AF)": "The AF dataset consists of two-channel ECG recordings aimed at predicting spontaneous termination of AF. Each instance in the dataset represents a 5-second segment of AF", "BM)": "The BM dataset was generated by four students performing four different activities while wearing a smartwatch. It includes four classes: walking", "CT)": "The CT dataset was captured using a tablet and includes three dimensions: x-coordinate", "HMD)": "The HMD dataset includes MEG activity modulated based on the direction of wrist movements. The data was recorded while subjects performed wrist movements in four different directions.\nNATOPS: The NATOPS dataset is generated using sensors placed on the hands", "locations.\nPenDigits": "The PenDigits dataset is used for handwritten digit classification. It involves 44 different writers drawing digits from 0 to 9. Each instance in the dataset consists of the x and y coordinates of the pen as it traces the digit on a digital screen.\nSelfRegulationSCP2 (SR2): The SR2 dataset was obtained from an artificially respirated patient. The subject controlled the movement of a cursor on a computer screen while cortical potentials were recorded.\nSpoken ArabicDigits (SAD): The SAD dataset comprises 8800 time series instances representing spoken Arabic digits. Each instance is composed of 13 Frequency Cepstral Coefficients extracted from the speech signals.\nStandWalk Jump (SWJ): The SWJ dataset includes short-duration ECG signals recorded from a healthy 25-year-old male performing various physical activities. The dataset aims to study the effect of motion artifacts on ECG signals and their sparsity.\n4.1.2. Experimental Environment\nAll models were trained in a Python 3.8 environment using PyTorch 1.10.0 with Cuda 11.3. The training environment was configured with the Ubuntu 20.04 operating system", "hyperparameters": "n1.  Training Parameters. When using the Adam optimizer to update model parameters", "methods": "DTW-1NN (Chen et al. (2013))", "comparison.\n1NN-DTW": "Introduces a variation of the semi-supervised DTW algorithm.\\"}]}