{"title": "EAB-FL: Exacerbating Algorithmic Bias through Model Poisoning Attacks in Federated Learning", "authors": ["Syed Irfan Ali Meerza", "Jian Liu"], "abstract": "Federated Learning (FL) is a technique that allows multiple parties to train a shared model collaboratively without disclosing their private data. It has become increasingly popular due to its distinct privacy advantages. However, FL models can suffer from biases against certain demographic groups (e.g., racial and gender groups) due to the heterogeneity of data and party selection. Researchers have proposed various strategies for characterizing the group fairness of FL algorithms to address this issue. However, the effectiveness of these strategies in the face of deliberate adversarial attacks has not been fully explored. Although existing studies have revealed various threats (e.g., model poisoning attacks) against FL systems caused by malicious participants, their primary aim is to decrease model accuracy, while the potential of leveraging poisonous model updates to exacerbate model unfairness remains unexplored. In this paper, we propose a new type of model poisoning attack, EAB-FL, with a focus on exacerbating group unfairness while maintaining a good level of model utility. Extensive experiments on three datasets demonstrate the effectiveness and efficiency of our attack, even with state-of-the-art fairness optimization algorithms and secure aggregation rules employed. Code is available at https://github.com/irfanMee/EAB-FL", "sections": [{"title": "1 Introduction", "content": "Federated Learning (FL) [Kone\u010dn\u00fd et al., 2016] has recently emerged as a promising solution that enables multiple clients to collaboratively learn a shared prediction model while keeping all the training data on the device. Due to its privacy-preserving nature, in recent years, FL has benefited a wide variety of privacy-sensitive application domains, such as medical research [Rauniyar et al., 2023], and financial fraud [Liu et al., 2023], etc. However, the distributed nature of FL makes it inherently vulnerable to poisoning attacks, in which the model can be compromised by malicious clients uploading malicious model updates. Without a central authority to validate clients' participation, these malicious clients can indirectly and consequently manipulate the parameters of the learned model and thereby reduce its overall performance [Cao and Gong, 2022].\nIn addition, compared to centralized learning, FL models are more susceptible to algorithmic bias against specific demographic groups (e.g., racial and gender groups) due to its inherent characteristics, such as data heterogeneity, party selection, and client dropping out [Abay et al., 2020]. Compounding this issue, the involvement of malicious participants within FL environments can further exacerbate these biases.\nAttacker's Motivations. Attacking FL models through poisoning attacks to exacerbate their unfairness could provide an attacker with various benefits. For instance, e-commerce websites can be targeted with fairness attacks on their recommendation algorithms to suggest certain groups of products or services for the benefit of their providers while harming others. In the case of loan applications, an attacker might attempt to manipulate the FL model to unfairly discriminate against or favor specific groups of people, resulting in unjust loan decisions. Furthermore, the attacker can also attack the models used in criminal justice areas to make them unfair to reduce the trust and credibility in the criminal justice system. Given the aforementioned potential and motivations of fairness attacks, it is crucial to have a comprehensive understanding of the attack surfaces of FL, particularly in the domain of fairness.\nExisting Attacks. There are a few studies exploring poisoning attacks against FL systems, either by adding poisonous instances or adversarially changing model updates [Cao and Gong, 2022; Bagdasaryan et al., 2020]. However, these attacks are proposed with the purpose of reducing the model's classification accuracy without any regard for the model's fairness. In centralized machine learning, there have been attacking efforts to exacerbate algorithmic bias, such as the gradient-based poisoning attacks [Solans et al., 2021] and the anchoring and influence attacks [Mehrabi et al., 2021], which aim to maximize the covariance between the sensitive attributes and the decision outcome to affect the model fairness. However, these attacks can hardly be adapted to the FL settings due to the challenges in measuring the impact of each data sample on fairness violation in the learned model. This is mainly because the data used in FL is often decentralized and not directly accessible, making it hard to evaluate the specific impact of each sample on the model. To the best of our knowledge, the potential of leveraging poisoning attacks in FL to exacerbate group unfairness remains unexplored.\nOur Attack. In this paper, we design a new type of model poisoning attack, EAB-FL, where an adversary can introduce"}, {"title": "2 Related Work", "content": "Model poisoning attacks against FL systems have received considerable attention in recent years. These attacks are designed to manipulate the global model by tampering with local training processes on a fraction of participating clients. For instance, malicious clients can significantly degrade the performance of the global model by adding random noise to the local model to mislead the global model [Hossain et al., 2021; Cao and Gong, 2022]. Xingchen et al. [Zhou et al., 2021] proposed an optimization-based model poisoning attack to inject poisonous neurons into the model's redundant space, identified using the Hessian matrix. Moreover, it has been shown that the adversary can replace the aggregated model with the malicious model through one compromised device to perform model-replacement attacks [Xie et al., 2020; Bagdasaryan et al., 2020]. Henger et al. [Li et al., 2022] proposed a model poisoning attack using reinforcement learning, where malicious clients collectively learn the data distribution to launch an optimal attack. While the aforementioned attacks have shown a great chance of compromising FL models, they only target the model's utility by either decreasing its overall classification accuracy or making specific test samples classified as adversary-desired labels.\nIn addition, it has been shown that FL models are more likely to suffer from algorithmic bias compared to centralized learning due to their inherent characteristics, such as data heterogeneity, party selection, and client dropping out [Abay et al., 2020]. To address this issue, initial studies [Li et al., 2020; Mohri et al., 2019; Lyu et al., 2020; Li et al., 2021a; Zhong et al., 2022] focus on client parity and aim to promote equalized accuracies across all participating clients in FL. This is typically achieved by reducing the client's performance disparity [Li et al., 2020] or maximizing the performance of the worst client [Mohri et al., 2019]. More recent studies have addressed group fairness [Abay et al., 2020; Zhang et al., 2020; Rodr\u00edguez-G\u00e1lvez et al., 2021; Chu et al., 2021; Ezzeldin et al., 2023; Du et al., 2021]. These works propose solutions for providing fair performance across different sensitive groups. These studies mainly utilize deep multi-agent reinforcement learning [Zhang et al., 2020], re-weighting mechanisms [Abay et al., 2020; Ezzeldin et al., 2023; Du et al., 2021], optimization with fairness constraints (e.g., via alternating gradient projection [Chu et al., 2021] or the modified method of differential multipliers [Rodr\u00edguez-G\u00e1lvez et al., 2021]) to achieve group fairness.\nIn adversarial scenarios, attacks targeting fairness measures in machine learning are a relatively new concept, and only a few studies have been proposed in this area. Solans et al. [Solans et al., 2021] were among the first to propose a fairness attack that uses a gradient-based poisoning attack to introduce classification disparities among different groups. Mehrabi et al. [Mehrabi et al., 2021] proposed anchoring and influence attacks to introduce algorithmic bias in machine learning algorithms. More recently, Chhabra et al. [Chhabra et al., 2023] proposed a black-box fairness attack on clustering algorithms. However, all these attacks have been proposed in centralized learning settings. To the best of our knowledge, the potential of adversarial attacks aiming to exacerbate model unfairness in FL remains unexplored, which is vital for us to fully understand the attack surfaces of FL and thereby help facilitate corresponding mitigations to improve its resilience."}, {"title": "3 Preliminaries", "content": "3.1 Federated Learning\nFederated Learning (FL) is a collaborative approach in machine learning where a global model is trained across multiple distributed clients under the supervision of a central server. This method is distinct because it does not require direct access to client data, thereby enhancing privacy and data security. FL's primary aim is to optimize the global model's parameters while effectively utilizing the diverse, decentralized data held by each client. The key formula governing FL is:\n$\\min_{\\theta} f(\\theta) = \\sum_{k=1}^n p_k L_k(\\theta), \\quad L_k = \\frac{1}{d_k} \\sum_{j=1}^{d_k} l_{jk}(\\theta),$                                                          (1)"}, {"title": "3.2 Group Fairness in FL", "content": "In a binary classification task, we deal with training samples of the form $(x_1, y_1, g_1), ..., (x_d, y_d, g_d)$ where each example consists of an instance $x_i \\in X$, a label $y_i \\in Y$, and a sensitive attribute $g_i \\in G$. The goal is to develop a classification model, denoted as $f(\\theta) : X \\rightarrow Y$, which aims to minimize the cumulative loss, $L_m (\\theta,D) = \\sum_{(x,y)\\in D}l(f(x, \\theta), y)$, over the training dataset $D = (X,Y, G)$ to find the optimal parameters. In the context of Federated Learning (FL), the framework strives not only for accuracy but also for fairness in model predictions concerning the sensitive attribute $g_i$. Fairness is evaluated based on certain notions, such as demographic parity and equal opportunity. A model is considered fair from a group fairness perspective if it performs equally well for both the privileged group ($g_i = 1$) and the underprivileged group ($g_i = 0$). For a model yielding binary predictions $\\hat{Y}$, given data samples X and their corresponding labels Y, we use the following two metrics to assess the model's fairness:\nDemographic Parity [Dwork et al., 2012]: If a classifier's predictions $\\hat{Y}$ is statistically independent of the sensitive characteristic G, it meets demographic parity under a distribution (X, Y, G). This is equivalent to $E[Y|G = a] = E[Y]$, where a = 0 or 1 for a binary group. Demographic parity can be defined as:\n$Pr{\\hat{Y} = 1|G = 1} = Pr{\\hat{Y} = 1|G = 0}.$                              (2)\nEqual Opportunity [Hardt et al., 2016]: If a classifier's predictions $\\hat{Y}$ is conditionally independent of the sensitive feature given the label, it meets equalized opportunity under a distribution (X, Y, G). This is the same as $E[\\hat{Y}|G = a, Y = 1] = E[\\hat{Y}|Y = 1]$. In this case, we want the true positive rate $Pr{\\hat{Y} = 1|Y = 1}$ to be the same for each population with no regard for the errors when $Y = 0$. Equal Opportunity thus can be defined as:\n$Pr{\\hat{Y} = 1|Y = 1,G = 1} = Pr{\\hat{Y} = 1|Y = 1, G = 0}.$                      (3)"}, {"title": "3.3 Influence Score", "content": "In EAB-FL, to exacerbate model bias, each malicious client needs to identify a subset of their local training samples that can detrimentally affect the performance of a specific demographic group (i.e., targeted group). To quantify the impact of each local training sample on the model's performance concerning the targeted demographic group, we use \u201cinfluence score\" [Wang et al., 2022] to assess how a model's prediction on the data samples from the targeted group would change if a training sample $(x_i, y_i, g_i)$ is excluded from the training dataset, particularly under fairness constraints imposed on the classifier. Specifically, the influence score of a training example $(x_i, y_i, g_i)$ concerning a specific demographic group (e.g., $g_j = \\tau$) can be represented as:\n$\\inf_i(D, i) \\approx \\int_{\\Omega} \\Theta(x_i, x_j; \\theta) \\Big( \\frac{\\partial \\Phi(f, g_i)}{\\partial f(x_i; \\theta)} \\frac{\\partial L(w, Y_i)}{\\partial w}|_{w=f(x_i; \\theta)} \\Big) dPr(x_j, y_j, g_j),$                                                 (4)\nwhere $\\inf(D, i) \\in \\mathbb{R}$, $\\Theta$ is the Neural Tangent Kernel (NTK) [Jacot et al., 2018] and $\\theta$ represents the parameters of the classification model. $\\Phi(f, g_i)$ denotes a differentiable surrogate for commonly used fairness constraints, such as Demographic Parity or Equal Opportunity, $\\mu$ is a tolerance parameter for fairness deviations, and $dPr(x_j, y_j, z_j)$ refers to the differential probability measure over the data distribution D. We solve this equation by calculating the Jacobian of the function that multiplies the model's output with the demographics attribute ($g_j = \\tau$), and then compute a kernel matrix from the gradients. A positive value suggests that including the training sample $(x_i, y_i, g_i)$ improves the model's performance for the targeted demographic group, while a negative value implies it hinders accuracy. The magnitude of the value shows the strength of this impact."}, {"title": "3.4 Layer-wise Relevance Propagation", "content": "Layer-wise Relevance Propagation (LRP) [Bach et al., 2015] is an explanation technique, aiming to propagate the model prediction $f(x, \\theta)$ backward in the model to quantify the contributions of each neuron to the model prediction. Specifically, during the backward propagation of LRP, as shown in Figure 1, each neuron redistributes the received relevance scores to the preceding layer in equal proportions. By examining the propagated relevance scores, LRP can determine the degree of influence each neuron has on the model prediction. Neurons with higher relevance scores are deemed more \u201cimportant\u201d in decision-making, while those with lower scores are considered relatively \u201credundant\u201d. If we consider p and q as neurons in two successive layers, the relevance scores $(R_q)_p$ at one layer are transferred to the neurons in the layer below by applying the following rule:\n$R_p = \\sum_q \\frac{Z_{pq}}{\\sum_p Z_{pq}} R_q$                                                     (5)\nwhere $Z_{pq}$ is the product of the activation of neuron p and the weight of the connection from neuron p to neuron q, which"}, {"title": "4 Attack Design Overview", "content": "In this work, we propose an optimization-based model poisoning attack in FL, EAB-FL, that targets the group fairness measure of the learned global model. As shown in Figure 2, the FL system consists of a central server and n clients, a small fraction of which have been compromised by an adversary. During each communication round, benign (non-compromised) clients (e.g., the client h) train the global model ($\\theta_g^t$) provided by the central server on their local datasets for a certain number of epochs and send the updated model ($\\theta_{h,b}^t$) back to the server. Additionally, the FL system may employ either secure aggregation rules (e.g., Krum [Yin et al., 2018] and FLDetector [Zhang et al., 2022]) and/or fairness optimization strategies (e.g., FEDFB [Zeng et al., 2022] and FAIRFED [Ezzeldin et al., 2023]) to defend against potential poisoning attacks and ensure model fairness.\nOn those malicious client devices, the adversary can launch the attack through three stages: (1) each client (e.g., the client k) follows the normal procedure by training the global model on their local datasets for a certain round of epochs, generating a benign model $\\theta_{k,b}^t$; (2) the adversary uses the global model ($\\theta_g^t$) to calculate the influence score of each local data sample from the privileged demographic group over the targeted group using Equation 4 and creates a local biasing dataset ($D_{bias}^k$) that can reduce the model's classification accuracy for the targeted group; and (3) the adversary first identifies the redundant space (i.e., the neurons that remain relatively invariant during training) using LPR in the benign model update $\\theta_{k,b}^t$, and then manipulates the neurons within this space using the created biasing dataset, leading to the poisoned model $\\theta_{k,p}^t$. This poisoned model is sent to the server, where it is aggregated with updates from other clients to update the global model ($\\theta_g^{t+1}$) for the next communication round. With this multi-step approach, the adversary can induce overfitting of the model to the privileged demographic group while decreasing accuracy for the targeted group, thereby exacerbating model bias, while the impact on its overall utility remains minimal."}, {"title": "4.3 Design of EAB-FL", "content": "The crux of EAB-FL lies in inducing bias without compromising the attack's persistence or stealthiness. Unlike conventional poisoning strategies that only attack the global model's utility, our attack employs an optimization-based approach that is more sophisticated and tailored for inducing bias in FL. The attack pipeline on each malicious client unfolds as:\n(1) Regular Local Training: The adversary intends to introduce or exacerbate model bias by maintaining a high model utility for the privileged demographic group while decreasing the model accuracy for the targeted demographic group. To ensure high utility for the privileged demographic, during the communication round t, each malicious client (e.g., client k) follows a similar procedure as benign clients to conduct training on their local dataset $D_k$. The objective function can be represented as:\n$\\min_{\\theta} \\frac{1}{|D_k|} \\sum_{i=1}^{|D_k|} L_i(f(x_i; \\theta_{k,b}), y_i) \\quad s.t. \\Phi(f, g_i) \\leq \\mu,$                                                 (6)\nwhere $\\Phi(f, g_i)$ serves as a differentiable proxy for the fairness constraints (i.e., Demographic Parity used in EAB-FL), with"}, {"title": "5 Evaluation", "content": "5.1 Federated Datasets\nWe evaluate the proposed EAB-FL using the following three datasets in non-IID settings:\n(1) CelebA [Liu et al., 2018]: A collection of 200k celebrity face images from the Internet that have been manually annotated. The dataset has up to 40 labels, each of which is binary-valued. For CelebA, each subject's gender (male or female) is a sensitive attribute. (2) Adult Income [Dua and Graff, 2017]: A tabular dataset that is widely investigated in machine learning fairness literature. It contains 48, 842 samples with 14 attributes. In this dataset, race (white or non-white) is used as the sensitive attribute. (3) UTK Faces [Zhang et al., 2017]: A large-scale face dataset with more than 20,000 face images with annotations of age, gender, and ethnicity. Race (white or non-white) is used as the sensitive attribute in this dataset.\nTo show the real-world implications, we also apply EAB-FL to the MovieLens IM dataset [Harper and Konstan, 2015] (a movie recommendation system).\n5.2 Evaluation Metrics\n(1) Equal Opportunity Difference (EOD): We use the EOD of each sensitive group to measure group fairness. Specifically, EOD = |Pr{\u0176 = 1|G = 0,Y = 1} \u2013 Pr{\u0176 = 1|G = 1, Y = 1}|.\n(2) Demographic Parity Difference (DPD): DPD is another metric used for measuring group fairness, which is calculated as DPD = Pr{\u00dd = 1|G = a} \u2013 Pr{\u0176 = 1} .\n(3) Utility: In our experiments, we use the model's prediction accuracy to quantify global model utility.\n5.3 Fairness Attack Baselines\nSince no fairness attack specifically designed for FL currently exists, we adapted two fairness attacks originally designed for centralized learning (i.e., gradient-based [Solans et al., 2021] and anchoring-based attack [Mehrabi et al., 2021]) to FL settings to demonstrate the superiority of EAB-FL. Specifically, to introduce or exacerbate model bias, the gradient-based attack employs a bi-level optimization process to inject a small fraction of poisoning points into the training data, while the anchoring-based attack strategically introduces poisoned data points near the targeted demographic group, sharing the same demographic characteristics but with opposite labels. In FL settings, we adapted these attacks by enabling malicious clients to employ them locally during their training processes. Unlike centralized learning, since we cannot measure the fairness impact of each local data sample on the global model, these attacks adapted for FL primarily target degrading the fairness level within their respective local datasets, rather than the global model as a whole.\n5.4 Fairness Optimization Strategies\nTo evaluate the effectiveness of our attack under certain fairness optimization strategies applied in FL, besides FEDAVG [McMahan et al., 2017], we also, adopt the following state-of-the-art FL fairness optimization methods to evaluate our attack's effectiveness: (i) Q-FFL [Li et al., 2020]: Q-FFL is one of the client-fairness-based methods, aiming to equalize the accuracies of all the clients by dynamically reweighting the"}, {"title": "5.5 Attack Performance", "content": "Attack Performance under FEDAVG. As shown in Table 1, if no attack is present under FEDAVG, the learned global model tends to be biased against certain demographic groups already (e.g., EOD and DPD on the CelebA dataset are 0.23 and 0.21, respectively), while the model utility is at a relatively good level (e.g., 91% on the CelebA dataset). Introducing an attack amplifies this inherent unfairness, as demonstrated in our evaluations where each participating client has an e probability of being malicious. The results indicate that compared to the two fairness attacks, EAB-FL is capable of introducing significantly greater model bias while maintaining a minimal impact on its utility. In the FEDAVG setting, without employing any fairness optimizations, these baseline attacks only slightly increase model bias. This is likely due to the adversarial updates being overshadowed by benign updates, leading to a catastrophic forgetting of the adversarial update (for instance, EOD values for the gradient-based and anchoring-based attacks on the CelebA dataset are just 0.25 and 0.24, respectively). However, EAB-FL significantly impacts the global model fairness (e.g., EOD and DPD values on the CelebA dataset are 0.41 and 0.43, respectively), meanwhile keeping its utility high.\nAttack Performance under Fairness Optimization. To validate whether our attack can remain effective under existing fair optimization strategies in FL, we evaluate our attack under four state-of-the-art fair FL methods, including Q-FFL [Li et al., 2020], GIFAIR-FL [Yue et al., 2023], FAIRFED [Ezzeldin et al., 2023] and FEDFB [Zeng et al., 2022]. The results, as shown in Table 1, indicate that while baseline attacks such as gradient-based and anchoring-based attacks yield a slight degradation in performance, EAB-FL significantly undermines fairness, even in the face of these advanced optimization strategies. For instance, when applying Q-FFL to the UTK Faces dataset, our attack results in an EOD of 0.33, signifying that the privileged racial group (white) is 33% more likely to receive correct classifications. Similarly, a DPD of 0.30 indicates a 30% higher likelihood for the privileged group to be positively classified. We also observe that EAB-FL is relatively more effective against Q-FFL and GIFAIR-FL than against FAIRFED and FEDFB. This is likely because FAIRFED and FEDFB implement fairness constraints at the local model level, enhancing each local model's fairness. In contrast, Q-FFL and GIFAIR-FL apply fairness during server-side aggregation, making them more susceptible to exploitation by EAB-FL.\nImpact of Malicious Participant Availability (\u20ac). As shown in Figure 3, we can see that the fairness measures (i.e., EOD and DPD) significantly increase from 0.23 and 0.21 to 0.41 and 0.43 for the CelebA data with the increasing probability of participating clients being malicious (e, from 0 to 0.3). However, further increasing e does not have a proportional impact on the model's fairness. This is due to the constraints on the model weights that make the malicious updates appear less suspicious to the server and prevent the client's local model from being trivial. We can observe a similar trend for other datasets as well, like for the Adult Income data dataset, the saturation comes at 0.4, and for UTK faces it comes at 0.3."}, {"title": "Attack Persistence", "content": "Attack Persistence. Attack persistence allows us to measure the durability of the attack's impact after the removal of malicious clients from the training process. To evaluate this, we conduct a single-shot attack scenario on the CelebA dataset, where the attack is initiated only once at round t = 230. For this round, we assume that all participating clients are malicious. Figure 4 presents a comparative analysis of EAB-FL and other baseline fairness attacks under this scenario, which indicates that EAB-FL maintains a high level of attack success (high EOD), over an extended period of model aggregation. The key to the proposed attack's sustained effectiveness lies in its strategic placement of poisoning neurons within the neural network's redundant space. By embedding the adversarial influence in these less dynamic areas of the network, we significantly reduce the likelihood of these neurons being altered during the training of the main task, thus ensuring the longevity of the attack's impact.\nEffectiveness against Secure Aggregations. We evaluate the effectiveness of EAB-FL against four robust aggregation rules: SparseFed [Panda et al., 2022], Krum [Yin et al., 2018], LoMar [Li et al., 2021b], and FLDetector [Zhang et al., 2022] on the CelebA dataset. As shown in Figure 5(a), our proposed EAB-FL can bypass the norm-bounded defense method (SparseFed) and achieves significantly high unfairness (e.g., EOD is over 0.35) in the global model. Although Krum, effective at detecting aberrant models, sometimes rejects our attack's poisonous updates, the adversary still maintains a much higher EOD (e.g., over 0.3 for most cases) compared to scenarios without attacks. Model-similarity-based methods, such as LoMar and FLDetector, fail to detect EAB-FL, as the poisonous updates closely resemble benign updates, constrained"}, {"title": "Comparison with Other Poisoning Attacks", "content": "Comparison with Other Poisoning Attacks. To show the attack's unique impact on the model's fairness and utility, we compare EAB-FL with various state-of-the-art poisoning attacks (i.e., DeSMP [Hossain et al., 2021], MPAF [Cao and Gong, 2022], Sign Flipping [Li et al., 2019a], DYN-OPT [Shejwalkar and Houmansadr, 2021], and BDB [Shejwalkar et al., 2022]) in FL on the CelebA dataset. As shown in Table 2, we observe that our attack achieves the highest overall impact on the model's fairness (EDO and DPD are as high as 0.45 and 0.50, respectively), and the model's utility is still at a relatively high level (i.e., over 83%). This indicates our attack's effectiveness in exacerbating group unfairness while preserving the utility of the global model. Conversely, the other poisoning attacks all have a negligible impact on fairness, i.e., less than 0.32 and 0.31 in EOD and DPD, while degrading utility to as low as 49%, mainly due to the non-uniform changes in local data or model.\nTime Complexity Analysis. Table 3 shows the average time required to successfully attack the global model per communication round on the CelebA dataset using an Nvidia Quadro A100 GPU. The results show that EAB-FL needs a slightly higher computation time compared to the no-attack (FEDAVG) scenario. To demonstrate EAB-FL's feasibility on lower computational power devices like smartphones and laptops, we estimated the required time by comparing their GPUs' FLOPS. The A100 GPU, chips used in recent Apple smartphones (e.g., Apple A17 Bionic), and commonly used chips in laptops (e.g., Intel Core i7 13700) have FLOPS ratings of 9.7 TFLOPS [Nvidia, 2022], 2.15 TFLOPS [Cpu-Monkey, 2024a], and 0.82 TFLOPS [Cpu-Monkey, 2024b], respectively. The estimated consumed time of EAB-FL on smartphones is 15.16 seconds and 39.74 seconds on laptops, which confirms the feasibility of launching EAB-FL on these edge devices."}, {"title": "6 Conclusion", "content": "In this work, we propose a new type of model poisoning attack, EAB-FL, in FL settings, with a focus on exacerbating group unfairness while maintaining a good level of model utility. The effectiveness and efficiency of the proposed attack are demonstrated through extensive experiments on three datasets in various FL settings. The results of this study highlight the importance of fully understanding the attack surfaces of current FL systems and the need for corresponding mitigations to"}, {"title": "10 Additional Evaluation", "content": "10.1 Impact of the Biasing Dataset Size\nIn our model poisoning fairness attack, the size of the biasing dataset is a critical factor. We conducted experiments to understand the impact of various biasing dataset sizes. These sizes are represented by the fraction \u041a, which indicates the proportion of data originating from the privileged group. It's important to note that the biasing dataset cannot be too large. It should not match the full size of the dataset from the privileged group. A biasing dataset of this magnitude could lead to overfitting, compromising the model's ability to generalize effectively.\nAdditionally, when k is substantial, it may not be possible to exclusively select samples with negative influence scores. In such scenarios, it becomes necessary to include samples with small positive influence scores. The results of our experiments, detailed in Table 4, reveal a clear trend: both Equal Opportunity Difference (EOD) and Difference in Positive Predictions (DPD) increase as the size of the biasing dataset grows. However, when \u043a exceeds 0.4, the model's accuracy significantly declines. This decline is attributed to the instability caused by the poison neurons introduced during adversarial training, leading to an overfitting of the model. This overfitting, in turn, adversely affects the model's accuracy, highlighting the need for a balanced biasing dataset size to maintain model effectiveness while executing the attack.\n10.2 Impact of Different FL Data Settings\nTo evaluate the effectiveness of EAB-FL under different levels of data heterogeneity, we conducted experiments considering different data distributions, including both IID and non-IID scenarios. The dataset was distributed among n clients in a way that the distribution of sensitive attributes (G) becomes non-IID and can be controlled using a heterogeneity parameter a, where a \u2192 \u221e corresponds to IID distributions. To introduce this heterogeneity, we utilized a power law distribution [Clauset et al., 2009].\nThe results in Table 5 demonstrate that data heterogeneity does have a notable impact on the attack effectiness. Non-IID data distribution is relatively easier to attack due to its general impact on fairness. However, it's worth noting that even in scenarios with an IID data distribution, EAB-FL can still significantly affect fairness, showcasing its effectiveness under various conditions.\n10.3 Real World Case Study\nIn order to illustrate the real-world implications of fairness attacks, we perform EAB-FL on a movie recommendation"}, {"title": "The attacker\u2019s optimization problem can be written as:", "content": "$\\max_{b} \\lambda b[i_t] - \\sum_{i \\neq i_t} b[i]$\ns.t. $\\frac{1}{N} \\sum_{(u,i,r) \\in R}(r- (E[i]+b[i])^TE[u])^2 \\leq \\epsilon,$\n$\\- c \\leq b[i] < c, \\forall i,$                                                                            (8)\nwhere X is a hyperparameter controlling the tradeoff between maximizing the bias for the targeted movie and minimizing the bias for other movies, N is the number of ratings in R, and e is the allowed error in the MSE loss. The second constraint ensures that the bias terms remain within a certain range, represented by the constant c. In this study, we conducted experiments using the MovieLens dataset, which was divided into 90% training data and 10% testing data. The training data was distributed among 500 clients, each with a participation probability of 0.2. Our targeted movie for the fairness attack was \"Mortal Kombat (1995),\" which had an average rating of 2.78."}, {"title": "EAB-FL Algorithm", "content": "Algorithm 1 EAB-FL Algorithm\nServer:\nInitialize global model parameter $\\theta_g^0$\nfor each round t = 0", "benign)": "nReceive global model parameter $\\theta_g^{t"}, "from the server;\nfor $(x_i, y_i, g_i) \\in D$ do\n$\\min_{\\theta \\in \\Theta} L(f(x_i, \\theta), y_i) \\quad s.t. \\Phi(f, g_i) \\leq \\mu$\nend for\nSend the updated $\\theta_{k}^t$ to the server.\nClient (malicious):\nReceive global model parameter $\\theta_g^{t}$ from the server;\nfor $(x_i, y_i, g_i) \\in D$ do\n$\\min"]}