{"title": "A Systematic Review on Long-Tailed Learning", "authors": ["Chongsheng Zhang", "George Almpanidis", "Gaojuan Fan*", "Binquan Deng", "Yanbo Zhang", "Ji Liu", "Aouaidjia Kamel", "Paolo Soda", "Jo\u00e3o Gama"], "abstract": "Long-tailed data is a special type of multi-class imbalanced data with a very large amount of minority/tail classes that have a very significant combined influence. Long-tailed learning aims to build high-performance models on datasets with long-tailed distributions, which can identify all the classes with high accuracy, in particular the minority/tail classes. It is a cutting-edge research direction that has attracted a remarkable amount of research effort in the past few years. In this paper, we present a comprehensive survey of latest advances in long-tailed visual learning. We first propose a new taxonomy for long-tailed learning, which consists of eight different dimensions, including data balancing, neural architecture, feature enrichment, logits adjustment, loss function, bells and whistles, network optimization, and post hoc processing techniques. Based on our proposed taxonomy, we present a systematic review of long-tailed learning methods, discussing their commonalities and alignable differences. We also analyze the differences between imbalance learning and long-tailed learning approaches. Finally, we discuss prospects and future directions in this field.", "sections": [{"title": "1 INTRODUCTION", "content": "The term \"Long Tail Theory\" was popularized by Chris Anderson's book \"The Long Tail: Why the Future of Business Is Selling Less of More\" [1], in which the shift in business from advertising and marketing a small number of bestseller products to paying attention to the vast amount of niche products was observed and analyzed, since their combined sales/revenues could be as significant as that of the head (most popular) products. It is closely related to \"the Pareto Principle\", also known as the \u201c80/20 Rule\", the Zipf's law, or the \"Vital Few Rule\u201d. Generally speaking, they are like two sides of the same coin. While the Pareto Principle focuses on the heads/hits and cuts off the rest, the Long Tail Theory aims to capture the tails and emphasizes the combined importance of the tail classes/niche items.\nLong-tailed data is essentially a special type of multi-class imbalanced data with a sufficiently large number of tail (minority) classes. Moreover, the combined importance of these tail classes is very significant, although each tail class by itself only has a small number of samples (sales). Long-tailed distributed data is a relatively common phenomenon in real-world scenarios, which often needs to be handled by artificial intelligence (AI) systems and applications, e.g. high-speed train fault diagnosis [2], [3], escalator safety monitoring [4], etc. Indeed, AI should not only satisfy the head/hit applications, but also be able to cover a large number of tail cases to ensure the robustness and generalization capability of the techniques and systems.\nLong-tailed learning (hereafter referred to as LTL for short) is a sub-domain of artificial intelligence/machine learning that aims to build effective models for application-s/tasks having long-tailed distributed data. Its main goal is to significantly improve the recognition accuracy on the tail (rare/minority) classes or cases while maintaining the same or similar accuracy on the head (frequent/majority) classes or cases. Specifically, 1) in object recognition/classification, researchers have proposed many LTL methods [5], [6], [7], [8], [9], [10], [11] that can substantially improve the recognition/prediction accuracy on the tail classes/cases, e.g. rare species, defective industrial products. Studies in [5], [10], [11], [12], [13] have shown promising results by disentangling deep feature representation learning and classifier training. There are also a large number of loss reweighting approaches that endow different weights to samples of the head and tail classes to adjust the decision boundary under the long-tailed settings [6], [14], [15], [16], [17], [18]. 2) in object detection, a few LTL methods [19], [20], [21] have also been designed to automatically locate rare objects or cases from images or videos, e.g. locating cracks in industry equipment or detecting the engineering vehicles near natural parks; 3) in image segmentation, researchers have also developed methods [22], [23], [24] that can identify and segment the rare objects/cases in images, such as abnormal actions, pathology areas in medical images, etc. In addition, specific data augmentation techniques for LTL have also been devised to mitigate the data scarcity problem in the tail classes [25], [26], [27], [28], [29].\nHowever, due to the rapid development of this field, keeping pace with recent advances in LTL is becoming increasingly difficult. As such, a comprehensive survey of existing methodologies in this field is urgent and beneficial to the community. This motivates us to conduct an in-depth survey of recent advances in long-tailed visual learning to gain insights into their principles and technical aspects in a systematic way. Based on the inherent learning process, we first propose a novel taxonomy that categorizes"}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Data imbalance has been a long-standing research issue in both machine learning and computer vision. In the past, researchers commonly used the term \u201cimbalance\" to refer to datasets with skewed class proportions [33]. However, in the last few years, many deep learning-based methods have emerged for overcoming the data imbalance challenges in computer vision, making this research field even more popular. For instance, [34], [35] are among the first methods that adopt deep learning to learn feature representations from imbalanced image data. In the meantime, people have been increasingly using the term \"long tail\" in these works. Today, these two terms are often used interchangeably.\nOne reason for the popularity of the \u201clong tail\u201d term in computer vision in recent years is that the training datasets nowadays are often of large scale and present highly imbalanced distributions, and they have a large number of tail (minority) classes. Another reason is the use of deep learning for overcoming the challenges of class-imbalanced distributions and the new problems and research opportunities brought by such deep learning methods. Specifically, the built-in data-driven feature extraction mechanism in deep learning is significantly influenced by the scale of the datasets and their distributions. For image datasets with long-tailed distributions, deep learning-based methods tend to become class-biased in the feature extraction step, hence the need for more advanced LTL approaches that can effectively extract representative features for the tail classes.\nIn comparison, traditional computer vision methods use hand-crafted feature extraction methods, which are often model-driven and independent of data distributions. After the feature extraction step, if the resulting features are class-imbalanced, these approaches rely on the subsequent data classification stage to counter such imbalance issues. That is, traditional methods mainly rely on the data classification algorithms/stage for tackling the imbalance problem, where many existing imbalance learning algorithms [36], [37], [38] can be straightforwardly applied. Early approaches such as [39], [40] fall into this category, which modifies the kernel features of SVM to optimize the class boundary when the extracted features are class-imbalanced.\nBesides computer vision, the term \"long tail\" has also been used in many others fields. 1) In search engines and recommender systems, the long tail issue has been studied in depth, aiming to suggest items or results that are highly relevant to infrequent queries [41] or specific users [42], [43]. 2) In network traffic modeling, it is widely accepted that network traffic follows a long-tailed distribution, and different distribution models such as hyper-exponential distribution [44] and mixed Erlang model [45] have been proposed to model such long-tailed network traffic data."}, {"title": "2.2 Related Work", "content": "In the literature, there are a few related survey papers on long-tailed learning [30], [31], [32]. In [30], the authors propose a taxonomy of three main categories, which are information augmentation, class re-balancing and module improvement. Their first category consists of data augmentation and transfer learning techniques; the second category contains resampling, class-sensitive learning and logit adjustment methods; the third category consists of metric learning, classifier design, decoupled training and ensemble learning approaches. In [31], the authors uses a taxonomy for the training stage of LTL that consists of data augmentation, resampling, cost-sensitive loss, multiple experts and transfer learning. In [32], the authors present a taxonomy of data processing, cost-sensitive weighting, decoupled learning and other methods.\nIn our taxonomy, the LTL process is organized into four major modules, containing a total of eight parts/categories. As can be observed from Figure 1, the first module is the (input) data pre-processing step, which aims to balance the number of samples of different classes to be used for neural network training via data balancing. The second module is the neural network design (modeling) step for effective representation / embedding learning. Based on the internal processing flow of a neural network, it consists of five parts/categories, which are neural architecture, feature enrichment, logits adjustment, loss function, and bells and whistles (i.e., various training strategies, techniques and practices for model performance improvement). The third module is the (internal) network optimization step, which aims to effectively update the huge amount of weights in a network via optimization algorithms and computing the gradients which are the partial derivatives of the loss function with respect to the weights for guiding the update process. The last module is the post-processing (output) step, which adjusts the confidence of the trained long-tailed classifiers or refines their predictions to better fit to the scenarios of real-world applications.\nComparing with the above surveys [30], [31], [32], our taxonomy is significantly different. First of all, we divide the overall LTL process into four major modules based on the internal learning process of a neural network, including the (input) data pre-processing/balancing step, the neural network design (modeling) step, the (internal) network optimization step, and the post hoc processing step, which is more holistic and natural, and easier to understand and accept for those who are new to the LTL field. Second, our taxonomy is more comprehensive, systematic and and in-depth. 1) Previous overview papers in LTL use the term/category \"loss reweighting\" or \"cost-sensitive weighting/learning\", which can not cover other loss functions for the general embedding learning purposes, which we will address in subsection 3.5.1. 2) Previous overview papers [31], [32] lack a systematic discussion on logit adjustment, which is a preliminary step ahead of loss function design. In subsection 3.4, we will specifically illustrate the logits calibration and Softmax margin learning techniques under this category. 3) We categorize weights and gradients rebalancing approaches [16], [46], [47], [48], [49], [50], [51] into network optimization [52], which can not be covered by"}, {"title": "3 LONG-TAILED LEARNING METHODOLOGIES", "content": "In Figures 1 and 2, we present a new taxonomy for long-tailed learning, in which we divide existing techniques into eight categories, which are 1) data balancing; 2) neural architecture; 3) feature enrichment; 4) logits adjustment; 5) loss functions; 6) bells and whistles; 7) network optimization; and 8) post hoc processing.\nData balancing approaches utilize data resampling or data augmentation or data synthesis techniques to build class-balanced training sets for training neural network models. Neural architecture approaches design specific network architectures to improve LTL performance. Feature enrichment methods aim to augment the features of the tail samples using a memory bank or the perturbation strategy, or utilize the additional or multi-modal features extracted from pre-trained/foundation models. Logits adjustment approaches adjust the logit values or enlarge the classification margins to improve LTL performance. Methods in loss function design either upweight the loss values of the tail-class samples or difficult samples, or devise comprehensive loss functions as optimization objectives to enhance the embedding/representation learning effectiveness. \u201cBells and whistles\" contains various strategies and technical details for network training and performance enhancement. Network optimization mainly involves the weights and gradients updating techniques for the internal network optimization. Finally, post hoc processing methods calibrate the confidence of a long-tail model to fit to real-world scenarios. In the following, we will introduce representative techniques in each category."}, {"title": "3.1 Data Balancing", "content": "Data balancing aims to increase the volume and diversity of the training samples of the minority (tail) classes to make the training samples class-balanced. Along this line, there are three representative subcategories of approaches, which are a) data resampling, which balances the number of samples in different classes; b) data augmentation, which expands the size of the training data via transformation operations; c) data synthesis, which generates synthetic samples using more advanced techniques such as GANs, distribution estimation/transfer methods, foundation models (such as GPT-4V, DALL-E [53]), etc. Table 1 summarizes representative methods in each subcategory."}, {"title": "3.1.1 Data Resampling", "content": "In machine learning, oversampling/upsampling the minority classes and undersampling/downsampling the majority classes are two commonly used strategies for handling class-imbalanced distributions [36]. In deep learning, class-wise sampling (e.g. [12]) and instance-wise sampling (e.g. [34], [54]) are two frequently used methods for balancing the number of per-class training samples in the mini-batches. Essentially, class-wise resampling may involve both oversampling (the samples of the minority classes) and undersampling (the samples of the majority classes), while instance-wise resampling (i.e. instance-wise resampling) adopts random/uniform sampling (undersampling) to select samples from the original datasets, regardless of their class labels. In [55], the authors show that class-balanced resampling can learn discriminative feature representations when the training samples are highly semantically related to their target labels; otherwise, uniform sampling is even better than class-balanced re-sampling.\nIn [34], the authors first perform clustering within each class, then select samples through a quintuplet sampling scheme that enforces both inter-cluster and inter-class margins. Such a sampling strategy can alleviate the information loss brought by random under-sampling.\nExisting approaches often treat samples of the same class equally, without taking into account the difficulty of individual instances. Hard example mining is a commonly adopted technique in deep learning for enhancing the learning capability of the models [56], [57], [58]. To improve the recognition performance on datasets with extremely imbalanced image attribute distributions, the authors in [59] propose to mine both the hard positives, i.e. samples from the minority classes that are predicted to belong to other classes or with low prediction scores on the ground-truth class, and the hard-negatives, i.e. samples from other classes that are misclassified to one of the minority classes, which can help enlarge the margins between classes."}, {"title": "3.1.2 Data Augmentation", "content": "Data augmentation is a commonly used technique in deep learning for artificially expanding the training set to improve the accuracy of the models while avoiding overfitting. It includes basic image transformations such as cropping and colour space transformations, and more advanced approaches such as image mixing [60].\nRandAugment and its variants. AutoAugment [61] and RandAugment [62] aim to find a group of transformation operations for data augmentation such that the deep model trained by a neural network can obtain better or the best accuracy on the target dataset. AutoAugment automatically searches for the best image transformation choices/policies for a dataset. RandAugment further investigates how to reduce the search space, where straightforward grid search is found to be very effective. Unlike RandAugment, Curriculum of Data Augmentation (CUDA) [63] is designed to find more fine-grained class-wise augmentations, i.e., the number of candidates for sequential augmentation operations for different classes. They reveal that strong/more data augmentations should be applied to samples of the majority (easy) classes instead of the minority (difficult) classes. Therefore, minority classes are given shallow-degree sequential augmentation operations in the training process for curriculum learning.\nMixup and its variants. Mixup [64] is a data augmentation technique that generates synthetic images with soft labels by performing linear interpolations in both the raw input space and label space. The soft label describes the prior inter-class relationship between the two input images. The main difference between Mixup and conventional transformation-based augmentation techniques lies in that Mixup alters both the raw input space and the label space when creating synthetic/augmented images. The idea of Manifold Mixup [65] is similar to Mixup [64], with the only"}, {"title": "3.1.3 Data Synthesis", "content": "While data augmentation techniques generate synthetic images based on image transformations, data synthesis approaches utilize more complex techniques for image generation, including GANs [81], distribution estimation/transfer [26], [72], [73], [74], pre-trained vision/vision-language models (foundation models [82], [83], [84], [85]), etc.\nGAN based data generation. [86] finds that information at the lower resolutions tends to be class-independent thus can be shared by all the classes, while class-specific features are usually unveiled at higher solutions. Inspired by such observations, in GAN-based image synthesis for long-tailed data, it proposes to calculate the unconditional (class-independent) objective for low-resolution images, while injecting conditional information at higher-resolution images.\nDistribution based data synthesis. The authors in [74] point out that estimating the centroids of the tail classes is much noisier because they have few samples and propose DisRobuLT, which optimizes against the worst-case centroids within a safety hyper ball around the empirical centroid. Rare-class sample generator (RSG) [26] is a data synthesis method which obtains the feature displacement of a real frequent-class sample by subtracting its feature vector with the closest cluster centre of the same class to remove the class-relevant information and obtain the divergence value. It next adds the above divergence value of the frequent-class samples directly to the feature vector of each rare-class sample to create synthetic minority samples. The authors in [72] propose LEAP, which first uses the angles of the cosine similarity between each feature vector and the centre of the corresponding class centre to model a Gaussian distribution for each class, next forces the variance of the Gaussian distribution for each tail class to be the same as the averaged variance of the head classes, then randomly selects feature vectors using the new distribution of each tail class. LEAP enlarges the variance of the feature distribution of each tail class so that they become more separated from other classes. GistNet [73] also proposes to share the geometry of the head classes with the tail classes. However, unlike LEAP, it uses an alternative implementation of the"}, {"title": "3.2 Neural Architecture", "content": "Neural architecture design, i.e., devising an effective neural network structure/model for a specified task, long with loss function design, are the two preliminary approaches to improving the overall performance of a neural network (model). Along this line, related methods can be divided into subcategories of decoupled learning framework, early-exit framework, knowledge distillation framework, dual streams inference framework, and multi-expert (ensemble) learning framework."}, {"title": "3.2.1 The Decoupled Learning Framework", "content": "Decouple [12] is a seminal work that reveals representation learning and classifier training should be carried out separately (i.e. decoupled) for improving long-tailed recognition. Specifically, as shown in Figure 3, the proposed decoupled LTL framework first adopts random sampling (instance-based sampling) and the Cross-Entropy loss for training a conventional feature extraction module in the first stage. It then freezes the backbone and uses class-balanced (class-wise) sampling to retrain the classifier in the second stage. Under the same decoupling framework as [12], [54] proposes to train several teacher models upon batches built via instance sampling in the first stage so that features extracted by different models can be more complementary, then distils the knowledge from the teachers to a student model. [89] also adopts the decoupling framework to address the class imbalance issue in medical image datasets, the difference is that it utilizes contrastive learning [90] to learn the feature space in the first stage. In [91], the authors use class-balanced sampling and conventional random sampling simultaneously to construct mini-batches of data for the feature extraction step. They reveal that random sampling avoids the risk of overfitting on the tail classes, while class-balanced sampling prevents the training process from being dominated by head classes.\nThe above methods have a clear boundary between the two decoupled learning stages. In Bilateral-Branch Network (BBN) [10], the authors propose to use an annealing factor to gradually transits between the two stages, which can adjust the model to learn the global features from the original distributions first, then gradually transit to the tail data modeling and discrimination learning. In Domain Balancing (DB) [5], the authors devise a soft gate to switch between a conventional feature learning module and a tail-class favoured feature learning module. For each tail-class image, the features from the two modules are integrated to improve the LTL performance. Hybrid-SC [11] proposes a hybrid network structure composed of a supervised contrastive learning branch to learn better feature representations from the long-tailed data and a conventional CNN branch with a cross-entropy loss to learn the classifiers. Like BBN, it smoothly transits between the two branches via a cumulative learning adaptor. The main difference between Hybrid-SC and BBN is that the former uses supervised contrastive learning in the first stage, which can learn better features. In [92], the authors reveal that the inference of visual relationships in a scene faces the problem of the dual long-tailed distributions of the entity (object) and predicate (relation) classes. To address this problem, they propose the DT2 model, which is based on an iterative decoupled training scheme. Like BBN, DT2 first trains (warms up) the model on the original data with uniform sampling, then fixes the backbone part and fine-tunes the classifier on the (remaining) network with the proposed Alternating Class Balanced Sampling (ACBS) method to capture the interplay between the long-tailed distributions of entities and predicates using knowledge distillation.\n[13] points out that the performance bottleneck of the two-stage (decoupling) learning framework seems to be the classifier retraining issue in the second stage. It thus proposes DisAlign for confidence calibration to adjust the decision boundary in the second (classifier retraining) stage. In [93], the authors propose the long-tailed class incremental learning problem and present a preliminary method under the two-stage LTL framework that adds a layer of learnable scaling weights to integrate the outputs of the classifier heads for class-incremental learning."}, {"title": "3.2.2 The Early-Exit Framework", "content": "In [94], the authors point out that existing approaches to long-tailed recognition reweight samples only by class size, but without considering the fact that there might be easy examples in the minority classes that get incorrectly up-weighted and hard examples in the majority classes that get erroneously down-weighted. They thus employ the early exiting framework (ELF) to early-exit easy examples in the shallow layers of the network to focus on the hard examples in each class in the deeper layers. In implementation, ELF augments a backbone network with several auxiliary classifier branches. During training, it finds and early-exits the correctly and confidently classified examples at the earliest possible exit branch, while harder examples will exit later in the network, sustaining their impact on the overall loss. Similarly, [95] proposes the Hardness Aware Reweighting (HAR) framework, which also adopts the early-exiting strategy in the training process. Specifically, HAR augments a backbone network with auxiliary classifier branches, and samples that are confidently and correctly classified at a branch will early exit from that branch."}, {"title": "3.2.3 The Knowledge Distillation Framework", "content": "In [96], the authors propose Self-Supervised Distillation for long-tailed visual recognition (SSD-LT). Adopting the decoupled learning framework, it trains two heads for knowledge distillation, which are the self-distillation head and the classification head, respectively. It thus owns the merits of rebalanced sampling and decoupled training strategy and is reported to obtain state-of-the-art performance in long-tailed recognition. The authors in [97] propose the \"Distill the Virtual Examples\" (DiVE) pipeline for long-tailed recognition. In DiVE, a teacher model is first trained for the long-tailed task with the existing methods, and the predictions from such a teacher model are used as virtual examples. It then uses knowledge distillation to transfer the knowledge from the teacher model to the student model.\n[98] discusses the use of ViT/Transformers for long-tailed learning. It proposes DeiT-LT which distills knowledge from a CNN teacher to a ViT backbone/student using out-of-distribution images generated via CutMix and Mixup, in which it reweights the distillation loss to enhance the focus on the tail classes. Recently, for long-tailed object detection, RichSem [99] adds a new branch to the detection framework for distilling semantics from CLIP, which can extract rich semantics for the bounding box patches within the image."}, {"title": "3.2.4 The Dual Stream Inference Framework", "content": "[100] designs a neural network architecture with two branches: one branch trains a baseline model on the original long-tailed distribution, while the other trains a second model upon the artificially balanced distributions. They add a cross-branch consistency loss based on their logit difference to enforce the two branches to train the multi-label visual recognition model collaboratively. For long-tailed face recognition, Center-dispersed (CD) Loss [7] first splits the original data into head-class data and tail-class data, then feeds it into the \u201cunequal-training\" framework that consists of a backbone and two learning branches for"}, {"title": "3.2.5 The Multi-Expert Learning Framework", "content": "Ensemble learning methods aim to utilize multiple experts/models to make more accurate predictions. Representative ensemble learning methods for LTL include ACE [101], RIDE [102], CBD [54], LFME [103], BalPoE [104], SHIKE [105], MDCS [106] and LGLA [107].\nThe authors in [101] propose ACE, a multi-expert ensemble network for long-tailed recognition. It aims to achieve biased representation learning and unbiased classifier training in a single stage. The class indices are first ordered descendingly by their number of samples, the first branch (expert) trains a classifier/model with the samples of all the classes; the next branch (expert) first eliminates the current head class, then trains a new classifier with the samples of the rest classes, and so on. Therefore, the number of classes handled by different classifiers decreases monotonically, and the medium-shot or tail classes will have chances to dominate a classifier, thus mitigating the bias towards the majority classes. Overall, ACE can simultaneously improve the recognition accuracy of both the head and tail classes. RIDE [102] trains multiple recognition models/experts on randomly sampled subsets and uses their logits mean to make predictions. It also devises a routing algorithm to skip the inferences of certain experts. CBD [54] also trains multiple recognition models/experts on randomly sampled subsets, it proposes to distil the knowledge from the multiple teachers/experts into a single student model. Similarly, LFME [103] also propose to distil the knowledge from multiple teachers into a unified/single student model. LFME differs from CBD in that the teacher models of the LFME are trained over non-overlapped partitions of classes and samples, while CBD trains multiple teacher models over randomly sampled subsets which may have overlapped classes/samples. Compared to CBD and LFME, ACE [101] trains cascaded models/classifiers by iteratively removing"}, {"title": "3.3 Feature Enrichment", "content": "Feature enrichment methods enrich/augments features of the tail classes using fusion-based approaches [5], [9], [21], [118], [119], [120], perturbation based approaches [121], [122], large pre-trained models/foundation models [87], [116], [123], etc."}, {"title": "3.3.1 Fusion Based Feature Enrichment", "content": "As depicted in Figure 4, the Retrieval Augmented Classification (RAC) method [118] uses an external memory module to store features of the tail samples explicitly. Next, for each input image, it finds in the memory bank the Top-K most similar features of the same class, which are then fed into a BERT-like encoder to be transformed into logits, then fused with the logits derived from a conventional backbone. Likewise, the authors in [124] propose PFENet, a prior guided feature enrichment network that enriches query features with support features and prior masks to overcome the spatial inconsistency between the query and support samples in few-shot segmentation. LOCE [21] also utilizes a memory module to store features of the tail-class samples for augmenting the features of the tail classes, which works collaboratively with the proposed Equilibrium Loss (EBL) that adjusts the margins for the tail classes in long-tailed detection. In OLTR [9], the authors combine the features extracted by a conventional CNN pipeline and the features stored in an additional memory module to generate features of the tail classes, which can transfer knowledge between head and tail classes, and increase the sensitivity of the model to novel classes.\n[120] proposes to disentangle image features into class-agnostic features (a.k.a. category-related/class-relevant) and class-generic features (a.k.a. category-independent/class-irrelevant) for one-shot image generation. It uses an auxiliary memory module to store class-generic features. The class-agnostic features from the input image and the class-generic features from the memory module are then fused and sent to the generator of a vanilla GAN for image generation. Likewise, in [119], the authors point out that some features are shared by the head and tail classes, while other features are class-agnostic (unique to certain classes). The authors thus propose to transfer the class-generic features from the head to the tail classes. [125] propose to augment tail classes by grafting the diverse semantic information from head classes, where it replaces a portion of feature maps from tail classes with those belonging to head classes. Such fused features substantially enhance the diversity of tail-class representations. To prevent the neural networks from fitting the head classes first, MFW [121] weakens features of the head classes by mixing them with features of other classes in the same mini-batches but without altering the corresponding labels to equalize the training progress across classes.\nCOSSL [126] devises a tail-class feature enhancement mechanism that gives a higher probability to the tail classes when blending features of unlabeled and labelled data."}, {"title": "3.3.2 Perturbation Based Feature Augmentation", "content": "The authors in [122] propose the FASA method for long-tailed instance segmentation that generates class-wise virtual features by random perturbation over a Gaussian distribution on the real samples of each class.\nCC-SAM [127] proposes to improve the generalization capability of the LTL models by robust training against class-conditioned perturbations on the features of the last convolutional layer. Its idea is similar to that of LogitAdjust [108]. The main difference is that LogitAdjust performs perturbations over logit values in the output layer, whereas CC-SAM adds noises to the features in the last convolutional layer (the last hidden layer)."}, {"title": "3.3.3 Foundation Model Based Feature Enrichment", "content": "Recently, foundation models such as CLIP [87] and ALIGN [123] have demonstrated impressive representation and generalization capabilities. In [116], researchers have explored using such foundation models for feature-level enhancements. They simultaneously utilize the visual and linguistic representations extracted by the foundation models CLIP [87] for long-tailed recognition. For long-tailed object detection, RichSem [99] adds a new branch to the detection framework for distilling knowledge from CLIP, which can extract rich visual and linguistic semantics to enhance the representations of the objects within the image."}, {"title": "3.4 Logits Adjustment", "content": "In the output layer, logits contain the raw prediction values of the neural network, which will be used as the input to the Softmax activation function to turn the logit values into a probability distribution over the input classes. Methods that fall into the logits adjustment category either optimize the inherent process in deriving the logit values [46], [128], [129], or calibrate the logit values in a post hoc manner [18], [130]. It also includes softmax margin learning methods [15], [110], [131], [132] that enforce margins into the logit values so as to influence the resulting Softmax classifier to have larger classification margins [133]. We note that, sometimes it is challenging to clarify the differences between logits calibration and Softmax margin learning. For instance, LogitAdjust [108] can be considered to be belonging to both subcategories. In Table 2, we summarize the formulas of representative methods in this category."}, {"title": "3.4.1 Logits Calibration", "content": "Inherent Logits Calibration. When deriving logits from the last hidden layer to the output layer, IR-Softmax [128] sets the weights for each class as their corresponding class centres in the feature space to avoid the shift between the weights and their centres. The authors in [129] propose Balanced MSE that straightforwardly uses the mean square error on each class to replace the corresponding logit element in imbalance regression.\nPost hoc Logits Calibration. In [18], the authors argue that the obscure regions between classes in classification are caused by Softmax saturation. To alleviate this issue, they propose the Gaussian Clouded Logit adjustment (GCL) loss that perturbs the normalized logit with an additive margin that has a random disturbing parameter. In [130], the authors reveal that pseudo-labels in semi-supervised learning are naturally imbalanced. They propose a Debiased Pseudo-Labeling method (DebiasPL) that deducts the probability for each class from the logit. After that, margin learning methods can still be applied.\nLike GCL [18], in BLV [138], category-wise variations are introduced into network predictions (logits) for long-tailed semantic segmentation, where tail classes obtain larger variations by multiplying the random value in a Gaussian distribution with a coefficient that is inversely proportional to the per-class sample size. This simple strategy improves"}, {"title": "3.4.2 Softmax Margin Learning", "content": "Softmax margin learning algorithms alter the logit values before sending them to the Softmax classifier (Softmax activation function) to improve the discrimination power of the classifiers. Therefore", "Post hoc Logits Calibration\" sub-category. In label-distribution-aware margin loss (LDAM) [15": "when using the logits to calculate the Softmax output", "131": "removes the logit values of other tail classes in the denominator of the Softmax activation function. AutoBalance [132", "110": "and LogitAdjust [108", "AutoBalance": "the main difference lies in the multiplicative and additive margin parameters used in the Softmax function, as can be seen from Table 2. Similarly, in [134", "141": "the authors deduce that margin-based binary classification provides a closed-form solution for the ideal margin of each object category in long-tailed object detection and propose the Effective Class-Margin (ECM) loss for optimizing margin-"}]}