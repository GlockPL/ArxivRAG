{"title": "Inevitable Trade-off between Watermark Strength and Speculative Sampling Efficiency for Language Models", "authors": ["Zhengmian Hu", "Heng Huang"], "abstract": "Large language models are probabilistic models, and the process of generating content is essentially sampling from the output distribution of the language model. Existing watermarking techniques inject watermarks into the generated content without altering the output quality. On the other hand, existing acceleration techniques, specifically speculative sampling, leverage a draft model to speed up the sampling process while preserving the output distribution. However, there is no known method to simultaneously accelerate the sampling process and inject watermarks into the generated content. In this paper, we investigate this direction and find that the integration of watermarking and acceleration is non-trivial. We prove a no-go theorem, which states that it is impossible to simultaneously maintain the highest watermark strength and the highest sampling efficiency. Furthermore, we propose two methods that maintain either the sampling efficiency or the watermark strength, but not both. Our work provides a rigorous theoretical foundation for understanding the inherent trade-off between watermark strength and sampling efficiency in accelerating the generation of watermarked tokens for large language models. We also conduct numerical experiments to validate our theoretical findings and demonstrate the effectiveness of the proposed methods.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable performance in various natural language processing tasks, enabling a wide range of applications such as chatbots [23], content generation [17], code generation [6], and more. However, the high training and inference costs of LLMs pose significant challenges. The substantial computational resources along with the high latency during inference can negatively impact user experience and limit their potential applications.\nTo address the issue of high inference costs, speculative sampling [16, 5] has emerged as a promising approach. This technique leverages a smaller, faster draft model to generate candidate results, which are then validated and corrected by a larger, more accurate target model. Compared with other acceleration methods such as knowledge distillation, model quantization, and model pruning, the key advantage of speculative sampling is that it can significantly reduce inference latency without compromising the quality of the generated content.\nIn addition to the challenge of high inference costs, protecting the intellectual property rights of LLMs generated content has become increasingly important. Digital watermarking techniques [1, 13] have been proposed to embed watermark information into the generated content, enabling the tracking of model usage. Unbiased watermarking schemes [12] have been developed to ensure that the watermarking process does not affect the quality of the generated content."}, {"title": "Preliminary", "content": "In this section, we will introduce the basic concepts and notations used throughout the paper, and provide a brief overview of watermarking and speculative sampling techniques for large language models.\nA language model defines a probability distribution over sequences of tokens from a vocabulary set \u03a3. It assigns a probability $P(x_{n+1}|x_1, x_2, ..., x_n)$ to the next token $x_{n+1}$ given the context of previous tokens $x_1, x_2, ..., x_n$. We use \u0394\u03a3 to denote the set of all possible probability distributions over the vocabulary \u03a3.\nFollowing Hu et al. [12], we define a watermarking scheme as a tuple $(E, P_E, R)$, where $E$ is a set of watermark codes, $P_E$ is a probability distribution over $E$, and $R : E \u00d7 \u0394\u03a3 \u2192 \u0394\u03a3$ is a reweighting function that maps a watermark code $E \u2208 E$ and a probability distribution $P\u2208 \u0394\u03a3$ to a watermarked distribution $R_E(P) \u2208 \u0394\u03a3$. We focus on unbiased watermarking schemes that satisfy $E_{E~P_E}[R_E(P)] = P$ for all $P \u2208 \u0394\u03a3$, unless explicitly stated otherwise.\nTo generate a watermarked token x, we first compute a watermark code $E ~ P_E$ based on the context, and then sample the token from the watermarked distribution, i.e., $x ~ R_E(P)$. The entropy of the distribution P determines the maximum amount of watermark that can be injected. For a distribution P with high entropy, the divergence between the watermarked distribution $R_E(P)$ and the original distribution P can be larger, allowing for more watermark information to be injected.\nThe presence of the watermark can be detected by statistical tests. The pivotal quantity used in these tests is often referred to as the watermark score. A higher watermark score implies a more detectable watermark. The log likelihood ratio (LLR) is the most powerful score for detecting watermarks in the absence of any perturbations. However, in practice, more robust scores such as the maximin-LLR or likelihood-agnostic scores are often used. In this paper, we consider two specific watermark scores: the maximin-LLR score, which is described in detail in [12], and the U score, which is a likelihood-agnostic score that can be defined for both DeltaGumbel reweight and Gamma reweight schemes. The details of the U score, DeltaGumbel reweight and Gamma reweight are provided in Appendix D.\nThe P-value can be computed by considering the absence of a watermark as the null hypothesis. For a score S with a known moment-generating function (MGF), the P-value can be upper bounded using the Chernoff bound:\n$P_{null} (S \u2265 \u015c) \u2264 min_{\u03bb\u22650} E[e^{\u03bbS}] exp(-\u03bb\u015c)$.\nSpeculative sampling [16, 5] is a technique for accelerating the generation of tokens from a target model P by leveraging a faster draft model Q. The key idea is to first sample a draft token \u00ee from the draft model Q, and then accept or reject it based on the ratio of the target and draft probabilities. If the draft token is rejected, a new token is sampled from a residual distribution proportional to the difference between the target and draft probabilities. Formally, the speculative sampling process generates a token x as follows:\n$P(x = j|x = i) =\\begin{cases}\nmin(1, \\frac{P(j)}{Q(j)}) & if i = j, \\\\\n\\frac{(P(j) - Q(j))_+}{\\sum_{z\u2208\u03a3} (Q(z) - P(z))_+} & if i \u2260 j,\n\\end{cases}$\nwhere $(x)_+ = max(0, x)$. The design of the speculative process ensures that the final distribution of the generated token x matches the target distribution P. The efficiency of speculative sampling can be measured by the overlap probability $\u03b1(P, Q) = \\sum_{t\u2208\u03a3} min(P(t), Q(t))$, which is the probability of accepting the draft token in each step. The overlap probability is related to the total variation distance between P and Q by $TV(P, Q) = 1 \u2212 \u03b1(P, Q)$. Such a speculative sampling process can be applied multiple times to generate and verify multiple draft tokens in one step.\nDue to space limitations, we have moved the discussion of other related works to the Appendix A."}, {"title": "Two Reweight Framework for Accelerated Generation of Watermarked Tokens", "content": "In this section, we propose a novel framework called the two reweight framework for accelerating the generation of watermarked tokens based on speculative sampling techniques. The motivation behind this non-trivial framework is that naively applying speculative sampling to a watermarked target distribution $R_E(P)$ may significantly reduce the overlap probability $\u03b1(R_E(P), Q)$ with the draft distribution Q, leading to a small sampling efficiency.\nThe key innovation of the two reweight framework is to apply a separate reweighting function R' to the draft distribution Q, using the same watermark code E as the one used for reweighting the target distribution. By doing so, we aim to increase the overlap probability between the watermarked target distribution $R_E(P)$ and the watermarked draft distribution $R'_E(Q)$, i.e., $\u03b1(R_E(P), R'_E(Q))$, thus improving the sampling efficiency.\nFormally, we define the watermarked draft distribution using another reweighting function $(E, P_E, R')$, where $R' : E \u00d7 \u0394\u03a3 \u2192 \u0394\u03a3$ is a function that maps a watermark code $E \u2208 E$ and a draft distribution $Q \u2208 \u0394\u03a3$ to a watermarked draft distribution $R'_E(Q) \u2208 \u0394\u03a3$. The framework itself does not require the watermarked draft distribution to be unbiased, i.e., $E_{E~P_E}[R'_E(Q)] = Q$ for all Q \u0395 \u0394\u03a3. However, we will see later that this unbiasedness property naturally emerges when we require the final output distribution to be unbiased and aim to improve the sampling efficiency (Lemma 3).\nTo generate a watermarked token, we first sample a draft token 2 from the watermarked draft distribution, i.e., $x ~ R'_E(Q)$ or equivalently $P(x = i) = R'_E(Q)(i)$ for all i \u2208 \u03a3. Then, we perform certain speculative sampling based on the draft token to obtain the generated token x. The speculative process is defined by a conditional probability distribution A(j|i) for all i, j \u2208 \u03a3, where A(\u00b7|\u03af) \u2208 \u0394\u03a3 for each i. The design of A can depend on the target distribution P, the draft distribution Q, and the watermark code E. The probability of generating a token x = j given a draft token x = i is given by $P(x = j|x = i) = A(j|i)$.\nThe distribution of the generated token, which we call the generation distribution, can be computed as follows:\n$P(x = j) = \\sum_{i\u2208\u03a3} P(x = j|x = i)P(x = i) = \\sum_{i\u2208\u03a3} A(j|i)R'_E(Q)(i) = (A \u25e6 R'_E(Q))(j)$.\nWe denote the generation distribution by $R_E(P) = A \u25e6 R'_E(Q)$.\nTo ensure that the two reweight framework produces an unbiased output distribution, we require that for all \u03a1 \u0395 \u0394\u03a3:\n$E_{E~P_E}[R_E(P)] = P$."}, {"title": "No-go Theorem", "content": "Despite the potential of the two reweight framework, we present a no-go theorem that shows the impossibility of simultaneously maintaining the watermark strength and sampling efficiency when the vocabulary size is greater than two.\nTheorem 1 (No-go Theorem). When the vocabulary size |\u03a3| > 2, there do not exist non-trivial reweighting functions $R : E \u00d7 \u0394\u03a3 \u2192 \u0394\u03a3$ and $R' : E \u00d7 \u0394\u03a3 \u2192 \u0394\u03a3$, and a speculative process A(j|i) such that for all P, Q \u2208 \u0394\u03a3:\n1.  The watermark strength is maintained: $R_E(P) = R_E(P)$.\n2.  The sampling efficiency is maintained: $\u03b1(P, Q) = E_{E~P_E}[\\sum_{i\u2208\u03a3} A(i|i)R'_E(Q)(i)]$.\nRemark 2 (Condition for maintaining the watermark strength). The condition $R_E(P) = R_E(P)$ in Theorem 1 ensures that the watermark strength is maintained by keeping the average watermark score unchanged, i.e.,\n$E_{E~P_E}E_{t~R_E(P)}[Score(t, E)] = E_{E~P_E}E_{t~R'_E(P)}[Score(t, E)]$"}, {"title": "Algorithms for Maintaining Watermark Strength or Sampling Efficiency", "content": "In this section, we present two algorithms that aim to maintain either the watermark strength or the sampling efficiency under the two reweight framework. In light of the no-go theorem (Theorem 1), which precludes the simultaneous maintenance of watermark strength and sampling efficiency, these algorithms provide deeper insights into the trade-offs between the two objectives."}, {"title": "Maintaining Watermark Strength", "content": "To maintain the watermark strength, we choose the reweight function for draft distribution to be the same as the reweight function for the target distribution, i.e., $R'_E(Q) = R_E(Q)$. The speculative process is designed as follows:\n$A(j|i) =\\begin{cases}\nmin(1, \\frac{R_E(P)(i)}{R_E(Q)(i)}) & if i = j, \\\\\n\\frac{(R_E(P)(j) - R_E(Q)(j))_+}{\\sum_{z\u2208\u03a3} (R_E(Q)(z) - R_E(P)(z))_+} & if i \u2260 j.\n\\end{cases}$"}, {"title": "Maintaining Sampling Efficiency", "content": "To maintain the sampling efficiency, we again choose the reweight function for draft distribution to be the same as the reweight function for the target distribution, i.e., $R'_E(Q) = R_E(Q)$. However, the speculative process is designed differently:\n$A(j|i) =\\begin{cases}\nmin(1, \\frac{P(i)}{Q(i)}) & if i = j, \\\\\n\\frac{(P(j) - Q(j))_+}{\\sum_{z\u2208\u03a3} (Q(z) - P(z))_+} & if i \u2260 j.\n\\end{cases}$"}, {"title": "Conclusion", "content": "Our work provides a rigorous theoretical foundation for understanding the trade-off between watermark strength and sampling efficiency in the context of accelerated generation of watermarked tokens from large language models. We prove a no-go theorem, showing that non-trivial trade-offs are inevitable when the vocabulary size is greater than two. To explore these trade-offs, we design algorithms that prioritize either watermark strength or sampling efficiency. Our findings contribute to the development of methods for protecting the intellectual property of language models while leveraging the efficiency of speculative sampling techniques."}, {"title": "Acknowledgments", "content": "ZH and HH were partially supported by NSF IIS 2347592, 2347604, 2348159, 2348169, DBI 2405416, CCF 2348306, CNS 2347617."}, {"title": "Related Works", "content": "Our work lies at the intersection of two active research areas: speculative sampling for accelerated inference and watermarking techniques for language models."}, {"title": "Speculative Sampling for Accelerated Inference", "content": "In the domain of speculative sampling, a common approach is to use a smaller language model as the draft model [16, 5]. Efforts have been made to further increase the overlap between the draft and target models through distillation [52]. Other works focus on modifying the target model itself, such as adding \"look ahead\" tokens [26], introducing new heads to predict future tokens [4], reusing the computation of the large model to achieve a better latency-overlap trade-off for the draft model [19], or using the target model with partial key-value cache as the draft model [35]. Alternative approaches include using document retrieval [45, 9] or n-gram models [28] as the draft model.\nWhen the draft sequence length is greater than one, vanilla speculative sampling is known to be suboptimal. Methods have been proposed to amend the verification process for the draft sequence, verifying the entire sequence at once instead of individual tokens, leading to a longer expected number of accepted tokens [38].\nAn extension of speculative sampling is to change the sequence input to a tree input. While typical language models take a sequence as input, which is a path in the symbol tree space, some works modify the input to be a tree with multiple branches. A single forward pass can then obtain probabilities on multiple branches, gathering more information to help accelerate decoding. This requires modifying the transformer implementation to change the causal attention to tree attention [46, 25, 4, 34]. Speculative sampling can also be used repeatedly, with an additional draft model to accelerate the draft model itself [34].\nOur work is independent of the specific draft model used. While many recent advancements stem from faster and more accurate draft models, our method does not rely on any assumptions about the draft model. A better draft model can always be plugged in to provide faster acceleration.\nThe methods in Section 5 of the main text only consider the basic speculative sampling approach and do not take into account other variants such as verifying the entire sequence, tree verification, or multi-candidacy. However, our ideas can be extended to these variants and still maintain either the watermark strength or the sampling efficiency, as discussed in Appendix E."}, {"title": "Watermarking Techniques for Language Models", "content": "In the domain of watermarking for language models, various approaches have been explored. Some works attempt to edit existing text to embed watermarks [3, 29, 31, 32, 41, 27, 47\u201349, 2, 40, 24]. Others try to incorporate watermarks during the training phase [22, 39, 36, 37].\nMore closely related to our work is the direction of modifying the sampling stage to directly generate watermarked results. Since the pioneering works of Aaronson [1] and Kirchenbauer et al. [13], watermarking techniques have seen significant development.\nTo address the bias introduced by watermarking, researchers have proposed skipping watermarking on low-entropy tokens [21, 43] or accumulating entropy during the generation process and only adding a watermark when the accumulated entropy exceeds a threshold [7]. Hu et al. [12] introduced a framework that includes unbiased reweighting and context code history to ensure that the output distribution is strictly unbiased.\nSubsequently, many variants have been proposed, including multi-bit watermarks [50, 8] and more robust watermarking schemes [30, 20, 14, 51, 15, 11]. Efforts have also been made to search for better watermark detection methods [44, 18].\nOur work builds upon the unbiased watermarking framework of Hu et al. [12] and explores the trade-off between watermark strength and sampling efficiency when integrating watermarking with speculative sampling. To the best of our knowledge, this is the first work to investigate this intersection and provide theoretical insights and practical algorithms for navigating the inherent trade-offs."}, {"title": "Proofs", "content": "Proof of Lemma 3. Let P = Q. Then we have\n$1 = a(P, Q) = E_{E~P_E} \\sum_{i\u2208\u03a3} A(i|i) R'_E(Q)(i)$\nNote that for all i \u2208 \u03a3, A(i|i) < 1, and thus\n$\\sum_{i\u2208\u03a3} A(i|i) R'_E(Q)(i) = \\sum_{i\u2208\u03a3} R'_E(Q)(i) = 1$.\nTherefore, we must have A(i|i) = 1 almost surely for random E and for all i \u2208 \u03a3. Considering the unbiasedness requirement for the final output distribution, i.e.,\n\u2200 P \u2208 \u0394\u03a3, $E_{E~P_E} [AR_E(Q)] = P$,\nwe obtain $E_{E~P_E}[R'_E(Q)] = P = Q$.\nProof of Lemma 4. Let P = Q. Following the proof of Lemma 3, we have A(i|i) = 1 almost surely for random E and for all i \u2208 \u03a3. Therefore,\n$R_E(P) = AR'_E(Q) = R'_E(Q)$.\nSince $R_E(P) = R_E(P)$, we conclude that $R'_E(Q) = R_E(Q)$.\nLemma 8 (A Function Equation). Given n monotonically increasing functions $F_i : [0, 1] \u2192 [0, 1]$ for i \u2208 {1, 2, 3, . . ., n}, i.e., x \u2265 x' $F_i(x) \u2265 F_i(x')$, satisfying\n\u2200 i \u2208 {1, 2, 3, ..., n}, $F_i(0) = 0, F_i(1) = 1$,\n$\\sum x_i = 1 \\implies \\sum F_i(x_i) = 1$,\nwe have $F_i(x) = F_i(x) = x$ for all i \u2208 {1, 2, 3, . . ., n} and x \u2208 [0, 1].\nProof of Lemma 8. We first prove that $F_1(x) = F_2(x)$ for all x \u2208 [0,1]. Let $x_1 = 0, x_2 = 1 - x_3$, and $x_i = 0$ for all i > 4. We obtain\n$F_3(x_3) = 1 - F_2(1 - x_3)$.\nNext, let $x_2 = 0, x_3 = 1 - x_1$, and $x_i = 0$ for all i > 4. This gives us\n$F_1(x_1) = 1 - F_3(1 - x_1)$"}, {"title": "DeltaGumbel Reweight", "content": "In the DeltaGumbel reweight scheme, the watermark code E is a list of || independent and identically distributed standard Gumbel variables. The reweighting function is defined as:\n$R_E(P) := \u03b4_{\u03b1^*}$,\n$\u03b1^* := argmax_\u03b1 {log P(\u03b1) + E(\u03b1)}$\nwhere $\u03b4_{\u03b1^*}$ is the Dirac delta function centered at a*.\nThe U score for the DeltaGumbel reweight is defined as:\n$U = exp(-exp(-E(x))) \u2208 [0, 1]$.\nIf there is no watermark added while generating x, in other word, if the token x is independent with E, then the random U is uniformly distribution in [0, 1].\nThe logarithm of the moment-generating function (MGF) of the U score for the DeltaGumbel reweight is given by:\n$log E[exp(\u03bbU)] = - log \u0393(\u03bb) + log (e^\u03bb \u2212 1)$."}, {"title": "Gamma Reweight", "content": "In the Gamma reweight scheme, the watermark code E is a random bijection from \u03a3 to the set {0, 1, 2, . . ., |\u03a3 \u2212 1|}. The reweighting function is defined as:\n$R_E(P)(t) := A_{E,P}(E(t)) \u2212 A_{E,P}(E(t) \u2212 1)$,\n$A_{E,P}(i) := max\\{\\sum_{t\\in \\Sigma}I(E(t) \\leq i)P(t) - 1, 0\\}$\nThe U score for the Gamma reweight is defined as:\n$U = \\frac{E(x) + \\frac{1}{2}}{|\u03a3|}, U\\in[0,1]$\nIf there is no watermark added while generating x, in other word, if the token x is independent with E, then the random U is uniformly distribution in $\\{\\frac{1}{|\\Sigma|},\\frac{2}{|\\Sigma|},\\cdots, \\frac{|\\Sigma|}{|\\Sigma|}\\}$.\nThe logarithm of the moment-generating function (MGF) of the U score for the Gamma reweight is given by:\n$log E[exp(\u03bbU)] = -log(\\frac{\u03bb|\u03a3|}{|\u03a3| sinh(\\frac{\u03bb}{2|\u03a3|})) + log(e^\u03bb - 1)$\nBoth the DeltaGumbel reweight and Gamma reweight schemes are unbiased [12], meaning that for any distribution P\u2208 \u0394\u03a3, we have:\n$E_{E~P_E} [R_E(P)] = P$.\nThe U scores defined for these reweight schemes are likelihood-agnostic, which means that they do not depend on the original distribution P. This property makes them possibly more robust to perturbations compared to likelihood-based scores such as the LLR score.\nTo compute the P-value for detecting watermarks using the U score, we can substitute the corresponding MGF into Equation (1)."}, {"title": "Extension to Variants of Speculative Sampling", "content": "The analysis and process presented in Section 5 focus on the basic speculative sampling approach, where a single draft token is sampled and then accepted or rejected. Algorithm 1 apply such process multiple times, accepting or rejecting tokens one by one, similar to vanilla speculative sampling.\nRecent developments in speculative sampling have introduced various new techniques, such as verifying the entire sequence, tree verification, or multi-candidacy (see Appendix A for details)."}, {"title": "Broader Impacts", "content": "This paper presents work whose goal is to accelerate the generation speed of existing watermarking methods for large language models. There are several potential positive societal impacts of our work. By making watermarking techniques more practical and efficient, it may encourage their wider adoption. This can help protect the intellectual property rights.\nHowever, there are also potential negative societal impacts to consider. Although our unbiased watermarking approach ensures the validity of the model outputs is not compromised, there is a risk that watermarking techniques could be abused. For example, unbiased watermarks are undetectable, which could enable tracking and surveillance, raising privacy concerns.\nTo mitigate potential negative impacts, it is important that watermarking techniques are used responsibly. This includes transparency about the use of watermarks, obtaining user consent where applicable, and putting safeguards in place to prevent misuse.\nIn conclusion, while our work on accelerating watermarking for language models has the potential to encourage wider adoption and protect intellectual property, it is important to carefully consider and address potential negative societal impacts to ensure the technology is used responsibly and ethically."}, {"title": "Limitation", "content": "Our work makes significant contributions to the field of watermarking and speculative sampling for large language models, but it also has several limitations.\nIn terms of theoretical analysis, the no-go theorem assumes a specific two reweight framework. Although this framework is general, it is possible that other frameworks or methods may lead to different theoretical results. This paper represents the first exploration in this direction, and the two reweight framework is also the first attempt. Future work may discover more powerful frameworks that yield different insights.\nRegarding experimental validation, we use relatively small language models and basic draft model. While our experiments verify the effectiveness of the theory, the acceleration ratio may not represent the state-of-the-art. Speculative sampling techniques have been developing rapidly in recent times. If combined with the latest advances, it should be possible to achieve even higher Average Accepted"}]}