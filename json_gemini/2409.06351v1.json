{"title": "MAGDA: Multi-agent guideline-driven diagnostic assistance", "authors": ["David Bani-Harouni", "Nassir Navab", "Matthias Keicher"], "abstract": "In emergency departments, rural hospitals, or clinics in less developed regions, clinicians often lack fast image analysis by trained radiologists, which can have a detrimental effect on patients' healthcare. Large Language Models (LLMs) have the potential to alleviate some pressure from these clinicians by providing insights that can help them in their decision-making. While these LLMs achieve high test results on medical exams showcasing their great theoretical medical knowledge, they tend not to follow medical guidelines. In this work, we introduce a new approach for zero-shot guideline-driven decision support. We model a system of multiple LLM agents augmented with a contrastive vision-language model that collaborate to reach a patient diagnosis. After providing the agents with simple diagnostic guidelines, they will synthesize prompts and screen the image for findings following these guidelines. Finally, they provide understandable chain-of-thought reasoning for their diagnosis, which is then self-refined to consider inter-dependencies between diseases. As our method is zero-shot, it is adaptable to settings with rare diseases, where training data is limited, but expert-crafted disease descriptions are available. We evaluate our method on two chest X-ray datasets, CheXpert and ChestX-ray 14 Longtail, showcasing performance improvement over existing zero-shot methods and generalizability to rare diseases.", "sections": [{"title": "Introduction", "content": "Radiology holds a critical position in contemporary healthcare, being integral to the treatment and management of most patients. However, the healthcare sector is currently grappling with what has been termed the \"radiologist shortage\" [12]. In the UK, this shortage stands at 29% and is predicted to worsen, reaching 40% within the next four years [15]. This effect is exacerbated in rural hospitals or clinics in less developed regions of the world, where the population per radiologist is much greater [11, 18]. When there is a lack of radiologists, the clinicians"}, {"title": "Methodology", "content": ""}, {"title": "Model Overview", "content": "We propose MAGDA, a multi-agent zero-shot method that can work with expert-crafted disease descriptions to provide transparent decision support. A general overview of the method is shown in Fig. 1. The LLMs used are not fine-tuned and all adaptions to the tasks are performed in-context. The multi-agent system consists of three agents that take over different tasks in the diagnosis procedure:"}, {"title": "Screening Agent", "content": "When diagnosing a specific patient $p \\in P$, this agent is run once for every disease $d \\in D$ that we want to evaluate. It is presented with expert-crafted fine-grained image findings and returns the positive or negative findings present in the image following the given diagnosis guidelines.\n\n$S(G_d, d) \\rightarrow F$,\n\nwhere $G_d$ are the disease guidelines, $d$ is the condition under evaluation, and $F$ are the patient and disease-specific positive and negative image findings. The guidelines $G_d$ can be provided in either an already expert-crafted fine-grained list of disease-specific image findings or an unstructured disease description from which the model can extract these fine-grained image findings. For example, in the case of an enlarged cardiomediastinum, one image finding may be described as \"Abnormal contour of the heart border\". In order to screen the image for the presence or absence of these image findings, we augmented the screening agent with the ability to prompt a CLIP model [10]. Following established works of classification-by-description [9, 13, 16], we task the agent to use contrastive prompting, i.e., prompting the model with both a positive and a negative de-scription. This has shown to be superior to just evaluating the similarity between"}, {"title": "Diagnosis Agent", "content": "The diagnosis agent is again run once per patient and condition under evaluation. It is given the list of findings extracted from the image by the screening agent and returns a positive or negative prediction, including the reasoning for that decision.\n\n$D(F, d) \\rightarrow p, r_d$,\n\nwhere $p$ is the binary disease prediction for patient p and disease d, and $r_d$ is the reasoning for that prediction. It has been shown that LLM reasoning can be significantly improved by chain-of-thought prompting [21], a prompt engineer-ing technique where the model is asked to provide step-by-step reasoning before answering a question. Additionally, to an increase in reasoning capabilities, this reasoning makes the method inherently explainable. As the model provides ex-planations for its predictions, clinicians can use these explanations to evaluate the decision process and increase trust in the model output. Specifically, we ask the model to provide reasoning before answering the question \"Does the patient have [d]?\". We prompt the model to use a specified format to make parsing the model output possible, ending the reasoning process with the sentence: \"Therefore, my answer is: [yes/no].\" Once the various predictions and reasonings have been collected, they are passed to the refinement agent for the final patient diagnosis."}, {"title": "Refinement Agent", "content": "The refinement agent is run once per patient. It is presented with all positive disease predictions and the diagnosis agent's reasonings for these predictions. It returns the final patient prediction.\n\n$RP({(p, r)|d \\in D, p_d \\text{ is positive}}) \\rightarrow {\\d \\in D}$\n\nThe refinement agent is tasked with evaluating the provided reasoning. So far, every disease has been evaluated on its own in order to not overload the agents. At this step in the diagnosis process, inter-dependencies between diseases can be considered. The refinement agent is queried for every disease under evaluation if that disease is present or not and again asked to provide chain-of-thought reasoning for that decision. From the model replies we parse the final patient predictions $p$. \"No Finding\" is predicted if all other disease predictions are negative."}, {"title": "Experimental setup", "content": ""}, {"title": "Datasets and evaluation metrics", "content": "We evaluate our method on two chest X-ray datasets, CheXpert [5] and ChestXRay 14 Longtail [4, 19]. The CheXpert dataset includes manually annotated validation and test sets comprising 200 and 500 patients, respectively. It encompasses 14 different categories, featuring \"No Finding\", 12 pathology labels, e.g., \"Pneumonia\", and a class \"Support Devices\". On CheXpert, we perform multi-label classification. Most comparable methods evaluate using the Area Under the ROC-curve (AUC) metric. As our method generates discrete predictions, threshold-independent metrics, like AUC, cannot sensibly be evaluated. Instead, we report micro and macro F1-score, precision, and recall. The CLIP finding probability threshold 4, which is used to combat the over-prediction of the CLIP model, is set to 0.55 based on experiments on the validation set.\nThe ChestXRay14 Longtail dataset is an extension of the common ChestXRay 14 dataset by adding five additional disease findings, expanding the classifica-tion to 20 categories. These are divided into 7 head classes (most common), 10 medium classes (moderately common), and 3 tail classes (least common). The dataset includes a balanced validation and test set, each offering 15 or 30 images per class, respectively, to ensure comprehensive coverage and evaluation capa-bilities across the spectrum of conditions. We evaluate on this balanced test set with equal number of cases per class. Here, we perform single-label classification and report the accuracy on the three tail classes. In this setting, we prompt the CLIP model without description negation. As the screening and diagnosis agents always perform multi-label classification, we further adapt our refinement agent to decide on exactly one positive prediction."}, {"title": "Implementation details", "content": "The backbone of our method lies in a powerful LLM instantiated in different ways as the various agents. Unless stated otherwise, we"}, {"title": "Results and discussion", "content": "In Table 1, we compare with state-of-the-art zero-shot classification methods CheXzero [16] and Xplainer [9] on the CheXpert test dataset. Because CheXzero is only evaluated on the six competition pathologies in the original paper, we use their public code and model to evaluate on all CheXpert classes. Most state-of-the-art methods only report the AUC, we can therefore only compare with methods with published code and calculate their respective F1-score, precision, and recall. For example, a comparison with Seibold et al. [13] or ELIXR [24] was not possible for that reason. We outperform all comparable methods on zero-shot classification on all metrics except macro precision, where CheXzero has a better score at the expense of a much lower recall. Here, we also compare the guideline-driven approach with the generation of findings by the model itself, showing that the provision of guidelines to our method increases performance. Table 2 shows"}, {"title": "Ablation studies", "content": "We now want to look at the benefits of different aspects of our method. First, in Table 4, we compare the rule-based negation by simply appending a \"no\" before the finding description with the LLM-created finding negation done by the screening agent. We see that the latter results in better performance, highlighting the benefit of descriptions that are more aligned with natural language and thus the style of radiology reports. In Table 5, different approaches for the refinement agent are compared. We compare combinations of chain-of-thought reasoning and including the CheXpert disease graph in tex-tual form [5]. This disease graph models the dependencies between classes, e.g. \"Enlarged Cardiomediastinum\" being a sign of \"Cardiomegaly\". However, both in the case of using chain-of-thought reasoning and not using it, the inclusion of the disease graph decreases the performance."}, {"title": "Conclusion", "content": "In this paper, we presented MAGDA, a novel multi-agent approach that inte-grates clinical guidelines, dynamic vision-language model prompting, and large language model reasoning to address the challenges of diagnostic assistance using LLMs. Our approach leverages the strengths of both LLMs and VLMs, enabling the zero-shot classification of diseases without the need for model retraining or fine-tuning. This guideline-driven methodology not only facilitates accurate di-agnoses from medical images but also introduces a transparent reasoning process that enhances the explainability and trustworthiness of the diagnostic outcomes. Our evaluation on the CheXpert and ChestXray14 LT datasets demonstrates the"}]}