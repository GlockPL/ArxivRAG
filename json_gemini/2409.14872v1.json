{"title": "FedSlate: A Federated Deep Reinforcement Learning Recommender System", "authors": ["Yongxin Deng", "Xiaoyu Tan", "Xihe Qiu", "Yaochu Jin"], "abstract": "Reinforcement learning methods have been used to optimize long-term user engagement in recommendation systems. However, existing reinforcement learning-based recommendation systems do not fully exploit the relevance of individual user behavior across different platforms. One potential solution is to aggregate data from various platforms in a centralized location and use the aggregated data for training. However, this approach raises economic and legal concerns, including increased communication costs and potential threats to user privacy. To address these challenges, we propose FedSlate, a federated reinforcement learning recommendation algorithm that effectively utilizes information that is prohibited from being shared at a legal level. We employ the SlateQ algorithm to assist FedSlate in learning users' long-term behavior and evaluating the value of recommended content. We extend the existing application scope of recommendation systems from single-user single-platform to single-user multi-platform and address cross-platform learning challenges by introducing federated learning. We use RecSim to construct a simulation environment for evaluating FedSlate and compare its performance with state-of-the-art benchmark recommendation models. Experimental results demonstrate the superior effects of FedSlate over baseline methods in various environmental settings, and FedSlate facilitates the learning of recommendation strategies in scenarios where baseline methods are completely inapplicable. Code is available at https://github.com/TianYaDY/FedSlate.", "sections": [{"title": "I. INTRODUCTION", "content": "IVEN the significant influence of users' instant reac- tions to recommended content on their future behavior, research in the realm of content recommendation systems has increasingly adopted deep reinforcement learning (DRL) strategies. These strategies aim to optimize the balance be- tween immediate user engagement and long-term retention [1]\u2013[3]. Additionally, recent advancements in advertising rec- ommendation systems have integrated reinforcement learning to achieve a balance between generating ad revenue and minimizing negative user experiences [4]. However, these systems often overlook an essential aspect: user behavior is not isolated but affected by recommendations from vari- ous platforms, indicating interdependencies between a user's activities across different services. An obvious solution to leverage these behavioral correlations is to consolidate user data from multiple sources for the development of a unified model. Nevertheless, the introduction of stringent data privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union, the Personal Data Protec- tion Act (PDPA) in Singapore, and the California Consumer Privacy Act (CCPA) in the United States, raises significant legal challenges. Directly employing user data could infringe upon privacy rights [5], [6]. Additionally, the communication overhead [7]\u2013[9] poses a barrier to the straightforward imple- mentation of centralized learning approaches. To circumvent these obstacles while harnessing the coherence in user behav- ior across platforms, we advocate for the adoption of federated learning (FL) techniques [10] to refine reinforcement learning- based recommendation algorithms.\nIn numerous contexts, the correlation between user behav- iors yields substantial benefits. Within the financial sector, for instance, customers often engage with both stock trading and online payment services. A collaborative model trained by these services can effectively ascertain a customer's risk profile, investment patterns, and spending behaviors, thereby facilitating tailored financial product recommendations that meet individual needs. Similarly, in the realm of online advertising, user interactions with multiple platforms reveal interconnected behaviors. A user may explore health foods on a social network while simultaneously shopping for wellness products on an e-commerce site. Utilizing FL, platforms can collectively develop a model that captures the user's interests and buying inclinations, enhancing the precision of targeted advertising. Consequently, a recommendation system under- pinned by FL presents an approach that optimizes privacy, minimizes communication overhead, and improves long-term value for users.\nRecommendation systems commonly adopt a slate-based approach, wherein multiple items are simultaneously sug- gested to the user, allowing them to select and view their preferred item. This presents a significant challenge for the direct application of reinforcement learning (RL) due to the large action space involved. SlateQ [1] is a recommendation algorithm that utilizes the slate decomposition technique to"}, {"title": "II. RELATED WORK", "content": "Recommender systems are a critical type of information filtering system that leverages user preference and behavior analysis to provide personalized suggestions [19]. These sys- tems are extensively applied in various sectors, including e-commerce, social media, and entertainment platforms like mu- sic and video streaming, aiming to enhance content discovery and improve the overall user experience. Supervised learning (SL) techniques are commonly utilized in these systems to perform predictive and recommendatory functions, relying on patterns and rules derived from labeled training data. In this realm, training datasets, which include users' historical interactions and their corresponding feedback or ratings, are instrumental. Among the prevalent SL-based methodologies for recommender systems, collaborative filtering (CF) stands out [20], [21]. User-based CF [22] suggests items by identi- fying similarities between users' past behaviors, positing that users with comparable preferences are likely to be interested in similar items. Conversely, item-based CF [23], recommends based on item similarities. These CF methods are favored for their simplicity and proven effectiveness. Content-based recommendations [24] represent another widespread SL ap- proach, employing item characteristics and user preferences to formulate suggestions. For example, a movie recommendation system may use a content-based method to recommend movies by considering aspects such as genre, actors, and directors, thus predicting a user's potential interests. Furthermore, SL- based recommender systems may integrate various machine learning models, including decision trees [25], support vector machines (SVM) [26], and deep neural networks (DNN) [27]. These models enhance the recommendation process by adapting to diverse dataset features and characteristics while learning from users' preferences and behavior patterns during training.\nSL-based recommender systems have shown proficiency in short-term prediction tasks; however, incorporating rein- forcement learning (RL) has been identified as critical for long-term prediction challenges [28]\u2013[30]. RL, a machine learning paradigm, seeks to establish optimal behavioral poli- cies through environmental interactions [31], [32] and dis- tinguishes itself by concentrating on goal-directed decision- making, employing a trial-and-error process with a rewards system. This method excels in scenarios requiring foresight, such as financial investment [33] and complex planning, due to its adeptness at managing delayed rewards. In the medical do- main, where uncertainty and dynamic conditions are prevalent, like ventilator management [34], [35], RL's attributes prove exceptionally beneficial. Within RL-based recommendations, there are \u201cmodel-based\u201d and \u201cmodel-free\u201d methods; our focus is on the \"model-free\" category, which is straightforward to implement and yields superior long-term performance. Before this approach, DRN [36] implemented a deep Q-network (DQN) to create user profiles and an activity score. Subse- quently, the social attentive deep Q-network (SADQN) [37] enhanced DQN with an attention mechanism to leverage social influences. Diverging from these, our contribution, FedSlate, leverages cross-platform user performance similarities. While some studies have adopted policy gradient methods, such as the Monte Carlo-based REINFORCE algorithm for large-scale recommendation environments [38], the SlateQ algorithm [1] is particularly influential in our approach. It decomposes the Q-value of a recommendation slate into individual item Q-values, effectively managing extensive action spaces and offering three item-wise Q-value-based selection strategies. Notwithstanding, existing RL-based systems primarily opti- mize for single-platform performance, neglecting the multi- platform influences on real-world users. To address this gap, we propose the incorporation of FL paradigms into RL-based recommendation systems.\nFL involves training statistical models directly on devices to develop a joint model capable of generating data across distributed nodes [10]. Traditional distributed learning meth- ods generally presuppose that local data samples are indepen- dently and identically distributed (IID); however, FL typically operates under the premise that data among clients is non-IID [39], [40]. Prior research has primarily utilized RL to enhance FL's performance [41]. For instance, [41] employs a Deep Q- Learning (DQL) strategy to select devices for participation"}, {"title": "III. PROBLEM DEFINITION", "content": "In this section, we introduce an augmented Markov Decision Process (MDP) model tailored for the single-user, multi- platform context. This model captures the dynamics wherein platforms employ recommendation systems to curate slates of content. Users engage with these slates by selecting an item-or opting out and post-consumption, decide whether to seek additional recommendations or end their session. It is important to note that users may transition across platforms in pursuing content that piques their interest, a pattern that closely reflects real-world user behavior. User responses to content are multifaceted, encompassing metrics such as brows- ing duration, \"likes\", and comments. However, for the sake of a generalized model, we limit our focus to user engagement as the singular metric of reward. Subsequently, we outline the assumptions underpinning our problem, some aligning with the SlateQ framework [1] and others specific to the single-user, multi-platform scenario. We conclude this section by detailing a precise formalization of our federated reinforcement learning recommendation problem."}, {"title": "A. An Extended MDP Model for Slate Recommendation", "content": "The recommendation and user interaction behaviors within a single platform are aptly modeled by an MDP, characterized by states $S$, actions $A$, a reward function $R$, a transition kernel $P$, and a discount factor $\\gamma$ [1]. We will now elucidate the critical elements of this model:\n\u2022 The state $S$ implements the user's condition, comprising both observable attributes such as age, gender, and self- reported interests, and historical interactions including prior browsing activity and responses to earlier recom- mendations."}, {"title": "B. Necessary Assumptions", "content": "In our federated reinforcement learning recommendation problem, we make the following assumptions:\n\u2022 A1: A user selects only one item at a time (or may choose not to select, represented as $I$ for a null item).\n\u2022 A2: Transitions depend solely on the selection. Specifi- cally, the user's state changes, and a reward (user engage- ment) is generated only when the user consumes item $i$. Additionally, while a user engages with a platform, the states of other platforms remain frozen.\n\u2022 A3: There is interconnectedness between the user's be- haviors on different platforms, and the impact of a single platform on the user is \"cross-platform\"."}, {"title": "C. Recommendation Problem", "content": "After extending the original MDP model, we can formally define our recommendation problem based on A1-A5. The existing platforms $\\alpha$ and $\\beta$ take turns randomly recommending slates to customers and recording transitions. This results in a series of transitions $D_\\alpha = \\{(s_\\alpha, A_\\alpha, s'_\\alpha, r_\\alpha)\\}$ for agent $\\alpha$ and transitions $D_\\beta = \\{(s_\\beta, A_\\beta)\\}$ for agent $\\beta$, where $D_\\alpha$ and $D_\\beta$ are one-to-one correspondence. Our goal is to learn a joint policy $\\pi^*$ that, based on $s_\\alpha$ and $s_\\beta$, maximizes the lifetime value (LTV) across all platforms. It should be noted that platform $\\beta$ does not record the user's response, i.e., $D_\\beta$ does not contain $r_\\beta$, which deviates from the previous MDP model. We adopt this setting because certain platforms may not have direct access to user feedback on recommended content (although the platform's impact on the user is real). User preferences or aversions may be reflected on other platforms. We aim to demonstrate the friendliness of FedSlate towards these \"unavailable feedback\" platforms-even if a platform cannot directly update its recommendation strategy based on user feedback, it can still benefit from performance improvements in the federation."}, {"title": "IV. OUR FEDSLATE METHOD", "content": "In this section, we will provide a detailed description of our FedSlate method. We utilize the SlateQ algorithm to assist us in evaluating the value of recommended content and tracking user feedback over the long term. Therefore, we will begin by briefly introducing the original SlateQ algorithm. Subsequently, we will present the components of our algorithm, followed by a description of the specific details of the algorithm. Lastly, we will propose an extended version of our algorithm to address situations where team rewards are excessively sparse."}, {"title": "A. SlateQ Algorithm", "content": "In Section 3.1, we presented the MDP model for the recommendation problem. In the case of a single user and a single platform MDP, the original SlateQ algorithm aims to find an optimal policy $\\pi^*$ that satisfies $\\pi^*(s) = argmax_{A \\in A}Q^*(s, A)$. However, in Slate recommendation problems, the action space becomes extremely large, resulting in excessive computational and time resource requirements. Specifically, if we attempt to select $k$ items from a set of $I$ items to form a slate and consider the impact of the position of recommended items within the slate on user feedback, the action space would be of size $A^I_k$.\nTo address this issue, SlateQ decomposes $Q^*(s, A)$ and rep- resents slate-level Q-values as item-level Q-values $Q^*(s, i)$, significantly reducing the agent's action space. The decompo- sition is achieved using the following formula:\n$Q^{\\pi}(s, A) = \\sum_{i\\in A}P(i|s, A)Q^{\\pi}(s, i)$                                     (5)\nThe decomposed Q-values can be updated using a simple Temporal Difference (TD) method:\n$Q^{\\pi}(s,i) \\leftarrow \\alpha(r+\\gamma\\sum_{j \\in A'}P(j|s', A')Q^{\\pi}(s', j))+(1-\\alpha)Q^{\\pi}(s,i)$   (6)\nTo fully satisfy the requirements of Q-learning, it is only necessary to introduce the usual maximization step:\n$Q(s,i) \\leftarrow \\alpha(r+\\gamma max_{A' \\in A} \\sum_{j \\in A'} P(j|s', A')Q(s', j))+(1-\\alpha)Q(s,i)$   (7)\nPlease note that SlateQ assumes the user choice model $P(i|s, A)$ is known. Models such as MNL, CL, and cascade can easily be learned using user response data, and this does not depend on LTV.\nSlateQ offers multiple strategies to select recommended slates based on item Q-values. We consider the trade-off be- tween computational and time resources and the effectiveness of the strategy, and we will only provide a detailed explanation of the Greedy optimization approach. A simple approach to utilizing item Q-values in slate construction is to use the Q-values as item scores, sort the items in descending order of scores, and select the top $k$ items to form the slate. However, this approach, known as the Top-k method, fails to capture the influence of the first $L - 1$ items on the $L^{th}$ slot (for $1 < L < k$). Greedy optimization differs from the afore- mentioned method as it updates the item scores based on the current partial slate. For example, given $A' = \\{i_{(1)}, ..., i_{(L-1)}\\}$ of size $L - 1 < k$, the $L^{th}$ item is selected based on the maximum marginal value it provides:\n$\\argmax_{i\\notin A'} v(s,i)Q(s, i) + \\sum_{l < L}v(s, i_{(l)})Q(s, i_{(l)})$ / $v(s, i) + v(s, I) + \\sum_{l < L}v(s, i_{(l)})$     (8)"}, {"title": "B. The FedSlate Algorithm", "content": "In this section, we will introduce our FedSlate algorithm in a bottom-up manner, starting from some necessary components."}, {"title": "V. EXPERIMENTAL SETUP", "content": "In this section, the RecSim platform is utilized to construct a simulation environment aimed at evaluating the efficacy of our FedSlate algorithm. Specifically, detailed information re- garding the simulation environment is provided. Subsequently, a comparative analysis is conducted between our algorithm and the SlateQ method, to ascertain whether agent a demon- strates enhanced performance subsequent to its participation in the FL process, as compared to its individual learning performance. A metric is proposed to assess this particular aspect. Furthermore, a comparison is made between FedSlate and a purely random recommendation approach, in order to ascertain if agent $\\beta$ (as agent $\\beta$ lacks access to user feedback and therefore cannot optimize the recommendation strategy using conventional methods) can derive benefits from the federated setting. By considering these two aspects, we aim to"}, {"title": "B. Algorithm Evaluation", "content": "We evaluate our algorithm in a simulated environment,the schematic representation of the interactive process between the environment and the agent is depicted as illustrated in Fig.4. First, let's define the states, actions, and rewards.\n\u2022 States: The environment state observed by Agent $\\alpha$ consists of the user's observable state (with a count of 1) and the features of candidate documents (with a count of N). Since Platform A has access to user feedback infor- mation, we incorporate the user's historical engagement records into the observable state. Specifically, we include the user's previous 5 engages in the state. Therefore, the state $\\alpha$ corresponding to Platform A is a tensor of size [1+5xn+N], where n represents the slate size we set. Platform B, which lacks access to user feedback informa- tion, does not include the user's historical engagement records in the observed environment state. Hence, the state $\\beta$ corresponding to Platform B is a tensor of size [1 + N], containing only the current user's observable state and the features of candidate documents.\n\u2022 Actions: The actions for both Agent $\\alpha$ and Agent $\\beta$ are similar. They involve recommending a slate of content determined by the algorithm. The actions are essentially an integer tensor of size [n], representing the specific item IDs that form the slate.\n\u2022 Rewards: As our objective is to maximize long-term user satisfaction, we employ cumulative user engagement as"}, {"title": "VI. CONCLUSION", "content": "To tackle the complexity of integrating user privacy data across diverse platforms into recommendation systems, we introduce a novel reinforcement learning algorithm, designated as FedSlate. This algorithm is designed to develop superior recommendation tactics collaboratively across multiple agents while safeguarding user privacy. Utilizing RecSim, we estab- lished a multi-platform recommendation simulation to assess how our algorithm benefits various participants within Feder- ated Learning (FL). Our research indicates that FedSlate ef- fectively resolves the challenges of cross-platform learning in recommendation systems without necessitating the exchange"}]}