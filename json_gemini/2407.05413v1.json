{"title": "SBORA: Low-Rank Adaptation with Regional Weight Updates", "authors": ["Lai-Man Po", "Yuyang Liu", "Haoxuan Wu", "Tianqi Zhang", "Wing-Yin Yu", "Zeyu Jiang", "Kun Li"], "abstract": "This paper introduces Standard Basis LORA (SBORA), a novel parameter-efficient fine-tuning approach for Large Language Models that builds upon the pioneering works of Low-Rank Adaptation (LORA) and Orthogonal Adaptation. SBORA further reduces the computational and memory requirements of LoRA while enhancing learning performance. By leveraging orthogonal standard basis vectors to initialize one of the low-rank matrices, either A or B, SBORA enables regional weight updates and memory-efficient fine-tuning. This approach gives rise to two variants, SBORA-FA and SBORA-FB, where only one of the matrices is updated, resulting in a sparse update matrix AW with a majority of zero rows or columns. Consequently, the majority of the fine-tuned model's weights (Wo + AW) remain unchanged from the pre-trained weights. This characteristic of SBORA, wherein regional weight updates occur, is reminiscent of the modular organization of the human brain, which efficiently adapts to new tasks. Our empirical results demonstrate the superiority of SBORA-FA over LORA in various fine-tuning tasks, including commonsense reasoning and arithmetic reasoning. Furthermore, we evaluate the effectiveness of QSBORA on quantized LLaMA models of varying scales, highlighting its potential for efficient adaptation to new tasks. Code is available at https://github.com/cityuhkai/SBORA", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) and diffusion models have become essential components of natural language processing [1,25] and multimodal AI applications [14,16]. Full fine-tuning (FFT) has been shown to significantly improve their performance in various downstream tasks [28] or introduce new concepts. However, fine-tuning pre-trained models, especially LLMs, by updating all parameters is computationally costly. As depicted in Fig. 1(a), FFT involves directly updating the high-dimensional pre-trained weight matrix Wo.\nTo address this issue, parameter-efficient fine-tuning (PEFT) methods [9] have gained momentum, focusing on updating only a small fraction of parameters, like adapter [7,9,19], prompt tuning [13,21,26]. Among these methods,"}, {"title": "2 Related Works", "content": "In deep learning, fine-tuning is a fundamental concept that enables leveraging pre-trained models as a starting point for new tasks [7,9]. This approach is based on the idea that a model trained on one task can be adapted to perform well on another related task."}, {"title": "2.1 Full Fine-Tuning (FFT)", "content": "FFT involves updating all of the parameters of a pre-trained model on a new dataset for a specific task as shown in Fig. 1(a). This process adjusts the model's weights to fit the new task, leading to improved performance. However, FFT has several drawbacks. For instance, it requires storing and deploying many copies of large models for different tasks, which can be storage-intensive. Additionally, FFT is computationally expensive, especially for LLMs, and can be time-consuming."}, {"title": "2.2 Parameter-Efficient Fine-Tuning (PEFT)", "content": "To address the limitations of FFT, researchers have proposed PEFT techniques [9]. These methods adapt only a small subset of a model's parameters to a new task, aiming to achieve comparable performance to FFT while reducing computational and memory requirements. Early approaches include the use of adapter layers, which add small, trainable modules to each Transformer layer. Another approach is prefix tuning [15], which optimizes a few continuous \"prefix\" vectors prepended to the input sequence. Both of these approaches significantly reduce the memory and computation needed to fine-tune large models. However, they also have their limitations, such as extra inference latency in the case of adapter layers, and difficulty in optimization for prefix tuning."}, {"title": "2.3 Low-Rank Adaptation (LoRA)", "content": "LORA [10], a prominent PEFT method, has garnered significant attention due to its effectiveness and efficiency. As depicted in Fig. 1(b), the fundamental concept underlying LoRA involves approximating full-rank updates with low-rank updates within the FFT domain. Specifically, given a pre-trained parameter matrix Wo \u2208 Rdxk, LoRA utilizes two low-rank matrices, A \u2208 Rr\u00d7k and B\u2208 Rdxr, to compute the weight update \u2206W \u2208 Rd\u00d7k. The output vector h \u2208 Rd\u00d71 of a LoRA linear layer with input x \u2208 Rkx1 can be expressed as\n$$h = Wox + \u2206Wx = Wox + BAx$$\nLORA ensures AW is initialized to zero by initializing A with a uniform distribution and B with zero. The low-rank decomposition of AW into BA indicates that the rank of AW is at most r, significantly lower than full-rank updating in FFT, with r being much smaller than the minimum of d and k.\nLORA's low-rank updating approach has demonstrated comparable performance to full-rank updating in tasks like text classification and instruction tuning. It reduces memory usage and computational requirements by decreasing the number of trainable parameters needed. LoRA injects low-rank matrices directly into existing layers without adding extra layers, resulting in no additional inference latency. Switching out different LoRA modules is easy, simplifying deployment in multi-task scenarios. Empirical studies show that LoRA can match or exceed FFT performance using only a fraction of the total parameters, reducing the need for high-performance hardware."}, {"title": "2.4 Variants of LoRA", "content": "LORA has sparked significant research in PEFT methods, including its own variants such as QLORA [4], QA-LORA [29], LongLoRA [3], S-LORA [23], LQ-LORA [6], MultiLoRA [27], LORA-FA [32], Tied-LoRA [22], GLORA [2], and DoRA [17]. QLORA is an industry-standard technique for PEFT of LLMs, it employs 4-bit quantization on pretrained weights and trains LoRA modules on this quantized representation. Techniques like 4-Bit NormalFloat (NF4) Format, Double Quantization, and Paged Optimizers further minimize memory usage. QA-LORA reduces the computational burden with group-wise quantization. LongLoRA enables fine-tuning for longer context lengths using sparse local attention. S-LORA presents a scalable strategy for deploying multiple LoRA modules efficiently. LQ-LORA refines the quantization scheme for improved performance. MultiLoRA handles complex multi-task learning, LoRA-FA reduces memory overhead, and Tied-LoRA leverages weight tying. GLORA adapts both weights and activations, and DoRA decomposes weights for enhanced learning capacity. Recently, Orthogonal Adaptation enables efficient merging of customized models without sacrificing fidelity or incurring additional computational costs."}, {"title": "3 Standard Basis Low-Rank Adaptation (SBORA)", "content": "SBORA, inspired by Orthogonal Adaptation [20], utilizes a predefined orthogonal basis OA to generate orthogonal subspaces. These subspaces, represented as low-rank projection-down matrices Ai, enable independent LoRA fine-tuning for multi-concept customization in diffusion models. The linear layer combining c custom concepts, given a pre-trained weight Wo, can be expressed as\n$$Linear(x) = (Wo + \\sum_{i=1}^{C} \\lambda_{i} B_{i}A_{i})x$$\nwhere i denotes the index of the i-th LoRA, and di are scalar factors determined through empirical tuning. The input is a column vector x \u2208 Rk\u00d71. A\u00bf \u2208 Rr\u00d7k matrices are constructed by selecting r non-overlapping orthogonal basis vectors from the shared orthogonal basis OA \u2208 Rk\u00d7k. OA consists of k orthogonal basis vector op \u2208 R1\u00d7k, where p = 1, 2, ..., k. It can be represented as\n$$0A = \\begin{bmatrix} o_{1} \\\\ o_{2} \\\\ : \\\\ o_{k} \\end{bmatrix} \\text{with } o_{p}o_{q}^T = 0 \\text{ if } p \\neq q$$\nTo maintain orthogonality between projection-down matrices, these basis vectors must be non-overlapping from OA, ensuring ApA = 0 if p \u2260 q. And to minimize crosstalk, BjAjx = 0 if j \u2260 i when x belongs to subspace Ai. During fine-tuning, the orthogonal matrices A\u00bf are frozen while updating the projection-up matrices Bi to learn different concepts."}, {"title": "3.1 Orthogonal Standard Basis", "content": "The orthogonal adaptation approach offers not only effective multi-concept merging but also memory efficiency. During LoRA fine-tuning, the A, matrices are frozen, which significantly reduces memory requirements for gradient and intermediate activation storage. This is similar to the LoRA-FA [32], where projection-down matrix A is randomly initialized and then frozen during training. However, we can further improve this orthogonal adaptation idea by utilizing a standard basis with one-hot vectors as basis vectors. In this case, the orthogonal basis OA becomes a k\u00d7k identity matrix I. Specifically, OA can be represented as:\n$$OA = I = \\begin{bmatrix} e_{1} \\\\ e_{2} \\\\ : \\\\ e_{k} \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & . & 0 \\\\ & & & \\\\ 0 & 0 & . & 1 \\end{bmatrix}$$\nwhere epe = 0 if p \u2260 q. The standard basis vector ek \u2208 R1\u00d7k are one-hot row vectors, each having a single non-zero entry of 1 at index p. For instance, ep can be represented as\n$$e_{p} = [0 ... 0 \\underset{p}{\\uparrow} 1 0 ... 0]$$\nwhere the non-zero component with value one at the index of p. In SBORA-FA, we don't need to randomly pre-generate an orthogonal-basis matrix OA. Instead,"}, {"title": "3.2 Regional Weight Update", "content": "An interesting property of the proposed SBORA is that the merged weight matrix W' of the fine-tuned model is only regionally updated, with most of the weights remaining unchanged from the original pre-trained weight Wo as depicted in Fig. 2. Specifically, the fine-tuned weight W' of SBORA-FA can be represented as:\n$$W' = Wo + \u2206W = Wo + BAsb$$\nIn which, the update matrix \u2206W = BAsb is very sparse, with most of the row having zero weights due to the one-hot nature of the standard basis subspace matrix Ast as shown in the upper part of Fig. 2. For example, when r = 2 and k = 4 with Asb = [e1 e4]T, the AW will only have two non-zero column as\n$$\\begin{aligned} \\Delta W &= BA_{sb} =\\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\\\ b_{31} & b_{32} \\\\ b_{41} & b_{42} \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\\\ &= \\begin{bmatrix} b_{11} & 0 & 0 & b_{12} \\\\ b_{21} & 0 & 0 & b_{22} \\\\ b_{31} & 0 & 0 & b_{32} \\\\ b_{41} & 0 & 0 & b_{42} \\end{bmatrix} \\end{aligned}$$\nThe fine-tuned weight W' is given by\n$$W' = Wo + \\Delta W = \\begin{bmatrix} w_{11} + b_{11} & w_{12} & w_{13} & w_{14} + b_{12} \\\\ w_{21} + b_{21} & w_{22} & w_{23} & w_{24} + b_{22} \\\\ w_{31} + b_{31} & w_{32} & w_{33} & w_{34} + b_{32} \\\\ w_{41} + b_{41} & w_{42} & w_{43} & w_{44} + b_{42} \\end{bmatrix}$$\nFor SBORA-FB, when r = 2 and d = 4 with Bsb = [e1 e4], the AW will only have two non-zero row as\n$$\\begin{aligned} \\Delta W &= B_{sb}A = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} b_{11} & b_{12} & b_{13} & b_{14} \\\\ b_{12} & b_{22} & b_{23} & b_{24} \\end{bmatrix} \\\\ &= \\begin{bmatrix} b_{11} & b_{12} & b_{13} & b_{14} \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ b_{21} & b_{22} & b_{23} & b_{24} \\end{bmatrix} \\end{aligned}$$\nThe fine-tuned weight W' is given by\n$$W' = Wo - \\Delta W = \\begin{bmatrix} w_{11} + b_{11} & w_{12} + b_{12} & w_{13} + b_{13} & w_{14} + b_{14} \\\\ w_{21} & w_{22} & w_{23} & w_{24} \\\\ w_{31} & w_{32} & w_{33} & w_{34} \\\\ w_{41} + b_{21} & w_{42} + b_{22} & w_{43} + b_{23} & w_{44} + b_{24} \\end{bmatrix}$$\nIn these examples, only half of the weight matrix W' updated, specifically two columns or two rows (r = 2), while the remaining weights remain unchanged. In practice, r < min(k, d), therefore only a small portion of the original weights will be updated. More details of the regional weight update property of SBORA-FA and SBORA-FB can be visualized in Fig. 2. It resembles the localized learning process observed in neuroscience, where specific cognitive functions are associated with distinct brain regions. SBORA efficiently adapts to new tasks by updating specific rows of the weight matrix, preserving existing knowledge, similar to how the brain reorganizes and refines neural connections in response to new experiences."}, {"title": "4 Complexity Analysis of SBORA", "content": "The implementation of SBORA offers a high degree of flexibility, and we propose sampling-based multiplication, sampleMul, to achieve lower memory and computational requirements. Essentially, both SBORA-FA/FB can be implemented similarly to conventional LoRA through matrix multiplication, with just frozen the standard basis matrix Asb/Bsb. As a result, the computational complexity and memory usage of SBORAs are similar to those of LoRA. However, we can"}, {"title": "5 Experiment", "content": "We conducted a comprehensive evaluation of SBORA's effectiveness across a range of tasks. To begin with, we compared the performance of SBORA-FA and SBORA-FB to that of several other well-known PEFT methods by fine-tuning LLMs, specifically LLaMA-7B, on both commonsense reasoning tasks and arithmetic reasoning tasks. Additionally, given the recent release of LLaMA3 [18], we expanded our evaluation to assess SBORA's performance using LLaMA3-8B. Furthermore, recognizing the growing interest in quantized models, we also evaluated SBORA on the quantized versions of LLaMA-7B LLaMA-13B and LLaMA3-8B models, utilizing the techniques introduced in QLORA [4]."}, {"title": "5.1 Evaluating SBORA on Commonsense Reasoning Tasks", "content": "We evaluated SBORA-FA/FB against LoRA, and DoRA approach on the LLaMA-7B and LLaMA3-8B language models for commonsense reasoning tasks. We followed the evaluation method introduced in DoRA [17] and LLM-Adapter [11] which used a comprehensive training dataset created by aggregating eight tasks, evaluating models on individual testing datasets for each task. In addition, the results of GPT-3.5 are included for reference as shown in Table 3.\nFor a comprehensive comparison, we initially fine-tuned models with SBORA-FA and SBORA-FB, both using a double rank of 64 to maintain a nearly identical number of trainable parameters (TP) as LoRA (rank=32), with only additional storage for standard basis indices. We then conducted experiments with rank"}, {"title": "5.2 Evaluating SBORA on Arithmetic Reasoning", "content": "We also evaluated the effectiveness of SBORA on the Arithmetic Reasoning task using the LLaMA-7B and LLaMA3-8B models. The fine-tuning and evaluation datasets are constructed following the dataset settings in LLM-Adapters [11]. We performed tests with ranks 32 and 64. As the training set for arithmetic reasoning is smaller (10k) compared to the commonsense reasoning training set,"}, {"title": "5.3 QSBORA Evaluation on MMLU", "content": "We explored the effectiveness of SBORAs in the memory-efficient QLORA framework [4]. QLORA utilizes techniques including 4-bit NormalFloat (NF4) double quantization, gradient checkpointing, and paged optimizer to reduce memory usage. Following the experiments in QLoRA, we fine-tuned the NF4 quantized"}, {"title": "6 Conclusion", "content": "We introduce SBORA, a novel approach for parameter-efficient fine-tuning of LLMs. Our proposed SBORA-FA and SBORA-FB methods utilize the low-rank projection-down and projection-up matrices constructed from standard basis vectors, leveraging one-dimensional index representations and sample-based multiplication. This innovative approach reduces memory and computational requirements while enhancing learning capacity, outperforming LoRA and matching the performance of DoRA. Empirical results demonstrate that SBORA-FA is the preferred method for single-task LLM fine-tuning, conserving activation memory while achieving superior performance.\nThe SBORA approach enables regional weight updates, preserving most of the pre-trained model weights while efficiently adapting to new tasks. This localized learning process draws parallels with the modular organization of the brain, where distinct cognitive functions are localized to specific brain regions. This analogy highlights the potential of SBORA to inspire AI architectures that mimic the efficiency and adaptability of biological neural systems.\nSBORA holds immense potential for further development, particularly in multi-task training. The introduction of Multi-SBORA would create a powerful"}]}