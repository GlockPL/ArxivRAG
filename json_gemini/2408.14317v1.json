{"title": "Claim Verification in the Age of Large Language Models: A Survey", "authors": ["Alphaeus Dmonte", "Roland Oruche", "Marcos Zampieri", "Prasad Calyam", "Isabelle Augenstein"], "abstract": "The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the development of automated claim verification systems. Several deep learning and transformer-based models have been proposed for this task over the years. With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based approaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG). In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs. We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning. Finally, we describe publicly available English datasets created for this task.", "sections": [{"title": "Introduction", "content": "False information is widely present on social media and on the Web motivating the development of automated fact ver-ification systems (Guo, Schlichtkrull, and Vlachos 2022). The introduction of LLMs has provided malicious actors with sophisticated ways of creating and disseminating false information. Recent election cycles saw a large number of claims spread across both social media and news platforms alike (Dmonte et al. 2024). Similarly, during the COVID-19 pandemic, many claims were spread across social media platforms. Many of these claims were factually inaccurate which led to the spread of misinformation (Zhou et al. 2023). Fact-checking potentially false claims is an essential mod-eration task to reduce the spread of misinformation. Several organizations such as FactCheck, PolitiFact, NewsGuard, and Full Fact perform manual fact-checking to verify claims in different domains. However, this has been regarded as a laborious task that requires domain expertise (Adair et al. 2017; Hanselowski 2020) and it has become less and less feasible due to the sheer volume of misinformation that can be generated by humans and AI models. Automated fact-checking has become an increasingly popular approach to verify the veracity of claims in a given text. There are sev-eral steps involved in the fact-verification pipelines, but the three main components are claim detection, evidence re-trieval, and veracity prediction. Models used in these steps have followed the general methodological developments of the field and we have thus observed an increase in the use of LLMs for claim verification (Zhang and Gao 2023; Wang and Shu 2023; Quelle and Bovet 2024). Figure 1 shows an example claim veracity scenario and compares the architec-tural differences between NLP-based (traditional) and LLM-based claim verification systems. Compared to traditional systems which use NLP-based models, fact verification are less prone to error propagation, and provide explainable/in-"}, {"title": "Search Criteria", "content": "We search various well-known repositories of scientific ar-ticles to collect the papers that serve as primary sources for this survey. We apply queries including LLMs, claim verification, fact verification and related keywords on these repositories. We focus primarily on the ACL Anthology, the ACM Digital Library, and IEEE Xplore, and proceedings of related conferences such as AAAI and IJCAI. We further search on Scopus, Springer Link, and Science@Direct, and ArXiv. We collected and reviewed over 100 papers based on our search terms and filter papers based on the following search criteria: (i) since the scope of our review is on text-based LLM claim verification systems, we omit papers re-lated to multimodal approaches (e.g., text-to-images), our papers from other modalities in claim verification (e.g., im-ages, graph-based systems). (ii) since our review focuses on LLM-based systems for claim veracity, we omit papers re-lated to claim identification, claim detection, and/or detect-ing LLM-generated content. Based on our filtering criteria, we collected a total of 49 papers related to LLM-based ap-proaches for veracity labeling. The papers on topic LLM-based veracity labeling have been published primarily in the ACL Anthology and ACM Digital Library, but also in other repositories such as the IEEE Xplore. We release this curated list to be publicly available."}, {"title": "Claim Verification Pipeline", "content": "Figure 2 shows an archetypal claim verification pipeline consisting of the following modules: claim detection, claim matching, claim check-worthiness, document/evidence re-trieval, rationale/sentence selection, veracity label predic-tion, and explanation/justification generation. Many pro-posed systems make use of only some of these modules. The following subsections describe each of these modules in de-tail. Finally, as described in Panchendrarajan and Zubiaga (2024), performance evaluation in these sub-tasks is usually carried out using well-established automatic evaluation met-rics used in text classification such as Precision, Recall, and F-score. Claim Detection Input texts may contain one or more statements but not all statements are claims. Given the input text, a claim detection module is designed to identify all the statements containing a claim. For example, the statement 'I loved the movie Oppenheimer.' is an opinion, whereas the statement 'The COVID-19 pandemic started in Texas.' con-tains a claim. Check-worthy Claim Identification Not all the identified claims are check-worthy. In the check-worthy claim identi-fication sub-task, given an input claim, the task is to identify the claims that include real-world assertions and that may need to be verified (Hassan, Li, and Tremayne 2015; Nakov et al. 2021). This is a subjective task and it relies on the factors like popularity of the claim, public interest in deter-mining the veracity of the claim, etc. For example, the claim 'The President met the State Governor to discuss the infras-tructure deal.' is less check-worthy than the claim 'Drinking salt water cures COVID.'. Claim Matching The identified check-worthy claims can be matched to the previously fact-checked claims. Given an input claim and a database of previously fact-checked claims, claim matching is used to determine if the input claim is previously fact-checked and exists in the database (Shaar et al. 2020; Nakov et al. 2021). This can help avoid the next steps in the pipeline and a veracity label can be pre-dicted directly. Document/Evidence Retrieval If the input claim does not exist in the database of fact-checked claims, the claim needs to be verified. In the document or evidence retrieval sub-task, all the relevant documents related to the input claim are extracted, either from an external database or through an internet search/information retrieval(Chen et al. 2017). A threshold of how many documents are to be retrieved can be predetermined. Rationale/Sentence Selection All the information in the retrieved documents is not relevant to the claim. Hence in the rationale selection task, only the information or evidence most relevant to the claim are selected to be used to predict the veracity label. Veracity Label Prediction Once the rationale is selected, the claim along with the rationale and additional features if any, are given as input to a machine learning model. The task of the classifier is to predict a veracity label from among the following three labels; 'SUPPORTED', 'REFUTED', or 'NOT ENOUGH EVIDENCE'. In some datasets, the labels can also be 'TRUE', or 'FALSE', depending on the specific task. Explanation/Justification Generation Recent works have focused on generating explanations for the veracity labels prediction. This specific task is focused on generating natural language justifications or explanations for the prediction considering the claim and evidence to generate these explanations."}, {"title": "LLM Approaches", "content": "In this section, we describe the recent advancements that en-able LLMs to be robust in fact verification scenarios. Figure 3 shows an example pipeline that encapsulates multi-ple component modules (i.e., Evidence Retrieval, Prompt Creation, Transfer Learning, and LLM Generation) for ver-ifying claims. Different from traditional fact verification pipelines that select evidence set for verifying claims, LLM-based claim verification conditions generated text based on the concatenated input claim and retrieved evidence. The ability of large, pre-trained LMs augmented with retrieval enables them to perform well on knowledge-intensive tasks such as text generation. Evidence Retrieval Strategies RAG models, which have been developed to combat the is-sue of hallucination in LLMs in knowledge-intensive tasks, have shown success in the scope of fact verification (Lewis et al. 2020; Izacard et al. 2023; Gao et al. 2023; Guan et al. 2023). Early work such as in Lewis et al. (2020) developed a framework that incorporates a retriever to retrieve evidence from an external database such as Wikipedia for condition-ally generating veracity labels in fact verification. Authors in Izacard et al. (2023) demonstrate that RAGs perform well on the FEVER shared task (Thorne et al. 2018b)) in few-shot settings, showing approximately 5% improvement over large-scale LLMs such as Gopher (Rae et al. 2021) with sig-nificantly fewer parameters (i.e., 11B compared to 280B). Other works consider the optimization of either document ranking (Glass et al. 2022; Chen et al. 2022c; Hofst\u00e4tter et al. 2023) or input claims (i.e., queries) (Hang, Yu, and Tan 2024) as a crucial step for improving evidence retrieval for veracity labeling. Authors in Hofst\u00e4tter et al. (2023) use an autoregressive re-ranker to get the most relevant passages from the retriever. These are then passed to the generation model to generate the veracity label. Despite this, RAG models can often fail when encoun-tering long or complex input claims, causing the model to incorrectly generate veracity labels or evidence sentences. Recent works have addressed this issue by segmenting long claims into smaller sub-claims and performing multiple rounds of retrieval (Khattab, Potts, and Zaharia 2021; Shao et al. 2023). Authors in Khattab, Potts, and Zaharia (2021) present a pipeline for multi-hop claim verification that uses an iterative retriever and neural methods for effective doc-ument retrieval and re-ranking. Shao et al. (2023) show that using a re-ranker to distill knowledge to a retriever helps close the semantic gaps between a query and docu-ment passage when verifying claims using iterative RAGs. Other works address the issue of complex claims by using fine-grained retrieval techniques based on claim decompo-sition (Chen et al. 2023a; Zhang and Gao 2023; Pan et al. 2023b).Chen et al. (2023a) generate sub-questions based on a claim, which a document retriever uses to retrieve rele-vant documents. A fine-grained retriever retrieves top-k text spans as evidence based on a k-word window and BM25. Authors in Zhang and Gao (2023) decompose a claim into sub-claims and generate questions to verify the sub-claims. External knowledge sources are used to retrieve relevant in-formation to verify the sub-claims and generate a final verac-ity label. Pan et al. (2023b) follow a programming paradigm, where the claim is broken down into subtasks and the final label is the aggregation of the execution of each subtask. The fact-verification subtask uses external knowledge to retrieve relevant evidence for a claim. Hang, Yu, and Tan (2024) retrieve evidence based on generated knowledge graphs of evidences. They generate a knowledge graph of user query or input claim and com-pare it to the database of knowledge graphs to retrieve the most relevant information for claim verification. Authors in Hu et al. (2023) propose a latent variable model that al-lows the retrieval of the most relevant evidence sentences from a document while removing the irrelevant sentences. This approach reduces the noisy data during the verification process. Stochastic-RAG, an approach proposed by Zamani and Bendersky (2024) uses a stochastic sampling without re-placement process for evidence retrieval and selection. This approach overcomes the ranking and selection of the evi-dence hence optimizing the RAG model. Authors in Zhang et al. (2023) optimize the evidence retrieval process by using feedback from the claim verifier. The divergence between the evidence from a retrieved evidence set provided to the verifier and the gold standard evidence, acts as a feedback signal used to train the retriever. Xu et al. (2024b) propose Search-in-the-Chain, an approach where an LLM generates a reasoning chain and based on the answer to each node in the chain, retrieval can be used to correct an answer or pro-vide additional knowledge. The approach improves the gen-eration accuracy of the LLM. Prompt Creation Strategies Text prompting has been shown to be an effective technique for improving the desired output of large-scale generative models. In the context of claim verification, several works investigate prompting strategies, using both manual and au-tomated techniques, for building more robust claim verifica-tion systems Zhang and Gao (2023); Li et al. (2023c); Zeng and Gao (2023); Chen et al. (2023b). Authors in (Zhang and Gao 2023) develop a hierarchical prompting technique that enables LLMs to verify multiple sub-claims using a step-by-step approach. The work in Li et al. (2023c) demonstrates a self-sufficient claim verification through prompting instruc-tions on multiple language models. ProToCo (Zeng and Gao 2023) demonstrates improved claim verification performance of LLMs by leveraging a consistency mechanism to construct variants of the original claim-evidence pair prompt based on three logical relations (i.e., confirmation, negation, uncertainty). Authors in Chen et al. (2023b) develop a unified retrieval framework that em-ploys discrete, continuous, and hybrid prompt strategies for adjusting to various knowledge-intensive tasks such as claim verification. Other works such as the FactualityPrompts (Lee et al. 2022) framework test the output generations of LLMs given an input prompt and use an external database such as Wikipedia to calculate factuality and quality measures com-pared to the ground truth. Other works aim to improve the LLM's reasoning abilities by appending the claims with ev-idence during prompting in the context of claim verifica-tion and text generation (Parvez 2024; Dougrez-Lewis et al. 2024). Transfer Learning Strategies Fine-Tuning. Although recent studies show the success of pre-trained LLMs on zero- or few-shot tasks, they of-ten fail to verify real-world claims given their limited in-ternal knowledge. The success of fine-tuning has motivated recent work on claim verification (Chen et al. 2022b; Pan et al. 2021; Zeng and Zubiaga 2024). The work in Chen et al. (2022b) leverages a language model to fine-tune over an ex-ternal corpus for retrieving passage titles and evidence sen-tences using constrained beam search. The results showed improved performance against the traditional fact verifica-tion pipeline over the FEVER dataset. Other work demon-strates that using GPT models to generate synthetic training data improves the performance of LLMs on various tasks such as fact checking (Tang, Laban, and Durrett 2024) and claim matching (Choi and Ferrara 2024). Authors in Pan et al. (2021) develop a pipeline for cre-ating a fact verification dataset and fine-tuning a language model by leveraging passages from Wikipedia to generate QA pairs related to claim veracity. The authors show that it can improve state-of-the-art language models in zero-shot settings. The work in (Zeng and Zubiaga 2024) shows that using unlabelled pairwise data to increase the alignment be-tween claim-evidence pairs shows significant improvement of LLM performance in few-shot claim verification tasks. In addition, other recent works leverage techniques such as re-inforcement learning to fine-tune models for improving the veracity of claims and supporting evidence Zhang and Gao (2024); Huang et al. (2024). A document-level and question-level retrieval policy is proposed by Zhang and Gao (2024), where the top-k and top-1 documents for the document and question-level policy respectively are used as input to a scoring function for label prediction during training. This approach outperforms retrieval and prompting approaches. Chiang et al. (2024) fine-tune LLMs for their multi-stage fact verification approach. They fine-tune a model to gen-erate answers based on claim-evidence pairs and a set of questions, whereas another model is fine-tuned to verify the claim based on the claim-evidence and question-answer pairs. Authors in Zhu et al. (2023) fine-tune a generation model to generate counterfactuals to train the fact verifica-tion model's performance on out-of-domain claims. In-Context Learning. The recent success in the perfor-mance of pre-trained LLMs in zero- and few-shot settings is largely attributed to its in-context learning (ICL) abili-ties (Kojima et al. 2022; Brown et al. 2020). In the scope of claim verification, popular ICL techniques with strong zero-and few-shot performance include chain-of-thought (CoT) reasoning (Wei et al. 2022). The work in Zhao et al. (2024) develops a multi-stage verification pipeline for claim veri-fication based on claim decomposition and self-reflection. An LLM-based verifier module is created using instruc-tion prompting to generate a reasoning analysis among all sub-claims created by the decomposer module. The results suggest that using zero-shot prompting techniques provides better performance in multi-hop claim verification domains such as HOVER and FEVEROUS compared to few-shot prompting and fine-tuning methods. Authors in Kanaani (2024) enable LLMs to generate reasons over retrieved ev-idence in claim verification using few-shot ICL and the STaR CoT technique inspired by the work of Zelikman et al. (2022). Similar work has leveraged CoT techniques for the purposes of effectively verifying complex claims using rea-soning steps (Yao et al. 2023; Ni et al. 2024). On the other hand, HiSS (Zhang and Gao 2023) demonstrates that using prompting techniques for few-shot learning and claim de-composition can substantially improve the performance of CoT models for complex news claim verification. Li et al. (2023b) leverage the ICL capability of LLMs to perform multiple tasks simultaneously. Their approach outperforms or achieves comparable task performance in a zero-shot set-ting on claim verification datasets. LLM Generation Strategies Label and Evidence Generation. While the majority of claim verification systems predict veracity labels based on the concatenation of the input claim and the set of evidence sentences (Pradeep et al. 2021), recent work has proposed alternate strategies for determining veracity labels as well as selecting/generating evidence pieces. The work in Cao et al. (2024) develops, SERIf, a claim verification pipeline that features an inference module that predicts the veracity label of scientific news articles based on a two-step summariza-tion (i.e., 'Extractive - Abstractive') and evidence retrieval technique. Each summary-evidence pair is fed into the LLM and it produces a binary label, indicating whether the news article is reliable (supported) or unreliable (refuted). Authors in Wadden et al. (2022b) leverage the Longformer trans-former model (Beltagy, Peters, and Cohan 2020) that uses a shared encoding over the claim and document abstracts for rationale identification and claim label prediction. The work in Li et al. (2024a) develops an algorithm that selects the minimal evidence group (MEG) within a set of retrieved candidate documents. The algorithm aims to min-imize the redundancy while also selecting the most relevant piece of evidence to prompt the language model. Authors in Chen et al. (2022b) create an LLM claim verification framework in which the authors use the BART model to en-code all candidate sentences from the most relevant retrieved documents. The BART decoder serves as an evidence de-coder to predict the g-th evidence sentence via generation conditioned on the top k retrieved documents and the input claim. Lee et al. (2022) developed a variant of nucleus sam-pling called, factual-nucleus sampling, in which the top-p sampling pool is selected as a set of sub-words whose cumu-lative probability exceeds p. The authors show that factual-nucleus sampling can improve evidence and label genera-tion without claim verification datasets such as FEVER. Kao and Yen (2024) propose a multi-stage approach, where the evidence sentences are retrieved from articles related to a claim, and arguments are generated by aggregating and re-constructing the evidence. The arguments are refined and passed to a LLM to generate a verification label. Other ap-proaches leverage LLMs reasoning capabilities to generate veracity labels and factual evidence (Cheng, Tan, and Lu 2024; Jafari and Allan 2024; Li et al. 2024b; Fang et al. 2024; Pan et al. 2023a). Explainable Generation. Recent studies investigate ex-plainable approaches to improve LLM-based claim verifi-cation systems (Wang and Shu 2023; Dammu et al. 2024; Si et al. 2023). Authors in Wang and Shu (2023) present an ex-plainable claim verification framework named FOLK, that leverages the explanation capabilities of LLMs when verify-ing a claim and justifies the prediction through a summary of its decision process. The work in Dammu et al. (2024) proposes a knowledge graph (KG)-based approach for text verification and evidence attribution. An objective function is used to fine-tune LLMs on evidence attribution based on the input text and retrieved triplets from KG, inducing expla-nations on claim predictions. While explainable techniques can aid humans in fact-checking, LLMs are prone to incor-rect explanations due to hallucinations, causing them to be unreliable in certain claim verification scenarios (Si et al. 2023). Ma et al. (2024) prompt an LLM in a few-shot set-ting to generate a concise summary of the gathered evidence documents and the input claim. This summary serves as an explanation for the verified claim. Other works in claim verification leverage reasoning techniques such as chain-of-thought (CoT) for enabling the LLM to be interpretable in its decision-making process when verifying claims (Yao et al. 2023; Pan et al. 2023a; Zhao et al. 2024; Kanaani 2024; Ni et al. 2024; Quelle and Bovet 2024; Fang et al. 2024). The authors in Pan et al. (2023a) and Fang et al. (2024) leverage"}, {"title": "Evaluation and Benchmarking", "content": "The F1 score is the most commonly used metric to mea-sure the performance of automatic claim verification sys-tems. Other metrics like Precision, Recall, and Accuracy are also used to evaluate the system performance. Katrani-dis and Barany (2024) used the error rate between the hu-man and automated fact-verification system to measure the verification accuracy. However, these metrics consider a single pipeline component to evaluate the system's overall performance. Hence, Thorne et al. (2018a) introduced the FEVER score, a metric that uses both the verification ac-curacy as well as the evidence retrieval accuracy to com-pute overall system performance. While these metrics are valuable for assessing the performance of the classifica-tion tasks, they are inadequate for evaluating the perfor-mance of the non-classification components of the pipeline. Hence, metrics like Recall@k are used to measure the per-formance of the retrieval task (Pan et al. 2023b; Pradeep et al. 2021), while BLEU and METEOR are used to eval-uate the quality of explanations or generated questions and answers. Schlichtkrull, Guo, and Vlachos (2024) propose a new evaluation metric AVeriTeC score that uses METEOR and accuracy metrics, for question-answer-based veracity la-bel prediction systems. Other metrics like Mean Absolute Error (MAE), Expected Calibration Error (ECE), Area Un-der ROC Curve (AUC-ROC), and Pearson's Correlation are also used. Most of these metrics are inadequate to evalu-ate the factual accuracy of the LLM-generated text. Sev-eral metrics like FactScore (Min et al. 2023), SAFE (Wei et al. 2024), and VERISCORE (Song, Kim, and Iyyer 2024) are introduced to evaluate the factual accuracy of the LLM-generated text. While these metrics evaluate the accuracy, other metrics and frameworks to evaluate the factual errors in generated text (Lee et al. 2022; Chern et al. 2023) as well as their alignment (Zha et al. 2023) and entailment (Lee et al. 2022) considering the factuality have been proposed."}, {"title": "Datasets", "content": "The fundamental resource for training and evaluating claim verification systems is datasets containing annotated texts. As most research in this area deals with English data, we col-lect information about all publicly available English datasets used in the papers discussed in this survey and present them in Table 1. Several general-domain datasets have been released over the years. Different online data sources and websites are used to extract facts and claims including Wikipedia (Thorne et al. 2018a; Jiang et al. 2020; Diggelmann et al. 2020; Eisenschlos et al. 2021; Schuster, Fisch, and Barzilay 2021; Kamoi et al. 2023), due to the extensive amount of infor-mation available spanning various topics and domains, and online fact-checking websites like PolitiFact (Wang 2017; Augenstein et al. 2019; Kao and Yen 2024). Factually in-correct claims are shared through social media channels like X, Reddit, etc. Saakyan, Chakrabarty, and Muresan (2021) introduced the COVID-Fact dataset consisting of claims ex-tracted from Reddit posts. Several datasets for scientific fact verification have also been introduced over the years (Wad-den et al. 2020, 2022a; Lu et al. 2023). Most fact-verification datasets rely on unstructured textual evidence. Hence, a few datasets with structured data sources as evidence have been introduced (Aly et al. 2021; Lu et al. 2023). While a large number of datasets are focused on only veracity la-bels, some datasets were deloped to address explainability as well (Chen et al. 2022a; Yang et al. 2022; Ma et al. 2024; Rani et al. 2023; Schlichtkrull, Guo, and Vlachos 2024). Most of the claim-verification datasets include claims extracted from available information sources. LLMs are widely used to generate content, given their superior text-generation capabilities. These models often generate fac-tually incorrect information. Efforts to identify such non-factual text have been undertaken. However, existing claim-verification datasets can be inadequate for this task due to the linguistic variations between the human- and LLM-generated text. To overcome this issue, LLM-generated claim verification datasets have been introduced (Li et al. 2023c; Cao et al. 2024). While these datasets are used to evaluate the automatic claim verification systems, there is a need to evaluate the factual accuracy of the LLM-generated text. Hence, a few datasets to evaluate the LLM generation's factual accuracy have been introduced (Lee et al. 2022; Wang et al. 2023b; Malaviya et al. 2024)."}, {"title": "Shared Tasks", "content": "Shared tasks are competitions where participating teams develop systems to solve a task using a common bench-mark dataset. There have been multiple shared tasks on the claim and fact-checking organized over the years like the Fact Extraction and Verification (FEVER) (Thorne et al. 2018b), TabFact, CLEF 2020 CheckThat! (Barr\u00f3n-Cedeno et al. 2020), SCIVER (Wadden and Lo 2021), SEM-TAB-FACT (Wang et al. 2021), FACTIFY-5WQA, and more re-cently AVeriTeC. While there have been various techniques like question-answer generation as a precursor task to label prediction as in the AVeriTeC shared task, all these shared tasks are centered on predicting a veracity label for a claim. Given the LLM's tendency to hallucinate generating plau-sible yet factually inaccurate text, there is a need to orga-nize shared tasks to evaluate the factual accuracy of LLM-generated content."}, {"title": "Open Challenges", "content": "Handling Irrelevant Context Retrieved evidence may be irrelevant, which is a challenge for LLMs, as they may not be trained to ignore such evidence. The lack of robustness to the noise can cause the LLM to produce misinformation and incorrect verification. Recent research on open-domain question answering shows that external knowledge relevant to the task can aid the model performance, however, irrele-vant context can also lead the model to make inaccurate pre-dictions (Petroni et al. 2020; Shi et al. 2023; Li et al. 2023a; Yu et al. 2023). For the fact-verification task, recent works have proposed techniques to identify the most relevant con-text while aiding the veracity label prediction and thus im-proving the overall system performance (Wang et al. 2023c; Yoran et al. 2024; Xia et al. 2024). However, more work on the topic is required to verify the effectiveness of this design approach across multiple fact verification domains. Handling Knowledge Conflicts Fact-verification ap-proaches reliance on retrieved evidence can cause knowl-edge conflicts in LLM-based approaches where retrieved ev-idence as the internal parameters of the pre-trained LLM may conflict with the external knowledge. This causes the LLM to ignore the retrieved evidence and produces hallu-cinations. Xu et al. (2024a) provides an in-depth analysis of this scenario. Works by (Li et al. 2023a; Neeman et al. 2023; Mallen et al. 2023; Longpre et al. 2021; Chen, Zhang, and Choi 2022) introduce approaches to avoid knowledge conflicts and mitigate hallucinations for question-answering. Expanding this work to fact-verification, especially to the approaches that use LLMs, is vital for an effective verifica-tion process. Multilinguality Most automated claim verification ap-proaches rely on English datasets. Furthermore, there are limited multilingual fact-verification datasets (Gupta and Srikumar 2021; Kazemi et al. 2022; Pikuliak et al. 2023; Singh et al. 2023). This hinders the development of ap-proaches for multilingual fact-verification, which achieve the best performance when trained on language-specific datasets (Panchendrarajan and Zubiaga 2024)."}, {"title": "Conclusion", "content": "We presented a survey on LLM approaches to claim verifi-cation. To the best of our knowledge, this is the first claim verification survey to focus exclusively on LLM approaches thus filling an important gap in the literature. We have de-scribed each of the sub-tasks of the typical claim verifica-tion pipeline and discussed various LLM-based approaches used in this task. Finally, we have also described publicly available English datasets providing important information to new and seasoned researchers on this topic. Advances in LLM development will likely continue im-proving the quality of claim verification systems. We hope this survey motivates future research on this topic taking advantage of recently-proposed LLMs, RAG methods, etc. Claim verification is a vibrant research topic and we see multiple open research directions as described in the next sub-section."}]}