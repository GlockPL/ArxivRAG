{"title": "HOW TRANSFORMERS SOLVE PROPOSITIONAL LOGIC PROBLEMS: A MECHANISTIC ANALYSIS", "authors": ["Guan Zhe Hong", "Nishanth Dikkala", "Enming Luo", "Cyrus Rashtchian", "Xin Wang", "Rina Panigrahy"], "abstract": "Large language models (LLMs) have shown amazing performance on tasks that require planning and reasoning. Motivated by this, we investigate the internal mechanisms that underpin a network's ability to perform complex logical reasoning. We first construct a synthetic propositional logic problem that serves as a concrete test-bed for network training and evaluation. Crucially, this problem demands nontrivial planning to solve, but we can train a small transformer to achieve perfect accuracy. Building on our set-up, we then pursue an understanding of precisely how a three-layer transformer, trained from scratch, solves this problem. We are able to identify certain \u201cplanning\u201d and \u201creasoning\u201d circuits in the network that necessitate cooperation between the attention blocks to implement the desired logic. To expand our findings, we then study a larger model, Mistral 7B. Using activation patching, we characterize internal components that are critical in solving our logic problem. Overall, our work systemically uncovers novel aspects of small and large transformers, and continues the study of how they plan and reason.", "sections": [{"title": "INTRODUCTION", "content": "Language models using the transformer architecture (Vaswani et al., 2017) have shown remarkable capabilities on many natural language tasks (Brown et al., 2020; Radford et al., 2019b). Trained with causal language modeling wherein the goal is next-token prediction on huge amounts of text, these models exhibit deep language understanding and generation skills. An essential milestone in the pursuit of models which can achieve a human-like artificial intelligence, is the ability to perform human-like reasoning and planning in complex unseen scenarios. While some recent works using probing analyses have shown that the activations of the deeper layers of a transformer contain rich information about certain mathematical reasoning problems (Ye et al., 2024), the question of what mechanisms inside the model enables such abilities remains unclear.\nWhile the study of how transformers reason in general remains a daunting task, in this work, we aim to improve our mechanistic understanding of how a Transformer reason through simple propositional logic problems. For concreteness' sake, consider the following problem:\nRules: A or B implies C. D implies E. Facts: A is true. B is false. D is true.\nQuestion: what is the truth value of C?\nAn answer with minimal proof is \"A is true. A or B implies C; C is true.\"\nThe reasoning problem, while simple-looking on the surface, requires the model to perform several actions that are essential to more complex reasoning problems, all without chain of thought (CoT). Before writing down any token, the model has to first discern the rule which is being queried: in this case, it is \"A or B implies C\". Then, it needs to rely on the premise variables A and B to the locate the relevant facts, and find \"A is true\" and \"B is false\". Finally, it needs to decide that \"A is true\u201d is the correct one to invoke in its answer due to the nature of disjunction. It follows that, to write down the first token \"A\", the model already has to form a \"mental map\" of the variable relations, value"}, {"title": "RELATED WORKS", "content": "Mechanistic interpretability. Our work falls in the area of mechanistic interpretability, which aims to understand the mechanisms that enable capabilities of the LLM; such studies involve uncovering certain \"circuits\" in the network (Elhage et al., 2021; Olsson et al., 2022; Meng et al., 2022; Vig et al., 2020; Feng & Steinhardt, 2024; Wu et al., 2023; Wang et al., 2023; Hanna et al., 2024; Merullo et al., 2024; McGrath et al., 2023; Singh et al., 2024; Feng et al., 2024). While the definition of a \"circuit\" varies across different works, in this paper, our definition is similar to the one in Wang et al. (2023): it is a collection of model components (attention heads, neurons, etc.) with the \"edges\" in the circuit indicating the information flow between the components in the forward pass; the \u201cexcitation\" of the circuit is the input tokens.\nEvaluation of reasoning abilities of LLMs.\nOur work is also related to the line of work which focus on empirically evaluating the reasoning abilities of LLMs across different types of tasks (Xue et al., 2024; Chen et al., 2024; Patel et al., 2024; Morishita et al., 2023; Seals & Shalin, 2024; Zhang et al., 2023; Saparov & He, 2023; Saparov et al., 2024; Luo et al., 2024; Han et al., 2024; Tafjord et al., 2021; Hendrycks et al., 2021; Dziri et al., 2024; Yang et al., 2024). While these studies primarily benchmark their performance on so-phisticated tasks, our work focuses on understanding \"how\" transformers reason on logic problems accessible to fine-grained analysis.\nAnalysis of how LLMs reason. There are far fewer studies that focus on providing fine-grained analysis of how LLMs reason. To the best of our knowledge, only a handful of works, such as Brinkmann et al. (2024); Xue et al. (2024); Ze\u010devi\u0107 et al. (2023); Ye et al. (2024), share similar"}, {"title": "DATA MODEL: A PROPOSITIONAL LOGIC PROBLEM", "content": "In this section, we describe the synthetic propositional logic problem that shall be the data model of this paper. Our problem follows an implicit causal structure, as illustrated in Figure 1. The structure consists of two distinct chains: One containing a logical operator at the end of the chain, and the other forming a purely linear chain.\nWe require the model to generate a minimal\nreasoning chain, consisting of \u201crelevant facts\u201d,\nproper rule invocations, and intermediate truth\nvalues, to answer the truth-value query. Con-\nsider an example constructed from the causal\ngraph in Figure 1, written in English:\n\u2022 Rules: K implies D. D or E implies A. V\nimplies E. T implies S. P implies T.\n\u2022 Facts: K is true. P is true. V is false.\n\u2022 Query: A.\n\u2022 Answer: K is true. K implies D; D is true.\nD or E implies A; A is true.\nIn this example, the QUERY token A is the ter-\nminating node of the OR chain. Since any true\ninput to an OR gate (either D or E) results in A\nbeing true, the minimal solution chooses only\none of the starting nodes from the OR chain to construct its argument: in this case, node K is cho-sen.\nMinimal proof and solution strategy. In general, the problem requires a careful examination of the rules, facts and query to correctly answer the question. First, the QUERY token determines the chain to deduce its truth value. Second, if it is the logical-operator (LogOp) chain being queried, the model needs to check the facts to determine the correct facts to write down at the start of the reasoning steps (this step can be skipped for queries on the linear chain). Third, the proof requires invoking the rules to properly deduce the truth value of the query token.\nImportance of the first answer token. Correctly writing down the first answer token is central to the accuracy of the proof, because as discussed in Section 1, it requires the model to mentally process every part of the context properly due to the minimal-proof requirement of the solution."}, {"title": "MECHANISMS OF PLANNING AND REASONING: A CASE STUDY OF THE LENGTH-3 PROBLEM", "content": "In this section, we study how small GPT-2-like transformers, trained solely on the logic problem, approach and solve it. While there are many parts of the answer of the transformer which can lead to interesting observations, in this work, we primarily focus on the following questions:\n1. How does the transformer mentally process the context and plan its answer before writing down any token? In particular, how does it use its \u201cmental notes\u201d to predict the crucial first token?\n2. How does the transformer determine the truth value of the query at the end?\nWe pay particular attention to the first question, because as noted in Section 2, the first answer token reveals the most about how the transformer mentally processes all the context information without any access to chain of thought (CoT). We delay the less interesting answer of question 2 to the Appendix due to space limitations."}, {"title": "LEARNER: A DECODER-ONLY ATTENTION-ONLY TRANSFORMER", "content": "In this section, we study decoder-only attention-only transformers, closely resembling the form of GPT-2 (Radford et al., 2019a). We train these models exclusively on the synthetic logic problem. The LogOp chain is queried 80% of the time, while the linear chain is queried 20% of the time during training. Details of the model architecture are provided in Appendix B.2.\nArchitecture choice for mechanistic analysis. We select a 3-layer 3-head transformer to initiate our analysis since it is the smallest transformer that can achieve 100% accuracy; we also show the accuracies of several candidate model sizes in Figure 7 in Appendix B for more evidence. Note that a model's answer on a problem is considered accurate only if every token in its answer matches that of the correct answer."}, {"title": "MECHANISM ANALYSIS", "content": "The model approximately follows the strategy below to predict the first answer token:\n1. (Linear vs. LogOp chain) At the QUERY position, the layer-2 attention block sends out a special \"routing\" signal to the layer-3 attention block, which informs the latter whether the chain being queried is the linear one or not. The third layer then acts accordingly.\n2. (Linear chain queried) If QUERY is for the linear chain, the third attention block focuses almost 100% of its attention weights on the QUERY position, that is, it serves a simple \"message passing\" role: indeed, layer-2 residual stream at QUERY position already has the correct (and linearly decodable) answer in this case.\n3. (LogOp chain queried) The third attention block serves a more complex purpose when the LogOp chain is queried. In particular, the first two layers construct a partial answer, followed by the third layer refining it to the correct one."}, {"title": "LINEAR OR LOGOP CHAIN: ROUTING SIGNAL AT THE QUERY POSITION", "content": "The QUERY token is likely the most important token in the context for the model: it determines whether the linear chain is being queried, and significantly influences the behavior of the third attention block. The transformer makes use of this token in its answer in an intriguing way.\nRouting direction at QUERY. There exists a \"routing\" direction $h_{route}$ present in the embedding generated by the layer-2 attention block, satisfying the following properties:\n1. $a_1(X)h_{route}$ is present in the embedding when the linear chain is queried, and $a_2(X)h_{route}$ is present when the LogOp chain is queried, where the two $a_i(X)$'s are sample dependent, and satisfy the property that $a_1(X) > 0$, and $a_2(X) < 0$.\n2. The \"sign\" of the $h_{route}$ signal determines the \"mode\" which layer-3 attention operates in at the ANSWER position. When a sufficiently \u201cpositive\u201d $h_{route}$ is present, layer-3 attention acts as if QUERY is for the linear chain by placing significant attention weight at the QUERY position. A sufficiently \"negative\u201d $h_{route}$ causes layer-3 to behave as if the input is the LogOp chain: the model focuses attention on the rules and fact sections, and in fact outputs the correct first token of the LogOp chain!\nWe discuss our empirical evidence below to support and elaborate on the above mechanism.\nEvidence la: chain-type disentanglement at QUERY. We first observe that, at the QUERY position, the layer-2 attention block's output exhibits disentanglement in its output direction depending on whether the linear or LogOp chain is being queried, as illustrated in Figure 2."}, {"title": "ANSWER FOR THE LINEAR CHAIN", "content": "At this point, it is clear to us that, when QUERY is for the linear chain, the third layer mainly serves a simple \"message passing\" role: it passes the information in the layer-2 residual stream at the QUERY position to the ANSWER position. One natural question arises: does the input to the third layer truly contain the information to determine the first token of the answer, namely the starting node of the linear chain? The answer is yes.\nEvidence 2: linearly-decodable linear-chain answer at layer 2. We train an affine classifier with the same input as the third attention block at the QUERY position, with the target being the start of the linear chain; the training samples only query for the linear chain, and we generate 5k of them. We obtain a test accuracy above 97% for this classifier (on 5k test samples), confirming that layer 2 already has the answer at the QUERY position."}, {"title": "ANSWER FOR THE LOGOP CHAIN", "content": "LogOp chain: partial answer in layers 1 & 2 + refinement in layer 3. To predict the correct starting node of the LogOp chain, the model employs the following strategy:\n1. The first two layers encode the LogOp and only a \u201cpartial answer\". More specifically, we find evidence that (1) when the LogOp is an AND gate, layers 1 and 2 tend to pass the node(s) with FALSE assignment to layer 3, (2) when the LogOp is an OR gate, layers 1 and 2 tend to pass node(s) with TRUE assignment to layer 3.\n2. The third layer, combining information of the two starting nodes of the LogOp chain, and the information in the layer-2 residual stream at the ANSWER position, output the correct answer.\nWe delay the full set of evidence for the above claims to Appendix B.3. Our argument mainly relies on linear probing and activation patching. At a high level, we show that while it is not possible to linearly decode the answer from layer-2 residual stream, the predictor's answer already takes on very specific structure resembling a \u201cpartial answer\"; this reisdual stream also contains rich answer-specific information such as which LogOp is in the context. Moreover, through activation patching, we found that the layer-2 residual stream indeed have strong causal influence on the correct answer, which strengthens the \u201cpartial answer in layer 2 refinement in layer 3\" claim.\""}, {"title": "THE REASONING CIRCUIT IN MISTRAL-7B", "content": "We now turn to examine how a pretrained LLM, namely Mistral-7B-v0.1, solves this reasoning problem. We choose this LLM as it is amongst the smallest accessible model which achieves above 70% accuracy on (a minimal version of) our problem. Our primary focus here is the same as in the previous section: how does the model infer the first answer token without any CoT? We are interested in this question as the first answer token requires to model to process all the information in the context properly without access to any CoT.\nWe describe the main properties of the reasoning circuit inside the model for this prediction task in Figure 3. At a high level, there are several intriguing properties of the reasoning circuit of the LLM:\n1. Compared to the attention blocks, the MLPs are relatively unimportant to correct prediction.\n2. There is a sparse set of attention heads that are found to be central to the reasoning circuit:\n\u2022 (Queried-rule locating head) Attention heads (9,25;26), (12,9), (14,24;26) locate the queried rule using the QUERY token, and stores this information at the QUERY position.\nWe use (l, h) to denote an attention head. When referencing multiple heads in the same layer, we write (l, h1; h2; ...; hn) for brevity."}, {"title": "MINIMAL PROBLEM DESCRIPTION", "content": "In our Mistral-7B experiments, the input samples have the following properties:\n1. We give the model 6 (randomly chosen) in-context examples before asking for the answer.\n2. The problem is length-2: only one rule involving the OR gate, and one linear-chain rule. Moreover, the answer is always true. In particular, the truth values of the two premise nodes of the OR chain always have one FALSE and one TRUE.\n3. The proposition variables are all (single-token) capital English letters.\nThe design decision in the first point is to ensure fairness to the LLM which was not trained on our specific logic problem. As for the last two point, we restrict the problem in this fashion mainly to ensure that the first answer token is unique, which improves the tractability of the analysis. Note that these restrictions do not take away the core challenge of this problem: the LLM still needs to process all the context information without CoT to determine the correct first token."}, {"title": "CAUSAL MEDIATION ANALYSIS", "content": "We provide evidence in this part of the paper primarily relying on a popular technique in mechanistic interpretability: causal mediation analysis. Our methodology is roughly as follows:\n1. Suppose we are interested in the role of the activations of certain components of the LLM in a certain (sub-)task. For a running example, say we want to understand what role the attention heads play in processing and passing QUERY information to the \u201c:\u201d position for inference. Let us denote the activations as $A_{l,h;t}(X)$, representing the activation of head h in layer l, at token position t.\n2. Typically, the analysis begins by constructing two sets of prompts which differ in subtle ways. A natural construction in our example is as follows: define sets of samples $D_{orig}$ and $D_{alt}$, where $X_{orig,n}$ and $X_{alt,n}$ have exactly the same context, except in $X_{orig,n}$, QUERY is for the LogOp chain, while in $X_{alt,n}$, QUERY is for the linear chain. Moreover, denote the correct targets $Y_{orig,n}$ and $Y_{alt,n}$ respectively.\n3. We run the LLM on $D_{orig}$ and $D_{alt}$, caching the attention-head activations. We also obtain the logits of the model. We can compute the model's logit differences\n$\\triangle_{orig,n} = logit(X_{orig,n})[Y_{orig,n}] \u2013 logit(X_{orig,n})[Y_{alt,n}].$\nFor a high-accuracy model, $\\triangle_{orig,n}$ should be large for most n's, since it must be able to clearly tell that on an $X_{orig,n}$, it is the LogOp chain which is being queried, not the linear chain.\n4. We now perform intervention for all n, l, h and t:\n(a) Run the model on $X_{orig,n}$, however, replacing the original activation $A_{l,h;t}(X_{orig,n})$ by the altered $A_{l,h;t}(X_{alt,n})$. Now let the rest of the run continue. Let us denote the logits obtained in this intervened run as $logit^{\\rightarrow alt;(l,h,t)}(X_{orig,n})$.\n(b) Now compute the intervened logit difference\n$\\triangle_{orig\\rightarrow alt,n;(l,h,t)} = logit^{\\rightarrow alt;(l,h,t)}(X_{orig,n})[Y_{alt,n}] - logit^{\\rightarrow alt;(l,h,t)}(X_{orig,n})[Y_{orig,n}].$\n5. Now average the $\\triangle_{orig\\rightarrow alt,n;(l,h,t)}$'s over n for every l,h and t (recall that n is the sample index)."}, {"title": "CIRCUIT ANALYSIS", "content": "In this section, we discuss properties of the reasoning circuit of Mistral-7B. The order by which we present the results will be from the coarser- to the finer-grained, roughly following the process which we discovered the circuit; we believe this adds greater transparency to the circuit discovery process. We delay the more involved (and complete) set of experimental results to Appendix C."}, {"title": "QUERY-BASED PATCHING: DISCOVERING THE IMPORTANT ATTENTION HEADS", "content": "We initiate our analysis with QUERY patching, following the same procedure as detailed in sub-section 4.2. In this set of experiments, we discover the main attention heads responsible for process-ing the context and performing inference as introduced in the beginning of this Section.\nWhy is QUERY-based patching important to reasoning circuit discovery? To answer this ques-tion, there are two points to emphasize first. (1) We know that to solve the reasoning problem, the QUERY token is critical to initiating the reasoning chain: without it, the rules and facts are completely useless; with it, the reasoner can then proceed to identify the relevant rules and facts to predict the answer. (2) The prompt pairs differ only by the QUERY token. Based on (1) and (2), we know that if performing the aforementioned QUERY-based causal intervention on a model component leads to a large intervened logit difference (i.e. it alters the model's \u201cbelief\u201d), then this component must be integral to the reasoning circuit, because the component is now identified to be QUERY-sensitive and has causal influence on (parts of) the model's reasoning actions."}, {"title": "FINER EXAMINATIONS OF THE ATTENTION HEADS", "content": "Attention-head sub-component patching (QUERY-based patching). We now aim to understand why the attention heads identified in the last sub-section are important. For now, we continue with QUERY altering in the prompt pairs. Through intervening on the sub-components of each attention head, namely their value, key, and query, and through examining details of their attention weights, we find that there are roughly four types of attention heads. We show the results in Figure 6:\n1. Queried-rule locating head. Attention head (12,9)'s query activation has a large intervened logit difference according to Figure 6(a), therefore, its query and attention patterns are QUERY-dependent and contribute to altering the model's \u201cbelief\u201d. Furthermore, at the QUERY position, we find that on average, its attention weight is above 90% at the \"conclusion\" variable of the rule being queried. In other words, it is responsible for locating the queried rule, and storing that rule's information at the QUERY position.\n2. Queried-rule mover head. Attention head (13,11)'s value activations have large intervened logit difference, and intriguingly, its query and key activations do not share that tendency. This already suggests that its attention pattern performs a fixed action on both the original and altered prompts, and only the value information is sensitive to QUERY. Furthermore, within the relevant"}, {"title": "CONCLUSION", "content": "We studied the reasoning mechanisms of both small transformers and LLMs on a synthetic propositional logic problem. We analyzed a shallow decoder-only attention-only transformer trained purely on this problem as well as a pretrained Mistral-7B LLM. We uncovered interesting mechanisms the small and large transformers adopt to solve the problem. For the small models, we found the existence of \"routing\u201d signals that significantly alter the model's reasoning pathway depending on the sub-category of the problem instance. For Mistral-7B, we found four families of attention heads that implement the reasoning pathway of \u201cQUERY\u2192Relevant Rule\u2192Relevant Facts \u2192 Decision\". These findings provide valuable insights into the inner workings of LLMs on mathematical reasoning problems.\""}, {"title": "PROPOSITIONAL LOGIC PROBLEM AND EXAMPLES", "content": "In this section, we provide a more detailed description of the propositional logic problem we study in this paper, and list representative examples of the problem.\nAt its core, the propositional logic problem requires the reasoner to (1) distinguish which chain type is being queried (LogOp or linear), and (2) if it is the LogOp chain being queried, the reasoner must know what truth value the logic operator outputs based on the two input truth values.\nBelow we provide a comprehensive list of representative examples of our logic problem at length 2 (i.e. each chain is formed by one rule). We use [Truth values] to denote the relevant input truth value assignments (i.e. relevant facts) to the chain being queried below.\n1. Linear chain queried, [True]\n\u2022 Rules: A or B implies C. D implies E.\n\u2022 Facts: A is true. B is true. D is true.\n\u2022 Question: what is the truth value of C?\n\u2022 Answer: D true. D implies E; E True.\n2. Linear chain queried, [False]\n\u2022 Rules: A or B implies C. D implies E.\n\u2022 Facts: A is true. B is true. D is false.\n\u2022 Question: what is the truth value of C?\n\u2022 Answer: D false. D implies E; E undetermined.\n3. LogOp chain queried, LogOp = OR, [True, True]\n\u2022 Rules: A or B implies C. D implies E.\n\u2022 Facts: A is true. B is true. D is true.\n\u2022 Question: what is the truth value of C?\n\u2022 Answer: B true. A or B implies C; C True.\nRemark. In this case, the answer \"A true. A or B implies C; C True\" is also correct.\n4. LogOp chain queried, LogOp = OR, [True, False]\n\u2022 Rules: A or B implies C. D implies E.\n\u2022 Facts: A is true. B is false. D is true.\n\u2022 Question: what is the truth value of C?\n\u2022 Answer: A true. A or B imples C; C True.\n5. LogOp chain queried, LogOp = OR, [False, False]\n\u2022 Rules: A or B implies C. D implies E.\n\u2022 Facts: A is false. B is false. D is true.\n\u2022 Question: what is the truth value of C?\n\u2022 Answer: A false B false. A or B implies C; C undetermined.\n6. LogOp chain queried, LogOp = AND, [True, True]\n\u2022 Rules: A and B implies C. D implies E.\n\u2022 Facts: A is true. B is true. D is true.\n\u2022 Question: what is the truth value of C?\n\u2022 Answer: A true B true. A and B implies C; C True.\n7. LogOp chain queried, LogOp = AND, [True, False]\n\u2022 Rules: A and B implies C. D implies E.\n\u2022 Facts: A is true. B is false. D is true.\n\u2022 Question: what is the truth value of C?\n\u2022 Answer: B false. A and B implies C; C undetermined.\n8. LogOp chain queried, LogOp = AND, [False, False]\n\u2022 Rules: A and B implies C. D implies E."}, {"title": "LENGTH-3 SMALL TRANSFORMER STUDY: EXPERIMENTAL DETAILS", "content": "As illustrated in Figure 1, the propositional logic problem always involve one logical-operator (Lo-gOp) chain and one linear chain. In this paper, we study the length-3 case for the small-transformer setting, and length-2 case for the Mistral-7B-v0.1 case.\nThe input context has the following form:\nRULES_START K implies D. V implies E. D or E implies A.\nP implies T. T implies S. RULES_END\nFACTS_START K TRUE. V FALSE. P TRUE. FACTS_END\nQUERY_START A. QUERY_END\nANSWER\nand the answer is written as\nK TRUE. K implies D; D TRUE. D or E implies A; A TRUE.\nIn terms the the English-to-token mapping, RULES_START, RULES_END, FACTS_START, FACTS_END, QUERY_START, QUERY_END ANSWER, . and; are all unique single tokens. The logical operators and and or and the connective implies are unique single tokens. The proposi-tion variables are also unique single tokens.\nRemark. The rules and facts are presented in a random order in the respective sections of the context in all of our experiments unless otherwise specified. This prevents the model from adopting position-based shortcuts in solving the problem.\nAdditionally, for more clarity, it is entirely possible to run into the scenario where the LogOp chain is queried, LogOp = OR and the two relevant facts both have FALSE truth values (or LogOp = AND and both relevant facts are TRUE), in which case the answer is not unique. For instance, if in the above example, both K and V are assigned FALSE, then both answers below are logically correct:\nK FALSE V FALSE. K implies D; D UNDETERMINED. V implies E;\nE UNDETERMINED. D or E implies A; A UNDETERMINED.\nand\nV FALSE K FALSE. V implies E; E UNDETERMINED.\nK implies D; D UNDETERMINED. D or E implies A; A UNDETERMINED.\nProblem specification. In each logic problem instance, the proposition variables are randomly sampled from a pool of 80 variables (tokens). The truth values in the fact section are also randomly chosen. In the training set, the linear chain is queried 20% of the time; the LogOp chain is queried 80% of the time. We train every model on 2 million samples.\nArchitecture choice. Figure 7 indicates the reasoning accuracies of several candidate model vari-ants. We observe that the 3-layer 3-head variant is the smallest model which achieves 100% accu-racy. We found that 3-layer 2-head models, trained of some random seeds, do converge and obtain near 100% in accuracy (typically above 97%), however, they sometimes fail to converge. The 3-layer 3-head variants we trained (3 random seeds) all converged successfully."}, {"title": "TRANSFORMER DEFINITION", "content": "The architecture definition follows that of GPT-2 closely.\nDefine input $x = (x_1,x_2,..., x_t) \\in \\Omega^t$, a sequence of tokens with length t. It is converted into a sequence of (trainable) token embeddings $X_{token} = (e(x_1), e(x_2), ..., e(x_t)) \\in \\mathbb{R}^{d_{in} \\times t}$. Adding to it the (trainable) positional embeddings $P = (p_1, p_2, ..., p_t) \\in \\mathbb{R}^{d_{in} \\times t}$, we form the zero-th layer embedding of the transformer $X_o = (e(x_1) + P_1, ..., e(x_t) + P_t)$. The input is processed by the attention blocks as follows.\nLet the model have L layers and H heads. For layer index $l \\in [L]$ and head index $j \\in [H]$, atten-tion head $A_{l,j}$ is computed by $A_{l,j}(X_{l-1}) = S \\big( causal[\\frac{X_{l-1}QK_{l-1}^T}{\\sqrt{d_H}}] \\big) X_{l-1}V,$ with $\\tilde{X}_{l-1} = LayerNorm(X_{l-1})$, $S(\\cdot)$ being the softmax operator, $causal[\\cdot]$ the causal mask opera-tor. The output of the attention block is $X_l = X_{l-1} + Concat[A_{l,1}(\\tilde{X}_{l-1}), ..., A_{l,H} (\\tilde{X}_{l-1})] We \\tilde{W}^{o} \\in \\mathbb{R}^d$ with $W_{ore}^{0}$ the square output matrix (with bias). Finally, we apply an affine classifier (with softmax) $f(x) = S(X_{L,t}W_{class} + b_{class})$ to predict the next word.\nIn this paper, we set the hidden space embedding to 768."}, {"title": "TRAINING DETAILS", "content": "In all of our experiments, we set the learning rate to $5 \\times 10^{-5}$, and weight decay to $10^{-4}$. We use a batch size of 512, and train the model for 60k iterations. We use the AdamW optimizer in PyTorch, with 5k iterations of linear warmup, followed by cosine annealing to a learning rate of 0. Each model is trained on a single V100 GPU; the full set of models take around 2 - 3 days to finish training."}, {"title": "ANSWER FOR THE LOGOP CHAIN", "content": "Evidence 3a: Distinct behaviors of affine predictors at different layers. We train two affine classifiers at two positions inside the model (each with 10k samples): $W_{resid,l=2}$ at layer-2 residual stream, and $W_{attn,l=3}$ at layer-3 attention-block output, both at the position of ANSWER, with the target being the correct first token. In training, if there are two correct answers possible (e.g. OR gate, starting nodes are both TRUE or both FALSE), we randomly choose one as the target; in testing, we deem the top-1 prediction \u201ccorrect\" if it coincides with one of the answers. We observe the following predictor behavior on the test samples:"}, {"title": "MISTRAL-7B EXPERIMENT DETAILS", "content": "We present six examples of the propositional-logic problem in context to the Mistral-7B model, and ask for its answer to the seventh problem. An example problem is presented below."}, {"title": "PROBLEM FORMAT", "content": "We present six examples of the propositional-logic problem in context to the Mistral-7B model, and ask for its answer to the seventh problem. An example problem is presented below."}, {"title": "FINER DETAILS OF QUERY-BASED ACTIVATION PATCHING", "content": "In this subsection, we present and visualize the attention heads with the highest average intervenes logit differences, along with their standard deviations (error bars)."}, {"title": "QUERY-BASED ACTIVATION PATCHING EXPERIMENTS: METRICS", "content": "We rely on a calibrated version of the logit-difference metric often adopted in the literature for the QUERY-based activation patching experiments (aimed at keeping the score's magnitude between 0 and 1). In particular", "t": "n$\\triangle_{alt}^{orig} = \\frac{\\mathbb{E}_{n \\in [N", "triangle_{alt}^{orig}": ""}, {"N": ""}, ["logit(X_n)[Y_{alt,n}"], "logit(X_n)[Y_{orig,n}"]}