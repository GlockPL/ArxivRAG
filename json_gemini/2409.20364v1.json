{"title": "Efficient Driving Behavior Narration and Reasoning on Edge Device Using Large Language Models", "authors": ["Yizhou Huang", "Yihua Cheng", "Kezhi Wang"], "abstract": "Deep learning architectures with powerful reasoning capabilities have driven significant advancements in autonomous driving technology. Large language models (LLMs) applied in this field can describe driving scenes and behaviors with a level of accuracy similar to human perception, particularly in visual tasks. Meanwhile, the rapid development of edge computing, with its advantage of proximity to data sources, has made edge devices increasingly important in autonomous driving. Edge devices process data locally, reducing transmission delays and bandwidth usage, and achieving faster response times. In this work, we propose a driving behavior narration and reasoning framework that applies LLMs to edge devices. The framework consists of multiple roadside units, with LLMs deployed on each unit. These roadside units collect road data and communicate via 5G NSR/NR networks. Our experiments show that LLMs deployed on edge devices can achieve satisfactory response speeds. Additionally, we propose a prompt strategy to enhance the narration and reasoning performance of the system. This strategy integrates multi-modal information, including environmental, agent, and motion data. Experiments conducted on the OpenDV-Youtube dataset demonstrate that our approach significantly improves performance across both tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "N the field of autonomous driving [1], [2], deep learning models [3] play a pivotal role, largely due to their powerful feature learning capabilities, end-to-end learning processes, and ability to integrate multi-modal data. These strengths contribute to the increased reliability, safety, and efficiency of autonomous driving technologies in real-world scenarios. Driving behavior description [4], a critical sub-task within au- tonomous driving, involves the deep understanding and precise interpretation of vehicle behavior in various traffic environ- ments. In this task, large language models (LLMs) [5], [6], [7], [8] based on deep learning have demonstrated exceptional performance, particularly due to their strong reasoning and contextual understanding abilities. This makes LLM highly effective for addressing high-level decision-making challenges in autonomous driving. Additionally, LLMs possess the capa- bility to combine driving rules with natural language, enabling them to generate explanatory narratives for driving behaviors.\nOn the other hand, edge computing [9] plays a critical role in the field of autonomous driving, primarily due to its advan- tages of low latency and rapid response, which meet the real- time and efficient decision-making demands of such systems. In autonomous driving, vehicles may make quick decisions in dynamic and constantly changing traffic environments, such as emergency braking, avoiding pedestrians, or handling complex traffic situations. By offloading computational tasks to edge devices located near the roadside units (RSU) or onboard devices [10], the delays caused by transmitting data to remote cloud servers can be avoided. Moreover, edge computing reduces bandwidth usage and the burden of data transmission, as it processes and analyzes data locally, sending only essential information to central servers or the cloud. This is particularly important in autonomous driving, where large volumes of sensor data may be continuously processed. The distributed nature of edge computing also enhances system reliability and scalability, preventing the overloading of a single central server and improving overall system stability and responsiveness. Therefore, the deployment of edge computing significantly enhances the real-time performance, safety, and efficiency of autonomous driving systems, especially in handling emergency situations and high-density traffic scenarios.\nIn this paper, we propose a framework that integrates large language models (LLMs) with edge devices. This approach combines the strengths of LLMs in understanding complex semantics and reasoning with the powerful image-processing capabilities of visual encoders, resulting in more efficient and flexible descriptions of driving scenes. Specifically, the visual encoder analyzes and extracts key visual features from the driving environment, such as vehicle positions and speed, which are then fed into the LLM for further processing. We deploy the LLMs across multiple roadside units (RSUs), each covering a specific area and interconnected through 5G NR/NSA technology [11], enabling decentralized deployment. Within this framework, each RSU processes only the traffic data from its coverage area, thereby avoiding redundant oper- ations and mitigating data congestion.\nThe LLM performs frame-by-frame analysis of driving behavior and road conditions, offering reasoning and explana- tions, while globally broadcasting warnings about emergencies or hazards, such as overspeeding. To enhance the accuracy of LLMs in processing visual features, we propose a three-stream"}, {"title": "II. TASK DEFINITION", "content": "Our task focus on describe and interpret driving behaviors in autonomous driving scenarios, with the objective of enabling real-time communication of the LLM-generated descriptions and interpretations between RSUs via 5G NR/NSA. As shown in Figure 1(a), this task involves deploying an LLM on each RSU, where each RSU functions as an edge server. Any RSU can independently broadcast driving behaviors to the global network, particularly when abnormal or hazardous driving behavior occurs. The RSU that first detects such behavior can quickly communicate with other RSUs. Therefore, this task requires the LLMs deployed on RSUs to effectively de- scribe autonomous driving scenarios and interpret the driving behaviors within those scenarios, while also ensuring a rapid response time for efficient operation.\nOur work performs narration and reasoning on each edge device. Narration involves creating coherent natural language descriptions of videos, for example, narration should be able to express details like \"a person is crossing the street\". This process requires the model to generate grammatically and semantically fluent text that aligns with human language patterns while accurately reflecting the main content of the video. Reasoning provides a deeper understanding of complex video scenarios, for instance, when a vehicle is stopping in front of the line, the model can infer that the traffic lights turn to red. Reasoning helps link different video segments through logical connections, providing semantic support that makes the descriptions more intelligent."}, {"title": "III. METHOD", "content": "Our task aims to test the performance of LLM on edge devices, including the accuracy of narration and reasoning of driving behavior and the response time. The framework we designed is LLM deployed on RSUs individually, this setup avoids redundant computations and reduces queuing delays during data transmission, minimizing response latency. Due to RSUs are stationary and may have blind spots. It is necessary to manually upload data from blind spot environments to enable the LLM to consider the entire scene more compre- hensively.\nIn this work, we propose a framework to integrate LLMs with edge devices. We deploy LLMs on each edge device and these edge devices communicate with each other using 5G NR/NSA technology. We also design a visual Q&A window for real-time interaction. Road users can upload dynamic road conditions to avoid blind spots on RSUs, then consult related information using this window. We further propose a multi- modal prompt strategy to enhance the performance of LLMs. We collect environment, agent and motion information, and input them into LLMs for reasoning. One effective selection strategy is used to select useful multi-modal information."}, {"title": "B. LLMs with Edge Device", "content": "We propose a framework to integrate LLMs with edge device in this section. In our framework, we use three laptops to simulate three edge servers and a 5G router provided by a network operator for 5G cellular communication. We connected the three RSUs to the 5G router and deployed LLMs on each. A single video clip was split into three sequential parts and fed into the RSUs in order. Our goal is for the LLMs on each RSU to provide coherent reasoning and explanations based on the video content, which will be evaluated against a baseline of human annotations. Additionally, we designed a visualization window at the end of the LLM to simulate road condition information uploaded by pedestrians.\nSpecifically, the RSUs collects data from vehicles, pedestri- ans, and infrastructure such as CCTV cameras. Vehicles use onboard sensors for real-time road condition updates, while pedestrians contribute localized data via mobile devices. Data is uploaded to nearby RSUs using dynamically generated IP addresses, and LLMs analyze data to generate text descriptions of road conditions, driving behaviors, and anomalies. Figure 1 (b) shows the workflow of our framework for edge computing, with RSUs processing data locally to reduce latency and meet real-time demands. Processed outputs are visualized for monitoring traffic, driving behavior, and safety.\nOne key strength of this system is its ability to enable real-time collaboration between RSUs. When an LLM on one RSU detects an event, such as speeding or an accident, this information is immediately communicated to neighboring RSUs. For instance, if the first RSU detects a speeding vehicle, it can alert the second RSU, which can then warn nearby vehicles and pedestrians. This inter-unit communication helps create a safer driving environment by predicting potential dangers before they escalate. The warnings and information are transmitted over the C-V2X network, ensuring timely and reliable message delivery."}, {"title": "C. Multi-modal Prompt Strategy", "content": "The LLMs deployed on the RSUs are not specifically optimized for autonomous driving and driving behaviour tasks. To further enhance the LLM's narration and reasoning capabil- ities, we design a multi-modal prompt strategy to improve the performance of LLM in narration and reasoning. The prompt includes environment, agent and motion information.\nEnvironment information contains external conditions around the vehicle, such as weather and lighting, which signif- icantly impact driving behavior and decisions. It ensures the model considers changes in visibility, road friction, and other environmental factors that may affect vehicle dynamics or necessitate more cautious driving. For example, during heavy rain, the model adjusts its reasoning to account for longer stopping distances and reduced lane visibility, enhancing the accuracy of its driving behavior descriptions.\nAgent information directs the model's attention to other entities in the driving environment, such as nearby vehicles and pedestrians. It aids in detecting interactions like lane merging, overtaking, or pedestrian crossings\u2014crucial aspects of safe driving that are challenging to identify without specific prompts. By activating this stream, the model can recognize the presence and behavior of other road users, enabling de- tailed and context-aware driving behavior descriptions.\nMotion information analyzes the movements of the sur- rounding vehicles to identify events like sudden braking, sharp turns, and speeding. It enables the model to assess these situations and generate responses, such as recommending slowing down when a vehicle ahead brakes suddenly. This stream detects behaviors that may indicate potential accidents or hazards, providing real-time reasoning for adaptive driving.\nWe selected 23 environment information to serve as prompts, with each information representing a specific en- vironmental condition, such as rainy day, fog, or nighttime. We first input videos containing one or more of these envi- ronmental information, along with their corresponding textual descriptions into the LLM, allowing it to learn and retain these information and their related descriptions. Similarly, we selected 15 agent information as prompts, including pedestri- ans, vehicles, and cyclists, and 47 motion information, such as turning, stopping, lane changing, and crossing. After the LLM forms a memory of individual information, we further enrich its understanding of complex scenarios by pairing environmental, agent, and motion information in combinations of two. Using the same strategy, we input these combined scenarios, along with their associated textual descriptions, then fed into the LLM to enhance its memory. This iterative and layered approach ultimately forms a comprehensive multi- modal prompts, enabling the LLM to describe and interpret complex scenes from the perspectives of environment, agents, and motion. We use the memory-enhanced LLM as the final"}, {"title": "IV. EXPERIMENT AND DISCUSSIONS", "content": "The OpenDV-YouTube dataset [12], developed by Open- DriveLab, is a large-scale dataset specifically designed for video-language research in the context of autonomous driving. It includes a wide range of driving videos sourced from YouTube, paired with corresponding text descriptions. These descriptions provide rich annotations that explain driving be- haviors, road conditions, and objects encountered in the video. The dataset is intended to support tasks like driving behavior prediction, and autonomous driving research."}, {"title": "B. Experimental Settings", "content": "We implement all models in PyTorch and validated using three NVIDIA RTX 3090 GPUs, each equipped with 24 GB of LPDDR4X memory. It is worth noting that we specifically chose the OpenDV-Youtube dataset for this study, in part be- cause none of the LLMs under evaluation had been previously trained on this dataset. This choice allowed us to fairly and impartially assess the performance of these models on data they had not encountered before, thus offering a more realistic measure of their generalization abilities when applied to new, unseen driving scenarios.\nWe evaluate model performance by comparing keywords in LLM outputs with the human annotations in the dataset. For instance, if the LLM's response is \"3 pedestrians, 1 vehicle in rainy weather\", while the human annotation in the dataset states \"3 pedestrians, 1 cyclist, 1 vehicle in rainy weather\", the LLM correctly identifies 3 keywords but misses one, resulting in a narration accuracy of 75%. When the LLM explains the scene using phrases like \"the motion occurred because of the environment and agent information,\" We compare the LLM's reasoning with the human annotations in the dataset, examining the frequency and overlap of keywords to determine the accuracy of the LLM's responses. It is worth noting that if the LLM's reasoning contains structural errors, such as \"The weather change is caused by the low speed of vehicles,\" it will be considered entirely incorrect due to the flawed cause-effect relationship, even if all the keywords are accurate."}, {"title": "C. Advantage of Multi-modal Prompt Strategy", "content": "We conducted a comprehensive series of tests on Video Chat [5], LLaMA Adapter [6], Video LLaMA [7], and Video- ChatGPT [8] to thoroughly assess their performance in terms of narration accuracy and reasoning correctness, as summa- rized in Table I. For consistency, the input provided to all four large language models was kept uniform, utilizing raw video data as the primary source. This consistency in input ensured that the results were directly comparable across different models. We designed two distinct sets of experiments: one set with the Prompt Strategy enabled and the other without it. This approach allowed us to effectively evaluate the impact of the Prompt Strategy on the performance of these models, pro- viding insights into how prompt-based optimization influences both narration and reasoning tasks."}, {"title": "D. Response Speed of LLMs", "content": "We tested the response speeds of these four LLMs, along- side the traditional deep learning method ADAPT [13]. For this aspect of the evaluation, we introduced a variety of conditions by dividing image frames into intervals of 1, 15, and 30. This was done to measure response times under different levels of input frequency. We locally reproduced the ADAPT model and trained it using the OpenDV-Youtube dataset to establish a performance baseline. The results for ADAPT presented in Table II and Table III were obtained from the validation set, providing a benchmark for comparison against the LLMs.\nThe result in Table II shows that LLMs have significantly fast response times when processing different numbers of image frames. Video-ChatGPT performs the fastest under all conditions, taking 0.3 seconds, 4.4 seconds, and 8.5 seconds to process 1, 15, and 30 frames, respectively. Video Chat, LLaMA Adapter, and Video LLAMA also demonstrate quick response times, particularly when handling smaller batches of images. We also implement the conventional deep learning model ADAPT, which shows much longer response times, taking as long as 1173 seconds to process 30 frames, which is significantly slower than the other models. This is because that ADAPT is deployed in the local, and has no optimization for the parallel computing. This is also demonstrate the advantage of LLMs in edge environments."}, {"title": "E. Discussion", "content": "In this subsection, we discuss extra optimizations in our experiments. When optimizing multi-modal prompts for the LLM, our initial setup involved adding text descriptions for all 30 frames of the video, covering environment, agent, and motion information. After obtaining experimental results from table II for single-frame response speeds, we observed that using only keyframes to add text descriptions for the environ- ment information resulted in the same narration and reasoning accuracy. For agent and motion information, using keyframes along with the trajectory from the preceding and following eight frames allowed the LLM to achieve the results shown in Table I. This optimization significantly reduced the time required for data pre-processing and lowered computational overhead. Additionally, we have listed the performance of the fully trained deep learning model ADAPT in terms of narration and reasoning in Table III. However, due to the high response speed requirements of this task, the performance of the ADAPT can only serve as a reference."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed framework that integrates large language models and edge devices, along with a multi-modal prompt strategy to enhance the accuracy of narration and reasoning of driving behavior on edge devices. After enabling the multi-modal prompt strategy, the overall performance of the LLM improved significantly. Furthermore, deploying the LLM directly on RSUs via 5G communication technology allows for real-time data processing at the source, signifi- cantly reducing the latency associated with data queuing and processing delays. By handling data closer to the source, this approach minimizes waiting times and enhances overall system efficiency."}]}