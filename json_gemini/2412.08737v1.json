{"title": "Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions", "authors": ["Jiarui Zhang", "Ollie Liu", "Tianyu Yu", "Jinyi Hu", "Willie Neiswanger"], "abstract": "Multimodal large language models (MLLMs) have made rapid progress in recent years, yet continue to struggle with low-level visual perception (LLVP)\u2014particularly the ability to accurately describe the geometric details of an image. This capability is crucial for applications in areas such as robotics, medical image analysis, and manufacturing. In this paper, we first introduce Geoperception, a benchmark designed to evaluate an MLLM's ability to accurately transcribe 2D geometric information from an image. Using this benchmark, we demonstrate the limitations of leading MLLMs, and then conduct a comprehensive empirical study to explore strategies for improving their performance on geometric tasks. Our findings highlight the benefits of certain model architectures, training techniques, and data strategies, including the use of high-fidelity synthetic data and multi-stage training with a data curriculum. Notably, we find that a data curriculum enables models to learn challenging geometry understanding tasks which they fail to learn from scratch. Leveraging these insights, we develop Euclid, a family of models specifically optimized for strong low-level geometric perception. Although purely trained on synthetic multimodal data, Euclid shows strong generalization ability to novel geometry shapes. For instance, Euclid outperforms the best closed-source model, Gemini-1.5-Pro, by up to 58.56% on certain Geoperception benchmark tasks and 10.65% on average across all tasks.", "sections": [{"title": "1. Introduction", "content": "Multimodal large language models (MLLMs) have rapidly progressed in recent years, demonstrating remarkable potential in understanding and reasoning about the visual world through the powerful capabilities of large language models (LLMs) (Liu et al., 2024c,a, Achiam et al., 2023, Team et al., 2023, Hu et al., 2023, Tong et al., 2024a, Wang et al., 2024a). These models have showcased strong performance in tasks such as visual question answering (VQA) (Goyal et al., 2017), image captioning (Lin et al., 2014), and multimodal reasoning (Liu et al., 2023). As one recent example, LLaVA-NeXT-34B (Liu et al., 2024b) achieves an impressive 83.7% accuracy on the VQAv2 benchmark (Goyal et al., 2017), a comprehensive benchmark on natural image question answering.\nWhile MLLMs achieve impressive results on tasks like VQA, their performance relies on high-level semantic extraction (Tong et al., 2024b); in contrast, they often fall short on low-level visual perception (LLVP)\u2014i.\u0435., the ability to accurately describe the geometric details of an image, such as the points, lines, angles, shapes, and spatial relationships among its constituent objects. This limitation becomes especially apparent in tasks requiring precise descriptions, such as mathematical visual problem solving (Zhang et al., 2024a, Lu et al., 2023), scientific visual understanding (Yue et al., 2024, Fu et al., 2024a), abstract visual reasoning (Jiang et al., 2024, Ahrabian et al., 2024), and even simple visual comprehension (Rahmanzadehgervi et al., 2024, Wang et al., 2024b). For example, when interpreting a graph diagram, precise recognition of edges is essential for extracting reliable information, and in geometry problem-solving, accurate identification of"}, {"title": "2. Background and Related Work", "content": "We provide an overview of prior efforts that assess and improve low-level perception and geometric reasoning in MLLMs, and highlight our contributions in data synthesis, evaluation, and training.\nVision-Language MLLMs. While recent iterations of LLMs feature a standardized model architecture and pretraining recipe, MLLMs still often differ in design choices for infusing visual inputs. One popular design"}, {"title": "Geometry-Oriented MLLMs.", "content": "At the core of these choices is the hardness in designing a module adept in general visual reasoning (McKinzie et al., 2024, Tong et al., 2024a). In this work, we explore the optimal design of MLLMs specialized in low-level visual perception, a crucial aspect for (among other applications) multimodal mathematical understanding (Lu et al., 2023, Zhang et al., 2024a). This paper supplements prior efforts in improving mathematical reasoning (Gao et al., 2023, Zhang et al., 2024b, Zhuang et al., 2024, Li et al., 2024, Peng et al., 2024, Shi et al., 2024b) with a detailed study on the effect of dataset mixture, curriculum, and visual encoder, to reach a recipe that elicits strong performance on geometric tasks (Kazemi et al., 2023) that require low-level perception."}, {"title": "Evaluating LLVP.", "content": "Many benchmarks (Rahmanzadehgervi et al., 2024) have reported that frontier-class MLLMs struggle with visual perception tasks, which are prerequisites for applications that emphasize low-level geometric perception (Chen et al., 2024, Fu et al., 2024c), including mathematical (Yue et al., 2024, Lu et al., 2023, Zhang et al., 2024a, Jiang et al., 2024) and spatial reasoning (Chen et al., 2024, Fu et al., 2024b). These findings collectively identify that MLLMs exhibit a language prior (Lin et al., 2023)\u2014a preference of textual inputs over visual inputs\u2014leading to a performance gap between modalities (Wang et al., 2024b, Zhang et al., 2024a, Fu et al., 2024a). Meanwhile, there lacks a high-quality benchmark that evaluates low-level geometric perception in MLLMs, and the Geoperception benchmark represents a first effort to narrow this gap. This type of efforts have led to significant improvements in certain capabilities of MLLMs, such as compositionality of objects (Yuksekgonul et al., 2022, Kong et al., 2023)."}, {"title": "Improving LLVP.", "content": "Many prior works study data-driven approaches to improve low-level perception skills. For example, Gao et al. (2023), Li et al. (2024), Zhuang et al. (2024) employ a standardized supervised finetuning recipe, and optionally adjust the training data mixture. This type of training data is often synthesized from text-only math problems (Lu et al., 2021, Trinh et al., 2024) or via rule-based systems (Kazemi et al., 2023). In parallel, Vishniakov et al. (2023), Shi et al. (2024a), Tong et al. (2024b) have explored the design space of visual encoders for general-purpose vision-language reasoning. We identify best practices over the union of these design spaces, and then train small MLLMs with strong performance in low-level perception tasks.\nLastly, several works (Schick et al., 2024, Sur\u00eds et al., 2023, Hu et al., 2024) have opted to augment an MLLM with external APIs that process low-level features with specialized vision modules, such as object detection (Redmon et al., 2016), segmentation (Kirillov et al., 2023), and depth estimation (Yang et al., 2024). While these agentic frameworks (Wu et al., 2023) present a promising alternative that directly addresses the shortcomings of visual encoders, they are limited by their scalability to novel use cases, and may be insufficient for precise tool routing that requires low-level perception as a primer (Picard et al., 2023, Wu et al., 2024, Buehler, 2024)."}, {"title": "3. Geoperception Benchmark", "content": "Recently, there has been a growing number of multimodal benchmarks across diverse domains beyond natural image understanding, including mathematical reasoning (Zhang et al., 2024a, Lu et al., 2023) and abstract visual reasoning (Jiang et al., 2024, Chia et al., 2024). Many of these prior works have realized the importance of accurate low-level visual perception. Specifically, Marvel (Jiang et al., 2024) introduces perception questions for various abstract reasoning patterns, and finds that the main bottleneck of MLLMs\u2019 performance on abstract visual reasoning is that they fail to accurately transcribe visual information into concepts; Mathverse (Zhang et al., 2024a) and IsoBench (Fu et al., 2024a) both test MLLMs on equivalent question represented by language and visual modalities, respectively. Both works find that language-only input always outperforms vision-language input, and that the vision component of MLLMs always fails to utilize low-level visual features. VDLM (Wang et al., 2024b) transcribes raster images into vector graphics and uses LLMs to reason over the SVG code. They find that although SVG code is not straightforward to understand, using LLMs to reason over SVG is consistently more effective than directly using MLLMs on original raster images. Blind-test (Rahmanzadehgervi et al., 2024) and BLINK (Fu et al., 2024c) also share similar findings with the works above."}, {"title": "A Benchmark for Geometric LLVP.", "content": "Although such shortcomings of MLLMs are commonly recognized, there is a lack of comprehensive benchmark that purely focuses on these abilities of MLLMs. Our goal is to construct a benchmark focusing solely on the perception ability of MLLMs, which is also representative enough of real-world applications. When humans perceive and memorize visual information, it is well-recognized that this procedure relies crucially on searching for the closest and simplest corresponding geometric shapes (Sabl\u00e9-Meyer et al., 2022). We posit that geometric perception is a fundamental and broadly representative LLVP ability in many applications. Hence, we select geometry understanding as our domain of dataset construction."}, {"title": "Benchmark Tasks.", "content": "Over two thousand years ago, Euclid introduced five axioms that underpin all further geometric reasoning. These axioms involve establishing and extending lines using points (Axioms 1 and 2), constructing circles from a point and a radius (Axiom 3), and defining perpendicularity (Axiom 4) and parallelism (Axiom 5). Additionally, Euclid provided common notions regarding the properties of equality. To capture these aspects, we define five tasks in our Geoperception dataset: PointLiesOnLine,"}, {"title": "Data Filtering.", "content": "Geoperception is sourced from the Geometry-3K (Lu et al., 2021) corpus, which offers precise logical forms for geometric diagrams, compiled from popular high-school textbooks. However, certain points in these logical forms are absent in the corresponding diagrams. To resolve this, we use GPT-40-mini MLLM to confirm the presence of all points listed in the logical forms. This process filters the 3,002 diagrams to retain 1,584, where at least one logical form fully represents its points in the diagram. A random inspection of 100 annotations reveals only two errors, indicating high annotation accuracy."}, {"title": "Converting Logical Forms Into Questions.", "content": "We convert logical forms into question-and-answer pairs for each of the seven tasks in Geoperception. In the Equals task, for example, we directly convert the logical form (e.g., Equals(LengthOf(Line(Q, T)), 86)) into a question-answer pair (e.g., Q: What is the length of line QT as annotated? A: 86). For PointLiesOnLine, two points on the line are chosen to form the question, with the remaining points on the line as the answer. Similarly, for PointLiesOnCircle, we ask which points lie on the circle, using its center as the basis for the question. For Parallel and Perpendicular, we represent each line by two points and query which other lines are parallel or perpendicular to it. In AngleClassification, we ensure the queried angle is in the range of [10, 80] U [100, 170] degrees to avoid ambiguity. For LineComparison, we ensure that the shorter line is less than 70% of the length of the longer line. Since multiple equivalent questions can be generated for a single logical form (e.g., a line containing five points generates  ${P}_{2}^{5}$ equivalent questions), we randomly select one to avoid redundancy."}, {"title": "Evaluation Details.", "content": "We evaluate seven leading MLLMs, both open source and closed source. The open source models include Molmo-7B-D (Deitke et al., 2024), Cambrian-1-8B (Tong et al., 2024a), Qwen2-VL-7B (Wang et al., 2024a), Llama-3.2-11B (Dubey et al., 2024), and Pixtral-12B (AI, 2023). The closed-source models include GPT-40-mini (Achiam et al., 2023), GPT-40 (Achiam et al., 2023), Claude-3.5-Sonnet (Anthropic, 2024), Gemini-1.5-flash (Team et al., 2023), and Gemini-1.5-pro (Team et al., 2023). Additionally, GPT-40-mini without image input is used for generating the random baseline, employing the same textual instructions. To prevent stretching, all images are padded to square dimensions before being fed into the models. During evaluation of a given question by an MLLM, let G denote the ground truth set of answers,"}, {"title": null, "content": "and let P denote the predicted set of answers; then the evaluation score is defined as\nEvaluation score = $\\begin{cases}\n    \\frac{|P|}{|G|} & \\text{if } P \\subseteq G, \\\\\n    0 & \\text{otherwise}.\n  \\end{cases}$"}, {"title": null, "content": "Current MLLMs struggle to perceive low-level geometry annotations and relationships. We show a comparison of all models on Geoperception in Table 2. Despite the simplicity of Geoperception for humans, it remains a considerable challenge for even the most advanced commercial MLLMs. Notably, all models fall short of achieving 30% accuracy on the PointLiesOnLine task and do not outperform the text-only GPT-40 mini model in AngleClassification task. Closed source models generally outperform open source ones, with Gemini-1.5-pro attaining the highest average score of 56.98%, followed by gemini-1.5-flash at 54.76%. Among open source models, Pixtral-12B achieves the best performance with an overall score of 41.95%. It is worth noting that Cambrian-1 (Tong et al., 2024a), which is reported to be trained on Geo-170K (Gao et al., 2023), a geometry multimodal instruction tuning dataset built on the logical annotation of Geometry-3K, the same source with our Geoperception, still faces challenges in our Geoperception task, despite being trained on the dataset having the same images and augmented text annotations."}, {"title": "4. Empirical Study on MLLM Design Space", "content": "Although large-scale web-crawled image-text pairs cover a variety of domains, including geometry, the textual descriptions often lack the necessary specificity and depth. To address this issue, current studies in this domain (Gao et al., 2023, Shi et al., 2024b, Zhang et al., 2024b) typically construct a geometry or"}, {"title": "Geometry Image Generation Engine.", "content": "To provide sufficient high-fidelity training datasets, we develop a synthetic dataset generation engine to programmatically produce geometry shapes. Our geometry shape generation engine is built on AlphaGeometry (Trinh et al.,"}, {"title": "Exploration of MLLM design space.", "content": "With sufficient training dataset, we now revisit the MLLMs architectural and training design space. We choose 2 primitive geometry tasks from Geoperception as the test bed for the exploration: logical task, PointLiesOnLine and numerical task, LineComparison. For each task, we carefully create three tasks with incremental difficulty levels. We name them as difficulty level easy, medium and hard. Based on the insight from our preliminary experiments, to increase the difficulty levels, for PointLiesOnLine, we increase the complexity of geometry shapes as is shown in Fig. 18, for LineComparison, we increase the total number of letters in letter pool while mixing geometry shapes. During our preliminary experiments, we find that sometimes the model fails to converge due to instability. To this end, for all experiment moving forward, we run the training for three times and report the best run among them (i.e., having the lowest overall training loss or testing accuracy)."}, {"title": "Lesson 1: Under the same training dataset, scaling LLM sizes does not lead to better performance.", "content": "It is commonly acknowledged that under the same training dataset, scaling up LLM can lead to better MLLM performance (Liu et al., 2024a). To this end, we first vary the sizes of LLMs, Qwen-2.5 (Team, 2024b) in a range of 0.5B, 1.5B, and 3B while keep other components consistent. The result is shown in Fig. 3. For LineComparison, Qwen-2.5-1.5B performs the best while Qwen-2.5-3B learns most slowly. For PointLiesOnLine, Qwen-2.5-1.5B and Qwen-2.5-3B performs almost the same. Qwen-2.5-0.5B learns relatively slower, but still reach almost the same final performance with two other models. In conclusion, we do not observe an obvious trend that larger LLMs can learn such low-level visual perception task faster or"}, {"title": "Lesson 2: CNN architecture performs better than ViT.", "content": "We then study the choice of visual encoder architectures, including two families of architectures: Vision Transformer (ViT) (Dosovitskiy, 2020) and ConvNeXT (Liu et al., 2022); as well as two visual representation learning objectives: language-supervised learning (Radford et al., 2021) and self-supervised learning (Oquab et al., 2023). We control the number of visual tokens to 256 for all of our vision encoders. The result is shown in Fig. 4. We find that ConvNeXt-XXLarge and ConvNeXt-Large consistently learns the geometric concept the fastest among all of the visual encoders. Notably, ConvNeXT-Large shows superior learning performance with the vision transformers which are 3-5 times larger. We hypothesize that CNN architecture extract visual features globally, effectively preserving low-level visual features. In contrast, ViT architectures split images into discrete patches, making it more challenging to retain the original low-level visual information. Self-supervised learning (SSL) visual encoders, DINO-v2, struggles to learn the geometry concept; we hypothesis this is due to the weak vision-language representation in these models. Surprisingly, although the SigLIP-family is widely-recognized as a better visual encoder (Tong et al., 2024a), we find that their performance in learning basic visual geometry attributes is limited."}, {"title": "Lesson 3: Tuning vision encoder does not provide significant help.", "content": "We next study the effect of tuning versus freezing the visual encoder. In Fig. 5, we show the testing accuracy curves of tuning and freezing visual encoders. We find that compared with using a frozen encoder, tuning the visual encoder does not help"}, {"title": "Lesson 4: Curriculum learning unleashes full potential.", "content": "Finally, we study training data composition. In our preliminary experiment Fig. 19, we observe that the model fails to converge on difficulty level 3 of PointLiesOnLine and difficulty level 2 and 3 of LineComparison. However, when using mixed training set of all three difficulty levels, the model achieves convergence, despite using the same amount of data for each difficulty levels. We hypothesize that including easier levels aids the model in learning more complex levels. To test this hypothesis, we report the test accuracy for three difficulty levels separately during the mixed training of ConvNeXt-XXLarge, in Fig. 6, on both tasks. We notice that the testing accuracy for easier tasks increase earlier and more quickly than difficulty tasks. In PointLiesOnLine tasks, we notice an apparent plateau for hard level tasks until the model has trained on approximately 20K samples. During this period, the testing accuracy for easy and medium continue to increase. This suggests that learning easier shapes can significantly help the model tackle more challenging shapes, comparing with directly learning the challenging ones, this finding align with the principles of curriculum learning.\nWhile mixed training enables effective spontaneous curriculum learning, we investigate whether a structured curriculum can further enhance model efficiency on challenging shapes. To this end, we train the model sequentially from simple to more complex shapes and compare testing accuracy just on hard level tasks. During training, we monitor the model's performance and dynamically adjust the distribution of"}, {"title": null, "content": "training data (i.e., the curriculum stage) based on this performance. Specifically, the model starts by training on the easy level data. and is evaluated when it finishes a training round, using testing accuracy from the current level of data. Upon evaluation, if the model achieves an accuracy exceeding a predefined threshold 0, the framework advances the level to the next. Formally, the update rule for advancing stages is given by:\nif accuracy > \u03b8 \u21d2 c \u2190 c + 1."}, {"title": null, "content": "The model is trained on a total of M rounds and K steps within each round. To avoid forgetting, we apply data smoothing at each stage. Specifically, we smooth our dataset distribution over all stages using an exponential attenuation function:\nratio = exp(-a\u00b7 |stages \u2013 c|),"}, {"title": null, "content": "where a denotes the attenuation rate. Eq. (3) ensures that stages proximal to the current stage receive higher sampling probabilities.\nWe refer to this as our curriculum training strategy. Specifically, the accuracy threshold for advancing training stage 0 is set to 0.99. We train all the models for M = 30 rounds, each round with K = 50 steps. The results are shown in Fig. 7. Firstly, we find that all of the models fail to converge when trained purely on hard level for PointLiesOnLine task. In contrast, the mixed training strategy shown by the red curve, consistently reaches faster convergence on hard level. Curriculum training strategy, shown by the purple curve, proves more efficient than mixed training."}, {"title": "5. Euclid: a Family of MLLMs for Geometric Visual Perception", "content": "In this section, we take all of the lessons we learned in the previous sections and train Euclid, a family of MLLMs specifically designed for strong geometric LLVP.\nOn-the-fly progressive training. We use the same strategy as the curriculum training in Section 4, but scale our training to all tasks in Geoperception. For each task, we create N stages of training dataset shapes with progressively increasing geometric complexity.\nSpecifications. For models, we select the best visual encoder architecture we found in our investigation, ConvNeXt, including ConvNeXt-Large@512 and ConvNeXt-XXLarge@512, and keep the same multimodal connector (2 layers MLP) and LLM (Qwen2.5-1.5B-instruct). The accuracy threshold for advancing training stage @ is set to 0.99. All models are trained on N = 3 stages with manually curated geometry shapes and M = 50 rounds with K = 500 steps in each round, and the batch size is 64 for each training step. The total training dataset volume for both of the models is 1.6M."}, {"title": "Evaluation results.", "content": "The results are shown in Table 4. Overall, although only trained on very simple synthetic geometry shapes, and using only a 1.5B language model, Euclid significantly outperforms current leading MLLMs in most of the tasks, showing strong generalization abilities on real-world geometry LLVP. Notably, in the PointLiesOnLine task, which is particularly challenging for existing MLLMs, Euclid achieves up to 82.98% accuracy, more than three times the performance of Gemini-1.5-Pro. On all both numerical tasks, LineComparison and AngleClassification, Euclid keeps superior performance. However, on three annotation tasks, Euclid's performance is limited. We hypothesis this is due to the limited setting of our annotation types and styles, making the model hard to generalize to diverse human geometry annotations."}, {"title": "Error analysis.", "content": "We take a deep look into Euclid's prediction on Geoperception, we find that our model's performance is hindered when diagrams are heavily annotated. An example is shown in Fig. 8, where a line is annotated by \u201cx\u201d, confusing the model from choosing the correct point. Incorporating training data with more diverse annotation types, geometry shape and can better distinguish different diagram annotation types could potentially help the model with such scenarios."}, {"title": "6. Conclusion and Future Work", "content": "In this work, we highlight the importance of accurate low-level visual perception(LLVP) in MLLMs. To this end, we first introduce Geoperception, a large-scale multimodal benchmark focused exclusively on geometry-domain visual perception. We evaluate leading MLLMs on Geoperception, find that even top models such as Gemini-1.5-Pro struggle significantly it, although it is straightforward for humans. We then conduct an empirical study to explore the design space of MLLM training and architectures using the dataset generated by a geometric high-fidelity synthetic-data engine that we develop. Our study indicate that convolutional neural network visual encoders outperform vision transformers in our tasks; tuning the visual encoder generally enhances performance; and employing a curriculum-based training approach yields much more model potential than direct task training. Based on insights from this study, we develop Euclid, a model trained purely on high-fidelity synthetic generated data, which generalizes effectively to real-world geometric shape understanding tasks, surpassing the leading MLLMs by a substantial margin.\nFuture work. Our work examines the potential of using synthetic multimodal data to improve MLLM performance in low-level geometric perception tasks. However, there are still directions that remain under-explored: (1) Automatic curriculum learning. Incorporating a more diverse dataset, including varied geometric shapes and different domain dataset, introduces challenges in defining the learning order. Rule based definition and manual curation may become impractical, necessitating automated strategies like hard negative sampling to organize the curriculum based on training loss or testing accuracy. This approach could streamline the process, reduce human effort, provide more suitable and efficient curriculum learning orders. (2) Using a more-diverse training dataset. Currently, the text portion of our synthetic multimodal training data uses a restricted set of templates, and the model trained on such templates could fail to generalize to other question types; it could therefore be beneficial to increase the diversity of our training images as well as the instruction-following formats. (3) Generalizing to other task domains. In this work, our study is focused on data from 2D geometry, as it provides a focused test bed of fundamental tasks. We believe the lessons we learn from this domain can be effectively generalized to a broader set of downstream domains that benefit from high-quality LLVP."}, {"title": "Reproducibility Statement", "content": "In Section 3, we provide a comprehensive description of the procedure for generating the Geoperception benchmark. This includes employing GPT-40-mini for dataset filtering and detailing the conversion of logical forms into questions and answers. Evaluation prompts for MLLMs on different types of Geoperception questions are presented in Appendix B. For model architecture exploration, we specify the visual encoders and provide corresponding Hugging Face links in Table 3. Additionally, we outline the LLMs and multimodal connector architectures used. For our Euclid model, we include all geometry shape code used for training, along with demonstration diagrams and pseudo-code for generating training questions and answers."}, {"title": "A. Geoperception Benchmark Details", "content": "In Table 5, we provide more details on the Geoperception benchmark, such as the number of logic forms present before and after filtering, the number of questions, and the number of images. AngleClassification and LineComparison are directly derived from points coordinates without filtering."}, {"title": "B. Prompts for the Geoperception Dataset Evaluation", "content": "PROMPT TEMPLATE FOR THE POINTLIESONLINE TASK\nAnswer me directly just with the all points lie on the line mentioned in the\nquestion (do not include the point mentioned in the question).\nAnswer template:\nOr\n(If only one point) The other point is: \"your point\".\n(if multiple points) The other points are: \"your points\".\nFor example:\nOr\nThe other point is:\nA\nThe other points are: A, B, C\nPROMPT TEMPLATE FOR THE POINTLIESONCIRCLE TASK\nAnswer me directly just with the all points lie on the circle mentioned in the\nquestion.\nAnswer template:\nOr\n(If only one point) The point is: \"your point\".\n(if multiple points) The points are: \"your points\".\nFor example:\nThe point is:\nOr:\nA\nThe points are: A, B, C\nPROMPT TEMPLATE FOR THE PARALLEL TASK\nAnswer me directly just with the all lines which are parallel to the line\nmentioned in the question (do not include the line mentioned in the question).\nAnswer template:\nOr\n(If only one line) The line is: \"your line\".\n(if multiple lines) The lines are: \"your lines\".\nFor example:\nThe line is: BC\nOr:\nThe lines are: BC, DE"}, {"title": null, "content": "PROMPT TEMPLATE FOR THE PERPENDICULAR TASK\nAnswer me directly just with the all lines which are perpendicular to the line\nmentioned in the question (do not include the line mentioned in the question).\nAnswer template:\nOr\n(If only one line) The line is: \"your line\".\n(If multiple lines) The lines are: \"your lines\".\nFor example:\nThe line is: BC\nOr:\nThe lines are: BC, DE\nPROMPT TEMPLATE FOR THE EQUALS TASK\nAnswer me directly just with the annotations presented on the image.\nAnswer template:\nThe annotation is: \"your annotation\".\nFor example:\nThe annotation is: 2x+4\nOr:\nThe annotations is: 90\nPROMPT TEMPLATE FOR THE ANGLE CLASSIFICATION TASK\nAnswer me directly just with the classification of the angle mentioned in the\nquestion.\nAnswer template:\nThe angle is: \"your angle\".\nFor example:\nThe angle is: acute\nOr:\nThe angle is: obtuse\nPROMPT TEMPLATE FOR THE LINECOMPARISON TASK\nAnswer me directly just with the longer line mentioned in the question.\nAnswer template:\nThe longer line is: \"your line\".\nFor example:\nThe longer line is: BC\nOr:\nThe longer line is: DE"}, {"title": "C. Details for Training Data Engine", "content": "In this section, we provide all geometry shapes we use for Euclid training, including the pseudocode for generating text describing the geometry shapes and diagram examples."}, {"title": "C.1. Pseudocode for Training Textual Dataset Synthesis", "content": "Algorithm 1 Data Synthesis for the POINTLIESONLINE Task\n1: Input: data_info, points_set\n2: Output: data\n3: for points_set \u2208 data_info do\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n,,\n,,\n12:\n13:\n14:\n15:\n16:\nfor (A, B) e permutations(points_set, 2) do\nall_rest_points \u2190 [p for p in points_set if p not in [A, B]]\nfor rest_points E permutations(all_rest_points) do\nverb_agreement \u2190 'is' if len(rest_points) == 1 else 'are'\nrest_points \u2190 [f\"{p}\" for p in rest_points]\nrest_points \u2190 sorted(rest_points)\nquestion \u2191 'What is the point lying on line ' + A + B + '?'\nanswer \u2190 'The point lying on line ' + A + B + ', ' + verb_agreement +\n+ ', '.join(rest_points)\ngt \u2190 \".join(rest_points)\ndata \u2190 {'question': question, 'answer': answer, 'gt': gt}\nend for\nend for\nend for"}, {"title": null, "content": "Algorithm 2 Data Synthesis for the POINTLIesOnCircle Task\n1: Input: data_info\n2: Output: data\n3: point_set \u2190 random.choice(list(data_info.items()))\n4: center_point \u2190 point_set[0]\n5: target_points \u2190 point_set[1]\n6: target_points <<<\u2190 sorted(target_points)\n7: question \u2190 'What are the point lying on circle + center_point + '?'\n8: answer \u2190 'The point lying on circle + center_point + are + ', ', '.join(target_points)\n9: gt \u2190 \". \".join(target_points)\n10: data \u2190 {\u2019question': question, 'answer': answer, 'gt': gt}"}, {"title": null, "content": "Algorithm 3 Data Synthesis for the ANGLECLASSIFICATION Task\n1: Input: data_info\n2: Output: data\n3: angle data_info\n4: angle_options \u2190 [f'{angle[1][0]}{angle[1][1]}{angle[1][2]}',f'{angle[1][2]}{angle[1][1]}{angle[1][0]}']\n5: angle_letter \u2190 random.choice(angle_options)\n6: angle_class \u2190 'acute' if angle[0] < 90 else 'obtuse'\n7: question \u2190 'Is angle + angle_letter + ' acute or obtuse?'\n8: answer \u2190 'Angle + angle_letter + ' is ' + angle_class\n9: gt angle_class\n10: data \u2190 {'question': question, 'answer': answer, 'gt': gt}"}]}