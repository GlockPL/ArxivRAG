{"title": "UNI: Unlearning-based Neural Interpretations", "authors": ["Ching Lam Choi", "Alexandre Duplessis", "Serge Belongie"], "abstract": "Gradient-based interpretations often require an anchor point of comparison to avoid saturation in computing feature importance. We show that current baselines defined using static functions-constant mapping, averaging or blurring-inject harmful colour, texture or frequency assumptions that deviate from model behaviour. This leads to accumulation of irregular gradients, resulting in attribution maps that are biased, fragile and manipulable. Departing from the static approach, we propose UNI to compute an (un)learnable, debiased and adaptive baseline by perturbing the input towards an unlearning direction of steepest ascent. Our method discovers reliable baselines and succeeds in erasing salient features, which in turn locally smooths the high-curvature decision boundaries. Our analyses point to unlearning as a promising avenue for generating faithful, efficient and robust interpretations.", "sections": [{"title": "1 Introduction", "content": "The utility of large models is hampered by their lack of explainability and robustness guarantees. Yet breakthroughs in language modelling (Meta, 2024; Anthropic, 2024; Jiang et al., 2023; Google, 2024; Achiam et al., 2023) and generative computer vision (Rombach et al., 2022; Liu et al., 2023; Deepmind, 2024; Brooks et al., 2024) yield promising high-stakes applications, spanning domains of healthcare, scientific discovery, law and finance. As such, being able to interpret these models has become a primary concern for researchers, policymakers and the general populace, with international calls for explainability, accountability and fairness in AI decision-making (European Commission, 2021; White House OSTP, 2022; Bengio et al., 2023). To this end, recent works focus on the 2 main directions of making models inherently explainable (B\u00f6hle et al., 2022; Brendel & Bethge, 2018; Koh et al., 2020; Bohle et al., 2021; Chen et al., 2019; Ross et al., 2017) and post-hoc interpretable (Bau et al., 2017; Kim et al., 2018; Zhou et al., 2018; Ghorbani et al., 2019b). Unfortunately, the former is marred by the status quo of proprietary models and prohibitive training costs. This motivates seeking robust attributions which reliably explain model predictions, to facilitate better risk assessment and trade-off calibration (B\u00f6hle et al., 2022; Doshi-Velez & Kim, 2017).\nPost-hoc methods explain a black-box model's output by attributing its decision back to predictive features of the input. They achieve this via leveraging components of the model itself (e.g. gradients and activations), or through approximation with a simpler, interpretable simulator. A desirable post-hoc explanation should exhibit high faithfulness to be rationale-consistent (Yeh et al., 2019; Atanasova et al., 2020) with respect to a model's decision function; low sensitivity to yield reliably similar saliency predictions for input features in the same local neighbourhood (Alvarez Melis & Jaakkola, 2018; Ghorbani et al., 2019b); low complexity \u2013 the explanation should be functionally simpler and more understandable than the original black-box model (Bhatt et al., 2021).\nGradient-based saliency methods are widely used for feature attribution, due to their simplicity, efficiency and post-hoc accessibility. This can be further decomposed into 3 families: perturbative, backpropagative and path-based, which we detail in Section 6. Gradient-based attribution is intuitive since the first-order derivative reveals which features significantly influence the model's classification decision. However, naively using local gradients yields unfaithful attributions due to saturation, where the non-linear output function flattens in vicinity of the input and zero gradients are"}, {"title": "2 Preliminaries", "content": "We consider feature attribution for trained deep neural networks within image classification. Informally, we seek to assign scores to each pixel of an image for quantifying the pixel's influence (sign and magnitude) on the predicted output class confidence. It is noteworthy that attributions can be signed: a negative value indicates that removing the pixel increases the target class probability."}, {"title": "2.1 Notation", "content": "The input (feature) space is denoted as $X \\subset \\mathbb{R}^{d_x}$, where $d_x$ is the number of pixels in an image. The output (label) space is $Y \\subset \\mathbb{R}^{d_y}$; $\\mathcal{Y}$ is the set of all probability distributions on the set of classes. The model space is denoted as $\\mathcal{F} \\subset \\mathcal{Y}^X$. A trained model $F : x \\rightarrow (F_1(x), ..., F_{d_y}(x))$ returns the probability score $F_c(x)$ of each class $c$. Attribution methods are thus functions $A : \\{1, ..., d_x\\} \\times \\mathcal{F} \\times \\{1, ..., d_y\\} \\times X \\rightarrow \\mathbb{R}$, where $A(i, F, c, x)$ is the importance score of pixel $i$ of image $x$ for the prediction made by $F_c$. For convenience, we use the shorthand $A_i(x)$ to refer to the attributed saliency score of a pixel $i$ for a specific class prediction $c \\in \\{1, ..., d_y \\}$. We express a linear path feature as $\\gamma(x', x, \\alpha) : \\mathbb{R}^{d_x} \\times \\mathbb{R}^{d_x} \\times [0,1] \\rightarrow \\mathbb{R}^{d_x}$, where $\\gamma = (1 - \\alpha)x' + \\alpha x$ and employ shorthands $\\gamma(0) = x'$, $\\gamma(1) = x$."}, {"title": "3 Gradient-based Attributions in a Nutshell", "content": ""}, {"title": "3.1 Limitations", "content": "Taking the local gradients of a model's output confidence map $F_c(x)$ \u2013 for target class $c$ \u2013 is a tried and tested method for generating explanations. Commonly termed Simple Gradients (Erhan et al., 2009; Baehrens et al., 2010; Simonyan et al., 2013), $A_i(x) = \\nabla_x F_c(x)$ can be efficiently computed for most model architectures. However, it encounters output saturation when activation functions like ReLU and Sigmoid are used, leading to zero gradients (hence null attribution) even for important features (Sundararajan et al., 2017; 2016). DeepLIFT (Shrikumar et al., 2016) reduces saturation by introducing a \u201creference state\". A feature's saliency score is decomposed into positive and negative contributions by backpropagating and comparing each neuron's activations to that of the baseline. Integrated Gradients (IG) (Sundararajan et al., 2017) similarly utilises a reference, black image and computes the integral of gradients interpolated on a straight line between the image and the baseline.\n$A_i^{IG}(x) = (x_i - x'_i)\\int_{\\alpha=0}^{1} \\nabla_{x}F_c (x' + \\alpha(x \u2013 x')) d\\alpha$\nPractically, the integral is approximated by a Riemann sum. Of existing methods, IG promises desirable, game-theoretic properties of \"sensitivity\u201d, \u201cimplementation invariance\u201d, \u201ccompleteness\u201d and \u201clinearity\u201d. We consequently focus"}, {"title": "3.2 Post-hoc Biases are Imposed", "content": "Since the baseline represents an absence of or reduction in salient features, static baseline functions (e.g. black, blurred, noised) implicitly assume that similar features (e.g. dark, smooth, high-frequency) are irrelevant for model prediction. To illustrate this intuition, we can consider IG with a black baseline, wherein it becomes more difficult to attribute dark but salient pixels. Due to the colour bias that \u201cnear-black features are unimportant\", the term $(x_i \u2013 x'_i)$ is small and requires a disproportionately large gradient $\\nabla_{x_i} F_c(.)$ to yield non-negligible attribution scores. Indeed, this is what we observe in Figures 2, 3, 8, where darker features belonging to the object-of-interest cannot be reliably identified. We further empirically verify that each static baseline imposes its own post-hoc bias by experimenting on ImageNet-C (Hendrycks & Dietterich, 2019). Corresponding to the 3 popular baseline choices for IG (all-black, gaussian blurred, gaussian noised), we focus on the families of digital (brightening and saturation), blur (gaussian and defocus blur) and noise (gaussian and shot noise) common-corruptions. Figures 4, 9 demonstrate that IG with a blurred baseline fails to attribute blurred inputs due to saturation and overly smoothed image textures; Figures 5, 10 visualise how a noised IG baseline encounters high-frequency noise and outputs irregular, high-variance attribution scores, even for adjacent"}, {"title": "4 UNI: Unlearning-based Neural Interpretations", "content": ""}, {"title": "4.1 Baseline desiderata", "content": "A desirable baseline should preserve the game-theoretic properties of path-attribution (Section 3.1) and refrain from imposing post-hoc attribution biases (Section 3.2). For every given task-model-image triad, a well-chosen baseline should be 1. image-specific be connected via a path feature of low curvature to the original image; 2. reflect only the model's predictive biases\u2014salient image features should be excluded from the baseline; be 3. less task-informative than the original image\u2014interpolating from the baseline towards the input image should yield a path of increasing predictive confidence. We now introduce the UNI pipeline: first, unlearn predictive information in the model space; then, use activation-matching between unlearned and trained models to mine a featurelesss baseline in the image space; finally, interpolate along the low-curvature, conformant and consistent path from baseline to image to compute reliable explanations in the attributions space."}, {"title": "4.2 Desirable Path Features", "content": "Proximity The meaningfulness of the attributions highly depends on the meaningfulness of the path. We aim for a smooth transition between absence and presence of features; and this intuitively cannot be achieved if the baseline and input are too far apart. (Srinivas & Fleuret, 2019) formalizes this intuition through the concept of weak dependence, and proves that this property can only be compatible with completeness in the case where the baseline and the input lie in the same connected component (in the case of piecewise-linear models). An obvious implementation of this proximity condition in the general case is to bound the distance $||x - x' ||$ to a certain value $\\epsilon$. This is strictly enforced in Algorithm 1 by normalizing the perturbation at each step $t$.\nLow Curvature. The curvature of the model prediction along the integrated path has been identified (Dombrowski et al., 2019) as one of the key factors influencing both the sensitivity and faithfulness of the computed attributions. We substantiate the intuition that a smooth and regular path is preferred by analysing the Riemannian sum calculation. Assuming that the function $g : \\alpha \\in [0,1] \\leftrightarrow \\nabla F_c(x' + \\alpha(x \u2013 x'))$ is derivable with a continuous derivative (i.e. $C^1$) on the segment $[x', x]$, elementary calculations and the application of the Taylor-Lagrange inequality give the following error in the Riemann approximation of the attribution,\n$\\begin{aligned}\n    \\left|\\int_{\\alpha=0}^{1} g(\\alpha) d\\alpha - \\frac{1}{B}\\sum_{k=1}^{B} g(\\frac{k}{B})\\right| &< \\frac{(x_i - x'_i)}{B}\\sum_{k=1}^{KB} |g'(\\alpha)| \\frac{\\| x - x'\\|^2}{2B}\n\\end{aligned}$\nwhere $M = \\max_{\\alpha\\in [0,1]} |\\frac{dg}{d\\alpha}| = \\max_{\\alpha\\in [0,1]} |\\frac{\\partial^2 F_c(x'+\\alpha(x-x'))}{\\partial \\alpha^2}|$ exists by continuity of $g'$ on $[0, 1]$.\nThus, lower curvature along the path implies a lower value of the constant $M$, which in turn implies a lower error in the integration calculation. A smaller value $B$ of Riemann steps is needed to achieve the same precision. More generally, a low curvature (i.e. eigenvalues of the hessian) on and in a neighborhood of the baseline and path reduces the variability of the calculated gradients under small-norm perturbations, increasing the sensitivity and consistency of the method. Empirically, we observe a much lower curvature of the paths computed by UNI, as can be seen in Table 1 and Appendix Figures 17, 18, 19, 20, 21, 22. Figure 7 also confirms the increased robustness to Riemann sum error induced.\nMonotonic. Intuitively, the path $\\gamma$ defined by interpolating from the \u201cfeatureless\" baseline $x'$ to the input image $x$ should be monotonically increasing in output class confidence; the cumulative attribution score should similarly increase. At the image level, for all $j$, $k$ such that $j \\leq k$, since $|\\gamma(j) \u2013 x| \\geq |\\gamma(k) \u2013 x|$, therefore the predictive confidence should be non-decreasing and order-preserving: $F_c(\\gamma(j)) \\leq F_c(\\gamma(k))$. Constraining $\\gamma$ to be monotonically increasing suffices to satisfy criteria for valid path features (Akhtar & Jalwana, 2023). Zooming in on input feature $z_i = \\gamma(k)_i$ on the linear path for $k < 1$, since the gradient of $\\gamma$ is always positive and $x_i$ maximises $F$ on the interval, therefore both conditions $\\text{sgn}(\\nabla_{x_i} F_c(x_i))\\cdot \\text{sgn}(\\nabla_{z_i} F_c(z_i)) = 1$ and $|\\nabla_{x_i} F_c(x_i)| > |\\nabla_{z_i} F_c(z_i)|$ are naturally met. This is crucial for preserving the completeness axiom of Integrated Gradients (Sundararajan et al., 2017), that attribution scores add up to the difference quantity $\\sum_i A_i(x) = F_c(x) \u2013 F_c(x')$, and that attribution score $A_i(x)$ is only non-zero when $x_i$ contributes to the prediction. Completeness does not hold when path features are non-conformant, i.e. $F_c(\\gamma(j)) > F_c(\\gamma(k))$; or when extremities exist along $\\gamma$, i.e. for $k \\in (0,1)$, $F_c(\\gamma(k)) > F_c((\\gamma(1))$ or $F_c(\\gamma(k)) < F_c((\\gamma(0))$."}, {"title": "5 Experiments", "content": "We experiment on ImageNet-1K (Deng et al., 2009), ImageNet-C (Hendrycks & Dietterich, 2019) and compare against various path-based and gradient-based attribution methods. This includes IG (Sundararajan et al., 2017), BlurIG (Xu et al., 2020), GIG (Kapishnikov et al., 2021), AGI (Pan et al., 2021), GBP (Springenberg et al., 2014) and DeepLIFT (Shrikumar et al., 2016). We consider a diverse set of pre-trained computer vision backbone models (Paszke et al., 2019), including ResNet-18 (He et al., 2016), EfficientNet-v2-small (Tan & Le, 2021), ConvNeXt-Tiny (Liu et al., 2022), VGG-16-bn (Simonyan & Zisserman, 2015), ViT-B_16 (Dosovitskiy et al., 2020) and Swin-Transformer-Tiny (Liu et al., 2021). Unless otherwise specified, we the following hyperparameters: unlearning step size $\\eta = 1$; $l_2$ PGD with $T = 10$ steps, a budget of $\\epsilon = 0.25$, step size $\\mu = 0.1$; Riemann approximation with $B = 15$ steps. Our results verify UNI's high faithfulness, stability and robustness."}, {"title": "5.1 Faithfulness", "content": "We report MuFidelity scores (Bhatt et al., 2021), i.e. the faithfulness of an attribution function $A$, to a model $F$, at a sample $x$, for a subset of features of size $|S|$, given by $\\mu_f(F, A; x) = \\text{corr}_{|S|}(\\{A(i, F, c, x)\\}, \\{F_c(x) - F_c(x[x_{|S|}=0])\\})$. We record the (absolute) correlation coefficient between a randomly sampled subset of pixels and their attribution scores. As from Table 2, UNI outperforms other methods across all settings but one, indicating high faithfulness. We supplement these numbers with visual comparisons in Appendix Figures 11, 12, 13, 14, 15, 16 against IG (black and noised baselines), BlurIG, GIG, AGI, GBP, DeepLift."}, {"title": "5.2 Robustness", "content": "Next, we evaluate UNI's robustness to fragility adversarial attacks on model interpretations. Following Ghorbani et al. (2019a), we design norm-bounded attacks to maximise the disagreement in attributions whilst constraining that the prediction label remains unchanged. We consider a standard $l_\\infty$ attack designed with FGSM (Goodfellow et al., 2014), with perturbation budget $\\epsilon_f = 8/255$.\n$\\begin{aligned}\n    \\delta_f = &\\underset{\\delta}{\\text{arg max}} \\frac{1}{d_x} \\sum_{i=1}^{dx} \\text{d}(A(i, F, c, x), A(i, F, c, x + \\delta_f))\n    \\\\ &\\text{subject to }  \\underset{c'}{\\text{arg max}} F_{c'}(x) = \\underset{c}{\\text{arg max}} F_{c}(x + \\delta_f) = c\n\\end{aligned}$\nWe report robustness results using 2 distance measures-Spearman correlation coefficient in Figure 3 and top-k pixel intersection score in Figure 4\u2014pre and post attack. While other methods like DeepLIFT (DL), BlurIG, Integrated Gradients (IG) are misled to output irrelevant feature saliencies, UNI robustly maintains attribution consistency and achieves the lowest attack attribution disagreement scores (before and after FGSM attacks) for both metrics."}, {"title": "5.3 Stability", "content": "We compare UNI and other methods' sensitivity to Riemann approximation noise, which manifests in visual artifacts and misattribution of salient features. As seen from Figures 6, 7, UNI reliably finds unlearned, \"featureless\" baselines for"}, {"title": "6 Related Work", "content": "Machine unlearning. We draw inspiration from the high-level principle of unlearning, which concerns the targeted \"forgetting\" of a data-point for a trained model, by localising relevant information stored in network weights and introducing updates or perturbations (Bourtoule et al., 2021). Formally, machine unlearning can be divided into exact and approximate unlearning (Nguyen et al., 2022). Exact unlearning seeks indistinguishability guarantees for output and weight distributions, between a model not trained on a sample and one that has unlearned said sample (Ginart et al., 2019; Thudi et al., 2022; Brophy & Lowd, 2021). However, provable exact unlearning is only achieved under full re-training, which can be computationally infeasible. Hence, approximate unlearning was proposed stemming from e-differential privacy (Dwork, 2011) and certified removal mechanisms (Guo et al., 2020; Golatkar et al., 2020). The former guarantees unlearning for $\\epsilon = 0$, i.e. the sample has null influence on the decision function; the latter unlearns with first/second order gradient updates, achieving max-divergence bounds for single unlearning samples. Unlearning naturally lends itself to path-based attribution, to localise then delete information in the weight space, for the purposes of defining an \"unlearned\" activation. This \u201cunlearned\" activation can be used to match the corresponding, \"featureless\" input, where salient features have been deleted during the unlearning process. While the connection to interpretability is new, a few recent works intriguingly connect machine unlearning to the task of debiasing classification models during training and evaluation (Chen et al., 2024; Kim et al., 2019; Bevan & Atapour-Abarghouei, 2022).\nPerturbative methods. Perturbative methods perturb inputs to change and explain outputs (Sculley et al., 2015), including LIME (Ribeiro et al., 2016), SHAP, KernelSHAP and GradientSHAP (Lundberg et al.), RKHS-SHAP (Chau et al., 2022), ConceptSHAP (Yeh et al., 2020), InterSHAP (Janzing et al., 2020), and DiCE (Kommiya Mothilal et al., 2021). LIME variants optimise a simulator of minimal functional complexity able to match the black-box model's local behaviour for a given input-label pair. SHAP (Lundberg et al.) consolidates LIME, DeepLIFT (Shrikumar et al., 2016), Layerwise Relevance Propagation (LRP) (Montavon et al., 2019) under the general, game-theoretic framework of additive feature attribution methods. For this framework, they outline the desired properties of local accuracy, missingness, consistency; they propose SHAP values as a feature importance measure which satisfies these properties under mild assumptions to generate model-agnostic explanations. However, such methods fail to give a global insight of the model's decision function and are highly unstable due to the reliance on local perturbations (Fel et al., 2022). Bordt et al. (2022) show that this leads to variability, inconsistency and unreliability in generated explanations, where different methods give incongruent explanations which cannot be acted on.\nBackpropagative methods. Beginning with simple gradients (Erhan et al., 2009; Simonyan et al., 2013), this family of methods also, LRP (Montavon et al., 2019), DeepLIFT (Shrikumar et al., 2016), DeConvNet (Zeiler & Fergus, 2014), Guided Backpropagation (Springenberg et al., 2014) and GradCAM (Selvaraju et al., 2017)\u2014leverages gradients of the output w.r.t. the input to proportionally project predictions back to the input space, for some given neuron activity of interest. Gradients of neural networks are, however, highly noisy and locally sensitive \u2013 they can only crudely localise salient feature regions. While this issue is partially remedied by SmoothGrad (Smilkov et al., 2017), we still observe that gradient-based saliency methods have higher sample complexity for generalisation than normal supervised training (Choi & Farnia, 2024) and often yield inconsistent attributions for unseen images at test time.\nPath-based attribution. This family of post-hoc attributions is attractive due to its grounding in cooperative game-theory (Friedman, 2004). It comprises Integrated Gradients (Sundararajan et al., 2017), Adversarial Gradient Integration (Pan et al., 2021), Expected Gradients (EG) (Erion et al., 2021), Guided Integrated Gradients (GIG) (Kapishnikov et al., 2021) and BlurIG (Xu et al., 2020). Path attribution typically relies on a baseline \u2013 a \u201cvanilla\" image devoid of features; a path-an often linear path from the featureless baseline to the target image-along which the path integral is computed for every pixel. Granular control over the attribution process comes with difficulties of defining an unambiguously featureless baseline (for each (model, image) pair) (Sturmfels et al., 2020) and then defining a reliable path of increasing label confidence without intermediate inflection points (Akhtar & Jalwana, 2023). To measure the discriminativeness of features identified by attribution methods, experimental benchmarks such as ROAR (Hooker et al., 2019), DiffRAOR (Shah et al., 2021), RISE (Petsiuk et al., 2018) and the Pointing Game (Zhang et al., 2018) have been proposed."}, {"title": "7 Conclusion", "content": "In this work, we formally discuss the limitations of current path-attribution frameworks, outline a new principle for optimising baseline and path features, as well as introduce the UNI algorithm for unlearning-based neural interpretations. We empirically show that present reliance on static baselines imposes undesirable post-hoc biases which are alien to the model's decision function. We account for and mitigate various infidelity, inconsistency and instability issues in path-attribution by defining principled baselines and conformant path features. UNI leverages insights from unlearning to eliminate task-salient features and mimic baseline activations in the \u201cabsence of signal\". It discovers low-curvature, stable paths with monotonically increasing output confidence, which preserves the completeness axiom necessary for path attribution. We visually, numerically and formally establish the utility of UNI as a means to compute robust, meaningful and debiased image attributions. Future extensions to UNI include going beyond first-order approximate unlearning towards adopting certified, second-order machine learning techniques; as well as granular investigations on how the baseline definition and model's robustness and inductive biases exert influence on path attribution results."}]}