{"title": "Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's Framework for Explainability in Al", "authors": ["Georgios PAVLIDIS"], "abstract": "The lack of explainability of Artificial Intelligence (AI) is one of the first obstacles that the industry and regulators must overcome to mitigate the risks associated with the technology. The need for 'eXplainable AI' (XAI) is evident in fields where accountability, ethics and fairness are critical, such as healthcare, credit scoring, policing and the criminal justice system. At the EU level, the notion of explainability is one of the fundamental principles that underpin the AI Act, though the exact XAI techniques and requirements are still to be determined and tested in practice. This paper explores various approaches and techniques that promise to advance XAI, as well as the challenges of implementing the principle of explainability in Al governance and policies. Finally, the paper examines the integration of XAI into EU law, emphasising the issues of standard setting, oversight, and enforcement.", "sections": [{"title": "The Imperative of Explainability: Navigating the Landscape of Al's Impact and Risks", "content": "Artificial intelligence (AI) has emerged as a fascinating and influential force in today's technological and business worlds. Al has already started to streamline mundane tasks, advance critical domains of scientific research and disrupt professions and industries. Whether in finance, commerce, healthcare or other fields, Al-based tools are steadily increasing their presence, thus permeating various organisational processes and decision-making.\nAs businesses realise the value of adopting Al, investment in the technology has surged, with global Al private investment reaching $91.9 billion in 2022, a sum 18 times greater than in 2013. As a result, the global Al market, valued at $428 billion in 2022, is projected to grow to approximately $515.31 billion by 2023 and $2,025.12 billion by 2030. AI tools have become essential for enterprises worldwide, with the percentage of companies using them doubling since 2017 and ranging between 50 and 60 percent in recent years. Moreover, the average number of Al capabilities (natural-language generation, robotic process automation, computer vision, etc.) employed in business units has also doubled, rising from 1.9 in 2018 to 3.8 in 2022.\nHowever, despite this excitement in the business world, serious concerns have arisen about the risks associated with AI. Historically, new technologies have often produced fear, but many of the concerns linked to AI seem justified. Ethical dilemmas have become more pronounced as Al systems have started making decisions that affect human lives. This raises questions about accountability and transparency as well as the integration of these principles into law. At the societal level, there is the risk of bias and discrimination, as Al tools might inadvertently perpetuate existing inequalities. Furthermore, the spectre of job displacement looms large, as automation and Al-driven processes threaten to reshape industries, increase labour market disparities and radically alter the employment landscape. Security risks are coming to the forefront because the increasing reliance on Al creates new avenues for cyberattacks and data breaches. Alongside these challenges, the concentration of power and data in the hands of a few dominant tech companies and entities raises concerns about the potential misuse of the technology in markets, public discourse and even politics.\nWe argue that the lack of explainability of Al is one of the first obstacles that the industry, regulators and supervisors must overcome to mitigate the above- mentioned risks. Usually, AI algorithms operate behind a veil of opacity, a situation also referred to as the 'black box problem'. In this situation, the inputs and outputs of an Al model may be known, but how exactly the inputs are transformed into outputs is difficult to determine. AI opacity arises due to the complex architectures of Al models (particularly deep learning and neural networks), the high degree of non-linearity, the numerous layers and interconnected nodes, the high-dimensional data with their numerous features, the fact that learnt representations in Al models might not directly correspond to human-understandable features, and other factors. As a result, after the design and the learning environment of the Al model are established, the model determines the value of specific parameters and the way the answer is reached, which sometimes baffles its developers, especially when deep learning and neural networks are used.\nThe field of 'eXplainable Al' attempts to (XAI) bridge the gap between the opacity of machine decision-making and the human demand for comprehensibility. While transparency aims to unveil the inner workings of these systems, explainability goes beyond mere disclosure, revealing the cognitive processes that underlie decision-making. Therefore, ensuring XAI is more important and much more difficult, than ensuring transparency. Nevertheless, XAI diverges significantly from human explanation and justification in several key aspects, rooted in the disparities between machine learning algorithms and human cognitive processes. Unlike human cognition, Al operates through complex algorithms and neural networks, often yielding outputs that defy straightforward explication. Human explanations often draw upon personal experiences, background knowledge, and intuition, allowing for a narrative that encompasses context and emotional nuances. In contrast, Al algorithms process vast amounts of data and identify patterns that may elude human perception. Furthermore, human justifications often involve ethical, moral, or subjective considerations, while AI lacks inherent moral reasoning and operates based on learned patterns, potentially leading to decisions devoid of ethical dimensions. Finally, human justifications are also subject to interpretability and may be shaped by cultural, historical, or individual factors, contrary to the objectivity sought in Al systems.\nThe need for XAI is evident in fields where accountability, ethics and fairness are critical, such as healthcare, credit scoring, policing and the criminal justice system. The opacity of Al models is the common denominator in most criticisms of this new technology. For example, Al systems are criticised for generating unintended consequences, bias and violations of human rights, but it is the lack of explainability that hinders the identification, prevention, and mitigation of these problems. In the absence of clear explanations of Al's underlying decision-making processes, the relevant authorities have no way to ensure compliance and supervision through regulations, regardless of when and how these will be established. Therefore, the principle of explainability is neither a simple software problem nor just an intellectually challenging quest for knowledge; it is a prerequisite for accountability, fairness, public trust, and effective regulation and supervision.\nRecognising both the transformative potential and the significant risks of digitalisation, in particular AI, the European Union (EU) has taken ambitious steps to regulate digital innovation. Following the adoption of two landmark legislative instruments \u2013 the Markets in Crypto-Assets Regulation (MiCA) and the Digital Operational Resilience Act (DORA) \u2013 the next breakthrough at the EU level has been the initiative to adopt the Artificial Intelligence Act, which aims to harness Al's potential while mitigating its dangers. The notion of explainability is one of the fundamental principles that underpin this legislative initiative, though the exact XAI techniques and requirements are still to be determined and tested in practice.\nIn the following sections, we will examine the philosophy, key provisions and intricacies of the EU AI Act (section 2); we will then delve deep into the principle of explainability, exploring various approaches and techniques that promise to advance XAI (section 3). We then shift our focus to the challenges of implementing the principle of explainability in Al governance and policies (section 4). Finally, we examine the integration of XAI into EU law (section 5), emphasising the issues of standard setting, oversight and enforcement."}, {"title": "The EU AI Act: Searching for Rules and Clarity", "content": "EU policymakers have consistently stressed the importance of fostering trustworthy, responsible and \u2018human-centric' innovation in the field of AI. From the 2016 EU Global Strategy for Foreign and Security Policy to the 2021 European Commission's plan for AI, the EU has recognised the need to adapt current legislation, support global rules on Al and assume a leading role in the development of responsible AI. The non-binding 2019 EU Ethics Guidelines for Trustworthy Al proposed the key principles in this context. First, AI should empower humans and allow for human oversight through approaches such as human-in-the-loop, although there is no one-size-fits-all solution. Second, Al systems must be technically robust and safe, and they must have contingency plans. Third, they should respect privacy, data protection and data governance. Fourth, transparency is crucial to ensure that stakeholders understand Al models and decisions. Fifth, AI systems must avoid unfair bias, promote diversity and sustainability and be accessible to all. Sixth, accountability mechanisms, such as auditability and redress options, must be deployed in responsible Al implementation. Soft-law principles for Al governance similar to the 2019 EU guidelines have been adopted by the Organisation for Economic Co-operation and Development (OECD) in 2019, as well as by numerous other public and private organisations, in an attempt to promote innovative yet responsible AI.\nRecently, the soft-law approach to Al regulation seems to have made way for a stronger legislative approach. This has happened not only in the EU but also in other jurisdictions, with something of a 'race to Al regulation' apparently taking place. The stronger approach advocates for the adoption of clear and binding rules for Al, with the support of effective enforcement mechanisms. The choice between hard and soft law has been the subject of scholarly interest in various areas of law and governance. We argue that the shift from soft to hard law in Al regulation is timely and appropriate given the magnitude of the risks associated with this new technology. This does not mean that soft-law rules should be discarded completely; they offer the advantages of flexibility and adaptability, and they can be employed to support hard law as mutually reinforcing complements. A set of enforceable rules, the violation of which would lead to legal consequences and penalties, would provide a stronger deterrent against non-compliance and help mitigate the dangers of Al more effectively. At the EU level, a clear legal framework for Al, applied uniformly across all member states, would reduce ambiguity and ensure that the rights and responsibilities of Al developers and users are interpreted in a standardised manner in the EU single market. A binding legal framework for Al would also enhance accountability and transparency by requiring entities in all EU member states to disclose their actions and practices to the relevant authorities, thus allowing supervisors to evaluate compliance and potential violations. These elements promise to instil greater confidence in businesses, investors and consumers, thereby increasing engagement with Al technologies; they would also effectively address risks and prevent forum shopping in the single market.\nTo make this happen, the EU must grapple with the challenge of balancing innovation and responsible Al when designing and implementing a new binding framework. In April 2021, the European Commission took on this challenge by putting forward an ambitious proposal for an EU regulatory framework concerning Al based on Articles 114 and 161 of the Treaty on the Functioning of the European Union. Following the approval of the European Council's general position in December 2021 and the European Parliament's vote in June 2023, EU lawmakers have entered into negotiations (trialogue) and finalised the text in December 2023. These discussions entailed several amendments to the European Commission's initial proposal, including revisions to the definition of Al systems and to the list of prohibited Al systems.\nWe argue that the decision to introduce a new instrument was appropriate and opportune. While existing EU rules, especially the General Data Protection Regulation (GDPR), do provide some guidance for safeguarding data in the realm of Al, they fail to directly address several data protection challenges associated with this technology. A 2020 study by the European Parliament identified the shortcomings of the GDPR in terms of the processing of personal data enabled by AI, including the lack of concrete obligations on XAI. A new harmonised framework would have the benefit of comprehensively addressing the development of Al in the EU single market, as well as the use of Al products and services. Furthermore, to ensure effectiveness and prevent forum shopping, the new regulatory framework should cover users of Al systems located in the EU and providers of such systems established both in the EU and in a third country if they place their products or services in the EU market.\nAfter choosing to put regulations in place, the next important question is: What exactly will be subjected to regulatory measures? In its initial proposal for the EU AI Act, the European Commission put forward a technology-neutral, albeit quite broad, definition of Al systems. This definition has faced criticism for its extensive scope, leading the EU Council to propose a more refined description with clearer criteria to distinguish Al from established software systems. Conversely, the European Parliament proposed to amend the definition to align it with the OECD one. Ultimately, this was the approach adopted by European Parliament and Council of the EU in the political agreement reached in December 2023. Nevertheless, no agreed definition of Al has been given at the level of the EU-U.S. Terminology and Taxonomy for Al initiative. In this context, it has been correctly pointed out that rather than relying on the definition of the term 'AI,' policymakers should focus on identifying the specific risks they want to reduce.\nThis brings us to the next challenge for the EU AI Act that is, the classification of the risks that are associated with specific uses of the technology. The classification system in the EU AI Act relies on a 'risk-based approach' that assigns different requirements and obligations to each risk category. Risk-based regulation, which has become somewhat of a buzzword in the past few decades, has proved to be effective in areas such as banking and the fight against money laundering. Applying this approach in the Al context holds substantial promise; it also builds on the principle of proportionality, since regulations apply to Al applications only to the extent required to handle the specific level of risk they pose. More specifically, Al systems deemed to pose 'unacceptable' risks would be banned. A range of 'high-risk' Al systems would be permitted only if they met specific requirements and obligations before entering the EU single market. Limited transparency obligations would apply to systems that present limited risks, such as the need to indicate that an Al system is being used and interacts with humans. In this model, the provision of information and transparency would be mandatory for high-risk Al systems. As we will see in the following section, the challenge here is to decide which criteria and methods of explainability should be required by an Al regulation."}, {"title": "Unravelling the Magic: Approaches to and Techniques for XAI", "content": "The principle of explainability and the field of XAI are often put forward as the solution to the above-mentioned 'black box problem'. Although XAI lacks a commonly agreed definition, it can be broadly described as Al techniques that enable human users to comprehend, trust and manage Al effectively. In the literature, 'explainability' is often used interchangeably with \u2018interpretability'. Explainability differs from simple transparency, a notion that refers to the degree to which an Al system is open and observable to humans. Transparency means that humans have access to critical information, such as the data used to train the Al system, the manner in which the data are gathered and stored, and who has access to the data the system collects. Still, making information available is not equivalent to making it comprehensible, which is the purpose of explainability. The importance of XAI has been recognised not only by the EU but also by key global players, such as the US Department of Defence, big tech companies, international organisations, smaller companies and start-ups. The question is how explainability can be defined and implemented in an optimal way in order to foster trust, fairness and accountability.\nXAI techniques can provide different types of explanations \u2013 local or global which explain individual predictions or offer insights into the model's behaviour, respectively. Several classification models and taxonomies for XAI methods have been proposed. Model architectures that are designed with interpretability in mind may use decision trees, linear models and other mechanisms, and their performance, limitations and applicability may vary. Without getting into too much technical detail, it is worth highlighting the variety of methods used in XAI, which is the key challenge when it comes to integrating this notion into law.\nXAI can take a quantitative route, which aims to provide insights into the importance of certain features and establish suitable scaling and normalisation methods for them. Because minimal feature changes in Al models can lead to different model predictions, several techniques attempt to determine the importance of features. A closely related approach to XAI focuses on feature interactions, i.e. how combinations of features influence the model's output. The objective here is to uncover intricate relationships in the data; however, this requires sophisticated visualisation techniques, as we will see in the next section. Several counterfactual explanation methods go beyond correlation and explore causality by examining what changes in the inputs would lead to different outcomes. XAI may also employ sensitivity analysis, in which input features are varied systematically to observe the corresponding changes in predictions, thereby identifying critical features. Doing so is computationally intensive when there are multiple features and high-dimensional data. Furthermore, XAI may go beyond deterministic explanations and employ probabilistic models to estimate uncertainty and prevent potentially risky predictions, especially when scenarios demand risk assessments. Finally, the temporal dimension is crucial, and many XAI methods are designed to understand how decisions unfold over time, how patterns change, and how time intervals affect accuracy, especially in fast-changing situations."}, {"title": "Challenges and Considerations regarding XAI Implementation", "content": "While the importance of explainability is widely acknowledged and despite years of research in the field, implementing explainability has proved challenging due to issues of complexity and scalability.\nAs Al models grow in size and complexity, so do the practical obstacles to producing XAI models that remain understandable and trustworthy. Another factor that should be considered is the cost of XAI, which rises with the increasing complexity of the technology. A major obstacle is the lack of agreement on how to perform an objective evaluation of XAI methods. Moreover, if explainability requirements vary by jurisdiction, businesses must develop different algorithms and explainability methodologies for different markets, which would increase their costs and put small and medium-sized enterprises in an unfavourable position. There is also an inherent trade-off between model transparency and performance, which may lead different industries and Al applications to prioritise one over the other. Certain categories of Al systems are more prone to experiencing a decline in accuracy and performance if they are mandated to be explainable. This is because the process of achieving explainability can involve simplifying the solution variables to the level where human comprehension is possible. Obviously, this approach might not be optimal for intricate, multidimensional problems.\nEthical and privacy considerations also arise in the context of XAI. First, providing explanations for ethical aspects of Al decision-making requires a prior and clear definition of fairness and bias. However, this definition might be subjective and highly dependent on context, which might lead to differing interpretations. Legal definitions of bias may also differ from one jurisdiction to another, which further complicates things. Second, XAI must take into account privacy concerns because explanations of model decisions must not disclose sensitive information, but they must still be able to provide meaningful insights. As is the case with bias, the legal requirements for privacy may differ across jurisdictions.\nFurthermore, XAI must balance the need for explanations with that to defend proprietary algorithms, as source code and datasets may be subject to intellectual property or trade secrets. Therefore, increased transparency obligations imposed by law should not disproportionately affect the intellectual property rights of Al developers. In the context of the EU AI Act, transparency requirements encompass the essential details needed for individuals to exercise their right to an effective remedy and for supervision and enforcement authorities to exercise their mandates. Also, any divulgence of information must adhere to relevant legal frameworks, including Directive 2016/943, which safeguards trade secrets from unlawful acquisition, use and disclosure. Therefore, when supervision and enforcement authorities require access to confidential information or source code in order to assess compliance, they are bound by strict confidentiality obligations.\nFinally, human-Al interaction plays a key role in XAI by highlighting methods for effectively communicating complex Al concepts to stakeholders, thus fostering transparency and trust. This issue poses several challenges. First, algorithms may be complex and non-linear; therefore, the explanations provided by them may not align with human intuition, which undermines trust in the Al system. Second, there is a trade-off between accurate representation and simplification; a balance must be found to avoid oversimplification and the loss of essential information. Indeed, the complexity of Al models makes it difficult to design visual representations that ensure user understanding and effectively communicate intricate model behaviours without overwhelming users or leaving out essential information. Third, when Al systems make biased or unfair decisions, it is necessary to ensure that the explanations given are not just post hoc justifications. Fourth, clarifications should be user-centric \u2013 that is, they should take into account the varying expertise levels of different user groups. Explanations must be tailored to the technical proficiency of users, who range from laypersons to experts (e.g., oversight authorities), and they must be meaningful and useful to each of them. This also means that clarifications must be relevant and informative for the decision-making processes of each group of users, without imposing an excessive cognitive load. This could be ensured by enabling interaction between humans and Al in the form of feedback loops. Users' feedback on clarifications could be employed to refine the explanation methods over time, thus factoring in the evolution of user preferences."}, {"title": "Integrating Al Explainability into EU Law", "content": "Given these challenges, it is worth examining how to harmoniously meld the principle of Al explainability with the current legal landscape of the EU. As already mentioned, the EU must first determine which Al systems qualify as high risk and thus fall under the obligation of explainability. The EU must also specify what exactly the obligation of explainability would entail; in other words, it must spell out which XAI methods and standards should be met to ensure compliance. Evidently, EU law should introduce a distinction between different levels of explainability and detail the technical aspects of implementing XAI. It must also address the intersection of XAI and privacy as well as consider the need to provide explanations to users in compliance with data protection regulations, such as the GDPR.\nTo do so effectively, the EU has opted for the method of co-regulation through standardisation based on the New Legislative Framework (NLF). The EU AI Act recognises the key role that standardisation should play in providing technical solutions to providers and in ensuring compliance with the new framework for AI, which will include the chosen standards for explainability. Some have argued that entrusting the creation of regulations to organisations operating under private legal frameworks, such as CENELEC, may be problematic at the level of judicial scrutiny. Nevertheless, amid the complexity of Al advancements, novel governance models (e.g., hybrid governance) will inevitably gain traction. This type of governance merges state and non-state actors as well as the public and private realms through varying degrees of regulation, such as co-regulation and self- regulation. These models, which are complementary to traditional command-and- control legislation, can identify the changing risks of evolving Al systems more effectively, thus prompting the public and different stakeholders to create the necessary norms.\nMore specifically, under the EU AI Act, the compliance and enforcement system for stand-alone, high-risk Al models (Annex III) is modelled after the NLF and combines ex ante conformity assessments (internal checks) with ex post enforcement. Ex ante assessment of stand-alone, high-risk Al systems (i.e., systems that are new or represent significant modifications of existing systems) requires compliance with key obligations, such as explainability, as well as robust quality and risk management systems and post-market monitoring. Following this assessment, providers must register high-risk Al models in an EU database managed by the European Commission to enhance public transparency, oversight, and ex post supervision by the authorities. In this context, EU law must establish effective accountability mechanisms to make developers, operators and users of Al systems comply with the EU AI Act and its XAI components. This will involve establishing clear lines of responsibility, requiring documentation of the Al development process, and setting up audit trails for Al-generated decisions. This is exactly what the EU AI Act's provisions concerning technical documentation, the EU declaration of conformity, and the conformity assessment procedures aim to achieve. Naturally, it will be essential to have effective, proportionate, and deterrent penalties, including administrative fines, for violations, as well as incentives for compliance.\nThe main oversight and enforcement authorities under the EU AI Act are 'market surveillance authorities' (MSAs), which constitute an oversight model that is common in EU product law. The MSAs in question could be new entities or extensions of existing institutions at the national level; they will be responsible for post-market monitoring and investigating AI-related incidents. Regarding XAI, these entities will also have the power to oversee and enforce the Al explainability standards. The need for a supranational oversight body at the EU level will inevitably emerge, as has been the case with anti-money laundering supervision, due to the absence of obstacles to the cross-border movement of Al systems. The European Parliament has correctly proposed to establish such a new EU body, an Al Office responsible for providing guidance and coordinating joint cross-border investigations. This oversight body, operating either at a national or supranational level, must have not only effective powers (audits, inspections and reporting requirements) but also the necessary resources (funding, staff and technology) to ensure ongoing compliance. The need for international cooperation in standardising XAI approaches will also certainly emerge as other jurisdictions develop their regulatory frameworks in addition to the EU. This will create the need for harmonising regulations internationally, promoting global best practices, and avoiding conflicts in cross-border Al deployments. The EU must be ready to work in this direction.\nFinally, the specific XAI framework needs to be dynamic and able to evolve based on advancements in Al technology and the evolution of the general regulatory framework for AI. Mechanisms must be put in place at the national and EU levels for periodic reviews of and updates to the relevant regulations in order to accommodate new challenges and developments in Al and XAI."}, {"title": "Fostering Trust and Accountability: The Broader Implications of Explainability in Al", "content": "The concept of explainability is not just a legal term and a software/technical challenge. It is a principle with far-reaching societal implications, as it can foster trust and empower the public to engage with Al technologies more actively. Furthermore, explainability is a prerequisite for accountability and fairness since a clear XAI decision-making process allows society to hold an entity responsible for its decisions. Enhancing the availability of explanations makes individuals, enterprises and public institutions more informed about the decision-making processes of AI, thus allowing them to make informed choices about Al applications. Hence, XAI is indispensable for ensuring trustworthy and responsible Al.\nOf course, policymakers must balance the need for detailed insights into Al models with the potential cognitive overload that threatens users. This requires a clear regulatory vision, standardised approaches to explainability and the deployment of innovative solutions in information architecture, visualisation, interaction design and education. Moreover, XAI requirements must take into account the concerns of the Al industry, such as the cost of XAI, the trade-off between model transparency and performance, and the protection of proprietary algorithms. As the EU's efforts to regulate this technology demonstrate, developing a standardised approach to Al and explainability proves challenging due to the need to accommodate diverse domains and applications in a rapidly evolving technological landscape. The new EU legal framework for Al must consider this diversity and the need to balance innovation with explainability, accountability and public trust."}]}