{"title": "Detect, Investigate, Judge and Determine:\nA Novel LLM-based Framework for Few-shot Fake News Detection", "authors": ["Ye Liu", "Jiajun Zhu", "Kai Zhang", "Haoyu Tang", "Yanghai Zhang", "Xukai Liu", "Qi Liu", "Enhong Chen"], "abstract": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news from real ones in extremely low-resource scenarios. This task has garnered increased attention due to the widespread dissemination and harmful impact of fake news on social media. Large Language Models (LLMs) have demonstrated competitive performance with the help of their rich prior knowledge and excellent in-context learning abilities. However, existing methods face significant limitations, such as the Understanding Ambiguity and Information Scarcity, which significantly undermine the potential of LLMs. To address these shortcomings, we propose a Dual-perspective Augmented Fake News Detection (DAFND) model, designed to enhance LLMs from both inside and outside perspectives. Specifically, DAFND first identifies the keywords of each news article through a Detection Module. Subsequently, DAFND creatively designs an Investigation Module to retrieve inside and outside valuable information concerning to the current news, followed by another Judge Module to derive its respective two prediction results. Finally, a Determination Module further integrates these two predictions and derives the final result. Extensive experiments on two publicly available datasets show the efficacy of our proposed method, particularly in low-resource settings.", "sections": [{"title": "1 Introduction", "content": "Fake News Detection (FND), aiming to distinguish between inaccurate news and legitimate news, has garnered increasing importance and attention due to the the pervasive dissemination and detrimental effects of fake news on social media platforms (Shu et al., 2017). Few-Shot Fake News Detection (FS-FND), as a subtask of FND, endeavors to identify fake news by leveraging only K instances per category (K-shot) in the training phase (Hu et al., 2024; Gao et al., 2021; Ma et al., 2023)."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Few-Shot Fake News Detection", "content": "The objective of fake news detection task is to distinguish inaccurate news from real ones (Shu et al., 2017). For few-shot fake news detection, only K instances per category (K-shot) are sampled for the training phase (Gao et al., 2021; Ma et al., 2023). Generally, fake news detection can be defined as a binary classification problem and is addressed by a variety of classification models. Initially, researchers mainly rely on feature engineering and machine learning algorithms. For example, Horne and Adali (2017) presented a set of content-based features to a Support Vector Machine (SVM) classifier. With the rapid development of computing power, significant improvements have been made with the help of various deep learning algorithms and Pre-trained Language Models (PLMs). Ghanem et al. (2021) combined lexical features and a Bi-GRU network to achieve accurate fake news detection. Jiang et al. (2022) proposed Knowledge-able Prompt Learning (KPL), incorporating prompt learning into fake news detection for the first time, and achieved state-of-the-art performance. Additionally, due to the specificity of news articles, researchers have incorporated external knowledge knowledge to contribute to traditional fake news detection models. For instance, Dun et al. (2021); Ma et al. (2023); Hu et al. (2021b) utilized knowledge graphs to enrich entity information and structured relation knowledge, leading to more precise news representations and improved detection performance. Meanwhile, Huang et al. (2023) adopted a data augmentation perspective, proposing a novel framework for generating more valuable training examples, which has proven to be beneficial in detecting human-written fake news. Currently, with the advent of large language models, many researchers are exploring few-shot fake news detection through in-context learning and data augmentation technologies (Hu et al., 2024; Wang et al., 2024; Teo et al., 2024). However, These methodologies either simply apply LLMs to judge the authenticity of the given news, or employ LLMs to rephrase the training data, thereby not"}, {"title": "2.2 Large Language Models", "content": "The emergence of Large Language Models (LLMs) such as GPT-4, LLama-3 and others (Hoffmann et al., 2022; OpenAI, 2023; AI@Meta, 2024; Tunstall et al., 2023), marks a significant advancement in the field of natural language processing. In-context learning, a novel few-shot learning paradigm, was initially introduced by Brown et al. (2020). To date, LLMs have exhibited remarkable performance across a range of NLP tasks, including text classification, information extraction, question answering and fake news detection (Hu et al., 2024; Wang et al., 2024; Teo et al., 2024; Liu et al., 2022; Zhao et al., 2021). Previous studies (Hu et al., 2024; Wang et al., 2024; Teo et al., 2024) have endeavored to solve few-shot fake news detection via directly asking LLMs or employing them to rephrase the training data. For example, Hu et al. (2024) explored the potential of LLMs in fake news detection, and further developed an Adaptive Rationale Guidance (ARG) network to synergize traditional methods with large language models. Similarly, Wang et al. (2024) leveraged LLMs to generate justifications towards evidence relevant to given news, which were subsequently fed into a trainable classifier."}, {"title": "3 Problem Statement", "content": "Generally, fake news detection can be framed as a binary classification problem, wherein each news article is classified as either real (y = 0) or fake (y = 1) (Dun et al., 2021). Formally, each piece of news S is composed of a sequence of words, i.e., S = {81, 82, ..., Sn}, encompassing its title, content text, and relevant tweets. The goal is to learn a detection function F : F(S) \u21d2 y, where y \u2208 {0,1} denotes the ground-truth label of news. In the few-shot settings, adhering the strategy employed in (Gao et al., 2021; Ma et al., 2023), we randomly sample K instances per category (K-shot)\u00b9 for the training phase. The entire test set is preserved to ensure the comprehensiveness and effectiveness of evaluation."}, {"title": "4 The DAFND Model", "content": "In this section, we will introduce the technical details of DAFND model. As depicted in Figure 2, DAFND comprises of four distinct components: (a) Detection Module; (b) Investigation Module; (c) Judge Module; (d) Determination Module. These modules are sequentially interconnected to achieve the final accurate detection of fake news."}, {"title": "4.1 Detection Module", "content": "In this module, we aim to identify the key information contained in the given news article, which will serve as the query for the subsequent modules. Specifically, we construct prompts to convey the original news article to the LLM. The LLM is then guided to extract N keywords {W1, W2, ..., WN}, which are expected to address the question: \u201cwhen, where, who, what, how and why did the given news S happen?\":\n{W1, ..., wn} = F(Insw, S), (1)\nwhere F represents the LLM, and Insw denotes the prompt for the in-context learning. We provide a more detailed description about it in Appendix A.1.\""}, {"title": "4.2 Investigation Module", "content": "In this module, we aim to investigate the relevant information to assist the LLM in conducting accurate inferences. As outlined in Section 1, this process is approached from two perspectives: Inside Investigation and Outside Investigation.\nInside Investigation. To address the Understanding Ambiguity problem introduced in the Introduction, we retrieve effective demonstrations to enhance the LLM's understanding during the in-context learning process (Liu et al., 2022; Rubin et al., 2022).\nSpecifically, we first concatenate the extracted N keywords of each news, and then utilize the pre-trained language model M to obtain the representation of these keywords {w1, ..., WN }:\nW = W1 \u03a8 W2 \u03a8... \u03a8WN,\nH = M(W), (2)\nwhere \u03a8 represents the concatenation operation. The derived representation H is used to represent each news sample. Along this line, we can further obtain the representation and label pairs (Hi, li)"}, {"title": "4.3 Judge Module", "content": "Following the paradigm designed in the Investigation Module (Section 4.2), we attempt to derive the inference results based on the investigated information from both inside and outside perspectives.\nInside Judge. After obtaining the effective demonstrations II from Inside Investigation, we design prompts to provide the essential information to the LLM, thereby generating the inside prediction. Specifically, inspired by the various attempts about in-context learning (Paranjape et al., 2023), we first describe the target of the fake news detection task through an inside instruction. Then, the retrieved inside investigation results II = {Upositive, Unegative} of current candidate news are followed, which augment the LLM's understanding of this task. Finally, we prompt the LLM to predict the result of current news and give its corresponding supportive explanation.\nIn summary, the inside judge process can be expressed as:\nPi, Ri = F(Insi \u03a8 II \u03a8 Xtest), (4)\nwhere Ins; is the inside instruction, \u03a8 denotes the concatenation operation of two textual pieces, and Xtest represents the current candidate news. Pi refers to the prediction result, while Ri denotes the corresponding explanation. You can move to Appendix A.2 for more details about this prompt.\nOutside Judge. With the outside investigation in-formation OI retrieved through the google search"}, {"title": "4.4 Determination Module", "content": "With the predictions Pi, Po and their corresponding explanations Ri, Ro, the final outputs are obtained by jointly considering these two perspectives.\nMore specifically, if the two predictions are identical (i.e., P\u2081 = Po), we can directly derive the final prediction with high confidence. Nevertheless, if two results diverge, indicating a conflict between the Inside Judge and Outside Judge, we further propose a determination selector to make a choice based on both sets of predictions and explanations:\nPf = F(Insd\u03a8 Xtest\u03a8 Pi\u03a8Ri \u03a8P\u03a8R), (6)\nwhere Insa denotes the determination instruction (see Appendix A.4 for more details). Pf is the final inference result of the DAFND model."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experiment Setup", "content": "Datasets and Evaluation Metrics. We conduct experiments on two datasets, PolitiFact and Gossipcop, both of which are proposed in a benchmark called FakeNewsNet (Shu et al., 2020). PolitiFact consists of various political news, while Gossipcop is sourced from an entertainment story fact-checking website. For the few-shot setting, following the strategy employed in (Jiang et al., 2022; Ma et al., 2023), we randomly select K \u2208 (8,32, 100) positive and negative news articles as the training set, respectively. More statistics about the datasets are illustrated in Table 1.\nGiven that the task focuses on detecting fake news, fake news articles are regarded as positive examples (Ma et al., 2023). We further adopt the F1-score and Accuracy (ACC) as the evaluation metrics to measure classification performance.\nImplementation Details. In DAFND architecture, we utilize the zephyr-7b-beta (Tunstall et al., 2023) model on Huggingface as the LLM to conduct experiments. When running Zephyr, we adhere to the default parameter values provided by the official, where the sampling temperature is 0.70, top_k is 50, and top_p is 0.95. The max_new_token is set to 256, and do_sample is set as True.\nIn the Detection Module (Section 4.1), the number of keywords to extract is set to N = 5. In the Inside Investigation part (Section 4.2), we employ the DeBERTa-base model (He et al., 2021) from Transformers (Wolf et al., 2020) as the representation model. The number of retrieved positive/negative nearest neighbors is set as k = 2.\nAll experiments are conducted on a Linux server with two 3.00GHz Intel Xeon Gold 5317 CPUs and two Tesla A100 GPUs.\nBenchmark Methods. In order to verify the effectiveness of our DAFND model, we compare DAFND with the state-of-the-art few-shot fake news detection methods. According to the model architecture, they can be grouped into three categories, including traditional fake news detection methods (\u2460~\u2463), LLM-based methods (\u2464 ~ \u2467), and hybrid methods (9)."}, {"title": "5.2 Experimental Result", "content": "The main results, presented in Table 2, indicate that our proposed DAFND model surpasses all baselines across various metrics, encompassing traditional, LLM-based and hybrid methods. This underscores the effectiveness of our design and the advantages of enhancing the LLM through both inside and outside perspectives. Furthermore, several notable phenomena emerge from these results:\nFirstly, for most baselines and our DAFND model, the performance on the PolitiFact dataset exceeds that on the Gossipcop dataset, suggesting that Gossipcop presents greater difficulty. Specifically, PolitiFact consists of political news while Gossipcop pertains to the entertainment domain. This disparity is reasonable as political news typically exhibits more organized format and content, which facilitates the fake news detection process. Secondly, with the increase of training instances (K), most traditional fake news detection methods (e.g., \u2463 PSM) and hybrid methods (\u2468 ARG) show improved performance. This is logical as more data enables better training of a supervised model, mitigating the lack of prior knowledge. However, an exception is observed in \u2460 PROPANEWS, whose performance appears relatively unaffected by K. As introduced in Section 5.1, different from other methods, PROPANEWS designs a data constructing strategy to supplement original training set. This significantly offsets the impact of training data quantity. Moreover, for LLM-based methods (5 ~\u2467), as outlined in Section 5.1, due to the limitation of maximum tokens, we all randomly select 6 samples as demonstrations. Hence, increasing the number of training instances does not substantially benefit the in-context learning of LLMs. Thirdly,"}, {"title": "5.3 Ablation Study", "content": "In this subsection, we conduct ablation experiments to assess the effectiveness of different components within our model. Specifically, we directly remove the Determination Module, consequently leading to two ablated variants: Inside Judge and Outside Judge. The results are depicted in Figure 3.\nFrom this figure, across all configurations on PolitiFact and Gossipcop datasets, noticeable decreases are observed between the full DAFND model and its two ablated variants. This thoroughly substantiates the essentiality and non-redundancy of our DAFND designs. Notably, when the training instances are relatively limited (K = 8), Inside Judge exhibits less effectiveness compared to Outside Judge. With the increase of training instances (K = 32, 100), Inside Judge progressively achieves more competitive performance than Outside Judge. This can be attributed to the increasing benefit of additional training data for the Inside Investigation and Inside Judge components. In contrast, Outside Investigation and Outside Judge rely on the retrieved information online, which remains"}, {"title": "5.4 Case Study", "content": "To further illustrate the effectiveness of different modules in our model, we conduct a case study on both PolitiFact and Gossipcop datasets. Specifically, Figure 4 presents the input information (i.e., target news), the ground truth label, DAFND results (including the intermediate results of each module and the final results).\nAs depicted Figure 4 (a), the Detection Module accurately identifies the key information within the target news. And based on the valid results from Inside Investigation and Outside Investigation, both the Inside Judge and Outside Judge correctly infer that: \"[This is fake news].\". These are fed into the Determination Module, leading to the final prediction, which is consistent with the ground truth label (i.e., [Fake]). Moreover, In Figure 4 (b), Inside Judge obtains the right inference (i.e., [Fake]), while Outside Judge makes the wrong inference (i.e., [Real]). Conversely, in Figure 4 (c), Inside Judge incorrectly infers \u201c[Real]\u201d, while Outside Judge accurately predicts ", "Fake]": "In both (b) and (c), with the design of Determination Module, DAFND finally makes the correct decision. These cases intuitively demonstrate the significant role of each module in DAFND, affirming its efficacy.\nMore experimental analyses, such as Bad Case Analysis, can be found in Appendix B."}, {"title": "6 Conclusions", "content": "In this paper, we explored a motivated direction for few-shot fake news detection. We began by analyzing the limitations of current LLM-based detection methods, identifying two primary challenges: (1) Understanding Ambiguity and (2) Information Scarcity. To address these issues, we developed a Dual-perspective Augmented Fake News Detection (DAFND) model. In DAFND, a Detection Module was designed to identify keywords from the given news. Then, we proposed an Investigation Module, a Judge Module to retrieve valuable information and further generate respective predictions. More importantly, a Determination Module integrated the predictions from both inside and outside perspectives to produce the final output. Finally, extensive experiments on two publicly available datasets demonstrated the effectiveness of our proposed method. We hope our work will lead to more future studies."}, {"title": "Limitations", "content": "In our proposed DAFND method, we need to integrate the LLM (i.e., zephyr-7b-beta introduced in Section 5.1). Due to the large scale of LLMs, it tends to consume more computing resources and time compared to traditional baselines, such as PSM (Ni et al., 2020) and FakeFlow (Ghanem et al., 2021). Essentially speaking, LLMs contain a vast amount of knowledge, much of which may be unnecessary for fake news detection. Distilling useful knowledge so as to accelerate the inference remains a valuable and intriguing research direction.\nAnother limitation is that our current approach only employs LLMs for inference. Although we design precise prompts to implement in-context learning, it still cannot fully exploit the capabilities of LLMs due to the inherent gap between the natural language and the knowledge encoded in the model parameters. In future work, we would like to explore the low resource scenario fine-tuning techniques (e.g., lora (Hu et al., 2021a)) to adapt LLMs for the few-shot fake news detection task."}, {"title": "A Prompts", "content": "In this section, we illustrate the prompts utilized in our DAFND methodology, which can serve as a valuable resource for future research in this area."}, {"title": "A.1 Detection Prompt", "content": "As a news keyword extractor, your task is to extract the N most important keywords from a given news text. The keywords should include when, where, who, what, how and why the news happened. Please give me the six keywords only. My first suggestion request is {Target News Document}."}, {"title": "A.2 Inside Judge Prompt", "content": "I need your assistance in evaluating the authenticity of a news article. I will provide you the news article and additional information about this news. You have to answer that [This is fake news] or [This is real news] in the first sentence of your output and give your explanation about [target news].\nI will give you some examples of news. Your answer after [output] should be consistent with the following examples:\n[example 1]:\n[input news]: [news title: {...}, news text: {...},\nnews tweet: {...}]\n[output]: [This is {...} news]\n[target news]:\n[input news]: [news title: {...}, news text: {...},\nnews tweet: {...}]\n[output]:"}, {"title": "A.3 Outside Judge Prompt", "content": "I need your assistance in evaluating the authenticity of a news article. I will provide you the news article and additional information about this news. Please analyze the following news and give your decision. The first sentence of your [Decision] must be [This is fake news] or [This is real news].\nThe news article is: {...}.\nThe additional information is: {...}.\n[Decision]:"}, {"title": "A.4 Determination Prompt", "content": "I need your assistance in evaluating the authenticity of a news article. This news article include news title, news text and news tweet.\nThe news article is: news title: {...}, news text:\n{...}, news tweet: {...}."}, {"title": "B Bad Case Analysis", "content": "In this section, we illustrate the bad cases that DAFND struggles with, with a goal to analyze its shortcomings and possible improvement directions.\nAs illustrated in Figure 5, a particular failure occurs when there are no relevant samples in the few-shot training set for retrieval. Consequently, the Inside Investigation and Inside Judge modules fail to work effectively. Meanwhile, although Out-"}]}