{"title": "Refining Positive and Toxic Samples for Dual Safety Self-Alignment of LLMs with Minimal Human Interventions", "authors": ["Jingxin Xu", "Guoshun Nan", "Sheng Guan", "Sicong Leng", "Yilian Liu", "Zixiao Wang", "Yuyang Ma", "Zhili Zhou", "Yanzhao Hou", "Xiaofeng Tao"], "abstract": "Recent AI agents, such as ChatGPT and LLaMA, primarily rely on instruction tuning and reinforcement learning to calibrate the output of large language models (LLMs) with human intentions, ensuring the outputs are harmless and helpful. Existing methods heavily depend on the manual annotation of high-quality positive samples, while contending with issues such as noisy labels and minimal distinctions between preferred and dispreferred response data. However, readily available toxic samples with clear safety distinctions are often filtered out, removing valuable negative references that could aid LLMs in safety alignment. In response, we propose PT-ALIGN, a novel safety self-alignment approach that minimizes human supervision by automatically refining positive and toxic samples and performing fine-grained dual instruction tuning. Positive samples are harmless responses, while toxic samples deliberately contain extremely harmful content, serving as a new supervisory signals. Specifically, we utilize LLM itself to iteratively generate and refine training instances by only exploring fewer than 50 human annotations. We then employ two losses, i.e., maximum likelihood estimation (MLE) and fine-grained unlikelihood training (UT), to jointly learn to enhance the LLM's safety. The MLE loss encourages an LLM to maximize the generation of harmless content based on positive samples. Conversely, the fine-grained UT loss guides the LLM to minimize the output of harmful words based on negative samples at the token-level, thereby guiding the model to decouple safety from effectiveness, directing it toward safer fine-tuning objectives, and increasing the likelihood of generating helpful and reliable content. Experiments on 9 popular open-source LLMs demonstrate the effectiveness of our PT-ALIGN for safety alignment, while maintaining comparable levels of helpfulness and usefulness.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT proliferation AI-assistant agents, such as OpenAI's ChatGPT [1], DeepSeek [2], Meta's LLaMA [3], and Claude [4], demonstrate the powerful ability of large language models (LLMs) to understand and generate natural language, positioning them as the semantic backbone for many generative systems. However, current models still exhibit safety shortcomings. LLMs can produce harmful re-sponses or disclose private information even without red-teaming prompts. Additionally, these generative models may facilitate the synthesis and distortion of false information or exacerbate broader information security threats through misuse. Furthermore, fine-tuning with commonly used datasets may inadvertently degrade the safety alignment of LLMs, posing significant risks to downstream applications [5], [6].\nWith the widespread deployment and application of LLMs, ensuring their alignment with human values is essential to produce outputs that are harmless, helpful, and honest [7], [8], [9], [10], [11]. Modern LLMs achieve safety alignment through Supervised Fine-Tuning (SFT) [12] and Reinforcement Learning from Human Feedback (RLHF) [13] to mitigate toxic, undesirable, or otherwise prohibited outputs. SFT-based methods align LLMs with human values by using large sets of manually annotated instruction-response pairs, primarily teaching the models the paradigm of generating safe responses. On the other hand, RL-based methods utilize a reward model to select relatively better responses based on human feedback, guiding LLMs to generate appropriate replies. In general, these methods are heavily relying on large amounts of harmless and positive responses meticulously designed and annotated by domain experts. Meanwhile, toxic and negative responses are typically cleaned or discarded, with limited consideration of their role in safety alignment.\nIn the post-training stage, particularly during instruction fine-tuning, including toxic data or even general-purpose datasets can compromise the model's safety performance [14]. Figure 1 illustrates that many well-aligned LLMs, such as Vicuna, LLaMA-chat, and DeepSeek, are susceptible to producing harmful content when exposed to adversarial in-structions or jailbreak attacks. These compelling examples strongly underscore the pressing necessity of implementing significantly more robust and effective safety mechanisms during the instruction fine-tuning process.\nHowever, directly applying methods like SFT or RLHF to expose toxic responses in LLMs may cause the model to internalize harmful text patterns. Moreover, recollecting toxic samples specifically for instructions and positive response pairs, which adds additional burdens to the already labor-intensive task of annotating safety samples, can substantially increase the manual workload. As safety scenarios become more complex and private safety requirements continue to grow, this manual process risks becoming both inadequate and prohibitively expensive. This raises a critical question: How can we enable LLMs to acquire safety knowledge from both positive and toxic samples with minimal manual effort, thereby improving model safety while maintaining effectiveness?\nAs early studies [17], [18], [19] that employ SFT and RLHF require costly human supervision and may lead to potential biases and unfairness [20], [21] for safety alignment, the recently proposed self-alignment method Dromedary [22] explores hundreds of human-written annotations and complex rules to guide agents based on LLMs for creating harmless and human-aligned content. A recent work [23] discusses that the existing Dromedary would be more effective on significantly large models, e.g., 65B-level LLMs. We argue that self-alignment methods are better suited for purely safety-alignment applications, as the ideal text paradigms for both positive and toxic samples are relatively simple, and the safety domain is significantly narrower and more target-specific compared to the breadth of human values. These factors considerably lower the demands on the fundamental capabilities of LLMs, enabling smaller-scale models to achieve effective self-safety alignment.\nMoreover, while safety alignment methods relying on care-fully curated human preference data[24] have made significant progress, constructing high-quality positive samples remains challenging due to the ambiguity[25] and diversity of re-quirements, which often result in noisy preference labels[26], [27]. Our observations reveal that each pair of positive and negative samples typically exhibits high semantic similarity, with only marginal differences in harmlessness rewards. More concerningly, harmful preference content frequently appears in preference datasets, potentially undermining the safety align-ment process and reinforcing negative outputs[13], [27], [18]. Therefore, we highlight a new research direction: construct-ing highly polarized sample pairs with extreme positive and negative samples for the same unsafe instructions, leveraging a larger safety disparity to minimize model harmfulness while preserving its usefulness[28], [29].\nInspired by these studies, we propose leveraging toxic samples as a novel supervisory signal, allowing the model to encounter and avoid harmful content through polarized sample pairs. This approach enables more direct learning of safe content, enhancing safety while preserving helpfulness. By employing self-constraint as in-context guidance and a dualistic sample synthesis process, the method minimizes the need for human supervision and avoids additional manual burdens when utilizing positive and toxic samples.\nTo answer the above challenging question, we introduce PT-ALIGN (Figure 2), a novel self-alignment method of safety for LLM that minimizes human supervision by refining positive and toxic samples in an automatic and contrastive manner, and performing fine-grained dual instruction tuning. Specifically, the target model first generates instructions tailored to the specific safety theme. It then infers paired positive and toxic responses contrastively, constrained by the model's self-generated context. Finally, it learns safe response patterns through a combination of MLE and fine-grained UT methods, guiding the probability distribution away from toxic content while preserving the model's utility. The contributions of this paper are summarized as follows.\n\u2022 PT-samples generation: We innovatively construct both positive and toxic responses dually for safety alignment, requiring minimal human supervision (<50 human an-notations). Guided by self-generated constraints, LLMs can synthesize both safe and malicious responses tailored to custom-defined target safety domains, significantly"}, {"title": "II. RELATED WORK", "content": "A. Safety alignment for LLMS\nLarge language models (LLMs) [30], through pre-training on vast text corpora, have acquired unparalleled capabilities [31] in text comprehension and generation. However, the next-word prediction objective in pre-trained LLMs, along with them being harmless, helpful, and honest, necessitates their safety alignment. Current alignment efforts often do not prioritize safety, and aligning models to follow instructions can compromise safety [32], [14]. One safety alignment approach involves using human feedback-based reinforcement learning (RLHF)[33], where human feedback signals serve as rewards. Safety alignment is also achieved through supervised fine-tuning (SFT), typically using positive question-answer pairs as training samples. Instruction tuning [18] is a highly effective method, enabling LLMs to learn safe response styles. Differing from this, our method guides the model to synthesize both safe and toxic samples for instruction tuning in a self-alignment manner, incorporating extremely negative samples as a new supervisory signal into the training process of LLMs, thereby refining the loss functions.\nB. Minimal human cost safety alignment\nAligning LLMs with humans has been widely studied [34], [18], but it requires significant human labor and introduces biases from alignment personnel [17], [21], [20]. Recent approaches leverage LLMs' capabilities to reduce human in-volvement, mitigating these issues. The reinforcement learning with AI feedback(RLAIF) [35] method replaces human feed-back with LLMs' feedback but requires a better-aligned LLM as the teacher model and often faces instability during training. The self-alignment [22] method uses manual principles as context for LLMs to generate Q&A pairs for alignment, but this requires maintaining complex principles and generally works better on larger models. The self-correction method [36] encourages LLMs to analyze negative samples and perform error analysis to enhance safety. On the contrary, our method directly uses severely toxic samples for instruction tuning to achieve safety alignment. We innovatively enable LLMs to generate and use self-constraint texts to guide their responses to inducing questions. Specifically, we encourage the model to generate negative self-constraints to synthesize severely toxic answers, enabling it to directly avoid harmful content.\nC. Multi-quality sample tuning\nGeneral SFT-based methods fine-tune LLMs using only optimal samples, neglecting negative samples as supervisory signals [37], [38]. Contrastive learning [39] methods maximize the similarity of positive samples by comparing them with negative and unrelated samples. However, this approach is difficult to apply to token-level text generation tasks. The RLHF method requires ranking sample quality and increasing human labor. The Unlikelihood Training (UT) [40] method avoids generating repetitive and meaningless text and can be combined with the MLE method on positive and negative sample pairs, i.e., safe and toxic responses. Different from"}, {"title": "III. PT-ALIGN METHOD", "content": "A. Overview\nOur proposed PT-ALIGN utilizes minimal seed samples and human annotations. The target LLMs synthesize instructions and responses under self-guidance. Additionally, instruction tuning is performed using both safe and severely harmful samples. It consists of two parts: Refining of positive and toxic samples and Fine-grained dual instruction tuning, following the process outlined in Figure 3.\nRefining of positive and toxic samples: An LLM subdi-vides the safety domains D into safety topics T. Based on {(D,T)}, the LLM generates a set of safety-related instructions I. Initial constraints are defined as Cp and Cn, guiding the model in generating safe responses P and toxic responses N, respectively. The LLM synthesizes two sets of complete constraints, $C_p$ and $C_n$. The model performs positive and toxic (negative) annotations for the instruction set I under self-constraints $C_p$ and $C_n$, resulting in dataset S = {(I, P, N)}.\nFine-grained dual instruction tuning: We apply MLE and fine-grained UT on the positive and toxic samples within dataset S. Specifically, to refine the loss function calculation at the token level, we exclude the first few identical tokens in positive and toxic sample pairs from the UT loss calcu-lation to prevent unintended penalization of positive samples during gradient descent. The implementation details of each component are outlined below.\nB. Refining of positive and toxic samples\na) Safety-driven red teaming: This process broadly fol-lows the self-instruct method [17]. To prioritize alignment safety, we directly integrate safety domain types as specified by OpenAI and MetaAI usage policies [41] [42], [43], re-sulting in ten safety domain categories D = {$D_1,..., D_{10}$}, such as privacy violation. To broaden the scope of refinement and maximize the diversity of the instruction set I, the LLM subdivides the safety domain categories $D_i$. Ten related safety topics $T_i$ are generated for each safety domain $D_i$ as $T_i$ = f($D_i$) = {$T_{i1}, T_{i2}, ..., T_{i10}$}. After deduplication, we obtain a dataset of safety topics along with their corresponding safety domains {($D_i$, $T_{ij}$) | $T_{ij}$ = f($D_i$)}, where f denotes the generation process of the LLM under a prompt. We prepend a concise prefix to each safety topic to streamline the instruction generation process.\nNext, we generate a large number of diverse, high-quality safety-related instructions rather than questions, as instructions are more likely to provoke LLMs into generating harmful content, and using the instruction format in training better ensures model safety [32]. Each instruction is associated with a safety topic and is capable of inducing the model to generate incorrect or toxic responses. Using annotation prompts for all safety topics and ten seeds (generated using Gemini-1.0-pro [44], one for each safety domain, totaling ten. The generation of different seed samples across the three sets had minimal impact on the experimental results) as ICL examples, the LLM will synthesize a large number of corresponding instructions for each topic. Based on predefined rules, it will then dedupli-cate and remove low-quality instructions. We ultimately refine a set of high-quality instructions I that cover various safety domains. Our researchers ensured that the seeds and training data we constructed did not overlap with the test data.\nb) Self-constraint-driven positive and toxic responses: To refine harmless and severely toxic responses P, N for the safety-related instruction set I established above, we leverage the self-constraint capabilities of the unaligned LLM, further minimizing the need for human supervision.\nFirst, taking the synthesis process of the positive sample P as an example (the negative sample N follows a similar pro-cess), we generate self-constraint prompts to serve as ICL for the LLM's harmless responses. Starting with a simple human-"}, {"title": "C. Fine-grained dual instruction tuning", "content": "a) Positive samples instruction tuning: To maximize the available capacity within the standard LLM's maximum token length and enable the model to directly learn the intrinsic relationships between instructions and responses, thereby en-hancing its defensive ability across a wider range of induc-ing questions, the triplet training set will exclude all ICL examples and inner thought information. Only instructions, safe responses, and toxic responses will be retained in the instruction tuning dataset.\nLet s be a training sample from dataset S, consisting of a triplet of textual information: instruction i, positive example p, and negative example n, represented as s = (i,p, n). The instruction i, with length \u03b1, can be represented as i = {$i_1,...,i_\u03b1$}. Similarly, p and n can be represented as"}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental setup\na) PT-ALIGN dataset for safety alignment: We use 10 safety-related seed samples to teach the LLM the safety patterns necessary for synthesizing safety-related instructions. These samples cover a comprehensive range of safety domains, consolidated from more than ten categories officially provided by OpenAI and MetaAI into the following ten: Illegal Activity, Hate Speech, Malware Generation, Physical Harm, Economic Harm, Fraud, Sex, Privacy Violation, Controversial Topics, and Unethical Activity. For annotating positive and toxic aspects, 6 seed examples (positive and negative samples are written by Gemini-1.0-pro with adjusted safety settings) with inner thoughts are provided to the LLM as ICL, fully utilizing its reasoning capabilities for safety annotation. After multiple refinements, the LLM synthesized 16,020 high-quality tripartite sample sets of instructions, positive answers, and toxic answers. These synthesized samples are exclusively used in subsequent experiments for safety self-alignment.\nb) Experimental setting: For the step of synthesizing safety topics, we use the nuclear sampling [49] with a top-p threshold of p = 0.98 and a temperature of t = 1.0. During the safety instruction synthesis phase, only the top-p threshold is adjusted to p = 0.95. For the self-constraint continuation phase, we use nucleus sampling with a top-p threshold of p = 0.98 and a temperature of t = 1.0. During the positive and toxic response phase, only the top-p threshold is adjusted to p = 0.95. During the instruction fine-tuning phase, whether using general Su-pervised Fine-Tuning or the fine-grained unlikelihood training method employed by PN-ALIGN, we exclusively fine-tune the"}, {"title": "B. Effectiveness of PT-ALIGN on various LLMs", "content": "Our PT-ALIGN method enhances the safety of many pop-ular open-source models. We applied the PT-ALIGN sam-ple synthesis and instruction tuning process on Alpaca-13B, Vicuna-13B, Koala-13B, ChatGLM3-6B, and LLaMA2-7B-chat. As shown in Table III, the models achieved sig-nificant improvements on four safety benchmarks (harm-less, pref, harml, and harm2, representing test variations"}, {"title": "C. Effectiveness of PT-ALIGN on a vanilla LLM", "content": "Our proposed PT-ALIGN method can enhance the safety of vanilla LLMs without diminishing their helpfulness. We con-ducted safety alignment on a pre-trained LLM with a relatively moderate parameter size, such as LLaMA2-13B, which has weaker fundamental performance compared to larger models (e.g., 70B). This setup demonstrates that models with fewer parameters can still achieve substantial safety improvements using the PT-ALIGN method (as shown in the previous section with even smaller models, e.g., 6B). The learning-from-scratch approach highlights the potential of LLMs for self-constraint and self-alignment. We compare the proposed approach with several baseline methods, including vanilla SFT (using only positive samples), DPO [59] (using same samples as PT-ALIGN), and KTO [60] (using the same samples as PT-ALIGN), all based on LLaMA2-13B.\nOn three safety benchmarks (as shown in the first three columns of Table II), PT-ALIGN achieves the best perfor-mance, surpassing the officially released LLaMA2-13B-chat with over a 24% improvement in harmless. Notably, PT-ALIGN does not compromise helpfulness. However, when negative samples are excluded as supervisory signals and fine-grained UT is not employed during instruction tuning (referred to as the PT-ALIGN-SFT-13B method), the model's helpful score significantly decreases, and its safety performance is generally inferior to our standard PT-ALIGN method. The improvement in the honest metric may be attributed to the presence of test samples that require the model to restrain its responses, partially aligns with our tuning objectives. The increase in the other metric may be attributed to PT-ALIGN redundantly generating certain non-harmful instructions dur-ing the sample synthesis phase, which likely contributes to stabilizing the model's overall helpfulness."}, {"title": "D. Impact of PT-ALIGN on general performance", "content": "To thoroughly assess the impact of the PT-ALIGN method on the model's general performance, we use Big-Bench [52] (TruthfulQA & SocialQA) and MMLU [56] to evaluate the model's basic cognitive and reasoning abilities. The setup for Big-Bench is consistent with the multiple-choice question testing mentioned earlier. Additionally, we conduct tests using 5-shot prompting on the MMLU benchmark.\nAs shown in Table V, none of the eight popular baseline models exhibited significant declines in key evaluation metrics across the three test sets. This demonstrates that the PT-ALIGN method not only preserves the model's helpfulness but also maintains its general performance to a certain extent. In other words, PT-ALIGN achieves a favorable trade-off between safety and general capabilities."}, {"title": "E. PT-ALIGN against jailbreak attacks", "content": "In the daily use scenarios of LLMs, it is usually sufficient to defend against straightforward and conventional instruction or question-based inducement prompts. However, LLMs may encounter more complex requests initiated by malicious users, known as jailbreak attacks. These requests can often deceive unaligned or poorly aligned LLMs, bypassing their existing safety defense mechanisms and reactivating compliance with inducement prompts, ultimately leading to the generation of harmful content. In this subsection, we employ more sophis-ticated jailbreak attack evaluations to further assess the safety alignment capability of the PT-ALIGN method.\nWe employ the AutoDAN attack method to further evaluate PT-ALIGN. AutoDAN systematically generates a series of candidate prompts through natural language variations, includ-ing rewrites, synonym substitutions, and structural rephrasings. These prompts are semantically similar but diverse in form, making them challenging for defense mechanisms to detect. Neural search algorithms, such as reinforcement learning or gradient optimization, are then employed to identify the most effective jailbreak prompts from the generated candidates. Furthermore, an iterative prompt refinement process, similar to the attack texts used against DeepSeek in Figure 1 but with increased attack intensity, is applied. The attack success rate (ASR%) is used as the metric to quantify safety performance. We continue to use the unaligned LLaMA2-13B as the base-line. As shown in Table 6, the LLaMA2-13B model aligned with PT-ALIGN achieves an exceptionally low attack success rate of 0.51%, significantly outperforming the LLaMA2-13B-Chat model, which achieves 5.06%. This result indicates that the PT-ALIGN method effectively resists prompt-based jailbreak attacks to a considerable extent, demonstrating its superior safety alignment performance. Although more com-plex and challenging jailbreak attack benchmarks are yet to be tested, the PT-ALIGN method demonstrates robustness and significant potential in resisting such attacks."}, {"title": "F. Scaling ability", "content": "We demonstrated the sample scaling performance of our proposed PT-ALIGN method (based on LLaMA2-13B). As shown in Figure 4, as the number of synthetic samples grad-ually increases from 2,000 to 16,000, the harmless attribute consistently rises from 86% to 98%, confirming the sample scaling ability of the PT-ALIGN method. It can achieve better safety performance with more synthetic samples without dete-rioration. With sufficient samples, helpful generally fluctuates around the baseline model performance. This aligns with our hypothesis that the LLM output's harmless is associated with the number of positive and toxic samples."}, {"title": "G. Case study", "content": "In this subsection, we present the actual output texts of vari-ous models in response to two typical adversarial instructions. These instructions are crafted to prompt the models to generate harmful content, disclose or extract private information poten-"}, {"title": "H. Ablation study", "content": "Our observations indicate that, although positive and toxic samples differ fundamentally in semantics, they often share certain similarities in word usage. This highlights the neces-sity of fine-tuning at a fine-grained, character-level. Ablation experiments on the fine-grained element of UT (i.e., Equation 4) demonstrate its effectiveness in enhancing safety while preserving helpfulness. As shown in Table VIII, both the harmlessness metric and other helpfulness indicators decline when the token-level fine-grained alignment process is omit-ted, underscoring the importance of this component."}, {"title": "I. The influence on different penalty coefficients", "content": "As shown in Table IX, We experiment with the penalty coefficient \u03bb of the Unlikelihood Training Loss, using \u03bb values from [0.1, 0.4, 0.7, 1.0], to observe its impact on the PT-ALIGN method. Excessively high or low values result in slight declines in safety fine-tuning performance, with optimal performance achieved around \u03bb = 0.4. This value effectively maximizes the improvement in safety while preserving the model's overall helpfulness and utility, clearly demonstrating its robustness and suitability for the current training setup."}, {"title": "J. The influence on different seeds", "content": "To evaluate the impact of seed randomness on the perfor-mance of our method, we used Alpaca-13B as the baseline model and examined its PT-ALIGN fine-tuning performance across different seed sets. All seeds were independently gen-erated by Gmini-1.0-pro (with adjusted safety settings). As"}, {"title": "V. DISCUSSION", "content": "In this section, we explore the advantages of our proposed PT-ALIGN method and its generated highly toxic samples. The discussion focuses on two key aspects: the semantics differences between positive/toxic samples and preference-based samples, and a comparison of the training processes of PT-ALIGN with conventional alignment methods. We present a detailed comparison between our positive and toxic samples and preference-based samples, emphasizing the stronger safety polarity of the former. This pronounced polarity enables toxic samples to serve as more effective supervisory signals, guiding the model to better distinguish between safe and harmful generated content. Additionally, we will use quantitative ex-periments to demonstrate the advantages of the PT-ALIGN method over the other methods, including differences in data representation and training efficiency."}, {"title": "A. Textual differences between PT-ALIGN samples and preference-based samples", "content": "Our proposed PT-ALIGN method constructs positive and toxic samples in a dual manner, aiming to maximize the semantic polarity difference between the two types of re-sponses (where semantic polarity refers to the contrast between safer, more reliable responses and more negative, harmful responses). In contrast, preference-based samples often lack pronounced semantic polarity differences, with both responses frequently sharing the same safety orientation and exhibiting minimal contrast."}, {"title": "RQ1: How do PT-ALIGN and preference-based samples differ in semantic variation and safety polarity?", "content": "Table VII provides textual examples of PT-ALIGN and preference-based samples, clearly illustrating the key differences between the two approaches. PT-Samples exhibit more pronounced semantic differences, with a stark contrast in safety polarity. In contrast, preference-based samples often exhibit minimal semantic variation, and the two preference samples may even share the same safety polarity, either both being safe or both being harmful. This lack of contrast can lead the model to inadvertently learn harmful content, thereby weakening its safety alignment effectiveness."}, {"title": "RQ2: How does the design of safety polarity differences between samples enhance the PT-ALIGN method's ability to achieve better safety alignment?", "content": "We apply Principal Com-ponent Analysis (PCA) to analyze their representations, further quantifying the semantic differences between PT-Samples and preference-based samples. Specifically, we use a sentence-BERT model to extract embedding features for both sample types and visualize their representations in two dimensions using PCA. As shown in Figure 5, panel (a) demonstrates that the representation differences between positive and toxic samples are significant, with an average cosine distance of 0.3824 per sample pair, and their vector space distributions are clearly distinct. In contrast, panel (b) illustrates that the representation differences between corresponding preference samples are minimal, with an average cosine distance of 0.2806 per sample pair, and each sample pair occupying the same position in the figure indicates that their vector space distributions are highly similar. The presence of non-negligible toxic content in the preferred responses of the preference-based sample set presents a critical issue, as it may undermine the alignment process and hinder the model's ability to effectively distinguish between safe and harmful content. This phenomenon motivates the development of a safety alignment method that leverages positive and highly toxic samples, enabling the model to learn more effectively how to generate safe and reliable responses."}, {"title": "B. Training performance analysis and comparison for fine-grained dual instruction fine-tuning", "content": "In this section, we examine the changes in training loss curves of PT-ALIGN compared to other methods or when using different types of samples, to further validate the training stability and efficiency of our approach."}, {"title": "RQ3: How Can Dual Instruction Tuning with Positive and Toxic Samples Enable the Model to Achieve Better Safety?", "content": "As shown in Figures 6a and 6b, the loss curve of the PT-ALIGN method exhibits fewer fluctuations and converges more smoothly compared to reinforcement learning methods such as DPO and KTO. This stability underscores its effectiveness in achieving robust safety alignment during training. Additionally, the faster initial descent of PT-ALIGN's loss curve demonstrates its efficiency in optimizing safety-related objectives. By leveraging both highly positive and toxic samples, PT-ALIGN enhances the model's safety polarity gap, resulting in a clearer distinction between safe and harmful content. These attributes highlight PT-ALIGN's potential to train models with greater reliability and safety."}, {"title": "RQ4: How Does PT-ALIGN Achieve Lower Helpfulness Loss Compared to SFT and Dual Instruction Tuning on Preference-Based Samples?", "content": "As illustrated in Figure 6c, the inclusion of toxic samples accelerates the model's training convergence and achieves significantly lower loss values. This enables the model to utilize toxic samples as a point of comparison, leading to safer performance. In Figure 6d, while preference-based samples achieve lower loss values and ex-hibit more stable training compared to using positive samples alone, the pronounced safety polarity differences between positive and toxic samples allow the model to more effectively and directly diverge from harmful distributions, thus reducing the tendency to diminish helpfulness during the alignment process, thereby maximizing its ability to learn safe paradigms."}, {"title": "VI. CONCLUSION", "content": "In this paper, we present PT-ALIGN, a novel safety self-alignment method that minimizes human supervision by au-"}, {"title": "Algorithm 1 Refining Positive and Toxic Samples", "content": "Require: LLM: target language model\n1: D = {$D_1,..., D_{10}$}: safety domains\n2: f(): generates topics for each domain\n3: Z: contextual ICL examples\n4: $C_p$: initial positive constraint\n5: $C_n$: initial negative constraint\nEnsure: S = {(Ik, Pk, Nk)}: instruction-response triplets\n6: function DOUBLESAMPLEDATASYNTHESIS(LLM, D, f, Z, Cp, Cn)\n7:   I \u2190 \u00d8\n8:   $C_p$ \u2190 AUTOCOMPLETE($c_p$, LLM)\n9:   $C_n$ \u2190 AUTOCOMPLETE($c_n$, LLM)\n10:  for each $D_i$ \u2208 D do\n11:    $T_i$ \u2190 f($D_i$)        \u25b7 Generate topics for Di\n12:    $T_i$ \u2190 DEDUPLICATETOPICS($T_i$)  \u25b7 Remove duplicate topics\n13:    for each $T_{ij}$ \u2208 $T_i$ do\n14:      $I_{ij}$ \u2190 GENERATEINSTR($T_{ij}$, Z, LLM)\n15:      $I_{ij}$ \u2190 DEDUPLICATE($I_{ij}$)     \u25b7 Remove duplicate instructions\n16:      $I_{ij}$ \u2190 FILTERLOWQUALITY($I_{ij}$)\n17:      I \u2190 I \u222a $I_{ij}$\n18:    end for\n19:  end for\n20:  S \u2190 \u00d8\n21:  for each Ik \u2208 I do\n22:    (Pk, Nk) \u2190 GENERATERESPONSESPAIR(Ik, Z, Cp, Cn, LLM)\n     \u25b7 Inner thought guides response generation\n23:    S \u2190 S \u222a {(Ik, Pk, Nk)}\n24:  end for\n25:  return S\n26: end function"}, {"title": "Algorithm 2 Fine-grained Dual Instruction Tuning", "content": "Require: LLM with parameters \u03b8\n1: S = {(i,p, n)}: instruction-positive-negative triplets\n2: \u03bb: UT penalty coefficient\nEnsure: LLM with updated parameters \u03b8*\n3: function FINEGRAINEDDUALINSTRUCTIONTUNING(LLM, S, \u03bb)\n4:   Initialize \u03b8\n5:   for each training epoch do\n6:     for each mini-batch B \u2286 S do\n7:       total_loss \u2190 0\n8:       for each (i, p, n) in B do\n9:         logits_pos \u2190 FORWARDPASS(LLM, (i, p))\n10:        $L_{MLE}$ \u2190 CROSSENTROPYLOSs(logits_pos, p)\n11:        logits_neg \u2190 FORWARDPASS(LLM, (i, n))\n12:        \u03b3* \u2190 FINDFIRSTMISMATCH(p, n)\n13:        $L_{UT}$ \u2190 UNLIKELIHOODLOSS(logits_neg, n, \u03b3*)\n14:        L \u2190 $L_{MLE}$ + \u03bb \u00d7 $L_{UT}$\n15:        total_loss \u2190 total_loss + L\n16:      end for\n17:    BACKPROPANDUPDATE(LLM, total_loss)\n18:   end for\n19: end for\n20: return LLM\n21: end function\nWith the aforementioned specific modifications to the model's instruction tuning loss function, the unaligned LLM will undergo safety self-alignment directly using the care-fully designed triplet dataset S without any additional ICL examples. The detailed pseudocode for the Fine-grained Dual Instruction Tuning process is provided in Algorithm 2."}]}