{"title": "Graph Retrieval-Augmented Generation: A Survey", "authors": ["BOCI PENG", "YUN ZHU", "YONGCHAO LIU", "XIAOHE BO", "HAIZHOU SHI", "CHUNTAO HONG", "YAN ZHANG", "SILIANG TANG"], "abstract": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges\nof Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge\nbase, RAG refines LLM outputs, effectively mitigating issues such as \u201challucination\u201d, lack of domain-specific\nknowledge, and outdated information. However, the complex structure of relationships among different\nentities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval, capturing relational knowledge\nand facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG,\na systematic review of current technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based\nIndexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and\ntraining methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation\nmethodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire\nfurther inquiries and advance progress in the field.", "sections": [{"title": "Introduction", "content": "The development of Large Language Models like GPT-4 [116], Qwen2 [170], and LLaMA [24] has\nsparked a revolution in the field of artificial intelligence, fundamentally altering the landscape of\nnatural language processing. These models, built on Transformer [149] architectures and trained\non diverse and extensive datasets, have demonstrated unprecedented capabilities in understanding,\ninterpreting, and generating human language. The impact of these advancements is profound,\nstretching across various sectors including healthcare [93, 154, 188], finance [84, 114], and educa-\ntion [38, 157], where they facilitate more nuanced and efficient interactions between humans and\nmachines.\nAlthough RAG has achieved impressive results and has been widely applied across various\ndomains, it faces limitations in real-world scenarios: (1) Neglecting Relationships: In practice, textual\ncontent is not isolated but interconnected. Traditional RAG fails to capture significant structured\nrelational knowledge that cannot be represented through semantic similarity alone. For instance, in\na citation network where papers are linked by citation relationships, traditional RAG methods focus\non finding the relevant papers based on the query but overlook important citation relationships\nbetween papers. (2) Redundant Information: RAG often recounts content in the form of textual\nsnippets when concatenated as prompts. This makes context become excessively lengthy, leading\nto the \u201clost in the middle\u201d dilemma [94]. (3) Lacking Global Information: RAG can only retrieve a"}, {"title": "Comparison with Related Techniques and Surveys", "content": "In this section, we compare Graph Retrieval-Augmented Generation (GraphRAG) with related\ntechniques and corresponding surveys, including RAG, LLMs on graphs, and Knowledge Base\nQuestion Answering (KBQA).\nRAG combines external knowledge with LLMs for improved task performance, integrating domain-\nspecific information to ensure factuality and credibility. In the past two years, researchers have\nwritten many comprehensive surveys about RAG [27, 37, 51, 54, 165, 180, 187]. For example, Fan et al.\n[27] and Gao et al. [37] categorize RAG methods from the perspectives of retrieval, generation, and\naugmentation. Zhao et al. [187] review RAG methods for databases with different modalities. Yu et al.\n[180] systematically summarize the evaluation of RAG methods. These works provide a structured\nsynthesis of current RAG methodologies, fostering a deeper understanding and suggesting future\ndirections of the area.\nFrom a broad perspective, GraphRAG can be seen as a branch of RAG, which retrieves relevant\nrelational knowledge from graph databases instead of text corpus. However, compared to text-\nbased RAG, GraphRAG takes into account the relationships between texts and incorporates the\nstructural information as additional knowledge beyond text. Furthermore, during the construction\nof graph data, raw text data may undergo filtering and summarization processes, enhancing the\nrefinement of information within the graph data. Although previous surveys on RAG have touched\nupon GraphRAG, they predominantly center on textual data integration. This paper diverges by\nplacing a primary emphasis on the indexing, retrieval, and utilization of structured graph data,\nwhich represents a substantial departure from handling purely textual information and spurs the\nemergence of many new techniques."}, {"title": "LLMs on Graphs", "content": "LLMs are revolutionizing natural language processing due to their excellent text understanding,\nreasoning, and generation capabilities, along with their generalization and zero-shot transfer\nabilities. Although LLMs are primarily designed to process pure text and struggle with non-\nEuclidean data containing complex structural information, such as graphs [41, 153], numerous\nstudies [13, 28, 65, 83, 92, 105, 119, 120, 161, 189] have been conducted in these fields. These papers\nprimarily integrate LLMs with GNNs to enhance modeling capabilities for graph data, thereby\nimproving performance on downstream tasks such as node classification, edge prediction, graph\nclassification, and others. For example, Zhu et al. [189] propose an efficient fine-tuning method\nnamed ENGINE, which combines LLMs and GNNs through a side structure for enhancing graph\nrepresentation.\nDifferent from these methods, GraphRAG focuses on retrieving relevant graph elements using\nqueries from an external graph-structured database. In this paper, we provide a detailed introduction\nto the relevant technologies and applications of GraphRAG, which are not included in previous\nsurveys of LLMs on Graphs."}, {"title": "KBQA", "content": "KBQA is a significant task in natural language processing, aiming to respond to user queries based\non external knowledge bases [33, 76, 77, 174], thereby achieving goals such as fact verification,\npassage retrieval enhancement, and text understanding. Previous surveys typically categorize\nexisting KBQA approaches into two main types: Information Retrieval (IR)-based methods and\nSemantic Parsing (SP)-based methods. Specifically, IR-based methods [60, 61, 102, 142, 155, 168, 181]\nretrieve information related to the query from the knowledge graph (KG) and use it to enhance the\ngeneration process. While SP-based methods [12, 15, 29, 40, 141, 177] generate a logical form (LF)\nfor each query and execute it against knowledge bases to obtain the answer.\nGraphRAG and KBQA are closely related, with IR-based KBQA methods representing a subset of\nGraphRAG approaches focused on downstream applications. In this work, we extend the discussion\nbeyond KBQA to include GraphRAG's applications across various downstream tasks. Our survey\nprovides a thorough and detailed exploration of GraphRAG technology, offering a comprehensive\nunderstanding of existing methods and potential improvements."}, {"title": "Preliminaries", "content": "In this section, we introduce background knowledge of GraphRAG for easier comprehension of\nour survey. First, we introduce Text-Attributed Graphs which is a universal and general format of\ngraph data used in GraphRAG. Then, we provide formal definitions for two types of models that\ncan be used in the retrieval and generation stages: Graph Neural Networks and Language Models."}, {"title": "Text-Attributed Graphs", "content": "The graph data used in Graph RAG can be represented uniformly as Text-Attributed Graphs (TAGs),\nwhere nodes and edges possess textual attributes. Formally, a text-attributed graph can be denoted\nas $G = (V,E, A, {xv}_{v\u2208V}, {e_{i,j}}_{i,j\u2208E})$, where $V$ is the set of nodes, $E \u2286 V \u00d7 V$ is the set of\nedges, $A \u2208 {0, 1}^{|V|\u00d7|V|}$ is the adjacent matrix. Additionally, ${x_v}_{v\u2208V}$ and ${e_{i,j}}_{i,j\u2208E}$ are textual\nattributes of nodes and edges, respectively. One typical kind of TAGs is Knowledge Graphs (KGs),\nwhere nodes are entities, edges are relations among entities, and text attributes are the names of\nentities and relations."}, {"title": "Graph Neural Networks", "content": "Graph Neural Networks (GNNs) are a kind of deep learning framework to model the graph data.\nClassical GNNs, e.g., GCN [74], GAT [150], GraphSAGE [44], adopt a message-passing manner to\nobtain node representations. Formally, each node representation $h_i^{(l-1)}$ in the $l$-th layer is updated\nby aggregating the information from neighboring nodes and edges:\n$h_i^{(l)} = UPD(h_i^{(l-1)}, AGG_{j\u2208N(i)}MSG(h_i^{(l-1)}, h_j^{(l-1)}, e_{i,j}^{(l-1)})),$ (1)\nwhere $N(i)$ represents the neighbors of node $i$. MSG denotes the message function, which computes\nthe message based on the node, its neighbor, and the edge between them. AGG refers to the\naggregation function that combines the received messages using a permutation-invariant method,\nsuch as mean, sum, or max. UPD represents the update function, which updates each node's\nattributes with the aggregated messages.\nSubsequently, a readout function, e.g., mean, sum, or max pooling, can be applied to obtain the\nglobal-level representation:\n$h_G = READOUT_{i\u2208V}(h_i^{(L)}).$ (2)\nIn GraphRAG, GNNs can be utilized to obtain representations of graph data for the retrieval\nphase, as well as to model the retrieved graph structures."}, {"title": "Language Models", "content": "Language models (LMs) excel in language understanding and are mainly classified into two types:\ndiscriminative and generative. Discriminative models, like BERT [22], RoBERTa [97] and Sentence-\nBERT [129], focus on estimating the conditional probability $P(y|x)$ and are effective in tasks such as\ntext classification and sentiment analysis. In contrast, generative models, including GPT-3 [10] and\nGPT-4 [116], aim to model the joint probability $P(x, y)$ for tasks like machine translation and text\ngeneration. These generative pre-trained models have significantly advanced the field of natural\nlanguage processing (NLP) by leveraging massive datasets and billions of parameters, contributing\nto the rise of Large Language Models (LLMs) with outstanding performance across various tasks.\nIn the early stages, RAG and GraphRAG focused on improving pre-training techniques for\ndiscriminative language models [22, 97, 129]. Recently, LLMs such as ChatGPT [117], LLaMA [24],\nand Qwen2 [170] have shown great potential in language understanding, demonstrating powerful\nin-context learning capabilities. Subsequently, research on RAG and GraphRAG shifted towards\nenhancing information retrieval for language models, addressing increasingly complex tasks and\nmitigating hallucinations, thereby driving rapid advancements in the field."}, {"title": "Overview of GraphRAG", "content": "GraphRAG is a framework that leverages external structured knowledge graphs to improve contex-\ntual understanding of LMs and generate more informed responses, as depicted in Figure 2. The\ngoal of GraphRAG is to retrieve the most relevant knowledge from databases, thereby enhancing\nthe answers of downstream tasks. The process can be defined as\n$a* = arg max_{\u03b1\u0395A} p(a|q, G),$ (3)\nwhere $a*$ is the optimal answer of the query $q$ given the TAG $G$, and $A$ is the set of possible\nresponses. After that, we jointly model the target distribution $p(a|q, G)$ with a graph retriever\n$p_\u03b8(G|q, G)$ and an answer generator $p_\u03c6(a|q, G)$ where $\u03b8, \u03c6$ are learnable parameters, and utilize the"}, {"title": "Graph-Based Indexing", "content": "The construction and indexing of graph databases form the foundation of GraphRAG, where\nthe quality of the graph database directly impacts GraphRAG's performance. In this section, we\ncategorize and summarize the selection or construction of graph data and various indexing methods\nthat have been employed."}, {"title": "Graph Data", "content": "Various types of graph data are utilized in GraphRAG for retrieval and generation. Here, we\ncategorize these data into two categories based on their sources, including Open Knowledge Graphs\nand Self-Constructed Graph Data."}, {"title": "Open Knowledge Graphs", "content": "Open knowledge graphs refer to graph data sourced from publicly\navailable repositories or databases [2, 7, 138, 151]. Using these knowledge graphs could dramatically\nreduce the time and resources required to develop and maintain. In this survey, we further classify\nthem into two categories according to their scopes, i.e., General Knowledge Graphs and Domain\nKnowledge Graphs."}, {"title": "General Knowledge Graphs", "content": "General knowledge graphs primarily store general, structured\nknowledge, and typically rely on collective input and updates from a global community, ensuring a\ncomprehensive and continually refreshed repository of information.\nEncyclopedic knowledge graphs are a typical type of general knowledge graph, which contains\nlarge-scale real-world knowledge collected from human experts and encyclopedias. For example,\nWikidata\u00b9 [151] is a free and open knowledge base that stores structured data of its Wikimedia\nsister projects like Wikipedia, Wikivoyage, Wiktionary, and others. Freebase\u00b2 [7] is an extensive,\ncollaboratively edited knowledge base that compiles data from various sources, including individual\ncontributions and structured data from databases like Wikipedia. DBpedia\u00b3 [2] represents informa-\ntion about millions of entities, including people, places, and things, by leveraging the infoboxes\nand categories present in Wikipedia articles. YAGO\u2074 [138] collects knowledge from Wikipedia,\nWordNet, and GeoNames.\nCommonsense knowledge graphs are another type of general knowledge graph. They include\nabstract commonsense knowledge, such as semantic associations between concepts and causal\nrelationships between events. Typical Commonsense Knowledge Graphs include: ConceptNet5 [91]\nis a semantic network built from nodes representing words or phrases connected by edges denoting\nsemantic relationships. ATOMIC [56, 131] models the causal relationships between events."}, {"title": "Domain Knowledge Graphs", "content": "As discussed in Section 1, domain-specific knowledge graphs are\ncrucial for enhancing LLMs in addressing domain-specific questions. These KGs offer specialized\nknowledge in particular fields, aiding models in gaining deeper insights and a more comprehensive\nunderstanding of complex professional relationships. In the biomedical field, CMeKG\u00ba encompasses\na wide range of data, including diseases, symptoms, treatments, medications, and relationships\nbetween medical concepts. CPubMed-KG7 is a medical knowledge database in Chinese, building on\nthe extensive repository of biomedical literature in PubMed. In the movie domain, Wiki-Movies [110]"}, {"title": "Self-Constructed Graph Data", "content": "Self-Constructed Graph Data facilitates the customization and\nintegration of proprietary or domain-specific knowledge into the retrieval process. For downstream\ntasks that do not inherently involve graph data, researchers often propose constructing a graph\nfrom multiple sources (e.g., documents, tables, and other databases) and leveraging GraphRAG to\nenhance task performance. Generally, these self-constructed graphs are closely tied to the specific\ndesign of the method, distinguishing them from the open-domain graph data previously mentioned.\nTo model the structural relationships between the documents, Munikoti et al. [113] propose to\nconstruct a heterogeneous document graph capturing multiple document-level relations, including\nco-citation, co-topic, co-venue, etc. Li et al. [87] and Wang et al. [160] establish relationship\nbetween passages according to shared keywords. To capture the relations between entities in\ndocuments, Delile et al. [20], Edge et al. [25], Guti\u00e9rrez et al. [43] and Li et al. [80] utilize the named\nentity recognition tools to extract entities from documents and language models to further extract\nrelations between entities, where the retrieved entities and relations then form a knowledge graph.\nThere are also some mapping methods for downstream tasks that need to be designed based on\nthe characteristics of the task itself. For example, to solve the patent phrase similarity inference\ntask, Peng and Yang [122] convert the patent database into a patent-phrase graph. Connections\nbetween patent nodes and phrase nodes are established if the phrases appear in the patents, while\nconnections between patent nodes are based on citation relations. Targeting customer service\ntechnical support scenarios, Xu et al. [169] propose to model historical issues into a KG, which\ntransforms the issues into tree representations to maintain the intra-issue relations, and utilize\nsemantic similarities and a threshold to preserve inter-issue relations."}, {"title": "Indexing", "content": "Graph-Based Indexing plays a crucial role in enhancing the efficiency and speed of query operations\non graph databases, directly influencing subsequent retrieval methods and granularity. Common\ngraph-based indexing methods include graph indexing, text indexing, and vector indexing."}, {"title": "Graph Indexing", "content": "Graph indexing represents the most commonly used approach, preserving\nthe entire structure of the graph. This method ensures that for any given node, all its edges and\nneighboring nodes are easily accessible. During subsequent retrieval stages, classic graph search\nalgorithms such as BFS and Shortest Path Algorithms can be employed to facilitate retrieval\ntasks [64, 66, 102, 142, 146, 175]."}, {"title": "Text Indexing", "content": "Text indexing involves converting graph data into textual descriptions to\noptimize retrieval processes. These descriptions are stored in a text corpus, where various text-based\nretrieval techniques, such as sparse retrieval and dense retrieval, can be applied. Some approaches\ntransform knowledge graphs into human-readable text using predefined rules or templates. For\ninstance, Li et al. [81], Huang et al. [55] and Li et al. [86] use predefined templates to convert each\ntriple in knowledge graphs into natural language, while Yu et al. [179] merge triplets with the same\nhead entity into passages. Additionally, some methods convert subgraph-level information into"}, {"title": "Graph-Guided Retrieval", "content": "In GraphRAG, the retrieval process is crucial for ensuring the quality and relevance of generated\noutputs by extracting pertinent and high-quality graph data from external graph databases. However,\nretrieving graph data presents two significant challenges: (1) Explosive Candidate Subgraphs: As the\ngraph size increases, the number of candidate subgraphs grows exponentially, requiring heuristic\nsearch algorithms to efficiently explore and retrieve relevant subgraphs. (2) Insufficient Similarity\nMeasurement: Accurately measuring similarity between textual queries and graph data necessitates\nthe development of algorithms capable of understanding both textual and structural information.\nConsiderable efforts have previously been dedicated to optimizing the retrieval process to address\nthe above challenges. This survey focuses on examining various aspects of the retrieval process\nwithin GraphRAG, including the selection of the retriever, retrieval paradigm, retrieval granularity,\nand effective enhancement techniques. The general architectures of Graph-Guided Retrieval are\ndepicted in Figure 3."}, {"title": "Retriever", "content": "In GraphRAG, various retrievers possess unique strengths for addressing different aspects of\nretrieval tasks. We categorize retrievers into three types based on their underlying models: Non-\nparametric Retriever, LM-based Retriever, and GNN-based Retriever. It is important to note that"}, {"title": "Non-parametric Retriever", "content": "Non-parametric retrievers, based on heuristic rules or traditional\ngraph search algorithms, do not rely on deep-learning models, thereby achieving high retrieval\nefficiency. For instance, Yasunaga et al. [175] and Taunk et al. [146] retrieve k-hop paths containing\nthe topic entities of each question-choice pair. G-Retriever [47] enhances the conventional Prize-\nCollecting Steiner Tree (PCST) algorithm by incorporating edge prices and optimizing relevant\nsubgraph extraction. Delile et al. [20] and Mavromatis and Karypis [108] first extract entities\nmentioned in the query and then retrieve the shortest path related to these entities. These methods\noften involve an entity linking pre-processing step to identify nodes in the graph before retrieval."}, {"title": "LM-based Retriever", "content": "LMs serve as effective retrievers in GraphRAG due to their strong natural\nlanguage understanding capabilities. These models excel in processing and interpreting diverse\nnatural language queries, making them versatile for a wide range of retrieval tasks within graph-\nbased frameworks. We primarily categorized LMs into two types: discriminative and generative\nlanguage models. Subgraph Retriever [181] trains RoBERTa [97] as the retriever, which expands\nfrom the topic entity and retrieves the relevant paths in a sequential decision process. KG-GPT [71]\nadopts LLMs to generate the set of top-K relevant relations of the specific entity. Wold et al. [164]\nutilize fine-tuned GPT-2 to generate reasoning paths. StructGPT [58] utilizes LLMs to automatically\ninvoke several pre-defined functions, by which relevant information can be retrieved and combined\nto assist further reasoning."}, {"title": "GNN-based Retriever", "content": "GNNs are adept at understanding and leveraging complex graph\nstructures. GNN-based retrievers typically encode graph data and subsequently score different\nretrieval granularities based on their similarity to the query. For example, GNN-RAG [108] first\nencodes the graph, assigns a score to each entity, and retrieves entities relevant to the query based\non a threshold. EtD [90] iterates multiple times to retrieve relevant paths. During each iteration, it\nfirst uses LLaMA2 [148] to select edges connecting the current node, then employs GNNs to obtain\nembeddings of the new layer of nodes for the next round of LLM selection."}, {"title": "Retrieval Paradigm", "content": "Within GraphRAG, different retrieval paradigms, including once retrieval, iterative retrieval, and\nmulti-stage retrieval, play crucial roles in improving the relevance and depth of the retrieved\ninformation. Once retrieval aims to gather all pertinent information in a single operation. Iterative\nretrieval conducts further searches based on previously retrieved information, progressively nar-\nrowing down to the most relevant results. Here we further divide iterative retrieval into adaptive\nretrieval and non-adaptive retrieval, with the only difference lying in whether the stopping of the\nretrieval is determined by the model. Another retrieval paradigm is multi-stage retrieval, where"}, {"title": "Once Retrieval", "content": "Once retrieval aims to retrieve all the relevant information in a single query.\nOne category of approaches [43, 50, 81] utilize embedding similarities to retrieve the most relevant\npieces of information. Another category of methods design pre-defined rules or patterns to directly\nextract specific structured information such as triplets, paths or subgraphs from graph databases.\nFor example, G-Retriever [47] utilizes an extended PCST algorithm to retrieve the most relevant\nsubgraph. KagNet [88] extracts paths between all pairs of topic entities with lengths not exceeding\nk. Yasunaga et al. [175] and Taunk et al. [146] extract the subgraph that contains all topic entities\nalong with their 2-hop neighbors.\nFurthermore, in this subsection, we also include some multiple retrieval methods that involve\ndecoupled and independent retrievals, allowing them to be computed in parallel and executed only\nonce. For example, Luo et al. [102] and Cheng et al. [16] first instruct LLMs to generate multiple\nreasoning paths and then use a BFS retriever to sequentially search for subgraphs in the knowledge\ngraphs that match each path. KG-GPT [71] decomposes the original query into several sub-queries,\nretrieving relevant information for each sub-query in a single retrieval process."}, {"title": "Iterative Retrieval", "content": "In iterative retrieval, multiple retrieval steps are employed, with sub-\nsequent searches depending on the results of prior retrievals. These methods aim to deepen the\nunderstanding or completeness of the retrieved information over successive iterations. In this\nsurvey, we further classify iterative retrieval into two categories: (1) non-adaptive and (2) adaptive\nretrieval. We provide a detailed summary of these two categories of methods below."}, {"title": "Non-Adaptive Retrieval", "content": "Non-adaptive methods typically follow a fixed sequence of retrieval,\nand the termination of retrieval is determined by setting a maximum time or a threshold. For\nexample, PullNet [139] retrieves problem-relevant subgraphs through T iterations. In each iteration,\nthe paper designs a retrieval rule to select a subset of retrieved entities, and then expands these\nentities by searching relevant edges in the knowledge graph. In each iteration, KGP [160] first\nselects seed nodes based on the similarity between the context and the nodes in the graph. It then\nuses LLMs to summarize and update the context of the neighboring nodes of the seed nodes, which\nis utilized in the subsequent iteration."}, {"title": "Adaptive Retrieval", "content": "One distinctive characteristic of adaptive retrieval is to let models au-\ntonomously determine the optimal moments to finish the retrieval activities. For instance, [42, 168]\nleverage an LM for hop prediction, which serves as an indicator to end the retrieval. There is also a\ngroup of researchers who utilize model-generated special tokens or texts as termination signals\nfor the retrieval process. For example, ToG [142] prompts the LLM agent to explore the multiple\npossible reasoning paths until the LLM determines the question can be answered based on the\ncurrent reasoning path. [181] trains a RoBERTa to expand a path from each topic entity. In the\nprocess, a virtual relation named as \u201c[END]\u201d is introduced to terminate the retrieval process.\nAnother common approach involves treating the large model as an agent, enabling it to directly\ngenerate answers to questions to signal the end of iteration. For instance, [58, 60, 66, 143, 158]\npropose LLM-based agents to reason on graphs. These agents could autonomously determine the\ninformation for retrieval, invoke the pre-defined retrieval tools, and cease the retrieval process\nbased on the retrieved information."}, {"title": "Multi-Stage Retrieval", "content": "Multi-stage retrieval divides the retrieval process linearly into multiple\nstages, with additional steps such as retrieval enhancement, and even generation processes occur-\nring between these stages. In multi-stage retrieval, different stages may employ various types of\nretrievers, which enables the system to incorporate various retrieval techniques tailored to different\naspects of the query. For example, Wang et al. [159] first utilize a non-parametric retriever to extract\nn-hop paths of entities in the query's reasoning chain, then after a pruning stage, it further retrieves\nthe one-hop neighbors of the entities in the pruned subgraph. OpenCSR [45] divides the retrieval\nprocess into two stages. In the first stage, it retrieves all 1-hop neighbors of the topic entity. In the\nsecond stage, it compares the similarity between these neighbor nodes and other nodes, selecting\nthe top-k nodes with the highest similarity for retrieval. GNN-RAG [108] first employs GNNs to\nretrieve the top-k nodes most likely to be the answer. Subsequently, it retrieves all shortest paths\nbetween query entities and answer entities pairwise."}, {"title": "Retrieval Granularity", "content": "According to different task scenarios and indexing types, researchers design distinct retrieval\ngranularities (i.e., the form of related knowledge retrieved from graph data), which can be divided\ninto nodes, triplets, paths, and subgraphs. Each retrieval granularity has its own advantages, making\nit suitable for different practical scenarios. We will introduce the details of these granularities in\nthe following sections."}, {"title": "Nodes", "content": "Nodes allow for precise retrieval focused on individual elements within the graph,\nwhich is ideal for targeted queries and specific information extraction. In general, for knowledge\ngraphs, nodes refer to entities. For other types of text attribute graphs, nodes may include textual\ninformation that describes the node's attributes. By retrieving nodes within the graph, GraphRAG\nsystems could provide detailed insights into their attributes, relationships, and contextual infor-\nmation. For example, Munikoti et al. [113], Li et al. [87] and Wang et al. [160] construct document\ngraphs and retrieves relevant passage nodes. Liu et al. [90], Sun et al. [139] and Guti\u00e9rrez et al. [43]\nretrieve entities from constructed knowledge graphs."}, {"title": "Triplets", "content": "Generally, triplets consist of entities and their relationships in the form of subject-\npredicate-object tuples, providing a structured representation of relational data within a graph. The\nstructured format of triplets allows for clear and organized data retrieval, making it advantageous\nin scenarios where understanding relationships and contextual relevance between entities is\ncritical. Yang et al. [171] retrieve triplets containing topic entities as relevant information. Huang\net al. [55], Li et al. [81] and Li et al. [86] first convert each triplet of graph data into textual\nsentences using predefined templates and subsequently adopt a text retriever to extract relevant\ntriplets. However, directly retrieving triplets from graph data may still lack contextual breadth\nand depth, thus being unable to capture indirect relationships or reasoning chains. To address this\nchallenge, Wang et al. [152] propose to generate the logical chains based on the original question,\nand retrieve the relevant triplets of each logical chain."}, {"title": "Paths", "content": "The retrieval of path-granularity data can be seen as capturing sequences of rela-\ntionships between entities, enhancing contextual understanding and reasoning capabilities. In\nGraphRAG, retrieving paths offers distinct advantages due to their ability to capture complex\nrelationships and contextual dependencies within a graph.\nHowever, path retrieval can be challenging due to the exponential growth in possible paths as\ngraph size increases, which escalates computational complexity. To address this, some methods\nretrieve relevant paths based on pre-defined rules. For example, Wang et al. [159] and Lo and Lim\n[98] first select entity pairs in the query and then traverse to find all the paths between them within\nn-hop. HyKGE [64] first defines three types of paths: path, co-ancestor chain, and co-occurrence\nchain, and then utilizes corresponding rules to retrieve each of these three types of paths. In\naddition, some methods utilize models to perform path searching on graphs. ToG [142] proposes to\nprompt the LLM agent to perform the beam search on KGs and find multiple possible reasoning\npaths that help answer the question. Luo et al. [102], Wu et al. [168] and Guo et al. [42] first utilizes\nthe model to generate faithful reasoning plans and then retrieves relevant paths based on these\nplans. GNN-RAG [108] first identifies the entities in the question. Subsequently, all paths between\nentities that satisfy a certain length relationship are extracted."}, {"title": "Subgraphs", "content": "Retrieving subgraphs offers significant advantages due to its ability to capture\ncomprehensive relational contexts within a graph. This granularity enables GraphRAG to extract\nand analyze complex patterns, sequences, and dependencies embedded within larger structures,\nfacilitating deeper insights and a more nuanced understanding of semantic connections.\nTo ensure both information completeness and retrieval efficiency, some methods propose an\ninitial rule-based approach to retrieve candidate subgraphs, which are subsequently refined or\nprocessed further. Peng and Yang [122] retrieve the ego graph of the patent phrase from the self-\nconstructed patent-phrase graph. Yasunaga et al. [175], Feng et al. [32] and Taunk et al. [146] first\nselect the topic entities and their two-hop neighbors as the node set, and then choose the edges\nwith head and tail entities both in the node set to form the subgraph. Besides, there are also some\nembedding-based subgraph retrieval methods. For example, Hu et al. [50] first encode all the k-hop\nego networks from the graph database, then retrieve subgraphs related to the query based on the\nsimilarities between embeddings. Wen et al. [163] and Li et al. [80] extract two types of graphs,\nincluding Path evidence subgraphs and Neighbor evidence subgraphs, based on pre-defined rules.\nOpenCSR [45] starts from a few initial seed nodes and gradually expands to new nodes, eventually\nforming a subgraph."}, {"title": "Hybrid Granularties", "content": "Considering the advantages and disadvantages of various retrieval gran-\nularities mentioned above, some researchers propose using hybrid granularities, that is, retrieving\nrelevant information of multiple granularities from graph data. This type of granularity enhances\nthe system's ability to capture both detailed relationships and broader contextual understanding,\nthus reducing noise while improving the relevance of the retrieved data. Various previous works\npropose to utilize LLM agents to retrieve complex hybrid information. Jin et al. [66], Jiang et al.\n[58], Jiang et al. [60], Wang et al. [158] and Sun et al. [143] propose to adopt LLM-based agents for\nadaptively selecting nodes, triplets, paths, and subgraphs."}, {"title": "Retrieval Enhancement"}]}