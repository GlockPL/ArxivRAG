{"title": "FD-SOS: Vision-Language Open-Set Detectors\nfor Bone Fenestration and Dehiscence Detection\nfrom Intraoral Images", "authors": ["Marawan Elbatel", "Keyuan Liu", "Yanqi Yang", "Xiaomeng Li"], "abstract": "Accurate detection of bone fenestration and dehiscence (FD)\nis crucial for effective treatment planning in dentistry. While cone-beam\ncomputed tomography (CBCT) is the gold standard for evaluating FD,\nit comes with limitations such as radiation exposure, limited accessibil-\nity, and higher cost compared to intraoral images. In intraoral images,\ndentists face challenges in the differential diagnosis of FD. This paper\npresents a novel and clinically significant application of FD detection\nsolely from intraoral images. To achieve this, we propose FD-SOS, a\nnovel open-set object detector for FD detection from intraoral images.\nFD-SOS has two novel components: conditional contrastive denois-\ning (CCDN) and teeth-specific matching assignment (TMA).\nThese modules enable FD-SOS to effectively leverage external dental\nsemantics. Experimental results showed that our method outperformed\nexisting detection methods and surpassed dental professionals by 35%\nrecall under the same level of precision.", "sections": [{"title": "1 Introduction", "content": "Bone fenestration and dehiscence (FD) are abnormal conditions that affect the\nsupporting structures of teeth, potentially leading to compromised oral health\nand tooth loss if left untreated. Achieving accurate detection of FD is crucial\nfor developing effective treatment plans and delivering optimal patient care in\ndentistry. Currently, dentists primarily rely on analyzing intraoral images of the\nteeth and gums to make a preliminary diagnosis of FD. However, this diagnostic\nprocess often lacks accuracy and suffers from subjectivity. While cone beam com-nputed tomography (CBCT) is considered the gold standard for diagnosing FD\nin clinical practice, its frequent use in orthodontic treatment, especially in pe-\ndiatric patients, is associated with radiation-related cancer risks [21]. Moreover,\nCBCT scans are costly and less accessible, particularly in low-income countries.\nTherefore, there is significant value in developing an FD detection approach that\ncan achieve higher accuracy than dentists while eliminating the need for CBCT\nscans, using only intraoral images.\nWhile there have been deep learning approaches for detecting FD from CBCT\nscans [15], no prior work has focused on FD detection from intraoral images. To\naddress this gap, we have collected an in-house dataset called FDTooth, which\nconsists of 150 intraoral images with annotated bounding boxes of anterior teeth\nprovided by dentists, as well as corresponding FD detection results obtained\nthrough CBCT. As shown in Fig. 1(a), the dataset only includes annotations\nfor FD and normal cases in anterior teeth. There are no specific annotations for\nposterior teeth provided, given FD is not visible in frontal view images. This\nleads to missing annotations for dashed bounding boxes for posterior teeth.\nFD detection from intraoral images can be considered an object detection\ntask that involves two subtasks: accurately localizing the teeth (\u201cAnterior\u201d or\n\u201cPosterior\u201d) and diagnosing them as either \u201cNo FD\u201d or \u201cFD\u201d. One possible so-\nlution is to employ object detectors such as DETR [1], YOLO [7], FCOS [20], or\nFaster-RCNN [17] or their variants like DDETR [25], DINO [23] and Diffusion-\nDETR [3], with multiple heads and multi-label optimization objectives such as\nbinary cross-entropy, Focal [13], TMLL [9]. Yet, directly applying these methods\nto our dataset for FD detection yields unsatisfactory results; see \"w/o pretrain-\ning\" in Tab. 2. This can be primarily attributed to the limited size of our dataset,\nwhich poses challenges in terms of enlarging it, as our dataset relies on CBCT as\nthe ground truth for each tooth. Therefore, leveraging publicly available teeth\ndetection datasets, which provide detection labels specifically for anterior and\nposterior teeth (as shown in Fig. 1 (b)), for pretraining the network and subse-\nquently fine-tuning it on our dataset, is a potential solution. Another potential\nsolution is to develop a multi-task object detection framework by using both the"}, {"title": "2 Methodology", "content": "Since there is a lack of publicly available datasets specifically designed for FD\ndetection from intraoral images, we collected a new dataset called FDTOOTH.\nTable 1 shows a comparison between our dataset and the existing public dataset.\nNotably, annotations for posterior teeth in our dataset are not available since\nFD in posterior teeth cannot be detected from a frontal intra-oral view. The\npublic dataset only provides bounding box annotations for all teeth, including\nboth anterior and posterior, without any FD-specific information."}, {"title": "2.2 Overall Framework", "content": "As depicted Fig. 2, we start by extracting text features from each dataset as \u201cAn-\nterior Teeth\u201d and \u201cPosterior Teeth\u201d for teeth detection and \u201cAnterior Teeth No\nFD\u201d and \u201cAnterior Teeth FD\u201d for FD detection. Leveraging Grounding DINO as\nour VLM baseline [14], we fuse image-text features at various stages, employing\na multimodal image-text contrastive loss to enhance the alignment between the\nvisual and textual modalities. Then, these aligned image-text features are used\nfor object localization and classification tasks. A novel Conditional Contrastive\nDenoising (CCDN) is proposed during the decoding process for the detection\nhead, DINO [23], exploiting the common semantics of dental structures avail-\nable in both datasets (e.g \"Anterior Teeth\"). Finally, we leverage a positional\nprior to introduce a Teeth-specific Matching Assignment (TMA) to mask out\npositive detections that are missing in the ground truth."}, {"title": "2.3 Conditional Contrastive Denoising", "content": "The ad-hoc approach of encoding the multi-task information in text, although a\npromising baseline, falls short by not considering the interconnected label space\nof dental attributes, thus limiting its effectiveness. For instance, G-DINO [14], an\nOSOD building on DINO [23], employs contrastive denoising to improve training\nstability. However, its approach to selecting positive and negative anchors for\ndenoising is indiscriminate, overlooking potential spurious correlations caused by\noverlapping labels across tasks. Specifically, categorizing \"FD\" in undiagnosed\n\"Anterior teeth\" as a negative query fails to recognize that these teeth may\nbe affected by \"FD\" in a cross-task context. To overcome these challenges, we\npropose a novel conditional contrastive denoising (CCDN) approach."}, {"title": "2.4 Teeth-Specific Hungarian Matching Assignment", "content": "Object detection models, including OSOD, utilize a sampling approach for the\ndetection head where positive and negative boxes are sampled for a better learn-\ning procedure. These models can be characterized by multi-stage [18,17] or single-\nstage object detectors [20], with the latter being more robust to missing annota-\ntions [22]. However, sampling negative queries for training in all object detection\nmodels can lead to problems when dealing with imperfect and missing annota-\ntions [6,19]. Since the frontal FD detection task is missing annotations within\nthe posterior teeth, it is essential to automatically exclude them during training.\nTo this end, we propose a Teeth-specific Matching Assignment (TMA).\nAs depicted in Fig. 3 (b). we leverage a positional prior about the anterior\nand posterior teeth, disregarding the posterior teeth during the loss calculation if\nits information is not available. Utilizing traditional image processing techniques,\nwe identify predicted bounding boxes as posterior teeth if they fall outside the\nvertical bounds yet lie within the horizontal confines of the anterior teeth region.\nGiven an image $I$ with a set of bounding boxes $\\{(X_1,Y_1,X_2,Y_2)_b\\}$, where\n$(X_1,Y_1)_b$ denotes the top-left corner and $(x_2, y_2)_b$ denotes the bottom-right corner\nof each bounding box $b$, we aim to identify the extremities, $E$, of the anterior\nteeth region. This can be accomplished by defining the following limits:\n$E_{left} = min(x_1), E_{right} = max(x_2), E_{top} = min(y_1), E_{bottom} = max(y_2)$   (3)\nWe then generate a mask for the posterior teeth region as colored in red\nin Fig. 3 (b) as:\n$mask = ((x < E_{left}) \\lor (x > E_{right})) \\land ((y > E_{top}) \\lor (y < E_{bottom}))$   (4)\nFollowing, we assign \"x\" as a no-care value for predicted bounding boxes\nwithin the red mask in the Hungarian matching for object detection [17,1], im-\nplicitly masking the unannotated true positives during gradient calculation. This\npositional prior assignment is simple and effectively handles missing posterior\nteeth annotations, increasing model confidence without the need for complex\narchitecture or pseudo-label mining, focusing on the differential diagnosis of FD\nin frontal intra-oral images."}, {"title": "3 Experiments", "content": "Implementation Details. For all baselines, we utilize SWIN-T transformer as\nthe image backbone [16], which is initialized with ImageNet weights. The models\nare trained using AdamW optimizer with a batch size of 16. The learning rate is\nsearched within the range of [2.5e-5,5e-4] with mmdetection framework [2]. For\nopen-set object detectors, we adopt the pre-trained BERT-base as the textual\nbackbone, which was frozen during the fine-tuning process [4].\nBaselines. Traditional object detectors include DeformableDETR [25], DINO [23],\nand DiffusionDet [3] as SOTA object detection models. Multi-Label Baselines\ninclude Hierarchical-Diff-DETR [8] that utilize DiffusionDet [3]. Additionally, we\nutilize all SOTA detectors with the cross-task data employing multi-head objec-\ntives. Open Set Object detectors baselines (OSOD) include GLIP [12] and\nGrounding DINO [14], SOTA OSOD.\nEvaluation Metrics. Metrics include Pascal VOC (AP50) and COCO evalu-\nation (AP75, AP averaged over IoU thresholds from 0.5 to 0.95 with a step of\n0.05). We report separate results for the positive FD class, resulting in a total\nof 6 metrics; For all methods, we exclude posterior teeth model output when\ncalculating metrics on the held-out testing set of FDTOOTH. see appendix for\nteeth detection (\u201cAnterior\u201d, \u201cPosterior\u201d) on the cross-task dataset."}, {"title": "3.1 Performance on FD Detection", "content": "Tab. 2 shows FD detection results on our FDTOOTH test set. A straightfor-\nward approach for our task involves applying object detection models without\nmulti-task training. Notably, SOTA object detectors without multi-task train-\ning and warmup perform poorly, as illustrated by Diffusion-DETR [3], which\nachieves an AP50 of 8.85%, compared to warming up the model, which signifi-\ncantly improves performance to 66.37%. Incorporating cross-task training in the\nfine-tuning phase, similar to multi-label objectives as seen in Hierarchical-Diff-\nDETR [8], leads to reduced performance. This decline is probably attributed\nto spurious correlations and the absence of annotations, with the model being\nlimited by the initial warmup stage's optimum. Consequently, this results in a\ndecrease in Diffusion-DETR's [3] AP50 from 66.37% to 64.40%. An alternative\napproach for missing annotations is employing SOTA sparse-detection solutions\nfrom computer vision. Specifically, SparseDet [19] shows enhanced performance\nachieving AP50 of 65.13%, improvement from Hierarchical-Diff-DETR's 64.40%.\nVision-Language Models (VLMs), trained on extensive multimodal data\nand fine-tuned for the FD task without multi-task training, show varied perfor-\nmance. GLIP [12] achieves an AP50 of 55.85%, and GDINO [14] achieves 65.89%,\nreflecting the detrimental effects of spurious correlations present in multi-task\nframeworks. GLIP struggles with missing annotations due to its use of full-level\ntext attention. In contrast, GDINO's sub-sentence attention mechanism offers an\nimplicit solution to missing annotations, outperforming GLIP and establishing it\nas a stronger baseline with an improvement of 10% on AP50. Nonetheless, intro-\nducing a multi-task framework does not improve GDINO's performance, likely\ndue to spurious correlations and the shared dental semantic and label space.\nFD-SOS address these challenges establishing new SOTA benchmarks across\nall metrics. Specifically, FD-SOS surpass the SOTA object detector Diffusion-\nDETR [3] by 6.9% and outperform SparseDet [19] by 11% on AP, a strict\ndetection metric. Importantly, FD-SOS improves the best-performing VLM,\nGDINO [14], by 3.89% on the AP metric and 6.09% on the APFD metric.\nAblation Study. Tab. 3 shows the ablation results of our FD-SOS. Using our\nproposed conditional contrastive denoising (CCDN) increases the performance\nacross all metrics, specifically by 2.3% on APFD. The increase is solely attributed\nto the removal of spurious correlation during the denoising process, enhancing\nthe performance of the baseline and extending it to multi-task detection. Using\nonly the teeth-specific matching increases the performance by 3.5% on APFD,\nindicating that OSOD is still not fully robust to missing annotations, limited by\nthe usage of the detection head, DINO [23]. Utilizing TMA and CCDN together\nsubstantially improves the performance for FD by 6.9% in AP75FD, 6.09% in\nAPFD, and 6.02% in AP50FD compared to the baseline, GDINO [14]."}, {"title": "4 Conclusion", "content": "This paper presents a clinically significant application - FD detection from in-\ntraoral images. We introduce FD-SOS, a novel OSOD-based framework, which\nincorporates two innovative components: CCDN and TMA. The results show\nthat our method significantly outperforms existing detection methods and even\nsurpasses the performance of dental professionals, highlighting its immense clin-\nical value in practical settings. Furthermore, this method can be extended to\nother dental diseases once corresponding datasets become available."}]}