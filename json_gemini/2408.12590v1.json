{"title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations", "authors": ["Can Qin", "Congying Xia", "Krithika Ramakrishnan", "Michael Ryoo", "Lifu Tu", "Yihao Feng", "Manli Shu", "Honglu Zhou", "Anas Awadalla", "Jun Wang", "Senthil Purushwalkam", "Le Xue", "Yingbo Zhou", "Huan Wang", "Silvio Savarese", "Juan Carlos Niebles", "Zeyuan Chen", "Ran Xu", "Caiming Xiong"], "abstract": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of producing realistic scenes from textual descriptions. Building on recent advancements, such as OpenAI's Sora, we explore the latent diffusion model (LDM) architecture and introduce a video variational autoencoder (VidVAE). VidVAE compresses video data both spatially and temporally, significantly reducing the length of visual tokens and the computational demands associated with generating long-sequence videos. To further address the computational costs, we propose a divide-and-merge strategy that maintains temporal consistency across video segments. Our Diffusion Transformer (DiT) model incorporates spatial and temporal self-attention layers, enabling robust generalization across different timeframes and aspect ratios. We have devised a data processing pipeline from the very beginning and collected over 13M high-quality video-text pairs. The pipeline includes multiple steps such as clipping, text detection, motion estimation, aesthetics scoring, and dense captioning based on our in-house video-LLM model. Training the VidVAE and DiT models required approximately 40 and 642 H100 days, respectively. Our model supports over 14-second 720p video generation in an end-to-end way and demonstrates competitive performance against state-of-the-art T2V models. Code will be available at https://github.com/SalesforceAIResearch/xgen-videosyn.", "sections": [{"title": "Introduction", "content": "Text-to-video (T2V) generation models are designed to create videos that depict both realistic scenes from textual descriptions. These models have garnered significant attention from both academia and industry due to current breakthroughs in the field. Recently, Sora [1] demonstrated that it is possible to generate realistic videos of over one minute in length. Despite such impressive advancements, the most capable video generation models remain proprietary and their details undisclosed. In the meantime, a number of open video generation models have emerged, but their capabilities seem to qualitatively under-perform proprietary models. In this work, our primary objective is to design an effective architecture for T2V with competitive performance compared to the state of the art; examine the associated modeling and training technologies; as well as explore the data collection pipeline.\n\nA popular approach for image and video generation builds upon the latent diffusion model (LDM) [2] architecture. In this framework, pixel information is typically compressed with a pre-trained VAE [3] into a latent encoded space. A diffusion process is then applied to this latent space either with a U-Net [4, 5] or DiT architecture [6]. Generally, this framework has been adapted to both text to image [2, 7\u20139] and text to video [10-12] generation tasks."}, {"title": "Related Work", "sections": [{"title": "Video Generation", "content": "Video generation has gained significant popularity in recent years, drawing considerable attention. Building on advancements in diffusion models for image generation, these techniques have been adapted for video generation, particularly through the use of 3D U-Net architectures [16]. To address the challenge of generating high-resolution videos, cascaded architectures have proven effective. For example, Imagen Video [17] and Make-a-Video [18] employ multi-stage pipelines that integrate spatial and temporal super-resolution networks. However, training such multi-stage models poses considerable difficulties with many hyper-parameters to tune.\n\nInspired by the success of Latent Diffusion, Video LDM [19] adapts a similar approach to the video domain by employing a Variational Autoencoder (VAE) to encode videos into latent representations. Other models such as Stable Video Diffusion (SVD) [10], Lavie [12], and ModelScope [20] utilize a 3D U-Net architecture to model diffusion processes in latent spaces.\n\nThe Diffusion Transformer (DiT) has gained prominence for its multi-scale flexibility and scalability. It effectively addresses the limitations of U-Net models which are often constrained by fixed-size data generation due to the inherent constraints of convolutional operations in local feature learning. DiT also benefits from acceleration techniques borrowed from Large Language Models (LLMs), facilitating easier scaling. Latte [11], a pioneering method, extends DiT to the video domain with the introduction of a spatial-temporal transformer block. Sora [1] employs DiT as its backbone, inspiring further developments in the field. Open-source projects like Open-Sora [13] and OpenSoraPlan [21] have emerged as leading open-source projects, continuing to push the boundaries in this field."}, {"title": "Variational Autoencoders", "content": "Variational Autoencoders (VAEs) [3] have become a prominent tool for image encoding. Two main approaches are typically employed: encoding images into continuous latent spaces [2, 3], and incorporating quantization techniques to learn discrete latent representations, as in VQVAE [22] and VQGAN [23]. Expanding the application of VAEs, recent research has delved into encoding videos, aiming to leverage these encoded representations in text-to-video generation models. VideoGPT [24] employs a variant of VQ-VAE using 3D convolutions and axial self-attention to learn downsampled, discrete latent representations of raw videos. MAGVIT [25] introduces a new 3D-VQVAE architecture focused on temporal compression; and its successor, MAGVIT-v2 [26], further refines the video"}]}, {"title": "Model Architecture", "content": "Our proposed xGen-VideoSyn-1 model comprises three main components: 1) a VideoVAE encoder and decoder, 2) a Video Diffusion Transformer (VDiT), and 3) a Language Model (Text Encoder). Further details are described below."}, {"title": "Video VAE", "content": "The task of the VideoVAE encoder is to take an input video and produce a latent encoding that can be used for reconstruction later. Our primary objective is to efficiently compress videos not only in the spatial dimension but also temporally, thereby enhancing training speed and reducing computation costs. Drawing inspiration from [21], we enhance the conventional 2D VAE-used predominantly for spatial compression of still images into a 3D variant capable of temporal compression by incorporating time-compacting layers. Originally introduced by [3], VAEs have been extensively utilized for image autoencoding, encoding an image into a latent feature space and subsequently reconstructing it from that space. Specifically, given an image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ in RGB format, the encoder $E$ maps $x$ to a latent representation $z = E(x)$, and the decoder $D$ reconstructs the image from $z$, such that $x = D(z) = D(E(x))$, where $z \\in \\mathbb{R}^{h \\times w \\times c}$. The encoder reduces the dimensionality in the feature space by a factor of $f = H/h = W/w$. To construct a 3D VideoVAE, we adapt a pre-trained 2D image VAE\u00b9, with a spatial compression rate of 1/8 from [2]. This adaptation involves the incorporation of time compression layers into the model. Similarly, for a video $x \\in \\mathbb{R}^{T \\times H \\times W \\times 3}$,"}, {"title": "Video Diffusion Transformer (VDIT)", "content": "Our Video Diffusion Transformer is based on the architecture of Open-Sora [13] and Latte [11], utilizing a stack of spatial-temporal transformer blocks as illustrated in Figure 3. Each transformer module incorporates a pre-norm layer and multi-head self-attention (MHA). We use Rotary Positional Embedding (RoPE) [14] to encode temporal information and sinusoidal encoding [15] for spatial information. For text feature extraction, we employ the T5 model [27] with a token length limit of 250. The extracted text features are integrated into the backbone through a cross-attention layer. We follow the PixArt-Alpha [8] model to encode the time-step embedding, incorporating a modulation layer within each transformer block.\n\nWe take the latent diffusion model (LDM) for training [2]. It follows the standard DDPM [5] with denoising loss and uses Diffusion Transformer (DiT) [6] as the diffusion backbone. To enable generative controllability, our model has applied conditioning caption signals (y), encoded aside by language model T5 and injected into the DiT, with the help of cross-attention layers. This can be formulated as:\n\n$\\mathcal{L}_{LDM} := \\mathbb{E}_{z \\sim \\mathbb{E}(x),\\epsilon \\sim \\mathcal{N}(0,1), t, y} [||\\epsilon - \\epsilon_{\\theta}(z_t, t, C_{\\phi}(y))||], $\n\nwhere t represents the time step, $z_t$ is the noise corrupted latent tensor at time step t, and $z_0 = G(x)$. $\\epsilon$ is the unscaled Gaussian noise, $C_{\\phi}$ is the conditioning network parameterized by $\\phi$ and $\\epsilon_{\\theta}$ is the Transformer-like denoising network (video decoder). The parameters of both conditioning"}, {"title": "Training Data Collection and Processing", "content": "As shown at Figure 5, the pipeline operates sequentially as follows: 1) Clipping: Splitting long videos into manageable clips; 2) Deduplication: Removing similar and redundant clips; 3) Motion Detection and Re-Clipping: Analyzing motion dynamics across frames to eliminate static video clips and inconsistent frames. 4) OCR Detection: Identifying and removing clips contaminated with text or watermarks. 5) Aesthetics Scoring: Evaluating and scoring the visual quality of clips. 6) Captioning: Adding descriptive captions to the clips."}, {"title": "Video Clipping", "content": "After loading raw long videos, we use the PySceneDetect\u00b2 tool to divide them into multiple clips with each clip intended to represent a distinct and clean scene. However, we found that some clips still contain redundant or inconsistent scenes, which we address in subsequent steps."}, {"title": "Deduplication", "content": "The clipping process can occasionally yield clips that are highly similar to one another. To address this, a de-duplication step is essential to filter out redundant clips. We use ffmpeg\u00b3 to extract frames and the clip-as-a-service tool to efficiently extract CLIP features and compute similarity scores between clips. In each duplicate pair, we remove the shorter clip based on a similarity score threshold, T. Through empirical analysis, we have found that a threshold of T = 0.9 is effective for identifying duplicates."}, {"title": "Aesthetic Scoring", "content": "To ensure high-quality training data, it is crucial to use video clips that are well-lit, well-composed, and have clear footage. To filter out poor-quality data, we compute the Aesthetic Score\u2014a measure of how visually pleasing a video is. We utilize a simple neural network trained on human aesthetic scores of images. This network, which takes CLIP features as input, outputs a score ranging from 0 to 10. Clips with an Aesthetic Score below 4.5 are filtered out."}, {"title": "Motion Detection and Re-clipping", "content": "In this video processing step, we aim to achieve two primary goals. Firstly, we want to eliminate videos that are nearly static. Secondly, after the initial video clipping, some videos may still exhibit sudden scene changes. For these videos, we will re-clip them to ensure consistency and maintain a unified topic throughout. Our approach utilizes frame differencing to detect motion within a video, followed by motion-based re-clipping. The process commences with the computation of grayscale frame differences, where we subtract each frame from its predecessor in the sequence. This technique,"}, {"title": "Optical Character Recognition (OCR)", "content": "We also conduct OCR to detect text in the video in order to get high quality video data. The tool we used is PaddleOCR6. We performed text detection on key frames from the videos. The text detection model we used is \u201cch_PPOCRv4_det_infer\u201d, a lightweight model supporting Chinese, English, and multilingual text detection. In this step, we only kept videos where the size of the bounding box is smaller than 20000 pixels."}, {"title": "Dense Captioning", "content": "We train a multimodal video LLM to generate video captions. This model takes a sequence of frames from the video as an input, and is trained to generate text captions describing the contents of the video as an output."}, {"title": "Distributed Data Processing Pipeline", "content": "The Distributed Data Processing Pipeline meets the following criteria:\n\n1.  Each process (one of the six steps above) is able to use its own resource specs. For example, clipping is CPU based, captioning is GPU based.\n\n2.  Each process is independently scalable without interrupting the process flow. For example, Clipping is extremely fast, while Similarity Score is time consuming. Hence, we independently scale-down the clipping process and scale-up the Similarity Score process.\n\n3.  The downstream processes are automatically triggered after a process is completed for a video. For example, after clipping is complete for video with ID 'A', the similarity score computation is started for that video automatically.\n\n4.  The activation of a downstream task optionally depends on a condition. For example, The motion detection for a clip is triggered only if the clip does not have a text, which is a result of the OCR detection process.\n\nTo achieve the pipeline, we use the RabbitMQ7 as a Task Orchestrator (Figure 11). Each process is a deployment with it's own resource specification, subscribed to a Task Queue. To trigger the pipeline, we start by pushing the video IDs to the initial queue. This is the only manual step required. Once this is done, the clipping process populates the Similarity Score Queue with the video ID. The Similarity Score deployement, subscribed to the corresponding queue, takes up the task, completes it, and pushes only the de-duplicated clip IDs to the Aesthetic Score queue. The Aesthetic score process, after computation, enqueues the OCR detection Queue with only the IDs of only those clips that meet the threshold. In this fashion, the number of clips that are being processed keeps reducing with each step in the pipeline by skipping the computation for clips that do not meet the passing criteria in the previous steps in the pipeline. In addition to the speed gain by skipping computation for failed clips, we also achieve the speed gain due to pipelining. The time taken for each step to process 1000 videos is given in Tab. 5. Equation 5 shows that the pipelined system is 1.5 times faster than the equivalent sequential system. There is scope to further improve this by speeding up the bottleneck process, as the processing time of the pipelined system is dependent on the time taken"}]}