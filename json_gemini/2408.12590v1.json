{"title": "xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations", "authors": ["Can Qin", "Congying Xia", "Krithika Ramakrishnan", "Michael Ryoo", "Lifu Tu", "Yihao Feng", "Manli Shu", "Honglu Zhou", "Anas Awadalla", "Jun Wang", "Senthil Purushwalkam", "Le Xue", "Yingbo Zhou", "Huan Wang", "Silvio Savarese", "Juan Carlos Niebles", "Zeyuan Chen", "Ran Xu", "Caiming Xiong"], "abstract": "We present xGen-VideoSyn-1, a text-to-video (T2V) generation model capable of producing realistic scenes from textual descriptions. Building on recent advancements, such as OpenAI's Sora, we explore the latent diffusion model (LDM) architecture and introduce a video variational autoencoder (VidVAE). VidVAE compresses video data both spatially and temporally, significantly reducing the length of visual tokens and the computational demands associated with generating long-sequence videos. To further address the computational costs, we propose a divide-and-merge strategy that maintains temporal consistency across video segments. Our Diffusion Transformer (DiT) model incorporates spatial and temporal self-attention layers, enabling robust generalization across different timeframes and aspect ratios. We have devised a data processing pipeline from the very beginning and collected over 13M high-quality video-text pairs. The pipeline includes multiple steps such as clipping, text detection, motion estimation, aesthetics scoring, and dense captioning based on our in-house video-LLM model. Training the VidVAE and DiT models required approximately 40 and 642 H100 days, respectively. Our model supports over 14-second 720p video generation in an end-to-end way and demonstrates competitive performance against state-of-the-art T2V models. Code will be available at https://github.com/SalesforceAIResearch/xgen-videosyn.", "sections": [{"title": "1 Introduction", "content": "Text-to-video (T2V) generation models are designed to create videos that depict both realistic scenes from textual descriptions. These models have garnered significant attention from both academia and industry due to current breakthroughs in the field. Recently, Sora [1] demonstrated that it is possible to generate realistic videos of over one minute in length. Despite such impressive advancements, the most capable video generation models remain proprietary and their details undisclosed. In the meantime, a number of open video generation models have emerged, but their capabilities seem to qualitatively under-perform proprietary models. In this work, our primary objective is to design an effective architecture for T2V with competitive performance compared to the state of the art; examine the associated modeling and training technologies; as well as explore the data collection pipeline.\nA popular approach for image and video generation builds upon the latent diffusion model (LDM) [2] architecture. In this framework, pixel information is typically compressed with a pre-trained VAE [3] into a latent encoded space. A diffusion process is then applied to this latent space either with a U-Net [4, 5] or DiT architecture [6]. Generally, this framework has been adapted to both text to image [2, 7\u20139] and text to video [10-12] generation tasks.\nA crucial component of such design is the dimensionality of the latent space determined by the output of the VAE. A latent space with small dimensionality means that the input pixel information needs to be highly compressed, which makes the reconstruction by diffussion more difficult but computationally less expensive. A latent space with large dimensionality makes reconstruction easier but computationally more expensive. In the case of image generation one can choose larger encoding spaces [2] to facilitate reconstruction quality. However, this trade-off is particularly critical for video generation. If we encode each frame independently using an image VAE [10, 11], a 100 frame video of 720p spatial resolution would translate into a latent space of size 100\u00d74\u00d790\u00d7160 containing 360, 000 tokens. This makes both training computationally very expensive and inference slow.\nTo address this issue, we focus on developing a text-to-video (T2V) generation model based on video-specific VAE and DiT technologies. We adopt a video VAE to achieve effective compression of the video pixel information by reducing both spatial and temporal dimensions. That is, instead of encoding each frame independently, we incorporate both temporal and spatial compression. This significantly decreases the token length, improves the computational cost of training and inference, and facilitates the generation of long videos. Additionally, to further reduce computation during long video encoding, we propose a divide-and-merge strategy. This approach splits a long video into multiple segments, which are encoded individually with overlapping frames to maintain good temporal consistency. With the aid of this advanced video VAE, our xGen-VideoSyn-1 model is able to generate videos with over 100 frames at 720p resolution in an end-to-end manner.\nIn terms of the diffussion stage, we adopt a video diffusion transformer (VDiT) model that is architecturally similar to Latte [11] and Open-Sora [13]. Our VDiT incorporates transformer blocks with both temporal and spatial self-attention layers. We use ROPE [14] and sinusoidal [15] encodings for spatial and temporal position information. This allows for effective generalization across different lengths, aspect ratios, and resolutions. Moreover, our DiT model is trained on a diverse dataset including 240p, 512\u00d7512, 480p, 720p, and 1024\u00d71024 resolutions. The video VAE training takes approximately 40 H100 days, while the DiT model requires around 642 H100 days.\nAnother crucial aspect of T2V models is the data used for training. Generally, these models require high-quality video-text pairs so that the model can better learn the mapping from the text to video modalities. We address this by designing a data processing pipeline that yields a large quantity of high-quality video-text pairs. Our pipeline involves deduplication, OCR, motion and aesthetics analysis, among other processing steps. It also includes a captioning stage, for which we developed"}, {"title": "2 Related Work", "content": "Video generation has gained significant popularity in recent years, drawing considerable attention. Building on advancements in diffusion models for image generation, these techniques have been adapted for video generation, particularly through the use of 3D U-Net architectures [16]. To address the challenge of generating high-resolution videos, cascaded architectures have proven effective. For example, Imagen Video [17] and Make-a-Video [18] employ multi-stage pipelines that integrate spatial and temporal super-resolution networks. However, training such multi-stage models poses considerable difficulties with many hyper-parameters to tune.\nInspired by the success of Latent Diffusion, Video LDM [19] adapts a similar approach to the video domain by employing a Variational Autoencoder (VAE) to encode videos into latent representations. Other models such as Stable Video Diffusion (SVD) [10], Lavie [12], and ModelScope [20] utilize a 3D U-Net architecture to model diffusion processes in latent spaces.\nThe Diffusion Transformer (DiT) has gained prominence for its multi-scale flexibility and scalability. It effectively addresses the limitations of U-Net models which are often constrained by fixed-size data generation due to the inherent constraints of convolutional operations in local feature learning. DiT also benefits from acceleration techniques borrowed from Large Language Models (LLMs), facilitating easier scaling. Latte [11], a pioneering method, extends DiT to the video domain with the introduction of a spatial-temporal transformer block. Sora [1] employs DiT as its backbone, inspiring further developments in the field. Open-source projects like Open-Sora [13] and OpenSoraPlan [21] have emerged as leading open-source projects, continuing to push the boundaries in this field."}, {"title": "2.1 Video Generation", "content": null}, {"title": "2.2 Variational Autoencoders", "content": "Variational Autoencoders (VAEs) [3] have become a prominent tool for image encoding. Two main approaches are typically employed: encoding images into continuous latent spaces [2, 3], and incorporating quantization techniques to learn discrete latent representations, as in VQVAE [22] and VQGAN [23]. Expanding the application of VAEs, recent research has delved into encoding videos, aiming to leverage these encoded representations in text-to-video generation models. VideoGPT [24] employs a variant of VQ-VAE using 3D convolutions and axial self-attention to learn downsampled, discrete latent representations of raw videos. MAGVIT [25] introduces a new 3D-VQVAE architecture focused on temporal compression; and its successor, MAGVIT-v2 [26], further refines the video"}, {"title": "3 Model Architecture", "content": "Our proposed xGen-VideoSyn-1 model comprises three main components: 1) a VideoVAE encoder and decoder, 2) a Video Diffusion Transformer (VDiT), and 3) a Language Model (Text Encoder). Further details are described below."}, {"title": "3.1 Video VAE", "content": "The task of the VideoVAE encoder is to take an input video and produce a latent encoding that can be used for reconstruction later. Our primary objective is to efficiently compress videos not only in the spatial dimension but also temporally, thereby enhancing training speed and reducing computation costs. Drawing inspiration from [21], we enhance the conventional 2D VAE-used predominantly for spatial compression of still images into a 3D variant capable of temporal compression by incorporating time-compacting layers. Originally introduced by [3], VAEs have been extensively utilized for image autoencoding, encoding an image into a latent feature space and subsequently reconstructing it from that space. Specifically, given an image $x \\in R^{H\\times W\\times 3}$ in RGB format, the encoder $E$ maps $x$ to a latent representation $z = E(x)$, and the decoder $D$ reconstructs the image from $z$, such that $x = D(z) = D(E(x))$, where $z \\in R^{h\\times w\\times c}$. The encoder reduces the dimensionality in the feature space by a factor of $f = H/h = W/w$. To construct a 3D VideoVAE, we adapt a pre-trained 2D image VAE, with a spatial compression rate of 1/8 from [2]. This adaptation involves the incorporation of time compression layers into the model. Similarly, for a video $x \\in R^{T\\times H\\times W\\times 3}$, where T represents the number of frames, the VideoVAE encodes $x$ into $z = E(x)$, and $D$ reconstructs the video from $z$, rendering $\\hat{x} = D(z) = D(E(x))$, where $z \\in R^{t\\times h\\times w\\times c}$. The encoder not only reduces the spatial dimensionality by a factor of $f = H/h = W/w$ but also compresses temporally by a factor of $s = T/t$. In our experiments, we achieve a temporal compression of 1/4.\nTo extend the 2D image-based VAE into a 3D VideoVAE, we implemented a series of modifica- tions: 1) We replaced all 2D convolutional layers (Conv2d) with Causal Convolutional 3D layers (CausalConv3D). We opted for CausalConv3D to ensure that only subsequent frames have access to information from previous frames, thereby preserving the temporal directionality from past to future. 2) We introduced a time downsampling layer following the spatial downsampling layers to compact the video data along the temporal dimension. For this purpose, we utilized a 3D average pooling technique. Specifically, we incorporated two temporal downsampling layers, each reducing the temporal resolution by half. Consequently, the overall time compression factor achieved is 1/4, meaning that every four frames are condensed into a single latent representation. The spatial compression ratio remains 1/8.\nDespite achieving a 4\u00d78\u00d78 compression, the computation cost remains a significant bottleneck, particularly as video sizes increase, leading to substantial memory demands. To address the out-of-memory (OOM) issues encountered during long video encoding, we propose a divide-and-merge strategy. As illustrated in Figure 4, this approach involves splitting a long video into multiple segments. Each segment consists of five frames, with duplicate frames at both the beginning and end. These segments are encoded individually, using overlapping frames to maintain strong temporal consistency. With this video Variational Autoencoder (VAE) framework, our xGen-VideoSyn-1 model can generate over 100 frames of 720p video in an end-to-end manner, while mitigating additional computation costs."}, {"title": "3.2 Video Diffusion Transformer (VDIT)", "content": "Our Video Diffusion Transformer is based on the architecture of Open-Sora [13] and Latte [11], utilizing a stack of spatial-temporal transformer blocks as illustrated in Figure 3. Each transformer module incorporates a pre-norm layer and multi-head self-attention (MHA). We use Rotary Positional Embedding (RoPE) [14] to encode temporal information and sinusoidal encoding [15] for spatial information. For text feature extraction, we employ the T5 model [27] with a token length limit of 250. The extracted text features are integrated into the backbone through a cross-attention layer. We follow the PixArt-Alpha [8] model to encode the time-step embedding, incorporating a modulation layer within each transformer block.\nWe take the latent diffusion model (LDM) for training [2]. It follows the standard DDPM [5] with denoising loss and uses Diffusion Transformer (DiT) [6] as the diffusion backbone. To enable generative controllability, our model has applied conditioning caption signals (y), encoded aside by language model T5 and injected into the DiT, with the help of cross-attention layers. This can be formulated as:\n$L_{LDM} := E_{z \\sim E(x), \\varepsilon \\sim N(0,1), t, y} [||\\varepsilon - \\varepsilon_{\\theta}(z_t, t, C_{\\phi}(y))||],$ where $t$ represents the time step, $z_t$ is the noise corrupted latent tensor at time step $t$, and $z_0 = G(x)$. $\\varepsilon$ is the unscaled Gaussian noise, $C_{\\phi}$ is the conditioning network parameterized by $\\phi$ and $\\varepsilon_{\\theta}$ is the Transformer-like denoising network (video decoder). The parameters of both conditioning"}, {"title": "4 Training Data Collection and Processing", "content": "As shown at Figure 5, the pipeline operates sequentially as follows: 1) Clipping: Splitting long videos into manageable clips; 2) Deduplication: Removing similar and redundant clips; 3) Motion Detection and Re-Clipping: Analyzing motion dynamics across frames to eliminate static video clips and inconsistent frames. 4) OCR Detection: Identifying and removing clips contaminated with text or watermarks. 5) Aesthetics Scoring: Evaluating and scoring the visual quality of clips. 6) Captioning: Adding descriptive captions to the clips."}, {"title": "4.1 Video Clipping", "content": "After loading raw long videos, we use the PySceneDetect\u00b2 tool to divide them into multiple clips with each clip intended to represent a distinct and clean scene. However, we found that some clips still contain redundant or inconsistent scenes, which we address in subsequent steps."}, {"title": "4.2 Deduplication", "content": "The clipping process can occasionally yield clips that are highly similar to one another. To address this, a de-duplication step is essential to filter out redundant clips. We use ffmpeg\u00b3 to extract frames and the clip-as-a-service tool to efficiently extract CLIP features and compute similarity scores between clips. In each duplicate pair, we remove the shorter clip based on a similarity score threshold, T. Through empirical analysis, we have found that a threshold of T = 0.9 is effective for identifying duplicates."}, {"title": "4.3 Aesthetic Scoring", "content": "To ensure high-quality training data, it is crucial to use video clips that are well-lit, well-composed, and have clear footage. To filter out poor-quality data, we compute the Aesthetic Score\u2014a measure of how visually pleasing a video is. We utilize a simple neural network trained on human aesthetic scores of images. This network, which takes CLIP features as input, outputs a score ranging from 0 to 10. Clips with an Aesthetic Score below 4.5 are filtered out."}, {"title": "4.4 Motion Detection and Re-clipping", "content": "In this video processing step, we aim to achieve two primary goals. Firstly, we want to eliminate videos that are nearly static. Secondly, after the initial video clipping, some videos may still exhibit sudden scene changes. For these videos, we will re-clip them to ensure consistency and maintain a unified topic throughout. Our approach utilizes frame differencing to detect motion within a video, followed by motion-based re-clipping. The process commences with the computation of grayscale frame differences, where we subtract each frame from its predecessor in the sequence. This technique, while effective, can introduce background noise, manifesting as speckles that falsely indicate motion. These artifacts typically stem from minor camera shakes or the presence of multiple shadows. To counteract this, we implement a threshold on the frame differences to create a binary motion mask. To refine the quality of this motion mask, we apply techniques such as blurring [29] and morphological operations [30]. Following this, we calculate a motion score by taking the mean of the motion mask values.\nGuided by the motion score, we perform both motion detection and re-clipping. An overall illustration is shown in Figure 6. We calculate the average motion score across the video and set a threshold. The overall distribution of the average motion score is illustrated in Figure 13 of the Appendix. Videos falling below this threshold are deemed nearly static and subsequently removed. For the re-clipping, our criteria focus on eliminating significant, sudden scene changes. We identify the frame with the highest motion score and analyze the motion score differences with its neighboring frames. If both the peak motion score and the differences surpass predefined thresholds, this flags a major scene change. Here, we segment the video at this critical frame. We retain the longer segment, ensuring it meets the length requirement and is devoid of further disruptive transitions."}, {"title": "4.5 Optical Character Recognition (OCR)", "content": "We also conduct OCR to detect text in the video in order to get high quality video data. The tool we used is PaddleOCR6. We performed text detection on key frames from the videos. The text detection model we used is \u201cch_PPOCRv4_det_infer\u201d, a lightweight model supporting Chinese, English, and multilingual text detection. In this step, we only kept videos where the size of the bounding box is smaller than 20000 pixels."}, {"title": "4.6 Dense Captioning", "content": "We train a multimodal video LLM to generate video captions. This model takes a sequence of frames from the video as an input, and is trained to generate text captions describing the contents of the video as an output."}, {"title": "4.7 Distributed Data Processing Pipeline", "content": "To efficiently orchestrate the six data processing and filtering steps described above with minimal manual intervention and optimal resource utilization, we employ a Distributed Data Processing Pipeline. We use RabbitMQ to manage a video processing pipeline. Each stage is deployed with specific resources and processes video IDs through multiple queues. The pipeline starts by adding video IDs to the initial queue, with each subsequent stage filtering and processing clips based on predefined criteria. Further details can be found in the Section A.1 of Appendix."}, {"title": "5 Evaluation", "content": "We empirically evaluate the effectiveness of xGen-VideoSyn-1 through a series of comprehensive experiments across various tasks, including video generation and compression. Details on the experimental setup, methodologies, and results analysis are provided in the following sections."}, {"title": "5.1 Implementation Details", "content": "Our proposed xGen-VideoSyn-1 model integrates a 731M diffusion transformer with a 244M video VAE model, trained sequentially. See more details in Table 2.\nThe video VAE model can compress the video by 4x8x8. It is trained on a subset of the Kinetics [49] dataset and additional high-quality internal videos. We sample multi-scale images and videos from"}, {"title": "5.2 Text-to-Video Generation", "content": null}, {"title": "5.2.1 Quantitative Results", "content": "We use Vbench [50] to quantitatively evaluate the text-to-video generation results. Tab. 4 presents various scores for comprehensive evaluation. These scores are categorized into the following metrics: \"Consistency\" (including Background Consistency, Subject Consistency, and Overall Consistency), \"Aesthetic\" (including Aesthetic, Image Quality, and Color), \u201cTemporal\u201d (including Temporal Flick- ering, Motion Smoothness, and Human Action), and \"Spatial\" (spatial relationship). OpenSora V1.1, which is comparable to our model in size (~700M) and training cost, provides a fair benchmark. The ModelScope [20] represents a Stable Diffusion-based method. We conduct the evaluation of OpenSora V1.1 and Ours under the same setting. ModelScope's scores are referred to the official"}, {"title": "5.2.2 User Study", "content": "We also conducted a user study on 2s 720p text-to-video generation to evaluate text controllability, as shown in Figure 9, using Amazon Mechanical Turk [51]. Approximately 100 prompts, each around 55 words in length and randomly generated by ChatGPT, were used to cover a wide range of scenarios and challenging cases. The percentages of user votes for the three methods are reported in Figure 9. In this study, our model outperformed the baseline by more than 15%, indicating a significant improvement. Additionally, the p-value, computed as 0.03 with three repetitions of the user study, is statistically significant with a threshold of <0.05."}, {"title": "5.2.3 Style Control", "content": "To demonstrate the capacity of our model in content creation, we conducted an ablation study on prompts related to style control. As illustrated in Figure 10, we applied a sample prompt with various styles. Our model successfully interprets and generates content in the desired styles, including \"Van Gogh\", \"Pixel Art\u201d, and \u201cCartoon\". By default, the style tends to be realistic. However, applying style control prompts can sometimes reduce the prominence of other elements, such as sunrise, as observed with static sun imagery in the latter two rows. This highlights a limitation of our current model, which may be mitigated by scaling up the model size."}, {"title": "5.3 Video Compression", "content": "To further assess the reconstruction capacity of our trained video VAE, we randomly sampled 1,000 videos from the Kinetics [49] and OpenVid1M [40] datasets, ensuring these videos were not included in the training set. We then used the VAE model to encode and decode these videos, expecting the outputs to be identical to the inputs. We evaluated the results using PSNR, SSIM [52], and mean squared error (MSE) metrics. As shown in Tab. 3, our model outperforms the baseline video VAE from OpenSoraPlan, which has the same compression ratio of 4x8x8, in most scenarios. Nevertheless, there remains a significant gap between the image VAE and our video VAE, indicating substantial potential for future improvements. The image VAE cannot compress videos at the time dimension which leaves huge redundancy in computation."}, {"title": "6 Conclusion", "content": "This work explores the architecture and technologies of the T2V model, focusing on the integration of video VAE and Diffusion Transformer (DiT) architectures. Unlike existing models that utilize image VAEs, our approach incorporates a video VAE to enhance both spatial and temporal compression, addressing the challenges of long token sequences. We introduce a divide-and-merge strategy to manage out-of-memory issues, enabling efficient encoding of extended video sequences. Our xGen-VideoSyn-1 model supports over 100 frames in 720p resolution, and the accompanying DiT model uses advanced encoding techniques for versatile video generation. A robust data pipeline for generating high-quality video-text pairs underpins our model's competitive performance in text-to-video generation."}, {"title": "Appendix", "content": null}, {"title": "A Data Processing", "content": null}, {"title": "A.1 Distributed Data Processing Pipeline", "content": "The Distributed Data Processing Pipeline meets the following criteria:\n1. Each process (one of the six steps above) is able to use its own resource specs. For example, clipping is CPU based, captioning is GPU based.\n2. Each process is independently scalable without interrupting the process flow. For exam- ple, Clipping is extremely fast, while Similarity Score is time consuming. Hence, we independently scale-down the clipping process and scale-up the Similarity Score process.\n3. The downstream processes are automatically triggered after a process is completed for a video. For example, after clipping is complete for video with ID 'A', the similarity score computation is started for that video automatically.\n4. The activation of a downstream task optionally depends on a condition. For example, The motion detection for a clip is triggered only if the clip does not have a text, which is a result of the OCR detection process.\nTo achieve the pipeline, we use the RabbitMQ7 as a Task Orchestrator (Figure 11). Each process is a deployment with it's own resource specification, subscribed to a Task Queue. To trigger the pipeline, we start by pushing the video IDs to the initial queue. This is the only manual step required. Once this is done, the clipping process populates the Similarity Score Queue with the video ID. The Similarity Score deployement, subscribed to the corresponding queue, takes up the task, completes it, and pushes only the de-duplicated clip IDs to the Aesthetic Score queue. The Aesthetic score process, after computation, enqueues the OCR detection Queue with only the IDs of only those clips that meet the threshold. In this fashion, the number of clips that are being processed keeps reducing with each step in the pipeline by skipping the computation for clips that do not meet the passing criteria in the previous steps in the pipeline. In addition to the speed gain by skipping computation for failed clips, we also achieve the speed gain due to pipelining. The time taken for each step to process 1000 videos is given in Tab. 5. Equation 5 shows that the pipelined system is 1.5 times faster than the equivalent sequential system. There is scope to further improve this by speeding up the bottleneck process, as the processing time of the pipelined system is dependent on the time taken"}]}