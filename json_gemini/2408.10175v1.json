{"title": "Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition", "authors": ["Rafael M. Mamede", "Pedro C. Neto", "Ana F. Sequeira"], "abstract": "This study investigates the effects of occlusions on the fair-ness of face recognition systems, particularly focusing on demographic biases. Using the Racial Faces in the Wild (RFW) dataset and syn-thetically added realistic occlusions, we evaluate their effect on the per-formance of face recognition models trained on the BUPT-Balanced and BUPT-GlobalFace datasets. We note increases in the dispersion of FMR, FNMR, and accuracy alongside decreases in fairness according to Equi-lized Odds, Demographic Parity, STD of Accuracy, and Fairness Dis-crepancy Rate. Additionally, we utilize a pixel attribution method to understand the importance of occlusions in model predictions, propos-ing a new metric, Face Occlusion Impact Ratio (FOIR), that quantifies the extent to which occlusions affect model performance across differ-ent demographic groups. Our results indicate that occlusions exacerbate existing demographic biases, with models placing higher importance on occlusions in an unequal fashion, particularly affecting African individ-uals more severely.", "sections": [{"title": "1 Introduction", "content": "The ever-growing interest in machine learning-based biometric applications has raised several questions regarding the safety, trustworthiness, and potentially biased behaviour of models [2,21]. Researchers, who initially focused on achieving great levels of recognition performance, are now investigating explainability and bias problems with great detail and interest [14, 16, 26]. This research direction has been propelled by unfortunate events regarding misclassification in criminal trials [8] and the inability to inform the user regarding the \"why\" behind the wrong prediction of the model. Even in more frequent scenarios, border control face recognition is limited by the lack of information on the model's reasoning. Additionally, the European Union (EU) General Data Protection Regulation (GDPR) [4] states that users have the right to an explanation."}, {"title": "2 Related Work", "content": "The issue of bias in FR models has garnered significant attention in recent years, with numerous studies investigating the extent and implications of these bi-ases [3, 21]. Grother et. al [7] reported in a comprehensive analysis of demo-graphic differentials in FR that these algorithms are less accurate for women, older adults, and individuals of African and Asian descent compared to men and individuals of European descent. Terh\u00f6rst et al. [23] expanded the comparison of the influence of demographics on FR, exposing the effects of other soft biometric characteristics (such as face shape and facial hair) on the verification perfor-mance of FR systems. Raji and Buolamwini [20] have also identified systemic biases present in commercial face recognition technologies, emphasizing the need for transparency and accountability in the deployment of these systems.\nRegarding ethnicity or race bias, current research highlights the different sources of biases (both from the model and the data) that can affect training [26]. The manifestation of racial bias in a face verification setting occurs when the prediction on a reference-probe pair of ethnicity A is less likely to be wrong when compared to a reference-probe pair of a different ethnicity. In addition, the majority of works on ethnicity bias for face recognition have focused their attention on a clear evaluation setting [2, 16, 26].\nRecent lines of research on Fairness in FR focus not only on the detection and prevention of these biases, but also on understanding how they arise. Fu and Damer [5] reported differences in pixel attribution on key facial landmarks across demographics, hinting at different model behaviour across ethnicities. Huber et al. [11] also verified differences in the mean importance map across different"}, {"title": "3 Methods", "content": null}, {"title": "3.1 Experimental Design", "content": "This study focuses on the assessment of Fairness of FR models trained and developed in clear settings, i.e. without occlusions, when faced with realistic and commonly occurring real-word occlusions. We perform our evaluation on the Racial Faces in the Wild (RFW) test dataset [25], containing images divided across 4 ethnicities: African, Asian, Caucasian, and Indian. We consider the proposed 6000 pairs of images for each ethnicity, 3000 of each being genuine pairs and 3000 impostor pairs. Our experimental approach can be described by the following steps:\n1. Generate realistic synthetic occlusions on facial images of the RFW\ndataset - We start by creating the synthetic occlusions to be used through-\nout our benchmarks by employing two occlusion protocols, proposed in the"}, {"title": "3.2 Models", "content": "In this work, we consider models using two distinct architectures: ResNet34 [9] and ResNet50 [9]. For each different architecture, we train the model versions using two datasets, BUPT-Balanced and BUPT-GlobalFace, proposed by Wang et al. [26], and intended to create a framework to study the biases of face recog-nition models. Each identity on the dataset has been labeled according to its skin tone into one of the following ethnicities: African, Asian, Caucasian, and Indian. BUPT-Balanced balances the number of identities that belong to each of these four categories and is composed of 1.3 million images with 28k identities, 7k identities per ethnicity. On the other hand, BUPT-Globalface contains two million images from 38k identities, and the ethnicity distribution of the identities follows the same distribution seen in the world's population.\nFor the training of all models, we used the ElasticArcFace loss function (with the hyperparameters m = 0.5, \u03c3 = 64), a training batch size of 256, weight decay set to 5e-4, optimization with momentum (\u03b2 = 0.9) over 26 epochs with a learning rate of 0.1 (updated by reducing an order of magnitude on epochs 8, 14, 20, and 25). We perform data augmentation on the training set with horizontal image flips."}, {"title": "3.3 Group Fairness Metrics", "content": "Effectively quantifying the inequalities of model adequacy in each demographic in question requires the use of well-defined fairness metrics. In this subsection, we will cover each of the metrics used in this study alongside some theoretical foundations.\nThe usage of the standard deviation of the verification accuracies of each protected group has been used in the field of FR as a metric for detecting disparities in quality of outcome [16] [26]. A fairer verification procedure minimizes the STD.\nSER is a fairness metric introduced by Wang et al. [24] in the context of FR. Considering the set of protected groups {91, 92, ..., gn}, the SER is defined as:\n$SER_{@th} = \\frac{max_{g_i} Error_{@th}(g_i)}{min_{g_i} Error_{@th}(g_i)} = \\frac{max_{g_i} (100 - Acc_{@th}(g_i))}{min_{g_i} (100 \u2013 Acc_{@th}(g_i))}$\nThe SER metric takes values from 1 to +\u221e, with lower values corresponding to lower discrepancy between the predictive performance of the groups, hence a fairer prediction.\nFDR, proposed by Pereira et al. [19], is a fairness evaluation metric applied in the field of FR. This met-ric combines the influence of both discrepancies in the False Match Rate (FMR)"}, {"title": "4 Experimental Results", "content": null}, {"title": "4.1 Assessing Model Performance and Fairness", "content": "In this section, we present and discuss the experimental results obtained regard-ing the performance and fairness of each considered model on the unoccluded and occluded scenarios. From analysing table 1, which summarizes our experimental fairness assessment, we can identify the following:\nEO and DP show a large increase on both occluded scenarios across all tested models. In general, we also see that this increase tends to be larger for the more occluded RFW0-RFW4 scenario. An increase in DP difference indicates different rates of positive predictions across our demographics; however, more concerning is the increase in EO, which suggests an increase of bias in the reliability of the predictions."}, {"title": "4.2 Fairness Increase of Ratio-based Metrics on the Occluded Scenarios", "content": "Initially, from interpreting the experimental results on table 1, the decrease of SER, IR, and GARBE can seem conflicting with the remaining results, as it"}, {"title": "4.3 Pixel Attribution in the Occluded Scenarios", "content": "After verifying an increase in the dispersion of FNMR and FMR across ethnici-ties, alongside an increase in fairness indicators such as STD, EO, DP, and FDR, we seek to understand if there are significant differences in the contribution of pixels on the occlusions across ethnicities. For this we designed a novel metric, FOIR, based on information extracted from explainability tools. For each pair in the occluded cases, we used the xSSAB method to extract saliency maps on the occluded image. Important pixels were considered those with contributions of at least 60% of the attribution on the most important pixel in the direction of the decision, that is we consider only negative contributions for negative predictions and positive contributions for positive predictions. These important pixels rep-resent the areas of the image that more strongly contribute to a given model's prediction. We then calculate the overlap between important pixels and added occlusions to determine the percentage of important pixels that fall onto occluded areas (FOIR). By performing one-way analysis of variance (ANOVA) [22] we seek to verify statistically significant differences in the averages of this quantity across ethnicities.\nThe results, on table 3, indicate that in FNM cases, when two images of the same identity are presented, the contributions of the mask to the erroneous decision vary significantly across ethnicities. African faces show a higher degree of importance of occluded pixels in all but one tested setting, meaning that genuine pairs of this particular ethnicity are consistently more affected by occlusions."}, {"title": "5 Conclusion", "content": "Our investigation into the effects of occlusions on the fairness of face recogni-tion systems has revealed significant disparities in performance across different demographic groups. The experimental results demonstrate that occlusions lead to a pronounced increase in error, with the impact being unequally distributed across ethnicities. These findings are supported by the values of the global fair-ness metrics STD, EO, DP, and FDR, in both occluded scenarios. Metrics such as SER, IR, and GARBE indicate fairer decisions in occluded scenarios. However, we verified that these results are mainly due to the high increase in overall error in the occluded scenarios since the dispersion of error also increases, indicating unfairer decisions.\nThrough the use of pixel attribution methods, we discovered that occlusions disproportionately affect certain ethnicities by contributing more heavily to er-roneous decisions. Our analysis, utilizing the novel metric Face Occlusion Impact"}]}