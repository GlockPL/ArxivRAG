{"title": "Fairness Under Cover: Evaluating the Impact of Occlusions on Demographic Bias in Facial Recognition", "authors": ["Rafael M. Mamede", "Pedro C. Neto", "Ana F. Sequeira"], "abstract": "This study investigates the effects of occlusions on the fairness of face recognition systems, particularly focusing on demographic biases. Using the Racial Faces in the Wild (RFW) dataset and synthetically added realistic occlusions, we evaluate their effect on the performance of face recognition models trained on the BUPT-Balanced and BUPT-GlobalFace datasets. We note increases in the dispersion of FMR, FNMR, and accuracy alongside decreases in fairness according to Equalized Odds, Demographic Parity, STD of Accuracy, and Fairness Discrepancy Rate. Additionally, we utilize a pixel attribution method to understand the importance of occlusions in model predictions, proposing a new metric, Face Occlusion Impact Ratio (FOIR), that quantifies the extent to which occlusions affect model performance across different demographic groups. Our results indicate that occlusions exacerbate existing demographic biases, with models placing higher importance on occlusions in an unequal fashion, particularly affecting African individuals more severely.", "sections": [{"title": "1 Introduction", "content": "The ever-growing interest in machine learning-based biometric applications has raised several questions regarding the safety, trustworthiness, and potentially biased behaviour of models [2,21]. Researchers, who initially focused on achieving great levels of recognition performance, are now investigating explainability and bias problems with great detail and interest [14, 16, 26]. This research direction has been propelled by unfortunate events regarding misclassification in criminal trials [8] and the inability to inform the user regarding the \"why\" behind the wrong prediction of the model. Even in more frequent scenarios, border control face recognition is limited by the lack of information on the model's reasoning. Additionally, the European Union (EU) General Data Protection Regulation (GDPR) [4] states that users have the right to an explanation."}, {"title": "2 Related Work", "content": "The issue of bias in FR models has garnered significant attention in recent years, with numerous studies investigating the extent and implications of these biases [3, 21]. Grother et. al [7] reported in a comprehensive analysis of demographic differentials in FR that these algorithms are less accurate for women, older adults, and individuals of African and Asian descent compared to men and individuals of European descent. Terh\u00f6rst et al. [23] expanded the comparison of the influence of demographics on FR, exposing the effects of other soft biometric characteristics (such as face shape and facial hair) on the verification performance of FR systems. Raji and Buolamwini [20] have also identified systemic biases present in commercial face recognition technologies, emphasizing the need for transparency and accountability in the deployment of these systems.\nRegarding ethnicity or race bias, current research highlights the different sources of biases (both from the model and the data) that can affect training [26]. The manifestation of racial bias in a face verification setting occurs when the prediction on a reference-probe pair of ethnicity A is less likely to be wrong when compared to a reference-probe pair of a different ethnicity. In addition, the majority of works on ethnicity bias for face recognition have focused their attention on a clear evaluation setting [2, 16, 26].\nRecent lines of research on Fairness in FR focus not only on the detection and prevention of these biases, but also on understanding how they arise. Fu and Damer [5] reported differences in pixel attribution on key facial landmarks across demographics, hinting at different model behaviour across ethnicities. Huber et al. [11] also verified differences in the mean importance map across different"}, {"title": "3 Methods", "content": ""}, {"title": "3.1\nExperimental Design", "content": "This study focuses on the assessment of Fairness of FR models trained and developed in clear settings, i.e. without occlusions, when faced with realistic and commonly occurring real-word occlusions. We perform our evaluation on the Racial Faces in the Wild (RFW) test dataset [25], containing images divided across 4 ethnicities: African, Asian, Caucasian, and Indian. We consider the proposed 6000 pairs of images for each ethnicity, 3000 of each being genuine pairs and 3000 impostor pairs. Our experimental approach can be described by the following steps:\n1. Generate realistic synthetic occlusions on facial images of the RFW\ndataset - We start by creating the synthetic occlusions to be used throughout our benchmarks by employing two occlusion protocols, proposed in the"}, {"title": "2022 Competition on Occluded Face Recognition [15] on the RFW dataset (see figure 2). The selected protocols, 1 and 4, use affine transformations of occlusion images (e.g. carnival masks, sunglasses, masks, etc.), warping them to fit the corresponding landmarks associated (left eye, right eye, nose, left mouth corner, and right mouth corner) identified in the RFW dataset using MTCNN [28]. The two protocols differ in the occlusions added, with protocol 1 adding a single occlusion (on either the upper face, lower face, eyes, or top of the head), and protocol 4 adding either a single occlusion as protocol one or two occlusions (either a combination of lower face and eye/top of head occlusions, or top of head and upper face/eye occlusions). The choice of these protocols reflects two degrees of severity of occlusions, with protocol 4, on average, imposing more obstructions to key facial landmarks. Throughout this document, we will refer to the occluded images using protocol 1 and protocol 4 of the competition as RFW1 and RFW4, respectively. We will also refer to the unoccluded case as RFW0, aligning with the terminology for protocol 0 used in the aforementioned competition.", "content": "2. Evaluate model performance and fairness in non-occluded and oc-\ncluded scenarios - Initially, we assess the baseline performance of the FR\nmodels (using unoccluded-unoccluded pairs of images) by measuring accu-\nracy, False Match Rate (FMR), and False Non-Match Rate (FNMR), at a\nthreshold optimized to maximize the difference between accuracy and stan-\ndard deviation of accuracies along ethnicities (enforcing a penalization on\nverification thresholds with high differences of performance across ethnici-\nties). We also evaluate a suite of Fairness metrics, described in section 3.3, to\ndetermine initial disparities in the baseline case. We then utilize the synthet-\nically occluded images to perform an assessment using unoccluded-occluded\npairs of images, utilizing both protocols of occlusions. By comparing the\nmetrics from the non-occluded and occluded scenarios, we aim to identify\nany changes in model performance and fairness caused by the occlusions. We\nalso seek to understand what demographics are particularly affected by the\nocclusions.\n3. Utilize explanation methods to infer model behavior differences\non the verification across demographics - When dealing with verifi-\ncation in the unoccluded-occluded pair case, the model decision should be\nsupported by the areas not affected by the occlusion since the occlusion is\nseparate from the identity being observed. As such, to infer the model be-\nhavior leading to verification errors, we used a state-of-the-art efficient pixel\nattribution method for verification scenarios, xSSAB [13], to obtain the im-\nportant regions for verification across our demographics. We then compute\nthe Face Occlusion Impact Ratio (FOIR), that is, the overlap between the\nimportant pixels and the added occlusions to calculate the percentage of\nimportant pixels (IP) that fall onto occluded areas (O), FOIR = $\\frac{IP \\cap O}{IP}$.\nWe look for statistical differences between the distributions of this quantity\nacross demographics in the erroneous cases (False Matches, FM, being pairs\nof images not belonging to the same identify that are classified by the model\nas such; and False Non-Matches, FNM, which are pairs of images belonging"}, {"title": "3.2 Models", "content": "In this work, we consider models using two distinct architectures: ResNet34 [9] and ResNet50 [9]. For each different architecture, we train the model versions using two datasets, BUPT-Balanced and BUPT-GlobalFace, proposed by Wang et al. [26], and intended to create a framework to study the biases of face recognition models. Each identity on the dataset has been labeled according to its skin tone into one of the following ethnicities: African, Asian, Caucasian, and Indian. BUPT-Balanced balances the number of identities that belong to each of these four categories and is composed of 1.3 million images with 28k identities, 7k identities per ethnicity. On the other hand, BUPT-Globalface contains two million images from 38k identities, and the ethnicity distribution of the identities follows the same distribution seen in the world's population.\nFor the training of all models, we used the ElasticArcFace loss function (with the hyperparameters m = 0.5, $\\sigma$ = 64), a training batch size of 256, weight decay set to 5e-4, optimization with momentum ($\\beta$ = 0.9) over 26 epochs with a learning rate of 0.1 (updated by reducing an order of magnitude on epochs 8, 14, 20, and 25). We perform data augmentation on the training set with horizontal image flips."}, {"title": "3.3 Group Fairness Metrics", "content": "Effectively quantifying the inequalities of model adequacy in each demographic in question requires the use of well-defined fairness metrics. In this subsection, we will cover each of the metrics used in this study alongside some theoretical foundations.\nSTD of Accuracy (STD) - The usage of the standard deviation of the verification accuracies of each protected group has been used in the field of FR as a metric for detecting disparities in quality of outcome [16] [26]. A fairer verification procedure minimizes the STD.\nSkewed Error Ratio (SER) SER is a fairness metric introduced by Wang et al. [24] in the context of FR. Considering the set of protected groups {$g_1$, $g_2$, ..., $g_n$}, the SER is defined as:\nSER@th = $\\frac{max_{g_i} Error@th(g_i)}{min_{g_i} Error@th(g_i)}$ = $\\frac{max_{g_i} (100 - Acc@th(g_i))}{min_{g_i} (100 \u2013 Acc@th(g_i))}$\nThe SER metric takes values from 1 to +\u221e, with lower values corresponding to lower discrepancy between the predictive performance of the groups, hence a fairer prediction.\nFairness Discrepancy Rate (FDR) - FDR, proposed by Pereira et al. [19], is a fairness evaluation metric applied in the field of FR. This metric combines the influence of both discrepancies in the False Match Rate (FMR)"}, {"title": "and the False Non-Match Rate (FNMR) in the protected groups. For our n protected groups, G = {$g_1$, $g_2$, ..., $g_n$}, FDR is given by:", "content": "FDR@th = 1- (A@th + (1-$\\alpha$)Bath),\nwith the auxiliary ranges A@th and Bath defined as:\nA@th = $max_{g_i,g_j \\in G}$|(FMRg,@th \u2013 FMRg;@th|)\nBath = $max_{g_i,g_j \\in G}$|(FNMRg,@th \u2013 FNMRg; @th|)\nHere, $\\alpha$ is a weight factor that allows for attributing the desired relative importance to the contributions for the disparity of each error rate. The values of FDR range from 0, most unfair, to 1, most fair.\nInequity Rate (IR) The IR is a demographic fairness metric proposed by NISP [6], in the context of FR, as an alternative to the FDR that leverages ratios between error rates rather than differences. For our n protected groups, G = {$g_1$, $g_2$, ..., $g_n$}, IR is given by:\nIRath = $\\frac{A@th}{B@th}$\nwith the auxiliary ranges A@th and Bath defined as:\nA@th = $\\frac{max_{g_i \\in G}FMRg;@th}{min_{g_i \\in G}FMRg; @th}$\nB@th = $\\frac{max_{gi \\in G}FNMRg; @th}{min_{gi \\in G}FNMRg; @th}$\nHere, $\\alpha$ is a weight factor that allows for attributing the desired relative importance to the contributions for the disparity of each error rate. The values of FDR range from 1, most fair, to +\u221e, most unfair.\nGini Aggregation Rate for Biometric Equitability (GARBE) - GARBE is a group fairness metric proposed for biometric applications [10], based on an upper bound normalized variant of the Gini coefficient. Given the formulation of the Gini coefficient, for n observations of a discrete variable x:\n$G_x = \\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{n}|x_i - x_j|}{\\frac{n}{n-1}2n^2\\bar x}$", "subsections": [{"title": "4.1 Assessing Model Performance and Fairness", "content": "The GARBE metric follows as a weighted average of the gini coefficients of the samples of FMR and FNMR across each protected group:\nGARBE@th = $\\alpha$GFMR@th + (1 \u2212 $\\alpha$)GFNMR@th\nThe values of GARBE range from 0 to 1, with lower values corresponding to fairer results.", "subsections": [{"title": "4.2 Fairness Increase of Ratio-based Metrics on the Occluded\nScenarios", "content": "In summary, while certain metrics like SER, IR, and GARBE indicate an increase in fairness, this is primarily driven by the overall increase in error rates. We note in all cases, the dispersion of the error or error rates increases as we add occlusions, suggesting unfairer results under occlusions. Future work should focus on refining these metrics and methodologies to ensure that they remain consistently applicable in scenarios where model errors are orders of magnitude apart, guaranteeing that improvements in fairness are genuine and not merely artifacts of increased error rates."}, {"title": "4.3 Pixel Attribution in the Occluded Scenarios", "content": "After verifying an increase in the dispersion of FNMR and FMR across ethnicities, alongside an increase in fairness indicators such as STD, EO, DP, and FDR, we seek to understand if there are significant differences in the contribution of pixels on the occlusions across ethnicities. For this we designed a novel metric, FOIR, based on information extracted from explainability tools. For each pair in the occluded cases, we used the xSSAB method to extract saliency maps on the occluded image. Important pixels were considered those with contributions of at least 60% of the attribution on the most important pixel in the direction of the decision, that is we consider only negative contributions for negative predictions and positive contributions for positive predictions. These important pixels represent the areas of the image that more strongly contribute to a given model's prediction. We then calculate the overlap between important pixels and added occlusions to determine the percentage of important pixels that fall onto occluded areas (FOIR). By performing one-way analysis of variance (ANOVA) [22] we seek to verify statistically significant differences in the averages of this quantity across ethnicities.\nAfter analyzing the various fairness metrics, we arrived at the"}]}]}, {"title": "Demographic Parity (DP)", "content": "DP [1] is a fairness property that a binary model, f, is said to satisfy if its selection rate is independent of membership in the sensitive groups, P(f(X) = 1|X \u2208 $g_i$) = P(f(X) = 1),\u2200$g_i$ \u2208 G. In practice, to measure whether membership in the sensitive groups affects the positive outcome of the model, we can measure the Demographic Parity Difference as:\nDP@th = $max_{g_i,g_j\\in G}$ |E(f(X)|X \u2208 $g_i$) \u2013 E(f(X)|X \u2208 $g_j$)|", "subsections": [{"title": "to fairer results.", "content": "The values of DP Difference range from 0 to 1, with lower values corresponding\nEqualised Odds (EO) - EO [1] is a fairness property that a binary model\nis said to satisfy if it performs equally well for each sensitive group. This can\nbe seen as a more restricted version of DP that enforces the same True Positive\nRate (TPR) and False Positive Rate (FPR) across all classes, P(f(X) = 1|y =\nv, X \u2208 $g_i$) = P(f(X) = 1|y = v), \u2200$g_i$ \u2208 G, v \u2208 {0,1} . Similar to DP, in practice,\nwe can use the difference between TPR and FPR across all groups to obtain\na proxy quantity for how well this property is satisfied. As such, the Equalised\nOdds Difference is defined as:\nEO@th = max(\u2206TPR@th, \u2206FPR@th),\nwith ATPR and AFPR defined as:\nATPR@th = $max_{g_i,g_j \\in G}$|TPR@thgi - TPR@thgj|\nAFPR@th = $max_{g_i,g_j\\in G}$|FPR@thgi - FPR@thgj|\nThe values of EO Difference range from 0 to 1, with lower values correspond-ing to fairer results."}]}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Assessing Model Performance and Fairness", "content": "In this section, we present and discuss the experimental results obtained regard-ing the performance and fairness of each considered model on the unoccluded and occluded scenarios. From analysing table 1, which summarizes our experimental fairness assessment, we can identify the following:\nEO and DP show a large increase on both occluded scenarios across all tested models. In general, we also see that this increase tends to be larger for the more occluded RFW0-RFW4 scenario. An increase in DP difference indicates different rates of positive predictions across our demographics; however, more concerning is the increase in EO, which suggests an increase of bias in the reliability of the predictions."}, {"title": "5 Conclusion", "content": "Our investigation into the effects of occlusions on the fairness of face recognition systems has revealed significant disparities in performance across different demographic groups. The experimental results demonstrate that occlusions lead to a pronounced increase in error, with the impact being unequally distributed across ethnicities. These findings are supported by the values of the global fairness metrics STD, EO, DP, and FDR, in both occluded scenarios. Metrics such as SER, IR, and GARBE indicate fairer decisions in occluded scenarios. However, we verified that these results are mainly due to the high increase in overall error in the occluded scenarios since the dispersion of error also increases, indicating unfairer decisions.\nThrough the use of pixel attribution methods, we discovered that occlusions disproportionately affect certain ethnicities by contributing more heavily to erroneous decisions. Our analysis, utilizing the novel metric Face Occlusion Impact"}, {"title": "Ratio (FOIR), revealed that important pixels, which are critical to the model's decision-making process, often overlap significantly with the occluded regions when dealing with genuine pairs that are misclassified by the model. This overlap was particularly pronounced in African faces compared to other ethnicities, indicating that occlusions play a larger role in the model's incorrect decisions for this group. The Face Occlusion Impact Ratio (FOIR) metric provides a vital tool for quantifying these biases, offering a clear and measurable way to assess how different demographic groups are affected by occlusions. By incorporating FOIR into the evaluation process, we can gain deeper insights into the specific ways occlusions impact model performance and fairness.\nThis study highlights the need for verifying fairness not only in clean scenarios but also under commonly occurring natural occlusions. Addressing this issue is crucial for developing face recognition systems that are robust and fair across diverse demographic groups. Future research should focus on enhancing training protocols, designing more robust model architectures, and implementing comprehensive fairness evaluations that account for real-world conditions, including occlusions. By ensuring that models perform fairly under occluded conditions, we can build trust in these technologies and ensure they benefit all users equitably."}]}