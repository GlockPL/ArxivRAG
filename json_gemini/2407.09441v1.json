{"title": "The \u00b5G Language for Programming Graph Neural Networks", "authors": ["MATTEO BELENCHIA", "FLAVIO CORRADINI", "MICHELA QUADRINI", "MICHELE LORETI"], "abstract": "Graph neural networks form a class of deep learning architectures specifically designed to work with graph-structured data. As such, they share the inherent limitations and problems of deep learning, especially regarding the issues of explainability and trustworthiness. We propose \u00b5G, an original domain-specific language for the specification of graph neural networks that aims to overcome these issues. The language's syntax is introduced, and its meaning is rigorously defined by a denotational semantics. An equivalent characterization in the form of an operational semantics is also provided and, together with a type system, is used to prove the type soundness of \u00b5G. We show how \u00b5G programs can be represented in a more user-friendly graphical visualization, and provide examples of its generality by showing how it can be used to define some of the most popular graph neural network models, or to develop any custom graph processing application.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning models are at the forefront of artificial intelligence research today. Among them, artificial neural networks are the most commonly used class of models for a wide range of different tasks, including natural language processing, computer vision, software engineering, and many more [29]. For applications where the inputs can be represented as graphs, the graph neural network [10] (GNN) model has been the architecture of choice, and has achieved state-of-the-art performance on many tasks.\nDespite these promising advancements, deep learning models, including graph neural networks, face a number of issues. These systems are difficult to engineer with, and are notoriously harder to debug and interpret compared to traditional software systems [23]. Another issue is the lack of guarantees that these systems offer regarding their outputs, which time and again have been shown to be easily foolable, not only using so-called \"adversarial examples\" [17], but also more generally in unpredictable and surprising ways [3, 12]. Furthermore, these systems act like a black-box and are opaque, in the sense that human users are unable to understand how they have reached their conclusions [4].\nFinally, there is a very strong bias in deep learning research against the usage of prior knowledge even when such usage is warranted, and even then, it is difficult to figure out how to integrate such knowledge in these systems [24].\nIn this paper, we tackle all these issues by proposing a new graph neural network specification language, called \u00b5G (pronounced as \u201cmee-gee\"). A graph neural network in \u00b5G is built up as a composition of simpler GNNs following the formal rules of the language. These GNNs are built up from the base terms, specifying functions to be applied to the single nodes in terms of themselves and/or their neighbors, which can then be composed sequentially, in parallel, selected according to a Boolean condition, or iterated.\nThe language's syntax is introduced using a context-free grammar, and the meaning of each term is formally and rigorously defined by a denotational semantics. Later on, we also introduce a structural operational semantics, and prove that both semantics are equivalent, i.e., they define the same graph neural network operations. Using the operational semantics and after having defined a type system for \u00b5G, we prove the type soundness of the language. The type soundness of \u00b5G guarantees that every well-typed \u00b5G program properly defines a GNN with the correct output type.\nThe language can be used both in textual form and in graphical form. The graphical representation of \u00b5G programs is introduced as a more user-friendly approach to use the language, as it keeps the flow of information and the type of labels easier to follow and reason about.\nOur language is framework and hardware-agnostic, as \u00b5G could be implemented in different ways. We opted to implement \u00b5G in TensorFlow so that it can use its automatic differentiation capabilities and allow the GNNs we program to be executable on CPUs, GPUs, or TPUs without changing their implementation. Furthermore, these GNNs can interoperate with any other TensorFlow feature as if they were TensorFlow models themselves.\nWe claim that \u00b5G helps with the aforementioned problems of deep learning. For the matter of explainability and interpretability, using \u00b5G helps by making the computations performed more explicit, by taking the form of a \u00b5G expression which has clearly definite semantics and types. Indeed, the user might even define functions and computations using the same terminology of the specific domain in which the GNN is going to be used in, making the purpose of each term easier to understand. Some, or all, parts of the GNN can be defined to be inherently interpretable, by virtue of being defined explicitly based on the available domain knowledge. Furthermore, determining the nodes, edges, or labels that contributed to a specific prediction becomes amenable to static analysis techniques [7].\nAs for the issues of trustworthiness, the use of a formal language like \u00b5G allows the formal verification of GNNs similarly to that of other programming languages, e.g., by abstract interpretation, symbolic execution, data-flow analysis, and so on. In particular, we are interested in the problem of formally verifying safety properties of GNNs, e.g. output reachability properties [18]. The verification of an output reachability property for a function f given an input region X and an output region Y consists in checking whether for all inputs x \u2208 X, f(x) \u2208 Y. As far as we know, no solution for this problem has been proposed for graph neural networks specifically [33]. The verification of properties of this kind can be used to prove the robustness of a GNN against adversarial examples, which is critical when such models are deployed in safety-critical systems, where guarantees of correct functioning are paramount.\nFinally, it is easy to include prior knowledge when defining a GNN in \u00b5G. The basic terms of the language need not make use of neural network layers as is typically the case for the more popular graph neural network models, but can in general use any kind of function, not necessarily depending on trainable parameters. This way is also possible to define a GNN which does not require training at all, as we did in a previous work where we used \u00b5G for model checking [6]. Prior, or innate, knowledge can be freely mixed with optimizable function in order to build hybrid, neural-symbolic systems [22, 25, 32].\""}, {"title": "Contributions. Our main contributions are:", "content": "\u2022 We define the syntax \u00b5G language, and provide its denotational and operational semantics.\n\u2022 We prove the equivalence of the denotational and operational semantics of \u00b5G.\n\u2022 We define the typing rules of \u00b5G and prove its type soundness.\n\u2022 We show how \u00b5G programs can be represented graphically.\n\u2022 We demonstrate how to use \u00b5G to program some of the most popular graph neural network models."}, {"title": "Structure of the paper.", "content": "We survey the related work in Section 2, while we introduce the necessary notation and preliminary information in Section 3 and 4. Then the \u00b5G language is described in detail in Section 5 by showing its syntax and semantics. A proof of equivalence of the denotational and operational semantics of \u00b5G is in Section 6, followed by a proof of type soundness in Section 7. The graphical representation of \u00b5G programs is introduced in Section 8. A brief discussion on the implementation of \u00b5G is in Section 9. Finally, we evaluate \u00b5G by showing its application in CTL model checking and its usage to define some graph neural network models in Section 10, and delineate the future direction of our work in the conclusions."}, {"title": "2 RELATED WORK", "content": "Many domain specific languages (DSL) have been developed over the years to ease the development of machine learning applications [28]. Far from being a complete survey, we will discuss here only the most recent ones or those that are most relevant for our work.\nDeepDSL [37] is a language based on Scala that allows the definition of neural networks through variable assignments. Variables can be assigned computational modules such as convolutional layers, pooling layers, activation functions and so on. Neural networks are defined by variable assignments where the right-hand side is the function composition of other computational modules. The code is compiled to Java and uses JCuda to communicate with CUDA and cuDNN.\nAiDSL [13] is a language for the specification of supervised feed-forward neural networks using a model driven engineering approach. The code is compiled into Encog, a machine learning framework in Java. SEMKIS-DSL [19] is another language developed using a model driven engineering approach, where the main focus is shifted from the specification of the neural network architecture to the requirements that the input dataset and the neural network's outputs must satisfy. For the time being there are no compilation targets for the language, so it is not possible to automatically produce a neural network from specification. On the other hand, Diesel [11] acts at a finer level of detail, allowing the specification of common linear algebra and neural network computations that are typically implemented by libraries such as cuBLAS and cuDNN. It uses a polyhedral model to schedule operations and perform optimizations, and the source code is compiled directly into CUDA code. So far, none of these languages take into consideration the definition of graph-structured inputs or the typical operations that characterize graph neural networks.\nA DSL where graphs are first-class objects is OptiML [31], which supports the specification of machine learning models to be run on heterogenous hardware (i.e. the CPU or GPU). The language can be used to implement any machine learning model that can be expressed using the Statistical Query Model [20], and the code can be compiled to Scala, C++, or CUDA code. Graph objects in OptiML support operations expressed in terms of vertices and edges, and graph operations are specified in terms of vertices and their neighbours in the graph. Furthermore, OptiML has a fixed point operator with a customizable threshold value just like \u00b5G. Another DSL that supports graphs is StreamBrain [27], which is a language for the specification of Bayesian Confidence Propagation Neural Networks. This model takes in input a graph where every node is a random variable and edges represent the dependencies between variables. StreamBrain"}, {"title": "3 BACKGROUND AND NOTATION", "content": "Graphs and their labelings. A directed graph is a pair G = (V, E) where V is a collection of vertices (which we also refer to as nodes) and & is a collection of ordered pairs of vertices, called edges. For any u, v \u2208 V, whenever (u, v) \u2208 & we say that u is a predecessor of v and that v is a successor of u. We also say that (u, v) is an incoming edge for v and an outgoing edge for u. Moreover, let Ng(v) denote the set of predecessors of v, formally Ng(v) = {u | (u,v) \u2208 E}, while NG (u) denotes the set of successors of u, namely \u00d1g(u) = {v | (u, v) \u2208 E}. Similarly, let IG (v) denote the set of incoming edges for v, formally IG (v) = {(u, v) | u, v \u2208 V}, and let Og (u) denote the set of outgoing edges for u, formally Og(u) = {(u, v) | u, v \u2208 V}.\nSometimes it is useful to associate nodes and edges with values. Given a graph G = (V, 8), we can consider its node-labeling and edge-labeling. The former is a function \u03b7 : V \u2192 Ty associating each node v \u2208 V with a value in the set Ty. Similarly, an edge-labeling is a function \u03be : & \u2192 T\u025b that maps an edge to its label in the set T\u025b. Let V\u2286 V (resp. E C &), we let \u03b7 (V) (resp. \u00a7(E)) denote the multi-set of labels associated to the elements of V (resp. E) by \u03b7 (resp. \u00a7). Likewise, we let (\u03b7, \u03be) (E) denote the multi-set of tuples (n(u), \u03be((u, v)), \u03b7(v)) for each (u, v) \u2208 \u0395.\nEncoding of graphs and labels. One way to represent graphs and their labeling functions on a computer is in the form of a matrix of node features X, an adjacency matrix A and a matrix of edge features E. The node features matrix X stores the features associated with each node in the graph. The i-th row of the matrix contains a value xi \u2208 Ty that represents the information associated with the i-th vertex of the graph in a given ordering. The adjacency matrix A encodes the architecture of the graph, with each non-zero element aij \u2208 R denoting the presence of an edge from a node i to a node j. The edge features matrix E stores the features associated with each edge, like the node features matrix. The i-th row of E is a value ei \u2208 T\u025b that represents the information associated with the i-th edge in the row-major (or any other) ordering of the edges in A.\nGraph Neural Networks. A graph neural network is a deep learning model that operates on graph-structured data. Introduced by Scarselli et al. [30], it emerged to overcome the limitations of graph kernel methods [26]. GNNs generalize many other classes of deep learning architectures, as other deep learning models can be seen as a particular case of graph neural networks [8]. Convolutional Neural Networks (CNNs), for example, can be seen as a graph neural networks where inputs can only be 1-dimensional sequences (such as texts) or 2-dimensional grids (such as images), which are both particular instances of graphs. Graph neural networks, more generally, can learn on any non-Euclidean structure representable as graphs, which contrary to grids such as images, can have differing shapes, number of nodes and connectivity properties.\nGraph neural networks can be used for many tasks, which can be roughly categorized according to the subject of prediction: nodes, edges, or entire graphs. As with other machine learning models, there are two main kinds of tasks, namely classification and regression, where the goal is to predict the class or some numerical quantity associated with the nodes in the graph, edges, or the entire graph."}, {"title": "4 THE DOMAIN OF GRAPH NEURAL NETWORKS", "content": "In this section, we introduce the necessary definitions and theorems that characterize graph neural networks in our work. We elaborate further on the notion of node-labeling function and formalize the graph neural network as a transformation between node-labeling functions, then move on to prove that the set of graph neural networks is a chain complete partially ordered set."}, {"title": "4.1 Node-labeling functions", "content": "We denote the set of all labeling functions with co-domain T as H[T], and we say that this set specifies its type. As examples, labeling functions of type H[B] map vertices to Boolean values, while functions of type H[N] with k \u2208 N map vertices to k-tuples of natural numbers. Each type also includes a bottom node-labeling function \u22a5H = \u03bb\u03c5.undef that is undefined for every node. Then (H[T], \u2286H) is a partially ordered set, where \u22a5H is the bottom element and \u2286H is the pointwise ordering relation such that, for all v \u2208 V\n\u03b71 \u2286H \u03b72 \u21d4 \u03b71 (v) = t \u21d2 \u03b72 (v) = t\nWe also consider the parallel composition of node-labeling functions n\u2081 and \u014b2, and we denote it as 71 72, to specify the node-labeling function \u03bb\u03c5. (\u03b71 (\u03c5), \u03b72 (v)) that maps vertices to (possibly nested) pairs of node labels. The inverse operation is given by the projection functions \u03c0\u03b9, \u03c0\u03c1: H[T\u00d7T] \u2192 H[T] such that\n\u03c0L (\u03b71 \u29eb \u03b72) = \u03b71\n\u03c0R(\u03b71 \u29eb \u03b72) = \u03b72\nWe call \u2081 the left projection and TR the right projection. Adequate composition of these two projection functions can be used to obtain any nested node-labeling function."}, {"title": "4.2 Graph neural networks", "content": "Given a graph G together with its edge-labeling function \u00a7, we define a graph neural network to be a function \u03a6G,\u03be : H[T\u2081] \u2192 H[T2] that maps a node-labeling function to another node-labeling function. We denote the set of such graph neural networks as \u00deG,\u03be [T1, T2] or, more succinctly, as \u00deG,\u03be. The subscript G, \u00a7 indicates that each graph neural network is parametrized by a graph and an edge-labeling function. Likewise for the node-labeling functions, the set of graph neural networks \u03a6G,\u03be[T1, T2] is a partially ordered set with a point-wise ordering relation \u2286oG,\u03be[T1,T2] such that, for all \u03b7 \u2208 \u0397[T1]\n\u03c61 \u2286\u03a6G,\u03be \u03c62 \u21d4 \u03c61(\u03b7) = \u03b7' \u21d2 \u03c62(\u03b7) = \u03b7'\nWhen the types are clear from the context or not revelant, we drop the subscript and simply write \u2286oG.\u00b7\nNext, we define the underlying relation of a GNN as the set of input-output tuples which characterize the GNN. Building on this concept, we also specify their sequential and parallel composition.\nDefinition 4.1. Given a graph neural network \u00a2 : \u03a6G,\u03be[T1, T2] we define its underlying relation, denoted by rel($), as\nrel($) = {(\u03b71, \u03b72) \u2208 \u0397[T1] \u00d7 H[T2] | \u03a6(\u03b71) = \u03b72}\nThe sequential composition, or simply composition, of relations A : H[T\u2081] \u00d7 H[T2] and B : H[T2] \u00d7 H[T3], denoted by A * B, is defined as\nA * B = {(\u03b71, \u03b73) | \u2203\u03b72 \u2208 H[T2] : (\u03b71, \u03b72) \u2208 \u0391 \u2227 (\u03b72, \u03b73) \u2208 \u0392}\nThe parallel composition, or concatenation, of relations A: H[T1] \u00d7 H[T2] and B : H[T1] \u00d7 H[T3], denoted by A \u29eb B, is defined as\nA \u29eb B = {(\u03b71, (\u03b72, \u03b73)) | (\u03b71, \u03b72) \u2208 \u0391 \u2227 (\u03b71, \u03b73) \u2208 \u0392}\nThe following lemma defines the relationship between the relations and the ordering of GNNs.\nLEMMA 4.2. Given two GNNs $1, $2 we have\n\u03a61 \u2286\u03a6G,\u03be \u03a62 \u21d4 rel(\u03c61) \u2286 rel(\u03c62)\nPROOF. First we show that $1 \u2286\u03a6G,\u03be \u03a62 \u21d2 rel(\u03c61) \u2286 rel(\u03c62). Since $1(\u03b7) = \u03b7' \u21d2 \u03a62(\u03b7) = \u03b7', any (\u03b7, \u03b7\u0384) \u2208 rel(1) is also a member of rel($2)."}, {"title": "5 THE \u00b5G LANGUAGE FOR GRAPH NEURAL NETWORKS", "content": "In this section, we introduce the syntax of \u00b5G as a programming language for the definition of graph neural networks. After specifying its syntax (Definition 5.1), we show its denotational semantics (Definition 5.2), and structural operational semantics (Definition 5.8)."}, {"title": "Definition 5.1 (Syntax of \u00b5G).", "content": "Given a set S of function symbols, we define an algebra of graph neural networks with the following abstract syntax:\nN ::= \u03b9 | \u03c8 | <\u03c3 | \u25b7\u03c8 | N1; N2 | N\u2081||N2 | N1 \u2295 N2 | N*\nwith \u03c6, \u03c3, \u03c8 \u2208 S. The operator precedence rules given by * > ; > || > \u2295 and parentheses are introduced to the syntax to make the meaning of expressions unambiguous.\nGiven a graph G and an edge-labeling \u00a7, the meaning of a \u00b5G expression is a graph neural network, a function between node-labeling functions. The term i represents the application of the identity GNN that leaves the node labels unaltered. Another of the basic \u00b5G terms is the function application \u03c8. This represents the GNN that applies the function referenced by \u03c8. Moreover, the pre-image term <\" and the post-image term \u25b7 define a GNN that computes the labeling of a node in terms of the labels of its predecessors and successors, respectively. Two GNNs can be composed by sequential composition N\u2081; N2 and parallel composition N\u2081||N2. The choice operator N1 \u2295 N2 allows to run different GNNs according to the values of a node-labeling function. Finally, the star operator N* is used to program recursive behavior.\""}, {"title": "5.1 Denotational semantics", "content": "Having defined the syntax, we are now ready to introduce the denotational semantics of \u00b5G.\nDefinition 5.2. (Denotational semantics) Given a graph G and an edge-labeling function \u03be, we define the semantic interpretation function Sds [[\u00b7]] 9,5 : N \u2192 \u03a6G,\u0118 on \u00b5G formulas N by induction in the following way:\nSds [[\u03b9]] 9,5 = id\nSds [[\u03c8]]G\u203a\u00a7 (\u03b7) = \u03bb\u03c5.ff (\u03b7(V), \u03b7(v))\nSds [[<\u03c3]]G,\u00a7 (\u03b7) = \u03bb\u03c5.fo (f\u03c6 ((\u03b7, \u03be) (Ig (v))), \u03b7(\u03c5))\nSds[[\u25b7\u03c8]]G,\u00a7 (\u03b7) = \u03bb\u03c5.fo(f\u03c6 ((\u03b7, \u03be) (Og (v))), \u03b7(\u03c5))\nSds [[N1; N2]] 9,5 = Sds [[N2]] G, Sds [[N\u2081]] G\nSds [[N1||N2]] G = Sds [[N1]]G,\u00a7 \u00d7 Sds [[N2]]G,\nSds [[N1 \u2295 N2]]G,\u00a7 = cond(\u03c0L, Sds[[N1]]G,\u00a7 \u03bf \u03c0\u03c1, Sds [[N2]]G,\u03be \u03bf \u03c0\u03c1)\nSds [[N*]]G\u203a\u00a7 = FIX(\u03bb\u03c6.cond(\u03bb\u03b5.\u03bbv.Sds [[N]]G\u203a\u00a7(e)(v) \u2243\u20ac e(v), id, \u00a2 \u2022 Sds [[N]]G,\u00a7))\nfor any \u03c8, \u03c6, \u03c3\u03b5 S. The functions cond and FIX are defined as:\ncond(t, f1, f2) (\u03b7) =\n{ f1(n) if t(n) = \u03bb\u03c5.True\nf2(n) otherwise\nFIX(f) = {fn (1) | n \u2265 0}\nwhere f\u00ba = id and fn+1 = f \u00b0 fn. For any label type T and any real value \u0454 \u2208 R, we define a binary predicate = such that\nXe Y \u21d4 |x - y| \u2264 \u20ac\nAdditionally, we provide the semantics for a term of the form N1 \u29eb N2 which will be useful later in the proof of the equivalence with the structural operational semantics\nSds [[N1 \u2295 N2]] G5 = (Sds [[N\u2081]]G,\u00a7 \u03bf\u03c0\u2081) \u00d7 (Sds [[N2]]G,\u00a7 \u03bf \u03c0\u03c1)\nIn the following paragraphs, we clarify the meaning of \u00b5G expressions.\nIdentity application. The term i is evaluated as the identity graph neural network that returns the input node-labeling function as is.\nFunction application. A function symbol 41, 42, ... \u2208 S is evaluated as the graph neural network that maps a node-labeling to a new node-labeling by applying the corresponding function f1, f2,... on both local and global node information. The local information is the label of each individual node, while the global information is the multiset of the labels of all the nodes in the graph. The graph neural network we obtain applies a (possibly trainable) function f to these two pieces of information. Two particular cases arise if the function ignores either of the two inputs. If f ignores the global information, the GNN returns a node-labeling function \u03b7\u2081, a purely local transformation of the node labels. On the other hand, if f ignores the local information, the GNN returns a node-labeling function ng that assigns to each node a label that summarizes the entire graph, emulating what in the GNN literature is known as a global pooling operator [16].\nPre-image and Post-Image. The pre-image < and the post-image \u25b7, together with function symbols \u03c6, \u03c3\u03b5 S are evaluated as the graph neural networks Sds [[<\u03c3]]9,5 and Sds [[\u25b7\u03c8]]G,\u00a7. In the case of the pre-image, for any symbol \u03c6 \u2208 S the corresponding function f generates a message from tuples (\u03b7(u), \u03be((u, v)), \u03b7(v)) for each (u, v) \u2208 IG (v). Then for any symbol \u03c3\u2208 S the corresponding function g generates a new label for a node v from the multiset of incoming messages for v obtained from f and the current label \u03b7(v). The functions f and g may be trainable. The case of the post-image is analogous, with the difference that f is applied to tuples (\u03b7(v), \u03be((v, u)), \u03b7(u)) for each (v, u) \u2208 Og (v) instead.\nSequential composition. An expression of the form N\u2081; N2 is evaluated as the graph neural network resulting from the function composition of Sds [[N2]] G,\u00a7 and Sds [[N\u2081]] G,\u00a7.\nParallel composition. An expression of the form N\u2081||N2 is evaluated as the graph neural network that maps a node-labeling function to the node-labeling function obtained from the parallel composition of Sds [[N\u2081]]G, and Sds [[N2]] G\nChoice. The choice operator N1 \u2295 N2 applied to \u00b5G formulas N1, N2 is evaluated as the graph neural network Sds [[N\u2081]] 9, if the left projection of the input node-labeling \u03b7' = \u03c0\u03b9(\u03b7) is a node-labeling function such that Vu e G, \u03b7' (v) = True. Otherwise, it is evaluated as the graph neural network Sds [[N2]] 9,5. In any case, the selected GNN is given in input the right projection of the input node-labeling function.\nFixed points. The star operator N*, or the fixed point operator, applied to a \u00b5G formula N is evaluated as the graph neural network that that maps a node-labeling function \u03b7 to a new node-labeling function n' that is the fixed point of N computed starting by n. In other words, the sequence"}, {"title": "5.2 Operational semantics", "content": "For the structural operational semantics, we consider computations of the form\n(\u039d, \u03b7)\nwhere N is a \u00b5G expression and n is a node-labeling function. The transition rules are shown in Table 1. For any label type T and any real value \u0454 \u2208 R, the function f : (T \u00d7 T)* \u00d7 (Tx T) \u2192 B associated to \u2248 in Table 1 is defined as\nf= (X, x) = \u03c0\u03b9(x) =\u03b5 \u03c0\u03c1(x)\nFinally, we can define the semantic interpretation function on the structural operational semantics.\nDefinition 5.8 (Structural operational semantics). Given a graph G and an edge-labeling function \u00a7, we define the semantic interpretation function Ssos [[\u00b7]]G\u203a\u00a7 : N \u2192 \u03a6G, such that\nSsos [[N]]G, (\u03b7) : =\n{ \u03b7' if (\u039d, \u03b7) \u2192G\u03be\u03b7'\nundef otherwise\nThe rules PAR1 and PAR2 in Table 1 make the evaluation of \u00b5G expressions non-deterministic. The next theorem shows that no matter the order of application of the structural semantics rules, the obtained GNN is the same.\nTHEOREM 5.9 (CONFLUENCE). If (N, \u03b7) \u2192G,& \u3008N1, 71) and \u3008N, \u03b7\u3009 \u2192G, \u3008N2, N2), then either N\u2081 = N2 and n\u2081 = n2 or there exists a term N' and a labeling function n' such that (N1, \u03b71) \u2192 G,& \u3008N', n') and (N2, \u03b72) \u2192G,\u03be \u3008\u039d', \u03b7\u0384).\nPROOF. By induction on the structure of N. The only interesting case is that for expressions of the form N1 \u2295 N2, where rules PAR\u2081 and PAR2 can be applied in any order. Suppose that (N\u2081 \u2295 N2, \u03b7) \u2192G,& \u3008N\u2081 \u2295 N2, \u03b71|\u03c0\u03c1(\u03b7)) by rule PAR\u2081 and that (N\u2081 \u2295 N2, \u03b7) \u2192G\u03be \u3008N1 \u2295 \u039d2, \u03c0\u03b9(\u03b7) \u03b72\u3009 by rule PAR2. Then the required term is N' = N\u2081 \u2295 N\u2019 and the labeling function is n' = \u03b71|\u03b72, because \u3008N1 \u2295 \u039d\u0384, \u03c0\u03b9(\u03b7) \u03b72\u3009 \u2192G,& \u3008N1 \u2295 \u039d2, 71|72) by rule PAR1 and \u3008N1 \u2295 N2, \u03b71|\u03c0\u03c1(\u03b7)) \u2192*G,\u03be\u3008N1 \u2295 N2, \u03b71|\u03b72) by rule PAR2."}, {"title": "5.3 Language macros", "content": "We can enrich \u00b5G with a number of macros that simplifies the job of programming a graph neural network. In this section, we describe the means to define variables, functions, and shortcuts for the definition of if-then-else and while loop expressions. Some of these extensions require us to introduce a set of variable symbols X ::= X | Y | Z | ... to the language's syntax. We will also use function symbols PL and PR to denote left and right projections.\nVariable assignments. It is useful sometimes to assign an entire expression to a single label, so that whenever that label occurs in a program, the referred expression is substituted to it. For this purpose, we introduce variable assignments in the form of let expressions. A let expression has the form let X = N in N'. The intuitive meaning of such expression is that all occurrences of the variable symbol X in the expression N' are substituted with the expression"}, {"title": "6 EQUIVALENCE OF OPERATIONAL AND DENOTATIONAL SEMANTICS", "content": "Both the denotational semantics and operational semantics of \u00b5G define a mapping from node-labeling functions to node-labeling functions, i.e., a graph neural network. In the next theorem, we show that they define the same graph neural network.\nTHEOREM 6.1 (EQUIVALENCE OF DENOTATIONAL AND STRUCTURAL OPERATIONAL SEMANTICS). For any \u00b5G expression N, any graph G and any edge-labeling function \u03be, we have that\nSds [[N]] 9,5 = Ssos [[N]]G,\u00a7\nPROOF. Since both functions return GNNs, which are members of a partially ordered set \u00deG, with ordering relation \u2286\u03a6., it is sufficient to show that for any \u00b5G expression N:\n\u2022 Ssos [[N]]G,\u00a7 \u2286\u03a6G.& Sds [[N]]G,\u03be\n\u2022 Sds [[N]]G, \u2286\u03a6G, Ssos [[N]]G,\nLemma 6.2 and Lemma 6.3 prove that both these conditions are satisfied by the semantics of \u00b5G.\nLEMMA 6.2. For every expression N of \u00b5G, we have Ssos [[N]]G, \u2286og, Sds [[N]]G,5.\nPROOF. We shall prove that for any expression N and any node-labeling function \u03b7, \u03b7'\n(\u039d, \u03b7) \u2192G\u03be \u03b7' \u21d2 Sds [[N]]9,5 (\u03b7) = \u03b7' (2)\nIn order to do that, we will show that\n(\u039d,\u03b7) \u2192G,\u03be \u03b7' \u21d2 Sds[[N]]&&& (\u03b7) = \u03b7' (3)\n\u3008\u039d, \u03b7\u3009 \u2192\u03c2,\u03be \u3008\u039d', \u03b7') \u21d2 Sds [[N]]&&& (n) = Sds [[N' ]]9,5 (\u03b7') (4)\nIf Equation 3 and Equation 4 hold, the proof of Equation 2 is a straightforward induction on the length k of the derivation sequence (\u039d, \u03b7) \u2192k \u03b7'. The proof of Equation 3 and Equation 4 is by induction on the shape of the derivation trees.\nThe case ID: We have (\u03b9, \u03b7) \u2192 G, \u03b7, and since Sds [[1]]9,5 (\u03b7) = \u03b7, the result follows.\nThe case APPLY: We have \u3008\u03c8, \u03b7) \u2192 G,\u03be \u3008\u03b9, \u03bb\u03c5.ff([\u03b7(u) | u \u2208 V], \u03b7(v))), and since"}, {"title": "7 TYPE SOUNDNESS", "content": "We say that a \u00b5G term is well-typed whenever it can be typed with the rules of Table 2. These rules guarantee that any well-typed \u00b5G term defines a graph neural network. Next, we will be using these typing rules to prove the type soundness of the operational semantics of \u00b5G. Proving the type safety of \u00b5G ensures that the GNNs we program with it handle the data types correctly and produce results of the expected type. Concretely, this means that well-typed GNNs can always progress to the next step of computation and that after each step the type of the output is preserved."}, {"title": "8 A GRAPHICAL REPRESENTATION", "content": "In this section, we present a graphical representation for \u00b5G programs and expressions. Using such graphical notation makes it easier to understand and communicate the structure and purpose of a \u00b5G program, as the programmer might easily lose track of label types and sizes when developing in textual form. The basic terms of \u00b5G can be represented graphically as boxes, as shown in Figure 1, while the composite terms of the language are shown in Figure 2. Each term has exactly one input node-labeling and one output node-labeling, and the graphical representation is built top-down starting from the main (usually composite) term and substituting each box with either a basic term or a composite one; in this last case, this procedure is repeated recursively until all boxes in the figure are basic terms. As an illustrative example, let 41, 42, 43, \u03c6, \u03c3 be function symbols. Figure 3 shows how we can represent graphically the \u00b5G expression (41||42);43; <\u03c3.\nIn the graphical representation for \u00b5G expressions we have used some auxiliary functions which we describe in turn:\n\u2022 The function \u2022 : H[T\u2081] \u2192 H[T\u2081] \u00d7 H[T1], \u2022(\u03b7) = (\u03b7, \u03b7) maps a node-labeling function to a tuple containing two copies of itself.\n\u2022 The function: H[T\u2081] \u00d7 H[T2] \u2192 H[T\u2081 \u00d7 T2],\no (\u03b71, \u03b72) = {\n71 if 72 = \u03b7\nn2 if n\u2081 = \u03b7\n\u03b71\u29eb\u03b72 otherwise\nmaps two node-labeling functions to a new node-labeling function which labels each node with the concatenation of their labels. If one of the input labeling functions is the empty labeling, o simply returns the other function."}, {"title": "9 IMPLEMENTATION", "content": "The \u00b5G language has been implemented as a Python library called LIBMG, which we described in detail in a previous work [7]. The library was developed on top of Spektral [15], a graph neural network library based on Tensorflow [1]. The main advantage of using a framework such as TensorFlow is that \u00b5G programs are compiled to graph neural networks that can be executed seamlessly on CPUs, GPUs, or in a distributed setting without changing the source code.\nThe functionalities of LIBMG include the typical language support tools in the form of a parser, unparser and normalizer for \u00b5G expressions. The main functionality that is provided consists in the \u00b5G compiler that transforms a \u00b5G program into a TensorFlow Model instance. The compiler can be set up to automatically memoize sub-expressions that occur multiple times in the same program. The library also implements basic graph visualization functions, that allows to view in any web browser the input or output graphs, or intermediate representation produced by a \u00b5G model.\nFinally, a simple explanatory algorithm is provided that computes the sub-graph that was used by a \u00b5G model to compute the label of a given node.\\    Using LIBMG, the \u03c8, \u03c6, and o functions are defined by instantiating the corresponding classes and are stored in dictionaries. These dictionaries, which contain a mapping between function names and the functions themselves, are used to create a \u00b5G compiler instance. The \u00b5G compiler can then parse \u00b5G programs containing the function names defined in its dictionaries.\nThe models generated by the compiler can be trained by TensorFlow as usual. Only in the case of the star operator, the corresponding layer implements a custom gradient computation. In that case implicit differentiation is used as to make the computation of the gradient tractable."}, {"title": "10 EVALUATION", "content": "We evaluate \u00b5G both in terms of its expressiveness, meaning that it can be used to define most, if not all, of the graph neural network architectures that have been developed over the years, and also as a specification language for the development of graph neural networks for specific tasks. In Section 10.1 we describe an updated version of the CTL model checker developed using \u00b5G introduced in our previous work [6, 7]. Then, in Section 10.2, we show how to define some of the more well-known GNN architectures using our language."}, {"title": "10.1 Model Checking", "content": "We evaluated \u00b5G in a previous work by using it to define a GNN that performs CTL model checking on Kripke structures. For that use case, the set of functions that we used is shown in Table 3, while in Table 4 we show the translation function from CTL to the \u00b5G expression denoting the GNN that verifies it.\nThe \u00b5G model checker implementation was compared to two other explicit-state model checkers, pyModelChecking\u00b9 and mCRL2 [9]. We used some of the benchmarks from the Model Checking Contest 2022 [2], namely those for which we could explicitly generate the reachability graph on our machine, shown in Table 5. The \u00b5G model checker was set up to either verify all the formulas in parallel (the full setup), or one by one as would have been the case for the other model checkers (the split setup). In the former case, there would have been additional advantages due to the memoization of common sub-formulas across multiple formulas. The experimental results are shown in Figure 4, and we notice that for the larger Kripke structures there is an evident speed advantage of \u00b5G compared to the other model checkers (in some instances, by an order of magnitude)."}, {"title": "10.2 Specification of Graph Neural Networks", "content": "In this section, we show how to define some of the most popular graph neural network models in \u00b5G. As is the case for any programming language, there are many different ways to obtain the same result, therefore the implementations"}, {"title": "that we show here are only intended to highlight the generality of the language. For specific use cases, it is up to the programmer to choose the most useful \u00b5G implementation.", "content": "Graph Convolutional Networks. A Graph Convolutional Network [21] (GCN) performs a graph convolution by multiplying the node features with a matrix of weights, normalized according to the degree of the nodes and assigning greater importance to the features received from nodes with fewer neighbors. The new node labels $x'_i \\in \\mathbb{R}^m$ are computed from the current node labels $x_i \\in \\mathbb{R}^n$ according to the equation\n$$x'_i = f(\\sum_{j\\in N_G(i) \\cup \\{i\\}} \\frac{x_j \\Theta}{\\sqrt{deg(i)} \\sqrt{deg(j)}})$$\nwhere deg(i) is the degree of node i, $\\Theta \\in \\mathbb{R}^{n\\times m}$ is a matrix of trainable weights, and f is an activation function. The set of neighbors of a node include both the predecessors and the successors, as the GCN was originally thought for the case of undirected graphs. To implement the GCN, we make use of the functions shown in Table 6. We start by defining a \u00b5G expression to compute the degree of each node. We recall that the degree of a node is the number of its outgoing edges (the outdegree) plus the number of incoming edges (the indegree), with self loops counting as both. It is easy to compute this number in \u00b5G: we use <+ to compute the indegree and \u25b7\u03c8 to compute the outdegree, then we add them.\n$(\\langle\\sigma+||\\triangleright \\psi); \\phi_{+2}$"}, {"title": "Graph Attention Networks. A Graph Attention Network", "content": "[34] (GAT), like the GCN, transforms the node labels using a learnable weight matrix $\\Theta \\in \\mathbb{R}^{n\\times m}$. Then it employs a self-attention mechanism $a : \\mathbb{R}^{m} \\times \\mathbb{R}^{m} \\rightarrow \\mathbb{R}$, shared by all the nodes, that given the labels of node j and node i such that (j, i) \u2208 &, it indicates the importance of node j's labels to node i. This value is then normalized using the softmax function. In the original formulation, the attention mechanism is a single-layer feedforward neural network which uses a weight vector $a \\in \\mathbb{R}^{2m}$ and the LeakyReLU activation function. Therefore, the attention coefficients $a_{ji}$ are given by\n$$a_{ji} =  \\frac{exp \\left(LeakyReLU\\left(a^T  (x_i \\Theta, x_j \\Theta)\\right)\\right)}{\\sum_{k\\in N_G(i) \\cup \\{i\\}} exp \\left(LeakyReLU\\left(a^T  (x_i \\Theta, x_k \\Theta)\\right)\\right)}$$\nGiven these attention coefficients, the GAT layer computes the new node labels $x_i' \\in \\mathbb{R}^m$ from the current labels $x_i \\in \\mathbb{R}^n$ according to the equation\n$$x'_i = f(\\sum_{j\\in N_G(i) \\cup \\{i\\}} a_{ji} x_j\\Theta)$$\nwhere f is the activation function. The authors then extend this basic mechanism by employing multi-head attention, that is, they define K independent attention heads and weight matrices whose outputs are then concatenated. Thus, we get\n$$x'_i = {||}^K_{k=1} f(\\sum_{j\\in N_G(i) \\cup \\{i\\}} a_{ji}^k x_j\\Theta^k)$$"}, {"title": "Graph Isomorphism Networks. A Graph Isomorphism Network", "content": "[36] (GIN) generalizes the Weisfeiler-Leman test [35] and thus has the greatest discriminative power among GNNs for determining whether two graphs are not isomorphic. The GIN convolution consists in multiplying each node label by (1+e), where \u0454 \u2208 R is a trainable parameter, then adding the sum of the labels of the neighbor nodes. The obtained values are then passed in input to a multilayer perceptron (MLP), that is, it gets multiplied with a learnable weight matrix $\\Theta^1$ followed by the application of an activation function f1 two or more times in sequence. Formally, the GIN computes the new node labels $x'_i \\in \\mathbb{R}^m$ from the current labels $x_i \\in \\mathbb{R}^n$ as\n$$x_i' = MLP((1+\\epsilon) \\cdot x_i + \\sum_{j\\in N_G(i)} x_j)$$\nIn Table 8 we show the functions we need to implement the GIN layer in \u00b5G. We use parallel composition to compute the product of the node labels with 1 + \u20ac and the sum of the labels of the neighbors of each node, then we add them using +.\n$(\\psi_{\\* \\epsilon} || \\langle\\sigma_+)$"}, {"title": "The Original Graph Neural Network Model.", "content": "The graph neural network model was first introduced by Scarselli et al. [30]. The new node labels $x'_i \\in \\mathbb{R}^m$ of the original GNN model are computed as a two-step process. In the first step the new \u201cstate\u201d $s_i \\in \\mathbb{R}^k$ of each node is computed from the current node labels $x_i \\in \\mathbb{R}^n$, the edge labels $e_{ji} \\in \\mathbb{R}^l$, and the neighbor nodes\u2019 labels $x_j \\in \\mathbb{R}^n$ and states $s_j \\in \\mathbb{R}^k$ as the fixed point of the equation\n$$s_i = \\sum_{j \\in N_G(i)} h(x_i, e_{ji}, x_j, s_j)$$\nwhere h is typically a two-layer perceptron with weight matrices $\\Theta^1 \\in \\mathbb{R}^{(2n+l+k) \\times n'}$, $\\Theta^2 \\in \\mathbb{R}^{n' \\times k}$ and activation functions $f_1, f_2$. Then, the second step consists in the application of a function g to the state and labels of each node to obtain the final node labels\n$$x_i' = g(x_i, s_i)$$\nwhere in this case too g is a two-layer perceptron, with weight matrices $\\Theta^3 \\in \\mathbb{R}^{(n+k) \\times m'}$, $\\Theta^4 \\in \\mathbb{R}^{m' \\times m}$ and activation functions $f_3, f_4$. In Table 9 we show the functions required to implement this GNN layer. The first step consists in initializing the node states. For this example, we initialize the node states to an array of k zeros and we append it to the current node labels using the identity term and parallel composition\n$(\\iota||\\phi_\\theta)$\nAfter this step, we compute the fixed point of the states using a two-layer perceptron to generate messages and aggregating them through summation\n$(\\iota||\\phi_\\theta); (\\iota ||\\langle\\phi_{MLP_1})^*$\nOnce a fixed point for the states is reached, we use another multi-layer perceptron to obtain the final node labels and obtain\n$(\\iota||\\phi_\\theta); (\\iota ||\\langle\\phi_{MLP_1})^\\ast; \\phi_{MLP_2}$"}, {"title": "11 CONCLUSION", "content": "We presented the syntax and semantics of \u00b5G, a novel programming language for the definition of graph neural networks, where a graph neural network is characterized as a higher-order function : H[T\u2081] \u2192 H[T2]. We proved the type soundness of \u00b5G and shown a graphical formalism for the representation of \u00b5G programs. The language was implemented as a Python library built on the TensorFlow framework. We evaluated \u00b5G on the application of CTL model checking and as a means to specify some of the most popular graph neural network models.\nIn future implementations, we plan to expand the language in a number of ways. A typical graph neural network operation that is still overlooked in \u00b5G is pooling. A pooling operator changes the underlying graph by reducing the number of nodes, producing a coarsened version of a graph. Graph pooling operators can be generally described in terms of the operations of selection, reduction, and connection [16]. The selection operator groups nodes together, the reduction operator aggregates the nodes in each group into a single node, and finally the connection operator generates the edges to link the newly generated nodes. The introduction of pooling operators requires careful consideration of how they can be composed with the existing terms, but it would allow \u00b5G to tackle graph-level tasks explicitly.\nThe main purpose for developing \u00b5G is to be eventually able to perform formal verification of graph neural networks and generate explanations for their outputs. For the time being, there are still no procedures that allow the verification of output reachability properties of GNNs [33]. In the future, we aim to define an abstract interpretation approach for these tasks."}]}