{"title": "The \u00b5G Language for Programming Graph Neural Networks", "authors": ["MATTEO BELENCHIA", "FLAVIO CORRADINI", "MICHELA QUADRINI", "MICHELE LORETI"], "abstract": "Graph neural networks form a class of deep learning architectures specifically designed to work with graph-structured data. As such, they share the inherent limitations and problems of deep learning, especially regarding the issues of explainability and trustworthiness. We propose \u00b5G, an original domain-specific language for the specification of graph neural networks that aims to overcome these issues. The language's syntax is introduced, and its meaning is rigorously defined by a denotational semantics. An equivalent characterization in the form of an operational semantics is also provided and, together with a type system, is used to prove the type soundness of \u00b5G. We show how \u00b5G programs can be represented in a more user-friendly graphical visualization, and provide examples of its generality by showing how it can be used to define some of the most popular graph neural network models, or to develop any custom graph processing application.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning models are at the forefront of artificial intelligence research today. Among them, artificial neural networks are the most commonly used class of models for a wide range of different tasks, including natural language processing, computer vision, software engineering, and many more [29]. For applications where the inputs can be represented as graphs, the graph neural network [10] (GNN) model has been the architecture of choice, and has achieved state-of-the-art performance on many tasks.\nDespite these promising advancements, deep learning models, including graph neural networks, face a number of issues. These systems are difficult to engineer with, and are notoriously harder to debug and interpret compared to traditional software systems [23]. Another issue is the lack of guarantees that these systems offer regarding their outputs, which time and again have been shown to be easily foolable, not only using so-called \"adversarial examples\" [17], but also more generally in unpredictable and surprising ways [3, 12]. Furthermore, these systems act like a black-box and are opaque, in the sense that human users are unable to understand how they have reached their conclusions [4].\nFinally, there is a very strong bias in deep learning research against the usage of prior knowledge even when such usage is warranted, and even then, it is difficult to figure out how to integrate such knowledge in these systems [24].\nIn this paper, we tackle all these issues by proposing a new graph neural network specification language, called \u00b5G (pronounced as \u201cmee-gee\"). A graph neural network in \u00b5G is built up as a composition of simpler GNNs following the formal rules of the language. These GNNs are built up from the base terms, specifying functions to be applied to the single nodes in terms of themselves and/or their neighbors, which can then be composed sequentially, in parallel, selected according to a Boolean condition, or iterated.\nThe language's syntax is introduced using a context-free grammar, and the meaning of each term is formally and rigorously defined by a denotational semantics. Later on, we also introduce a structural operational semantics, and prove that both semantics are equivalent, i.e., they define the same graph neural network operations. Using the operational semantics and after having defined a type system for \u00b5G, we prove the type soundness of the language. The type soundness of \u00b5G guarantees that every well-typed \u00b5G program properly defines a GNN with the correct output type.\nThe language can be used both in textual form and in graphical form. The graphical representation of \u00b5G programs is introduced as a more user-friendly approach to use the language, as it keeps the flow of information and the type of labels easier to follow and reason about.\nOur language is framework and hardware-agnostic, as \u00b5G could be implemented in different ways. We opted to implement \u00b5G in TensorFlow so that it can use its automatic differentiation capabilities and allow the GNNs we program to be executable on CPUs, GPUs, or TPUs without changing their implementation. Furthermore, these GNNs can interoperate with any other TensorFlow feature as if they were TensorFlow models themselves.\nWe claim that \u00b5G helps with the aforementioned problems of deep learning. For the matter of explainability and interpretability, using \u00b5G helps by making the computations performed more explicit, by taking the form of a \u00b5G expression which has clearly definite semantics and types. Indeed, the user might even define functions and computations using the same terminology of the specific domain in which the GNN is going to be used in, making the purpose of each term easier to understand. Some, or all, parts of the GNN can be defined to be inherently interpretable, by virtue of being defined explicitly based on the available domain knowledge. Furthermore, determining the nodes, edges, or labels that contributed to a specific prediction becomes amenable to static analysis techniques [7].\nAs for the issues of trustworthiness, the use of a formal language like \u00b5G allows the formal verification of GNNs similarly to that of other programming languages, e.g., by abstract interpretation, symbolic execution, data-flow analysis, and so on. In particular, we are interested in the problem of formally verifying safety properties of GNNs, e.g. output reachability properties [18]. The verification of an output reachability property for a function f given an input region X and an output region Y consists in checking whether for all inputs x \u2208 X, f(x) \u2208 Y. As far as we know, no solution for this problem has been proposed for graph neural networks specifically [33]. The verification of properties of this kind can be used to prove the robustness of a GNN against adversarial examples, which is critical when such models are deployed in safety-critical systems, where guarantees of correct functioning are paramount.\nFinally, it is easy to include prior knowledge when defining a GNN in \u00b5G. The basic terms of the language need not make use of neural network layers as is typically the case for the more popular graph neural network models, but can in general use any kind of function, not necessarily depending on trainable parameters. This way is also possible to define a GNN which does not require training at all, as we did in a previous work where we used \u00b5G for model checking [6]. Prior, or innate, knowledge can be freely mixed with optimizable function in order to build hybrid, neural-symbolic systems [22, 25, 32].\""}, {"title": "2 RELATED WORK", "content": "Many domain specific languages (DSL) have been developed over the years to ease the development of machine learning applications [28]. Far from being a complete survey, we will discuss here only the most recent ones or those that are most relevant for our work.\nDeepDSL [37] is a language based on Scala that allows the definition of neural networks through variable assignments. Variables can be assigned computational modules such as convolutional layers, pooling layers, activation functions and so on. Neural networks are defined by variable assignments where the right-hand side is the function composition of other computational modules. The code is compiled to Java and uses JCuda to communicate with CUDA and cuDNN.\nAiDSL [13] is a language for the specification of supervised feed-forward neural networks using a model driven engineering approach. The code is compiled into Encog, a machine learning framework in Java. SEMKIS-DSL [19] is another language developed using a model driven engineering approach, where the main focus is shifted from the specification of the neural network architecture to the requirements that the input dataset and the neural network's outputs must satisfy. For the time being there are no compilation targets for the language, so it is not possible to automatically produce a neural network from specification. On the other hand, Diesel [11] acts at a finer level of detail, allowing the specification of common linear algebra and neural network computations that are typically implemented by libraries such as cuBLAS and cuDNN. It uses a polyhedral model to schedule operations and perform optimizations, and the source code is compiled directly into CUDA code. So far, none of these languages take into consideration the definition of graph-structured inputs or the typical operations that characterize graph neural networks.\nA DSL where graphs are first-class objects is OptiML [31], which supports the specification of machine learning models to be run on heterogenous hardware (i.e. the CPU or GPU). The language can be used to implement any machine learning model that can be expressed using the Statistical Query Model [20], and the code can be compiled to Scala, C++, or CUDA code. Graph objects in OptiML support operations expressed in terms of vertices and edges, and graph operations are specified in terms of vertices and their neighbours in the graph. Furthermore, OptiML has a fixed point operator with a customizable threshold value just like \u00b5G. Another DSL that supports graphs is StreamBrain [27], which is a language for the specification of Bayesian Confidence Propagation Neural Networks. This model takes in input a graph where every node is a random variable and edges represent the dependencies between variables. StreamBrain"}, {"title": "3 BACKGROUND AND NOTATION", "content": "Graphs and their labelings. A directed graph is a pair $G = (V, E)$ where V is a collection of vertices (which we also refer to as nodes) and $\\mathscr{E}$ is a collection of ordered pairs of vertices, called edges. For any $u, v \\in V$, whenever $(u, v) \\in \\mathscr{E}$ we say that u is a predecessor of v and that v is a successor of u. We also say that (u, v) is an incoming edge for v and an outgoing edge for u. Moreover, let $N_G(v)$ denote the set of predecessors of v, formally $N_G(v) = \\{u | (u,v) \\in \\mathscr{E}\\}$, while $N_G^\\prime(u)$ denotes the set of successors of u, namely $N_G^\\prime(u) = \\{v | (u, v) \\in \\mathscr{E}\\}$. Similarly, let $IG(v)$ denote the set of incoming edges for v, formally $IG(v) = \\{(u, v) | u, v \\in V\\}$, and let $OG(u)$ denote the set of outgoing edges for u, formally $OG(u) = \\{(u, v) | u, v \\in V\\}$.\nSometimes it is useful to associate nodes and edges with values. Given a graph $G = (V, \\mathscr{E})$, we can consider its node-labeling and edge-labeling. The former is a function $\\eta : V \\rightarrow T_V$ associating each node $v \\in V$ with a value in the set $T_V$. Similarly, an edge-labeling is a function $\\xi : \\mathscr{E} \\rightarrow T_\\mathscr{E}$ that maps an edge to its label in the set $T_\\mathscr{E}$. Let $V\\prime \\subseteq V$ (resp. $\\mathscr{E}\\prime \\subseteq \\mathscr{E}$), we let $\\eta(V\\prime)$ (resp. $\\xi(\\mathscr{E}\\prime)$) denote the multi-set of labels associated to the elements of $V\\prime$ (resp. $\\mathscr{E}\\prime$) by $\\eta$ (resp. $\\xi$). Likewise, we let $(\\eta, \\xi)(\\mathscr{E})$ denote the multi-set of tuples $(\\eta(u), \\xi((u, v)), \\eta(v))$ for each $(u, v) \\in \\mathscr{E}$.\nEncoding of graphs and labels. One way to represent graphs and their labeling functions on a computer is in the form of a matrix of node features X, an adjacency matrix A and a matrix of edge features E. The node features matrix X stores the features associated with each node in the graph. The i-th row of the matrix contains a value $x_i \\in T_V$ that represents the information associated with the i-th vertex of the graph in a given ordering. The adjacency matrix A encodes the architecture of the graph, with each non-zero element $a_{ij} \\in \\mathbb{R}$ denoting the presence of an edge from a node i to a node j. The edge features matrix E stores the features associated with each edge, like the node features matrix. The i-th row of E is a value $e_i \\in T_\\mathscr{E}$ that represents the information associated with the i-th edge in the row-major (or any other) ordering of the edges in A.\nGraph Neural Networks. A graph neural network is a deep learning model that operates on graph-structured data. Introduced by Scarselli et al. [30], it emerged to overcome the limitations of graph kernel methods [26]. GNNs generalize many other classes of deep learning architectures, as other deep learning models can be seen as a particular case of graph neural networks [8]. Convolutional Neural Networks (CNNs), for example, can be seen as a graph neural networks where inputs can only be 1-dimensional sequences (such as texts) or 2-dimensional grids (such as images), which are both particular instances of graphs. Graph neural networks, more generally, can learn on any non-Euclidean structure representable as graphs, which contrary to grids such as images, can have differing shapes, number of nodes and connectivity properties.\nGraph neural networks can be used for many tasks, which can be roughly categorized according to the subject of prediction: nodes, edges, or entire graphs. As with other machine learning models, there are two main kinds of tasks, namely classification and regression, where the goal is to predict the class or some numerical quantity associated with the nodes in the graph, edges, or the entire graph."}, {"title": "4 THE DOMAIN OF GRAPH NEURAL NETWORKS", "content": "In this section, we introduce the necessary definitions and theorems that characterize graph neural networks in our work. We elaborate further on the notion of node-labeling function and formalize the graph neural network as a transformation between node-labeling functions, then move on to prove that the set of graph neural networks is a chain complete partially ordered set.\n4.1 Node-labeling functions\nWe denote the set of all labeling functions with co-domain T as H[T], and we say that this set specifies its type. As examples, labeling functions of type H[B] map vertices to Boolean values, while functions of type $H[\\mathbb{N}^k]$ with $k \\in \\mathbb{N}$ map vertices to k-tuples of natural numbers. Each type also includes a bottom node-labeling function $\\bot_H = \\lambda v.undef$."}, {"title": "4.2 Graph neural networks", "content": "Given a graph G together with its edge-labeling function $\\xi$, we define a graph neural network to be a function $\\Phi_{G,\\xi} : H[T_1] \\rightarrow H[T_2]$ that maps a node-labeling function to another node-labeling function. We denote the set of such graph neural networks as $\\Phi_{G,\\xi}[T_1, T_2]$ or, more succinctly, as $\\Phi_{G,\\xi}$. The subscript G, \u03be indicates that each graph neural network is parametrized by a graph and an edge-labeling function. Likewise for the node-labeling functions, the set of graph neural networks $\\Phi_{G,\\xi}[T_1, T_2]$ is a partially ordered set with a point-wise ordering relation $\\subseteq_{\\Phi_{G,\\xi}[T_1,T_2]}$ such that, for all $\\eta \\in H[T_1]$\n$$\\phi_1 \\subseteq_{\\Phi_{G,\\xi}[T_1,T_2]} \\phi_2 \\Leftrightarrow \\phi_1(\\eta) = \\eta' \\Rightarrow \\phi_2(\\eta) = \\eta'$$\nWhen the types are clear from the context or not revelant, we drop the subscript and simply write $\\subseteq_{\\Phi_{G,\\xi}}$.\nNext, we define the underlying relation of a GNN as the set of input-output tuples which characterize the GNN. Building on this concept, we also specify their sequential and parallel composition.\nDefinition 4.1. Given a graph neural network $\\phi : \\Phi_{G,\\xi}[T_1, T_2]$ we define its underlying relation, denoted by $rel(\\phi)$, as\n$$rel(\\phi) = \\{(\\eta_1, \\eta_2) \\in H[T_1] \\times H[T_2] | \\Phi(\\eta_1) = \\eta_2\\}$$\nThe sequential composition, or simply composition, of relations $A : H[T_1] \\times H[T_2]$ and $B : H[T_2] \\times H[T_3]$, denoted by $A * B$, is defined as\n$$A * B = \\{(\\eta_1, \\eta_3) | \\exists \\eta_2 \\in H[T_2] : (\\eta_1, \\eta_2) \\in A \\land (\\eta_2, \\eta_3) \\in B\\}$$\nThe parallel composition, or concatenation, of relations $A: H[T_1] \\times H[T_2]$ and $B : H[T_1] \\times H[T_3]$, denoted by $A \\parallel B$, is defined as\n$$A \\parallel B = \\{(\\eta_1, (\\eta_2, \\eta_3)) | (\\eta_1, \\eta_2) \\in A \\land (\\eta_1, \\eta_3) \\in B\\}$$\nThe following lemma defines the relationship between the relations and the ordering of GNNs.\nLEMMA 4.2. Given two GNNs $\\phi_1, \\phi_2$ we have\n$$\\phi_1 \\subseteq_{\\Phi_{G,\\xi}} \\phi_2 \\Leftrightarrow rel(\\phi_1) \\subseteq rel(\\phi_2)$$\nPROOF. First we show that $\\phi_1 \\subseteq_{\\Phi_{G,\\xi}} \\phi_2 \\Rightarrow rel(\\phi_1) \\subseteq rel(\\phi_2)$. Since $\\phi_1(\\eta) = \\eta' \\Rightarrow \\Phi_2(\\eta) = \\eta'$, any $(\\eta, \\eta') \\in rel(\\phi_1)$ is also a member of $rel(\\phi_2)$."}, {"title": "5 THE \u00b5G LANGUAGE FOR GRAPH NEURAL NETWORKS", "content": "In this section, we introduce the syntax of \u00b5G as a programming language for the definition of graph neural networks. After specifying its syntax (Definition 5.1), we show its denotational semantics (Definition 5.2), and structural operational semantics (Definition 5.8)."}, {"title": "5.1 Denotational semantics", "content": "Having defined the syntax, we are now ready to introduce the denotational semantics of \u00b5G.\nDefinition 5.2. (Denotational semantics) Given a graph G and an edge-labeling function \u03be, we define the semantic interpretation function $\\mathcal{S}_{ds} [[\\cdot]]_{G,\\xi} : \\mathcal{N} \\rightarrow \\Phi_{G,\\xi}$ on \u00b5G formulas N by induction in the following way:\n$$\\begin{aligned} &\\mathcal{S}_{ds} [[\\iota]]_{G,\\xi} = id \\\\ &\\mathcal{S}_{ds} [[\\psi]]_{G,\\xi} (\\eta) = \\lambda v.f_{\\psi} ([\\eta(v) | v \\in V], \\eta(v)) \\\\ &\\mathcal{S}_{ds} [[<\\varphi]]_{G,\\xi} (\\eta) = \\lambda v.f_{\\sigma} (f_{\\varphi} ((\\eta, \\xi) (I_G (v))), \\eta(v)) \\\\ &\\mathcal{S}_{ds} [[\\triangleright \\varphi]]_{G,\\xi} (\\eta) = \\lambda v.f_{\\sigma}(f_{\\varphi} ((\\eta, \\xi) (O_G (v))), \\eta(v)) \\\\ &\\mathcal{S}_{ds} [[N_1 ; N_2]]_{G,\\xi} = \\mathcal{S}_{ds} [[N_2]]_{G,\\xi} \\circ \\mathcal{S}_{ds} [[N_1]]_{G,\\xi} \\\\ &\\mathcal{S}_{ds} [[N_1 || N_2]]_{G,\\xi} = \\mathcal{S}_{ds} [[N_1]]_{G,\\xi} \\times \\mathcal{S}_{ds} [[N_2]]_{G,\\xi} \\\\ &\\mathcal{S}_{ds} [[N_1 \\oplus N_2]]_{G,\\xi} = cond(\\pi_L, \\mathcal{S}_{ds}[[N_1]]_{G,\\xi} \\circ \\pi_R, \\mathcal{S}_{ds}[[N_2]]_{G,\\xi} \\circ \\pi_R) \\\\ &\\mathcal{S}_{ds} [[N^*]]_{G,\\xi} = FIX(\\lambda \\varphi.cond(\\lambda e.\\lambda v.\\mathcal{S}_{ds} [[N]]_{G,\\xi} (e)(v) \\approx_{\\epsilon} e(v), id, \\varphi \\circ \\mathcal{S}_{ds} [[N]]_{G,\\xi})) \\end{aligned}$$\nfor any \u03c8, \u03c6, \u03c3 \u2208 S. The functions cond and FIX are defined as:\n$$cond(t, f_1, f_2) (\\eta) = \\begin{cases} f_1(\\eta) & \\text{if } t(\\eta) = \\lambda v.True \\\\ f_2(\\eta) & \\text{otherwise} \\end{cases}$$\n$$FIX(f) = \\{ f^n (\\bot_H) | n \\geq 0 \\}$$\nwhere $f^0 = id$ and $f^{n+1} = f \\circ f^n$. For any label type T and any real value $\\epsilon \\in \\mathbb{R}$, we define a binary predicate $\\approx_{\\epsilon}$ such that"}, {"title": "5.2 Operational semantics", "content": "For the structural operational semantics, we consider computations of the form\n$$(N, \\eta)$$\nwhere N is a \u00b5G expression and \u03b7 is a node-labeling function.\nFinally, we can define the semantic interpretation function on the structural operational semantics.\nDefinition 5.8 (Structural operational semantics). Given a graph G and an edge-labeling function \u03be, we define the semantic interpretation function $\\mathcal{S}_{sos} [[\\cdot]]_{G,\\xi} : \\mathcal{N} \\rightarrow \\Phi_{G,\\xi}$ such that\n$$\\mathcal{S}_{sos} [[N]]_{G,\\xi} (\\eta) := \\begin{cases} \\eta' & \\text{if } \\langle N, \\eta \\rangle \\rightarrow_{G,\\xi}^* \\eta' \\\\ undef & \\text{otherwise} \\end{cases}$$\nThe rules PAR1 and PAR2 in Table 1 make the evaluation of \u00b5G expressions non-deterministic. The next theorem shows that no matter the order of application of the structural semantics rules, the obtained GNN is the same.\nTHEOREM 5.9 (CONFLUENCE). If $(N, \\eta) \\rightarrow_{G,\\xi} \\langle N_1, \\eta_1 \\rangle$ and $(N, \\eta) \\rightarrow_{G,\\xi} \\langle N_2, \\eta_2 \\rangle$, then either $N_1 = N_2$ and $\\eta_1 = \\eta_2$ or there exists a term $N\\prime$ and a labeling function $\\eta'$ such that $(N_1, \\eta_1) \\rightarrow_{G,\\xi} \\langle N\\prime, \\eta'\\rangle$ and $(N_2, \\eta_2) \\rightarrow_{G,\\xi} \\langle N\\prime, \\eta'\\rangle$.\nPROOF. By induction on the structure of N. The only interesting case is that for expressions of the form $N_1 \\oplus N_2$, where rules PAR1 and PAR2 can be applied in any order. Suppose that $(N_1 \\oplus N_2, \\eta) \\rightarrow_{G,\\xi} \\langle N_1 \\oplus N_2, \\eta|\\pi_R(\\eta) \\rangle$ by rule PAR1 and that $(N_1 \\oplus N_2, \\eta) \\rightarrow_{G,\\xi} \\langle N_1 \\oplus N_2, \\pi_L(\\eta) | \\eta_2 \\rangle$ by rule PAR2. Then the required term is $N' = N_1 \\oplus N'$ and the labeling function is $\\eta' = \\eta_1|\\eta_2$, because $(N_1 \\oplus N^\\prime, \\pi_L(\\eta) | \\eta_2 \\rangle \\rightarrow_{G,\\xi} \\langle N_1 \\oplus N_2, \\eta_1|\\eta_2 \\rangle$ by rule PAR1 and $(N_1 \\oplus N_2, \\eta|\\pi_R(\\eta)) \\rightarrow_{G,\\xi} \\langle N_1 \\oplus N_2, \\eta_1|\\eta_2 \\rangle$ by rule PAR2.\n5.3 Language macros\nWe can enrich \u00b5G with a number of macros that simplifies the job of programming a graph neural network. In this section, we describe the means to define variables, functions, and shortcuts for the definition of if-then-else and while loop expressions. Some of these extensions require us to introduce a set of variable symbols $X ::= X | Y | Z | ...$ to the language's syntax. We will also use function symbols $p_L$ and $p_R$ to denote left and right projections.\nVariable assignments. It is useful sometimes to assign an entire expression to a single label, so that whenever that label occurs in a program, the referred expression is substituted to it. For this purpose, we introduce variable assignments in the form of let expressions. A let expression has the form let X = N in N'. The intuitive meaning of such expression is that all occurrences of the variable symbol X in the expression N' are substituted with the expression"}, {"title": "6 EQUIVALENCE OF OPERATIONAL AND DENOTATIONAL SEMANTICS", "content": "Both the denotational semantics and operational semantics of \u00b5G define a mapping from node-labeling functions to node-labeling functions, i.e., a graph neural network. In the next theorem, we show that they define the same graph neural network.\nTHEOREM 6.1 (EQUIVALENCE OF DENOTATIONAL AND STRUCTURAL OPERATIONAL SEMANTICS). For any \u00b5G expression N, any graph G and any edge-labeling function \u03be, we have that\n$$\\mathcal{S}_{ds} [[N]]_{G,\\xi} = \\mathcal{S}_{sos} [[N]]_{G,\\xi}$$\nPROOF. Since both functions return GNNs, which are members of a partially ordered set $\\Phi_{G,\\xi}$ with ordering relation $\\subseteq_{\\Phi_{G,\\xi}}$, it is sufficient to show that for any \u00b5G expression N:\n$$\\begin{aligned} &\\bullet \\ \\mathcal{S}_{sos} [[N]]_{G,\\xi} \\subseteq_{\\Phi_{G,\\xi}} \\mathcal{S}_{ds} [[N]]_{G,\\xi} \\\\ &\\bullet \\ \\mathcal{S}_{ds} [[N]]_{G,\\xi} \\subseteq_{\\Phi_{G,\\xi}} \\mathcal{S}_{sos} [[N]]_{G,\\xi} \\end{aligned}$$\nLemma 6.2 and Lemma 6.3 prove that both these conditions are satisfied by the semantics of \u00b5G.\nLEMMA 6.2. For every expression N of \u00b5G, we have $\\mathcal{S}_{sos} [[N]]_{G,\\xi} \\subseteq_{\\Phi_{G,\\xi}} \\mathcal{S}_{ds} [[N]]_{G,\\xi}$.\nPROOF. We shall prove that for any expression N and any node-labeling function \u03b7, \u03b7'\n$$(N, \\eta) \\rightarrow_{G,\\xi}^* \\eta' \\quad \\Leftarrow \\quad \\mathcal{S}_{ds} [[N]]_{G,\\xi} (\\eta) = \\eta'$$\nIn order to do that, we will show that\n$$(N, \\eta) \\rightarrow_{G,\\xi} \\eta' \\quad = \\quad \\mathcal{S}_{ds}[[N]]_{G,\\xi} (\\eta) = \\eta'$$\n$$\\langle N, \\eta \\rangle \\rightarrow_{G,\\xi} \\langle N', \\eta' \\rangle \\quad \\Rightarrow \\quad \\mathcal{S}_{ds} [[N]]_{G,\\xi} (\\eta) = \\mathcal{S}_{ds} [[N' ]]_{(G,\\xi)} (\\eta')$$\nIf Equation 3 and Equation 4 hold, the proof of Equation 2 is a straightforward induction on the length k of the derivation sequence $(N, \\eta) \\rightarrow_{G,\\xi}^k \\eta'$. The proof of Equation 3 and Equation 4 is by induction on the shape of the derivation trees.\nThe case ID: We have $(\\iota, \\eta) \\rightarrow_{G,\\xi} \\eta$, and since $\\mathcal{S}_{ds} [[\\iota]]_{G,\\xi} (\\eta) = \\eta$, the result follows.\nThe case APPLY: We have $\\langle \\psi, \\eta \\rangle \\rightarrow_{G,\\xi} \\langle \\iota, \\lambda v.f_{\\psi} ([\\eta(u) | u \\in V], \\eta(v)) \\rangle$, and since"}, {"title": "7 TYPE SOUNDNESS", "content": "We say that a \u00b5G term is well-typed whenever it can be typed with the rules of Table 2. These rules guarantee that any well-typed \u00b5G term defines a graph neural network. Next, we will be using these typing rules to prove the type soundness of the operational semantics of \u00b5G. Proving the type safety of \u00b5G ensures that the GNNs we program with it handle the data types correctly and produce results of the expected type. Concretely, this means that well-typed GNNs can always progress to the next step of computation and that after each step the type of the output is preserved."}, {"title": "8 A GRAPHICAL REPRESENTATION", "content": "In this section, we present a graphical representation for \u00b5G programs and expressions. Using such graphical notation makes it easier to understand and communicate the structure and purpose of a \u00b5G program, as the programmer might easily lose track of label types and sizes when developing in textual form. The basic terms of \u00b5G can be represented graphically as boxes, as shown in Figure 1, while the composite terms of the language are shown in Figure 2. Each term has exactly one input node-labeling and one output node-labeling, and the graphical representation is built top-down starting from the main (usually composite) term and substituting each box with either a basic term or a composite one; in this last case, this procedure is repeated recursively until all boxes in the figure are basic terms. As an illustrative example, let \u03c81, \u03c82, \u03c83, \u03c6, \u03c3 be function symbols. Figure 3 shows how we can represent graphically the \u00b5G expression (\u03c81||\u03c82);\u03c83; <\u03c3.\nIn the graphical representation for \u00b5G expressions we have used some auxiliary functions which we describe in turn:\n$$\\begin{aligned} &\\bullet \\ \\text{The function } \\bullet : H[T_1] \\rightarrow H[T_1] \\times H[T_1], \\bullet(\\eta) = (\\eta, \\eta) \\text{ maps a node-labeling function to a tuple containing two copies of itself.} \\\\ &\\bullet \\ \\text{The function }\\circ : H[T_1] \\times H[T_2] \\rightarrow H[T_1 \\times T_2], \\\\ &\\eta_1 \\circ \\eta_2 = \\begin{cases} \\eta_1 & \\text{if } \\eta_2 = \\bot_H \\\\ \\eta_2 & \\text{if } \\eta_1 = \\bot_H \\\\ \\eta_1|\\eta_2 & \\text{otherwise} \\end{cases} \\\\ &\\text{maps two node-labeling functions to a new node-labeling function which labels each node with the concatenation of their labels. If one of the input labeling functions is the empty labeling, } \\circ \\text{ simply returns the other function.} \\end{aligned}$$"}, {"title": "9 IMPLEMENTATION", "content": "The \u00b5G language has been implemented as a Python library called LIBMG, which we described in detail in a previous work [7]. The library was developed on top of Spektral [15], a graph neural network library based on Tensorflow [1]. The main advantage of using a framework such as TensorFlow is that \u00b5G programs are compiled to graph neural networks that can be executed seamlessly on CPUs, GPUs, or in a distributed setting without changing the source code.\nThe functionalities of LIBMG include the typical language support tools in the form of a parser, unparser and normalizer for \u00b5G expressions. The main functionality that is provided consists in the \u00b5G compiler that transforms a \u00b5G program into a TensorFlow Model instance. The compiler can be set up to automatically memoize sub-expressions that occur multiple times in the same program. The library also implements basic graph visualization functions, that allows to view in any web browser the input or output graphs, or intermediate representation produced by a \u00b5G model. Finally, a simple explanatory algorithm is provided that computes the sub-graph that was used by a \u00b5G model to compute the label of a given node.\nUsing LIBMG, the \u03c8, \u03c6, and o functions are defined by instantiating the corresponding classes and are stored in dictionaries. These dictionaries, which contain a mapping between function names and the functions themselves, are used to create a \u00b5G compiler instance. The \u00b5G compiler can then parse \u00b5G programs containing the function names defined in its dictionaries.\nThe models generated by the compiler can be trained by TensorFlow as usual. Only in the case of the star operator, the corresponding layer implements a custom gradient computation. In that case implicit differentiation is used as to make the computation of the gradient tractable."}, {"title": "10 EVALUATION", "content": "We evaluate \u00b5G both in terms of its expressiveness, meaning that it can be used to define most, if not all, of the graph neural network architectures that have been developed over the years, and also as a specification language for the development of graph neural networks for specific tasks. In Section 10.1 we describe an updated version of the CTL model checker developed using \u00b5G introduced in our previous work [6, 7]. Then, in Section 10.2, we show how to define some of the more well-known GNN architectures using our language.\n10.1 Model Checking\nWe evaluated \u00b5G in a previous work by using it to define a GNN that performs CTL model checking on Kripke structures. For that use case, the set of functions that we used is shown in Table 3, while in Table 4 we show the translation function from CTL to the \u00b5G expression denoting the GNN that verifies it.\nThe \u00b5G model checker implementation was compared to two other explicit-state model checkers, pyModelChecking\u00b9 and mCRL2 [9]. We used some of the benchmarks from the Model Checking Contest 2022 [2], namely those for which we could explicitly generate the reachability graph on our machine, shown in Table 5. The \u00b5G model checker was set up to either verify all the formulas in parallel (the full setup), or one by one as would have been the case for the other model checkers (the split setup). In the former case, there would have been additional advantages due to the memoization of common sub-formulas across multiple formulas. The experimental results are shown in Figure 4, and we notice that for the larger Kripke structures there is an evident speed advantage of \u00b5G compared to the other model checkers (in some instances, by an order of magnitude)."}, {"title": "10.2 Specification of Graph Neural Networks", "content": "In this section, we show how to define some of the most popular graph neural network models in \u00b5G. As is the case for any programming language, there are many different ways to obtain the same"}]}