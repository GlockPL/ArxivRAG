{"title": "Man Made Language Models? Evaluating LLMs' Perpetuation of Masculine Generics Bias", "authors": ["Enzo Doyen", "Amalia Todirascu"], "abstract": "Large language models (LLMs) have been shown to propagate and even amplify gender bias, in English and other languages, in specific or constrained contexts. However, no studies so far have focused on gender biases conveyed by LLMs' responses to generic instructions, especially with regard to masculine generics (MG). MG are a linguistic feature found in many gender-marked languages, denoting the use of the masculine gender as a \"default\" or supposedly neutral gender to refer to mixed group of men and women, or of a person whose gender is irrelevant or unknown. Numerous psycholinguistics studies have shown that MG are not neutral and induce gender bias. This work aims to analyze the use of MG by both proprietary and local LLMs in responses to generic instructions and evaluate their MG bias rate. We focus on French and create a human noun database from existing lexical resources. We filter existing French instruction datasets to retrieve generic instructions and analyze the re- sponses of 6 different LLMs. Overall, we find that \u224839.5% of LLMs' responses to generic instructions are MG-biased (~73.1% across re- sponses with human nouns). Our findings also reveal that LLMs are reluctant to using gender- fair language spontaneously.", "sections": [{"title": "Introduction", "content": "Masculine generics (MG) are a linguistic feature found in many gender-marked languages, among which French, German, or Dutch. MG denote the use of the masculine gender in gendered languages as a \"default\" or supposedly neutral gender to refer to either a) a mixed group of men and women, or b) a person whose gender is irrelevant or unknown within the context, as in the following examples in French:\n(a) Les \u00e9tudiants ont particip\u00e9 \u00e0 l'atelier.\n(Students participated to the workshop.)\n(b) Un athl\u00e8te doit s'entrainer r\u00e9guli\u00e8rement pour progresser.\n(An athlete needs to train regularly to progress.)\nWhile such usage of the masculine gender is supposed to act as a neutralizer and contrast with the \"specific\" interpretation of the masculine (i.e., referring exclusively to men), empirical research in psycholinguistics has revealed that this supposed genericness is in fact not processed as such by na- tive speakers (Gygax et al., 2008; Rothermund and Strack, 2024), thereby exposing individuals to cog- nitive biases that amplify male-centric mental repre- sentations (Braun et al., 2005; Gygax et al., 2012).\nThe rapid development of large language models (LLMs) and their remarkable capabilities across various textual tasks, such as translation (Alves et al., 2024; Zhu et al., 2024), summarization (Pu et al., 2023), and other text generation tasks, have made them indispensable tools for numerous ap- plications. Relatedly, the release of ChatGPT by OpenAI and similar chatbots through user-friendly web applications have made it extremely trivial for anyone to generate textual content. However, these models inherently reflect the biases to be found in the training data, including those related to gender representation.\nEven though the detection of text generated by artificial intelligence (AI) remains a challenging task (Tang et al., 2023), an increasing amount of works tackle this issue (Abassy et al., 2024; Marchitan et al., 2025), and preliminary research seems to indicate that a growing amount of text on the Internet is AI-generated, whether it be on social media (Sun et al., 2024; Wei and Tyson, 2024) or even in Wikipedia edits (Brooks et al., 2024). Such an increasing use of LLMs to publish content on- line could result in bias propagation among people who are exposed to that content. Similarly, the use of synthetically generated data for the training of AI systems (Long et al., 2024) also poses a risk of"}, {"title": "Related Work", "content": "MG and the associated male bias have been ex- tensively studied in the field of psycholinguistics for a variety of gender-marked languages (Silveira, 1980; Stahlberg et al., 2001; Safina, 2024), includ- ing French (Gygax et al., 2012; Richy and Bur- nett, 2021). For example, a study conducted by Gygax et al. (2008) in English, French and Ger- man had participants read a sentence A with a role noun (e.g., \"social workers\u201d; MG in both French and German). Then, when presented with a sen- tence B with a noun referring to the members of the noun group in A (e.g., \u201cwomen\u201d), participants had to judge as quickly as possible if it was a co- herent continuation of sentence A. Results showed that in English, gender representations aligned with stereotypes: female-stereotyped roles (e.g., \"beau- ticians\") led to female-biased interpretations, while male-stereotyped roles (e.g., \u201cpoliticians\u201d) led to male-biased interpretations. Neutral roles showed no bias. In French and German, grammatical gen- der overrode stereotypes: masculine plural forms led to predominantly male-biased interpretations, even for female-stereotyped roles.\nSimilarly, a survey conducted by Harris Interac- tive (2017) regarding the use of inclusive or gender- neutral language in French highlighted the impact of such language on mental representations. It was found that respondents, when asked to name a celebrity using a formulation that was either in- clusive (i.e., using both masculine and feminine forms of a role noun) or gender-neutral (i.e., using a non-gender-specific role noun or an equivalent) were more likely to name a woman as opposed to a man, while MG formulations led to more male- centric answers. Similar findings were reported by Stahlberg et al. (2001) for German using MG role nouns.\nRecently published studies have resorted to more precise methods such as EEG to track how language-related gender information is processed by the brain to evaluate the effects of MG. Glim et al. (2024) showed that even explicitly disam- biguated MG nouns used to refer to women led to higher cognitive load from participants for the task of noun phrase reference resolution, further indicating that the alleged genericness of MG lacks empirical backing.\nWhen it comes to natural language processing (NLP), gender bias is by far the most studied type of bias (Ducel et al., 2024b). Numerous researchers have drawn attention to gender bias in word em- beddings (Bolukbasi et al., 2016), in machine trans- lation systems (Savoldi et al., 2021; Wisniewski et al., 2021) or in text classification tasks (Sobhani and Delany, 2024).\nMore recently, LLMs too have been shown to exhibit gender biases and convey stereotypes when generating context-restricted textual content (see Kotek et al. (2023) for pronoun disambiguation; D\u00f6ll et al. (2024) for pronoun prediction; You et al. (2024) for neutral name prediction), including in languages other than English (Zhao et al., 2024; Ducel et al., 2024a). Nonetheless, in spite of the"}, {"title": "Methodology", "content": "Our methodology is divided into two main tasks. First, we create a database of French human nouns (HN), which will be used both to detect occurrences of MG and evaluate the ratio of HN to MG uses in LLM instructions and outputs. Second, we re- trieve and filter different datasets of human-written instruction/output pairs as well as AI-generated in- structions to remove entries exhibiting specific uses of the masculine gender. We send a filtered and nar- rowed set of generic instructions to a wide range of LLMs, and retrieve their responses. Leverag- ing these responses, we perform quantitative and qualitative analyses of the use of MG by LLMs."}, {"title": "Database of French Human Nouns", "content": "Since MG strictly refer to human beings, the first step was to use a database of French HNs. To our knowledge, the only existing database is that of the NHUMA project (Stosic and Lagae, 2013)\u00b9, whose goal is to provide an extensive linguistic de- scription of HN in French. However, this database has a limited number of entries (1,107), many of which are not MG. We thus extended it taking the following steps.\nWe first leveraged several existing public French lexical databases, namely Demonette (Namer et al., 2023), TLFi\u00b2 and Wiktionary\u00b3. While Demon- ette provides an easily accessible list of masculine- feminine noun pairs\u2074, no such option is offered for TLFi. As a result, we built a custom scraper using the Playwright Node.js library to retrieve TLFi entries that could refer to human beings. We restricted the search to entries having one of their definitions starting with gender-neutral or gender-specific nouns or pronouns commonly used in dic- tionary entries related to human beings (see list in Appendix E). We also performed recursive search"}, {"title": "HScorer: a French Human Noun Classifier", "content": "To build our classifier, we created golden HN and non-HN datasets. The golden HN dataset was built from previously scraped entries from Wikidata and Wiktionary, as well as entries from the NHUMA database. For the golden non-HN dataset, we lever- aged the WordNet database (Princeton University, 2010) and its collection of universal synsets to filter out human-related nouns and retrieve non- HN with synsets artifact.n.01, object.n.01 and living_thing.n.018. We retrieved a total of 14,942 HN and 17,912 non-HN.\nThis dataset of HN and non-HN (32,854 entries in total) was then used to train three types of mod- els: a logistic regression (LR) model, a XGBoost model (Chen and Guestrin, 2016) and a Trans- former model (Vaswani et al., 2017). We did this to take advantage of the strengths of each model type. In particular, we wanted to avoid adding false positives to our final database: only words classi- fied as HNs by the three models (full agreement) would be part of the final HN database. The LR and XGBoost model types were chosen based on their best performance compared to a set of classical machine learning model types. For these models, we calculated four different scores to be used as features (see Appendix A for more details). For the Transformer model, we used CamemBERT (Martin et al., 2020) and forwarded tokenized $W$ as input."}, {"title": "Analyzing MG Use in LLM Instructions and Outputs", "content": "Our main objective is to comprehensively evaluate the extent to which LLMs are prone to using MG in their answers to generic (non-specific) instructions. To this goal, we leveraged the French HN database introduced in Section 3.1 to build a MG subset (5,140 entries) comprised of male-only HNs to be used in our analyses. This subset was created by removing epicene (i.e., words whose masculine and feminine forms are identical) and feminine words from the original HN database.\nFor the purposes of the analysis, we resorted to four different datasets. Two of them are human-written instruction/output datasets, while the two others are AI-generated/translated instruc- tion datasets. Human-generated instruction/out- put datasets are included to gauge the extent to which French human speakers are prone to resort- ing to MG in written texts, and thus have a base- line when comparing with LLM outputs (see Sec- tion 3.4). We separated human-written instructions from AI-translated French instructions in our anal- ysis as we wanted to analyze the writings by native French speakers, and because numerous studies have shown that AI translation from and to lan- guages with different gender systems could lead to biases, notably by favoring the use of the mascu- line gender (Zaranis et al., 2024; Vanmassenhove, 2024).\nThe human-written instruction/output datasets include \u201coracle\u201d, a set of question-answer pairs from Wikipedia users 10 (4,613 pairs), and \"oasst2\" (Open Assistant Conversations Dataset Release 2)11, filtered to only get French-tagged entries (1,773 chats). Both datasets only contain human-generated conversations.\nIn addition, we use two other instruction AI- generated/translated datasets: \u201cfrench_hh_rlhf\u201d12 and \"French-Alpaca-dataset-Instruct-55K\u201d13. The \"french_hh_rlhf\" dataset (henceforth, \u201chh_rlhf\") is a direct translation of Anthropic's hh-rlhf dataset14, and contains 161,000 instruction/out- put pairs. The \u201cFrench-Alpaca-dataset-Instruct- 55K\" dataset (henceforth, \u201calpaca\") is comprised of 55,184 French instruction/output pairs generated by OpenAI's GPT-3.5. For both datasets, we re- trieved only the corresponding instructions. For the hh_rlhf dataset specifically, only positive-rated (\"chosen\") instructions were selected."}, {"title": "Instruction and Output Filtering", "content": "To conduct a comprehensive and precise analysis of how MG nouns are used both in human-written instructions/outputs and in LLMs' responses to generic instructions, we designed filtering rules to remove from the datasets entries exhibiting spe- cific uses of the masculine gender, that is contexts where a masculine word does indeed refer to a male individual. This was done to avoid biasing the MG use analysis results.\nFirst, we removed instructions and outputs con- taining names of people or personalities. We used spaCy v3.8 (Montani et al., 2024) and the French model fr_core_news_lg and the integrated NER component to detect words labelled with \u201cPER\u201d. As some first name occurrences failed to be de- tected as \"PER\u201d, we also used the publicly avail- able dataset of given names on Open Data Paris15 and checked if names with the \u201cMISC\u201d label were in that list.\nSimilarly, leveraging the French spaCy Trans- former model fr_dep_news_trf for state-of-the- art performance, we performed POS tagging and dependency parsing for entries in our two datasets and removed instructions containing the interrog- ative pronoun \u201cqui\" (\"who\"), strictly used to re- fer to people, as this could bias answers towards referring to a specific person. For the same rea- son, instructions including a singular possessive determiner (e.g., \u201cmon\u201d, 'my') or a definite deter- miner (e.g., \"ce\u201d, 'this') followed by a HN were excluded. Finally, and only for the \u201coracle\u201d dataset, we left out parts of instructions containing \u201coracle\u201d or \"pythie\", jargon used by the French Wikipedia community to refer to people answering user in- quiries.\nAt this point, preliminary experiments revealed that many nouns considered MG were in fact con- textually ambiguous nouns not always referring to human beings. For instance, in French, \"fac- teur\" can refer to a mailman, but can also mean \"factor, cause\". Similarly, \u201cnavigateur\u201d can refer either to a (male) sailor or to an Internet browser. Including non-human-related occurrences to our analysis would obviously invalidate our approach. To remedy this problem, we added two more steps:\n1. Ambiguous Noun List Filtering. We specif- ically filtered a certain number of nouns found to be ambiguous during our preliminary tests16. We also scraped Wiktionary's list of 1,750 most common French words17 to a stan- dard one-word-per-line format and manually removed nouns referring to human beings to add to our filtering list. Note that only nouns with a primary non-human-related meaning were filtered; we did not filter nouns which are commonly used to either refer to human or non-human entities (such as the two pre- vious examples, \u201cfacteur\u201d and \u201cnavigateur\u201d), as we leave that for the LLM post-processing step (see below). Moreover, as we had pre- viously used data from Wiktionary to fill our MG dictionary, we removed all nouns from Wiktionary whose human-related definition in- dex was greater than 218. This helped reduce"}, {"title": "LLM HN Classification.", "content": "As a post- processing step, we leverage GPT-40 mini19 and in-context learning (Brown et al., 2020) to validate nouns considered HNs. GPT-40 mini was chosen for its great performance/cost ra- tio. The words and the context in which they appear are forwarded to the LLM. We use Py- dantic (Colvin et al., 2025) to constrain GPT- 40 mini's output to JSON format. We set max tokens to 500, temperature to 0. See Appendix G for prompting details.\nWe evaluate GPT-40 mini's HN classification. We extract 1,000 instructions from the hh_rlhf and French-Alpaca datasets (see Section 3.2) and apply our MG use analysis pipeline. We narrow down the analysis results to 500 to only retrieve con- texts where at least one HN has been found. Those results are then sent to GPT-40 mini for valida- tion. Two annotators classified the nouns in the 500 contexts as HN or non-HN. Cohen's kappa (Cohen, 1960), noted $k$, was used to calculate inter- annotator agreement, where $\u03ba = 0.944$. We built a reference annotation dataset and compared it with GPT-40 mini's output, where $\u03ba = 0.855$."}, {"title": "LLM Instructing and Answer Retrieval", "content": "To gauge the use of MG by LLMs, we query a set of LLMs of different sizes and types (both proprietary and local). We use a total of 6 different models. This includes 3 proprietary models (GPT-40 mini, Claude 3 Haiku and Gemini 1.5 Flash), and 3 lo- cal models (Llama 3 8B, Ministral 8B and Mistral Small 3) using OpenRouter. Figure 1 illustrates our methodology.\nWe retrieved instructions from our four filtered instruction/output datasets. In addition to previous filtering steps, we also removed instructions con- taining nouns from our MG dataset to avoid biasing the LLM towards generating MG.20\nWe collected 42,896 unique instructions in total. The number of instructions was narrowed down"}, {"title": "Measuring MG Bias", "content": "We created an analysis pipeline to automatically evaluate MG bias in human-written instructions and in human/LLM-generated responses after HN validation by GPT-40 mini. In a first step, we focus on responses and calculate the percentage of biased responses based on the presence of MG, both across responses with HNs as well as all responses. Then, in a second step, we calculate the ratio of MG terms to HNs in both human-written instructions and human-written/LLM-generated outputs. This ratio is noted \"M Score\". For each text, we divide the number of MG terms by the number of HNs found in the text. A score is computed only if the number of HNs is greater than zero. Then, for each dataset/model output, we compute an overall M Score (total number of MG nouns divided by the total number of HNs) as well as a mean M Score (sum of M Scores divided by the total count of M Scores). The overall score gives a dataset- wide measure of MG bias, while the mean score represents the average bias per text.\nIn addition to MG use, we also aim to see if LLMs' answers contain gender-fair language terms. Indeed, in response to MG, several writing tech- niques commonly referred to as \u201c\u00e9criture inclusive\" ('inclusive writing') were developed in French and other languages to either promote the visibility of women or reduce the prominent use of MG terms (Viennot, 2014). Consequently, we defined several language marker lists to capture such occurrences (see Appendix F).\""}, {"title": "Results", "content": "Figure 2 shows the percentage of MG in generic instruction outputs across models and datasets. We find that LLMs use MG in 39.5% of all their responses on average. In particular, we find that gpt4o_mini is generally the most MG-biased model, with 42.03% of its responses overall ex- hibiting at least one occurrence of MG. Conversely, llama is the least biased model overall with 36.61% of its total responses containing MG. Bias rate for human-written instructions (oracle and oasst2) is generally on par with that of LLMs, with the ex- ception of oasst2, which achieves an exception- ally low MG bias rate overall compared to other models (32.26%). When considering responses with HNs only, the average percentage of biased responses across LLMs amounts to 73.1%. minis- tral has the highest bais rate with 75.81% of its responses with HNs containing MG, while claude- 3-haiku achieves the lowest bias rate (69.13%).\nM Score results are displayed in Figure 3. Human-written responses datasets oasst2_assistant and oracle_assistant show the highest M Score (0.644 mean and 0.612 mean, respectively). When comparing LLMs only, M Score is generally cor- related with the bias rate: ministral has the highest mean M Score (0.603), followed by gemini (0.591) and gpt4o_mini (0.585). mistral-small and llama both have the lowest overall score (0.542 and 0.545 respectively). The lowest mean scores are achieved by mistral-small, claude-3-haiku and llama (0.557, 0.570 and 0.577 respectively).\nFigure 4 reveals that the top 5 MG human noun classes found across responses are \u201cprofession\", \"doer\", \"speciality\u201d, \u201crelationship\u201d and \u201cstatus\u201d. More precisely, nouns with the \"profession\" and \"doer\" classes are the most used (freq. between 120 and 44 for \"profession\u201d; between 57 and 24 for \"doer\"). Among LLMs, gemini, claude-3-haiku and gpt4o_mini (all proprietary models) are the ones that are the most using nouns from the \"profes- sion\" and \"doer\" MG human noun classes. We find that respectively 120, 119 and 114 unique human nouns with class \u201cprofession\u201d were used in the re- sponses of gemini, claude-3-haiku and gpt4o_mini (57, 55 and 51 for the \"doer\" class).\nAs seen in Figure 5, we find that LLMs use neutral words such as \u201cpersonne\u201d or \u201cindividu\u201d in 10.7% of their responses on average, with gpt4o- mini having the highest rate of responses with neu- tral words (12.8%), and claude-3-haiku the lowest (9.4%). For other gender-related language markers, their use is very sporadic, and even sometimes non- existent. The only model which shows a relatively notable use of markers other than neutral words is llama, with 0.7% of responses with feminine end- ings (the highest), 0.1% with inclusive greetings (same score as gpt4o-mini) and 0.1% with inclusive pairs (same score as mistral-small). Across LLMs, no model responses exhibit the use of neutral neo- pronouns such as \u201ciel"}, {"title": "Discussion", "content": "Surprisingly, the highest M Scores are to be found in human-written responses. It should however be noted that this score is calculated based on the num- ber of HNs and MG found in the responses, and that there is a large difference in the number of re- sponses analyzed (2,359 human-written responses on average vs. 4,888 LLM responses on average; more than the double). Looking at the bias rate in models and datasets, however, we find that LLMs are more prone to using MG in their responses.\nGenerally, llama is the most gender-fair model out of all tested models, considering its bias rate and its use of inclusive language markers. Given the results, and comparing with other models, we find it likely that extra care has been taken dur- ing the training of this model to promote language fairness. We give a few examples of gender-fair language outputs in Appendix H. While these re- sults definitely show a step in the right direction, efforts should be further intensified, as the percent- age of responses with gender-fair language remains extremely low. Local models in general appear to be slightly less biased compared to proprietary models, with claude-3-haiku being the least biased proprietary model.\nOur results for MG human noun classes use un- surprisingly show that the \u201cprofession\u201d class is the one whose nouns are the most used as MG. This is consistent with the methodology of psycholinguis- tics works focusing on MG as they prevalently use this type of noun in their experiments. Still, not all MG nouns have had their classes annotated for our analysis, so our findings need to be completed.\nThat no models appear to use neutral neopro- nouns is not that surprising given that those pro- nouns are rather novel and are still used by a small (but growing) section of the population. As a result, there may not be many occurrences in the train- ing data. Nonetheless, these neopronouns play a key role as they challenge the binary male/female gender dichotomy, and some people may not feel represented when simply reading masculine/fem- inine pair forms such as \u201cils et elles\u201d instead of \"iels\". It is thus important for LLMs to integrate such neopronouns to their responses, both to pro- mote gender diversity and to reflect new language usage trends.\nOverall, our results indicate that LLMs largely exhibit MG bias when generating responses to generic instructions. While previous research has clearly demonstrated that LLMs display gender bias in specific or constrained contexts, our results provide evidence that LLMs' responses are inher- ently gender-biased in normal use contexts. That such bias can be found in everyday, instruction- based interactions with LLMs is obviously concern- ing, as it participates in further reinforcing other existing gender bias, increases male mental rep- resentations and defies efforts to promote more inclusion and equality. These results show that fair- ness in language should be attentively considered when training LLMs in heavily gender-marked lan- guages. Linguistics gender bias may be reduced by filtering or rewriting texts that exhibit MG bias (Vanmassenhove et al., 2021; Veloso et al., 2023; Doyen, 2024; Lerner and Grouin, 2024), or using data augmentation to include texts featuring more diverse gender forms to the training data (Zmigrod et al., 2019). We leave these considerations for future work."}, {"title": "Conclusion", "content": "Our work expands on prior research by analyz- ing gender bias in everyday, instruction-based in- teractions with LLMs. We created a French HN database using HScorer and found that \u224839.5% of responses in average exhibit MG bias (~73.1% across responses with HNs), with LLMs generally avoiding gender-fair language. Our methodology is adaptable to other languages and LLMs. We hope this work encourages further research on MG bias beyond French and contributes to promoting gender-fair language in LLM-generated texts."}, {"title": "Limitations", "content": "Our work has several limitations. First, the French HN database that we created is not exhaustive and may have missing nouns. Similarly, even though we took care in not adding non-HNs to our database, a very small number of words not used to refer to human beings may be present in the data. Second, even though we took several steps to validate human nouns, both by pre-filtering our datasets and using an LLM for automatic verifica- tion (which we evaluated), many human nouns are polysemic, and some nouns may have incorrectly been detected as human nouns, or incorrectly left undetected. While we experimentally tried multiple human noun validation runs and did not see much change in the results, errors in validation might still have slightly impacted the results. Third, detection of masculine specific uses remains a challenging task. Even though we designed several filtering steps to remove instructions and responses which may contain specific uses of the masculine so as to not bias the results, a small portion might still have been included to the data. Finally, our analysis only focuses on a small set of local and proprietary LLMs. More models, and especially local models with higher parameter counts, should be analyzed to have a better overview of MG bias in LLMs."}, {"title": "Ethics Statement", "content": "The LLM instruction/output datasets were not fil- tered for harmful or malicious content. Similarly, LLM responses to generic instructions were not checked or filtered for such content. In addition, a very small portion of the HN database we cre- ated contains slurs and pejorative or discriminative terms used to refer to human beings. We deliber- ately did not filter these nouns, as they may be used in non-prejudicial contexts or with a non-harmful meaning. Finally, results of this work might be used in a counteractive way to increase gender bias in LLMs, for instance by removing occurrences of inclusive language markers from the training data or by increasing the number of MG occurrences in responses to generic instructions."}, {"title": "HScorer Scoring Functions", "content": "This section details the scoring functions used as features of LR and XGBoost models, along with the FastText vector for word embedding represen- tation. We calculate four different scores to be used: WordNet Hypernym Score (H), WordNet Definition Score (D), FastText Score (F) and Suf- fix Score (S). Let W be a word to be classified. The final feature vector for word W is the concate- nation:\n$\u0e1a\u0e35(W) = [H(W)\u2295D(W)\u2295F(W)\u2295S(W)\u2295vw]$\nLet H(W) = (hs, ns) where:\n$hs = \\frac{\\Sigma_{p \\in P} \\Sigma_{y \\in p} 1_h(y)}{|P|}$\n$Ns = \\frac{\\Sigma_{p \\in P} \\Sigma_{y \\in p} 1_n(y)}{|P|}$\nWhere P is the set of hypernym paths for W, $1_h(y)$ is the indicator function for human hyper- nyms, and $1_n(y)$ is the indicator function for non-human hypernyms.\nLet $I_h$ be a set of human indicator words and $I_n$ a set of non-human indicator words (see com- plete list in Appendix). Those indicator words are manually defined English words related to human (\"someone\", \"person\", \"who\") or non-human (\u201cob- ject\", \"plant\u201d, \u201cchemical\u201d) entities, and are used to search in WordNet definitions. Since WordNet def- initions are universally in English, we used English words. See our GitHub repo for the full list. Let D(W) = (hd, nd) where:\n$h_d = \\frac{\\Sigma_{s \\in S} \\Sigma_{i \\in I_h} f(i, d(s))}{|S|}$\n$n_d = \\frac{\\Sigma_{s \\in S} \\Sigma_{i \\in I_n} f(i, d(s))}{|S|}$\nWhere S is the set of synsets for W and f(i, d) counts occurrences of indicator i in definition d.\nWe define a set of human prototypes $P_h$ and non-human prototypes $P_n$ (see complete list in Ap- pendix). Those prototype words are manually de- fined French words prototypically related to human (\u201cpersonne\u201d (person), \u201chomme\u201d (man), \u201cfemme\u201d (woman)) or non-human (\u201cobjet\u201d (object), \u201cchose\u201d (item), \u201cmachine\u201d (machine)) entities, and are used to calculate semantic similarity. The contents of the prototype sets are stricter than those of indicator sets. The complete list of prototypes words is avail- able on GitHub. Let F(W) = (hf, nf) where:\n$h_f = \\frac{\\Sigma_{p \\in P_h} Cos(v_W, v_p)}{P_h}$\n$n_f = \\frac{\\Sigma_{p \\in P_n} COS(v_W, v_p)}{P_n}$\nWhere $v_W$ is the 300-dimension FastText (Bo- janowski et al., 2016) vector for word W and cos(va, vb) is the cosine similarity between vectors a and b.\nFinally, S(W) = 1 if W ends with one of the suffixes \u00a7 \u2208 S (see our GitHub repo for the com- plete list), else S(W) = 0."}, {"title": "HScorer Hyperparameters", "content": "Logistic Regression (LR)\npenalty = 11\nsolver = saga\nC = 100\nXGBoost\nbooster = gbtree\nlearning_rate = 0.22394632872649503\nmax_depth = 10\nmin_child_weight = 78\nsubsample = 1\ncolsample_bytree = 1\nn_estimators = 912\ngamma 0\nreg_alpha 0\nreg_lambda = 0\nobjective = binary:logistic\nearly_stopping_rounds 20\nrandom_state = 42\ntree_method = gpu_hist\nn_jobs = 24\nTransformer\ntrain_batch_size = 16\neval_batch_size = 16\neval_strategy = epoch\nsave_strategy = epoch\nnum_train_epochs = 5\nweight_decay = 0.061748150962771656\nlearning_rate = 8.497821083760116 \u00d7 10-6"}, {"title": "Narrowed Instruction Dataset Count Details", "content": "Table 1: Narrowed instruction dataset counts by original dataset\nDataset name\nOriginal count\nNarrowed count\nalpaca\n29,179\n6,803\nhh_rlhf\n10,806\n2,520\noracle\n2,600\n605\noasst2\n311\n72"}, {"title": "Number of Non-Specific LLM Responses", "content": "Table 2: Number of LLM responses without specific masculine markers by model\nModel\nCount\nclaude-3-haiku\n7,221\ngemini\n5,224\ngpt4o-mini\n4,873\nministral\n4,660\nllama\n3,920\nmistral-small\n3,432"}, {"title": "MG Human Noun Classes", "content": "Table 3: Nouns and pronouns used to perform definition search\nNoun\nEnglish"}]}