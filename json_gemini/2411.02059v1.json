{"title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration", "authors": ["Aofeng Su", "Aowen Wang", "Chao Ye", "Chen Zhou", "Ga Zhang", "Guangcheng Zhu", "Haobo Wang", "Haokai Xu", "Hao Chen", "Haoze Li", "Haoxuan Lan", "Jiaming Tian", "Jing Yuan", "Junbo Zhao", "Junlin Zhou", "Kaizhe Shou", "Liangyu Zha", "Lin Long", "Liyao Li", "Pengzuo Wu", "Qi Zhang", "Qingyi Huang", "Saisai Yang", "Tao Zhang", "Wentao Ye", "Wufang Zhu", "Xiaomeng Hu", "Xijun Gu", "Xinjie Sun", "Xiang Li", "Yuhang Yang", "Zhiqing Xiao"], "abstract": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI applications, presenting vast new opportunities across industries. Yet, the integration of tabular data remains notably underdeveloped, despite its foundational role in numerous real-world domains.\nThis gap is critical for three main reasons. First, database or data warehouse data integration is essential for advanced applications; second, the vast and largely untapped resource of tabular data offers immense potential for analysis; and third, the business intelligence domain specifically demands adaptable, precise solutions that many current LLMs may struggle to provide.\nIn response, we introduce TableGPT2, a model rigorously pre-trained and fine-tuned with over 593.8K tables and 2.36M high quality query-table-output tuples, a scale of table-related data unprecedented in prior research. This extensive training enables TableGPT2 to excel in table-centric tasks while maintaining strong general language and coding abilities.\nOne of TableGPT2's key innovations is its novel table encoder, specifically designed to capture schema-level and cell-level information. This encoder strengthens the model's ability to handle ambiguous queries, missing column names, and irregular tables commonly encountered in real-world applications. Similar to the VLMs, this pioneering approach integrates with the decoder to form a robust large multimodal model.\nWe believe the results are compelling: over 23 benchmarking metrics, TableGPT2 achieves an average performance improvement of 35.20% in the 7B model and 49.32% in the 72B model over prior benchmark-neutral\u00b2 LLMs, with robust general-purpose capabilities intact.\nWe release an open-source repository (Section 2) that includes both the model and a comprehensive agent workflow, along with a subset of RealTabBench. This release aims to foster further exploration and application in real-world data-driven and BI production environments.", "sections": [{"title": "Introduction", "content": "The emergence of large language models (LLMs) has driven remarkable progress in artificial in-telligence (AI), reshaping its applications across various domains. Models like ChatGPT [5] have enhanced the machine's ability to comprehend and produce human-like language, opening new possibilities in diverse fields."}, {"title": "Motivation behind TableGPT2", "content": "The key motivation for proposing TableGPT2 is the need to address the limitations of current large language models (LLMs) in real-world, data-driven applications. No system operates in a vacuum, yet many LLMs are designed to function in an end-to-end fashion without external data integration. This approach is inherently flawed. For example, relying on an LLM to make stock investment recommendations without access to real-time market data is impractical. Similarly, an LLM making healthcare diagnoses without direct access to patient records or analyzing financial statements without the full dataset would lack the necessary depth and accuracy.\nEven when LLMs are integrated with external data sources, such as databases, their performance is often suboptimal. One approach involves using tools like natural-language-to-sql (NL2SQL) [6], which provide schema metadata, table descriptions, and sample values to bridge the gap. However, as mentioned earlier, this method falls short in complex scenarios. Additionally, thanks to innovations like longer context windows or novel architectures, some models attempt to handle large datasets within a single context. This, too, is inefficient, as digesting detailed information from each cell of a large database or Excel file exceeds the capacity and utility of the extended context window, which was primarily designed for textual data.\nThus, there is a pressing need for new techniques that move beyond the vacuum operation paradigm. This is the motivation behind the development of TableGPT2-a model designed to integrate and process tabular data directly and efficiently, overcoming the inherent limitations of current LLMs, especially towards production-level deployment.\nA key factor in the success of large language models like GPT-4 [7] has been the availability of vast datasets sourced from the open web. GPT-4, for instance, was trained on hundreds of billions of tokens, drawn from a wide range of publicly available text data. This abundance of text allowed for effective scaling, enabling the model to capture complex linguistic patterns and perform well across various tasks.\nSimilarly, vision-language models (VLMs) have thrived by leveraging large-scale datasets that pair images with descriptive text [8]. These models, such as CLIP [9] and DALL\u00b7E [10], succeed by aligning visual data with language, creating robust representations that allow them to excel in multimodal tasks. The richness and scale of both image and text data have been crucial to this success, demonstrating the power of large, diverse datasets.\nDespite the significant focus on text and visual data, tabular data is equally abundant and critical across industries. It's estimated that over 70% of global data is in structured tabular form, whether stored in databases or spreadsheets. This vast resource points to the potential for developing a large-scale tabular model capable of harnessing the same scaling laws applied to text and images. By leveraging massive datasets of tabular data, along with schema metadata, we aim to explore whether these data formats can be modeled effectively, leading to functional and powerful models for business intelligence and other applications.\nThis approach aligns directly with our novel encoder design, which focuses on modeling the structure and content of tabular data. The tabular data encoder within TableGPT2 enables it to capture schema-level and cell-level information, opening the door for large-scale, data-driven performance improvements, similar to those observed in text and vision-based models.\nBusiness intelligence (BI), in particular, stands out as an area primed for innovation based upon LLMs. Traditional BI systems typically depend on fixed queries, static data structures, and inflexible interaction methods, which restrict their adaptability to dynamic business"}, {"title": "Upon the Previous Version of TableGPT", "content": "In the initial version of TableGPT [18], we introduced approaches such as structured Domain-Specific Languages (DSLs) and specialized table encoder to manage complex table-based queries. These tech-niques showed promise in structured data handling but also highlighted areas for further refinement. Our latest iteration, TableGPT2, advances these foundations with substantial improvements. We have scaled up data and training protocols, carefully redesigned each component, and introduced additional technical enhancements to improve robustness, broaden applicability, and optimize performance in diverse BI tasks.\nTo provide context, TableGPT2 is a large-scale multimodal model available in two configurations\u2014a 7-billion-parameter model and a 72-billion-parameter model\u2014both based on the Qwen2.5 model family. Training encompassed over 86 billion tokens for continual pretraining (CPT), more than 437.5K table-language interleaved samples for encoder training and over 2.36M high quality query-table-output tuples being utilized for supervised fine-tuning. This scale of data is unprecedented in related research, ensuring that TableGPT2 meets the rigorous demands of modern applications involving structured or tabular data."}, {"title": "General Introduction of TableGPT2", "content": "TableGPT2's language framework is built on the Qwen2.5 [1] architecture. It underwent continual pretraining (CPT), supervised fine-tuning (SFT), and an agent framework for production-level ability. These steps differ from traditional LLMs as our pretraining and fine-tuning place a stronger emphasis on coding, multi-turn reasoning, and tool usage. These characteristics ensure the model is not only adept at natural language processing but also highly capable of handling the intricate requirements of table-related tasks.\nIn addition, we preliminarily explored the possibilities of multimodal alignment for tabular data. Specifically, TableGPT2 innovatively incorporates a separate modality module specifically for reading and interpreting tabular data. Similar to vision-language models (VLMs), TableGPT2 involve a tabular data reading module that generates specialized embeddings concatenated with the token embeddings from textual input. This additional module allows TableGPT2 to better capture the structure and semantics of tabular data, enabling more accurate tabular comprehension in complex BI scenarios.\nFor the goals of TableGPT2, we first focused on enhancing the model's coding and reasoning abilities through Continual Pretraining (CPT) [19]. Specifically, 80% of the CPT data was dedicated to well-commented code, aligning with approaches of DeepSeek-v2 [20], ensuring robust coding capabilities. To complement this, we also curated substantial reasoning"}, {"title": "Disclaimer of Open Source", "content": "Our open-source repository comprises two main components: (i) a standalone decoder opened weight on Hugging Face\u00b3 that achieves superior performance on table- and BI-related tasks without compromising general coding and conversational capabilities, as discussed in Section 7, and (ii) a holistic agent workflow, detailed in Section 6, accessible via a GitHub repository4. Additionally, a portion of RealTabBench data is included in the repository.\nBased on this, we offer a few disclaimers:\n1. The TableGPT2 decoder functions effectively as a standalone model, particularly in scenarios without ambiguous queries, missing column names, or irregular tables (as in most existing benchmarks).\n2. The encoder-decoder configuration is recommended for more complex, real-world production scenarios, as the encoder's pretraining enhances table comprehension and robustness.\n3. The encoder model is currently under preparation due to engineering considerations, includ-ing its complete integration with DeepSpeed for distributed training and vLLM for efficient inference.\n4. For RealTabBench, only a subset of the dataset is publicly released, with the remainder reserved for future evaluation as part of a comprehensive public benchmark to rigorously assess LLM performance in realistic BI environments."}, {"title": "Continual Pretraining from Qwen", "content": "We selected Qwen-2.5 [1], the latest generation of open-sourced LLMs with strong multilingual support, as the base model for TableGPT2. To better align the model with the specific requirements of business intelligence, we initiated a continual pretraining (CPT) process.\nCompared to general usage, LLM for BI, or database-involved applications, often requires a distinct set of skills, with particular emphasis on tasks like data comprehension, complex analysis, and code generation. Recognizing these needs, we incorporated additional data types such as college-level textbooks, data analysis materials, and codes into our CPT process. This domain-specific data was supplemented with general corpora to ensure that the model retained broad language capabilities while gaining a deeper understanding of the targeted tasks."}, {"title": "Data Collection", "content": "Given the immense importance of coding abilities for BI applications, our data curation process is divided into two main categories: coding data and general data.\nFor coding data, we sourced primarily from StackOverflow (stack-v2) and GitHub, two of the most comprehensive repositories of code examples and developer discussions. After gathering the data, we performed a deduplication process to eliminate overlapping code and ensure that the dataset remained diverse and non-redundant. For general data, we selected textbooks from fields such as finance, mathematics, and biology, ensuring a well-rounded representation of domain-specific knowledge. In addition to these, we incorporated data analysis materials (from the source including Kaggle [26]) to further refine the model's understanding of data-driven decision-making processes.\nSpecial emphasis was placed on selecting data that included numbers, code snippets, and tables, as these elements are critical in the BI space. It ensures that TableGPT2 can efficiently handle tasks that require reasoning across both textual and structured data formats."}, {"title": "Data Curation", "content": "We adopted a two-stage filtering flow, including document-level and token-level filtering to ensure the data quality of the CPT procedure.\nThe primary objective of doc-level filtering is to remove low-quality data and ensure that only high-quality samples are used in the pretraining process. For general data, we employed methods similar to those used in C4 [27] and RefineWeb [28], focusing on removing irrelevant or poor-quality content. We omit the detailed steps for simplicity, but it is important to note that keyword filtering was also applied to exclude sensitive and toxic corpora.\nFor coding data, we implemented a more complex selection scheme. After applying methods similar to those used in StarCoder2 [29], we tagged the coding data across 54 distinct categories (as shown in Table 2) to ensure that the data spans various programming domains and scenarios, covering a broad range of code quality and use cases. We utilized tools like FastText [30], which were distributed across a Ray thread pool for efficient processing. The filtering process was finalized through a set of heuristics, informed by the computational results from these tools."}, {"title": "Semantic Table Encoder", "content": "The standard approach for utilizing LLMs in table-related tasks involves serializing tables into formats like Markdown, HTML, or XML to make them compatible with text-based inputs [31, 32]. While larger context windows enable LLMs to handle more table content, a fundamental gap remains between language-based models and structured table understanding. This gap is evident in three key aspects: 1) Structural incompatibility: the bi-dimensional structure of tabular data is inherently"}, {"title": "Model Architecture", "content": "Our design for the Semantic Table Encoder consists of three main components: 1) the semantic Table Encoder, which generates structure-enriched semantic embeddings from the input tables; 2) the table-language Adapter, which aggregates the encoder's output embeddings and aligns them with the textual features; and 3) the backbone LLM, which utilizes these high-level table representations to perform reasoning within the joint table-text space.\nThe design of the Semantic Table Encoder is tailored to the unique structural characteristics of tabular data. Given a table $T = [C_{11}, ..., C_{1n}; ..., C_{mn}]$ with m rows and n columns, where $c_{ij}$ denotes the cell content in the i-th row and j-th column, we first apply a sentence transformer \u03a6 to obtain compact semantic embeddings for each cell:\n$E(T) = [\\Phi(c_{11}), ..., \\Phi(c_{mn})] \\in \\mathbb{R}^{m \\times n \\times d},$\\nwhere d is the dimension of each cell embedding. These embeddings are then processed through a stack of bi-dimensional attention modules [36, 37], where they interact with other relevant cells to capture global structural table semantics:\n$E'(T) = 2D-Attn(E(T)) \\in \\mathbb{R}^{m \\times n \\times d}$\nWithin each module, BERT-style bi-directional attention [38] is alternated along rows and columns, allowing the model to capture both intra-column distributional properties and inter-column rela-tionships. To maintain the permutation invariance of (relational) tables, positional embeddings are intentionally excluded from these bi-dimensional attention modules.\nTo generate more compact table representations, we introduce an Adapter g that aggregates cell-level embeddings at the column level and aligns them with the LLM's text embeddings. The Adapter g performs Q-Former-style cross-attention [24, 39, 40] between k learnable queries and the cell embeddings from each column, transforming tables with m rows into fixed-length column representations of size k, matched to the LLM's embedding dimension d'. The resulting embedding for the i-th column is computed as:\n$C(T)_i = g([E'(T)_{1i},..., E'(T)_{mi}]) \\in \\mathbb{R}^{k \\times d'}.$\\nIn this way, for an entire table, we obtain a compact representation $C(T) \\in \\mathbb{R}^{n \\times k \\times d'}$ that en-capsulates the global structure-aware table semantics and is directly compatible with the LLM for downstream tasks.\nTo seamlessly incorporate column embeddings with other textual table metadata (e.g., schemas and example cell values), we introduce a hybrid table representation that unifies these elements for efficient integration:"}, {"title": "Training", "content": "Similar to pre-training in other modalities, our objective in this stage is to equip the encoder with foundational tabular knowledge using a large corpus of raw tables. Inspired by how humans naturally interpret tables - identifying patterns within columns and distinguishing differences between them - we leverage contrastive learning to explicitly guide the Table Encoder in capturing intra-column semantics while differentiating across columns in a schema-independent manner. As shown in Figure 2, we first apply random row sampling (without replacement) on each table $T_i$ within the mini-batch, creating two snapshots, $S_i$ and $S'_i$, which share the same schema but varying cell values. The Table Encoder is then used to generate an embedding pool P consisting of column embeddings from each snapshot in the mini-batch. Then we perform contrastive learning, where positive pairs are formed by the embeddings from the same columns across the two snapshots. The contrastive loss [41, 42] is defined as:\n$L_{cont}(T, P) = \\frac{1}{\\mid P \\mid} \\sum_{e \\in P} - log \\frac{exp(e^T e^+/\\tau)}{\\sum_{e' \\in P \\\\ {e}} exp(e^T e'/\\tau)},$\nwhere $e^+$ is the embedding of the same column as e but from a different snapshot, and \u03c4 is the temperature. This approach encourages the model to learn distinct and context-sensitive column embeddings, capturing the unique semantics of each column within a given schema.\nTo align tabular and textual features, we construct a set of multi-task table-language interleaved datasets and perform joint training of the Table Encoder and Adapter alongside a designated backbone LLM. Specifically, we create two synthetic interleaved datasets: \u2460 column prediction - predicting which column a given cell value belongs to, and \u2461 cell prediction - identifying which cell value corresponds to a given column, using pre-defined instruction templates and sampling questions and answers from raw tables without requiring manual annotations. Additionally, to further increase data diversity, we adapt three existing tabular datasets: FetaQA [43], WikiTableQuestion [44], and ToTTo [45], for augmented tasks: \u2462 question generation \u2013 generating a question based on a given answer from a specific table, \u2463 table titling - creating a brief title for a given table, and 5 row summarization - summarizing the content of a specific row. Each of these tasks is designed to maintain a high-level abstraction by minimizing dependency on specific cell values, promoting semantic table understanding. In total, we have collected 437K samples for feature alignment.\nTasks above all require the model to leverage the column embeddings to identify distributional patterns and inter-column relationships, facilitating a deeper integration and mutual alignment of the tabular and textual representations.\nTo further enhance the model's instruction-following abilities for optimal performance on downstream tasks, we conduct joint supervised fine-tuning of the Semantic Table Encoder, Adapter, and LLM using a curated dataset for code generation. Relevant details will be elaborated on in Section 5.4."}, {"title": "Supervised Fine-tuning over Tabular Data", "content": "Recall that our aim is to handle flexible and dynamic data analysis tasks. We believe that supervision of processes involving the selection, analysis, computation, and manipulation of table contents is crucial. However, existing table analysis datasets fall short of providing such process-level supervision. A significant portion of TableQA datasets only contain table-question-answer pairs, which limits the activation of large models' capabilities for quantitative table operations. Meanwhile, available NL2SQL datasets are either too simplistic in question complexity or lack sufficient data volume. To address this gap, in addition to gathering existing tabular datasets, we have collected an additional batch of data containing 115K tables along with 479K samples. Particularly, our approach involves several key types of data: general corpora, coding corpora, and table-related query-answer pairs.\nIn addition to these data types, our SFT process also incorporates both single-turn and multi-turn interactions, particularly for table-related tasks. This reflects real-world scenarios where multi-turn dialogue is often necessary to retrieve or analyze tabular data over multiple exchanges."}, {"title": "Table Collection", "content": "The tables to be collected are central to improving the performance of TableGPT2. In general, the tables we collect cover a variety of domains, including but not limited to public government files, academics, manufacturing, finance, education, and healthcare.\nThe collected tables involve both regular and irregular structures. Regular tables are generally organized in a way that can be stored in databases or read into frameworks like pandas dataFrame. In contrast, irregular tables often contain merged cells or exhibit hierarchical structures. These irregular tables are often found in Excel files and tend to be smaller, with fewer columns and rows. Below is a summary of the types of table sources, their features, and relevant use cases:\n\u2022 database tables: These tables typically come from databases and involve multi-table fields, rows, columns, and large numeric data. These are often used in business scenarios involving frequent data queries. Example sources include public datasets such as MySQL's employee dataset, PostgreSQL's Chinook, and DBI documents.\n\u2022 web page tables: These tables mainly record information within a single web page, such as data from official websites or academic journals. They are usually simple in form and include relevant contextual data.\n\u2022 excel tables: These standalone files (e.g., .xls, .xlsx, .csv) contain various structured and unstructured information. These tables are often sourced from government data, financial reports, and business reports.\n\u2022 academic task tables: These tables contain data used in research, often in structured forms suitable for TableQA or NL2SQL tasks. Sources include public datasets like WikiTable-Questions and other academic studies.\n\u2022 special format tables: These tables have specific formats and usages, such as invoices, bills, and receipts, often used in online reporting systems and specialized software-generated documents.\n\u2022 pre-test task tables: These tables are used for forecasting, prediction tasks, or scientific projects. Sources include Kaggle [26], UCI Machine Learning Repository [47], and Tianchi datasets [48].\nThe collection processes generally gather more than 160 sources, and the number of tables we collected varies from source to source."}, {"title": "Query Collection Based on Tables", "content": "Generating meaningful queries based on tables is a non-trivial task, though it is crucial for effective supervised fine-tuning of TableGPT2. The goal is to provide a human or machine labeler with a table and prompt them to ask relevant and meaningful questions. These queries should not only reflect the data in the table but also be diverse in nature, ensuring they cover different types of instructions and are non-repetitive.\n8 classes of queries. To ensure comprehensive coverage of most Business Intelligence questions, we categorize them into 8 different categories: retrieval, insertion, deletion, modification, computation, statistics, visualization, and fuzzy queries.\nThe last type, fuzzy queries, targets questions that cannot be fully answered in a single conversation round. In such scenarios, TableGPT2 is designed to respond with follow-up questions to gather additional necessary information. For example, if a user asks: \"What are the top-performing products?\", TableGPT2 may not have enough context about which metric (e.g., sales, ratings, or profit margin) the user considers as a performance indicator. In this case, the model would respond with a clarifying question like: \"Would you like to define performance by sales, profit, or customer reviews?\""}, {"title": "Answer Generation", "content": "Given a provided table and a corresponding query, the main objective is to derive an accurate and relevant answer. Our approach follows a synthesize-then-refine paradigm to ensure high-quality responses. We focus on two important aspects: single-turn and multi-turn answer generation.\nSingle-turn answer generation. As a simpler process, a single-turn answer does not require reflection on code execution errors, making it a good starting point for basic query handling. The aim of synthetic single-turn answer generation is to produce query-answer pairs where the code in the answers is executable and, ideally, as accurate as possible.\nTo achieve this, we use over 10 different prompt templates and, for each selected table and generated query set, randomly combine a template with the table information and a random query. One example template is:"}, {"title": "Overall SFT Data Composition", "content": "We iterated through our SFT (Supervised Fine-Tuning) data collection and refinement for 18 rounds, adjusting it in tandem with our training cycles. The final version of the SFT data composition reflects a comprehensive mixture of various query types, agent calls, programming languages, and table tasks. The overall data volume amounts to approximately 2.36M query-answer samples.\nOverall, our SFT dataset includes table-specific tasks such as code generation (Python and SQL), table querying, data visualization, statistical testing, and predictive modeling. Additionally, it spans a wide variety of tasks like table comprehension, table generation, missing value imputation, and table-based question-answering, covering almost all stages of table usage. The input formats were combined with random arrangements of table metadata like field descriptions, schema information,"}, {"title": "Data Cleaning", "content": "The data cleaning phase is crucial and continues throughout the training process, ensuring the quality and relevance of the data used. The goal is to remove irrelevant, inconsistent, or erroneous data while preserving the core information that enhances the performance of the model for the wide application scenarios. Here's a breakdown of the most critical aspects."}, {"title": "Table Selection and Column Manipulation", "content": "To ensure high-quality and consistent data for the BI application scenarios, we apply a systematic set of rules for table filtering and selection. Below are the key filtering rules we follow:\n\u2022 remove horizontal tables: Any tables structured horizontally are transposed or removed to maintain consistency in data orientation.\n\u2022 remove duplicate columns or columns ignoring case sensitivity: Columns with identical names or names that differ only by case (e.g., \u201cColumn\u201d and \u201ccolumn\u201d) are merged or discarded.\n\u2022 remove columns containing only underscores: Columns that only contain underscores or similar meaningless data are removed.\n\u2022 remove columns where field content exceeds 100 characters: Columns containing overly long text fields, which may be irrelevant or quite uncommon realistically, are filtered out.\n\u2022 remove rows or columns with more than 30% NaN Values: Any tables where a significant portion of the data is missing are filtered out to ensure data completeness.\n\u2022 remove tables with fewer than 5 rows or 2 columns: Tables with insufficient data (less than 5 rows or 2 columns) are considered too sparse and are discarded.\n\u2022 remove columns where the first value directly matches the column name: When a column's first entry repeats its own column name, it's a sign of an error or redundant information, so the column is removed.\n\u2022 more to be omitted in this report...\nThese rules ensure that tables used in our model training are relevant, clean, and structured, optimizing the quality of the data pipeline for better performance."}, {"title": "Table-query-answer Tuple Cleaning", "content": "The tuple cleaning process follows a systematic pipeline to ensure the quality and accuracy of the table-query-answer data used in our model training. The pipeline is structured as follows:"}, {"title": "Data Augmentation", "content": "Data augmentation plays a critical role in improving the robustness of our training datasets. We employ several strategies to diversify the input and enhance the model's ability to handle varied data scenarios. The key augmentation methods are as follows:\n\u2022 diverse table serialization input formats: We utilize multiple formats for serializing table input data, including html, xml, df.markdown(), df.to_string(), json, and various custom table serialization methods. These are randomly combined with table field descriptions, schema information, and value enumeration data, resulting in more than 20 different ways of constructing the input table-info.\n\u2022 query augmentation: In BI-related queries, a significant number of queries tend to be ambiguous. To ensure that our model handles query ambiguity effectively, we aim to avoid direct mapping between fields in the query and table names or field names. Specifically, we ensure that quoted field or table names are removed from queries. Experimental results show that this method significantly reduces instances of key-errors in TableGPT2, especially in realistic BI cases."}, {"title": "SFT with the Encoder", "content": "Our supervised fine-tuning (SFT) process is generally divided into two primary scenarios. The first scenario involves standard SFT with input-output data, which covers most general-purpose tasks that do not involve structured tables. These tasks typically involve common textual inputs and outputs, where the focus is on aligning user queries with generated responses. The details of these general types of datasets can be found in Table 3.\nThe second scenario deals with table-related data, where we combine the decoder and encoder using a query-based transformer mechanism inspired by Q-Former [24]. This mechanism is specifically trained on table-query-answer tuples, which allows the model to process structured data efficiently. In this setup, the encoder processes table information, generating embeddings that are then combined with the query, aiding the decoder in producing accurate results.\nNoticed that the encoder are first pretrained in a self-supervised manner prior to the SFT process, focusing on tasks such as schema inference, which helps the model understand general table semantics more effectively. While these two SFT scenarios (general and table-related) are distinct in their approach, they are combined during the training process. This combined approach allows the model to handle both free-form text queries and structured table data, making it flexible and adaptable across various BI tasks while keeping the general abilities in TableGPT2. By dynamically switching between these two modes, the model can effectively process both unstructured and structured information depending on the task at hand.\nFinally, we experimented with several training configurations to optimize the SFT process. In some cases, we froze the decoder and only trained the encoder and projection layers. In other setups, we trained the entire model end-to-end, which allowed us to learn simultaneously from both general and table-related data. These different training configurations provided valuable insights into how to best balance the model's training between its encoder and decoder components."}, {"title": "Agent Framework", "content": "To seamlessly integrate the TableGPT2 model with enterprise-level data analysis tools, we designed the agent workflow runtime framework. This framework consists of three core components: runtime prompt engineering, code sandbox, and an independent agent evaluation module. Together, these elements enhance the agent's intelligent data analysis capabilities and reliability, while ensuring that the generated code is executed safely and evaluated automatically. This structure allows for efficient management and monitoring of the agent's overall performance.\nThe agent workflow runtime aims to automate support for complex data analysis tasks by integrating data warehouse connection, visualization, and analysis functions into a cohesive workflow. The core execution process, as shown in the Figure 3, is divided into the following key steps:\n\u2022 input table normalization (optional): This step ensures that the input dataset is in a format easily processed by TableGPT2. Although TableGPT2 is designed to handle various types of tabular data, certain formats such as those with merged cells or multiple head-ers can present challenges for LLM interpretation. In such cases, the dataset undergoes normalization, which involves two key processes:\n* table transformation: This process identifies issues such as merged cells or multiple headers in the input dataset. When such features are present, the dataset is adjusted ac-"}, {"title": "Prompt Engineering", "content": "The prompt engineering primarily involves the design of effective system instructions and the integration of RAG infrastructure."}, {"title": "System Instruction", "content": "The system instruction is used to define the agent's role in the dialogue, the scope of the analysis, and how to organize the format of the answers. In designing the System Instruction, we adopted a"}, {"title": "Retrieval-Augmented Generation", "content": "Enterprise-level data warehouses often contain petabytes (PB) of data and complex schemas with thousands of tables. Even though traditional large language models (LLMs) possess large context-handling capabilities, they are still unable to process such vast amounts of data at once. Inputting excessive context into the model can lead to distracted attention, significantly affecting both the accuracy and the response speed of the model. Therefore, RAG technology [25] plays a crucial role in such scenarios.\nWhen dealing with large datasets and complex database schemas, our RAG engine first retrieves enterprise-specific data (such as database schemas and business documents) and selects highly relevant contextual information for the current analysis task through two stages: embedding and reranking. This avoids unnecessary information overload. The agent then generates code or interprets the data based on this refined context, focusing on the most important data and analysis goals within the limited context window.\nIn data analysis tasks, RAG is applied in two primary scenarios:\n\u2022 data schema retrieval and context aggregation: when facing multiple tables or complex database structures, the agent uses RAG to retrieve table structures, field information, and relationship dependencies that are relevant to the current analysis task, and then generates the appropriate code. This approach effectively reduces unnecessary context loading, allowing the agent to quickly pinpoint data sources and generate more accurate code.\n\u2022 domain-specific knowledge retrieval and analysis: in scenarios where the analysis in-volves specific business contexts (e.g., interpreting enterprise documents or historical analy-sis reports), RAG retrieves relevant document fragments and merges them with the current task context. This enables the agent to leverage domain-specific knowledge, improving the professionalism and credibility of its responses.\nBy integrating RAG technology, the agent can intelligently understand context when faced with complex data warehouses or multi-dimensional business needs. This allows it to generate analysis code or interpret data in a way that aligns with business requirements, significantly improving its usefulness and reliability in enterprise-level data environments."}, {"title": "Code Sandbox", "content": "In the agent workflow, the code generated by the agent needs to be executed in a safe, isolated, and controlled environment. To ensure that the execution of the code does not pose a security threat to the main system and can be quickly isolated and recovered in case of an error, we designed and implemented a specialized code sandbox environment. The code sandbox has the following core design objectives:\n\u2022 security: prevent unauthorized operations on the host system or other critical services by the generated code.\n\u2022 isolation: ensure that each code execution environment is independent and does not share any system resources or files.\n\u2022 easy recovery: in case of errors or improper behavior, the environment can be quickly destroyed and recreated without affecting other running tasks.\nWe adopt a containerized approach to package the iPython executor as a code sandbox, using Kubernetes as the management platform for these sandboxes. Each piece of code generated by the agent is executed within an independent sandbox, providing the following safeguards:\n\u2022 isolated execution environment: each container has its own isolated file system, process space, and network space, ensuring no direct impact on the host system.\n\u2022 resource quota control: Kubernetes' resource quota functionality (such as CPU and memory limits) is used to prevent any single task from consuming excessive resources.\n\u2022 timeout execution strategy: all code execution tasks are set with a maximum execution time (timeout), after which the environment is automatically terminated and destroyed.\nThe data analysis tasks typically consist of multiple steps, with potential dependencies between each step. Therefore, the lifecycle of a code sandbox must cover the entire duration of a complete data analysis task. However, enterprise-level data analysis tools often handle large numbers of independent analysis tasks, each executed in its own sandbox, which can consume significant cluster resources. To ensure efficient utilization of resources, we designed an automatic destruction mechanism for the code sandbox. When a sandbox has been idle for an extended period, the container is automatically destroyed to free up resources.\nWe have made the entire agent workflow open-source on our Github repository. See Section 2."}, {"title": "Evaluation of TableGPT2", "content": "To effectively evaluate TableGPT2, we constructed a comprehensive benchmark for table-related tasks, integrating a diverse range of existing datasets along with a newly collected, realistic"}]}