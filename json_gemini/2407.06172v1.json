{"title": "On Speeding Up Language Model Evaluation", "authors": ["Jin Peng Zhou", "Christian K. Belardi", "Ruihan Wu", "Travis Zhang", "Carla P. Gomes", "Wen Sun", "Kilian Q. Weinberger"], "abstract": "Large language models (LLMs) currently dominate the field of natural language processing (NLP), representing the state-of-the-art across a diverse array of tasks. Developing a model of this nature, from training to inference, requires make numerous decisions which define a combinatorial search problem. For example, selecting the optimal pre-trained LLM, prompt, or hyperparameters to attain the best performance for a task often requires evaluating multiple candidates on an entire test set. This exhaustive evaluation can be time-consuming and costly, as both inference and metric computation with LLMs are resource-intensive. In this paper, we address the challenge of identifying the best method within a limited budget for evaluating methods on test examples. By leveraging the well-studied multi-armed bandit framework, which sequentially selects the next method-example pair to evaluate, our approach, combining multi-armed bandit algorithms with low-rank factorization, significantly reduces the required resources. Experiments show that our algorithms can identify the top-performing method using only 5-15% of the typically needed resources, resulting in an 85-95% reduction in cost.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable proficiency in diverse tasks such as question answering, machine translation and mathematical reasoning [5, 16, 43]. They are employed across numerous applications, ranging from automated customer support to content generation [45]. As the development of new LLMs continues, practitioners find themselves with a plethora of options, selecting the optimal model, prompt [40], or hyperparameters for their specific needs from hundreds available. For instance, initiatives like the Chatbot Arena [15] actively maintain nearly 100 models for benchmarking on user-specified open-ended queries. Similarly, the AlpacaEval [44] project benchmarks over 200 models against a diverse set of 805 questions.\nQuerying an LLM is resource-intensive, and therefore such extensive evaluations require significant investments in time, compute and financial resources. In Figure 1 Left, we show the estimated cost to fully evaluate (denoted as full evaluation) the 153 models officially included in AlpacaEval [44] as of May 20, 2024 to be almost 800 USD. In Figure 1 Right, we show that 78.2 Nvidia A6000 GPU hours are needed for evaluating 205 zero-shot prompts on 784 GSM8K [18] questions using Mistral-7B [32].\nDespite the convention of exhaustive evaluations of all data points in a test set from a task, practitioners often only care about the overall performance rankings. Typically, the goal is to identify the top- performing methods or simply the best ones, discarding the lower-ranked alternatives. Therefore, while full-scale evaluation of each method on every data point is thorough, it may not be cost-effective when the goal is merely to identify the superior method.\nIn this paper, we explore a limited-budget setting. The best method among a set of methods is the method that has the best average performance among a set of test examples. We want to identify this best method given a fixed budget for evaluating method-example pairs. A simple baseline is to evenly split the budget for each method. However, this can be very inefficient \u2013 as we observed in our experiments, for some datasets, we need to evaluate more than 90% of all method-example pairs to predict the best method correctly with high probability. Intuitively, it is easier to recognize that low-performance methods are unlikely to be the best. Therefore, a more effective budget allocation strategy is to spend less on low-performing methods and more on promising ones. Multi-armed bandit algorithms enable this dynamic allocation by actively selecting the next method-example pair to evaluate based on the results of previous evaluations.\nWe propose two active selection algorithms UCB-E and UCB-E-LRF. Our first algorithm is an extension of the classical UCB-E [2] to solve the multi-armed bandit problem. It estimates the upper confidence bound (UCB) to guide the selection of the method which is paired with a randomly chosen example for the next evaluation in order to efficiently estimate the best method. This algorithm enjoys a theoretical guarantee that the chance of selecting the best arm converges to 100% by an exponential decay of the number of evaluations. Our second algorithm, UCB-E-LRF, leverages the intrinsic low-rankness of the scoring matrices. We find that in practice, scoring matrices, where each row (column) is a method (an example) and the value is some metric score to measure how good the method is on the example, usually can be well-approximated by a low-rank matrix. Intuitively we can predict the remaining unobserved method-example pairs by the low-rank factorization (LRF) and prioritize evaluations of the pairs with large uncertainties in this prediction. By deploying this intuition in addition to UCB-E, UCE-E-LRF actively selects both the method and example to evaluate and it potentially improves the efficiency of the budget usage.\nWe study the performance of these algorithms as well as common baselines in a number of practical settings. Through the empirical analysis, we find that our two active selection algorithms are much better than non-active baselines. In addition, we find for all algorithms, the harder settings, where the method set is large or the gaps between the top method and other methods are small, generally need more budget for evaluating the method-example pairs. More interestingly, we observe that the UCB-E works best for the easier settings; conversely, UCB-E-LRF has superiority over all other algorithms when the settings are harder."}, {"title": "Preliminaries", "content": "A typical evaluation workflow in large language model (LLM) applications involves three steps: inference, scoring, and performance aggregation, the first two of which LLMs can play important roles.\n1. Inference. Given a dataset such as one shown in Table 1 and an LLM-based method, the output from this method is generated through an LLM. Each method can be a distinct LLM for benchmarking different LLM performance, the same LLM with different prompts for prompt engineering [60], or other different configurations such as temperature, decoding strategies, etc for hyperparameter tuning.\n2. Scoring. The outputs from different methods are scored with a scoring function i.e. metric. The scoring function can either be rule-based (exact string match, BLEU [50], ROUGE [46]), LLM-based (BERTScore [65], LLM judge [66]), or human-based i.e. user study. Depending on the task and dataset format, researchers have employed different types of scoring functions. In Table 1, we classify a selected group of commonly used datasets according to the task type and scoring function applied to them. We group LLM-based and human-based scoring together since they are usually considered alternatives to each other and have been shown to have relatively high correlation [44].\n3. Performance aggregation. The performance of each method is aggregated across the dataset, typically with a simple average over all examples in the dataset.\nNotice that LLMs can play an important role in both inference and scoring. However, LLMs are becoming increasingly large and many state-of-the-art LLMs for LLM-based scoring functions are black-box APIs [42]. Both inference and scoring in the evaluation workflow start to become massively resource-intensive.\nDespite the intensive resources needed, in many practical scenarios, we are only interested in identi- fying the best method among all methods. For example, for prompt engineering and hyperparameter tuning, knowing which method (prompt / configuration) is the best is usually sufficient for next round of iteration. Intuitively, evaluating all methods on all examples on a dataset is excessive for this purpose and therefore we are interested in studying the identification of the best method with as few evaluation pairs as possible."}, {"title": "Notations and Problem Formulation", "content": "With the motivation above, we now define our notation and provide a formal problem formulation. Suppose we have a set of methods \\(F = \\{ f_1, ..., f_n \\}\\) and a set of examples \\(X = \\{x_1,...,x_m\\}\\). Let \\(e: F \\times X \\rightarrow [0, 1]\\) denote our scoring function and without loss of generality, we assume \\(e(f, x) > e(f', x)\\) indicates \\(f\\) has better performance than \\(f'\\) on the example \\(x\\). Suppose \\(E \\in [0, 1]^{n\\times m}\\) is the underlying scoring matrix for a given problem \\((F, X, e)\\) by defining \\(E_{i,j} := e(f_i, x_j)\\). The score of a given method \\(f_i\\) is defined as \\(\\mu_i = \\frac{1}{m} \\sum_{j=1}^m E_{i,j}\\) and want to find the best method \\(i^* = \\arg \\max_i \\mu_i\\).\nAs motivated above, we are interested in the scenario where we have a limited budget of \\(T\\) evaluations. The question is: how can we select these \\(T\\) method-example pairs to maximize the chance of finding the best method \\(i^*\\)? Formally, we want to study the evaluation algorithm \\(A\\). The input is the"}, {"title": "Algorithms", "content": "One simple baseline for designing the evaluation algorithm \\(A\\) is: for each method \\(f_i \\in F\\), we uniformly sample \\([T/n]\\) examples \\(X_{i,r}\\) from \\(X\\), estimate the mean \\(\\hat{\\mu_i} = \\frac{1}{[T/n]} \\sum_{x \\in X_{i,r}}e(f_i, x)\\), and pick the method \\(f_i\\) with the highest estimated mean \\(\\hat{\\mu}\\) as the prediction for the best method. However, this algorithm can be very inefficient. We will see in the experiments that for some datasets, we need to evaluate at least 90% of all method-example pairs to predict the best method correctly with high probability. Different from this simple baseline, two algorithms proposed in this section actively decide the method-example pair to evaluate next and each decision is made based on previous evaluations; Figure 2 illustrates this high level idea."}, {"title": "Algorithm 1 \u2013 UCB-E", "content": "The algorithm. Notice the simple baseline evenly distributes its total budget across the different methods \\(f_i\\). This allocation of resources is the main limitation of the simple baseline. In order to distinguish the best method \\(f_{i^*}\\) from other good methods \\(f_i\\) (i.e. \\(\\mu_{i^*} - \\mu_i\\) is small), we may need more than \\([T/n]\\) examples, while \\(\\leq [T/n]\\) examples are sufficient to distinguish \\(f_{i^*}\\) from bad methods (i.e. \\(\\mu_{i^*} - \\mu_i\\) is large).\nWe address the limitation above with our first algorithm UCB-E (\\(A_{ucb-e}\\); Algorithm 1), a simple extension of the classic multi-arm bandit algorithm UCB-E [2]. At every step \\(t\\), we estimate the upper confidence bound of each method \\(f_i\\) from the examples evaluated with that method and pick the method \\(f_{i_t}\\) with the largest upper confidence bound. Then we uniformly sample one example \\(x_{j_t}\\)"}, {"title": "Corollary 1 (Theoretical guarantee of UCB-E)", "content": "Define \\(H_1 = \\sum_{i=1, i \\neq i^*}^{n} \\frac{1}{(\\mu_{i^*} - \\mu_i)^2}\\) and suppose \\(\\eta = \\frac{25}{36}\\frac{T}{H_1}\\), \\(P_A(A(T, F, X; \\eta) = i^*) \\geq 1 - 2Tn \\exp(-\\frac{T \\eta}{H_1})\\).\nComment. Given a problem set-up \\((F,X, e)\\), \\(H_1\\) in the corollary indicates the hardness of the problem. The larger the gaps between the highest score and the remaining scores are, the higher the probability UCB-E will pick the best method given a fixed budget \\(T\\). In our experiments, we will observe that the datasets with smaller \\(H_1\\) tend to have a higher chance of predicting \\(i^*\\) correctly given the same evaluation budget \\(T\\)."}, {"title": "Algorithm 2 \u2013 UCB-E with Low-Rank Factorization (UCB-E-LRF)", "content": "Main idea/Inspiration/Motivation. We find that the scoring matrix \\(E\\) for real-world problem instances can often be approximated well by a low rank matrix where rank \\(r < n, m\\). This observation means that it is possible to estimate the score of all method-example pairs from the scores of just a few. Consider if the matrix \\(E\\) were exact rank-1, then only \\(n + m - 1\\) scores would be needed to exactly recover the full scoring matrix \\(E\\), which is extremely efficient compared to exhaustively evaluating all \\(n \\cdot m\\) combinations. Suppose \\(E_{obs}\\) is a partially observed scoring matrix, where the non-zero elements are evaluated pairs with the ground truth score for that pair, and \\(\\hat{E}\\) is the estimated scoring matrix, where the non-zero elements are unevaluated pairs with the estimated score. Suppose \\(O \\in \\{0,1\\}^{n \\times m}\\) is the observation matrix where \\(O_{i,j}\\) indicates whether the method-example pair \\((f_i, x_j)\\) has been observed. When the scores of unevaluated method-example pairs are estimated accurately, the score estimation \\(\\hat{\\mu_i} = \\frac{1}{m} \\sum_{j=1}^m (O_{i,j}E_{obs} + (1 \u2013 O_{i,j})\\hat{E}_{i,j})\\), which combines both evaluated pairs and the estimated pairs, has the potential to be more accurate than the estimation by only the evaluated pairs.\nThe algorithm. Leveraging the intuition of estimating the unevaluated method-example pairs, we adopt a low rank factorization (LRF) approach and propose our second algorithm UCB-E-LRF"}, {"title": "Choice of scoring matrix estimation oracle", "content": "We define our \\(M\\) as an ensemble of \\(K\\) low-rank factorization (LRF) solutions [59, 14]. The low-rank factorization assumes the ground truth matrix is (approximately) low-rank with rank \\(r\\). We optimize the low-rank representations \\(U \\in \\mathbb{R}^{n \\times r}\\) and \\(V \\in \\mathbb{R}^{m \\times r}\\) for following optimization problem by the alternating least squares method [26]:\n\\[\\min_{U \\in \\mathbb{R}^{n \\times r}, V \\in \\mathbb{R}^{m \\times r}} \\sum_{i \\in [n], j \\in [m]} O_{i,j} (U_i V_j^T - E^{obs}_{i,j})^2, \\quad (1)\\]\nwhere \\(U_i\\) is the \\(i\\)th row of \\(U\\) and \\(V_j\\) is the \\(j\\)th row of \\(V\\). We further produce both the estimation and the uncertainty by bootstrapping [23, 67]. In order to get a diverse ensemble of low-rank factorizations for each factorization \\((U^{(k)}, V^{(k)})\\), we randomly zero out 5% of the \u201c1", "as\n\\[\\hat{E}": 1, "R_{i,j}": "sqrt{ \\frac{1}{K} \\sum_{k=1}^K ((U^{(k)} (V^{(k)})^T)_{i,j} - \\hat{E}_{i,j})^2} \\quad (2)\\]\nwhere 1 is an all-one matrix with the shape of \\(n \\times m\\) and \\(\\circ\\) is the element-wise multiplication.\nThis oracle introduces four additional hyperparameters for our UCB-E-LRF algorithm, which deter- mine performance and efficiency:\n\u2022 The rank \\(r\\) of the low-rank factorization. The value of \\(r\\) adjusts the expressiveness of the factorization when fit to the data in Equation 2. Setting \\(r\\) requires considering the bias-variance trade-off of choosing smaller vs. larger \\(r\\). The representations of smaller \\(r\\) inherently introduce larger approximation error, however, the representations of large \\(r\\) are harder to be estimated accurately given only a few observations in \\(E_{obs}\\).\n\u2022 The ensemble size \\(K\\). Sufficiently large \\(K\\) is necessary to avoid additional noise when estimating \\(\\hat{E}\\) and \\(R\\) in Equation 2. Yet, the computational cost of the factorization scales with \\(K\\) and therefore \\(K\\) cannot be too large.\n\u2022 The warm-up budget \\(T_0\\). The oracle \\(M\\) requires a minimum number of initial observations in order to estimate the low-rank representation accurately. Therefore, before the active selection phase (line 3-9 in Algorithm 2), we have a warm-up phase (line 1-2 in Algorithm 2), where \\(T_0\\) method-example pairs are sample uniformly at random to evaluate before moving on to the \"active\" phase.\n\u2022 The batch size \\(b\\). The oracle \\(M\\) must be fit and refit which involves optimizing the ensemble of low-rank factorizations. As the computational cost of this procedure cannot be neglected, we introduce a batch size hyperparameter. At each time step \\(t\\), we select \\(b\\) examples \\(J_t\\) paired with the method \\(i_t\\) (line 4 in Algorithm 2), evaluate the \\(b\\) method-example pairs (line 6 in Algorithm 2), and update the intermediate variables with these \\(b\\) scores (line 8 in Algorithm 2). Notice that although increasing \\(b\\) is more computationally friendly as it calls the oracle \\(M\\) less, it may hurt the performance as it reduces the granularity of the decisions, making the algorithm less \"active\". The choice of \\(b\\) is ultimately a trade-off between computational cost and the performance, however, we will see in the experiments section that performance does not vary drastically for many choices of \\(b\\).\nWe analyze these four UCB-E-LRF hyperparameters in our experiment sections, along with the uncertainty scaling \\(\\eta\\) in both UCB-E and UCB-E-LRF."}, {"title": "Experiments", "content": "To assess the performance of our algorithms under a variety of use cases, we test with three datasets AlpacaEval [44], Grade School Math 8K (GSM8K) [18] and Physical Interaction: Question An- swering (PIQA), together with different settings of the method set \\(F\\) and the scoring function \\(e\\); Table 2 summarizes the statistics of each dataset along with what method \\(F\\) and scoring function \\(e\\). For AlpacaEval, we design two sets of calF. The first set \\(F\\) contains all LLMs reported by [44]. The second \\(F\\) contains the same LLMs except for the annotator model GPT-4 Turbo. The latter \\(F\\) that does not contain GPT-4 Turbo makes the learning more challenging and interesting since the annotator, GPT-4 Turbo, is a clear winner among all LLMs. For GSM8K and PIQA, we set \\(F\\) as a set of prompts to simulate prompt engineering, or a set of LLMs with various sampling configurations, to mimic model selection and hyperparameter tuning. We provide more information about these datasets in Appendix A.1."}, {"title": "Metrics", "content": "Top 1 Precision. To determine if an algorithm finds the best method, we can directly check if the algorithm's prediction matches with the best method from our empirical data. However, because every method is empirically evaluated on limited number of examples, it is possible that one method has slightly lower average performance than another method whereas if we were to evaluate on more examples, the former would achieve a higher performance. To this end, we calculate top 1 precision by first determining a set of methods we consider equally good and we check if the predicted method from an algorithm is a member of that set. We propose two ways to determine if a set of methods are"}, {"title": "Set-Up of Our algorithms and Baselines", "content": "We introduce the set-up of our two algorithms, UCB-E and UCB-LRF, together with three more baselines, Row Mean Imputation, Filled Subset and LRF.\nUCB-E. Our proposed algorithm as shown in Algorithm 1. We use \\(\\eta = 1\\) since this consistently yields the best performance across all datasets.\nUCB-E-LRF. Our proposed algorithm as shown in Algorithm 2. For all datasets, we use rank \\(r = 1\\) for low rank factorization with an ensemble size of \\(K = 64\\). We use 5% of data for warm up i.e. \\(T_0 = 0.05 \\times n \\times m\\) and \\(\\eta = 5\\). We use a batch size of \\(b = 32\\).\nRow Mean Imputation. In this baseline, we select method-example pairs uniformly at random from all pairs. The score for each method is calculated as the average of all the scores evaluated so far for that method. The detailed algorithmic description is in Algorithm 3.\nFilled Subset. Instead of randomly selecting method-example pairs, filled subset first selects an example index \\(j\\) uniformly at random. Then all methods are evaluated on example \\(j\\). If all methods have been evaluated, the algorithm selects a new example index from remaining ones. The score for each method is calculated as the average of all the scores evaluated so far for that method. The detailed algorithmic description is in Algorithm 4. Although row mean imputation and filled subset do not have learning component, they both produce an unbiased estimate of the real score for each method at all time."}, {"title": "Main Results", "content": "In Figure 3, we plot the performance of baselines and our algorithms on all six datasets (columns) as the budget \\(T\\) increases from 5% to 100% of the total number of method-example pairs. In each row, we evaluate the algorithms on a different metric (either top 1 precision with different \\(\\epsilon\\), \\(p\\) or NDCG@10). Each line in the figure is the average result over 50 independent trials with different seeds. For example, an average top 1 precision of 0.9 indicates that 45 out of 50 trials predict the best method correctly at the budget level that its x-coordinate represents. In addition, we calculate \\(H_1\\) as defined in Corollary 1 that quantifies the difficulty of finding the best method on a dataset. Intuitively, a higher \\(H_1\\) value suggests that the method set \\(F\\) is large or there are many methods that have similar performance with the best one and distinguishing them can be challenging. In Appendix A.1 Figure 5, we also plot the histogram to show the distribution of performance among \\(F\\) on these datasets.\nHow do our algorithms compare with the baselines? As seen from Figure 3, both UCB-E and UCB-E-LRF consistently achieve high precision and NDCG with much less budget compared to the baselines. For example, on AlpacaEval (Drop Annotator), our proposed algorithms can reach precision of 1 with just 8% budget whereas the baselines require 80-90%, an order of magnitude more budget needed. These results suggest that it is entirely possible to identify the best method without exhaustively evaluating all method-example pairs. They further demonstrate that the active selection algorithms (our two algorithms) are more efficient than the non-active algorithms (three baselines). Additionally, the better NDCG performance from our proposed algorithms shows that our methods can more correctly rank top-performance methods.\nHow is the comparison between our two algorithms UCB-E and UCB-E-LRF? The datasets from left to right are ranked by the hardness indicated by \\(H_1\\). Interestingly, we find that on easier datasets such as AlpacaEval, UCB-E performs better and saves 2-3% more on budget compared to UCB-E-LRF, while on harder datasets, such as GSM8K Prompts and PIQA Prompts, UCB-E- LRF achieves higher precision faster than UCB-E, saving about 10% in absolute budget. These observations give us a hint on what algorithm to apply in practice."}, {"title": "More Empirical Analysis", "content": "We provide more empirical analysis and the ablation results can be found in Figure 4.\nDoes \\(H_1\\) correctly reflect the hardness of a dataset in the empirical experiments? Yes, going through Figure 3 from left to right, as \\(H_1\\) decreases, the percentage of matrix evaluation needed to reach precision of 1 also generally decreases from more than 20% to just under 5%. Moreover, the \\(H_1\\) values seem to be related to the tasks. That is, prompt engineering datasets typically have higher \\(H_1\\) possibly due to the homogeneity of prompt performance with the same LLM. In contrast, datasets that benchmark different LLMs such as AlpacaEval is much easier to find the best performing model.\nScore Only Ablation. To study the effect of selecting both next method and next example to evaluate using uncertainty matrix \\(R\\), we consider an ablation of UCB-E-LRF by reverting the place of using \\(R\\) to the UCB-E: UCB-E-LRF (Score Only) where the upper confidence bound is computed by the confidence interval in original UCB-E in addition to the mean estimation through low rank factorization; the example is uniformly selected, same as UCB-E. The detailed algorithmic description can be found in Algorithm 6. We see that on the hard dataset GSM8K Prompts where UCB-E-LRF has certain benefit over UCB-E, there is a significant gap between UCB-E-LRF and UCB-E-LRF (Score Only). This means that the component of uncertainty matrix \\(R\\) in UCB-E-LRF is crucial to contribute the benefit over UBC-E.\nBatch Size b Ablation. Intuitively, a smaller batch size is more flexible can have more fine grained selection. We experiment with \\(b \\in \\{2,8,32, 128\\}\\) and as shown in the plot, smallest batch size"}, {"title": "Ensemble Size K Ablation.", "content": "We vary the ensemble size \\(K \\in \\{4, 16, 64, 256\\}\\) and find that very small ensemble size gives suboptimal performance on hard datasets. There is almost no performance difference on easy dataset like AlpacaEval (Drop Annotator). The performance is robust to different \\(K\\) as long as it is larger than 64."}, {"title": "Uncertainty Scaling Factor \u03b7 Ablation.", "content": "We experiment with different uncertainty scaling values \\(\\{0.1, 0.5, 1, 5, 10\\}\\). In Figure 4, it can be seen that a certain range of \\(n\\), from 0.1 to 5, gives similar performance on both two datasets. This demonstrate the robustness of selecting \\(\\eta\\)."}, {"title": "Warm-up Budget T0 Ablation.", "content": "Our algorithm UCB-E-LRF by default randomly evaluates 5% of the method-example pairs in the data matrix before actively selects. We analyze the effect of varying the budget \\(T_0\\) among \\(\\{0.5\\%, 1\\%, 5\\%, 10\\%\\} \\times n \\times m\\) on the algorithm performance on the two datasets. We see that a very small warm-up budget with 0.5% of data can achieve decent precision initially, but fall behind compared to larger warm-up budget as more data are evaluated. In contrast, a very large warm-up budget of 10% delays the active selection algorithm too much and also achieves suboptimal performance. We therefore use 5% as a generally strong starting point. Note that on AlpacaEval, it is possible to achieve the same performance with even smaller warm-up budget, suggesting more saving is possible."}, {"title": "Rank r Ablation.", "content": "As discussed in the Algorithm Section, \\(r\\) adjusts the bias-variance trade-off. Empirically we experiment with \\(r\\in \\{1, 2, 5\\}\\) and find that \\(r = 1\\) is consistently better than larger value counterparts. The results can be explained by the fact that a larger \\(r\\) requires more evaluated data in order to prevent overfitting, which might not have advantage in the limited budget setting. Therefore, we recommend using \\(r = 1\\) for all datasets."}, {"title": "Related Work", "content": "Best-arm identification in multi-arm bandits. The goal of best-arm identification [6, 2] is to find the arm with the highest reward by pulling these arms and getting the feedback. By making an analogy, in our problem method \\(f_i\\) is the arm and the score \\(\\mu_i\\) of \\(f_i\\) is the reward. There are two ways to define the best-arm identification problem: fixed budget and fixed confidence. In the fixed budget setting, the budget for the arm pulls is fixed and the algorithm is designed for better chances to identify the correct best arm \u2013 our problem defined in this paper has the similar evaluation budget. UCB-E [2] and Successive Elimination [2] are two pioneering algorithms proposed for this setting, followed by a line of improvement [30, 38, 10, 39]. Qin [51] states the optimal fixed budget best-arm identification as an open problem. In the setting of fixed confidence, the algorithms [35, 37, 31] work towards fewer number of arms to guarantee the given confidence of getting the best arm. Garivier and Kaufmann [24] gives an optimal algorithm in terms of the minimum of arms to pull. Another extension beyond the setting of fixed budget or confidence is the PAC learning framework, where the target is to maximize the chance of getting an mostly best arm, with a tolerance of \\(\\epsilon\\) gap to the highest reward [36, 31, 12].\nLow-rank factorization for (noisy) matrix completion. As shown in the objective function of Equation 1, low-rank factorization is a non-convex optimization problem. Therefore a line of work focus on how to solve this non-convex optimization [9, 8, 14], while another line of work [13, 7] study the approximation error between the estimated low-rank matrix and the target matrix in terms of \\(p\\) when assuming the observations are i.i.d. sampled with a pre-assumed chance \\(p\\) and the additive noise to each observation is i.i.d. Gaussian noise.\nLLM performance evaluation functions. Tremendous effort has been devoted to developing effective evaluation functions to assess the quality of open-ended generations from language models. Early work in this direction such as BLEU [50] and ROUGE [46] are rule-based that use lexical"}, {"title": "Discussion and Conclusion", "content": "One limitation of our proposed algorithms is that we assume there is a two-dimensional fixed-size matrix as our dataset for active method-example selection. In some real-world applications, new rows or columns can be gradually incorporated and the matrix size is dynamic. In other scenarios such as Chatbot Arena, instead of being able to decide what examples to select, we can only select a pair of models to compare for a user-specified example. We leave studying how to minimize budget to find the best method for these new settings as future work. Other promising extensions of our work are to apply more advanced best-arm identification algorithm in the literature, and fixed confidence setting instead of fixed budget setting.\nIn conclusion, we formulate the problem that when the evaluation for each model-example is resource intensive in regard to money, compute and time, given a certain budget of resource, how can we have still have a great chance to identify the best model on a specific dataset among a set of candidates. We propose two algorithms to tackle the problem, which all sequentially decide which model-example pair to evaluate next by observing the previous evaluated pairs. The first algorithm follows the idea of a classic multi-arm bandit algorithm UCB-E and enjoys the similar theoretical guarantee to lower bound the chance given any budget. The second algorithm, UCB-E-LRF extends UCB-E by leveraging the finding of approximate low-rank of the target evaluation matrix. In the experiment, we show that both algorithms introduced in our paper are significantly better than one that just uniformly samples method-example pairs. Moreover, we identified the condition when the UCB-E or UCB-E-LRF works better than the other. We do not foresee any negative societal impacts for our work."}]}