{"title": "On Speeding Up Language Model Evaluation", "authors": ["Jin Peng Zhou", "Christian K. Belardi", "Ruihan Wu", "Travis Zhang", "Carla P. Gomes", "Wen Sun", "Kilian Q. Weinberger"], "abstract": "Large language models (LLMs) currently dominate the field of natural language processing (NLP), representing the state-of-the-art across a diverse array of tasks. Developing a model of this nature, from training to inference, requires make numerous decisions which define a combinatorial search problem. For example, selecting the optimal pre-trained LLM, prompt, or hyperparameters to attain the best performance for a task often requires evaluating multiple candidates on an entire test set. This exhaustive evaluation can be time-consuming and costly, as both inference and metric computation with LLMs are resource-intensive. In this paper, we address the challenge of identifying the best method within a limited budget for evaluating methods on test examples. By leveraging the well-studied multi-armed bandit framework, which sequentially selects the next method-example pair to evaluate, our approach, combining multi-armed bandit algorithms with low-rank factorization, significantly reduces the required resources. Experiments show that our algorithms can identify the top-performing method using only 5-15% of the typically needed resources, resulting in an 85-95% reduction in cost.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated remarkable proficiency in diverse tasks such as question answering, machine translation and mathematical reasoning [5, 16, 43]. They are employed across numerous applications, ranging from automated customer support to content generation [45]. As the development of new LLMs continues, practitioners find themselves with a plethora of options, selecting the optimal model, prompt [40], or hyperparameters for their specific needs from hundreds available. For instance, initiatives like the Chatbot Arena [15] actively maintain nearly 100 models for benchmarking on user-specified open-ended queries. Similarly, the AlpacaEval [44] project benchmarks over 200 models against a diverse set of 805 questions.\nQuerying an LLM is resource-intensive, and therefore such extensive evaluations require significant investments in time, compute and financial resources.\nDespite the convention of exhaustive evaluations of all data points in a test set from a task, practitioners often only care about the overall performance rankings. Typically, the goal is to identify the top- performing methods or simply the best ones, discarding the lower-ranked alternatives. Therefore, while full-scale evaluation of each method on every data point is thorough, it may not be cost-effective when the goal is merely to identify the superior method."}, {"title": "2 Preliminaries", "content": "A typical evaluation workflow in large language model (LLM) applications involves three steps: inference, scoring, and performance aggregation, the first two of which LLMs can play important roles.\n1. Inference. Given a dataset such as one shown in Table 1 and an LLM-based method, the output from this method is generated through an LLM. Each method can be a distinct LLM for benchmarking different LLM performance, the same LLM with different prompts for prompt engineering [60], or other different configurations such as temperature, decoding strategies, etc for hyperparameter tuning.\n2. Scoring. The outputs from different methods are scored with a scoring function i.e. metric. The scoring function can either be rule-based (exact string match, BLEU [50], ROUGE [46]), LLM-based (BERTScore [65], LLM judge [66]), or human-based i.e. user study. Depending on the task and dataset format, researchers have employed different types of scoring functions.\n3. Performance aggregation. The performance of each method is aggregated across the dataset, typically with a simple average over all examples in the dataset.\nNotice that LLMs can play an important role in both inference and scoring. However, LLMs are becoming increasingly large and many state-of-the-art LLMs for LLM-based scoring functions are black-box APIs [42]. Both inference and scoring in the evaluation workflow start to become massively resource-intensive.\nDespite the intensive resources needed, in many practical scenarios, we are only interested in identi- fying the best method among all methods. For example, for prompt engineering and hyperparameter tuning, knowing which method (prompt / configuration) is the best is usually sufficient for next round of iteration. Intuitively, evaluating all methods on all examples on a dataset is excessive for this purpose and therefore we are interested in studying the identification of the best method with as few evaluation pairs as possible."}, {"title": "2.2 Notations and Problem Formulation", "content": "With the motivation above, we now define our notation and provide a formal problem formulation. Suppose we have a set of methods \\(F = \\{ f_1, ..., f_n\\}\\) and a set of examples \\(X = \\{x_1,...,x_m\\}\\). Let \\(e: F \\times X \\rightarrow [0, 1]\\) denote our scoring function and without loss of generality, we assume \\(e(f, x) > e(f', x)\\) indicates \\(f\\) has better performance than \\(f'\\) on the example \\(x\\). Suppose \\(E \\in [0, 1]^{n \\times m}\\) is the underlying scoring matrix for a given problem \\((F, X, e)\\) by defining \\(E_{i,j} := e(f_i, x_j)\\). The score of a given method \\(f_i\\) is defined as \\(\\mu_i = \\frac{1}{m} \\sum_{j=1}^{m} E_{i,j}\\) and want to find the best method \\(i^* = \\arg \\max_i \\mu_i\\).\nAs motivated above, we are interested in the scenario where we have a limited budget of \\(T\\) evaluations. The question is: how can we select these \\(T\\) method-example pairs to maximize the chance of finding the best method \\(i^*\\)? Formally, we want to study the evaluation algorithm \\(A\\). The input is the"}, {"title": "3 Algorithms", "content": "One simple baseline for designing the evaluation algorithm \\(A\\) is: for each method \\(f_i \\in F\\), we uniformly sample \\([\\frac{T}{n}]\\) examples \\(X_{i,r}\\) from \\(X\\), estimate the mean \\(\\hat{\\mu_i} = \\frac{1}{[T/n]} \\sum_{x \\in X_{i,r}}e(f_i, x)\\), and pick the method \\(f_i\\) with the highest estimated mean \\(\\hat{\\mu}\\) as the prediction for the best method. However, this algorithm can be very inefficient. We will see in the experiments that for some datasets, we need to evaluate at least 90% of all method-example pairs to predict the best method correctly with high probability. Different from this simple baseline, two algorithms proposed in this section actively decide the method-example pair to evaluate next and each decision is made based on previous evaluations; Figure 2 illustrates this high level idea."}, {"title": "3.1 Algorithm 1 \u2013 UCB-E", "content": "The algorithm. Notice the simple baseline evenly distributes its total budget across the different methods \\(f_i\\). This allocation of resources is the main limitation of the simple baseline. In order to distinguish the best method \\(f_{i^*}\\) from other good methods \\(f_i\\) (i.e. \\(\\mu_{i^*} - \\mu_i\\) is small), we may need more than \\([\\frac{T}{n}]\\) examples, while \\(\\leq \\frac{T}{n}]\\) examples are sufficient to distinguish \\(f_{i^*}\\) from bad methods (i.e. \\(\\mu_{i^*} - \\mu_i\\) is large).\nWe address the limitation above with our first algorithm UCB-E (\\(A_{ucb-e}\\); Algorithm 1), a simple extension of the classic multi-arm bandit algorithm UCB-E [2]. At every step \\(t\\), we estimate the upper confidence bound of each method \\(f_i\\) from the examples evaluated with that method and pick the method \\(f_{i_t}\\) with the largest upper confidence bound. Then we uniformly sample one example \\(x_{j_t}\\)"}, {"title": "Corollary 1 (Theoretical guarantee of UCB-E)", "content": "Define \\(H_1 = \\sum_{i=1, i \\neq i^*}^{n} \\frac{1}{(\\mu_{i^*} - \\mu_i)^2}\\) and suppose\n\\(\\eta \\geq \\frac{25}{36}TH_1\\), then \\(P_{A_{UCB-E}} (A_{UCB-E}(T, F, X; \\eta) = i^*) \\geq 1 - 2Tn \\exp(-\\frac{\\eta}{T})\\).\nComment. Given a problem set-up \\((F,X, e)\\), \\(H_1\\) in the corollary indicates the hardness of the problem. The larger the gaps between the highest score and the remaining scores are, the higher the probability UCB-E will pick the best method given a fixed budget \\(T\\). In our experiments, we will observe that the datasets with smaller \\(H_1\\) tend to have a higher chance of predicting \\(i^*\\) correctly given the same evaluation budget \\(T\\)."}, {"title": "3.2 Algorithm 2 \u2013 UCB-E with Low-Rank Factorization (UCB-E-LRF)", "content": "Main idea/Inspiration/Motivation. We find that the scoring matrix \\(E\\) for real-world problem instances can often be approximated well by a low rank matrix where rank \\(r < n, m\\). This observation means that it is possible to estimate the score of all method-example pairs from the scores of just a few. Consider if the matrix \\(E\\) were exact rank-1, then only \\(n + m - 1\\) scores would be needed to exactly recover the full scoring matrix \\(E\\), which is extremely efficient compared to exhaustively evaluating all \\(n \\cdot m\\) combinations. Suppose \\(E^{obs}\\) is a partially observed scoring matrix, where the non-zero elements are evaluated pairs with the ground truth score for that pair, and \\(\\hat{E}\\) is the estimated scoring matrix, where the non-zero elements are unevaluated pairs with the estimated score. Suppose \\(O \\in \\{0,1\\}^{n \\times m}\\) is the observation matrix where \\(O_{i,j}\\) indicates whether the method-example pair \\((f_i, x_j)\\) has been observed. When the scores of unevaluated method-example pairs are estimated accurately, the score estimation \\(\\hat{\\mu}_i = \\frac{1}{m} \\sum_{j=1}^{m} (O_{i,j}E_{i,j}^{obs} + (1 - O_{i,j})\\hat{E}_{i,j})\\), which combines both evaluated pairs and the estimated pairs, has the potential to be more accurate than the estimation by only the evaluated pairs.\nThe algorithm. Leveraging the intuition of estimating the unevaluated method-example pairs, we adopt a low rank factorization (LRF) approach and propose our second algorithm UCB-E-LRF"}, {"title": "4 Experiments", "content": "To assess the performance of our algorithms under a variety of use cases, we test with three datasets AlpacaEval [44], Grade School Math 8K (GSM8K) [18] and Physical Interaction: Question An- swering (PIQA), together with different settings of the method set \\(F\\) and the scoring function \\(e\\);\nWe analyze these four UCB-E-LRF hyperparameters in our experiment sections, along with the uncertainty scaling \\(\\eta\\) in both UCB-E and UCB-E-LRF."}, {"title": "4.2 Metrics", "content": "Top 1 Precision. To determine if an algorithm finds the best method, we can directly check if the algorithm's prediction matches with the best method from our empirical data. However, because every method is empirically evaluated on limited number of examples, it is possible that one method has slightly lower average performance than another method whereas if we were to evaluate on more examples, the former would achieve a higher performance.\nNDCG. Although our focus is to identify the best method quickly, it is sometimes also desirable to generally rank top-P promising methods high. We therefore evaluate with normalized discounted cumulative gain (NDCG) at P. NDCG@Ptakes as input the top-P prediction by an algorithm and the higher their true ranks are, the higher the NDCG is. We choose P = 10 to evaluate all algorithms."}, {"title": "4.3 Set-Up of Our algorithms and Baselines", "content": "We introduce the set-up of our two algorithms, UCB-E and UCB-LRF, together with three more baselines, Row Mean Imputation, Filled Subset and LRF.\nUCB-E. Our proposed algorithm as shown in Algorithm 1. We use \\(\\eta = 1\\) since this consistently yields the best performance across all datasets.\nUCB-E-LRF. Our proposed algorithm as shown in Algorithm 2. For all datasets, we use rank \\(r = 1\\) for low rank factorization with an ensemble size of \\(K = 64\\). We use 5% of data for warm up i.e. \\(T_o = 0.05 \\times n \\times m\\) and \\(\\eta = 5\\). We use a batch size of \\(b = 32\\).\nRow Mean Imputation. In this baseline, we select method-example pairs uniformly at random from all pairs. The score for each method is calculated as the average of all the scores evaluated so far for that method.\nFilled Subset. Instead of randomly selecting method-example pairs, filled subset first selects an example index \\(j\\) uniformly at random. Then all methods are evaluated on example \\(j\\). If all methods have been evaluated, the algorithm selects a new example index from remaining ones. The score for each method is calculated as the average of all the scores evaluated so far for that method."}, {"title": "4.4 Main Results", "content": "In Figure 3, we plot the performance of baselines and our algorithms on all six datasets (columns) as the budget T increases from 5% to 100% of the total number of method-example pairs. In each row, we evaluate the algorithms on a different metric (either top 1 precision with different \\(\\epsilon\\), \\(p\\) or NDCG@10). Each line in the figure is the average result over 50 independent trials with different seeds. For example, an average top 1 precision of 0.9 indicates that 45 out of 50 trials predict the best method correctly at the budget level that its x-coordinate represents. In addition, we calculate \\(H_1\\) as defined in Corollary 1 that quantifies the difficulty of finding the best method on a dataset. Intuitively, a higher \\(H_1\\) value suggests that the method set \\(F\\) is large or there are many methods that have similar performance with the best one and distinguishing them can be challenging. \nHow do our algorithms compare with the baselines? As seen from Figure 3, both UCB-E and UCB-E-LRF consistently achieve high precision and NDCG with much less budget compared to the baselines. For example, on AlpacaEval (Drop Annotator), our proposed algorithms can reach precision of 1 with just 8% budget whereas the baselines require 80-90%, an order of magnitude more budget needed. These results suggest that it is entirely possible to identify the best method without exhaustively evaluating all method-example pairs. They further demonstrate that the active selection algorithms (our two algorithms) are more efficient than the non-active algorithms (three baselines). Additionally, the better NDCG performance from our proposed algorithms shows that our methods can more correctly rank top-performance methods.\nHow is the comparison between our two algorithms UCB-E and UCB-E-LRF? The datasets from left to right are ranked by the hardness indicated by \\(H_1\\). Interestingly, we find that on easier datasets such as AlpacaEval, UCB-E performs better and saves 2-3% more on budget compared to UCB-E-LRF, while on harder datasets, such as GSM8K Prompts and PIQA Prompts, UCB-E- LRF achieves higher precision faster than UCB-E, saving about 10% in absolute budget. These observations give us a hint on what algorithm to apply in practice."}, {"title": "4.5 More Empirical Analysis", "content": "We provide more empirical analysis and the ablation results can be found in Figure 4.\nDoes \\(H_1\\) correctly reflect the hardness of a dataset in the empirical experiments? Yes, going through Figure 3 from left to right, as \\(H_1\\) decreases, the percentage of matrix evaluation needed to reach precision of 1 also generally decreases from more than 20% to just under 5%. Moreover, the \\(H_1\\) values seem to be related to the tasks. That is, prompt engineering datasets typically have higher \\(H_1\\) possibly due to the homogeneity of prompt performance with the same LLM. In contrast, datasets that benchmark different LLMs such as AlpacaEval is much easier to find the best performing model.\nScore Only Ablation. To study the effect of selecting both next method and next example to evaluate using uncertainty matrix \\(R\\), we consider an ablation of UCB-E-LRF by reverting the place of using \\(R\\) to the UCB-E: UCB-E-LRF (Score Only) where the upper confidence bound is computed by the confidence interval in original UCB-E in addition to the mean estimation through low rank factorization; the example is uniformly selected, same as UCB-E.\nBatch Size b Ablation. Intuitively, a smaller batch size is more flexible can have more fine grained selection. We experiment with \\(b \\in \\{2,8,32, 128\\}\\) and as shown in the plot, smallest batch size"}, {"title": "5 Related Work", "content": "Best-arm identification in multi-arm bandits. The goal of best-arm identification [6, 2] is to find the arm with the highest reward by pulling these arms and getting the feedback. By making an analogy, in our problem method \\(f_i\\) is the arm and the score \\(\\mu_i\\) of \\(f_i\\) is the reward. There are two ways to define the best-arm identification problem: fixed budget and fixed confidence. In the fixed budget setting, the budget for the arm pulls is fixed and the algorithm is designed for better chances to identify the correct best arm \u2013 our problem defined in this paper has the similar evaluation budget.\nLow-rank factorization for (noisy) matrix completion. As shown in the objective function of Equation 1, low-rank factorization is a non-convex optimization problem. Therefore a line of work focus on how to solve this non-convex optimization [9, 8, 14], while another line of work [13, 7] study the approximation error between the estimated low-rank matrix and the target matrix in terms of \\(p\\) when assuming the observations are i.i.d. sampled with a pre-assumed chance \\(p\\) and the additive noise to each observation is i.i.d. Gaussian noise.\nLLM performance evaluation functions. Tremendous effort has been devoted to developing effective evaluation functions to assess the quality of open-ended generations from language models."}, {"title": "6 Discussion and Conclusion", "content": "One limitation of our proposed algorithms is that we assume there is a two-dimensional fixed-size matrix as our dataset for active method-example selection. In some real-world applications, new rows or columns can be gradually incorporated and the matrix size is dynamic. In other scenarios such as Chatbot Arena, instead of being able to decide what examples to select, we can only select a pair of models to compare for a user-specified example. We leave studying how to minimize budget to find the best method for these new settings as future work. Other promising extensions of our work are to apply more advanced best-arm identification algorithm in the literature, and fixed confidence setting instead of fixed budget setting.\nIn conclusion, we formulate the problem that when the evaluation for each model-example is resource intensive in regard to money, compute and time, given a certain budget of resource, how can we have still have a great chance to identify the best model on a specific dataset among a set of candidates. We propose two algorithms to tackle the problem, which all sequentially decide which model-example pair to evaluate next by observing the previous evaluated pairs. The first algorithm follows the idea of a classic multi-arm bandit algorithm UCB-E and enjoys the similar theoretical guarantee to lower bound the chance given any budget. The second algorithm, UCB-E-LRF extends UCB-E by leveraging the finding of approximate low-rank of the target evaluation matrix. In the experiment, we show that both algorithms introduced in our paper are significantly better than one that just uniformly samples method-example pairs. Moreover, we identified the condition when the UCB-E or UCB-E-LRF works better than the other. We do not foresee any negative societal impacts for our work."}, {"title": "7 Acknowledgement", "content": "We thank Justin Lovelace and Varsha Kishore for their helpful discussion and feedback on the paper draft. JPZ is supported by grant from the Natural Sciences and Engineering Research Council of Canada (NSERC) (567916). CKB and CPG are support by the Cornell University AI for Science Institute, the AI Climate Institute, the National Science Foundation, the National Institute of Food and Agriculture, and the Air Force Office of Scientific Research. RW is supported by grants from"}, {"title": "A Appendix", "content": "A.1 Additional Description of Datasets Used\nFigure 5: Histogram of model performance on all six datasets along with their H\u2081 values. It can be seen that datasets such as AlpacaEval have larger gap between the best and second best method leads to a much smaller H1, making identifying the best method easier.\nAlpacaEval and AlpacaEval (Drop Annotator). AlpacaEval benchmarks various models on a fixed set of 805 questions.\nGSM8K Prompts and PIQA Prompts. To simulate a prompt engineering use case, we create two datasets: GSM8K Prompts and PIQA Prompts.\nGSM8K Models and PIQA Models. Lastly, for the model selection and hyperparameter tuning use case, we similarly create two more datasets on GSM8K and PIQA."}, {"title": "A.2 Algorithmic Description of Baselines and Ablations", "content": "The algorithms for Row Mean Imputation, Filled Subset, LRF and UCB-E-LRF (Score Only) are shown in Algorithm 3, 4, 5 and 6 respectively."}]}