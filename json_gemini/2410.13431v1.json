{"title": "Solving Prior Distribution Mismatch in Diffusion Models via Optimal Transport", "authors": ["Zhanpeng Wang", "Shenghao Li", "Chen Wang", "Shuting Cao", "Na Lei", "Zhongxuan Luo"], "abstract": "In recent years, the knowledge surrounding diffusion models(DMs) has grown significantly, though several theoretical gaps remain. Particularly noteworthy is prior error, defined as the discrepancy between the termination distribution of the forward process and the initial distribution of the reverse process. To address these deficiencies, this paper explores the deeper relationship between optimal transport(OT) theory and DMs with discrete initial distribution. Specifically, we demonstrate that the two stages of DMs fundamentally involve computing time-dependent OT. However, unavoidable prior error result in deviation during the reverse process under quadratic transport cost. By proving that as the diffusion termination time increases, the probability flow exponentially converges to the gradient of the solution to the classical Monge-Amp\u00e8re equation, we establish a vital link between these fields. Therefore, static OT emerges as the most intrinsic single-step method for bridging this theoretical potential gap. Additionally, we apply these insights to accelerate sampling in both unconditional and conditional generation scenarios. Experimental results across multiple image datasets validate the effectiveness of our approach.", "sections": [{"title": "Introduction", "content": "In the field of artificial intelligence, the task of generating realistic images using probabilistic models is extremely challenging and has many potential applications in data enhancement(Ruthotto and Haber 2021), image editing, style transfer, etc. Recently, diffusion models(DMs)(Song and Ermon 2019; Ho, Jain, and Abbeel 2020; Song and Ermon 2020; Song et al. 2020) have received much attention for their remarkable achievements in synthesizing audio(Kong et al. 2020; Lee et al. 2021), images generation(Dhariwal and Nichol 2021; Nichol and Dhariwal 2021), and other data modalities(Tashiro et al. 2021; Ho et al. 2022). They go beyond many existing deep generation models, such as generative adversarial networks(GANs)(Goodfellow et al. 2014; Radford, Metz, and Chintala 2015; Arjovsky, Chintala, and Bottou 2017), and variational autoencoders(VAEs)(Kingma and Welling 2013; Rezende, Mohamed, and Wierstra 2014).\nDMs include forward process and reverse process. The forward process refers to the add smaller noise gradually to corrode the original data distribution, which can be understood as a standard diffusion process(Song et al. 2020; Sohl-Dickstein et al. 2015). Subsequently, one can perform reverse inference from noise to image based on the learned score function, i.e., the gradient of the logarithmic probability density function(Anderson 1982).\nIt can be seen from the characteristics of DMs that we usually need to design a reasonable noise scheme in the generative modeling so that the forward process has a unique Gibbs steady state distribution(Gardiner et al. 1985). But it is worth noting that we can only actually terminate diffusion at finite time. Therefore, the noisy data distribution at the end is often not equal to the prior distribution of the sampling process, the existence of prior error leads to theoretically we cannot get the desired goal even if the score match exactly. Although in (Khrulkov et al. 2022; Franzese et al. 2023), authors all show that as diffusion duration increases, the prior error will converges exponentially to zero. However, this also means that more computing resources are required(Lyu et al. 2022).\nTo eliminate prior error without increasing the calculation cost, it is intuitive to use single step generator to construct probability transformation from the prior distribution of the reverse process to the posterior distribution of the forward process, such as GANs(Lyu et al. 2022; Zheng et al. 2022), VAEs(Lyu et al. 2022), conditional transport(CT)(Zheng et al. 2022) and optimal transport(OT)(Li et al. 2023). While these models are empirical and no more detailed theoretical explanations are provided for elimination of prior error. Therefore, a natural question is troubling us\u2014which generator is the best used to eliminate prior error in DMs?\nIf the economic issue of transport cost is under consideration, the OT map should be the focal point due to its geodesic properties within the probability space(Villani et al. 2009). But the straightforward decision is not based on the deeper connection between DMs and OT theory, so it is a little \"blunt\". Up to now, there have been many researches involving this link. In (Kwon, Fan, and Lee 2022), the authors show that the matching loss of DMs can constrain the Wasserstein metric. Khrulkov et al.(Khrulkov et al. 2022) and Lavenant et al.(Lavenant and Santambrogio 2022) give simple positive and negative examples respectively to investigate whether the diffusion process can be regarded as geodesic. Whereafter, the latest work (Zhang et al. 2024a) generalizes their conclusions and shows that the continuous probability flow coincides with OT map under certain conditions. These works all point to that there exist some consistency between DMs and OT.\nInspired by previous works, this paper further explores the intrinsic relationship between probability flow and OT theory, and answers mathematically the question above: Optimal transport provides the most intrinsic prior error eliminator for diffusion models. Our main contributions and findings are summarized as follows:\n\u2022 We show that the diffusion model is a two-stage time-dependent optimal transport calculation method. The error between the learned reverse transport map and the ground truth is bounded by the root of score matching loss (8).\n\u2022 Theoretically, we analyze that the ineliminable prior error in classical diffusion models will lead to a potential gap. Even if the score is accurately learned, the deterministic sampling locus still not be a geodesic.\n\u2022 We prove that the probability flow can converge exponentially to the gradient of the solution to the Monge-Amp\u00e8re equation. Therefore, we recommend that the termination part of the diffusion framework be replaced by the static optimal transport, which can completely eliminate the prior error without increasing the computational cost.\n\u2022 Our viewpoint can be naturally extended to sampling acceleration for unconditional and conditional generation. More importantly, experimental results obtained from various image datasets corroborate our conclusions.\nNotations x represents the state variable in n-dimensional Euclidean space Rn and t is for time; d and \u2202 imply differential and partial differential operators, respectively; \u2207 and \u2207\u00b7 mean the gradient and the divergence operators, respectively; \u2225 \u22c5 \u22252 and \u2225 \u22c5 \u2225\u221e provide the 2-norm of a vector or matrix and \u221e-norm of a scalar function, respectively; R\u22650 and R>0 signify the set of non-negative and positive real numbers, respectively; \u2022 shows the composition of multiple function maps; | \u22c5 | denotes the cardinality of set; # indicates push-forward of probability density; W2(\u22c5, \u22c5) represents the Wasserstein distance between two probability densities; sum(\u22c5) shows the sum of all the elements of a vector."}, {"title": "Preliminaries and related works", "content": "In this section, we introduce the concise description of the static/dynamic OT problem with quadratic cost c (x, y) = \u2225x \u2212 y\u22252 in turn and illustrate their compatibility.\nStatic optimal transport Assuming that \u03c10 and \u03c11 \u2208 P (Rn, W2) represent two continuous probability density functions, we call a map M: Rn \u2192 Rn measure-preserving M#\u03c10 = \u03c11, if \u222bB \u03c11 (y) dy = \u222bM\u22121(B) \u03c10 (x) dx holds for any Borel set B \u2282 Rn. The Monge problem(Monge 1781) aims to find the OT map Mot among all measure-preserving maps, that is\n$M_{ot} = \\underset{M\\#P_0=p_1}{\\arg \\inf} \\frac{1}{2} \\int_{\\mathbb{R}^n} ||x \u2013 M (x) ||^2\\rho_0 (x)dx$,\n(1)\nthe one that minimizes the total transport cost. Moreover, there exists a convex function u(x) : Rn \u2192 R(also known as the Brenier potential(Brenier 1991)) that satisfies that \u2207u = Mot is the unique solution to (1) when quadratic cost. From Jacobian equation, one can get the necessary condition for Brenier potential, namely, Monge-Amp\u00e8re equation(Villani et al. 2009)\n$det \\nabla^2u (x) = \\frac{\\rho_0 (x)}{\\rho_1(\\nabla u (x))}$ (2)\nDynamic optimal transport Benamou and Brenier introduce an alternative analytical framework by connecting (1) to continuum mechanic frameworks(Benamou and Brenier 2000). They investigate the time-dependent transport through the dynamic evolution from \u03c10 at initial time t = 0 to \u03c11 at end time t = 1 and denote the intermediate process as \u03c1t, t \u2208 [0, 1], which satisfies the continuity equation\n$\\frac{\\partial \\rho_t (x)}{\\partial t} + \\nabla \\cdot (v (x, t) \\rho_t (x)) = 0,$\n(3)\nwhere v (x, t) : Rn \u00d7 [0, 1] \u2192 Rn represents the time-dependent velocity field that describes the dynamic transport process. Therefore, Benamou-Brenier formulation aims to find the velocity field with minimum total kinetic energy under the premise of satisfying the boundary conditions \u03c10, \u03c11 and conservation of mass formula (3), that is,\n$v_{ot} (x, t) = \\underset{v}{\\arg \\inf} \\int_0^1 \\int_{\\mathbb{R}^n} \\frac{1}{2} ||v (x, t) ||^2\\rho_t (x) dx dt.$ (4)\nCompatibility The static Monge problem (1) and the dynamic Benamou-Brenier problem (4) are equivalent and compatible in terms of the quadratic transport cost(Villani et al. 2009). Specifically, when the boundary conditions are the same, the total transport cost of the OT map Mot is equal to the total kinetic energy of the optimal velocity field vot, so they are both taken as W2 metrics. In addition, the two can also be transformed by McCann displacement interpolation(McCann 1997)."}, {"title": "Diffusion models and probability flow", "content": "The intrinsic driver of DMs is stochastic differential equation(SDE), which is usually expressed as\n$dx = -f (t) xdt + g (t) dW_t, x (0) \\sim p_0 (x),$ (5)\nwhere f (t): [0, T] \u2192 R\u22650 represents the non-negative drift coefficient, g (t): [0, T] \u2192 R>0 implies the positive diffusion term, p0 is the initial data distribution and Wt stands for a time-dependent n-dimensional standard Wiener process. The marginal distribution pt of the solution to (5) at time t is strictly described by the Fokker-Planck equation(FPE)(Risken and Risken 1996)\n$\\frac{\\partial p_t (x)}{\\partial t} + \\nabla \\cdot (- f(t) x p_t (x)) - \\frac{g (t)^2}{2} \\Delta p_t (x) = 0,$\n(6)\nand has a unique steady state Gibbs distribution(Gardiner et al. 1985), which we refer to as p\u221e. According to (6), there exists a deterministic process, which we call the probability flow ordinary differential equation(ODE)(Song et al. 2020)\n$\\frac{dx}{dt} = -f (t) x - \\frac{g (t)^2}{2} \\nabla log p_t (x), x (0) \\sim p_0 (x)$ (7)\nthat shares the same marginal probability density pt with (5). The training process of DMs refers to use a parameterized neural network S\u03b8 (x, t) to approach score \u2207 log pt (x), and the target loss function JSM (\u03b8, \u03c6, 0, T) is defined as the weighted mean square error(MSE)(Song et al. 2020)\n$ \\phi (t) E_{p_t} [|| S_\\theta (x, t) \u2013 \\nabla log p_t (x)||^2]dt,$ (8)\nwhere \u03b8 is a learnable parameter and \u03c6 (t) : [0, T] \u2192 R>0 means a positive weight associated with time t. In practice, however, JSM is often difficult to quantify due to lack of information about \u2207 log pt (x)(Song et al. 2021). Therefore, it is common to convert it to a treatable conditional score matching loss JDSM (\u03b8, \u03c6, 0, T)(Vincent 2011) via letting pt (x) = pt (x|x(0)) p0|t(x|x(0)), here p0|t indicates the probabilistic transition kernel from time 0 to t."}, {"title": "Related works", "content": "Relationship between DMs and OT theory The authors of (Khrulkov et al. 2022) show that the probability flow can be considered as an OT map under the initial assumption of a multivariate normal distribution. However, a counterexample of continuous initial distribution is provided in (Lavenant and Santambrogio 2022) to show that the probability flow is not optimal. Recently, Zhang et al.(Zhang et al. 2024a) further extend their conclusion and argue that the probability flow over any closed interval in (0,\u221e) coincides with the OT map under the condition of finite training samples. In addition, Kwon et al.(Kwon, Fan, and Lee 2022) indicate that DMs also minimize the Wasserstein distance between the true distribution and the generated distribution.\nSingularity of DMs Under the initial condition of discrete probability density, the score \u2207 log pt (x) does not exist at t = 0, resulting in S\u03b8 (x, t) cannot be Lipschitz continuous across [0, T](Zhang et al. 2024b). Therefore, Song et al.(Song et al. 2020) suggest to choose a small \u03b5 > 0 to terminate the sampling process to ensure the quality of the generated image(Lu et al. 2022; Song et al. 2023; Zhang et al. 2024b). In addition, there are numerous theoretical analysis based on this idea(Benton et al. 2023; Li, Chen, and Li 2024).\nMain results\nIn this section, we will elaborate the main theoretical results of this paper. For a more detailed analysis and proof, refer to section C of the Appendix."}, {"title": "Diffusion models secretly compute the dynamic optimal transport", "content": "Under the initial condition of discrete probability density \u03c10 (x) = \u03a3i\u2208I \u03b4(x \u2212 xi), |I| < \u221e, Zhang et al.(Zhang et al. 2024a) prove that the probability flow, the analytical solution of (7), conform to dynamic OT map on any closed time interval in (0,\u221e). Therefore, vot (x, t) = \u2212 f (t) x \u2212 g(t)2/2 \u2207 log pt (x) is the corresponding optimal velocity field with the lowest kinetic energy under the W2 metric, and then we express the probability flow from time s to t as Motts : Rn \u2192 Rn, M\u00f6tt (xs) = xt, here 0 < s, t < \u221e. It is worth noting that the probability flow is reversible(Lavenant and Santambrogio 2022), and (Mots)\u22121 = Msot. In this view, the training and sampling processes of DMs are equivalent to different steps in calculating time-dependent OT.\n(I) Training stage: Matching the optimal velocity field. If only optimization results are considered, then the weighted MSE loss function (8) over [\u03b5, T] is equal to\n$\\int_\\epsilon^T \\phi (t) E_{p_t} [||v_\\theta (x, t) \u2013 v_{ot} (x,t)||^2]dt,$\n(9)\nwhere v\u03b8 (x, t) = \u2212 f (t) x \u2013 g(t)2/(2) S\u03b8 (x, t).\n(II) Sampling stage: Solving the probability flow ODE. Substituting the learned v\u03b8 (x, t) into (7) gives an approximation reverse probability flow ODE\n$\\frac{dx}{dt} = v_\\theta (x, t) dt, t \\in [\\epsilon, T], x (T) \\sim q_T (\\textrm{x}),$\n(10)\nwhich is also the locus equation of the sampling process with qT as the initial prior noise distribution. The solution of (10) provides an approximation MT\u03b8,\u03b5 : Rn \u2192 Rn of the time-dependent OT map MTot.\nTheorem 1 Assume that MTot, and MT\u03b8,\u03b5, are the true solution of reverse probability flow ODE (7) and (10), respectively, with qT = pT as the same initial condition, then\n$||M^{T,\\epsilon}_{ot} - M^{T,\\epsilon}_{\\theta}||_{L^2(q_T)} \\le \\frac{\\sqrt{2I (\\epsilon)}}{(I (T) + \\overline{I (T)})} e^{\\frac{T \u2013 J_{DSM}(\\theta,\\phi,\\epsilon, T)}{2}},$ (11)\nhere I (t) = exp (\u222bt0 f (\u03c4) d\u03c4) and I\u0304(t) = \u222bt0 exp (\u222bt0 f (\u03c4) d\u03c4) d\u03c4 stands for integrating factor, \u03a6 (t) = g (t)/(I (t))^2, and LS\u03b8 implies the continuous Lipsitz constant of S\u03b8 (x, t) over [\u03b5, T].\nCorollary 1 Under the same setting as in Theorem 1. If pot satisfies Var[E[(\u2207log p0|t (x|x0)) |x0]] = 0, then we have\n$||M^{T,\\epsilon}_{ot} - M^{T,\\epsilon}_{\\theta}||_{L^2(q_T)} \\le \\frac{\\sqrt{2(T-\\epsilon)}}{(I (\\epsilon))^2} e^{\\frac{T-J_{DSM}(\\theta,\\phi,\\epsilon, T)}{2}}.$ (12)\nCorollary 2 Under the same setting as in Corollary 1. Let q\u03b5 = (MT\u03b8,\u03b5)#qT, if vo (x, t) solves problem (4) with two boundary conditions p0 = qT and p1 = q\u03b5, then we have\n$W_2 (p_\\epsilon, q_\\epsilon) \\le \\frac{\\sqrt{T \u2013 J_{DSM} (\\theta, \\phi,\\epsilon,T)}}{I(\\epsilon)^2}.$ (13)\nRemark 1 As can be seen from Theorem 1 and Corollary 1, the optimization of DMs also secretly makes the parameterized transport map close to the optimal ground truth.\nRemark 2 We need to point out that Denoising Diffusion Probabilistic Models(DDPM)(Ho, Jain, and Abbeel 2020) satisfy variance condition in Corollary 1 refer to (Kwon, Fan, and Lee 2022)."}, {"title": "The potential gap prevents the reverse process from optimality", "content": "It is worth noting that in practice we take the simple Gibbs distribution p\u221e instead of pT as the vanilla distribution qT of the reverse process(Ho, Jain, and Abbeel 2020; Song et al. 2020), namely qT = p\u221e, so there exist a prior error W2 (pT, qT) \u2260 0, which leads to an insurmountable gap between MTot and MT\u03b8,\u03b5. We still have W2(p\u03b5, q\u03b5) \u2260 0 in the light of Theorem 2 even if S\u03b8 (x, t) reaches precisely the optimal target \u2207 log pt (x) on Rn \u00d7 [\u03b5, T]. In other words, the irreducibility of the potential gap makes the sampling process not optimal in any way."}, {"title": "Theorem 2", "content": "If qT = p\u221e, S\u03b8 (x, t) = \u2207 log pt (x) on Rn \u00d7 [\u03b5, T], we denote q\u03b5 = (M\u03b8,\u03b5)#qT, then W2(p\u03b5, q\u03b5) can be estimated as follows\n$\\frac{I (T)}{\\overline{I (\\epsilon)}} W_2(p_T, q_T) \\le W_2(p_\\epsilon, q_\\epsilon) \\le \\frac{I (T)}{\\overline{I (\\epsilon)}} W_2(p_T, q_T),$ (14)\nwhere I (t) = exp (\u222bt0 f (\u03c4) d\u03c4).\nRemark 3 The Theorem 2 means that the reverse process obtained by optimizing the loss function (8) must have deviation as long as W2(pT, qT) \u2260 0."}, {"title": "Optimal transport is the most intrinsic prior error eliminator", "content": "Based on the contraction property of the Wasserstein distance(Carrillo, McCann, and Villani 2006; Kwon, Fan, and Lee 2022), one can exponentially mitigate the effect of the prior error by prolonging the diffusion time T, which dramatically increases the computing cost. In order to escape this contradictory dilemma, we consider the compatibility between the OT problem (1) and (4). In particular, we use \u2207u as a static OT map from pT to p\u221e, which obviously cannot be regarded as the probability flow defined on finite interval [T, s], here s > T. However, Theorem 3 gives an upper bound of the difference between them and Remark 4 states that the dissimilarity will exponentially approach to zero as s increases. Besides that, it also recommends that \u2207u be the generalized probability flow over [T, \u221e).\nTheorem 3 Suppose u implies the analytical solution of the Monge-Amp\u00e8re equation (2) with boundary condition p0 = pT and p1 = p\u221e, then there exists a constant K independent of s that satisfies\n$||\\nabla u_{T,s}^{ot} - M^{T,s}||_{L^2 (p_T)} \\le K \\{ \\frac{\\overline{I (s)}}{\\overline{I (T)}} W_2 (p_\\infty, p_T) \\}.$ (15)\nRemark 4 Since \u222b0\u221e f(t) dt = \u221e, we have lims\u2192\u221e I(s) = 0 in Theorem 3.\nRemark 5 Similarly, there exists a unique Brenier potential uf, which satisfies \u2207u = (\u2207uT )\u22121 and the Monge-Amp\u00e8re equation (2) with boundary constraints with boundary condition p0 = p\u221e and p1 = pT .\nThe above analysis reveals that static OT is the best single-step prior error eliminator, providing not only the most economical transport cost but also intrinsic compatibility with probability flow. Specifically, we leave the forward process unchanged, and instead treat qT = u(p\u221e) as the initial distribution of the sampling process. Corresponding error analysis is shown in Theorem 4 and Corollary 3."}, {"title": "Extended discussion", "content": "In this section, we will show that our approach can be extended to sampling acceleration in unconditional and conditional generation. It is worth mentioning that a more detailed algorithm flow is shown in section D of the Appendix. Subsequently, we will verify each of them in the experimental section.\n(I) OT accelerated sampler. Based on the previous analysis of Theorem 3, we know that static OT provides the most intrinsic accelerated sampler for DMs. Therefore, we choose a smaller time \u03b5 < T' \u226a T to terminate the diffusion, and then replace the remaining complicated two-stage calculation of time-dependent OT in DMs with solving the Monge-Amp\u00e8re equation (2), where p0 = p\u221e and p1 = pT'. Assuming that the parameterized Brenier potential energy we learn is u, then our accelerated generation distribution can be concretized as q\u03b5 = M\u2207u(p\u221e).\n(II) Geometric view for solving the Monge-Amp\u00e8re equation. The geometric variational method, which was established by (Gu et al. 2016) and was widely applied in deep learning through convex optimization(Lei et al. 2019; An et al. 2020), reveals the intrinsic connection between Brenier(Brenier 1991) and Alexandrov theory(Alexandrov 2005). We briefly describe it from the discrete perspective, and when the number of samples is large enough, it can be generalized to the continuous case.\nGiven discrete data set XTr' = {xi}i\u2208I, pT' the target Dirac probability density PXT\u2032 = \u03a3xi\u2208Tr\u2032 \u03b4(x \u2212 x). Then the static OT map \u2207u : (\u03a9, p\u221e) \u2192 (XT\u2032, PXT\u2032 ) induces the cell decomposition in the source domain \u03a9, namely, \u03a9 = \u222ai\u2208I Wi, which satisfies \u2207u (Wi) = xi and \u222bWi p\u221e(z)dz = I (z). We construct triangulation T based on {cri}i\u2208I to reflect the adjacency of the cell cavity, where cri = \u222bWi zp\u221e(z)dz means the center of gravity of cell Wi. However, there exist some polyhedrons that transverse the singular set when the support of the data distribution is non-convex(Figalli 2010; Chen and Figalli 2017), which means that different vertices of the polyhedron belongs to different mode. Specifically, we can detect if edge [cri, crj] crosses the singular set(Luo et al. 2022) by checking the angles arccos((crj\u2212cri), (ori\u2212crj)/||cri||2||crj||2like (An et al. 2020). Eventually, we discard these edges that span singular sets and the remainder of T is written as T\u02c6, which can be decomposed into different simple connected branches T\u02c6 = \u222ak\u2208K Tk, where K implies a categories set, Tk stands for complex and Tk \u2229 Tk\u2032 = \u00d8 for \u2200k \u2260 k\u2032.\nUnconditional generation. Sampling latent code z \u223c p\u221e, if z \u2208 Tk, we can find the n-dimensional simplex o that contains z. Let the matrix of all vertices of o be represented by Co \u2208 Rn\u00d7(n+1), therefore there exists unique positive weight vector \u03bb \u2208 Rn+1 that satisfies z = Co\u03bb and sum(\u03bb) = 1. Then xT\u2032 = \u2207u (Co) \u03bb. The whole process above is summarized in schematic Fig. 2 and Algorithm 1 in the Appendix.\nConditional generation. Assume that the noisy data set XTr\u2032 has a corresponding label L = {li}i\u2208I, li \u2208 K and C(x) : XT \u2192 K means classification map. Then we have C \u25e6 \u2207u (cr) = li and C \u25e6 \u2207u(cr) = C \u25e6 \u2207u(cr\u2032) for any cri, crj \u2208 Tk, k \u2208 K. If one want to generate a new sample labeled lnew \u2208 K, he or she just need to randomly sample z the complex region Tk with the known label lnew, namely z \u2208 Tk and C \u25e6\u2207u(Tk) = lnew. Specifically, we start by randomly uniformly selecting a n-dimension simplex in Tk, denoted as \u03c3, whose vertex matrix is represented by Co \u2208 Rn\u00d7(n+1). Then the vector \u03b2 is extracted from n+1-dimensional uniform distribution to construct the weight \u03bb = \u03b2/sum(\u03b2). Then the sampling point in the complex Tk is z = Co\u03bb, so the final new sample generated by linear expansion is xT\u2032 = \u2207u(Co) \u03bb. The Fig. 3 and Algorithm 2 in the attachment record the entire process.\n(III) Error analysis. Let the analytical Brenier potential satisfies the Monge-Amp\u00e8re equation (2) with boundary constraint p0 = p\u221e and p1 = pT' as uf, and the geometric variational guides us to approximate it by \u0398\u2032-parameterized convex continuous network u\u0398. Therefore we have a substitute \u2207u for the true OT map \u2207uf, and Theorem 4 measures the upper bound of the error between them.\nTheorem 4 Suppose u\u0398 is an approximation of true Brenier potential energy uT,, then we have\n$|| u_{T} \u2013 \\nabla u_{\\Theta} ||_{L_2(p_\\infty)} \\le K || u_{T} \u2013 \\nabla u_{\\Theta} ||,$\n(16)\nwhere K is a positive constant that only relates to \u03a9 and XT'.\nCorollary 3 The error upper bound of the generation distribution of DMs with OT prior eliminator is\n$W_2(p_\\epsilon, q_\\epsilon) \\le \\sqrt{2(T' \u2013 \\epsilon)J_{SM}(\\theta, \\phi,\\epsilon, T')} + K || u_{T} \u2013 \\nabla u_{\\Theta} ||.$\n(17)\n(IV) Relationship with ODE accelerated sampling of DMs. In order to speed up the inference process of images, this kind of method aims to explore the higher-order numerical solution of probability flow ODE (10) with initial condition qT = p\u221e. Existing accelerated solver of probability flow ODE include DPM-Solver(Lu et al. 2022), diffusion exponential integrator sampler(DEIS)(Zhang and Chen 2022), predictive correction framework(UniPC)(Zhao et al. 2024), and so on. They usually ignore the impact of prior error, and the numerical errors will gradually accumulate due to long-term diffusion. By proving that the probability flow converges to the gradient of solution to the Monge-Amp\u00e8re equation(see Theorem 3), we give a more analytical single step ODE accelerated sampler, which eliminates the prior error. More importantly, our approach only needs to train DMs over time interval [\u03b5, T'], thus reducing training cost, which is not covered by other above-mentioned existing ODE acceleration methods.\n(V) Relationship with early stop strategy of DMs. This is a more direct acceleration strategy designed to replace most diffusion steps with single step probability distribution transformation, such as GANS & CT(Zheng et al. 2022), GANS & VAEs(Lyu et al. 2022), etc. Therefore the early stop strategy is only a special case of the prior error problem. Moreover, we show that OT is the eliminator that best fits DMs, which provides deeper insight into (Li et al. 2023)\u2019s framework. This dropout idea is extended by (Chung, Sim, and Ye 2022) to super-resolution, image inpainting and MRI reconstruction task. It is worth mentioning that both (Li et al. 2023) and (Franzese et al. 2023) try to find the optimal number of diffusion steps by experiment, but the emphasis is different."}, {"title": "Experiments", "content": "In this section, we will present extended visualizations of OT as a prior error eliminator for accelerating sampling in unconditional and condition generation. All experiments were conducted by training the model on two RTX 3090 GPUs and performing sampling on a single NVIDIA A40. More detailed experimental parameters and visual results are shown in section E of the Appendix.\nExperiment settings\nDatasets. In this paper, we perform the proposed approach mainly on three public datasets, consisting of Cifar-10 32\u00d732(Krizhevsky, Hinton et al. 2009), CelebA 64\u00d764 (Liu et al. 2015) and FFHQ 256\u00d7256(Russakovsky et al. 2015).\nBaseline methods. Comparison models can be divided into two classes, the first is ODE or truncated acceleration strategies that are extremely relevant to our model, including DPM-Solver(Lu et al. 2022), DEIS(Zhang and Chen 2022), UciPC(Zhao et al. 2024), ES-DDPM(Lyu et al. 2022) and TDPM & TDPM+(Zheng et al. 2022). The other part is the classical diffusion modeling method used to compare the generating effect, including DDIM(Song, Meng, and Ermon 2020), NCSNv2(Song and Ermon 2020) and EDM(Karras et al. 2022).\nEvaluation metrics. We consider diffusion Terminate Time T, Sampling Time and number of function evaluations(NFE) as criteria for modeling efficiency, whereas Prior Error W2(pT, qT) is taken to mean the effects of various eliminators. In addition, Fr\u00e9chet Inception Distance(FID)(Heusel et al. 2017) represents the quality of the generated image, while Precision & Recall in (Kynk\u00e4\u00e4nniemi et al. 2019) reflect generative diversity.\nDiffusion schedule. All acceleration methods are compared strictly in the same DDPM(Ho, Jain, and Abbeel 2020) backbone with noisy schedule (\u03b21 = 2 \u00d7 10\u22124, \u03b2T = 0.01, T = 1000), which correspond to discretization of Variance Preserving(VP) SDE(Song et al. 2020), namely, f (t) and g (t) in (5) satisfy f (t) = \u03b2 (t) and g (t) = \u221a\u03b2 (t). In this setting, (5) has a standard Gaussian steady state distribution when T \u2192 \u221e, i.e., p\u221e = N(0, I). If we want to construct OT accelerated DM with T' = 100, we first initialize \u03b21, \u03b22, . . . , \u03b2T\u2032, and then terminate diffusion process at step 100.\nEnd time of sampling process \u03b5. In ref. (Lu et al. 2022), the authors suggest that the diffusion time interval [0, T = 1000] of DDPM is uniformly normalized to [0, 1], and then end time of the sampling process is set to 10\u22123. In this article, we also follow this choice, therefore \u03b5 = 1.\nRemark 6 It is worth mentioning that for the convenience of expression in the experiment, the diffusion termination time of all acceleration models is uniformly denoted as T.\nEvaluation of prior error\nIn Tab. 1, we list the termination time T required for the benchmark accelerated sampling strategy of DMs. It can be seen that the existing methods for accelerating solve the approximate inverse probability flow ODE (10) do not pay attention to the forward process, so they all need to complete DMs, resulting in high training cost. However, the early stop strategy saves a lot of training resources by selecting a relatively small T. This is confirmed by the experimental results in section A.3 of (Lyu et al. 2022)."}, {"title": "Measurement of sampling efficiency", "content": "We use the total running time of sampling process to measure the sampling efficiency. Specifically, we refer to the random noise set as {zi}Nst i=1 \u223c N(0, I), and the time required to deduce the image xi from the i-th noise by a certain modeling method is RunTime(zi, xi). Then the average sampling time(second/image) is formulated as\n$Sampling \\ Time = \\frac{1}{N_{st}} \\sum_{i=1}^{N_{st}}RunTime(z^i,x^i).$ (18)\nIn this article, we take Nst = 1500. By reporting the values of (18) for all ODE or truncation acceleration strategies in Tab. 1, it is not difficult to see that our model has the highest sampling efficiency"}]}