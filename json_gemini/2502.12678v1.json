{"title": "Multi-Step Alignment as Markov Games:\nAn Optimistic Online Gradient Descent Approach with Convergence Guarantees", "authors": ["Yongtao Wu", "Luca Viano", "Yihang Chen", "Quanquan Gu", "Zhenyu Zhu", "Kimon Antonakopoulos", "Volkan Cevher"], "abstract": "Reinforcement Learning from Human Feedback\n(RLHF) has been highly successful in aligning\nlarge language models with human preferences.\nWhile prevalent methods like DPO have demon-\nstrated strong performance, they frame interac-\ntions with the language model as a bandit prob-\nlem, which limits their applicability in real-world\nscenarios where multi-turn conversations are com-\nmon. Additionally, DPO relies on the Bradley-\nTerry model assumption, which does not ade-\nquately capture the non-transitive nature of hu-\nman preferences. In this paper, we address these\nchallenges by modeling the alignment problem as\na two-player constant-sum Markov game, where\neach player seeks to maximize their winning rate\nagainst the other across all steps of the conver-\nsation. Our approach Multi-step Preference Op-\ntimization (MPO) is built upon the natural actor-\ncritic framework (Peters & Schaal, 2008). We fur-\nther develop OMPO based on the optimistic online\ngradient descent algorithm (Rakhlin & Sridharan,\n2013; Joulani et al., 2017). Theoretically, we pro-\nvide a rigorous analysis for both algorithms on\nconvergence and show that OMPO requires $O(\u20ac^{-1})$\npolicy updates to converge to an $\\epsilon$-approximate\nNash equilibrium. We also validate the effective-\nness of our method on multi-turn conversations\ndataset and math reasoning dataset.", "sections": [{"title": "1. Introduction", "content": "In recent years, the integration of large language models\n(LLMs) (Brown et al., 2020; Achiam et al., 2023; Team\net al., 2023; Dubey et al., 2024) into various applications\nhas highlighted the need for advanced preference alignment\nmethods (Ziegler et al., 2019; Stiennon et al., 2020; Bai\net al., 2022; Ouyang et al., 2022; Rafailov et al., 2023). As\nmodels increasingly engage in complex decision making\nor reasoning scenarios, e.g., GPT-40 and 01\u00b9, the ability\nto align their outputs with user preferences has received\nmore attention. However, existing works on reinforcement\nlearning from human feedback (RLHF) focus mostly on\none-step preference (Rafailov et al., 2023; Meng et al.,\n2024; Munos et al., 2024; Azar et al., 2024; Zhang et al.,\n2024; Wu et al., 2025), which neglects indispensable\nintermediate preferences within the answer and limits the\nmodel's alignment ability. For example, in multi-round\nconversations, alignment must occur at each turn to meet\nuser needs. Similarly, in mathematical reasoning with\nchain-of-thought prompting, step-by-step validation is\nessential to ensure accuracy in the final result. The reliance\non final-output feedback in most existing RLHF meth-\nods (Wang et al., 2023; Shani et al., 2024) neglects these\nintermediate steps, highlighting the need for multi-step\npreference optimization to enhance alignment capabilities.\nMeanwhile, earlier alignment methods e.g., DPO and its\nvariants step-DPO (Lai et al., 2024; Lu et al., 2024), typi-\ncally model the pairwise preference by the Bradley-Terry\n(BT) model (Bradley & Terry, 1952), which assigns a score\nfor each answer based on its preference. This assumption\nof the model cannot capture the non-transitive preference,\nwhich is often observed in the averaged human preferences\nfrom the population (Tversky, 1969; Gardner, 1970). While\na recent line of work has modeled the alignment process un-\nder the framework of general preference (Azar et al., 2024;\nMunos et al., 2024; Wu et al., 2025; Rosset et al., 2024), and\nthus bypasses the BT model assumption, the challenge of\nmulti-step preference optimization remains underexplored.\nIn this paper, we first address this gap by formulating multi-\nstep general preference optimization within the framework\nof two-player Markov games (Shapley, 1953), where each\nplayer seeks to maximize their winning rate against the\nother across all steps of the conversation. Next, we intro-\nduce Multi-step Preference Optimization (MPO) drawing on\ninsights from the natural actor-critic framework (Peters &\nSchaal, 2008). We further develop OMPO which leverages\nthe optimistic online gradient descent algorithm and benefits"}, {"title": "2. Related work", "content": "In this section, we present a brief summary of the related lit-\nerature, while a more comprehensive discussion is deferred\nto Appx. C due to the limited space.\nRLHF under Bradley-Terry model. Over the years, sig-\nnificant strides have been made towards developing RLHF\nalgorithms from various perspectives under the Bradley-\nTerry (BT) model (Bradley & Terry, 1952). Earlier RLHF\npipelines usually included supervised fine-tuning, learning\na reward model, and reinforcement learning optimization\nwith PPO (Ziegler et al., 2019; Stiennon et al., 2020; Bai\net al., 2022; Ouyang et al., 2022). Due to the instability and\nscaling issues of such a pipeline, direct alignment methods\nsuch as DPO have been proposed to bypass the training of\nthe reward model (Rafailov et al., 2023). Several follow-up\nmethods, such as generalized preference optimization (Tang\net al., 2024), use offline preference data to directly optimize\npairwise preferences against a fixed opponent. A number of\nworks have proposed reference-model-free method (Meng\net al., 2024; Hong et al., 2024). In Meng et al. (2024), the\nimpact of sequence length is mitigated by averaging the\nlikelihood over the length of the sequence. In the multi-step\nscenario, several multi-step variants of DPO are introduced\nin the math reasoning task. Lu et al. (2024) initiate from\nan intermediate step in a correct reasoning process and in-\ncrease the temperature to produce a flawed reasoning path\nleading to an incorrect answer. Meanwhile, Lai et al. (2024)\nleverage GPT-4 to detect the first incorrect step in a multi-\nstep reasoning trajectory, then regenerate from that point to\nobtain the correct path. Together, these serve as the pair of\nsamples for DPO.\nRLHF under general preferences. The reward model in\nthe BT model inherently implies transitivity in preferences.\nHowever, human preferences, especially the resulting av-\neraged human preferences from populations, are usually\nnontransitive (Tversky, 1969; Gardner, 1970). To this end,\nAzar et al. (2024) outline a general framework for RLHF\nstarting from general preference optimization and shows\nthat DPO is a special case with the assumption of BT model.\nThey further proposed IPO without such an assumption.\nSubsequently, Munos et al. (2024) try to solve the alignment\nof non-transitive general preferences using two-player Nash\nlearning in a bandit setting. In their work, preferences are\nregularized through KL divergence to a reference policy, and\nthey prove the convergence of the last iterative. In Swamy\net al. (2024), multi-step alignment is considered while pref-\nerence signals are only applied at the final step. Swamy\net al. (2024) do not demonstrate the effectiveness of this\nframework in large language models. Wu et al. (2025) pro-\npose SPPO, studying bandit alignment under general pref-\nerences. They introduce a novel loss function that increases\nthe log-likelihood of the selected response while decreasing\nthat of the rejected response, in contrast to DPO. Rosset\net al. (2024) start with the Nash learning framework and\npropose Online DPO, which is an iterative version of DPO.\nWang et al. (2023) provide theoretical analysis on multi-step\nRLHF under general preference while practice application is\nnot explored. In Wang et al. (2023), the preference signal is\ngiven for the entire trajectory of an MDP while in this paper\nit is step-wise. Shani et al. (2024) study multi-step alignment\nunder general preferences. However, unlike their approach\nwhere only preferences at the final states are considered, our\nwork is built on a two-player Markov game which assumes\nthat human preference is received at each step. Addition-\nally, we leverage the optimistic online gradient descent to\nachieve a better convergence rate than Wang et al. (2023);\nShani et al. (2024), and utilize Monte Carlo estimation with\na small-scale pairwise reward model, avoiding the need for\nan additional function approximator for the critic network."}, {"title": "3. Problem setting: Multi-step RLHF as\ntwo-player Markov games", "content": ""}, {"title": "3.1. Notation", "content": "We define the prompt to the language model as $x$ and the\nanswer from the language model as $a$. For a multi-turn\nconversation with turn $H$, the prompts and the answers are\ndenoted by $x_h$ and $a_h$, $\\forall h \\in [H]$. The concatenation of a\nprompt $x$ and an answer $a$ is denoted by $[x, a]$ and can be\ngeneralized to the concatenation of multiple prompts and\nanswers, e.g., $[x_1, a_1, ..., x_H, a_H]$. For any two sentences,\ne.g., $[x, a]$ and $[x', a']$, we define a preference oracle as\n$o([x, a]>[x', a']) \\in \\{0,1\\}$, which can provide preference\nfeedback with 0-1 scores, where 1 means the conversation\n$[x, a]$ is preferred and 0 otherwise. We denote $P([x, a] >\n[x', a']) = E[o([x, a] > [x', a'])] $ as the probability that the\nconversation $[x, a]$ is preferred over $[x', a']$. Moreover, we\nhave $P([x, a] > [x',a']) = 1 - P([x', a'] > [x,a])$. An\nautoregressive language model is denoted by $\\pi(a|x)$ which\nreceives input $x$ and generates answer $a$. We denote the\nKL divergence of two probability distributions $p$ and $q$ by\n$D(p||q)$. The Bregman Divergences between two points\nare denoted by $D(p||q)$. The sigmoid function is defined\nby $\\sigma(z) := \\frac{1}{1+e^{-z}}$. Detailed definitions for the notations\nare summarized in Appx. A."}, {"title": "3.2. Problem formulation of multi-step RLHF", "content": "In this section, we introduce the problem setting for multi-\nstep RLHF and we defer the preliminaries on single-step\nRLHF to Appx. B. Specifically, we can cast the multi-step\nalignment process as a finite-horizon Markov Decision Pro-\ncess (MDP). We define $s_h = [x_1, a_1, ..., x_{h-1}, a_{h-1}, x_h]$\nas the state at $h > 1$. We define the action $a_h$ as the an-\nswer given $s_h$. Particularly, we have $s_1 = x_1$. The prompt\nin the next state is sampled under the transition $x_{h+1} \\sim$\n$f(\\cdot|s_h, a_h)$, which is equivalent to $s_{h+1} \\sim f(\\cdot|s_h, a_h)$. The\nequivalence comes from the fact $s_{h+1} = [s_h, a_h, x_{h+1}]$ by\nusing the concatenation operator between sentences. The\nterminal state is $s_{H+1}$. Our setting covers a number of\nalignment problems, and we list some examples below.\nExample 1 (Single-step alignment). In single-step align-\nment, a language model receives one prompt and outputs\none answer. Our framework covers the single-step align-\nment by dissecting the answer into single tokens. Specif-\nically, we set $x_1$ as the prompt, $x_2,...,x_{H+1}$ as empty\nsentences, and the answer $a_h$ at each turn consists of only\none token. Then the horizon $H$ is the number of tokens in the\nanswer. The transition between each state is deterministic.\nExample 2 (Chain-of-thought reasoning alignment). In the\nchain-of-thought reasoning, the horizon $H$ denotes the num-\nber of reasoning steps, where $x_1$ is the initial prompt and\n$x_2,..., x_{H+1}$ are empty. Each $a_h$ corresponds to a reason-\ning step. The transition between each state is deterministic."}, {"title": "", "content": "Example 3 (Mutli-turn conversation alignment). In multi-\nturn conversation, the horizon $H$ denotes the total number\nof turns in the conversation. In the h-th turn, $x_h$ is the\nprompt, and $a_h$ is the answer. The prompt in the terminal\nstate, $x_{H+1}$, is an empty sentence. The transition between\neach state can be deterministic or stochastic.\nNext, we define the pair-wise reward function of two state-\naction pairs as the preference of two trajectories:\n$r(s_h, a_h, s'_h, a'_h) = P([s_h, a_h] > [s'_h, a'_h])$.\nUpon this point, we can define the MDP as a tuple $M =$\n$(S, A, f, r, \\nu_1, H)$, where $S$ is the state space, $A$ is the ac-\ntion space, $H$ is the horizon (total steps), the initial state\ndistribution $\\nu_1$ is a distribution over the initial prompt $x_1$.\nNote that in a two-player game environment, each state in $S$\nis a pair of $s_h$ and $s'_h$ generated by two policies. Our goal is\nto identify the Nash equilibrium (or von Neumann winner)\nof the following two-player constant-sum Markov game:\n$(\\pi^*, \\pi'^*) = \\arg \\max_\\pi \\min_{\\pi'} E_{s_1\\sim\\nu_1, s_h, a_h, s'_h, a'_h} [\\sum_{h=1}^H r(s_h, a_h, s'_h, a'_h)], \\tag{Game}$\nwhere $s_1 = s'_1 = x_1, a_h \\sim \\pi(\\cdot|s_h), a'_h \\sim \\pi'(\\cdot|s'_h), s_{h+1} \\sim$\n$f(\\cdot|s_{h-1}, a_{h-1}), s'_{h} \\sim f(\\cdot|s'_{h-1}, a'_{h-1})$.\nHere we make a few remarks on the benefit of incorporat-\ning human preferences at each step. More detail on the\nmotivation can be found at Appx. G.\nRemark 1. If two conversations of $H$ turns, $s_{H+1}$ and\n$s'_{H+1}$, are globally similar but differ in the early turns (e.g.,\n$s_2$ are better than $s'_2$), more credit should be assigned to\n$s_{H+1}$, encouraging the model to align with it. This fol-\nlows the principle that humans typically master simpler and\nearlier tasks before progressing to more complex ones.\nRemark 2. From a practical standpoint, including per-\nstep preference data generates a richer dataset for training,\nhelping the model learn which reasoning steps are correct\nor wrong. This incremental feedback can enhance overall\nperformance by reinforcing the importance of foundational\nsteps in reasoning.\nNext, we present some additional notation. We define the\npair-wise value function as follows\n$V_h^{\\pi, \\pi'}(s, s') = E [ \\sum_{h=h}^H r(s_h, a_h, s'_h, a'_h) | s_h = s, s'_h = s'],$\nwhere $a_{\\hat{h}} \\sim \\pi_{\\hat{h}}(\\cdot|s_h), a'_{\\hat{h}} \\sim \\pi'_{\\hat{h}}(\\cdot|s'_h), s_{\\hat{h}+1} \\sim f(\\cdot|s_{\\hat{h}}, a_{\\hat{h}}),$\nand $s'_{\\hat{h}+1} \\sim f(\\cdot|s'_{\\hat{h}}, a'_{\\hat{h}})$. We will often denote $V_h^{\\pi, \\pi'}$\nomitting the subscript, i.e., as $V^{\\pi, \\pi'}$. Moreover, notice that\nwe consider potentially non stationary policies, i.e. they are"}, {"title": "4. Method", "content": "indexed by $h$. We denote by $\\pi$ the non stationary policy and\nby $\\pi_h$ the distribution over actions at step $h$ corresponding\nto the non stationary policy $\\pi$.\nWe define the pair-wise Q-function as follows:\n$Q_h^{\\pi, \\pi'}(s, a, s', a') = r(s, a, s', a') + E[ \\sum_{h'=h+1}^H r(s_{h'}, a_{h'}, s'_{h'}, a'_{h'}) ],$\nwhere $s_{h+1} \\sim f(\\cdot|s_h, a_h)$ and $s'_{h+1} \\sim f(\\cdot|s'_h, a'_h)$.\nLemma 1. (Adapted from (Puterman, 1994)) The pair-wise\nvalue function and pair-wise Q-value function satisfy the\nBellman equation, i.e., for all $h \\in [H]$: $Q_h^{\\pi, \\pi'}(s, a, s', a') =$\n$r(s, a, s', a') + E_{s\\sim f(\\cdot|s, a), s'\\sim f(\\cdot|s', a')} [V_{h+1}^{\\pi, \\pi'}(s, s')]$\nand\n$V_h^{\\pi, \\pi'}(s, s') = E_{a\\sim \\pi_h(\\cdot|s), a'\\sim \\pi'_h(\\cdot|s')} Q_h^{\\pi, \\pi'}(s, a, s', a').$\nBy Lemma 1, we can rewrite Game as follows:\n$(\\pi^*, \\pi'^*) = \\arg \\max_\\pi \\min_{\\pi'} E [ \\sum_{h=1}^H r(s_h, a_h, s'_h, a'_h)]$\n$ = \\arg \\max_\\pi \\min_{\\pi'} E_{s_1\\sim\\nu_1} V^{\\pi, \\pi'}(s_1, s'_1). \\tag{1}$\nGiven the above notation, we can formalize our objective.\nWe look for a policy $\\pi$ satisfying the following definition of\napproximate equilibrium.\nDefinition 1 ($\\epsilon$-approximate Nash equilibrium). A policy\n$\\pi$ is said to be an approximate Nash equilibrium if it holds\nthat:\n$(V_1, V^{\\pi, \\pi^*}) - \\min_{\\pi \\in \\Pi} (V_1, V^{\\pi, \\pi'}) \\le \\epsilon,$\nand\n$\\max_{\\pi \\in \\Pi} (V_1, V^{\\pi, \\pi'}) - (V_1, V^{\\pi^*, \\pi^*}) \\le \\epsilon.$\nDefinition 2 (Occupancy measures). Given the policy $\\pi$,\nthe occupancy measure of $\\pi$, is defined at stage $h$ as\n$d^{\\pi}(s, a) = Pr(s_h = s, a_h = a)$, where $s_1 = x_1$\n$a_h \\sim \\pi_h(\\cdot|s_h), s_h \\sim f(\\cdot|s_{h-1}, a_{h-1})$. We also de-\nfined^\\pi(s, a) = Pr(s_h = s, a_h = a | s_1 = s_1)$. In\naddition, given the policies $\\pi, \\pi'$, the occupancy measure of\n$(\\pi, \\pi')$ at stage $h$ is defined as $d^{\\pi, \\pi'}(s, a, s', a') = Pr(s_h =$\n$s, a_h = a, s'_h = s', a'_h = a')$, where $s_1 = s'_1 = x_1$\n$a_h \\sim \\pi(\\cdot|s_h), a'_h \\sim \\pi'(\\cdot|s'_h), s_h \\sim f(\\cdot|s_{h-1}, a_{h-1}),$\nand $s'_h \\sim f(\\cdot|s'_{h-1}, a'_{h-1})$.\nRemark: The value function at the initial state can be rep-\nresented as an inner product between the reward function\nand the occupancy measure, i.e., $V^{\\pi, \\pi'} = \\sum_{h=1}^H (r, d)$.\nGiven the structure of the game where the sequences of\nsentences and answers are generated independently by the\ntwo agents given the initial state $s_1$, the occupancy mea-\nsure at each step can be factorized as the product of the\ntwo agents occupancy measures given $s_1$. In particular, we\nhave $d^{\\pi, \\pi'}(s, a, s', a')|s_1 = d^{\\pi}(s, a)|s_1 \\cdot d^{\\pi'}(s', a')|s_1$ for\nall $h, s, a, s', a'.$"}, {"title": "4. Method", "content": "We first develop our method Multi-Step Preference Opti-\nmization (MPO) based on the natural actor-critical frame-\nwork (Peters & Schaal, 2008; Alacaoglu et al., 2022) in\nSec. 4.1. Next, we introduce Optimistic Multi-Step Prefer-\nence Optimization, dubbed OMPO, in Sec. 4.2. The frame-\nwork is inspired by the idea of optimism used in online learn-\ning and in min-max optimization with improved theoretical\nguarantees (Popov, 1980; Chiang et al., 2012; Rakhlin &\nSridharan, 2013).\n4.1. MPO with natural actor-critic\nThis section presents our first method to find an approximate\nsolution to Game. In order to find an $\\epsilon$-approximate Nash\nequilibrium, the MPO method builds upon the next lemma\nwhich decomposes the difference of two value functions to\nthe Q function at each step. Lemma 2 is the extension of\nKakade & Langford (2002) to the multi-agent setting where\nthe dynamics are controlled independently by each player\nbut the reward depends on the joint-state action tuple. In\nKakade & Langford (2002), the Q function is a function of\nonly one state-action pair while in our setting the Q function\nis based on two state-action pairs.\nLemma 2 (Value difference lemma (Adapted from Kakade\n& Langford (2002))). For a finite horizon MDP with initial\ndistribution $\\nu_1$ it holds that:\n$(V_1, V^{\\pi, \\pi^*} \u2013 V^{\\pi', \\pi'}) = E_{s_1\\sim\\nu_1} [\\sum_{h=1}^H E_{s\\sim d^{\\pi}|s_1}\nE_{s'~d^{\\pi'}|s_1} [Q_h^{\\pi, \\pi^*}(s, \\cdot, s', a'), \\pi_h(\\cdot|s, s_1)\n- Q_h^{\\pi, \\pi^*}(s, \\cdot, s', a'), \\pi'_h(\\cdot|s, s_1)]].$\nThe proof can be found at Appx. D.2. In our setting, the\ninitial state $s_1$ is a deterministic function of the state $s$ so\nwe can remove $s_1$ from the conditioning in the policy\u00b2. To\nhighlight this fact we denote as $s_1(s)$ the only initial state\nthat can lead to $s$. By setting $\\pi' = \\pi = \\pi^*$ in Lemma 2 and\n$\\pi = \\pi^*$ and summing from $t = 1$ to $T$ we obtain:\n$\\sum_{t=1}^T (V_1, V^{\\pi^*, \\pi^*} - V^{\\pi, \\pi^*}) = E_{s_1\\sim\\nu_1} [\\sum_{t=1}^T \\sum_{h=1}^T E_{s\\sim d^{\\pi^*}|s_1}\nE_{s'~d^{\\pi^*}|s_1} [Q_h^{\\pi, \\pi^*}(s, \\cdot, s', a'), \\pi_h(\\cdot|s, s_1)\n- Q_h^{\\pi, \\pi^*}(s, \\cdot, s', a'), \\pi'_h(\\cdot|s, s_1)]].$\nSince the sum over t commutes with the expecta-\n\u00b2This is motivated by practical LLM training, where system\nprompts such as \"user\" and \"assistant\u201d are inserted before every\n$x_h$ and $a_h$, respectively. As a result, one can infer a unique $s_1$ for\nevery $s$. The conditioning of the policy on the initial state might\nappear unusual at the first glance but it is in fact common in the\nsetting of Contextual MDPs (see for example (Levy et al., 2023)).\nIndeed, the initial state $s_1$ could be interpreted as a context and we\noptimize over policies that depend on both the initial context and\nthe current state."}, {"title": "Algorithm 1 MPO (Theoretical Version)", "content": ""}, {"title": "5. Experiments", "content": "In this section, we provide several numerical results while\nadditional detail on the dataset, experimental set-up, and\nablation studies are deferred to Appx. F.\n5.1. Tabular experiment\nFirst, we consider a synthetic experiment in which the state\naction functions can be computed exactly for both OMPO\nand MPO. We generate 10 random gridworlds with a number\nof states and actions sample uniformly from the intervals\n[1, 100] and [2, 10]. We plot the exploitability computed as\n$V_1, max_{\\pi} V^{\\pi, \\pi^*} - min_{\\pi'} V^{\\pi^*, \\pi'}$\\n$V_1, V^{\\pi^*, \\pi^*}$, which is a standard metric to\nevaluate the distance from a Nash equilibrium. In particular,\nwhen $(\\pi^k, \\pi^k)$ is a Nash equilibrium, the exploitability is\n0. We can see that OMPO achieves very low exploitability\nafter 100 updates while 2000 updates are needed by MPO."}, {"title": "Algorithm 3 OMPO (Theory Version)", "content": ""}, {"title": "5.2. Experiment on multi-turn conversation dataset", "content": "In this section, we test the proposed algorithms with\nmulti-turn conversations in MT-bench-101 (Bai et al.,\n2024). We choose Mistral-7B-Instruct-v0.2 as the base\nmodel (Jiang et al., 2023). We use a pre-trained PairRM 4 as\nthe preference oracle. Specifically, given two conversations\n$[s_h, a_h]$ and $[s'_h, a'_h]$, PairRM will return a score that\nindicates the probability that $[s_h, a_h]$ is better than $[s'_h, a'_h]$,\nwhich can be used to considered as the preference oracle\n$P$ defined in the previous section. We select iterative\nDPO (Dong et al., 2024), iterative SPPO (Wu et al., 2025),\nand iterative Step-DPO as our baselines. For both iterative\nDPO and iterative SPPO, we sample $K = 5$ complete\nconversations starting from $s_1$, and estimate the winning\nrate $P([s_{H+1}, a_{H+1}] > (s'_{H+1}, a'_{H+1}]) \\forall k, k' \\in [K]$. Then\nwe select both the best and worst conversations according\nto their winning rates against others, which is defined as\n$\\frac{1}{K} \\sum_{k=1}^K P([s_{H+1}, a_{H+1}] > [s'_{H+1}, a'_{H+1}])$ for the con-\nversation $[s_{H+1}, a_{H+1}]$. Such a pair is used to train DPO"}, {"title": "5.3. Experiment on math reasoning dataset", "content": "As discussed in Sec. 3, our framework can also cover the\nalignment of chain-of-thought reasoning. In this section,\nwe validate the proposed methods in two widely used math\nreasoning datasets: MATH (Hendrycks et al., 2021) and\nGSM8K (Cobbe et al., 2021). We use Qwen2-7B-Instruct\nas the base model and follow the same evaluation procedure\nas in Lai et al. (2024). We adopt the dataset for alignment\nfrom Lai et al. (2024), which contains 10795 samples of aug-\nmented mathematical problems from MetaMath (Yu et al.,\n2024) and MMIQC (Liu et al., 2024b)5. For both MPO and\nOMPO, we select the Llama-3-based model as the prefer-"}, {"title": "Algorithm 4 OMPO (Practical version)", "content": ""}]}