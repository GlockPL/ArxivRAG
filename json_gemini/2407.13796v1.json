{"title": "Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models", "authors": ["Zihao Xu", "Yi Liu", "Gelei Deng", "Kailong Wang", "Yuekang Li", "Ling Shi", "Stjepan Picek"], "abstract": "Security concerns for large language models (LLMs) have recently escalated, focusing on thwarting jailbreaking attempts in discrete prompts. However, the exploration of jailbreak vulnerabilities arising from continuous embeddings has been limited, as prior approaches primarily involved appending discrete or continuous suffixes to inputs. Our study presents a novel channel for conducting direct attacks on LLM inputs, eliminating the need for suffix addition or specific questions provided that the desired output is predefined. We additionally observe that extensive iterations often lead to overfitting, characterized by repetition in the output. To counteract this, we propose a simple yet effective strategy named CLIP\u00b9. Our experiments show that for an input length of 40 at iteration 1000, applying CLIP improves the ASR from 62% to 83%.", "sections": [{"title": "Introduction", "content": "Recent advancements in the security of Large Language Models (LLMs) have exposed multiple jailbreak methods, underscoring the limitations of current Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) safety protocols. These jailbreak attacks use intricately designed prompts to compel LLMs to produce harmful content. They typically occur in two settings: white-box and black-box (Shayegani et al., 2023). In the black-box setting, where attackers lack model access, Deng et al. (2023, 2024b,a); Li et al. (2024a); Liu et al. (2024) and Yong et al. (2023) demonstrated that utilizing a blend of low-resource languages can circumvent model alignment efforts. Xu et al. (2023) described techniques for manipulating the model to generate harmful content by exploiting its inferential capabilities. For white-box attacks, which are often the most effective, the state-of-the-art method involves appending a discrete suffix to the user input and optimizing it via gradient descent (Zou et al., 2023b).\nJailbreak LLM research faces two significant challenges: model overfitting and random outputs leading to jailbreak failure. Schwinn et al. (2024) highlights that employing direct optimization strategies on inputs can cause model overfitting, resulting in increased repetitive responses. Although Schwinn et al. (2023) demonstrates the effectiveness of continuous space attacks using suffixes, direct attacks on inputs and the redundancy of gradient descent on the suffix once the attack target is predetermined have not been thoroughly explored.\nThe second challenge arises when sampling inputs from a standard normal distribution, which can result in random patterns and failed jailbreak attacks. While a single character (e.g., \u201c[\u201d) can jailbreak the model, random outputs derived from sampling inputs can lead to unsuccessful jailbreak attempts. Addressing these challenges is important for enhancing the robustness of continuous attacks in a white-box setting and may potentially aid in understanding the model's inner mechanisms. Therefore, we conduct an empirical study to address two key challenges: (1) sampling inputs without triggering random patterns (RQ1: How to sample input to avoid random patterns), and (2) mitigating the overfitting problem in high iteration counts (RQ2: How to avoid overfitting).\nWe observe that certain subspaces in high-dimensional spaces are comprehensible to LLMs. Our proposed solution, CLIP, is a straightforward yet effective method that projects the input within bounds defined by the mean of the model vocabulary. This approach mitigates overfitting and reduces random variability.\nDuring the Input Construction phase, we identify three types of input: discrete, continuous, and hybrid. Our observations indicate that continuous input sampled from a normal distribution can lead"}, {"title": "Preliminary", "content": "In white-box attacks that exploit gradient information, researchers have predominantly adopted two methodologies: optimizing over discrete suffixes (Zou et al., 2023b) or continuous suffixes (Schwinn et al., 2023). Both methods involve applying gradient descent on the suffix using loss information. Our approach demonstrates that a direct attack on the input is feasible once the target output is specified, as depicted in Figure 1, despite encountering two major challenges.\nThe first challenge is the occurrence of random outputs when employing a standard normal distribution as input (see Figure 3).\nThe second challenge is the tendency for overfitting at a high number of iterations, such as 1000 iterations (see Figure 4)."}, {"title": "Methodology", "content": "This section delineates the context of the attack, its operational mechanism within the model, and the preparatory steps for the input. Followed by this, we detail the development of the CLIP method."}, {"title": "Overview", "content": "We consider a white box scenario where the attackers have full access to the target model M, with aims to manipulate an input X with length N to elicit a specific, malicious output $\\tilde{Y}$ over the intended output Y. The objective is to minimize the loss $L(Y, \\tilde{Y})$, which quantifies the discrepancy between M's output for X and the desired malicious output $\\tilde{Y}$, employing gradient descent. We denote the model's vocabulary as $V_{T\\times H}$, consisting of T distinct tokens, each represented in an H-dimensional hidden space. The mean of this vocabulary is denoted by $\\bar{V}_H$, and the accompanying standard deviation is $\\Sigma = {\\sigma_1, \\sigma_2, . . .,\\sigma_H}$.\nThe input is iteratively updated as follows:\n$X_{t+1} = Clip(X_t - \\eta\\cdot sign(\\nabla_{X_t}L(M(X_t), \\tilde{Y})))$\nThe $Clip()$ function is a projection and will be elaborated upon in Section 3.3. The$X_t$ denotes the input at iteration t, \u03b7 represents the learning rate, and $\\nabla_{X_t}L(M(X_t), \\tilde{Y})$ is the gradient of the loss with respect to $X_t$, directing the adjustments in X to decrease L. This approach is similar to the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014), extensively utilized in image-based adversarial attacks. The process is illustrated in Figure 2."}, {"title": "Input Construction", "content": "Our methodology explores token generation through both discrete and continuous methods."}, {"title": "Discrete Input Construction", "content": "In the discrete framework, we introduce $X_D = {x_{d1},x_{d2},...,x_{dN}}$, a sequence generated by independently sampling each $X_{di}$ from vocabulary V. Each $X_{di}$ adheres to a categorical distribution across T tokens, denoted by $X_{di} \\sim Categorical(V)$. This process generates $X_D$, a sequence structured within an N \u00d7 H matrix."}, {"title": "Continous Input Construction", "content": "For the continuous token generation, we utilize the $\\bar{V}_H$, effectively compressing the vocabulary space into a single vector representative of the average token. This procedure employs a multivariate Gaussian distribution characterized by a variance set \u03a3 for each dimension of H. Consequently, for each continuous token $X_c = {X_{c1}, X_{c2},...,x_{cN}}$, any given element $x_{cij}$ (where i represents the token index and j the dimension in the hidden space) is derived from a Gaussian distribution $\\mathcal{N}(\\bar{V}_{H_j}, \\Sigma_j)$. The reasons for using this distribution will be discussed in Section 3.3.1."}, {"title": "Mixture Input Construction", "content": "For the mixture input scenario, we define $X_M$ as a combination of discrete and continuous sequences, represented by $X_M = X_D \\oplus X_c$, with \u2295 symbolizing the concatenation of these sequences."}, {"title": "Empirical Analysis", "content": ""}, {"title": "Solution to RQ1", "content": "Despite applying representational engineering methods as discussed in (Zou et al., 2023a; Li et al., 2024b), the random pattern challenge persists. However, visualizations reveal that these patterns are separable (see Figure 5), implying the model may comprehend a subspace in high-dimensional space. To address this, we constrain the input to the mean of V (discussed in Section 3.2.2), which effectively mitigates the randomness."}, {"title": "Solution to RQ2", "content": "We find that using both the mean and standard deviation V can mitigate the overfitting issue. Specifically, We clip the input embedding using $\\bar{V}_H$ and \u03a3 with a multiplier to control the range, as detailed in Algorithm 1."}, {"title": "Empirical Evaluator", "content": "We propose a refined jailbreak criteria $JC(\\cdot)$ for evaluating model outputs, different from the conventional reliance on refuse set as utilized in prior works (Zou et al., 2023b; Schwinn et al., 2023). Our approach entails the collection and categorization of model-generated responses into five distinct patterns. The categories include: Random Output is characterized by nonsensical and incoherent text (see Figure 3), indicative of outputs that appear random to human observers. Repetitions are observed when inputs are either initialized with the standard normal distribution or after extensive iterations, as shown in Figure 4. Irrelevant Text refers to outputs that lack relevance, producing sentences that do not connect with the specified target. Refusal to Answer is identified by the generation of an initial target output followed by a subsequent text that exhibits reluctance or refusal to continue, typically with phrases like \"I am sorry\" or \"I cannot\". Jailbreak Text occurs when the model not only generates the requested target output but also supplements it with relevant additional information.\nBased on these observations, we define common criteria for an intended output: (i) It begins with the target output; (ii) It does not incorporate tokens from the refuse set; (iii) Post-target in the output, no more than 50% of the tokens replicate. This would rule out all but the intended Jailbreak Text pattern.\nSince overfitting typically occurs during high iterations, we therefore have established multiple checkpoints throughout the iterations. The attack success rate (ASR) of our study is defined as:\n$ASR@K = \\frac{JC(X_k)}{Dataset}, \\forall X \\in Dataset,$\nwhere k denotes the number of iterations. This checkpoint will reflect the jailbreak status of the method at K iterations. The observation is that if the method is stable and robust, the output quality will not be negatively impacted. Therefore, the Jailbreak Text status will always be maintained."}, {"title": "Evaluation", "content": "In this section, we present the evaluation to access the effectiveness of CLIP."}, {"title": "Models and Dataset", "content": "In this work, we use LLaMa7b (Hugging Face, 2023a), which aligns with the previous study (Schwinn et al., 2023). Our experiments use a dataset characterized by Xu et al. (Xu et al., 2024), which comprises pairs of provocative questions and their corresponding affirmative responses."}, {"title": "Configurations", "content": "To ensure consistency and reproducibility, we set the learning rate to 0.009 across all trials. We employ greedy decoding to ensure replication of our results. These experiments were conducted on a single RTX 4090 GPU using Pytorch (Paszke et al., 2019) and HuggingFace (Wolf et al., 2019)."}, {"title": "Flexibility of the Input Format", "content": "To investigate the impact of input format on our method, we selected input lengths of 1, 40, and 100, along with three distinct types of input: discrete, continuous, and a hybrid format comprising 50% discrete and 50% continuous inputs. However, for input length 1, only discrete or continuous types were considered, given the impracticality of dividing a single token into a mixed format. We employ ASR@100, ASR@500, and ASR@1000 metrics to measure accuracy at specified iterations, acknowledging that these values do not directly represent the method's overall jailbreak rate.\nHowever, our primary objective is to demonstrate the robustness of the CLIP method; thus, we limit our presentation to several checkpoints, as shown in Table 1.\nIn examining the results, we observe a significant relationship where shorter sequence lengths are a good regularizer with an increasing number of iterations. Notably, most scenarios exhibit a reduction in ASR with an increase in the number of iterations, with this effect being particularly pronounced for sequences of length 100. Subsequent analysis, presented in the following section, will demonstrate the efficacy of the CLIP method in further enhancing method stability."}, {"title": "Robustness with the Clip Method", "content": "From Table 1, we have observed the decrease of ASR as iterations number going higher up, For example, in the scenario of continuous type of input length 100, the ASR@100 is 72%, but it dramatically falls to 18% at ASR@1000.\nWith the CLIP method, shown in Table 2, we observed a significant improvement in robustness, particularly as the iteration count increases. Specifically, in the case of Length 1 (an extreme scenario), an increase in \u03b1 from 2 to 10 correlates with a rise in ASR@1000, indicating the model still requires larger exploration space. Further exploration with \u03b1 set to 20 yields superior results, underscoring the importance of adjusting \u03b1 based on the sequence length. Our findings suggest that the optimal \u03b1 value varies across different lengths, with an empirical value of 7 identified as the most effective. For optimal performance, we recommend integrating a shorter length with the CLIP method. However, it may be hard for shorter length to approximate the target output in constrained, like length 1, and therefore require a loose multiplier \u03b1 in Algorithm 1."}, {"title": "Conclusions", "content": "In this study, we demonstrate the effectiveness of an alternative attack channel that utilizes direct input without necessitating a suffix. The nature of the input is versatile and not restricted to a defined question. Our findings suggest that employing the CLIP method with an appropriate choice for \u03b1 to constrain the input domain within a predetermined range is beneficial for mitigating overfitting scenarios. Additionally, reducing the length of the input contributes to improved ASR."}, {"title": "Limitations and Ethical Statement", "content": "This work does not extend to examining the Frobenius norm's impact on jailbreak rates or conducting detailed experiments on the explainability of regularizers, as these topics warrant separate investigations. However, preliminary assumptions are discussed in the early sections. Additionally, while empirical analyses on Vicuna (Hugging Face, 2023b) indicated similar patterns as that of LLaMA, these results are omitted to maintain focus and also to align with the same model choice with prior research (Schwinn et al., 2023). Nevertheless, the data supporting these findings are made available in our code repository to facilitate further exploration.\nWe conducted this research adhering to ethical standards and ensuring our findings are accurately reported. Our aim is to enhance the security of LLMs not to disseminate harmful information or facilitate misuse. We thoroughly examined the released intermediate jailbreak results dataset to ensure no instructions are practical or usable in real-world scenarios."}]}