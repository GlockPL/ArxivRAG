{"title": "AN EXPLAINABLE ATTENTION MODEL FOR CERVICAL\nPRECANCER RISK CLASSIFICATION USING COLPOSCOPIC\nIMAGES *", "authors": ["Smith K. Khare", "Berit Bargum Booth", "Victoria Blanes-Vidal", "Lone Kjeld Petersen", "Esmaeil S. Nadimi"], "abstract": "Cervical cancer remains a major worldwide health issue, with early identification and risk assessment\nplaying critical roles in effective preventive interventions. This paper presents the Cervix-AID-Net", "sections": [{"title": "1 Introduction", "content": "Cervical cancer is the fourth leading cause of death among female malignancies, with high morbidity and mortality\nrates if diagnosed in late stages, mainly affecting sexually active adult women aged over 30 years [1, 2, 3]. The cervix\nis part of the female reproductive system, located in the lowest fibromuscular section of the uterus and accessible for\ninspection and sampling though the vagina. The cervix has different linings. The endocervical canal is lined with\nglandular epithelium, and the ectocervix is lined with squamous epithelium. The squamous epithelium meets the\nglandular epithelium at the squamocolumnar junction (SCJ). The SCJ moves during early adolescence and during a first\npregnancy. The original SCJ originates in the endocervical canal, but over time, the SCJ comes to lie on the ectocervix\nand becomes the new SCJ [4]. In colposcopy terminology, the SCJ is this new SCJ. The epithelium between these two\nSCJs is the transition zone (or transformation zone, TZ), and its position is variable, depending on factors such as age,\nhormonal status, birth trauma, use of oral contraceptives, and pregnancy [5, 6]. Colposcopically the TZ is classified as:\nType 1, Type 2 and Type 3, depending on its visibility [7, 8]. Cervical cancer is preceded by Cervical Intraepithelial\nNeoplasia (CIN) which arises in the TZ. In the year 2018, the World Health Organization (WHO) issued a worldwide\ncall to eliminate cervical cancer [9]. Cervical cancer develops from CIN lesions over years making time for screening,\ndiagnosis and preventive treatment. There is an urgent need for accurate and timely detection of cervical cancer. This is\nparticularly so in low- and middle-income nations, where severe poverty and gender discrimination significantly restrict\na woman's ability to seek care, accounting for almost 88% of cervical cancer fatalities [10].\nMany screening methods are available today to detect precancerous lesions and abnormal growth of epithelial cells on\nthe cervix. These methods include primary screening with cervical cytology (Pap tests), the human papillomavirus\n(HPV) test or co-testing. Cervical cytology requires competent cytologists to perform microscopic analyses, often\nunavailable in low-resource settings [11]. The HPV test is recommended by the WHO strategy due to the high sensitivity\nof the test, but the associated lower specificity increases the number of screen positive females referred for secondary\nscreening by colposcopy and adds to the burden of specialists. Medical authorities recommend colposcopy as the gold\nstandard for assessing cervical cancer precursors. Examining colposcopy images is time-consuming and requires trained\nmedical specialists. Even skilled colposcopist miss cervical precancerous lesions in need for preventive treatment in\n40% of examinations [12]. In recent years, automated decision-making using medical imaging techniques has increased\nmulti-fold due to the advancement of artificial intelligence (AI). Automated decision-making AI models has reflected\nan immense potential for the detection of malignant tumors [13, 14, 15]. These advancements in AI have also attracted\ncolposcopists to get assistance from AI in clinical decision-making. Therefore, there is an increasing interest in using\nAI to automate colposcopy image assessments, so that, identifying the presence of precancerous or cancerous cells in\nthe cervix, becomes feasible at a large scale, with higher speed and lower costs, than nowadays."}, {"title": "2 Literature review", "content": "The literature review discusses the recently developed automated decision-making AI models for assessment of cervical\nprecancer. Li et al. [16] implemented a graph convolutional network with edge features (E-GCN) for classifying"}, {"title": "3 Methodology", "content": "The description of the proposed model comprises four subsections: dataset description, details of the proposed\nCervix-AID-Net model, performance evaluation, and explainable AI. Figure 1 shows an overview of the proposed\nCervix-AID-Net."}, {"title": "3.1 Dataset details", "content": "All data and images were collected from patients examined using a DYSIS colposcope Version 3 at the Department of\nGynecology and Obstetrics, Randers Regional Hospital, Denmark between 2017-2020. Colposcopy examinations were\nincluded if the colposcopists had reported a full or partially visible transformation zone, and four cervical biopsies taken.\nImages not suitable for annotation were excluded (blurry, too much light, mucous or blood covering visible changes).\nBased on international guidelines for risk assessment and treatment of cervical cancer precursors histological diagnoses\nof normal, inflammation and CIN1 were considered to represent low-grade disease. Other histological diagnoses present\nwere CIN grade 2, CIN3, carcinoma in situ (CIS), adenocarcinoma in situ (AIS) and squamous cell carcinoma (SCC)\nwere classified to represent high-grade disease. Images from women with a diagnosis of only ungradable CIN were\nremoved (n=1). Women who had one or more other diagnosis were still included and the worst of the other diagnoses\nwas considered to be the grade of dysplasia present. [34]. The details of the dataset is shown in Table 1."}, {"title": "3.2 Cervix-AID-Net", "content": "Attention has a significant influence on human perception [35]. The key feature of human visual systems is that they do\nnot process entire scenes at once. Instead, humans use a series of partial glimpses, deliberately focusing on crucial\nregions, to better capture visual structure [36]. Inspired by this, we developed a lightweight CNN-based CBAM module\nto classify high-risk and low-risk cervical precancer."}, {"title": "3.2.1 Convolutional Block Attention Module (CBAM)", "content": "Fig. 2 illustrates the overview of the CBAM. It comprises two successive sub-modules: channel-attention and spatial-\nattention [37]. The CBAM adapts to enhance the intermediate feature map at each convolutional block of deep\nnetworks. The CBAM consecutively evaluates 1D channel attention map $M_{ch} \\in R^{C \\times 1 \\times 1}$ and a 2D spatial attention\nmaps $M_{sp} \\in IR^{1 \\times H \\times W}$ for a given intermediate feature map $S \\in R^{C \\times H \\times W}$ as input. The complete attention process can\nbe described as:\n$S' = M_{ch} (S) \\otimes S$\n$S\" = M_{sp} (S') \\otimes S'$\nwhere $M_{sp}$ and $M_{ch}$ are spatial and channel attention map, $C,H,and W$ are the channel, height, and width of input\nfeature map, and $\\otimes$ is element-wise multiplication (Hadamard product)."}, {"title": "3.2.2 Channel attention module (CAM)", "content": "Each channel in a feature map serves as a feature detector, with channel attention focusing on 'what' is significant to an\ninput image. Fig. 3 shows a graphical illustration of the steps involved in computing CAM. As shown in Fig 3, the\nspatial size of the input feature is squeezed to optimize the channel attention. Therefore, CAM uses average-pooled and\nmax-pooled features simultaneously. The CAM module is evaluated as [37]:\n$M_{ch}(S) = sigmoid(MLP(AVP(S))+MLP(MP(S)))$\n$= sigmoid(W_{1}(W_{o}(S_{avg}))+W_{1}(W_{o}(S_{max})))$\nwhere $W_{0}$ and $W_{1}$ are the weights of MLP, spatial context descriptors generated by average-pooling and max-pooling\nare denoted by $S_{avg}$ and $S_{max}$, $M_{ch}$ is channel attention map, $AVP$ is average-pooling, MP is max-pooling, and MLP\ndenotes a shared multi-layer perceptron network with one hidden layer."}, {"title": "3.2.3 Spatial attention module (SAM)", "content": "CAM focuses on 'what' is significant to an input image, whereas SAM focuses on 'where' an informative part of\nan image is located. The evaluation of spatial attention covers applying average-pooling and max-pooling along the\nchannel axis and concatenating them to extract representative features. After concatenation, a convolutional layer is\napplied to generate SAM, $M_{sp} (S) \\in R^{H \\times W}$. Fig. 4 shows the steps involved in computing the SAM. The mathematically\nspatial attention module can be evaluated as [37]:\n$M_{sp}(S) = sigmoid(f^{7\\times 7}([AVP(S);MP(S)]))$\n$= sigmoid(f^{7\\times 7}([S_{avg}; S_{max}]))$\nwhere $M_{sp}$ is spatial attention map, $S_{max}$ and $S_{avg}$ are max-pooled and average-pooled feature maps across the channel,\nand $f^{7\\times 7}$ represents convolutional operations using a filter size of 7. We developed a novel lightweight Cervix-AID-Net"}, {"title": "3.3 Performance evaluation", "content": "We used HO validation and ten-fold cross-validation (10-FCV) strategies to evaluate model performance. For HO\nvalidation, training uses 80% data and the remaining for testing. Out of 20% testing data, 6% for validation and 94%\nfor testing. The model training, validation, and testing utilized 3153 images. Therefore, the final train, validation, and\ntest set contains 2524, 37, and 593 images, respectively. For 10F-CV, we randomly split the entire dataset into ten equal\nparts, of which training uses nine parts while the tenth part is for testing. The process is repeated ten times to estimate\nthe average model performance. We have tested the Cervix-AID-Net model performance by evaluating accuracy\n(ACC), precision (PRC), negative predicted value (NPV), false positive rate (FPR), F1 measure (F1), sensitivity (SEN),\nspecificity (SPF), balanced accuracy (B-ACC), and Mathew's correlation coefficient (MCC), details in [38]."}, {"title": "3.4 Explainable AI module", "content": "Explainable AI (XAI) are AI systems and models that can offer transparent and intelligible explanations for their\ndecision-making processes [39]. Traditional ML models, such as deep neural networks, are sometimes referred to\nas \"black boxes\" since it can be hard to understand how they arrive at certain conclusions or predictions [40]. \u03a7\u0391\u0399\nfacilitates transparency, interpretability, trust, and human-readable explanations in the decisions yielded by ML or\nDL models. We use four explainable modules: Grad-CAM, LIME, CartoonX, and pixel RDE to visualize and better\nunderstand the decisions of our proposed model."}, {"title": "3.4.1 Grad-CAM", "content": "The Grad-CAM technique operates on output feature maps. It identifies crucial regions in the input image by computing\nthe gradients of the last convolutional layer, generating heat maps. These heat maps visually represent the significant\nregions in the input image, which helped the network to arrive at certain decisions [41, 42]. The mathematical\nformulation of grad-CAM technique is denoted by [41]\n$\\alpha_{i}^{m}=\\frac{1}{Z} \\sum_{i} \\sum_{j} \\frac{\\partial y^{m}}{\\partial F_{i j}}$\nwhere $\\frac{\\partial y^{m}}{JFk}$ represents the gradient score for class m with respect to feature maps Fk. $\\beta^{m}$ denotes a partial linearization\nof the deep network downstream from F, and captures the 'importance' of feature map k for a target class m. Finally,\ngrad-CAM is obtained by taking a ReLU of a weighted combination of forward activation maps, denoted as [41]:\n$L_{Grad-CAM}^{m}= ReLU (\\sum_{k} \\beta_{k}^{m} F^{k})$"}, {"title": "3.4.2 LIME", "content": "LIME is model-agnostic, which means it may be used on any machine/deep learning model independent of architecture\nor complexity [43]. The goal of LIME is to train surrogate models locally and explain a single prediction. It generates\na synthetic dataset by randomly permuting samples from a normal distribution and collects predictions based on the\\opaque model to be explained. LIME uses the perturbed dataset to train an interpretable model. The mathematical\nformulation for LIME is denoted by [44]\n$\\xi(x) = arg \\min _{m \\in M} L(g,m,\\pi_{x}) + \\Omega(m)$\nwhere $M$ is a class of Cervix-AID-Net, $I$ is a fidelity function, and $\\Omega(m)$ measures complexity of the explanations\n$m \\in M$."}, {"title": "3.4.3 Pixel RDE", "content": "Pixel RDE are model-independent explanations inspired by rate distortion theory, which examines lossy data compres-\nsion [45]. In pixel RDE, explanations use a sparse mask to highlight relevant features from incoming data. The mask is\ntailored to minimize distortion in model output after perturbing unselected input features while remaining sparse. RDE\ntries to address the constrained optimization problem over a mask denoted by [45]:\n$min\\newline{S_{m}\\in{0,1}^n: ||S_{m}||_0\\leq l}} \\mathbb{E} _{k~K} [d(\\Psi(x), \\Psi(x\\odot S_m+(1-S_m) \\odot k))]$\nwhere $Y: R^n \\rightarrow R^m$ is a pre-trained Cervix-AID-Net model with m(High \u2013 risk and Low \u2013 risk) classes, dimension of\nmodel input is represented by n, x denotes relevant input features, sparse marks is represented by $S_m \\in {0,1}^n$, K is a\ndistribution over input perturbations $k \\in R^n$, $\\odot$ is element-wise multiplication (Hadamard product), $l \\in {1,2,...,n}$ is a\nsparsity level for mask explanation $s_m$, and $d(\\Psi(x))$ is a measure of distortion. However, in practice, RDE optimization\nproblem is relaxed to continuous masks denoted by [45]:\n$min\\newline{S_{m}\\in [0,1]^n}} \\mathbb{E} _{k~K} [d(\\Psi(x), \\Psi(x\\odot S_m+(1-S_m) \\odot k))] + \\lambda||S_m||_1$\nwhere $\\lambda$ takes values > 0, which is a hyper-parameter for the sparsity level."}, {"title": "3.4.4 CartoonX", "content": "CartoonX is a novel explanation technique that is a special case of RDE. CartoonX first executes RDE in the discrete\nwavelet position-scale domain of an image x and then visualizes the wavelet mask sm as a pixel-wise smooth picture.\nWavelets efficiently represent 2D piece-wise smooth pictures, commonly known as cartoon-like images, along with\nproviding optimum representations for piece-wise smooth 1D functions [45]. Algorithm 1 illustrates the steps for\nobtaining CartoonX explanations."}, {"title": "4 Experimental setup and Results", "content": "This section covers experimental setup, including evaluation criteria and evaluation metrics and results."}, {"title": "4.1 Experimental setup", "content": "The experiment was performed on an AMD Ryzen 7 PRO 6850U with Radeon Graphics with a frequency of 2.70\nGHz. The experimental setup used a 64-bit operating system, x64-based processor, and 16 GB RAM. We used Jupyter\nNotebook and Python version 3.11.4 to build the model. The high-risk class contains 1404 images, while the low-risk\nclass contains 1749 images. We reshaped all the images to 224 x 224, as the models processes an input image of 224 x\n224. We used an \"Adam\" optimizer, loss function as \u201csparse categorical cross entropy\", and accuracy as an evaluation\nmatrix to compile the model. The model iterates for 25 epochs with a batch size of 32."}, {"title": "4.2 Results", "content": "The proposed Cervix-AID-Net model is trained and evaluated in multiple scenarios for comprehensive data analysis.\nTo have a faithful comparison concerning performance and tuning parameters, we compared the performance of the\nproposed model with benchmark CNN models like AlexNet, GoogleNet (G-Net), and ECANet, respectively [46, 47, 48].\nAlexNet is a simple eight-layered CNN model capable of performing 1000 class classifications, G-Net includes\ninception layers, and ECANet is an efficient channel attention module for the CNN model. Table 3 shows the accuracy\nof Cervix-AID-Net as compared with the benchmark models, using HO and 10-FCV techniques. The results show\nthat our proposed Cervix-AID-Net surpasses the performance obtained using benchmark CNN models for both, HO\nand 10-FCV validation techniques. To further evaluate, validate, and compare the performance of the Cervix-AID-Net\nmodel, we obtained evaluation matrices and compared them with the performance of benchmark CNN models. Table 4\nindicates the performance report of various models using the HO validation technique. It is evident from Table 4 that\nthe proposed Cervix-AID-Net model surpasses most other models in terms of accuracy, sensitivity, F1 measure, MCC,\nNPV, and balanced accuracy. However, ECANet and G-Net provide slightly better precision, specificity, and FPR than\nthe Cervix-AID-Net model. However, the main limitation of the HO validation technique is that model performance\ndepends significantly on the random split. Also, training uses only a portion of the data, which increases the probability\nof over-fitting and bias. Therefore, to avoid instability of sampling (where different results are obtained when the\nexperiment is repeated with a new division), we validated our model performance using the 10-FCV technique. Table 4\nprovides the performance of the benchmark CNN models and the proposed Cervix-AID-Net model using the 10-FCV\ntechnique. Table 4 confirms that our developed Cervix-AID-Net model is superior to other benchmark techniques. The\nCervix-AID-Net model yielded the highest performance of all the evaluation metrics. Therefore, analysis shows that\nour Cervix-AID-Net model is robust and accurate over existing CNN models."}, {"title": "5 Discussion", "content": "Our Cervix-AID-Net model aims to provide transparent and effective explanations for its decisions using four explainable\ntechniques. These XAI techniques require tuning of hyper-parameters to generate explanations for the model's\ndecisions. Table 6 represents various tuning parameters used for LIME, pixel RDE, and CartoonX. Grad-CAM provides\nexplanations based on output feature maps that generate the gradients from the last convolutional layer. Fig. 7 indicates\nthe grad-CAM obtained from the last convolutional layer of our proposed Cervix-AID-NET model. Our analysis\nconfirms that the heat maps from grad-CAM outline the relevant regions around the cervix. However, there are some\ninstances where the network does not outline any regions around the cervix. It could be because of uneven light\ndistribution on the images, motion artifacts, or instances of outliers, shown in the second image in Fig. 7. On the other\nhand, LIME explains specific instances using input features. LIME approximates the pre-trained model to generate a\nsurrogate model that explains local approximations around the input image. As evident from Fig. 7, LIME highlights\ncrucial regions in the neighborhood of the input image, which provides the most relevant input features for that particular\ninstance. Fig. 7 reveals that LIME maps the region around the cervix, marking relevant features to classify correctly in"}, {"title": "6 Conclusion", "content": "The paper presents an effective AI framework for the classification of low-risk and high-risk cervical precancer\nlesions from colposcopy images. The proposed Cervix-AID-Net model captures representation features from the\ncolposcopic images by maintaining attention over the channel and spatial domain. Our proposed model has effectively\nidentified what is the significant region in the image and where it is located within the image using CBAM. Therefore,\nour Cervix-AID-Net model yielded the highest accuracy in classifying high-risk and low-risk cervical precancer\ninstances, compared to existing benchmark CNN models. In addition, our model facilitates visually readable transparent\nexplanations for decision-making, which makes it robust and effective. The proposed model explains relevant output\nfeature maps and important input features required for accurate decision-making. We show that piece-wise smooth\nexplanations extract representative and discriminant features of the input image. Thus, the explanations yielded by\nCartoonX were the most interpretable and discriminant compared to pixel RDE, LIME, and grad-CAM techniques. The\nexperimental analysis demonstrated that the proposed model may yield effective decision-making on high-resolution\nand low-resolution images. The Cervix-AID-Net model is lightweight, has been tested on multiple scenarios were it\nyielded the best performance matrices compared to current models, and it has integrated XAI techniques. In conclusion,\nthe Cervix-AID-Net model, presented in this article, is an accurate, robust, transparent, effective, and simple, that could\nbe used in future for early identification and assessment of cervical precancer lesion during colposcopy."}, {"title": "7 Funding Information", "content": "For this study we got funding from the BetaHealth Program from NOVO Nordisk (ID-nummer 2023-1384) and an\nInnovation Grant from Region Syddanmark."}]}