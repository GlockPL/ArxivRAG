{"title": "PTEENet: Post-Trained Early-Exit Neural Networks\nAugmentation for Inference Cost Optimization", "authors": ["Assaf Lahiany", "Yehudit Aperstein"], "abstract": "For many practical applications, a high computa-\ntional cost of inference over deep network architectures might\nbe unacceptable. A small degradation in the overall inference\naccuracy might be a reasonable price to pay for a significant\nreduction in the required computational resources. In this work,\nwe describe a method for introducing \"shortcuts\" into the DNN\nfeedforward inference process by skipping costly feedforward\ncomputations whenever possible. The proposed method is based\non the previously described BranchyNet [1] and the EEnet [2]\narchitectures that jointly train the main network and early\nexit branches. We extend those methods by attaching branches\nto pre-trained models and, thus, eliminating the need to alter\nthe original weights of the network. We also suggest a new\nbranch architecture based on convolutional building blocks to\nallow enough training capacity when applied on large DNNs.\nThe proposed architecture includes confidence heads that are\nused for predicting the confidence level in the corresponding\nearly exits. By defining adjusted thresholds on these confidence\nextensions, we can control in real-time the amount of data exiting\nfrom each branch and the overall tradeoff between speed and\naccuracy of our model. In our experiments, we evaluate our\nmethod using image datasets (SVHN and CIFAR10) and several\nDNN architectures (ResNet, DenseNet, VGG) with varied depth.\nOur results demonstrate that the proposed method enables us\nto reduce the average inference computational cost and further\ncontrolling the tradeoff between the model accuracy and the\ncomputation cost.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Neural Networks (DNN) models are frequently used\nfor solving difficult machine learning problems in various\ndomains including computer vision [3], natural language pro-\ncessing [4] and sensory data processing [5]. Provided enough\ntraining data is available, deeper, and more complex neural\narchitectures frequently lead to better performance [6]. For\nexample, for a visual object classification problem, a deeper\narchitecture (e.g., AlexNet [3]) almost always leads to better\nperformance. However, processing inputs using deeper and\nmore complex neural networks requires a significant amount\nof computing power and may increase the overall processing\nlatency [7], [8]. These processing power requirements may\nprevent the deployment of deep learning applications at edge\ndevices (e.g., smartphones or sensors) that have severe limita-\ntions in processing power or battery life [9], [10].\nDistributed processing approach allows one to alleviate\ncomputing power limitations by employing multiple comput-\ning nodes for performing necessary computations [11]. For in-\nstance, the DNN computation load can be partitioned between\nedge devices (e.g., smartphones) and more powerful server\nnodes. In this case, the edge devices might perform necessary\ncomputations for extracting features from the raw input (e.g.,\nvideo). Then, the extracted features can be sent to a server\nwhere the rest of the computations are performed. In the case\nof DNN, an edge device might compute a few lower layers of\nthe network, while transmitting the output of an intermediate\nlayer to the server for the rest of the processing [12]. The\ndistributed computing might employ more than two types of\ncomputing nodes besides the server and edge device nodes. For\ninstance, fog computing [13] architectures define a hierarchy\nof computing nodes according to their computing power and\ntheir role within the platform's network topology (e.g., gate-\nways). Unfortunately, the distributed processing scheme might\nresult in a significant network traffic due to data transmissions\namong computing nodes. Depending on the depth of a neural\nnetwork and the partition of the network among computing\nnodes, the intermediate layers might produce tens of thousands\nof real-valued values that needed to be communicated between\ncomputing nodes [10].\nIn this work we propose an optimization scheme that\nextends the work of BranchyNet [1]. The BranchyNet concept\nsuggests augmenting the original network with small inter-\nmediate decision networks attached to selected hidden layers\nwithin the DNN. These small networks (called Branches) are\ntrained to infer network outputs (e.g., classification labels) on\n\"easy\" input cases. The branch networks are trained jointly\nto make decisions solely based on the values produced by\nintermediate layers of the main DNN. When such inference is\npossible, the inference through the rest of the DNN layers is\ninterrupted, saving precious resources including network and\ncomputing capacities.\nOur approach extends the original BranchyNet concept\nfor supporting two practical requirements. We assume that\nthe original training data might not be available during the\noptimization and deployment time. Moreover, the parameters\nof the training procedure for the main network might not be\nknown, or training process might not be easily reproducible.\nWe also assume that resource limitation and accuracy re-\nquirements are application dependent. Therefore, the augment\nmodel should have a \u201cknob\u201d providing explicit and predictable\ncontrol over the tradeoff between accuracy and computational\ncosts.\nMotivated by these assumptions, we attach branches to\nan already trained network to address the cases where no"}, {"title": "II. PRIOR WORK", "content": "There are several strategies for neural network optimization\nthat have been explored in the past. The existing approaches\ncan be categorized into two categories: network complex-\nity reduction [16] and distributed computing [13]. In this\nsection, we cover common methods in both categories. It\nshould be noted that we focus on techniques for runtime\nperformance optimization during inference as compared to\nruntime performance optimization of training procedures. The\noptimization of DNN training time is also frequently discussed\nin this context of distributed neural networks [16]. Some of\nthe common approaches for reducing the computation and\ncommunication requirement for DNN inference are based\non weight quantization. Weight quantization methods enable\nreduction of the number of bits required for storing network\nweights. Reducing the number of bits results in simpler and\nfaster computations [17]. There are more advanced approaches\nfor quantization that are based on weights clustering. In these\napproaches, a network weight is approximated by the center\nof its closest cluster and, thus, can be encoded by a smaller\nnumber of bits.\nNetwork complexity reduction can also be achieved using\nnetwork pruning [18] and connection sharing [19]. Network\npruning techniques reduce the network complexity by drop-\nping some less important neurons and connections. There\nare different methods for selecting the most optimal pruning\nstrategies [16]. For example, one method suggests dropping\nall connections from a fully connected layer whose weights\nare lower than a predefined threshold [20]. This technique\nbasically converts a dense layer to a sparse layer and reduces\nthe storage and computation requirements by an order of\nmagnitude.\nConnection sharing methods allow to reduce the complexity\nby sharing parameters between multiple neurons [19]. For\nexample, in CNN models the main assumption is that one\nfilter which is useful for computation at some specific data\nrange in one of the model layers, can also be useful for\ncomputation at a different data range in the same layer [3].\nWhenever a deep learning network is deployed in a dis-\ntributed environment, efficient partitions of the DNN between\nnodes and different communications schemes might lead to\nreduced communication network and processing load. For\ninstance, [12] suggests an efficient way for mapping sections\nof DNN onto a distributed computing hierarchy. In [21],\nauthors suggest to deploy shallow networks to edge devices for\nperforming a \u201cgating function"}, {"title": "III. OUR CONTRIBUTION", "content": "We extend the original BranchyNet model by introducing\nlogic basic-blocks which in the case of ResNet architecture\nseveral critical enhancements:\n\u2022 We define a new early exit branch architecture which\nincludes both classification output and confidence head\nfor early termination decision making. Those branches\nare attached to the pre-trained backbone network to form\na PTEEnet (Post Trained Early Exit network).\n\u2022 Branch placement is done using suitable distribution\nmethods to allow computational cost optimization, while\nconsidering architectural constrains of the original net-\nwork.\n\u2022 We train only the attached branches using a loss func-\ntion that combines both the accumulated prediction loss"}, {"title": "IV. PTEE METHODOLOGY", "content": "Our PTEE (Post Trained Early exit) model can be casted on\nmany deep neural network architectures with various branch\ndistribution suited for each architecture. We first describe\nthe global PTEEnet network architecture (which is based on\nprevious EEnet work [2]) and then expand it to define the new\nmethodology."}, {"title": "A. EE-Blocks Distribution", "content": "The number of the early exit branches and their placement\nare important factor in the model architecture. As introduced\nin [2], many distribution methods are suitable. Pareto method\nis based on the 80/20 principle where 80% of the samples\nare classified by 20% of the total computational cost and\ntherefore define a 0.2 ratio between the added computational\ncost and the total cost calculate from each previous branch. In\nthe same manner, Fine and Golden distribution methods define\nratios to be at least 0.05 and 0.618 respectively depending on\nthe internal design of the network. Linear distribution method\ndefines a fixed computational cost gap. To reduce freedom\ndegree to our exploration we use the Fine distribution method.\nFigure 1 shows 3 branches distributed on ResNet20 main\nnetwork, using Fine distribution method. Choosing the optimal\nnumber of branches depends mainly on main network size,\ndistribution method and dataset characteristics. We follow the\ncomputational cost levels defined by the distribution method\nto place the branches along the main network. The original\nnetwork is organized in stages, defined by the placement of the\nattached branches. These are only semantic structure to help\nlocate branches and define network segments. Each branch and\nits leading stages are grouped into segments which are used\nfor further complexity analysis. For simplicity, branches are\nattached only between logic basic-blocks which in the case of\nResNet architecture consist of a single residual block."}, {"title": "B. Cumulative Predictions and Computational Cost", "content": "We begin our definition of our loss elements by using the\ncross-entropy function as the classification loss for each exit\nhead:\n$L_{MC} = CE (\\hat{y},Y) = - \\sum_{n=1}^{K} Y_n log (\\hat{y_n})$        (1)\nwhere K is the number of classes in our dataset (K=10 for\nCIFAR10). The final classification output vector, Y, can be\nobtained from a single equation which consists of the outputs\nof all exit blocks.\n$\\hat{Y} = I(h_0 \\ge T) \\hat{Y}_0 + I(h_0<T) \\cdot$\n{\u2026\u2026$I(h_{N-1}>T)\\cdot\\hat{Y}_{N-1} + I(h_{N-1}<T) \\cdot\\hat{Y}_{N} $}...(2)\nwhere Y denotes the final classification output vector and\nN defines the number of early-exit blocks. $\\hat{y_i}$ and $h_i$ are\nthe classification output vector and confidence score of the\nnth early-exit block respectively. $\\hat{y_n}$ is the predicted output\nvector of the pre-train network classifier (early exit branches\nare indexed from 0 to N-1 where N is the index of the main\nbackbone network). In addition, $I{h_0 \\ge T}$ denotes the binary\nindicator function mapping a confidence score onto 0 or 1\nwhich results in continuation or an exit decision, respectively.\nHowever, to perform backpropagation during training, we\nneed to define a differentiable version of equation (2) by\napproximating the binary indicator function. To do so, we\nuse the continuous sigmoid function, where hn denotes the\nconfidence score of the nth early-exit block. Finally, we can\nderive the cumulative prediction $\\hat{Y}_n$, as defined in (3), where\neach $\\hat{Y}_n$, (n = 0....N \u2013 1) defines the classification output\nvector derived by all proceeding exit blocks from n to N-1.\nThe last $\\hat{Y}_{n+1}$ (n = N \u2212 1) is $\\hat{Y}_{N}$, the output of the main\npre-trained classifier which has no corresponding confidence\nhead.\n$\\hat{Y}_n = h_n \\cdot \\hat{y_n} + (1 - h_n) \\cdot \\hat{Y}_{n+1}; n = 0....N-1$       (3)"}, {"title": "C. Training", "content": "As a preliminary stage to training our branches, we freeze\nthe pre-trained weights of the backbone network and disable\ntheir gradients calculation. To eliminate our dependency on\ndata labels, we use the main network outputs as the ground\ntruth values (replacing the original labeled data). This is done\nby executing forward pass of each batch of training data and\nuse the main network classifier predictions to determine the\ncorresponding labels. These labels are used both for training\nand validation. The loss is back-propagated only till the\nstitching point of each branch and the weights are updated."}, {"title": "D. Inference", "content": "During inference we apply a stop criteria in the form\nof a dedicated confidence threshold level T on each of the\nconfidence heads in the early exit branches. This will be used\nfor the early propagation termination. If the confidence score\nof an early-exit block is above the threshold, the classification\noutput of the current stage will be the final prediction. Each\ninput is classified based on their individual confidence scores\npredicted by the early-exit blocks. Thus, one input can be\nclassified and terminated early while others continue to prop-\nagate through the model. The threshold-confidence mechanism\ncan serve as \"knob\", updated in real time to ensure desired\naccuracy-computational cost trade-off. The impact of threshold\nlevels on model performance, should be examined on the final\nmodel a-priory to produce desired known result. The inference\nprocedure of the early exit model is given in the following\npseudo-code:"}, {"title": "E. Branch Architecture", "content": "Using shallow capacity branch architecture, as in [2], is only\nallowed while using small size backbone networks. To account\nfor deeper network architectures, typically ResNet20 and on,\nwe need higher learning capacity branches. Exploring different\nbranch architectures yield ConvX, as presented in Figure 2,\nwhich consist of sequential convolutional blocks attached to\nclassifier and confidence heads by an average pooling layer.\nEach block consists of a convolutional layer using a 3x3\nkernel followed by a batch normalization layer and a ReLU\nactivation layer. The confidence head uses a sigmoid output\nand is trained to yield high values (towards 1) reflecting high\nconfidence levels in the corresponding classifier prediction.\nAppling threshold value T to the confidence head output is\nused during validation/test accuracy calculation, and act as\na \"knob\" during real-time inference of the PTEEnet model,\ncontrolling accuracy-cost tradeoff."}, {"title": "F. Complexity", "content": "To measure the inference computational complexity of any\nnetwork segment in our model we use FLOPs (floating point\noperations) as a common measurement unit used also in\nthe original ResNet paper [32]. Each FLOP is defined by a\npair of multiple-accumulate (MAC) operations. Those MAC\noperations are sometimes referred to as FMA (fused multiple\naccumulate) when using more modern hardware architectures.\nAn exit network segment includes all stages (see Figure 1)\npreceding the branch attach point and the branch itself. To\ncalculate the number of FLOPs in each exit segment we\nperform a forward pass and accumulate the total number of\nFLOPs from all segment layers. For example, the number\nof FLOPs in a 2D convolution layer with single stride and\npadding, square kernel of size k, Ci input channels, Co output\nchannels and (H, W) as the size of the input feature map, is\n$C_i\\times k^2 \\times C_o \\times H \\times W $. Finally, we define the branch relative\ncost as the ratio between the number of exit segment FLOPS\n(from input to branch exit) and the number of FLOPs in the\nmain backbone network. Table II presents the total FLOPs and\nrelative cost of each exit segment using VGG19, DenseNet121,\nand ResNet110 models. For each PTEEnet model architecture\nwe used Fine distribution method, Conv2 branch architecture\nand 32x32 input size (as in CIFAR10 and SVHN datasets).\nEach branch could be attached only between consecutive basic\nblocks of each network architecture.\nThe main exit branch in each model corresponds to the main\nbackbone network exit and yields relative cost of 1."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "To evaluate our PTEEnet methodology we use 3 state of\nthe art vanilla backbone network architectures - ResNet, VGG\nand DenseNet to create their corresponding PTEEnet variants:\nResEEnet, VGGEEnet and DenseEEnet. We evaluate those\nPTEEnet using CIFAR10 and SVHN datasets. For the Re-\nSEENet architecture we use pre-trained ResNet20, ResNet32,\nResNet110 backbones, and attach 3,5,10 branches respectively.\nThis results in ResEEnet20/3, ResEEnet32/5, ResEEnet110/10\nmodels. For VGG and DenseNet architectures, we use a\nVGG19 and DenseNet121 backbones, each attached with 3\nbranches to construct a VGGEEnet19/3 and DenseEEnet121/3.\nFor each model we \u201cfreeze\u201d the pre-trained weights, and train\nonly the branches. We define cost penalty A range from 0.2 to\n2.3. As described in the previous section, the cost penalty \u03bb,\ndefines the amount of penalty we induce on high relative cost\nbranches during training stage. Increasing A leads to a decrease\nin the number of calculations and as a result a decrease in\naccuracy.\nFigure 3 presents accuracy and computational costs for dif-\nferent values of penalty parameter \u5165 in three PTEEnet models.\nValidation accuracy and cost reduction pairs calculated using\nthe stop inference criteria with fixed confidence level threshold\nof To = 0.5.\nBlue curve in Figure 3 presents accuracy and computational\ncost reduction behaviour of ResEEnet110/10 on CIFAR10\ndataset. The model trained with X = 0.5 produced ~8% com-\nputational cost reduction while maintaining 99% validation\naccuracy; x = 0.9 yield ~27% cost reduction and maintained\naccuracy of 97.15%. Increasing A to 1.3 produced 42% compu-\ntational cost reduction and accuracy level of 93.6%. The black\nand red curves correspond to the results of VGGEEnet19/3 and\nDenseEENet121/3 respectively on the same data. For fixed\naccuracy level of 97% computational cost reduction (hori-\nzontal dotted line) levels for each PTEENet: ResEEnet110/10\n27%, VGGEEnet19/3- 42%, and DenseEEnet121/3 - 9%.\nThe major reduction of computational cost using CIFAR10\non VGGEEnet19/3 can indicate that the original capacity size\nof the VGG19 is more than enough to handle the samples\nclassification difficulty.\nSetting a parameter value X can be used for selecting the\nadequate accuracy-cost ratio suitable for deploying a system in\nreal-world scenarios. For instance, if the goal is to maximize\nration between the computational cost reduction and the accu-\nracy reduction, for ResEEnet110/10 on CIFAR10 then X = 1.3\nis an optimal value and it results in 6% accuracy drop while\nachieving almost 42% computational cost reduction.\nAs discussed in section IV E, during inference the decision\nabout a sample class is made based on confidence threshold\nlevel T on each of the confidence heads in the early exit\nbranches.\nSince T can be externally controlled during inference it can\nact as a \"knob\" to control the real-time accuracy-cost tradeoff.\nIn this work we set the same threshold value to all branches,\nalthough further optimization of each branch threshold can be\ndone. Using the previous ResEEnet110/10, trained with x =\n0.9, we evaluate the accuracy-cost pairs for each value of T in\nthe range of 0.1 to 0.95. Figure 4 presents the accuracy-cost\nreduction pairs using different levels of T. The interpolated\ncurve has a \"knee\u201d like shape with slow decrease in accuracy\nfor T\u2265 0.5 and significant rapid decrease in accuracy for\nT < 0.5. The confidence threshold T* = 0.5 is an optimal\nlevel for maintaining high accuracy with large reduction in\ncomputational cost.\nThe final optimal confidence level threshold selection de-\npends on the application requirements in terms of the amount\nof accuracy decrease allowed and computational resources at\nhand.\nTable III presents a performance comparison of various\nPTEEnet models based on maximum 3% drop tolerance in\nvalidation accuracy. For each model we set a A value that\nproduced the maximum reduction in computational cost and\nwhile using fixed confidence threshold of T=0.5 for validation\naccuracy and cost calculation. ResEEnet110/10 produced 25%\ncost reduction while ResEEnet20/3 and ResEEnet32/5 had\nonly 15% reduction in cost. It seems that using high num-\nber of branches on high-capacity backbones allow for more\n\"accurate\u201d exit options in terms of minimal loss and higher\nconfidence for the propagated samples thus reducing computa-\ntional load. Different datasets with different complexity levels\nyields different cost reduction levels, as can be seen using\nVGGEEnet19 and DenseEEnet121 on CIFAR10 and SVHN.\nThis is probably due to the different distribution of classifi-\ncation difficulty of the samples in each dataset. VGGEEnet19\non CIFAR10 produce 42% cost reduction compared to SVHN\nwith 37% cost reduction."}, {"title": "VI. SUMMARY", "content": "In this work we proposed PTEEnet a methodology for\nattaching and training early exit branches to pre-trained state-of-the-art deep neural networks. It has been shown that the\noutput produced by the original network can be successfully\nused as labels for training the exits classifier and confidence\nheads, removing the need for the original labeled training data.\nFurthermore, we used a single confidence threshold parameter\nfor controlling the accuracy vs cost tradeoff, allowing easy\nselection of an optimal point based on specific application\nrequirements and constraints. Using several examples, we\nshowed that a significant reduction in average computational\ncost can be achieved by selecting optimal confidence thresh-\nolds while sustaining only a small impact on the overall\naccuracy.\nAlthough, the applicability of the approach is not limited to\nany specific task, the current work demonstrates the benefits\nof the method for image classification using several popular"}]}