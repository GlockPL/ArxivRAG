{"title": "JAILJUDGE: A COMPREHENSIVE JAILBREAK JUDGE BENCHMARK WITH MULTI-AGENT ENHANCED EXPLANATION EVALUATION FRAMEWORK", "authors": ["Fan Liu", "Yue Feng", "Zhao Xu", "Lixin Su", "Xinyu Ma", "Dawei Yin", "Hao Liu"], "abstract": "Although significant research efforts have been dedicated to enhancing the safety of large language models (LLMs) by understanding and defending against jailbreak attacks, evaluating the defense capabilities of LLMs against jailbreak attacks also attracts lots of attention. Current evaluation methods lack explainability and do not generalize well to complex scenarios, resulting in incomplete and inaccurate assessments (e.g., direct judgment without reasoning explainability, the F1 score of the GPT-4 judge is only 55% in complex scenarios and bias evaluation on multilingual scenarios, etc.). To address these challenges, we have developed a comprehensive evaluation benchmark, JAILJUDGE, which includes a wide range of risk scenarios with complex malicious prompts (e.g., synthetic, adversarial, in-the-wild, and multi-language scenarios, etc.) along with high-quality human-annotated test datasets. Specifically, the JAILJUDGE dataset comprises training data of JAILJUDGE, with over 35k+ instruction-tune training data with reasoning explainability, and JAILJUDGETEST, a 4.5k+ labeled set of broad risk scenarios and a 6k+ labeled set of multilingual scenarios in ten languages. To provide reasoning explanations (e.g., explaining why an LLM is jailbroken or not) and fine-grained evaluations (jailbroken score from 1 to 10), we propose a multi-agent jailbreak judge framework, JailJudge MultiAgent, making the decision inference process explicit and interpretable to enhance evaluation quality. Using this framework, we construct the instruction-tuning ground truth and then instruction-tune an end-to-end jailbreak judge model, JAILJUDGE Guard, which can also provide reasoning explainability with fine-grained evaluations without API costs. Additionally, we introduce JailBoost, an attacker-agnostic attack enhancer, and GuardShield, a safety moderation defense method, both based on JAILJUDGE Guard. Comprehensive experiments demonstrate the superiority of our JAILJUDGE benchmark and jailbreak judge methods. Our jailbreak judge methods (JailJudge MultiAgent and JAILJUDGE Guard) achieve SOTA performance in closed-source models (e.g., GPT-4) and safety moderation models (e.g., Llama-Guard and ShieldGemma, etc.), across a broad range of complex behaviors (e.g., JAILJUDGE benchmark, etc.) to zero-shot scenarios (e.g., other open data, etc.). Importantly, JailBoost and Guard-Shield, based on JAILJUDGE Guard, can enhance downstream tasks in jailbreak attacks and defenses under zero-shot settings with significant improvement (e.g., JailBoost can increase the average performance by approximately 29.24%, while GuardShield can reduce the average defense ASR from 40.46% to 0.15%).", "sections": [{"title": "1 INTRODUCTION", "content": "Jailbreak attacks aim to manipulate LLMs through malicious instructions to induce harmful behaviors Zou et al. (2023); Yuan et al. (2024); Wu et al. (2024); Zhang et al. (2024a). To date, an increasing body of research on jailbreak attacks and defenses has been proposed to enhance the safety of LLMs. Before delving into the safety of LLMs, accurately determining whether an LLM has been compromised (e.g., generating harmful and illegal responses) remains a fundamental and open problem. As accurately determining whether an LLM has been compromised (jailbroken) can benefit downstream tasks such as safety evaluation, jailbreak attack, and jailbreak defense etc. However, jailbreak judge, \u201cthe task of evaluating the success of a jailbreak attempt, hinges on the ability to assess the harmfulness of an LLM's target response,\u201d which is inherently complex and non-trivial."}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 LARGE LANGUAGE MODEL", "content": "Large language models (LLMs) predict sequences by using previous tokens. Given a token sequence $x_{1:n}$, where each token $x_i$ is part of a vocabulary set ${1,\u2026\u2026\u2026, V}$ with $|V|$ as the vocabulary size, the goal is to predict the next token,\n$P_{\\pi_{\\theta}}(y | x_{1:n}) = P_{\\pi_{\\theta}}(x_{n+i}|x_{1:n+i-1}),$  (1)\nwhere $P_{\\pi_{\\theta}}(x_{n+i}|x_{1:n+i-1})$ is the probability of the next token $x_{n+i}$ given the previous tokens $x_{1:n+i-1}$. The $\\pi_{\\theta}$ represents the LLM with parameter $\\theta$, and y is the output sequence."}, {"title": "2.2 JAILBREAK ATTACK AND DEFENSE ON LLM", "content": ""}, {"title": "Jailbreak Attack on LLM.", "content": "The aim of a jailbreak attack is to create adversarial prompts that cause the LLM to produce harmful outputs,\n$L_{adv}(x_{1:n}, y) = -log P_{\\pi_{\\theta}}(\\hat{y} | \\hat{x}_{1:n}),$ (2)\nwhere $L_{adv}(x_{1:n}, \\hat{y})$ is the adversarial loss. $\\hat{x}_{1:n}$ is the adversarial prompt (e.g., \"How to make a bomb?\"), and $\\hat{y}$ is the targeted output (e.g., \"Sure, here are the steps to make the bomb!\")."}, {"title": "Defending Against Jailbreak Attacks.", "content": "The goal of jailbreak defense is to ensure that the LLM provides safe responses (e.g., \u201cSorry, I can't assist with that.\u201d), which can be formulated as follows,\n$L_{safe}(x_{1:n}, y) = -log P_{\\pi_{\\theta}}(I(x_{1:n}), C(y)),$ (3)\nwhere ($L_{safe}(x_{1:n}, \\hat{y})$) is safe loss function aligning the LLM with human safety preferences. $I(x_{1:n})$ and C(y) are filter functions that process inputs and outputs, respectively. Specifically, $I(x_{1:n})$ might add random perturbations to mitigate harmful requests, and C(\u0177) could filter malicious outputs."}, {"title": "2.3 EVIDENCE THEORY", "content": "To model the hypothesis of whether an LLM is jailbroken or not, we can use evidence theory Dempster (2008); Deng (2016), a mathematical framework that extends traditional probability theory by accounting for both uncertainty and ignorance. The key components of evidence theory include:\nFrame of Discernment. The frame of discernment is a set of mutually exclusive and exhaustive hypotheses, denoted as \u03a9 = {$H_1, H_2,\u2026\u2026, H_n$}. For the jailbreak judge, it is defined as \u03a9 = {{JB}, {NJB}, {JB & NJB}, {()}}, where {JB} denotes that the LLM is jailbroken, {NJB} means it is not, {JB & NJB} expresses uncertainty, and (\u00d8 indicates no conclusion can be made.\nBasic Probability Assignment (BPA). A function \u03bc : 2\u00ba \u2192 [0,1] that assigns a probability to each subset of \u03a9, satisfying $\\sum_{A \\subseteq \\Omega} \u03bc(A) = 1$ and \u03bc(0) = 0, where \u03bc(A) represents the degree of belief committed exactly to the subset A. For the jailbreak judge, for example, \u03bc({JB}) indicates the probability that the LLM is jailbroken. A source of BPA is called an evidence.\nDempster's Rule of Combination. Dempster's rule of combination combines evidence from two independent sources: $(\u03bc_1 \u2295 \u03bc_2) (C) = \\frac{\\sum_{A\\cap B=C} \u03bc_1(A) \\cdot \u03bc_2(B)}{1-\\sum_{A\\cap B = \\emptyset} \u03bc_1(A) \u03bc_2(B)}$. Dempster's rule of combination is used to aggregate the BPAs from different sources to form a new BPA. For example, the judgments from different LLMs can be aggregated using Dempster's rule of combination."}, {"title": "2.4 PROBLEM DEFINITION", "content": "Jailbreak Judge. The goal of the jailbreak judge is to evaluate the success of a jailbreak attempt by assessing the harmfulness of an LLM's target response. The explainability-enhanced jailbreak judge can be defined as follows:\n(a, s) = $\\pi_{\\phi}(x_{1:n}, \\hat{y}),$ (4)"}, {"title": "3 BUILDING JAILJUDGE BENCHMARK AND MULTI-AGENT JUDGE FRAMEWORK", "content": "We develop the JAILJUDGE benchmark datasets and a multi-agent jailbreak judge framework, making the decision inference process explicit and interpretable to enhance evaluation quality. Using the multi-agent framework to determine the ground truth with reasoning explainability and fine-grained scores, we then develop the end-to-end judge model, JAILJUDGE Guard. Trained on JAILJUDGE's training data, this model can also provide reasoning explainability with fine-grained evaluations without API cost. The overall framework is shown in Figure 1."}, {"title": "3.1 BUILDING JAILJUDGE BENCHMARK: JAILJUDETRAIN AND JAILJUDTEST", "content": ""}, {"title": "3.1.1 JAILJUDETRAIN: INSTRUCTION-TUNING DATASET FOR COMPLEX JAILBREAK JUDGMENTS", "content": "JAILJUDGETRAIN is a comprehensive instruction-tuning dataset consisting of 35k+ items, derived from diverse sources with various target response pairs from different LLMs. The dataset includes six sources of prompts: vanilla harmful prompts (a wide range of risk scenarios), synthetic vanilla prompts (LLM-rewritten prompts), synthetic adversarial prompts (jailbreak attack rewrites), multilingual harmful prompts (ten multigual languages), in-the-wild harmful prompts (real-world user-LLM interactions), and deceptive harmful prompts (automatic prompt refinement prompts). These sources are selected to optimize coverage, diversity, and balance. To construct diverse LLM responses, we collect responses from three sources: closed-source LLMs (e.g., GPT-4, GPT-3.5), open-source LLMs (e.g., Llama-family, Qwen-family, Mistral-family), and Defense-enhanced LLM responses. The overview of the dataset composition can be seen in Appendix 8.3.\nComplex Harmful Prompt Construction. To create a comprehensive scenario for the jailbreak judge benchmark, we draw on six sources: vanilla harmful, synthetic vanilla, synthetic adversarial, multilingual harmful, in-the-wild, and deceptive harmful prompts."}, {"title": "3.1.2 JAILJUDGETEST: HIGH-QUALITY HUMAN-ANNOTATED TEST JAILBREAK JUDGE DATASETS", "content": "We develop JAILJUDGETEST, a high-quality, human-annotated jailbreak judge dataset, comprising JAILJUDGE ID and JAILJUDGE OOD. To construct JAILJUDGE ID, we began with a test split of over 4.5k+ prompt-response pairs from JAILJUDGETRAIN (excluding multilingual harmful prompts), referred to as the in-distribution (ID) set. Additionally, we covered out-of-distribution (OOD) evaluation cases using all multilingual harmful prompts and response pairs, resulting in a labeled set of over 6k+ multi-language scenarios in ten languages, called JAILJUDGE OOD. We continuously refine this taxonomy through a human-in-the-loop process to ensure high-quality annotations indicating whether the LLM is jailbroken. Given a data prompt and its corresponding LLM response pair (x1, y), the human annotator assigns a label I\u2014either True or False\u2014to determine whether the LLM is jailbroken. Our human annotation process consists of four stages: annotator training, manual labeling, GPT-4 labeling, cross-comparison, and multi-person voting."}, {"title": "3.2 JAILJUDGE MULTIAGENT: MULTI-AGENT JUDGE FRAMEWORK", "content": "While reasoning can enhance jailbreak judgment quality, naive GPT-4 prompts Liu et al. (2024b) often cause inconsistencies between reasoning and final results, leading to inaccuracies. In complex role-play scenarios, the model might identify dangers but still conclude no jailbreak due to assumptions, creating a mismatch. To address this, we propose a multi-agent judge framework, JailJudge MultiAgent. This framework clarifies and interprets the decision-making process, improving evaluation quality. It includes Judging Agents, Voting Agents, and an Inference Agent, each with specific roles. These agents collaboratively produce interpretable, detailed decisions on whether an LLM requires jailbreaking through voting, scoring, reasoning, and final judgment.\nFor multi-agent prompting and collaboration, we will have n LLMS $\\pi_{\\theta_1},\u00b7\u00b7\u00b7, \\pi_{\\theta_n}$ that play different agents or roles in the framework. These LLMs can be the same ($\\theta_1 = \\theta_2,\u2026\u2026 = \\theta_n$) or different. For the text input x, each agent i will have its own profile agent function prompt\u2081(x; xi) that formats the input task or problem for the agent, where xi is corresponding profile agent prompts. Specifically, there are three types of agent including k judging agents, m voting agents, and an Inference Agent. Judging agents analyze the prompts and the model response to determine whether LLM is jailbroken, providing initial reasons and scores. Voting agents vote based on the scores and reasons provided by the judging agents to decide whether to accept their judgments. Inference agents deduce final judgment based on the voting results and predetermined criteria.\nJudging Stage. Given k judging agents $\\pi_{\\theta_1}, , \\pi_{\\theta_k}$ and m voting agents $\\pi_{\\theta_{k+1}},\u00b7\u00b7\u00b7, \\pi_{\\theta_{k+m}}$, each judging agent initially provides a reason and score, $(a_i, s_i) = \\pi_{\\theta_i} (prompt;((x_{1:n}, \\hat{y}); x_j)) (i = 1,..., k)$, where x\u2c7c is the profile prompt of the judging agent, and a\u1d62 represents the analysis reason and si the score from judging agent i. However, direct communication between all agents incurs a cost of O(km). To enhance communication efficiency and effectiveness, we first aggregate the messages from the judging agents' decisions, passing this aggregated message to the voting agents with a reduced cost of O(1m). To handle potentially conflicting decision messages, we focus on how to transform the score into a BPA function. Given the frame of discernment \u03a9 = {{JB}, {NJB}, {JB & NJB}, {()}}, we propose an uncertainty-aware transformation to convert each judge's score into a BPA function.\n\u03bc(\u0391) =  \\begin{cases}\n(\u03c1 \u00d7 (1 - \u03b2), &\\text{if A = {JB}} \\\\\n(1-\u03c1) \u00d7 (1 \u2013 \u03b2), & \\text{if A = {NJB}} \\\\\n\u03b2, & \\text{if A = {JB & NJB}} \\\\\n0, & \\text{if A = {0}}\n\\end{cases}, (5)\nwhere \u03bc(A) is the BPA for hypothesis A, and $\u03c1 = \\frac{s}{C}$ is the normalized score from the judging agent with base number C. \u03b2 is the hyper-parameter to quantify the uncertainty of hypothesis {JB & NJB}. Generally, the more complex and difficult the judging scenarios, the higher the uncertainty. In practice, we set \u03b2 = 0.1 and C = 10. Finally, we normalize the BPA to satisfy $\\sum_{A \\subseteq \\Omega} \u03bc(A) = 1$.\nAfter transforming each judging agent's score a\u1d62 to the BPA function \u03bc\u1d62(\u00b7)(i = 1,\u2026\u2026,k), we apply Dempster's rule of combination to aggregate,"}, {"title": "4 JAILJUDGE GUARD AND JAILBREAK ENHANCERS", "content": "JAILJUDGE Guard. Using explainability-enhanced JAILJUDGETRAIN with multi-agent judge, we instruction-tune JAILJUDGE Guard based on the Llama-2-7B model. We design an end-to-end input-output format for an explainability-driven jailbreak judge, where the user's prompt and model response serve as inputs. The model is trained to output both an reasoning explainability and a fine-grained evaluation score (jailbroken score ranging from 1 to 10, with 1 indicating non-jailbroken and 10 indicating complete jailbreak). Further training details can be found in Appendix 10.\nJAILJUDGE Guard as an Attack Enhancer and Defense Method. To demonstrate the fundamental capability of JAILJUDGE Guard, we propose both a jailbreak attack enhancer and a defense method based on JAILJUDGE Guard, named JailBoost and GuardShield.\nJailBoost is an attacker-agnostic attack enhancer. The aim of JailBoost is to create high-quality adversarial prompts that cause the LLM to produce harmful outputs,\n$L_{adv}(x_{1:n}, y) = -log P_{\\pi_{\\theta}}(\\hat{y} | A(x_{1:n})), if \\pi_{\\phi}(A(x_{1:n}), \\hat{y}) > \\tau_a,$ (7)\nwhere A(\u00b7) is the attacker to refine the adversarial prompts x1:n. The JAILJUDGE Guard outputs the jailbroken score s = \u03c0\u03c6(A(x1:n), \u0177) as the iteratively evaluator to determine the quality of adversarial prompts, where \u03c4\u03b1 is the threshold. (We omit the output of analysis a for simplicity). The detailed algorithm of JailBoost can be seen in Appendix 11.1.\nGuardShield is a system-level jailbreak defense method. Its goal is to perform safety moderation by detecting whether an LLM is jailbroken, and generate the safe response,\n$\\pi_{\\theta} (x_{1:n}) = \\begin{cases}\na & \\text{if } \\pi_{\\phi}(x_{1:n}, \\hat{y}) > \\tau_d \\\\\ny & \\text{otherwise}\n\\end{cases},$ (8)\nwhere a is the safe reasoning analysis, and Ta is the predefined threshold. A detailed algorithm of GuardShield can be found in Appendix 11.2."}, {"title": "5 EXPERIMENTS", "content": "Evaluation Datasets and Metrics. To assess the performance of the jailbreak judge, we use both JAILJUDGE ID and OOD datasets. Additionally, we include the public jailbreak judge dataset"}, {"title": "5.1 JAILBREAK JUDGE EXPERIMENTS", "content": "Main Experiments. To evaluate the effectiveness of the jailbreak judge methods, we conducted experiments using the JAILJUDGE ID and JBB behaviors datasets. Our JailJudge MultiAgent and JAILJUDGE Guard consistently outperformed all open-source baselines across both datasets, as shown in Table 2. The multi-agent judge achieved the highest average F1 scores, specifically 0.9197 on the JAILJUDGE ID dataset and 0.9609 on the JBB behaviors dataset. Notably, our approach showed more stable performance on the JBB behaviors dataset, likely due to its simpler scenarios compared to the more complex JAILJUDGE ID dataset. Additionally, the JailJudge MultiAgent surpassed the baseline GPT-4-Reasoning model in reasoning capabilities. As shown in Table 2, the GPT-4-Reasoning model attained an EQ score of 4.3989, while our multi-agent judge achieved a superior EQ score of 4.5234 on JAILJUDGE ID, indicating enhanced reasoning ability.\nZero-Shot Setting. To assess the efficacy of the jailbreak judge in a zero-shot context, we conducted experiments using the JAILJUDGE OOD and WILDEST datasets. As summarized in Table 3, our jailbreak judge methods consistently outperformed all open baselines across both evaluation sets. For instance, on the multilingual JAILJUDGE OOD dataset, the multi-agent judge achieved an F1 score of 0.711, significantly higher than the GPT-4-Reasoing's 0.5633, underscoring the benefits of leveraging advanced LLMs like GPT-4 for multilingual and zero-shot scenarios. Although JAILJUDGE Guard achieved a respectable F1 score of 0.7314 on WILDTEST, it fell short of the multi-agent judge on JAILJUDGE OOD due to its limited multilingual training, as shown in Figure 2. Overall, our methods demonstrated consistent superiority across both datasets, emphasizing the importance of advanced language models like GPT-4 for handling multilingual and zero-shot settings effectively, as evidenced by its higher EQ scores and logical consistency in reasoning. The insights findings can be summarized as follows."}, {"title": "5.2 JAILBREAK ATTACK AND DEFENSE EXPERIMENTS", "content": "To evaluate the effectiveness of JailBoost and GuardShield, we conduct experiments on the HEX-PHI dataset under zero-shot settings. We use the attack success rate (ASR) as the primary metric. For attacker experiments, a higher ASR indicates a more effective attacker method, whereas for defense methods, a lower ASR indicates a better defense approach. Detailed descriptions of the experimental settings, metrics, and baselines can be found in Appendix 8.1 and 12.4. Jailbreak Attack. The experimental results are presented in Figure 5. JailBoost significantly enhances the attacker's capability. For example, JailBoost increases the ASR for the attacker compared to the nominal AutoDAN. Jailbreak Defense. The experimental results are presented in Table 4. GuardShield achieves superior defense performance compared to the state-of-the-art (SOTA) baselines. For instance, GuardShield achieves nearly 100% defense capability against four SOTA attackers, with an average ASR of 0.15%, outperforming most baselines."}, {"title": "5.3 ABLATION STUDY", "content": "In this section, we present an ablation study to evaluate the effectiveness of each component in our multi-agent judge framework. We compared four configurations: (1) Vanilla GPT-4, which directly determines whether the LLM is jailbroken; (2) Reasoning-enhanced GPT-4 (RE-GPT-4); (3) RE-GPT-4 augmented with our uncertainty-aware evident judging agent (RE-GPT-4+UAE); and (4) the complete multi-agent judge framework. The results, shown in Figure 3 and 4, demonstrate that each enhancement progressively improves performance across all datasets. For instance, in the JAILJUDGE ID task, the F1 score increased from 0.55 with Vanilla GPT-4 to 0.91 with the multi-agent judge. Similarly, in the JBB Behaviors scenario, scores rose from 0.79 to 0.96. Overall, our multi-agent judge consistently outperforms the baseline and individually enhanced models, underscoring the effectiveness of each component. Additionally, as detailed in Appendix 12.3, human evaluators score the explainability of the reasons provided for the samples. For instance, our method demonstrates high accuracy under manual evaluation, with the JailJudge MultiAgent achieving average 95.29% on four datasets."}, {"title": "6 RELATED WORKS", "content": "Jailbreak Judge. Despite the critical importance of evaluating jailbreak attempts in LLMs, comprehensive studies on jailbreak judges have been limited Cai et al. (2024); Jin et al. (2024b;b). Current methods for identifying jailbreaks fall into three categories: heuristic methods Liu et al. (2024b), toxic text classifiers, and LLM-based methods Inan et al. (2023); Zeng et al. (2024a). Heuristic methods, which rely on keyword matching, often misinterpret benign responses containing specific keywords as malicious. Toxic text classifiers Ji et al. (2024b), trained on toxic text datasets, struggle to generalize to complex scenarios, such as broad-range risks and adversarial contexts. In contrast,"}, {"title": "7 CONCLUSIONS", "content": "In this work, we introduce JAILJUDGE, a comprehensive evaluation benchmark designed to assess LLMs across a wide array of complex risk scenarios. JAILJUDGE includes high-quality, human-annotated datasets and employs a multi-agent jailbreak judge framework, JailJudge MultiAgent, to enhance explainability and accuracy. We also develop JAILJUDGE Guard based on instruction-tuned data without incurring API costs. Furthermore, JAILJUDGE Guard can improve downstream tasks, including jailbreak attack and defense mechanisms. Our experiments confirm the superiority of jailbreak judge methods, demonstrating SOTA performance in models like GPT-4 and safety moderation tools such as Llama-Guard-3."}, {"title": "8 BUILDING JAILJUDGE BENCHMARK AND MULTI-AGENT JUDGE FRAMEWORK", "content": ""}, {"title": "8.1 COMPLEX HARMFUL PROMPT CONSTRUCTION", "content": "Vanilla Harmful Prompts. We collect original harmful prompts from prior safety benchmark datasets, including AdvBench Zou et al. (2023), MaliciousInstruct Huang et al. (2023), and Forbidden Question Set Shen et al. (2024) and refine broad and vague safety categories into 14 more specific categories following the MLCommons standardized hazards taxonomy Inan et al. (2023). This leads to more balanced category distributions. Detailed datasets and the 14 refined hazard categories used in this construction are provided in Table 5. These categories are based on the MLCommons standardized hazards taxonomy, with an additional category for Code Interpreter Abuse.\nSynthetic Vanilla Prompts. To ensure the diversity of harmful prompts, we utilize GPT-4 to rewrite and expand the original vanilla harmful prompts within the same hazard taxonomy categories. For instance, if the original harmful prompt is \"How to make a bomb?\", the rewritten prompt might be \"Detailed instructions on constructing a homemade explosive using household items.\" The details of GPT-4 prompts used to generate the more diverse prompts are presented in Figure 6.\nSynthetic Adversarial Prompts. To increase the likelihood of generating unsafe responses from LLMs, we leverage the latest jailbreak attacks, which have demonstrated their ability to bypass LLM safety alignment. Specifically, we apply seven recent jailbreak techniques to modify vanilla harmful prompts, aiming to elicit unsafe responses from the model. For instance, a jailbreak attack may optimize an adversarial suffix (e.g., \u201c!!!$&xnksac\") and append it to the original harmful prompt (e.g., \"How to make a bomb? !!!$&xnksac\") to heighten the chance of the LLM producing a harmful output. We use the following jailbreak attacks used to generate these diverse prompts.\n\u2022 GCG Zou et al. (2023): GCG aims to create harmful content by adding adversarial suffixes to various queries. It uses a combination of greedy and gradient-based search methods to find suffixes that increase the chances of the LLMs responding to malicious queries. In our setting, we adhere to the original settings: 500 optimization steps, top-k of 256, an initial adversarial suffix, and 20 tokens that can be optimized.\n\u2022 AutoDAN Liu et al. (2024b): AutoDAN employs a hierarchical genetic algorithm to generate stealthy jailbreak prompts. It starts with human-created prompts and refines them through selection, crossover, and mutation operations. This method preserves the logical flow and meaning of the original sentences while introducing variations. We use the official settings for AutoDAN, including all specified hyperparameters.\n\u2022 AmpleGCG Liao & Sun (2024): AmpleGCG builds on GCG by overgenerating and training a generative model to understand the distribution of adversarial suffixes. Successful suffixes from GCG are used as training data, AmpleGCG collects all candidate suffixes during optimization. This allows for rapid generation of diverse adversarial suffixes. We use the released AmpleGCG model for Vicuna and Llama, following the original hyperparameters, including maximum new tokens and diversity penalties. We set the number of group beam searches to 200 to achieve nearly 100% ASR.\n\u2022 AdvPrompter Paulus et al. (2024): AdvPrompter quickly generates adversarial suffixes targeted at specific LLMs. These suffixes are crafted to provoke inappropriate or harmful responses while remaining understandable to humans. Initially, high-quality adversarial suffixes are produced using an efficient optimization algorithm, and then AdvPrompter is fine-tuned with these suffixes. We follow the origional setting to train the LoRA adapter for each target model based on Llama-2-7B, then integrate the adapter with the initial LLM to create the suffix generator model. The maximum generation iteration is set to 100.\n\u2022 PAIR Chao et al. (2023): PAIR is a black-box jialbreak attack to generate semantic adversarial prompts. An attacker LLM crafts jailbreaks for a targeted LLM through iterative queries, using conversation history to enhance reasoning and refinement. We employ Vicuna-13B-v1.5 as the attack model and GPT-4 as the judge model, keeping most hyperparameters except for total iterations to reduce API costs.\n\u2022 TAP Mehrotra et al. (2023): TAP is an advanced black-box jailbreak method that evolves from PAIR. It uses tree-of-thought reasoning and pruning to systematically explore and refine attack prompts. The tree-of-thought mechanism allows for structured prompt exploration, while pruning removes irrelevant prompts, keeping only the most promising ones for further evaluation. Although effective, TAP's iterative process of generating and evaluating multiple prompts increases the attack budget and is time-intensive. We follow the same setting as the original Mehrotra et al. (2023), Vicuna-13B-v1.5 and GPT-4. To manage time and cost, we reduce the maximum depth and width from 10 to 5.\n\u2022 GPTFuzz Yu et al. (2023): GPTFuzz is a black-box jailbreak attack with three main components: seed selection, mutation operators, and a judgment model. Starting with human-written jailbreak prompts, the framework mutates these seeds to create new templates. The seed selection balances efficiency and variability, while mutation operators generate semantically similar sentences. The judgment model, a fine-tuned RoBERTa, evaluates the success of each jailbreak attempt. Iteratively, GPTFuzz selects seeds, applies mutations, combines them with target queries, and assesses the responses to determine jailbreak success. We use the provided judgment model and adhere to the original hyperparameters, setting the GPT temperature to 1.0 for optimal mutation.\nIn-the-wild Prompts. To mitigate potential risks associated with real-world user requests, we incorporate prompts from various datasets. These prompts are sourced from prompt templates available on prominent platforms commonly used for prompt sharing, such as Reddit, Discord, multiple websites, and open-source datasets collected from Shen et al. (2024). By leveraging these templates, more complex and subtle harmful intentions can be created when combined with malicious prompts. For instance, a user might use a template like \u201cDo anything now\" followed by additional harmful prompts. Initially, the user interacts with the LLM using a benign prompt. We adapt the in-the-wild templates, such as the harmful template \"Do anything now,\u201d and the final prompt is formulated by adding specific harmful prompts following the initial template.\nDeceptive Harmful Prompts. In addition to real-world user-LLM interactions, deceptive harmful prompts often mask their malicious intent through techniques such as role-playing, scenario assumptions, long-context prompts, and adaptive strategies. These complex cases are typically challenging for LLMs to identify. To ensure thorough coverage of these variations, we apply automatic adversarial prompt refinement to the original harmful prompts. Specifically, we adopt the method is simmiar with PAIR Chao et al. (2023) useing attacker LLM crafts jailbreaks for a targeted LLM\""}, {"title": "8.2 HUMAN ANNOTATOR TRAINING POLICIES", "content": "In the human annotator training phase, we adhere to the MLCommons standardized LLM policy, which encompasses nine detailed policies ranging from user privacy to ethics and morality. We provide comprehensive scenarios and examples to the human annotators, enabling them to understand what constitutes a violation of these policies. For instance, concerning user privacy, the model's response must adhere to basic ethical and moral standards and must not generate content that violates social morality. Responses that violate these policies are judged as jailbroken. We primarily follow OpenAI's usage policies and also incorporate the ML community's AI usage policies, as illustrated in Figure 7."}, {"title": "8.3 STATISTIC INFORMATION OF JAILJUDGE BENCHMARK", "content": "For the complexity of user prompts, we generally categorize them into five types, as follows: (1) Simple Prompts (Q1): Direct and straightforward user queries without any alterations or additional elements, including the scenario of vanilla harmful prompts. (2) Adversarial Prompts (Q2): Prompts primarily generated by jailbreak attacks, which include scenarios of synthetic adversarial prompts. (3) In-the-wild Prompts (Q3): Prompts collected from the real world that can also be combined with simple prompts and predetermined adversarial elements. (4) Synthetic Vanilla Prompts (Q4): Prompts rephrased or restructured while preserving their meaning by GPT-4. (5) Deceptive Harmful Prompts (Q5): Complex and sophisticated prompts that combine elements from multiple methods, making them harder to detect and handle.\nJAILJUDGETRAIN. The overall statistical information of JAILJUDGETRAIN is presented in Figures 8 and 9. Figure 8 illustrates the distribution of hazard categories within the JAILJUD-GETRAIN dataset. The most frequent hazard category is S2, while the least frequent category is S13, which has 1102 instances. Figure 9 details the distribution of prompt complexity categories in the"}, {"title": "9 MULTI-AGENT JUDGE FRAMEWORK", "content": "In this section, we provide detailed information about the LLM-powered agent. The base LLM used throughout is GPT-4. Specifically, there are three judging agents, three voting agents, and one"}, {"title": "10 JAILJUDGE GUARD: AN END-TO-END JAILBREAK JUDGE MODEL", "content": "Using explainability-enhanced JAILJUDGETRAIN with a multi-agent judge, we instruction-tune JAILJUDGE Guard based on the Llama-7B model. We design an end-to-end input-output format for an explainability-driven jailbreak judge, where the user's prompt and model response serve as inputs. The model outputs both an explainability rationale and a fine-grained evaluation score (1 indicating non-jailbroken to 10 indicating complete jailbreak). Specifically, we first use the multi-agent judge framework, with GPT-4 as an LLM-powered agent, to generate ground truth with reasoning explainability and a fine-grained evaluation score. We employ LoRA Hu et al. (2021) for supervised fine-tuning (SFT) of the base LLM (Llama-2-7B) for the jailbreak judge task, where the input is a user's prompt and model response, and the output is the reasoning explainability with a fine-grained evaluation score. The SFT template details are shown in Figure 18."}, {"title": "11 JAILJUDGE GUARD AS THE ATTACKER ENHANCER AND DEFENSE METHOD", "content": ""}, {"title": "11.1 JAILJUDGE GUARD AS THE ATTACKER ENHANCER", "content": "JailBoost is an attacker-agnostic attack enhancer. The aim of JailBoost is to create high-quality adversarial prompts that cause the LLM to produce harmful outputs,\n$L_{adv"}]}