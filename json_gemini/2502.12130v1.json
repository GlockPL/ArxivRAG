{"title": "ARMAP: SCALING AUTONOMOUS AGENTS VIA AUTOMATIC REWARD MODELING AND PLANNING", "authors": ["Zhenfang Chen", "Delin Chen", "Rui Sun", "Wenjun Liu", "Chuang Gan"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a range of text-generation tasks. However, LLMs still struggle with problems requiring multi-step decision-making and environmental feedback, such as online shopping, scientific reasoning, and mathematical problem-solving. Unlike pure text data, collecting large-scale decision-making data is challenging. Moreover, many powerful LLMs are only accessible through APIs, which hinders their fine-tuning for agent tasks due to cost and complexity. To address LLM agents' limitations, we propose a framework that can automatically learn a reward model from the environment without human annotations. This model can be used to evaluate the action trajectories of LLM agents and provide heuristics for task planning. Specifically, our approach involves employing one LLM-based agent to navigate an environment randomly, generating diverse action trajectories. Subsequently, a separate LLM is leveraged to assign a task intent and synthesize a negative response alongside the correct response for each trajectory. These triplets (task intent, positive response, and negative response) are then utilized as training data to optimize a reward model capable of scoring action trajectories. This reward model can be integrated with LLM-based agents and various planning algorithms to enhance task-solving performance. The effectiveness and generalizability of our framework are demonstrated through evaluations conducted on different agent benchmarks. In conclusion, our proposed framework represents a significant ad-vancement in enhancing LLM agents' decision-making capabilities. By automating the learning of reward models, we overcome the challenges of data scarcity and API limitations, potentially revolutionizing the application of LLMs in complex and interactive environments. This research paves the way for more sophisticated AI agents capable of tackling a wide range of real-world problems requiring multi-step decision-making.", "sections": [{"title": "1 INTRODUCTION", "content": "Developing AI agents capable of perceiving environments, understanding instructions, and acting to accomplish a wide range of tasks in interactive settings have many real-world applications, including virtual human assistants, business process management, and robotic process automation.\nThe recent advent of large generative models has revolutionized numerous applications, such as question answering, text summarization, and multi-modal understanding. However, while these models excel in text comprehension and generation tasks, their performance in decision-making scenarios such as online shopping and scientific reasoning falls relative short of human capabilities."}, {"title": "2 RELATED WORK", "content": "LLMs for Agent tasks. Our research is related to deploying large language models (LLMs) as agents for decision-making tasks in interactive environments. Earlier works fine-tuned models like BERT for decision-making in simplified environments, such as online shopping or mobile phone manipulation. With the advent of large language models , it became feasible to perform decision-making tasks through zero-shot or few-shot in-context learning. To better assess the capabilities of LLMs as agents, several models have been developed. Most approaches provide the agent with observation and action history, and the language model predicts the next action via in-context learning. Additionally, some methods attempt to distill trajectories from state-of-the-art language models to train more effective policy models. In contrast, our paper introduces a novel framework that automatically learns a reward model from LLM agent navigation, using it to guide the agents in making more effective plans.\nLLM Planning. Our paper is also related to planning with large language models. Early re-searchers often prompted large language models to directly perform agent tasks. Later, proposed ReAct, which combined LLMs for action prediction with chain-of-thought prompting. Several other works have focused on enhancing multi-step reasoning capabilities by integrating LLMs with tree search methods. Our model differs from these previous studies in several significant ways. First, rather than solely focusing on text generation tasks, our pipeline addresses multi-step action planning tasks in interactive environments, where we must consider not only historical input but also multimodal feedback from the environment. Additionally, our pipeline involves automatic learning of the reward model from the environment without relying on human-annotated data, whereas previous works rely on prompting-based frameworks that require large commercial LLMs like GPT-4 to learn action prediction. Furthermore, ARMAP supports a variety of planning algorithms beyond tree search.\nLearning from AI Feedback. In contrast to prior work on LLM planning, our approach also draws on recent advances in learning from AI feedback. These studies initially prompt state-of-the-art large language models to generate text responses that adhere to predefined principles"}, {"title": "3 MODEL", "content": "In this section, we provide a detailed introduction to our framework, autonomous Agents from automatic Reward Modeling And Planning (ARMAP). The framework includes automated reward data generation in section 3.2, reward model design in section 3.3, and planning algorithms in section 3.4."}, {"title": "3.1 BACKGROUND", "content": "The planning tasks for LLM agents can be typically formulated as a Partially Observable Markov Decision Process (POMDP): (X, S, A, O, T), where:\n\u2022 X is the set of text instructions;\n\u2022 S is the set of environment states;\n\u2022 A is the set of available actions at each state;\n\u2022 O represents the observations available to the agents, including text descriptions and visual information about the environment in our setting;\n\u2022T : S \u00d7 A \u2192 S is the transition function of states after taking actions, which is given by the environment in our settings.\nGiven a task instruction x \u2208 X and the initial environment state so \u2208 S, planning tasks require the LLM agents to propose a sequence of actions {an}Nn=1 that aim to complete the given task, where an \u2208 A represents the action taken at time step n, and N is the total number of actions executed in a trajectory. Following the n-th action, the environment transitions to state sn, and the agent receives a new observation on. Based on the accumulated state and action histories, the task evaluator determines whether the task is completed.\nAn important component of our framework is the learned reward model R, which estimates whether a trajectory h has successfully addressed the task:\nr = R(x, h), (1)\nwhere h = {{an}Nn=1, {On}Nn=0}, {an}Nn=1 are the actions taken in the trajectory, {on}Nn=0 are the corresponding environment observations, and r is the predicted reward from the reward model. By integrating this reward model with LLM agents, we can enhance their performance across various environments using different planning algorithms."}, {"title": "3.2 AUTOMATIC REWARD DATA GENERATION.", "content": "To train a reward model capable of estimating the reward value of history trajectories, we first need to collect a set of training language instructions {xm}Mm=1, where M represents the number of instruction goals. Each instruction corresponds to a set of positive trajectories {hm}M_m=1 that match the instruction goals and a set of negative trajectories {hm}M_m=1 that fail to meet the task requirements. This process typically involves human annotators and is time-consuming and labor-intensive. As shown in Fig. 8 of the Appendix. we automate data collection by using Large Language Model (LLM) agents to navigate environments and summarize the navigation goals without human labels.\nInstruction Synthesis. The first step in data generation is to propose a task instruction for a given observation. We achieve this using the in-context learning capabilities of LLMs. The prompt for instruction generation is shown in Fig. 9 of the Appendix. Specifically, we provide some few-shot examples in context along with the observation of an environment state to an LLM, asking it to summarize the observation and propose instruction goals. In this way, we collect a set of synthesized language instructions {xraw}Mm=1, where M represents the total number of synthesized instructions.\nTrajectory Collection. Given the synthesized instructions xrawm and the environment, an LLM-based agent is instructed to take actions and navigate the environment to generate diverse trajectories {xraw, hmm=0 aimed at accomplishing the task instructions. Here, hm represents the m-th history trajectory, which consists of N actions {an}Nn=1 and N + 1 environment observations {on}Nn=0. Due to the limited capabilities of current LLMs, the generated trajectories hm may not always align"}, {"title": "3.3 REWARD MODEL DESIGN.", "content": "Reward Model Architectures. Theoretically, we can adopt any vision-language model that can take a sequence of visual and text inputs as the backbone for the proposed reward model. In our implementation, we use the recent VILA model as the backbone for reward modeling since it has carefully maintained open-source code, shows strong performance on standard vision-language benchmarks and support multiple image input.\nThe goal of the reward model is to predict a reward score to estimate whether the given trajectory (xm, hm) has satisfied the task instruction or not, which is different from the original goal of VILA models that generate a series of text tokens to respond to the task query. To handle this problem, we additionally add a fully-connected layer for the model, which linearly maps the hidden state of the last layer into a scalar value.\nOptimazation Target. Given the pairwise data that is automatically synthesized from the environments in Section 3.2, we optimize the reward model by distinguishing the good trajectories (xm, hm) from bad ones (xm, hm). Following standard works of reinforcement learning from human feedback, we treat the optimization problem of the reward model as a binary classification problem and adopt a cross-entropy loss. Formally, we have\nL(\u03b8) = -E(xm, h, hm) [log \u03c3 (R\u03b8(xm, hm) \u2013 R\u03b8(xm, hm))], (2)\nwhere \u03c3 is the sigmoid function and \u03b8 are the learnable parameters in the reward model R. By optimizing this target, the reward model is trained to give higher value scores to the trajectories that are closer to the goal described in the task instruction."}, {"title": "3.4 PLANNING WITH LARGE VISION-LANGAUGE REWARD MODEL.", "content": "After getting the reward model to estimate how well a sampled trajectory match the given task instruction, we are able to combine it with different planning algorithms to improve LLM agents' performance. Here, we summarize the typical algorithms we can adopt in this paper.\nBest of N. This is a simple algorithm that we can adopt the learned reward model to improve the LLM agents' performances. We first prompt the LLM agent to generate n different trajectories independently and choose the one with the highest predicted reward score as the prediction for evaluation. Note that this simple method is previously used in natural language generation and we adopt it in the context of agent tasks to study the effectiveness of the reward model for agent tasks.\nReflexion. Reflexion is a planning framework that enables large language models (LLMs) to learn from trial-and-error without additional fine-tuning. Instead of updating model weights, Reflexion agents use verbal feedback derived from task outcomes. This feedback is converted into reflective summaries and stored in an episodic memory buffer, which informs future decisions. Reflexion supports various feedback types and improves performance across decision-making, coding, and reasoning tasks by providing linguistic reinforcement that mimics human self-reflection and learning.\nMCTS. We also consider tree search-based planning algorithms like Monte Carlo Tree Search (MCTS) to find the optimal policy. There is a tree structure"}, {"title": "4 EXPERIMENTS", "content": "In this section, we conduct a series of experiments to demonstrate the effectiveness of the proposed framework for agent tasks. First, we evaluate the framework's performance on standard agent benchmarks, detailed in Section 4.2. Next, we show how customizing the reward target during inference allows us to generate more tailored action plans, as described in Section 4.3. Finally, we conduct ablation studies in Section 4.4. Before delving into the experimental results, we provide an overview of our experimental setup."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Environments. We evaluate the ARMAP framework in three different environments:\n\u2022 Webshop is a well-known environment for online shopping, where the agent must search for and select products on the website to obtain a final result. Following the setup of AgentBench for LLM evaluation, we test the model on the validation split, using the default matching reward as the evaluation metric.\n\u2022 Science World is an interactive benchmark designed for embodied science experiments. It places agents in a simulated text-based environment where they must perform elementary science experiments by navigating the environment, manipulating objects, and observing outcomes. The aim is to assess whether AI models can apply scientific knowledge, rather than merely retrieve or assemble information. We evaluate the framework on both seen and unseen splits.\n\u2022 Game of 24 is a mathematical game where the agent is given four numbers and must use arithmetic operations (addition, subtraction, multiplication, and division) to make the number 24. For instance, given the input '3, 5, 7, 11', one possible solution is '(7-3)*(11-5) = 24'. Following we selected 100 challenging puzzles, specifically those indexed from 901 to 1,000, and the performance metric is the success rate across these puzzles. As shown in Fig. 7 of the Appendix, we use the chain-of-thought prompting technique, prompting the LLM agents to output intermediate steps followed by the final answer. Each step of the solution is considered an action.\nLLM Setup. Our framework requires LLM models to act as agents, generating synthetic task instructions from the environment along with few-shot examples in the prompt context. We also deploy agents to perform these synthetic tasks in the environment, collecting diverse trajectories for further analysis. In this paper, we primarily use the Llama3-70b-instruct model to synthesize training data for the automatic reward models, as it is open-source, easy to deploy locally, and delivers robust performance. We avoid state-of-the-art commercial models like GPT-4 or Gemini due to their high costs and the complexity of reproducing results caused by frequent model updates, making them less suitable for our research objectives.\nTo evaluate the performance of various LLM agents, we serve a representative set of LLM APIs locally, balancing model diversity with affordable serving costs. We identify the LLMs by their model family and size. Specifically, these are Llama70B, Llama8B, Mistral7B, and Phi3.8B. We note that these open-source model families are frequently updated, and we provide the current model links in the Appendix A.3. All models can be easily set up using the vLLM library and a single H100 GPU."}, {"title": "4.2 EFFECTIVENESS FOR REWARD PLANNING.", "content": "In this section, we investigate the effectiveness of the framework across different language mod-els and various planning algorithms. The results are shown in Table 1. Based on the table, we have the following observations. First, our proposed pipeline is effective, as it consistently outperforms the Sampling and Greedy baselines across different planning algorithms. Additionally, we observe that the average improvement is more significant on weaker models, such as Phi and Mistral-7B, compared to stronger models like Llama3-1-70B. We believe this is because weaker models explore more low-reward trajectories, providing greater opportunities for the reward model to improve performance.\nAmong the three planning algorithms, MCTS performs the best on average, likely due to its superior mechanisms for identifying higher-reward trajectories and searching less-explored trajectories. We also notice that Reflexion performs the worst on weaker models like Mistral7B and Phi3.8B. We suspect this is because Reflexion was designed for ChatGPT-family-based agents and requires the LLM agent to possess strong capabilities for learning from trial and error. Finally, we present qualitative results of different methods in Fig. 3, where it is clear that our ARMAP generates better trajectories than the baselines, aided by the guidance of automatic reward models. In Appendix A.5, we analyze several failure cases, offer more detailed insights into the limitations of the current approach, and suggest potential improvements in reward modeling."}, {"title": "4.3 CONTROLLABLE GENERATION.", "content": "Another benefit of our ARMAP pipeline is that we can customize our reward targets during inference, allowing us to generate more controllable action sequences, rather than solely maximizing the predicted rewards. Agent fine-tuning methods find it challenging to achieve this goal since agent behaviors are typically fixed during inference. We conducted experiments in the Webshop environment to evaluate the impact of customizable reward targets. In addition to the original objective of maximizing the predicted reward R(x, h), we defined two additional optimization targets. First, we aimed to minimize the number of actions in the trajectory history, defining the reward target as R(x, h) \u2013 NumberOfAction(h). Second, we sought to minimize the price of the target product, with a customized target of R(x, h) \u2013 PriceOfProduct(h). Table 2 presents the results. By applying a length penalty on the reward target for ARMAP-M, we reduced the average action length from 4.5 to 4 and the average product price from 97.9 to 69.0, while maintaining comparable performance on the default matching reward. Similar performance was observed for ARMAP-B. Additionally, we provide a qualitative example in Fig. 4. From this example, we can see that our customized reward target successfully guided the LLM agent to purchase products with fewer action steps while still finding the target product."}, {"title": "4.4 ABLATION STUDIES.", "content": "We conduct ablation studies to investigate the effectiveness of the framework. Specifically, we aim to answer the following questions: Q1. Can we train a policy model with fully supervised learning to handle multi-step tasks from the synthesized trajectory data? Q2. Can a large, general language model be used as the reward model to perform guidance without automatic reward learning?"}, {"title": "5 CONCLUSION", "content": "We propose a framework, ARMAP, for large language model (LLM) agents to manage tasks that require multi-step decision-making and environmental feedback, such as online shopping or scientific"}, {"title": "A APPENDIX", "content": "In this section, we provide supplementary material for the main paper."}, {"title": "A.1 EXPERIMENTS ON ALFWORLD AND AGENTCLINIC.", "content": "We extend our experiment on ALFWorld, a classic environment for House-Holding, where the agent must accomplish tasks in physical house-holding environments, like \"Put a pan on the dining table\". Following the setup of AgentBench for LLM evaluation, we test the model on the dev and std split, using the default success rate as the evaluation metric. Specifically, we used LLaMa-3.1-70B to generate around 1600 pairs of positive and negative samples with our data generation pipeline. Then we train a reward model with these synthesized data. We evaluate our ARMAP framework on ALFWorld using various planning algorithms, including Reflexion and Best-of-N, which we refer to as ARMAP-R and ARMAP-B, respectively. Additionally, we compare our approach with two baseline methods that do not incorporate reward model guidance: Sampling and Greedy. The results are shown below. As shown in Table 4, our model still performs well in this challenging environment, which contains diverse scenes and long-horizon planning tasks.\nWe also extended our experiments to ClinicalAgent, an environment designed for medical decision-making tasks. ClinicalAgent evaluates models on their ability to interpret clinical scenarios and make accurate, high-stakes decisions. Results of ClinicalAgent are provided in Table 5, further supporting the versatility of ARMAP in domains requiring precise reasoning."}, {"title": "A.2 ABLATION STUDY.", "content": "Dependence on Quality of Synthetic Data from Various LLMs. We choose ScienceWorld and conduct experiments to study the effectiveness of different reward models. As shown in Table 6, the left column represents the results of using LLaMA-8B greedy directly and the Best of N results of LLaMA-8B with the reward model trained by the data generated from LLaMA-70B, LLaMA-8B, Mistral-7B, and Phi-3.8B, respectively. Greedy is our baseline result and it can be observed that using the reward model leads to better experimental outcomes. Among all the results, LLaMA-70B achieves the best performance. Compared to the other three models, LLaMA-70B has the largest scale and is naturally the most capable model. LLaMA-8B and Mistral-7B have a similar number of parameters, and in the ScienceWorld task, Mistral-7B performs better than LLaMA-8B. Phi-3.8B is the smallest of these models, yet it still achieved very good results. Notably, compared to the larger-scale LLaMA-8B and Mistral-7B, Phi-3.8B still scored better. These results indicate that our method exhibits good robustness when faced with LLMs of different scales and capabilities. Even with the smallest model, our method can still achieve good results. From these experimental"}, {"title": "A.3 IMPLEMENTATION DETAILS.", "content": "Large Pretrain Model Setup. We serve a diverse set of open-source LLM APIs to evaluate the effectiveness of the proposed pipeline. We list all the open-source models and their weights on huggingface in table 13. All these models can be easily setup and reproduced with the VLLM libarary. We prove the effectiveness of our ARMAP framework across different LLM APIs.\nEnvironment Setup. We build our environments based on the environment setup of the previous works. For Webshop and ALFWorld environment, we start these docker environments from AgentBench and implement different planning algorithms, Reflexion, Best-of-N and MCTS on it. Similarly, we build our ScienceWorld, Game of 24 and AgentClinic environments from respectively.\nPlanning Algorithm Details. We compare the performance of different planning algorithms by limiting their maximum explored trajectory number. We set the maximum number to be 10 on Webshop and ScieneWorld in consideration of effectiveness and efficiency. We set the maximum number to be 100 on Game of 24 following the setup of Yao et al. In Webshop, ScienceWorld, ALFWorld and AgentClinic benchmarks, we only consider the top 10 available actions suggested by the LLM agent at each state to reduce search space. We also set a trajectory's maximal action number length to 10 for simplicity.\nFor Reflexion, we set the maximum trial number to be 10 for all tasks. For different tasks and models, we set the threshold of Reflexion separately. During the iteration process, if the reward of the current"}, {"title": "A.5 FAILURE CASE ANALYSIS", "content": "In this section, we investigate the common failure cases of our framework, aiming to provide data points and insights for future research.\nThe most common error occurs when there are multiple restrictions in the instruction, the reward model overlooks some of these key conditions. A representative example is illustrated in Fig. 15, the model focuses on price and size but ignores the details about 'Fluoride' hidden in the product description.\nAnother common failure mode occurs when commonsense knowledge is involved. As demonstrated in Fig. 16, the agent was tasked with buying a blackout shade but failed to choose both the color and the size. While, in everyday life, size is generally more important, the reward model prioritized color instead. In Fig. 17, the reward model cannot assess the lifespan of dragonflies and chipmunks because it lacks the necessary biological knowledge.\nDiscussion. The analysis of failure modes highlights the significant potential of our framework. To improve its performance, we propose two possible strategies for improvements in reward modeling: (a) Constructing Data with Focus on Complex and Detailed Conditions: enhancing the dataset to include scenarios with higher complexity and more nuanced conditions will help the framework better handle intricate situations and edge cases. (b) Intervening in Reward Scoring with External Knowledge: incorporating external knowledge by combining a prompted Large Language Model with the trained reward model. This approach allows the LLM's generalized knowledge to calibrate the reward scores in a controllable manner, improving the overall accuracy and robustness of the reward model."}]}