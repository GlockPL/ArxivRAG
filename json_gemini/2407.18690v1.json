{"title": "Collaborative Evolving Strategy for Automatic Data-Centric Development", "authors": ["Xu Yang", "Haotian Chen", "Wenjun Feng", "Haoxue Wang", "Zeqi Ye", "Xinjie Shen", "Xiao Yang", "Shizhao Sun", "Weiqing Liu", "Jiang Bian"], "abstract": "Artificial Intelligence (AI) significantly influences many fields, largely thanks to the vast amounts of high-quality data for machine learning models. The emphasis is now on a data-centric AI strategy, prioritizing data development over model design progress. Automating this process is crucial. In this paper, we serve as the first work to introduce the automatic data-centric development (AD\u00b2) task and outline its core challenges, which require domain-experts-like task scheduling and implementation capability, largely unexplored by previous work. By leveraging the strong complex problem-solving capabilities of large language models (LLMs), we propose an LLM-based autonomous agent, equipped with a strategy named Collaborative Knowledge-STudying-Enhanced Evolution by Retrieval (Co-STEER), to simultaneously address all the challenges. Specifically, our proposed Co-STEER agent enriches its domain knowledge through our proposed evolving strategy and develops both its scheduling and implementation skills by accumulating and retrieving domain-specific practical experience. With an improved schedule, the capability for implementation accelerates. Simultaneously, as implementation feedback becomes more thorough, the scheduling accuracy increases. These two capabilities evolve together through practical feedback, enabling a collaborative evolution process. Extensive experimental results demonstrate that our Co-STEER agent breaks new ground in AD\u00b2 research, possesses strong evolvable schedule and implementation ability, and demonstrates the significant effectiveness of its components. Our Co-STEER paves the way for AD2 advancements.", "sections": [{"title": "1 Introduction", "content": "Scientific advances have proceeded via a combination of different paradigms [13, 33], where data-driven discovery is an emerging paradigm formalized in recent years and more significantly accelerates scientific advances [13]. All the paradigms experience impediments to progress, including the expensive time cost of verifying a scientific hypothesis, the extremely vast and complex range of candidate theories, and the requirements of enormous amounts of computational resources. Data-driven discovery particularly suffers from these impediments. It is a long-standing aspiration to reduce the impediments and accelerate the rate of scientific progress [36]. With the recent advances in AI, especially in AI agents based on LLMs [9, 37, 22, 24, 10], the aspiration is more likely to become a reality [36].\nIn this paper, we serve as the first effort to Automate Data-centric Development (AD2) [45] with the aid of LLM-based agents, propelling the data-driven discovery paradigm forward. Specifically, we take the first step toward tackling a representative yet unexplored real-world scenario of automatic data-centric development, where agents are expected to substitute human researchers to first schedule to prioritize candidate methods (solutions) due to the enormous number of candidates and the limited computational resources, then prepare the appropriate engineering implementation of the prioritized methods, and finally execute their implementations to obtain accurate results. Each candidate method in AD\u00b2 is a data development task, such as feature extraction. A typical example of this is the implementation of financial factors, a domain where numerous researchers publish their research finding about developing advanced financial datasets [17]. The common AD2 scenario is shown in Figure 1. The distinctive challenges of AD2 and our corresponding contributions are as follows.\nThe first challenge is that AD\u00b2 requires agents to be more efficient by scheduling to prioritize the vast array of candidate methods to gain maximum value (i.e., realizing more valuable methods) within the given limitation of supporting resources. For example, candidate methods, including hundreds of thousands of chip designs, compounds for medicine or materials, and model architectures are waiting for validation from human experts [4, 23, 44, 19]. The experimental results of certain candidates will provide crucial information that supports the implementation of others: CoT [37] inspired other methods such as ReAct [42] and Tree-of-Thought [41]. Therefore, an effective schedule can streamline the implementation process, ensuring the successful delivery of more implementations. Furthermore, a significant discovery can effectively guide the research agenda, enabling better prioritization of tasks that yield more significant results. Previous work [27, 22, 31, 43] focuses on implementation while rarely considering scheduling them. We argue that an effective AD\u00b2 agent can 1) schedule to accomplish more implementation tasks within limited resources 2) and evolve through practical feedback like human experts.\nThe second challenge lies in the fact that AD2 tasks are fundamentally research implementation tasks, which are significantly more complex than standard coding tasks. 1) These tasks demand that agents are equipped with a higher level of domain-specific knowledge, which in turn bolsters their implementation capabilities. Coding agents based on LLM typically incorporate general coding datasets during training. However, these datasets often lack the specific knowledge required for AD2 tasks, resulting in subpar performance in these complex tasks. Previous works boost the general implementation ability of LLM agents by adopting code-specific pretraining, self-correction, and planning strategies [40, 39, 12, 38, 20]. However, they overlook the importance of enhancing the domain-specific knowledge of LLM agents, which is typically acquired through practical experience in specific scenarios. For example, a programmer may neglect the unique characteristics when he starts in a new domain-specific scenario. With repeated practice, he gains expertise and is able to deliver effective solutions. 2) On the other hand, simply incorporating knowledge through demonstrations in the learning context may not suffice for complex AD2 tasks. Much like real-world development, developers often solve a series of problems through multiple iterations before delivering a solution. Finding a similar demonstration that aligns with the current series of problems can be challenging. This presents a challenge in effectively transferring previously acquired knowledge"}, {"title": "2 Related Work", "content": "LLM-based agents refer to the software agents that leverage the capabilities of LLMs to perform a wide range of tasks. Our method, which focuses on the data-centric development problem, belongs to the big family of LLM-based agents. In the following, we introduce the mainstream techniques frequently used by LLM-based agents, and the differences between those techniques and our method.\nPlanning. Planning is critical to achieving predefined goals. CoT [37] proposes to decouple a task into several steps and make LLM follow these steps by prompting. Subsequent methods further automate the process of decoupling a task [48], integrate the actions into reasoning [42] and introduce multiple reasoning paths [41]. These studies focus on decomposing a task into steps. Different from them, we put effort into prioritizing candidate steps, which is a critical part of planning but receives little attention.\nSelf-correction. Self-correction aims at improving an agent's performance by assessing the quality of LLM-generated results and giving feedback to LLM. Feedback could be one-time [30, 8] or in an iterative manner [29, 20, 12], and it could come from LLMs [40, 2] or existing knowledge [26, 25]. Our method leverages the main idea of self-correction (i.e., using feedback), but is significantly different from existing work. Specifically, we accumulate practical experience from real experiments to build a knowledge base and leverage the knowledge base to give feedback, while existing studies"}, {"title": "2.1 Agent Workflows", "content": "to current, complex tasks. We argue that a qualified AD\u00b2 agent should 1) continuously learn domain knowledge from practice and 2) then effectively use it.\nThe potential ideal agents, capable of tackling the two challenges, share a common philosophy: evolving through practice. Specifically, accumulating a knowledge base through practice strengthens both the scheduling and implementation abilities of agents. Meanwhile, a proficient scheduling agent enhances its scheduling strategies by integrating valuable feedback from practical implementations. Likewise, an implementation agent accelerates its evolution by leveraging sensible schedules. Thus, an effective AD2 solution should progress through cooperative evolution."}, {"title": "2.2 Agents in Related Scenarios", "content": "Recently, various LLM-based agents have been developed to solve problems in different scenarios. In the following, we discuss the relationships of those agents to our proposed method.\nScientific research. Scientific research is a complex process consisting of two key components: the formulation of a new research idea and the development of the proposed idea. A recent work [5] shows the potential of LLM in scientific research by case studies with human evaluation. A subsequent work [3] presents a more systematic way to leverage LLM for idea formulation. In this work, we delve into another crucial component of scientific research which has not been well studied yet, i.e., the development problem, especially the data-centric one.\nMachine learning. Recently, there has been a growing interest in designing an agent to automate machine learning. CAAFE [14] focuses on the feature engineering problem, MLCopilot [46] investigates hyper-parameter tuning, and MLAgentBench [15] introduces 13 machine learning tasks and compares agents based on different LLMs. In this work, we focus on a different task compared to machine learning, i.e., data-centric development, which faces distinct challenges. Specifically, the key challenges of our task lies in prioritizing candidate methods and preparing engineering implementation, while the agents for machine learning tackle the challenge of automating specific components of the machine learning pipeline, e.g., feature engineering, hyper-parameter tuning and architecture search.\nSoftware development. To assist software development, LLM-based coding assistants, e.g., GitHub Copilot [1], have advanced into integrated development environments (IDEs). Besides, AutoDev [34] develops more powerful agents which can perform diverse operations on a codebase, including file editing, build processes, execution, testing, and git operations. Unlike traditional software development, our focus is on data-driven development in the context of scientific research. Specifically, the challenge of our task includes both prioritization and implementation while software development only considers implementation. Moreover, even for implementation, an evolving knowledge base is indispensable in our method, while its role in the software development agent is not clearly discussed."}, {"title": "Tool learning", "content": "Tool learning refers to the process by which an LLM learns to use external tools or resources to enhance its problem-solving capabilities. Recent efforts focus on teaching LLMs to be proficient with existing tools either by fine-tuning [28, 31, 18] or tuning-free methods [43, 35, 47, 11]. Differently, we consider a more advanced way of tool learning. First, instead of simply calling APIs from existing tools, we utilize and combine given tools to build a new tool. Second, we leverage the knowledge base, especially the accumulated practical experience in it, to help tool learning."}, {"title": "3 Co-STEER Agent", "content": "To address these challenges, we propose an LLM-based automatic developing agent, equipped with a strategy named Collaborative Knowledge-STudying-Enhanced Evolution by Retrieval (Co-STEER). Co-STEER simulates the career progression of an engineer from junior to senior level: The abilities of agents evolve through their own knowledge. Specifically, Co-STEER offers a co-evolving solution through a scheduling agent and an implementation agent. The scheduling agent evolves by considering the dependencies of candidate methods and learning from implementation feedback, thereby creating more effective schedules. The implementation agent, on the other hand, evolves through practical experience, developing a transferrable knowledge base for complex tasks such as AD2. As these agents evolve, they share feedback and status updates with each other, fostering collaborative evolution. To highlight our innovative approach, we have compared our method, Co-STEER, with previous works in the field of natural-language-to-code. The comparison is presented in Table 1."}, {"title": "3.1 Problem Formulation", "content": "To get a clear understanding of the problem we are focusing on, as shown in Figure 1, we'll start with a formal definition in this section. Given raw textual information comprising the descriptions of N candidate tasks for implementation (e.g., some methods to be implemented & verified), the goal of an AD\u00b2 agent is to deliver as many completed implemented tasks as possible. Due to AD2 tasks being fundamentally research implementation tasks, which are usually novel and usually can't be achieved by calling existing tools, the task has to be implemented by creating a solution through coding. Thus, the outcome of implementation would be code. The evaluation will be based on the execution results of the code, comparing results implemented by agents with the ground truth. We will have a quality score for the solution. The more completed tasks, the higher the quality of the completed tasks, and the more value the AD\u00b2 agent can achieve. But the supporting resources are limited, like the development in the real world. In our scenario, the agent has only a limited number of trials. The goal of the system is to gain maximum value within the given limitations of supporting resources.\nEach agent can have multiple rounds of trials to implement tasks. The knowledge gained from practice will be very important to minimize the cost of implementing a task. For example, after gaining"}, {"title": "3.2 Overall Design", "content": "Our proposed innovative approach, Co-STEER, presents a pioneering solution to address the problem outlined above. Illustrated in Figure 2, the design integrates two core components: a scheduling agent and an implementation agent, each aiming to handle the challenge of scheduling and implementation, respectively.\nThe novelty of Co-STEER lies in its evolution through practice, which is crucial for achieving high performance in AD2 tasks. It features a scheduling agent that refines its task schedules iteratively, drawing on feedback from practical implementations. Meanwhile, the implementation agent enhances its capabilities through ongoing practice, creating a knowledge base that can be transferred across various tasks. The two agents, rather than operating in isolation, evolve collaboratively. The scheduling agent refines its task schedules by incorporating feedback from the implementation phase, leading to a more efficient schedule. Concurrently, the implementation agent evolves more smoothly under the optimized schedules provided by the scheduling agent, fostering a mutually beneficial growth dynamic."}, {"title": "3.3 Scheduling Agent", "content": "We'll introduce the design of the scheduling agent shown on the left in Figure 2. The scheduling agent plays a crucial role in determining the sequence of task attempts, thereby influencing the system's evolution. On one hand, experimenting with diverse ideas not only garners valuable practical knowledge but also aids in the efficient completion of subsequent tasks, reducing overall costs. On the other hand, acquiring practical feedback enables the scheduling agent to deepen its comprehension of the task's essence, facilitating the formulation of a more effective schedule."}, {"title": "3.4 Implementation Agent", "content": "The implementation agent is on the right in Figure 2. A bad implementation agent needs more trials before getting a correct solution, which incurs greater cost. Previous works boost the general implementation ability with a series of tricks, which are listed in Table 1. Our agent incorporates these widespread capabilities as well. However, the AD\u00b2 tasks it tackles are fundamentally research-oriented implementation tasks, far surpassing the complexity of standard coding tasks. To effectively address these tasks, domain-specific knowledge is indispensable for devising viable solutions. Consequently, Co-STEER has been equipped with a growing practical knowledge base. This knowledge base is designed to continuously gather insights from practical experiences and transfer them to facilitate other tasks.\nIn the following section, we delve into the innovative aspects of this design. The design of Co-STEER's growing practical knowledge mainly focuses on the knowledge base and feedback."}, {"title": "3.4.1 Knowledge Base Design", "content": "The design of Co-STEER's knowledge base aims to build a knowledge vault that can grow and transfer.\nGrowing Practical Knowledge. Practical knowledge represents the knowledge collected from practice by an agent in a specific domain. It aims to enhance the agent's capabilities in domain-specific tasks. The design of the growing practical knowledge base is illustrated in the green box in Figure 2. This knowledge base archives all successfully completed tasks. Each entry in the database not only records the final outcome but also documents the iterative process of trials and feedback encountered along the way. This detailed trace offers a wealth of information, highlighting various potential errors and the corresponding solutions to address them. This approach records a comprehensive practical problem-solving path, far beyond just knowing the successful solution.\nTransferable Knowledge Usage. In addressing new tasks, incorporating examples of similar tasks into the prompt for in-context learning is a widely used strategy to apply past knowledge. However, as the complexity of tasks, such as those in AD2, increases, finding relevant examples becomes increasingly difficult. This limitation hinders the effectiveness of this common approach, making it challenging to transfer previously acquired knowledge to new, complex tasks. Co-STEER introduces an innovative solution for complex tasks like AD2. As depicted on the right in Figure 2, Co-STEER adopts a unique approach when encountering new tasks and errors (feedback). Instead of relying on task similarity, Co-STEER searches solutions based on feedback similarity within the practical knowledge base. The querying result is a list of steps to fix the current error. This detailed"}, {"title": "3.4.2 Feedback Design", "content": "Feedback is essential for guiding agents towards the correct solutions. It plays a pivotal role in enabling agents to learn practical knowledge effectively. Therefore, it's imperative to design feedback that not only points agents in the right direction but also boosts their speed of evolution by learning from more informative practical knowledge. Our evaluators are divided into two categories: the first operates autonomously, leveraging internal feedback mechanisms. The second category requires ground truths crafted by human experts, providing more insightful feedback by comparing the current trial and the ground truth.\nUnsupervised Feedbacks. Unsupervised feedback is divided into two types: LLM-based and tool-based. LLM-based feedback, generated by LLM, evaluates the current solution and task, drawing insights similar to those in Reflexion [32]. On the other hand, tool-based feedback utilizes a Python compiler to run the proposed solutions, providing practical feedback on their execution.\nSupervised Feedbacks. In the presence of expert-crafted solutions, agents can enhance their learning by comparing their solutions and execution outcomes to these ground truths, thereby deriving deeper insights. The comparison process involves two key steps: Firstly, for contrasting different solutions, LLMs are employed to succinctly articulate the distinctions between solutions generated by the agent and those crafted by experts. This step aids in identifying areas for improvement and refinement in the agent's solution. Secondly, when evaluating execution results, feedback is grounded in quantitative measures such as the correlation coefficient and value accuracy, alongside other metrics. This quantitative feedback provides insights from various perspectives, enriching the information available for improvement.\nIn the evolving process of the Co-STEER, it begins with an initial phase that leverages a small-sized knowledge base filled with expert-crafted solutions, ensuring the generation of high-quality feedback. This foundational knowledge base serves as a springboard for a warm start. As it progresses to address new tasks, the agent iteratively refines its approach through unsupervised feedback, continuously evolving and enhancing its implementation capabilities."}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to answer the following research questions.\n\u2022 RQ1: Does the implementation agent of Co-STEER outperform the previous (SOTA) natural-language-to-code baselines in AD\u00b2 tasks?\n\u2022 RQ2: Does the scheduling agent of Co-STEER make a reasonable schedule to boost the performance further?"}, {"title": "4.1 Datasets", "content": "To verify the effectiveness of our method on AD2 tasks, we conduct experiments in a typical AD2 scenario: financial factor implementation. This domain is where numerous researchers publish their findings on developing advanced financial datasets [17]. Daily financial research and development work involves scheduling and implementing factors across a vast array of candidate methods. To represent a real-world AD\u00b2 scenario, we conduct experiments on RD2Bench [7], a benchmark consisting of 27 human-annotated implementable factors and 13 mistakenly extracted unimplementable factors. All factors are divided into three categories: fundamental, high-frequency, and price-volume factors. The difficulty of factors is categorized into three levels: easy, medium, and hard. The implementation of each category of factors requires different sources of data, which are provided in the benchmark."}, {"title": "4.2 Experimental Settings", "content": "Baselines We include the following baselines in our experiments: Few-shot [6], CoT [37], Reflexion [32], Self-Debugging [8], and Self-Planning [16]. Few-shot learning is a context learning method that enhances the model's response formulation by providing several task-relevant examples and their answers. The Chain-of-Thought (CoT) model promotes logical progression in reasoning by necessitating step-by-step thinking. The Reflexion model, capable of introspection, identifies and corrects its own mistakes, thereby improving over time. The Self-Debugging model, through code analysis and feedback interpretation, can autonomously rectify errors. Lastly, the Self-Planning model demonstrates high autonomy by formulating its own action plan and making decisions or executing actions based on this plan.\nEvaluation Metrics We include four evaluation metrics to evaluate the performance of the implementation agent: average execution, average format, average correlation, and maximum correlation. The average execution metric measures the average execution rate of the generated code; any error encountered during the execution will be counted as 0. The average format metric measures the average format correctness of the generated code, for example whether the column name is equalized as expected. The average correlation indicates the average correlation between the output series generated by the code generated by the model and the ground truth. For example, for the same input features, we evaluate the correlations of factors both produced by LLM's implementations. and the ground truth implementations. The maximum correlation indicates the maximum correlation between the output series generated by the code generated by the model and the ground truth."}, {"title": "4.3 Results of Method Implementation (RQ1)", "content": "We compare the implementation ability of our proposed Co-STEER with the baseline agent workflows. The experimental results are shown in Table 2. We observe that our proposed Co-STEER significantly outperforms the baseline models on all four evaluation metrics across 27 test cases, demonstrating the overall effectiveness of Co-STEER. We attribute the significant improvement in implementation ability to both the dynamically expanded knowledge and the retrieval mechanism of the Co-STEER agent. Specifically, Table 1 showcases that, similar to Co-STEER, both Reflexion and Self-Debugging adopt the feedback from the environment to enhance the implementation ability of models. The difference is that Co-STEER accumulates practical knowledge through environmental feedback and retrieves its experience (knowledge) according to the current situation, while the other two agents merely consider the feedback of the current implementation and neglect their experience. Intuitively, Co-STEER continuously learns domain knowledge from practice and then effectively uses it, which bridges the gap between a junior and senior engineer, thus leading to significant performance gain. We refer the readers to the appendix for more details."}, {"title": "4.4 Overall Results of AD\u00b2 (RQ2)", "content": "We study the overall performance of Co-STEER and baselines in the AD2 scenario: Given the limited number of attempts (representing the real-world limited computational resources) and 40 candidate methods consisting of 27 implementable methods and 13 vague and unimplementable methods, models are expected to achieve the optimal overall performance by the collaborative evolving between their scheduling and implementation abilities. The experimental results are shown in Table 3. We obtain the following findings according to the table.\nThe evolving scheduler of Co-STEER effectively contributes to its overall performance on AD\u00b2. We observe that the performance of Co-STEER equipped with an evolving scheduler significantly outperforms that of Co-STEER equipped with a random scheduler in the same experimental settings. The results demonstrate that 1) our scheduler exerts a significant positive effect on the performance of Co-STEER, and 2) the order of method implementation plays an important role in achieving a better overall performance. Intuitively, accumulating different kinds of practical knowledge forms different engineering insights, thus affecting the performance of agents within a certain implementation order. The agent learns from practical experience and \u201cknows\u201d 1) which method is hard to implement and 2) whether the implementation of a method will facilitate the implementation of another method.\nLarger computational resources unlock more potential of evolving-based methods. We observe from Table 3 that the performance of the Co-STEER agent improves when more computational resources are available, regardless of the scheduler. The experimental results demonstrate the advantage of our proposed evolving strategy and our constructed knowledge. Specifically, computational resources do not affect the performance of baseline self-correction models, since they always focus on the feedback obtained in the current attempt without knowledge accumulation and retrieval. In contrast, given more computational resources, Co-STEER accumulates more practical knowledge and retrieves it when needed, thus forming its evolutionary characteristic. Since the quality of the scheduler affects the practical experience (an easy or tough beginning) of the agent, both the success and failure experiences contribute to the evolution of Co-STEER: Co-STEER evolves through practice (more computational resources) with both random and evolving schedulers."}, {"title": "4.5 Visualization of the Evolution of Co-STEER", "content": "We visualize the evolution of the implementation ability of Co-STEER. As shown in Figure 3, the implementation ability of Co-STEER has improved compared with its former state in each evolving step. With the proceeding of evolution, Co-STEER gradually approaches its performance boundary. The boundary is decided by the inner intelligence level of the foundation models (e.g., GPT-4) adopted by the current agent."}, {"title": "5 Conclusion", "content": "In this research, we highlight a critical yet overlooked scenario, AD\u00b2, which holds the potential to significantly advance AI research. Through a detailed examination of this scenario and its associated challenges in method scheduling and implementation, we introduce a groundbreaking autonomous agent, Co-STEER. This agent leverages the power of LLMs and is designed to tackle these challenges. Specifically, it enhances the agent's domain knowledge via a co-evolving strategy and bolsters its scheduling and implementation skills through the accumulation and retrieval of domain-specific experience. Our extensive experiments showcase that the Co-STEER agent, equipped with an evolvable capability of scheduling and implementation, can significantly outperform previous SOTA methods."}, {"title": "A Broader Impacts", "content": "The broader impacts of Co-STEER include accelerating innovation by allowing developers to focus on more creative tasks, economic implications such as potential workforce reductions due to automation, and the need for updated educational programs to equip future workers with relevant skills. Ethical considerations must also be addressed to ensure that such automated systems do not perpetuate biases, especially in sensitive decision-making processes."}, {"title": "B Limitation", "content": "The proposed Co-STEER agent showcases significant advancements in automating and optimizing research and development tasks, particularly in financial factor implementation, potentially enhancing R&D efficiency across various industries. However, its effectiveness heavily relies on the availability of high-quality data and substantial computational resources, which may limit its applicability in resource-constrained environments or other domains without extensive customization. However, such a limitation can be solved in the future as we are having more and more lower price for tokens from LLM."}, {"title": "C LLM Prompt Design", "content": ""}, {"title": "C.1 Scheduling Agent Prompt", "content": ""}, {"title": "C.2 Scheduling Agent Response", "content": ""}, {"title": "C.3 Latest attempt with corresponding feedback", "content": ""}, {"title": "C.4 Retrieved similar correct implementation and error", "content": ""}, {"title": "D Experimental Results", "content": ""}]}