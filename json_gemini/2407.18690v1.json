{"title": "Collaborative Evolving Strategy for Automatic Data-Centric Development", "authors": ["Xu Yang", "Haotian Chen", "Wenjun Feng", "Haoxue Wang", "Zeqi Ye", "Xinjie Shen", "Xiao Yang", "Shizhao Sun", "Weiqing Liu", "Jiang Bian"], "abstract": "Artificial Intelligence (AI) significantly influences many fields, largely thanks to\nthe vast amounts of high-quality data for machine learning models. The emphasis\nis now on a data-centric AI strategy, prioritizing data development over model\ndesign progress. Automating this process is crucial. In this paper, we serve as\nthe first work to introduce the automatic data-centric development (AD\u00b2) task and\noutline its core challenges, which require domain-experts-like task scheduling and\nimplementation capability, largely unexplored by previous work. By leveraging the\nstrong complex problem-solving capabilities of large language models (LLMs), we\npropose an LLM-based autonomous agent, equipped with a strategy named Collab-\norative Knowledge-STudying-Enhanced Evolution by Retrieval (Co-STEER), to\nsimultaneously address all the challenges. Specifically, our proposed Co-STEER\nagent enriches its domain knowledge through our proposed evolving strategy and\ndevelops both its scheduling and implementation skills by accumulating and re-\ntrieving domain-specific practical experience. With an improved schedule, the\ncapability for implementation accelerates. Simultaneously, as implementation\nfeedback becomes more thorough, the scheduling accuracy increases. These two\ncapabilities evolve together through practical feedback, enabling a collaborative\nevolution process. Extensive experimental results demonstrate that our Co-STEER\nagent breaks new ground in AD\u00b2 research, possesses strong evolvable schedule\nand implementation ability, and demonstrates the significant effectiveness of its\ncomponents. Our Co-STEER paves the way for AD2 advancements.", "sections": [{"title": "Introduction", "content": "Scientific advances have proceeded via a combination of different paradigms [13, 33], where data-driven discovery is an emerging paradigm formalized in recent years and more significantly accelerates scientific advances [13]. All the paradigms experience impediments to progress, including the expensive time cost of verifying a scientific hypothesis, the extremely vast and complex range of candidate theories, and the requirements of enormous amounts of computational resources. Data-driven discovery particularly suffers from these impediments. It is a long-standing aspiration to reduce the impediments and accelerate the rate of scientific progress [36]. With the recent advances in AI, especially in AI agents based on LLMs [9, 37, 22, 24, 10], the aspiration is more likely to become a reality [36].\nIn this paper, we serve as the first effort to Automate Data-centric Development (AD2) [45] with the\naid of LLM-based agents, propelling the data-driven discovery paradigm forward. Specifically, we\ntake the first step toward tackling a representative yet unexplored real-world scenario of automatic\ndata-centric development, where agents are expected to substitute human researchers to first schedule\nto prioritize candidate methods (solutions) due to the enormous number of candidates and the limited\ncomputational resources, then prepare the appropriate engineering implementation of the prioritized\nmethods, and finally execute their implementations to obtain accurate results. Each candidate method\nin AD\u00b2 is a data development task, such as feature extraction. A typical example of this is the\nimplementation of financial factors, a domain where numerous researchers publish their research\nfinding about developing advanced financial datasets [17]. The common AD2 scenario is shown in\nFigure 1. The distinctive challenges of AD2 and our corresponding contributions are as follows."}, {"title": "Related Work", "content": "LLM-based agents refer to the software agents that leverage the capabilities of LLMs to perform a wide range of tasks. Our method, which focuses on the data-centric development problem, belongs to the big family of LLM-based agents. In the following, we introduce the mainstream techniques frequently used by LLM-based agents, and the differences between those techniques and our method.\nPlanning. Planning is critical to achieving predefined goals. CoT [37] proposes to decouple a task into several steps and make LLM follow these steps by prompting. Subsequent methods further automate the process of decoupling a task [48], integrate the actions into reasoning [42] and introduce multiple reasoning paths [41]. These studies focus on decomposing a task into steps. Different from them, we put effort into prioritizing candidate steps, which is a critical part of planning but receives little attention.\nSelf-correction. Self-correction aims at improving an agent's performance by assessing the quality of LLM-generated results and giving feedback to LLM. Feedback could be one-time [30, 8] or in an iterative manner [29, 20, 12], and it could come from LLMs [40, 2] or existing knowledge [26, 25]. Our method leverages the main idea of self-correction (i.e., using feedback), but is significantly different from existing work. Specifically, we accumulate practical experience from real experiments to build a knowledge base and leverage the knowledge base to give feedback, while existing studies"}, {"title": "Co-STEER Agent", "content": "To get a clear understanding of the problem we are focusing on, as shown in Figure 1, we'll start with a formal definition in this section. Given raw textual information comprising the descriptions of N candidate tasks for implementation (e.g., some methods to be implemented & verified), the goal of an AD\u00b2 agent is to deliver as many completed implemented tasks as possible. Due to AD2 tasks being fundamentally research implementation tasks, which are usually novel and usually can't be achieved by calling existing tools, the task has to be implemented by creating a solution through coding. Thus, the outcome of implementation would be code. The evaluation will be based on the execution results of the code, comparing results implemented by agents with the ground truth. We will have a quality score for the solution. The more completed tasks, the higher the quality of the completed tasks, and the more value the AD\u00b2 agent can achieve. But the supporting resources are limited, like the development in the real world. In our scenario, the agent has only a limited number of trials. The goal of the system is to gain maximum value within the given limitations of supporting resources.\nEach agent can have multiple rounds of trials to implement tasks. The knowledge gained from practice will be very important to minimize the cost of implementing a task. For example, after gaining"}, {"title": "Overall Design", "content": "Our proposed innovative approach, Co-STEER, presents a pioneering solution to address the problem outlined above. Illustrated in Figure 2, the design integrates two core components: a scheduling agent and an implementation agent, each aiming to handle the challenge of scheduling and implementation, respectively.\nThe novelty of Co-STEER lies in its evolution through practice, which is crucial for achieving high performance in AD2 tasks. It features a scheduling agent that refines its task schedules iteratively, drawing on feedback from practical implementations. Meanwhile, the implementation agent enhances its capabilities through ongoing practice, creating a knowledge base that can be transferred across various tasks. The two agents, rather than operating in isolation, evolve collaboratively. The scheduling agent refines its task schedules by incorporating feedback from the implementation phase, leading to a more efficient schedule. Concurrently, the implementation agent evolves more smoothly under the optimized schedules provided by the scheduling agent, fostering a mutually beneficial growth dynamic."}, {"title": "Scheduling Agent", "content": "We'll introduce the design of the scheduling agent shown on the left in Figure 2. The scheduling agent plays a crucial role in determining the sequence of task attempts, thereby influencing the system's evolution. On one hand, experimenting with diverse ideas not only garners valuable practical knowledge but also aids in the efficient completion of subsequent tasks, reducing overall costs. On the other hand, acquiring practical feedback enables the scheduling agent to deepen its comprehension of the task's essence, facilitating the formulation of a more effective schedule."}, {"title": "Implementation Agent", "content": "The implementation agent is on the right in Figure 2. A bad implementation agent needs more trials before getting a correct solution, which incurs greater cost. Previous works boost the general implementation ability with a series of tricks, which are listed in Table 1. Our agent incorporates these widespread capabilities as well. However, the AD\u00b2 tasks it tackles are fundamentally research-oriented implementation tasks, far surpassing the complexity of standard coding tasks. To effectively address these tasks, domain-specific knowledge is indispensable for devising viable solutions. Consequently, Co-STEER has been equipped with a growing practical knowledge base. This knowledge base is designed to continuously gather insights from practical experiences and transfer them to facilitate other tasks.\nIn the following section, we delve into the innovative aspects of this design. The design of Co-STEER's growing practical knowledge mainly focuses on the knowledge base and feedback."}, {"title": "Knowledge Base Design", "content": "The design of Co-STEER's knowledge base aims to build a knowledge vault that can grow and transfer.\nGrowing Practical Knowledge. Practical knowledge represents the knowledge collected from practice by an agent in a specific domain. It aims to enhance the agent's capabilities in domain-specific tasks. The design of the growing practical knowledge base is illustrated in the green box in Figure 2. This knowledge base archives all successfully completed tasks. Each entry in the database not only records the final outcome but also documents the iterative process of trials and feedback encountered along the way. This detailed trace offers a wealth of information, highlighting various potential errors and the corresponding solutions to address them. This approach records a comprehensive practical problem-solving path, far beyond just knowing the successful solution.\nTransferable Knowledge Usage. In addressing new tasks, incorporating examples of similar tasks into the prompt for in-context learning is a widely used strategy to apply past knowledge. However, as the complexity of tasks, such as those in AD2, increases, finding relevant examples becomes increasingly difficult. This limitation hinders the effectiveness of this common approach, making it challenging to transfer previously acquired knowledge to new, complex tasks. Co-STEER introduces an innovative solution for complex tasks like AD2. As depicted on the right in Figure 2, Co-STEER adopts a unique approach when encountering new tasks and errors (feedback). Instead of relying on task similarity, Co-STEER searches solutions based on feedback similarity within the practical knowledge base. The querying result is a list of steps to fix the current error. This detailed"}, {"title": "Feedback Design", "content": "Feedback is essential for guiding agents towards the correct solutions. It plays a pivotal role in enabling agents to learn practical knowledge effectively. Therefore, it's imperative to design feedback that not only points agents in the right direction but also boosts their speed of evolution by learning from more informative practical knowledge. Our evaluators are divided into two categories: the first operates autonomously, leveraging internal feedback mechanisms. The second category requires ground truths crafted by human experts, providing more insightful feedback by comparing the current trial and the ground truth.\nUnsupervised Feedbacks. Unsupervised feedback is divided into two types: LLM-based and tool-based. LLM-based feedback, generated by LLM, evaluates the current solution and task, drawing insights similar to those in Reflexion [32]. On the other hand, tool-based feedback utilizes a Python compiler to run the proposed solutions, providing practical feedback on their execution.\nSupervised Feedbacks. In the presence of expert-crafted solutions, agents can enhance their learning by comparing their solutions and execution outcomes to these ground truths, thereby deriving deeper insights. The comparison process involves two key steps: Firstly, for contrasting different solutions, LLMs are employed to succinctly articulate the distinctions between solutions generated by the agent and those crafted by experts. This step aids in identifying areas for improvement and refinement in the agent's solution. Secondly, when evaluating execution results, feedback is grounded in quantitative measures such as the correlation coefficient and value accuracy, alongside other metrics. This quantitative feedback provides insights from various perspectives, enriching the information available for improvement.\nIn the evolving process of the Co-STEER, it begins with an initial phase that leverages a small-sized knowledge base filled with expert-crafted solutions, ensuring the generation of high-quality feedback. This foundational knowledge base serves as a springboard for a warm start. As it progresses to address new tasks, the agent iteratively refines its approach through unsupervised feedback, continuously evolving and enhancing its implementation capabilities."}, {"title": "Experiments", "content": "In this section, we conduct experiments to answer the following research questions.\n\u2022 RQ1: Does the implementation agent of Co-STEER outperform the previous (SOTA) natural-language-to-code baselines in AD\u00b2 tasks?\n\u2022 RQ2: Does the scheduling agent of Co-STEER make a reasonable schedule to boost the performance further?"}, {"title": "Datasets", "content": "To verify the effectiveness of our method on AD2 tasks, we conduct experiments in a typical AD2 scenario: financial factor implementation. This domain is where numerous researchers publish their findings on developing advanced financial datasets [17]. Daily financial research and development work involves scheduling and implementing factors across a vast array of candidate methods. To represent a real-world AD\u00b2 scenario, we conduct experiments on RD2Bench [7], a benchmark consisting of 27 human-annotated implementable factors and 13 mistakenly extracted unimplementable factors. All factors are divided into three categories: fundamental, high-frequency, and price-volume factors. The difficulty of factors is categorized into three levels: easy, medium, and hard. The implementation of each category of factors requires different sources of data, which are provided in the benchmark."}, {"title": "Experimental Settings", "content": "Baselines We include the following baselines in our experiments: Few-shot [6], CoT [37], Reflexion [32], Self-Debugging [8], and Self-Planning [16]. Few-shot learning is a context learning method that enhances the model's response formulation by providing several task-relevant examples and their answers. The Chain-of-Thought (CoT) model promotes logical progression in reasoning by necessitating step-by-step thinking. The Reflexion model, capable of introspection, identifies and corrects its own mistakes, thereby improving over time. The Self-Debugging model, through code analysis and feedback interpretation, can autonomously rectify errors. Lastly, the Self-Planning model demonstrates high autonomy by formulating its own action plan and making decisions or executing actions based on this plan.\nEvaluation Metrics We include four evaluation metrics to evaluate the performance of the implementation agent: average execution, average format, average correlation, and maximum correlation. The average execution metric measures the average execution rate of the generated code; any error encountered during the execution will be counted as 0. The average format metric measures the average format correctness of the generated code, for example whether the column name is equalized as expected. The average correlation indicates the average correlation between the output series generated by the code generated by the model and the ground truth. For example, for the same input features, we evaluate the correlations of factors both produced by LLM's implementations. and the ground truth implementations. The maximum correlation indicates the maximum correlation between the output series generated by the code generated by the model and the ground truth."}, {"title": "Results of Method Implementation (RQ1)", "content": "We compare the implementation ability of our proposed Co-STEER with the baseline agent workflows. The experimental results are shown in Table 2. We observe that our proposed Co-STEER significantly outperforms the baseline models on all four evaluation metrics across 27 test cases, demonstrating the overall effectiveness of Co-STEER. We attribute the significant improvement in implementation ability to both the dynamically expanded knowledge and the retrieval mechanism of the Co-STEER agent. Specifically, Table 1 showcases that, similar to Co-STEER, both Reflexion and Self-Debugging adopt the feedback from the environment to enhance the implementation ability of models. The difference is that Co-STEER accumulates practical knowledge through environmental feedback and retrieves its experience (knowledge) according to the current situation, while the other two agents merely consider the feedback of the current implementation and neglect their experience. Intuitively, Co-STEER continuously learns domain knowledge from practice and then effectively uses it, which bridges the gap between a junior and senior engineer, thus leading to significant performance gain. We refer the readers to the appendix for more details."}, {"title": "Overall Results of AD\u00b2 (RQ2)", "content": "We study the overall performance of Co-STEER and baselines in the AD2 scenario: Given the limited number of attempts (representing the real-world limited computational resources) and 40 candidate methods consisting of 27 implementable methods and 13 vague and unimplementable methods, models are expected to achieve the optimal overall performance by the collaborative evolving between their scheduling and implementation abilities. The experimental results are shown in Table 3. We obtain the following findings according to the table.\nThe evolving scheduler of Co-STEER effectively contributes to its overall performance on AD\u00b2. We observe that the performance of Co-STEER equipped with an evolving scheduler significantly"}, {"title": "Visualization of the Evolution of Co-STEER", "content": "We visualize the evolution of the implementation ability of Co-STEER. As shown in Figure 3, the implementation ability of Co-STEER has improved compared with its former state in each evolving step. With the proceeding of evolution, Co-STEER gradually approaches its performance boundary. The boundary is decided by the inner intelligence level of the foundation models (e.g., GPT-4) adopted by the current agent."}, {"title": "Conclusion", "content": "In this research, we highlight a critical yet overlooked scenario, AD\u00b2, which holds the potential to significantly advance AI research. Through a detailed examination of this scenario and its associated challenges in method scheduling and implementation, we introduce a groundbreaking autonomous agent, Co-STEER. This agent leverages the power of LLMs and is designed to tackle these challenges. Specifically, it enhances the agent's domain knowledge via a co-evolving strategy and bolsters its scheduling and implementation skills through the accumulation and retrieval of domain-specific experience. Our extensive experiments showcase that the Co-STEER agent, equipped with an evolvable capability of scheduling and implementation, can significantly outperform previous SOTA methods."}, {"title": "Broader Impacts", "content": "The broader impacts of Co-STEER include accelerating innovation by allowing developers to focus on more creative tasks, economic implications such as potential workforce reductions due to automation, and the need for updated educational programs to equip future workers with relevant skills. Ethical considerations must also be addressed to ensure that such automated systems do not perpetuate biases, especially in sensitive decision-making processes."}, {"title": "Limitation", "content": "The proposed Co-STEER agent showcases significant advancements in automating and optimizing research and development tasks, particularly in financial factor implementation, potentially enhancing R&D efficiency across various industries. However, its effectiveness heavily relies on the availability of high-quality data and substantial computational resources, which may limit its applicability in resource-constrained environments or other domains without extensive customization. However, such a limitation can be solved in the future as we are having more and more lower price for tokens from LLM."}, {"title": "LLM Prompt Design", "content": null}, {"title": "Scheduling Agent Prompt", "content": null}, {"title": "Scheduling Agent Response", "content": null}, {"title": "Latest attempt with corresponding feedback", "content": null}, {"title": "Retrieved similar correct implementation and error", "content": null}]}