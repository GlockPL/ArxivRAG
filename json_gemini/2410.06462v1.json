{"title": "Hallucinating Al Hijacking Attack:\nLarge Language Models and Malicious Code Recommenders", "authors": ["David Noever", "Forrest McKee"], "abstract": "The research builds and evaluates the adversarial potential to introduce copied code or hallucinated AI\nrecommendations for malicious code in popular code repositories. While foundational large language\nmodels (LLMs) from OpenAI, Google, and Anthropic guard against both harmful behaviors and toxic\nstrings, previous work on math solutions that embed harmful prompts demonstrate that the guardrails may\ndiffer between expert contexts. These loopholes would appear in mixture of expert's models when the\ncontext of the question changes and may offer fewer malicious training examples to filter toxic comments\nor recommended offensive actions. The present work demonstrates that foundational models may refuse to\npropose destructive actions correctly when prompted overtly but may unfortunately drop their guard when\npresented with a sudden change of context, like solving a computer programming challenge. We show\nempirical examples with trojan-hosting repositories like GitHub, NPM, NuGet, and popular content\ndelivery networks (CDN) like jsDelivr which amplify the attack surface. In the LLM's directives to be\nhelpful, example recommendations propose application programming interface (API) endpoints which a\ndetermined domain-squatter could acquire and setup attack mobile infrastructure that triggers from the\nnaively copied code. We compare this attack to previous work on context-shifting and contrast the attack\nsurface as a novel version of \u201cliving off the land\u201d attacks in the malware literature. In the latter case,\nfoundational language models can hijack otherwise innocent user prompts to recommend actions that\nviolate their owners' safety policies when posed directly without the accompanying coding support request.", "sections": [{"title": "Introduction", "content": "Recent research in cybersecurity, artificial intelligence (AI), and software supply chain vulnerabilities has\nhighlighted the growing complexity and impact of attacks on digital systems and AI-based technologies.\nThe present work highlights novel dangers posed by automated programming interfaces or hybrid scenarios\nthat leverage the software supply chain, particularly in \u201ccopy-paste\u201d or rapid development sprints. Several\nstudies focus on the threat landscape within the supply chain domain, identifying the rising number of\nattacks targeting popular software packages and development environments (Andreoli et al. 2023).\n\nTo frame this challenging vulnerability, it is essential to understand the rapidly evolving nature of large\nlanguage models (LLMs) and the implications of mixture of experts (MoE) in scaling up their changing\ncontexts. Such shifts of user contexts can reveal behaviors in foundational models that are otherwise hidden,\nparticularly when switching between expert domains that unlock different, harmful, or unanticipated\ncapabilities. This transition highlights the current problem statement: as LLMs attempt to be universally\napplicable, do they expose vulnerabilities when context-specific guardrails are insufficiently trained or\ninadequately enforced, allowing unintended behaviors to emerge?\n\nWe focus on the coding assistant role and suggest novel attack frameworks for general assessment of LLM\nvulnerabilities to respond with more information than their traditional guardrails might suggest. A framing\nexample asks a LLM to deliver ransomware (which it refuses), then to embed a contextual cue that asks for\na public repository to deliver the same ransomware (which it accepts) and delivers code to magnify the\ndamage in a semi-automated update or vast digital supply chain endpoints like GitHub, NPM, NuGet, and\nfake or hallucinated example APIs and CDNs that a determined threat actor hijacks. Similarly, when asked\nto design a fake login page, the foundational models refuse this request as harmful behavior. But when\nasked the same question as part of a programming challenge in HTML, however, the LLM provides code\nin a test case to mimic the PayPal website."}, {"title": "Previous Work", "content": "Foundational LLM safety teams focus on four primary threats including cybersecurity (e.g. authoring zero-\nday attacks), biology (e.g. generating novel viruses or chemical agents), deception (e.g. manipulating\nhumans), and model autonomy (e.g., acquiring emergent or unintended skills). The cornerstone scenario\nof a rogue LLM involves an unintended consequence of surfing a vast programming repository like GitHub\nand learning some previously unknown but deceptive threat and magnifying it at scale to unassuming users\nwhile acting as a helpful code assistant. In this hypothetical case, the LLM is the bootloader to global\nmalware outbreaks. One may question the efficacy of current safeguards against such a red teaming scenario\nand LLM foundational models hosted by Open AI, Google, or Anthropic (along with fine-tuned small\nlanguage models trained to exploit these scenarios)."}, {"title": "Research Question", "content": "The current study examines the topical cybersecurity challenge in the software supply chain: what is the\nrisk of introducing copied or hallucinated recommendations for malicious code into popular code\nrepositories?\n\nFoundational models from major AI developers like OpenAI, Google, and Anthropic have implemented\nguardrails against harmful behaviors; however, these measures vary across different contexts, especially\nwhere the models serve as mixtures of experts (Masoudnia, et al. 2014). Previous research indicates that\nwhile these models may correctly filter direct prompts with toxic intent, the embedded contexts within\nseemingly innocuous programming challenges can bypass safety mechanisms. The present work seeks to\nunderstand if foundational models (that are broadly resistant to proposing destructive actions when\naddressed directly) can be compromised into advising harmful software practices. In other words, when a\nrisky suggestion gets framed within a technical challenge or programming scenario, do the LLMs provide\nresponses that drop their safety guard and suggest risky practices or reveal security weaknesses. This is\ntested empirically through examples involving trojan-hosting repositories such as GitHub, NPM, NuGet,\nand content delivery networks (CDN)."}, {"title": "Results and Discussion", "content": "Appendix A highlights the results across a range of programming recommendations that reference or direct\nthe user to apply malicious supply chain endpoints. These endpoints include major coding libraries known\nto be compromised along with non-library source list traditional blacklisted URLs. The specific scenarios\ndiscovered range from the suggestion of compromised API endpoints and hijacked RSS feeds to the\nrecommendation of malicious GitHub repositories and npm packages. The Appendix also demonstrates\nmore subtle attack methods, such as iframe-based attacks loading content from blacklisted domains and\nCDN-based attacks utilizing obfuscated malicious payloads in minified code. Language-specific package\nmanagers like Python's PIP, Ruby's bundler, and Rust's Cargo are also found to be potential injection points\nin the software supply chain for malicious library installations.\n\nThe implications of these dependencies (as vulnerable injection points) grow as the software development\nindustry increasingly relies on AI-assisted coding and recommendations. Not only do the foundational\nmodels violate their companies' own safety guards when given out-of-context requests, these supply chain\nattack vectors have already compromised multiple libraries, potentially affecting multiple applications and\noperating systems. The straightforward example of this lowered safety guard is the refusal of GPT-40 to\nassist in authoring a fake login page as unacceptable but proceed to build a PayPal phishing page when\nasked for HTML programming assistance.\n\nThe bulk of the demonstrations feature a supply chain injection where the LLM is simply exploitable as a\nrecommender system to known malicious libraries in its suggested code. One analogy to consider is whether\na search engine like Google should filter blacklisted websites in search results to save the na\u00efve user from\nclicking on them, but a helpful Al assistant can alternatively recommend software dependencies without\nany concern for its own blacklist safety requirements.\n\nAn innocent user placing their trust in LLM recommendations could be weaponized against developers,\nturning a tool meant to enhance productivity into a trojan horse for malware and data exfiltration. To realize\nthe latter case in the wild, the malicious creator of the library referenced in a popular LLM response would\nlikely have some prior use of typo-squatting domains from their known uses of typo-squatting library names\nlike \u201ccolourspaces\u201d vs. \u201ccolorspace\".\n\nA notable aspect of these findings is how the LLMs' directive to be helpful inadvertently supports potential\nthreat actors. For instance, the models may recommend application programming interface (API) endpoints\nthat a domain-squatter could exploit, setting up infrastructure that weaponizes the copied code. This\nsituation draws a parallel to \"living off the land\" attacks\u2014where benign elements are repurposed for\nmalicious intent\u2014by demonstrating how foundational language models can recommend actions violating\nsafety policies without explicitly dangerous prompts. This novel attack vector underlines the need for\nenhancing context-aware safety measures in LLMs, especially as the complexity and diversity of their\napplications continue to grow.\""}, {"title": "Survey of Previous Related Work", "content": "Our findings on LLMs' potential to recommend malicious resources in software development contexts build\nupon and extend existing research in AI security and software supply chain vulnerabilities. The ability of\nLLMs to suggest compromised API endpoints, RSS feeds, and GitHub repositories aligns with the software\nsupply chain attack concerns raised by Andreoli et al. (2023) and Mart\u00ednez and Dur\u00e1n (2021). Their analysis\nof the SolarWinds case demonstrates how trusted infrastructures can be exploited, a scenario our research\nsuggests could be unintentionally facilitated by LLMs in development environments.\n\nThe vulnerability of LLMs recommending malicious NPM packages relates to the frequent automated\nacceptance of library dependencies in active projects, as observed in JavaScript frameworks. This risk is\namplified by the minified and often obfuscated nature of NPM code, a practice noted by Hammi, Zeadally,\nand Nebhen (2023) in their overview of digital supply chain threats.\n\nOur exploration of iframe-based and CDN-based attacks facilitated by LLM recommendations extends the\nwork of Bethany et al. (2024) and Chowdhury et al. (2024) on LLM vulnerabilities. These attack vectors\nrepresent a new dimension in the challenges facing AI-assisted development, where the trust placed in AI\nassistants could be exploited to introduce vulnerabilities.\n\nThe observed ability of LLMs to bypass their own safety measures in programming contexts extends the\nresearch on LLM jailbreaking by Jiang et al. (2024), Xu et al. (2024), and Yong et al. (2023). Our findings\nsuggest that code generation contexts might serve as a novel form of jailbreaking, allowing LLMs to\nrecommend potentially harmful actions they would otherwise avoid. The \"hallucinations\" in LLM-\ngenerated code recommendations, particularly in suggesting non-existent or potentially malicious\nresources, align with the concerns raised by Liu et al. (2024) and Spracklen et al. (2024). These\nhallucinations represent a significant risk in AI-assisted programming, potentially introducing\nvulnerabilities that are difficult to detect through traditional code review processes. These results also relate\nto the work of Koutsokostas and Patsakis (2021) on developing stealth malware without obfuscation, and\nKarantzas and Patsakis (2021) on evaluating endpoint detection systems. The ability of LLMs to suggest\nseemingly innocuous code that could harbor malicious intent presents similar challenges to cybersecurity\nsystems and human code reviewers.\n\nThe potential for LLMs to facilitate \"living off the land\" style attacks, as implied by our findings, connects\nwith the work of Adobe's Security Intelligence team (2021) on classifying such techniques. Our research\nsuggests that LLMs could inadvertently become a vector for these types of attacks in software development\nworkflows, a concern also raised by Hartmann and Steup (2020) in their exploration of AI system hijacking.\n\nConsidering these connections, these novel attacks underscore the need for more robust security measures\nin Al-assisted programming. The work on red teaming strategies by Deng et al. (2023) and Thompson and\nSklar (2024) could be extended to address the vulnerabilities we've identified in code-generation contexts.\nFurthermore, the ALERT benchmark proposed by Tedeschi et al. (2024) could be adapted to include\nscenarios that test LLMs' ability to maintain security awareness in programming tasks. As the software\ncommunity continues to integrate AI into development processes, addressing these vulnerabilities will be\nimportant. The continued monitoring of malicious software packages, as detailed by Phylum (2024), further\nunderscores the importance of proactive security measures in the face of evolving threats in AI-assisted\nsoftware development."}, {"title": "Conclusions and Future Work", "content": "This research has collected potential vulnerabilities in the integration of large language models (LLMs) into\nsoftware development workflows. Our findings suggest that while LLMs from foundational providers like\nOpenAI, Google, and Anthropic have strong safeguards against overtly harmful behaviors, these protections\nmay be inadvertently bypassed in specific contexts, particularly when offering programming assistance.\n\nThe demonstrated ability to introduce potentially malicious code recommendations through context-shifting\nreveals a novel gap in current LLM safety measures. This vulnerability magnifies its importance in more\nautomated or hybrid workflows, which depend heavily on widespread use of code repositories like GitHub,\npackage managers such as NPM and NuGet, and content delivery networks like jsDelivr, all of which could\namplify the impact of such attacks.\n\nFuture work should focus on several key areas:\n\n1. Comprehensive evaluation of LLM behavior across diverse programming contexts to identify\npotential weak points in their safety mechanisms.\n2. Development of more sophisticated context-aware safeguards that maintain vigilance even when\nthe conversation topic shifts abruptly.\n3. Creation of tools and methodologies to detect and mitigate potential security risks in LLM-\ngenerated code recommendations.\n4. Investigation into the prevalence and impact of \"living off the land\" style attacks facilitated by\nLLM recommendations in real-world development environments.\n5. Exploration of methods to enhance LLM understanding of secure coding practices and the ability\nto recognize potentially malicious patterns in recommended resources or code snippets.\n\nIn conclusion, this research underscores the double-edged nature of AI assistance in programming. While\nLLMs offer potential to enhance developer productivity, they also introduce new attack vectors that must\nbe managed. The ability of these models to unwittingly recommend actions that violate their intended safety\npolicies when presented in the context of coding support requests is a new guardrail to support.\n\nAs we continue to integrate AI into software development processes, more work is needed to quantify the\nprevalence of these vulnerabilities in real-world scenarios and to develop effective mitigation strategies.\nGiven the complex and hidden nature of current foundational models, future efforts may involve enhancing\nthe security or \u201cblack-list\u201d awareness of LLMs, implementing more vetting processes for AI-recommended\nresources, and creating tools to detect potential security risks in LLM outputs. The findings also underscore\nthe importance of ongoing education for developers about the potential risks associated with copy-paste\ncycles with Al-assisted coding and the need for critical evaluation of AI-generated recommendations in\nmost hybrid programming models."}]}