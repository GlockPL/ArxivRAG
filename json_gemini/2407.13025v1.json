{"title": "From Principles to Practices: Lessons Learned from Applying Partnership on AI's (PAI) Synthetic Media Framework to 11 Use Cases", "authors": ["Claire R. Leibowicz", "Christian H. Cardona"], "abstract": "2023 was the year the world woke up to generative AI, and 2024 is the year policymakers are responding more firmly. In the past year, Taylor Swift fell victim to non-consensual deepfake pornography [3], and a misleading political narrative [22]. A global financial services firm lost $25 million [5] due to a deepfake scam. And politicians around the world [27] have seen their likenesses used to mislead in the lead up to elections. In the U.S., on the heels of a White House Executive Order [23], NIST will be \u201cidentifying the existing standards, tools, methods, and practices...for authenticating content and tracking its provenance, [and] labeling synthetic content.\u201d Importantly, this policy momentum is taking place alongside real world creation and distribution of synthetic media. Social media platforms, news organizations, dating apps, courts, image generation companies, and more are already navigating a world of AI-generated visuals and sounds, already changing hearts and minds, as policymakers try to catch up. How, then, can AI governance capture the complexity of the synthetic media landscape? How can it attend to synthetic media's myriad uses, ranging from storytelling to privacy preservation, to deception, fraud, and defamation, taking into account the many stakeholders involved in its development, creation, and distribution? And what might it mean to govern synthetic media in a manner that upholds the truth while bolstering freedom of expression? To spur innovation while reducing harm? What follows is the first known collection of diverse examples of the implementation of synthetic media governance that responds to these questions, specifically through Partnership on AI's (PAI) Responsible Practices for Synthetic Media [13] a voluntary, normative Framework for creating, distributing, and building technology for synthetic media responsibly, launched in February 2023. In this paper, we present a case bank of real world examples that help operationalize the Framework highlighting areas synthetic media governance can be applied, augmented, expanded, and refined for use, in practice.\nThese eleven stakeholders are a seemingly eclectic group; they vary along many axes implicating synthetic media governance. But they're all integral members of a synthetic media ecosystem that requires a blend of technical and humanistic might to benefit society. As Synthesia rightfully notes in their case, \u201cNo single stakeholder can enact system-level change without public-private collaboration.\u201d", "sections": [{"title": "Theme 1: Creative vs. Malicious Content", "content": "The debate [7] typically includes some suggesting that moderation by Builders or Creators would stifle innovation and expression, thereby putting too much power in the hands of a few institutions. However, others argue that failing to moderate at the model, technology development, and even infrastructure layer makes it harder to prevent harm downstream.\nOne of the most public examples of this debate took place far upstream from the institutions featured in this case, yet illustrates these tradeoffs: in 2019, the CEO of Cloudflare, an internet security company, reversed course and terminated 8chan [19], a media platform that allowed \u201cextremists to test out ideas, share violent literature, and cheer on the perpetrators of mass killings.\" In explaining his decision, and his conflictedness, Cloudflare's CEO mapped out the many institutions undergirding the Internet while questioning [16] how to balance freedom of expression with safety, and the roles they should play in doing so.\nBuilders, Creators, and policymakers, often face a similar conflict. In our cases, though, several Builder and Creator platforms engaged in normative content moderation (or training data decision making, which in essence affects content development) to support harm mitigation, despite the fact that they are not Distributors of content who are typically those assumed to be responsible [8] for moderating content and for whom much regulatory activity is focused. By doing so, they provide a degree of redundancy in content moderation systems later downstream, possibly minimizing the harmful content that eventually reaches audiences.\nFor instance, Synthesia, a Builder of synthetic media technology, has implemented detection and moderation capabilities at the point of creation. As they note, \u201cUntil recently, most content moderation has happened at the point of distribution: a user of digital creation tools could create content without any restrictions.\u201d As with all content moderation, there is inevitably ambiguity in content evaluations, and they differentiate between \u201cobviously harmful content,\u201d \u201cobviously harmless content,\" and \"gray zone\" content for which they provide a few examples. However, this moderation taking place before content gets to social media platforms can help support harm mitigation further downstream, though it should be pursued transparently in order to illuminate the often subjective decision-making that takes place when moderating gray area content.\nSynthesia describes choices they made about misleading videos about sexual health or cryptocurrency and how by thwarting their development, they provide meaningful support for eventual social media platform moderation processes that might need to filter out this harmful content. Given such a fast moving field, and the limits of moderation on social media platforms, this might support the actual reduction of harmful content's spread downstream (when done transparently)."}, {"title": "Theme 2: Transparency via Disclosure", "content": "uploading] a photo of themselves to their profile that has been digitally altered to show them in a location they've never been to before.\u201d Synthesia highlights the \u201cgray zone\" as part of their analysis, including examples of such content related to sexual health and cryptocurrency contexts.\nThe cases that explicitly describe gray areas, rather than overarchingly describing them as concepts, help the field understand tradeoffs and how decisions are being made at institutions that implicate the distribution and spread of speech. They have several benefits: they serve as a model for other institutions looking for guidance around exact or analogous scenarios, support broader openness by institutions in this sector, and help users and audiences navigate interactions with the institution in a more informed way.\nWhile it is difficult for institutions to build out a comprehensive set of all of the decisions they have made related to gray area cases, a best practice approach to sharing edge cases and tricky calls must be pursued to ensure that the field is adequately balancing creative expression and harm mitigation. And, of course, different institutions and individuals may have varied perspectives on the appropriate balance between these two considerations. Further, while we encouraged institutions to ground cases in real-world examples of these gray areas, to begin building up these more specific case resources, it will likely take more than this voluntary case study exercise to ensure they are shared at scale."}, {"title": "Theme 3: Consent", "content": "Consent proved challenging for many institutions across cases. While legal boundaries offer some guidance, responsible creation requires more than achieving the legal bare minimum around topics like intellectual property, and the Framework begins to provide this guidance. WITNESS suggested that consent is even more vital when real people are depicted, advocating for an amendment to the Framework that emphasizes the benefit of \u201cseeking consent when the likeness of real people is directly involved in the input or output of the AI-generation process.\" They go on to highlight that this should not be mandatory, since there are \u201csome circumstances in which consent may not be pertinent, feasible, or even needed.\"\nThe WITNESS case, alongside the D-ID case, dealt with creative projects including real people who could not provide consent either because they were no longer alive or had been kidnapped and both provide insight into how to navigate this scenario.\nD-ID, writing about a particularly sensitive context domestic violence talked to the nuclear family of the featured individual who was no longer alive. Of course, they first needed to deem the social impact goals of educating the public about domestic abuse via the project to be worth the potential emotional tumult of reaching out to families. They even went a step further to bolster consent, allowing the families to actively participate in \u201cco-creating the content and scripts\u201d for the development of the media. This takes informed and active consent not just about the sheer fact that a creator is using the likeness of their kin, but consent with how that likeness is being used to the next level."}, {"title": "PAI Reflections on the Case Exercise", "content": "The case studies in this collection offer the AI field greater transparency into synthetic media governance, highlighting how PAI's Responsible Practices for Synthetic Media [13] can be applied, augmented, expanded, and refined for use in practice. Here, we reflect briefly on several aspects of the governance process, including accountability, transparency, adaptability, and complexity.\nVoluntary frameworks for Al governance are often (understandably) critiqued [15] for providing a facade of rigor and lack of commitment. Many have written [26] on the attempts by technology companies in particular to tout voluntary governance that serves their interests in order to stave off government regulation. This is often true. At the same time, it has become clear through our years of work on synthetic media that in the absence of specific government regulation on synthetic media that can keep pace with the field's development, as well as appetite from stakeholders across sectors for guidance on synthetic media practices"}, {"title": "Case Guidance", "content": "These eleven examples provide a rich tapestry of the challenges and opportunities synthetic media governance presents. We were struck by the variety across cases. Some include specific artistic examples, while others focus on broad tradeoffs that implicate AI model development, or specific considerations of news organizations using synthetic media. While they cannot cover the entire surface area of synthetic media impacts, by providing a body of, in essence, case law for synthetic media, we offer the field a starting point for navigating their own synthetic media challenges. For example, if one is navigating a creative project that deals with posthumous consent, they can consult the D-ID or WITNESS cases. Notably, these cases required enormous effort and time across PAI staff and Framework supporters, and we are interested in developing methods for collecting cases and instances of synthetic media decision making that might not require long-form writing something akin to the AI Incident Database [2] that was created at PAI. Starting with the level of depth exemplified in the cases, though, provides a useful foundation for understanding the complexity of case examples featuring synthetic media challenges and opportunities, and also allows us to put them in context and dialogue with other actors in the synthetic media pipeline."}, {"title": "Framework Adaptability and Refinement", "content": "Another benefit of the case process was pointing out ways that the Framework can be augmented or adapted over time. A key principle of the Framework's launch was that, in direct response to the rapid pace of Al development, we would revise the Framework. Several details emerged throughout the case process that will inform future versions of the Framework, including but not limited to:\nProposing that Builders, Creators, and Distributors should enable and/or use more than one disclosure mechanism to offset shortcomings.\nIncluding a provision to highlight the need to develop standardized and interoperable solutions for disclosure.\nSuggesting clear guidance on how to label different creative types of synthetic media.\nOffering details on consent that can help address gray-area cases when dealing with the likeness of a deceased or missing person.\nProviding insight into seeking consent from real people whose images are included in media.\nDescribing clearer thresholds for what makes something \u201csynthetic enough\" to be directly disclosed."}, {"title": "Institutional Transparency", "content": "The transparency afforded by these cases is a step in the right direction for the field of course, the cases reveal instances of synthetic media development, creation, and distribution that shed light on institutional practices and tactics. In addition, the manner in which the institutions described and analyzed their decision making, and chose to share it, also offers transparency into institutional practices. One of the benefits, and challenges, from an open-ended case template was that institutions had quite a bit of flexibility in how they could focus, and describe their cases; one could focus on something as broad as general policy development or as specific as a particular gray area case that prompted debate, with varying levels of detail (though PAI pushed emphatically for more detail, across cases, with methods we will describe in more depth in future reporting). This flexibility was both practical (to enable us to learn more about how institutions would respond to our first foray into case studies of this sort) and useful (since we were interested in learning more about many levels of implementation of Framework principles and practices).\nWe were particularly heartened by the cases that offered frank introspection and were written in a manner that acknowledged when they changed course, and meaningfully, described why like in the case of OpenAI describing how they navigated their text detection decision making. This is the type of honest reflection we hope to promote, stylistically and substantively, in all future versions of the cases."}, {"title": "Complexity", "content": "One of the trickiest realities of the case study effort is the extent to which universal themes emerged, but so too did very unique, specific elements come through for each case. Ecosystem actors face similar value tradeoffs regardless of their positions in the synthetic media pipeline, but their specific institutional considerations and even specific case considerations need to guide their responses to those tradeoffs. This makes the job of those creating frameworks that move beyond merely stating \u201cdo no harm\u201d and thus apply"}, {"title": "Where We Go From Here", "content": "Government regulation and policy are key complements to the Synthetic Media Framework and governance activities at PAI more broadly. Our hope is that policymakers not only learn from the emergent best practices in these cases, but also consider:\nThe interconnectedness of Builders, Creators, and Distributors in the synthetic media pipeline.\nThe need for flexibility, and specificity, in synthetic media policymaking.\nHow the narrative considerations accompanying policymaking focused on synthetic media transparency may impact their efficacy for example, how is the impact of something like indirect disclosure's adoption conveyed to the public?\nThe need for synthetic media policy to adapt over time.\nThe ways in which different sectors social media platforms, media institutions, dating applications, synthetic media creator platforms, AI technology companies might require distinct recommendations for how to enact certain values.\nThe centrality of consent, transparency, support for creative expression, and harm mitigation to synthetic media policymaking.\nWe plan to report in more depth on PAI's analysis of the case study process soon. In the coming months, PAI will be working to analyze and refine this case study process for the eight additional institutions who have joined the Framework. Through our engagement with policymakers, including the NIST Safety Institute in the US, we will be sharing insights from these case studies and this exercise in synthetic media governance with the policy community. And further, we hope to drill deeper into some of the open questions underscored in the cases just as we further operationalized key elements of the Framework, like indirect disclosure methods [17], through multistakeholder convening and collaboration."}]}