{"title": "DeepDFA: Automata Learning through Neural Probabilistic Relaxations", "authors": ["Elena Umilia", "Roberto Capobianco"], "abstract": "In this work, we introduce DeepDFA, a novel approach to identifying Deterministic Finite Automata (DFAs) from traces, harnessing a differentiable yet discrete model. Inspired by both the probabilistic relaxation of DFAs and Recurrent Neural Networks (RNNs), our model offers interpretability post-training, alongside reduced complexity and enhanced training efficiency compared to traditional RNNs. Moreover, by leveraging gradient-based optimization, our method surpasses combinatorial approaches in both scalability and noise resilience. Validation experiments conducted on target regular languages of varying size and complexity demonstrate that our approach is accurate, fast, and robust to noise in both the input symbols and the output labels of training data, integrating the strengths of both logical grammar induction and deep learning.", "sections": [{"title": "1 Introduction", "content": "The problem of identifying a deterministic finite state automaton (DFA) from labeled traces is one of the best-studied problems in grammatical inference [11]. The latter sees applications in various areas, including Business Process Management (BPM) [1], non-Markovian Reinforcement Learning [14, 26], automatic control [7], speech recognition [3], and computational biology [25]. Both passive [17] and active [4] exact methods have been proposed for DFA identification. These methods are guaranteed to succeed in theory. However, in practice, they require a notable amount of computation, and they are unable to handle errors in the training dataset, making them of limited applicability, especially to real applications and large DFAs. Unlike exact approaches, Recurrent Neural Networks (RNN) tolerate errors in the training data, and they have proven highly effective at learning classifiers for sequential data [12]. DFAs and RNNS can both be used for language recognition, which is essentially binary classification over strings. Many works highlight the similarities between RNNs and finite-state machines [27]. The two differ in the transition representation: RNNs learn a parametrized transition function in a continuous hidden state space, while DFAs have a finite state space and completely transparent transitions. Furthermore, designing an RNN requires many choices: deciding the number of layers, the number of features of each hidden layer, and all the activation functions. Each of these decisions can affect the final performance and must be taken carefully. By contrast, exact methods do not require nearly any hyperparameter fine-tuning. Many approaches have been proposed to extract a DFA from a pre-trained RNN [24, 31, 32], generally adapting techniques from the grammar induction literature and suggesting ways to discretize or cluster the continuous RNN hidden states in a finite structure. The purpose of these works is not DFA induction but rather to enhance the explainability of black-box sequence classifiers. However, they open the door to DFA induction through neural networks, and join two fields that are classically kept separated.\nIn order to take the benefits from both worlds, grammar induction on one side and recurrent neural networks on the other side, we present DeepDFA: a transparent neural network design specialized in learning DFAs from traces with gradient-based optimization. The model resembles a recurrent neural network, but, differently from RNNs, it is completely transparent after training, as much as a DFA. Furthermore, thanks to its specialized design, it uses fewer weights than the most commonly used recurrent architectures, such as LSTMs and GRUs, and it only has one hyperparameter. This results in faster training and less memory consumption and a significantly reduced hyperparameter search. At the same time, since it is trained with back-propagation, it is able to learn DFAs in a significantly shorter time than grammar induction methods, even for large automata; and it can tolerate errors in the training labels and the training symbols. Our method is based on defining a neural network that behaves as a probabilistic finite automaton. We control how much the probabilities are close to categorical one-hot distribution through a temperature value. During training, we smoothly drive the network activations to be close to discrete 0/1 values by changing the temperature. When the gap between the discrete and actual activations is small enough, the network behaves precisely as a DFA.\nWe evaluate our method on the popular Tomita languages benchmark [28] and random DFAs of different sizes and different sets of symbols. Results show that DeepDFA is fast and accurate. It outperforms an exact SAT-based method [34] when the target DFA has more than 20 states, or the training set contains mislabeled examples, and it reaches competitive results in the other cases. We also compared DeepDFA to DFA extraction from a pre-trained RNN [32], finding it reaches better accuracy and predicts DFA of size closer to the target DFA size. The remainder of this paper is organized as follows: in section 2 we report related works; in section 3 we give some preliminaries on Deterministic and Probabilistic Finite Automata and Recurrent Neural Networks; in section 4 we formulate our problem and illustrate in detail the framework used to solve it; we report the experiments evaluating our approach in section 5; and finally we conclude and discuss directions for future work in section 6."}, {"title": "2 Related works", "content": "Combinatorial methods for grammar induction Many approaches have been proposed to identify a target DFA from a set of positive and negative traces. The L* algorithm [4], is an exact active learning algorithm to learn a DFA from an oracle through membership and equivalence queries. Another approach is to apply the evidence-driven state-merging algorithm [22, 8], which is a greedy algorithm that is not guaranteed to converge to the global optimum. A more modern approach is to leverage highly-optimized SAT solvers [17]. This approach is guaranteed to find the minimal DFA consistent with all the training examples, but suffers from scalability problems. In this regard, several symmetry-breaking predicates have been proposed for the SAT encoding to reduce the search space [33, 34].\nDFA extraction from recurrent neural networks Prior works extract a DFA from a pre-trained RNN, to explain the network behavior. Weiss et al. [32] adapt the L* algorithm [4] to work with an RNN oracle. Other work uses k-means clustering on the RNN hidden states to extract a graph of states [31]. Merrill et al. extract a DFA from an RNN by first building a prefix tree and then merging states with close state representation in the RNN hidden space [24]. These approaches train an RNN with the labeled strings and then extract an equivalent DFA from the trained model. Our approach differs from these since we directly train an RNN equivalent to a Probabilistic Finite Automaton (PFA), and we force the probabilities to become close to one-hot categorical distributions during training. In this way, our model becomes a DFA. In other words, there is no difference between the trained neural model and the automaton, and there is no risk that the abstraction does not represent the network, as for previous works. For example, Wang et al. [31] cluster the RNN states, so the automaton depends on the clustering algorithm performance. Performances from [24] instead rely on a similarity threshold and, as the paper shows, also on the number of epochs the RNN is trained after convergence. Our work in this sense is more similar to [32], since it computes the abstraction automatically. The difference is in how the abstraction is computed: we use gradient descent optimization while [32] starts with a hand-crafted discretization that is automatically refined during automata learning with L*.\nLearning crispy logical models through gradient descent Classically, the induction of logic models, including DFAs, is not approached with gradient-descent optimization methods (as deep learning methods) since their finite and 'crispy' nature prevents the gradient computation. However, recent works in neurosymbolic AI [15] propose techniques to reduce the gap between the induction of crispy models and that of continuous ones. Walke et al. [30] proposes a recurrent neural model with specialized filters to learn Linear Temporal Logic formulas over finite traces (LTLf) from labeled traces. Collery et al. [9] discovers logical rules describing patterns in sequential data using a differentiable model. Aichernig et al. [2] learn the DFA by constraining the outputs of a simple RNN to be close to one-hot vectors through a regularization term in the loss. Differently from [2], we define a new recurrent model specialized in learning PFAs, which we discretize at training time with temperature annealing and train with the classical cross-entropy between the predicted and the ground-truth label. Grachev et al. [16] proposes a neural network model similar to ours to learn DFA from traces. However, this model does not use activation functions with discrete outputs, and it is affected by the vanishing gradient problem for automata larger than 5 states, while our method can effectively learn target automata with up to 30 states."}, {"title": "3 Background", "sections": [{"title": "3.1 Deterministic Finite Automata", "content": "A Deterministic Finite Automaton (DFA) A is a tuple (P,Q, \u03b4, qo, F), where P is a set of propositional symbols called the alphabet, Q is a finite set of states, qo\u2208Q is the initial state, \u03b4: Q \u00d7 P \u2192Q is the transition function, and FCQ is the set of final states. Let P* be the set of all finite strings over P, and \u20ac the empty string. The transition functions over strings 8* : Q \u00d7 P* \u2192Q is defined recursively as\n$\\delta^* (q, \\epsilon) = q$\n$\\delta^* (q, \\alpha x) = \\delta^* (\\delta(q, \\alpha), x)$\nWhere a \u2208 P is a symbol and x \u2208 P* is a string, and ax is the concatenation of a and x. A accepts the string x (i.e., x is in the language of A, L(A)) if and only if d* (qo, x) \u2208 F. Let x = x[1]x[2]...x[l] be the input string, where x[t] is the t-th character in the string, we denote as q = q[0]q[1]...q[l] the sequence of states visited by the automaton while processing the string, namely q[0] = qo and q[t] = d(q[t - 1], x[t]) for all t > 0."}, {"title": "3.2 Probabilistic Finite Automata", "content": "A Probabilistic Finite Automaton (PFA) Ap is a tuple (P,Q, ip, dp, fp), where P is the alphabet, Q is the finite set of states, ip: Q\u2192 [0, 1] is the probability for a state to be an initial state, dp: Q \u00d7 P \u00d7 Q \u2192 [0, 1] is the transition probability function, and fp: Q\u2192 [0, 1] is the probability of a state to be final. We have therefore \u03a3 \u03b4\u03c1 (q, p, q') = 1, and \u2211 i(q) = 1 \u221aq \u2208 Q, \u2200a \u2208 P.\nq'EQ\nqEQ\nWe can represent the PFA in matrix form as a transition matrix T, an input vector vi and an output vector vo. MatrixT\u2208 RPXQXQ contains at index (p,q, q') the value of op (q, p, q'). We denote as T[p] \u2208 Rx the 2D transition matrix for symbol p.\nThe input vector vi \u2208 R1\uc774 contains at index k the probability of state qe to be an initial state, while the output vector vo\u2208 RIQI\u00d71 has in position k the probability of state qe to be accepting. This matrix representation is shown in Figure 1(b).\nGiven a string x = x[0]x[1]...x[l\u22121], we denote as qp,0, qp,1...qp,l the sequence of probabilities to visit a certain state, where qp,t E R1\u00d71QT is a row vector containing at position k the probability to stay in state k at time t.\n$\\qp,0 = Vi$\n$\\qp,t=qp,t-1T[x[t]]\\quad\\forall t > 0$\nThe probability of being in a final state at time t is the inner product qp,tvo.\nTherefore the probability of a string to be accepted is the probability to be in a final state in the last computed state qp,l, and it is calculated as follows\n$viT[x[0]]T[x[1]]..T[x[l \u2212 1]]\u03c5\u03bf$"}, {"title": "3.3 Recurrent Neural Networks", "content": "A Recurrent Neural Network (RNN) is a parameterized function ht = f(ht-1,xt; 0h), having trainable parameters \u03b8h, that takes as input a state-vector at time t \u2013 1, ht\u22121, and the input vector at time t, xt, and returns the current state-vector at time t, ht. An RNN"}]}, {"title": "4 DeepDFA", "content": "We consider the problem of inferring a DFA from a training set of labeled strings D = {(x1, y1), (X2,Y2), ..., (Xn, Yn)}, where xi is a string of length I of symbols x[0], x[1], ..., x[l \u2013 1], in the automaton alphabet P, xi[t] \u2208 P with 0 < t < l, and yi \u2208 {0,1} is the associated label, denoting whether the string is accepted or not by the target automaton.\nTo infer the automaton, we define a recurrent neural network model that mimics the behavior of a PFA in a state space Qmax and action space P, where P is the target automaton alphabet while Qmax is our hypothesis on the state space, which will generally differ from the true Q. For this reason, the size of Qmax is a hyperparameter of our model.\nWe cannot use gradient-based optimization to learn the DFA directly because of its non-differentiable transitions and output vector. The intuition behind our work is that PFAs are closely related to recurrent neural networks, since they calculate the next state and output using multiplications between continuous vectors and matrices, in the same way RNNs do. However, DFAs can also be represented in matrix form, with the difference that their matrix representation is composed of only one-hot row vectors. As example we show in Figure 1 a simple PFA (subfigure (a)) and the DFA obtained by approximating all the PFA matrix representation row vectors to the closest one-hot (subfigure (b)).\nFollowing this idea, we define DeepDFA as a parametric PFA in which we can drive the representation to be close to one-hot during training. We obtain this effect using an activation function that smoothly approximates discrete output. Many works use this technique [20, 30, 21], especially in neurosymbolic AI [5], to learn symbolic logic structures by using differentiable models such as neural networks. In particular, we use a modified version of the classical softmax and sigmoid activation functions, which we call softmax_with_temp and sigmoid_with_temp. Given a function f(x) we define f_with_temp(x, t) = f(x/t), with 7 being a positive temperature value.\nOur RNN comprises two trainable modules: a transition function and an output function. The transition function ht (ht\u22121, x[t-1]; 0h) has parameters \u03b8\u03b7, takes as input the probabilities over the previous state, ht\u22121 \u2208 [0,1]|2|max, and the previous symbol, x[t \u2212 1] \u2208 P, and predicts the current state ht. The output function y(ht; \u03b8y) implements the classification module: given the current state estimation, ht, predicts the probability of the current state to be an accepting state using its parameters \u03b8y. In particular, fixed a temperature value T\n$\\h_0 = v_i = [1, 0, ..., 0]$\n$\\h_t=h_{t-1}T[x[t - 1]]$\n$\\y_t = h_t\\upsilon_o$\n$\\T = softmax\\_with\\_temp(\\theta_{\\eta}, \\tau)$\n$\\v_o = sigmoid\\_with\\_temp(\\theta_y, \\tau)$\nWhere T is the PFA transition matrix, T[x[t]] is the transition matrix for symbol x[t], and vo is the output vector, as defined in Section 3.2. They are obtained by applying discrete activation functions on parameters On \u2208 R|P|\u00d7|Qmax|\u00d7|Qmax| and dy \u2208 R|Qmax |\u00d71. In particular, the softmax activation applied to the third dimension of the matrix \u03b8\u03b7 ensures \u2211q, dp(q, a, q') = 1, and the sigmoid activation ensures values of vo stay in [0, 1].\nIn the forward pass, we calculate the probability of a string x in the dataset to be accepted by the RNN by applying Equation 5 recursively to the input sequence of symbols x[0], x[1], ..., x[I - 1]. The final output y\u0131 is compared with the ground truth label y. We update the model weights with back-propagation by minimizing the binary cross-entropy between model predictions and the ground truth labels.\nTemperature Annealing Cold temperatures force the PFA to behave as a DFA, since the activation values become closer to boolean values as the temperature decreases. When the temperature is low enough, the model transforms into a DFA. Let us notice that using"}, {"title": "5 Experimental Evaluation", "content": "In this section, we report the main results supporting our method. We provide our code at https://github.com/whitemech/DeepDFA.\nTarget DFAs We test our approach on two different sets of DFAs. The first experiment is on the Tomita languages [28], which are a standard benchmark for grammar induction and DFA extraction from RNNs [32, 31]. The benchmark comprises seven formal languages of increasing difficulty defined on the binary alphabet P = {a,b}. Despite Tomita languages being a popular benchmark, they are represented by small DFAs with state size smaller than six. In order to test our approach on bigger DFAs, we conduct a second experiment on randomly generated DFAs with state size |Q| between 10 and 30, and alphabet size P between 2 and 3. For each setting, we generate 5 random DFAs as described in [33]. Q states are generated and enumerated between 1 and Q, we set 1 as the initial state, and each state is equiprobable to be accepting. We connect each state i with a random state in [i + 1, Q] with a random-labeled transition. In this way, we partially build an automaton where all states are reachable from the initial state. Finally, we complete the DFA with random transitions.\nDataset For each target DFA, we create a train, a dev, and a test dataset by sampling strings of various lengths and labeling them with the target DFA. The training dataset contains strings with a length between 1 and lentrain, the dev set is composed of strings of length lendev, and the test set by sequences of length lentest. In order to test the model's capability to generalize to longer unseen sequences, for each dataset, we set lentrain < lendev < lentest. To prevent the models from learning degenerate solutions, all the train datasets are nearly perfectly balanced. We set lentrain = 30, lendev=60, and lentest 90 for all the Tomita datasets and the random DFAs datasets with Q < 30. For the random DFAs with state size of 30, we set lentrain=50, lendev=100, and lentest=150. We report the size of each dataset in the supplementary material. To test the resiliency of different methods to errors in the training data, we also create a corrupted version of each training dataset by flipping 1% of the labels.\nBaselines Our approach is a hybrid between a RNN and a DFA. These two types of sequence acceptors are trained with very different methods and present different strengths and weaknesses. In order to better understand the benefits of having a hybrid method, we compare it with one state-of-the-art from the literature on grammar induction, DFA inductor [34], one state-of-the-art method to extract DFAs from RNNs, L* extraction [32], and one state-of-the-art neurosymbolic method for automata learning, DFA Generator [16]. Notice we cannot compare with the other neurosymbolic methods cited in Section 2. Indeed, Walke et al. [30] learns an LTLf formula from data, LTLf formulas are less expressive than DFAs, since they can capture only star-free regular languages [10], while DFAs can capture both star-free and non-star-free languages. Note that Tomita 3,5 and 6 are non-star-free grammars [6], and our schedule to produce random DFAs may generate non-star free regular languages. Collery et al. [9] learn a set of local and global rules that are ultimately combined, and the relationship between their formalism and that of DFAS is not clear. Regarding the chosen baselines, we remind that DFA-inductor is a SAT-based approach for exact DFA identification. In particular, we use the shared implementation code at \u00b9, and we test with breadth-first search (BFS) symmetry breaking, shown to be the most effective in the paper. Instead, L* extraction abstracts a finite state automaton from a pretrained RNN, starting from a predefined discretization of the hidden state space and applying the L* algorithm and abstraction refinement when required. To test this method, for each language: (i) we train many RNNs of different types, different number of layers and different hidden-state size, (ii) we record the performances on the dev set (iii) we apply L* extraction on the RNN achieving the best dev accuracy. In particular, we use the code in the public repository 2 with 10 as the initial split depth, and the starting examples set composed of the shortest positive string and the shortest negative string in the train set, as suggested in the paper. DFA Generator trains directly a neural network shaped as a DFA, as our method. The algorithm needs as hyperparameter the maximum number of states Qmax. We train with Qmax of different sizes (the"}, {"title": "5.1 Results", "content": "Results on Tomita Languages For each Tomita language, we report the results averaged over 5 experiments with different random seeds for L* extraction, DFA-generator and DeepDFA, and the results of one application of DFA-inductor, since the latter has a deterministic behavior. To explore the influence of hyperparameter choices on DeepDFA, DFA-generator and L* extraction, we test 3 different architectures for DeepDFA and DFA-generator and 6 different RNNs for L*. In particular, we test DeepDFA and DFA-generator with the maximum number of states equal to 10, 30, and 100 (denoting the models as DeepDFA(|Qmax|) and DFAGen((|Qmax|)). We test L* extraction on both LSTMs and GRUs, for each type of RNN we test three architectures (denoted here as RNN(#layers, #hidden neurons per layer)), namely: RNN(1,30), RNN(1,100), RNN(2,100). For space reasons, we report in the supplementary material the results obtained for each model tested and here only the results of the best model for each approach. We select he best model per approach as the"}, {"title": "Ablation study: the effect of changing the state space size", "content": "RNNS have many hyperparameters and design choices that can affect performance: the model type, the number of layers, the number of features per layer, and more. By contrast, our recurrent neural model is a simplified structure with only one hyperparameter: the hidden state size. In this section, we discuss this hyperparameter choice. Figure 2(c-d) shows the effect on the test accuracy and the predicted number of states of tuning this hyperparameter. We conducted experiments using a random DFA with a size of 20 and an alphabet size of 3. The hidden state size was varied within the range of 10 to 120. Results show that our model demands a hidden state size surpassing the actual number of states. The best performances for the random DFA of size 20, are achieved for Qmax \u2265 70-exceeding its actual state count by 3.5 times. Remarkably, the model maintains its robust performance even when the state size is markedly overestimated. Even at the largest state size, test accuracy remains commendably high, and the number of states inferred remains proximate to the actual count. This suggests that the model evades overfitting; despite being dimensioned to represent a substantially greater number of states, it leverages only a subset of them."}, {"title": "6 Conclusions and Future Work", "content": "In conclusion, we propose DeepDFA: a hybrid between a DFA and a recurrent neural network, which can be trained from samples with backpropagation as usual deep learning models, but that is completely interpretable, as a DFA, after training. Our approach takes the best from the two worlds: grammar induction on one side and recurrent neural networks on the other side. It uses fewer weights and only requires setting one hyperparameter compared to recurrent neural nets. At the same time, it can tolerate errors in the training labels and noise in the input symbols, and can be applied to large target DFAs, differently from exact methods. DeepDFA can find applications in various scenarios. In this paper, we present the framework in a general context, without specifying any particular application. However, we are keen to apply it in the area of non-Markovian Deep Reinforcement Learning [14, 26, 23]. Furthermore, we are developing a multi-layered version of DeepDFA, having the potential to expand its capabilities to even more intricate regular languages without compromising the explainability, which we leave for future research."}]}