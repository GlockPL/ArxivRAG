{"title": "A Comparison of Prompt Engineering Techniques for Task Planning and Execution in Service Robotics", "authors": ["Jonas Bode", "Bastian P\u00e4tzold", "Raphael Memmesheimer", "Sven Behnke"], "abstract": "Recent advances in Large Language Models (LLMs) have been instrumental in autonomous robot control and human-robot interaction by leveraging their vast general knowledge and capabilities to understand and reason across a wide range of tasks and scenarios. Previous works have investigated various prompt engineering techniques for improving the performance of LLMs to accomplish tasks, while others have proposed methods that utilize LLMs to plan and execute tasks based on the available functionalities of a given robot platform. In this work, we consider both lines of research by comparing prompt engineering techniques and combinations thereof within the application of high-level task planning and execution in service robotics. We define a diverse set of tasks and a simple set of functionalities in simulation, and measure task completion accuracy and execution time for several state-of-the-art models. We make our code, including all prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering.", "sections": [{"title": "I. INTRODUCTION", "content": "Since the unveiling of OpenAI's ChatGPT in November 2022, the rapid emergence of Large Language Models (LLMs) as a major contributor to human-machine interaction has been rippling through society. With advances in scaling, instruction tuning, and alignment, models like GPT [1], [2], Gemini [3], Llama [4], or Mistral [5] can seemingly understand tasks presented in natural language and respond with answers in natural language that are often appropriate. In this work, we investigate approaches that leverage these advances for robot control and human-robot interaction [6]-[9], as well as prompt engineering techniques that aim to increase performance across various benchmarks [10]\u2013[12]. In particular, we focus on comparing and combining prompt engineering techniques for the application of task planning and execution in the context of service robotics.\nMany applications require autonomous robots to perform tasks in complex, cluttered, dynamic, or unknown environments. In order for such a robot to adapt to new tasks and environments, an expert is often required to understand the required task logic and translate it into an implementation based on the robot's capabilities. This severely limits speed and flexibility for robot deployment. LLMs promise to relax this requirement while allowing for intelligent replanning during task execution. To this end, we focus on the challenging field of service robotics, requiring robots to interact with non-expert operators in open-ended, highly unstructured environments designed for and shared with humans.\nTo use an LLM for task planning, one must provide all the necessary information about the environment, the task, and the robot capabilities in textual form. Similarly, for task execution, the implementation of the robot's high-level functionalities must be robust and general enough to account for unknown features of the environment and unexpected events during task execution. In this work, we obviate these problems by employing a simulated environment depicted in Figure 1 to systematically examine the effect of prompt engineering techniques on task completion across several types of tasks and state-of-the-art LLMs.\nThe contributions of this paper are as follows:\n\u2022 We derive an LLM-based method allowing zero-shot task execution by sampling relevant action sequences from sets of pre-defined actions received via natural language commands from non-expert operators.\n\u2022 We thoroughly investigate and evaluate the feasibility and effectiveness of integrating LLMs with various"}, {"title": "II. RELATED WORKS", "content": "The use of LLMs for high-level robot task planning and for low-level execution control are hot research topics [13], [14].\nA. Using LLMs for Robot Task Planning & Execution\nIn recent years, LLMs have been explored and adopted across a wide range of applications, motivated by harnessing the world knowledge extracted from the vast text data sets they have been trained on.\nVemprala et al. [6] are among the proponents for high-level robot task planning using GPT. They outline the design principles of a robotics pipeline that integrates GPT to plan and implement task execution given an objective in textual form. They define a function library that implements various core functionalities for a given robot platform. They then let GPT generate code that implements a task given by a (non-technical) user based on this function library. Finally, the user can provide feedback and corrections to the code before approving it, allowing the robot to execute it. Their pipeline is evaluated in various domains such as manipulation, aerial navigation, and logical reasoning. This approach can generate complex code that incorporates appropriate branching to handle unexpected events during task execution, provided that the library functions support such information. However, the generated code must explicitly anticipate any such cases beforehand, as the method does not allow for dynamic replanning within GPT.\nIchter et al. [7] present a similar approach where a set of core functionalities is implemented for a given robot platform and exposed to an LLM input as context. Instead of relying on human feedback, they assign affordance values to all available functions and execute the highest-ranking one. They recalculate the affordance values before each function call to account for the current state of the robot and the environment.\nAnother LLM-based method for robot task planning is presented by Singh et al. [8], combining strengths in common-sense reasoning and code understanding to gen-erate executable plans. Their experiments demonstrate that incorporating programming language features enhances task performance and adapts well to diverse scenarios.\nDing et al. [9] propose a method for open-world task planning and situation handling that dynamically integrates commonsense knowledge into a robot's action knowledge, assesses the feasibility of progressing with the existing plan, or determines to adapt the plan accordingly. They integrate their approach on a real robot platform and demonstrate it executing service tasks in a domestic environment.\nB. Prompt Engineering Techniques\nWhile LLMs are capable of performing tasks with zero-shot prompting [15], they benefit from in-context examples with few-shot prompting [2]. To further improve performance"}, {"title": "III. DIALOGUE WITH A SIMULATED ENVIRONMENT", "content": "We investigate the effect of prompt engineering techniques across multiple tasks in a simulated environment. In this environment, the robot is provided with pre-defined functions from which an LLM must select and execute the appropriate ones to accomplish a task given in natural language. This process is modeled as a conversation between the robot and the LLM assistant. Once the LLM signals that it has completed the task, we probe the environment to determine if the target condition indicating task success is met.\nA. Environment\nIn order to create an environment that allows an accurate comparison of the effect of prompt engineering techniques, we adopt the setting found at RoboCup@Home [16], which is a major international competition that focuses on domestic service robots.\nThe arena is designed to replicate a typical domestic dwelling with several rooms: A study room, a parlor, a kitchen, and a bedroom. All tasks start with both robot and operator located in the parlor. Each room contains multiple objects, including common food and household items, with which the robot can interact with.\nB. Action Set\nWe use the function-calling feature introduced by OpenAI's API for their GPT line of models [2], which is adopted by other LLMs such as Mistral [5], as well as several open-source inference pipelines. It allows for specifying the signatures of the pre-defined robot functions as a JSON schema. This exposes the function names, parameters,"}, {"title": "C. Tasks", "content": "In our experiments, we define distinct tasks with varying levels of complexity aimed at testing different types of abilities. All tasks are designed to resemble realistic use cases in the sense that they demonstrate a useful application when implemented with a real robot platform, and are related to tasks found in the RoboCup@Home [16] competition. To demonstrate robustness and generalization, we randomly assign the objects and their respective locations for each task. We define the following four tasks:\n1) Fetch: The Fetch task requires the robot to pick up an object from another location and return it to the user. Since it can be solved with a simple sequence of function calls, it mainly tests whether the LLM, in combination with the applied prompt engineering technique, is able to comprehend the scenario, use the function-calling feature, and generate valid JSON appropriate to solve the task. An example of this task is \"Please get me a pen from the study\".\n2) Conditional: The Conditional task extends the Fetch task by adding the requirement to gather knowledge about the environment during the execution of the task and to adapt to it. In particular, the robot is asked to search for an object and return one of two other objects to the user. The choice of the latter depends on the first object found. This task therefore tests whether the LLM is capable of branching logic. An example of this task is \"Check if there is a spoon in the kitchen. If you find one, bring me a pen from the study. If not, bring me a comb from the bedroom\".\n3) Equals: The Equals task requires the robot to make a numerical observation and repeatedly retrieve an object according to that observation. It therefore tests basic mathematical ability and requires the execution of a logical loop. This results in a long task that requires many function calls, challenging the LLM to maintain focus on accomplishing the task objective. An example of this task is \"For every orange in the kitchen, move a fork from the kitchen to the parlor\".\n4) Distribute: In the Distribute task, a given object must be distributed so that every location in the simulation contains at least one instance. The LLM must therefore not only visit all locations, but also make numerous decisions regarding the movement of objects and keep track of their locations. An example of this task is \"Please distribute the pens evenly so that each location contains at least one pen. You can start with the pens in the study\"."}, {"title": "IV. EXPERIMENTS", "content": "A. LLM Variants\nWe focus on evaluating GPT [2] model variants, due to the function-calling feature of the corresponding API, and their top-of-the-line results on benchmarks regarding this feature [17]. In particular, we use GPT-3.5-Turbo-0125 and GPT-4-0125-Preview, referred to as GPT 4 Turbo. We evaluate each condition with respect to tasks and prompt engineering techniques with 50 repetitions for GPT 3.5 Turbo and 20 repetitions for GPT 4 Turbo due to budgetary considerations. For both model variants and all tasks we use a temperature setting of T = 0 to elicit factual responses. All other model parameters are set to the default values suggested by OpenAI.\nB. Prompt Engineering Techniques\nOur simulation procedure allows the robot to execute tasks immediately after providing the LLM with the task description in an initial prompt along with the function definitions. Task completion then proceeds as a dialog of function calls generated by the LLM and corresponding function responses generated by the simulation. This approach serves as a baseline for investigating and comparing the effects of various prompt engineering techniques on task"}, {"title": "V. RESULTS", "content": "For illustration, Figure 3 shows a sample transcript of a Fetch task using GPT 4 Turbo and CoT prompting. The averaged results of our experiments for all tasks and prompt engineering techniques are shown in Table I for GPT 3.5 Turbo and in Table II for GPT 4 Turbo. For all test conditions, we report the success rate and the average time required to generate a sample. Note that these times only reflect the time spent waiting for LLM responses from the OpenAI API, while the required time of robot actions and simulation feedback can be considered negligible.\nOn the Fetch task, GPT 4 Turbo achieves a perfect success rate for all prompting techniques. In comparison, GPT 3.5 Turbo requires more complex prompting techniques to achieve a high success rate. The baseline for GPT 3.5 Turbo fails to solve the task in almost all cases. Using adaptive functions, CoT or state descriptions increases the success rate. A perfect success rate for GPT 3.5 Turbo is only achieved by providing an example. Similar results can be observed for the success rate of the Conditional task with GPT 4 Turbo. Again, a perfect success rate is only achieved by providing an example. For the Conditional task with GPT 3.5 Turbo, we observe a significant drop in the success rate when an example is missing. This supports the argument that LLMs are few-shot learners [2]. However, a single example seems to be sufficient to accomplish the task at hand.\nWe find that more complex prompt engineering techniques do not necessarily lead to higher success rates. On the Equals and Distribute tasks with GPT 3.5 Turbo, the techniques without state descriptions outperform the corresponding technique with state descriptions. However, state descriptions do not always have a negative impact. For the GPT 3.5 Turbo Conditional task, the addition of state descriptions (AF + ReAct + EiP + StD) achieves the highest success rate of 72%. We also observe that using only adaptive functions achieves the best success rate with GPT 4 Turbo for the Distribute task, while achieving the worst success rate for the Conditional and Equals tasks. There's also no clear favorite between CoT and ReAct, as the success rates vary depending on the task, model variant, and other prompt engineering techniques used in conjunction.\nDue to the additional prompts used in all prompting techniques besides the adaptive functions, the measured task completion time depends on the the technique used. In general, we observe that CoT and even more so ReAct show a longer runtime due to the relatively large number of tokens generated by the LLM for planning and reasoning compared to function calls. Using state descriptions increases the time even more for the same reason. Even though the adaptive functions do not utilize additional prompts, we can see a slight reduction in time compared to the baseline, because the LLM is less likely to call functions that result in failures. In a real application, the response time of an LLM can significantly increase the execution time of a task, although functions that require considerable time themselves provide leeway to mask this problem.\nWe have successfully used GPT for task planning and execution in several service robotics tasks. We show that the use of appropriate prompt engineering techniques can effectively improve performance. However, overall performance is significantly dependent on the model variant used."}, {"title": "VI. CONCLUSION", "content": "In this work, we demonstrated the use of LLMs to understand user commands to plan and execute tasks given an action set and an environment state in the context of service robotics. We evaluated several prompt engineering techniques in a simulated environment, across different tasks and models. GPT 3.5 Turbo performed well on simpler tasks using advanced prompt engineering techniques, but struggled with complex scenarios. GPT 4 Turbo managed to complete complex tasks with mixed reliability, while significantly outperforming GPT 3.5 Turbo on simpler tasks.\nWe found that while CoT [11] and ReAct [12] improve task completion rates, the best performance is achieved when combined with an example of a successful completion of a"}]}