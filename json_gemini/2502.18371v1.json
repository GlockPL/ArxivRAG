{"title": "MindMem: Multimodal for Predicting Advertisement Memorability Using LLMs and Deep Learning", "authors": ["Sepehr Asgarian", "Qayam Jetha", "Jouhyun Jeon"], "abstract": "In the competitive landscape of advertising, success hinges on effectively navigating and leveraging complex interactions among consumers, advertisers, and advertisement platforms. These multifaceted interactions compel advertisers to optimize strategies for modeling consumer behavior, enhancing brand recall, and tailoring advertisement content. To address these challenges, we present MindMem, a multimodal predictive model for advertisement memorability. By integrating textual, visual, and auditory data, MindMem achieves state-of-the-art performance, with a Spearman's correlation coefficient of 0.631 on the LAMBDA and 0.731 on the Memento10K dataset, consistently surpassing existing methods. Furthermore, our analysis identified key factors influencing advertisement memorability, such as video pacing, scene complexity, and emotional resonance. Expanding on this, we introduced MindMem-ReAd (MindMem-Driven Re-generated Advertisement), which employs Large Language Model-based simulations to optimize advertisement content and placement, resulting in up to a 74.12% improvement in advertisement memorability. Our results highlight the transformative potential of Artificial Intelligence in advertising, offering advertisers a robust tool to drive engagement, enhance competitiveness, and maximize impact in a rapidly evolving market.", "sections": [{"title": "Introduction", "content": "The advertising industry operates within a highly competitive landscape, where the ability to capture and sustain consumer attention is paramount. The intricate interactions among consumers, advertisers, and platforms within multi-agent strategic settings are crucial for businesses to effectively navigate this complex landscape. These settings enable the simulation of diverse consumer behaviors, brand recall and engagement, and platform optimizations, allowing advertisers to refine their strategies\u2014from understanding consumer preferences to fine-tuning advertisement placements and crafting persuasive messages. Predicting advertisement memorability is crucial to bridge the gap between understanding consumer interactions and crafting advertisements that effectively capture and retain consumer attention.\nDeep learning algorithms and large language models (LLMs) have significantly improved our ability to predict and enhance advertisement memorability. Memorability is a critical driver of consumer engagement, brand loyalty, and purchase decisions, yet it remains a challenging factor to measure and optimize due to its inherent complexity. By integrating textual, visual, and auditory data, Deep Learnings and LLMs provide a more comprehensive understanding of the elements that amplify an advertisement's impact (Li et al. 2022). However, many existing methods are limited by their reliance on single-modal data and their inability to account for the complexities of human cognition. To effectively model human cognition and memorability, a multimodal approach is essential, as it more closely mirrors the way humans perceive and process information from their environment (Wang et al. 2024). By leveraging such multimodal datasets and developing adaptive multimodal ensemble methods, advertisers are allowed to craft impactful content but also simulate long-term consumer engagement within multi-agent strategic setting.\nIn this study, we introduce MindMem, a multimodal framework for predicting advertisement memorability, and MindMem-ReAd (MindMem-Driven Re-generated Advertisement), a scalable method for enhancing memorability by fine-tuning language models on advertisement datasets. Focusing on the advertiser's role within multi-agent strategic settings, these tools demonstrate how generative AI can bridge the gap between theoretical multi-agent strategies and practical advertising solutions. Our approach aims to assist businesses in creating more targeted, memorable, and effective campaigns in an incresingly competitive market."}, {"title": "Related Work", "content": null}, {"title": "Factors influencing Memorability", "content": "Bainbridge et al. explored how humans process and retain visual stimuli, emphasizing the importance of emotionally salient and visually distinctive elements in enhancing memorability (Khosla et al. 2013). Their findings suggested that humans are more likely to remember visual scenes that contain unique or emotionally charged content, as opposed to mundane or repetitive scenes. Additionally, the other study focused on the concept of intrinsic memorability revealing that certain visual characteristics, such as color, object saliency, and scene composition, naturally influence memory retention, independent of individual viewer biases (Khosla et al. 2015). These studies laid the groundwork for understanding the cognitive processes involved in memorizing visual information and provided key insights into the types of visual content that are more likely to be remembered. Several studies have aimed to identify the specific features or characteristics that contribute to the memorability of visual content. One study assessed the memorability of various objects within scenes, and found that certain object categories, like faces and animals, are inherently more memorable than others, such as buildings or landscapes (Isola et al. 2011). It highlighted the role of object prominence and scene context in shaping human memory. Similarly, it has been shown that the memorability of a scene is largely driven by its most memorable object (Dubey et al. 2015). Despite these valuable insights, these studies were limited in their focus on static images and often failed to account for the dynamic, multimodal nature of real-world stimuli, such as advertisements or videos. Moreover, these works largely overlooked the temporal and emotional dimensions that play a critical role in memory formation."}, {"title": "Machine Learning Approaches for Multimodal Memorability Prediction", "content": "More recently, multimodal approaches have emerged as a powerful method for memorability prediction, integrating visual, textual, and audio features to capture a broader spectrum of the factors that contribute to memory retention. Several studies have investigated predicting memorability from video content, integrating audio and emotional cues to enhance model accuracy (Dudzik et al. 2020). Other study leveraged video-triggered Electroencephalogram (EEG) data to examine how emotions evoked by videos influence memorability (Hu et al. 2020). Another study has integrated LLMs with deep learning to process not just visual features, but also audio and textual elements, highlighting the benefit of capturing the complex interactions across modalities in advertisements (HariniSI et al. 2024). Although these models have improved prediction accuracy, they often fail to fully capture the complexity of human cognition, as they process modalities separately rather than integrating them into cohesive multimodal representations: an essential aspect for modeling human memory, particularly in scenarios requiring temporal processing and adaptability."}, {"title": "Methods", "content": null}, {"title": "Dataset", "content": "To develop and evaluate MindMem, we use two datasets, Long-term Ad Memorability DAtaset (LAMBDA) (HariniSI et al. 2024) and Memento10K (Newman et al. 2020), which provide complementary settings for assessing advertisement memorability and general video memorability.\nTo train and build our models to predict advertisement memorability, we used the LAMBDA dataset. The dataset consists of 2,205 commercial advertisements from 276 brands across 113 industries. The LAMBDA dataset includes videos released between 2008 and 2023, with an average duration of 33 seconds. These videos feature diverse characteristics, such as varying scene velocities, the presence of humans or animals, visual and audio branding, emotional content, scene complexity, and different audio types. Participants viewed those advertisements, and their brand recall, advertisement recall, scene recall, and audio recall were assessed after a minimum of 24 hours. Memorability scores were calculated by averaging brand recall scores from 1,749 participants to determine the overall long-term advertisement memorability. The memorability scores were scaled ranging from 0 to 1. In total, 1,963 advertisements with memorability scores were used to train models, and 219 used to test model performance. Percentage of speech in a video, video length, and time of day to watch advertisements showed non-significant correlations with memorability score. Meanwhile, negative emotions are more memorable than positive emotions (HariniSI et al. 2024). Video popularity and memorability show a positive correlation.\nTo assess the reliability of the MindMem architecture, we evaluate it using the Memento10K dataset (Newman et al. 2020). This dataset was constructed by scraping natural videos from the Internet and filtering out artificial scenes and undesirable features (e.g., watermarks), resulting in a collection of 10,000 videos. The dataset emphasizes both the visual and semantic aspects of video memorability and includes human-annotated memorability scores, action labels, and textual descriptions (five human-generated captions per video). It is partitioned into training (7,000 videos), validation (1,500 videos), and test (1,500 videos) sets. For our analysis, we applied the MindMem architecture to the training set and evaluated its performance on the validation set, the results of which are presented here."}, {"title": "Multimodal Data Embeddings", "content": "In MindMem, we leverage pre-trained LLMs as our cognitive modules (Figure 1). For video embedding, Long Video Assistant (LongVA) model was used to extract visual features from the dataset (Zhang et al. 2024). By leveraging the last hidden layer of the LongVA, we capture both visual and temporal information from long video sequences. For audio embedding, we first extracted audio from videos and fed them into Qwen2 (7B) audio model (Chu et al. 2024), leveraging its last hidden layer to produce audio embeddings. For text embedding, Gemini Pro 1.5 (Team et al. 2024) was used to generate detailed textual descriptions of video content by posing targeted questions about scenes and visual details (Appendix 1). These descriptions were then processed by the Qwen2 (7B) text model (Yang et al. 2024), which extracted embeddings from the last hidden layer."}, {"title": "Model Generation and Evaluation", "content": "Figure 1 shows the procedure to train and build MindMem. As described previously, visual, auditory, and textual embeddings are performed, and the encoded representations of those modalities are expressed:\n$h_v = LongVA(x_v),$\n$h_a = Qwen2_Audio(x_a),$\n$h_t = Qwen2_Text(x_t),$\nwhere $x_v$, $x_a$, and $x_t$ are the raw inputs for visual, auditory, and textual data, respectively, while $h_v$, $h_a$, and $h_t$ represent their corresponding embeddings.\nTo predict memorability scores, the MindMem architecture processes these embeddings through several key components, which are detailed below:"}, {"title": "Projection Layers", "content": "To ensure compatibility across modalities, the extracted embeddings ($h_v$, $h_a$, $h_t$) are projected into a shared latent space of dimension 1,024. This involves linear transformation, layer normalization, and dropout:\n$h'_v = Dropout(LayerNorm(Linear(h_v))),$\n$h'_a = Dropout(LayerNorm(Linear(h_a))),$\n$h'_t = Dropout(LayerNorm(Linear(h_t))).$\nwhere, $h_v$, $h_a$, and $h_t$ represent the initial embeddings from the visual, auditory, and textual modalities, while $h'_v$, $h'_a$, and $h'_t$ are the projected embeddings. These transformations reduce the original dimensionality while preserving the essential features necessary for downstream tasks."}, {"title": "Self-Attention Pooling", "content": "Since visual, audio, and text embeddings have variable sequence lengths, we use self-attention pooling to aggregate each modality's embeddings into fixed-length vectors. This process captures intra-modal dependencies and emphasizes the most relevant features. Self-attention operates on the query Q, key K, and value V vectors, which are derived from the modality embeddings $h'_v$, $h'_a$ and $h'_t$. The formula for self-attention is as follows:\n$Attn(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}}) V$\nwhere $d_k$ is the dimensionality of the query and key vectors. Notably, in the self-attention mechanism, Q, K, and V are representations of the same modality.\nThese self-attention outputs are then pooled to produce fixed-length representations for each modality. This yields the fixed-length pooled representations $h''_v$, $h''_a$, and $h''_t$ for the visual, auditory, and textual modalities, respectively.\n$h''_v = SelfAttentionPooling(h'_v),$\n$h''_a = SelfAttentionPooling(h'_a),$\n$h''_t = SelfAttentionPooling(h'_t).$\nHere, $h''_v$, $h''_a$, and $h''_t$ are the pooled representations for the visual, auditory, and textual modalities. By applying multi-head attention, the self-attention pooling mechanism ensures that the model prioritizes contextually important elements in each sequence."}, {"title": "Cross-Attention", "content": "To capture cross-modal dependencies, we employ multi-head cross-attention mechanisms, where each modality aligns with and incorporates information from the other two. For instance, the visual representation $h''_v$ attends to the audio $h''_a$ and text $h''_t$ modalities as follows:\n$hoa_v = CrossAttention(h''_v, h''_a, h''_t)$\nSimilarly, the audio and text modalities are cross-attended using visual and textual embeddings, or visual and audio embeddings, respectively:\n$haa_v = CrossAttention(h''_a, h''_v, h''_t),$\n$hat_v = CrossAttention(h''_t, h''_v, h''_a).$\nEach cross-attended output $hoa_v$, $haa_v$, and $hat_v$ combines modality-specific features with contextual information from the other modalities. This step allows the model to simulate human-like sensory integration by combining complementary information across modalities."}, {"title": "Fusion Network", "content": "The cross-attended embeddings are concatenated into a unified representation:\n$f = [hoa_v, haa_v, hat_v]$\nThis fused embedding f is passed through a fully connected fusion network with ReLU activations and dropout layers. The network reduces the dimensionality to produce a single memorability score for each advertisement:\n$\\hat{y} = Sigmoid(Linear(ReLU(Dropout(f))))$\nThe final output $\\hat{y}$ is a scalar value between 0 and 1, representing the predicted memorability score."}, {"title": "Advertisement Regeneration", "content": "To examine the potential application MindMem, we generate more memorable advertisements for the commercial market. To achieve this, we gathered two types of information from the LAMBDA dataset (2,000 advertisements): (1) general video description and (2) scene-specific description. General video description represents details about the video itself, such as the brand name, advertisement orientation, advertisement pace, sentiment and audio description. Scene-specific features encompass detailed elements like scene descriptions, the emotion or mood of each scene, associated tags, dominant color theme, photography style, on-screen text, and the overall tone of each scene. Gemini Pro 1.5 was used to get those descriptions (prompt is shown in Appendix 1). We fine-tuned the LLaMA 3.1 (8B) (Dubey et al. 2024) model using titles and key messages from the advertisements as input. The output of the model was to generate detailed descriptions of the advertisements, closely aligned with the output structure of Gemini Pro 1.5. We refer to the model developed through this process as MindMem-ReAd, designed to enhance the creation of highly memorable advertisements. MindMem-ReAd generated textual descriptions of individual scenes, incorporating both general video and scene-specific features."}, {"title": "Result", "content": null}, {"title": "Performance of MindMem to Predict Advertisement Memorability", "content": "We trained MindMem models using the LAMBDA training set, constructing models with varying modalities (single-modal, dual-modal, and multimodal) and incorporating different advanced attention mechanisms in the multimodal models, such as multi-head self-attention for capturing intra-modal dependencies and multi-head cross-attention layers for integrating and aligning information across modalities. In total, we developed 11 models and compared their performance to predict memorability on the LAMBDA test set. Then, we further compared the performance of MindMem with those of other cutting-edge methods such as Henry (HariniSI et al. 2024), ViT-Mem(Hagen and Espeseth 2023), GPT 3.5 and GPT 4O (Achiam et al. 2023). As shown in Table 1, MindMem outperformed both single- and dual-modal models. MindMem achieved a Spearman's correlation coefficient (p = 0.631) with statistical significance (p-value = 1.26 \u00d7 10-13), improving p by an average of 21% compared to single-modal models and by 5% compared to dual-modal models. It also showed the smallest mean squared error (Mean Squared Error, MSE = 0.048), indicating strong correlation between predictive and actual memorability scores. Among the single-modal approaches, the text-based model performed best with p = 0.589 (MSE = 0.062). Single audio was not enough by itself to produce good results. For dual-modal models, the combination of textual and video information yielded the highest performance, with p = 0.615 (MSE = 0.053). Meanwhile, three single-modal models showed relatively lower performance underscoring the limitation of relying on a single modality for memorability prediction. These results support the importance of a multimodal approach in capturing the intricate dynamics of human memory, particularly in memorability prediction."}, {"title": "Content Factors Influencing Video Memorability", "content": "Next, we investigated the relationship between content factors and memorability on the LAMBDA samples in the test set. We found a positive correlation between predicted memorability and video pace (overall video speed, rhythm, tone, or flow at which the content of a video unfolds). Videos with a higher pace tend to be remembered for a longer duration by the audience. High-paced videos had an average memorability score of 0.672 \u00b1 0.221, whereas low-paced videos scored 0.499 \u00b10.229, reflecting about 30% lower memorability for slower-paced videos with a statistical significance (p-value = 8.32 \u00d7 10-4, one-way ANOVA test; Figure 3a). The number of scenes in an advertisement also exhibited a positive relationship with memorability. Advertisements with a greater number of scenes were remembered for longer durations by audiences (p-value = 5.12 \u00d7 10\u22125, one-way ANOVA test; Figure 3b). Interestingly, advertisements that evoked more emotions were significantly more memorable (p = 0.366, p-value = 1.29 \u00d7 10-7; Figure 3c).\nIn contrast, factors such as the orientation of the advertisement (portrait vs. landscape; Figure 3d), the advertisement's duration (Figure 3e), and the number of color themes (Figure 3f) showed an insignificant relationship with memorability scores (p-value > 0.05)."}, {"title": "MindMem Architecture Validation", "content": "To further evaluate the reliability of the MindMem architecture, we conducted experiments using the Memento10K dataset. Unlike the LAMBDA dataset, Memento10K features distinct characteristics, consisting of relatively short (3-second) natural videos. A total of 7,000 videos were used to train the MindMem model, and it was evaluated on a validation set of 1,500 videos. We observed that the MindMem architecture is stable and consistently delivers reliable prediction performance across various datasets. Specifically, in models based on Memento10K, MindMem achieved a Spearman's correlation coefficient (p) of 0.731 (MSE = 0.0055) when all three types of multimodal information were fed into the model (Table 3). The dual-modal model combining text and video information demonstrated similar performance (p = 0.728) to MindMem's. We suspect that the 3-second audio clips provide insufficient information for accurate memorability predictions. Indeed, single-modal approaches showed the lowest performance, with the audio-only model achieving a p of 0.291, making it the poorest performance among the three.\nWe also compared the performance of our model with other models that tested the Memento10K dataset. MindMem demonstrated superior results, outperforming the other methods (Table 4) and achieving an average of 1.3 times higher accuracy in predicting memorability."}, {"title": "Generating Memorable Advertisements", "content": null}, {"title": "Quantitative Evaluation of Advertisement Regeneration", "content": "We investigated the practical application of MindMem-driven memorability prediction by targeting the creation of more memorable advertisements for the commercial market. To achieve this, we developed MindMem-ReAd, a system built by fine-tuning the LLaMA 3.1 (8B) model to simulate advertisement content and predict memorability scores. We applied MindMem-ReAd to a set of 50 commercial advertisements. These videos were randomly selected from YouTube and represent 10 diverse industries, including food and beverage, technology and gadgets, beauty and personal care, health and wellness, fashion and apparel, automotive, entertainment and media, travel and hospitality, home and living, and finance and insurance.\nTo evaluate the effectiveness of MindMem-ReAd, we assessed both the original and the regenerated advertisements using our text-only trained model as an objective measure of memorability. By using the text-only model as a judge, we were able to predict memorability scores for the advertisements based solely on their textual content, allowing us to directly compare the impact of MindMem-ReAd on enhancing advertisement memorability.\nMindMem-ReAd improved overall 19.14% of memorability compared to original advertisements (Table 5). Of 50 tested advertisements, 16 had an original memorability score of \u2264 0.5, representing low-memorable advertisements that demonstrated an average improvement of 74.12%. Additionally, advertisements with medium memorability scores (0.5 < original memorability < 0.7) and high memorability scores (original memorability \u2265 0.7) showed improvements of 14.82% and 2.13%, respectively."}, {"title": "Case Studies", "content": "We provide a detailed analysis of two re-generated advertisements as case studies to demonstrate our approach. The evaluation focuses on four key metrics that assess whether the memorable advertisement holds greater marketing appeal or impact on general audiences: (1) memorability score predicted by the single-modal text model, (2) clarity, (3) visual impact, and (4) customer retention, assessed using GPT-01-preview and Perplexity.\ni. Advertisement #1: Technivorm Moccamaster Coffee Machine\nThe original version of Advertisement #1 achieved a memorability score of 0.19. In contrast, the MindMem-ReAd advertisement attained a significantly higher memorability score of 0.62, reflecting an improvement of more than 3-fold. According to the evaluation from GPT-01-preview and Perplexity, the MindMem-ReAd advertisement excels in clarity, visual impact, and its potential to enhance customer retention and engagement (Figure 4 and Appendix 2).\nii. Advertisement #2: Choice Hotels\nThe original advertisement achieved a memorability score of 0.23, while the MindMem-ReAd version excelled with a score of 0.46, marking an improvement of 2-fold. Feedback from GPT-01-preview and Perplexity commonly highlighted the MindMem-ReAd output for its effective use of dynamic visuals, clear textual messaging, and energetic audio, which successfully conveyed the brand's appeal to both business and leisure travelers, resulting in greater impact and broader retention (Figure 5 and Appendix 3). These results underscore the potential of MindMem not only to predict but also to create highly memorable advertising content."}, {"title": "Discussion", "content": null}, {"title": "Our Contributions", "content": "Deep learning algorithms and large language models have the potential to transform commercial advertisement generation by enhancing strategic interactions for advertisers within multi-agent settings. Focusing on the advertiser's role in these complex interactions, our research aims to optimize advertising strategies\u2014specifically in predicting and enhancing advertisement memorability. We introduce MindMem, a multimodal framework that utilizes advanced attention mechanisms on textual, visual, and auditory data to achieve high accuracy in predicting memorability, which is a key aspect of strategic communication. To demonstrate real-world applicability, we developed MindMem-ReAd, an LLM-driven system that optimizes advertisement content to enhance memorability and boost consumer engagement. This work bridges memorability prediction with practical multi-agent advertising strategies, highlighting the potential of generative AI to drive targeted and impactful marketing campaigns."}, {"title": "The Impact of Neuro-inspired Approaches On Predictive Performance", "content": "The incorporation of neuro-inspired mechanisms, particularly advanced attention models, has been instrumental in enhancing the predictive performance of MindMem. Drawing inspiration from human cognitive processes, these mechanisms enable the model to simulate how the brain selectively focuses on and integrates multimodal information, thereby improving its ability to predict advertisement memorability.\nIn the architecture of MindMem, we implemented multi-head self-attention pooling and cross-attention layers to capture both intra-modal and inter-modal dependencies. The self-attention pooling mechanism allows the model to weigh the importance of different elements within each modality's sequence, akin to how human attention selectively prioritizes certain stimuli over others. This is crucial for handling variable-length sequences and emphasizing contextually relevant features within the visual, auditory, and textual data.\nThe cross-attention layers further enhance this capability by enabling the model to align and integrate information across different modalities. This mirrors the human brain's ability to synthesize sensory information from various sources to form a coherent perception of an event or scene. By allowing each modality to attend to the others, the model captures complex interactions and dependencies that are essential to predict memorability, which is inherently a multimodal cognitive function."}, {"title": "Relationship between Video Dynamics and Memory Formation", "content": "We observed a positive correlation between memorability and dynamic content factors such as video pace and diversities of scene and emotion. The positive correlation between video pace and memorability aligns with recent research showing that faster-paced content can lead to better engagement and information retention (Murphy et al. 2022). Similarly, incorporating a greater number of scenes contributes to a faster-paced video, fostering sustained interest and offering more cognitive hooks to aid memory retention. We suspect that the relationship between emotional diversity and memorability represents a complex interaction of cognitive and emotional processes. It has been suggested that higher emotional diversity, characterized by the richness and balance of emotional experience is associated with improved cognitive functioning (Urban-Wojcik et al. 2022). This enhanced cognitive state would facilitate more effective memory encoding and retrieval processes. Meanwhile, traditional design elements, such as advertisement orientation, color themes, and advertisement duration, showed insignificant relationship with memorability, suggesting that content richness and emotional resonance would outweigh static structural features. These findings highlight the need for advertisers to prioritize dynamic and emotionally engaging elements over conventional design considerations to create more impactful and memorable advertisements."}, {"title": "Limitation", "content": "While our study demonstrates the effectiveness of neuro-inspired techniques in improving memorability prediction, several limitations remain. First, the size and variety of data, though substantial, may still limit the generalizability of our findings across different types of advertisements and industries. Additionally, our models rely on specific multimodal inputs, which might not capture other relevant factors like cultural context or individual-specific biases that could influence memorability. Future research require investigation of more diverse datasets and consideration of broader contextual factors, such as language variations, cultural diversity, or individual preferences. Additionally, integrating more advanced neuro-inspired mechanisms could further refine the model's ability to mimic human cognitive processes, potentially improving predictive accuracy and explainability."}, {"title": "Conclusion", "content": "In this study, we presented MindMem, a multimodal model designed to predict advertisement memorability. MindMem mimicked human cognitive processes, and significantly enhanced the model's ability to integrate visual, auditory, and textual inputs, leading to more accurate predictions compared to currently available other models. In addition, we developed MindMem-ReAd, a scalable method to generate memorable advertisements that achieved significantly higher memorability scores than their original versions. These findings underscore the potential of combining generative Al with cognitive modeling to optimize advertising strategies and enhance consumer engagement. Future work will focus on extending this framework by integrating more diverse datasets, applying it to practical advertisement content generation in multi-agent strategic settings, and exploring additional cognitive mechanisms to further improve model performance and broaden its applicability."}]}