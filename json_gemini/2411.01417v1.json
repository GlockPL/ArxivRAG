{"title": "BF-IMNA: A Bit Fluid In-Memory Neural Architecture for Neural Network Acceleration", "authors": ["Mariam Rakka", "Rachid Karami", "Ahmed M. Eltawil", "Mohammed E. Fouda", "Fadi Kurdahi"], "abstract": "Abstract\u2014Mixed-precision quantization works defining per- layer, per-network, per-channel, or per-parameter precisions for Neural Networks (NNs) are gaining traction for their efficient realization on the hardware leading to higher throughput and lower energy. In-Memory Computing (IMC) accelerator archi- tectures like PUMA [1], PipeLayer [41], and ISAAC [35], are offered as alternatives to traditional architectures relying on a data-centric computational paradigm, diminishing the memory wall problem, and scoring high throughput and energy efficiency. These accelerators can support static fixed-precision but are not flexible to support mixed-precision NNs.\nIn this paper, we present BF-IMNA, a bit fluid IMC accelerator for end-to-end Convolutional NN (CNN) inference that is capable of static and dynamic mixed-precision without any hardware re-configuration overhead at run-time. At the heart of BF-IMNA are Associative Processors (APs), which are bit-serial word-parallel Single Instruction, Multiple Data (SIMD)-like engines. We report the performance of end-to-end inference of ImageNet on AlexNet, VGG16, and ResNet50 on BF-IMNA for different technologies (eNVM and NVM), mixed-precision configurations, and supply voltages. To demonstrate bit fluidity, we implement HAWQ-V3's [53] per-layer mixed-precision configurations for ResNet18 on BF-IMNA using different latency budgets, and results reveal a trade-off between accuracy and Energy-Delay Product (EDP): On one hand, mixed-precision with a high latency constraint achieves the closest accuracy to fixed-precision INT8 and reports a high (worse) EDP compared to fixed-precision INT4. On the other hand, with a low latency constraint, BF-IMNA reports the closest EDP to fixed-precision INT4, with a higher degradation in accuracy compared to fixed-precision INT8. We also show that BF-IMNA with fixed-precision configuration still delivers perfor- mance that is comparable to current state-of-the-art accelerators: BF-IMNA achieves 20% higher energy efficiency compared to PipeLayer and 2% higher throughput compared to ISAAC.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep Learning (DL) algorithms, especially Convolutional Neural Networks (CNNs), have gained widespread popularity, playing a pivotal role in applications like computer vision, natural language processing, and recommendation systems [7], [10], [25], [39]. Deploying and scaling CNNs on resource- constrained edge devices amid Moore's law stagnation and the rise of IoT applications requires maximizing performance while minimizing computational overhead [13], [22], [42]. \u03a4\u03bf achieve this \"Pareto\" optimality, literature explores a mix-and- match plethora of techniques including hardware customiza- tion and algorithmic optimizations.\nHardware customization encompasses developing acceler- ator platforms that maximize efficiency and throughput. Ex- amples of State-of-The-Art (SOTA) specialized von Neumann architectures include the Tensor Processing Unit (TPU) which relies on systolic array architectures [17], Eyeriss which relies on two-dimensional spatial arrays of processing elements [4], and the architecture in [37] which has dedicated dual-range Multiply-Accumulate (MAC) blocks. Beyond von Neumann, PUMA [1], ISAAC [35], PipeLayer [41], and [43], [52] leverage In-Memory Computing (IMC)-based paradigms for CNN acceleration, relying on analog/digital computing. IMC is a data-centric computational paradigm that abolishes the memory wall problem suffered by Von-Neumann by allowing computation to be performed inside memory [28].\nMixed-precision quantization has recently emerged as an algorithmic optimization that enables a fast, low-memory footprint deployment of CNNs without compromising per- formance. With mixed-precision quantization, the model's weights/activations are quantized to a certain precision cho- sen from a set of possible bitwidths, in a way that the overall mixed-precision configuration yields high accuracy. The granularity of mixed-precision can be fine-grained (at the weight/activation level; each weight/activation can be assigned a precision) or coarse-grained (at the layer level; all weights/activations in one layer are assigned the same pre- cision, but precisions vary between layers). Mixed-precision CNN frameworks [34] are categorized based on their opti- mization techniques: 1-) gradient-based works like Bayesian Bits [44], 2-) Reinforcement Learning-based works like AutoQ [27], 3-) heuristic-based works like HAWQ-V3 [53], and 4- ) meta-heuristic-based works like APQ [47]. Several works concluded by calling for hardware-aware mixed-precision frameworks; capable of accommodating run-time changing requirements of the target platforms and call for \"hardware platforms capable of supporting dynamic mixed-precision, where the precision can be changed at run-time\" [34].\nAll of the aforementioned SOTA CNN accelerators were designed with fixed-precision (all weights/activations have the same precision across all models). Even if the same design rules/considerations of these accelerators were followed to accommodate for mixed-precision, those accelerators will end up supporting one mixed-precision configuration which is chosen statically at design time. This is not practical as different mixed-precision works have shown that the optimal configuration changes with the model [27], [46], [53]. More- over, such accelerators would not be able to accommodate changing requirements at run-time which would require a change in the mixed-precision configuration [2].\nWhile the list of hardware accelerators that support fixed- precision CNN inference is long, the literature currently lacks end-to-end hardware accelerators tailored for mixed-precision CNN inference, especially dynamic mixed-precision. SOTA mixed-precision CNN accelerators include Stripes [18] and BitFusion [36]. While Stripes is a bit-serial CNN acceler- ator that enables mixed-precision, it is not an end-to-end accelerator design but rather focuses on accelerating only convolution. BitFusion is a more recent, spatial, end-to-end CNN accelerator capable of bit-level fusion and outperforms Stripes [36].\nIn this paper, we propose BF-IMNA: A Bit Fluid In- Memory Neural Architecture for end-to-end full-fledged fixed and dynamic CNN acceleration. Bit fluidity is a term we henceforth use to describe the ability to enable mixed- precision inference. Unlike BitFusion, BF-IMNA relies on the IMC-based Associative Processor (AP) as its building block. The AP shown in Fig. 1 relies on a Content Addressable Mem- ory (CAM) to carry out massively parallel logical/arithmetic operations in a bit-serial word parallel manner [9], [20]. The bit-serial operation inherently supports mixed-precision without the need of dynamic reconfigurability. CAMs are IMC architectures utilized in several applications that require parallel searching like IP routing, text analytics, data mining, and data analytics [8], [19], [54]. APs perform arithmetic and logical operations by a series of search/write stages following the operation's Look-Up Table (LUT) applied in a bit-serial word-parallel manner (i.e. between pairs of columns of the CAM). An extension of the AP is a 2D AP proposed in [51] which provides the capability of performing logical/arithmetic operations between pairs of rows in addition to being able to perform those between pairs of columns. The 1D and 2D APs are the only building blocks of BF-IMNA: we use 1D APs as Memory APs (MAPs); the on-chip memory storage, and we use 2D APs as Computation APs (CAPs), the on- chip computational units capable of performing any CNN operation. The architecture of BF-IMNA is shown in Fig. 3. Our contributions are below.\n1- We formulate the complexities of performing CNN-related operations such addition, multiplication, ReLU activation, av- erage pooling, and max pooling on the 2D AP, taking into consideration the data movements.\n2- We map the end-to-end inference operations into AP blocks, and we present, based on different hardware configurations, two BF-IMNA designs: one that offers maximum parallelism and another with limited resources.\n3- We develop an in-house simulator for BF-IMNA that estimates the performance metrics (energy, area, latency) of end-to-end inference of ImageNet on AlexNet, ResNet50, and VGG16. Then we carry out a design space exploration for different CAM cell technologies (SRAM/ReRAM), different per-layer mixed-precision configurations, and different supply voltages.\n4- We demonstrate BF-IMNA's bit fluidity by implementing HAWQ-V3's [53] per-layer mixed-precision configurations for different latency budgets. The different mixed-precisions reveal a trade-off between accuracy and Energy-Delay Product (EDP).\n5- We compare BF-IMNA using fixed-precision configurations to SOTA accelerators. Our results demonstrate that BF-IMNA achieves comparable results even at a high fixed-precision (16 bits) where APs start to lose their advantage: 1.02\u00d7 higher throughput with 3.66\u00d7 lower energy efficiency and 1.19\u00d7 higher energy efficiency with 2.95\u00d7 lower throughput compared to ISAAC and PipeLayer.\nThe rest of the paper is organized as follows: Section II provides background, and section III elaborates on BF-IMNA. Sections IV and V present the implementation details, results, and discussion. Section VI concludes the paper."}, {"title": "II. BACKGROUND", "content": "CNNs, a subset of DL widely used in computer vision [24], typically incorporate convolutional, pooling, and fully con- nected layers. Convolutional layers perform high-dimensional convolutions on input feature maps using weighted filters. After convolution, an activation function, such as ReLU, intro- duces non-linearity to generate the layer's output feature map. Pooling layers (max and average) employ sliding filters with a zxz size and a stride of St. Max pooling selects the maximum data point in the window, while average pooling calculates the mean. Fully connected layers involve linear matrix-vector- multiplication connecting all inputs to all outputs."}, {"title": "B. Two Dimensional Associative Processors", "content": "The hardware architecture of a 2D Computational AP (CAP) is depicted in Fig. 3, comprising a 2D CAM for data storage whose building block cells are either SRAM-based or ReRAM-based. Note that compared to the cells of 1D APs (MAPs), the cell designs in 2D APs incorporate an additional nmos/pmos device for each ReRAM/SRAM, controlled by vertical search lines (S(V)s) to enable vertical search.\nArithmetic/Logical Operations: An arithmetic/logical oper- ation is performed in the 2D AP by following a sequence of compare-write stages applied to the CAM's data, following the operation's LUT. Particularly, the controller receives log- ical/arithmetic operations to be performed on the data stored in the 2D CAM and translates those into key-mask values, which will be used in the compare-write stages following the operation's LUT. The key register stores the value that is written or compared against, and the mask register indicates bit(s) that is(are) activated during comparison or write. The compare stage is a search applied to the CAM's data. To facili- tate search, each row/column terminates in precharge circuitry, and a capacitor that feeds into a sense amplifier connected to the tag. Tag registers indicate matched rows/columns during a search and facilitate bit-sequential/word-sequential reading. The 2D AP supports vertical and horizontal search, where search drivers drive cells of interest.\nReading Modes: Two reading modes are supported in the 2D AP: bit-sequential and word-sequential. For bit-sequential reading, all columns are masked except the one of interest, then the key corresponding to that column is set to \"one\", which is then used by the corresponding search driver to drive the search lines of that column. Since cells in the column of interest match with the input \"one\" if they store one (hence setting the corresponding tag registers) or mismatch with it if they store a zero (hence resetting the corresponding tag registers), the tag registers will eventually hold the values that are stored in that column. The read column (now stored in the tag) can be used to rewrite another column in the CAM (via the write driver) or to be passed to a general-purpose register with the help of the interconnection interface. Word-sequential reading is done via a search operation similar to bit-sequential mode, but relying on the vertical search lines, vertical key and mask registers, and the horizontal tag registers.\nWriting Modes: Writing occurs in bit-sequential or word- sequential mode, with a two-cycle requirement per writing a row/column. The 2D-AP presented in [50] enables vertical segmentation, treating each group of rows as a separate 2D AP. Segmentation reduces operation complexity and offers energy efficiency but limits hardware programmability. To render our design more general-purpose, instead of using vertical segmentation, we assume that we can perform operations on multiple 2D APs that operate in parallel."}, {"title": "C. General Matrix Multiply", "content": "In our proposed architecture, a convolution is mapped onto the APs using General Matrix Multiply (GEMM), which implements the convolution by transforming one input matrix into a Toeplitz matrix. This transformation involves replicating image pixels across matrix columns, and the convolution is executed through a general matrix multiplication approach known as \"im2col\", widely utilized in DL frameworks like Caffe and Torch [5], [16], [45]. Given an input size of {height, width, channels} = {$H_1,W_1,C_1$} and Ck kernels each of {height, width, channels} = {$H_K,W_K,C_1$}, the \"im2col\" technique constructs an \"input-patch\u201d matrix, P, by unrolling patches from the input into columns. The size of P is ($H_K * W_K * C_1$) \u00d7 ($H_O * W_O$), where $H_O = (H_1 - H_K +2* (\\#ZeroPaddings))/Stride +1$ and $W_O = (W_1-W_K+2*(\\#ZeroPaddings))/Stride+1$. A \"kernel- patch\" matrix, K, is formed by unrolling each of the \u0421\u043a kernels of shape k \u00d7 k\u00d7 C into a row. The size of K is CK \u00d7 ($H_K * W_K * C_1$). Performing the general matrix multiplication K \u00d7 P yields an output matrix, O, of size \u0421\u043a\u0445 ($H_O * W_O$)."}, {"title": "III. BF-I\u039c\u039d\u0391", "content": "BF-IMNA comprises cluster(s), each containing multiple CAPs and one MAP as shown in Fig. 3. MAPs communicate with both CAPs and off-chip memory via an on- and off- chip interconnect, respectively. We map (based on GEMM) workloads into BF-IMNA. We note that the mapping does not change for different precisions: all precisions map the same way to the hardware, and for lower precision, Most Significant Bits (MSBs) are deactivated for energy savings. We study two configurations of BF-IMNA: maximum parallelism and limited resources, that we elaborate on next.\nMaximum Parallelism: Here, we aim to achieve full spatial dimension computation unrolling, utilizing Infinite Resources (IR) to exploit maximum intra-layer parallelism. In this configuration, a single large cluster houses all required CAPS for computing the largest layer in one step, along with a sufficiently large MAP for streaming inputs to CAPs through an on-chip mesh interconnect. Configuring the accelerator size is based on the dimensions of the convolutional layer with the highest number of multiply-accumulates (MACs). We transfer all weights offline from off-chip memory to CAPs before inference. BF-IMNA assumes a sufficiently large on- chip memory (MAPs) to store the model weights, which avoids off-chip memory accesses that significantly increase energy consumption, particularly in the Limited Resources (LR) set- ting (see below). The area reported in Table V includes enough on-chip area to store the weights of our largest studied model (VGG16). During inference, input streaming involves moving data from off-chip memory to MAP via off-chip interconnect and then to CAPs via mesh for processing. Between layers, two data movements are necessary: 1) streaming current layer weights and 2) rearranging outputs of one layer to serve as inputs for the next layer. The MAPs act as intermediate storage for rearranging CAP outputs and managing streaming weights to CAPs. To reshape the output of a layer in preparation for the next layer, we perform the following steps: 1- outputs are read in word-sequential mode from CAPs, then 2- transferred over the bus to the MAP, where they are 3- written in word-sequential mode, in consecutive rows. At this point, the outputs of the current layer are reshaped. 4- The reshaped outputs are read from the MAP, 5- transferred over the on-chip bus, and 6- written back to the CAPs in the rearranged format word-sequentially. All reshaping overheads are factored into our results. Additionally, the latency of writing input/weights and intermediate outputs in the MAP is hidden by data transfer through the mesh.\nLimited Resources IR-based mapping is resource-intensive. A more practical approach restricts the accelerator size to avoid an expensive design. In this configuration, we use 4096 CAPs, organized as an 8 \u00d7 8 CAPs within each cluster of the 8 x 8 clusters. We chose these numbers because our experiments have shown such a configuration achieves nearly 100% hardware utilization while striking a balance with time- folding overheads. Fig. 3 illustrates a smaller version of this hardware configuration, where clusters operate independently and in parallel, eliminating the need for interconnect between them. Although an interconnect is necessary between off-chip memory and MAPs inside each cluster, its cost is omitted from our results section, as there is sufficient on-chip memory to program all weights in MAPs offline. Due to insufficient com- putational resources for computing an entire layer output in a single step, we use a weight stationary mapping, performing GEMM over multiple time steps. The weight matrix streams from MAPs onto CAPs via the mesh and remains stationary in time as new columns from the input matrix are introduced at every step. Particularly, if the weight kernel Ki in layer i exceeds CAP capacity, we fold the mapping in time. Each clus-"}, {"title": "B. AP Modeling", "content": "We formulate the runtime models of performing some functionalities on the 1D and 2D AP (with and without segmentation). We divide the models into three categories: micro functions", "Functions": "Micro functions comprise preliminary operations whose models will be used to build the models of macro functions (below). We focus on three such functions: addition", "reduction.\nAddition": "Given two vectors A = [$a_1", "a_{L/2}$": "and B = [$b_1", "b_{L/2}$": "performing the in-place addition A + B = B on the AP requires performing the following steps: 1-) popu- lating the AP with data by performing 2M column writes: bit- sequential mode is assumed for purposes of modeling as the number of rows is assumed to exceed the number of columns and hence a word-sequential write mode is inefficient. 2-) Performing the addition by following the series of compare and write as dictated by the corresponding LUT (can be found in [50", "pass.\nMultiplication": "Similar to addition", "operands.\nReduction": "As for reduction", "a_1,a_2,..a_L$": "the reduction operation adds all of the elements of the vector", "steps": 1, "50": "to get the sums $a_1 + a_2 = a_4"}, {"steps": 1, "Functions": "We define a macro function as one that relies on more than one micro function. Matrix-matrix multiplication and dot product are macro functions of interest. Matrix-matrix Multiplication: Matrix-matrix multiplication comprises multiplying an i\u00d7 j matrix by an j \u00d7 u matrix. As dot product is a special case of matrix-matrix multiplication where i = u = 1"}, {"steps": 1}, {"steps": 1, "mode": "depending on the sizes of the matrices and type of 2D AP", "Functions": "CNN functions are specific to CNN inference: average pooling", "Pooling": "Assuming a window size of S and K pooling operations", "a_{L/2}$": "and B = [$b_1", "b_{L/2}$": "."}, {"steps": 1}, {"steps": 1, "Pooling": "For max pooling", "a_{L/2}$": "and B = [$b_1", "b_{L/2}$": "max pooling in the 1D AP is performed by following five steps: 1-) populating AP bit-sequentially (in 2M column writing steps), 2-) Performing the sequences of compare/write of the corresponding LUT that we have formu- lated in Table IV once in the horizontal direction (again here we refer to the sequence of all compares and writes applied to all column pairs as one in-place max pooling operation), and 3-) Resetting the flag bits (two additional columns written). 4- ) Transfer the words (one transfer is one read and one write) in word-sequential mode. Steps 2-), 3-), and 4-) are repeated as needed (words are transferred K * (S/2 \u2013 1) times and in-place max pooling is applied $log_2(S)$ \u2013 1 times). 5-) The resulting words are read in bit sequential mode (M reads). In 2D AP (with and without segmentation), max pooling is performed by 1-) populating AP with data, 2-) performing the max pooling operation in horizontal mode, 3-) resetting the flags, 4-) performing max pooling in the vertical mode. Steps 2-), 3-), and 4-) are repeated as needed (K*(S/2-1) times and $log_2(S/2)$ in cases of 2D AP and 2D AP with segmentation respectively). Finally, 5-) the resulting words are read in bit sequential mode. Below are the run times of max pooling for 1D, 2D AP, and 2D AP with segmentation respectively.\n$r_{tmaxpool1D} =(2M)_{write}+$\n$log_2(S) * ((4M)_{compare} + (4M)_{write} + 2_{write})$\n+ $(1_{read} +1_{write}) * K * (S/2 - 1) + M_{read}$\n(12)\n$r_{tmaxpool2D} =(2M)_{write} + (4M)_{compare} + (4M)_{write}$\n+ K * (S/2 - 1) * ($4_{compare} + 4_{write} + 2_{write}$)\n+ $M_{read} + 2_{write}$\n(13)\n$r_{tmaxpool2DwSeg} =(2M)_{write} + (4M)_{compare} + (4M)_{write}+$\n$"}]}