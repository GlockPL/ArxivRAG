{"title": "Interplay between Federated Learning and Explainable Artificial Intelligence: a Scoping Review", "authors": ["Luis M. Lopez-Ramos", "Florian Leiser", "Aditya Rastogi", "Steven Hicks", "Inga Str\u00fcmke", "Vince I. Madai", "Tobias Budig", "Ali Sunyaev", "Adam Hilbert"], "abstract": "The joint implementation of Federated learning (FL) and Explainable artificial intelligence (XAI) will allow training models from distributed data and explaining their inner workings while preserving important aspects of privacy. Towards establishing the benefits and tensions associated with their interplay, this scoping review maps those publications that jointly deal with FL and XAI, focusing on publications where an interplay between FL and model interpretability or post-hoc explanations was found. In total, 37 studies met our criteria, with more papers focusing on explanation methods (mainly feature relevance) than on interpretability (mainly algorithmic transparency). Most works used simulated horizontal FL setups involving 10 or fewer data centers. Only one study explicitly and quantitatively analyzed the influence of FL on model explanations, revealing a significant research gap. Aggregation of interpretability metrics across FL nodes created generalized global insights at the expense of node-specific patterns being diluted. 8 papers addressed the benefits of incorporating explanation methods as a component of the FL algorithm.\nStudies using established FL libraries or following reporting guidelines are a minority. More quantitative research and structured, transparent practices are needed to fully understand their mutual impact and under which conditions it happens.", "sections": [{"title": "I. INTRODUCTION", "content": "The development of trustworthy AI systems depends on multiple different ethical principles like privacy preservation and explicability [1], [2]. Ensuring that AI systems address such principles is crucial for maintaining trust, compliance, and ethical standards. Such principles are critical in data-sensitive applications such as banking [3] and healthcare [4], [5].\nPrivacy concerns are the main reason why healthcare data for AI models, such as patient outcome prognosis, mainly originate from single institutions. Anonymizing extensive healthcare data effectively is challenging due to the high risk of re-identifying patients [6]. Allowing models to be trained across multiple institutions without sharing raw data, for example, with federated learning (FL), addresses this issue. FL has gained prominence as a solution to train models from distributed data while preserving these privacy aspects [7], [8]. Specifically, FL enables the training of machine learning (ML) models from distributed data that are available to different users or institutions, without data being explicitly transferred. Therefore, FL solves certain privacy-related governance issues as it restricts access to sensitive information from each institution while still gaining insights from all available data [9], [10]. Using data from different institutions can lead to more diverse training data, which reduces the bias of ML models and improves generalizability compared to models trained on homogeneous data [11]. The three main categories within FL are horizontal FL (HFL), vertical FL (VFL), and federated transfer learning (FTL). HFL involves data scattered across the sample space, where multiple parties have different subsets of data points but all data have the same feature space. VFL allows learning from data distributed across different different feature spaces for the same data sample. FTL focuses on transferring knowledge across different federated learning settings [12].\nAnother ethical principle often advocated by ethicists and policymakers in high-risk AI domains such as healthcare is explicability. This means that: a) processes must be transparent, b) the capabilities and purpose of AI systems must be openly communicated, and c) one must be able to explain the decisions and predictions made by AI systems to those directly or indirectly affected by them [1], [2]. When a model's inference or decision-making process is transparent and understandable, it is more likely to be trusted and accepted by stakeholders, such as customers, regulators, and users [13]. As stated by the European Union's regulations, aspects (a) and (c) of explicability, whose technical manifestation is known as explainability [14], need to be provided through additional information next to the performance of the AI system on how an AI system arrives at a certain output [15]. The concept of explainable artificial intelligence (XAI) aims to explain the inner workings of ML algorithms and increasing model comprehension. XAI is thereby concerned with the development of algorithms and methods that achieve explainability [16]. Explicability can be achieved by either training interpretable ML models, or by using post-hoc explanation methods (i.e., (a) and (c) respectively regarding the European Union guideline).\nUnfortunately, XAI literature has not agreed on one definition of explainability. The terms \u201cexplainability"}, {"title": "II. RELATED WORK", "content": "Review papers on the fields of XAI and FL separately are very common, and there are even meta-reviews [25] and systematic reviews from different points of view (see, e.g. [26], [27] regarding FL applications for biomedical data). An extensive review on XAI methods can be found in [16] or [28]. Even though, there are many fewer reviews concerning the joint application of XAI and FL, none analyzes the interplay in depth, which motivates our work."}, {"title": "A. Reviews about FL and XAI", "content": "The review in [29] introduces Fed-XAI, which involves learning interpretable models in federation, in addition to explanation methods applied on any federated trained model. They used [18] as a source for XAI taxonomy and highlight a general diversity in definitions used in the studied literature. Additionally, they gave an overview of the \u201ccurrent status in Fed-XAI\" by discussing a set of works relating to the two concepts with no analysis on the interplay between FL and XAI.\nThe review in [30] gathers relevant works concerning the \"FED-XAI\" concept, defined by them as a discipline \"which aims to bring together these two approaches into one\u201d. Differently from our search terms, which include variations of the words \"explanation\u201d and \u201cinterpretability", "FED-XAI": "returning a reduced number of publications. The authors do not provide a comparative analysis of the interplay between FL and XAI among the found papers.\nThe paper [31] surveys \u201cinterpretable federated learning"}, {"title": "B. Contribution-aware FL", "content": "A small but salient subset of the approaches investigating the intersection of FL and XAI has utilized feature relevance methods (e.g. Shapley values) to measure the contribution of each client participating in a FL process. These approaches, commonly referred to as contribution-aware FL, aim to incentivize participants to engage during model training and to fairly distribute rewards based on contributions. One of the first approaches to contribution-aware FL suggested using Shapley values to interpret contributions in FL networks in [32]. The contributions are diverse, ranging from node liability [33] to biases within data sets [34]. Recent approaches have also extended SHAP-based [20] contribution determination to provide visualizations to evaluate data privacy [35] and to improve the reliability of prediction via clustering patients [36]. Such methods do not necessarily constitute an influence of XAI on the FL process and are therefore not central to the present review."}, {"title": "III. METHODS", "content": "The present scoping review addresses the concept of joint application of FL and XAI and their potential interplay. Two research types have been addressed, namely the methodological advances and the experimental results.\nWe aimed at examining the range and nature of research activity on the topic, summarizing the research findings, and identifying research gaps, following a standard scoping review methodology [37]. The presence of two subconcepts of explainability (interpretable models and post-hoc explanation), the two research types (methodological and experimental), and the diversity of joint approaches to FL and XAI increased the complexity of the study. Understanding and summarizing the diverse approaches required a deeper analysis of each included paper. A more detailed description of the methodology is available in the pre-published research protocol [38].\nIn our survey, the time frame of the published papers was between 2019 and April 2023 (date of protocol pre-publication). No relevant papers were found before 2019."}, {"title": "A. Research questions", "content": "The research questions of this work address both the training of interpretable ML models using FL, and the explanation methods applied to ML models trained via FL. Furthermore, the questions were categorized into two groups:\n1) Methodological advances:\nWhich interpretable ML models can be trained via FL?\nWhat are existing methods to train interpretable ML models via FL?\nAre there explanation methods that take into account that the ML model was trained via FL?\nHas any work proposed an FL method that takes into account the ulterior application of explanation methods?\nFor what subtypes of FL have methods been proposed to a) learn interpretable ML models; b) explain the outputs of ML models?\n2) Experimental results:\nIn which contexts (fields of application) have results been obtained regarding FL and XAI?\nUnder what conditions does training a model via FL affect a) its interpretability? b) the obtained explanations?\nHas any work compared the effects of FL on the interpretability of the resulting models against a centralized learning algorithm?\nHas any work compared the explanations obtained from a model trained via FL against a centralized learning algorithm?\nWe used the following search engines: Google Scholar, IEEExplore, PubMed, Scopus, and Web of Science. The terminology issue described in the introduction influenced the search strategy, which aimed at identifying all papers mentioning FL jointly with a word related to either interpretability or explainability."}, {"title": "B. Inclusion/exclusion criteria", "content": "We initially identified all research articles published in peer-reviewed conferences and journals and pre-published works made available in preprint services such as arXiv over the past three years. We required that the full text was available.\nTo determine their inclusion in the review, papers had to either discuss in depth the relation and interaction between XAI and FL, or apply both technologies jointly in a practical setting. To rule out papers that mentioned both technologies without detailed discussion or practical application, we performed a screening procedure, in which the article must answer positively to at least one of the following screening questions:\nSQ1) Does the paper propose a novel method integrating FL and XAI?\nSQ2) Does the paper report results from experiments applying FL and XAI in a real dataset?\nSQ3) Does the paper discuss or assess the impact of FL on explanations or model interpretability?\nThe exclusion criteria were defined as follows. Theses were excluded. Reviews/meta-reviews/surveys were excluded. Preprints published more than 3 years before the protocol prepublication, for which a final published version did not exist, were excluded."}, {"title": "C. Extraction", "content": "To facilitate efficient and parallel data extraction, we utilized a shared online spreadsheet. All authors were involved in the data extraction which allowed us double-coding of all information. The extracted items were detailed in the protocol [38], and a summarized listed is provided here:\nStudy identification: title, authors, publication year, publication outlet.\nNomenclature: whether the paper provides or cites a definition of explainability, and whether the nomenclature regarding explainability coincides with the one introduced here.\nNature of the paper: whether it is general/theoretical or applied; if applied, what field it is applied on, and whether the proposed idea is practically validated in the field of application.\nData characteristics: information modality, type and amount (number of unique data samples) of the data used in the experiments.\nFL-specific characteristics: type of FL (HFL, VFL, FTL) used, setup (simulated or not, number of data centers), and which FL library is used (e.g., FLWR, openFL).\nXAI-specific characteristics: whether an interpretable model or an explanation method is used\u00b9, its type, and the specific explanation method (e.g.: SHAP, GradCAM [39]) or way of interpreting the model (e.g., weights at the first layer of DNN).\nInterplay between FL and XAI: a paragraph summarizing how the paper deals jointly with FL and XAI; influence of FL on explanations or interpretability (e.g. significant differences in variable importance ratings between centrally learned and model trained via FL); influence of XAI onto federated training (e.g., modified merging step in central server after collecting locally trained models); and whether this influence is quantified, and how.\nMethodology notes: how the novel method is designed (e.g., optimization-based approach), and how rigorous the methodology is.\nDuring both the screening and the extraction process, double coding was practiced. Each screened paper was independently assessed by two different authors, and answers to the extraction points for each of the included papers were obtained by two different authors, resolving differences by discussion between the two involved authors; disagreements were solved by discussion within the entire author consortium. While extracting, we distinguished between explanation methods and model interpretability according to the nomenclature above introduced, even though some papers did not agree with the latter."}, {"title": "IV. INTERPLAY BETWEEN FL AND XAI", "content": "This section explores studies that have either integrated explainability into FL methods or analyzed how one technology influences the other. Studies that jointly deal with FL an XAI but do not necessarily report an influence are discussed in the Appendix. Here, we first discuss instances where FL has impacted post-hoc explanations (Sec. IV-A). Next, we examine works where explanation methods have influenced FL training as a design step in the FL process (Sec. IV-B). Lastly, we cover studies where FL has altered model interpretability, or the implementation of interpretable models has influenced FL training (Sec. IV-C)."}, {"title": "A. FL impacting XAI", "content": "Among all papers reviewed, only [41] quantitatively ana-lyzes the impact of FL on explanations. Additionally, [42]"}, {"title": "B. Explanation method impacts FL training", "content": "Among the papers that reported that the use of an explanation method impacts the FL training, a salient subset uses XAI to defend the FL process from the negative effects of defaulting nodes [33], malicious behavior from certain FL nodes [52], [53], and instances of the GAN attack in FL [54]. Other positive effects reported include improved accuracy in [55], [36], [56] and enhanced learning efficiency [57]. It must be noted that such benefits stem from incorporating XAI as a component of the FL algorithm design.\nWith the aim of detecting malicious attacks on FL operations, [53] uses a random forest (RF) to identify features causing incorrect predictions. Each participant trains both a DL and an RF model on their training data. For samples that are misclassified, it calculates the feature importance of each feature regarding incorrect classification, from all decision trees and using LIME [49]. The change in a feature's importance over time is used to assign the contribution of each feature in misclassifying the data. Such an importance value provides insights into the level of influence of each log key in a sequence during an attack, thereby indicating which features should be most protected.\nA novel FL protocol was proposed in [36] where a subset of all centers available online were selected to participate in each FL round, using Shapley values (computed using SHAP) as a heuristic to estimate FL contributions from each client. More specifically, at each round, the difference between local (feature-wise) and global aggregated Shapley values at each node is used to select participating parties in an FL process. The paper reports that the proposed method improves both the efficiency and accuracy compared to the FedAvg protocol [10], which does not consider client contributions.\nThe idea in [52] is to use explanation methods (specifically Backpropagation, GuidedBP, DeepLift, GradCAM [39], and Integrated Gradients) to detect whether each participant is using malicious data to enact a backdoor attack. To this end, so-called detection filters are developed. These consist of a classifier and an explanation method that respectively identify a likely backdoor attack and triggering features in the input data. The effectiveness of various explanation methods with different classifiers is tested, strengthening the FL process against backdoor attacks. Upon testing the detection accuracy in the presence of variable proportions of backdoor attacks, the proposed methodology proves the usefulness of the different explanation methods for backdoor attack prevention.\nThe technique proposed in [56] uses Shapley values and the Lipschitz constant to generate both local (feature-wise) and global explanations. Local Shapley values are compared with global Shapley values to refine the training of the local model, ensuring that only necessary characteristics are retrained, which allows for the personalization of the FL model for each user so that only the necessary characteristics of"}, {"title": "C. FL impacting model interpretability and vice versa", "content": "This subsection examines the relationship between Federated Learning (FL) and model interpretability. The featured studies, [59] and [60], discuss both the benefits and challenges of interpretability in distributed learning environments. These examples illustrate the trade-offs between model complexity and transparency that are inherent in FL setups.\nAn aggregation of client-based attention weights is investigated in [59] for a threat-detection task in a cloud scenario. Using system logs as input data, each client predicts cyberattacks and computes local attention weights, which are claimed to enhance interpretability. The central server subsequently aggregates these attention weights to build a saliency map that provides insights into the impact of the different log keys on the threat prediction. The influence of FL on model interpretability is assessed by comparing insights and interpretations from attention mechanisms for individual cyberattacks per client, individual attacks per aggregated models,"}, {"title": "V. RESULTS", "content": "In this section, the information extracted from the papers included in the review is presented in a graphical form, to provide an overview of the different fields of application, explainability techniques, FL setups, and data types and modalities.\nFig. 3 shows the proportion of papers across the different fields of application, where medicine and life sciences are pre-dominant (40.5%), followed by finance (16.2%), cybersecurity (13.5%), telecommunication (8.1%) and optimization in other engineering fields (8.1%) such as electricity, mechanics, and transportation. A small proportion (13.5%) of the analyzed papers focused on the theoretical side of the ML techniques without mentioning any specific application.\nThe proportion of interpretability and explanation methods is shown in Fig. 4 and described in Tab. II. The most frequently used type of interpretability of FL models is algorithmic transparency, with some use of decomposability and simulatability [18]. While algorithmic transparency allows users to algorithmically trace a model's processes, decomposability describes models of which fractions can be understood by humans. Simulatability refers to models that are sufficiently interpretable for a human to understand, or \"simulate\", as a whole [18]. Among the explanation methods applied with FL models, feature relevance is predominant, followed by local explanations, and a smaller amount of model simplification-based methods [18]. Feature relevance methods quantify the impact of the model's input features, local explanations explain specific model predictions, and simplification refers to rebuilding the entire model for easier explainability."}, {"title": "VI. DISCUSSION", "content": "In our sample of papers jointly investigating FL and XAI, we found mostly post-hoc explanation methods. The application of explanation methods on FL systems is predominantly based on feature importance followed by local explanations, reflecting the prevalence of such methods in the literature.\nNone of the reviewed works have proposed an ad-hoc FL method tailored to the ulterior application of explanation methods, highlighting the need for future approaches in that domain. Explanation methods designed for federated trained ML models were limited to feature aggregation [41], control of information sharing [42], and counterfactual explanations in VFL [77]. The works dealing with FL of interpretable models focused mostly on algorithmic transparency, and in the case of [60] the proposed method learns a set of interpretable rules that reflect the structure of the FL network. Interpretable models trained with FL methods were limited to fuzzy rule-based systems [60], time-series classifiers [71], SVM [78], Cox proportional hazards [79], sparse Bayesian models [80], and decision trees [69]. Fewer works deal with decomposability or simulatability, probably due to an increased difficulty in imposing those properties in an FL system.\nRegarding novel methods to train interpretable ML models via FL, one very promising approach allows building federated classification models without relying on gradient descent-based methods [67]. This differs from most methods in the current literature, which focus on differentiable models. The approach of [67] is based on adapting previously existing algorithms in the boosting family to FL. Other approaches proposed an optimization method to solve the sparse SVM problem in FL [78], a novel technique called Vanishing Boosted Weights [81], or an FL network based on gradient-boosting decision trees [75].\nA preference for HFL is observed in the studies surveyed, while combinations of FL types and the use of TFL are relatively rare. This is in accordance with the observed higher prevalence of HFL in the literature compared to VFL [82], [83]. Moreover, most papers apply cross-silo FL with a small set of data centers. Whether this is due to data access or this is an accurate representation of FL networks remains unclear.\nIn general, experiments on the effect of FL on interpretability were very sparse. [60] observed an influence of FL on model interpretability in the FRBS trained using their own method, namely a larger number of model rules. Similarly, in [59], by interpreting the attention weights produced by transformer models based on a transformer architecture [84], different insights for each client were observed.\nThe fact that a majority of the reviewed papers focus on healthcare, finance, and engineering applications such as networking, highlights a high degree of interdisciplinarity and suggests cross-fertilization among fields. Conversely, this contrasts with the relative lack of studies addressing high-stakes applications in social networks, language models, and supply-chain management, where user privacy and transparent decision-making are also crucial. The need to define explicability and privacy requirements usually originates from the end users' perspective, where data interoperability and reasoning behind automated decisions are key. The implementation of enabling technologies originates research towards using XAI for more technical goals such as improving model accuracy, assessing the training process, and prevent malicious behavior, as observed in Sec. IV-B.\nIn several reviewed cases, aggregating interpretability metrics from different nodes led to more general but less interpretable global insights. This fact supports the idea that FL promotes generalizability at the cost of interpretable insights from the local nodes of the FL framework. More specifically, aggregating feature relevance or attention weights across nodes can yield more generalized global insights but often sacrifices some degree of localized interpretability.\nThis was reported in the case of an attention mechanism [59] where each client trained locally on system log data and generated local attention weights. The global saliency map resulting from aggregating the local attention weights from the different nodes, provided a global saliency map where the contribution of individual client models is diluted, possibly leading to unique local patterns getting lost during global aggregation. The FRBS applied by [60] in a federated context involves rule generation at each data silo and merging them at the central server. Experiments observed an increased number of rules in the global model compared to local ones, indicating a growth in model complexity upon aggregating the local models, yielding a more expressive global model that is less interpretable than individual local models.\nThis result suggests that while FL can improve overall model performance and integrate information from diverse data sources, outputs from the global FL model could become more difficult to interpret than those from local models. This may negatively affect applications where the transparency of the local models is beneficial. One example is medical prognosis, where each local model is associated with a clinical site or hospital, and the distributions of disease features vary across hospitals. This issue could benefit from new aggregation strategies or federated explanation techniques that retain node-specific insights without compromising global model integrity.\nDespite the prevalence of SHAP among the included studies, we did not find any mention of a potential federated implementation of SHAP when the supporting dataset is distributed among several data centers. Such a contribution would help exploit the representativeness of supporting data from diverse centers, potentially helping generate more accurate Shapley values. Among the studies using small amounts of data, very few reported an influence of FL on XAI, suggesting"}, {"title": "VII. CONCLUSION", "content": "This review highlights the interplay between FL and XAI, both methodologically and experimentally. We have identified a research gap in which few studies quantify the impact of FL on explanations. Moreover, the impact of FL on model interpretability and explanations remains unclear, revealing the need for studies that quantify such impact. There is a need for rigorous experimental and analytical research to assess how FL training influences the structure of the model and its implications for explainability and interpretability. Additional research should also provide guidance for practitioners on the responsible implementation of FL with XAI, particularly in critical fields such as healthcare, finance, and engineering. This guidance will help ensure responsible deployment of AI systems that balance model performance with due transparency.\nMany papers do not use consistent terminology for explain-ability, which complicated the analysis. It is important for future research to clearly define terms and use standardized nomenclature. Another important finding is that a minority of the studies specify which FL libraries were used. Future pub-lications should include details on the libraries employed or provide source code for custom implementations. Furthermore, the lack of adequate reporting of data characteristics calls for strict adherence to reporting guidelines. These are important improvements towards ensuring reproducibility, transparency, and auditability, which are crucial ethical aspects in sectors such as medicine.\nIn conclusion, this review underscores the need for more structured and transparent research practices in the intersection of FL and XAI. Establishing clear definitions and consistent methodologies will be key in advancing the field and address-ing the identified gaps. As demand for FL and XAI continues to grow, particularly in high-stakes environments, the importance of rigorous, transparent, and reproducible research cannot be overstated."}, {"title": "APPENDIX A", "content": "In this section, we describe how each selected paper deals with FL and XAI in combination. The first part of the section discusses works where FL is combined with explanation methods, both methodological (Sec. A-A) and applied (Sec. A-B). The second part of the section discusses papers combining FL and interpretable models, both methodological (Sec. A-C) and applied (Sec. A-D)."}, {"title": "A. Combining FL and explanation methods: methodological contributions", "content": "[36] proposes a novel FL protocol where a subset of all centers available online are selected to participate in each FL round by calculating the difference between the aggregated global and local feature contribution (using SHAP [20]). A more efficient and improved performance is shown when using this selection.\n[87] proposes the first counterfactual explanation method for VFL. They show the validity by retraining VFL models on banking data while leaving out a varying number of features with respect to their counterfactual importance rate and comparing against random selection of variables.\n[52] uses explanation methods such as GradCAM [39] to detect whether each participant is using malicious data to enact a backdoor attack by developing so-called detection filters. These consist of a classifier and an explanation method that identify a likely backdoor attack and triggering features in the input data, respectively. The effectiveness of various explanation methods with different classifiers is tested, strengthening the FL process against backdoor attacks.\n[54] tackles an image classification task by calculating Shapley values by SHAP and using them to find the most important pixels for every local FL model and mask the pixels with highest SHAP score. The resulting dataset is used for training the FL model. This method is proposed to protect the FL setup from adversarial attacks, specifically poisoning GAN attacks.\n[53] uses random forest (RF) algorithms to detect features causing wrong predictions, with emphasis on detecting malicious attacks on the FL operation. Each participant trains a DL and RF model on its training data. For the samples that are wrongly classified, it calculates the average feature importance of all decision trees with LIME [49] that will result in the same wrong classification. It uses the change in this feature's importance over time to predict the contribution of each feature in wrongly classifying the data.\n[55] proposes and designs an algorithm for explaining the output of a time-series classifier. It extracts and visualizes the input subsequences that highly activate a convolutional neural network. A graph capturing temporal dependencies is computed at each learning node. The central server aggregates the obtained graphs into a global temporal evolution graph.\n[88] introduces PrADA, a privacy-preserving federated adversarial domain adaptation technique addressing cross-silo federated domain adaptation issues. PrADA mitigates sample and feature scarcity by employing VFL with a feature-rich party and implementing adversarial domain adaptation from a sample-abundant source. For interpretability, features are segregated into semantically meaningful groups for fine-grained adaptation based on Shapley values computed using SHAP.\n[33] proposes a method, namely node liability in federated learning (NL-FL), to trace back ML decisions to training data sources in distributed settings. The method allows for the identification of misbehaving nodes that can be excluded from the training process, resulting in improved prediction results.\n[57] implements interpretable adaptive sparse deep networks which exchange NN parameters by means of a multi-level federated network. Whether those weights are shared at the \"top sharing level\" of the FL architecture depends on the relevance values of the network calculated through layerwise relevance propagation (LRP). The approach provides good diagnostic results even when the FL dataset is under a non-independent identical distribution (NOIID)."}, {"title": "B. Combining FL and explanation methods: applied contri-butions", "content": "[64] trains FL models to segment lung X-ray images and detect signs of pneumonia. Grad-CAM is used to highlight parts of the images that contribute to a detection. It is concluded that a model trained on segmented images has less accuracy, but the pixels highlighted by Grad-CAM focus more on the lung area. It is also reported that training the model in the FL manner helps maintain generalizability and avoid overfitting. A fixed number of FL rounds and greater number of local iterations result into more accuracy.\n[63] aims to predict residential load using a recurrent neural network (RNN). To explain the importance of features, the authors propose a novel automatic relevance determination (ARD) method. An iterative federated clustering algorithm (IFCA) is used, which keeps several central models while clustering the input data sequences, and each model is updated using data in its associated cluster. No interaction between ARD and IFCA is reported.\n[41] develops an FL procedure for taxi travel-time prediction based on time-series and geographical data. Authors develop a federated feature attribution aggregation method and test how similar the FL calculated explanations are compared to central calculation. Many XAI techniques are tested, all result in similarly low differences.\n[56] compares the use of Shapley values and Lipschitz constant for generating both local and global explanations, and uses this information to update the model. It allows for the personalization of the FL model for each user, so that only the necessary characteristics of the model are retrained, based on the respective needs and the events that it is called to respond.\n[89] uses Shapley values computed via SHAP to explain outputs of an FL model trained on edge devices to its operators. It takes the model and the test data as inputs to construct a local linear regression explanation model. Subsequently, the explanatory model computes the Shapley values of classified anomalies and displays them visually. As feature values are measured by sensors, the explanations help operators deter-mine the sensors likely causing an abnormality and make a faster detection response."}, {"title": "C. Papers combining FL and interpretable models: method-ological contributions", "content": "[67", "93": "it effectively combines gradient-free classifiers which may be learned independently by the FL clients.\n[60", "61": "in federation via a one-shot communication scheme where each data silo computes their own FRBS and the individual models are merged by the central server. The proposed FRBS uses a maximum-matching inference rule so the inferred regression function is piecewise linear, which is inherently explainable.\n[71", "protocols.\n[72": "highlights the importance of using information granules for better interpretability, focusing on unsupervised federated learning and enhancing rule-based models through granule decomposition and linguistic approximation.\n[70", "privacy.\n[76": "presents an interpretable data interoperability method for FL called iFedAvg to address the low interoperability due to client data inconsistencies, among other challenges. The iFedAvg method uses personalized layers to adjust for local data shifts, like age differences, directly within input features, which maintains privacy while allowing for direct interpretability. The difference in values of private weight and bias of input layer of each participant captures the inherent shift in data. It was tested on public benchmarks and on a large, real-world Ebola dataset.\n[94", "principles.\n[69": "introduces an interpretable FL system for collaborative data analysis across distributed networks using interpretable models such as decision trees, sharing intermediate represen-tations of the data rather than models. The result is an inter-pretable model that performs better than individual analyses and nearly as well as centralized methods.\n[65", "interpretabil-ity.\n[78": "proposes a framework for solving sparse support vector machine (SVM) classification in a distributed fashion"}]}