{"title": "Development of REGAI: Rubric Enabled Generative Artificial Intelligence", "authors": ["Zach Johnson", "Jeremy Straub"], "abstract": "This paper presents and evaluates a new retrieval augmented generation (RAG) and large language model (LLM)-based artificial intelligence (AI) technique: rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics, which can be created manually or automatically by the system, to enhance the performance of LLMs for evaluation purposes. REGAI improves on the performance of both classical LLMs and RAG-based LLM techniques. This paper describes REGAI, presents data regarding its performance and discusses several possible application areas for the technology.", "sections": [{"title": "1. Introduction", "content": "The advent of large language models (LLMs), like ChatGPT 3.5 and 4.0, has sparked the growth of artificial intelligence (AI) applications for both personal and business uses [1]. These models excel in various language-related tasks including summarization, essay writing, and creative content production [2]. However, their adoption in critical business applications has been hindered by concerns over hallucinations - instances where LLMs generate responses that are disconnected from the prompt, factually incorrect, or nonsensical [3][4].\nAs the use of generative AI (GAI) in business settings grows, regulatory bodies are focusing on its implications and potential risks. Recent developments in AI regulation aim to ensure responsible and ethical use of AI systems in commercial applications. For instance, the European Union's AI Act includes stringent rules for high-risk AI applications, such as employment decisions and educational assessments [5]. In the United States, agencies like the Federal Trade Commission are scrutinizing the use of AI in business to prevent deceptive or unfair uses [6]. These regulatory frameworks emphasize the need for transparency, accountability, and robust evaluation mechanisms in AI systems \u2013 particularly those that impact critical decision-making processes in business environments.\nLLMs often struggle with tasks requiring human-like reasoning and planning capabilities [7]. While their outputs may appear superficially plausible, they frequently lack the logical coherence and depth of human-generated content. This limitation has led to limited use of LLMs in non-chatbot business applications [8][9][10], despite the significant economic potential in automating tasks that require human reasoning and language skills.\nTo address these challenges, rubric enabled generative artificial intelligence (REGAI), a technology for evaluating unstructured text data, was developed. REGAI aims to closely align LLM outputs with human reasoning and interpretation capabilities, making it suitable for a wide range of applications including academic grading, job candidate evaluation, and professional-client interaction assessment. The REGAI system is comprised of several key components: an (optionally) LLM-generated, human expert-reviewed rubric that defines detailed criteria for text evaluation; a scoring LLM that generates initial scores based on the rubric and examples of other scores; a critiquing LLM, trained on human critiques, which reviews and provides feedback on the initial scoring; and an iterative draft-critique cycle that refines the evaluation until it meets defined quality criteria.\nREGAI incorporates several features to enhance its performance and alignment with human judgment. These include retrieval-augmented generation (RAG), a curated knowledge base of expert evaluations [11], and a self-strengthening mechanism that continuously improves the system's performance through expert validation. Flexible tuning mechanisms are also incorporated, including few-shot and one-shot prompt engineering, to align the system's outputs with human-generated evaluations [1]. REGAI addresses critical challenges in automated text evaluation, such as maintaining consistency, reducing subjectivity, and scaling to large volumes of data [12]. By combining the strengths of LLMs with human expertise, REGAI offers a potential solution for high-volume, objective, and structured text evaluation across various domains."}, {"title": "2. Background", "content": "This section provides an overview of key concepts and technologies relevant to the development of advanced text evaluation systems. It focuses on rubrics, generative AI, retrieval augmented generation, and previous attempts at aligning LLMs with human judgment."}, {"title": "2.1. Rubrics", "content": "Rubrics are structured scoring guides used to evaluate performance in various domains, such as education and professional assessment. They typically consist of a set of criteria and descriptors for different levels of performance for each category. They also typically include a scoring system [13].\nThe use of rubrics has been shown to increase the reliability and validity of assessment processes [14]. They provide a standardized framework for evaluation, reducing subjectivity and enhancing consistency among raters [15]. In education, rubrics have been found to improve student learning by clarifying expectations and facilitating targeted feedback [16].\nRecent research has explored the potential of automated rubric-based LLM assessment systems. They have been studied for evaluating workshop courses led by experts [17], essay feedback generation [18], and evaluating student computer programming assignments [19]. These systems aim to apply rubric criteria consistently to large volumes of work, addressing challenges of scale in assessment tasks [12]. However, the complexity of interpreting nuanced criteria and applying them to diverse texts remains a significant challenge for automated systems."}, {"title": "2.2. Generative Al", "content": "Generative artificial intelligence systems create new content, such as text, images, and other media. In natural language processing, generative AI has delivered advancements in the development of large language models such as generative pre-trained transformers (GPTs) [1], bidirectional encoder representations from transformers (BERT) [20], and their derivatives.\nThese models are trained with large amounts of text data and use sophisticated neural network architectures, primarily based on the transformer model [21]. Transformers revolutionized"}, {"title": "2.3. Retrieval Augmented Generation", "content": "RAG is a hybrid approach that combines the strengths of retrieval-based and generation-based natural language processing techniques. It enhances the performance of generative models by incorporating relevant information retrieved from a knowledge base during the generation process [11].\nIn a RAG system, when a query or prompt is received, the model retrieves relevant documents or passages from a large corpus. This information is used to condition the language model's output, allowing it to generate responses that are fluent and grounded in accurate and relevant information [24].\nRAG has shown promising results in improving the accuracy and relevance of generated text, particularly in knowledge-intensive tasks such as open-domain question answering and fact verification [11]. It mitigates some of the limitations of generative models, such as hallucinations and a lack of up-to-date information. RAG models have demonstrated significant improvement over traditional language models. For instance, in testing using a natural questions dataset [11], RAG achieved a top-1 accuracy of 44.5%, compared to 32.6% for a T5-base model and 29.8% for BART-large. Similarly, with the WebQuestions dataset, RAG provided a 4 to 5 percentage point improvement in exact match accuracy over baseline models [11]."}, {"title": "2.4. Previous Attempts at LLM Alignment with Human Scoring/Judging", "content": "Aligning the output of LLMs with human judgment is critical for the use of LLM-based AI systems for evaluation and decision-making tasks [25]. Several approaches have been explored to do this. One approach is fine-tuning on human-labeled data. To do this, models are trained on datasets that include human-generated scores or judgments. This approach has been used for automated essay scoring systems [26] and sentiment analysis tasks [27]."}, {"title": "2.5. Challenges with Automated Text Evaluation", "content": "Automated text evaluation presents several key challenges in automatically assessing written content [35]. Contextual understanding is a significant hurdle. Capturing the nuances of context, tone, and implicit meaning in text remains difficult for AI systems [31][36]. This is particularly problematic when evaluating complex written works where meaning often extends beyond literal interpretation [37].\nSubjectivity in evaluation poses another significant challenge. Many evaluation tasks involve subjective elements that are challenging to quantify and automate [38]. This is especially pronounced with creative writing or opinion documents. Human evaluators will often disagree regarding these documents, making automated assessment difficult [39].\nThe diversity of writing styles also presents a complex challenge for automated systems [40]. Maintaining consistent evaluation criteria across various writing styles and structures is difficult [41]. Variability in language makes it difficult to create a one-size-fits-all evaluation system, as different styles may require different assessment approaches [42].\nEthical considerations, including issues of fairness, bias, and transparency in automated evaluation systems, are ongoing concerns [43][44]. These challenges are growing in importance as automated systems increasingly influence high-stakes decisions in education, employment, and other areas [45]. Ensuring that these systems do not create, perpetuate, or exacerbate existing biases is crucial for their ethical implementation [46].\nThe foregoing challenges demonstrate the need for innovative approaches that can leverage the strengths of both AI and human expertise in text evaluation tasks [47]. Addressing these issues is"}, {"title": "3. System Overview", "content": "This section presents an overview of REGAI. It details each step of the process from rubric generation to final scoring, including the iterative critique cycle. Figure 1 provides a high-level overview of the processes presented in this section. A comprehensive diagram of the system is shown in Appendix A."}, {"title": "3.1. Rubric Generation", "content": "The first step in the REGAI pipeline is the generation of a rubric that will be used to evaluate text data. This can be done manually or with assistance from the system.\nIf working with an expert, it is not necessary to include rubric auto-generation, if the expert is confident in their ability to provide scoring criteria for the task at hand. This is especially true for educational professionals, where rubric development will likely be a task they have done before.\nHowever, for many experts, creating rubrics to frame the exact goals of a task may not be intuitive. Therefore, it may be necessary to provide a starter rubric that provides ideas for how to accurately frame the task in terms of a rubric."}, {"title": "3.2. Human Expert Review", "content": "Once a rubric is generated, it is important that it be reviewed by a human expert. This step is crucial for aligning the process with human judgement. If the rubric has logical errors in it, any records scored using it will receive inaccurate evaluations.\nThe proposed system has functionality to allow rubrics to be revised, and records to be re-scored. This is especially useful in cases where an error is discovered in the rubric. In this process, a"}, {"title": "3.3. Initial Scoring Phase", "content": "The initial scoring phase is a critical step in the REGAI pipeline. During this phase, the system applies the human-reviewed rubric to evaluate the input text. This phase leverages the power of large language models and the accumulated knowledge from previous evaluations. The process is designed to be robust, accurate, and continuously improving. It sets the stage for the subsequent critique and refinement cycles."}, {"title": "3.5. Use of and Addition to the Knowledge Base", "content": "Once the final scores have been created, the system must decide whether the text produced by the LLM should be added to the knowledge base to serve as an example for future scoring.\nA key-value pair consisting of {[text to be evaluated]: [document]} format is used. Documents considered for knowledge base inclusion include the initial human-created or human-reviewed rubric, and the final score (and justifications) given to a document. A second knowledge base stores key-value pairs of the form {[draft score generated by scorer (with text under evaluation)]:\n[critiques given by critic]}. This knowledge base is used during the critique cycle to give the critic effective examples to reference.\nThe goal of this step is twofold. First, it seeks to add the highest performing pipeline documents to the knowledge base to increase the quality of the knowledge base. Second, it seeks to increase the volume of documents such that, over time, the similarity between text being analyzed and documents in the knowledge base is increased."}, {"title": "4. System Uses", "content": "This section discusses several prospective uses for REGAI. These application areas include academic grading, applicant application evaluation, performance evaluation, interaction evaluation, proposal evaluation, email screening, paper review and the review of court and interrogation transcripts. Each is now discussed."}, {"title": "4.1. Academic Assignment Grading", "content": "REGAI can automate the grading process for academic assignments, ensuring that the grader adheres closely to the instructor-created rubric. The system features a portal for instructors to submit assignment descriptions and rubrics. There is an option to create rubrics collaboratively, if they are not already available. Instructors can grade sample assignments to provide a baseline for the system and offer critiques of LLM-generated grades to refine the process. Due to FERPA considerations, the system may need to be implemented on-premises using a specially secured server.\nThis system can significantly improve grading objectivity, reduce the time spent on grading, and provide more specific and actionable feedback to students. An appeal process allows students to challenge grades, with the instructor making final decisions. While currently limited to writing assignments, this approach could potentially enhance academic assessment by combining AI efficiency with human oversight."}, {"title": "4.2. Human Resources: Applicant Tracking Systems", "content": "The accurate evaluation and ranking of job candidates, based on resumes, is a challenging task for hiring managers. This is especially true in sectors with high applicant volumes. Current methods often rely on keyword searches or parsed resume data, which can be gamed by applicants. This can advance those who game they system while also leading to qualified candidates being overlooked. REGAI addresses this by providing a fair and thorough evaluation"}, {"title": "4.3. Human Resources: Performance Evaluation", "content": "REGAI can assist human resources staff in evaluating employee performance. The ability to evaluate employee performance equally for all employees can facilitate cross-unit comparisons and reduce bias complaints.\nTo do this, REGAI is provided employee performance data in natural language. Many companies conduct weekly or daily stand-up meetings where employees say what they have done in the previous week or day, and what they plan to do the next. These could be recorded and used, in aggregate, for review.\nThe scoring process would follow the one described for applicant evaluation. A manager first defines a rubric to score employees' collection of language-described completed tasks. Examples are provided to serve as the initial knowledge base. Additional natural language performance data can be collected from other sources and assessed. Feedback can be provided to tune the system to deal with the subjectivity inherent in quantifying work."}, {"title": "4.4. Client Interaction Evaluation", "content": "REGAI can be used for evaluating interactions between professionals-in-training and simulated or real clients. This may be particularly valuable in fields such as medicine, law, therapy, and customer service, where interpersonal skills and domain knowledge are crucial. By developing rubrics that capture the nuances of effective communication, empathy, and technical competence, the system can provide consistent and objective feedback on trainee performance. This can decrease the cost of evaluation and allow more resources to be devoted to providing targeted training to those showing the greatest need.\nFor instance, in medical education, REGAI could be used to analyze recordings of student-patient interactions during clinical simulations. The rubric could assess history-taking skills, bedside manner, and diagnostic reasoning. This automated evaluation could supplement human instructor feedback, allowing for more frequent practice opportunities, faster feedback, and personalized learning experiences."}, {"title": "4.5. Solicited Proposal Evaluation Screening", "content": "Organizations that receive and evaluate proposals, such as grant-making agencies, can benefit from REGAI's capabilities for screening and making a preliminary ranking of submissions. The system can use rubrics that reflect the organization's priorities and evaluation criteria. Past successful proposals can be provided as exemplars. If novelty is required, RAG can be used to"}, {"title": "4.6. Email Filtering and Prioritization", "content": "Many email systems employ basic filtering algorithms. REGAI offers a more sophisticated approach to email management. By developing rubrics that capture the nuances of email importance, urgency, and relevance to the user's role and priorities, the system can provide an intelligent sorting mechanism.\nREGAI can analyze incoming emails based on factors such as sender relationship, content relevance, action items, and time sensitivity. This allows for more granular categorization beyond simple spam detection, potentially creating dynamic folders or priority levels that adapt to the user's changing needs over time."}, {"title": "4.7. Technical Paper Review", "content": "The peer review process for academic and technical papers is time-consuming and subject to variability in reviewer assessments. REGAI can assist in this process by providing an initial evaluation of submitted papers, based on rubrics that reflect journal or conference standards. These rubrics might include criteria such as originality, methodology, clarity of presentation, and potential impact.\nWhile not replacing human reviewers, the system could offer a standardized baseline assessment, highlight potential areas of concern, and even suggest relevant literature that the authors may have overlooked. Publications and conferences could allow authors to provide an initial revision in response to REGAI's feedback, improving the quality of papers reaching human reviewers. This could streamline the review process, reduce workload on human reviewers, and potentially improve the consistency of initial screenings and paper quality."}, {"title": "4.8. Court Case Transcript Analysis", "content": "The legal system generates vast amounts of court transcript text, which often requires careful analysis for purposes such as appeal preparation, legal research, and precedent identification. REGAI could be employed to analyze transcripts based on rubrics designed to identify key legal arguments, procedural issues, and potential grounds for appeal.\nThe system could flag important sections of testimony, highlight inconsistencies, or categorize arguments according to relevant areas of law. This would assist legal professionals in quickly"}, {"title": "4.9. Signals Intelligence and Interrogation Transcript Evaluation", "content": "Law enforcement and intelligence agencies often need to analyze large volumes of signals intelligence recordings and interrogation transcripts to extract valuable information and assess the credibility of statements. REGAI can potentially be used for this by crafting rubrics to identify indications of deception, consistency in narratives, and the presence of key information.\nThe system can help prioritize which data requires immediate human attention, identify patterns across multiple interrogations, and flag potential breaches of interrogation protocols."}, {"title": "5. Key Requirements for Use", "content": "The effective use of REGAI depends on several critical factors. This section outlines two key requirements: tracking model changes over time and incorporating explainability mechanisms."}, {"title": "5.1. Tracking Model Changes Over Time", "content": "As REGAI relies heavily on large language models (LLMs) and their outputs, it is crucial to implement a robust system for tracking changes in these models over time. This requirement stems from several factors inherent to the nature of LLMs and the REGAI system.\nFirst, model updates must be tracked. LLMs like GPT-4 are frequently updated by their developers. These updates can introduce changes in model behavior, potentially altering the output quality or consistency of the REGAI pipeline. A tracking system allows for the identification of any significant shifts in performance that coincide with model updates. Alternatively, the pipeline could use static models and perform thorough testing prior to any migration to new updates.\nSecond, the knowledge base will evolve over time. REGAI is a self-reinforcing system and adds high-quality outputs to its knowledge base. This necessitates careful monitoring of how the evolving dataset impacts system performance. Tracking over time can identify trends in output quality and consistency as the knowledge base grows.\nThird, domain drift my occur. In some applications, the nature of the text being evaluated may change over time due to shifts in language use, industry standards, or societal norms. Tracking model performance longitudinally helps detect when the system's effectiveness declines due to these factors. This phenomenon typically occurs over a much longer time scale than the other items discussed.\nFourth, calibration of the critique cycle is required over time. The effectiveness of the critique cycle, a key component of REGAI, may change over time as the scoring and critiquing agents evolve through exposure to more examples. Tracking these changes can inform adjustments to the critique process to maintain its alignment with human expert judgment."}, {"title": "5.2. Explainability Mechanism in REGAI", "content": "Explainability is crucial for the widespread adoption and trust in the REGAI system. This is important for most of the applications discussed in Section 4, given their high-stakes nature, such as academic grading and job candidate evaluation. The explainability mechanism serves several key purposes.\nFirst, it provides transparency by providing insight into how the system arrives at its evaluations. This allows users to understand and evaluate the reasoning behind each score.\nSecond, it promotes trust by making the decision-making process transparent. It helps build trust among users, whether they are educators, hiring managers, or other professionals relying on the system's outputs.\nThird, it facilitates error detection. Explainable outputs make it easier to identify when the system is making a mistake or deviating from the intended rubric application."}, {"title": "6. Data and Analysis", "content": "This section presents a preliminary evaluation of the REGAI system, comparing its performance across two configurations: a REGAI critique cycle with a single critique iteration and a rubric-only approach. These initial results provide a proof of concept and are indicative, rather than definitive, of the system's potential capabilities."}, {"title": "6.1 Methodology", "content": "Evaluation was conducted using a dataset of the first 500 essays from essay set eight of the Hewlett Foundation Automated Essay Scoring Dataset [48]. Each essay was scored by the REGAI system under two conditions:\n1. A single critique iteration configuration.\n2. A rubric-only approach without the critique cycle.\nThe first method is a preliminary implementation of REGAI with a limited number of human-approved examples. For the second, the LLM is simply given the rubric and asked to score the essay based on it.\nThe automated scores were then compared against human-provided scores, which served as the basis for this evaluation. The analysis focuses on three key aspects: overall performance metrics, category-wise performance, and score distribution."}, {"title": "6.2 Overall Performance Metrics", "content": "The overall performance of both configurations was assessed using three key metrics: correlation with human scores (Corr), mean absolute error (MAE), and quadratic-weighted kappa (QWK). These metrics characterize the system's accuracy and reliability. These metrics are provided in Table 1. SC refers to the single critique method, while RO denotes the rubric-only approach."}, {"title": "6.3 Category-wise Performance", "content": "Next, analysis of behavior across the six categories scored in the essay evaluation dataset was performed. These areas are ideas and content (I&C), organization (Org), voice, word choice (WC), sentence fluency (SF), and conventions (Conv)."}, {"title": "6.4. Score Distribution Analysis", "content": "To further understand how the REGAI system compares with human scoring patterns, the distribution of scores across both configurations and human-resolved scores (HRS) were analyzed. Table 3 provides descriptive statistics for these score distributions."}, {"title": "6.5. Statistical Significance", "content": "To assess the statistical significance of the difference between the two REGAI configurations, a t-test was conducted on their respective overall scores. Table 4 presents the results of t-tests comparing the different scoring methods."}, {"title": "6.6. Discussion and Limitations", "content": "The results presented offer insights into the potential of the REGAI system. In particular, they demonstrate the benefit of incorporating a critique cycle. The single critique method has the clear advantage of reducing error magnitude and improving the differentiation of essay quality, as evidenced by the lower MAE and higher QWK values. However, the lower correlation with human scores, as compared to the rubric-only approach, suggests that there is room for refinement of how the critique cycle aligns the final scores with human judgment.\nSeveral limitations that impacted this experimentation should be noted. First, the evaluation only considered a single critique iteration. Future work can explore the impact of multiple critique cycles on performance. Second, the evaluation was conducted on a single set of 500 essays. The system's performance may vary with different essay types and subject matters. Finally, there is substantial room for enhancement of various aspects of the system. These include prompt engineering, knowledge base expansion, and fine-tuning of the critique process.\nDespite these limitations, the results presented herein provide encouraging evidence for the potential of the REGAI approach. In particular, they show the value of incorporating a critique cycle in automated essay scoring. The system's ability to reduce error magnitude and improve quality differentiation suggests that, with further refinement, it could offer a powerful tool for large-scale, consistent, and nuanced text evaluation."}, {"title": "7. Conclusion and Future Work", "content": "The rubric enabled generative artificial intelligence system presented in this paper is an approach to the automated evaluation of unstructured text data. By combining the power of large language models, retrieval-augmented generation, and human expertise, REGAI addresses many of the challenges associated with current AI-based evaluation systems. In particular, it provides benefits in terms of alignment with human judgment, explainability, and adaptability across diverse domains.\nThe key strengths of REGAI lie in its iterative critique cycle, which promotes adherence to human-defined rubrics, and its self-reinforcing knowledge base, which allows the system to continuously improve its performance over time. The system's versatility has been demonstrated through its potential applications in various fields, including academic grading, human resources, professional training, and legal document analysis.\nThe development, implementation and evaluation of REGAI have also highlighted several areas that require further research and development. Comprehensive empirical studies are needed to quantitatively assess its performance across different domains and compare it with existing evaluation methods and human experts. Additionally, further work is required to address potential biases in the system and ensure fairness in high-stakes decision-making processes, particularly for applications like job candidate evaluation and legal analysis.\nContinuous monitoring of the pipelines' results should be implemented to find biases, if they exist. Research should also be performed to optimize the critique cycle and knowledge base. This will be crucial for deploying REGAI in large-scale, real-time applications.\nInvestigating techniques to efficiently adapt REGAI to new domains with minimal expert input could significantly broaden its applicability. Exploring the integration of REGAI with other Al technologies, such as computer vision and speech recognition, could extend its capabilities to multimodal evaluation tasks (e.g., videos of client interactions could be included with the transcript for evaluation).\nDeveloping robust methods for tracking and maintaining REGAI's performance over extended periods, especially as language models and evaluation standards evolve, will be essential for its long-term viability. Finally, creating intuitive interfaces for experts to interact with REGAI, particularly in rubric creation and review processes, will be crucial for its adoption in various professional settings.\nWhile REGAI is a promising framework for automated text evaluation, its full potential can only be realized through additional research, rigorous testing, and thoughtful implementation. As AI continues to evolve, systems like REGAI have the potential to significantly enhance decision-making processes across numerous fields, provided they are developed and deployed with careful consideration of their limitations and ethical implications."}]}