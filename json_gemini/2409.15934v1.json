{"title": "Automated test generation to evaluate tool-augmented LLMs as conversational AI agents", "authors": ["Samuel Arcadinho", "David Apar\u00edcio", "Mariana Almeida"], "abstract": "Tool-augmented LLMs are a promising approach to create AI agents that can have realistic conversations, follow procedures, and call appropriate functions. However, evaluating them is challenging due to the diversity of possible conversations, and existing datasets focus only on single interactions and function-calling. We present a test generation pipeline to evaluate LLMs as conversational AI agents. Our framework uses LLMs to generate diverse tests grounded on user-defined procedures. For that, we use intermediate graphs to limit the LLM test generator's tendency to hallucinate content that is not grounded on input procedures, and enforces high coverage of the possible conversations. Additionally, we put forward ALMITA, a manually curated dataset for evaluating AI agents in customer support, and use it to evaluate existing LLMs. Our results show that while tool-augmented LLMs perform well in single interactions, they often struggle to handle complete conversations. While our focus is on customer support, our method is general and capable of AI agents for different domains.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) are revolutionizing AI agents and have demonstrated remarkable generalization capabilities across various domains (Wu et al., 2023; Lan and Chen, 2024; Li et al., 2024). In particular, LLMs have made a profound impact as chatbots and as AI agents in customer support systems (Dam et al., 2024; Katragadda, 2024). Nevertheless, carelessly deploying an LLM as an AI agent, and allowing them to interact with real users and APIs, can lead to misinformation, reputational damage and costs to the company. Thus, it is critical to evaluate AI agents beforehand. Despite this need, evaluating the performance of LLMs in real-world scenarios remains a significant challenge. This is specially true in a conversational context, which is more complex than answering single-interaction requests. Most current approaches to evaluate LLMs focus primarily on specific tasks such as multi-QA (Zhuang et al., 2024; Kamalloo et al., 2024) or code generation (Liu et al., 2024b,a), which do not fully evaluate the broader set of capabilities that LLMs are expected to possess to truly function as an effective conversational AI agents. Focusing on customer support, an effective AI agent is should be capable of interacting with tools and the customer in order to resolve customer issues, while stricly adhering to procedures described by customer support admins. In order to assess the AI agent's performance, it is crucial to measure its ability to follow a given set of procedures and their resilience against potential customer manipulations. For that, it is key to have a comprehensive evaluation dataset, which can lead to valuable insights into the agent's abilities and limitations.\nWe propose a method to generate evaluation datasets for tool-augmented LLMs as conversational AI agents. Our method automates dataset generation using an LLM to create conversations based on procedures, which are then transformed into tests. We use intermediate graph structures to improve the quality of the generated dataset (i.e., tests follow user-defined procedures) and make it more comprehensive (i.e., tests cover most relevant cases). To assess the AI agent's ability to handle attacks, we incorporate red teaming in our examples.\nOur generation pipeline, illustrated in Figure 1, builds diverse datasets autonomously by using synthetically generated intents as seeds for procedures. Additionally, our pipeline also allows for the inclusion of real data where available, such as actual procedures or APIs used by a company to generate synthetic conversations. While datasets can be created fully automatically, we also put forward ALMITA (Automated benchmark of Language Models for Intelligent Tool-augmented Agents), a manually curated dataset. We use this high-quality dataset to benchmark LLMs as conversational tool-augmented AI agents."}, {"title": "Related work", "content": "With the increasing use of LLMs as AI agents, significant efforts have been made to develop benchmarks to evaluate their ability to correctly answer customer requests in conversational settings. GAIA proposes 466 human-annotated questions covering tasks like general knowledge, daily tasks, and data analysis (Mialon et al., 2023). Recently, AgentInstruct introduced a framework for generating synthetic data from diverse sources, such as code, web articles, and textbook chapters, to help agents generate and refine instruction sets (Mitra et al., 2024). Unlike our work, these datasets do not assess tool-augmented AI agents.\nDatasets to evaluate tool-augmented LLMs have been proposed. Zeng et al. (2023) propose AgentTuning and compile multiple agent datasets to create sequences of API calls. AgentBench features multi-step interactions between an agent and the environment, using various tools to solve user requests (Liu et al., 2023). Patil et al. (2023) and Qin et al. (2023) build datasets of APIs from sources like TorchHub, TensorHub, and rapidAI, prompting an LLM to generate instructions solvable by these APIs. Basu et al. (2024) combine multiple datasets to convert user instructions into API calls. APIGen introduced an automatic method to generate synthetic datasets for tool function calling (Liu et al., 2024c). Unlike our work, these datasets are not conversational and just focus on mapping utterances to API calls, and they do not use intermediate structures (i.e., graphs) to ensure coverage and reduce hallucinations in generated tests."}, {"title": "Method", "content": "Our automated test generation pipeline, illustrated in Figure 1, begins by generating textual procedures from input intents. While one could use an LLM to directly generate conversations from procedures, our approach converts the procedures into a flowgraph and then into a conversation graph. Our assumption is that using these intermediate structured representations makes the task of creating the conversations grounded on the procedures more accurate; see Section 4.2 for supporting evidence. Additionally, the graphs allow us to introduce noise into the conversations, making conversations more realistic and challenging, and enable us to sample paths, ensuring path coverage and conversation diversity. We then generate conversations from the sampled paths. Finally, we extract tests from these conversations by breaking down the conversation at each user message, storing the context, and recording the generated response as the correct reply."}, {"title": "Intent generator", "content": "Intents (or issues, e.g., cancel order) serve as the seeds for our automated test generation method. Intents can be generated by an LLM (as is the case in this work), sourced from predefined domain-specific intents, or a mix of both. The prompt used to generate intents is shown in Appendix A.1."}, {"title": "Procedure generator", "content": "A procedure describes how a given issue/intent should be solved by an agent. We use an LLM to generate a procedure for each input intent by asking it to provide a list of instructions that helps an agent fulfill a given task. We enforce in the prompt to avoid outputting general statements (e.g., \"cancelling policies might depend on the company\" or \"explain the company's policy\") since our goal is to generate specific and unambiguous procedures with precise and granular steps. We also enforce that conditionals are possible but that they need to have a clear solution in the steps of the procedure. Finally, steps might contain actions based on APIs (e.g., search a database, escalate an issue) but they cannot be browsing actions (e.g., click on the login page). The full prompt is shown in Appendix A.2. Similarly to what we described for intents, existing procedures (e.g., of a company) can be included as input for our method. Moreover, procedures can be generated based on existing knowledge, namely existing tickets or help center articles.\nConsider the intent \"order not received\": a simple procedure could be \"If the customer did not receive their order, allow the customer to cancel or refund their order given that they provide a correct order id\". We use this procedure as an illustrative example throughout the paper (see Figures 2 to 4.)"}, {"title": "API extractor", "content": "Our target use-case is tool-augmented AI agents. We use an LLM to generate APIs that are useful for an input procedure. We enforce in the prompt that the extracted APIs are agent APIs and not customer facing APIs. Generated APIs include not only the API name, but also their input output parameters, as well as a small description. The full prompt is shown in Appendix A.3. These APIs should be explicitly called by the agent to fulfill the procedure. Similarly to intents and procedures, existing APIs can be easily included in our pipeline."}, {"title": "Flowgraph generator", "content": "The flowgraph generator receives as input a procedure and relevant APIs and generates a directed graph encapsulating the logic of the procedure from the agent's perspective: nodes are agent actions and edges are customer replies or API outputs. Nodes are of 4 different types: (i) a single start_message node is the initial message sent by the agent to the customer, (ii) message nodes are additional messages sent by the agent to the customer, (iii) api nodes are API calls performed by the agent, and (iv) end_message nodes are messages by the agent that end the interaction. To reduce hallucinations and increase completeness, we enforce in the prompt (Appendix A.4) that every detail from the procedure needs to be in message nodes.\nAn example of a flowgraph is given in Figure 2. Nodes in the flowgraph have a node_id (e.g., \"N1\"), a node_type (one of the four described above), and a node_description, which should be related to a step in the procedure (e.g., \"Tell the user the order was not found\") or an API call (e.g., \"refund_order\"). Edges in the graph are either the user interaction (e.g., \"Gives order id and email\") or the result of an API call (e.g., \"Found order\"). Edges in the flowgraph have an edge_id (e.g., \"E1\"), a tuple with the source node and the target node (e.g., \"(N1, N2)\"), and an edge description, as described previously. We do one-shot prompting, providing an example to the LLM; thus, a complete flowgraph can be seen in flowgraph prompt in Appendix A.4."}, {"title": "Conversation graph generator", "content": "A flowgraph represents a sequence of agent steps to fulfill a procedure. The flowgraph's structure does not directly map to a conversation, which can make the task of creating a conversation from a flowgraph hard. Thus, the goal of the conversation graph generator is to convert the flowgraph into a a conversation graph, which is a structure that is more akin to a dialogue. The generated conversation graph is a directed graph that is expected to have nodes of three different types: (i) agent nodes are messages sent by the agent, (ii) customer nodes are messages sent by the customer, and (iii) api nodes are API calls by the agent.\nAn example of a conversation graph is given in Figure 3. Nodes in the conversation graph have a node_id (e.g., \"N1\"), a node_type (one of the three described above), and a node_description, which is a message for agent and customer nodes, and an API call for api nodes. Edges in the conversation graph connect consecutive messages/api calls. Some conversation paths have conditions, such as an API call returning that the order was found or not; in these cases, edges have an edge description, otherwise the edge description is empty. Edges in the flowgraph have an edge_id (e.g., \"E1\"), a tuple with the source node and the target node (e.g., \"(N1, N2)\"), and an edge description.\nIn an effort to mitigate incorrect conversation graphs, we provide the LLM with additional graph construction rules, e.g., customer nodes should be followed by either agent or api nodes, leaf nodes should be assistant nodes. We use one-shot prompting by giving the LLM as input an example of a flowgraph and the corresponding conversation graph, as shown in Appendix A.5. Similarly to flowgraphs, we load the generated graph into networkx and verify if the required conditions are met, otherwise the graph is discarded."}, {"title": "Noise generator", "content": "Conversation graphs are built from agent procedures, thus they are expected to only contain good behaviour by both the agent and the customer (i.e., happy paths). To make AI agents more resilient to unexpected customer behaviour, which might be malicious or not, we augment the conversation graphs with behaviour outside of the procedure.\nThe noise generator traverses the agent nodes in the conversation graph and, with a certain probability (e.g., 20%), inserts an edge to a new customer node with a node_description message which can either be an \"out-of-procedure\" message or an \"attack\" message. These messages are generated beforehand by an LLM. Additionally, we add an edge from the noisy customer node to a new agent node with node_description as \"Say you're only here to help with the original issue.\""}, {"title": "Path sampler", "content": "We extract conversations between a customer and an agent by sampling paths from the conversation graph. Given a conversation graph G with N nodes and a desired number of conversations M, we employ a weighted random walks algorithm to sample paths, Algorithm 1, which is an enhanced version of vanilla random walks, designed to improve node coverage. For that, we use a weighting vector w with N elements initialized with ones (line 3). Each path p is built by iteratively sampling nodes using sample_node (line 7). A node n, which is a child of the last node in the current path p, is sampled with a probability inversely proportional to its weight wi, where wi is the number of times node n was visited plus one (line 9). The index i of node n in graph G is provided by node_index (line 8). Path construction terminates when a leaf node is reached (lines 11-13)."}, {"title": "Conversation generator", "content": "The conversation generator creates synthetic conversations from an input conversation graph, a sampled path from the conversation graph, and relevant APIs. We provide the LLM with context about the conversation graph structure and the APIs. Using one-shot prompting, we present the LLM with an example triplet consisting of a conversation graph, a list of APIs, and a sampled path, as well as a possible conversation based on these conditions (see Appendix A.6). In an effort to generate valid conversations, we include specific conditions in the prompt, such as always generating a message with the API output following an API message, alternating customer and assistant messages, ensuring agents act on API output messages, and verifying API input and output types."}, {"title": "Test extractor", "content": "The test extractor converts a single conversation into one or more tests. It iteratively breaks down the conversation into sub-conversations (or contexts), each ending with a customer message (e.g., \"Cancel my order\") or an API output (e.g., \"success\" following a cancel function call). The rationale is that since the generated conversations exemplify correct flows, we can construct contexts using the preceding messages, with the expected output being the next non-customer message, whether it's an agent response or an API call. Figure 4 illustrates an example of three tests extracted from a generated conversation. Tests are used to evaluate an AI agent by providing it with the context and comparing its response with the expected output."}, {"title": "Noise generator", "content": "Conversation graphs are built from agent procedures, thus they are expected to only contain good behaviour by both the agent and the customer (i.e., happy paths). To make AI agents more resilient to unexpected customer behaviour, which might be malicious or not, we augment the conversation graphs with behaviour outside of the procedure.\nThe noise generator traverses the agent nodes in the conversation graph and, with a certain probability (e.g., 20%), inserts an edge to a new customer node with a node_description message which can either be an \"out-of-procedure\" message or an \"attack\" message. These messages are generated beforehand by an LLM. Additionally, we add an edge from the noisy customer node to a new agent node with node_description as \"Say you're only here to help with the original issue.\""}, {"title": "Results", "content": "In Section 4.1, we detail the creation of ALMITA, a curated dataset for evaluating LLMs as AI customer support agents. To ensure dataset quality, we employ manual filtering. Two annotators independently review each datapoint to identify incorrect instances, followed by a discussion to align their assessments and minimize disagreements. Any datapoint deemed incorrect by at least one annotator is then removed. GPT-4 is used for all generation steps (see Figure 1). To assess the benefits of the graph intermediate structures, we conduct an ablation study comparing conversations generated directly from procedures to those using the intermediate structures, with manual curation for quality assessment (Section 4.2). In Section 4.3, we evaluate various AI agents on ALMITA. Finally, in Section 4.4, we assess the effectiveness of our pipeline in generating high-quality test sets automatically. We do this by comparing the AI agents' performance on ALMITA with those on its fully automated counterpart, auto-ALMITA."}, {"title": "Dataset generation: ALMITA", "content": "We begin by asking the LLM to generate intents using the prompt from Appendix A.1, resulting in 84 intents. Using them as input, we prompt the model to generate two procedures per intent, for a total of 168 procedures. After manual annotation, we remove 36 procedures that did not comply with the rules from Section 3.2. The valid procedures average 315 words (ranging from 171 to 535) and 11 steps (ranging from 6 to 19). Next, we extract APIs for each procedure as outlined in Section 3.3. APIs not in the correct JSON format are automatically filtered out, along with procedures with invalid APIs, resulting in 70 valid procedures. Each of these procedures, on average, includes 4 APIs (ranging from 2 to 9). For each of the 70 procedures with APIs, we generate the corresponding flowgraph. We automatically filter out 15 flowgraphs and manually filter 6 more that do not adhere to the rules discussed in Section 3.4. The valid flowgraphs average 15 nodes (ranging from 10 to 20) and 17 edges (ranging from 10 to 25). For each of the remaining 49 valid flowgraphs, we generate the corresponding conversation graph. We automatically exclude 16 conversation graphs and manually exclude 7 more based on adherence to rules (Section 3.5). The valid conversation graphs average 23 nodes (ranging from 16 to 37) and 24 edges (ranging from 15 to 37). From these conversation graphs, we generate 217 conversations after path sampling (Section 3.7). We manually filter out 25 conversations for not following the rules (Section 3.8). Thus, from the original 84 intents, we obtain 192 valid conversations. Each conversation traverses an average of 12 nodes (ranging from 3 to 24). Finally, tests are extracted from these conversations as detailed in Section 3.9, resulting in 1420 generated tests. Table 1 summarizes the dataset statistics. In the end, the ALMITA dataset comprises 14 intents, 18 procedures, 18 flowgraphs, 18 conversations graphs, 192 conversations and 1420 tests."}, {"title": "Ablation study: conversations from procedures", "content": "We conduct an ablation study to validate the effectiveness of our intermediate graph representations in generating correct conversations. We remove the flowgraph generator, conversation graph generator, noise generator, and path sampler, and generate conversations directly from the procedures and APIs using the prompt from Appendix A.7. Annotating conversations directly generated from procedures showed to be a much more complex and time-consuming than annotating conversations"}, {"title": "Evaluation of LLM AI agents", "content": "We use ALMITA to evaluate LLMs serving as customer support AI agents. The dataset allows us to evaluate the following dimensions, which we report in Table 2: (i) reply recall: when the correct action is to reply, the agent correctly sends a reply message instead of calling an unnecessary API, (ii) correct reply: when both the correct and the predicted action is to reply, the agent's reply matches the expected reply (we use BERTScore with a similarity threshold of 0.55 after inspecting of some examples), (iii) API recall: when the correct action is to do an API call, the agent correctly detects that it needed to perform an API call instead of replying, (iv) correct API: when both the correct and the predicted action is to perform an API call, the agent calls the correct API; (v) correct API parameters: when both the correct and the predicted action are the same API call, the agents calls the API with the correct parameter values, (vi) test correctness (or test accuracy): whether the test is fully correct (i.e., call the correct reply/API and, if the correct action is an API, call the correct API and use the correct parameters, or if the correct action is a reply, provide a correct reply), (vii) conversation correctness (or conversation accuracy): whether the sequence of all tests from the conversation where all correct.\nWe evaluate 5 different LLMs: GPT4-0, GPT-4, Claude3-sonnet, Mistral-NeMo-Instruct, and Llama3.1-8b-Instruct. To ensure fairness, we use a uniform prompt for all models (details in Appendix A.8). Our prompt aims to be general, avoiding any favoritism towards a specific model, although we acknowledge that different models may excel with different prompting styles. Since the dataset includes API calling, we also test GPT4-0 with function calling, denoted as GPT-40 w/F.\nWe observe that all LLMs demonstrate high accuracy when responding with an API, achieving over 85% correctness in both the correct API and correct API parameters dimensions. With the exception of Llama3.1-8b-I, which performs considerably worse, the other models correctly determine when an API should be called, with an API recall exceeding 90%. However, performance in other dimensions is notably lower, suggesting that datasets focused solely on API calls do not comprehensively evaluate an AI agent's capabilities.\nInterestingly, GPT-4 tends to call APIs even when unnecessary, resulting in a lower reply recall compared to other models. In terms of correct reply, GPT models outperform the others, though this may be biased by the use of GPT-4 for test generation. For test correctness, GPT-4o, Claude3-s, and Mistral-NeMo-Instruct show the highest performance, while GPT-4 and Llama3.1-8b-Instruct rank among the lowest.\nMost critically, we see that all models have very low performance regarding correct conversation. In practice, this would mean that these AI agents would very likely fail at some step of a conversation with a user. This showcases that current LLMs have some limitations that require either better models or very engineered prompts to suitably serve as fully autonomous customer support AI agents.\nOur dataset could, potentially, be useful to evaluate future models and/or strategies on their AI agent capabilities. Furthermore, since the pipeline is automated, the dataset could be updated to include more (and harder) tests, as well as adapted to new or more specific domains."}, {"title": "Fully automated tests: auto-ALMITA", "content": "In this section, we analyze the results obtained by AI agents on auto-ALMITA, the fully automated version of the ALMITA dataset. This dataset was created using the same seed intents from the ALMITA dataset, described in section 4.1. Then we run the same pipeline without the manual filtering steps. Auto-ALMITA retains more data points and greater diversity (see Table 1), albeit with some reduction in quality. Being fully automatically generated, auto-ALMITA can also be easily extended without additional curation efforts.\nWe evaluate the same LLM agents from Table 2 and compare the global metric test correct obtained by the AI agents both auto-ALMITA and ALMITA in Figure 5. Both datasets rank the LLMs in the same order, with a high correlation value of 0.98 (detailed results are provided in Supplementary Table 1). These findings suggest that the proposed pipeline can generate evaluation datasets for AI agents entirely automatically, which lead to conclusions similar to those derived from curated datasets."}, {"title": "Conclusions", "content": "LLMs are being used as customer support AI agents. However, existing evaluation datasets are limited in their scope. We propose an automated test generation pipeline to evaluate tool-augmented conversational AI agents. Our proposed method uses LLMs to generate intermediate graph structures that help limit hallucinations and improve diversity in the generated tests. We evaluate different LLMs to analyze the current capabilities of LLMs implemented as AI agents.\nTo facilitate this, we developed the ALMITA dataset, which we used to thoroughly evaluate these AI agents and identify their limitations. ALMITA allows for a multifaceted evaluation across several key dimensions, such as reply accuracy, API call correctness, and overall conversation integrity. Our findings highlighted significant limitations in current LLMs, particularly in maintaining correct conversations throughout a user interaction.\nImportantly, the ALMITA dataset can be used by other researchers to evaluate AI agents, providing a comprehensive benchmark for assessing various aspects of their performance, possibly in other target domains. Additionally, since our test generation pipeline is fully automated, we have the capability to create new, more challenging versions of the dataset. This adaptability ensures that our framework can be continually updated to reflect more complex and realistic scenarios, further enhancing its utility for ongoing research and development of AI agents in customer support and beyond."}, {"title": "Limitations", "content": "Our evaluation has some limitations. Namely, we did not evaluate the diversity of the generated tests quantitatively. We performed human annotation, to verify correctness at each step, but the number of annotations and of annotators was small. Our test generation pipeline only used a single LLM as the generator, namely GPT4 and this might influence evaluation. A possible mitigation for this is to repeat the test generation pipeline for other LLMs and aggregate the tests. We evaluated multiple LLMs but only using a single prompt. Our goal was to test different models on the generated dataset, but more advanced AI agents could be considered. Additionally, we acknowledge that some metrics may be too strict. As a future direction, we would like to consider the severity of the errors of an AI agent in a conversation. Conversations are relatively fluid and we may have other replies/actions that are somehow acceptable for a given procedure besides of the most obvious and direct one that was annotated in the dataset. There is still to be develop more advanced and more semantic conversational metrics allowing for some path variations, similarly to what has been happening for the comparison of two sentences where different words and order of words can lead to similar meanings."}, {"title": "Algorithm 1 Conversation path sampling", "content": "Inputs: G, M\nP \u2190 []\nw \u2190 1N\nwhile |P| < M do\np \u2190 []\nwhile True do\nn \u2190 sample_node(G,p, w)\ni \u2190 node_index(G,n)\nwi \u2190 wi + 1\np \u2190 p|n\nif n is EndNode then\nP \u2190 P|p\nbreak"}]}