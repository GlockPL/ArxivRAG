{"title": "Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization", "authors": ["Daniel Palenicek", "Florian Vogt", "Jan Peters"], "abstract": "Reinforcement learning has achieved significant milestones, but sample efficiency remains a bottleneck for real-world applications. Recently, CrossQ has demonstrated state-of-the-art sample efficiency with a low update-to-data (UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with higher UTD ratios. We identify challenges in the training dynamics, which are emphasized by higher UTD ratios. To address these, we integrate weight normalization into the CrossQ framework, a solution that stabilizes training, has been shown to prevent potential loss of plasticity and keeps the effective learning rate constant. Our proposed approach reliably scales with increasing UTD ratios, achieving competitive performance across 25 challenging continuous control tasks on the DeepMind Control Suite and Myosuite benchmarks, notably the complex dog and humanoid environments. This work eliminates the need for drastic interventions, such as network resets, and offers a simple yet robust pathway for improving sample efficiency and scalability in model-free reinforcement learning.", "sections": [{"title": "1. Introduction", "content": "Reinforcement Learning (RL) has shown great successes in recent years, achieving breakthroughs in diverse areas. Despite these advancements, a fundamental challenge that remains in RL is enhancing the sample efficiency of algorithms. Indeed, in real-world applications, such as robotics, collecting large amounts of data can be time-consuming, costly, and sometimes impractical due to physical constraints or safety concerns. Thus, addressing this is crucial to make RL methods more accessible and scalable.\nDifferent approaches have been explored to address the problem of low sample efficiency in RL. Model-based RL, on the one hand, attempts to increase sample efficiency by learning dynamic models that reduce the need for collecting real data, a process often expensive and time-consuming (Sutton, 1990; Janner et al., 2019; Feinberg et al., 2018; Heess et al., 2015). Model-free RL approaches, on the other hand, have explored increasing the number of gradient updates on the available data, referred to as the update-to-data (UTD) ratio (Nikishin et al., 2022; D'Oro et al., 2022), modifying network architectures (Bhatt et al., 2024), or both (Chen et al., 2021; Hiraoka et al., 2021; Hussing et al., 2024; Nauman et al., 2024).\nIn this work, we build upon CrossQ (Bhatt et al., 2024), a recent model-free RL algorithm that recently showed state-of-the-art sample efficiency on the MuJoCo (Todorov et al., 2012) continuous control benchmarking tasks. Notably, the authors achieved this by carefully utilizing Batch Normal-"}, {"title": "2. Preliminaries", "content": "This section briefly outlines the required background knowledge for this paper.\nReinforcement learning. A Markov Decision Process (MDP) (Puterman, 2014) is a tuple M = (S, A, P, r, \u03bc\u03bf, \u03b3), with state space SCR"}, {"content": "action space AC Rm, transition probability P : S \u00d7 A \u2192 \u25b3(S), the reward function r: S \u00d7 A \u2192 R, initial state distribution \u03bc\u03bf and discount factor \u03b3. We define the RL problem according to Sutton & Barto (2018). A policy \u03c0 : S \u2192 \u2206(A) is a behavior plan, which maps a state s to a distribution over actions a. The discounted cumulative return is defined as\nR(s, a) = \u03a3\u03c4\u03bf vtr (st, at),\nwhere so = s and a\u2081 = a. Further, st+1 ~ P(\u00b7 |st, at) and at ~ \u03c0(st). The Q-function of a policy is the expected discounted return Q\" (s, a) = \u0395\u03c0,\u03c1 [R(s, a)].\nThe goal of an RL agent is to find an optimal policy \u03c0* that maximizes the expected return from the initial state distribution\n\u03c0* = arg max Es\u223c\u03bc\u03bf [Q\" (s, a)] .\nSoft Actor-Critic (SAC, Haarnoja et al. (2018)) addresses this optimization problem by jointly learning neural network representations for the Q-function and the policy. The policy network is optimized to maximize the Q-values, while the Q-function is optimized to minimize the Bellmann error\nL=E [1/2 (Q\u014d(St, at) - r(st, at) +YE [V(St+1)])^2],\nwhere the value function is computed by taking an expectation over the learned Q function\nV(St+1) = \u0395\u03a1,\u03c0\u03bf [Q\u014d(St+1, at+1)].     (1)\nTo stabilize the Q-function learning, Haarnoja et al. (2018) found it necessary to use a target Q-network in the computation of the value function instead of the regular Q-network. The target Q-network is structurally equal to the regular Q-network, and its parameters @ are obtained via Polyak Averaging over the learned parameters 0. While this scheme ensures stability during training by explicitly delaying value function updates, it also arguably slows down online learning (Plappert et al., 2018; Kim et al., 2019; Morales, 2020).\nInstead of relying on target networks, CrossQ (Bhatt et al., 2024) addresses training stability issues by introducing Batch Normalization (BN, Ioffe (2015)) in its Q-function and achieves substantial improvements in sample and computational efficiency over SAC. A central challenge when using BN in Q networks is distribution mismatch: during training, the Q-function is optimized with samples st, at from the replay buffer. However, when the Q-function is evaluated to compute the target values (Equation (1)), it receives actions sampled from the current policy at+1 ~ \u03c0\u03bf(\u00b7 |St+1).\nThose samples have no guarantee of lying within the training distribution of the Q-function. BN is known to struggle with out-of-distribution samples, as such, training can become unstable if the distribution mismatch is not correctly accounted for (Bhatt et al., 2024). To deal with this issue, CrossQ removes the separate target Q-function and evaluates both Q values during the critic update in a single forward pass, which causes the BN layers to compute shared statistics over the samples from the replay buffer and the current policy. This scheme effectively tackles distribution mismatch problems, ensuring that all inputs and intermediate activations are effectively forced to lie within the training distribution.\nNormalization techniques in RL. Normalization techniques are widely recognized for improving the training of neural networks, as they generally accelerate training and improve generalization (Huang et al., 2023). There are many ways of introducing different types of normalizations into the RL framework. Most commonly, authors have used Layer Normalization (LN) within the network architectures to stabilize training (Hiraoka et al., 2021; Lyle et al., 2024). Recently, CrossQ has been the first algorithm to successfully"}, {"title": "3. CrossQ fails to scale up stably", "content": "Bhatt et al. (2024) demonstrated CrossQ's state-of-the-art sample efficiency on the MuJoCo task suite (Todorov et al., 2012), while at the same time also being very computationally efficient. However, on the more extensive DMC and Myosuite task suites, we find that CrossQ requires tuning. We further find that it works on some, but not all, environments stably and reliably.\nMixed performance of CrossQ. Figure 2 shows CrossQ training performance on a subset of DMC tasks. Namely, the dog-stand, dog-trot and humanoid-walk, selected for their varying difficulty levels to demonstrate a wide range of behaviors, including both successful learning and challenges encountered during training. The figure compares a SAC baseline with standard hyperparameters against tuned CrossQ agents with UTD ratios of 1 and 5, where the hyperparameters were identified through a grid search over learning rates and network sizes, as detailed in Table 1. The first row of the figure shows the IQM training performance and 95% confidence intervals for each agent across 10 seeds. Here, we identify three different training behaviors. On dog-stand CrossQ trains stably at UTD= 1, but increasing the UTD to 5 introduces instabilities and decreases performance. On dog-trot, both UTD ratios perform very similarly. Finally, on humanoid-walk, UTD=5 outperforms UTD=1, although it merely manages to catch up to the SAC baseline in this case. Overall, for all CrossQ runs we notice very large confidence intervals.\nQ-function bias analysis. The second row in Figure 2 shows the standard deviation of the normalized Q-function bias. This bias measures how much the Q-function is over-estimating or underestimating the true expected return of the current policy.\nTo compute the normalized Q-function bias, we follow the protocol of Chen et al. (2021). We gather 5 trajectories in the environment using the current policy and use each trajectory's first 350 state-action pairs to calculate the bias. For this, we compare the cumulative discounted rewards for each state-action pair with its Q-value predicted by the Q-function. This bias is then normalized using the cumulative discounted rewards. The mean over these normalized Q-function biases measures the expected bias. Even if the mean is high, as long as the bias is consistent, the selected actions of the policy do not change and, therefore, it is not a problem (Van Hasselt et al., 2016). If the standard deviation"}, {"title": "4. Combining batch normalization and weight normalization for scaling up", "content": "Inspired by the combined insights of Van Hasselt et al. (2019) and Lyle et al. (2024), we propose to integrate CrossQ with Weight Normalization (WN) as a means of counteracting the rapid growth of weight norms we observe with increasing update-to-data (UTD) ratios.\nOur approach is based on the following reasoning: Due to the use of BN in CrossQ, the critic network exhibits scale invariance, as previously noted by Van Laarhoven (2017).\nTheorem 4.1 (Van Laarhoven (2017)). Let f(X; \u03c9, \u03b4, \u03b3, \u03b2) be a function, with inputs X and parameters w and b and \u03b3 and \u1e9e batch normalization parameters. When f is normalized with batch normalization, f becomes scale-invariant with respect to its parameters, i.e.,\nf(X; cw, cb, \u03b3, \u03b2) = f(X;w,b,\u03b3, \u03b2),\nwith scaling factor c > 0.\nProof. Appendix A\nThis property allows us to introduce WN as a mechanism to regulate the growth of weight norms in CrossQ without affecting the critic's outputs. Further, it can be shown, that for such a scale invariant function, the gradient scales inversely proportionally to the scaling factor c > 0.\nTheorem 4.2 (Van Laarhoven (2017)). Let f(X; \u03c9, \u03b4, \u03b3, \u03b2) be a scale-invariant function. Then, the gradients of f scale inversely proportional to the scaling factor c\u2208 R of its parameters w,\n\u2207 f(X; cw, cb, \u03b3, \u03b2) = \u2207 f(X;w,b,\u03b3, \u03b2)/c.\nProof. Appendix B\nRecently, Lyle et al. (2024) demonstrated that the combination of LN and WN can help mitigate loss of plasticity. Since the gradient scale is inversely proportional to c, keeping norms constant helps to maintain a stable effective learning rate (ELR, Van Hasselt et al. (2019)), further enhancing training stability.\nWe conjecture that maintaining a stable ELR could also be beneficial when increasing the UTD ratios in continuous control RL. As the UTD ratio increases, the networks are updated more frequently with each environment interaction. Empirically, we find that the network norms tend to grow quicker with increased UTD ratios (Figure 2), which in turn decreases the ELR even quicker and could be the case for instabilities and low training performance. As such, we empirically investigate the effectiveness of combining CrossQ with WN with increasing UTD ratios.\nImplementation details. We apply WN to the first two linear layers, ensuring that their weights remain unit norm after each gradient step by projecting them onto the unit ball, similar to Lyle et al. (2024). Since this constraint effectively stabilizes the ELR in these layers, we found it beneficial to slightly reduce the learning rate from le-3 to 1e-4. While"}, {"title": "5. Experiments", "content": "To evaluate the effectiveness of our proposed CrossQ + WN method, we conduct a comprehensive set of experiments on the DeepMind Control Suite (Tassa et al., 2018) and MyoSuite (Caggiano et al., 2022) benchmarks. Our primary goal is to investigate the scalability of CrossQ + WN with increasing UTD ratios and to assess the stabilizing effects of combining CrossQ with WN. We compare our approach to several baselines, including the recent BRO (Nauman et al., 2024), CrossQ (Bhatt et al., 2024) and SR-SAC (D'Oro et al., 2022) a version of SAC (Haarnoja et al., 2018) with high UTD ratios and network resets.\n5.1. Experimental setup\nOur implementation is based on the SAC implementation of jaxrl codebase (Kostrikov, 2021). We implement CrossQ following the author's original codebase and add the architectural modifications introduced by (Bhatt et al., 2024), incorporating batch normalization in the actor and critic networks. We extend this approach by introducing WN to regulate the growth of weight norms and prevent loss of plasticity and add target networks. We perform a grid search to focus on learning rate selection and layer width.\nWe evaluate 25 diverse continuous control tasks, 15 from DMC and 10 from MyoSuite. These tasks vary significantly in complexity, requiring different levels of fine motor control and policy adaptation with high dimensional state spaces up to R223.\nEach experiment is run for 1 million environment steps and across 10 random seeds to ensure statistical robustness. We evaluate agents every 25, 000 environment steps for 5 trajectories. As proposed by Agarwal et al. (2021), we report the interquartile mean (IQM) and 95% stratified bootstrap confidence intervals (CIS) of the return, if not otherwise stated.\nFor the BRO baseline results, for computational reasons, we take the official evaluation data the authors provide. The official BRO codebase is also based on jaxrl, and the authors followed the same evaluation protocol, making it a fair comparison.\n5.2. Weight normalization allows CrossQ to scale effectively\nWe provide empirical evidence for our hypothesis that controlling the weight norm and, thereby, the ELR can stabilize training. We show that through the addition of WN, CrossQ + WN shows stable training and can stably scale with increasing UTD ratios.\nFigure 3 shows per environment results of our experiments encompassing all 25 tasks evaluated across 10 seeds each. Based on that, Figure 1 shows aggregated performance over all environments from Figure 3 per task suite, with a separate aggregation for the most complex dog and humanoid environments.\nThese results show that CrossQ + WN UTD=5 is competitive to the BRO baseline on both DMC and Myosuite, especially on the more complex dog and humanoid tasks. Notably, CrossQ + WN UTD=5 uses only half the UTD of BRO and does not require any parameter resets and no additional exploration policy. Further, it uses ~ 90% fewer network parameters-BRO reports ~ 5M, while our proposed CrossQ + WN uses only ~ 600k (these numbers vary slightly per environment, depending on the state and action dimensionalities).\nIn contrast, vanilla CrossQ UTD=1 exhibits much slower learning on most tasks and, in some environments, fails to learn performant policies. Moreover, the instability of vanilla CrossQ at UTD=5 is particularly notable, as it does not reliably converge across environments.\nThese findings highlight the necessity of incorporating additional normalization techniques to sustain effective training at higher UTD ratios. This leads us to conclude that CrossQ benefits from the addition of WN, which results in stable training and scales well with higher UTD ratios. The resulting algorithm can match or outperform state-of-the-art"}, {"title": "5.3. Stable scaling of CrossQ + WN with UTD ratios", "content": "To visualize the stable scaling behavior of CrossQ + WN we ablate across three different UTD ratios \u2208 {1,2,5}. Figure 4 shows training curves aggregated over all 15 DMC tasks. As expected, CrossQ + WN shows reliable scaling behavior, with the learning curves ordered in increasing order accordance to their respective UTD ratio."}, {"title": "5.4. Hyperparameter ablation studies", "content": "We also ablate the different hyperparameters of CrossQ + WN UTD=5, by changing each one at a time. Figure 5 shows aggregated results of the final performances of each ablation. We will briefly discuss each ablation individually.\nRemoving weight normalization. Not performing weight normalization results in the biggest drop in performance across all our ablations. This loss is most drastic on the Myosuite tasks and often results in no meaningful learning. Showing that, as hypothesized, the inclusion of WN into the CrossQ framework yields great improvements in terms of sample efficiency and training stability, especially for larger UTD ratios."}, {"title": "6. Related work", "content": "RL has demonstrated remarkable success across various domains, yet sample efficiency remains a significant challenge, especially in real-world applications where data collection is expensive or impractical. Various approaches have been explored to address this issue, including model-based RL, UTD ratio scaling, and architectural modifications.\nModel-based RL methods enhance sample efficiency by constructing predictive models of the environment to reduce reliance on real data collection (Sutton, 1990; Janner et al., 2019; Feinberg et al., 2018; Heess et al., 2015). However, such methods introduce additional complexity, computational overhead, and potential biases due to model inaccuracies.\nUpdate-to-data ratio scaling. Model-free RL methods, including those utilizing higher UTD ratios, focus on increasing the number of gradient updates per collected sample to maximize learning from available data. High UTD training introduces several challenges, such as overfitting to early training data, a phenomenon known as primacy bias (Nikishin et al., 2022). This can be counteracted by periodically resetting the network parameters (Nikishin et al., 2022; D'Oro et al., 2022). However, network resets introduce abrupt performance drops. Alternative approaches use techniques such as Q-function ensembles (Chen et al., 2021; Hiraoka et al., 2021) and architectural changes (Nauman et al., 2024).\nNormalization techniques in RL. Normalization techniques have long been recognized for their impact on neural network training. LN (Ba et al., 2016) and other architectural modifications have been used to stabilize learning in RL (Hiraoka et al., 2021; Nauman et al., 2024). Yet BN has only recently been successfully applied in this context (Bhatt et al., 2024), challenging previous findings, where BN in critics caused training to fail (Hiraoka et al., 2021). WN has been shown to keep ELRS stable and prevent loss of plasticity (Lyle et al., 2024), when combined with LN, making it a promising candidate for integration into existing RL frameworks."}, {"title": "7. Limitations & future work", "content": "In this work, we only consider continuous state-action benchmarking tasks. While our proposed CrossQ + WN performs competitively on these tasks, its performance on discrete state-action spaces or vision-based tasks remains unexplored. We plan to investigate this in future work."}, {"title": "8. Conclusion", "content": "In this work, we have addressed the instability and scalability limitations of CrossQ in RL by integrating WN. Our empirical results demonstrate that WN effectively stabilizes training and allows CrossQ to scale reliably with higher UTD ratios. The proposed CrossQ + WN approach achieves competitive or superior performance compared to state-of-the-art baselines across a diverse set of 25 complex continuous control tasks from the DMC and Myosuite benchmarks. These tasks include complex and high-dimensional humanoid and dog environments. This extension preserves simplicity while enhancing robustness and scalability by eliminating the need for drastic interventions such as network resets."}, {"title": "A. Proof Scale Invariance", "content": "Proof of Theorem 4.1.\nf(X; cw, cb, \u03b3, \u03b2) = {g(cXw + cb) \u2013 \u03bc(g(cXw + cb))}/{\u03c3(g(cXw + cb))} \u03b3+ \u03b2\n= {cg(Xw + b) \u2013 c\u00b5(g(Xw + b))}/{|c|\u03c3(g(Xw + b))}\u03b3+ \u03b2\n= {g(\u03a7\u03c9 + b) \u2013 \u03bc(g(Xw + b))}/{\u03c3(g(\u03a7\u03c9 + b))}\u03b3+ \u03b2 = f(X; \u03c9, b, \u03b3, \u03b2)\nB. Proof Inverse Proportional Gradients\nTo show that the gradients scale inversely proportional to the parameter norm, we can first write\nf(X; cw, cb, \u03b3, \u03b2) = {g(cXw + cb) \u2013 \u03bc(g(cXw + cb))}/{\u03c3(g(cXw + cb))}\u03b3+ \u03b2\n= {g(cXw + cb)}/{\u03c3(g(cXw + cb))} - {\u03bc(g(cXw + cb))}/{\u03c3(g(cXw + cb))}\u03b3+ \u03b2.\nAs the gradient of the weights is not backpropagated through the mean and standard deviation, we have\n\u2207w f(X; cw, cb, \u03b3, \u03b2) = {g' (cXw + cb)X}/{|c|\u03c3(g(Xw + b))}\nThe gradient of the bias can be computed analogously\n\u2207bf (X; cw, cb, \u03b3, \u03b2) = {g'(cXw + cb)}/{|c|\u03c3(g(Xw+b))} \u03b3."}]}