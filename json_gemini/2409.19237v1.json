{"title": "The Price of Pessimism for Automated Defense", "authors": ["Erick Galinkin", "Emmanouil Pountourakis", "Spiros Mancoridis"], "abstract": "The well-worn George Box aphorism \"all models are wrong, but some are useful\" is particularly salient in the cybersecurity domain, where the assumptions built into a model can have substantial financial or even national security impacts. Computer scientists are often asked to optimize for worst-case outcomes, and since security is largely focused on risk mitigation, preparing for the worst-case scenario appears rational. In this work, we demonstrate that preparing for the worst case rather than the most probable case may yield suboptimal outcomes for learning agents. Through the lens of stochastic Bayesian games, we first explore different attacker knowledge modeling assumptions that impact the use-fulness of models to cybersecurity practitioners. By considering different models of attacker knowledge about the state of the game and a de-fender's hidden information, we find that there is a cost to the defender for optimizing against the worst case.", "sections": [{"title": "1 Introduction", "content": "Cybersecurity incidents continue to grow in frequency and volume each year, and are estimated to cause $8 billion worth of damage in 2023 [6]. While cybersecurity awareness continues to grow and companies continue to invest in their security functions, the majority of threat response functions are still carried out manually by cybersecurity analysts. As a result, there is a move to automate parts of cyber threat response - something clearly illustrated by the wide availability and marketing of security orchestration, automation, and response (SOAR) systems. These systems, however, are largely rule-based - they take some specific action when some specific criterion is met. This is because SOAR lacks any direct knowledge of the network system it is responsible for helping defend. We instead consider the use of attacker simulation to train defensive agents and aim to answer the question of what assumptions about attacker information should be made to train the most robust defensive agents.\nAbstractions of complex systems generally trade accuracy for tractability because there is some use in modeling that system in different scenarios. In cybersecurity, we use modeling to consider potential future states of the system we are tasked with defending and how different threats and risks [28] may be realized. As an additional constraint in cybersecurity modeling, accuracy and timeliness are constantly in tension. Responding inaccurately to a potentially malicious event that was triggered by benign behavior can have a significant cost, but not responding fast enough to a malicious event can be even more expensive. In the automation of these responses, there is a tendency to optimize for the worst-case. However, as we know from e.g. linear programming [16], there are cases where a solution that is worst-case optimal is empirically suboptimal in practice. Our work seeks to understand the \"price of pessimism\" - that is, the cost to a computer network defender for overestimating the knowledge or capability of an attacker.\nWhile a number of different game types have been used across the security games space [30,20], a small handful comprise the preeminent models used in the space The Bayesian leader-follower game\u00b9, the stochastic Bayesian game, and the two player zero sum game. When training a learning agent for cyberdefense, decisions about attacker modeling directly impact the policy learned by the defending agent. We concern ourselves primarily with the two Bayesian game variants under differing prior knowledge and observability assumptions where the attacker is an actor who seeks to deploy ransomware across a target network. While the impact of altering a game's state or action space is clear and the implications are well-understood, there are other modeling assumptions that are often made implicitly. These assumptions are critical parts of the game design that impact its usefulness to cybersecurity practitioners. In this work, we make assumptions explicit about the presence of attackers and noise, and consider the impact of assumptions about attacker knowledge.\nOur work begins with a presentation of related work in the field of security game theory. We then define the stochastic Bayesian game model and setting we use as an abstraction of attacker-defender interaction on a computer net-work before presenting a theoretical model of how defender belief about attacker knowledge is liable to influence their behavior. In this manuscript, we are par-ticularly interested in the case we deem most realistic one where the attacker has limited knowledge of the target network. Our model, being built on partially observable stochastic Bayesian games which have no known general solution con-cept [14], requires that we develop a decision theory for the players. To overcome the limitations of an attacker to optimize against a particular defender, we intro-duce the use of the restricted Bayes/Hurwicz criterion [11] for decision making under uncertainty. In order to validate our theoretical findings, we leverage re-inforcement learning in a YAWNING-TITAN [5] environment modified to allow attackers and defenders to act as independent learning agents. We leverage the proximal policy optimization reinforcement learning algorithm of Schulman et al. [26], an on-policy deep reinforcement learning algorithm with generally good performance that is used in the default implementation of YAWNING-TITAN2. We conclude with a discussion of our results and avenues for further work."}, {"title": "Contributions", "content": "This work evaluates the \u201cprice of pessimism\", a phenomenon wherein the a priori assumption about an adversary's knowledge of a system results in a suboptimal response pattern. In particular, this manuscript contributes the following:\n1. For reinforcement learning agents in a stochastic Bayesian game, optimizing against a worst-case adversary leads to suboptimal policy convergence.\n2. Defending agents trained against attacking agents that learn are also highly capable against algorithmic attackers even when they have not seen those algorithmic attackers during training.\n3. An extension of the YAWNING-TITAN [5] reinforcement learning framework for training independent attacking and defending agents\n4. A novel use of the Bayes-Hurwicz criterion for parameterizing attacker de-cision making under uncertainty"}, {"title": "2 Related Work", "content": "Security game theory is a broad field informed by cybersecurity, decision theory, and game theory. Recent challenges like CAGE [33] have encouraged develop-ment of models like CybORG [13] and YAWNING-TITAN [5] that use reinforce-ment learning to train autonomous agents that defend against cyber attacks. The Ph.D thesis of Campbell [7] also considers a similar problem space to our work and leverages the same game theoretic model. These works address a similar problem space to our work: the development of a defensive agent that disrupts an adversary while minimizing impact to network users. This paper builds on prior work by the authors [14] that uses a simple state and action space for Stochastic Bayesian Games (SBG) as introduced by Albrecht and Ramamoorthy [3]. The partial observability of the proposed SBG relates closely to the work of Tom\u00e1\u0161ek, Bo\u0161ansk\u00fd, and Nguyen [32] on one-sided partially observable stochastic games. Their work considers sequential attacks on some target and develops scalable algorithms for solving zero-sum security games in this setting and present algo-rithms to compute upper and lower value bounds on a subgame. By contrast, our work seeks to understand how the defender's beliefs about the attacker im-pacts the rewards and outcomes for defenders. Additionally, the aforementioned works and other related works like Khouzani et al. [17] and Chatterjee et al. [8] consider the attacker as either a deterministic operator or leverage epidemic modeling techniques to describe an attacker's movements through a network. Our work here is unique in the respect that we model the attacker as a learning agent, a phenomenon more in line with real-world attackers.\nThe work of Thakoor et al. [31] and subsequent work by Aggarwal et al. [2] informs our point of view on how attackers respond to risk. In this work, we im-plicitly assume bounded rationality and account for risk and uncertainty within our model. While Thakoor et al. and Aggarwal et al. use cumulative prospect theory [34] to address deception as a source of uncertainty, we instead consider it one component of a larger overall framework."}, {"title": "3 Modeling Assumptions", "content": "Since our work is concerned with modeling assumptions, we aim to make our own assumptions as explicit and general as possible. We extend the state and action space used in prior work [14], but operate under the same assumption that attackers and defenders choose their next action simultaneously at each time step. We maintain, without loss of generality, that there is a single attacker and a single defender present in the game."}, {"title": "3.1 State Space", "content": "The state space S of this game consists of a network graph G = (V, E), where each v \u2208 V is a defender-owned computer and each edge e \u2208 E is a tuple indicating a network connection between two nodes (u, v); u, v \u2208 V. The state of each machine v \u2208 V is a tuple (Up, Va, Us).\nUp \u2208 [0, 1] is the \"vulnerability\" of a node: the probability that a basic attack will be successful\nVa = {0,1} is the \"true\" state of compromise and is visible only to the attacker\nUs = {0,1} is the defender-visible state of compromise\nAt each time step, with probability q, an \"alert\" is generated independent of attacker or defender action that sets vs = 1 even if the true compromise state \u03bd\u03b1 = 0, corresponding to a false positive alert. This phenomenon is justified and described in further depth in Section 3.4."}, {"title": "3.2 Attackers", "content": "The attacker's action space, Aa consists of actions on elements of V subject to visibility constraints. We define a \"compromised\" node as a node that the attacker has gained access to and thus has va = 1. An \"accessible\" node is any node with some edge connecting it to any compromised node. The observable state space of an attacker, Sa consists of all compromised nodes and all accessible nodes. The attacker's action space consists of the following actions, which each incur some cost c:\nBasic Attack: Compromise an accessible v \u2208 V with probability Up."}, {"title": "3.3 Defenders", "content": "The defender's action space, As consists of actions on V and E. Although the defender can take only one action at each time step, they may take that ac-tion on a set of nodes or edges. The defender's observable space, Ss consists of the entirety of E and the number of alerts, Udelta, for all v \u2208 V at all times. Specifically, the defender may:\nReduce Vulnerability: For some v \u2208 V, slightly decrease the probability that a basic attack will be successful\nMake Node Safe: For some v \u2208 V, reduce the probability that a basic attack will be successful to 0.01\nRestore Node: For some v \u2208 V, reset the node to its initial, uncompromised state, including the probability that a basic attack will be successful\nScan: With some probability, detect the true compromised status of each \u03bd\u2208V\nDo Nothing: Take no action\nThe objective of cybersecurity, broadly, is to maintain the confidentiality, availability, and integrity [4] of a system. The defender's utility thus arises from"}, {"title": "3.4 Presence of Noise", "content": "Security detections are not infallible, and some number of both false positives - detections that alert on benign behavior, and false negatives - failures to detect malicious behavior, must be expected. Attackers are incentivized to and have adopted techniques like using cloud infrastructure and software as a service (SaaS) providers to conduct attacks [15] and the use of legitimate executables or \"lolbins\" for malicious purposes [19]. Attackers seek to blend in, so detection of malicious behavior that is similar to benign behavior is important for defenders. Since there is no way to definitively determine whether or not a program is malicious, these rules and algorithms yield some number of false positive alerts. The empirical rate of these false positive alerts, according to surveys, appears to be somewhere between 20% [27] and 32% [18]. In cases where some number of alerts are not an indicator of actual attack activity, any probabilistic approach to network security must grapple with this noise. The security game setting has modeled this sort of behavior in the realm of deception and counter-deception. Work by Nguyen and Yadav [23] shows that while attacker payoffs are improved by deception, learning defenders can reduce the value of this deception.\nAssuming that an attacker's behavior is detected with some probability p, then there is an independent probability of false positives q. Letting p and q characterize two independent Bernoulli processes that may each yield an alert, we can treat the emission of an alert as the joint probability of these two processes. The probability of an alert occurring at all is thus p + q - pq, as described in earlier work by the authors [14]. The expected probability that a particular alert is attributable to benign activity is (1-p)q. For simplicity, we assume that p and q are the same across all nodes.\nIn the absence of the noise assumption, attacker-defender interaction becomes a game of Cops and Robbers on a graph [29] where a defender can eliminate the attacker by finding the \"cop number\" - the number of nodes required to \"sur-round\" the attacker and eliminate all of their access at once for the subgraph the attacker has explored. This is still an extremely challenging problem, since even without noise, the defender only has a belief about the extremal edges of that subgraph and finding the attacker's possible subgraphs has exponential complexity. As a result, the importance of an assumption about noise relates with assumptions about under what circumstances a defender realizes a reward."}, {"title": "3.5 Presence of Attackers", "content": "In non-cooperative game theory, two players are playing a game and each seeks to optimize against some utility function. This comes, of course, with the implicit assumption that both players know they are playing a game. In cybersecurity games, when modeling the beliefs of the defender, we frequently imply that the defender knows an attacker is present a priori. In reality, attackers are not always present in our system, and this has a substantial impact on defender expectations. Clearly, any response taken when an attacker is not present incurs a cost and yields no reward.\nLet \u03bc\u2208 [0,1] be the probability that an attacker is present in a system. If our game has alert probability p and no noise - that is, q = 0 - then although we know the probability of an alert is up, the occurrence of an alert allows us to set \u03bc = 1 and the defender can directly pursue the attacker as described in Section 3.4. Assuming noise is present in the system, the probability of an alert occurring at any time step is then\n(1 \u2212 \u03bc)q + \u03bc(p + q - pq)\nIn this setting, since an attacker may not be present, the probability of a false positive event occurring at any time step is\n(1 \u2212 \u03bc)q + \u03bc(1 \u2013 p)q"}, {"title": "4 Attacker Knowledge", "content": "As one might predict and as was demonstrated in prior work [14], attacker util-ity increases monotonically with the knowledge available to them. Knowing the parameters of a simplified game allows defenders to set a threshold of alerts in expectation - that is, if they know the values of p, q, and u, they can compute the number of alerts that might be expected at time t and shut down any system that has generated more than the threshold number of alerts. However, consid-ering only the value in expectation can lead to poor outcomes for the defender, as even for small values of q, any deviation from expectation can lead to sub-optimal outcomes for the defender e.g. shutting down systems when no attacker is present. As such, the defender's threshold for taking an action should instead be influenced by the level of deviation from their expectation.\nAs in Section 3.5, our per-node expectation of alerts, conditioned on the presence of an attacker, is (1 \u2013 \u03bc)q + \u03bc(p + q \u2013 pq). The defender establishes some prior \u03bc and at each time step t, observes some number of alerts across the n nodes of the network, where n = |V|. The total number of alerts, Al, expected at time t is therefore nt((1 \u2212 \u03bc)q+ \u03bc(p+q - pq)), which can be treated as a random sample drawn from a Beta-binomial distribution. Given Al, the defender performs a Bayesian update on \u03bc. The posterior distribution of u is a beta distribution and so at time t, the defender updates \u03bc as follows:\n\u03bc\u03b5 = $\\int_{0}^{1} \\frac{\\mu^{\\alpha - 1}(1 - \\mu)^{\\beta - 1}}{B(\\alpha, \\beta)}d\\mu$ (2)"}, {"title": "4.1 Zero Knowledge", "content": "In real-world settings, attackers are unaware of parameters like p and q, so they cannot ex ante optimize their actions and must instead seek to achieve their goal by balancing the risk of being caught with the need to achieve their goal. Thus, the \"optimistic\", from the defender's perspective, zero knowledge setting is the most consequential for generating real security impacts. In the zero knowledge setting, the defender is still armed with full knowledge and chooses some thresh-old for u to take an action. However, the attacker does not know any of the parameters of the network and must balance exploration and exploitation given only the state, as each action they take may alert the defender of their presence. Given their limited information, they can use the Restricted Bayes/Hurwicz cri-terion [11] to choose their action."}, {"title": "5 Empirical Evaluation", "content": "Game theoretic proofs about behavior in cybersecurity environments can pro-vide powerful tools for thinking about how attacker-defender interaction occurs in practice. However, they do not always carry over to real-world environments. To gain empirical insight into the way these assumptions manifest in practice, we modify the YAWNING-TITAN (YT) framework [5] to include noise and al-low two independent agents one attacker and one defender to be trained simultaneously. To do this, we create a new multiagent environment with a sin-gle state space where an attacking agent and a defending agent both operate but have their own separate observation and action spaces. This environment extends the functionality of YT by providing the ability to treat the attacking agent as a learner, rather than following a fixed algorithm for determining at-tacker actions. Moreover, since our zero-knowledge training case involves training both an attacker and a defender with different observation spaces, methods like Multi-Agent DDPG [21] are not suitable, as such methods would disclose hid-den information about the environment to each agent. Despite issues of known overfitting to suboptimal policies due to non-stationarity [22], we opt to use two distinct instances of proximal policy optimization (PPO) [26] \u2013 one each for the attacker and defender to train our agents, as this the algorithm generally performs well on a variety of tasks and has been used in prior, related work by others [5].\nIn this environment, we associate a cost to each attacker and defender action in accordance with those included in YT, and associate a positive reward for the agent that \"wins\" the episode along with a negative reward for the agent that \"loses\" the episode. To improve learning, we scale the negative reward for the defender such that they achieve a lesser negative reward for closer failures. Specifically, this scaling factor is the number of timesteps that the game has taken divided by the maximum number of timesteps required for a defender victory. We find that our defensive agent trained to expect the adversary has complete knowledge of the system will make worse decisions by overestimating the adversary when less information is available. For the sake of evaluating de-fensive agents against a programmatic, non-learning attacker, we adapt YT's NSARed agent [25], based on a description of cyber attack automation, to work within our environment. The implementation of the NSARed agent is purely algo-rithmic - there is no machine learning component - and defensive agents are not exposed to the agent at training time, making it a stable baseline for compar-ison. The full code for our training and evaluation implementation is available on GitHub\u00b3."}, {"title": "5.1 Establishment of Priors", "content": "The attacker must consider three distributions when playing the game: P, P, P. For the \"best case\" distribution P, we train an attacking agent against a defender whose only action is to do nothing. For the \"worst case\" distribution P, we train an attacking agent against a defender who has access to both attacker and defender observation spaces and thus has full-knowledge of the attacker's moves and the network. The remainder of this subsection describes the training of the model P is drawn from.\nOur setting assumes an adversary who uses ransomware, where the attacking player \"wins\" when they control more than 80% of the network. For each train-ing episode, we instantiate a random entrypoint for the attacker on a 50-node network whose edges are randomly generated to ensure the network has 60% connectivity, and that there are no unconnected nodes. We leverage proximal policy optimization (PPO) [26] for our learning agents in two settings:\n1. Optimistic (zero-knowledge): The attacking agent can see only the nodes they control and adjacent nodes. They cannot see the vulnerability status of any nodes.\n2. Pessimistic (full-knowledge): the attacking agent has access to the same in-formation and observation space as the defending agent.\nIn accordance with our modeling assumptions, the attacking and defending player simultaneously decide their moves for timestept from their action space. Each agent is trained in either the optimistic or pessimistic environment for 3000 episodes and evaluated for 500 episodes across randomly generated environments in both the optimistic and pessimistic setting. Based on empirical results from experiments in the environment, the actor learning rate is set to 0.0002 and the critic learning rate is set to 0.0005. Higher learning rates were tried and led to fast convergence to suboptimal policies, as PPO assumes full observability to achieve globally optimal policies and our environment is only partially observ-able. Values for all hyperparameters and action costs for both players were fixed across all settings and are included in Tables 1 and 2.\nEvaluating reinforcement learning findings is notoriously difficult. Therefore, in line with Agarwal et al. [1], we look to more robust measurements that cap-ture the uncertainty in results. Specifically, in addition to standard evaluation"}, {"title": "5.2 Use of Bayes-Hurwicz Decision Criterion for Attackers", "content": "As mentioned in Section 4.1, attackers in the real world do not have the luxury of training their priors to convergence against a target and must combine their prior knowledge with what is observed during an attack. Since the attacker is making decisions under uncertainty, some criterion must be used to allow them to do that subject to their own parameters. Using the pretrained, frozen models from Section 5.1, we consider how the application of the Restricted Bayes/Hurwicz criterion for the attacker as defined in Equation 3 impacts the outcomes of the attacking player. Aside from the use of the Restricted Bayes/Hurwicz criterion for the attackers, all of the defender and environmental evaluation settings re-main the same.\nThe attacking agent draws independent prior \u00fb and 6 values from PERT distributions in accordance with Algorithm 1 at the start of each round and updates their \u00fb at each timestep if defender activity is observed - that is, if the defender takes an action that removes access to an attacker controlled node. For each evaluation scenario, the relevant trained attacker model zero knowledge or full knowledge \u2013 is used to establish P, P, and P for the observed state of the game. Specifically, each policy model \u3160,\u3160,\u3160 accepts an attacker-observed state St and outputs a distribution over the attacker's action space Aa such that (St) ~ P, \u4e93(St) ~ P, and \u03c0(St) ~ P. The attacker leverages these model outputs and the values of \u00fb, and y with Equation 3 to determine their next best action."}, {"title": "6 Conclusion and Future Work", "content": "The game design assumptions made about an attacker's capabilities and knowl-edge grow increasingly important as we seek to develop improved mitigation ca-pabilities for cyber defenders. In this work, we have demonstrated a phenomenon we call \"the price of pessimism\" - the cost incurred by a defender by assuming a more pessimistic view of what an attacker is likely to know. Practically, this sug-gests that the development of defensive agents necessitates careful consideration of what real-world attackers are likely to know, and model those assumptions correctly. Specifically, assumptions made about attacker knowledge considerably influence what defenders learn and the efficacy of their response. In future work, we aim to define more precisely how environmental factors should be determined and the impact of per-node and subnet variability of true and false positive alert rates on both attacker and defender performance.\nOur results demonstrate that a defender's assumptions about a priori at-tacker knowledge of an environment have a measurable impact on how that defender responds to potential intrusions. An assumption that overestimates an attacker's knowledge and the concomitant learned response dynamics from this assumption leads to overreaction to false positives on the part of defending agents, incurring unnecessary costs and leading to poor convergence in reinforce-ment learning settings. We conclude that future work in this space seeking to have impact on systems in the real world should account for the likely knowledge and learning dynamics of attackers in addition to those of defenders and should aim to more accurately capture the learning behavior of attackers.\nIn future work, we aim to apply these findings to automated threat response by incorporating human factors, threat modeling, and using more complex sim-ulation frameworks. Although our agents are learning agents, attacks both to-day and in the foreseeable future are conducted not by pure utility maximizing agents, but by human beings. As a result, we look to incorporate prospect the-ory [34] in future work similar to how such models have been used to align large language models [12]. We also aim to explore how incorporating threat in-telligence information about sequences of attacker actions and how they lead to different outcomes can constrain the defender's action space, allowing for threat-informed defense. Finally, given the restricted node states of the YT reinforce-ment learning environment, research incorporating threat intelligence may need to leverage a simulation framework more similar to real-world environments, like CybORG [10]."}]}