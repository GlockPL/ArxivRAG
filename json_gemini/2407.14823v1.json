{"title": "CrossDehaze: Scaling Up Image Dehazing with Cross-Data Vision Alignment and Augmentation", "authors": ["Yukai Shi", "Zhipeng Weng", "Yupei Lin", "Cidan Shi", "Xiaojun Yang", "Liang Lin"], "abstract": "In recent years, as computer vision tasks have increasingly relied on high-quality image inputs, the task of image dehazing has received significant attention. Previously, many methods based on priors and deep learning have been proposed to address the task of image dehazing. Ignoring the domain gap between different data, former de-hazing methods usually adopt multiple datasets for explicit training, which often makes the methods themselves be violated. To address this problem, we propose a novel method of internal and external data augmentation to improve the existing dehazing methodology. By using cross-data external augmentor. The dataset inherits samples from different domains that are firmly aligned, making the model learn more robust and generalizable features. By using the internal data augmentation method, the model can fully exploit local information within the images, thereby obtaining more image details. To demonstrate the effectiveness of our proposed method, we conduct training on both the Natural Image Dataset (NID) and the Remote Sensing Image Dataset (RSID). Experimental results show that our method clearly resolves the domain gap in different dehazing datasets and presents a new pipeline for joint training in the dehazing task. Our approach significantly outperforms other advanced methods in dehazing and produces dehazed images that are closest to real haze-free images.", "sections": [{"title": "I. INTRODUCTION", "content": "In the field of remote sensing images and natural images, haze usually leads to a decrease in image contrast and clarity. It makes the originally distinct boundaries of objects blurry, greatly affecting the visual effect of the image. Therefore, developing effective dehazing methods is of great significance for enhancing the application value of remote-sensing images. Previous dehazing methods mainly address hazy images by using atmospheric scattering models [3], [4], [5]. The model explains how light interacts with particulate matter in the atmosphere under hazy conditions, thereby affecting the imaging process of the images. The specific representation is as follows:\n$I(x) = J(x)t(x) + A(1 \u2212 t(x))$,\nwhere x is a pixel position in the image, J(x) represents the true haze-free image, and A is the global atmospheric light. t(x) represents the medium transmission rate, which ranges from 0 to 1. Here, 0 indicates no transmission, meaning complete opacity, while 1 indicates complete transparency. I(x) represents the hazy image obtained after the haze-free image undergoes atmospheric scattering.\nEarly dehazing methods utilize prior knowledge of the image to separately estimate the haze parameterst(x) and A, and then further use the atmospheric scattering model to obtain the haze-free image J(x). GDCP [6] improves the steps of estimating atmospheric light and transmission rate in the original DCP [7] by introducing new assumptions and constraints, thereby effectively estimating these parameters from hazy images. ALC [8] proposes an image dehazing method that combines atmospheric light white balance correction, local light filtering, and high-altitude perspective priors to restore clarity to hazy images. While these methods are simple and intuitive, they struggle to effectively remove haze in non-uniformly hazy images.\nRecently, many methods have emerged for use in the field of image dehazing. ROP [9] introduces a rank-one matrix to represent complex scenes as linear combinations of a series of simple basic elements and then uses these basic elements for image dehazing. Cycle-SNSPGAN [10] improves the quality of dehazed images by combining spectral normalization and patch discriminator techniques. DEFADE [11] achieves perceptual prediction of haze density by analyzing the statistical features of natural scenes and haze-sensitive features in the image. In addition, some deep learning methods train on a large number of hazy and haze-free image pairs, and use models such as CNNs or Transformers to learn the mapping from hazy images to clear ones. UHD [12] proposes a multi-guided bilateral learning method, which combines features from multiple scales and levels of images as well as bilateral filtering techniques to obtain dehazed images. Dehamer [1] introduces a transmission-aware 3D positional embedding module into the Transformer, providing relative positional information while incorporating priors related to haze density. Trinty-Net [2] consists of three parameter estimation networks: the haze thickness estimator network(S-Net), the ambient light estimator network(A-Net), and the medium transmission estimator network(T-Net). Obtaining these parameters further generates dehazed images.\nHowever, the aforementioned methods mostly consider the model perspective and rarely take the data aspect into account. The dehazing results obtained by the aforementioned methods still have limitations due to the narrow training data. Specifically, the training dataset lacks diversity in terms of scenes and types of haze, resulting in the model's inability to achieve good robustness. Moreover, there is a domain gap between existing datasets, and directly using multiple datasets for training often results in limited performance improvement.\nTo address the issue of data scarcity and uniqueness, we propose a novel method of external data augmentor and"}, {"title": "II. RELATED WORK", "content": "Internal data augmentor to improve the dehazing model. Specifically, we first expand the dataset externally using a channel alignment method. Next, we apply strong-weak augmentation methods internally and perform self-supervised learning on the resulting images within the dataset. Finally, we compare our approach with the advanced dehazing works, and the performance metrics demonstrate the superiority of our method. As shown in Fig. 1, it can be observed that our method outperforms Dehamer and Trinity-Net in terms of quantitative metrics such as PSNR and SSIM from the obtained dehazed results. Additionally, the dehazing results produced by our method visually appear closer to real haze-free images. In summary, the contributions of this paper can be summarized as follows:\n\u2022 We proposed a cross-data external augmentor. The dataset inherits samples from different domains that are aligned by the external data augmenter, making the model learn more robust and generalizable features.\n\u2022 We proposed an internal data augmentor in a strong-weak fashion. The model can effectively explore local information within images. While the model focuses on the overall image externally, the local details of the image also be fully explored.\n\u2022 Experiments have shown that our method achieves state-of-the-art results. Our method outperforms other advanced methods in terms of PSNR and SSIM metrics.\nA. Remote-sensing Image Dehazing\nIn the fields of geography and remote sensing, image dehazing techniques play an important role in image preprocessing. Due to the poor visibility of remote sensing images or natural images acquired in hazy environments, it is visually easy to cause the loss of details [7], [13]\u2013[16]. Hazy images are not conducive to the related tasks of computer vision, thus causing a lot of challenges to a certain extent. For example, hazy images make it difficult for object detection models to accurately identify and localize objects in the images [17], [18]. In semantic segmentation tasks [19], the presence of haze makes scene boundaries that should be clear become blurred, leading to segmentation models struggling to differentiate between different objects or scene elements. In image recognition tasks [20], the presence of haze interferes with the model's overall understanding of the image content.\nThe methods for dehazing remote sensing images can be mainly classified into two categories: those based on priors and those based on learning. Early, He et al. proposed the Dark Channel Prior (DCP) [7] method. They observed that in at least one color channel, there are always some pixels in local regions of the image with very low intensity. This prior is used to estimate the distribution of haze in the image. Color Attenuation Prior (CAP) [21] is a method based on physical models. It assumes that in a hazy environment, the color of a scene gradually attenuates as the depth increases. This prior can be used to estimate the depth map of the scene, thereby restoring the clear image. Color-lines [22] analyze the color information in the image to estimate the distribution of haze"}, {"title": "B. Data Augmentation", "content": "and depth information of objects in the scene. Haze-lines [23] involves a deep understanding and analysis of image features under hazy conditions. By identifying hazy regions in the image and decomposing and processing them, haze-lines aim to remove the effects of haze.\nRecently, with the development of deep learning techniques, deep learning-based dehazing methods have gradually become a research hotspot. DehazeNet [24] utilizes the deep architecture of CNN to output the medium transmission map. Then, it uses the atmospheric scattering model to recover the haze-free image. MSCNN [25] combines a multi-scale information extraction subnetwork and a multi-scale residual dilated convolution module. It integrates features from different receptive fields to estimate the transmission map of hazy images. DCPDN [26] employs two generators, one for generating the transmission map and the other for estimating the atmospheric light. GFN [27] processes images through operations such as white balancing and utilizes an end-to-end trainable neural network with a structure similar to Unet for dehazing. Dehamer [1] introduces a three-dimensional positional embedding module into the Transformer, incorporating priors related to haze density to estimate the haze density in different spatial regions. Trinity-Net [2] consists of three parameter estimation networks, each estimating haze thickness, ambient light, and medium transmission, respectively.\nB. Data Augmentation\nData augmentation is an important technique [28] in deep learning, which mainly involves artificially increasing the amount of data by generating new data samples from existing data. Previous image data augmentation operations often increase the size of the dataset by altering the image shape or visual appearance. Common techniques include injecting noise, flipping, cropping, resizing, and color space operations [29]\u2013[31]. Current data augmentation methods also include techniques based on image erasure. Cutout [32] is a data augmentation technique that involves randomly erasing subregions during training and filling them with 0 or 255 in the image. Random erasing [33] is an augmentation method that randomly erases subregions in the image, similar to cropping. However, it also randomly determines whether to mask and decides the aspect ratio and size of the masked region. The key idea of Hide-and-Seek [34] is to divide the image into randomly sized uniform squares and randomly delete a random number of squares. When important information is hidden, it forces the neural network to learn relevant features.\nRecently, non-single-image mixed data augmentation methods have become increasingly common techniques in data augmentation. Mixup [35] combines two randomly selected images and their corresponding labels, where the mixing ratio is determined by a mixing coefficient a. This method can improve the accuracy of the model. CutMix [36] addresses the problem of information and local feature loss. CutMix randomly selects a region from each image when mixing images. Then it combines them to form a mixed image after swapping these regions. Puzzle Mix [37] introduces a puzzle-based mixed data augmentation technique. This method focuses on flexibly utilizing key information and basic statistical data of images, aiming to break the neural network's reliance on existing data augmentation. RandomMix [38] aims to enhance the generalization performance of the model. It randomly selects mixed augmentations from a set of augmentations and applies them to the image, allowing the model to encounter different samples. However, directly using mixed data augmentation methods is not suitable for image dehazing tasks. Using mixed data augmentation methods directly alters the visual content of the original image, potentially resulting in the loss of crucial information during the dehazing process.\nIn image dehazing tasks, data augmentation operations often need to be designed specifically for the characteristics of images under hazy conditions. To address this issue, we propose an external data augmentation and an internal data augmentation method. This method can enhance the model's ability to handle hazy images and improve its robustness."}, {"title": "C. Self-supervised Learning", "content": "Self-supervised learning(SSL) has achieved significant success in the field of computer vision [39]\u2013[45]. It trains models by generating labels automatically, without the need for manually annotated datasets. Many researchers have proposed different solutions for creating pseudo-labels. For example, flipping images, cropping or merging image patches, etc. The key idea of self-supervised learning is to utilize the inherent structure and relationships within the data itself to learn effective feature representations. Self-supervised learning can be trained on large amounts of unlabeled data, which is highly valuable in fields like remote sensing image dehazing and natural image dehazing. This is because acquiring large amounts of labeled data is often expensive and time-consuming.\nIII. METHODOLOGY\nAs shown in Fig. 2, our proposed method consists of two parts: external data augmentor and internal data augmentor. Firstly, we propose an external data augmentation method to reduce the contrast differences between different hazy datasets. External data augmentor makes the augmented dataset approximate the target dataset in the color space. The augmented samples obtained by our method can better match the distribution of the target domain. To this end, dehaze networks trained with consistent yet more quantity of samples have the potential to show superior results.\nBuilding upon the augmentation of different domain datasets externally, we further explore additional locally effective information by augmenting the dataset internally. The model becomes more robust by coordinating external and internal augmentation. For internal data augmentation, we introduce a weak-to-strong coordinated data augmentation self-supervised learning method to improve the local details of images. The model can focus more on the details of the images by calculating the difference between weakly and strongly augmented images."}, {"title": "A. Cross-data External Augmentor for Alignment", "content": "As described in Sec. II, traditional data augmentation methods often rely on simple geometric and color transformations. However, this approach to processing images generated from the original dataset lacks diversity. Additionally, there are often significant differences between different dehaze datasets in terms of color and contrast. Suppose we directly use several datasets for joint training, the corresponding experiments often show a degradation.\nIn particular, we propose an external-augmentor to address the domain gap between different datasets. The proposed external-augmentor achieves efficient domain adaptation without an additional training phase. Specifically, we first calculate the gamma values YR,G,B for the three channels to be transferred to the target domain. Then, we perform gamma calibration on the images to be transferred, as follows:\n$O_{R,G,B} = M(I_{R,G,B}) = (\\frac{I_{R,G,B}}{255})^{\\gamma_{R,G,B}} * 255$,\nwhere IR,G,B and OR,G,B represent the input and output pixel intensities ([0,255]), respectively, and y is the gamma factor. The subscripts R, G, B represent the color channels, and the values of different channels are unique. Our method adjusts the RSID dataset to make it more similar to the NID dataset from color space. As shown in Fig. 3, the RSID images better be aligned with the RGB distribution of the NID dataset after external augmentation. By using external augmentation, our method effectively expands the dataset's quantity and introduces external datasets, enriching the diversity of the dataset. Some of the transformed data is shown in Fig. 4. The first column shows the original RSID images, the second column shows the new RSID images after data preprocessing, and the third column shows the target dataset NID. As shown in Fig. 4, the RSID images become closer to the NID dataset after the external augmentor. To illustrate our method intuitively, we demonstrate the algorithmic flow of external augmentation. As shown in Algorithm 1, we apply the RSID dataset as an example to present the implementation of the channel correction."}, {"title": "B. Feature Extractor", "content": "As shown in Fig. 2, the dehazing network is composed of a 5-layer U-Net structure [46]. The network includes two downsampling and upsampling steps. In the network, the input hazy image passes through Dehaze Block modules and feature fusion modules to finally obtain the output dehazed image y. As shown in part B of Fig. 2, in the Dehaze Block module, the input feature maps undergo normalization and padding layers. Then they are projected into Q, K, and V (query, key, value) using linear layers, used to calculate attention scores [47]. Additionally, the Dehaze Block module applies multi-head self-attention within a window to handle relationships between image patches. It allows the dehazing network to consider correlations between different positions while extracting features. Thus, the model can learn structural information in the image more effectively to improve the model performance.\nDifferent from directly integrating with the traditional U-Net network architecture, the feature fusion module learns channel attention weights of different feature maps through operations like global average pooling [48]. Thus, the network can dynamically adjust its contributions based on the importance of each feature map. In the feature fusion module, assuming there are two feature maps f1 and f2, We project f\u2081 to f1 using a linear layer. We then use operations such as GAP(.) sequentially to obtain fusion weights w\u2081 and w2.\nNext, we use the weights to fuse f1 and f2 with the following additional short residual:\n$x = w_1 f_1 + w_2f_2 + f_2$\nIn the final short residual fusion stage, the two feature maps are fused through linear combination while introducing an additional short residual term. This design helps preserve the information from the original input while introducing the fused features. It allows the network to better utilize multi-scale and multi-level feature information, thus improving the accuracy and robustness of the dehazing effect.\nThen, we use the L\u2081loss function to train the feature extractor, obtaining the loss for external data:\n$L_{external} = |y - Y|$\nC. Internal-Augmentor with Self-supervised Learning"}, {"title": "C. Internal-Augmentor with Self-supervised Learning", "content": "For the internal dataset, we propose an internal data augmentation strategy to improve the model's ability to preserve details and contrast in dehazed images. Building upon external dataset augmentation, it emphasizes attention to local details in images, making the model more robust. Specifically, we applied different degrees of data augmentation to the output results y of the augmentation model. This is represented as follows:\n$I_{weak} = P_{weak}(Y)$\nWhere Pweak() represents a weak data augmentation operation, and I weak denotes the image generated by performing weak augmentation on y. Subsequently, we apply strong augmentation to Iweak, which is specifically represented as follows:\n$I_{aggr} = P_{aggr}(I_{weak})$\nWhere Iaggr is the image generated through strong augmentation based on Iweak, and Paggr represents the strong augmentation operation. Specifically, weak augmentation refers to random cropping, and strong augmentation Paggr is Gaussian blurring. Finally, we compute the L2 norm between Iweak and Iaggr, obtained through weak-strong augmentation to optimize the model's internal loss as:\n$L_{internal} = \\alpha \u00d7 || I_{weak} - I_{aggr} ||_2$\n$\\alpha = \\alpha_x * \\frac{1}{2}(cos(\\frac{\\pi_s}{S})+1)$,\nwhere a is a hyperparameter that varies with cosine decay. In experiments, we set the initial value of a = 0.1, s is the current training iteration and S is the total number of training iterations. By utilizing the self-supervised learning with Linternal, we can obtain the internal similarity between data, which allows the model to learn richer features and be more robust.\nAs shown in Fig. 5, we visualize the difference in image quality with and without internal data augmentation. The first and last columns represent the hazy images and ground truth images, respectively. The second column shows the dehazed images obtained using the external data augmentation method. The third column shows dehazed images obtained using both external data augmentor and the strong-weak augmentation self-supervised learning (SW-SSL) approach for internal data augmentor. As shown in Fig. 5, by using the internal data augmentation self-supervised method, the details of the out-of-distribution image are closer to the real haze-free image. In Algorithm 2, we present the implementation of strong-weak self-supervised augmentation on internal data."}, {"title": "D. Loss Function", "content": "The original dehazing loss compares the generated dehazed images with the ground truth haze-free images using L1 loss. Then, we incorporate the self-supervised loss obtained from the internal dataset into the original loss. Therefore, we obtain the total loss from both external and internal data as follows:\n$L_{total} = L_{external} + L_{internal}$\nIV. EXPERIMENT\nA. Experimental Settings\nOur model is implemented using PyTorch 1.13.0 on an NVIDIA RTX 3090 GPU. The model is trained using the AdamW optimizer [49] with a cosine annealing strategy [50]. The batch size and training epochs are set to 8 and 3000, respectively. The learning rate is reduced from an initial rate of"}, {"title": "C. Evaluation Metrics and Comparison Objects", "content": "To evaluate our method's performance, we adopt Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) as evaluation metrics. These metrics are commonly used as standards for evaluating image quality in image dehazing tasks.\nPSNR(Peak Signal to Noise Ratio)is an objective measure for evaluating image quality. A higher value indicates better image quality, and it's commonly used to quantify the visual difference between two images. Here, it represents the difference between the resulting dehazed image and the ground truth haze-free image.\n$PSNR = 10 \u00d7 log_{10}(\\frac{MAX^2}{MSE})$,\nwhere MAX is the possible maximum pixel value in the image. MSE stands for Mean Squared Error, which represents the average of the squared differences between the pixel values of the true clear image and the dehazed image.\nSSIM (Structural Similarity Index) is another metric used to measure the quality of images. The SSIM metric considers three aspects of similarity in images: luminance, contrast, and structure. Its calculation formula is as follows:\n$SSIM(a, b) = \\frac{(2\\mu_a \\mu_b + C_1) (2\\sigma_{ab} + C_2)}{(\\mu_a^2 + \\mu_b^2 + c_1)(\\sigma_a^2 + \\sigma_b^2 + c_2)}$,\nwhere a and b represent two images.ua andus are the mean values of the images. \u03c3\u03b5 and of are the variances of the images. ab is the covariance between the two images. The SSIM value ranges between -1 and 1, where a value closer to 1 indicates greater similarity between the two images.\nD. Comparison with Baseline Methods\nWe compare our method with state-of-the-art dehazing works. The experiments show that our results are significantly"}, {"title": "E. Ablation Study", "content": "better than other methods, as shown in Tab. I with specific experimental results.\nObserving Tab. I, we can see that our method first achieves a PSNR metric reaching 31.14. Moreover, it significantly outperforms other methods in terms of both PSNR and SSIM metrics. We observe that our method achieves a PSNR improvement of 2.34 dB compared to the Trinity-Net [2]. After our method, its SSIM value has increased by 0.0153 compared to Trinity-Net's 0.9743. Compared to the UHD [12], our method has improved by 7.42 dB in terms of PSNR and 0.0714 in terms of SSIM. These results indicate that our method is better at restoring image visibility.\nAs shown in Fig. 6, to specifically demonstrate the superiority of our method, we compared our model results with other methods. The first column represents the hazy image. The second, third, fourth, and fifth columns respectively depict the dehazed images obtained by the Zero-store [51], UHD [12], Dehamer [1], and Trinity-Net [2] methods. The sixth column displays the dehazed image achieved through our method. The last column shows the ground truth haze-free image. We compare the dehazed images obtained by our method with those obtained by other methods on the N100 test set. For example, from the images obtained above, it can be seen that our method approaches the real image more closely in terms of texture details. From the zoomed-in images, it can be observed"}, {"title": "V. CONCLUSION", "content": "that our dehazed images are noticeably closer to the real haze-free images in terms of texture and color. Moreover, the edge details are clearer compared to the Trinity-Net [2] method.\nWe also apply our method to the remote sensing dataset RSID. Fig. 7, Fig. 8 specifically illustrates the comparison between our method and other methods. From the figures, it can be seen that the dehazed images obtained by the zero-store method [51] still contain haze, while the dehazed images obtained by the UHD method [12] exhibit partial distortion in terms of color. In comparison to DeHamer [1], in the results shown in the fourth column, there is still a small amount of residual haze along the edges. In comparison to Trinity-Net [2], in the results shown in the fifth column, Trinity-Net [2] results exhibit differences in local color compared to the ground truth haze-free image. In our method, the haze has been removed, and there is no distortion in color. The dehazed images obtained through our method are closest to the real haze-free images.\nIn Tab. II, we observe that our method outperforms other methods significantly in both PSNR and SSIM metrics. For the zero-store [51], UHD [12], DeHamer [1], and Trinity-Net methods [2], we obtained actual quantitative results through reproduction. The experimental results of other methods are obtained based on the Trinity-Net [2] paper. Compared to Trinity-Net [2], our method achieved an improvement of 1.66 dB in the quantitative metric PSNR and an improvement of 0.0175 in SSIM. Compared to Dehamer [1], our method achieved an improvement of 1.83 dB in the quantitative metric PSNR and an improvement of 0.0409 in SSIM. These quantitative results indicate that our method also produces dehazed images with higher visual quality on the remote sensing dataset RSID.\nTo demonstrate the effectiveness of our proposed method, we conduct ablation studies to analyze the effects of different modules, including external data augmentation, internal data augmentation, and self-supervised methods. We add different"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose a novel method that performs data augmentation separately on the external and internal aspects of the data. Specifically, we achieve external data augmentation using channel transfer and internal data augmentation using strong-weak augmentation self-supervised learning. Our method leverages data diversity at the external level and further explores internal information to effectively dehaze images. Experimental results show that our method achieves outstanding performance after applying both internal and external data augmentation methods. Compared to other advanced methods, our method performs better in terms of PSNR and SSIM metrics on both the Natural Image Dataset (NID) and the Remote Sensing Image Dataset (RSID). The resulting dehazed images also contain rich details and closely resemble the ground truth haze-free images."}]}