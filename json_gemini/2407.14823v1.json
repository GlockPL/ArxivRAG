{"title": "CrossDehaze: Scaling Up Image Dehazing with\nCross-Data Vision Alignment and Augmentation", "authors": ["Yukai Shi", "Zhipeng Weng", "Yupei Lin", "Cidan Shi", "Xiaojun Yang", "Liang Lin"], "abstract": "Abstract-In recent years, as computer vision tasks have\nincreasingly relied on high-quality image inputs, the task of image\ndehazing has received significant attention. Previously, many\nmethods based on priors and deep learning have been proposed\nto address the task of image dehazing. Ignoring the domain gap\nbetween different data, former de-hazing methods usually adopt\nmultiple datasets for explicit training, which often makes the\nmethods themselves be violated. To address this problem, we\npropose a novel method of internal and external data augmen-\ntation to improve the existing dehazing methodology. By using\ncross-data external augmentor. The dataset inherits samples from\ndifferent domains that are firmly aligned, making the model learn\nmore robust and generalizable features. By using the internal\ndata augmentation method, the model can fully exploit local\ninformation within the images, thereby obtaining more image\ndetails. To demonstrate the effectiveness of our proposed method,\nwe conduct training on both the Natural Image Dataset (NID) and\nthe Remote Sensing Image Dataset (RSID). Experimental results\nshow that our method clearly resolves the domain gap in different\ndehazing datasets and presents a new pipeline for joint training in\nthe dehazing task. Our approach significantly outperforms other\nadvanced methods in dehazing and produces dehazed images that\nare closest to real haze-free images. The code will be available\nat: https://github.com/wengzp1/ScaleUpDehazing", "sections": [{"title": "I. INTRODUCTION", "content": "I\nN the field of remote sensing images and natural images,\nhaze usually leads to a decrease in image contrast and\nclarity. It makes the originally distinct boundaries of objects\nblurry, greatly affecting the visual effect of the image. There-\nfore, developing effective dehazing methods is of great signif-\nicance for enhancing the application value of remote-sensing\nimages. Previous dehazing methods mainly address hazy im-\nages by using atmospheric scattering models [3], [4], [5]. The\nmodel explains how light interacts with particulate matter in\nthe atmosphere under hazy conditions, thereby affecting the\nimaging process of the images. The specific representation is\nas follows:\n\\(I(x) = J(x)t(x) + A(1 \u2212 t(x)),\\)\nwhere x is a pixel position in the image, J(x) represents\nthe true haze-free image, and A is the global atmospheric\nlight. t(x) represents the medium transmission rate, which\nranges from 0 to 1. Here, 0 indicates no transmission, meaning\ncomplete opacity, while 1 indicates complete transparency.\nI(x) represents the hazy image obtained after the haze-free\nimage undergoes atmospheric scattering.\nEarly dehazing methods utilize prior knowledge of the\nimage to separately estimate the haze parameterst(x) and\nA, and then further use the atmospheric scattering model to\nobtain the haze-free image J(x). GDCP [6] improves the steps\nof estimating atmospheric light and transmission rate in the\noriginal DCP [7] by introducing new assumptions and con-\nstraints, thereby effectively estimating these parameters from\nhazy images. ALC [8] proposes an image dehazing method\nthat combines atmospheric light white balance correction, local\nlight filtering, and high-altitude perspective priors to restore\nclarity to hazy images. While these methods are simple and\nintuitive, they struggle to effectively remove haze in non-\nuniformly hazy images.\nRecently, many methods have emerged for use in the field\nof image dehazing. ROP [9] introduces a rank-one matrix to\nrepresent complex scenes as linear combinations of a series of\nsimple basic elements and then uses these basic elements for\nimage dehazing. Cycle-SNSPGAN [10] improves the quality\nof dehazed images by combining spectral normalization and\npatch discriminator techniques. DEFADE [11] achieves per-\nceptual prediction of haze density by analyzing the statistical\nfeatures of natural scenes and haze-sensitive features in the\nimage. In addition, some deep learning methods train on a\nlarge number of hazy and haze-free image pairs, and use\nmodels such as CNNs or Transformers to learn the mapping\nfrom hazy images to clear ones. UHD [12] proposes a multi-\nguided bilateral learning method, which combines features\nfrom multiple scales and levels of images as well as bilateral\nfiltering techniques to obtain dehazed images. Dehamer [1]\nintroduces a transmission-aware 3D positional embedding\nmodule into the Transformer, providing relative positional\ninformation while incorporating priors related to haze density.\nTrinty-Net [2] consists of three parameter estimation networks:\nthe haze thickness estimator network(S-Net), the ambient\nlight estimator network(A-Net), and the medium transmission\\estimator network(T-Net). Obtaining these parameters further\ngenerates dehazed images.\nHowever, the aforementioned methods mostly consider the\nmodel perspective and rarely take the data aspect into account.\nThe dehazing results obtained by the aforementioned methods\nstill have limitations due to the narrow training data. Specifi-\ncally, the training dataset lacks diversity in terms of scenes and\ntypes of haze, resulting in the model's inability to achieve good\nrobustness. Moreover, there is a domain gap between existing\ndatasets, and directly using multiple datasets for training often\nresults in limited performance improvement.\nTo address the issue of data scarcity and uniqueness, we\npropose a novel method of external data augmentor and"}, {"title": "II. RELATED WORK", "content": "II. RELATED WORK\nA. Remote-sensing Image Dehazing\nIn the fields of geography and remote sensing, image dehaz-\ning techniques play an important role in image preprocessing.\nDue to the poor visibility of remote sensing images or natural\nimages acquired in hazy environments, it is visually easy\nto cause the loss of details [7], [13]\u2013[16]. Hazy images are\nnot conducive to the related tasks of computer vision, thus\ncausing a lot of challenges to a certain extent. For example,\nhazy images make it difficult for object detection models to\naccurately identify and localize objects in the images [17],\n[18]. In semantic segmentation tasks [19], the presence of haze\nmakes scene boundaries that should be clear become blurred,\nleading to segmentation models struggling to differentiate be-\ntween different objects or scene elements. In image recognition\ntasks [20], the presence of haze interferes with the model's\noverall understanding of the image content.\nThe methods for dehazing remote sensing images can be\nmainly classified into two categories: those based on priors\nand those based on learning. Early, He et al. proposed the\nDark Channel Prior (DCP) [7] method. They observed that in\nat least one color channel, there are always some pixels in\nlocal regions of the image with very low intensity. This prior\nis used to estimate the distribution of haze in the image. Color\nAttenuation Prior (CAP) [21] is a method based on physical\nmodels. It assumes that in a hazy environment, the color of a\nscene gradually attenuates as the depth increases. This prior\ncan be used to estimate the depth map of the scene, thereby\nrestoring the clear image. Color-lines [22] analyze the color\ninformation in the image to estimate the distribution of haze"}, {"title": "B. Data Augmentation", "content": "B. Data Augmentation\nData augmentation is an important technique [28] in deep\nlearning, which mainly involves artificially increasing the\namount of data by generating new data samples from ex-\nisting data. Previous image data augmentation operations\noften increase the size of the dataset by altering the image\nshape or visual appearance. Common techniques include in-\njecting noise, flipping, cropping, resizing, and color space\noperations [29]\u2013[31]. Current data augmentation methods also\ninclude techniques based on image erasure. Cutout [32] is a\ndata augmentation technique that involves randomly erasing\nsubregions during training and filling them with 0 or 255 in the\nimage. Random erasing [33] is an augmentation method that\nrandomly erases subregions in the image, similar to cropping.\nHowever, it also randomly determines whether to mask and\ndecides the aspect ratio and size of the masked region. The\nkey idea of Hide-and-Seek [34] is to divide the image into\nrandomly sized uniform squares and randomly delete a random\nnumber of squares. When important information is hidden, it\nforces the neural network to learn relevant features.\nRecently, non-single-image mixed data augmentation meth-\nods have become increasingly common techniques in data\naugmentation. Mixup [35] combines two randomly selected\nimages and their corresponding labels, where the mixing ratio\nis determined by a mixing coefficient a. This method can\nimprove the accuracy of the model. CutMix [36] addresses\nthe problem of information and local feature loss. CutMix ran-\ndomly selects a region from each image when mixing images.\nThen it combines them to form a mixed image after swapping\nthese regions. Puzzle Mix [37] introduces a puzzle-based\nmixed data augmentation technique. This method focuses on"}, {"title": "C. Self-supervised Learning", "content": "C. Self-supervised Learning\nSelf-supervised learning(SSL) has achieved significant suc-\ncess in the field of computer vision [39]\u2013[45]. It trains mod-\nels by generating labels automatically, without the need for\nmanually annotated datasets. Many researchers have proposed\ndifferent solutions for creating pseudo-labels. For example,\nflipping images, cropping or merging image patches, etc. The\nkey idea of self-supervised learning is to utilize the inherent\nstructure and relationships within the data itself to learn\neffective feature representations. Self-supervised learning can\nbe trained on large amounts of unlabeled data, which is highly\nvaluable in fields like remote sensing image dehazing and nat-\nural image dehazing. This is because acquiring large amounts\nof labeled data is often expensive and time-consuming."}, {"title": "III. METHODOLOGY", "content": "III. METHODOLOGY\nAs shown in Fig. 2, our proposed method consists of\ntwo parts: external data augmentor and internal data aug-\nmentor. Firstly, we propose an external data augmentation\nmethod to reduce the contrast differences between different\nhazy datasets. External data augmentor makes the augmented\ndataset approximate the target dataset in the color space. The\naugmented samples obtained by our method can better match\nthe distribution of the target domain. To this end, dehaze\nnetworks trained with consistent yet more quantity of samples\nhave the potential to show superior results.\nBuilding upon the augmentation of different domain datasets\nexternally, we further explore additional locally effective in-\nformation by augmenting the dataset internally. The model\nbecomes more robust by coordinating external and internal\naugmentation. For internal data augmentation, we introduce a\nweak-to-strong coordinated data augmentation self-supervised\nlearning method to improve the local details of images. The\nmodel can focus more on the details of the images by calcu-\nlating the difference between weakly and strongly augmented\nimages."}, {"title": "A. Cross-data External Augmentor for Alignment", "content": "A. Cross-data External Augmentor for Alignment\nAs described in Sec. II, traditional data augmentation meth-\nods often rely on simple geometric and color transformations.\nHowever, this approach to processing images generated from\nthe original dataset lacks diversity. Additionally, there are often\nsignificant differences between different dehaze datasets in\nterms of color and contrast. Suppose we directly use several\ndatasets for joint training, the corresponding experiments often\nshow a degradation.\nIn particular, we propose an external-augmentor to address\nthe domain gap between different datasets. The proposed\nexternal-augmentor achieves efficient domain adaptation with-\nout an additional training phase. Specifically, we first calcu-\nlate the gamma values YR,G,B for the three channels to be\ntransferred to the target domain. Then, we perform gamma\ncalibration on the images to be transferred, as follows:\n\\(OR,G,B = M(IR,G,B) = (\\frac{IR,G,B}{255})^{\\frac{1}{YR,G,B}} 255,\\)\nwhere IR,G,B and OR,G,B represent the input and output pixel"}, {"title": "B. Feature Extractor", "content": "B. Feature Extractor\nAs shown in Fig. 2, the dehazing network is composed\nof a 5-layer U-Net structure [46]. The network includes two\ndownsampling and upsampling steps. In the network, the input\nhazy image passes through Dehaze Block modules and feature\nfusion modules to finally obtain the output dehazed image\ny. As shown in part B of Fig. 2, in the Dehaze Block\nmodule, the input feature maps undergo normalization and\npadding layers. Then they are projected into Q, K, and\nV (query, key, value) using linear layers, used to calculate\nattention scores [47]. Additionally, the Dehaze Block module\napplies multi-head self-attention within a window to handle\nrelationships between image patches. It allows the dehazing\nnetwork to consider correlations between different positions\nwhile extracting features. Thus, the model can learn structural\ninformation in the image more effectively to improve the\nmodel performance.\nDifferent from directly integrating with the traditional U-Net\nnetwork architecture, the feature fusion module learns channel\nattention weights of different feature maps through operations\nlike global average pooling [48]. Thus, the network can\ndynamically adjust its contributions based on the importance\nof each feature map. In the feature fusion module, assuming\nthere are two feature maps f1 and f2, We project f\u2081 to fi\nusing a linear layer. We then use operations such as GAP(.)\nsequentially to obtain fusion weights w\u2081 and w2.\nNext, we use the weights to fuse f1 and f2 with the\nfollowing additional short residual:\n\\(x = w1 f1 + w2f2 + f2\\)\nIn the final short residual fusion stage, the two feature maps\nare fused through linear combination while introducing an\nadditional short residual term. This design helps preserve the\ninformation from the original input while introducing the fused\nfeatures. It allows the network to better utilize multi-scale and\nmulti-level feature information, thus improving the accuracy\nand robustness of the dehazing effect.\nThen, we use the L\u2081loss function to train the feature extrac-\ntor, obtaining the loss for external data:\n\\(Lexternal = |y - Y\\)"}, {"title": "C. Internal-Augmentor with Self-supervised Learning", "content": "C. Internal-Augmentor with Self-supervised Learning\nFor the internal dataset, we propose an internal data aug-\nmentation strategy to improve the model's ability to preserve"}, {"title": "IV. EXPERIMENT", "content": "IV. EXPERIMENT\nA. Experimental Settings\nOur model is implemented using PyTorch 1.13.0 on an\nNVIDIA RTX 3090 GPU. The model is trained using the\nAdamW optimizer [49] with a cosine annealing strategy [50].\nThe batch size and training epochs are set to 8 and 3000,\nrespectively. The learning rate is reduced from an initial rate of"}, {"title": "V. CONCLUSION", "content": "V. CONCLUSION\nIn this paper, we propose a novel method that performs data\naugmentation separately on the external and internal aspects\nof the data. Specifically, we achieve external data augmen-\ntation using channel transfer and internal data augmentation\nusing strong-weak augmentation self-supervised learning. Our\nmethod leverages data diversity at the external level and further\nexplores internal information to effectively dehaze images. Ex-\nperimental results show that our method achieves outstanding\nperformance after applying both internal and external data\naugmentation methods. Compared to other advanced methods,\nour method performs better in terms of PSNR and SSIM\nmetrics on both the Natural Image Dataset (NID) and the\nRemote Sensing Image Dataset (RSID). The resulting dehazed\nimages also contain rich details and closely resemble the\nground truth haze-free images."}]}