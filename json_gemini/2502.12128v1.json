{"title": "LaM-SLidE@ : Latent Space Modeling of Spatial Dynamical Systems via Linked Entities", "authors": ["Florian Sestak", "Artur Toshev", "Andreas F\u00fcrst", "G\u00fcnter Klambauer", "Andreas Mayr", "Johannes Brandstetter"], "abstract": "Generative models are spearheading recent progress in deep learning, showing strong promise for trajectory sampling in dynamical systems as well. However, while latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems \u2013 from chemical molecule structures to collective human behavior are described by interactions of entities, making them inherently linked to connectivity patterns and the traceability of entities over time. Our approach, LAM-SLIDE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), combines the advantages of graph neural networks, i.e., the traceability of entities across time-steps, with the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder are frozen to enable generative modeling in the latent space. The core idea of LAM-SLIDE is to introduce identifier representations (IDs) to allow for retrieval of entity properties, e.g., entity coordinates, from latent system representations and thus enables traceability. Experimentally, across different domains, we show that LAM-SLIDE performs favorably in terms of speed, accuracy, and generalizability.", "sections": [{"title": "1. Introduction", "content": "Understanding the dynamics of spatial systems is a fundamental challenge in many scientific and engineering domains. In this paper, we focus on spatial dynamical systems, where scenes are composed of distinguishable entities at defined spatial locations. Modeling temporal trajectories of such entities quickly becomes challenging, especially when (i) stochasticity is involved, and (ii) when entities should be traceable. A prime example is molecular dynamics, where trajectories of individual atoms are modeled via Langevin dynamics, which accounts for omitted degrees of freedom by using of stochastic differential equations. Consequently, the trajectories of the atoms themselves become non-deterministic, but the atoms remain traceable over time.\nA conventional approach to predict spatial trajectories of entities is to represent scenes as neighborhood graphs and to subsequently process these graphs with graph neural networks (GNNs). When using GNNs, each entity is usually represented by a node, and the spatial entities nearby are connected by an edge in the neighborhood graph. Neighborhood graphs have extensively been used for trajectory prediction tasks, especially for problems with a large number of indistinguishible entities, Recently, GNNs have been integrated into generative modeling frameworks to effectively capture the behavior of stochastic systems.\nDespite their widespread use in modeling spatial trajectories, GNNs hardly follow recent trends in latent space modeling, where unified representations together with universality and scalability of transformer blocks offer simple application across datasets and tasks, a behavior commonly observed in computer vision and language processing. Notably, recent breakthroughs in image and video generation can be accounted to latent space conditioned generative modeling. In such paradigms, pre-trained encoders and decoders are employed to map data into a latent space, where subsequent modeling is performed, leveraging the efficiency and expressiveness of this representation. This poses the question: what does it take to leverage recent techniques from generative latent space modeling to boost the modeling of stochastic trajectories of entities? Recently, it has been shown that it is possible to model the bulk behavior of large particle systems purely in the latent space, at the cost of sacrificing the traceability of individual particles, which is acceptable or even favorable for systems where particles are indistinguishable, but challenging for, e.g., molecular modeling, where atom assignments are essential.\nIn order to combine the advantages of GNNs, i.e., the traceability of entities across time-steps, with the efficiency and scalability of latent space approaches, we introduce LAM-SLIDE (see Figure 1). The core idea of LAM-SLIDE is the introduction of identifier representations (IDs) that allow for retrieval of entity properties, e.g., entity coordinates, from latent system representations. Consequently, we can train generative models, such as stochastic interpolants, purely in the latent space, where pre-trained decoder blocks map the generated states back to the physics domain. Qualitatively, LAM-SLIDE demonstrates flexibility and favorable performance across a variety of different modeling tasks.\nIn summary, our contributions are the following:\n\u2022 We propose LAM-SLIDE for generative modeling of stochastic trajectories, which combines the advantages of GNNs, concretely traceable entities, with the scaling properties of latent space models.\n\u2022 We introduce entity structure preservation to recover the encoded system structure from latent space.\n\u2022 We perform experiments in different domains with varying degrees of difficulty, focusing on molecular dynamics. LAM-SLIDE performs favorably with respect to all other architectures, showcasing scalability with model size."}, {"title": "2. Background & Related Work", "content": "We approach the modeling of trajectories of spatial dynamical systems with a latent space approach and employ deep generative models.\nDynamical systems. Formally, we consider a dynamical system to be defined by a state space S, representing all possible configurations of the system, and an evolution rule \u03a6: \u211d \u00d7 S \u2192 S that determines how a state s \u2208 S evolves over time, and which exhibits the following properties for the time differences 0, t\u2081, and, t\u2082:\n\u03a6(0, s) = s\n\u03a6(t\u2082, \u03a6(t\u2081, s)) = \u03a6(t\u2081 + t\u2082, s)\nWe note, that \u03a6 does not necessarily need to be defined on the whole space \u211d \u00d7 S, but assume this is the case for notational simplicity. The exact formal definition of random dynamical systems is more involved and consists of a base flow (noise) and a cocycle dynamical system defined on a physical phase space. We skip the details, but assume to deal with random dynamical systems for the remainder of the paper. The stochasticity of such random dynamical systems suggests generative modeling approaches.\nGenerative modeling. Recent developments in generative modeling have captured widespread interest. The breakthroughs of the last years were mainly driven by diffusion models, a new modeling paradigm that transforms a simple distribution p\u2080 into a target data distribution p\u2081 via iterative refinement steps. Flow Matching, which is built upon continuous normalizing flows"}, {"title": "3. Latent Space Modeling of Spatial Dynamical Systems via Linked Entities", "content": "LAM-SLIDE introduces an identifier (ID) pool and an identifier assignment function which allow us to effectively map and retrieve latent system representations. The ID components preserve the relationships between entities, making them traceable across time-steps. LAM-SLIDE follows an encoder \u2130 - approximator \ud835\udc9c - decoder \ud835\udc9f paradigm."}, {"title": "3.1. Problem Formulation", "content": "State space. We consider spatial dynamics. Our states s \u2208 \ud835\udcae describe the configuration of entities within the scene together with their individual features, and possibly together with global scene properties. We assume that a scene consists of \ud835\udc41 entities e\u1d62 with i \u2208 1,..., \ud835\udc41. An entity e\u1d62 is described by its spatial location x\u1d62 \u2208 \u211d\u1d30 and some further properties m\u1d62 \u2208 \u211d\u1d30\u1d50 (e.g., atom type, etc.). We further assume that the whole scene itself might be described by some global properties g \u2208 \u211d\u1d30\u1d4d. A state is time-specific. We denote the scene state at a certain time point t as s\u209c. Here t denotes a specific absolute time. Often, we consider sequences of absolute time points t\u2080, ..., t\u209c, ..., t\u1d1b\u208b\u2081. Instead of referring to a state s\u209c\u209c, we just denote this state as s\u209c, when we refer to the state at a time point at index t within the given sequence. Analogously, we use x\u209c, m\u209c, g\u209c to describe coordinates and properties at time point t\u209c. We refer to the coordinate concatenation [x\u2081 ... x\u2099] of the \ud835\udc41 entities in s\u209c as X\u209c \u2208 \u211d\u1d3a\u00d7\u1d30. Analogously, we use M\u209c \u2208 \u211d\u1d3a\u00d7\u1d30\u1d50 to denote [m\u2081 ... m\u2099]. When properties are conserved over time, i.e., M\u209c = M\u00b0 and/or g\u209c = g\u00b0, we just skip the time index and the time-wise repetition of states and use M \u2208 \u211d\u1d3a\u00d7\u1d30\u1d50 and g \u2208 \u211d\u1d30\u1d4d, respectively. Further, without loss of generality, we skip g, since we did not make use of it in our experiments. It however is straightforward to include g into the architecture. \nSampled trajectories. We assume sample trajectories of system states s\u209c sampled at a sequence of time steps t\u2080,..., t\u209c,..., t\u1d1b\u208b\u2081. These samples may, e.g., originate from molecular dynamics (MD) simulations with a starting state s\u209c\u2080 and discrete time steps of equal size \u0394\u209c\u209b\u1d62\u2098 = t\u209c\u208a\u2081 - t\u209c or from pedestrian observations by a camera, which store their observation at time intervals of size \u0394\u209c\u2092b\u209b. We concatenate sequences of coordinate states X\u209c with t \u2208 0.. \ud835\udc47 \u2212 1 to a tensor X \u2208 \u211d\u1d40\u00d7\u1d3a\u00d7\u1d30, which describe a whole sampled coordinate trajectory of a system with \ud835\udc47 time points and \ud835\udc41 entities. An example for such trajectories from a dynamical systems are molecular dynamics trajectories (see\nPredictive aim. Our aim is to generate the spatial continuation X[T\u2080: T-1] = [XT\u2080,...,Xt,...,XT-1] \u2208 \u211d^(T-T\u2080)\u00d7N\u00d7D of a system trajectory, given a short (observed) initial spatial trajectory X[0: T\u2080-1] = [X\u2070, ..., Xt,...,XT-1] \u2208 \u211d^(T\u2080)\u00d7N\u00d7D together with general (time-invariant) entity properties M."}, {"title": "3.2. Entity Structure Preservation", "content": "In order to maintain the integrity of scene entity structures when mapping to and processing in latent space, the core new idea is to randomly assign an ID from an ID pool to each entity of the system. These IDs allow us to retrieve"}, {"title": "3.3. Model Architecture: Latent Space Modeling", "content": "Since predicting continuations of system trajectories is a conceptually similar task to generating videos from an initial sequence of images, we took inspiration from  in using a latent diffusion architecture. We also took inspiration from  to decompose our model architecture as follows: To map the state of the system composed of N entities to a latent space containing L latent tokens (\u2208 \u211d^(D\u2097)), we use a cross-attention mechanism. In the resulting latent space, we aim to train an approximator to predict future latent states based on the embedded initial states. Inversely to the encoder, we again use a cross-attention mechanism to retrieve the latent information for the entities of the system. To wrap it up, LAM-SLIDE, is built up by an encoder (\u2130) - approximator (\ud835\udc9c) - decoder (\ud835\udc9f) architecture, which represents the following function:\n\ud835\udc9f \u2218 \ud835\udc9c \u2218 \u2130: \u211d^(T\u2080\u00d7N\u00d7Dx) \u00d7 \u211d^(N\u00d7Dm) \u2192 \u211d^((T-T\u2080)\u00d7N\u00d7Dx)\nA detailed composition of LAM-SLIDE is shown in the left part of Encoder. The encoder \u2130 aims to encode a state of the system such that the properties of each individual entity e\u2099 can be decoded (retrieved) later. At the same time the structure of the latent state representation (\u2208 \u211d^(L\u00d7Dz)) should not depend on \ud835\udc41, i.e., \ud835\udc3f and \ud835\udc37z are constants and serve as hyperparameters, while individual samples may be composed of different numbers \ud835\udc41 of entities, as opposed to GNNs, for which the size of the latent representation depends on the number of nodes throughout each message passing step.\nTo allow for traceability of the entities, we first embed each identifier i in the space \u211d^(Du) by a learned embedding \ud835\udd40\ud835\udd3b\u2091\u2098\u1d66: \u2110 \u2192 \u211d^(Du). Then we draw a random number \u03c9 \u2208 \u03a9 and map all (n = 1, ..., N) system entities e\u2099 to u\u2099 \u2208 \u211d^(Du) as follows:\nu\u2099 = \ud835\udd40\ud835\udd3b\u2091\u2098\u1d66(\ud835\udd40\ud835\udd3b(e\u2099, \u03c9))\n\u2200 n \u2208 1,..., N"}, {"title": "5. Conclusion", "content": "We have introduced LAM-SLIDE, which combines the advantages of GNNs and latent space models. Its novel"}]}