{"title": "Introducing VaDA: Novel Image Segmentation Model for Maritime Object Segmentation Using New Dataset", "authors": ["Yongjin Kim", "Jinbum Park", "Sanha Kang", "Hanguen Kim"], "abstract": "The maritime shipping industry is undergoing rapid evolution driven by advancements in computer vision artificial intelligence (AI). Consequently, research on AI-based object recognition models for maritime transportation is steadily grow- ing, leveraging advancements in sensor technology and computing performance. However, object recognition in maritime environ- ments faces challenges such as light reflection, interference, intense lighting, and various weather conditions. To address these challenges, high-performance deep learning algorithms tailored to maritime imagery and high-quality datasets specialized for maritime scenes are essential. Existing AI recognition models and datasets have limited suitability for composing autonomous navigation systems. Therefore, in this paper, we propose a Vertical and Detail Attention (VaDA) model for maritime object segmen- tation and a new model evaluation method, the Integrated Figure of Calculation Performance (IFCP), to verify its suitability for the system in real-time. Additionally, we introduce a benchmark maritime dataset, OASIS (Ocean AI Segmentation Initiatives) to standardize model performance evaluation across diverse maritime environments. OASIS dataset and details are available at our website: https://www.navlue.com/dataset.", "sections": [{"title": "I. INTRODUCTION", "content": "The maritime shipping industry has been transitioning into a smart maritime system recently driven by digitization, in- formatization, and now advancements in artificial intelligence (AI). Indeed, at the core of these changes is the advancement of computer vision technology tailored for autonomous navi- gation systems. These technological strides are not only revo- lutionizing smart ports but also reshaping the entire maritime industry. Research into AI-based object recognition models in maritime transportation has steadily increased over time. Advancements in sensor technology, computing power, and AI have accelerated the development of these models. Au- tonomous navigation systems based on computer vision must collect and analyze sensor data to make real-time decisions. In this context, the recognition performance and speed of Al models related to maritime object recognition are crucial aspects. These technological advancements play a crucial role in enhancing safety and efficiency in the maritime transporta- tion industry. Accurate object recognition and tracking help minimize the risk of accidents and optimize overall operational performance.\nIn marine environments, object recognition from images presents significant challenges due to factors such as light reflection, interference, intense illumination, and varying weather conditions. In addition, the object information may be distorted or noisy due to the image sensor or lens spec- ifications. To address this issue, there is a need for research focused on developing high-performance deep-learning algo- rithms specifically tailored to the unique characteristics of maritime data. In addition, it is essential to construct high- quality datasets specialized in maritime-related images to effectively train and evaluate these algorithms.\nThe performance of deep learning models is strongly cor- related with the quality and quantity of data as well as the complexity of the models. However, improvements in performance often come at the expense of increased model"}, {"title": "II. RELATED WORK", "content": "Several datasets have been proposed to evaluate algorithms for detection and segmentation in marine environments. Fefi- latyev et al. [12] proposed a dataset captured on the same day, including 10 sequences of open-view sea scenes. However, since this dataset is constructed to evaluate algorithms for detecting the horizon of the sea, the diversity of scenes is very limited and objects are not included.\nAs vision algorithms for object detection have developed a lot, several datasets for detecting objects/obstacles in the marine environment have also been proposed. Kristan et al. [13] introduced a dataset containing 12 different sequences captured by the USV, and Bovcon et al. [14] later extended it to a sea obstacle detection dataset containing large and small objects and a large number of horizontal lines using 28 stereo camera sequences synchronized with the Inertial Measurement Unit (IMU). Furthermore, this dataset contains many scenes of different weather and has a greater diversity of scenes.\nPrasad et al. [15] proposed a multi-sensor acquisition dataset containing 51 RGB and 30 Near-Infrared (NIR) image se- quences, but the scene diversity is also large because it is acquired under different weather conditions and on different days, but since it is a survival dataset, most of the sequences consisted of fixed terrestrial views and highly static scenes.\nRibeiro et al. [2] proposed a multi-sensor acquisition dataset containing 51 RGB and 30 NIR image sequences. This dataset provides considerable scene diversity, as it was captured under various weather conditions and on different days. However, since it is a surveillance dataset, most of the sequences consist of fixed terrestrial views and highly static scenes.\nSeaships [3] introduced 31,455 images with 1920\u00d71080 res- olution including six types of ships as target objects. The data set was acquired at a set time through the inland waterway's monitoring video system. That's why all the scenes have a certain directional view. The datasets currently available for marine environments focus on object detection models, with relatively fewer for segmentation. Recently, there have been proposals for benchmark datasets to evaluate the performance of segmentation models at sea. Bovcon et al. [4] proposed a dataset called \"MaSTr1325\" which is a marine semantic segmentation training dataset specifically designed to advance obstacle detection techniques in small coastal USVs. It consists of 1,325 different images taken over a two-year period using USVs, the various realistic conditions encountered during coastal surveillance missions. While each image was carefully labeled pixel-wise and synchronized with an onboard sensor for semantic understanding, only three labels (sky, water, and obstacle/environment) were labeled by annotation.\nMost recently, Bovcon et al. [16] released an evaluation dataset that complements the previously published maritime segmentation datasets [4], [13], [14]. This dataset consists of over 80k stereo images and has been recorded in multiple loca- tions, including various obstacles. Measurements were made at various times and weather environments for about seven months. The dataset is in two stages, with experts refining the per-pixel labeling following initial labeling tasks obtained from internet platforms. It is considered the most challenging benchmark dataset for marine environments because existing state-of-the-art models do not perform well on it."}, {"title": "B. Semantic Segmentation", "content": "It is important to consider the performance and inference time of deep learning models in developing solutions for real- world industrial applications; hence, we have focused on real- time and state-of-the-art performance segmentation models."}, {"title": "1) Real-Time Segmentation:", "content": "It is natural in AI models that a trade-off occurs, in which the performance decreases as the model operates fast. For real-time operation of the model, there are very simple methods such as using a lightweight backbone or applying a limited-sized input image. However, this lightweight backbone of the classification model is not perfectly suited to semantic segmentation and also cannot extract clear features from the small objects. Thus, various studies have been proposed to operate segmentation models in real-time in a way that utilizes lightweight backbones or extracts sufficient features with low computing costs. Fan et al. [17] proposed a backbone with a Short-Term Dense Concatenate (STDC) module that reduces composition costs and extracts rich features through scalable receptive fields and multi-scale information by reducing redundancy of two branch structures but failed to produce sufficient trade-off in terms of accountability and speed. BiSeNet [18] presented a two-branch network, allowing large receptive fields of detail and contextual information that are important in semantic segmentation. However, it was still difficult to expect high performance of real-time models due to the structural latency and expensive computational costs in Two-Branch Network (TBN) architectures. Therefore, many studies [18], [19] have been conducted in the direction of enriching deep features and reducing model computation costs while maintaining a TBN framework. Motivated by the fact that the architecture of TBN is similar to that of PI controllers, PIDNet [20], a novel three-branch network, was proposed to solve the overshooting problem occurring in the existing TBNs [17]\u2013[19]."}, {"title": "2) State-of-the-Art Segmentation:", "content": "Driven by the great achievements of transformer [21] in NLP, Vision Transformer (ViT) [22] introduced a transformer architecture for image classification that processes input images as sequential patch tokens. For semantic segmentation, SETR [23] adopts ViT [22] as a backbone for extracting features, achieving promising performance. Segmenter [24] proposes a transformer encoder- decoder architecture for semantic image segmentation. This approach relies on the backbone of ViT [22] and introduces a mask decoder inspired by DETR [25]. PVT [26] enriches features by constructing ViTs into pyramid structures, just as it extracts sufficient features through the connection of pyramid structures in CNN models. SegFormer [27] is designed to perform more simple and efficient segmentation tasks using hierarchically structured transformer encoders and lightweight decoders for multi-scale features. However, despite the high performance of these transformer-based methods [25]\u2013[27] the high cost makes them difficult to deploy in real-time applica- tions. Recently, InternImage [28] has designed convolutions in custom block-level architectures such as Transformers to de- sign a CNN-based foundation model. Using variations of flex- ible convolutions, called deformable convolutions (DCN) [29], [30], it performs comparably to transformer-based models."}, {"title": "III. OASIS: OCEAN AI SEGMENTATION INITIATIVES", "content": "Our main goal was to build a comprehensive dataset for evaluating the performance of vision models operating in ma- rine environments. The dataset mentioned above (Section II-A) is limited in diversity because it is difficult to collect data from various ships and ports, and it costs a lot to process the collected data. Therefore, we present OASIs, a more realistic and comparable important dataset of marine environments.\nFirst, we explain our data collection system and method (Section III-A). We next describe how the dataset images are processed (Section III-B) and how it is selected and configured (Section III-C)."}, {"title": "A. Dataset Acquisition", "content": "In this paper, we constructed a dataset by acquiring images related to the marine environment using the sensor module \u201cSxSM200N\u201d (Figure 2) from 2017 to 2023. This allowed us to gather images from various berths and ships, enabling us to construct an OASIs that encompasses a wide range of environmental variables. Unlike existing datasets that capture the sea at specific locations and times, resulting in limited scenes and styles, our proposed dataset encompasses a broader range of environmental conditions. The collection locations are major ports and waterways, including Ulsan and Busan in South Korea."}, {"title": "B. Dataset Annotation", "content": "The images in the OASIs varies in resolution, ranging from 1280\u00d7720 to 4032\u00d73024 pixels. The dataset, used for both training and evaluation, is labeled by experts. The images provided to the experts are selected from real-time captures that meet specific quality standards. This labeling process is consistently performed at the pixel level according to internally established guidelines. Semantic segmentation labels of OASIS are annotated to provide pixel-level classification, where each pixel in an image is assigned a color label corresponding to a particular class."}, {"title": "C. Dataset Configuration", "content": "The marine environment exhibits unique characteristics, with rapidly and dramatically changing weather conditions that highlight its distinct features. However, existing benchmark datasets do not contain images of different weather environ- ments and times, making it difficult to grasp the general perfor- mance of the models. Large-scale detection and segmentation learning datasets such as COCO [31] and PASCAL VOC [32] also lack images depicting various weather conditions at sea, even though they include a small number of images featuring marine environments and different weather scenarios. Several recent studies [33], [34] in the marine field have shown that the intensity and frequency of various extreme weather events have increased in the marine environment. Chen et al. [35] found that mAP values of DETR [25] appear with significant performance degradation of 93% and 78%, respectively, when synthesized rain-noise and haze-noise are added to existing data. In this paper, we propose a marine environment dataset OASIS, which is categorized into three types: daytime, weather conditions, and nighttime. Through these segmented datasets, the performance of AI models can be evaluated in each environment. OASIs was divided into three types as shown in Table II. The detailed characteristics of each type are described below."}, {"title": "1) Type-1: Day-Time Environment:", "content": "The most basic envi- ronment images collected during the daytime are included in the data Type 1. It contains data from general weather environments including normal lights, backlit, and several mild cloudy situations. Therefore, long-distance objects can be seen well, also it is an environment that can distinguish objects well and the boundary between the sea and the sky can be clearly distinguished. However, it also includes some scenes where the characteristics of a specific object are lost due to backlit, or where it is difficult to distinguish long-distance objects due to cloudy weather."}, {"title": "2) Type-2: Adverse Weather Environment:", "content": "In the case of Type-2, images collected in poor-weather environments such as rain and fog are included. As mentioned above, Collect- ing images in a marine environment are exposed to various weather environments (rain, sea fog, snow, and others). Type 2 includes foggy and rainy data. In a haze situation, long- distance objects are faint or only partially visible. In rainy conditions, raindrops cover the lens, and these physical con- straints show features of unclear boundaries between the sky, the ocean, the ground, and objects."}, {"title": "3) Type-3: Night-Time Environment:", "content": "To effectively utilize the vision model, it must perform well not only during the day but also at night when lighting conditions are insufficient. Type 3 includes images captured during the evening hours at ports and on ships. The evening maritime environment, with its significant lack of visible light, poses a substantial challenge for RGB sensors to capture information effectively. In these images, features for object identification are often barely visible, and sensor pixel saturation frequently occurs due to strong light. Additionally, the boundaries between the sea, land, and sky are often indistinct.\nThe three divided datasets vary in scene environments, including weather, lighting, and time. However, an analysis of the labeled pixel distribution reveals a similar pattern across all datasets. The label \u201cSea\u201d is the most prevalent, followed by \"Others\". The label \u201cLand\" and \"Sea Objects\" each constitute less than 7% of the total image pixels. This indicates that our evaluation dataset, OASIS is well balanced in terms of annotated labels."}, {"title": "IV. VADA: VERTICAL AND DETAIL ATTENTION", "content": "In this paper, we proposed a model for improving image recognition performance in marine environments, focusing on the following areas:"}, {"title": "A. Feature Extraction Backbone", "content": "The backbone of the proposed model is designed to extract robust features from various conditions present in marine environments. It has been proven effective in extracting fea- tures from factors such as strong lighting, sunlight-induced sea surface reflection, and interference. This allows the model to significantly enhance the accuracy of object recognition in marine environments. Additionally, the model is designed with fewer parameters to ensure fast inference speed."}, {"title": "B. Attention Module", "content": "Since data collected through installed cameras typically exist objects horizontally, an attention module operating ver- tically is added to the proposed model to distinguish horizon- tally existing objects. Particularly, this attention module greatly enhances the recognition performance of crucial elements, such as horizontal lines, in identifying maritime situations."}, {"title": "C. Loss Function", "content": "The model was trained using detail loss to ensure stable recognition of object edges even in images with diverse weather conditions and camera noise. This approach led to improvements in performance both in recognizing the edge portions of small objects under perspective and in capturing the detailed shapes of objects.\nThrough the proposed methods, this study aims to contribute to the enhancement of image recognition performance in marine environments."}, {"title": "V. EXPERIMENTS", "content": "In this section, we train the proposed model and state-of-the- art models using the training dataset provided by Seadronix Corp. We then compare and analyze their performance eval- uation on the proposed Seadronix evaluation dataset, OASIS. The key metrics used to evaluate the models are as follows:"}, {"title": "1) Intersection over Union (IoU):", "content": "An essential metric for evaluating accuracy in image segmentation. It calculates the ratio of the intersection area between the predicted segmenta- tion mask and the ground truth mask to the union area of the predicted and ground truth areas."}, {"title": "2) Integrated Figure of Calculation Performance (IFCP):", "content": "There are various metrics used to measure the performance of AI models. However, there isn't a single comprehensive performance metric that considers all of these metrics. In existing research, models' performance has been evaluated by comparing metrics such as CPU time, GPU time, mIOU, Multiply-Adds, Params, and Latency in a single table [36], [37]. However, this makes it difficult to judge the model's performance at a glance. To address this issue, we introduced a comprehensive metric named Integrated Figure of Calculation Performance (IFCP) to evaluate the overall performance of the model. IFCP is a metric that considers IoU, FLOPS[GB], GPU usage[GB], and parameters of models (Params[MB]). The calculation method of IFCP is as follows: The IoU value and other parameters are divided individually. Then, the harmonic mean (H) of these values is calculated. The formula for the harmonic mean is as follows:\n$H = \\frac{n}{\\sum_{i=0}^{n} \\frac{1}{x_i}}$"}, {"title": "", "content": "Through this process, the difference in measurement units of each parameter is adjusted, ensuring that each parameter is equally weighted. This computation enables fair contribution from all parameters and ensures an accurate evaluation of overall performance. It is defined by the following equation:\n$IFCP = \\frac{3}{\\frac{1}{IoU} \\times(\\frac{FLOPS}{IoU} + \\frac{GPU usage}{IoU} + \\frac{Params}{IoU})} = \\frac{3 \\times IoU}{FLOPS + GPU usage + Params}$"}, {"title": "A. Implementation Details", "content": "1) Training: In this paper, pretraining was applied to the models being compared according to the methods proposed in their respective papers. For the training protocols, efforts were made to maintain common settings across all models. The Rectified Adam (RAdam [38]) algorithm was chosen as the optimizer. The training was conducted for 100 epochs with an initial learning rate of 1e-3. Augmentation strategies were kept consistent as common training parameters across all models. For CNN-based models such as VaDA, PP-LiteSeg [17], PIDNet [20], BiSeNet [18], and InternImage [28], the training image size was set to 1280\u00d7720. For self-attention- based models, the training image size was set to 896\u00d7896.\n2) Inference: Inference speed was measured on a platform of NVIDIA TU104 based GPU, PyTorch 1.8 under Ubuntu. Batch normalization was integrated into each convolution layer, and the batch size was set to 1 for pair comparison. In the inference environment, the Frame Per Sec(FPS) of each model is as follows: [VaDA : 63.87, PP-LiteSeg: 64.64, BiSeNet: 53.15, PIDNet-L: 26.52, Internimage: 12.85, Segformer-B1: 8.30, ViT-Adapter : 8.11]"}, {"title": "B. Comparison of Model Computational Complexity", "content": "Table III provides a comparative analysis of metrics, includ- ing FLOPS, GPU usage, and GPU time, for models utilized in the inference environment, categorized by batch size:"}, {"title": "", "content": "By analyzing the table, we can understand how the model's operational characteristics and memory usage vary with batch size. In addition, we can determine the maximum batch size by examining the empty cells in the table. In the case of VaDA, it is evident that it can operate with up to 10 batches. This demonstrates that VaDA exhibits maximum operational efficiency in constrained environments."}, {"title": "C. Performance on evaluation OASIs", "content": "IFCP is a metric that demonstrates the overall performance of the model. Through this metric, a comprehensive assess- ment of the applicability of AI models on edge devices deployed in maritime environments can be made. Since IFCP evaluates not only recognition performance but also the overall metrics of the model, it's notable that transformer models and large models, which are unsuitable for edge devices, exhibit low IFCP values. In contrast, the proposed VaDA model exhibited the highest performance, with a value of 0.6422, as depicted in Figure 1. This suggests that when deployed on edge devices in maritime environments, the VaDA model is expected to demonstrate excellent performance.\nThe performance of the proposed VaDA model was evalu- ated through a comparison of real-time semantic segmentation models. Table IV presents the IoU and FPS in the evaluation dataset, OASIs, of real-time models. The mIoU performance of the proposed VaDA is the best at 0.7993. In terms of FPS, it is 0.772 slower than PP-LiteSeg, but this difference is negligible. Table V presents the mean IoU performance of each model for various types of OASIs. The VaDA model demonstrates the highest IoU values across all data types and maintains superiority even when compared on a per- class basis. Furthermore, Table VI illustrates the comparison results of the VaDA model with state-of-the-art models. In this comparison, the VaDA model exhibits the highest IoU values for Type-2 data. This finding indicates that the VaDA model delivers superior performance even in adverse weather conditions. Thus, it can be concluded that utilizing the VaDA model is advantageous, especially in maritime scenarios with rapidly changing weather environments."}, {"title": "VI. CONCLUSION", "content": "Existing large-scale benchmark datasets such as COCO [31], PASCAL VOC [32], ADE20K [39], Cityscapes [40], and KITTI [41] are suitable for evaluating segmentation models in autonomous vehicle applications. However, they lack suf- ficient representation of maritime environments, making them inadequate for evaluating segmentation models specifically designed for these settings. To address this gap, we annotated a dataset using images gathered from \"SxSM200N\" deployed in ports and aboard ships, following internal guidelines. The OASIS is better suited for evaluating segmentation models in maritime environments and will significantly facilitate the comprehensive evaluation of segmentation models for recog- nizing maritime objects under diverse conditions. Moreover, the VaDA model demonstrated remarkable recognition per- formance under varied weather conditions and proved to be highly suitable for deployment on edge devices. Utilizing the OASIS dataset, VaDA exhibited superior segmentation accu- racy compared to existing state-of-the-art real-time models. Additionally, it outperformed others in the newly proposed model evaluation metrics.\nThese research findings highlight the potential of the VaDA model as an efficient AI solution for real-world applications. Additionally, these results represent pioneering work in the de- sign and evaluation of computer vision AI systems specifically tailored for maritime environments.\nThe OASIS dataset and details are available at our website: https://www.navlue.com/dataset. While the OASIs benchmark dataset primarily serves as an evaluation resource for marine environments, we plan to release additional datasets obtained from various sensors installed on ports and vessels where our products are deployed. Furthermore, we are researching and developing a multimodal model that complements the deep- learning model in maritime environments and mitigates image information degradation caused by diverse weather conditions at sea through sensor fusion."}]}