{"title": "NeurLZ: On Enhancing Lossy Compression Performance based on Error-Controlled Neural Learning for Scientific Data", "authors": ["Wenqi Jia", "Youyuan Liu", "Zhewen Hu", "Jinzhen Wang", "Boyuan Zhang", "Wei Niu", "Junzhou Huang", "Stavros Kalafatis", "Sian Jin", "Miao Yin"], "abstract": "Large-scale scientific simulations generate massive datasets that\npose significant challenges for storage and I/O. While traditional\nlossy compression techniques can improve performance, balancing\ncompression ratio, data quality, and throughput remains difficult.\nTo address this, we propose NeurLZ, a novel cross-field learning-\nbased and error-controlled compression framework for scientific\ndata. By integrating skipping DNN models, cross-field learning, and\nerror control, our framework aims to substantially enhance lossy\ncompression performance. Our contributions are three-fold: (1) We\ndesign a lightweight skipping model to provide high-fidelity detail\nretention, further improving prediction accuracy. (2) We adopt a\ncross-field learning approach to significantly improve data predic-\ntion accuracy, resulting in a substantially improved compression\nratio. (3) We develop an error control approach to provide strict er-\nror bounds according to user requirements. We evaluated NeurLZ on\nseveral real-world HPC application datasets, including Nyx (cos-\nmological simulation), Miranda (large turbulence simulation), and\nHurricane (weather simulation). Experiments demonstrate that our\nframework achieves up to a 90% relative reduction in bit rate under\nthe same data distortion, compared to the best existing approach.", "sections": [{"title": "1 INTRODUCTION", "content": "The exponential growth in computational power has enabled the ex-\necution of increasingly complex scientific simulations across a wide\nrange of disciplines. Supercomputers are instrumental in conduct-\ning these simulations, allowing users to derive critical insights from\nvast amounts of generated data. However, despite the enhanced\ncomputational capabilities, users often face bottlenecks related to\ndata storage and network bandwidth, particularly when local anal-\nysis is required or when large datasets must be distributed across\nmultiple endpoints via data-sharing services. The sheer volume\nand high velocity of such data present substantial challenges for\nstorage and transmission, underscoring the urgent need for more\nefficient data compression solutions.\nInitially, researchers developed lossless compression algorithms,\nsuch as LZ77 [52], GZIP [34], FPZIP [23], Zlib [8], and Zstandard\n[3], to mitigate data storage and transmission challenges. However,\nthese techniques typically achieve only modest compression ratios\n(1x-3x) when applied to scientific data [50]. This limitation stems\nfrom the fact that lossless compression relies on identifying re-\npeated byte-stream patterns, while scientific data is predominantly\ncomposed of highly diverse floating-point numbers. To address\nthis challenge more effectively, researchers have recently turned to\nerror-bounded lossy compression as a promising alternative.\nCompared to lossless compressors, lossy compression algorithms\n[5, 18-20, 22, 27, 29, 41, 43, 49, 51] offer substantially higher com-\npression ratios (e.g., 3.3x to 436\u00d7 for SZ [5]) while preserving criti-\ncal information within user-defined error bounds. Many of these\nlossy compressors rely on predictive techniques, such as curve-\nfitting [5] or spline interpolation [17], to estimate the data values.\nThe compression efficiency is closely tied to the local smoothness of\nthe data. However, due to the complexity of scientific simulations,\ndatasets often contain spiky or irregular data points, which pose\nchallenges for predictive compressors. As a result, these compres-\nsors frequently need to store the exact values of spiky data points,\nthereby reducing the overall compression ratio while maintaining\na fixed level of error tolerance. To address these challenges, re-\nsearchers have explored more sophisticated prediction techniques.\nFor example, Zhao et al. introduced higher-order predictors, specifi-\ncally a second-order predictor [51], to improve compression perfor-\nmance. Similarly, Tao et al. advanced prediction accuracy for spiky\ndata by employing multidimensional predictors [41].\nAdditionally, implementing a selection mechanism from a pool\nof predictors has been shown to be advantageous [5, 41]. Di et al.\nproposed a method to identify the optimal predictor using just two\nbits during compression, with the predictor size being negligible,\nthereby improving the overall compression ratio [5]. Similarly, Tao\net al. introduced a suite of multilayer prediction formulas, allowing\nusers to select the most appropriate one based on the dataset charac-\nteristics, as different datasets may respond better to specific layers of\nprediction [41]. These predictor-based compressors predominantly\nemploy adaptive error-controlled quantization to ensure error lev-\nels remain within user-defined bounds. However, despite the high\ncompression ratios achieved by existing lossy compression algo-\nrithms, there remains a significant gap between the decompressed\ndata and the original scientific data, particularly when using high"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "er bounds. This discrepancy limits the reconstruction quality,\npotentially impeding scientific discovery.\nOver the past decades, deep neural networks (DNNs) have driven\nsignificant breakthroughs across a wide range of foundational and\napplied tasks in artificial intelligence, including image classification\n[35], object detection [30], action recognition [15], super-resolution\n[46], and natural language processing [45]. Researchers have at-\ntempted to leverage DNN models, such as those in [25, 26], to im-\nprove compression quality. However, integrating DNNs into lossy\ncompression algorithms presents several key challenges: 1) To effec-\ntively retain important information, DNN models are typically large.\nFor instance, the HAT model contains approximately 9 million pa-\nrameters [25], while the AE model has around 1 million parameters\n[26], leading to significant storage overhead when saving these mod-\nels to enhance compression. 2) DNN models often require retraining\nwhen applied to data from different scientific domains, which can\nbe both time- and resource-intensive. Although pre-trained weights\ncan be provided to eliminate the need for domain-specific retraining,\nthis approach may reduce specificity and compromise performance.\n3) DNN models may introduce additional computational costs com-\npared to traditional methods, such as interpolation-based predictors,\nwhich typically have linear time complexity.\nThis paper aims to address the aforementioned challenges and\nsignificantly enhance the quality of existing lossy compression tech-\nniques. We introduce NeurLZ, a novel cross-field learning-based,\nerror-controlled compression framework. The core innovation of\nNeurLZ lies in learning the residual information between the decom-\npressed data produced by lossy compressors and the original data using\nlightweight skipping DNN models, which integrate cross-field learning\nwith error management. In NeurLZ, decompressed values from each\nfield are processed by the skipping DNN models, which function\nas learned enhancers. Once trained, the models are appended to\nthe lossy compressors to improve the quality of the decompressed\ndata. Additionally, due to their lightweight design, these DNN mod-\nels introduce minimal storage and computational overhead, with\nshort training and inference times. Unlike compressors that require\nextensive pre-training [25, 26], NeurLZ offers rapid training of the\nlightweight models, making it highly adaptable to a wide range of\nscientific data. In summary, the key contributions of this paper are\noutlined as follows:\n\u2022 We propose NeurLZ, a novel cross-field learning-based, error-\ncontrolled compression framework, where lightweight skip-\nping DNN models act as enhancers. These models are trained\nin tandem with the lossy compression process and attached\nto the compressed data with negligible overhead. To the best\nof our knowledge, NeurLZ is the first learn-to-compress,\nerror-controlled framework that addresses the challenges\noutlined earlier.\n\u2022 We designed skipping DNN models as enhancers, utilizing\na skip connection mechanism to retain high-fidelity details,\ncapture multi-scale information, and recognize complex\npatterns. This design is particularly suited for scientific\ndata, significantly improving reconstruction quality by pre-\nserving essential information.\n\u2022 By leveraging cross-field relationships derived from the gov-"}, {"title": "2.1 Lossy Compression", "content": "Error-bounded lossy compression offers users the flexibility to se-\nlect their desired error bound, denoted by e. For the given error\nbound e, different lossy compressors may exhibit varying compres-\nsion quality. Within the lossy compression community, two primary\nmetrics are typically employed to evaluate the compression quality\nof a lossy compressor: compression ratio and data distortion. Tao et\nal. pioneered a selection method to optimize between SZ and ZFP\ncompressors [43]. Subsequently, Liang et al. further enhanced cus-\ntomization through the modular SZ3 framework [21]. Data analysis\nand evaluation were streamlined by Tao et al.'s Z-checker [42]. Liu\net al. introduced auto-tuning with QoZ, incorporating quality met-\nrics and dynamic dimension freezing [27]. Their FAZ framework\nbuilds upon these advancements, offering a broader range of data\ntransforms and predictors for even greater pipeline customization\n[28].\nError-bounded lossy compression empowers users to define their\ndesired error tolerance, denoted by e. For a specific error bound e,\ndifferent lossy compressors may yield varying levels of compression\nquality. Within the lossy compression community, two primary\nmetrics are commonly utilized to assess the effectiveness of a lossy\ncompressor: compression ratio and data distortion.\nCompression Ratio. The compression ratio CR quantifies the\nreduction in data size achieved by compression. It is calculated as\nthe ratio of the original data size |X| to the compressed data size |Z|,\nexpressed as CR = |X|/|Z|. A higher CR indicates more effective\ncompression, meaning the compressed data occupies less storage\nspace.\nData Distortion. Data distortion, inherent to lossy compression,\nis commonly measured using the Peak Signal-to-Noise Ratio (PSNR).\nPSNR is a key metric for evaluating the quality of decompressed\ndata from lossy compression. PSNR is defined as follows:\nPSNR = 20 log10 vrange(X) \u2013 10 log10 mse (X, X'). (1)\nConsidering input data X and its corresponding decompressed\ncounterpart X', we define vrange as the difference between the"}, {"title": "2.2 Deep Neural Networks", "content": "maximum and minimum values present within the input data array.\nFurthermore, let mse represent the mean-squared error quantifying\nthe discrepancy between the original and decompressed data. It is\nnoteworthy that a decrease in mse corresponds to an increase in\nthe PSNR, which serves as an indicator of enhanced fidelity in the\ndecompressed data.\nLossy Compressor and Information Entropy. In 1948, Shan-\nnon introduced the concept of information entropy, as defined in\nEq. 2 [38]. This definition focuses on a discrete source with a finite\nnumber of states, where successive symbols are assumed to be in-\ndependent. The quantity H(X) is also referred to as the first-order\nentropy of the source. Due to the lack of prior knowledge about the\nsource, each symbol X\u00a1 is treated as independently and identically\ndistributed (i.i.d.).\nH(X) = \u2212 \u2211 P(Xi) log2 P(Xi), where X\u00a1 are i.i.d.. (2)\nFirst-order entropy typically serves as an upper bound estimate for\nthe true entropy. This is because it is generally larger than higher-\norder entropy, which accounts for correlations between the Xi. The\nconsideration of these correlations is equivalent to modeling the\nsource. A common approach for representing such correlations\nand modeling the source is through the use of Markov models, as\nillustrated in Eq. 3 [38].\nP(xn Xn-1,...,xn-k) = P(xn | Xn-1,..., Xn-k....). (3)\nWhen a model of the source is sufficiently accurate, the estimated\nentropy can converge to the true entropy of the source. This pro-\nvides crucial guidance for data compression, as the entropy defines\nthe minimum average number of bits per symbol needed to encode\nthe data stream. Consequently, Shannon's theory laid the founda-\ntion, and researchers in the field of data compression strive to dis-\ncover correlations within data to create lower entropy data streams,\nwhich can then be efficiently compressed. Lossless compression\nalgorithms such as Huffman Coding [11], Arithmetic Coding [36],\nand LZ77 [52] all exploit correlations between symbols to reduce\nthe estimated entropy, thereby improving the compression ratio.\nHowever, for scientific data, which is typically represented in\nfloating-point format, traditional lossless compression algorithms\nmay not yield satisfactory results. These algorithms are designed to\noperate on discrete symbols, while floating-point numbers are con-\ntinuous. Directly applying lossless compression to float data often\nresults in a very low compression ratio. To harness the advantages\nof lossless compression, float data can be quantized and converted\ninto discrete integer representations. This introduces a degree of\nlossiness to the compression process, which is often acceptable\nin scientific applications. Moreover, some lossy compression tech-\nniques employ predictors to capture correlations in the data prior\nto quantization [2, 5, 16, 21, 24, 25, 41, 51]. These techniques are\nreferred to as prediction-based lossy compressors [6].\nPrediction-based lossy compressors typically involve four steps:\npointwise data prediction, quantization, variable-length encoding,\nand dictionary encoding [6]. The data prediction step aims to iden-\ntify hidden correlations or patterns within the data, allowing data\nentries to be predicted from others within a certain tolerance. This\nprediction is called the first-phase predicted value. In the quan-\ntization stage, the error between the first-phase predicted value"}, {"title": "2.3 Skip Connection Structure", "content": "Deep neural networks (DNNs) are a class of artificial neural net-\nworks distinguished by their multiple layers of interconnected\nnodes, or neurons, organized hierarchically. Each layer in a DNN\ntypically performs a nonlinear transformation on its inputs, en-\nabling the network to progressively extract increasingly complex\nand abstract features from the input data. To train a DNN repre-\nsented by function f, input data x is passed forward through the\nmodel, generating predictions \u0177 = f (x; 0), where represents the\nnetwork's parameters. The discrepancy between these predictions\nand the true values, or ground truth y, is then assessed. To optimize\nthe training objective, which is to minimize the loss function (as Eq.\n4) with respect to the parameters, the backpropagation algorithm\nis employed. Backpropagation calculates the gradients of the loss\nfunction with respect to the network's parameters, and these gra-\ndients are subsequently utilized to update the parameters through\noptimization algorithms such as stochastic gradient descent (SGD).\nmin l(f(x; 0), y), (4)\nIn this research, we adopt an image translation perspective to en-\nhance compression quality. Specifically, we treat the decompressed"}, {"title": "3 NEURLZ- LEARN FOR COMPRESSION", "content": "data as the input image x, and the enhanced data as the output\nimage f (x; O). The original, uncompressed data serves as the target\ny that we aim for the model to produce. The loss function l() is\ntypically chosen to be the mean square error (MSE), quantifying\nthe difference between the model's output and the target. Through\nthis process, the DNN is trained to learn the mapping that trans-\nforms the decompressed data into a higher-quality representation,\neffectively enhancing the overall compression quality.\nImage translation, also known as image-to-image translation, poses\na significantly greater challenge compared to traditional tasks like\nimage classification. It demands pixel-level accuracy in the output\nimage. This is particularly critical in scientific or medical domains\nwhere images contain crucial information across multiple scales.\nThe task necessitates considering both high-level abstract informa-\ntion and low-level details. Therefore, the ability to capture multi-\nscale features while preserving low-level details is paramount in\nimage translation tasks.\nResNet's skip connection architecture [9], empowers it to extract\nfeatures at various levels, ranging from low to high. These skip con-\nnections effectively propagate information from the earlier layers\nto the deeper ones by directly passing low-level features from the\ninput image to later layers, where they are added to the extracted\nfeatures. This mechanism aids in retaining the original image's fine\ndetails while allowing deeper layers of the network to focus on\nextracting high-level features.\nThe skip connection mechanism is not restricted to the \"add\"\noperation and can also be implemented as a concatenation oper-\nation [37]. In this approach, high-level information channels are\nconcatenated with low-level information channels, and these com-\nbined features are then fused to generate the output. Compared\nto ResNet's \"add\" skip connection, concatenation allows for richer\ninformation preservation without merging or losing any details.\nBy having both low-level and high-level features readily available,\nthe network is better equipped to make nuanced decisions during"}, {"title": "2.4 Problem Formulation", "content": "In this section, we present the detailed design of our NeurLZ. The\nproposed NeurLZ aims to enhance reconstruction quality with mini-\nmal overhead by utilizing lightweight learnable enhancers. These\nenhancers learn the mappings from decompressed data to the orig-\ninal data in a cross-field learning framework with integrated error\ncontrol mechanisms."}, {"title": "3.1 Design Overview", "content": "Our research objective in this work is to significantly enhance the\ncompression quality of error-bounded lossy compression. The core\nproblem addressed by our learning-based compression framework\ncan be summarized as follows: How can we substantially improve\nthe fidelity of the reconstructed data (denoted by X') from a struc-\ntured mesh dataset (referred to as X) while adhering to a strict\nuser-defined error bound (i.e., e) with minimal overhead?\nDrawing inspiration from the principles of image translation,\nwhere the model learns a mapping from the input image to the\noutput image, our NeurLZ framework leverages this concept to\nmap decompressed data from lossy compressors to the original\nscientific data. Specifically, we treat the decompressed data as the\ninput image and the original data as the target output image. In this\ncontext, the enhanced data generated by the model corresponds to\nthe output of an image translation process. The DNN-based models\nin our framework are trained to minimize the difference between the\ndecompressed and original data, effectively learning to \"translate\"\nthe decompressed data back into its original form. This approach\nnot only improves the quality of the reconstructed data but also\nensures that the data adheres to strict error bounds throughout the\ncompression process. Mathematically, the optimization problem\ncan be formally written as:\nmax PSNR(X, R(X'; \u0398)),\ns.t. xi - x \u2264 e, \u2200xi \u2208 X,\nsize(X)\n= T.\nsize(Z) + overHead\n(5)\nHere, X represents the input scientific data, Z denotes the com-\npressed data, and T is the target compression ratio. The total storage\noverhead, denoted as overHead, can be expressed as the sum of two\ncomponents:\noverHead = coordsOverhead + modelOverhead\nwhere coordsOverhead refers to the storage overhead incurred by\nstoring the coordinates of outlier points (discussed in detail in\nSection 3.5), and modelOverhead refers to the overhead associated\nwith storing the model parameters. O represents the parameters of\nthe skipping DNN models R that are trained during the compression\nprocess.\nThe objective is to maximize the PSNR between the original\ndata X and the reconstructed data R(X'; 0), while ensuring that\nthe error between the original data points xi and their correspond-\ning reconstructed values x remains within the user-defined error\nbound e, and that the overall compression ratio meets the target T.\nBy solving the problem formulated above, our NeurLZ framework\ncan either enhance the PSNR given a specified error bound, or sig-\nnificantly improve the compression ratio while maintaining a fixed\nPSNR. The following sections will provide a detailed discussion of\nthe specific implementation of NeurLZ."}, {"title": "3.2 Residual Learning: Improving Training Performance", "content": "The NeurLZ framework comprises two primary modules: the com-\npression module and the reconstruction module. These modules\nare visually represented in Figure 2 and Figure 3, respectively.\nIn the compression module, a continuous stream of scientific\ndata, exemplified by Nyx, Miranda, and Hurricane, is segmented\ninto blocks of a fixed size, such as (512, 512, 512) for Nyx. Each block\nis then individually processed by a standard lossy compressor, like\nSZ3, resulting in compressed data. Conventional lossy compressors\nrequire knowledge of whether the decompressed values are within\nan error bound. Therefore, decompressed data is also generated\nduring the compression process. NeurLZ temporarily stores this\ndecompressed data, and a residual map is calculated between the\ndecompressed data and the original data. The residual map has\nthe same dimensions as both the decompressed and original data.\nSubsequently, NeurLZ initializes a lightweight DNN model with a\nskip connection architecture for each block. This model is trained\nto predict the residual map, essentially representing the compres-\nsion error, by feeding the decompressed data into the DNN model.\nFinally, NeurLZ appends the trained weights, W, of these DNN mod-\nels, which are negligible in size due to their lightweight nature, to\nthe compressed data.\nIn addition to the trained weights, the coordinates of data points\nthat the DNN model fails to predict accurately are also attached\nto the compressed data. Since the output of the DNN cannot be\nfully controlled, some data points may fall outside the error bound\neven with the predicted residual value, despite the decompressed\ndata generated by the lossy compressor being within the error\nbound. To address this, we identify and store the coordinates of such\ndata points, referred to as outlier points. This indicates that during\ndecompression, the original decompressed data should be retained\nfor these points, with no residual value added at all. The final\ncompressed format thus comprises the compressed data itself, the\ntrained DNN weights, and the coordinates of these outlier points.\nThe reconstruction module functions as the counterpart to the\ncompression module. In this module, NeurLZ first extracts the\ntrained weights from the compressed format and uses them to ini-\ntialize DNN-based enhancer models. Subsequently, NeurLZ decom-\npresses the scientific data block by block. Next, NeurLZ leverages\nthe lightweight DNN models to predict the corresponding residual\nmaps. Finally, the residual map is added to the decompressed data\nto obtain the initial enhanced data. For data points identified as\nunpredictable based on the saved coordinates, the corresponding\ndecompressed values replace the enhanced values. This process\nresults in the generation of final enhanced data, which serves as\nthe output of our reconstruction module, as will be introduced in\nSection 3.5."}, {"title": "3.3 Lightweight Skipping DNNs: Capturing Complex Patterns and Providing High-Fidelity Detail Retention", "content": "The primary challenge in the NeurLZ learning process is effectively\nlearning to enhance the decompressed data to closely match the\noriginal data. In scientific simulations, the value range can be quite\nlarge. For instance, in the Nyx dataset, the Temperature field ex-\nhibits a significant range, with the difference between the maximum\nand minimum values reaching as much as 4.78 \u00d7 106 [50]. This wide\nrange exacerbates training instabilities due to gradient oscillations,\nleading to suboptimal training performance.\nWhile it may be challenging to directly predict the original data,\nit is feasible to estimate the residual data, which is the difference\nbetween the original data and the decompressed data. In the field of\nimage restoration, models such as DnCNN [48], MemNet [40], and\nResDNet [14] employ residual learning to mitigate training difficul-\nties. Drawing inspiration from this approach, our NeurLZ also incor-\nporates a residual learning strategy. Specifically, NeurLZ treats slices\nof scientific data as single-channel images. Our residual learning\napproach focuses on learning the residual information (R = X \u2013 X')\nbetween the decompressed data (X') and the original data (X) on a\nslice-by-slice basis. By leveraging this design, the magnitude of the\nlearning output is constrained to the level of the error bound, which\nis significantly smaller than the range of the original data values.\nConsequently, the model benefits from more stable learning and,\nonce well-trained, can accurately predict the residual information,\nthereby enhancing the decompressed data to more closely resemble\nthe original data.\nThe PSNR curves in Figure 4 illustrate the effectiveness of our\nresidual learning strategy over the conventional non-residual ap-\nproach. We evaluated this across four fields from the Nyx dataset:\nDark Matter Density, Baryon Density, Temperature, and Velocity Y.\nThese fields were compressed using the SZ3 lossy compressor with\na relative error bound of 1E-3, and then decompressed. In residual\nlearning, we predict the difference between the original and de-\ncompressed values (the residual). This approach (orange solid line)\nconsistently boosts PSNR across all fields, indicating a substantial\nreduction in reconstruction error. In contrast, non-residual learning\n(green solid line), which directly targets the original values, ex-\nhibits less consistent and overall lower performance. These results\ndemonstrate that residual learning provides a robust framework for\nreducing errors and enhancing the quality of data reconstruction\nin lossy compression scenarios.\nUpon completion of the learning process in the NeurLZ compres-\nsion module, the DNN model predicts the residual map R from the\ndecompressed data X' generated by the lossy compressor in the\nNeurLZ reconstruction module. The predicted residual information\nis then added to the decompressed data, thereby improving the\nreconstruction quality, i.e., X = X' + R."}, {"title": "3.4 Cross-Field Learning: Capturing the Complex Patterns across Domains", "content": "As elaborated in Section 2.3, the skip connection structure em-\npowers DNNs to capture information across various scales. Conse-\nquently, we implement this structure and subsequently introduce"}, {"title": "3.5 Outlier Points Management: Providing the Strict Error Bounds", "content": "lightweight skipping DNNs, as illustrated in Figure 6. The input\ndata undergoes multiple down-sampling and up-sampling opera-\ntions via convolution and deconvolution, respectively, to acquire\ninformation at different scales. The skip connection (the curved\ndashed arrow in Figure 6) then concatenates low-level features with\nlater layers to create a richer representation. Finally, the concate-\nnated features are fused, yielding a single-channel output from\nthe skipping DNN. Furthermore, to ensure stable learning, residual\nlearning is implemented for optimal training performance [12, 48].\nThe decompressed data serves as the model's input, and the model\ngenerates a predicted residual map, which is added to the decom-\npressed data to produce the enhanced data via NeurLZ.\nLeveraging the advantages of the skip connection structure,\nspecifically its ability to capture multi-scale information, our skip-\nping DNNs can effectively capture complex patterns and preserve\nhigh-fidelity details. As demonstrated in Figure 5, the non-skipping\nmodel, which lacks these skip connections, exhibits a lower PSNR\n(green solid line) compared to our NeurLZ model (orange solid line),\nwhich utilizes skip connections. Additionally, the use of convo-\nlution and deconvolution operations contributes to the model's\nlightweight nature. For instance, a 10-layer network can be con-\nstructed with just 3,000 parameters. This compactness makes it\nparticularly well-suited as part of format transmitted to the user,\nespecially when compared to Transformers, which can necessitate\nbillions of parameters [25, 32].\nScientific applications often involve modeling multiple physical\nfields within a given spatiotemporal domain, such as temperature,\nvelocity, and density in the Nyx cosmological simulation. These\nsimulations are governed by complex equations, such as the Dark\nMatter Evolution and Self-gravity Equations, which describe the\ninteractions between different fields [1, 31, 33]. Existing prediction-\nbased lossy compression techniques [5, 21, 24, 25, 41, 51] typically\nrely on predictors that are only capable of capturing localized,\nsimple patterns in time and space. Consequently, these methods\nfail to exploit the intricate correlations between different physical\nfields.\nIn this research, we implement cross-field learning using skip-\nping DNNs to capture the complex patterns that span across dif-\nferent domains. An illustration of cross-field learning is shown in\nFigure 7. To predict the value of a single pixel (indicated by the\nred cube in the Temperature field), our approach utilizes not only\nthe values of neighboring pixels but also the values from pixels in\ntwo other fields. Moreover, the patterns of values utilized are not\nfixed but are instead learnable. The skipping models are trained to\nidentify these patterns across different fields. Consequently, once\nthe model is trained, the prediction for a single pixel is informed by\ndata from both the same field and from different fields. By learning\nthese cross-domain correlations, the skipping model provides a\nmore accurate representation of the underlying information source\nthat generates the scientific data. This leads to a lower entropy\nestimation, consequently yielding a higher compression ratio, as\ndiscussed in Section 2.1."}, {"title": "4 EVALUATION", "content": "As outlined in Section 3.1, the inherent uncontrollability of DNN\noutput means that some data points, even when enhanced with\npredicted residual values, might exceed the error bound, even if the\nlossy compressor's decompressed data remain within it. We refer\nto these points as outlier points. To uphold the error-bounded guar-\nantee, we implement a strategy that draws inspiration from SZ3's\napproach of differentiating between predictable and unpredictable\nvalues using distinct quantization codes. Just as SZ3 retrieves the\noriginal value from an unpredictable queue upon encountering a 0\nquantization code during decompression [41], we similarly store\nthe coordinates of outlier points - those data points whose enhanced\nvalues, derived from the addition of predicted residual values, lie\nbeyond the acceptable error threshold as depicted on the left side\nof Figure 8. Subsequently, once the enhanced values are computed,\nwe leverage these stored coordinates to pinpoint and address these\noutliers. We replace their out-of-bound enhanced values with their\ncorresponding in-bound decompressed values, ensuring that all\ndata points remain within the predefined error limits, as illustrated\non the right side of Figure 8. This approach, mirroring SZ3's mech-\nanism of handling unpredictable values, empowers us to effectively\nmanage and rectify outliers. By adopting this strategy, we can fully\nleverage the capabilities of DNNs in discerning complex patterns\nwhile simultaneously mitigating the inherent risks associated with\ntheir uncontrollability. This guarantees that the enhanced data con-\nsistently adheres to the specified error bound, thereby preserving\nthe accuracy and reliability of the data representation.\nStorage Overhead for Outlier Coordinates. As discussed ear-\nlier, NeurLZ provides a strict error bound by storing the integer\ncoordinates of outlier points to index the decompressed data and\nretrieve the corresponding decompressed values. The storage over-\nhead for these coordinates is calculated as follows:\nN\navgBit = \u2211 log2(dimi)\ni=1\ncoordsOverhead = numOutliers \u00d7 avgBit (6)\nHere, dimi represents the size of the i-th dimension, while avg-\nBit denotes the average number of bits required to store the N-\ndimensional coordinates of a single datum. numOutliers refers to\nthe number of outlier points. coordsOverhead is the storage over-\nhead incurred by storing the coordinates of outlier points, expressed\nin bits, calculated by multiplying avgBit by numOutliers. For the\nscientific datasets discussed in this paper, the block size for Nyx is\n(512, 512, 512) with an avgBit of 27.0 bits; for Miranda, the block\nsize is (256, 384, 384) with an avgBit of 25.2 bits; and for Hurricane,\nthe block size is (100, 500, 500) with an avgBit of 24.6 bits. These\nvalues highlight the variability in storage overhead depending on\nthe dataset's dimensionality and structure."}, {"title": "4.1 Experimental Setup", "content": "This section outlines the experimental design used to assess the\nperformance of NeurLZ", "experiments": "nNyx", "1": "is a popular choice in the cosmological research\ncommunity. It contains six 3D array fields", "fields": "Temperature\nand Dark Matter Density. The Nyx dataset is stored in FP32 format.\nThe Miranda dataset, produced by a structured Navier-Stokes flow\nsolver with multi-physics capabilities, is designed for large turbu-\nlence simulations [4", "50": "."}]}