{"title": "Efficient Bias Mitigation Without Privileged Information", "authors": ["Mateo Espinosa Zarlenga", "Swami Sankaranarayanan", "Jerone T. A. Andrews", "Zohreh Shams", "Mateja Jamnik", "Alice Xiang"], "abstract": "Deep neural networks trained via empirical risk minimization often exhibit significant performance disparities across groups, particularly when group and task labels are spuriously correlated (e.g., \"grassy background\" and \"cows\"). Existing bias mitigation methods that aim to address this issue often either rely on group labels for training or validation, or require an extensive hyperparameter search. Such data and computational requirements hinder the practical deployment of these methods, especially when datasets are too large to be group-annotated, computational resources are limited, and models are trained through already complex pipelines. In this paper, we propose Targeted Augmentations for Bias Mitigation (TAB), a simple hyperparameter-free framework that leverages the entire training history of a helper model to identify spurious samples, and generate a group-balanced training set from which a robust model can be trained. We show that TAB improves worst-group performance without any group information or model selection, outperforming existing methods while maintaining overall accuracy.", "sections": [{"title": "1 Introduction", "content": "It is often difficult to place the resounding success of Deep Neural Networks (DNNs) [32,60,70,79] in conjunction with their tendency to exploit biases within their training sets [12,16,27,31,51]. Nevertheless, in visual datasets, arguably the driving force of several consumer-facing AI models, such biases have been long known to be exploited and amplified by models trained on their data [84, 93]. The difficulty of addressing these biases lies in their ubiquity and diversity: biases may arise from spurious correlations between salient attributes (e.g., \"water background\") and classification task labels (e.g., \u201cboat\") [16,27,71,75], as well as from underrepresentation of diverse geographical/cultural artifacts [15,81], gender-related attributes [34,82,92], object subcategories [95], and skin tones [12]. Such multi-source provenance of dataset biases begs one to question the ethical [52,64]"}, {"title": "2 Problem Formulation", "content": "Setup In this paper, we study bias in the form of spurious correlations within supervised learning. We assume we have a training set $D_{train} := \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$ where each sample $x^{(i)} \\in R^n$ and label $y^{(i)} \\in \\{1,\\dots,L\\}$ are sampled from a distribution $P_{train}(x, y)$. Our goal is to learn the parameters $\\theta \\in \\Theta$ of a DNN $f_{\\theta} : R^n \\rightarrow \\{1,\\dots,L\\}$ so that $f_{\\theta}$ maps each sample $x^{(i)}$ to its label $y^{(i)}$.\nTraditionally, $\\theta$ is learned by minimising the expected value of a loss function $l(f_{\\theta}(x^{(i)}), y^{(i)})$, which reflects how accurately $f_{\\theta}(x^{(i)})$ approximates $y^{(i)}$. As $P_{train}$ is generally unknown in practice, given a finite training set $D_{train}$, the optimal parameters $\\theta^*$ are usually estimated by optimising the following Empirical Risk Minimisation (ERM) [78] objective via gradient descent:\n$\\theta^* = arg \\min_{\\theta \\in \\Theta} E_{(x,y)\\sim P_{train}(x,y)} [l(f_{\\theta}(x), y)] \\approx arg \\min_{\\theta \\in \\Theta} \\frac{1}{N} \\sum_{i=1}^N l(f_{\\theta}(x^{(i)}),y^{(i)})$   (1)\nWorst-group Performance When spurious correlations exist between irrelevant yet salient features (e.g., \"grass\") and downstream labels (e.g., \"cow\u201d), ERM is known to encourage the exploitation of such \u201cshortcuts\", resulting in the unfair treatment of bias-conflicting samples\u2014those lacking the spurious correlation [16, 27, 49]. Such behaviour can go unnoticed when using traditional population-level evaluation metrics (e.g., average accuracy). For example, in an object detection dataset where 98% of images depict \u201ccows\u201d on \u201cgrass\u201d, a model could achieve a mean accuracy of 98% by predicting \"cows\" whenever \"grass\", a salient feature, appears. This statistic, however, hides the model's inability to detect cows on diverse backgrounds, such as \"roads\", where its accuracy could be 0%, rendering it unfit for deployment in, say, self-driving car pipelines.\nTherefore, we study the performance of DNNs across all population subgroups. That is, we assume every sample $(x^{(i)}, y^{(i)})$ belongs to one of k groups $g^{(i)} \\in \\{1,\\dots, k\\}$, and analyse models through their worst-group accuracy (WGA):\nWGA(f_{\\theta}, P) := \\min_{g \\in \\{1,2,...,k\\}} E_{(x,y)\\sim P(x,y|g)} [1(f_{\\theta}(x) = y)]     (2)\nwhere 1(.) is the indicator function and P(x,y|g) is the joint distribution of samples in group $g\\in \\{1,\\dots, k\\}$. We note that we assume knowledge of group identities only during test time and not for training or validation. With these labels, we can compute test WGA via the empirical worst-group mean accuracy."}, {"title": "3 Related Work", "content": "Previous research studying the effect of spurious correlations on DNNs falls into two camps: (1) bias mitigation (BM), and (2) bias discovery (BD). Here, we first discuss BM methods, separating them between supervised and unsupervised approaches, and then discuss related BD works.\nGroup-supervised BM When group annotations are available (i.e., we know each sample's subgroup), group-supervised BM methods improve a DNN's worst-group performance by leveraging these labels during training. Distributionally robust optimisation (DRO) methods [9,20,65,86], for example, introduce a group-aware loss function to learn models that are robust to distribution shifts. Within these approaches, Group DRO (G-DRO) [33,53,65] is typically used to train oracle-like unbiased models, minimising the empirical worst-group training loss. Other works instead propose reweighting [63] or rebalancing [38,42,83] samples in the training distribution, assigning larger weights to underrepresented subgroups. In contrast, representation learning approaches, such as adversarial methods [61,90] and invariant representation learning [1], propose learning robust representations from which protected attributes (e.g., \"gender\") cannot be predicted. Finally, recent re-annotation methods explore using a small group-annotated set to generate pseudo-labels for a larger annotated set [50].\nGroup-unsupervised BM The need for group annotations during training renders group-supervised BM approaches impractical for real-world workloads where biases are unknown and annotations are prohibitively expensive. Several group-unsupervised BM methods have been proposed to circumvent this limiting requirement. Promising directions include: (1) discovering pseudo-group labels to run a group-supervised pipeline [8,14,46,49,72]; (2) introducing a loss that incentivises group-robustness [43,58]; and (3) learning a rebalancing of the data to perform group-balanced retraining [42, 46]. Other approaches involve constructing ensembles for learning robust classifiers [35], or dynamically labelling pseudo-groups through adversarial training [45,55]. More recent works have explored self-supervised pretraining pipelines for robust representation learning [41], and introduced neural architectures with robustness-aware inductive biases [6,69,73].\nAmong these works, the methods closest to ours are \"Just Train Twice\" (JTT) [46] and \"Learning from Failure\" (LfF) [49], as they both use an \"identifier\" model's output to identify bias-conflicting samples. JTT achieves this through a two-pass solution where an identifier model $f_{(id)}$ is trained for a small number of epochs T, and then $f_{\\theta}$ is trained from scratch on a training set where samples mispredicted by $f_{(id)}$ fo are upweighted by a factor of $\\lambda_{up}$ (a hyperparameter). In contrast, LfF jointly learns a bias identifier $f_{(id)}$ and a debiased model $f_{\\theta}$ by dynamically weighting $f_{\\theta}$'s samples proportionally to the biased model's loss."}, {"title": "4 The Price of Unawareness", "content": "In this paper, we are interested in exploring bias mitigation without privileged information such as group labels during training or validation. Here, we discuss why this task is particularly challenging, highlighting the inherent difficulty of model selection when validation group labels are absent. For this, we analyse the training dynamics of a ResNet-34 [30] model trained on Waterbirds [33], a bird detection task with known spurious correlations (see \u00a76), to illustrate why mean accuracy-based model selection is prone to selecting suboptimal hyperparameters for BM methods (Figure 4). We use our observations to first frame this difficulty as a cost paid by BM pipelines requiring model selection when they are unaware of biases in their validation sets and then motivate our method in the next section."}, {"title": "5 Targeted Augmentations", "content": "In the previous section, we observed that existing BM approaches may fail to address biases in the absence of group information during model selection. Specifically, we argued that this is an inherent limitation of pipelines that involve several hyperparameters or where the number of hyperparameters leading to an ERM-like hypothesis is high. This limitation underscores the importance of considering the type and number of hyperparameters when designing BM pipelines. In this section, we build on this realization to design a new hyperparameter-free unsupervised BM pipeline. Our approach is inspired by two key insights:\n1. Loss histories are rich representations. Both JTT [46] and LfF [49] use a biased helper model to identify error slices for upweighting. Their high PoUs (seen in Figure 3) suggest that when an identifier model learns a biased hypothesis, either by stopping at the \"right\" epoch (in JTT) or by properly"}, {"title": "5.1 Proposed Pipeline", "content": "Based on these insights, we introduce Targeted Augmentations for Bias mitigation (TAB). Our approach, summarised in Figure 1 and in Algorithm D.1 in Appendix D, is a hyperparameter-free unsupervised BM pipeline that exploits the entire training history of a helper model to generate a group-balanced dataset for robust training. Given a training set $D_{train}$, a model $f_{\\theta}$, and a learning algorithm L (e.g., ERM with SGD), TAB learns a robust classifier as follows:\n1. (Loss history embedding construction) First, we learn an identifier helper model $f_{\\theta_{base}}$ by running L on the training set $D_{train} = \\{(x^{(i)},y^{(i)})\\}_{i=1}^N$. While running L, we keep track of the training loss of all samples in $D_{train}$ throughout training. Hence, if we run L for T steps, we construct a set of loss history embeddings $H = \\{h^{(i)} \\in R^T\\}_{i=1}^N$ where $h_{t}^{(i)}$ encodes the loss $l(f_{\\theta_{base}}^{t}(x^{(i)}), y^{(i)})$ for sample $(x^{(i)},y^{(i)})$ at step t.\n2. (Loss-aware partitioning) Once loss history embeddings are constructed, we exploit their discriminative power to separate bias-conflicting from bias-aligned samples (insight #1). For this, we assign each sample in class $y \\in \\{1,\\dots, L\\}$ one of two pseudo-groups discovered by splitting the history embeddings of samples in class y into two clusters. Although several clustering methods would fit this purpose [3, 68, 85], we cluster loss histories using k-means (k = 2) for simplicity. This partitions $D_{train}$ into 2L \"pseudo-groups\" $\\{(D_l^+, D_l^-) | l \\in \\{1,\\dots, L\\}\\}$, two per class, where, without loss of generality, we let $D_l^+$ be the larger cluster for class l. We partition the data on a per-class basis since spurious correlations tend to be class-aligned.\n3. (Group-balanced dataset generation) Next, we generate a pseudo-group-balanced training set $D'_{train}$ by upsampling each underrepresented subgroup, $D_l^-$, to match the representation of samples in $D_l^+$. Inspired by our insight #2 above, we achieve this as follows: for each class $l \\in \\{1,\\dots ,L\\}$, we construct an \"augmented\" multiset $D'^-$ by selecting $z_l = |D_l^+|-|D_l^-|$ samples from $D_l^-$ uniformly at random. Then we construct a group-balanced dataset $D'_{train}$ by adding all augmentations to $D_{train}$:\n$D'_{train} := (\\bigcup_{l=1}^L D_l^+) \\cup (\\bigcup_{l=1}^L (D_l^-, \\bigcup D_l^-)) = D_{train} \\cup (\\bigcup_{l=1}^L D_l^-)$   (3)\n4. (Robust model training) Finally, we train a robust model $f_{\\theta_{TAB}}$ from scratch by running L on $D'_{train}$. As we expect $D'_{train}$ to be more group-balanced than $D_{train}$, this results in $f_{\\theta_{TAB}}$ learning to generalise better."}, {"title": "6 Results", "content": "We study TAB via four questions capturing important practical considerations:"}, {"title": "6.1 Setup", "content": "Datasets We explore a combination of synthetic and real-world vision tasks. Our synthetic tasks are based on the Colour-MNIST [18,49] dataset where samples are handwritten digits. We construct two setups enabling evaluation on binary (i.e., L = 2) and multi-class (i.e., L > 2) tasks: (1) Even-Odd, whose task is to predict the digit's evenness (L = 2), and (2) cMNIST, whose task is to predict the digit's value (L = 10). All digits are coloured with one of L colours where there is a spurious correlation of strength $p\\in [50\\%, 100\\%]$ between the l-th colour and the l-th class. This is done by colouring p% of all images of label $l\\in \\{1,..., L\\}$ with the l-th colour, and the rest with a randomly selected colour.\nWe include three real-world datasets commonly used to evaluate model robustness: (1) Waterbirds [65] (L = 2), a binary bird classification where there is a spurious correlation between the background of the image and the class of the image; (2) CelebA [47] (L = 2), a facial dataset where perceived gender is spuriously correlated with \"Blonde Hair\"; and, (3) BAR [49] (L = 6), an action dataset where each action is spuriously correlated with a scene. Finally, we use CUB [80] (L = 200), a bird classification task, as a control task without biases.\nAlthough BAR has no group annotations, its test set has out-of-distribution background scenes for each action w.r.t. the train set. Hence, for BAR we use worst-class accuracy as a proxy for the WGA. Furthermore, to tractably explore different method hyperparameters within our computational budget (see below), for CelebA we use 15% of the official training set (N \u2248 25,000). Finally, as Waterbirds's standard validation and test sets are group-balanced, when computing mean accuracies we weight each sample based on their training group distributions to simulate in-distribution evaluation and avoid implicit privileged information leakage. We include further details for all datasets in Appendix E.\nBaselines We compare TAB against JTT [46] and LfF [49] due to their overlapping motivation with TAB. We also include MaskTune [6], a method that finetunes an ERM model after masking $D_{train}$ using its saliency maps, as a recent popular unsupervised BM method. Moreover, we include: (1) an ERM baseline as an upper-bound for mean accuracy and a lower-bound for WGA, and (2) a"}, {"title": "6.2 Worst-group Performance (Q1)", "content": "TAB improves worst-group performance across all tasks. In Table 1 (top) we see that TAB achieves better WGA than the equivalent ERM baselines"}, {"title": "6.3 Average Performance (Q2)", "content": "TAB achieves competitive mean accuracy against ERM models. In Table 1 (bottom), we show the average test accuracies for all baselines. We observe that TAB achieves a competitive average accuracy w.r.t. ERM while maintaining a higher WGA across all tasks. In particular, TAB attains the highest mean accuracies across all baselines for Even-Odd, CMNIST, and BAR, while suffering a small hit w.r.t. ERM for Waterbirds and CelebA (less than a 0.9%"}, {"title": "6.4 Computational Efficiency (Q3)", "content": "Considering model selection, TAB's pipeline leads to shorter training times. In Figure 5 (right), we show the total wall-clock times taken to generate the results in Table 1 for the cMNIST task (N \u2248 40,000). We consider both the training time of the hyperparameters resulting in the longest run (left/solid) and the total time taken to train every attempted hyperparameterisation of each method as a function of TAB's single worst run (right/crossed). Our results show that although a single TAB run can come with a computational penalty due to its identifier model (at its worst, about 2.5\u00d7 slower than ERM), this cost is significantly less than that of competing methods when considering their need for model selection (here, TAB is 1.14\u00d7 faster than MaskTune, the method with the second shortest total model-selection time, and 14.8\u00d7 faster than JTT, the method with the longest total time). Overall, this makes TAB appealing for practical use in modern workloads where model selection scales poorly."}, {"title": "6.5 Bias-conflicting Identification (Q4)", "content": "TAB augmentation leads to more group-balanced datasets. In Table 2 (top) we show the representation of bias-conflicting examples in the original training sets of each task vs their representation in TAB's augmented dataset"}, {"title": "7 Discussion and Conclusion", "content": "Limitations Although TAB improves WGA without group labels or model selection, it comes with some limitations. First, the model training loop needs to be run twice, which can become a limitation in massive datasets. Moreover, TAB's augmentations require storing the loss history matrix H (which can have O(NT) size) and can lead to the training set doubling in size (in the worst case), increasing training times. We argue, however, that these computational costs are amortised because TAB operates without a costly model selection. A related limitation is that TAB requires k-means to operate, a method known to have scalability issues for large datasets. Nevertheless, as shown in Appendix I, TAB's improvements can transfer when using a mini-batched k-Means [66], a scalable clustering method. Finally, we observe that in some highly imbalanced tasks, such as CelebA, TAB's WGA improvements over ERM are moderate compared to its improvements in other tasks (e.g., Waterbirds). Future work could study how to improve this by (a) performing symmetry-invariant transformations of the augmented samples (we explore with some success in Appendix H), (b) learning better discriminative embeddings through representations that capture both a sample's loss history and its semantics, or (c) exploiting additional information, such as the number of groups, by splitting the history embedding space into more than two clusters. For a further discussion on ethical considerations and potential negative societal impacts of this work, please refer to Appendix A.\nConclusion In this paper, we studied biases in the form of spurious correlations and their effects on DNNs. We first argued that current unsupervised bias-mitigation methods are prone to learning biased models due to the inherent difficulty of group-unsupervised model selection. Then, we addressed this gap by proposing Targeted Augmentations for Bias mitigation (TAB), a simple but powerful hyperparameter-free bias mitigation pipeline that can be used on any learning pipeline. TAB learns a robust classifier by first discovering bias-conflicting training samples based on the entire loss histories of a pre-trained model, and then generating a group-balanced dataset from which a new model can be trained. We showed that TAB significantly increases worst-group accuracy across multiple tasks without any model selection or a severe sacrifice to mean accuracy. These results suggest that TAB, an approach requiring only a few lines of code to be implemented, represents a move towards practical debiasing."}, {"title": "A Ethical Considerations", "content": "Our work improves a DNN's fairness through a pragmatic and effective pipeline. We hope this may lead to TAB being included as a standard debiasing good practice in real-world workloads. Nevertheless, we recognise there are potential negative societal aspects to consider. First, TAB trains its model twice, doubling the carbon footprint of training [4]. This, however, can be amortised in practice by TAB's lack of model selection. Second, the ability of TAB to identify a large portion of bias-conflicting samples implies that bad actors may use this information to further increase a model's bias (e.g., by removing these training samples). Nevertheless, we argue that this ill-intended use of our method is not unique to our approach, but rather an inherent but unlikely negative aspect of all BM methods."}, {"title": "B Summary of Previous Bias Mitigation Methods", "content": "To complement our related work discussion in Section 2, in this section we include a summary of previous work that is closely aligned with our proposed method. In Table B.1, we show several bias mitigation methods together with (1) their level of group supervision (whether they need group labels in the training set, in the validation set, or in both), and (2) the set of key hyperparameters that require fine-tuning each method. We note, however, that most of these approaches have more underlying hyperparameters than those shown as they may still require fine-tuning of hyperparameters specific to the target model (e.g., the underlying model's weight decay, learning rate, batch size, etc). Nevertheless, as these are hyperparameters that are not unique to any BM method but rather a necessity of the model chosen to be debiased, in Table B.1 we show only the BM-specific hyperparameters. By contrasting methods this way, we see how TAB is unique within fully unsupervised BM approaches by not requiring any extra hyperparameters, avoiding a potentially costly model selection and enabling easy adoption in modern training pipelines."}, {"title": "C Price of Unawareness and Mean Model Selection", "content": "In this section, we formally define the metrics discussed in Section 4 and describe how we can compute such metrics in practice.\nPrice of Unawareness (PoU) Borrowing inspiration from similar con-cepts in game theory [40,54], we capture the cost \u201cpaid\u201d by a BM pipeline when lacking proper group labels during model selection through its price of unawareness. Formally, given a BM pipeline $B_{\\gamma}$ with hyperparameters $\\gamma \\in \\Gamma$, let $f_{\\theta(B_{\\gamma}, P_{train})}$ be the parameters $\\theta \\in \\Theta$ learnt by running $B_{\\gamma}$ on a training set formed by N i.i.d. samples from $P_{train}$. In this setup, we define the PoU of B as follows:\nPOU(B, Ptrain, Ptest, Pval) := $\\frac{\\max_{\\gamma' \\in \\Gamma} WGA(f_{\\theta(B_{\\gamma'},P_{train})}, P_{test})}{WGA(f_{\\theta(B_{\\gamma val-acc},P_{train})}, P_{test})}$ (C.1)\nwhere $P_{train}$, $P_{test}$, and $P_{val}$ are the training, testing, and validation distributions (assumed to be the same but written separately for clarity), and $\\gamma_{val-acc}$ are the hyperparameters chosen when performing model selection based on the validation average accuracy (for notational clarity, we do not explicitly write their dependency on B, $P_{train}$, and $P_{val}$). That is:\n$\\gamma_{val-acc} := arg \\max_{\\Gamma} E_{(x,y)\\sim P_{val}(x,y)} [1(f_{\\theta(B_{\\gamma},P_{train})}(x) = y)]$   (C.2)\nIn practice, we estimate the PoU through a discrete grid search over $\\Gamma_{cands}$ while ensuring that samples in the training, testing, and validation sets are en-tirely disjoint. If during this grid search we find that the test WGA of $f_{\\theta(B_{\\gamma_{val-acc}},P_{train})}$ is 0 for some hyperparameters $\\gamma$, then we discard these hyperparameters as oth-erwise the PoU may become infinite and non-informative. Hence, we estimate this value based on hyperparameterisations with non-zero validation WGA only.\nWe note that, if the validation distribution $P_{val}$ is the same as the testing distribution $P_{test}$, as it is commonly the case for in-distribution domains, then for"}, {"title": "D Targeted Augmentations for Bias Mitigation", "content": "In Section 5 we provide a detailed motivation and description of TAB, our pro-posed unsupervised bias mitigation pipeline. Here, we complement that discus-sion by providing pseudocode for TAB (Algorithm D.1). This format highlights two key things: first, as discussed in Section 5, TAB takes no extra hyperpa-rameters on top of those of its underlying learning algorithm L. Second, TAB's implementation is extremely simple and can be easily added to existing train-ing pipelines. We believe that both of these key properties make TAB a likely candidate for adoption in practical real-world scenarios."}, {"title": "E Dataset Details", "content": "In this section, we provide a detailed description of the datasets we use for our evaluation in Section 6. Specifically, we show the main characteristics of each task in Table E.2 and describe each of the specific tasks in more detail below."}, {"title": "F Experimental Details", "content": "In this section, we describe our experimental setup across all tasks and baselines for our experiments described in Section 6. We begin by describing the underlying architecture and optimisation setup used across baselines. Then, we proceed to describe each method's hyperparameter selection process."}, {"title": "F.1 Architecture and Optimisation Setups", "content": "For all MNIST-based synthetic tasks (i.e., Even-Odd and cMNIST), we train as the underlying model of all baselines a six-layered Convolutional Neural Network (CNN). Specifically, we construct a CNN formed by six 2D Convolutional layers with 3\u00d73 filters, \"same\" padding, and {16, 16, 32, 32, 64, 64} output feature maps. A ReLU non-linear activation follows each of these layers and a single linear layer is used to map the output of the last convolution to the number of output classes (no activation is used after this layer as we work with logit outputs). We learn this model's parameters via an Adam optimiser [37] with its default configuration (learning rate \u03b7 = 10\u22123, \u03b2\u2081 = 0.9, \u03b22 = 0.999). Moreover, given the smaller model sizes for these tasks, we use a relatively large batch size of 2,048 samples to better utilise our hardware. Notice that in these synthetic tasks, we do not perform a hyperparameter search on the optimiser's learning rate as we observed good performance with the default optimiser configuration"}, {"title": "F.2 Model Selection", "content": "To conduct a fair evaluation of all baselines, we perform an extensive hyper-parameter search for all methods. Here, we discuss which hyperparameters we search over for all methods, as well as the hyperparameters selected for each task after averaging the results over three distinct runs. This hyperparameter selec-tion is done by performing a grid search over all combinations of hyperparame-ter candidates and selecting the model with the highest validation accuracy. The only exception for this is G-DRO, for which we perform model selection based on validation WGA given that we assume group labels are provided during training and we use G-DRO as an upper bound for unsupervised BM. Finally, to further reduce the computational cost of our model selection, the learning rate \u03b7 and weight decay de\u2082 hyperparameters used for all non-ERM and non-G-DRO meth-ods are set to those selected for their ERM equivalent (i.e., we first train the ERM baseline, find the best learning rate n and weight decay Ae, and fix those when performing model selection and training of all other baselines). Below we describe the hyperparameters we searched over for each baseline."}, {"title": "G Details for Motivation Experiments", "content": "In this section we provide details on the constructions of the figures used in Section 4, where we describe the importance of validation group annotations for model selection in BM pipelines. Figures 2, 3, and 4 were all generated by training a single Resenet-34 model for 50 epochs. This model is trained on our Waterbirds task for all figures and on the Celeba task for Figure 4. We follow the same approach as described in Appendix F to train this model in each respective task with the difference that, for the models shown in Figures 2 and 4, we do not perform early stopping (as we want to emphasise how biases in the validation set can in fact lead to an unwanted early stopping). Notice that in Figure 2 we use colours to indicate points in a curve that are higher (green) or lower (red) than the the metric at the beginning of the marked grey zone. Finally, in Figure 4 we split samples between bias-conflicting and bias-aligned by identifying minority groups lacking the spurious correlation as bias-conflicting. In Waterbirds we use (\u201cwaterbirds on land backgrounds\u201d and \u201clandbirds on water backgrounds\u201d) as our bias-conflicting groups, as they are both similarly underrepresented and lacking the spurious correlation, while in CelebA we use \"not blonde male faces\" as the bias-conflicting slice shown (as they are severely underrepresented and lacking the exploitable gender-based spurious correlation)."}, {"title": "H Random Transformations to Augmented Samples", "content": "In this section, we explore how domain-specific knowledge can be used to im-prove TAB's performance even further through the use of augmentation trans-formations. For this, when we construct TAB's augmented set $D_l^-$, we apply an stochastic transformation function $\u03b7 : R^n \\rightarrow R^n$ to each sample (i.e., $D_l^- = \\{(\\eta(x'^{(i)}), y'^{(i)}) | (x'^{(i)}, y'^{(i)}) \\in D_l^-\\}_{i=1}^{|D_l^-|}$). We explore this in our MNIST-based tasks as we know handwritten digits should be invariant to small perturba-tions in their angles and locations. Specifically, we let \u03b7 be a transformation pipeline that applies small random rotations ($\\theta$ ~ Unif([\u2212\u03c0/6, \u03c0/6])), transla-tions (\u2206x, \u2206y ~ Unif([0, 1/10])), and Gaussian blurring (using a 5x5 filter)."}, {"title": "I Effect of Batching Clustering for TAB", "content": "In this section, we explore the effect of using mini-batched k-means [66] for clustering history losses in TAB's pipeline. Exploring how mini-batching can be used as a part of TAB can be insightful to understanding how our method may be able to scale to datasets where k-means may become a bottleneck (i.e., datasets with a size in the order of millions or billions of samples). With this goal in mind, in Figure 1.2 we show the effect of using batches for k-means in our synthetic datasets. For these results, we use Scikit-learn's official MiniBatched-k-Means [57] implementation with its default settings except for the \u201creassignment ratio\u201d, which we decrease to 0.00001 to enable small clusters to be discovered (as we operate under the assumption that one of the clusters will be significantly smaller)."}]}