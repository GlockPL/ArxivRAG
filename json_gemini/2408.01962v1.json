{"title": "The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations", "authors": ["Robert Wolfe", "Tanushree Mitra"], "abstract": "Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.", "sections": [{"title": "Introduction", "content": "Generative AI models have rapidly become a component of organizational infrastructure, with more than 90% of Fortune 500 companies now using ChatGPT (Porter 2023). Such models promise to transform information work by providing approachable conversational interfaces for performing complex tasks involving large quantities of text and data (Hassani and Silva 2023; Eloundou et al. 2023). Recent research indicates that organizational integration of generative AI can complement the skills of educated professionals, especially early in their careers, increasing productivity and job satisfaction by automating repetitive tasks and making know-how of experienced workers more available to entry-level staff (Noy and Zhang 2023; Brynjolfsson, Li, and Raymond 2023). Despite this potential for positive impact, however, many scholars have voiced concerns over the growing reliance on closed, proprietary models (Birhane et al. 2022). Concerns have arisen from scholars in both Natural Language Processing (NLP) and the social sciences (Rogers et al. 2023; Ollion et al. 2023), responding to a growing body of research contending that ChatGPT and similar proprietary models can be used as a substitute for human subjects, both for labeling data in scientific studies (Gilardi, Alizadeh, and Kubli 2023; Amin, Cambria, and Schuller 2023), and simulating the behavior of human subjects (Park et al. 2023; Shanahan, McDonell, and Reynolds 2023), ignoring the paucity of technical information available about proprietary models and the uncertain reproducibility of results obtained. Palmer, Smith, and Spirling (2023) contend that academic researchers should prioritize the use of open models for which the weights are available for download, and training data is specified to the end user, unless they can provide an explicit, study-specific justification for choosing a proprietary model (e.g., studying the impact of OpenAI's DALL-E models on artists due to their widespread adoption (Jiang et al. 2023b)). While these studies address the importance of open models for scientific integrity, they do not consider the impact that choosing open models can have on organizations that adopt generative language models as technological infrastructure. In the present work, we seek to better understand the societal implications of open models by studying their use at fact-checking organizations, a group that shares several characteristics that render them worthy of consideration in this context. First, fact-checking organizations routinely employ state-of-the-art language models in their work, lest they find themselves overwhelmed by large volumes of misinformation (Das et al. 2023; Guo, Schlichtkrull, and Vlachos 2022). Second, they must ensure the reproducibility, reliability, and impartiality of their work, or they will compromise both trust with their audiences and their membership in organizations such as the International Fact-Checking Network (IFCN)"}, {"title": "Related Work", "content": "We review related work on open models, fact-checking organizations, and human-centered approaches to data science."}, {"title": "Open and Proprietary Models", "content": "While scholars have observed a spectrum of relative openness in Al releases (Solaiman 2023; Langenkamp and Yue 2022), we adopt the definition of open models proposed by Palmer, Smith, and Spirling (2023) and Rogers et al. (2023). Specifically, open models can be: 1) downloaded locally; 2) run without a call to an API; 3) shared with others; and 4) versioned. The contents of open models' training data must be disclosed, even if the data itself is not available (Palmer, Smith, and Spirling 2023). This definition is satisfied by recent releases of generative language models such as Meta's LLaMA-2 (Touvron et al. 2023) and its finetuned variants such as Stable Vicuna (Chiang et al. 2023). Some generative models, such as the popular Mistral-7B, are better characterized as \"open weight\" models (Jiang et al. 2023a), as they make the model's weights available, but do not disclose training data to preserve competitive advantage. We note while many transformer-based models, such as Google's BERT (Devlin et al. 2019), would qualify as open models under these definitions, we study specifically generative models. The dominance of OpenAI's ChatGPT (OpenAI 2022) has engendered studies comparing its performance against that of finetuned open models. Koco\u0144 et al. (2023) find that ChatGPT's few-shot performance lags that of task-specific models such as fine-tuned transformers. Moreover, Wolfe et al. (2024) find that small, open generative models can match or exceed ChatGPT after fine-tuning, while Thalken et al. (2023) show that fine-tuned Legal-BERT outperforms generalist models on classifying legal reasoning. While open generative chatbot models without fine-tuning have not matched the performance of GPT-4, which regularly tops evaluation leaderboards such as Stanford's Holistic Evaluation of Language Models (HELM) (Liang et al. 2023), recent open-weight models such as 01.ai's Yi (AI et al. 2024) and Mistral AI's Mixtral 8x7B (Jiang et al. 2024) have surpassed proprietary competitors including Anthropic's Claude 2 (Anthropic 2024) and OpenAI's GPT-3.5-Turbo (OpenAI 2024b), indicating that the best open chat models are comparable to proprietary models six months earlier. Our interviews show that perceived performance shortcomings of open models play a role in fact-checking organizations' use of proprietary models. Most recently, proprietary model providers have allowed users to create customized versions of generative models, incorporating specialized prompts to govern behavior, and equipping models with external sources of data and programmatic actions. For example, OpenAI refers to customized versions of ChatGPT as \u201cGPTs\u201d (OpenAI 2023), and allows ChatGPT Premium subscribers to build and share GPTs via OpenAI's \"GPT Store\u201d web platform, through which OpenAI has promised that builders of custom GPTs will be able to generate revenue (OpenAI 2024a). The GPT Store is an example of an emerging AI ecosystem, wherein users select from a range of generative models based on their needs. Our work offers insight into how fact-checkers foresee such AI ecosystems impacting their financial models."}, {"title": "Sociotechnical Infrastructure of Fact-Checking", "content": "Fact-checking refers to assessing of the veracity of ostensibly factual information circulating in an information ecosystem that has the potential to harmfully mislead its audience (Graves 2017). While a version of fact-checking has long existed in the form of investigative journalism (Dickey 2019), modern fact-checking coincides with the rise of the internet and social media in particular (Graves and Amazeen 2019), which provided new conduits for the spread of misinforming content among vast networks of people. Fact-checking is typically described as a sociotechnical task (Graves 2018), meaning in this case that it is successfully accomplished by human investigators working with technological tools that support their expertise (Chopra and Singh 2018; Radiya-Dixit and Neff 2023). Fact-checking organizations mostly embrace language models as essential infrastructure, expressing optimism about their potential to reduce manual workloads and exposure to harmful content, though they remain skeptical of AI that promises full automation of work that relies on human expertise (Juneja and Mitra 2022). Many modern fact-checkers adhere to the principles set forth by the International Fact Checking Network (IFCN) (Poynter 2024a), an organization created to establish common standards for fact-checking (Poynter 2024b). We interview professionals working primarily at signatory organizations of the IFCN to understand how they perceive the merits of open and proprietary models."}, {"title": "Human-Centered Data Science", "content": "As noted by Berman (2023), interaction between data practitioners and the tools they use constitutes a social context that shapes the ethics of AI practices in organizations. Much work in social computing seeks to describe these tools and in interaction with human practitioners in data science pipelines (Wang et al. 2019). For example, in a study of 183 data scientists, Zhang, Muller, and Wang (2020) describe a common data science pipeline consisting of three high-level steps, including preparation, modeling, and deployment. However, as described by Hopkins and Booth (2021), studies of data practitioners typically center on participants from big tech and academia, and may overlook the challenges faced by smaller organizations facing resource constraints, such as tensions between user privacy and organizational growth, a finding also echoed in Bessen, Impink, and Seamans (2022), who note that AI startups may face tradeoffs between building more competitive and more ethical products. Human-centered data science centers the context in which data practitioners perform their work, acknowledging that data work work may be undertaken by domain experts or other workers not traditionally considered data scientists (Muller et al. 2019), a perspective that can yield domain-specific understandings of data pipelines. For example, Rothschild et al. (2022) note that civic workers at public and non-profit institutions exhibit skill with data contextualization that provides value far in excess of their sometimes less-developed computational abilities. Adopting a human-centered approach is well-motivated for fact checking, because despite the many benchmarks and techniques to support detection of misinformation (Russo, Tekiro\u011flu, and Guerini 2023; Raj, Mukherjee, and Zhu 2023; Choi and Ferrara 2023), fact-checking organizations often view academic research as too detached from the real world (Juneja and Mitra 2022; Wolfe and Mitra 2024), and recent work argues that even core NLP research on fact-checking should also study human factors (Das et al. 2023). We privilege the views of fact-checking professionals, surfacing where generative models fit into fact-checking data pipelines, and contextualizing the value fact-checking organizations see in open and proprietary models within those pipelines."}, {"title": "Approach", "content": "We conducted an interview study with N=24 professionals at the 20 organizations shown in Table 1 to better understand the use of open models at fact-checking organizations. The study was approved by our university's IRB."}, {"title": "Participants", "content": "We reached out to 92 organizations via cold email, explaining our research and asking for an interview. We employed primarily purposive sampling (Etikan et al. 2016) in emailing member organizations of the International Fact Checking Network (IFCN) (Poynter 2024b) and their partner organizations, and snowball sampling (Naderifar, Goli, and Ghaljaie 2017) when individuals at these organizations offered to connect us with another organization well-positioned for participation. Five participants at five organizations enrolled in our study as a result of snowball sampling; the rest enrolled as a result of purposive sampling. Individuals at ten additional fact-checking organizations responded to our emails but lacked technical knowledge needed to respond to our questions about open and proprietary models, as their roles were related to editing or upper management. We thus excluded them from this study. Most participants were engineers, research scientists, or department managers, with experience ranging from two years to 18 years in their current role. We refer to participants using a randomly assigned number between 1 and N (e.g., P24 said \"...\")."}, {"title": "Interview Process", "content": "We developed a semi-structured interview protocol that asked participants about their organization's use of generative language models; the opportunities and challenges of generative AI in fact-checking; their organization's use of open models, and their motivations for adopting them; their reasons for using proprietary models; and what research could support the use of language models in fact-checking work. Where participants raised topics germane to the research but not covered in our interview protocol, we asked follow-up questions; for example, we asked P4 clarifying questions about their organization's beta release of a generative chatbot to collect misinformation circulating on platforms such as WhatsApp (WhatsApp 2024). Interviews lasted between twenty-five and ninety minutes and averaged approximately forty-five minutes. All interviews were conducted in English."}, {"title": "Data Analysis", "content": "Interviews were recorded over Zoom (Zoom 2024), transcribed using Rev Max AI (Rev 2024), and manually corrected as necessary by the first author. To answer the study's research questions, we adopted a deductive-inductive approach to coding the interview transcripts. We employed the following deductive codes: Uses of Open Models, Motivations for Using Open Models, Limitations of Open Models, Motivations for Using Proprietary Models, and Implications for Research. The authors first coded four transcripts, and the first author created an initial codebook that included inductively generated themes. The last author reviewed the codebook and the authors jointly revised the codes. The authors then coded four additional transcripts at a time until all transcripts had been coded, reviewing and revising the codebook after each round of coding. Finally, the authors followed a thematic analysis process (Braun and Clarke 2022) to generate themes that answered the study's research questions, using shared memos to precisely define the themes."}, {"title": "Data Pipelines in Fact-Checking", "content": "Consistent with prior work in human-centered data science, we found during thematic analysis that understanding the motivations of fact-checking organizations for using open or proprietary models requires understanding the ways in which they collect, analyze, and exchange fact-checking data and specifically where they use generative models in these processes. To that end, we begin by providing a conceptual model of five components of fact-checking data science pipelines in which participants described using generative models. We assign each component an icon subsequently used in the Motivations and Limitations sections to associate participant perspectives with components of the pipeline."}, {"title": "Data Ingestion", "content": "Participants reported using generative AI to collect and preprocess data, whether via media monitoring efforts employing AI-driven tools designed by social media companies or by the organizations themselves, or via tipline interfaces wherein a user can submit misinformation for fact-checking. We refer to this component of the data science pipeline as Data Ingestion, and denote it using an RSS icon to suggest the role of monitoring novel information. Most participants described media monitoring pipelines like that illustrated in Figure 2 as an essential means of observing circulating misinformation. P3 noted that \"social media listening is the main point of entry\" to their data pipeline, while P11 said that they focus on monitoring WhatsApp because \"the coverage is so massive\" in their"}, {"title": "Data Analysis", "content": "Participants reported using generative AI to analyze large volumes of potentially misinforming content, leveraging its capabilities to parse highly contextual information, and utilizing few-shot approaches to avoid fine-tuning additional models. We denote Data Analysis with a magnifying glass to suggest the closer study of information. Participants reported using generative AI to support classification and synthesis of text and multimodal content. P4 used generative AI to classify text based on constructs like urgency that they previously captured using proxies like message formatting: \"instead of counting how many exclamation points a post has or how many caps it uses... Gen AI... give [s] us a score from zero to a hundred of the sense of urgency.\" P15 noted using generative models as part of an ensemble of deep learning and rule-based NLP classifiers. P2 used generative AI to extract and classify \u201cpatterns of manipulative messages... like the government doesn't want you to know this or share it,\" noting that generative AI can be \u201csomething like an anti-spam filter\u201d for misinformation. P3 said they use generative AI for classifying multimodal misinformation, including memes: \"the text itself isn't disinformation. The image without the text isn't disinformation. The image plus the text can feed very clearly into a disinformation narrative.\" P2 noted that user-friendly generative AI enables less technical fact checkers to create classifiers: \"Generative AI... democratizes who can work with AI. . . with an API and a little magic with a prompt, you can have something really powerful.\u201d Participants also embraced uses of generative AI for synthesizing data. P22 said that \"one of the values of generative Al is really synthesis, and looking and combing through tons of material, which... [we] will not have time for.\" P1 noted using ChatGPT to synthesize hundreds of documents collected every day via media monitoring, and to structure the data \"in a tabular format... in 90% of the cases, it gives me a nice table.\" P3 used generative AI to synthesize component claims into narratives, improving the scalability of fact-checking work: \"large language models are super good at basically clustering claims into a narrative. Fact checking just individual claims is whack-a-mole, a losing proposition. . . you'll never be able to scale it.\" P11 demoed a GPT-4 driven narrative system for us, explaining that it synthesizes new claims into overarching narratives already associated with fact checks, pending human review: \"this summary is linked to these four different contexts that we have already received. . . generative AI here has proposed to us something that is a bit overarching... [it] is seeing what we produce, and the evidence that accompanies [our] debunks, and is proposing to us [a fact check] that has already been verified by a human.\""}, {"title": "Data Retrieval", "content": "Participants reported using generative AI to facilitate Data Retrieval from catalogues of past fact checks or other verified sources of information maintained by the organization. We denote Data Retrieval with a database to suggest the retrieval of stored data. Many participants described using Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) to allow GPT models"}, {"title": "Data Delivery", "content": "Participants reported using generative AI to support Data Delivery to end users on websites or social media, as well as by providing automatic responses to users via conversational tiplines. We denote Data Delivery with a download icon to suggest the transfer of data to the end user. Among the most common uses of generative AI reported was to format content or generate metadata prior to sharing it with audiences. P1 described using GPT models to \"generate hashtags for... mini FactCheck videos that we publish on TikTok.\" P8 said that \"in most daily use cases, in terms of generative AI use, I would say it's help with promotion. So all of the social media content, coming up with summaries for SEO purposes for article publishing, title generation.\" P5 noted that they use a generative AI-backed tool that \"suggests times the best times for us to post our content on social media based on the type of demographic our subscribers are, or our audiences are, and when they're using social media.\" Participants who used conversational tiplines reported using generative AI to deliver information to end users, in addition to ingesting misinforming content. P4 said that incorporating ChatGPT into their tipline was \u201can obvious use case to improve a product that was already relevant for our readers,\" noting their conversational tipline had \"over 70,000 users, which for an organization our size is quite a lot.\" Several participants described an evolving information ecosystem wherein users sought information from specialized conversational agents, rather than traditional search applications. P21 said that they see generative AI as \u201ca preferred medium for somebody to get at the work that we have done... it is summarizing or reporting on work that was done by the trusted fact checkers.\" P24 said that they use a WhatsApp chatbot that \u201callows us to answer to a high volume of messages,\" noting that \"if you could actually ask the [chatbot], can you please tell me what the inflation was in the last five years? And it could actually answer you with information that comes from a reliable source, which we know is one of the big problems of generative AI, we think that's an enormous leap forward in the way that we can actually reach people with verified information.\" P19 contended that conversational agents can assist users \u201ceven if we don't have a fact check... explain this persuasion technique that's being used or the trope that is being repeated.\" P9 envisioned reaching younger audiences with an in-progress tool allowing a user to \"chat with our archive... this is a hurdle for young people to get into the discourse, that some background knowledge is missing. Maybe the AI could help.\""}, {"title": "Data Sharing", "content": "Participants noted that generative AI assists in Data Sharing between organizations, in that they structure data for sharing, and serve as shared computational infrastructure. We denote Data Sharing with a Users icon suggesting transfer of data between organizations. P18 described generative AI as a tool for exchanging fact check data and collectively scaling audiences to address problematic information: \u201cespecially during elections, we try at (anonymized) to bring together other fact checkers so that people are not working in their own little silos... that's one area in which generative AI can really help. If fact-checkers are working together, whatever data they have, they can help scale the impact of their fact check to different segmented audiences that they serve.\u201d P3 described a prototype generative Al system operating on shared data, noting that especially in the case of elections, \"fact checkers are banding together to offer a united response... European checkers will put their claims in a common database, and we will build systems in which, when we detect a new narrative. . . [if] it's present more than one or two countries, there will be a special task force that will be tasked with producing a debunk... we could not do it with a BERT or a Sentence BERT... early tests are promising with a [generative] large language model. This is also where the multilingual aspect comes in very handy.\u201d P11 described a similar multi-organization \"project where we have installed [generative] technology... at fact-checkers in all Russia bordering countries. And we are also looking at... Spanish speaking Latin America. And we can see how common threats appear... where there are common narratives that point to particular actors.\""}, {"title": "Motivations for Open Models", "content": "We found that four primary concerns motivate the use of open models: Organizational Autonomy, Data Privacy and Ownership, Task Specificity, and Capability Transparency. We describe these concerns in turn, making reference to the components of the data pipeline with which they intersect."}, {"title": "Organizational Autonomy", "content": "Participants expressed concern that dependence on proprietary generative models could compromise the autonomy of their organizations. They noted that open models offer more Reliable Availability than proprietary models, which"}, {"title": "Data Privacy and Ownership", "content": "Participants described concerns surrounding data privacy and ownership as a central motivation for using open models. P9 preferred using open models for Protecting of Internal Data, saying that \"we have very sensitive material that we're working with here, investigative reporting and investigative stories, and we don't want this to be used in [corporate] models and as training material,\" and further noting that they use cloud instances hosted only in Europe for \"a sort of legal safety\" due to stricter European data protection laws. P4 said that, due to data privacy concerns, \u201cwe have as a policy to always prioritize using open source software where we can.\" P21 noted \"I blocked [OpenAI's] crawlers from being able to train on our content until some type of commercial compensation becomes available. I think most of us are kind of waiting, holding our breaths for the New York Times case lawsuit to play out because otherwise individual orgs the size of the fact checkers, we don't really have the leverage to accomplish what that lawsuit stands to do in setting a precedent.\" Participants also said that Publisher Solidarity motivated use of open models over proprietary models that profit from journalistic organizations' data without consent. P4 noted their discomfort with \"the notion that those companies are profiting using other companies' and other people's work.\" Participants also preferred open models that disclosed their training datasets, providing a sense of Legal Safety, especially for user-facing applications. P4 said that \"the issue of copyright is a big one, especially for image generation... we could never use anything, any tool that generate images in our workflow, because we don't know how most models were trained.\" P8 noted that they delayed using generative models due to fears of copyright infringement: \u201cWith generative AI, we were... scared to use it, because of the fact that we don't want to feel like we are plagiarizing... because of the possibility of... [copying] other articles.\" Participants noted that open models at least disclose their training datasets, offering some clarity concerning the risk of infringement. Finally, participants preferred open models for Mitigating Shared Risk when building technologies for the fact-checking community. P4 created a transcription tool for the community using OpenAI's models, but noted is not in production because \"a lot of people have concerns about sending their data interviews, important interviews - to OpenAI.\u201d"}, {"title": "Application Specificity", "content": "Participants preferred open models for specific applications that demanded high performance, domain-specific tone, and"}, {"title": "Capability Transparency", "content": "While participants agreed that GPT-4 outperformed open models, they also said that the capabilities of open models were presented with more Performance Transparency. P3 expressed surprise that GPT-4 performed poorly for restructuring their data, a task they thought fell within the model's capabilities, noting that while it looked reasonable \"on the surface... when you actually dug into whether it was structured coherently, it wasn't as good.\u201d Participants also said that Avoiding Deceptive Marketing motivated use of open models in settings involving user interaction. P17 said the presentation of models like ChatGPT encouraged inappropriate trust by non-experts: \u201cYes, there is a popup on ChatGPT, and a line at the ending that tells you the results could be inaccurate, so beware. But how the interface is built, and how it is marketed, how it is presented, and the fact that it answers you in a confident way... there is a constant behavioral trick... that what you have in front of you, what a machine is telling you, answering to your prompt, is the truth.\" P1 echoed this sentiment, noting that \"There's a problem with people assigning too much credibility to large language models.\" Finally, participants said that providing Tools for Replication motivated open models. P8 noted that \"we want to give a reader the ability to replicate our research in total... every source, everything.\" P9 said that, while all models created problems of explainability, API-gated proprietary models introduced more \"black box problems\" than open models, rendering reproducibility uncertain for end users."}, {"title": "Limitations of Open Models", "content": "Despite the many motivations participants gave for preferring open models, most nonetheless used primarily proprietary models, especially OpenAI's GPT models (OpenAI 2024b), citing their Performance, Usability, Safety, and the Opportunity Costs of not participating in proprietary AI ecosystems."}, {"title": "Performance", "content": "While fine-tuned open models may achieve the strongest performance on a given task, all participants said that GPT-4 was the best-performing general-purpose generative model, motivating its adoption over open models in many settings where general-purpose reasoning is preferable to task specialization. P13 noted that they use GPT-4 over LLaMA models due to Few-Shot Reasoning disparities, while P9 echoed this in noting that \"if you want to work with a narrative generative AI model, then I think there is, at the moment, no alternative to GPT-4.\" Participants also noted that open models lag OpenAI significantly in Multilingual Performance. P4, who reported creating primarily non-English models, said that \"we tried the largest [LLaMA models]... usually the models tend to perform poorly in languages that are not English. And in the case of OpenAI, it's pretty good.\" P4 also said that performance disparities affected the End User Readiness of open models: \u201cwhen we were developing [our conversational tipline], we tested a bunch of models, especially LLaMA-2, but in terms of performance, it's behind OpenAI significantly. . . I'd love to use open source models.\u201d P2 said that the Pace of Innovation in generative AI made it untenable to try to build open models: \"the pace of innovation nowadays is so quick that it's very difficult to keep the pace... you don't know which is the new tech that you need to use or which is the model that is going to work... even research institutions, they don't know.\" In some cases, participants reported that GPT-3.5-Turbo's inexpensive Average Performance was good enough. P1 said that, for retrieving data internally, they needed high recall and not necessarily high accuracy, since human fact-checkers would see the data. They noted \"OpenAI is... so easy and cheap. We pay a couple of dollars per month... I can use the old (anonymized) servers to run an open language model and see what it can do. But the savings would be minimal, like a couple of dollars, and the results would probably be worse.\""}, {"title": "Usability", "content": "Most participants said the ease of calling proprietary APIs motivated their use of proprietary generative models. P19 said that, even though they prefer to build on open source models, API Simplicity motivated them to OpenAI: \u201cOpenAI models are convenient for prototyping because it's just an API call, and you don't have to worry too much about it.\" P2 said, \"what is the good part of OpenAI? Everything with an API is much easier.\u201d However, P2 also pointed to the possibility that OpenAI's perceived edge in usability is actually due more to its market dominance, noting that \"there are a lot of different frameworks now... [such as] LangChain, there are a lot of frameworks that make it easier to work with [open models], and they are the solution.\" Participants noted the Demands of Fine-Tuning open models drove them to use few-shot proprietary models. Even if task-specific performance exceeding GPT-4 is achievable with fine-tuned open models, data scientists at fact-checking organizations"}, {"title": "Safety", "content": "Participants said that proprietary models offer advantages for user-facing applications because of the safety features built into them by larger technology corporations. P13 noted that because user-facing fact-checking technologies typically involved Handling Charged User Input, reliably adhering to ethical guardrails was essential for maintaining user trust, and open models could not always accomplish this. P4 said that user-facing technologies also had to be prepared for Handling Bias Stress Tests, noting that users actively attempt to probe their in-beta conversational model (leveraging GPT-4) for political biases: \u201cone of the most common types of questions that I think people were asking the bot... [was] just swapping the name of the [politician] you're asking about... and so far we haven't noticed anything that would be concerning for us... it's not symmetric, but I think it usually gives you a quite nuanced answer.\" P4 also noted that proprietary models tend to outperform open models in Abstaining from False Output, noting that users most frequently complained that their RAG-enabled GPT-4 chatbot couldn't answer a question, but that \"in most cases, we didn't have the answer. So the bot behaved as expected as it should, which is to say that it doesn't know the answer to a question instead of trying to come up an invented answer.\" Finally, participants said that proprietary model developers were better positioned to perform Ongoing Safety Updates. P21 said that \"fact check organizations are underequipped to really test everything and keep up with everything on an ongoing basis\"; accomplishing this with open models, they said, would require \"revenue streams...that's still trying to be figured out.\""}, {"title": "Opportunity Costs", "content": "Participants worried that using open models would entail foregone opportunities for integration into an emerging in-"}, {"title": "Discussion", "content": "Our findings add new perspective to the evolving conversation about the use of open or proprietary generative AI, surfacing the tradeoffs that face organizations as they consider whether and where to adopt open models. Some tradeoffs would seem contradictory without being contextualized within the fact-checking data science pipeline. For example, participants chafed at limitations imposed by toxicity guardrails when using OpenAI models for internal analysis tasks, yet reported relying on those guardrails for user-facing applications. Moreover, while participants preferred fine-tuned open models for strong performance on internal tasks, they also accepted \"good enough\u201d performance via the cheap, easy-to-use GPT-3.5-Turbo API for high-recall data retrieval tasks, over the time cost incurred for an open model. Table 2 outlines research questions and directions arising from this work, highlighting the need to address issues with both open and proprietary models. Fact-checking organizations appear at first to face diverging futures: one in which they adopt open models and control their AI infrastructure; and another operating profitably in the emerging ecosystems of chat-driven interfaces now both collecting and delivering information in interactions with end users. However, our findings surface that the future may resemble the present, with organizations employing specialized open models for mission-critical internal tasks, proprietary generalist models to handle interactions with end users, and the least expensive option when performance just needs to be good enough. Yet even in this blended future, proprietary models face challenges around data ownership. Participants in our study consistently voiced unease with their work and that of their colleagues being used without consent to create a lucrative competing product. Some participants also avoided generative AI for fear of inadvertently committing plagiarism. These concerns echo those of many journalists, as well as domains like visual art, wherein artists find their work devalued by a tool trained on their own content (Jiang et al. 2023b). Addressing issues of data privacy, ownership, and compensation may prove paramount for proprietary model providers to establish relationships with fact-checking organizations and other producers of factual and creative content."}, {"title": "Limitations and Future Work", "content": "While we sought to capture perspectives of fact-checking organizations globally, our participants all spoke English, and most used models in English. Aside from noting that OpenAI's models outperform open models in multilingual settings for generalist conversational tasks, our work does not speak to the additional intricacies of multilingual NLP, including NLP in low-resource languages. Additionally, our work is concerned with open generative models, rather than with all open models (such as autoencoder transformers models). Future work might study perspectives on open models and proprietary models of any architecture."}, {"title": "Conclusion", "content": "We contribute an organizational perspective on open and proprietary models, finding that while fact-checking organizations prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency, they nonetheless use proprietary models for their Performance, Usability, and Safety. We suggest open models could benefit from increased usability, while proprietary model providers must address concerns over data ownership and compensation. As generative AI ecosystems become more mature, financial models may determine whether fact-checking organizations disseminate information in"}]}