{"title": "The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations", "authors": ["Robert Wolfe", "Tanushree Mitra"], "abstract": "Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.", "sections": [{"title": "Introduction", "content": "Generative AI models have rapidly become a component of organizational infrastructure, with more than 90% of Fortune 500 companies now using ChatGPT (Porter 2023). Such models promise to transform information work by providing approachable conversational interfaces for performing complex tasks involving large quantities of text and data (Hassani and Silva 2023; Eloundou et al. 2023). Recent research indicates that organizational integration of generative AI can complement the skills of educated professionals, especially early in their careers, increasing productivity and job satisfaction by automating repetitive tasks and making know-how of experienced workers more available to entry-level staff (Noy and Zhang 2023; Brynjolfsson, Li, and Raymond 2023). \nDespite this potential for positive impact, however, many scholars have voiced concerns over the growing reliance on closed, proprietary models (Birhane et al. 2022). Concerns have arisen from scholars in both Natural Language Processing (NLP) and the social sciences (Rogers et al. 2023; Ollion et al. 2023), responding to a growing body of research contending that ChatGPT and similar proprietary models can be used as a substitute for human subjects, both for labeling data in scientific studies (Gilardi, Alizadeh, and Kubli 2023; Amin, Cambria, and Schuller 2023), and simulating the behavior of human subjects (Park et al. 2023; Shanahan, McDonell, and Reynolds 2023), ignoring the paucity of technical information available about proprietary models and the uncertain reproducibility of results obtained. Palmer, Smith, and Spirling (2023) contend that academic researchers should prioritize the use of open models for which the weights are available for download, and training data is specified to the end user, unless they can provide an explicit, study-specific justification for choosing a proprietary model (e.g., studying the impact of OpenAI's DALL-E models on artists due to their widespread adoption (Jiang et al. 2023b)). \nWhile these studies address the importance of open models for scientific integrity, they do not consider the impact that choosing open models can have on organizations that adopt generative language models as technological infrastructure. In the present work, we seek to better understand the societal implications of open models by studying their use at fact-checking organizations, a group that shares several characteristics that render them worthy of consideration in this context. First, fact-checking organizations routinely employ state-of-the-art language models in their work, lest they find themselves overwhelmed by large volumes of misinformation (Das et al. 2023; Guo, Schlichtkrull, and Vlachos 2022). Second, they must ensure the reproducibility, reliability, and impartiality of their work, or they will compromise both trust with their audiences and their membership in organizations such as the International Fact-Checking Network (IFCN) (Poynter 2024a; Walter et al. 2020). And third, they play a vital role in maintaining the health of information ecosystems around the world (Li and Chang 2023). Understanding use of open models at fact-checking organizations can thus provide insight into the experiences of public interest organizations leveraging generative AI in an impactful sociotechnical context. In this work, we address three research questions:\n\u2022 RQ1: Where do fact-checking organizations employ generative AI models in their data science pipelines?\n\u2022 RQ2: What motivates the adoption of open models by fact-checking organizations?\n\u2022 RQ3: What prevents fact-checking organizations from further employing open models in their work?\nTo answer these questions, we conducted an interview study with N=24 professionals working at 20 fact-checking organizations across six continents. Adopting a human-centered approach to contextualize fact-checker perspectives on generative AI within the context of its use by practitioners, we found that fact-checking organizations reported employing generative models for Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. Most participants preferred open models over proprietary models due to concerns related to organizational autonomy, data privacy and ownership, application and domain specificity, and model capability transparency. However, with a few exceptions, their use of open generative models was largely aspirational, as participants cited significant perceived shortcomings in the performance, usability, and safety of open models, as well as opportunity costs associated with not participating in emerging generative AI ecosystems offered by companies like OpenAI and Google. We make three contributions:\n\u2022 We offer a five-component conceptual model to describe where fact-checker organizations employ generative models in sociotechnical fact-checking data science pipelines. We offer two concrete examples of in-use pipelines that employ generative models: media monitoring pipelines, and retrieval-augmented conversational tiplines (illustrated in Figure 1, the latter of which has seen significant improvements since the advent of general-purpose conversational models such as ChatGPT.\n\u2022 We offer taxonomies of 1) motivations of fact-checking organizations for preferring open models, and 2) limitations that prevent further adoption of open models. We contextualize motivations and limitations by identifying the components of the data science pipeline wherein participants most located their impact, providing a grounded view of the relationship between the model itself and its organizational and societal impacts.\n\u2022 We propose a research agenda for addressing the concerns of fact-checking organizations with both open and proprietary generative models. We offer concrete suggestions for research addressing the performance, usability, and safety of open models, which we suggest can help further their adoption. Given that general-purpose performance of proprietary models will mostly exceed open models, and the revenue of fact-checking organizations may be dependent on producing custom models that integrate with proprietary ecosystems, we also offer directions for research addressing transparency, agency, privacy, and specificity in proprietary models.\nRather than offering a prescriptive approach to open models or an empirical study of their effectiveness, we contribute an understanding of open models in a consequential setting, including how open and proprietary models are valued in practice. We believe these insights can inform the perspectives and research agenda of the AI ethics community."}, {"title": "Related Work", "content": "We review related work on open models, fact-checking organizations, and human-centered approaches to data science."}, {"title": "Open and Proprietary Models", "content": "While scholars have observed a spectrum of relative openness in AI releases (Solaiman 2023; Langenkamp and Yue 2022), we adopt the definition of open models proposed by Palmer, Smith, and Spirling (2023) and Rogers et al. (2023). Specifically, open models can be: 1) downloaded locally; 2) run without a call to an API; 3) shared with others; and 4) versioned. The contents of open models' training data must be disclosed, even if the data itself is not available (Palmer, Smith, and Spirling 2023). This definition is satisfied by recent releases of generative language models such as Meta's LLaMA-2 (Touvron et al. 2023) and its finetuned variants such as Stable Vicuna (Chiang et al. 2023). Some generative models, such as the popular Mistral-7B, are better characterized as \"open weight\" models (Jiang et al. 2023a), as they make the model's weights available, but do not disclose training data to preserve competitive advantage. We note while many transformer-based models, such as Google's BERT (Devlin et al. 2019), would qualify as open models under these definitions, we study specifically generative models. \nThe dominance of OpenAI's ChatGPT (OpenAI 2022) has engendered studies comparing its performance against that of finetuned open models. Koco\u0144 et al. (2023) find that ChatGPT's few-shot performance lags that of task-specific models such as fine-tuned transformers. Moreover, Wolfe et al. (2024) find that small, open generative models can match or exceed ChatGPT after fine-tuning, while Thalken et al. (2023) show that fine-tuned Legal-BERT outperforms generalist models on classifying legal reasoning. While open generative chatbot models without fine-tuning have not matched the performance of GPT-4, which regularly tops evaluation leaderboards such as Stanford's Holistic Evaluation of Language Models (HELM) (Liang et al. 2023), recent open-weight models such as 01.ai's Yi (AI et al. 2024) and Mistral AI's Mixtral 8x7B (Jiang et al. 2024) have surpassed proprietary competitors including Anthropic's Claude 2 (Anthropic 2024) and OpenAI's GPT-3.5-Turbo (OpenAI 2024b), indicating that the best open chat models are comparable to proprietary models six months earlier. Our interviews show that perceived performance shortcomings of open models play a role in fact-checking organizations' use of proprietary models. \nMost recently, proprietary model providers have allowed users to create customized versions of generative models, incorporating specialized prompts to govern behavior, and equipping models with external sources of data and programmatic actions. For example, OpenAI refers to customized versions of ChatGPT as \u201cGPTs\u201d (OpenAI 2023), and allows ChatGPT Premium subscribers to build and share GPTs via OpenAI's \"GPT Store\u201d web platform, through which OpenAI has promised that builders of custom GPTs will be able to generate revenue (OpenAI 2024a). The GPT Store is an example of an emerging AI ecosystem, wherein users select from a range of generative models based on their needs. Our work offers insight into how fact-checkers foresee such AI ecosystems impacting their financial models.\""}, {"title": "Sociotechnical Infrastructure of Fact-Checking", "content": "Fact-checking refers to assessing of the veracity of ostensibly factual information circulating in an information ecosystem that has the potential to harmfully mislead its audience (Graves 2017). While a version of fact-checking has long existed in the form of investigative journalism (Dickey 2019), modern fact-checking coincides with the rise of the internet and social media in particular (Graves and Amazeen 2019), which provided new conduits for the spread of misinforming content among vast networks of people. Fact-checking is typically described as a sociotechnical task (Graves 2018), meaning in this case that it is successfully accomplished by human investigators working with technological tools that support their expertise (Chopra and Singh 2018; Radiya-Dixit and Neff 2023). Fact-checking organizations mostly embrace language models as essential infrastructure, expressing optimism about their potential to reduce manual workloads and exposure to harmful content, though they remain skeptical of AI that promises full automation of work that relies on human expertise (Juneja and Mitra 2022). Many modern fact-checkers adhere to the principles set forth by the International Fact Checking Network (IFCN) (Poynter 2024a), an organization created to establish common standards for fact-checking (Poynter 2024b). We interview professionals working primarily at signatory organizations of the IFCN to understand how they perceive the merits of open and proprietary models."}, {"title": "Human-Centered Data Science", "content": "As noted by Berman (2023), interaction between data practitioners and the tools they use constitutes a social context that shapes the ethics of AI practices in organizations. Much work in social computing seeks to describe these tools and in interaction with human practitioners in data science pipelines (Wang et al. 2019). For example, in a study of 183 data scientists, Zhang, Muller, and Wang (2020) describe a common data science pipeline consisting of three high-level steps, including preparation, modeling, and deployment. However, as described by Hopkins and Booth (2021), studies of data practitioners typically center on participants from big tech and academia, and may overlook the challenges faced by smaller organizations facing resource constraints, such as tensions between user privacy and organizational growth, a finding also echoed in Bessen, Impink, and Seamans (2022), who note that AI startups may face tradeoffs between building more competitive and more ethical products. Human-centered data science centers the context in which data practitioners perform their work, acknowledging that data work work may be undertaken by domain experts or other workers not traditionally considered data scientists (Muller et al. 2019), a perspective that can yield domain-specific understandings of data pipelines. For example, Rothschild et al. (2022) note that civic workers at public and non-profit institutions exhibit skill with data contextualization that provides value far in excess of their sometimes less-developed computational abilities. \nAdopting a human-centered approach is well-motivated for fact checking, because despite the many benchmarks and techniques to support detection of misinformation (Russo, Tekiro\u011flu, and Guerini 2023; Raj, Mukherjee, and Zhu 2023; Choi and Ferrara 2023), fact-checking organizations often view academic research as too detached from the real world (Juneja and Mitra 2022; Wolfe and Mitra 2024), and recent work argues that even core NLP research on fact-checking should also study human factors (Das et al. 2023). We privilege the views of fact-checking professionals, surfacing where generative models fit into fact-checking data pipelines, and contextualizing the value fact-checking organizations see in open and proprietary models within those pipelines."}, {"title": "Approach", "content": "We conducted an interview study with N=24 professionals at the 20 organizations shown in Table 1 to better understand the use of open models at fact-checking organizations. The study was approved by our university's IRB."}, {"title": "Participants", "content": "We reached out to 92 organizations via cold email, explaining our research and asking for an interview. We employed primarily purposive sampling (Etikan et al. 2016) in emailing member organizations of the International Fact Checking Network (IFCN) (Poynter 2024b) and their partner organizations, and snowball sampling (Naderifar, Goli, and Ghaljaie 2017) when individuals at these organizations offered to connect us with another organization well-positioned for participation. Five participants at five organizations enrolled in our study as a result of snowball sampling; the rest enrolled as a result of purposive sampling. Individuals at ten additional fact-checking organizations responded to our emails but lacked technical knowledge needed to respond to our questions about open and proprietary models, as their roles were related to editing or upper management. We thus excluded them from this study. Most participants were engineers, research scientists, or department managers, with experience ranging from two years to 18 years in their current role. We refer to participants using a randomly assigned number between 1 and N (e.g., P24 said \"...\")."}, {"title": "Interview Process", "content": "We developed a semi-structured interview protocol that asked participants about their organization's use of generative language models; the opportunities and challenges of generative AI in fact-checking; their organization's use of open models, and their motivations for adopting them; their reasons for using proprietary models; and what research could support the use of language models in fact-checking work. Where participants raised topics germane to the research but not covered in our interview protocol, we asked follow-up questions; for example, we asked P4 clarifying questions about their organization's beta release of a generative chatbot to collect misinformation circulating on platforms such as WhatsApp (WhatsApp 2024). Interviews lasted between twenty-five and ninety minutes and averaged approximately forty-five minutes. All interviews were conducted in English."}, {"title": "Data Analysis", "content": "Interviews were recorded over Zoom (Zoom 2024), transcribed using Rev Max AI (Rev 2024), and manually corrected as necessary by the first author. To answer the study's research questions, we adopted a deductive-inductive approach to coding the interview transcripts. We employed the following deductive codes: Uses of Open Models, Motivations for Using Open Models, Limitations of Open Models, Motivations for Using Proprietary Models, and Implications for Research. The authors first coded four transcripts, and the first author created an initial codebook that included inductively generated themes. The last author reviewed the codebook and the authors jointly revised the codes. The authors then coded four additional transcripts at a time until all transcripts had been coded, reviewing and revising the codebook after each round of coding. Finally, the authors followed a thematic analysis process (Braun and Clarke 2022) to generate themes that answered the study's research questions, using shared memos to precisely define the themes."}, {"title": "Data Pipelines in Fact-Checking", "content": "Consistent with prior work in human-centered data science, we found during thematic analysis that understanding the motivations of fact-checking organizations for using open or proprietary models requires understanding the ways in which they collect, analyze, and exchange fact-checking data and specifically where they use generative models in these processes. To that end, we begin by providing a conceptual model of five components of fact-checking data science pipelines in which participants described using generative models. We assign each component an icon subsequently used in the Motivations and Limitations sections to associate participant perspectives with components of the pipeline."}, {"title": "Data Ingestion", "content": "Participants reported using generative AI to collect and preprocess data, whether via media monitoring efforts employing AI-driven tools designed by social media companies or by the organizations themselves, or via tipline interfaces wherein a user can submit misinformation for fact-checking. We refer to this component of the data science pipeline as Data Ingestion, and denote it using an RSS icon to suggest the role of monitoring novel information. \nMost participants described media monitoring pipelines like that illustrated in Figure 2 as an essential means of observing circulating misinformation. P3 noted that \"social media listening is the main point of entry\" to their data pipeline, while P11 said that they focus on monitoring WhatsApp because \"the coverage is so massive\" in their country. P17 said that automated approaches including generative AI were necessary for media monitoring \u201cgiven the volume of production and how much content we can reasonably digest.", "pipeline": "we're totally focused on this pipeline, and we're capable of automating the monitoring and the detection phases.\" Participants also used generative AI to preprocess data for other pipeline stages. P16 said that they use generative AI to \\\"clean up data\\\" and that \\\"those things [language models] are a time saver", "content can be synthesized and reformatted\" for analysis. \nParticipants also described gathering information using conversational tiplines (i.e., chatbot interfaces) via which audiences can submit potential misinformation. Such interfaces are novel in comparison to media monitoring": "P24 described an internal conversational interface that existed as early as 2016, but this interface was never used for data collection. P11 said they first developed a user-facing tipline in 2019, and that they had to significantly scale the tipline during the COVID-19 pandemic, as user interactions increased more than tenfold. Conversational models utilizing modern generative AI first saw release in 2023, as P4 released a beta for a tipline utilizing ChatGPT for its back end, and P11 improved their existing framework with generative models. Generative tiplines serve several purposes, including engaging audiences and bringing in data from sources not easily observed through media monitoring. P24 noted conversational tiplines help to observe \u201cespecially provincial media that we are not that aware of or that we're not looking at regularly,", "constraining the conversation with [the user] to elicit more data that's actionable.\\\"\"\n    },\n    {\n      \"title\": \"Data Analysis\",\n      \"content\": \"Participants reported using generative AI to analyze large volumes of potentially misinforming content, leveraging its capabilities to parse highly contextual information, and utilizing few-shot approaches to avoid fine-tuning additional models. We denote Data Analysis with a magnifying glass to suggest the closer study of information. \nParticipants reported using generative AI to support classification and synthesis of text and multimodal content. P4 used generative AI to classify text based on constructs like urgency that they previously captured using proxies like message formatting: \\\"instead of counting how many exclamation points a post has or how many caps it uses... Gen AI... give [s] us a score from zero to a hundred of the sense of urgency.\\\" P15 noted using generative models as part of an ensemble of deep learning and rule-based NLP classifiers. P2 used generative AI to extract and classify \u201cpatterns of manipulative messages... like the government doesn't want you to know this or share it,\" noting that generative AI can be \u201csomething like an anti-spam filter": "or misinformation. P3 said they use generative AI for classifying multimodal misinformation, including memes: \"the text itself isn't disinformation. The image without the text isn't disinformation. The image plus the text can feed very clearly into a disinformation narrative.", "classifiers": "Generative AI... democratizes who can work with AI. . . with an API and a little magic with a prompt, you can have something really powerful.\u201d \nParticipants also embraced uses of generative AI for synthesizing data. P22 said that \"one of the values of generative AI is really synthesis, and looking and combing through tons of material, which... [we] will not have time for.", "in a tabular format... in 90% of the cases, it gives me a nice table.": 3, "work": "large language models are super good at basically clustering claims into a narrative. Fact checking just individual claims is whack-a-mole, a losing proposition. . . you'll never be able to scale it.\" P11 demoed a GPT-4 driven narrative system for us, explaining that it synthesizes new claims into overarching narratives already associated with fact checks, pending human review: \"this summary is linked to these four different contexts that we have already received. . . generative AI here has proposed to us something that is a bit overarching... [it] is seeing what we produce, and the evidence that accompanies [our] debunks, and is proposing to us [a fact check] that has already been verified by a human.\"\""}, {"title": "Data Retrieval", "content": "Participants reported using generative AI to facilitate Data Retrieval from catalogues of past fact checks or other verified sources of information maintained by the organization. We denote Data Retrieval with a database to suggest the retrieval of stored data. \nMany participants described using Retrieval-Augmented Generation (RAG) (Lewis et al. 2020) to allow GPT models to incorporate factual data. P4 described \u201ca RAG pipeline that connects OpenAI's GPT-4 with our database of fact checks... we have all the articles and fact checks that we ever published stored as embeddings. And then... we perform semantic search using cosine similarity, and we take the most relevant results.", "to go beyond very basic keyword searches... so that the search in the database was actually fruitful and accurate.": "ome participants adopted more complex methods. P22 described creating an internal knowledge graph from which generative models could retrieve content: \"We started building our own knowledge graph, our own ontology, and using that, the structured data from that, to generate content.", "data": "we built it on the claim review database... only our factcheck articles. so interacting with that search persona would give results only from the database along with a source link."}, {"data": "Parliament data is completely public, so we had the tech team... scrape the entire database ... and the persona only picks up responses from this dataset.\""}, {"title": "Data Delivery", "content": "Participants reported using generative AI to support Data Delivery to end users on websites or social media, as well as by providing automatic responses to users via conversational tiplines. We denote Data Delivery with a download icon to suggest the transfer of data to the end user. \nAmong the most common uses of generative AI reported was to format content or generate metadata prior to sharing it with audiences. P1 described using GPT models to \"generate hashtags for... mini FactCheck videos that we publish on TikTok.", "in most daily use cases, in terms of generative AI use, I would say it's help with promotion. So all of the social media content, coming up with summaries for SEO purposes for article publishing, title generation.": 5, "suggests times the best times for us to post our content on social media based on the type of demographic our subscribers are, or our audiences are, and when they're using social media.": "nParticipants who used conversational tiplines reported using generative AI to deliver information to end users, in addition to ingesting misinforming content. P4 said that incorporating ChatGPT into their tipline was \u201can obvious use case to improve a product that was already relevant for our readers,", "over 70,000 users, which for an organization our size is quite a lot.": "everal participants described an evolving information ecosystem wherein users sought information from specialized conversational agents, rather than traditional search applications. P21 said that they see generative AI as \u201ca preferred medium for somebody to get at the work that we have done... it is summarizing or reporting on work that was done by the trusted fact checkers.", "allows us to answer to a high volume of messages,\" noting that \"if you could actually ask the [chatbot], can you please tell me what the inflation was in the last five years? And it could actually answer you with information that comes from a reliable source, which we know is one of the big problems of generative AI, we think that's an enormous leap forward in the way that we can actually reach people with verified information.\" P19 contended that conversational agents can assist users \u201ceven if we don't have a fact check... explain this persuasion technique that's being used or the trope that is being repeated.\" P9 envisioned reaching younger audiences with an in-progress tool allowing a user to \\\"chat with our archive... this is a hurdle for young people to get into the discourse, that some background knowledge is missing. Maybe the AI could help.\\\"\"\n    },\n    {\n      \"title\": \"Data Sharing\",\n      \"content\": \"Participants noted that generative AI assists in Data Sharing between organizations, in that they structure data for sharing, and serve as shared computational infrastructure. We denote Data Sharing with a Users icon * suggesting transfer of data between organizations. \nP18 described generative AI as a tool for exchanging fact check data and collectively scaling audiences to address problematic information: \u201cespecially during elections, we try at (anonymized) to bring together other fact checkers so that people are not working in their own little silos... that's one area in which generative AI can really help. If fact-checkers are working together, whatever data they have, they can help scale the impact of their fact check to different segmented audiences that they serve.": 3, "fact checkers are banding together to offer a united response... European checkers will put their claims in a common database, and we will build systems in which, when we detect a new narrative. . . [if] it's present more than one or two countries, there will be a special task force that will be tasked with producing a debunk... we could not do it with a BERT or a Sentence BERT... early tests are promising with a [generative] large language model. This is also where the multilingual aspect comes in very handy.": 11}, {"title": "Motivations for Open Models", "content": "We found that four primary concerns motivate the use of open models: Organizational Autonomy, Data Privacy and Ownership, Task Specificity, and Capability Transparency. We describe these concerns in turn, making reference to the components of the data pipeline with which they intersect."}, {"title": "Organizational Autonomy", "content": "Participants expressed concern that dependence on proprietary generative models could compromise the autonomy of their organizations. They noted that open models offer more Reliable Availability than proprietary models, which could be affected by unexpected deprecation or provider instability. Several participants described uncertainty about using OpenAI's models in the wake of its CEO's firing and reinstatement. P4 said \u201cthe whole OpenAI drama was an eye opener. If OpenAI goes bankrupt tomorrow, then it's really bad to build products that depend on their software and on their API. We'd rather just host all the models that we use.", "deprecation": "We primarily rely on open source technologies. When Facebook releases their models and they open source it, that's what we use. We don't primarily rely on any [closed] corporate models.", "are targeting brands. And brands like McDonald's will have certain sets of keywords that will not change over time... we want to have this additional aspect, which is discovering new keywords because we want to stay on top of narratives.": 24, "for example, social media monitoring or social listening tools... we end up developing a lot of things out of need... because we don't have the same needs as a marketing agency.": "imilarly, participants noted that Avoiding Gatekeeping by corporations motivates open models. P10 said that \"we have to prove ourselves to the social media companies\" to gain access to the only tools to monitor and address misinformation on their platforms, noting this process is onerous for resource-challenged local organizations. Finally, participants said open models could help fact-checking organizations in Shaping the Future of fact-checking. P9 argued for accelerating adoption of open generative models: \u201cThis all brings chances and holds a lot of potential for us, and we should be the one who shape this development, and not let others be the one who dictate how we have to deal with this at some point because we waited too long."}, {"title": "Data Privacy and Ownership", "content": "Participants described concerns surrounding data privacy and ownership as a central motivation for using open models. P9 preferred using open models for Protecting of Internal Data, saying that \"we have very sensitive material that we're working with here, investigative reporting and investigative stories, and we don't want this to be used in [corporate] models and as training material,", "a sort of legal safety\" due to stricter European data protection laws. P4 said that, due to data privacy concerns, \u201cwe have as a policy to always prioritize using open source software where we can.": 21, "I blocked [OpenAI's] crawlers from being able to train on our content until some type of commercial compensation becomes available. I think most of us are kind of waiting, holding our breaths for the New York Times case lawsuit to play out because otherwise individual orgs the size of the fact checkers, we don't really have the leverage to accomplish what that lawsuit stands to do in setting a precedent.": "articipants also said that * Publisher Solidarity motivated use of open models over proprietary models that profit from journalistic organizations' data without consent. P4 noted their discomfort with \"the notion that those companies are profiting using other companies' and other people's work.", "the issue of copyright is a big one, especially for image generation... we could never use anything, any tool that generate images in our workflow, because we don't know how most models were trained.": 8, "infringement": "With generative AI, we were... scared to use it, because of the fact that we don't want to feel like we are plagiarizing... because of the possibility of... [copying] other articles.\" Participants noted that open models at least disclose their training datasets, offering some clarity concerning the risk of infringement. Finally, participants preferred open models for Mitigating Shared Risk when building technologies for the fact-checking community. P4 created a transcription tool for the community using OpenAI's models, but noted is not in production because \\\"a lot of people have concerns about sending their data interviews, important interviews - to OpenAI."}, {"title": "Application Specificity", "content": "Participants preferred open models for specific applications that demanded high performance, domain-specific tone, and control over toxic content. P14 said that their organization preferred small, fine-tuned open models for internal data analysis, noting that they exhibit stronger Q Task Performance and Cost Efficiency than proprietary generative models. P6 echoed this, noting that their organization still prefers thoroughly vetted, task-specific models for many tasks, despite hype about replacing these methods with proprietary models. P10 considered GPT models one of many tools, not the sole solution to any problem involving language, despite the marketing of proprietary models. Participants also noted that proprietary models intended for general-purpose use couldn't achieve Domain-Specific Tone for end user applications. P4 said ", "generic.": 8, "sounds completely unnatural,\" rendering it difficult to incorporate in their user-facing applications. Participants also reported using open models to gain more granular Toxic Content Control. Participants including P17 took issue with corporate models' one-size-fits-all approach to alignment with human values, which they noted could hamper the model's ability to respond to toxic and hateful content that fact-checking organizations handle during their work. P5 noted that the OpenAI's RLHF process makes it difficult for models \u201cto unlearn things that you've already taught it through human feedback,\" which are not advantageous for many fact-checking applications.\"\n    },\n    {\n      \"title\"": "Capability Transparency"}, {"content": "While participants agreed that GPT-4 outperformed open models, they also said that the capabilities of"}]}