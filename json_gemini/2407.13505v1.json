{"title": "Robots Can Multitask Too: Integrating a Memory Architecture and LLMs for Enhanced Cross-Task Robot Action Generation", "authors": ["Hassan Ali", "Philipp Allgeuer", "Carlo Mazzola", "Giulia Belgiovine", "Burak Can Kaplan", "Stefan Wermter"], "abstract": "Large Language Models (LLMs) have been recently used in robot applications for grounding LLM common-sense reasoning with the robot's perception and physical abilities. In humanoid robots, memory also plays a critical role in fostering real-world embodiment and facilitating long-term interactive capabilities, especially in multi-task setups where the robot must remember previous task states, environment states, and executed actions. In this paper, we address incorporating memory processes with LLMs for generating cross-task robot actions, while the robot effectively switches between tasks. Our proposed dual-layered architecture features two LLMS, utilizing their complementary skills of reasoning and following instructions, combined with a memory model inspired by human cognition. Our results show a significant improvement in performance over a baseline of five robotic tasks, demonstrating the potential of integrating memory with LLMs for combining the robot's action and perception for adaptive task execution.", "sections": [{"title": "I. INTRODUCTION", "content": "Despite the physical limitations due to their embodiment, humanoid robots are particularly effective tools because of their anthropomorphic shape, which can significantly improve the versatility and effectiveness of robots, especially in environments designed for human interaction [1]. Moreover, the humanoid physical shape supports collaborating with humans whose legibility and predictability of robot actions are fostered by the robot's physicality, with positive impacts on the safety and trust during the interaction [2]. Inspired by the effectiveness and adaptability of human cognitive abilities, cognitive robotics aims to draw from more than embodiment when designing robotic platforms. Key attributes like human perception, selective attention, different types of memory, and other facets of human cognition are essential to improve the autonomy, flexibility, and capabilities of robots [3].\nThe development and utilization of foundational models in robotic platforms have been fundamental in driving advancement toward building autonomous artificial agents. Large Language Models (LLMs) have been used as symbolic reasoning instruments in various robotic applications such as reasoning about user's intentions in human-robot collaboration [4], generating robot action plans in grounded environments [5], and task planning in environment-sensitive"}, {"title": "II. RELATED WORK", "content": "Integrating sensory input with memory is essential in artificial systems as it enables them to retain sensory information for environment recognition and adaptive responses [11]. Moreover, memory plays a vital role in humanoid robots since it brings together conceptual knowledge gained through experience with the robot's behavior, enabling adjusting to complex interactive scenarios and tasks [12]. Memory-based architectures have various applications such as transferring robot manipulation skills across different human environments [13], memory-powered incremental learning with human-in-the-loop corrections [14], and supporting tasks like action-planning for localizing a target and navigating towards it [15]. Existing work incorporates models of memory into robot tasks like planning with occluded objects [16], reasoning about object permanence [17], and learning human personal attributes like voices and faces [18]. However, such approaches model memory representations and reasoning as distinct components or separate processes while embedding memory directly within a reasoning backbone, like an LLM, is beneficial for context-aware decision-making.\nLLMs have recently been used to support robots' reasoning and decision-making. However, relying solely on LLM's own internal memory is insufficient for handling complex multi-task reasoning demands of dynamic robotic environments. Some approaches address the limitation in LLM memory by injecting curated memory cues as contextual information into the LLM's prompts [19], using previous structurally stored dialogues to enrich current LLM input [20], or caching previous conversations in a long-term memory storage [21]. Other methods rely on retraining or fine-tuning LLMs with contextual memory information [22][23]. However, relying on LLM context memory is constrained by context length and can lead to reasoning instability, while updating LLM parametric memory requires retraining and fine-tuning, causing cost and robustness concerns [7]. The limitations of LLM's own memory are also underscored in our experiments, highlighting the need for combining LLMs with efficient and up-to-date robust memory systems.\nOur work addresses these limitations by incorporating LLMs for reasoning and action generation in a hierarchical framework. The idea of leveraging a hierarchy of LLMS to support reasoning and task specialization exists in the"}, {"title": "III. METHODOLOGY", "content": "We take inspiration from human cognition to integrate a memory framework with the robot's reasoning and action generation. In Fig. 2, we show a simplified overview of our system's workflow. The robot continuously observes the objects in its proximate environment through its visual perception and has specifications for pre-defined tasks. Upon a new task command, the system triggers the LLM to store and recall relevant memory components. Consequently, a suitable robot action is generated, applicable to the objects, and suitable to the task. Every time an action is performed, the memory records are updated, creating and maintaining a live memory repository and forming a continuous and adaptive feedback loop between the robot's actions, its memory, and the environment. We utilize two LLMs for managing the memory models, each optimized with prompts designed to achieve specific stages of the memory pipeline. In this section, we introduce our robot, its visual perception module, and present our proposed architecture and memory models.\nOur system runs on the Neuro-Inspired COLlaborator (NICOL) [27] built in the Knowledge Technology group at the University of Hamburg. NICOL is a tabletop robotic platform with a humanoid head and two 8 DoF robotic arms. The robot features social cues like facial expressions as well as object and arm manipulation capabilities like grasping and neuro-genetic inverse kinematics [28]. The NICOL platform is based on the Robot Operating System (ROS), facilitating integration with various sensory modules and deep learning"}, {"title": "A. Robotic Platform: NICOL"}, {"title": "B. NICOL's Visual Perception and Action Parsing", "content": "We use NICOL's 4K camera located inside the robot's left eye to capture the environment visually and the objects on the table. We use the ViLD [30] object detector, enabling zero-shot object detection and recognition without pre-training. The NICOL's ViLD object detector has been optimized to work in real-time on continuous camera streams with high efficiency and reliability [29]. The detector generates bounding boxes for the objects on the table and their locations. All objects detected outside the area of interest, i.e., the robot's table, are removed. The objects detected are expressed by text so all object labels can be streamlined within the system's pipeline, making it compatible with an LLM application. NICOL can also parse textual LLM-generated actions and translate them into robot physical actions [29]. The robot can perform arm manipulations like pointing to objects on the table, grasping objects, and pushing objects to the user. The robot also has other head manipulation and facial expression capabilities. However, our system utilizes only the robot's arm manipulation skills in its action and task specifications."}, {"title": "C. Proposed Architecture", "content": "Our architectural design comprises a stack of two layers (cf. Fig. 3), which work collaboratively to achieve adaptive task execution. Each layer features an LLM corresponding to a specialized level of reasoning and memory capabilities but works in tandem with the other LLM to generate a set of robotic actions and achieve task-oriented goals. In the lower level (level 0), a worker LLM is dedicated to creating and maintaining two memory variants (working and declarative memory) based on real-time inputs from the environment and interaction history, and supplying foundational information for the subsequent decision-making processes within the system. The worker LLM represents our instruction-based LLM which receives structured commands triggered by the coordinator LLM, resulting in structured and precise outputs.\nThe coordinator LLM in level 1 of the stack represents our reasoning LLM backbone which triggers the different functions of the worker LLM, integrates its output with the task's context, while focusing on complex task reasoning and decision-making. The coordinator LLM integrates input from the environment like the objects on the table and task-specific memory cues received from the worker LLM to generate robot-actionable commands and instructions, suitable to the robotic task at hand. For example, the output of the coordinator LLM in a tower building task might be an action like \"put the yellow cube on top of the cube tower\" that requires reasoning and understanding of the task at hand, objects on the table, and overall context of previous interactions (which objects should be stacked in the tower, remaining cubes on the table, cubes already stacked in the tower, etc.).\nCollectively, the two layers (level 0 and level 1) enable the system to adapt to the changing environment as the objects on the table and task requirements are continuously changing, i.e., smoothly and seamlessly switching between tasks. Since both LLMs use text for input and output, the communication of information across the two layers can be easily standardized and unified, thus, providing a robust framework for task execution that is responsive to the environment and also adaptive in terms of decision-making."}, {"title": "D. LLMs for Instructions and Reasoning", "content": "We distinguish between the coordinator and worker LLMs optimized for reasoning and following instructions, respectively. The role of the coordinator LLM is to orchestrate the task execution process. At the system start, it receives a base prompt with specifications that ground the robot's knowledge of the possible tasks and their respective actions. This information represents the system's procedural memory, informing the agent about the steps needed to achieve each task goal (see Prompt 1 and Prompt 2). These specifications are modular and can be easily swapped or extended. The coordinator LLM receives the objects on the table from the open-vocabulary object detector and can then call two unique actions to invoke the worker LLM (see Prompt 3) for triggering the rest of the memory process. After retrieving the output from the worker LLM, the knowledge can be injected within the existing specifications in order to prompt the coordinator LLM to generate a suitable robot action.\nAt level 0, the worker LLM is responsible for parsing and interpreting the environment input (passed from the coordinator LLM), and thus, dynamically managing the real-time sensory perception of the robot. Upon receiving the instruction from the coordinator LLM, it creates the working memory, which represents the selective attention of the robot. Additionally, it maintains the declarative memory, which represents facts about the history of interaction over a time period. Unlike the coordinator LLM which has an overview of all the interactions, i.e., its conversations include all the previous dialogues, the worker LLM receives only a single prompt at a time without any previous dialogue or context."}, {"title": "E. Working Memory", "content": "The worker LLM extracts and refines the working memory, which is a compact representation of crucial information to task completion. Each working memory is specialized for a specific task without knowledge of other tasks, and consists of several task- and environment-related cues. Task-related cues are: 1) Task Reminders: additional short descriptions of the task actions and desired goal, and 2) Task State: which tracks the current state of the objects in relevance to the current task progress. For example, in a fruit arrangement task (the fruits must be placed in the bowl), it might include information such as \"the banana and apple are in the bowl\".\nThe working memory also includes a selective object state extracted by combining the information of the declarative and procedural memory. Specifically, it receives all the objects on the table and outputs only the task-relevant objects, representing the robot's visual selective attention. In Prompt 4, we highlight an example of the sorting task (fruits and dishware items must be placed in different bins). The first part of the prompt is the task description, which is then combined with the visual input identifying the objects: apple, banana, cup, bowl, baseball, pear. The prompt's final part sets rules for the worker LLM on output formulation, ensuring the working memory remains consistent before passing it on back to the coordinator LLM. As a result, only necessary objects are retained, i.e., objects not task-relevant are removed from the working memory (the baseball in the given example)."}, {"title": "F. Declarative Memory", "content": "When the robot modifies the environment with object manipulation actions, these changes are logged into the sys-tem's declarative memory, ensuring that the robot's internal state through its memory is precisely synchronous with the external world. Since all the robot actions are recorded, the system always has knowledge of the interaction history over any duration of time. We store this memory persistently as log files, one file per task. Similar to working memory, declarative memory is specialized to one task and does not contain information about other tasks. As a task newly starts, the interaction history is empty and data is accumulated as the task progresses. If the task gets interrupted, i.e., the robot is asked to switch to another one, the interaction history is kept safe. When the robot continues the interrupted task, it retrieves essential information like previous actions, environment state, and task state at the interruption point.\nThe worker LLM is responsible for creating and maintaining the logs of the declarative memory. An example using the sorting task is shown in Prompt 5 (for simplicity, we only show the information extraction of box 1). The worker LLM is given the persistent task logs and prompted to extract the remaining objects on the table (environment state) and the objects in box 1 and box 2 (task state). Similar to the working memory, the prompt has a clearly defined output structure for consistency of the information in the system across the two LLMs. The log file is structured by entries ordered by the executed actions, which allows the LLM to efficiently identify the most recent environment and task states."}, {"title": "IV. EXPERIMENTS AND EVALUATION", "content": "We evaluate our system using five robotic tasks, each requiring a set of objects as input and an appropriate sequence of robot actions as output to achieve the task expected goal. Each task involves both a cognitive aspect, where the robot must analyze the objects and reason about the task definition to identify the ones required to achieve it, and a physical aspect, in which the robot performs the appropriate object manipulation function. The objects necessary to perform tasks vary per task, however, we limit the total input to six objects since our early experiments showed that an input size of six objects is most reasonable given the limited number of tokens the LLM can process within its context window. We use YCB objects for the tasks in addition to colored cubes for the tower task. An overview of the tasks is as follows: 1) Sorting Task: the robot places fruits and dishwasher items in different containers, 2) Arrangement Task: the fruits must be arranged by placing them inside a bowl, 3) Pointing Task: the robot points to all the yellow objects on the table and then to all the red objects, 4) Recipe Task: objects that are required to make a jello recipe with banana topping are given to the user, and 5) Tower Task: the robot builds a tower by stacking only colored cubes on top of each other. A summary of the tasks is provided in Table I. Our system runs on the NICOL robot, but for evaluation, we disable the robot's physical control parts of our system since we focus on evaluating its action-generating aspect and not the robot's arm manipulation.\nWe conducted a total of 50 trials for each experiment using two LLMs: ChatGPT-3.5-Turbo-0125 and Llama 3. The GPT-3.5 model has 175B parameters and a context window length of approximately 16k tokens. We opted for GPT-3.5 over the GPT-4.0 models since it is more efficient in terms of cost and speed. Experiments with this model were conducted using the API services of OpenAI. In contrast, Llama 3 has 70B parameters and a context window size of 8k tokens. An advantage of open-weight models is that they allow potentially controlling the seed values, supporting result reproducibility. All the Llama experiments were conducted using a single NVIDIA A100 GPU with 80 GB of VRAM. Due to the GPU's capacity constraints, we utilized the 8-bit quantized version of Llama 3, requiring approximately 72 GB of VRAM. This model is provided by Ollama\u00b9 and compatible with Langchain\u00b2. We used identical parameters for both LLMs to ensure a fair comparison. The temperature was set to 0.2 for controlled results. Given that consistency is already regulated by the temperature value, we set top_p to 1 to avoid eliminating any potential candidate responses. Both the frequency-penalty and presence_penalty were set to 0, acknowledging that the same tokens may recur due to the defined scope of objects and actions in the experiment."}, {"title": "A. Single and Multi-task Scenarios", "content": "Our evaluation scheme consists of three modes (as sketched in Fig. 4). Standalone mode refers to running each task independently, followed by a complete LLM chat history reset. This experiment represents the baseline performance for each of the defined tasks. In consecutive mode, we run the tasks back to back in a sequence and reset the chat only after executing the last task. In the intervened mode, the system performs a multi-task scenario, in which each task is interrupted at an intervention point (defined at a 50% task completion based on the total number of required actions). After each intervention point, the system switches to the next task and back to Task 1 after the completion of the last task. A chat reset follows a 100% completion of the last task. The consecutive and intervened scenarios require memory components, and thus, we evaluate them with and without memory for comparison. We present the results of our experiments in the next subsections, using the following metrics: 1) Success Rate: the ratio of the correctly completed tasks to the total trials. For a task to be considered complete, the system must generate all the required actions correctly, 2) Task Retention: assesses whether the system can correctly identify and recall the state of the task after its completion, e.g., which objects are in box 1 and box 2 and, 3) Environment Retention: verifies whether the system can correctly recognize the state of the environment after the task completion, i.e., which objects remain on the table."}, {"title": "B. Results for Standalone Tasks", "content": "This experiment is our single-task scenario, meaning the system is configured to execute one task at a time with knowledge limited to the task at hand and its actions. Therefore, the system does not require memory of previous interactions and runs with the memory components disabled. As shown in Table II, the system completes the tasks with a high success rate as they are presented individually, and with comparable performance across both LLMs. Furthermore, the system maintains task- and environment-related information with high accuracy when evaluated at the end of each task, even reaching a perfect score for some tasks. The Llama 3 success rate slightly dropped for the arrangement and recipe tasks. In the arrangement task, Llama 3 occasionally generated a batch of actions instead of a single action per prompt. Despite the soundness of the generated actions, generating multiple actions at a time violates our strictly defined evaluation scripts. In the recipe task, Llama 3 occasionally generated actions for giving the wrong toppings, likely due to the model's training containing real data of recipes with jello and fruits. However, this setup demonstrates that the tasks are inherently manageable for the LLM hierarchy to handle when presented in a standalone manner. Thus, this scenario represents our baseline of task performance compared to the more complex evaluation modes: consecutive and intervened."}, {"title": "C. Results for Consecutive Tasks", "content": "In this setup, the system has knowledge of all the defined tasks and possible actions through its declarative memory. Hence, the LLM must not only reason about the task requirements but also choose the appropriate actions. In this experiment, we observe that running the tasks consequently presents a considerable challenge to the LLMs when tuned without the working memory (cf. Table III). The system achieves average performance for some of the tasks (sorting and recipe), however, the success rate and retention drop significantly for most tasks compared to the standalone experiment. The pointing task is particularly challenging since the LLM must reason about the order of objects, not only their type, as well as the appropriate pointing action. Similarly, the tower task is challenging to the system, and we observe failures in correctly reasoning that black and white cubes (not colored) must not be stacked in the tower.\nIn contrast, we report a noticeable boost in the success rate and retention metrics when running the experiment with the working memory enabled (see Table III). Both LLM variants perform comparably and even reach a perfect score for some of the tasks, e.g., arrangement. Noticeably, Llama 3 outperforms ChatGPT 3.5 on the sorting task due to errors in the coordinator bypassing the worker LLM output and generating redundant unnecessary actions, negatively influencing its overall success and retention values. Overall, both LLMs achieve success rates that approximate the baseline (standalone setup) and even surpass it for tasks like arrangement and recipe (see Fig. 5). This experiment shows a significant improvement in performance due to the utilization of the worker LLM and working memory, thus, suggesting that the coordinator LLM becomes a bottleneck in action generation over a longer period of interaction time."}, {"title": "D. Results for Intervened Tasks", "content": "We evaluate the declarative and working memory by creating intervention points, i.e., interruptions, in which the execution of a task is paused, and the system must switch to the next task. Each trial in this experiment involves two execution rounds. In the first round, only 50% progress of each task is accomplished, and in the second, the system resumes every paused task from the interruption points until full completion. For consistency, we run the tasks by their definition order: sorting, arrangement, pointing, recipe, and tower. However, the task order does not influence the experiment since each task has its own input and runs independently, i.e., one task success or failure does not impact other tasks\u2019 performance. We run the intervention mode twice: without memory and with declarative and working memory. Similar to the consecutive mode, the tasks are challenging to both LLMs without memory (cf. Table IV). Noticeably, the recipe task performance drops for GPT 3.5 due to errors in the coordinator LLM, generating noisy actions on wrong objects. Moreover, the performance of Llama 3 is below average for all tasks. The main error source is the LLM \"forgetting\" the task specifications due to the longer interaction duration, and thus, generating false actions or no actions. Also, the logical task reasoning was challenging, and the coordinator LLM generated incorrect actions, e.g., pointing to an object neither yellow nor red, placing a non-fruit object in the bowl, or giving the wrong object in the recipe task.\nHere again, our experiment demonstrates noticeable improvement in the success rate, task retention, and environment retention scores across both LLMs (see Table IV). The scores for the pointing, recipe, and tower tasks are comparable for both models and approximate the baseline (see Fig. 6). Interestingly, Llama 3 scored higher success and retention values in the sorting task due to the task grounding of GPT 3.5 leading to faulty commonsense reasoning, thus, occasionally generating wrong actions such as <move_to_box_2(pear)> instead of <move_to_box_1(pear)>. We also observed Lama 3 generating a wrong list of objects in the working memory of the arrangement task, which explains the slight drop in its performance. However, when comparing against the previous experiment (consecutive with memory), this seems to be a challenge only when the system must provide this information in the second round of task execution. We hypothesize this to be due to the context size of Llama 3 being roughly half the size of this ChatGPT model (8k vs 16k) since this mode requires multiple rounds. We also observed high variability in the output with these answers, hinting that the parameters chosen for Llama 3 could be further optimized for more stable output (ex: we used top-p of 1 for both models)."}, {"title": "V. DISCUSSION AND CONCLUSION", "content": "In this work, we proposed an architecture for combining cognitively inspired memory processes with a hierarchical LLM-powered framework for enabling the robot to smoothly and effectively switch between tasks. Our layered approach leverages the efficiency and accuracy of LLMs at executing instructions and their more advanced reasoning skills, resulting in the ability to maintain contextually rich interactions across multiple tasks. Our system has been applied to the physical NICOL robot but generally has various applications for increasing the productivity of human-robot collaboration, e.g., enabling the robot to simultaneously perform multiple tasks and manage various high-level goals in parallel. Our system can be extended with more task and action specifications and can be used for adaptive task execution that considers the interaction context at runtime. Thanks to our modular architecture, the system can integrate different LLM types, e.g., a powerful LLM for reasoning and a smaller lower-cost LLM for executing instructions, supporting cost and time efficiency while maintaining effective task handling.\nOur experiments showed an inherent challenge for the current generation of LLMs (ChatGPT and Llama) to manage different tasks and goals at the same time by relying on the LLM's own internal memory or the context of previous dialogues in the prompt. However, our implemented memory process (declarative, procedural, and working memory) demonstrated a significant boost in the performance of both models compared to our baseline. In future work, memory-based architectures like our proposed one can be expected to be beneficial for human-robot collaboration besides action generation. For example, we plan research on managing high-level goals in multi-party robotic scenarios, where humans are interacting with the robot, each having their own defined goal. As LLMs with larger context windows become more available and accessible, we will also evaluate our system with a larger number of tasks and input objects."}]}