{"title": "G-Style: Stylized Gaussian Splatting", "authors": ["\u00c1ron Samuel Kov\u00e1cs", "Pedro Hermosilla", "Renata G. Raidou"], "abstract": "We introduce G-Style, a novel algorithm designed to transfer the style of an image onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting is a powerful 3D representation for novel view synthesis, as-compared to other approaches based on Neural Radiance Fields-it provides fast scene renderings and user control over the scene. Recent pre-prints have demonstrated that the style of Gaussian Splatting scenes can be modified using an image exemplar. However, since the scene geometry remains fixed during the stylization process, current solutions fall short of producing satisfactory results. Our algorithm aims to address these limitations by following a three-step process: In a pre-processing step, we remove undesirable Gaussians with large projection areas or highly elongated shapes. Subsequently, we combine several losses carefully designed to preserve different scales of the style in the image, while maintaining as much as possible the integrity of the original scene content. During the stylization process and following the original design of Gaussian Splatting, we split Gaussians where additional detail is necessary within our scene by tracking the gradient of the stylized color. Our experiments demonstrate that G-Style generates high-quality stylizations within just a few minutes, outperforming existing methods both qualitatively and quantitatively.", "sections": [{"title": "1. Introduction", "content": "While humans excel at creating paintings with specific contents and styles, this task has proven challenging for computers to replicate. With the advent of neural networks and in particular, Convolutional Neural Networks-algorithms have been developed to transfer the style of one image onto another [GEB15a]. In this process, a content image refers to the original image whose subject matter we aim to retain, while a style image is the one whose artistic style we want to apply to the content image. These algorithms enabled the modification of the style of an image while preserving its content by matching the statistical properties of the embeddings of both content and style images, as obtained from a deep neural network.\nWith the appearance of novel view synthesis methods based on neural networks (NeRFs) [MST*20], researchers turned their attention to applying style transfer techniques to entire 3D scenes. Style transfer for 3D scenes aims to generate novel views of a scene from a finite number of images of the same scene with a particular style specified by a style image examplar. To succeed in this task, the style transfer"}, {"title": "2. Related Work", "content": "2D Style Transfer. Gatys et al. [GEB15a] first introduced a method\nfor neural style transfer, where the artistic style of a provided image is\nused to visually reconstruct the content of another image. The method\nis based on iterative optimization to match the output and second-order\nstatistics, expressed as the Gram matrices of hidden layers of a pre\ntrained network. If preserving the content is not necessary, the method\ncan synthesize a texture resembling the provided style image, initializing\nthe process with a noise image [GEB15b] and disregarding the part\nof the original loss function that is responsible for content preservation.\nThis approach has been since refined to enable transferring features\non multiple scales [ZGW*22], by utilizing Generative Adversarial\nNetworks (GANs) in a course-to-fine fashion [JBV17] or diffusion\nmodels [ZHT*23, WZX23,CHH24]. Also, different ways of capturing\nand matching the style statistics have been proposed [LYY*17,GCLY18,\nKSS19]. All aforementioned methods aim to improve style details by\nintroducing pen or brush strokes, which were too blurry in the original\nwork of Gatys et al., or by better preserving semantic consistency.\nInstead of relying on matching extracted style statistics, searching\nfor nearest neighbors in the feature space and minimizing distances\nbetween them is another option for transferring style [KSS19, CS16,\nLYY*17, LW16, ZKB*22a]. This strategy can lead to significant\nimprovements in transferring high-frequency features, by avoiding the\naveraging of features that match possibly multi-modal style statistics.\nExisting methods can be further subdivided based on whether they\niteratively modify an image, just like in the original work of Gatys et al.,\nor explicitly minimize an objective function with a single feed-forward\npass [CS16, HB17, AHS*21]. While the feed-forward-based approaches\nare orders of magnitude faster, their results are generally of lower\nquality than the slower optimization-based techniques.\n3D Style Transfer. 3D style transfer refers to modifying the\nappearance of a 3D object or a scene so that when viewed from\ndifferent angles, it matches the style of a given exemplar. To reuse\na 2D style transfer method, one needs to extract and employ a 2D\nimage of a given scene. This can be either obtained by utilizing\na differentiable renderer [MPSO18, ZKB*22a], by slicing the 3D\nvolume [HMR20, GRGH19, ZGW*22, CW10, KFCO*07], or by\ndirectly working on the surface manifold of textured meshes [KHR24].\nExisting approaches usually rely on a single way of representing\nobjects and whole scenes, which is also the main point of influence for\ntheir performance. The approach of Cao et al. [CWNN20] uses point\nclouds, yielding holes due to measurement errors during scanning and\nbeing unsuitable for representing surfaces and real-world scenes. Con\nversely, Mordvintsev et al. [MPSO18] and Hoellein et al. [HJN22] use\ntextured meshes, which can be reconstructed from the aforementioned\npoint clouds. Due to the discrete and restrictive nature of mesh repre\nsentations, these approaches can suffer from different types of artifacts.\nLately, Neural Radiance Fields (NeRFs) [MST*20] and Gaussian\nSplatting [KKLD23] have become very prominent in reconstructing\nobjects and scenes. Unlike point clouds and meshes, NeRFs and\nGaussians are soft volumetric representations. Naturally, these\nrepresentations can be utilized for scene reconstruction or style transfer.\nLike in the 2D case, these style transfer methods can be subdivided\ninto two categories: zero-shot and iterative methods. Zero-shot meth\nods [LZL*23, LZC*23a, XCX*24] focus on matching colors, relighting,\nor transferring details on a small scale to achieve multi-view consistency.\nThis limitation arises from their inability to quickly and consistently\nembed large features into scenes. Thus, these methods struggle to\nsynthesize large patterns that span significant portions of a given scene.\nOn the other hand, iterative methods leverage rendering scenes from\nmultiple viewpoints. Hence, they can construct large-scale patterns that\nremain consistent across different views, while any inconsistencies are\ncorrected during the training process. Zhang et al. [ZKB*22a] propose a\nnew loss based on nearest neighbor feature matching (NNFM) which bet\nter preserves details and also optimizes backpropagation by deferring the\nchanges to the network after first computing the losses for full-resolution"}, {"title": "3. Background: NeRFs and Gaussian Splatting", "content": "In this section, we briefly describe the two most commonly used\n3D representations for novel view synthesis, NeRFs [MST*20] and\nGaussian Splatting [KKLD23].\nNeRFs. Neural radiance fields have revolutionized the field of novel\nview synthesis by introducing a new scene representation, and an\noptimization algorithm to train this representation only from images.\nThe outgoing radiance at any point x in the scene for any view direction\nv is modeled by a parametric model \\(\\Phi(x,v)\\) with parameters 0. This\nmodel is usually a Multi-layer Perceptron (MLP), which is chosen due\nto the universality provided by this type of model. To train this model,\nthe scene is rendered from multiple views using the volume rendering\nalgorithm [Max95]. By comparing the generated image I\u2207 to a real\npicture of the scene Igt, gradients can be computed for the parameters\n0 through the rendering operation and the scene representation can\nbe updated to match the ground truth images. Despite the high-quality\nimages generated by NeRFs, the control provided to the user is\nlimited, and they demand extensive rendering times due to the multiple\nevaluations required to compute the value of a pixel.\nGaussian Splatting. Gaussian Splatting [KKLD23] has emerged as\na viable alternative to address the main limitations of NeRFs: limited\nscene control and low rendering speed. To reconstruct a real-life scene\nfrom ground truth images Igt, Kerbl et al. use the Structure from Motion\nalgorithm [Ull79] to obtain camera poses for each image and a spare set\nof initial 3D points. The 3D points are then transformed into Gaussian\nfunctions, each representing a point in the scene. In this way, Gaussian\nSplatting represents the function \\(\\Phi(x,v)\\) as a set of 3D Gaussians.\nEach Gaussian is defined by its mean u, covariance matrix \u2211, opacity\n8, and color c. By representing the color with spherical harmonics,\nview-dependent effects can also be captured. Since the covariance\nmatrix \u2211 has to be positive semi-definite, which is difficult to enforce\nduring optimization, the covariance matrix is obtained by RSSTRT,\nwhere S is a scaling matrix and R a rotation matrix obtained from a\nquaternion. The covariance matrices of the Gaussians are initialized\naccounting for their neighbors to conservatively cover the surfaces and\nprevent holes in the reconstruction."}, {"title": "4. Methodology of I-Style: Gaussian Splatting with Style", "content": "In this section, we describe our proposed algorithm: I-Style. We first\nprovide an overview of our approach (also illustrated in Figure 2),\nfollowed by a detailed explanation of its substeps.\n4.1. Overview\nOur algorithm takes as input a scene represented with a set of Gaussians\nG and a set of ground truth images Igt. Subsequently, it modifies\nG based on the style provided from a style exemplar Is. First, we\npre-process G to remove long narrow Gaussians and Gaussians covering\nlarge areas, making the initial set of Gaussians uniform. Once the\ninitial representation has been pre-processed, we create an additional\ncolor cs associated with each Gaussian which is initialized with the\noriginal color Cgt. These new colors cs are modified during a stylization\nprocess that uses a composition of several losses to preserve different\nproperties of the style of Is. Since the geometry provided by the initial\npre-processing step can be limited to represent detailed style features,\nG undergoes a geometric fine-tuning step. In this step, the Gaussians\nare split based on the gradient of cs and fine-tuned to match the original\nscene images Igt by modifying \u03bc, \u03a3, \u03b4, and Cgt. The stylization and\ngeometric fine-tuning steps are repeated until convergence.\n4.2. Pre-processing Step\nAlthough Gaussian Splatting offers high-quality scene representation,\nthe original approach has limitations. It often generates large flat Gaus\nsians to depict uniform, flat surfaces (e.g., walls), and narrow elongated\nGaussians to capture high-frequency details. The latter might also come\nas a byproduct of the optimization process. When incorporating an\nadditional style into the scene, large flat areas might need additional\nGaussians to incorporate extra details, while already highly detailed\nareas may not need more Gaussians to stylize them properly. Therefore,\nin the initial step of our approach, the scene undergoes a Gaussian nor\nmalization process where the resulting representation G is composed of\nGaussians of similar size and shape.\nFlat Gaussian Split. To detect under-sampled areas, we compute\nthe approximated maximum projected area of each Gaussian, A\u00a1.\nWe compute A; by multiplying the two highest components of the\nGaussian's scaling matrix S\u00a1. Then, we mark for splitting all Gaussian\nabove a threshold \\(t_f = \\mu_a + \\gamma \\sigma_a\\), where ua is the mean of all A in"}, {"title": "4.3. Stylization Step", "content": "Once we have pre-processed the scene, we start the stylization process.\nDuring stylization, we render the scene from multiple views using\nCs and compute several losses over the images carefully designed to\npreserve the style of Is at different scales. We hereby describe in detail\nthe different losses used in our algorithm.\nLow-frequency Style. The style of an image is determined by color\nand high-frequency details and also by large-scale patterns and features.\nTo preserve these low-frequency features of the style image, we employ\na CLIP-based [RKH*21] loss. We encode our rendered images T and\nthe style image Is into a feature vector using the image encoder of\nthe CLIP model, C. Then, our loss is defined as the similarity between\nthese feature vectors where similarity is measured as the 12:\n\\[L_{CLIP} = \\frac{1}{K} \\sum_{k=1}^{K} ||C(I_r^k) - C(I_s)||_2\\]\nwhere K is the number of rendered images in the batch.\nHigh-frequency Style. Our CLIP-based loss can capture large-scale\npatterns in the style image. Yet, fine details, such as brush strokes in\na painting, might not be well represented with this loss. Therefore,\nfollowing Zhang et al. [ZKB*22a], we use a nearest neighbor feature\nmatching (NNFM) loss utilizing a pre-trained VGG-16 network [SZ14]\nto capture the high-frequency style patterns. The architecture of a\nfeature extractor can play a significant role in the quality of the\ngenerated results. We selected VGG-16 for its proven effectiveness in\nprevious style transfer solutions, facilitating easier comparisons with\nthose methods. The NNFM loss is a replacement for the widely used\nGram matrix-based loss of Gatys et al. [GEB15a]. Instead of matching\nstatistics of feature maps Fr and Fs, this loss searches for nearest\nneighbors in the feature space. Let Fr and Fs be the VGG-16 features\nmaps of Ir and Z5 respectively, and let F(i,j) be the feature vector at\npixel location (i, j). The NNFM loss is then defined as:\n\\[L_{NNFM}(F_r, F_s) = \\underset{(i',j')}{Emin} D(F_r(i,j), F_s(i',j'))\\]\nwhere N is the number of pixels in Fr, and D is the cosine distance\nbetween two vectors. As in Zhang et al. [ZKB*22a], we use feature\nmaps from the third block of VGG-16.\nRegularization. To avoid the deterioration of features that are\nnecessary for scene understanding, like in the work of Zhang et\nal. [ZKB*22a], we utilize a content loss LC, which is the 12 loss\nbetween the features of rendered images Fr and ground truth images\nFgt. This loss also uses the features from the third block of VGG-16.\nAdditionally, we incorporate a total variation term in our loss Lyy to\nprevent noise in the resulting renderings.\nComplete Style Loss. Our final loss is a weighted sum of all the losses,\ngiven by the equation:\n\\[L = \\lambda_{CLIP}L_{CLIP} + \\lambda_{NNFM}L_{NNFM} + \\lambda_{C}L_{C} + \\lambda_{TV}L_{TV}\\]\nWe perform the stylization process for 15 epochs. Note that\nforward-facing scenes do not have as strict multi-view consistency\nrequirements as 360\u00b0 scenes due to the limited viewing angles of the\nground truth images. As such, they converge more easily compared to\n360\u00b0 scenes. To account for the differences between these two types of\nscenes, we use different parameterizations. For forward-facing scenes,\nwe use an exponentially decaying learning rate from le-1 to le-2, and\nfor 360\u00b0 scenes the learning rate decays from le-2 to 5e-3. Also, we\nset ACLIP to 10, ANNFM to 100, \u03bb\u03b5 to 0.05, and dry to 1e-4. For 360\u00b0\nscenes, we set ANNFM to 10."}, {"title": "4.4. Geometric Fine-tuning Step", "content": "In the pre-processing step, we place more Gaussians in the undersam\npled areas, making the Gaussians more uniform in size. This step is\nconservative enough to not overly increase the number of Gaussians\nand, thus, not oversample a given scene. With the new Gaussians, it is\npossible to synthesize features that otherwise could not be represented.\nHowever, the size of the Gaussians still limits the synthesis of very fine\nfeatures in our stylized scene.\nTo overcome this, during the stylization process, we periodically split\nGaussians based on their cs gradient. Similarly to the work of Kerbl et\nal. [KKLD23], we keep an accumulation buffer B to store the norm of\nthe gradient cs. After each iteration and for each Gaussian, we add the\nnorm of the cs gradient to B and periodically split a user-defined percent\nage of Gaussians with the highest value. Since splitting Gaussians mod\nifies the geometry of the scene, after each splitting, we optimize their u,\n\u03a3, \u03b4, and cgt in the same optimization process as Kerbl et al. [KKLD23]."}, {"title": "4.5. Style Color Matching", "content": "To ensure a similar color distribution between the resulting stylized 3D\nscene and the original style image, at the beginning of our algorithm, we\nmatch the mean and covariance matrix of colors from our ground truth\nimages Igt and the style image Zs. Let C be a matrix containing pixel\ncolors of the ground truth images of the scene Igt and S be a matrix\nof pixels from Is, where each row is one pixel and columns are used\nfor RGB components. We analytically solve for a linear transformation\nA, such that E (AC) = E(S) and Cov(AC) = Cov(S), and finally modify\nour initial Gaussian Splatting scene representation so that the rendered\ncolor images match with the color-corrected ground truth images. Even\nthough this color transfer step assumes unimodal distributions of colors,\nin our experiments, we empirically identified that it is sufficient for\nall tested style images. Since optimizing with a loss that considers the\nactivations of hidden layers does not guarantee that the resulting colors\nare accurate to the style, we apply the same correction at the end of\nour algorithm to Ir."}, {"title": "5. Results", "content": "In this section, we provide an analysis of the results pro\nduced by our method. Additionally, we offer a comparison\nto other leading state-of-the-art approaches. All of our results\nare generated using a modified 3D Gaussian Splatting code\nbase [Ker23], which is publicly available in our GitHub Repository\n(https://github.com/AronKovacs/g-style)."}, {"title": "5.1. Datasets", "content": "To evaluate our approach, we prepared a series of forward-facing\nscenes: Flower, Horns, Orchid, T-Rex, and Fern (employed in the work\nof Mildenhall et al. [MSOC* 19]) and 360\u00b0 scenes: Playroom [HPP*18],\nand Truck and Train [KPZK17] together with a variety of styles\nranging from classical paintings to abstract images: The Starry Night\nby Vincent van Gogh, The Scream by Edvard Munch, The Great Wave\noff Kanagawa by Hokusai, On White II by Wassily Kandinsky, The Kiss\nby Gustav Klimt, and a colored image of the Mandelbrot Set created\nby Wolfgang Beyer (CC BY-SA). The scenes contain vastly different\ncomplexities in the represented stimuli\u2014including highly detailed areas,\nsuch as the bookcases in the Playroom, but also flat white walls. Our\ngenerated results for the forward-facing scenes can be seen in Figure 5\nand for the 360\u00b0 scenes in Figure 6."}, {"title": "5.2. Comparison to the State of the Art", "content": "We compare our approach to three state-of-the-art approaches: a recent\nNeRF-based approach, Artistic Radiance Fields (ARF) [ZKB*22a],\na recent zero-shot style transfer method for NeRF scenes,\nStyleRF [LZC*23a], and a recent pre-print that performs style\ntransfer for Gaussian Splatting scenes, StyleGaussian [LZX*24].\nFor this comparison, we used their official implementations and\npre-trained checkpoints [ZKB*22b, LZC*23b, LZX*24]. We do\nnot compare against Gaussian Splatting in Style [SGC*24] and\nStylizedGS [ZCY*24] as their implementations are not publicly\navailable at the time of writing this paper.\nARF uses the NNFM loss to transfer artistic style and also utilizes\nexplicit color matching to achieve more accurate colors. This approach\ncan use different volumetric scene representations as its backbone.\nHowever, the only publicly available implementation [LZC*23b] uses\nTensoRF [CXG*22], which in essence is a 3D voxel-based representa\ntion, decomposed into several lower-rank tensors. Other representations\nshowcased in the original paper are neural radiance fields and Plenox\nels [FKYT*22], but these could not be tested as they were not publicly\navailable. StyleRF also uses TensoRF as its backbone. This approach is\nbased on embedding high-dimensional features into the structure of the"}, {"title": "Qualitative Comparison", "content": "A comparison for the forward-facing scenes\ncan be seen in Figure 7 and for the 360\u00b0 scenes in Figure 8. In these\nfigures, we present the comparison of our method to the available check\npoints of other approaches for different scenes. Our approach exhibits\nbetter or, at least, comparable results to the other methods. Both StyleRF\nand StyleGaussian do not faithfully capture the visual style, because\nthey are not able to synthesize small patterns (as is the case for the\nMandelbrot style in Figure 7, second and seventh row) nor brushstrokes\n(for The Starry Night and The Scream styles) in Figures 7 and 8. These\ntwo approaches also fail to recreate any bigger patterns, such as in the\ntwo Truck scenes of Figure 8, especially in the stylization with On White\nII. Furthermore, StyleGaussian maintains the original Gaussians as re\nsulting from the reconstruction phase. It is, thus, not able to create any\nmeaningful patterns in undersampled areas, which can be easily seen on\nthe walls in the examples of Figure 7, and on the ground or in the sky in\nall the examples depicted in Figure 8. Moreover, as shown in the Train\nscene in Figure 8, StyleGaussian produces large colorful Gaussians in\nthe sky which do not match any of the used style images. Thus, for the\nscenes and styles we chose, StyleRF and StyleGaussian cannot reliably\ngenerate style-specific details and patterns; but rather focus on recoloring\nthe scenes. Yet, the visual style of an image goes beyond just the colors.\nSimilarly to ARF, we produce high-frequency details, but by utilizing\nthe CLIP loss, we can also create bigger patterns. This is something\nthat ARF seems to struggle with. In the case of The Starry Night (in\nboth Figure 7 and 8), our stylized scenes contain brush-like patterns,\nbut also moon-like and star-like shapes not present in the results of\nARF. When using The Scream our method occasionally also creates\nhead-like shapes (see the femur of the T-Rex in Figure 7 and the surface\nof the Train in Figure 8). If this is not desired, it could be removed\nby modifying the style image and/or cropping it. Furthermore, On\nWhite II consists of simple patterns using only a single color. While our\nmethod can capture those patterns and create colorful shapes, ARF only\nproduces desaturated areas, which are not truthful to the painting. This\nis evident in the sixth row of Figure 7 and the Truck in Figure 8. Finally,\nwhen using The Kiss, the flower pattern in Figure 7 is less blurry and\nmore recognizable with our method, as opposed to ARF. We conclude\nthat, while ARF can produce highly stylized images that match certain\nstyle images, it focuses only on fine details and ignores bigger patterns\nwhich may be important to capture a given style successfully."}, {"title": "Speed Comparison", "content": "We measured the optimization times of all the\ncompared methods, and we performed all of our measurements on an\nNVIDIA L4 in Google Colab. The chosen approaches are fundamentally\ndifferent: our approach and ARF [ZKB*22b] optimize directly the col\nors of a scene, while StyleRF [LZC*23a] and StyleGaussian [LZX*24]\nembed VGG features in the scene and later use them during rendering\nto stylize them. For the tested forward-facing scenes, our pre-processing\nstep takes approximately 5 minutes, and stylization 3\u20138 minutes depend\ning on the complexity of the scene, the number of ground truth images,\nand the size of the style image. For the same scenes, ARF needs 2\u20137 min\nutes. For the tested 360\u00b0 scenes, the preprocessing step of our method\ntakes 8 minutes, and stylization 20-28 minutes. For the same scenes,\nARF requires 23-33 minutes. Note that depending on the particular\nscene and the quality of its reconstruction, we may not need to perform\nthe pre-processing step, as its purpose is to ensure an approximately"}, {"title": "5.3. User Study", "content": "To evaluate our method, we conducted an informal, online user study\nwith 24 participants, where we used several of the cases shown in\nFigures 5-8. We presented each participant with the results of our\napproach, ARF, StyleRF, and StyleGaussian together with the respective\nstyle exemplars and original scenes. Without disclosing any information\nabout any of the approaches, we interviewed the participants to gain\nfeedback about the outputs. Specifically, we asked them to rank the\napproaches w.r.t. their similarity to the provided style exemplar. For\neach of the generated results, we also asked the study participants\nto rate their visual appeal and ability to recognize the original scene\ncontent on a 1-5 Likert scale.\nThe analyzed outcomes of the user study are shown in Figure 9.\nThe study participants ranked our approach as most similar to the\nstyle exemplar (47.9% of the participants for the forward-facing\nscenes vs. 67.4% for the 360\u00b0 scenes), followed by ARF (44.4%\nfor the forward-facing scenes vs. 32% for the 360\u00b0 scenes). StyleRF\n(7.6% for the forward-facing scenes vs. 0% for the 360\u00b0 scenes) and\nStyleGaussian (0.7% for the 360\u00b0 scenes) ranked last. The differences\nbetween our approach and ARF are statistically significant only for the\n360\u00b0 scenes, as shown with an ANOVA test followed by pairwise t-tests.\nThe most visually appealing approaches are deemed to be ours and ARF,\nas indicated visually in the plots of Figure 9. However, the distributions\nof the ratings of our approach and ARF do not statistically significantly\ndiffer. In terms of content recognition, our approach and ARF are\ncomparable to each other. Both are better than StyleRF for forward\nfacing scenes, but this changes for the 360\u00b0 scenes, where the ratings of\nStyleRF (followed by StyleGaussian) are higher. This is to be expected\nas the effect of the latter two on the stylization of the scenes is less\ndrastic than the former. To sum up, according to our study participants,\nour approach is comparable to ARF in all investigated aspects-yet,\nfor 360\u00b0 scenes, ours yields results more faithful to the style."}, {"title": "5.4. Ablations", "content": "Our loss does not enforce color transfer and relies on pre-trained net\nworks to synthesize new features. Often, these new features are patterns\nthat do not necessarily have the same color as in the style image. When\nwe do not perform any explicit color matching-both during and after\ntraining-synthesized images are discolored (see Figure 10). Conversely,\nwith color matching, the colors are truthful to the style image, as also\nshown in the work of Zhang et al. [ZKB*22a]. Introducing another style\nloss, LCLIP, still does not ensure that the optimization process matches\nthe color statistic in synthesized images. Without splitting, the number\nand shape of Gaussians are the same as after the initial optimization with\nthe ground truth images. This results in blurred areas, where not many\nGaussians were originally needed, such as the walls marked in Figure 10.\nBy splitting Gaussians, we are able to introduce details even in those\nareas. The content loss conditions the stylization. This loss helps to keep\ncertain features intact or recognizable, e.g., the letter on the floor in the"}, {"title": "6. Limitations", "content": "By utilizing Gaussian Splatting as the underlying data representation, we\nalso inherit certain visual artifacts that are either caused by their construc\ntion process or are fundamental to this representation. Namely, when\nreconstructing a real-world scene, some \"stray\" Gaussians may appear as\nfloaters, potentially inhibiting the stylization process. This could be reme\ndied by using a modified version of the original Gaussian Splatting ap\nproach [KKLD23], which is currently an active area of research [CW24,\nFXZ*24, WYZ*24]. Furthermore, splatting can lead to compositing ar\ntifacts. Changing the viewpoint slightly may cause a Gaussian to pop in\nfront of another one, given that during rendering the Gaussians are sorted\nbased on the distance from their mean to the camera. Moreover, the styl\nization process is unguided, which means that an artist lacks full control\nover the placement of style features. Also, there is no straightforward\nway to ensure that patterns such as brush strokes point in the desired\ndirection. To the best of our knowledge, there are no reliable ways of"}, {"title": "7. Conclusions", "content": "We introduced a novel algorithm for stylizing a 3D scene represented by\na set of Gaussians to match the style of a given image, I-Style. By op\ntimizing the geometry of the scene based on the needs of the stylization\nprocess and by using a dual loss that captures high and low-frequency\nstyle patterns, we generate stylized scenes with higher quality than ex\nisting methods in a matter of minutes per style image. In the future, we\nwould like to address the aforementioned limitations, inherited from\nthe Gaussian Splatting representation and the employed neural network\narchitectures, aiming to further improve the quality of the stylized scenes.\nFinally, we intend to perform additional editing experiments, such as\nblending multiple styles into one content image or conducting localized\nor semantic style transfer on distinct regions of the content image, to un\nderstand further the strengths and limitations of our proposed approach."}]}