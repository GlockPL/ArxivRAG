{"title": "I-Style: Stylized Gaussian Splatting", "authors": ["\u00c1ron Samuel Kov\u00e1cs", "Pedro Hermosilla", "Renata G. Raidou"], "abstract": "We introduce G-Style, a novel algorithm designed to transfer the style of an image onto a 3D scene represented using Gaussian Splatting. Gaussian Splatting is a powerful 3D representation for novel view synthesis, as-compared to other approaches based on Neural Radiance Fields-it provides fast scene renderings and user control over the scene. Recent pre-prints have demonstrated that the style of Gaussian Splatting scenes can be modified using an image exemplar. However, since the scene geometry remains fixed during the stylization process, current solutions fall short of producing satisfactory results. Our algorithm aims to address these limitations by following a three-step process: In a pre-processing step, we remove undesirable Gaussians with large projection areas or highly elongated shapes. Subsequently, we combine several losses carefully designed to preserve different scales of the style in the image, while maintaining as much as possible the integrity of the original scene content. During the stylization process and following the original design of Gaussian Splatting, we split Gaussians where additional detail is necessary within our scene by tracking the gradient of the stylized color. Our experiments demonstrate that G-Style generates high-quality stylizations within just a few minutes, outperforming existing methods both qualitatively and quantitatively.", "sections": [{"title": "1. Introduction", "content": "While humans excel at creating paintings with specific contents and styles, this task has proven challenging for computers to replicate. With the advent of neural networks and in particular, Convolutional Neural Networks-algorithms have been developed to transfer the style of one image onto another [GEB15a]. In this process, a content image refers to the original image whose subject matter we aim to retain, while a style image is the one whose artistic style we want to apply to the content image. These algorithms enabled the modification of the style of an image while preserving its content by matching the statistical properties of the embeddings of both content and style images, as obtained from a deep neural network.\nWith the appearance of novel view synthesis methods based on neural networks (NeRFs) [MST*20], researchers turned their attention to applying style transfer techniques to entire 3D scenes. Style transfer for 3D scenes aims to generate novel views of a scene from a finite number of images of the same scene with a particular style specified by a style image examplar. To succeed in this task, the style transfer"}, {"title": "2. Related Work", "content": "2D Style Transfer. Gatys et al. [GEB15a] first introduced a method\nfor neural style transfer, where the artistic style of a provided image is\nused to visually reconstruct the content of another image. The method\nis based on iterative optimization to match the output and second-order\nstatistics, expressed as the Gram matrices of hidden layers of a pre-\ntrained network. If preserving the content is not necessary, the method\ncan synthesize a texture resembling the provided style image, initializing\nthe process with a noise image [GEB15b] and disregarding the part\nof the original loss function that is responsible for content preservation.\nThis approach has been since refined to enable transferring features\non multiple scales [ZGW*22], by utilizing Generative Adversarial\nNetworks (GANs) in a course-to-fine fashion [JBV17] or diffusion\nmodels [ZHT*23, WZX23,CHH24]. Also, different ways of capturing\nand matching the style statistics have been proposed [LYY*17,GCLY18,\nKSS19]. All aforementioned methods aim to improve style details by\nintroducing pen or brush strokes, which were too blurry in the original\nwork of Gatys et al., or by better preserving semantic consistency.\nInstead of relying on matching extracted style statistics, searching\nfor nearest neighbors in the feature space and minimizing distances\nbetween them is another option for transferring style [KSS19, CS16,\nLYY*17, LW16, ZKB*22a]. This strategy can lead to significant\nimprovements in transferring high-frequency features, by avoiding the\naveraging of features that match possibly multi-modal style statistics.\nExisting methods can be further subdivided based on whether they\niteratively modify an image, just like in the original work of Gatys et al.,\nor explicitly minimize an objective function with a single feed-forward\npass [CS16, HB17, AHS*21]. While the feed-forward-based approaches\nare orders of magnitude faster, their results are generally of lower\nquality than the slower optimization-based techniques.\n3D Style Transfer. 3D style transfer refers to modifying the\nappearance of a 3D object or a scene so that when viewed from\ndifferent angles, it matches the style of a given exemplar. To reuse\na 2D style transfer method, one needs to extract and employ a 2D\nimage of a given scene. This can be either obtained by utilizing\na differentiable renderer [MPSO18, ZKB*22a], by slicing the 3D\nvolume [HMR20, GRGH19, ZGW*22, CW10, KFCO*07], or by\ndirectly working on the surface manifold of textured meshes [KHR24].\nExisting approaches usually rely on a single way of representing\nobjects and whole scenes, which is also the main point of influence for\ntheir performance. The approach of Cao et al. [CWNN20] uses point\nclouds, yielding holes due to measurement errors during scanning and\nbeing unsuitable for representing surfaces and real-world scenes. Con-\nversely, Mordvintsev et al. [MPSO18] and Hoellein et al. [HJN22] use\ntextured meshes, which can be reconstructed from the aforementioned\npoint clouds. Due to the discrete and restrictive nature of mesh repre-\nsentations, these approaches can suffer from different types of artifacts.\nLately, Neural Radiance Fields (NeRFs) [MST*20] and Gaussian\nSplatting [KKLD23] have become very prominent in reconstructing\nobjects and scenes. Unlike point clouds and meshes, NeRFs and\nGaussians are soft volumetric representations. Naturally, these\nrepresentations can be utilized for scene reconstruction or style transfer.\nLike in the 2D case, these style transfer methods can be subdivided\ninto two categories: zero-shot and iterative methods. Zero-shot meth-\nods [LZL*23, LZC*23a, XCX*24] focus on matching colors, relighting,\nor transferring details on a small scale to achieve multi-view consistency.\nThis limitation arises from their inability to quickly and consistently\nembed large features into scenes. Thus, these methods struggle to\nsynthesize large patterns that span significant portions of a given scene.\nOn the other hand, iterative methods leverage rendering scenes from\nmultiple viewpoints. Hence, they can construct large-scale patterns that\nremain consistent across different views, while any inconsistencies are\ncorrected during the training process. Zhang et al. [ZKB*22a] propose a\nnew loss based on nearest neighbor feature matching (NNFM) which bet-\nter preserves details and also optimizes backpropagation by deferring the\nchanges to the network after first computing the losses for full-resolution"}, {"title": "3. Background: NeRFs and Gaussian Splatting", "content": "In this section, we briefly describe the two most commonly used\n3D representations for novel view synthesis, NeRFs [MST*20] and\nGaussian Splatting [KKLD23].\nNeRFs. Neural radiance fields have revolutionized the field of novel\nview synthesis by introducing a new scene representation, and an\noptimization algorithm to train this representation only from images.\nThe outgoing radiance at any point x in the scene for any view direction\nv is modeled by a parametric model \u03a6(x,v) with parameters \u03b8. This\nmodel is usually a Multi-layer Perceptron (MLP), which is chosen due\nto the universality provided by this type of model. To train this model,\nthe scene is rendered from multiple views using the volume rendering\nalgorithm [Max95]. By comparing the generated image I\u2207 to a real\npicture of the scene Igt, gradients can be computed for the parameters\n\u03b8 through the rendering operation and the scene representation can\nbe updated to match the ground truth images. Despite the high-quality\nimages generated by NeRFs, the control provided to the user is\nlimited, and they demand extensive rendering times due to the multiple\nevaluations required to compute the value of a pixel.\nGaussian Splatting. Gaussian Splatting [KKLD23] has emerged as\na viable alternative to address the main limitations of NeRFs: limited\nscene control and low rendering speed. To reconstruct a real-life scene\nfrom ground truth images Igt, Kerbl et al. use the Structure from Motion\nalgorithm [Ull79] to obtain camera poses for each image and a spare set\nof initial 3D points. The 3D points are then transformed into Gaussian\nfunctions, each representing a point in the scene. In this way, Gaussian\nSplatting represents the function \u03a6(x,v) as a set of 3D Gaussians.\nEach Gaussian is defined by its mean \u03bc, covariance matrix \u03a3, opacity\n\u03b4, and color c. By representing the color with spherical harmonics,\nview-dependent effects can also be captured. Since the covariance\nmatrix \u03a3 has to be positive semi-definite, which is difficult to enforce\nduring optimization, the covariance matrix is obtained by RSSTRT,\nwhere S is a scaling matrix and R a rotation matrix obtained from a\nquaternion. The covariance matrices of the Gaussians are initialized\naccounting for their neighbors to conservatively cover the surfaces and\nprevent holes in the reconstruction."}, {"title": "4. Methodology of I-Style: Gaussian Splatting with Style", "content": "In this section, we describe our proposed algorithm: I-Style. We first\nprovide an overview of our approach (also illustrated in Figure 2),\nfollowed by a detailed explanation of its substeps.\n4.1. Overview\nOur algorithm takes as input a scene represented with a set of Gaussians\n\ud835\udca2 and a set of ground truth images Igt. Subsequently, it modifies\n\ud835\udca2 based on the style provided from a style exemplar Is. First, we\npre-process \ud835\udca2 to remove long narrow Gaussians and Gaussians covering\nlarge areas, making the initial set of Gaussians uniform. Once the\ninitial representation has been pre-processed, we create an additional\ncolor cs associated with each Gaussian which is initialized with the\noriginal color Cgt. These new colors cs are modified during a stylization\nprocess that uses a composition of several losses to preserve different\nproperties of the style of Is. Since the geometry provided by the initial\npre-processing step can be limited to represent detailed style features,\n\ud835\udca2 undergoes a geometric fine-tuning step. In this step, the Gaussians\nare split based on the gradient of cs and fine-tuned to match the original\nscene images Igt by modifying \u03bc, \u03a3, \u03b4, and Cgt. The stylization and\ngeometric fine-tuning steps are repeated until convergence.\n4.2. Pre-processing Step\nAlthough Gaussian Splatting offers high-quality scene representation,\nthe original approach has limitations. It often generates large flat Gaus-\nsians to depict uniform, flat surfaces (e.g., walls), and narrow elongated\nGaussians to capture high-frequency details. The latter might also come\nas a byproduct of the optimization process. When incorporating an\nadditional style into the scene, large flat areas might need additional\nGaussians to incorporate extra details, while already highly detailed\nareas may not need more Gaussians to stylize them properly. Therefore,\nin the initial step of our approach, the scene undergoes a Gaussian nor-\nmalization process where the resulting representation \ud835\udca2 is composed of\nGaussians of similar size and shape.\nFlat Gaussian Split. To detect under-sampled areas, we compute\nthe approximated maximum projected area of each Gaussian, Ai.\nWe compute Ai by multiplying the two highest components of the\nGaussian's scaling matrix Si. Then, we mark for splitting all Gaussian\nabove a threshold tf = \u03bc\u03b1 + \u03b3\u03c3\u03b1, where \u03bc\u03b1 is the mean of all A in\nthe scene, \u03c3 is the standard deviation of A, and \u03b3 is a user-defined\nparameter controlling the number of Gaussian mark for splitting. As\nsplitting Gaussians modifies the geometry of the scene, we are obliged\nto optimize them. For this, we employ the same optimization algorithm\nas in the original work of Kerbl et al. [KKLD23].\nNarrow Gaussian Normalization. To identify Gaussians with a\nnarrow shape, we compute their elongation factor Ei by dividing the\nhighest component of each Gaussian's scaling matrix Si by its second\nhighest component. If Ei is above a certain user-defined threshold\nte, we mark it for normalization, which sets the largest component\nto the average of the largest and second-largest components. We\nrepeatedly perform this operation during the optimization process after\nthe flat Gaussian split. This optimization process retrains the scene\nto match the appearance of the ground truth images Igt. As such, it\ncorrects the deformations caused by this narrowing step while keeping\nthe Gaussians more rounded. In Figure 3, we illustrate the effect of\nnormalizing Gaussians (right) as opposed to not normalizing them (left).\nDiffuse Color Transform. In 3D style transfer, where there is\nno concrete ground truth image associated with a view, enforcing\nmulti-view consistency is key for seamless navigation through the\nscene. In the original Gaussian Splatting algorithm, spherical harmonics\nenable modeling view-dependent effects such as reflections. However,\nthey also generate undesirable artifacts on the stylized version of the\nscene, where each view direction could result in a completely different\nstylization. To avoid this, we only use the zeroth term of the spherical\nharmonics representation, limiting the model to represent diffuse\nobjects only, therefore, enforcing view consistency.\nParameterizations. We perform five rounds of the splitting-\nnormalization-optimization process, which in our experiments offers\na good balance between the pre-computation time and the even\ndistribution of Gaussian sizes. Initially, we set \u03b3 to 1.1, but in each\nsubsequent round, we multiply it by 1.125 to deal with very large\nGaussians that may still be remaining. The initial value for \u03b3 is quite\nlow and, thus, we split at most 5% of the largest Gaussian in each round.\nAdditionally, we set the threshold for elongation te to 1.5, which allows\na degree of elongation but normalizes the shape of those Gaussians that\nheavily affect the final appearance. After pre-processing, the resulting\nset of Gaussians still matches the visual characteristics of a scene, but\nthe distribution of their perceived sizes is uniform, as can be seen in\nFigure 4. Note that incorporating more Gaussians into the scene results\nin additional details and additional memory demands. However, since"}, {"title": "4.3. Stylization Step", "content": "Once we have pre-processed the scene, we start the stylization process.\nDuring stylization, we render the scene from multiple views using\nCs and compute several losses over the images carefully designed to\npreserve the style of Is at different scales. We hereby describe in detail\nthe different losses used in our algorithm.\nLow-frequency Style. The style of an image is determined by color\nand high-frequency details and also by large-scale patterns and features.\nTo preserve these low-frequency features of the style image, we employ\na CLIP-based [RKH*21] loss. We encode our rendered images \ud835\udcaf and\nthe style image Is into a feature vector using the image encoder of\nthe CLIP model, \ud835\udc9e. Then, our loss is defined as the similarity between\nthese feature vectors where similarity is measured as the \u21132:\nLCLIP = \\frac{1}{K} \\sum_{k=1}^{K} ||\ud835\udc9e(\ud835\udcafk) - \ud835\udc9e(I_S)||_2\n(1)\nwhere K is the number of rendered images in the batch.\nHigh-frequency Style. Our CLIP-based loss can capture large-scale\npatterns in the style image. Yet, fine details, such as brush strokes in\na painting, might not be well represented with this loss. Therefore,\nfollowing Zhang et al. [ZKB*22a], we use a nearest neighbor feature\nmatching (NNFM) loss utilizing a pre-trained VGG-16 network [SZ14]\nto capture the high-frequency style patterns. The architecture of a\nfeature extractor can play a significant role in the quality of the\ngenerated results. We selected VGG-16 for its proven effectiveness in\nprevious style transfer solutions, facilitating easier comparisons with\nthose methods. The NNFM loss is a replacement for the widely used\nGram matrix-based loss of Gatys et al. [GEB15a]. Instead of matching\nstatistics of feature maps \u2131\ud835\udcaf and \u2131S, this loss searches for nearest\nneighbors in the feature space. Let \u2131\ud835\udcaf and \u2131S be the VGG-16 features\nmaps of \ud835\udcaf and Is respectively, and let \u2131(i,j) be the feature vector at\npixel location (i, j). The NNFM loss is then defined as:\nLNNFM(\u2131\ud835\udcaf, \u2131S) = min_{Nj} ||\u2131\ud835\udcaf(i,j) - \u2131S(i',j')||_2\n(2)\nwhere N is the number of pixels in \u2131\ud835\udcaf, and D is the cosine distance\nbetween two vectors. As in Zhang et al. [ZKB*22a], we use feature\nmaps from the third block of VGG-16.\nRegularization. To avoid the deterioration of features that are\nnecessary for scene understanding, like in the work of Zhang et\nal. [ZKB*22a], we utilize a content loss LC, which is the \u21132 loss\nbetween the features of rendered images \u2131\ud835\udcaf and ground truth images\n\u2131gt. This loss also uses the features from the third block of VGG-16.\nAdditionally, we incorporate a total variation term in our loss Ltv to\nprevent noise in the resulting renderings.\nComplete Style Loss. Our final loss is a weighted sum of all the losses,\ngiven by the equation:\nL = \u03bbCLIPLC + \u03bbNNFMLNNFM + \u03bbcLC + \u03bbtvLtv\n(3)\nWe perform the stylization process for 15 epochs. Note that\nforward-facing scenes do not have as strict multi-view consistency\nrequirements as 360\u00b0 scenes due to the limited viewing angles of the\nground truth images. As such, they converge more easily compared to\n360\u00b0 scenes. To account for the differences between these two types of\nscenes, we use different parameterizations. For forward-facing scenes,\nwe use an exponentially decaying learning rate from 1e-1 to 1e-2, and\nfor 360\u00b0 scenes the learning rate decays from 1e-2 to 5e-3. Also, we\nset \u03bbCLIP to 10, \u03bbNNFM to 100, \u03bbc to 0.05, and \u03bbtv to 1e-4. For 360\u00b0\nscenes, we set \u03bbNNFM to 10."}, {"title": "4.4. Geometric Fine-tuning Step", "content": "In the pre-processing step, we place more Gaussians in the undersam-\npled areas, making the Gaussians more uniform in size. This step is\nconservative enough to not overly increase the number of Gaussians\nand, thus, not oversample a given scene. With the new Gaussians, it is\npossible to synthesize features that otherwise could not be represented.\nHowever, the size of the Gaussians still limits the synthesis of very fine\nfeatures in our stylized scene.\nTo overcome this, during the stylization process, we periodically split\nGaussians based on their cs gradient. Similarly to the work of Kerbl et\nal. [KKLD23], we keep an accumulation buffer B to store the norm of\nthe gradient cs. After each iteration and for each Gaussian, we add the\nnorm of the cs gradient to B and periodically split a user-defined percent-\nage of Gaussians with the highest value. Since splitting Gaussians mod-\nifies the geometry of the scene, after each splitting, we optimize their \u03bc,\n\u03a3, \u03b4, and Cgt in the same optimization process as Kerbl et al. [KKLD23]."}, {"title": "4.5. Style Color Matching", "content": "To ensure a similar color distribution between the resulting stylized 3D\nscene and the original style image, at the beginning of our algorithm, we\nmatch the mean and covariance matrix of colors from our ground truth\nimages Igt and the style image Is. Let C be a matrix containing pixel\ncolors of the ground truth images of the scene Igt and S be a matrix\nof pixels from Is, where each row is one pixel and columns are used\nfor RGB components. We analytically solve for a linear transformation\nA, such that \ud835\udd3c(AC) = \ud835\udd3c(S) and Cov(AC) = Cov(S), and finally modify\nour initial Gaussian Splatting scene representation so that the rendered\ncolor images match with the color-corrected ground truth images. Even\nthough this color transfer step assumes unimodal distributions of colors,\nin our experiments, we empirically identified that it is sufficient for\nall tested style images. Since optimizing with a loss that considers the\nactivations of hidden layers does not guarantee that the resulting colors\nare accurate to the style, we apply the same correction at the end of\nour algorithm to \ud835\udcaf."}, {"title": "5. Results", "content": "In this section, we provide an analysis of the results pro-\nduced by our method. Additionally, we offer a comparison\nto other leading state-of-the-art approaches. All of our results\nare generated using a modified 3D Gaussian Splatting code-\nbase [Ker23], which is publicly available in our GitHub Repository\n(https://github.com/AronKovacs/g-style)."}, {"title": "5.1. Datasets", "content": "To evaluate our approach, we prepared a series of forward-facing\nscenes: Flower, Horns, Orchid, T-Rex, and Fern (employed in the work\nof Mildenhall et al. [MSOC* 19]) and 360\u00b0 scenes: Playroom [HPP*18],\nand Truck and Train [KPZK17] together with a variety of styles\nranging from classical paintings to abstract images: The Starry Night\nby Vincent van Gogh, The Scream by Edvard Munch, The Great Wave\noff Kanagawa by Hokusai, On White II by Wassily Kandinsky, The Kiss\nby Gustav Klimt, and a colored image of the Mandelbrot Set created\nby Wolfgang Beyer (CC BY-SA). The scenes contain vastly different\ncomplexities in the represented stimuli\u2014including highly detailed areas,\nsuch as the bookcases in the Playroom, but also flat white walls. Our\ngenerated results for the forward-facing scenes can be seen in Figure 5\nand for the 360\u00b0 scenes in Figure 6."}, {"title": "5.2. Comparison to the State of the Art", "content": "We compare our approach to three state-of-the-art approaches: a recent\nNeRF-based approach, Artistic Radiance Fields (ARF) [ZKB*22a],\na recent zero-shot style transfer method for NeRF scenes,\nStyleRF [LZC*23a], and a recent pre-print that performs style\ntransfer for Gaussian Splatting scenes, StyleGaussian [LZX*24].\nFor this comparison, we used their official implementations and\npre-trained checkpoints [ZKB*22b, LZC*23b, LZX*24]. We do\nnot compare against Gaussian Splatting in Style [SGC*24] and\nStylizedGS [ZCY*24] as their implementations are not publicly\navailable at the time of writing this paper.\nARF uses the NNFM loss to transfer artistic style and also utilizes\nexplicit color matching to achieve more accurate colors. This approach\ncan use different volumetric scene representations as its backbone.\nHowever, the only publicly available implementation [LZC*23b] uses\nTensoRF [CXG*22], which in essence is a 3D voxel-based representa-\ntion, decomposed into several lower-rank tensors. Other representations\nshowcased in the original paper are neural radiance fields and Plenox-\nels [FKYT*22], but these could not be tested as they were not publicly\navailable. StyleRF also uses TensoRF as its backbone. This approach is\nbased on embedding high-dimensional features into the structure of the"}, {"title": "5.3. User Study", "content": "To evaluate our method, we conducted an informal, online user study\nwith 24 participants, where we used several of the cases shown in\nFigures 5-8. We presented each participant with the results of our\napproach, ARF, StyleRF, and StyleGaussian together with the respective\nstyle exemplars and original scenes. Without disclosing any information\nabout any of the approaches, we interviewed the participants to gain\nfeedback about the outputs. Specifically, we asked them to rank the\napproaches w.r.t. their similarity to the provided style exemplar. For\neach of the generated results, we also asked the study participants\nto rate their visual appeal and ability to recognize the original scene\ncontent on a 1-5 Likert scale.\nThe analyzed outcomes of the user study are shown in Figure 9.\nThe study participants ranked our approach as most similar to the\nstyle exemplar (47.9% of the participants for the forward-facing\nscenes vs. 67.4% for the 360\u00b0 scenes), followed by ARF (44.4%\nfor the forward-facing scenes vs. 32% for the 360\u00b0 scenes). StyleRF\n(7.6% for the forward-facing scenes vs. 0% for the 360\u00b0 scenes) and\nStyleGaussian (0.7% for the 360\u00b0 scenes) ranked last. The differences\nbetween our approach and ARF are statistically significant only for the\n360\u00b0 scenes, as shown with an ANOVA test followed by pairwise t-tests.\nThe most visually appealing approaches are deemed to be ours and ARF,\nas indicated visually in the plots of Figure 9. However, the distributions\nof the ratings of our approach and ARF do not statistically significantly\ndiffer. In terms of content recognition, our approach and ARF are\ncomparable to each other. Both are better than StyleRF for forward-\nfacing scenes, but this changes for the 360\u00b0 scenes, where the ratings of\nStyleRF (followed by StyleGaussian) are higher. This is to be expected\nas the effect of the latter two on the stylization of the scenes is less\ndrastic than the former. To sum up, according to our study participants,\nour approach is comparable to ARF in all investigated aspects-yet,\nfor 360\u00b0 scenes, ours yields results more faithful to the style."}, {"title": "5.4. Ablations", "content": "Our loss does not enforce color transfer and relies on pre-trained net-\nworks to synthesize new features. Often, these new features are patterns\nthat do not necessarily have the same color as in the style image. When\nwe do not perform any explicit color matching-both during and after\ntraining-synthesized images are discolored (see Figure 10). Conversely,\nwith color matching, the colors are truthful to the style image, as also\nshown in the work of Zhang et al. [ZKB*22a]. Introducing another style\nloss, LCLIP, still does not ensure that the optimization process matches\nthe color statistic in synthesized images. Without splitting, the number\nand shape of Gaussians are the same as after the initial optimization with\nthe ground truth images. This results in blurred areas, where not many\nGaussians were originally needed, such as the walls marked in Figure 10.\nBy splitting Gaussians, we are able to introduce details even in those\nareas. The content loss conditions the stylization. This loss helps to keep\ncertain features intact or recognizable, e.g., the letter on the floor in the\nPlayroom scene (see Figure 10). Furthermore, it helps to suppress noise,\nwhich is not present in the ground truth images. Moreover, the CLIP loss\nenables our method to focus on bigger features. Without it, only very fine\nfeatures are transferred. For example, without the CLIP loss, the \"melt-\ning\" patterns visible in The Scream are not synthesized as depicted in\nFigure 10. On the other hand, the NNFM loss focuses on high-frequency\npatterns. For instance, the brush strokes are not as visible when switching\noff the NNFM loss, as depicted in Figure 10. The pre-trained networks\nused for the style losses were not originally trained for this task but they\nwere trained for classification. As such, using them for this purpose\nmay cause them to produce noise-like artifacts (see Figure 10, no total\nvariation loss). By using the total variation loss, we can remove these\nartifacts. If an overly smooth appearance is desired, this can be achieved\nby using an even higher weight for this loss, which needs to be tuned\nfor this specific purpose (see Figure 10, high total variation loss)."}, {"title": "6. Limitations", "content": "By utilizing Gaussian Splatting as the underlying data representation, we\nalso inherit certain visual artifacts that are either caused by their construc-\ntion process or are fundamental to this representation. Namely, when\nreconstructing a real-world scene, some \"stray\" Gaussians may appear as\nfloaters, potentially inhibiting the stylization process. This could be reme-\ndied by using a modified version of the original Gaussian Splatting ap-\nproach [KKLD23], which is currently an active area of research [CW24,\nFXZ*24, WYZ*24]. Furthermore, splatting can lead to compositing ar-\ntifacts. Changing the viewpoint slightly may cause a Gaussian to pop in\nfront of another one, given that during rendering the Gaussians are sorted\nbased on the distance from their mean to the camera. Moreover, the styl-\nization process is unguided, which means that an artist lacks full control\nover the placement of style features. Also, there is no straightforward\nway to ensure that patterns such as brush strokes point in the desired\ndirection. To the best of our knowledge, there are no reliable ways of"}, {"title": "7. Conclusions", "content": "We introduced a novel algorithm for stylizing a 3D scene represented by\na set of Gaussians to match the style of a given image, I-Style. By op-\ntimizing the geometry of the scene based on the needs of the stylization\nprocess and by using a dual loss that captures high and low-frequency\nstyle patterns, we generate stylized scenes with higher quality than ex-\nisting methods in a matter of minutes per style image. In the future, we\nwould like to address the aforementioned limitations, inherited from\nthe Gaussian Splatting representation and the employed neural network\narchitectures, aiming to further improve the quality of the stylized scenes.\nFinally, we intend to perform additional editing experiments, such as\nblending multiple styles into one content image or conducting localized\nor semantic style transfer on distinct regions of the content image, to un-\nderstand further the strengths and limitations of our proposed approach."}]}