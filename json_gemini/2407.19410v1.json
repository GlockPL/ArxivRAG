{"title": "AdaCoder: Adaptive Prompt Compression for Programmatic Visual Question Answering", "authors": ["Mahiro Ukai", "Shuhei Kurita", "Atsushi Hashimoto", "Yoshitaka Ushiku", "Nakamasa Inoue"], "abstract": "Visual question answering aims to provide responses to natural language questions given visual input. Recently, visual programmatic models (VPMs), which generate executable programs to answer questions through large language models (LLMs), have attracted research interest. However, they often require long input prompts to provide the LLM with sufficient API usage details to generate relevant code. To address this limitation, we propose AdaCoder, an adaptive prompt compression framework for VPMs. AdaCoder operates in two phases: a compression phase and an inference phase. In the compression phase, given a preprompt that describes all API definitions in the Python language with example snippets of code, a set of compressed preprompts is generated, each depending on a specific question type. In the inference phase, given an input question, AdaCoder predicts the question type and chooses the appropriate corresponding compressed preprompt to generate code to answer the question. Notably, AdaCoder employs a single frozen LLM and pre-defined prompts, negating the necessity of additional training and maintaining adaptability across different powerful black-box LLMs such as GPT and Claude. In experiments, we apply AdaCoder to ViperGPT and demonstrate that it reduces token length by 71.1%, while maintaining or even improving the performance of visual question answering.", "sections": [{"title": "1 INTRODUCTION", "content": "Visual question answering (VQA), which aims to automatically provide answers to questions related to visual content, is a challenging research topic in the fields of multimedia analysis, computer vision, and natural language processing [3, 7, 14, 18, 35, 42]. Thanks to the advantages of deep learning techniques, significant progress has been made in VQA over the past decade with end-to-end learning models such as GLIP [26]. However, these models do not explicitly distinguish between visual processing and reasoning, which limits their generalizability and interpretability.\nTo overcome this limitation, several pioneering studies have introduced visual programmatic models (VPMs), models that generate executable programs specifically designed to answer questions, providing a more manageable and transparent inference process [11, 15, 16, 33, 34, 36]. VPMs typically consist of a large language model (LLM) for code generation and a set of APIs for image processing. Given an input question, the LLM analyzes the text to understand the intent and the required computational steps. It then generates a program that, when executed, can manipulate and analyze images by using APIs to produce the desired answer, where the APIs include both low-level modules (e.g., image cropping) and high-level modules (e.g., object detection). VPMs have proven effective and are gaining traction; however, they also face challenges in terms of computational complexity, as long prompts are required to enable the LLM to understand API usage for generating appropriate programs.\nTo reduce computational costs, the development of efficient neural network architectures has been extensively studied. However, these approaches require retraining or additional learning, which is not feasible for application to LLMs trained with huge data, such as GPT and Claude, which we refer to as black-box LLMs. Recently, research has begun to focus on prompt compression [9, 19, 29], which involves optimizing input prompts to achieve high performance with shorter inputs. For example, LLMLingua [19, 30] compresses prompts using smaller models before using black-box LLMs.\nInspired by these studies, we introduce AdaCoder, a framework of adaptive prompt compression for VPMs. More specifically, AdaCoder operates in two phases: a compression phase and an inference phase. The compression phase generates a set of compressed preprompts, each depending on a specific question type, given a preprompt that describes all API definitions in the Python language with example snippets of code.\nThe inference phase adaptively selects a compressed prompt by classifying the question type and generates a Python program to answer the input question, as shown in Figure 1. Notably, we implement all of the modules of AdaCoder with a single frozen LLM, which allows implementation with black-box LLMs. Our contributions are summarized as follows:\n1) We propose AdaCoder, a novel prompt compression framework for VPMs. It adaptively selects a short instruction for code generation based on question type.\n2) We define and formulate all procedures of AdaCoder with a single frozen LLM. This avoids additional training and enables implementation with black-box LLMs.\n3) We demonstrate the effectiveness of AdaCoder over the state-of-the-art ViperGPT [36] model on three VQA datasets with GPT and Claude. We show that the token length of input prompts is reduced by 71.1%, while maintaining or even improving question answering performance. We also show that AdaCoder outperforms LLMLingua [19] in our evaluations."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Visual question answering", "content": "End-to-end models. In the early phase of VQA research history, a number of neural network architectures designed to process multimodal inputs were introduced. These include a combination of a convolutional neural network (CNN) for visual feature extraction and a recurrent neural network (RNN) for textual feature extraction [17, 28, 47]. Recent models often include attention modules to enhance individual feature extraction for each modality and to combine features of multiple modalities effectively [1, 32, 41, 43]. Large-scale pre-training has become a critical component in improving the performance of these models, enabling them to answer complex questions by implicitly associating words with specific regions in images [24-26]. More recently, LLMs have been incorporated into VQA frameworks with prompt tuning techniques such as self-prompt tuning [46]. However, these models do not explicitly distinguish between visual processing and reasoning, limiting their interpretability. Some recent studies have focused on techniques to improve interpretability such as causal inference [6], reasoning path [27], reasoning prompts [23] and gradient-based explainability method [39].\nVisual programmatic models. To improve interpretability and generalizability, VPMs that generate programs to answer questions based on visual input have been gaining research attention. This is a novel approach that leads to more manageable and traceable inferences because the generated programs contain logical sequences that are understandable to humans and articulate a step-by-step methodology for reaching conclusions. Examples of VPMs include ViperGPT [36], VisProg [15], and CodeVQA [31]. All of these generate Python programs utilizing image processing APIs, such as object detection, through a frozen LLM. However, generating programs to answer complex and compositional questions requires many APIs and example codes for them. As a result, the length of the input prompt becomes long. To the best of our knowledge, this work is the first to propose adaptive prompt compression for VPMs."}, {"title": "2.2 Large language models", "content": "Code generation. Extensive research and development in the field of natural language processing (NLP) has led to the creation of LLMs that excel at a variety of NLP tasks. Among these, a distinct group of LLMs is specifically designed for programming code generation, having been trained on large amounts of programs and documents related to programming. For example, Codex [8], a variant of the GPT-3 lineup, demonstrates its proficiency in multiple programming languages. CodeLlama [31], which is built on Llama2 [38] and has an expanded code dataset, shows improved performance in handling larger contexts in programming.\nMost recently, black-box LLMs such as GPT-3.5/4 [5], Claude[2] and Gemini [37] integrate extensive knowledge from a broad spectrum of domains, including programming, allowing them not only to generate code but also to understand and execute complex instructions given by humans. Since their zero-shot performance on programming tasks is remarkably high, they are expected to automate many aspects of coding in future that were previously manual and time-consuming, and are also useful for integration into VPMs for visual question answering.\nReasoning and interpretability. Interpretability is an important consideration when integrating LLMs into real-world systems, especially in contexts that require high reliability and accountability. Various prompting techniques have significantly improved the interpretability of LLMs. For example, chain-of-thought prompting [20], which provides an LLM with a series of contextual examples, enables intermediate reasoning to reach final conclusions. Tree-of-thought prompting [44] constructs a tree structure of thoughts, enriching the decision-making process by branching out various reasoning pathways. VPMs can also be viewed as an extended prompting method that improves interpretability because they show a sequence of logical steps leading to a conclusion by understandable programs. However, these methods also increase the complexity of input prompts because the instructions for LLMs need to be detailed, thus increasing computational costs.\nPrompt compression. Several strategies have been developed to compress prompts, notably by creating specialized tokens through prompt-based fine-tuning of LLMs [9, 12, 29, 40], with the goal of minimizing the number of tokens processed during inference. However, fine-tuning of LLMs often limits their generalizability and is not always applicable to black-box LLMs. Other efforts have focused on token reduction. These include token pruning during inference [13, 21, 22] and token merging [4]. However, these methods are generally proposed for small models such as BERT and ViT, and rely on fine-tuning or intermediate inference results. Most recently, Jiang et al. [19] have introduced LLMLingua, which compresses prompts with a small model and feeds the compressed prompts to an LLM."}, {"title": "3 ADACODER FRAMEWORK", "content": "This section introduces AdaCoder, a framework for adaptive prompt compression for VPMs. Figure 2 shows an overview of the AdaCoder framework, which consists of two phases: the compression phase and the inference phase. The compression phase is run only once to prepare compressed prompts, each of which is specialized for a specific question type. The inference phase classifies question type and adaptively selects a compressed preprompt to generate code for visual question answering. Below, we begin with a preliminary formulation of a VPM. We then present each phase of AdaCoder."}, {"title": "3.1 Preliminary", "content": "Notation and settings. We follow the notation used in previous work on VPMs [34, 36]. Let $x \\in X$ be an input image and $q \\in Q$ be an input question about the image, where $X$ is a set of images and $Q$ is a set of questions. VPMs aim to generate a code $z \\in Z$ that returns the answer $a \\in A$ to the question, where $Z$ is a set of executable codes and $A$ is a set of answers.\nThe process of answering questions is divided into two steps: the code generation step and the execution step. The former generates a code as\n$z = \\Pi(q)$,\nwhere $\\Pi : Q \\rightarrow Z$ is a code generation module. The latter executes the code with an input image by\n$a = \\Lambda(x, z)$,\nwhere $\\Lambda : X \\times Z \\rightarrow A$ is the execution engine. This work utilizes the Python execution engine for $\\Lambda$."}, {"title": "Large language model", "content": "To implement the code generation module, a single frozen LLM $\\pi : T \\rightarrow T$ is often used, where $T$ is a set of texts\u00b9. For example, the code generation module $\\Pi$ can be defined by\n$\\Pi(q) = \\pi(p_{pre} + q)$,\nwhere $p_{pre} \\in T$ is a preprompt that gives instructions to generate code using image and text processing APIs, $q \\in Q$ is an input question, and + indicates textual concatenation. Here, APIs include both low-level functions, such as image cropping, and high-level functions, such as object detection.\nPreprompt definition. In order to provide the LLM with detailed instructions on how to use the APIs, the preprompt $p_{pre}$ typically includes API definitions, coding instructions, and example snippets of code. We define a preprompt $p_{pre}$ by\n$p_{pre} = (p_{def}, \\Psi(C), p_{inst})$,\nwhere $p_{def}$ is a text of API definitions, $c \\in Z$ is textually concatenated example snippets of Python code, $p_{inst} \\in T$ is a coding instruction written in a natural language, and $\\Psi$ is a structural aggregation function to insert code snippets to immediately after function definitions as comments. For example, a code snippet for comparing the positions of objects is inserted immediately after the definition of the object detection function. Below, we review the preprompt of ViperGPT [36], which we use in Section 4.\n1) API definitions. The text of API definitions for $p_{def} \\in Z$ is written in Python and includes both class, method and function definitions. Specifically, it consists of the Python class ImagePatch to represent an image patch and a set of auxiliary functions.\n2) Code snippets. For each function and method, one or two code snippets are provided. Each code snippet calls the function or method at least once. For example, the following code snippet is given for the find method that detects objects in images."}, {"title": "3) Coding instruction", "content": "The coding instruction provides short instructions describing how to write code, specifying a programming language and how APIs should be used. Specifically, $p_{inst}$ is the following text:\nWrite a function using Python and the ImagePatch class (above) that could be executed to provide an answer to the query.\nConsider the following guidelines:\nUse base Python (comparison, sorting) for basic logical operations, left/right/up/down, math, etc.\nUse the 1lm_query function to access external information and answer informational questions not concerning the image.\nToken length. One of major limitations of previous VPMs is that the input token length is long, resulting in a large computational load. This work addresses this limitation by introducing an adaptive prompt compression method. More specifically, we define the input token length of the code generation module in Eq. (3) as\n$l(q; \\Pi) = |p_{pre}| + |q|$,\nwhere $l$ is the number of tokens of a text $t \\in T$. Our goal is to reduce this length."}, {"title": "3.2 Compression phase", "content": "As shown in Figure 2a, the compression phase creates a set of compressed prompts $C = \\{p_{def}\\} \\cup \\{\\hat{c_t} : t \\in Y\\}$, where $p_{def}$ is a compressed text of API definitions, $\\hat{c_t}$ is a compressed code snippet for question type $t \\in Y$, and $Y$ is a set of question types. Below, we detail the two-step process for compressing API definitions and code snippets.\nCompressing API definitions. This step compresses the API definitions by\n$p_{def} = \\pi(p_{pre} + r_{pre})$,\nwhere $p_{pre}$ is the original preprompt in Eq. (4), $\\pi$ is a frozen LLM, and $r_{pre}$ is the instruction to rewrite API definitions. \nCompressing code snippets. This step compresses the code snippets for each question type as follows:\n$\\hat{c_t} = \\pi(p_{pre} + r_{code} + r_{sp} (d_t))$,\nwhere $p_{pre}$ is the original preprompt, $r_{code}$ is the instruction to write code snippets, $r_{sp}$ is an additional instruction to write code specialized for a specific question type with a placeholder to insert the definition of question type $d_t$, and $t \\in Y$ is a question type. Here, we assumed that a pre-defined set of question types $Y$ is given. For example, with the GQA dataset [18], five question types shown in Table 1 are provided with their definitions."}, {"title": "3.3 Inference phase", "content": "The inference phase generates a code to answer the input question by utilizing a compressed preprompt adaptively selected based on the question type as shown in Figure 2b. More specifically, this phase consists of four steps: question classification, preprompt generation, code generation, and execution.\nQuestion classification. Given an input question $q \\in Q$, this step predicts the question type. We define classification prompt $p_{cls}$ and use the LLM $\\pi$ for question classification as follows:\n$t = \\pi(p_{cls} + q)$,\nwhere $t$ is the predicted question type. The classification prompt consists of a short instruction for classification and a list of definitions of question types. \nPreprompt generation. This step generates a compressed preprompt given the question type as follows:\n$p_{pre} = (p_{def}, \\Psi(p_{inst}, \\hat{c_{\\hat{t}}}))$,\nwhere $p_{def} \\in C$ is the compressed API definitions in Eq. 6, $p_{inst}$ is the coding instruction, $\\hat{c_{\\hat{t}}} \\in C$ is the snippets of code for the question type $\\hat{t}$, and $\\Psi$ is the structural aggregation function in Eq. (4). Note that the computational cost of this step is almost negligible because both compressed prompts, $p_{def}$ and $\\hat{c_{\\hat{t}}}$, have already been computed in the compression phase. We do not compress the coding instruction $p_{inst}$, because it is already short.\nCode generation. This step generates a Python code $z$ as follows:\n$z = \\pi(p_{pre} + q)$,\nwhere $p_{pre}$ is the compressed preprompt.\nExecution. Finally, the predicted answer $a$ to the question is obtained by executing the code as follows:\n$a = \\Lambda(x, z)$,\nwhere $x$ is an input image, and $\\Lambda$ is the Python execution engine."}, {"title": "3.4 Discussion", "content": "AdaCoder formulation. By substituting Eqs. (8) and (9) into Eq. (10), we can finally define the code generation module $\\Pi_{Ada}$ of AdaCoder as follows:\n$\\Pi_{Ada}(q) = \\pi (\\Psi(p_{defs}, p_{inst}, c_{\\pi(p_{cls}+q)}) + q)$,"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Experimental settings", "content": "Datasets. We use three VQA datasets for evaluation: GQA [18], VQAv2 [14], and NLVR2 [35]. The GQA dataset is designed to test a model's visual reasoning abilities, encompassing five question types: existence of objects (obj), category of objects (cat), attributes of objects (attr), relationships between subjects and/or objects (rel), and global questions (global). The VQAv2 dataset contains open-ended questions about images that require an understanding of visual content to generate answers. The NLVR2 dataset is designed to test a model's ability to understand complex natural language statements and their correspondence to a given pair of images. From each of these two dataset, we randomly choose 2,000 QAs\u00b2.\nEvaluation metrics. We use the exact match accuracy (%) for case-insensitive answers as a QA performance evaluation metric. The reduction rate (%) of the input token length is used to evaluate the compression performance."}, {"title": "4.2 Experimental results", "content": ""}, {"title": "4.2.1 Main results", "content": "QA accuracy. Table 2 shows QA accuracy in comparison to the ViperGPT baseline. We see that AdaCoder reduces the input token length by 71.1%, while improving QA accuracy on all of the three datasets. This shows the effectiveness and efficiency of the proposed prompt compression method.\nThe simple compression setup omits the instruction prompt $r_{sp}$ to compress for specific question type (i.e., question type classification is omitted). We see that the QA accuracy is significant degraded by this omission.\nWith LLMLingua, we observed that it cannot maintain the structure of code snippets in the preprompt after compression at a reduction rate of 71.1% (the same rate as ours), resulting in a QA accuracy of 0%. Therefore, the results in Table 2 are given at a lower reduction rate = 25% by adjusting the compression ratio parameter accordingly. With this setting, LLMs can generate executable code with a probability of 98%; however, the QA accuracy is degraded by 2.2 points on GQA. This shows that prompt compression for VPMs is challenging. It is also worth noting that while our method is effective, it relies on QA classification and is not inherently a robust compression approach across all problems.\nQuestion type classification. Figure 5 shows the confusion matrix of question type classification for the GQA dataset. We observe that two question types, \"attr\" and \"global\", achieve accuracies greater than 75%. The types \"rel\" and \"obj\" are often misclassified as \"attr\" and \"cat\", respectively. This is because the questions are often short, making it difficult to distinguish between them.\nTo investigate how these classification errors affect the final QA accuracy, Table 3 compares AdaCoder using 1) predicted question types, 2) ground-truth question types, 3) random question types, and 4) without using question type based compression. We observe three key findings. First, the best performance is achieved by using ground truth question types. This highlights the importance of classifying question types to improve overall accuracy. Second, the performance drop due to classification errors is less than 1.0 points. This suggests that AdaCoder effectively classified the critical question types necessary for code generation, even though the accuracy for question classification is not very high. Third, the method using random question types, which compresses prompts for each question type and randomly choose one of them in inference, is better than the method without question type based compression. This is because the instruction prompt $r_{sp}$ in Eq. (7) for specializing code snippets to each question type makes it more likely to provide code snippets that are related to each other, thereby increasing the probability of completing the program. When this instruction is omitted and compression is performed regardless of the question type, code snippets that are effective for any question type tend to be retained after compression. However, this approach results in the loss of some specific snippets that are necessary to complete the program, thereby reducing QA accuracy. These results suggest that the instruction $r_{sp}$ is important for compressing code snippets.\nCompressed prompts. Table 4 summarizes the token length and compression performance for each component of the input preprompt. We see that both API definitions and code snippets are significantly compressed. A comparison of the original and compressed API definitions is shown in Figure 6. We see that descriptions of methods unnecessary for coding, such as those for the initialization method, are omitted, and the remaining sections are condensed into shorter sentences. This is an effective compression achieved by the language understanding and summarization capabilities of black-box LLMs.\nComputational time. Since the model weights and details of the black-box LLMs are not publicly available, and API response times can be affected by server congestion, a detailed analysis of computation times is not possible. However, the total time for experiments on the GQA dataset was reduced by 55%."}, {"title": "4.2.1 Ablation study and analysis", "content": "Ablation study. Table 5 presents the results of an ablation study. We see that both compression of API definitions and code snippets contribute to each other for both reducing the input token length and improving QA accuracy. Table 6 summarizes the QA accuracy obtained by using a single compressed prompt. We see that even with one prompt of either \"attr\" or \"rel\u201d, our method achieves comparable or slightly better performance than the ViperGPT baseline (41.3%). However, using one prompt of either \"obj\" or \"global\", the QA accuracy is significantly degraded. These results demonstrate that our adaptation approach is essential for improving QA accuracy while compressing input prompts. The detailed QA accuracy by question type is analyzed in Table 7. We see that the four compressed prompt specialized for \"obj\u201d, \u201ccat", "attr": "and \"rel\" performed the best for corresponding questions. For the \"global\" questions, the prompt for \"attr\" was the best. This is because \"global\" questions are highly varied and not easily categorized. Defining fine-grained QA types would be interesting as a next step in future research.\nError analysis. Table 8 shows an error analysis, where we manually counted the occurrence of four types of errors. \"Coding error\" indicates that the generated program is not executable or returns nothing. \"Cannot answer to simple query\" indicates that the program is correct but the simple_query method returned a response such as \"I cannot answer\". \"No object detected\" indicates that no object is detected by the find method. \u201cWrong answer"}, {"title": "5 CONCLUSION", "content": "We introduced AdaCoder, a framework for adaptive prompt compression for visual programmatic models. AdaCoder efficiently generated programs for visual question answering by compressing and selecting prompts depending on the question type. A single black-box LLM is effectively employed to perform question type classification, textual compression and code generation, eliminating the need for additional training. In experiments, we demonstrated the effectiveness and efficiency of AdaCoder in comparison to ViperGPT and LLMLingua. Finally, we discuss limitations and future work.\nLimitations. As this work relies on black-box LLMs, analysis from the perspective of neural network architecture is limited. Alternative choices to LLMs for code generation may include open-source white-box models, such as CodeLlama and StarCoder. However, since AdaCoder requires high-quality text classification and summarization, these models were not suitable for prompt compression. New research directions leveraging the combination of white-box and black-box LLMs need to be further explored.\nFuture work. To advance multimodal automated programming, future research directions that focus on pushing the boundaries beyond the traditional scope of VQA would be interesting. This includes developing methods for interactive code modification to enable a more dynamic and responsive programming environment. Additionally, we plan to explore the automatic extension of APIs to facilitate their evolution in becoming more efficient and effective in addressing the complex requirements of multimodal interactions. From a long-term perspective, we believe that adaptively extracting necessary API definitions and coding examples will be a critical step in automating large-scale multimodal software development to reduce computational costs and save energy."}]}