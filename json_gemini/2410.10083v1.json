{"title": "BEYOND GRAPHS: CAN LARGE LANGUAGE MODELS COMPREHEND HYPERGRAPHS?", "authors": ["Yifan Feng", "Chengwu Yang", "Xingliang Hou", "Shaoyi Du", "Shihui Ying", "Zongze Wu", "Yue Gao"], "abstract": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise relationships, offer a more robust framework but are still underexplored in the context of LLMs. To address this gap, we introduce LLM4Hypergraph, the first comprehensive benchmark comprising 21,500 problems across eight low-order, five high-order, and two isomorphism tasks, utilizing both synthetic and real-world hypergraphs from citation networks and protein structures. We evaluate six prominent LLMs, including GPT-40, demonstrating our benchmark's effectiveness in identifying model strengths and weaknesses. Our specialized prompting framework incorporates seven hypergraph languages and introduces two novel techniques, Hyper-BAG and Hyper-COT, which enhance high-order reasoning and achieve an average 4% (up to 9%) performance improvement on structure classification tasks. This work establishes a foundational testbed for integrating hypergraph computational capabilities into LLMs, advancing their comprehension.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) (Vaswani, 2017; Devlin, 2018; Brown, 2020; Ouyang et al., 2022) have made significant strides in domains such as dialogue systems (Bubeck et al., 2023) and image understanding (Zhao et al., 2023). However, they often produce untruthful or unsupported content, known as hallucinations (Wang et al., 2023). To mitigate this, Retrieval-Augmented Generation (RAG) (Vu et al., 2023) enhances prompts with relevant, factual, and up-to-date information (Khandelwal et al., 2019), thereby grounding outputs more effectively. RAG typically retrieves structured data with complex relational dependencies (Guu et al., 2020), such as social networks or molecular structures, which are efficiently represented as graphs. Graph representations capture intricate inter-dependencies and provide a concise encapsulation of data relationships. This has spurred research to improve LLMs' understanding of graph-structured data (Guo et al., 2023), leading to benchmarks like NLGraph (Wang et al., 2024), GraphQA (Fatemi et al., 2023), and LLM4DyG (Zhang et al., 2023). These benchmarks evaluate and enhance LLMs' capabilities in handling graph-related tasks, promoting the integration of graph-based representations in large language models.\nHowever, real-world data often involve complex correlations beyond simple pairwise relationships (Zhou et al., 2006). For example, sentences within a document sharing common keywords may exhibit high-order correlations that traditional graph models fail to capture (PM et al., 2017). In multimodal scenarios (Kim et al., 2020; Feng et al., 2023), interactions across different data types further increase correlation complexity, exceeding the capabilities of conventional graphs, which are limited to pairwise correlations. In contrast, hypergraphs can model high-order correlations, capturing intricate interdependencies among multiple entities simultaneously. This capability allows"}, {"title": "LLM4HYPERGRAPH BENCHMARK", "content": "In this section, we introduce the LLM4Hypergraph Benchmark, designed to evaluate LLMs' ability to comprehend the intricate higher-order structures of hypergraphs. To ensure a comprehensive assessment, the benchmark encompasses a diverse array of tasks and hypergraph configurations, as shown in Figure 2. It includes Low-order tasks (basic hypergraph properties and pairwise relational inferences), High-order tasks (understanding complex multi-vertex interactions and higher-"}, {"title": "TASK DESIGN", "content": "In this subsection, we briefly introduce the 14 tasks included in our LLM4Hypergraph Benchmark. Detailed descriptions and methodologies for these tasks are provided in Appendix B.\nIsomorphism Tasks. Isomorphic tasks are a fundamental category within the LLM4Hypergraph Benchmark, designed to evaluate LLMs' ability to recognize structural equivalences and classify hypergraphs based on their overall architectural patterns. These tasks are crucial for applications in specialized fields such as molecular and protein structure analysis, where distinguishing between subtly different structural configurations is essential. We introduce two key isomorphic tasks:\n\u2022 Isomorphism Recognition. This task assesses the model's ability to determine whether two hypergraph representations correspond to the same underlying structure.\n\u2022 Structure Classification. This task evaluates the model's proficiency in distinguishing hypergraphs based on their macro-level architectural frameworks.\nLow-Order Tasks. In addition to isomorphic tasks, our LLM4Hypergraph Benchmark incorporates a series of low-order tasks designed to test LLMs' understanding of fundamental hypergraph properties and vertex relationships. These tasks focus on basic hypergraph attributes and simple connectivity patterns essential for more complex structural analyses.\n\u2022 Hyperedge Count. Counts the total number of hyperedges.\n\u2022 Vertex Count. Counts the total number of vertices.\n\u2022 Vertex Degree. Counts the hyperedges connected to a specific vertex.\n\u2022 Vertex Connection. Checks if two vertices are directly connected by a hyperedge.\n\u2022 Connected Vertices. Lists all vertices connected to a given vertex.\n\u2022 Disconnected Vertices. Lists all vertices not connected to a given vertex."}, {"title": "BENCHMARK STATISTICS", "content": "Our LLM4Hypergraph Benchmark comprises 14 tasks totaling 21,500 problems, as detailed in Table 1. These tasks are categorized into five types: Counting Problems (counting specific elements), Computational Problems (numerical answers), Decision Problems (yes/no responses), Descriptive Problems (listing sets of vertices or hyperedges), and Classification Problems (categorical labels). Each type includes 1,500 samples, providing a comprehensive assessment of LLMs' abilities in numerical computations, binary decisions, descriptive generation, and hypergraph classification."}, {"title": "PROMPT ENGINEERING FOR HYPERGRAPHS", "content": "Prompt design is essential for accurately evaluating LLMs in our LLM4Hypergraph Benchmark, as prompts guide the models' understanding of hypergraph structures. Here, we integrate existing strategies such as CoT (Wei et al., 2022; Kojima et al., 2022) and Few-Shot Prompting (Brown, 2020; Zhou et al., 2022a) to develop a prompt framework tailored for hypergraph-related tasks. Additionally, we introduce a hypergraph language for textual descriptions and adapt these techniques with modifications like Hyper-BAG and Hyper-COT to better accommodate hypergraphs."}, {"title": "PROMPT FRAMEWORK", "content": "Prompt design is crucial for accurately evaluating LLMs in our LLM4Hypergraph Benchmark, as it directs the models' understanding of hypergraph structures. Figure 3 illustrates our prompt framework, which consists of six components: Example, Hypergraph Description, Hyper-BAG, Question, Tail, and Hyper-COT. The Example section includes question-answer pairs and may optionally contain COT-for-Answer for detailed reasoning. Optional modules are indicated by brackets in the figure. More details of the framework are provided in Appendix C. In the following, we briefly introduce the following prompt configurations:\n\u2022 ZERO-SHOT: Includes only Hypergraph Description, Question, and Tail, without examples or additional reasoning prompts.\n\u2022 ZERO-HYPER-COT: Extends Zero-Shot by adding Hyper-COT after the Tail, introducing hypergraph-specific reasoning.\n\u2022 FEW-SHOT: Adds Example content with Hypergraph Description, Question, Tail, and Answer to provide concrete instances for guidance.\n\u2022 COT: Enhances Few-Shot by including COT-for-Answer in each example, offering detailed reasoning steps for more accurate responses.\n\u2022 COT-HYPER-BAG: Builds on the CoT setup by inserting Hyper-BAG between Hypergraph Description and Question, contextualizing the model's focus on hypergraph construction and comprehension."}, {"title": "HYPERGRAPH LANGUAGE", "content": "Unlike traditional graphs, hypergraphs allow hyperedges to connect any number of vertices, requiring more complex textual descriptions. Abstract high-order descriptions may hinder LLM comprehension, while low-order descriptions enhance understanding but omit essential information. To address this, we designed two hypergraph languages: Low-Order Structure Language and High-Order Structure Language, as shown in Figure 4. The former focuses on pairwise relationships, while the latter preserves complex multi-vertex correlations. More details can refer to Appendix D.\nLow-Order Structure Language. To facilitate LLMs' understanding of hypergraph structures through pairwise correlations, we introduce three low-order structure languages as follows:\n\u2022 Low-Order-Incidence Description (LO-Inc): Describes pairwise connections between vertices, e.g., \"Vertex v\u2081 is connected to vertices v2 and 03.\"\n\u2022 Neighbor-Pair Description (N-Pair): Lists all pairs of vertices that share a hyperedge, e.g., \u201c(V1, V2), (V1, 03).\"\n\u2022 Raw Adjacency Matrix Description (Adj-Mat): Uses a numerical adjacency matrix where binary values indicate connections between vertex pairs.\nHigh-Order Structure Language. To further enhance LLMs' comprehension of hypergraph structures through higher-order correlations, we introduce four high-order structure languages as follows:"}, {"title": "SPECIFICAL PROMPTING FOR HYPERGRAPHS", "content": "Given that hypergraphs are difficult to describe using conventional language, we introduce two specialized prompting techniques to enhance LLMs' comprehension of hypergraph structures: Hyper-BAG and Hyper-COT. More details can be found in Appendix E.\n\u2022 Build-a-Hypergraph Prompting (Hyper-BAG) facilitates mental visualization of hypergraphs by guiding LLMs to imagine their architecture, helping the model form a coherent representation of hypergraph connections (Wang et al., 2024).\n\u2022 Chain-of-Thought for Hypergraphs (Hyper-COT) adapts traditional CoT prompting by incorporating step-by-step reasoning tailored for hypergraphs. It includes specific prompts that guide the model to break down complex hypergraph tasks into manageable steps:\nv1: \"Let's think step by step. Make sure the data is calculated and recorded accurately at each step.\"\nv2: \"Let's analyze the connectivity by considering hyperedges linked to vertices and vertices linked through hyperedges.\u201d\nv3: \"Let's think hyperedges connected by vertices then vertices connected by hyperedges.\""}, {"title": "EXPERIMENTS", "content": "Based on the LLM4Hypergraph Benchmark, we aim to investigate whether language models can comprehend hypergraphs by evaluating LLMs and different prompting settings.\nExperimental Settings. We evaluate six LLMs: ERNIE-Lite-8K (Zhang et al., 2019), ERNIE-Speed-128K (Sun et al., 2021), Qwen-Long (Bai et al., 2023), LLaMA3-8B (Touvron et al., 2023), GPT-3.5-Turbo (Brown, 2020), and GPT-4 (Achiam et al., 2023), selected for their diverse architectures. In our experiments, we employ Qwen-Long as the LLM due to its robust performance and scalability. In the \u201cFew-Shot\u201d, \u201cCoT\u201d, and \u201cCoT-Hyper-BAG\u201d settings, we provide two examples by default. For Decision Problems, we maintain a balanced positive-to-negative ratio of 1:1 to ensure fairness. Other tasks assess consistency by comparing LLM outputs to ground truth."}, {"title": "RESULTS OF DIFFERENT HYPERGRAPH LANGUAGES", "content": "We evaluated different hypergraph languages under various settings using synthetic hypergraphs, with the experimental results presented in Table 2 and Table 3.\nObservation 1: Low-order structure languages enhance LLMs' understanding of vertex correlations within hypergraphs. As shown in Table 2, high-order structure languages achieve higher accuracy in simple low-order tasks by effectively describing high-order structures. However, in medium and difficult low-order tasks, low-order structure languages outperform high-order ones due to the redundancy of high-order descriptions when focusing on pairwise vertex correlations. Therefore, low-order languages are more effective for analyzing individual vertex relationships, thereby improving LLMs' comprehension of hypergraphs.\nObservation 2: High-order structure languages enhance LLMs' comprehension of vertex set correlations within hypergraphs. As shown in Table 3, high-order structure languages significantly outperform low-order structure languages in high-order and isomorphism tasks across all"}, {"title": "RESULTS OF DIFFERENT HYPER-COT PROMPTINGS", "content": "Observation 6: Hyper-COT enhances LLMs' hypergraph comprehension. Hypergraphs' high-order correlations challenge LLMs more than low-order structures. We introduce Hyper-COT, adding step-by-step prompts. Tested on structure classification tasks (Table 4), Hyper-COT consistently outperforms the \u201cNaive COT\". Note that \"Naive COT\u201d refers to \u201cLet's think step by step\". Specifically, Hyper-COT v3 boosts performance by 4% on average across seven encodings and up to 9% in N-Pair and N-Set encodings by guiding LLMs to interpret connections hierarchically."}, {"title": "RESULTS OF HYPER-BAG PROMPTING", "content": "Observation 7: Hyper-BAG enhances LLMs' performance on high-order tasks. While Build-A-Graph (BAG) prompting Wang et al. (2024) is effective for graph understanding, we introduce Hyper-BAG tailored for hypergraphs. Experiments in Table 5 show that BAG does not significantly improve simple low-order tasks. However, both BAG and Hyper-BAG enhance performance by over 2% in high-order tasks, with Hyper-BAG achieving a 2.8% improvement in the Vertex Set Connection task compared to no BAG usage. These findings demonstrate that Hyper-BAG effectively aids LLMs in understanding and performing high-order hypergraph tasks."}, {"title": "RESULTS OF HYPER-COT WITH DIFFERENT NUMBER OF SHOTS", "content": "Observation 8: Providing example solutions enhances LLMs' understanding of hypergraphs. In structure classification tasks, supplying example solutions improves LLMs' comprehension of hypergraphs. Experiments in Table 6 show that increasing the number of examples does not enhance performance in low-order tasks. However, in high-order tasks, more examples lead to significant performance gains of approximately 6% to 9% compared to no examples. Specifically, example prompts help LLMs better understand and distinguish complex hypergraph structures, thereby enabling more accurate classifications."}, {"title": "RESULTS OF DIFFERENT HYPERGRAPH SIZE ON REAL-WORLD HYPERGRAPHS", "content": "Observation 9: Larger hypergraphs pose greater challenges for LLMs' comprehension. We investigated the impact of hypergraph size on LLMs' understanding, with results shown in Table 7. Using hypergraphs sampled from citation networks and protein structures (see Appendix A), we found that as hypergraph size increases, LLM performance deteriorates. This decline is more pronounced in high-order tasks, with up to a 13% decrease in citation data. The reduced performance is attributed to the increased complexity of larger hypergraphs, which hinders LLMs' ability to comprehend and reason effectively."}, {"title": "RELATED WORKS", "content": "LLMs for Graphs. LLMs' ability to address graph-based problems has garnered significant attention. Benchmarks like NLGraph Wang et al. (2024), GraphInstruct Luo et al. (2024), GraphQA Fatemi et al. (2023), and LLM4DYG Zhang et al. (2023) demonstrate LLMs' potential in handling structured and dynamic graph data, emphasizing the importance of encoding strategies. Additionally, in structured commonsense reasoning, LLMs generate various graph structures from natural language inputs (Tandon et al., 2019; Madaan et al., 2022; Saha et al., 2021). However, LLMs' ability to perform reasoning within hypergraphs remains underexplored. To bridge this gap, we introduce the benchmark, designed to enhance LLMs' understanding of hypergraph tasks.\nLLMs for Few-Shot Reasoning. Extensive research has assessed LLMs' few-shot reasoning across arithmetic, logical, and commonsense domains. Arithmetic tasks use datasets like AQUA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and SVAMP (Patel et al., 2021). For mathematical reasoning, NaturalProofs (Welleck et al., 2022) evaluates LLMs' ability to generate proof steps and complete proofs. Logical reasoning includes symbolic problems such as Coin Flip and Last Letter Concatenation (Wei et al., 2022), and the Logic Grid Puzzle from BIG-BENCH (Srivastava et al., 2022). Commonsense reasoning utilizes datasets from Talmor et al. (2018) and Geva et al. (2021). Additionally, methods to enhance and evaluate LLMs' algorithmic reasoning have been developed"}, {"title": "CONCLUSION", "content": "In this paper, we present LLM4Hypergraph, the first benchmark to evaluate and enhance LLMs' understanding of hypergraph data, comprising 21,500 low-order, high-order, and isomorphism tasks using synthetic and real-world hypergraphs. By assessing six leading LLMs, the benchmark effectively identifies model strengths and weaknesses. This work paves the way for integrating hypergraph capabilities into LLMs, enabling more advanced data comprehension."}, {"title": "HYPERGRAPH GENERATION", "content": "We create a diverse series of hypergraphs that cover both synthetic and real-world instances. The LLM4Hypergraph Benchmark encompasses a wide spectrum of hypergraph types and scales, thereby providing a robust and versatile framework for assessing the proficiency of large language models in understanding and processing hypergraph-based data structures."}, {"title": "SYNTHETIC HYPERGRAPHS", "content": "We first introduce the construction of synthetic hypergraphs. The synthetic hypergraphs can be categorized into two distinct types: random hypergraphs and regular-structured hypergraphs. Random hypergraphs are generated using the 'hypergraph_Gnm()' function\u00b9 from the DHG toolkit\u00b2. Importantly, we configure these hypergraphs with a \u201clow-order first\" structure. This configuration is grounded in the principle of Occam's razor, which posits that simpler, lower-order associations are more prevalent and thus more representative of real-world scenarios where low-order relationships typically dominate over high-order ones. Consequently, random hypergraphs are extensively utilized across various tasks within the benchmark, with the exception of isomorphic tasks where structured classification is paramount. In contrast, regular-structured hypergraphs are specifically tailored for structural classification tasks. We select three classical hypergraph structures Feng et al. (2024) known for their distinct and regular patterns: the Hyper Pyramid, Hyper Checked Table, and Hyper Wheel. To introduce variability and enhance the robustness of the benchmark, we systematically modify the hyperparameters of these base structures. For instance, we vary the number of layers in the Hyper Pyramid and the number of vertices per blade in the Hyper Wheel, thereby generating a multitude of structurally diverse yet regularly patterned hypergraphs."}, {"title": "REAL-WORLD HYPERGRAPHS", "content": "As for the real-world instances, we source data from citation networks and protein structure datasets. The construction process involves randomly sampling sub-hypergraphs by selecting a central vertex and performing a random walk through selected hyperedges. During this walk, the vertices encountered are randomly retained until the sub-hypergraph reaches the predetermined vertex count. This methodology ensures that the sampled hypergraphs retain the intricate and authentic correlations inherent in real-world data."}, {"title": "MULTI-SCALE SETTINGS", "content": "To ensure scalability and applicability across a range of scenarios, both synthetic and real-world hypergraphs are categorized based on their size, defined by the number of vertices: small-scale hypergraphs (Containing between 5 to 10 vertices), medium-scale hypergraphs (Containing between 10 to 15 vertices), and large-scale hypergraphs (containing between 15 to 20 vertices). Additionally, to maintain structural generality, synthetic hypergraphs adhere to a hyperedge-to-vertex ratio ranging from 0.2 to 1.5 times the number of vertices. This ratio ensures a balanced complexity that is neither too sparse nor excessively dense, thereby facilitating meaningful evaluations of LLM capabilities. In contrast, real hypergraphs do not impose such restrictions on the number of hyperedges, allowing them to naturally reflect the inherent connectivity patterns of their source datasets."}, {"title": "DETAILS OF TASK DESIGN", "content": "In this section, we will introduce the proposed three types of tasks in detail, respectively."}, {"title": "ISOMORPHISM TASKS", "content": "In the LLM4Hypergraph Benchmark, we prioritize isomorphic tasks as a fundamental component of our evaluation framework. This prioritization is grounded in the belief that a comprehensive understanding of the overall structural architecture is essential for LLMs to be effectively utilized"}, {"title": "LOW-ORDER TASKS", "content": "In addition to isomorphic tasks, our LLM4Hypergraph Benchmark incorporates a series of low-order tasks designed to enhance LLMs' understanding of fundamental hypergraph properties and the relationships between vertices. These tasks facilitate the comprehension of basic hypergraph attributes and simple connectivity patterns, which are essential for more complex structural analyses.\n\u2022 Hyperdge Count. This task aims to evaluate the model's ability to determine the total number of hyperedges within a given hypergraph. Given a hypergraph $G = \\{V,E\\}$, the target is to return $|E|$, the cardinality of the hyperedge set $E$.\n\u2022 Vertex Count. This task assesses the model's capability to ascertain the total number of vertices in a hypergraph. Given $G = \\{V,E\\}$, the target is to return $|V|$, the cardinality of the vertex set $V$.\n\u2022 Vertex Degree. This task assesses the model's capability to count the number of hyperedges that connects a specific vertex. Given $G = \\{V,E\\}$ and vertex $v$, the target is to return $|\\{e | v \\in e\\}|$.\n\u2022 Vertex Connection. This task is designed to evaluate whether a specific pair of vertices is directly connected by at least one hyperedge. Given $G = \\{V,E\\}$ and a pair of vertices $(u, v)$, the target is to determine if there exists a hyperedge $e \\in E$ such that $u \\in e$ and $v \\in e$.\n\u2022 Connected Vertices. This task examines, given a vertex $u$, the model's ability to output the set of vertices directly connected to $u$ by at least one hyperedge. Formally, given $G = \\{V,E\\}$ and a vertex $u \\in V$, the target is to output the subset $W \\subseteq V$ such that for each $w \\in W$, there exists a hyperedge $e \\in E$ where $u \\in e$ and $w \\in e$.\n\u2022 Disconnected Vertices. This task requires, given a vertex $u$, the identification and output of the set of vertices that are not connected to $u$ by any hyperedge. Formally, given $G =$"}, {"title": "HIGH-ORDER TASKS", "content": "Building upon the foundation of low-order tasks, our LLM4Hypergraph Benchmark introduces a series of high-order tasks tailored to assess LLMs' comprehension of complex correlations between vertex sets within hypergraphs. Unlike low-order tasks that focus on individual vertices and hyperedges, high-order tasks evaluate the model's ability to understand and manipulate relationships involving groups of vertices and their interactions through hyperedges.\n\u2022 Hyperedge Degree. This task aims to determine the degree of a given hyperedge, which is defined as the number of vertices it encompasses. Formally, given a hypergraph $G = \\{V,E\\}$ and a hyperedge $e \\in E$, the target is to compute $deg(e) = |\\{v \\in V | v \\in e\\}|$.\n\u2022 Vertex Set Connection. This task evaluates whether two distinct sets of vertices $A$ and $B \\subseteq V$ are jointly contained within at least one hyperedge, thereby determining if $A$ and $B$ are connected through a common hyperedge. Formally, given $G = \\{V,E\\}$ and two vertex subsets $A, B \\subseteq V$, the target is to ascertain whether there exists a hyperedge $e \\in E$ such that $A \\cup B \\subseteq e$.\n\u2022 Vertex-Set-in-Hyperedge Check. This task involves determining whether a specific subset of vertices $S \\subseteq V$ is entirely contained within any hyperedge of the hypergraph. Formally, given $G = \\{V,E\\}$ and a vertex subset $S \\subseteq V$, the target is to evaluate if there exists a hyperedge $e \\in E$ such that $S \\subseteq e$.\n\u2022 Hyperedge-in-Hyperedge Check. This task assesses whether one hyperedge is completely contained within another, thereby identifying hierarchical or nested structures within the hypergraph. Formally, given $G = \\{V,E\\}$ and two hyperedges $e_1, e_2 \\in E$, the target is to determine if $e_1 \\subseteq e_2$.\n\u2022 Shared-Vertices between Hyperedges. This task requires the model to identify and output the set of vertices shared between any two given hyperedges, thus revealing the overlapping structures and potential intersections within the hypergraph. Formally, given $G = \\{V,E\\}$ and two hyperedges $e_1, e_2 \\in E$, the target is to return $S = e_1 \\cap e_2$, where $S \\subseteq V$."}, {"title": "COMPARISON WITH OTHER BENCHMARKS", "content": "In this subsection, we compare our LLM4Hypergraph Benchmark with existing graph benchmarks, categorizing them into two main types: purely synthetic graph structure benchmarks and real-world graph structure benchmarks. Prominent examples of purely synthetic benchmarks include NLGraph Wang et al. (2024) and CLEAR Chen et al. (2024), while real-world benchmarks encompass GPT4Graph Guo et al. (2023) and GraphArena Tang et al. (2024). These existing benchmarks predominantly focus on tasks that assess the understanding of local, low-order structural properties within graphs. In contrast, our LLM4Hypergraph Benchmark not only includes low-order structural understanding tasks but also introduces high-order tasks specifically designed to evaluate models' comprehension of associations between sets of vertices. This dual focus enables a more comprehensive exploration of LLMs' ability to grasp complex multi-way relationships inherent in hypergraphs. Moreover, our benchmark uniquely incorporates isomorphic tasks, marking the first instance of such an inclusion in this domain. Isomorphic tasks serve to assess the models' proficiency in understanding and discerning global structural properties of graphs and hypergraphs, thereby providing a more holistic evaluation of their structural comprehension capabilities. Additionally, our benchmark integrates both synthetic hypergraph data and real-world hypergraph data, ensuring a diverse and representative dataset that mirrors the complexities found in practices.\nBy encompassing both low-order and high-order tasks, along with the novel isomorphic tasks, our LLM4Hypergraph Benchmark extends beyond the scope of existing benchmarks. It not only evaluates foundational structural understanding but also delves into advanced relational reasoning and global structural interpretation. This comprehensive approach facilitates a thorough assessment of LLMs' abilities to comprehend and process hypergraph structures, thereby advancing the field of hypergraph computational intelligence. Consequently, our benchmark provides a robust framework for identifying both the strengths and limitations of current models, guiding future improvements in model architecture and training methodologies."}, {"title": "DETAILS OF PROMPT FRAMEWORK", "content": "The overall prompt framework tailored for hypergraph structures is illustrated in Figure 3. A complete prompt comprises six components: Example, Hypergraph Description, Hyper-BAG, Question, Tail, and Hyper-COT. The Example section includes question-answer (QA) pairs for each hypergraph and may additionally contain COT-for-Answer, which provides a standard chain-of-thought (CoT) explanation detailing the step-by-step reasoning process to derive the answer. In the depicted figure, certain modules within the prompt are enclosed in brackets \u201c[]\u201d, indicating their optional nature depending on the specific prompt configuration employed. We introduce a set of distinct prompt configurations as follows:\n\u2022 ZERO-SHOT: This configuration includes only the Hypergraph Description, Question, and Tail sections, providing the model with the essential information required to address the query without any illustrative examples or additional reasoning prompts.\n\u2022 ZERO-HYPER-COT: Building upon the Zero-Shot setup, this configuration incorporates the Hyper-COT component after the Tail, introducing a hypergraph-specific chain-of-thought prompt that facilitates structured reasoning tailored to hypergraph-related tasks.\n\u2022 FEW-SHOT: Extending the Zero-Shot approach, the Few-Shot configuration precedes the main prompt with Example content. These examples consist solely of Hypergraph Description, Question, Tail, and Answer, thereby providing the model with concrete instances to guide its understanding and response generation.\n\u2022 COT: Enhancing the Few-Shot configuration, the CoT setup adds COT-for-Answer to each example within the Example section. This addition entails detailed explanations of how each answer is systematically derived, enabling the model to follow a logical reasoning pathway for more accurate and transparent responses.\n\u2022 COT-HYPER-BAG: This advanced configuration builds upon the CoT setup by inserting the Hyper-BAG component as a transitional sentence between the Hypergraph Description and the Question. The Hyper-BAG serves to contextualize the model's focus on hyper-graph construction and comprehension, thereby guiding it into the appropriate analytical framework for handling hypergraph-specific queries."}, {"title": "DETAILS OF HYPERGRAPH LANGUAGE", "content": "Unlike traditional graphs, where an edge connects exactly two vertices, hypergraphs allow hyperedges to connect any number of vertices, thereby necessitating more intricate textual descriptions. However, directly employing abstract language to describe high-order associative structures in hypergraphs can impede LLMs' comprehension of hypergraph architectures, subsequently diminishing performance on downstream tasks. Conversely, utilizing language that describes low-order structures facilitates better understanding by LLMs but may result in the loss of essential high-order information. Inspired by graph description languages, we have designed two distinct types of languages for describing hypergraph structures: Low-Order Structure Language and High-Order Structure Language, as shown in Figure 4. The former primarily adopts a graph-like approach, leveraging low-order structural descriptions to represent hypergraphs by focusing on pairwise relationships and individual connections. This method enhances LLMs' ability to parse and interpret the hypergraph by simplifying its representation into more familiar graph terminology. In contrast, the High-Order Structure Language directly conveys the complex, multi-vertex correlations inherent in hypergraphs, encapsulating high-order relational information without reducing them to mere pairwise interactions. This approach preserves the richness of hypergraph structures, allowing LLMs to grasp the full extent of multi-way relationships that define hypergraph topology. By employing these two complementary descriptive languages, our framework ensures that LLMs can effectively balance the simplicity of low-order descriptions with the comprehensive detail of high-order structures, thereby optimizing their ability to understand and perform accurately on hypergraph-related tasks within the LLM4Hypergraph Benchmark."}, {"title": "LOW-ORDER STRUCTURE LANGUAGE", "content": "To effectively facilitate LLMs' understanding of hypergraph structures through pairwise correlations, we introduce three distinct low-order structure languages as follows: Low-Order Incidence Description (LO-Inc), Neighbor-Pair Description (N-Pair), and Raw Adjacency Matrix Description (Adj-Mat). Each method provides a unique approach to textualizing hypergraph relationships, enhancing the models' ability to interpret and process hypergraph data.\n\u2022 Low-Order-Incidence Description (LO-Inc): This method directly describes how a given vertex is connected to other vertices through hyperedges by explicitly stating the connections in a pairwise manner. For example, consider a hypergraph where vertex $v_1$ is connected to vertices $v_2$ and $v_3$. The LO-Inc representation would be: Vertex $v_1$ is connected to vertices $v_2$ and $v_3$. This description clearly indicates the direct correlations involving $v_1$, facilitating the model's understanding of its immediate connections within the hypergraph.\n\u2022 Neighbor-Pair Description (N-Pair): This method focuses on enumerating all pairs of vertices that share a hyperedge, thereby explicitly listing the connected vertex pairs within the hypergraph. Using the same hypergraph example where vertex $v_1$ is connected to vertices $v_2$ and $v_3$, the N-Pair representation would be: $(v_1, v_2), (v_1, v_3)$. This method emphasizes pairwise relationships without referencing the broader connectivity of individual vertices, providing a straightforward enumeration of connected pairs.\n\u2022 Raw Adjacency Matrix Description (Adj-Mat): This method utilizes an adjacency matrix to represent the hypergraph structure numerically. In this matrix-based approach, the presence or absence of a connection between any two vertices is indicated by binary values. For the hypergraph where vertex $v_1$ is connected to vertices $v_2$ and $v_3$, the Adj-Mat representation"}, {"title": "HIGH-ORDER STRUCTURE LANGUAGE", "content": "To further enhance LLMs' comprehension of hypergraph structures through higher-order correlations, we introduce four distinct high-order structure languages as follows: High-Order Neighbor Description (HO-Neigh), High-Order Incidence Description (HO-Inc), Neighbor-Set Description (N-Set), and Raw Incidence Matrix Description (Inc-Mat).\n\u2022 High-Order Neighbor Description (HO-Neigh): This method leverages the neighbor definitions from HGNN+, describing hypergraph relationships in a two-stage manner by first detailing the connections between vertices and hyperedges, and subsequently outlining the connections between hyperedges and their constituent vertices. For instance, it may state, \"Vertex $v_1$ is connected to hyperedge $e_1$\", followed by \"Hyperedge $e_1$ is connected to vertices $v_1$, $v_2$, and $v_3$\", thereby providing a comprehensive view of the local neighborhood structure.\n\u2022 High-Order Incidence Description (HO-Inc): This method extends the LO-Inc by incorporating higher-order correlations between vertices. An example of HO-Inc would be, \"Vertex $v_1$ is connected to vertices $v_2$ and $v_3$ with hyperedge $e_1$\", which succinctly captures the multi-vertex association facilitated by a single hyperedge.\n\u2022 Neighbor-Set Description (N-Set): This method builds upon the N-Pair by enumerating entire sets of vertices connected by each hyperedge, thus providing a more holistic representation of the hypergraph's connectivity. For example, it may list, \u201c$(V_1, V_2, V_3)$\" and \u201c$(v_2, v_4)$\u201d, explicitly indicating the groups of vertices that jointly form hyperedges.\n\u2022 Raw Incidence Matrix Description (Inc-Mat): This method employs an incidence matrix to numerically represent the hypergraph structure, where each entry indicates the presence or absence of a connection between vertices and hyperedges. An example of an incidence matrix is [[1,0], [1, 1], [1, 0], [0, 1]]. In this matrix, rows correspond to vertices (v1 to v4) and columns correspond to hyperedges (e1 and e2), where a value of 1 denotes the inclusion of a vertex in a hyperedge and 0 denotes absence."}, {"title": "SPECIFICAL PROMPTING FOR HYPERGRAPHS", "content": "Given that hypergraphs do not naturally lend themselves to description using conventional natural language constructs, we introduce two specialized instruction-based prompting techniques designed to enhance LLMs' ability to comprehend hypergraph structures: Hyper-BAG and Hyper-COT.\n\u2022 Build-a-Hypergraph Prompting (Hyper-BAG) addresses the inherent complexity of hypergraphs by facilitating a mental visualization of their structures (Wang et al., 2024). Recog-"}, {"title": "EXAMPLES OF PROMPTS UNDER DIFFERENT SETTINGS", "content": "ZERO SHOT\nPrompt: In an undirected hypergraph, (i, j, k) means that vertex i, vertex j, and vertex"}]}