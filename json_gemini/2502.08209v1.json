{"title": "EQUIVARIANT MASKED POSITION PREDICTION FOR EF-\nFICIENT MOLECULAR REPRESENTATION", "authors": ["Junyi An", "Chao Qu", "Yunfei Shi", "Xinhao Liu", "Qianwei Tang", "Fenglei Cao", "Yuan Qi"], "abstract": "Graph neural networks (GNNs) have shown considerable promise in computational\nchemistry. However, the limited availability of molecular data raises concerns\nregarding GNNs' ability to effectively capture the fundamental principles of physics\nand chemistry, which constrains their generalization capabilities. To address this\nchallenge, we introduce a novel self-supervised approach termed Equivariant\nMasked Position Prediction (EMPP), grounded in intramolecular potential and\nforce theory. Unlike conventional attribute masking techniques, EMPP formulates a\nnuanced position prediction task that is more well-defined and enhances the learning\nof quantum mechanical features. EMPP also bypasses the approximation of the\nGaussian mixture distribution commonly used in denoising methods, allowing for\nmore accurate acquisition of physical properties. Experimental results indicate that\nEMPP significantly enhances performance of advanced molecular architectures,\nsurpassing state-of-the-art self-supervised approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph neural networks (GNNs) have found widespread application in computational chemistry.\nHowever, unlike other fields such as natural language processing (NLP), the limited availability of\nmolecular data hampers the development of GNNs in this domain. For example, one of the largest\nmolecular dataset, OC20 (Chanussot et al., 2021), contains only 1.38 million samples, and collecting\nmore molecular data with ab initio calculations is both challenging and expensive. To address this\nlimitation, molecular self-supervised learning has gained increasing attention. This approach enables\nmolecular GNNs to learn more general physical and chemical knowledge, enhancing performance\nin various computational chemistry tasks, such as drug discovery (Hasselgren & Oprea, 2024) and\ncatalyst design (Chanussot et al., 2021).\nCurrent self-supervised methods for molecular learning contain two mainstream categories: masking\nand denoising. Masking methods (Hu et al., 2020; Hou et al., 2022; Inae et al., 2023) adapt the\nconcept of masked token prediction from natural language processing (NLP) to graph learning, where\ngraph information, such as node attribute, is masked instead of token. However, there are two major\nlimitations: underdetermined reconstruction and lack of deep quantum mechanical (QM) insight. As\nillustrated in Figure 1(a), (i) reconstructing attribute of the masked iodine atom can yield multiple\npossible solutions, and as the number of masked atoms increases, the number of solutions will\nincrease rapidly, making it difficult for training data to cover all possibilities; (ii) the attribute of the\nmasked carbon atom can be inferred from the 2D geometric principles of the benzene ring, causing\nthe model to overlook essential atomic interactions needed for learning QM properties (Messiah,\n2014). In contrast, denoising methods (Zaidi et al., 2023; Feng et al., 2023) are physics-informed\nand facilitate self-supervised learning of QM information in equilibrium structures. Their core idea\ninvolves adding noise to atomic positions and predicting them (see Figure 1(b)). In this process,"}, {"title": "2 PRELIMINARIES", "content": "In this section, we briefly review the mathematical background, which includes molecular property\nprediction, equivariance, spherical harmonics, and so on. More detailed introductions are deferred to\nAppendix A. We list the notations frequently used in the following. We denote the unit sphere as S2,\nwhere the spherical coordinates (0, $) are the polar angle and the azimuth angle, respectively. The"}, {"title": "2.1 MOLECULAR PROPERTY PREDICTION", "content": "Molecular property prediction aims to construct a projection from the molecular 3D structure to the\nmolecular properties. In the following, we use the $z \\in \\{1, ..., 118\\}$ to denote the atomic number, or\nz to denote richer atomic attributes, including atomic numbers, chemical environments, and other\nfeatures. We use the term p to denote atomic 3D position. In a N-nodes molecule, properties can\nbe divided into global properties $y \\in R$ and node-wise properties $y \\in R^N$. In detail, given a 3D\nmolecular $S = \\{(z_i, p_i)|i \\in \\{1, . . ., N\\}\\}$, the properties can be predicted by a GNN:\n$y = PRED(f)$, and $f = GNN(S)$,\n(1)\nwhere f represents the node embeddings in the final GNN layer, and PRED(\u00b7) is the prediction head."}, {"title": "2.2 EQUIVARIANCE", "content": "Given any transformation parameter $g \\in G$, a function $\\phi: X \\rightarrow Y$ is called equivariant to g if it\nsatisfies:\n$T'(g)[\\phi(x)] = \\phi(T(g)[x])$,(2)\nwhere T' (g): \u0423 \u2192 Y and T(g) : X \u2192 X denote the corresponding transformations over Y and X,\nrespectively. Invariance is a special case of equivariance where T'(g) is an identity transformation.\nIn this paper, we mainly focus on the SO(3) equivariance and invariance, since it is closely related to\nthe interactions between atoms in molecule \u00b9. In other words. The backbone and prediction head\nshould adhere to equation 2."}, {"title": "2.3 SPHERICAL HARMONICS AND STEERABLE VECTOR", "content": "Spherical harmonics, a class of functions defined over the sphere $S^2$, form an orthonormal basis and\nhave some special algebraic properties widely used in equivariant models (Kondor et al., 2018; Cohen\net al., 2018). In this paper, we use the real-valued spherical harmonics denoted as $\\{Y_l^m: S^2 \\rightarrow R\\}$,\nwhere l and m denote degree and order, respectively. It is known that any square-integrable function\ndefined over $S^2$ can be expressed in a spherical harmonic basis via\n$f(\\theta,\\phi) = \\sum_{l=0}^{\\infty} \\sum_{m=-l}^{l} f_l^m Y_l^m(\\theta,\\phi)$,\n(3)\nwhere $f_l^m$ is the Fourier coefficient. For any vector r with orientation $(\\theta, \\varphi)$, we define $Y^l(r/||r||) =$\n$[Y_l^l(\\theta, \\psi); Y_l^{-l+1}(\\theta, \\psi); ...; Y_l^l(\\theta,\\psi)]^T$, a vector with 2l + 1 elements. Furthermore, we\ndefine the spherical harmonics representation for any direction:\n$\\sh(\\frac{r}{||r||}) = [\\frac{r}{||r||}; \\frac{Y^1(\\frac{r}{||r||})}{||r||};\\frac{Y^2(\\frac{r}{||r||})}{||r||^2}]$.\n(4)\nIt forms a $(l + 1)^2$ vector. Equivariant models (Zitnick et al., 2022; Liao & Smidt, 2023; An et al.,\n2024) typically set a maximum degree Lmax and construct node embeddings with C channels,\nresulting in an embedding size of $(L_{max} + 1)^2 \\times C$. The full mathematical form of spherical\nharmonics can be found in Appendix A.1.\nA commonly used property of the spherical harmonics is that for any $R \\in SO(3)$, we have $Y^l(Rr) =$\n$D^l(R)Y^l(r)$, where $D^l (R)$ is a $(2l + 1) \\times (2l + 1)$ matrix known as a Wigner-D matrix with degree\nl. Therefore, R and $D^l(R)$ corresponds to T(g) and T'(g), respectively in equation 2. Following the\nconvention in (Chami et al., 2019; Brandstetter et al., 2022), we say $Y^l (r)$ is steerable by Wigner-D\nmatrix of the same degree l. The 2l + 1-dimensional vector space on which a Wigner-D matrix of\ndegree l act is termed a type-l steerable vector space, denoted by the superscript (l) in this paper.\nEquivariant operations. To ensure the equivariance of the entire model, each operation must\nmaintain equivariance. The e3nn library (Geiger et al., 2022) offers common equivariant operations,\nincluding SO(3) linear transformations, SO(3) normalizations, gate activations, and Clebsch-Gordan\n(CG) tensor products. Passaro & Zitnick (2023) further extends certain nonlinear equivariant opera-"}, {"title": "3 METHODOLOGY", "content": "In this section, we first revisit the existing molecular self-supervised methods and outline their primary\nlimitations. Subsequently, we present our approach, Equivariant Masked Position Prediction (EMPP),\nand elaborate on its implementation details."}, {"title": "3.1 REVISITING THE VANILLA MASK METHOD IN MOLECULE LEARNING", "content": "Similar to NLP, molecular self-supervised methods aim to learn the underlying chemical and physical\nmechanisms in molecular systems, such as valence bond theory (Shaik & Hiberty, 2007) and force\nfields (Ponder & Case, 2003). To this end, AttrMask (Hu et al., 2020) pioneers a method that randomly\nmasks atoms and predicts their attributes. More formally, we assume the i, j, ...-th atoms are masked\nand the modified molecule is denoted as $\\hat{S} = \\{(z_1,P_1), ..., (M, p_\u017c), ..., (M, p_j), ..., (z_n, P_N)\\}$,\nwhere M denotes the mask vector (like the [MASK] token in BERT (Devlin et al., 2019)). The\nattributes of the masked atoms are predicted by a GNN as follows:\n$\\check{z}_{i,j,...} = PRED(f_{i,j,...}), f_1, ..., f_v = GNN(\\hat{S})$,\n(5)\nwhere $f_{i,j,...}$ represents the GNN output features of the masked atoms, and PRED(\u00b7) typically refers\nto a neural network. The objective is to minimize the discrepancy between the predicted attributes\n$\\check{z}_{i,j,...}$ and the actual attributes $z_{i,j,...}$, forming a self-supervised learning. Two key limitations emerge:\nthe ill-posedness of attribute prediction and the inability to capture deep quantum mechanical features.\nThese limitations, mentioned in the introduction, can be observed in Figure 1(a). Additionally,\ndenoising methods (Zaidi et al., 2023; Feng et al., 2023) noise the atomic positions, where the\nmodified molecule is denoted as $\\hat{S} = \\{(z_1, P_1), ..., (z_i, P_i + \\epsilon_1), ..., (z_i, P_j + \\epsilon_2), ..., (z_v, P_N)\\}$.\nThe GNNs are required to produce equivariant features and the PRED(\u00b7) is used to predict noises\n$\\epsilon_1, \\epsilon_2, ...$ These methods assume that Boltzmann distribution (i.e. exponent of PES) around\nequilibrium positions can be approximated by Gaussian mixture distribution (Zaidi et al., 2023).\nHowever, the assumed distribution can not always approximate the true distribution. In fact, the\nshapes of PES in local minima are diverse and unknown in advance (Messiah, 2014), making it\nchallenging to define the parameters of the Gaussian mixture distribution."}, {"title": "3.2 EQUIVARIANT MASKED POSITION PREDICTION (EMPP)", "content": "The overall framework of EMPP is depicted in Figure 2. We begin the process by masking the position\nof an atom, causing the corresponding node in the graph to vanish. Next, we utilize equivariant\nbackbones to generate equivariant node embeddings for the masked molecule. These embeddings are\nthen input into a position prediction module, which outputs distributions for directions and radius.\nBoth of them determine the predicted position. By aligning the true and predicted positions, EMPP\nenables the GNNs to capture atomic interactions within 3D molecular structure."}, {"title": "3.2.1 MASK POSITION AND NEIGHBOUR ENCODING", "content": "To address the limitations of previous works, EMPP only masks the atomic position and predicts it\nusing its atomic attributes and the representations of unmasked atoms. Assuming the i-th atom is\nmasked, the modified molecule is denoted as $\\hat{S} = \\{(z_1, P_1), ..., (z_i), ..., (z_N, P_N)\\}$. According to\nphysics, the force at equilibrium is zero, with atomic force primarily governed by atomic interactions,\nsuch as Coulombic forces (Messiah, 2014). Thus, EMPP aims to find a position in equilibrium\nstructures that satisfies the condition $\\sum_{j=1, j\\neq i}^{N} force_{ji} = 0$, where $force_{ji}$ represents the inter-\natomic force and is a function of the unknown masked position $p_i = (x, y, z)$, given that the atomic\nattributes are fixed. Intuitively, there exists a unique optimal position under the force condition,\nsince the number of unknown variables matches the number of equations. Organic chemistry further\nsupports that the position of atoms is uniquely determined in most cases. For instance, in Figure 1,\nwhen any carbon atom is masked, its position can still be uniquely determined. Similarly, for the\niodine atom, its position can be uniquely identified once the structure of its neighboring atoms is\nknown. Further details and examples can be found in Appendix B.1. Additionally, EMPP employs\nequivariant representation capable of storing vector features, enabling the accurate description of the\ntrue $force_{ji}$. Therefore, EMPP can be regarded as a nearly well-posed method, effectively avoiding\nthe approximation of Gaussian mixture distributions to learn forces or other quantum features.\nIn the feedforward process of backbone GNN, the node of masked atom is completely removed.\nWe encode the attributes $z_i$ of the masked atom into the embedding of unmasked atoms. Note that\ntraditional backbone GNNs use an embedding layer to project atomic attributes into the embedding\nspace; we employ the same module augmented with a two-layer multi-layer perceptron (MLP) to\nembedding the attributes $z_i$:\n$e_i = MLP(EMBED(z_i))$.\n(6)\nNext, we aggregate $e_i$ with the embeddings of the unmasked nodes:\n$v_j \\leftarrow v_j + e_i, j \\in \\{1, 2, ...i - 1, i + 1, ..., N\\}$,\n(7)\nwhere v denotes the node embedding. We apply equation 7 at each layer of the backbone GNN.\nNotably, $e_i$ is invariant under SO(3) transformation. If the node embedding $v_j$ is a spherical harmonic\nrepresentation, we aggregate $e_i$ with the its type-0 vector $v_j^{(0)}$.\nMasking multiple atoms in EMPP increases the complexity of solution space, may make the\nposition prediction ill-posed\u00b2. To mitigate this issue, we mask different atoms sequentially\nwithin a single molecule. First, we randomly generate the indices of the masked atoms as\n$Mask = \\{Mask_1, Mask_2, ..., Mask_n\\}$. We then create multiple masked molecules, each con-\ntaining only one masked atom, denoted as $\\hat{S}_1, \\hat{S}_2, \u2026, \\hat{S}_n$. Each masked molecule independently\npredicts the position of its masked atom, and we average the resulting loss values. The objective\nfunction for training multiple masked molecules is given by:\n$\\mathcal{L}_{multi} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathcal{L}_{single} (P_{Mask_i}, Pred (GNN(\\hat{S}_i)))$,\n(8)\nwhere $Pred()$ refers to the position prediction block, and $\\mathcal{L}_{single}(\u00b7)$ represents the loss function for\neach individual masked atom. Details of them will be provided in the following sections. Since\nthe nodes of masked atoms in EMPP are completely deleted, there is a distinct difference between\n$(\\hat{S}_1, \\hat{S}_2, ..., \\hat{S}_n)$. In contrast, previous methods do not delete nodes but perturb the features of the\nnodes. Intuitively, EMPP has a stronger capability to generate a vast array of diverse data."}, {"title": "3.2.2 POSITION PREDICTION", "content": "Equivarance for predicted position. After masking i-th atom and encoding its attributes into the\nbackbone GNN, we obtain the node representations of the masked molecule. Notably, we retain\nthe neighbor set of the masked atom, denoted as $N_i$, and use the embeddings of the neighboring\nnodes to predict the position. For an unmasked node with embedding $f_k$ (where $k \\in N_i$) and its\nposition $p_k$, the probability distribution of the masked atom's position is given by $p_k (r|f_k)$, with the\npredicted position denoted as $p_i = p_k + r$. This distribution $p_k$ must satisfy both normalization and\nnon-negativity constraints: $\\int_{\\Omega_{xyz}} p_k(r)dV = 1$ and $p_k(r) \\geq 0$ where $dV = dxdydz$ is the volume"}, {"title": "3.2.3 LoSS FUNCTION", "content": "Radius loss. During training, the model is optimized by the distribution of true radius $||\\check{r}_{ik}||$ and\ndirection $\\frac{\\check{r}_{ik}}{||\\check{r}_{ik}||}$, where $\\check{r}_{ik} = p_i \u2013 p_k$. Intuitively, the deterministic vector $\\check{r}_{ik}$ should correspond\nto a Dirac delta distribution denoted as $\\delta(r \u2013 \\check{r}_{ik})$. In practice, however, the model can learn the\naccurate $\\check{r}_{ik}$ as long as the defined distribution can uniquely represent $\\check{r}_{ik}$. The Dirac delta distribution\ncan reduce training stability and lacks the ability to transfer to other conformations. To address this,"}, {"title": "3.3 APPLICATION OF EMPP", "content": "Pre-training without annotation. The previously described EMPP can learn quantum knowledge\nfrom equilibrium molecules without quantum property labels. Therefore, EMPP can be used to\npre-train GNNs to learn transferable knowledge. For example, the PCQM4Mv2 dataset (Nakata &\nShimazaki, 2017) contains a vast collection of equilibrium molecules but provides labels for only one\nchemical property, the gap between HOMO and LUMO. We can use EMPP to pre-train GNNs on\nPCQM4Mv2 and use the pre-train models to predict additional chemical properties in other datasets.\nAuxiliary task for property prediction. When training a model for a specific property prediction,\nwe calculate two losses: the prediction loss for the targeted property and the EMPP loss as defined in\nequation 8. These losses are then combined for gradient descent. In this case, the goal of EMPP is not\nto learn the equilibrium position of the masked atom, but rather to learn the position corresponding to\nthe known property value, which may be in a non-equilibrium state. For example, when the force\non the masked atom is non-zero (indicating non-equilibrium), EMPP can capture the relationship\nbetween the true position and this force. Similarly, for other quantum labels, implicit relationships\nexist with the masked atom's position. In practice, we encode the label into the GNN. If the label is\nglobal invariant property like energy, the encoding can be written as:\n$v^{(0)} \\leftarrow v^{(0)} + LINEAR(GAUSS(y_{energy}^*))$,\n(19)\nwhere GAUSS(\u00b7) denotes a Gaussian block. Specifically, for node-wise equivariant labels such as\nforce $y_{forces}^*$, we map it into the spherical harmonic representations and add it to embeddings:\n$v_k \\leftarrow v_k + ELINEAR(sh(y_{forces}^*))$.\n(20)\nThen, we use the GNN output embeddings to predict position. In this scenario, we treat EMPP as an\nauxiliary task for property prediction. By modeling the relationship between the target property and\nposition, EMPP further enhances the generalization ability of the property prediction model."}, {"title": "4 RELATED WORK", "content": "Graph self-supervised methods. Leveraging the inherent graph structure of molecules, many graph-\nbased self-supervised methods have the potential to train molecular models that capture transferable\nknowledge. For example, Hu et al. (2020) proposed graph context prediction and attribute masking\nmethods to enhance molecular property prediction. GraphMAE (Hou et al., 2022) pre-trained"}, {"title": "5 EXPERIMENTS", "content": "In this section, we present experiments to evaluate the effectiveness of EMPP across several 3D\nmolecular benchmarks. Since EMPP can be applied in both unlabeled and labeled scenarios, we\nevaluate it in two settings: (i) self-supervised tasks for learning transferable molecular knowledge,\nand (ii) auxiliary tasks for enhancing the prediction of supervised molecular properties."}, {"title": "5.1 DATASETS AND CONFIGURATIONS", "content": "Datasets. We evaluate quantum property prediction using the QM9 (Ramakrishnan et al., 2014)\nand MD17 (Chmiela et al., 2017) datasets. QM9 comprises 134,000 stable small organic molecules\nmade up of C, H, O, N, and F atoms, with one conformation per molecule, and includes labels\nfor 12 quantum properties. MD17 contains molecular dynamics trajectories for 8 small organic\nmolecules, providing between 150,000 and nearly 1 million conformations per molecule, along\nwith corresponding total energy and force labels. Notably, MD17 features a significant number of\nnon-equilibrium molecules. Additionally, we utilize the PCQM4Mv2 (Nakata & Shimazaki, 2017)\ndataset to pre-train GNN backbones, which consists of 3.4 million organic molecules, each with one\nequilibrium conformation, and is widely used for pre-training.\nBaselines. Our baselines include state-of-the-art self-supervised methods for 3D molecular structures,\nsuch as AttrMask (Hu et al., 2020), DP-TorchMD-NET (Zaidi et al., 2023), 3D-EMGP (Jiao et al.,\n2023), SE(3)-DDM (Liu et al., 2022), Transformer-M (Luo et al., 2022), and Frad (Feng et al., 2023).\nThese methods pre-train GNNs on the PCQM4Mv2 dataset before predicting molecular properties in\nQM9 and MD17, leveraging additional molecular data. In contrast, EMPP can operate without extra\ndata, so we also compare it to molecular models trained solely on QM9 or MD17, including SchNet\n(Sch\u00fctt et al., 2018), PaiNN (Sch\u00fctt et al., 2021), DimeNet++ (Gasteiger et al., 2020), TorchMD-NET\n(Doerr et al., 2021), SEGNN (Brandstetter et al., 2022), and Equiformer (Liao & Smidt, 2023).\nDetailed configurations for EMPP and the aforementioned baselines can be found in Appendix C.1."}, {"title": "5.2 RESULTS WITHOUT PRE-TRAINING", "content": "QM9. We first evaluate EMPP without incorporating additional data, treating it as an auxiliary task\nfor QM9 and MD17. The total loss comprises equation 8 combined with the MAE loss from the\noriginal property prediction. In these experiments, EMPP is implemented using the Equiformer\nbackbone (Liao & Smidt, 2023) due to its high-degree equivariant representation, which effectively\ncaptures interatomic features. As shown in Table 1, two key observations emerge. First, EMPP\nenhances prediction accuracy across all QM9 tasks, achieving the best results in 11 tasks without\nadditional data. Moreover, multi-masking show better performance, demonstrating that the data"}, {"title": "5.3 RESULTS WITH PRE-TRAINING ON PCQM4Mv2", "content": "In this section, we assess the effectiveness of EMPP as a pre-training task. To maintain consistency\nwith state-of-the-art self-supervised methods (Zaidi et al., 2023; Feng et al., 2023), we use TorchMD-\nNet (ET) (Th\u00f6lke & Fabritiis, 2022) as the backbone model. To enhance the fine granularity of the\nspherical representation after Fourier transformation, we extend the node representation of TorchMD-\nNet to $L_{max} = 3$ while retaining all other core operations (see details in Appendix B.3). During the\npre-training phase, EMPP encodes only the atomic numbers of the masked atoms into the backbone.\nIn the fine-tuning stage, EMPP encodes properties following the same approach outlined in Section\n5.2.\nAs shown in Table 3, the attribute masking method produces poorer results for quantum property\nprediction, consistent with our earlier observations: recovering attributes based on simple features\nlimits the model's ability to capture deep quantum characteristics. In contrast, denoising methods such\nas DP-TorchMD-Net and Frad deliver competitive performance due to their physically interpretable\nparadigms. Notably, our EMPP surpasses denoising methods in nine tasks, achieving the best results\nin seven tasks. This success is attributed to EMPP's more precise paradigm, which employs a precise\nparadigm to learn interactions instead of approximation of mixture distribution."}, {"title": "6 CONCLUSION", "content": "We identified key limitations in mainstream molecular self-supervised learning: attribute masking\nintroduces ill-posedness and fails to capture quantum features, while denoising struggles with\naccurately modeling Gaussian mixture in unknown distributions. To address these issues, we propose\nEMPP, which predicts masked atom positions through neighbor structures, bypassing Gaussian\nmixture approximation and turning the task into a well-posed one. Our experiments show EMPP\nachieves state-of-the-art results in quantum property prediction. While we employ equivariant\nrepresentations in EMPP, extending to higher-order representation (l > 3) remains an open question."}, {"title": "A THE MATHEMATICS", "content": null}, {"title": "A.1 THE MATHEMATICS OF SPHERICAL HARMONICS", "content": null}, {"title": "A.1.1 THE PROPERTIES OF SPHERICAL HARMONICS", "content": "The spherical harmonics Y\u2081m(0, $) are the angular portion of the solution to Laplace's equation\nin spherical coordinates where azimuthal symmetry is not present. Some care must be taken in\nidentifying the notational convention being used. In this entry, @ is taken as the polar (colatitudinal)\ncoordinate with \u03b8 in [0, \u03c0], and 4 as the azimuthal (longitudinal) coordinate with $ in [0, 2\u03c0).\nSpherical harmonics satisfy the spherical harmonic differential equation, which is given by the angular\npart of Laplace's equation in spherical coordinates. If we define the solution of Laplace's equation as\nF = \u03a6(\u03c6)\u0398(\u03b8), the equation can be transformed as:\n$\\Phi(\\varphi) \\frac{d}{sin \\theta d\\theta} (sin\\theta \\frac{d\\Theta(\\theta)}{d\\theta}) + \\frac{\\Theta(\\theta) d^2\\Phi(\\varphi)}{sin^2 \\theta d\\varphi^2} + l(l + 1)\\Theta(\\theta)\\Phi(\\varphi) = 0$\n(21)\nHere we omit the derivation process and just show the result. The (complex-value) spherical\nharmonics are defined by:\n$Y_l^m(\\theta, \\varphi) = \\sqrt{\\frac{2l + 1}{4\\pi} \\frac{(l - m)!}{(l + m)!}} P_l^m(cos \\theta)e^{im\\varphi}$,\n(22)\nwhere $P_l^m(cos \\theta)$ is an associated Legendre polynomial. Spherical harmonics are integral basis,\nwhich satisfy:\n$\\int \\int_{0}^{2\\pi} Y_{l_1}^{m_1}(\\theta, \\varphi) Y_{l_2}^{m_2}(\\theta, \\varphi) Y_{l_3}^{m_3}(\\theta, \\varphi) sin \\thetad\\thetad\\varphi = \\\\sqrt{\\frac{(2l_1+1)(2l_2+1)(2l_3+1)}{4\\pi}} \\begin{pmatrix} l_1 & l_2 & l_3 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} l_1 & l_2 & l_3 \\\\ m_1 & m_2 & m_3 \\end{pmatrix}$,\n(23)"}, {"title": "A.1.2 FOURIER TRANSFORMATION OVER $S^2$", "content": "It is well known that the spherical harmonic $Y_l^m$ form a complete set of orthonormal functions and\nthus form an orthonormal basis of the Hilbert space of square-integrable function. On the unit sphere\nS2, any square-integrable function f can thus be expanded as a linear combination of these:\n$F(\\theta,\\varphi) = \\sum_{l=0}^{\\infty} \\sum_{m=-l}^{l} f_l^m Y_l^m(\\theta,\\varphi)$,\n(25)\nThe coefficient $f_l^m$ can be obtained by the Fourier transformation over S2, which is\n$f_l^m = \\int_{S_2} f(r)Y_l^{m*}(r)dr = \\int_0^{2\\pi} \\int_0^{\\pi}d\\varphi d\\theta sin \\theta f(\\theta, \\psi)Y_l^m(\\theta,\\psi)$.\n(26)\nUsually we define a vector $f^l = [f_{-l}, f_{-l+1}, ..., f_l]$ to denote the Fourier coefficients with degree\nl. We now investigate how the fourier coefficients transforms if we rotate the input signal. More\nprecisely, we want to calculate the coefficient for of the signal $f (Rr)$, where $R \\in SO(3)$ is a rotation\nmatrix.\nUsing the fact $Y^l(Rr) = D^l(R)Y^l(r)$, and equation 25, we know\n$f(Rr) = \\sum_{l=0}^{\\infty} \\sum_{m=-l}^{l} f_l^m(R) Y_l^m(r) = \\sum_{l=0}^{\\infty} \\sum_{m=-l}^{l} \\sum_{m'} f_{l}^{m'}D^{l}_{mm'} Y_l^m (r)$.\nTherefore $f^l_r = D^{l T}f^l$ and it is steerable."}, {"title": "A.1.3 THE RELATIONSHIP BETWEEN SPHERICAL HARMONICS AND WIGNER-D MATRIX", "content": "A rotation R sending the r to r' can be regarded as a linear combination of spherical harmonics that\nare set to the same degree. The coefficients of linear combination represent the complex conjugate of\nan element of the Wigner D-matrix. The rotational behavior of the spherical harmonics is perhaps"}, {"title": "A.2 EQUIVARIANT OPERATION", "content": null}, {"title": "A.2.1 EQUIVARIANCE OF CLEBSCH-GORDAN TENSOR PRODUCT", "content": "The Clebsch-Gordan Tensor Product shows a strict equivariance for different group representations", "satisfies": "n$\\sum_{m_1'", "Product": "n$CG \\Big( \\sum_{m_1, m_2}  D^{l_1}_{m_1'm_1}(g) D^{l_2}_{m_2'm_2}(g)   Y^{l_1} Y^{l_2} \\Big) =   \\sum_{l,m} \\sum_{m_1, m_2}  C_{(l,m)}^{(l_1,m_1)(l_2,m_2)}   \\Big( \\sum_{m} D^{l_o}_{mom_o}  Y_l"}]}