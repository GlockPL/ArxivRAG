{"title": "Data Pruning Can Do More: A Comprehensive Data Pruning Approach for Object Re-identification", "authors": ["Zi Yang", "Haojin Yang", "Soumajit Majumder", "Jorge Cardoso", "Guillermo Gallego"], "abstract": "Previous studies have demonstrated that not each sample in a dataset is of equal importance during training. Data pruning aims to remove less important or informative samples while still achieving comparable results as training on the original (untruncated) dataset, thereby reducing storage and training costs. However, the majority of data pruning methods are applied to image classification tasks. To our knowledge, this work is the first to explore the feasibility of these pruning methods applied to object re-identification (ReID) tasks, while also presenting a more comprehensive data pruning approach. By fully leveraging the logit history during training, our approach offers a more accurate and comprehensive metric for quantifying sample importance, as well as correcting mislabeled samples and recognizing outliers. Furthermore, our approach is highly efficient, reducing the cost of importance score estimation by 10 times compared to existing methods. Our approach is a plug-and-play, architecture-agnostic framework that can eliminate/reduce 35%, 30%, and 5% of samples/training time on the VeRi, MSMT17 and Market1501 datasets, respectively, with negligible loss in accuracy (< 0.1%). The lists of important, mislabeled, and outlier samples from these ReID datasets are available at https://github.com/Zi-Y/data-pruning-reid.", "sections": [{"title": "1 Introduction", "content": "Object re-identification (ReID) is a computer vision task that aims to identify the same object across multiple images. ReID has a wide range of applications in security surveillance (Khan et al., 2021), robotics (Wengefeld et al., 2016), and human-computer interaction (Wang et al., 2019) among others. Similar to several other downstream computer vision applications, the performance of ReID algorithms is contingent upon the quality of data. ReID datasets are constructed by first collecting a pool of bounding boxes containing the"}, {"title": "2 Related Works", "content": "Data Pruning aims to remove superfluous samples in a dataset and achieve comparable results when trained on all samples (Toneva et al., 2018; Feldman & Zhang, 2020; Paul et al., 2021; Sorscher et al., 2022). Different to dynamic sample selection (Chen et al., 2017; Hermans et al., 2017) and online hard example mining (Shrivastava et al., 2016), which emphasize on informative sample selection for a mini-batch at each iteration, (static) data pruning aims to remove redundant or less informative samples from the dataset in one shot. Such methods can be placed in the broader context of identifying coresets, i.e., compact and informative data subsets that approximate the original dataset. K-Center (Sener & Savarese, 2017) and"}, {"title": "3 Identifying Important Samples", "content": "Our work focuses on supervised ReID. Similar to image classification (Zeng et al., 2014; Rawat & Wang, 2017), a standard ReID network includes a feature extraction backbone, e.g., CNN-based (He et al., 2016) or transformer-based model (Dosovitskiy et al., 2021), followed by a classification head (Song et al., 2019; Liao & Shao, 2022). The backbone network extracts representations from images of persons (Wei et al., 2018) or vehicles (Liu et al., 2016). The classification head, typically a fully connected layer, maps these extracted features to class labels, which in ReID refer to the individuals or vehicles in the respective dataset. The training of ReID models varies with respect to classification counterparts in the type of loss functions. In addition to the standard cross-entropy loss, ReID training often involves metric losses, e.g., triplet loss (Schroff et al., 2015) and center loss (MacDonald, 2013), to learn more discriminative features: $L_{ReID} = L_{CE} + \\alpha L_{metric}$, where \u03b1 is a hyper-parameter to balance the loss terms."}, {"title": "3.2 Motivation", "content": "Typically, data pruning approaches estimate the sample importance after training for several or all epochs. However, the majority of data pruning methods do not fully exploit or take into account the training dynamics in evaluating sample importance. We observe that underestimating the training dynamics can lead to less accurate and unreliable estimations of the sample's importance."}, {"title": "3.3 Methodology", "content": "Let (x, y) \u2208 Dtrain be an image-label pair and $z^{(t)}(x) \\in \\mathbb{R}^c$ be the logit vector of dimension c at epoch t, i.e., the un-normalized output of the classification head (pre-softmax). Here c refers to the number of classes in the training set. A logit value, presented on a logarithmic scale, represents the network's predicted probability of a particular class (Anderson et al., 1988), with large values indicating high class probability. We first calculate the average logits of a sample over all training epochs T. We then apply Softmax \u03c3(.) to generate the soft label \u1ef9 of this sample, which portrays the relationship between this sample and all classes,\n$\\tilde{y} = \\sigma \\left(\\frac{1}{T} \\sum_{t=1}^{T} z^{(t)}(x)\\right) \\in \\mathbb{R}^c$.\nThis soft label \u1ef9 summarizes the model's predictions at each training epoch. We conjecture that this soft label can effectively and accurately depict the ground truth characteristics of a sample. Next, we quantify"}, {"title": "4 Data Purification", "content": "Although most samples with high importance scores contribute significantly to model training, some high-scoring samples may be noisy (Feldman & Zhang, 2020; Paul et al., 2021) and degrade model performance. To address this limitation, in this section we further exploit the generated soft label to \"purify\" samples in two ways: by correcting mislabeled samples and by eliminating outliers.\nDataset Noise. Noise in ReID datasets can be broadly categorized into (Yu et al., 2019): (i) label noise (i.e., mislabeled samples) caused by human annotator errors, and (ii) data outliers caused by object detector errors (e.g., heavy occlusion, multi-target coexistence, and object truncation; see examples in Appendix C).\nOur method aims to reuse mislabeled samples by label correction and eliminating outliers."}, {"title": "5 Experiments", "content": "We demonstrate the effectiveness of our method via detailed experimentation. In Sec. 5.3, we evaluate our pruning metric on ReID and classification datasets, and verify that it effectively quantifies the sample importance. In Sec. 5.4, we train models on noisy datasets after correcting labels and removing outliers to validate the data purification ability. In Sec. 5.5, we present the results of our comprehensive data pruning method, which includes removing less important samples, correcting mislabeled samples, and eliminating outliers. Finally, we perform several detailed ablation studies in Sec. 5.6 to evaluate the impact of hyper-parameters and verify the importance of logit accumulation."}, {"title": "5.1 Datasets and Evaluation", "content": "Datasets. We evaluate our method across three standard ReID benchmarks: Market1501 (Zheng et al., 2015) and MSMT17 (Wei et al., 2018) for pedestrian ReID, and VeRi-776 (Liu et al., 2016) for vehicle ReID. Market1501 is a popular ReID dataset consisting of 32, 668 images featuring 1,501 pedestrians, captured across six cameras. Compared to Market1501, MSMT17 is a more challenging and large-scale person ReID dataset (Wei et al., 2018), comprising 126, 441 images from 4,101 pedestrians. VeRi-776 is a widely used vehicle ReID benchmark with a diverse range of viewpoints for each vehicle; it contains 49, 357 images of 776 vehicles from 20 cameras.\nEvaluation. Given a query image and a set of gallery images, the ReID model is required to retrieve a ranked list of gallery images that best matches the query image. Based on this ranked list, we evaluate ReID models using the following metrics: the cumulative matching characteristics (CMC) at rank-1 and mean average precision (mAP), which are the most commonly used evaluation metrics for ReID tasks (Bedagkar-Gala & Shah, 2014; Ye et al., 2021; Zheng et al., 2016). As an evaluation metric, we adopt the mean of rank1 accuracy and mAP (He et al., 2020), i.e., $\\frac{1}{2}(rank1 + mAP)$, to present the results in a single plot."}, {"title": "5.2 Implementation Details", "content": "Estimating the Sample Importance Score. For ReID tasks, we follow the training procedure in (Luo et al., 2019): a ResNet50 (He et al., 2016) pretrained on ImageNet (Deng et al., 2009) is used as the backbone and trained on a ReID dataset for 12 epochs to estimate sample importance. For optimization, we use Adam (Kingma & Ba, 2014) with a learning rate of 3.5\u00d710-4. We do not apply any warm-up strategy or weight decay. The batch size is set to 64. Following Luo et al. (2019), We use a combination of the cross-entropy loss and the triplet loss as the objective function. During training, we record the logit values of each sample after each forward pass. Then, we generate the soft label of each sample based on Eq. 1 and"}, {"title": "5.3 Find Important Samples", "content": "This section aims to verify the effectiveness of our proposed importance score by data pruning experiments. Because our work predominantly concentrates on data pruning applied to the ReID tasks, we demonstrate the efficacy of our proposed importance score on three ReID datasets in Sec. 5.3.1. Considering the substantial similarity between ReID and classification tasks, we also extend our data pruning experiments to CIFAR-100 and CUB-200-201 datasets in Sec. 5.3.2, thereby ensuring a more comprehensive validation of our method."}, {"title": "5.3.1 Data Pruning on ReID Datasets", "content": "We observe that the majority of data pruning methods have been evaluated on standard image classification datasets. However, to date, no pruning metric has been evaluated on ReID datasets. To this end, we perform a systematic evaluation of three standard pruning metrics on ReID datasets: EL2N score (Paul et al., 2021), forgetting score (Toneva et al., 2018), supervised prototypes (Sorscher et al., 2022). Following the standard protocol (Toneva et al., 2018; Sorscher et al., 2022), the forgetting scores and supervised prototype scores are calculated at the end of training. The EL2N score (Paul et al., 2021) and our proposed metric are estimated early in training (i.e., after 12 epochs - which is 10% of the total training epochs and recommended by the standard EL2N protocol). For EL2N, we report two scores: one generated by a single trained model and the other one by 20 trained models, each trained with a different random seed. Training epochs for the importance score estimation of standard data pruning methods are shown in Tab. 1. In addition to standard data pruning methods, we also conducted additional comparisons with RoCL (Zhou et al., 2020), which is a curriculum learning method designed for learning with noisy labels but also utilizes training dynamics to select samples. To fairly compare with it, we utilize its core idea of moving average loss as the pruning metric to realize static data pruning. The training sets are constructed by pruning different fractions of the lowest-scored samples. In all experiments, training on the full dataset (no pruning) and a random subset of the corresponding size (random pruning) are used as baselines (Sorscher et al., 2022). We present the results of data pruning approaches on three different ReID datasets in Fig. 4."}, {"title": "5.3.2 Data Pruning on Classification Dataset", "content": "To comprehensively verify the effectiveness of our proposed metric, we conduct supplementary experiments on two classification datasets. We demonstrate the efficacy of our method on CIFAR-100 (Krizhevsky et al., 2009) and CUB-200-2011 (Wah et al., 2011) datasets and compare our method with the forgetting (Toneva et al., 2018) and EL2N (Paul et al., 2021) scores in Fig. 6. For a fair comparison, all methods employ the same training time or cost to estimate the importance scores of samples. In detail, following the standard protocol of EL2N (Paul et al., 2021) we solely train a model for 20 epochs on CIFAR-100 and 3 epochs on CUB-200-"}, {"title": "5.4 Robust Training on Noisy Datasets", "content": "We test the efficacy of our data purification method on random noisy datasets. Herein, we start with the original images with the assigned labels. A certain percentage (i.e., from 10% to 50%) of training images are randomly selected and intentionally assigned the wrong labels. We compare our approach with a state-of-the-art method for identifying mislabeled samples, i.e., AUM (Pleiss et al., 2020), and evaluate each of our used components, i.e., label correction and removing outliers."}, {"title": "5.5 Putting It All Together", "content": "By integrating data purification we build a comprehensive data pruning approach. Samples are removed or rectified according to the sequence below: 1) all outliers (i.e., samples whose highest class score is lower than 10%) are removed; 2) all samples with incorrect labels are rectified; 3) easy samples (i.e., samples whose soft labels have low entropy) are pruned. Please note that the total number of pruned samples equals the sum of all outliers and the pruned easy samples (i.e., all outliers are removed). We test our approach on three ReID datasets and evaluate the benefit of our data purification approach. In Fig. 8, we observe marked performance differences between datasets. Data purification on MSMT17 shows a notable effect, potentially due to the presence of more outliers and mislabeled samples in this dataset. The performance on VeRi is slightly improved as well. However, the impact on Market1501 is comparatively limited, plausibly owing to stricter annotation protocols and fewer outliers compared to the other datasets. In summary, the seamless integration of data purification and data pruning can effectively boost overall performance. Especially, on MSMT17 our approach can even eliminate/reduce 30% of samples/training time with almost no compromise in performance."}, {"title": "5.6 Sensitivity Analysis and Ablation Studies", "content": "Our approach involves two hyper-parameters: (i) the number of training epochs required to generate the soft labels, i.e., T from Eq. 1, and (ii) the threshold d to remove outliers. We first explore the impact of the hyper-parameter T. Next, we verify the importance of logit accumulation through ablation experiments, and finally analyze the effect of the threshold d on the performance of data purification."}, {"title": "6 Discussion and Conclusion", "content": "In this work, we have addressed two issues of ReID datasets (less informative samples and noise) by proposing a plug-and-play architecture-agnostic data pruning framework. Firstly, by leveraging the training dynamics, we have provided a more accurate and robust pruning metric with extremely low computational overhead. In the generalization test, we have demonstrated that our metric reflects the ground-truth characteristics of samples independently of the model architecture used for training, i.e., the sample ordering (ranking) obtained via a ResNet remains effective in training a vision transformer. For completeness, we have also verified our method on two image classification datasets, observing that our approach exhibits superior performance over competing methods as well. Secondly, we have proposed an efficient data purification method, enabling the correction of mislabeled samples and the removal of outliers. Experiments on noisy datasets validate that our data purification method exhibits robustness, achieving remarkable results. Finally, by integrating data purification we have built a comprehensive data pruning framework and demonstrated that it achieves state-of-the-art performance when pruning ReID datasets, allowing for the removal of 35%, 30%, and 5% of samples from the VeRi, MSMT17, and Market1501 datasets respectively, thereby leading to an equivalent reduction in training time. Like previous works (Toneva et al., 2018; Paul et al., 2021), our data pruning method requires labels. In the future, we intend to investigate data pruning methods based on logit trajectories in an unsupervised setting."}, {"title": "A Implementation Details for ReID Tasks", "content": "All experiments are implemented in PyTorch (Paszke et al., 2019) using the FastReID (He et al., 2020) toolbox\u00b9 on 4 NVIDIA Tesla V100 GPUs. A general workflow for supervised ReID is shown in Fig. 13. In our work, we follow the training procedure of Luo et al. (2019). For feature extraction, we employ a ResNet50 (He et al., 2016) pre-trained on ImageNet (Deng et al., 2009); the model is trained for 12 epochs. All images from Market1501 and MSMT17 are resized to 256 \u00d7 128 pixels, while images from VeRi are resized to 256 \u00d7 256 pixels. During training, we record the logits for each sample after each forward pass. For optimization, we use a combination of the cross-entropy loss and the triplet loss, i.e., $L_{train} = L_{ce} + L_{triplet}$.\nFeatures FC Layers\nClassification\nLoss\nTriplet Loss"}, {"title": "A.2 Existing Methods", "content": "E2LN. We use the same model configuration and settings as in our approach, except for the loss function. When calculating the EL2N score, we strictly follow its procedure and definition (Paul et al., 2021), i.e., we only use the cross-entropy loss and do not utilize additional loss functions (e.g., metric loss). We train till 12 epochs and then compute the EL2N score for each sample. For EL2N (20 models), we run the same experiments 20 times but with different random seeds and obtain the score by averaging over the runs."}, {"title": "A.3 Data Pruning Experiments on ReID datasets", "content": "Following the default model configuration and settings\u00b2, we train four independent ReID models using different random seeds on the pruned dataset, where the samples are pruned based on different types of importance score, i.e., the forgetting, EL2N, supervised prototype and our scores. We then report the mean performance across these four models."}, {"title": "A.4 Generalization from ResNet to ViT", "content": "We initialize a vision transformer ViT-B/16 (Dosovitskiy et al., 2021) with its weight pre-trained on ImageNet21K and use the default configuration\u00b3. The model is trained on a pruned dataset, which is pruned based on the ranking list of samples generated by a ResNet50. We report the average performance across four runs with different random seeds."}, {"title": "A.5 Robust Training on Noisy Datasets", "content": "To evaluate the performance of AUM, we follow their proposal (Pleiss et al., 2020) to fine-tune the threshold value using threshold samples. For our method, we set the threshold of outlier removal to d = 10% on all three ReID datasets. After the outlier removal and label correction, we train four ReID models using the default settings\u00b2 and present the mean accuracy derived from these four runs."}, {"title": "A.6 Putting It All Together", "content": "In our proposed framework, samples are either removed or rectified according to the sequence below:\n1. Eliminating all outliers according to the highest class scores of the samples, as described in Sec. 4.\n2. Removing easy samples, as described in Sec. 3.3.\n3. Rectifying mislabeled samples. Details are presented in Sec. 4.\nPlease note that the total number of pruned samples equals the sum of all outliers and the pruned easy samples (i.e., all outliers are removed). For instance, if we set the threshold 8 to 10% the noisy sample rate of the MSMT17 dataset is 1.1%. If 10% of the samples are removed, it includes 1.1% outlier samples and 8.9% easy samples."}, {"title": "B Implementation Details for Classification Tasks", "content": "For image classification on CIFAR-100 and CUB-200-2011 datasets, we begin with computing the forgetting score (Toneva et al., 2018), EL2N score (Paul et al., 2021), and our proposed metric for each sample. For a fair comparison, all methods employ the same training time or cost to estimate the importance scores of samples."}, {"title": "D Ablation Study", "content": "In Sec. 5.6, we investigate how early in training our metric is effective at estimating the importance scores of samples and observe that after 12 epochs, the importance scores of samples estimated by our method tend to be stabilized. However, it beckons the consideration: Are the generated soft labels trained for 12 epochs sufficient to achieve a good performance in label correction? To answer this question, we explore the impact of score computation epoch T, which is the soft labels generation epoch as well, on label correction. We present results in Fig. 15. Our sensitivity analysis reveals that the soft labels generated between the 8th - 20th epochs demonstrate a superior label correction capability, even approaching the model performance trained on the clean dataset. This observation clarifies our earlier question: Even in the relatively early stages of training (after 8 epochs), the soft labels we proposed still exhibit a notable capability of label correction.\nFurthermore, we observe that the label correction capability of soft labels generated after 20 epochs exhibits a decline, particularly pronounced under high noise ratios such as 30%. The underlying rationale is that, as the model undergoes prolonged training on a noisy dataset, the model starts memorizing a huge amount of mislabeled samples, leading to fitting these erroneous labels (Pleiss et al., 2020). Overall, our approach exhibits robustness to hyper-parameter T, particularly within an acceptable range of noise ratios., e.g. 10%, 20%."}, {"title": "E Example Images", "content": "We present some samples from three ReID datasets and CIFAR-100 dataset sorted from easy to hard based on our proposed pruning metric. From Figs. 16 to 19, we observe that the samples with the smallest importance scores tend to be simple and are canonical representations of each class. In contrast, the samples with higher scores are harder to identify: they have different backgrounds or even suffer from occlusion or truncation."}, {"title": "D.1 Impact of Score Computation Epoch T on Label Correction", "content": "In Sec. 5.6, we investigate how early in training our metric is effective at estimating the importance scores of samples and observe that after 12 epochs, the importance scores of samples estimated by our method tend to be stabilized. However, it beckons the consideration: Are the generated soft labels trained for 12 epochs sufficient to achieve a good performance in label correction? To answer this question, we explore the impact of score computation epoch T, which is the soft labels generation epoch as well, on label correction. We present results in Fig. 15. Our sensitivity analysis reveals that the soft labels generated between the 8th - 20th epochs demonstrate a superior label correction capability, even approaching the model performance trained on the clean dataset. This observation clarifies our earlier question: Even in the relatively early stages of training (after 8 epochs), the soft labels we proposed still exhibit a notable capability of label correction.\nFurthermore, we observe that the label correction capability of soft labels generated after 20 epochs exhibits a decline, particularly pronounced under high noise ratios such as 30%. The underlying rationale is that, as the model undergoes prolonged training on a noisy dataset, the model starts memorizing a huge amount of mislabeled samples, leading to fitting these erroneous labels (Pleiss et al., 2020). Overall, our approach exhibits robustness to hyper-parameter T, particularly within an acceptable range of noise ratios., e.g. 10%, 20%."}, {"title": "A.1 Estimating the Importance Score of Samples from ReID datasets", "content": "All experiments are implemented in PyTorch (Paszke et al., 2019) using the FastReID (He et al., 2020) toolbox\u00b9 on 4 NVIDIA Tesla V100 GPUs. A general workflow for supervised ReID is shown in Fig. 13. In our work, we follow the training procedure of Luo et al. (2019). For feature extraction, we employ a ResNet50 (He et al., 2016) pre-trained on ImageNet (Deng et al., 2009); the model is trained for 12 epochs. All images from Market1501 and MSMT17 are resized to 256 \u00d7 128 pixels, while images from VeRi are resized to 256 \u00d7 256 pixels. During training, we record the logits for each sample after each forward pass. For optimization, we use a combination of the cross-entropy loss and the triplet loss, i.e., $L_{train} = L_{ce} + L_{triplet}$."}]}