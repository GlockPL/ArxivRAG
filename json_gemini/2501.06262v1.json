{"title": "Towards smart and adaptive agents for active sensing on edge devices", "authors": ["Devendra Vyas", "Miguel de Prado", "Tim Verbelen"], "abstract": "TinyML has made deploying deep learning models on low-power edge devices feasible, creating new opportunities for real-time perception in constrained environments. However, the adaptability of such deep learning methods remains limited to data drift adaptation, lacking broader capabilities that account for the environment's underlying dynamics and inherent uncertainty. Deep learning's scaling laws, which counterbalance this limitation by massively up-scaling data and model size, cannot be applied when deploying on the Edge, where deep learning limitations are further amplified as models are scaled down for deployment on resource-constrained devices.\nThis paper presents a smart agentic system capable of performing on-device perception and planning, enabling active sensing on the edge. By incorporating active inference into our solution, our approach extends beyond deep learning capabilities, allowing the system to plan in dynamic environments while operating in real-time with a modest total model size of 2.3 MB. We showcase our proposed system by creating and deploying a saccade agent connected to an IoT camera with pan and tilt capabilities on an NVIDIA Jetson embedded device. The saccade agent controls the camera's field of view following optimal policies derived from the active inference principles, simulating human-like saccadic motion for surveillance and robotics applications.", "sections": [{"title": "I. INTRODUCTION", "content": "The human visual system has a unique ability to focus on key details within complex surroundings, a process known as saccading [1]. This quick and dynamic scanning allows us to gather essential information. Saccading is part of a larger concept known as active (visual) sensing [2], [3], an innate capability that enables organisms to forage for information and dynamically adapt to an evolving environment [4].\nActive sensing is critical in various applications, particularly when the information is unavailable or too vast to process. For instance, in remote sensing for Earth observation [5] or aerial search-and-rescue operations [6], the system must parse vast, detailed scenes, focusing only on critical features, e.g., ice-sea or missing person. Similarly, in sports events, tracking dynamic scenes requires a system to zoom in on players, capturing players' faces or gestures while not missing the play. Other areas, such as smart cities and surveillance systems [7],\ndemand robust monitoring solutions for crowded areas to track movement, anticipate potential issues, and enhance safety. Active sensing becomes even more apparent in robotics, where the agent's actions determine the next observations for the system, driving exploration [8].\nRecent advances in machine learning, particularly in deep learning, have substantially improved sensing accuracy and complexity. However, state-of-the-art deep learning models show limitations in their adaptability [9], i.e., the ongoing accumulation and refinement of knowledge over time. These limitations are further amplified when these models are scaled down for deployment on resource-constrained edge devices, where memory, computational power, and energy efficiency are limited. As a result, true active sensing-requiring both perception and planning-remains challenging to implement on embedded systems.\nActive inference, an approach rooted in the first principles of physics, offers a promising alternative to address these limitations [10]. Emerging as a viable paradigm, active infer-ence grounds learning within probabilistic principles, enabling smart systems, or agents, to model the uncertainty and variabil-ity inherent in dynamic environments, making it well-suited for continual learning and adaptive decision-making [11]. This\nshift represents a move beyond perception-focused AI toward adaptive systems capable of adjusting their actions based on environmental feedback. Thus, agents on the edge provide a powerful framework for real-time perception and planning without dependence on cloud resources, ensuring low-latency responses and enhanced data privacy.\nThis work introduces an integrated system combining deep learning and active inference to realize an intelligent, adaptive, real-time saccade agent on edge devices. Our system leverages a deep learning-based object detection module for initial perception and an active inference planning module to actively sense and adapt to the environment. The saccade agent can observe, plan, and control a camera for strategic informa-tion gathering, demonstrating adaptive decision-making and exploration. Our deployment on an Nvidia Jetson platform showcases the potential for responsive applications in robotics and smart city environments, highlighting the feasibility of edge-based adaptive systems for complex, real-world tasks."}, {"title": "II. RELATED WORK", "content": "We categorize the related work in three main areas:\n\nActive sensing is an essential building block across diverse fields where efficient scanning is needed to locate and focus on critical details. In Earth observation applications, the AutoICE Challenge [5] addresses sea ice detection where maximizing area coverage and detection through adaptive zooming is indispensable for safe navigation. Similarly, active search strategies are also explored in rescue operations [6], stressing techniques like saccading to enhance search efficiency over large areas. For public safety in smart cities, deep learning methods are proposed for people tracking and counting [12], enabling security, crowd management, and urban analytics applications.\n\nTinyML has made deploying ML models on low-power edge devices feasible, bringing opportunities for real-time perception in constrained embedded devices. The edge de-ployment pipeline is summarized in [13], streamlining end-to-end model deployment on embedded platforms to enhance the accessibility of edgeAI applications. A popular example of this process is YOLO [14], an efficient architecture for deploying object detection and tracking at the edge, improving the responsiveness of embedded applications. Recent works have made progress in enabling on-device domain adaptation, adjusting deployed applications to account for data distribution shifts between training and target environment [15], [16]. However, these adaptations remain limited to addressing data drifts and lack broader capabilities for behavioral changes. Our approach extends this by integrating active inference on top of a deep learning module, allowing the system to plan and adapt to the environment accordingly.\n\nProbabilistic computing has shown promise for active sens-ing by optimizing information acquisition in dynamic envi-ronments. Probabilistic principles can be used to maximize information gain through camera adjustment [17], which is valuable in applications like surveillance, sports analysis, and patient monitoring. This is extended to maximize mutual information gain in multi-camera setups, combining objectives like exploration and tracking to enable adaptive, informed scene monitoring [18]. Unlike these methods, we base our probabilistic agent on active inference, grounding our ap-proach in the Free Energy Principle."}, {"title": "III. METHODOLOGY", "content": "To develop an effective active sensing solution, it is essential to consider the unpredictable and dynamic nature of real-world environments. Smart sensors must be able to handle uncertainty and adapt to constant changes. Therefore, any change in the observed environment must influence the policy selection for the following action. For a system to operate autonomously and intelligently, it must be able to adjust its perception and actions in real time without relying on cloud processing. This requirement for on-device adaptation supports faster decision-making and enhances data privacy.\nIn this work, we propose an efficient active sensing agent composed of two modules: i) a deep learning-based percep-tion module and ii) an active inference module that enables planning and control. This architecture combines the precision and scalability of deep learning with active inference's Bayes-optimal control, presenting an adaptable and scalable solution for various resourced-constrained edge applications."}, {"title": "A. Perception", "content": "Deep learning techniques have achieved remarkable success detecting features of interest in images, audio, or textual data [19]. Convolutional neural networks (CNNs) have demon-strated exceptional performance in visual tasks like object detection and segmentation [20]. By employing deep learning for perception, active sensing agents can rapidly process high-dimensional data and identify patterns that provide relevant spatial information.\nRecent advances in transformer architectures and large language models (LLMs) have further expanded the scope of deep learning. Transformers excel in capturing complex dependencies and long-range relationships in data, making them very powerful for tasks requiring a deeper contextual understanding. The self-attention mechanism, being a core component of transformers, enables the models to focus selec-tively on the most relevant aspects of the data, enhancing the agent's ability to perform active sensing by prioritizing critical visual cues.\nHowever, while deep learning and LLMs offer a powerful feature extraction, their static nature limits adaptability when deployed in uncertain or changing environments that can only be counteracted by massively scaling data or model size. Thus, to address this challenge, our approach integrates deep learning\nfor perception but relies on active inference as a much lighter, sample- and parameter-efficient higher-level module, enabling the agent to plan and dynamically adapt based on ongoing observations in real-time."}, {"title": "B. Planning", "content": "Active inference builds on the Free Energy principle, stating that agents minimize surprise or free energy on their generative model. Concretely, the agent's generative model is a joint probability distribution over states $s$ and observations $o$. As inferring hidden states $s$ given some observations $o$ is typically intractable, an approximate posterior $Q(s|o)$ is introduced and optimized by minimizing the free energy $F$ [10]:\n$\\min_{Q(s|o)} F = DKL[Q(s|o)||P(s)] \u2013 EQ(s|o) [log P(o|s)] $ (1)\nHence, the agent strives to give the most accurate predic-tions $P(o|s)$ while minimizing the complexity of the model with respect to the prior $P(s)$. To select actions or policies $\\pi$ for the future $T$, an active inference agent will evaluate and minimize the expected free energy $G$ [10]:\n$G(\\pi) = EQ(o_T|\\pi) [DKL[Q(s_T|o_T, \\pi)||Q(s_T|\\pi)]]$\n- $EQ(o_T|\\pi) [log P(o_T|C)] $ (2)\nThe agent now averages across expected future outcomes $o_T$ and balances the expected information gain (i.e., explo-ration) with the expected utility (i.e., reward) encoded in prior preferences $C$. When both observations $o$ and hidden states $s$ can be modeled as discrete variables with Categorical distributions, optimizing $F$ and $G$ can be done using tractable update rules [11]. Therefore, we convert the outputs of the deep learning perception module into a discrete observation space and use active inference for the action selection of our agent."}, {"title": "IV. SMART SACCADE EDGE AGENT", "content": "Our smart saccade edge agent comprises two modules, as introduced in the previous section, combining deep learn-ing perception capabilities with active inference planning, as shown in Figure 1. Specifically, we employ i) a deep-learning object detection model, offering efficient object and human detection capabilities directly at the edge, and ii) an active inference module that enables adaptive motion control (pan and tilt), allowing the agentic system to dynamically adjust its field of view or track detected entities autonomously. This adaptive behavior enhances the camera's utility as an intelligent surveillance IoT tool or for scene exploration and information foraging in robotic applications."}, {"title": "A. Object detection using YOLOv10", "content": "We chose YOLOv10 [21] from the YOLO family for our perception module due to its strong balance of detection accu-racy and computational efficiency. YOLO models are single-pass object detectors that predict object categories and loca-tions, making them ideal for real-time applications. YOLOv10 consistently demonstrates state-of-the-art performance and re-duced latency across various model scales (N/S/M/L/X).To deploy the YOLOv10 network as efficiently as possible, we export it to ONNX [22]. ONNX has become a standard for neural network representation and exchange and is widely supported by the hardware vendor's software stacks, i.e., inference engines. This positions ONNX as a strong candidate for deployment space exploration and optimizations on a range of embedded devices. Thus, we create an edge-deployment workflow to find the most suitable deployment for YOLOv10.\nThe edge-deployment workflow contains several inference engines, namely ONNX-runtime [23], TFlite [24], and Ten-sorRT [25] that can input an ONNX model and generate an optimized implementation for a given target hardware platform. These frameworks apply several optimizations across the network's graph, e.g., operator fusion, quantization, on the software stack, e.g., algorithm optimization, and leverage parallel hardware acceleration, vectorization, and optimized memory scheduling. The process results in a bespoke network description and a runtime that is ready for deployment on the hardware platform."}, {"title": "B. Planning using Active Inference", "content": "To enable an efficient saccade agent with active inference planning, we define a discrete action, observation, and hidden state space. To this end, we divide the full area the camera can pan and tilt into a discrete grid of $K \\times L$ blocks, see Fig. 2. Given a particular fixation point, the camera's field of view will only span $W \\times H$ blocks, highlighted in blue. For each block, at each timestep, an observation $o_{w,h}$ is a Categorical distribution with three bins, i.e., the block can have no object detected (0 - blue), an object detected (1 - red) or not visible (2 - gray). The confidence of the bounding box outputs of the object detection neural network provides the probability of an object being detected. As state space, we similarly have a state variable $s_{k,l}$ per block, which is Bernoulli, i.e., object not present (0) or present (1). In addition, we also equip the agent with a proprioceptive state $s_p$ and observation, i.e., it observes the fixation point it is currently looking at. We specify the likelihood mapping $A$ which predicts observation $o_{w,h}$ given state $s_{k,l}$ and fixation point $s_p$:\n$A_{w,h,k,l,p} =\\begin{cases}\n0 & \\text{if } s_{k,l} = 0, k \\rightarrow p \\rightsquigarrow w,l \\rightarrow p_h\\\\\n1 & \\text{if } s_{k,l} = 1, k \\rightarrow p \\rightsquigarrow w,l \\rightarrow p_h \\\\\n2 & \\text{otherwise}\n\\end{cases}$ (3)\nwhere $k \\rightarrow p_w$ means \u201cblock $k$ maps to observation $w$ given the agent looks at fixation point $p$\u201d. We currently also assume that objects don't move (a lot) between timesteps\nand use the previous timestep posterior as the current prior, i.e., $P(s_t) = Q(s_{t-1}|o_{t-1})$, but we could expand this with dynamics modeling of the objects as future work.\nThe saccade agent uses the object detections received to perform inference, updating its beliefs about the hidden states. For example: Detecting a \u201cperson\u201d with high confidence would increase the probability assigned to the hidden state \u201cperson present\u201d corresponding to the particular block. The absence of detection, on the other hand, would decrease the probability of that object being present.\nAfter each observation, the agent evaluates possible actions, i.e., the next fixation points, based on their predicted outcomes. Actions are chosen to minimize expected free energy, which combines two key factors:\n1) Expected observations matching prior preferences: This essentially means choosing the most likely actions expected to lead to desired outcomes. For example, if the agent's goal is to locate a specific object, we set a preference $C_{w,h} = 1$, which favors actions that increase the probability of detecting that object in the field of view. Similarly, if we only set $C_{cw,ch} = 1$, with $(C_w, C_h)$ the center coordinates of the camera, this yields a \"tracking\" agent that pan/tilts to keep the object of interest in center view.\n2) Epistemic value: The agent also aims to reduce un-certainty about the hidden states. Actions expected to provide more informative observations about the envi-ronment have higher epistemic value (i.e., information gain in eq. 2). In our model, moving the field of view to previously unobserved blocks will result in a high amount of information gain. Hence, in the absence of objects of interest, the camera will pan and tilt to cover the whole area with as few moves as possible."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "Our experimental setup comprises the following compo-nents:\n1) Tapo Camera: We use an IoT Tapo camera equipped\nwith pan and tilt capabilities, which serve both as the input for the observations, i.e., images/frames, forwarded to the object detector, and the actuator for our saccade\nagent. The agent commands the camera to perform dy-namic adjustments in its field of view based on optimal policies derived by minimizing free energy, simulating a human-like saccadic motion to maintain focus on areas of interest.\n2) Locobot WX250: We also deploy our agent on the Lo-cobot WX250, a mobile robot platform for autonomous mobility. Equipped with a 6-DOF manipulator and pan/tilt camera, the Locobot enables active exploration of the environment. This capability complements the Tapo camera's pan and tilt actions with navigation. The robot's mobility enhances the agent's capacity for active sensing in new environments, allowing it to reposition itself and collect additional perspectives to refine per-ception in complex settings.\n3) Nvidia Jetson Orin Nano NX: Our setup leverages the Nvidia Jetson Orin Nano NX. Equipped with an 8-core ARM Cortex-A78 CPU and a 1024-core Ampere GPU, the Jetson Orin Nano NX represents a powerful edge AI platform designed for accelerated machine learning tasks with up to 100 TOPs of processing capability and 16 GB of memory, running at 25W.\nA pre-trained YOLOv10 model on the COCO dataset [28] is deployed on the Nvidia Jetson Orin Nano NX for real-time detection. The active inference planning module is im-plemented using the PyMDP library [29], with JAX as the backend. The active inference module receives bounding boxes as observation and returns the optimal pan and tilt actions for the IoT or robotic camera. This configuration enables a robust, real-time active sensing solution for robotics and surveillance IoT cameras where detections drive the camera's motion, providing an intelligent and responsive monitoring system without reliance on cloud resources."}, {"title": "B. Results", "content": "Next, we summarize our experimental results:\n1) Perception: We optimized the YOLOv10 model, as introduced in Section IV, by exporting the original Torch model to ONNX and compiling it with different deployment inference engines. Figure 4 depicts the average results of 50-inference runs, with one warm-up sample, while deploying the different implementations on the Nvidia Jetson's CPU, single-and multi-core, and GPU.\nTorch's deployment on a single-core CPU provides a slow inference, running at 0.5 FPS on single-floating-point precision (FP32). When the model is exported to ONNX and compiled with ONNX-runtime and TFLite, the performance (FPS) goes up to 2 and 3 FPS, respectively. We obtain higher gains when parallelizing across all available 8 CPU cores, achieving 4 FPS for Torch and 9 FPS for ONNX-runtime. Finally, when off-loading inference to GPU using TensorRT, designed explicitly for Nvidia Jetson devices, we accomplish an optimized de-ployment with 28 FPS with FP32 precision and 45 FPS when quantizing the model to FP16 as it leverages the native parallel compute units of the device.\nIn future work, we aim to enable TFLite to run on all CPU cores and further quantize the model to INT8, obtaining higher throughput and memory footprint benefits.\n2) Planning: Next, we deploy the active inference planning module, which only contains 1.34K parameters, on the Nvidia Jetson with PyMDP and measure the latency and CPU load during the process. Figure 5 shows the main functions of the planning module, namely, observation to decode and map the object detection bounding boxes to generative model, infer_states which updates the agent's beliefs about the hidden states given the observations, and infer_policies, which defines the set of policies from which the agent will pick the best action. It can be observed that the planning process takes around 231 ms, 4 FPS, on a single-core CPU, achieving effective planning and adaptation to the environment for IoT and robotics saccading applications.\nThe relatively low throughput of the planning model, de-spite its reduced parameter count, can be attributed to the use of PyMDP, which currently lacks optimization for edge deployment. Similar behavior is observed in Figure 4 with Torch. The planning module's CPU usage is slightly over 20%, significantly lower than the highly optimized TFLite or ONNX-runtime implementations, which utilize over 70% of CPU resources. Future work will focus on improving the deployment of Active Inference models by exporting them to\nONNX, enabling more efficient and scalable performance for complex scenarios.\n3) System: Integrating the perception and planning modules produced a highly efficient saccade agent capable of real-time performance. Optimized with TensorRT FP16, the perception module processes frames at a rate of 45 FPS, enabling rapid feature detection. Complementing this, the active inference module operates at 4 FPS for planning and decision-making. The overall system presents, as shown in Table I, a lightweight architecture of just 2.3 million parameters and a minimal mem-ory footprint under 1 GB, including all necessary libraries and runtimes. This configuration balances high-speed perception and adaptive decision-making, allowing the agent to adjust its focus dynamically on constrained resource devices."}, {"title": "C. Discussion", "content": "A key challenge in balancing performance and accuracy on edge devices for active sensing lies in the trade-off between perception and planning modules. While the perception mod-ule shall provide low latency to detect the movement of people or objects in the image, demanding high compute and memory requirements, the active inference planning module should conserve computational and memory resources, running at a lower frequency as camera movements are less frequent and slower in IoT surveillance and robotic applications.\nEdge devices' limited memory and computational power highlight the need for innovative approaches to optimizing the planning and perception modules. Developing smaller and more efficient algorithms that work within these constraints is essential for improving overall system performance. Following this principle, we propose an active inference module that, in contrast to massive foundation LLMs that employ billions of parameters to achieve generalization, our model only contains 1.34K parameters, achieving effective planning and adaptation on resource-constrained devices."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "This work has introduced a smart edge agent composed of a deep learning-based perception module and an active inference planning module for active sensing. The system demonstrates the feasibility of adaptable, on-device surveillance and robotic solutions and their potential to effectively handle real-world challenges.\nOur study highlights the potential of active inference in edge-based systems. Active inference offers a powerful frame-work that accounts for the environment's underlying dynamics and inherent uncertainty, allowing the agent to actively reduce ambiguity in its field of view or explore the environment for new information. This approach not only enhances the functionality but also points to new possibilities for adaptive, resource-efficient decision-making in IoT and edge computing applications.\nIn future work, we aim to optimize the deployment of the active inference module, enhancing its efficiency and scalability to address more complex scenarios. In addition, we plan to compare the proposed system to other state-of-the-art works and benchmark it on popular challenges, such as the Habitat Navigation Challenge. Finally, extending this edge deployment workflow to other active inference-based models could unlock further applications, enabling a new generation of adaptive edge devices capable of context-driven interactions in complex environments."}]}