{"title": "RE-Bench: Evaluating frontier AI R&D capabilities of language model agents against human experts", "authors": ["Hjalmar Wijk", "Tao Lin", "Joel Becker*", "Sami Jawhar", "Neev Parikh", "Thomas Broadley", "Lawrence Chant", "Michael Chen", "Josh Clymer\u2020", "Jai Dhyani\u2020, \u00a7", "Elena Ericheva", "Katharyn Garcia", "Brian Goodrich", "Nikola Jurkovict,", "Megan Kinniment", "Aron Lajko\u2020 \u2021", "Seraphina Nix", "Lucas Sato", "William Saunders\u2020", "Maksym Tarant", "Ben West", "Elizabeth Barnes"], "abstract": "Frontier AI safety policies highlight automation of AI research and development (R&D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-k with varying time budgets and agent designs, and find that the best AI agents achieve a score 4x higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2x the score of the top AI agent when both are given 32 total hours (across different attempts). Qualitatively, we find that modern AI agents possess significant expertise in many ML topics\u2014e.g. an agent wrote a faster custom Triton kernel than any of our human experts' and can generate and test solutions over ten times faster than humans, at much lower cost. We open-source the evaluation environments, human expert data, analysis code and agent trajectories to facilitate future research.", "sections": [{"title": "1 Introduction", "content": "Large language models are increasingly capable of complex programming tasks and are already being used to accelerate AI research, from generating training data to serving as programming tools [1, 2, 3, 4, 5, 6]. As these capabilities grow, there is increasing concern about the potential for AI systems to fully automate frontier AI research and development (henceforth AI R&D) with minimal human involvement [7, 8, 9].2 Multiple AI developers and governments have identified the need for evaluations that can provide advance warning of these capabilities, to allow for the implementation of key security and deployment mitigations [10, 11, 12, 13, 14].\nIn this work, we present RE-Bench, which aims to evaluate whether AI agents can fully automate the work of expert AI R&D researchers, using direct performance comparisons between AI agents and human experts under equivalent conditions and resources. These provide a clear logical connection to automation risk: if AI agents perform significantly worse than human ML experts given equivalent resources and conditions, then the AI agents likely cannot automate these experts' research work.\nTo implement this approach, we present:\n\u2022 Seven novel, hand-crafted evaluation environments covering realistic ML research tasks\n\u2022 Data from 71 attempts by 61 ML experts completing these tasks under equivalent conditions to those used for agents\n\u2022 score@k results (best score obtained across sample of k runs, similar to pass@k [1]) from o1-preview and Claude 3.5 Sonnet in two different scaffolds across different time limits and number of samples.\n\u2022 Qualitative analysis of task characteristics and agent limitations.\nAll of the AI agents we evaluated outperform humans with a 2-hour time budget (Figure 2). However, humans display better returns to increasing time budgets, exceeding the best agent scores when given 8 hours, and continuing to improve rapidly under longer total time budgets when evaluated through best-of-k (i.e., using the result from the most successful of k independent experts) with more samples.\nQualitatively, we find that most agent solutions score close to 0 (i.e. do not improve on the refer-ence solution), but that agents submit new solutions over 10 times faster than human experts, and occasionally find very successful approaches. Notably, both o1-preview and Claude 3.5 Sonnet find distinct solutions to our kernel optimization problem that beat the efforts of all 9 human experts (see Figure 18).\nWe then discuss ways in which this evaluation may over- or under-estimate the capabilities of AI agents relative to humans. For example, our environments are smaller in scope and have clearer objectives and shorter feedback loops compared to real-world research engineering. On the other hand, the cost of generation tokens is very cheap relative to human labor, suggesting that optimization"}, {"title": "2 Background", "content": ""}, {"title": "2.1 Risks from AI R&D Automation", "content": "Recent developments in frontier AI capabilities have prompted leading AI developers and gov-ernments to establish frameworks for identifying and mitigating catastrophic risks from advanced AI systems [12, 11, 10]. An emerging consensus, reflected in commitments like the Bletchley Declaration [15] and the Seoul AI Safety Summit [16], is that systematic evaluation of dangerous capabilities should be central to effective AI governance. Progress has been made in developing safety policies and evaluation frameworks around specific capability risks, particularly those involving potential misuse such as bioweapons development [17] and cyber attacks [18]. However, there exists a distinct challenge in assessing risks downstream of AI systems' capacity for autonomous operation-particularly in the domain of AI research and development itself.\nThe concern is that AI agents automating AI R&D could drastically increase the amount of skilled research labor available to AI developers, which would likely accelerate further progress, potentially"}, {"title": "2.2 Early warning evaluations", "content": "Effective early warning evaluations need to have a low risk of underestimating AI capabilities, while being practical and avoiding early saturation. Some key challenges of achieving this include:\n\u2022 Feasibility: Achieving a high score on the evaluation has to be feasible without demanding extreme levels of capability or guesswork. To be solvable, an evaluation needs to (among other things) provide unambiguous instructions, avoid blocking technical issues, and provide enough time and resources. This has come up as a frequent issue with many complex evaluations; for instance, many SWE-bench [22] problems turned out to be impossible and under-specified on closer inspection. This continued to hold true even after an attempt was made to identify a verified-to-be-feasible subset [23].\n\u2022 Ecological validity: To be able to calibrate risk thresholds without adding unnecessary margin or taking on excessive risk, there needs to be a clear link between evaluation results and risk levels. This is often challenging. For example, it is unclear how scores in MLE-bench translate into the ability of AI agents to automate AI R&D.\n\u2022 Resistance to saturation: In recent years, most AI benchmarks and evaluations have been saturating rapidly [24]. If early warning evaluations saturate long before Al agents pose significant risks, they can no longer serve as an early warning. There are many potential causes of this, including the risk of contamination and the limited scope of many evaluations, and we discuss some approaches to mitigate this in Section 3.\nWe think that more emphasis on faithful human comparisons can help significantly with the first two challenges (Section 3). By comparing to humans under equivalent conditions, we can confirm that our evaluations are feasible and provide crucial grounding for interpreting agent results."}, {"title": "2.3 Related work", "content": "Beyond benchmarking, much prior research has studied the usage of LLMs to assist with diverse aspects of machine learning research, from ideation to experimentation. LLMs are used to generate synthetic data for pretraining LLMs more efficiently and are used as reward models for fine-tuning LLMs with reinforcement learning [3, 4, 34, 35, 36].\nMost relevant to our work are other evaluations and benchmarks attempting to capture challenging engineering, autonomy or research skills in LLMs, many of which have been used as a part of early warning evaluations.\nOf particular relevance for our work is the usage of LLMs for code generation in the domain of machine learning, either as an LLM agent or, more commonly, in a fixed scaffold that does not allow the LLM to choose what tools to use. Focusing in on code generation, LLMs have been used to do autonomous machine learning research, neural architecture search, data science problems, paper reproduction, writing research papers, and reward function design, including preference optimization for LLM fine-tuning [37, 38, 39, 40, 41, 42, 43, 44].\nLLMs for science. Besides machine learning, LLMs have also been applied to R&D in other scientific domains to assist with coding, chemical synthesis, biology, and virtual scientific discovery [26, 45, 46, 47, 32].\nAlso relevant to our work is the field of automated machine learning (AutoML), which also seeks to automate machine learning model development, including hyperparameter and model architecture selection. However, standard AutoML techniques are less applicable to LLMs, due to the greater complexity of LLM training and assessment [48].\nWe focus on AI capabilities in software aspects of AI R&D, as opposed to AI usage in hardware design [49], though progress in hardware development is a driver of AI progress [50]."}, {"title": "3 RE-Bench", "content": ""}, {"title": "3.1 Design goals", "content": "To meet the three challenges to robust and effective early warning evaluations identified in Section 2.2-feasibility, ecological validity, and resistance to saturation\u2014we set out to create a benchmark that directly compared human experts to AI agents in realistic AI R&D environments. To enable us to iterate quickly and gather data at a reasonable cost, we decided on two practical constraints: we evaluate human experts over 8 hours, and make sure all environments can be run with 8 or fewer H100 GPUs. The environments were designed with two key pillars in mind: (i) maximize coverage of key frontier AI R&D challenges, while (ii) ensuring that human experts under the same conditions as the agents can reliably make steady progress on the task, without running into issues or hitting a score ceiling.\nCovering a large variety of the challenges involved in AI R&D reduces the risk of early saturation if Al agents become capable of some but not all of the key sub-skills needed to automate AI R&D. Similarly, ensuring that environments have a high ceiling, beyond what humans can achieve within the time budget, reduces the risk that the evaluation saturates due to both agents and humans reaching"}, {"title": "3.2 Evaluation Description", "content": ""}, {"title": "3.2.1 Environments", "content": "The current RE-Bench suite includes seven hand-crafted novel evaluation environments (see Table 2). Each environment presents a unique ML optimization problem, where achieving a high score generally requires significant experimentation, implementation, and efficient use of compute resources. We designed these environments to have a high ceiling, but still allow significant progress with just 8 hours of time and limited hardware resources (at most 6 H100s).\nEach evaluation environment includes:\n\u2022 A scoring function, which defines the goal of the environment, and can be run by the agent at any time. Each time the scoring function is run a timestamped entry is added to a score log, allowing us to reconstruct progress over time. In almost all environments, the agent can see its score log, and inspect the details of the scoring function, to help it understand its goal.7\n\u2022 A (simple but poorly performing) starting solution which is provided to the agent and demonstrates what a valid solution looks like. This helps clarify the environment setup and allows the agent to get to the challenging research components of the evaluation faster. For example, in \u2018Optimize a Kernel,' the agent is provided with a simple but slow Python solution.\n\u2022 A reference solution, created by the task author, which scores highly. This solution is not provided to the agent, but is used in normalizing scores, and serves as an example of a good solution for researchers.\nFor all environments except 'Scaling Law Experiment' (where only the final submission counts), the agent is assessed by the highest score registered in the score log at the end of the run."}, {"title": "3.2.2 Normalizing scores", "content": "Since each evaluation environment defines a different scoring metric, we use a normalization scheme to make scores comparable between them. For each environment we linearly transform the raw score y to a normalized score $y_n$ so that the starting score $y_s$ (achieved by running the scoring function on the starting solution) is 0, and the reference solution score $y_r$ (achieved by running the scoring function on the reference solution) is 1:\n$y_n = \\frac{y-y_s}{y_r - y_s}$\nIt is possible to achieve scores significantly exceeding the reference solution, though we have not observed any normalized scores above 2. In addition, we assign solutions that score worse than the starting solution a score of 0.\nSince the difficulty of matching the reference solution varies by task, the average normalized score achieved also varies (in experiments with human experts over 8 hours, the average score is between 0.5-1.5 for each environment)."}, {"title": "3.2.3 Creation Process", "content": "We evaluated environments at 3 stages of development.\n\u2022 Specification: Environment ideas were generated through discussions with ML practitioners, reading relevant literature or from past work experiences of the METR staff. To assess the idea, written descriptions of the proposed environments, scoring functions, and starting solutions were evaluated and reworked until we were optimistic that implementation would not be too challenging, and that the specification could do well on our two design pillars."}, {"title": "3.3 Coverage of key AI R&D challenges", "content": "Achieving good coverage of the many challenges involved in advancing AI R&D while ensuring experts can make significant progress in just 8 hours is challenging, since real AI R&D research projects usually take months to complete. This creates a trade-off between different types of difficulty (e.g. engineering complexity, novelty, slow feedback loops), where human experts or AI agents have to split their time between different kinds of difficulty. For instance, an environment that has significant engineering complexity may not leave as much time to come up with and test novel ideas, and thus will not emphasize those skills as much.\nTherefore, we have focused on creating a highly varied suite, covering different pipeline stages (pre-training, post-training, scaffolding), and types of difficulty.\nTo assess our coverage of particular frontier AI R&D tasks, we compare our environments to the key areas identified in Epoch Al's recent survey on AI R&D automation. We observe that our suite is lacking environments focused on organizing distributed training, setting research directions, creating datasets and investigating hardware issues, but otherwise has reasonable coverage. We have also conducted informal interviews with frontier ML researchers at several AI developers, who identified the following additional missing areas:"}, {"title": "3.4 Comparison to human experts", "content": "To facilitate direct comparison with human expert performance and ensure our evaluation achieves our second design pillar-that humans reliably make progress without hitting a ceiling-we collected baselines of our environments with human experts under conditions equivalent to those used for AI agents (see detailed evaluation procedure in Appendix A). For practical reasons, we limited all expert baselines to 8 hours.\nWe selected human experts from three sources: the professional networks of METR staff, applicants to a Machine Learning Research Scientist/Research Engineer position at METR, and graduate student outreach.\nExperts selected from the professional networks of METR staff members all have more than 5 years experience in an area highly relevant to the particular environment they baselined or have recently worked at organizations with strong ML research outputs (Google DeepMind, Google, Anthropic, OpenAI, FAR Labs, Redwood Research). Experts selected via the Research Scientist/Research Engineer hiring process needed to complete a CV and CodeSignal screen, short interview, and short task before being asked to complete a baseline. Graduate students were selected for having relevant advisors, and/or for being in an ML PhD program at University of California Berkeley, Carnegie Mellon University, Stanford University, or Massachusetts Institute of Technology. The average scores achieved were meaningfully different for different sources, with experts from our professional networks scoring 0.96 normalized score on average, while hiring applicants only achieved an average of 0.46.\nWe attempted to match experts to environments where they had relevant experience, whilst trying to maintain a balanced distribution of human expert experience between environments.\nThe results achieved by human experts vary significantly, with some attempts failing to improve on the starting solution and others significantly exceeding scores of the reference solutions. Because of the high variance of human results, and the significant differences between baseliners from different sources, we caution against narrowly focusing AI agent comparisons on the average human result. In fact, when assessing the potential of AI agents to automate researchers at their best, working in their own fields with significant experience and expertise, it is more suitable to compare to the most successful human attempts."}, {"title": "3.5 Validating feasibility and non-saturation", "content": "To assess our second design pillar (that human experts reliably make progress and avoid issues, while not saturating the environments by running into a score ceiling), we investigated 4 metrics.\nIn all of our environments we find that by 8 hours, more than 70% of experts have made some progress, and greater than 80% of experts say that the score they achieved matched their expectation. This indicates that the environments are all possible to make progress on, and that the instructions and feedback provided are sufficient to make the scoring unambiguous."}, {"title": "4 Experiments and results", "content": "For both humans and AI agents, our experiments use the open-source Vivaria platform [51] to set up secure VMs equipped with 20\u201348 vCPUs, 200\u2013400 GB RAM and between 0 and 6 H100s (depending on the requirements specified by the task, see Appendix C for details), which the human or agent can use freely. We evaluate the most recent version of Claude 3.5 Sonnet (claude-3-5-sonnet-20241022 [52] along with ol-preview [53], in two scaffolds. The first is METR's Modular agent, a basic general-purpose scaffold which METR has used for general au-tonomy evaluations before, and the second is AIDE [54], which performed the best in recent studies of MLE-bench [25]. Both scaffoldings were lightly modified to be able to run and ob-serve the output of the scoring function in each environment, and to deal with command timeouts (see Appendix A for details). We also compare to the previous version of Claude 3.5 Sonnet (claude-3-5-sonnet-20240620 in the Modular scaffold. Preliminary studies found that o1-preview performed poorly in the Modular scaffold [55], so we focused our evaluation of this model on the AIDE scaffolding.\nSimilar to MLE-bench [25], we ran the agent with a wall-clock time limit, rather than a token usage limit, to make sure that agents and humans are given an equal amount of GPU hours with which to complete experiments (while this means API latency can affect results, we believe this has only a modest effect since agents spend a majority of the time waiting for computations to finish). We did not count pauses due to API rate limits or other API errors toward the agents' time usage. More details on agent and human evaluation procedures and API latency can be found in Appendix A."}, {"title": "4.1 Agent performance by time spent", "content": "We evaluate how agent scores evolve over time, by averaging over log-linear interpolations of scores achieved by the agent across 3 to 5 8-hour attempts per environment. We observe ol-preview and Claude 3.5 Sonnet making rapid progress initially, but slowing down after the first few hours (see Figure 5), while human experts are slow to start but then make rapid progress.\nWe also note that the AIDE scaffold seems to do better in this setting, continuing to make occasional progress after the Modular agent has stopped."}, {"title": "4.2 Agent performance by number of samples", "content": "Instead of doing one long run, we also consider best-of-k evaluations with shorter time limits for individual attempts, spending the same total time budget. In these experiments we sample k results (with replacement) for each environment, and pick the highest score achieved across all k runs (except for Scaling Law Experiment, where the agent cannot access the score achieved, and instead selects randomly). Note that this is a procedure which AI agents could implement themselves by regularly resetting their environment and deleting their context, and so we believe this is a fair comparison.\nIn Figure 6 we find that the agents in the Modular scaffold do significantly better over many short 30-minute attempts, while agents in the AIDE scaffold do best with fewer 2-hour attempts. We also find that ol-preview in AIDE and Claude 3.5 Sonnet (claude-3-5-sonnet-20241022) in the Modular scaffold achieve the highest scores overall, matching the 36th and 37th percentile of human experts respectively.\nWe also evaluate how the best-of-k score (score@k) changes as you continue to increase the number of samples beyond the 8-hour time budget.\nFigure 2 aggregates these results, comparing the scores achieved with the best allocation of time budget we found for each agent to human performance. We find that overall, humans are slower to get started, but seem to be on a much steeper improvement trajectory, and reach a much higher score with a 32-hour time budget than any agent. For the Modular agents, this plots the time budget"}, {"title": "4.3 Agent performance by environment", "content": "In Figure 9, we compare the highest performing allocation of 32 hours for our two best agents (Claude 3.5 Sonnet in Modular, and o1-preview in AIDE) to human results (the best over four 8-hour runs) to match the agent time budget. We find significant heterogeneity in which environments different agents excel at. The Claude 3.5 Sonnet agent gets close to human performance in 'Finetune GPT-2 for QA' and 'Scaling Law Experiment' environment, which o1-preview does poorly in, while o1-preview exceeds human performance at 'Optimize a Kernel'."}, {"title": "5 Qualitative analysis of human-AI gap", "content": "To better understand under what conditions human experts do better or worse than AI agents and why, we qualitatively analyzed the agent transcripts and solutions."}, {"title": "5.1 Investigating agent successes", "content": "While agents are generally unable to find solutions matching the top human experts, they have some striking successes. For instance, both Claude 3.5 Sonnet (New) and ol-preview in the AIDE scaffold"}, {"title": "5.2 Investigating agent failures", "content": "Despite often having more knowledge about the domain, and rapidly proposing and evaluating many more solutions, agents still do not reach the level of strong human experts in most environments.\nOne reason for this is a lack of variety in the solutions proposed by agents. For example, in \"Restricted Architecture MLM\", the agent attempts to use lightly modified transformer architectures 84% of the time, despite the fact that transformers work very poorly without division and exponentiation. It is possible that future scaffolding improvements could better incentivize variety in agent solutions and achieve higher scores.\nAnother limitation is consistent misunderstandings of instructions, particularly in \u201cRestricted Archi-tecture MLM\" and \"Optimize LLM Foundry\".\nPerhaps the biggest observed limitations are in the area of long-horizon agency: effectively learning from new information, building on previous work and recovering from failures. As seen in Figure 15, AI agents often make stubborn and incorrect assumptions, and do not notice or correctly interpret contradictory information. Figure 13 demonstrates how they struggle to recover from zombie processes (e.g. from timeouts) taking up VRAM. The fact that Modular does better when previous work and context is wiped every 30 minutes is also an indication that the AI agent overall accumulates more issues and false assumptions than useful insights and code to build on over time (AIDE avoids this issue by not accumulating large contexts by design, instead doing tree-search over whole solutions)."}, {"title": "5.3 Characteristics of environments with small human-AI gap", "content": "Based on these observations, we might expect that AI agents would do especially well compared to humans in environments with:\n\u2022 Short and high-fidelity feedback loops, which allow AI agents to lean into their ability to rapidly try many solutions.\n\u2022 Low engineering complexity, so that AI agents can solve the environments in a few steps rather than needing to build complex programs over longer time horizons.\n\u2022 Specialized expertise requirements, which AI agents are more likely to have than most human experts.\n\u2022 Significant noisiness which benefits the many attempts AI agent make over the fewer human expert attempts.\n\u2022 Few surprises and little need for exploration or discovery, which AI agents may struggle with.\nIt is difficult to validate these empirically with so few environments, but in Figure 10 we find that our results are compatible with low engineering complexity contributing to a smaller human-AI gap (R2 = 0.602), while we did not find a clear connection with length of feedback loops or novelty. The two environments that require the most exploration/discovery are \"Optimize LLM Foundry\" (where profiling the script and codebase for inefficiencies is critical) and \u201cFix Embedding\u201d (where the issue with the embeddings has to be investigated), which both have a large gap."}, {"title": "6 Discussion", "content": ""}, {"title": "6.1 Agent performance on RE-Bench may overestimate AI R&D automation capabilities", "content": "We expect the human-AI gap in real-world AI R&D to be much larger than the gap observed on these evaluations, and find it fairly plausible that the first agents that match top human performance in these environments may still be far from capable of AI R&D automation. Specifically, we believe there"}, {"title": "6.2 Agent performance on RE-Bench may underestimate AI R&D automation capabilities", "content": "At the same time, it's possible that AI agents that end up automating substantial fractions of the AI R&D process may nonetheless not exceed or even match human expert performance.\nAgents are substantially cheaper than human experts: On average, our agents use ~29M input tokens and ~499K output tokens in each 8-hour run, at a cost of approximately $123. This is only a small fraction of the approximately $1,855 that we paid our human experts on average (and what a skilled human researcher costs in general), and the cost of agent runs could potentially be much lower if proper prompt caching was used.\nEven if an AI agent takes longer time (or many more attempts) than a human expert to accomplish AI R&D tasks, they might nonetheless end up economically competitive with human researchers due to lower costs.\nDifferent workflows or tooling. The tasks in RE-Bench are designed using the AI R&D workflows of human researchers. It may be possible that AI agents can automate AI research using alternative workflows that do not require performing similar tasks; for example, AI agents are capable of"}, {"title": "6.3 Other Limitations", "content": "There are also other limitations to our results and benchmark, which we hope to address in future work.\nEvaluation design criteria lead to unrepresentative environments: In order to create high-reliability evaluations matching our design criteria we have tried to ensure that instructions and scoring are easily understood, that significant progress is possible within 8 hours, and that all necessary resources are provided. We have also had to select for environments that are easy for us to construct and assess. These constraints make the evaluation environments less representative of real research, where unclear goals, poor instructions, slow feedback and impossible problems are common.\nNoisy agent results due to rare successes: Our results have significant noise from the small number of environments. Since agent scores are heavily skewed toward the right, with most runs scoring 0 and a few achieving very high scores, our estimates are sensitive to sampling noise.\nCost and complexity of evaluation: Running agents for many hours with access to H100 GPUs requires significant infrastructure and a large budget. This makes the evaluation less accessible to researchers, and makes it more challenging to run large-scale experiments comparing many models, scaffoldings and parameters.\nLack of scaffold iteration: Different agent scaffolds or prompts might be able to achieve a much better score on our benchmark in a similar time. In particular, we expect better performance from giving the agents better tools for managing GPU resources, and from approaches that make use of a larger number of tokens, e.g., by exploring many solutions in parallel (though limited compute resources pose limitations for this approach)."}, {"title": "6.4 Conclusion", "content": "In this work, we presented RE-Bench, a suite of environments that measure the ability of AI agents to automate AI R&D tasks. From our human experts baselines, we believe that achieving a good score on RE-Bench is feasible-though there is significant variance in the results. We also find that the environments are challenging, and most are not saturated even by the top human experts in 8 hours. We hope that these properties will allow for useful direct comparisons between human expert performance and AI agent performance on (short and self-contained) AI R&D activities.\nEvaluating some current frontier AI agents we find that, when evaluated through best-of-k with 8 hours of total compute budget, they achieve scores close to the average human expert, demonstrating very impressive capabilities. However a significant gap remains compared to the top human performance in most environments, as seen in Figure 9. Monitoring whether and how quickly AI agents are bridging this gap may help predict the emergence of autonomous AI R&D automation.\nHowever, as discussed above, there are some significant limitations to this work. Future work may want to focus on expanding these evaluations to focus on longer time horizons, more expensive or challenging feedback loops and greater engineering complexity. We expect bridging these gaps while preserving feasibility and direct human comparisons to be a challenging and expensive development direction, but one that might be needed in the near future to avoid having to make compromises on the sensitivity and reliability of AI R&D evaluations."}, {"title": "A Evaluation procedure", "content": ""}, {"title": "A.1 Human evaluation details", "content": "For human expert runs, we did the following:\n\u2022 We had human participants SSH into a VM set up through Vivaria [51].\n\u2022 For practical reasons, each human run was limited to 8 hours.\n\u2022 We gave participants some context on what we were trying to accomplish, and how to ap-proach the environments. In particular we encourage participants to be \"greedy,\" optimizing for improving their score within the first 2 hours, and then taking an iterative approach from there. We find that this encouragement helps avoid overambitious attempts that do not reach a score in time.\n\u2022 Participants were allowed to browse the internet and use LLMs and other tools in solving the problem.\n\u2022 Participants were instructed to make frequent commits to a GitHub repository, which we could inspect to check their progress at different points during the run.\n\u2022 Participants were encouraged to run the scoring function frequently (which would be recorded in a score log).\n\u2022 Participants were encouraged to write timestamped research logs and reach out about about any technical issues or ambiguities during the run. Through inspecting these we can reconstruct the frequency of issues with the environment, and make improvements.\n\u2022 Participants were allowed to take breaks by reporting their start and stop times. We did not carefully monitor whether people spent exactly 8 hours in all cases, though it is unlikely that there are very significant discrepancies, since commit logs roughly match stated start/stop points, and we expect most participants were honest.\n\u2022 After runs were completed, we manually inspect score logs and GitHub submissions for signs of cheating (e.g. using disallowed operations in 'Restricted architecture MLM')."}, {"title": "A.2 Agent evaluation details", "content": "The evaluation process for agents is broadly similar to that for human experts. The main differences are:\n\u2022 While agents had unrestricted internet access, they did not have dedicated tooling for accessing the web, and would have to rely on bash commands like curl. We did not observe agents making use of this, though they did frequently install packages through pip.\n\u2022 We were not able to inspect all agent runs manually, instead we only carefully inspected the 2 best performing runs of each task, and inspected the top performing runs on 'Restricted Architecture MLM' and 'Optimize LLM Foundry' until we knew that the top 15 scores were all valid, since those are the only environments where we are aware of simple ways to cheat.\n\u2022 Instead of submitting code to a GitHub repository and writing a research log, we simply tracked all the input/output to the agent and generated a log through Vivaria.\n\u2022 We paused the agent's clock for time limit purposes whenever there were issues or slowdowns in generating completions, to make sure the agent was given enough time."}, {"title": "A.2.1 Agent scaffolding", "content": "We used two different scaffolds, the Modular agent, a general-purpose agent scaffold which METR has used for general autonomy tasks before, and a lightly adapted version of AIDE.\nModular: This scaffold provides the model with Python and Bash commands, and some very simple context management to keep the input within the context window length of the LM. The same agent was used across the suite\u2014there was no environment-specific prompting or scaffolding.\nThe simple agent scaffolding worked as follows:"}, {"title": "A.2.2 Token usage", "content": "Since agents are limited by wall-clock time rather than tokens, the latency and throughput of generating tokens can determine how many actions the agent can take in a run.\nWe find that the vast majority of runs generate between 20 to 120 output tokens per second on average (see Figure 12), which seems to be within the expected range [56] especially considering that this includes time spent reading the frequently long contexts provided to the agents at each step. We also find that in most runs the agents spend a majority of their time waiting for commands to run on the VM, rather than on generating tokens. This means changes to the API speed would likely only have a minor effect on the pace of progress for these agents."}, {"title": "C Full environment details", "content": "For each of our seven environments, we describe the hardware setup and task instructions provided to the human experts/AI agents, as well as the scoring function, starting solution, and reference solution."}, {"title": "C.1 Optimize LLM Foundry", "content": ""}, {"title": "C.1.1 Hardware setup", "content": "4\u00d7H100, 1\u00d752 core CPU, 400 GB RAM"}, {"title": "C.1.2 Task instructions", "content": ""}, {"title": "C.1.3 Scoring function", "content": "Log time taken by the optimized script to finetune the model on 1000 datapoints. Score is marked as invalid if the L1 norm difference betweten the produced model and the original reference trained model is >5.3."}, {"title": "C.1.4 Starting solution", "content": "The initial solution uses LLM Foundry [59], with unoptimized hyperparameters and settings, along with a finetuning script that takes 272 seconds to run."}, {"title": "C.1.5 Reference solution", "content": "The official solution runs in 94 seconds. The primary edits it makes are:\\"}]}