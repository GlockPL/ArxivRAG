{"title": "Boosting Illuminant Estimation in Deep Color Constancy through Enhancing Brightness Robustness", "authors": ["Mengda Xie", "Chengzhi Zhong", "Yiling He", "Zhan Qin", "Meie Fang"], "abstract": "Color constancy estimates illuminant chromaticity to correct color-biased images. Recently, Deep Neural Network-driven Color Constancy (DNNCC) models have made substantial advancements. Nevertheless, the potential risks in DNNCC due to the vulnerability of deep neural networks have not yet been explored. In this paper, we conduct the first investigation into the impact of a key factor in color constancy\u2014brightness-on DNNCC from a robustness perspective. Our evaluation reveals that several mainstream DNNCC models exhibit high sensitivity to brightness despite their focus on chromaticity estimation. This sheds light on a potential limitation of existing DNNCC models: their sensitivity to brightness may hinder performance given the widespread brightness variations in real-world datasets. From the insights of our analysis, we propose a simple yet effective brightness robustness enhancement strategy for DNNCC models, termed BRE. The core of BRE is built upon the adaptive step-size adversarial brightness augmentation technique, which identifies high-risk brightness variation and generates augmented images via explicit brightness adjustment. Subsequently, BRE develops a brightness-robustness-aware model optimization strategy that integrates adversarial brightness training and brightness contrastive loss, significantly bolstering the brightness robustness of DNNCC models. BRE is hyperparameter-free and can be integrated into existing DNNCC models, without incurring additional overhead during", "sections": [{"title": "1. Introduction", "content": "Humans are capable of perceiving the canonical colors of objects under various illumination conditions, a visual system feature known as color constancy. In computer vision, the color constancy models aim to estimate illuminant color, followed by calibrating color-biased images to ensure accuracy in downstream tasks like object detection and segmentation. In recent years, the rise of the Deep Neural Network (DNN) has driven substantial advancements in color constancy research. However, extensive research Goodfellow et al. (2015); Kurakin et al. (2017); Madry et al. (2018); Carlini and Wagner (2017) has revealed inherent vulnerabilities in DNN, where even minor perturbations can cause significant degradation in performance. Unfortunately, the robustness of DNN-driven Color Constancy (DNNCC) has largely been overlooked, resulting in insufficient exploration of potential perturbations that could limit the performance of DNNCC models.\nDNNCC primarily focuses on the chromaticity of the illuminant rather than its brightness levels. Nonetheless, brightness remains a critical factor in color constancy tasks, providing valuable cues for identifying key features such as light sources, specular reflections, and achromatic surfaces, ultimately enhancing the accuracy of illuminant estimation. For instance, Land et al. Land (1977) proposed the white patch hypothesis, which posits that the maximum response in an image's RGB channels is produced by surfaces that perfectly reflect incident light, thus, the pixels with the highest brightness can be considered indicative of the illuminant's color. Additionally, Bianco et al. Bianco and Cusano (2019) utilized an image-to-image translation network to identify achromatic"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Image Formation for Color Constancy", "content": "Based on the Dichromatic Reflection Model Klinker et al. (1990), the image f(x) = (fr(x), fg(x), fb(x)) is determined by the scene illuminant I(x, \u03bb), the camera sensor response function p(\u03bb) = (pr(\u03bb), pg(\u03bb),\u03c1\u03b2(\u03bb)), the surface reflectance S (x, \u03bb), and the parameters mp(x) and ms(x) which represent body reflection and surface reflection, respectively Gijsenij et al. (2011), such that:\n\nfc(x) = \u0442\u044c(x) \\int_{\\omega} I(x, \u03bb)\u03c1(\u03bb)S (x, \u03bb) d\u03bb + ms(x) \\int_{\\omega} I(x, \u03bb)\u03c1(\u03bb) d\u03bb,\n\nwhere c \u2208 {R, G, B}, A denotes the illuminant wavelength, and w represents the visible wavelength range. The scaling factors mp(x) and ms(x) depend on the viewing angle, illuminant direction, and surface orientation Van De Weijer and Schmid (2006). The objective of the color constancy model is to estimate the illuminant chromaticity from the color-biased image f(x), denoted as e(x):\n\ne(x) = (er(x), eg(x), \u0435\u0432(x)) = \\int_{\\omega} I(x, \u03bb)\u03c1\u03b5(\u03bb) d\u03bb."}, {"title": "2.2. Color Constancy", "content": "Color constancy methods typically assume that the scene is illuminated by a single illuminant. Under this assumption, the objective of color constancy model is to predict the R, G, and B values for each image. In this paper, we follow the assumption and categorize these methods into two groups\u2014DNN-based and non-DNN-based\u2014providing an overview of each."}, {"title": "2.2.1. Non-DNN Methods", "content": "Early non-DNN color constancy methods typically estimate the illuminant through statistical assumptions, including the gray-world Buchsbaum (1980), white-patch Land (1977), and gray-edge assumptions Van De Weijer et al. (2007), among others. These"}, {"title": "2.2.2. DNN-based Methods", "content": "With the advancement of deep learning, Deep Neural Network-driven Color Constancy (DNNCC) has attracted significant attention from researchers. Bianco et al. Bianco et al. (2015) first introduced CNN for illuminant chromaticity estimation. Given the limited number of training samples typically found in color constancy datasets, they divided images into patches to alleviate data insufficiency. Additionally, Yu et al. Yu et al. (2020) proposed a cascading architecture to capture dependencies between light source hypotheses, facilitating a coarse-to-fine estimation. Furthermore, both IGTN Xu et al. (2020) and CLCC Lo et al. (2021) emphasized that the performance of DNNCC is highly sensitive to variations in scene content. They utilize metric learning and contrastive learning, respectively, to focus on illuminant-dependent features.\nGiven the ill-posed nature of the color constancy problem, researchers have explored various strategies to handle the inherent ambiguities. For instance, Shi et al. Shi et al. (2016) developed DS-Net to mitigate ambiguities from unknown reflections and object appearances. Similarly, Song et al. Hu et al. (2017) framed color constancy as a grouped regression problem, generating multiple possible illuminant solutions to tackle these ambiguities. Moreover, Hu et al. Hu et al. (2017) introduced a confidence-weighted"}, {"title": "2.3. Data Augmentation for DNNCC", "content": "Data augmentation diversifies the training data through various transformations. Common geometric transformations, such as random flipping, cropping, and rotating, are also used for color constancy Hu et al. (2017); Yu et al. (2020); Xu et al. (2020); Lo et al. (2021); Hernandez-Juarez et al. (2020). Nevertheless, due to the high sensitivity to color variations, most color transformations, such as channel dropping and swapping, are unsuitable for color constancy. The most prevalent color augmentation technique for color constancy involves linearly scaling the illuminant and adjusting image colors simultaneously Hu et al. (2017), thus creating richer scene-light combinations. A more precise method Lo et al. (2021) identifies 24-color swatch values and uses a transformation matrix to swap illuminants between images. Although the aforementioned data augmentation significantly enhanced the performance of DNNCC, these strategies did not explicitly consider brightness transformations. Our study reveals a general issue with the brightness robustness of DNNCC models. To tackle this challenge, we propose an adversarial brightness augmentation strategy. This approach effectively mitigates performance degradation caused by brightness sensitivity by learning from high-risk brightness transformations, demonstrating its effectiveness as an innovative augmentation method for DNNCC models."}, {"title": "2.4. Adversarial Attack and Defense", "content": "DNN have achieved state-of-the-art performance across a wide range of tasks. However, extensive research has revealed that DNN are highly susceptible to adversarial examples. Szegedy et al. Szegedy et al. (2013) were the first to introduce the concept of adversarial examples, employing the L-BFGS method to generate small perturbations that, when added to clean samples, cause DNN to produce incorrect predictions with high confidence. Subsequently, Goodfellow et al. Goodfellow et al. (2015) proposed the Fast Gradient Sign Method (FGSM) as a more efficient approach to generating"}, {"title": "3. Brightness Robustness Evaluation for DNNCC", "content": "We start with conducting quantitative evaluations of the impact of brightness variation on DNNCC models, providing valuable insights into their sensitivity to such changes. This is accomplished by separately assessing the model's illuminant estimation"}, {"title": "4. Brightness Robustness Enhancement in DNNCC Models", "content": "In light of the observed limitations in brightness variation within the DNNCC model, this section aims to enhance the model's robustness to brightness, thereby improving its illuminant estimation accuracy. The proposed strategy, BRE, consists of two main components: (1) adversarial brightness augmentation, and (2) brightness-robustness-aware model optimization. It is noteworthy that the proposed framework can be seamlessly integrated into existing DNNCC models as an additional module without modifying the architecture, while also introducing no extra computational overhead during the testing phase."}, {"title": "4.1. Adversarial Brightness Augmentation", "content": ""}, {"title": "4.1.1. Brightness Transformation Modeling with Parameterized Brightness Curves", "content": "In this section, we focus on modeling brightness transformations to enable controlled brightness adjustments, which serve as the basis for adversarial brightness augmentation. For the transformation to be practical, it must satisfy two key requirements:"}, {"title": "4.1.2. Adversarial Brightness Parameter Optimization", "content": "The core motivation of the proposed BRE is to enhance the brightness robustness of the DNNCC model by augmenting images with diverse brightness levels. While simple global random intensity adjustments provide a broad range of brightness variations, it lacks model-specific optimization and fails to pinpoint high-risk brightness levels, thereby limiting the improvements in brightness robustness. In contrast, we introduce adversarial brightness parameter optimization for image augmentation, which identifies challenging brightness variation by maximizing the model's loss function. These variations reveal the model's instability under varying lighting conditions, effectively guiding it to learn brightness-robust features. Building upon the parametrized brightness curve F defined in Section 4.1.1, the adversarial brightness parameters * that define the optimization objective for brightness transformations can be mathematically modeled as follows:\n\n0* = arg max J(M(F(f(x), 0); w), l),\n\nwhere M(F(f(x), 0); w) denotes the normalized illuminant estimated by the DNNCC model M for the image adjusted by the parametrized brightness curve F, with x as the clean image, O as the brightness curve parameter, and w as the model parameters. The symbol l denotes the ground truth normalized illuminant. The loss function I, commonly used in color constancy, is defined as the angular error I = 180 arccos (M(F(f(x), 0); w) \u2022l). The Eq. 6 aims to identify adversarial brightness\n\u03c0\nparameters 0* that maximize the prediction error of the DNNCC model on brightness-adjusted images.\nIn white-box scenario, Eq. 6 can be efficiently optimized by the basic iterative method Kurakin et al. (2017):\n\n0* = 0 - \u03b1  where g = VoT(M(F(f(x), 0); w), l).\n\nVe denotes the loss gradient with respect to the parameters 0, and a represents the step size controlling update magnitude during optimization."}, {"title": "4.2. brightness-robustness-aware Model Optimization", "content": "In this section, we focus on enhancing the DNNCC model's robustness to brightness variation through joint adversarial and contrastive loss optimization. Building upon the adversarial brightness augmentation strategy introduced in Section 4.2, the core training process of the DNNCC model can be modeled as a min-max problem Madry et al. (2018):\n\nmin E(x,t)~D[max I (M(F(f(x), 0); w), l)],"}, {"title": "5. Experiment", "content": "In this section, we evaluate the effectiveness of the proposed strategy in improving performance on color constancy datasets by enhancing brightness robustness. The experimental setup is as follows:\nDatasets and Metric. Two publicly available color constancy datasets- ColorChecker Gehler et al. (2008) and Cube+Bani\u0107 et al. (2017)\u2014along with the brightness robustness color constancy dataset introduced in Section3, were utilized. Each image in these datasets is accompanied by ground-truth illuminant chromaticity. The illuminant estimation error for each image of the DNNCC model was evaluated using the angular error function J. Following prior work Hu et al. (2017), we conducted three-fold cross-validation on both the ColorChecker and Cube+ datasets. For the brightness robustness color constancy dataset, we employed the cross-dataset evaluation protocol detailed in Section 3, where models were trained on one dataset and evaluated on another, to quantitatively assess how the proposed BRE improves the DNNCC model's robustness to brightness variations. To evaluate the performance of DNNCC models, we smoothed the test error metrics curve over epochs by applying a moving average with a window size of 500, thereby reducing the influence of outliers. From the resulting smoothed curves, we then calculated five metrics for the angular errors across all images: the median (Med.), mean, trimean (Tri.), best-25% (B25%), and worst-25% (W25%)."}, {"title": "Baselines and Training Settings", "content": "According to the classification of color constancy models in Section 2.2, we selected several non-DNNCC models, including WP Land (1977), SoG Finlayson and Trezzi (2004), GE Van De Weijer et al. (2007), Cheng et al. Cheng et al. (2015), GI Qian et al. (2019), FFCC Barron and Tsai (2017), RCC Li et al. (2023), as well as DNNCC models such as DS-Net Shi et al. (2016), CNN Bianco et al. (2017), FC4 Hu et al. (2017), Quasi-UCC Bianco and Cusano (2019), C4 Yu et al. (2020), C5 Afifi et al. (2021), CLCC Lo et al. (2021), EIL-Net Cun et al. (2022), SS (FC4) Buzzelli and Bianco (2024b), ECF Buzzelli and Bianco (2024a), and GC\u00b3 Zhengguang et al. (2025) as comparison methods. Among them, the recently published or high-performance methods FC4, Quasi-UCC, C4, EIL-Net, ECF, and GC\u00b3 were selected as baseline models for our experiments. The proposed BRE was integrated into all baseline models to assess its effectiveness in improving illuminant estimation performance. Specifically, for Quasi-UCC, the brightness robustness enhancement excludes the contrastive loss component of BRE since brightness variation alters its ground truth.\nAll DNNCC models were configured with a batch size of 16, while all other parameters were kept consistent with the official code. Training was conducted for 4000 epochs on the ColorChecker dataset and 2000 epochs on the Cube+ dataset. During the first 2000 epochs for the ColorChecker dataset and the first 1000 epochs for the Cube+ dataset as well as the brightness robustness dataset, the weights of adversarial loss lady and contrastive loss Actr (as defined in Eq. 12) were set to 1 and 10, respectively. For the remaining epochs across all datasets, these weights were adjusted to 1 and 0.1. The random seed was fixed at 0, and all experiments were conducted on Nvidia GeForce RTX 3090 and A10 GPUs."}, {"title": "5.1. Performance Evaluation of Illuminant Estimation", "content": "In this section, we evaluate how the proposed BRE influences the illuminant estimation performance of DNNCC models on standard color constancy datasets, with the goal of determining whether BRE can address performance limitations caused by insufficient brightness robustness."}, {"title": "6. Conclusion", "content": "In this paper, we identify a potential challenge associated with brightness robustness in color constancy. By constructing a specialized dataset focused on brightness"}]}