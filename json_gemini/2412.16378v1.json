{"title": "REFA: Reference Free Alignment for multi-preference optimization", "authors": ["Taneesh Gupta", "Rahul Madhavan", "Xuchao Zhang", "Chetan Bansal", "Saravan Rajmohan"], "abstract": "We introduce REFA, a family of reference-free alignment methods that optimize over multiple user preferences while enforcing fine-grained length control. Our approach integrates deviation-based weighting to emphasize high-quality responses more strongly, length normalization to prevent trivial short-response solutions, and an EOS-probability regularizer to mitigate dataset-induced brevity biases. Theoretically, we show that under the Uncertainty Reduction with Sequence Length Assertion (URSLA), na\u00efve length normalization can still incentivize length-based shortcuts. By contrast, REFA corrects these subtle incentives, guiding models toward genuinely more informative and higher-quality outputs. Empirically, REFA sets a new state-of-the-art among reference-free alignment methods, producing richer responses aligned more closely with human preferences. Compared to a base supervised fine-tuned (SFT) mistral-7b model that achieves 8.4% length-controlled win rate (LC-WR) and 6.2% win rate (WR), our best REFA configuration attains 21.62% LC-WR and 19.87% WR on the AlpacaEval v2 benchmark. This represents a substantial improvement over both the strongest multi-preference baseline, InfoNCA (16.82% LC-WR, 10.44% WR), and the strongest reference-free baseline, SimPO (20.01% LC-WR, 17.65% WR).", "sections": [{"title": "1 Introduction", "content": "As large language models (LLMs) find widespread deployment in real-world applications\u2014such as customer support, educational tutoring, and medical advice\u2014ensuring that their outputs adhere to human values, quality standards, and usage guidelines has become a fundamental research objective (Ouyang et al., 2022; Liu et al., 2023; Rafailov et al., 2024). Alignment methods, notably Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020) and Direct Preference Optimization (DPO) (Rafailov et al., 2024; Xu et al., 2024b), have proven essential for guiding models toward producing safer, more helpful content that meets user expectations.\nWhile many alignment techniques rely on a reference model (Ouyang et al., 2022; Christiano et al., 2017), there is growing interest in reference-free alignment approaches (Cui et al., 2023; Liu et al., 2024; Zhou et al., 2023; Wang et al., 2024). By directly optimizing from scalar rewards or scores, these methods sidestep complex ratio modeling and exploit richer information from datasets that annotate multiple positive and negative responses per query. Utilizing such diverse signals has been shown to improve downstream performance (Cui et al., 2023; Yang et al., 2024), indicating that reference-free alignment can offer more robust and transferable improvements in language model capabilities.\nTraditional alignment formulations often adopt a pairwise stance, contrasting a single preferred response against a single dispreferred one (Rafailov et al., 2024; Xu et al., 2024b). However, real-world user preferences are more nuanced. Users may have multiple acceptable responses to a given query, as well as several suboptimal or even unacceptable ones. This observation motivates multi-preference optimization, an approach that jointly considers entire sets of positive and negative responses (Gupta et al., 2024; Chen et al., 2024), enabling models to align with a richer, more representative distribution of desirable outputs.\nYet, multi-preference optimization introduces new challenges, particularly the model's tendency to exploit dataset-induced biases toward brevity. Humans often produce concise training examples due to cognitive and economic constraints, inadvertently suggesting shorter answers as the norm. In contrast, users at inference time frequently prefer richer, more detailed explanations\u2014especially for complex tasks requiring extended reasoning. Without careful design, a model may prematurely terminate responses, undermining genuine quality improvements (Holtzman et al., 2019; Wolf et al., 2023). Controlling length via length-regularization is thus critical. Merely normalizing by length, \u00e0 la SIMPO(Meng et al., 2024) or RRHF(Yuan et al., 2023), is insufficient. Instead, we must realign training incentives to produce elaborated outputs rather than taking shortcuts through brevity, ensuring that improvements arise from genuinely better-quality responses rather than truncated answers exploiting training biases.\nIn this work, we propose REFA, a family of reference-free multi-preference alignment strategies that tackle these issues. Our approach integrates deviation-based weighting, ensuring that responses far above or below average quality receive proportionally more influence (Gupta et al., 2024), and introduces length normalization coupled with an EOS-probability regularizer. These techniques mitigate dataset-induced biases and encourage models to produce the richer, detailed answers often required in real-world scenarios. Moreover, we provide theoretical insights and gradient-level analyses of our loss function with respect to both response probabilities and length, offering guarantees on the fixed points of the INFONCA loss and our REFA objective. By combining strong theoretical grounding with empirical validation on benchmark datasets, we demonstrate that our REFA-dynamic formulation achieves state-of-the-art results, advancing the alignment techniques toward more robust, controllable, and preference-aware LLM training."}, {"title": "1.1 Our Contributions", "content": "In this work, we develop REFA, a family of reference-free, multi-preference alignment methods that integrate length normalization, deviation-based weighting, and an EOS-probability regularizer to encourage richer responses. Our key contributions include:\n1.  Algorithmic Novelty: We introduce a new REFA-dynamic variant that leverages deviation-based weighting and a carefully designed length-regularizer. This combination resolves the subtle incentives that previous methods faced, ensuring that improvements in alignment arise from genuinely higher-quality responses rather than trivial length manipulations.\n2.  Theoretical Insights: We formalize the Uncertainty Reduction with Sequence Length Assertion (URSLA), a conjecture stating that, for certain subsets of responses, increasing sequence length reduces per-token uncertainty. Under URSLA, we prove that na\u00efve length normalization can inadvertently incentivize shorter negative responses, illuminating why additional constraints (like EOS probability control) are essential. Furthermore, we offer theoretical insights relating to the stationary point, and gradient updates of the various preference-optimization algorithms.\n3.  State-of-the-art Results: Our experiments on the AlpacaEval v2 benchmark demonstrate that REFA significantly outperforms strong baselines. Compared to the strongest multi-preference baseline (InfoNCA) and the best reference-free baseline (SimPO), our REFA-dynamic (p=2) formulation achieves the highest length-controlled and raw win rates (21.62% LC-WR vs. 20.01% LC-WR for SimPO, and 19.87% WR vs. 17.65% WR for SimPO), validating the effectiveness of our approach in producing richer, higher-quality responses."}, {"title": "2 Related Work", "content": "Preference Optimization for Alignment. Early alignment efforts focussed on RLHF through learning an intermediate reward model using reinforcement learning algorithms like PPO (Schulman et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). While effective, this can be computationally expensive and noisy due to the intermediate reward estimation. Recent approaches, such as Direct Preference Optimization (DPO) (Rafailov et al., 2024), streamline alignment by directly optimizing a contrastive loss over pairs of preferred and dispreferred responses, bypassing explicit reward models. Subsequent works have extended this idea, exploring variants like IPO (Azar et al., 2023), CPO (Xu et al., 2024a), ORPO (Hong et al., 2024a), and R-DPO (Park et al., 2024), each offering alternative formulations or regularizations. Additionally, methods like RRHF (Yuan et al., 2023), SLIC-HF (Zhao et al., 2023a), KTO (Ethayarajh et al., 2024), and SimPO (Meng et al., 2024) propose diverse preference optimization objectives, some relying on reference models and others operating reference-free. A recent unifying framework by Tang et al. (2024) shows that many such offline algorithms can be viewed as special cases of a generalized preference optimization approach.\nReference-Free Alignment. While many alignment techniques rely on a reference model (Ouyang et al., 2022), there is growing interest in reference-free methods that directly optimize from scalar feedback. Reference-free approaches avoid the complexity of ratio modeling and can leverage richer data containing multiple candidate responses per query (Cui et al., 2023; Liu et al., 2024; Zhou et al., 2023; Wang et al., 2024). Recently, objectives like SimPO (Meng et al., 2024) have shown that focusing on the log-probability of sequences without a separate reference model can achieve strong performance, making the training pipeline simpler and potentially more robust.\nMulti-Preference Optimization. The development of multi-preference and reference-free approaches is facilitated by datasets like the UltraFeedback dataset (Cui et al., 2023) that provide scalar rewards corresponding to multiple responses related to a single query. Methods like InfoNCA (Chen et al., 2024) and SWEPO (Gupta et al., 2024) use such datasets to generalize beyond pairwise comparisons, simultaneously considering sets of positive and negative responses. Multi-preference objectives better approximate the true preference landscape, better enabling models to estimate the accepted and rejected distributions. While InfoNCA leverages a noise-contrastive framework to align responses according to scalar rewards, SWEPO introduces deviation-based weighting to emphasize highly positive or highly negative deviations more strongly, thus improving alignment quality.\nIn summary, our work extends the literature on reference-free, multi-preference optimization methods. By integrating length normalization, deviation-based weighting, and EOS probability regularization, we build on principles introduced by methods like InfoNCA and SWEPO to address subtle length-based incentives, ultimately producing richer and more human-aligned responses."}, {"title": "3 Notation and Preliminaries", "content": "In this section, we establish the notation and fundamental concepts used throughout the paper. We consider a reference-free multi-preference alignment problem where each query can be paired with multiple responses of varying quality. The objective is to shape the policy distribution so that it assigns higher probability mass to higher-quality responses, while pushing the probabilities of lower-quality ones closer to zero.\nQueries and Responses Let $\\mathcal{X}$ denote the space of possible queries. For each query $x \\in \\mathcal{X}$, consider a set of candidate responses $\\{y_i\\}_{i=1}^K \\subseteq \\mathcal{Y}$. Each response $y_i$ is a sequence of tokens whose length is denoted by $|y_i|$. This length $|y_i|$ plays a crucial role in later sections, where we introduce length normalization to prevent trivial short-response biases.\nPolicy Model We consider a policy model $\\pi_{\\theta}(y | x)$, parameterized by $\\theta$, that assigns probabilities to candidate responses for each query. In particular, $\\pi_{\\theta}(y | x) = \\exp(\\log \\pi_{\\theta}(y | x))$, where $\\log \\pi_{\\theta}(y | x)$ is the log-probability of $y$. Throughout this work, we focus on a reference-free scenario, meaning we do not rely on a fixed baseline distribution $\\mu(y | x)$."}, {"title": "4 Methodology: Deriving the REFA Loss Function", "content": "In this section, we present a step-by-step derivation of our REFA loss function, starting from the InfoNCA formulation, removing reference models, introducing multi-preference sets, incorporating deviation-based weighting, length normalization, and finally addressing length-related biases through an EOS-regularization mechanism. For detailed proofs, intermediate steps, and nuanced discussions, we refer the reader to the appendices: Appendix A through Appendix F.4."}, {"title": "4.1 Preliminary Setup and Notation", "content": "We consider a query $x \\in \\mathcal{X}$ and a set of candidate responses $\\{y_i\\}_{i=1}^K \\subseteq \\mathcal{Y}$. Each response $y$ is a sequence of tokens, and we define the full context (e.g., prompt or conversation history) under"}, {"title": "4.2 From InfoNCA to Reference-Free Alignment", "content": "InfoNCA with a Reference Model: The InfoNCA approach (Chen et al., 2024) originally assumes access to a fixed reference model $\\mu(y | x)$ and aligns the ratio $\\frac{\\pi_{\\theta}(y|x)}{\\mu(y|x)}$ to a distribution induced by rewards. In its original form, given a target distribution $p^{\\text{target}}$ derived via a softmax over rewards (Appendix A), the InfoNCA loss can be expressed as:\n$L_{\\text{InfoNCA}}(\\theta) = -\\sum_{i=1}^K p_i^{\\text{target}} \\log p_i^{\\text{model}}$, where $p_i^{\\text{model}} = \\frac{\\frac{\\pi_{\\theta}(y_i|x)}{\\mu(y_i|x)}}{\\sum_{j=1}^K \\frac{\\pi_{\\theta}(y_j|x)}{\\mu(y_j|x)}}$ (4)\nRemoving the Reference Model: Eliminating the reference distribution $\\mu(y | x)$ yields a simpler, reference-free setting (Appendix B). Without $\\mu$, the model distribution directly becomes:\n$p^{\\text{model}}_i = \\frac{\\pi_{\\theta}(y_i | x)}{\\sum_{j=1}^K \\pi_{\\theta}(y_j | x)}$ (5)\nHowever, this direct approach has several issues (Appendix C): it can promote low-quality responses, create contradictory optimization signals among high-quality responses, and fail to distinguish subtle differences in reward quality."}, {"title": "4.3 Multi-Preference Alignment via Positive and Negative Sets", "content": "To address these weaknesses, we consider a multi-preference formulation (Appendix D): rather than focusing on pairwise or single-response comparisons, we split responses into sets $Y^+$ and $Y^-$. We want to collectively increase probabilities for all $y \\in Y^+$ and decrease probabilities for all $y \\in Y^-$. A starting point is:\n$\\mathcal{L}(\\theta) = - \\log \\frac{\\sum_{y \\in Y^+} \\pi_{\\theta}(y|x)}{\\sum_{y \\in Y^+ \\cup Y^-} \\pi_{\\theta}(y|x)}.$ (6)\nThis ensures that to decrease $\\mathcal{L}(\\theta)$, the model must raise the relative mass of $Y^+$ and reduce that of $Y^-$. Yet, this basic version treats all responses in $Y^+$ or $Y^-$ equally, ignoring finer reward gradations."}, {"title": "4.4 Deviation-Based Weighting and Hyperparameters", "content": "To incorporate the full spectrum of quality differences, we introduce a deviation-based weighting (Appendix C) defined by:\n$\\Delta S_y = r_y - \\bar{r}$, (7)\n$w_y = e^{\\alpha \\Delta S_y}$, (8)"}, {"title": "4.5 Length Normalization and EOS Probability Regularization", "content": "A key insight from Appendix F is that without length normalization, shorter responses gain an unfair advantage as they have fewer tokens to predict. To counter this, we replace $\\log \\pi_{\\theta}(y | x)$ with its length-normalized counterpart $\\overline{\\log \\pi_{\\theta}(y | x)}$ as defined in Eq. equation 3:\n$\\mathcal{L}(\\theta) = -\\log \\frac{\\sum_{y \\in Y^+} e^{\\beta(\\overline{\\log \\pi_{\\theta}(y|x)} + \\alpha \\Delta S_y)}}{\\sum_{y \\in Y^+} e^{\\beta(\\overline{\\log \\pi_{\\theta}(y|x)} + \\alpha \\Delta S_y)} + \\gamma \\sum_{y \\in Y^-} e^{\\beta(\\overline{\\log \\pi_{\\theta}(y|x)} + \\alpha \\Delta S_y)} }.$ (10)\nFinally, due to dataset-induced biases and the risk of models exploiting length shortcuts (Appendix F.4), we introduce an EOS-regularization term $R(\\theta)$. This term encourages or discourages early termination, ensuring that the model does not trivially shorten responses to reduce loss. The final REFA objective is:\n$\\mathcal{L}_{\\text{REFA}}(\\theta) = - \\log \\frac{\\sum_{y \\in Y^+} e^{\\beta(\\overline{\\log \\pi_{\\theta}(y|x)} + \\alpha \\Delta S_y)}}{\\sum_{y \\in Y^+} e^{\\beta(\\overline{\\log \\pi_{\\theta}(y|x)} + \\alpha \\Delta S_y)} + \\gamma \\sum_{y \\in Y^-} e^{\\beta(\\overline{\\log \\pi_{\\theta}(y|x)} + \\alpha \\Delta S_y)}} + R(\\theta)$. (11)"}, {"title": "5 Algorithm", "content": "The following procedure outlines how to train a model with the proposed REFA loss. We assume a dataset of queries and associated responses with scalar rewards, as well as known hyperparameters.\nExplanation: The algorithm implements a contrastive learning approach for response alignment. At each iteration, it samples query-response pairs (line 2) and computes mean rewards (line 4) to partition responses into above-mean ($Y^+$) and below-mean ($Y^-$) sets (line 5). Length-normalized log-probabilities (line 6) prevent degenerate solutions, while deviation-based weights (line 7) sharpen the preference signal.\nThe core learning objective aggregates weighted probabilities through partition functions $A$ and $B$ (lines 8-9), with an EOS regularizer $R(\\theta)$ (line 10) to control response termination. The final loss (line 11) drives gradient updates to optimize the model's policy distribution (lines 13-14)."}, {"title": "6 Theoretical Analysis", "content": "We now highlight the central theoretical insights derived from our reference-free multi-preference alignment framework. Starting from a reference-free version of the InfoNCA objective, we identify fundamental issues\u2014such as encouraging low-quality responses and creating contradictory optimization signals\u2014when dealing with multiple responses of varying quality. We then show how length normalization and additional regularizers (like EOS probability control) address subtle incentives that arise due to the relationship between sequence length and token-level uncertainty."}, {"title": "6.1 Single-Term Objectives and Their Gradients", "content": "In a reference-free setting, the InfoNCA loss reduces to a cross-entropy objective over multiple candidate responses. However, analyzing it at the granularity of single responses (i.e., single-term objectives) reveals how the model's gradient signals can inadvertently promote low-quality outputs.\nLemma 1 (Gradient of a Single-Term Objective). (Originally stated in Appendix B.2) Let $l_i(\\theta) := -p_i^{\\text{target}} \\log p_i^{\\text{model}}$, where\n$p_i^{\\text{model}} = \\frac{\\pi_{\\theta}(y_i|x)}{\\sum_{j=1}^K \\pi_{\\theta}(y_j|x)}$\nand $p_i^{\\text{target}} > 0$. Suppose $\\pi_{\\theta}(y_j|x) > 0$ for all $j$. Then:\n$\\frac{\\partial l_i(\\theta)}{\\partial \\pi_{\\theta}(y_i|x)} = -p_i^{\\text{target}} \\cdot \\Biggl( \\frac{1}{\\pi_{\\theta}(y_i|x)} - \\frac{1}{\\sum_j \\pi_{\\theta}(y_j|x)} \\Biggl),$\nand for each $j \\neq i$,\n$\\frac{\\partial l_i(\\theta)}{\\partial \\pi_{\\theta}(y_j|x)} = p_i^{\\text{target}} \\cdot \\frac{1}{\\sum_k \\pi_{\\theta}(y_k|x)}.$\nProof Sketch: 1. Express $l_i(\\theta)$ in terms of $\\log \\pi_{\\theta}(y_i|x)$ and $\\log(\\sum_j \\pi_{\\theta}(y_j|x))$. 2. Differentiate w.r.t. $\\pi_{\\theta}(y_i|x)$ using the chain rule: $\\frac{\\partial \\pi_{\\theta}(y_i|x)}{\\partial \\log \\pi_{\\theta}(y_i|x)} = 1/\\pi_{\\theta}(y_i|x)$ and $\\frac{\\partial \\pi_{\\theta}(y_i|x)}{\\partial \\log(\\sum_j \\pi_{\\theta}(y_j|x))} = 1/\\sum_j \\pi_{\\theta}(y_j|x)$. 3. For $j \\neq i$, only the denominator $\\sum_j \\pi_{\\theta}(y_j|x)$ affects the gradient. (For full proof see Appendix B.2.)\nLemma 2 (Directional Influence of a Single-Term Objective). (Originally stated in Appendix B.2) Under the same assumptions as Lemma 1, minimizing $l_i(\\theta)$ encourages increasing $\\pi_{\\theta}(y_i|x)$ (especially if initially small) and decreasing $\\pi_{\\theta}(y_j|x)$ for $j \\neq i$.\nProof Sketch: 1. From Lemma 1, note that if $\\pi_{\\theta}(y_i|x)$ is small, then $\\frac{\\partial l_i}{\\partial \\pi_{\\theta}(y_i|x)} < 0$, so increasing $\\pi_{\\theta}(y_i|x)$ reduces $l_i(\\theta)$. 2. For $j \\neq i$, $\\frac{\\partial l_i}{\\partial \\pi_{\\theta}(y_j|x)} > 0$, so decreasing $\\pi_{\\theta}(y_j|x)$ reduces $l_i(\\theta)$. (For full proof see Appendix B.2.)\nImplication: These lemmas show that when optimizing each response independently, even low-quality responses receive upward pressure if their target weight $p_i^{\\text{target}}$ is nonzero. Moreover, multiple good responses compete with each other, causing contradictory optimization signals."}, {"title": "6.2 Length as a Shortcut: Probability Inflation and Length-Induced Certainty", "content": "Without length normalization, the model can exploit shorter sequences to artificially inflate their probabilities. Even with length normalization, subtle effects arise from the interplay between sequence length and token-level uncertainty.\nLemma 3 (Length-Induced Probability Inflation). (Originally stated in Appendix E, Lemma: Length-Induced Probability Inflation) Shorter responses typically accumulate less negative log-probability because they have fewer tokens. Thus, holding per-token quality constant, shorter responses achieve higher overall probability, incentivizing the model to prefer brevity over meaningful improvement.\nProof Sketch: 1. Consider two responses $y_1, y_2$ with similar per-token likelihoods. 2. If $|y_1| < |y_2|$, then $\\log \\pi_{\\theta}(y_1|x) = -|y_1|c$ and $\\log \\pi_{\\theta}(y_2|x) = -|y_2|c$ for some average cost $c > 0$. 3. Since $|y_1| < |y_2|$, $e^{-|y_1|c} > e^{-|y_2|c}$, so $\\pi_{\\theta}(y_1|x) > \\pi_{\\theta}(y_2|x)$. (For full proof see Appendix E.)"}, {"title": "6.3 Uncertainty Reduction with Sequence Length Assertion (URSLA)", "content": "We introduce a conjecture that articulates how model uncertainty changes with response length. Let $y$ denote a response (sequence of tokens) and $LP(y)$ be the average per-token negative log-probability of $y$ under the model $\\pi_{\\theta}$, i.e. $LP(y) = P_\\theta(y)/len(y)$. Then, lower $LP(y)$ corresponds to higher model confidence at the per-token level.\nConjecture 1 (Uncertainty Reduction with Sequence Length Assertion (URSLA)). There exists a non-empty subset of responses $\\mathcal{Y}$ such that for all $y \\in \\mathcal{Y}$, increasing the length of $y$ reduces the average per-token negative log-probability $LP(y)$. Formally:\n$\\frac{\\partial LP(y)}{\\partial len(y)} < 0$ for all $y \\in \\mathcal{Y}$.\nIn other words, for these responses, as sequences grow longer, they become progressively easier for the model to predict, lowering the per-token uncertainty.\nLimitations of Conjecture: This conjecture aligns with empirical observations: once the model establishes a coherent semantic trajectory, subsequent tokens become more predictable. However, URSLA may fail in scenarios where the model encounters domain shifts, complex reasoning, or adversarial inputs. Domain-specific empirical verification is needed to confirm when this conjecture holds. (See Appendix F for a detailed discussion.)"}, {"title": "6.4 Consequences of URSLA for Negative Responses", "content": "Under many preference optimization objectives, certain responses\u2014labeled as \"negative\"\u2014are penalized if they have too high a probability. To reduce their probability, one can equivalently seek to increase $LP(y)$ for these responses. If URSLA holds, longer sequences are inherently more predictable (lower $LP(y)$), so to raise $LP(y)$ for a negative response, the model may choose to shorten it.\nLemma 4 (Length Reduction for Negative Responses). (Originally stated in Appendix F, Lemma: Length Reduction for Negative Responses) Suppose we have a loss function $\\mathcal{L}(\\theta)$ that penalizes high-probability negative responses. Increasing the average log-perplexity $LP(y)$ for these negative responses decreases $\\mathcal{L}(\\theta)$. Under the URSLA Conjecture 1, because longer sequences have lower $LP(y)$, the model can increase $LP(y)$ by shortening these responses. Formally, if:\n$\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial len(y)} > 0$ for all $y \\in \\mathcal{Y}^-$, (12)\nthen minimizing $\\mathcal{L}(\\theta)$ encourages decreasing $len(y)$ for negative responses $y \\in \\mathcal{Y}^-$.\nProof (Sketch): If the objective penalizes a negative response $y$, it aims to lower $\\pi_{\\theta}(y|x)$. Equivalently, it must increase $LP(y)$. Under URSLA, $LP(y)$ decreases with length, so making $y$ shorter increases $LP(y)$, aiding in lowering $\\log \\pi_{\\theta}(y|x)$, which is the expression in the second term in the denominator of the loss function. Thus, reducing length is a shortcut to achieving the training objective, which implies a positive partial derivative of $\\mathcal{L}(\\theta)$ w.r.t. response length for negative responses. This creates an incentive to shorten negative responses to reduce the loss."}, {"title": "6.5 Summary of Theoretical Insights", "content": "The above results and observations from the appendices collectively show why simple reference-free InfoNCA-style objectives are not enough. Without careful design, these objectives:\n1.  Promote low-quality responses as long as they are not fully excluded.\n2.  Create competition among responses, delaying convergence to a collectively beneficial solution.\n3.  Incentivize trivial length-based shortcuts that do not yield genuinely higher-quality answers.\n4.  Under length normalization, subtle length-uncertainty dynamics still allow the model to shortcut improvements by manipulating response length.\nThese insights provide a theoretical foundation for integrating additional strategies\u2014such as deviation-based weighting, multi-preference sets, length normalization, and EOS-probability control\u2014into the training objective. By doing so, we can guide models toward producing richer, more accurate, and user-aligned responses, overcoming the subtle incentive structures uncovered by these lemmas and the length-certainty observation."}, {"title": "7 Experimental Setup", "content": "Model and Training Settings: For our experiments, we utilized the Ultrafeedback Dataset Cui et al. (2023), an instruction-following benchmark annotated by GPT-4. This dataset consists of approximately 64,000 instructions, each paired with four responses generated by different language models. GPT-4 assigned scalar rewards on a 0-to-10 scale for each response, which prior research has shown to correlate strongly with human annotations. This establishes GPT-4 ratings as a reliable and cost-efficient alternative to manual feedback.\nIn our broader framework, we first trained a base model (mistralai/Mistral-7B-v0.1) on the UltraChat-200k dataset to obtain an SFT model. This SFT model, trained on open-source data, provides a transparent starting point. Subsequently, we refined the model by performing preference optimization on the UltraFeedback dataset. Once fine-tuned, the model was used for alignment. This two-step process ensures the model is well-prepared for tasks.\nIn our experiments, we observed that tuning hyperparameters is critical for optimizing the performance of all offline preference optimization algorithms, including DPO, SimPO, REFA-InfoNCA, REFA-1-vs-all and REFA-dynamic. Carefully selecting hyperparameter values significantly impacts the effectiveness of these methods across various datasets.\nFor REFA-InfoNCA, we found that setting the beta parameter in the range of 2.0 to 2.5 consistently yields strong performance. Similarly, for REFA-1-vs-all and REFA-dynamic, optimal results were achieved with beta values between 2.0 and 4.0, while tuning the gamma parameter within the range of 0.7 to 1.4 further improved performance. These observations highlight the importance of systematic hyperparameter tuning to achieve reliable outcomes across diverse datasets.\nEvaluation Benchmarks We evaluate our models using three widely recognized open-ended instruction-following benchmarks: MT-Bench, AlpacaEval 2, AlpacaEval and Arena-Hard v0.1. These benchmarks are commonly used in the community to assess the conversational versatility of models across a diverse range of queries.\nAlpacaEval 2 comprises 805 questions sourced from five datasets, while MT-Bench spans eight categories with a total of 80 questions. The recently introduced Arena-Hard builds upon MT-Bench,"}, {"title": "8 Experimental Results", "content": "REFA-dynamic outperforms existing preference baselines: We conducted an extensive comparison of the REFA-Dynamic framework against various preference optimization methods using the Mistral-7B model, as summarized in Table 7. The results demonstrate that REFA-Dynamic consistently outperforms all baselines across the primary evaluation metrics. Specifically, REFA-Dynamic achieves improvement of 8% on LC-WR and 12.57% on WR over SimPO, which is the second-best baseline. Compared to other competitive approaches such as R-DPO and InfoNCA, REFA-Dynamic delivers significant gains, showcasing its ability to handle diverse preferences. These results highlight the efficacy of REFA-Dynamic's reference-free optimization in addressing alignment biases and enhancing response quality, reinforcing its state-of-the-art performance in preference optimization."}, {"title": "Effect of EOS Probability Regularization on REFA-dynamic", "content": "We analyzed the effect of the EOS probability regularization term $\\lambda$ on the performance of REFA-dynamic by comparing configurations with and without regularization, as shown in Table 8. The results reveal that incorporating EOS regularization ($\\lambda$=0) significantly improves model performance across all metrics. For instance, with p=2, enabling EOS regularization increases LC-WR, WR, and response length significantly, compared to without regularization. A similar trend is observed for p=1. These findings indicate that EOS regularization effectively balances response length control and alignment, ensuring that the model generates coherent and appropriately extended outputs. Our theoretical analysis in (Appendix F.4) supports this by demonstrating that regularizing the EOS probability stabilizes sequence generation, reducing overly terse or excessively verbose responses, thereby enhancing overall preference alignment."}, {"title": "Effect of oversampling term y on REFA-dynamic", "content": "We analyzed the impact of the oversampling term $\\gamma$ on the performance of REFA-Dynamic by evaluating different configurations of $\\gamma$ and preference strength p, as shown in Table 8. The results indicate that incorporating the oversampling term ($\\gamma\\neq 0$) consistently enhances the model's alignment across all evaluation metrics. For instance, with p = 2, REFA-Dynamic achieves the improvement of 15.98% (LC-WR), 11.88% (WR on AlpacaEval2), and 1.65% (WR on AlpacaEval1) when $\\gamma$ is non-zero over the setting $\\gamma = 0$. Similar trends are observed for p = 1, where the inclusion of $\\gamma$ shows improvement of 16.76% (LC-WR), 11.62% (WR on AlpacaEval2). These findings highlight the importance of the oversampling term in enhancing preference adaptation and alignment."}, {"title": "REFA-InfoNCA (1-vs-all) vs REFA-InfoNCA (dynamic) vs REFA-InfoNCA", "content": "We evaluated the performance of REFA-InfoNCA and its variants REFA-InfoNCA (dynamic) and REFA-InfoNCA (1-vs-all) to assess the impact of dynamic adaptation and 1-vs-all contrastive optimization, as summarized in Table 7. The results demonstrate that REFA-InfoNCA (1-vs-all) outperforms both REFA-InfoNCA (dynamic) and the baseline REFA-InfoNCA across all metrics. Specifically, REFA-InfoNCA (1-vs-all) achieves slight improvement (nearly 1%) over REFA-InfoNCA (dynamic) and 7.93% on LC-WR and 10.8% on WR over the baseline. These improvements validate the theoretical advantages of the 1-vs-all contrastive optimization framework, which ensures better utilization of negative samples by contrasting each positive instance against all available negatives, leading to more robust preference alignment. Furthermore, the superior performance of REFA-InfoNCA (dynamic) over the baseline underscores the effectiveness of incorporating instance-specific dynamic adjustments. Our theoretical in (Appendix C) and empirical analysis corroborates these findings by proving that the 1-vs-all strategy enhances the representational capacity of the model, resulting in superior generalization and preference alignment compared to other baselines."}, {"title": "REFA-1-vs-all vs REFA-dynamic", "content": "We compared the performance of REFA-1-vs-all and REFA-dynamic, as shown in Table 7, to evaluate the impact of dynamic adaptation in preference optimization. REFA-dynamic outperforms REFA-1-vs-all across all metrics, achieving"}]}