{"title": "Remote Inference over Dynamic Links via Adaptive Rate Deep Task-Oriented Vector Quantization", "authors": ["Eyal Fishel", "May Malka", "Shai Ginzach", "Nir Shlezinger"], "abstract": "Abstract-A broad range of technologies rely on remote inference, wherein data acquired is conveyed over a communication channel for inference in a remote server. Communication between the participating entities is often carried out over rate-limited channels, necessitating data compression for reducing latency. While deep learning facilitates joint design of the compression mapping along with encoding and inference rules, existing learned compression mechanisms are static, and struggle in adapting their resolution to changes in channel conditions and to dynamic links. To address this, we propose Adaptive Rate Task-Oriented Vector Quantization (ARTOVeQ), a learned compression mechanism that is tailored for remote inference over dynamic links. ARTOVeQ is based on designing nested codebooks along with a learning algorithm employing progressive learning. We show that ARTOVeQ extends to support low-latency inference that is gradually refined via successive refinement principles, and that it enables the simultaneous usage of multiple resolutions when conveying high-dimensional data. Numerical results demonstrate that the proposed scheme yields remote deep inference that operates with multiple rates, supports a broad range of bit budgets, and facilitates rapid inference that gradually improves with more bits exchanged, while approaching the performance of single-rate deep quantization methods.", "sections": [{"title": "I. INTRODUCTION", "content": "As data demands and data diversity grow, digital communica- tion systems are increasingly embracing collaborative networks designed for reliable and task-specific communication. This trend is particularly evident in next-generation technologies such as the Internet of Things and autonomous vehicles, where achieving accurate inference over rate-limited communication channels with low latency is essential [2]. Task-based (or goal-oriented) com- munication has emerged as a necessary and innovative solution for remote inference systems [3], which tend to operate in two distinct stages. The first stage occurs at the edge or sensing device, where acquired data is conveyed over a rate-limited channel after undergoing compression (source coding) and channel coding [4]. The second stage takes place at the receiver, which extracts the information needed for the task, e.g., classify an image [5]. Separating the processing involved with communicating data from that associated with inference facilitates the design of remote inference systems, and supports implementation on top of existing communication protocols. However, separation also often comes at the cost of notable overhead in communication resources, leading to excessive latency, which is often a crucial factor [6]. This downgrade in performance is a result of the inference task being typically very specific, while the data source is encoded such that it can be entirely recovered, regardless of the task at hand [7]. As such, several studies have attempted to bridge this gap in order to facilitate remote inference over-rate limited links. These include task-based quantization [8], [9], semantics- aware coding [10], [11], and goal-oriented communications [12], [13]. A common characteristic of these works involves encoding the source based on the inference task rather than prioritizing complete signal reconstruction. This approach supports compact representations, which in turn facilitate lower communication latency compared with the separation based designs [14]. Designing task-based compression mechanisms based on statistical models tends to be complicated and is limited to simple tasks that can be represented as linear [8] and quadratic mappings [15], [16]. Yet, data-driven approaches have been shown to yield accurate remote inference mechanisms for generic tasks with compact representations. This is achieved by leveraging joint learning of the compression mechanism along with a deep neural network (DNN)-aided inference rule [17], [18]. Such designs employ DNN-based encoder-decoder architectures, while constraining the latent features to a fixed bit representation via uniform quantization [19]\u2013[21], scalar quantization [22]\u2013[24], and vector quantization [25], [26]. Such forms of neural compression, which were shown to achieve highly compressed representation of image [27], video [28], and audio [29] (see detailed survey in [30]), can be naturally converted into remote inference systems. This is achieved by assigning the encoder and decoder to the sensing and inferring devices, respectively, while training the overall system for the desired inference metric [31]. The majority of DNN-aided compression algorithms operate in a static single-rate manner. Namely, the encoder maps the sensed data into a fixed-length bit sequence, which is then processed by the decoder module [27], [32]. In the context of remote inference, this operation induces two notable challenges when communicating over time-varying links: (i) Once trained, the model's compression rate can not be modified, making it difficult for remote inference systems to adapt to changing channel conditions. Consequently, the system must either adopt a worst-case compression rate, increasing latency, or maintain multiple encoder-decoder model's for different rates, adding complexity. (ii) Inference only begins after all the compressed features arrive and are decoded at the inferring device, which has to wait for the entire bit sequence representation to be received before it can provide any form of output. These limitations highlight the need for DNN-aided remote inference systems that can operate at different rates and perform inference with minimal latency, ideally starting as soon as the first bits are received. Several studies have proposed DNN-aided compression meth-"}, {"title": "II. SYSTEM MODEL AND PRELIMINARIES", "content": "In this section, we review some essential preliminaries and present the system model under consideration. We begin by reviewing basic quantization principles in Subsection II-A. Then, we formulate our remote inference problem in Subsection II-B, and discuss existing mechanisms for DNN-aided remote inference in Subsection II-C."}, {"title": "A. Quantization", "content": "Quantization is concerned with the representation of a continuous-valued signal using a finite number of bits [51]. The discrete representations produced through quantization should in general effectively represent signals, even at low resolution, while maintaining acceptable reconstruction performance. Formally, a quantizer, denoted by $\\mathcal{Q}^{*}(\\cdot)$ is a mapping from continuous-valued inputs in $\\mathbb{R}^{n}$ into discrete-valued outputs in $\\mathcal{Q} \\subset \\mathbb{R}^{k}$ using $\\log _{2} S$ bits. The set $\\mathcal{Q}$, whose cardinality is $|\\mathcal{Q}|=S$, represents the quantization codebook. This codebook defines the set of possible discrete outputs, forming the basis for the two-stage quantization mapping: Initially, an encoding function maps the continuous input $\\mathbf{x} \\in \\mathbb{R}^{n}$ into a discrete set $\\{1,2, \\ldots, S\\}$. Then, a decoding function maps each item in this discrete set into an associated codeword. Conventionally, $n=k$, and the codeword constitutes a reconstruction of the input. However, in task-based quantization, the codeword represents some desired information that must be extracted from the input, and thus $k$ can differ from $n$ [8]. When $n=1$, the quantizer is scalar, while $n>1$ denotes a vector quantizer."}, {"title": "B. Problem Formulation", "content": "We consider a remote inference setting comprised of a sensing device and an inferring user. At time $t$, the sensing device cap- tures an input data sample $\\mathbf{x}_{t}$, which is conveyed to the inferring user for providing a prediction $\\hat{y}_{t}$. For instance, $\\mathbf{x}_{t}$ can represent an image captured at a remote camera, while $\\hat{y}_{t}$ is the predicted class of the content of the image. The users communicate over a rate-limited channel which is modeled as a bit-pipeline with time-dependent capacity, denoted as $C_{t}$, measuring bits per time unit [26]. Consequently, the latency required to transmit $B_{t}$ bits at time $t$ is given by $T_{t}=\\frac{B_{t}}{C_{t}}$. The system is illustrated in Fig. 1. For conveying $\\mathbf{x}_{t}$, a quantization mechanism is employed, consisting of: (i) an encoder at the sensing device, denoted $f_{e}(\\cdot)$, that maps $\\mathbf{x}_{t}$ into a $B_{t}$ bits representation denoted $\\mathbf{z}_{t}$; and (ii) a decoder $f_{d}(\\cdot)$ implementing a decision rule at the inferring user that outputs $\\hat{y}_{t}$ based on $\\mathbf{z}_{t}$. It is assumed that the sensing device knows the current channel capacity $C_{t}$, and that the capacity has some lower bound $C_{\\min }>0$.\nWe focus on a data-driven setting. During design, one has access to a data set consisting of labeled examples $\\mathcal{D}=\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$, that is, $N$ pairs of inputs and desired outputs for design purposes. Our goal is to design a remote inference system based on two performance measures:\nP1 Accuracy of the predictions $\\hat{y}_{t}$, where we specifically focus on classification tasks;\nP2 Latency of the inference procedure, which we constrain to be at most $T_{\\max }$ (with $T_{\\max } \\geq \\frac{B_{\\min }}{C_{1}}$).\nIn principle, the sensing device can be designed to carry out the complete inference procedure. However, we concentrate on the common setting in which only partial pre-processing can be applied due to, e.g., hardware limitations [2]."}, {"title": "C. DNN-Aided Remote Inference", "content": "A natural approach to design data-driven remote inference system is to partition a DNN suitable for the task at hand between the sensing and inferring devices, resulting in a trainable encoder- decoder model [52]. However, compressing the latent representa- tion using a finite number of bits poses a problem owing to the non-differentiable nature of continuous-to-discrete mappings, and the desire to adjust the bit rate based on channel variations to meet latency constraints. This limits the ability to jointly learn the encoder and decoder mappings using conventional gradient-based deep learning tools. As such, various solutions have been pro- posed, including modeling scalar quantizers as additive noise dur- ing training [19], [21], and soft-to-hard approximations [23], [32]. Another approach to bypass the non-differentiable step is to use straight-through gradient estimators. A well-known example of this approach is the well-established VQ-VAE [25], illustrated in Fig. 2. Gradients are passed through the quantization step, allow- ing for joint learning of the encoder, codebook, and the decoder, despite the non-differentiable nature of the quantization. This joint optimization forms the foundations for the VQ-VAE model, which consists of three components: a DNN encoder, $f_{e}(\\cdot)$, a quantization codebook, $\\mathcal{Q}$, and a DNN decoder $f_{d}(.)$. The codebook $\\mathcal{Q}$ is comprised of $|\\mathcal{Q}|=S$ vectors of size $d$. The input sample, $\\mathbf{x}_{t}$, is processed by the encoder into $\\mathbf{x}=f_{e}(\\mathbf{x}_{t})$ which serves as a low-dimensional representation of the input. Subse- quently, the vector $\\mathbf{x}$ is decomposed into $M$ vectors of size $d$, denoted $\\{\\mathbf{x}_{t, m}\\}_{m=1}^{M}$, and each is represented by the closest code- word in $\\mathcal{Q}$. Thus, the latent representation $\\mathbf{z}_{t}$ is the stacking of\n$\\mathbf{z}_{t, m}=\\arg \\min _{\\mathbf{e}_{j} \\in \\mathcal{Q}}\\left\\|\\mathbf{x}_{t, m}-\\mathbf{e}_{j}\\right\\|_{2}^{2}$.\n(1)\nThe quantized $\\mathbf{z}_{t}$ is processed by the decoder into $\\hat{y}_{t}=f_{d}(\\mathbf{z}_{t})$, and the number of bits conveyed is $B_{t}=M \\cdot \\log _{2} S$.\nTo jointly train the encoder-decoder while learning the codebook $\\mathcal{Q}$, the VQ-VAE uses a loss function comprised of three terms as follows:\n$\\begin{aligned} \\mathcal{L}_{t o t}\\left(y_{t} ; \\mathbf{x}_{t}\\right)= & \\mathcal{L}\\left(y_{t} ; \\hat{y}_{t}\\right)+\\left\\|\\operatorname{sg}\\left(\\mathbf{x}_{t}\\right)-\\mathbf{z}_{t}\\right\\|_{2}^{2} \\\\ & +\\beta\\left\\|\\mathbf{x}-\\operatorname{sg}\\left(\\mathbf{z}_{t}\\right)\\right\\|_{2}^{2}, \\end{aligned}$\n(2)"}, {"title": "III. ARTOVEQ", "content": "In this section we introduce the proposed ARTOVeQ, designed for remote inference over dynamic channels as formulated in Subsection II-B. We commence by detailing its high level rationale in Subsection III-A, after which we present its trainable rate-adaptive vector codebook in Subsection III-B. We then show in Subsections III-C-III-D how the design of ARTOVeQ naturally extends to support multi-rate and progressive quantization, respectively, with a single codebook. We conclude with a discussion provided in Subsection III-E"}, {"title": "A. High Level Rationale", "content": "The VQ-VAE algorithm of [25], recalled in Subsection II-C, can be used for high performance remote inference (in the sense of P1) when employed over a static channel (in which the capacity and latency constraints, dictating the bit budget $B_{t}$, are fixed), owing to its ability to learn task-oriented vector quantization codebooks. Nonetheless, its application for remote inference is not suitable for dynamic channels, as it cannot adapt its bit rate to the the channel conditions. Moreover, its operation is non-progressive, i.e., the decoder needs to receive all bits representing the codeword for inference, which limits its minimal inference latency (P2). Our proposed ARTOVeQ builds on the ability of VQ-VAE to learn task-oriented multi-resolution codebooks, while overcoming its lack of flexibility and progressiveness by handling a codebook that accommodates multiple-bit resolutions. This is achieved by incorporating the following aspects:\nA1 A single codebook $\\mathcal{Q}$ is designed to support all multi-level bit resolutions by restricting it to be decomposable into sub-codebooks that are used for reduced bit rates.\nA2 A dedicated training algorithm is proposed, which combines principled initialization for vector quantization based on the Linde-Buzo-Gray (LBG) algorithm [57], alongside a gradual learning mechanism that allows the same decoder to be reused with all sub-codebooks.\nA3 By further restricting the learned codebook to take the form of nested vector quantization [46], we enable a progressive operation, where on each incoming bit the decoder can successively refine its predication.\nIn the following subsections we design ARTOVeQ by gradually incorporating A1-A3 into its design."}, {"title": "B. Rate-Adaptive Learned Vector Codebook", "content": "Here, we design a VQ-VAE-based architecture that enables multi-rate vector quantization, thus meeting A1, and present a training algorithm that enables rate adaptive task-oriented quantization following A2.\n1) Architecture: Using the VQ-VAE architecture outlined in Subsection II-C, which is generic in the sense that it is invariant of the specific DNNs used for the encoder and decoder, we construct a single codebook that accommodates multiple resolutions by iteratively doubling its number of codewords. Specifically, for each quantization level, $l=1,2, \\ldots, \\log _{2} S$, up to some maximum compression rate $S=|\\mathcal{Q}|$, a dedicated codebook is maintained $\\mathcal{Q}_{l}$. The process begins with constructing the 1-bit resolution codebook, followed by the 2-bit resolution codebook, and so forth, until the maximum compression rate is reached. This design guarantees that\n$\\mathcal{Q}_{1} \\subset \\mathcal{Q}_{2} \\subset \\cdots \\subset \\mathcal{Q}_{\\log _{2} S}$.\n(3)\nFrom (3) it follows that the first two codewords are derived from $\\mathcal{Q}_{1}$; the first four are derived from $\\mathcal{Q}_{2}$; and so on. The users thus manage a unified codebook encompassing all quantization resolu- tions, avoiding storing individual codebooks for each resolution. As new samples $\\mathbf{x}_{t}$ become available to the sensing device, the quantization level is initially determined using\n$\\begin{aligned} l_{t}= & \\max \\left\\{l \\in\\left\\{1,2, \\ldots, \\log _{2} S\\right\\}: \\frac{l}{M \\cdot C_{t}}<T_{\\max }\\right\\} . \\end{aligned}$\n(4)\nAfter determining the quantization level, remote inference is performed on the discrete outputs at the central server.\n2) Training: Given a dataset $\\mathcal{D}=\\{(\\mathbf{x}_{i}, y_{i})\\}_{i=1}^{N}$, the training algorithm sets the encoder $f_{e}(\\cdot)$, the codebook $\\mathcal{Q}$, and the decoder $f_{d}(\\cdot)$, through a gradual learning process. This approach is organized in three stages, designed to enable the model to operate at progressively higher resolutions over time while retaining previously acquired knowledge.\nStage 1: Encoder-Decoder Initialization: The first training stage uses $\\mathcal{D}$ to obtain a warm start for the encoder-decoder configuration $f_{e}(\\cdot), f_{d}(\\cdot)$. This is achieved by training both models as a sequential DNN without including quantization, i.e., mapping an input $\\mathbf{x}$ into $f_{d}\\left(f_{e}(\\mathbf{x})\\right)$. In particular, using the"}, {"title": "C. Mixed Resolution ARTOVeQ", "content": "ARTOVeQ learns a task-oriented vector quantizer using a single codebook that can be applied across multiple resolutions. Still, once a bit budget $l \\in\\{1, \\ldots, \\log _{2} S\\}$ is fixed, the same $l$-bit codebook is applied to each features sub-vector, at an overall budget at time $t$ of $B_{t}=M \\cdot l$ bits per input. However, the fact that the same codebook $\\mathcal{Q}$ can be decomposed into multiple codebooks of different resolutions can be leveraged to quantize high dimensional inputs with mixed resolutions applied to different features.\n1) Architecture: To formulate the mixed resolution ARTOVeQ, we recall that the encoder output $\\mathbf{x}$ is divided into the $M$ sub-vectors $\\{\\mathbf{x}_{t . m}\\}_{m=1}^{M}$. Each features segment is assigned a specific sub-codebook based on a designated bit resolution $S_{m}$, with a total bit budget at time $t$ is $B_{t}=\\sum_{m=1}^{M} \\log _{2} S_{m}$ representing the sum of bits allocated across all segments. The resulting bit budget $B_{t}$ can thus take any value in the range $[M, M+1, \\ldots, M \\log _{2} S]$, indicating that the mixed resolution design provides high bit budget flexibility and granularity, a property not achieved with alternative variable rate learned quantizer architectures whose focus is on the encoder-decoder architecture, e.g., [41]\u2013[45]. An illustration of the mixed resolution ARTOVeQ can be seen in Fig. 4.\n2) Training: The training of mixed resolution ARTOVeQ follows the same procedure as in Algorithm 1, with a slight modification applied in Stage 3. Here, instead of progressively increasing the resolution of the codebook and having it employed for quantizing all $M$ features, we gradually increase the resolution of the first $d \\times 1$ features $\\mathbf{x}_{1}$, after which we increase the resolution of quantizing $\\mathbf{x}_{2}$, and so on. The rationale in this form of gradual learning draws inspiration from classical image compression methods based on quantizing different components with different resolution, e.g., [58]. In doing so, we aim to con- sistently have some features quantized with improved resolution, such that the task-based encoder-decoder be encouraged to embed there features that are more informative with respect to the task."}, {"title": "D. Progressive ARTOVeQ", "content": "While the training procedure used by ARTOVeQ is based on progressive learning, where the resolution of intermediate features gradually grows during training [49], the resulting quantizer does not immediately support progressive quantization. Specifically, for a chosen bit budget $B_{t}$, the codewords do not support progres- sive decoding, namely, the decoder has to have access to all bits representing the compressed features in order to infer. Nonethe- less, while the formulation of the codebook in Subsection III-B only allows variable-rate operation, the fact that what one learns is the multi-resolution codebook implies that it can naturally extend to have a progressive codebooks, whose codewords incrementally build on prior representations, as a form of successive refinement.\n1) Architecture: To support progressive quantization, we alter the codebook constraint of (3) to be one which supports succes- sive refinement of initial low-resolution representations of the codewords. Drawing inspiration from nested quantization, which is typically considered in the context of uniform [46] and lattice codebooks [59], we constrain each intermediate codebook $\\mathcal{Q}_{l}$ to represent a one bit refinement of $\\mathcal{Q}_{l-1}$. Mathematically, for each $l \\in\\{1, \\ldots, \\log _{2} S\\}$ there exist $d \\times 1$ vectors $\\overline{\\mathbf{e}}_{l}, \\underline{\\mathbf{e}}_{l}$ such that\n$\\mathcal{Q}_{l}=\\mathcal{Q}_{l-1}+\\{\\overline{\\mathbf{e}}_{l}, \\underline{\\mathbf{e}}_{l}\\},\n(9)\nwith + being the Minkowski set sum, thus $\\left|\\mathcal{Q}_{l}\\right|=2 \\cdot\\left|\\mathcal{Q}_{l-1}\\right|$.\nThe constrained codebook form in (9) enables progressive recovery via successive refinement. Specifically for an encoder output $\\mathbf{x}$ and its decomposition into $\\{\\mathbf{x}_{t, m}\\}$, the decoder only needs one bit per each sub-vector to recover their representation in $\\mathcal{Q}_{1}$ and use it for inference. With the next $M$ bits, the decoder obtains the improved representation in $\\mathcal{Q}_{2}$, and uses it to improve its inference output, and so on.\n2) Training: Progressive ARTOVeQ is based on the learned task-based vector quantizer detailed in Subsection III-B, while introducing an alternative constraint on the learned multivariate"}, {"title": "E. Discussion", "content": "The proposed ARTOVeQ is designed to facilitate learning a single task-based vector quantization codebook that supports finer granularity and progressive decoding through the use of multiple resolution. Accordingly, it is particularly suitable for remote inference over time-varying communication links, e.g., with mobile users [2]. This flexibility allows the compression rate to be adjusted according to dynamic channel conditions, ensuring accurate inference, while supporting a broad range of multiple resolutions (via mixed-resolution among different feature sub-vectors), as well as allowing the decoder to provide rapid inference and gradually improve it via successive refinement. Adaptivity is achieved through nested codebooks, and progressive learning techniques, allowing the system to refine its performance over successive iterations or stages of operation. As ARTOVeQ focuses on learning the quantization codebook and does not restrict the task-based mappings $f_{e}$ and $f_{d}$, it can be integrated in various DNN architectures. While our setup primarily focuses on a pair of sensing and inferring users, this methodology is extensible to collaborative inference among multiple edge devices. Our approach assumes that the instantaneous channel capacity $(C_{t})$ is known, enabling the sensing user to determine the appropriate quantization level. However, a potential extension of our scheme could allow it to function without prior knowledge of channel capacity, dynam- ically tuning the quantization rate during the remote inference process. Another potential aspect for future exploration, which stems from the ability to learn a variable rate and progressive vector quantization codebook integrated into a remote inference system, is its ability to enhance data privacy and security. Recent studies have shown that well-designed compression strategies can enhance privacy, providing an additional benefit to our rate- adaptive scheme [60], [61]. Furthermore, recent advancements in randomized neural networks demonstrate their potential for ensuring privacy [62]. While ARTOVeQ has the potential of sup- porting such extensions, they would necessitate reformulation of the learning procedure, and are thus left for future investigation."}, {"title": "IV. NUMERICAL EXPERIMENTS", "content": "In this section, we present the results of our numerical experiments\u00b9. We first detail our experimental setup in Subsection IV-A, after which we detail our four main studies, each focusing on a distinct aspect of our approach for image classification: variable-rate task-based compression (Subsection IV-B), mixed resolution compression (Subsection IV-C), progressive compression (Subsection IV-D), and remote inference over dynamic channels (Subsection IV-E)."}, {"title": "A. Experiential Setup", "content": "To evaluate our quantization scheme, we use two datasets: CIFAR-100 and Imagewoof. CIFAR-100 consists of 60,000 diverse images with dimensions $3 \\times 32 \\times 32$, spanning 100 classes and thus encompassing a wide variety of source distributions. Imagewoof contains 10,000 images at a higher resolution of $3 \\times 64 \\times 64$, but with a slightly narrower set of 10 classes, each representing different dog breeds. This combination allows us to assess our method's robustness across a large number of source distributions in CIFAR-100, and under a more specific, yet high-resolution, distribution in Imagewoof. For our evaluation, we employed the MobileNetV2 [50] architecture to accommodate edge device constraints, partitioning it into an encoder $f_{e}(\\cdot)$, comprising the first four residual blocks, and a decoder $f_{d}(\\cdot)$ with the remaining blocks. The encoder-decoder and codebooks were jointly trained using the Adam optimizer, with learning rate $1 \\cdot 10^{-4}$ and batch sizes 32 and 16 for CIFAR-100 and Imagewoof, respectively. We evaluate the performance in terms of test accuracy achieved with the following quantization methods: 1) ARTOVeQ (as detailed in Subsection III-B); 2) a single-rate VQ-VAE, in which a different codebook is trained for each bit budget, constituting an upper-bound on the performance achievable with a single codebook shared among all resolutions; 3) mixed resolution ARTOVeQ (detailed in Subsection III-C); 4) progressive ARTOVeQ (detailed in Subsection III-D); 5) residual VQ-VAE (RVQ-VAE) [41]; and 6) Single-Rate LBG, in which LBG [57] is applied anew to the learned encoder output for quantization for each codebook size. In each experiment, the encoder's output was divided into $M$ segments, and the quantizer is applied to each sub-vector."}, {"title": "B. Variable-Rate Task-Based Compression", "content": "We first assess the performance of variable-rate ARTOVeQ in an environment where bit-rate availability may vary over time, demonstrating its capability to enable remote inference across a broad range of communication conditions. We show that, despite utilizing a single codebook across multiple resolutions, ARTOVeQ remains competitive with single-rate VQ-VAE and outperforms other benchmark models.\nLearned Codebooks: A defining characteristic of ARTOVeQ is its nested codebook structure. Fig. 5 illustrates the progression of codebooks $\\mathcal{Q}_{1}, \\ldots, \\mathcal{Q}_{8}$ for $d=2$ on the CIFAR-100 dataset. As the bit resolution increases, the codebook vectors progressively capture the latent state distribution with greater precision, leading to improved performance at higher resolutions.\nPerformance Evaluation: Having showcased the codebook structures learned by ARTOVeQ, we proceed to evaluating its performance when integrated into a remote inference system. The results achieved for CIFAR-100 are reported in Fig. 6. There, we observe the trade-offs between compression via quantization and performance in ARTOVeQ compared to other variable-rate and single-rate baselines. As expected, each approach exhibits an increasing trend in performance before tapering off and saturating at higher resolutions, typically around 4\u20135 bits for per sub-vector. ARTOVeQ consistently performs just slightly below the single- rate VQ-VAE, with a performance drop of approximately 0.8% for $d=2$. Some performance degradation is observed when tran- sitioning from $d=2$ to $d=4$ as the number of bits per codeword is remains constant, while the dimensionality of the codewords increases, i.e., the quantizaiton rate is reduced. Despite this, ARTOVeQ still outperforms both single-rate LBG and RVQ-VAE. This accuracy degradation relative to single-rate VQ-VAE can be attributed to the constraints imposed by ARTOVeQ's nested code-"}, {"title": "C. Mixed-Resolution Compression", "content": "We proceed to evaluating the ability of ARTOVeQ to leverage its multi-resolution codebook to quantize different sub-vectors with different resolutions. For this task, we partitioned the encoder output, $\\mathbf{x}$, into four blocks with a manual bit allocation strategy. The first segments were assigned the highest bit representations, following a policy where the largest bit share is allocated to the first segment, with subsequent segments receiving progressively lower bit resolutions that collectively sum to a predefined bit budget $B_{t}$. The aim of this study is the examine the performance of using mixed resolution codebooks compared to identical resolution ones with the same overall bit budget (which we contrasted with various benchmarks in Subsection IV-B). The CIFAR-100 and Imagewoof results corresponding to $d=2$ and $d=4$ are shown in Figs. 8-9, respectively. There, we compare accuracy for different values of total number of bits assigned across four quantizers (that are applied to each four sub-vectors). In the identical resolution case, all quantizers have the same codebook, while in the mixed resolution case, the first quantizer uses more codewords compared to the remaining ones. Our findings reveal that, for both datasets, mixed-resolution configurations consistently outperform their identical resolution counterparts, though the improvement varies with the number of bits. This demonstrates that the finer granularity enabled by mixed-resolution compression, combined with task-based learning, allows for a more refined latent space representation, resulting in improved performance. As seen in Fig. 8, this effect is particularly evident in lower bit budgets, between 4-10 bits, where the richer bit spectrum enables more effective learning and performance gains. Quantitatively, we observe a performance gap of approximately 0.4% - 0.7% for $d=2$ and 0.2% - 1.5% for $d=4$ on the CIFAR-100 dataset within the 8-20 bit range. Similarly, for the Imagewoof dataset, the gap ranges from 0.2% - 3% for $d=2$ and 1.6% - 4% for $d=4$. These findings suggest that improved performance can be achieved with a smaller bit budget. At the higher end of the bit spectrum, identical-resolution configurations tend to closely match the performance of mixed-resolution ones for both datasets. However, due to the redundancy and narrower distribution of Imagewoof, this alignment is reached at a lower bit budget. In all cases, mixed-resolution configurations offer the advantage of flexible memory usage."}, {"title": "D. Progressive Compression", "content": "We proceed by evaluating the progressive quantization codebook version of ARTOVeQ, with its successive refinement approach. Specifically, we aim to assess how effectively the progressive constraint and its corresponding learning technique balance compression efficiency and task accuracy, and to compare the performance of successive bit increments against the variable-rate ARTOVeQ evaluated in Subsection IV-B. Our findings, shown in Fig. 10 for CIFAR-100 and in Fig. 11 for Imagewoof, demonstrate that, as expected, variable-rate ARTOVeQ consistently outperforms the more constrained progressive codebook across all bit resolutions and quantization embeddings $(d=2$ and $d=4)$. Nonetheless, progressive codebooks manage to approach the performance of variable-rate ARTOVeQ owing to its dedicated learning technique, within some performance gap that varies between the considered tasks. The discrepancy is attributed to the the strict progressive constraint, which, while allowing for incremental decoding with minimal latency, comes at the cost of some performance degradation compared to variable rate ARTOVeQ. This is particularly evident for Imagewoof with $d=4$. Despite the performance gap, the results show that both variable-rate ARTOVeQ and progressive ARTOVeQ begin to saturate around 6 bits, with only marginal improvement beyond this point. From a complexity standpoint, both techniques operate in a one-shot fashion; however, progressive quantization has the advantage of minimal latency, as inference can begin immediately after the first bit is received, whereas variable-rate ARTOVeQ requires the entire bit sequence."}, {"title": "E. Remote Inference over Dynamic Channels", "content": "The experimental studies so far have evaluated the different versions of ARTOVeQ", "Setup": "To evaluate the performance of models in a dynamic channel environment", "scenarios": "S1 a uniform distribution of bit-rates; S2 scenarios where lower bit-rates are more likely", "Evaluation": "We compare the average accuracy of ARTOVeQ and Progressive ARTOVeQ", "benchmarks": "The first is remote inference system that maintains eight different fixed-rate encoder-VQ-VAE-decoder chains, constituting the most flexible yet extremely costly alternative. We also compare to using a single fixed-rate encoder-VQ-VAE-decoder designed with codebook sizes $\\log _{2} S \\in\\{1,4,8\\}$. As the latter operates at a fixed rate, it fails to convey the samples within the coherence time when its rate surpasses that supported by the channel. The results obtained with the CIFAR-100 and the Imagewoof datasets are reported in Table I. The experimental results across CIFAR-100 and Imagewoof datasets reveal consistent performance behaviors for both $d=2$ and $d=4$. As expected, Multiple Fixed-Rate VQ-VAE consistently achieves the highest inference accuracy, while being only within a minor gap from ARTOVeQ across all scenarios. The progressive ARTOVeQ, designed with the mechanism of incremental codebook vector's improvement, shows a slight performance drop of approximately $0.5 \\%-2 \\%$ on CIFAR-100 and $3 \\%-4 \\%$ on Imagewoof. Conversely, the Single-Rate VQ-VAE struggles in scenarios where the channel's supported bit-rate is"}]}