{"title": "PropaInsight: Toward Deeper Understanding of Propaganda in Terms of Techniques, Appeals, and Intent", "authors": ["Jiateng Liu", "Lin Ai", "Zizhou Liu", "Payam Karisani", "Zheng Hui", "May Fung", "Preslav Nakov", "Julia Hirschberg", "Heng Ji"], "abstract": "Propaganda plays a critical role in shaping public opinion and fueling disinformation. While existing research primarily focuses on identifying propaganda techniques, it lacks the ability to capture the broader motives and the impacts of such content. To address these challenges, we introduce PropaInsight, a conceptual framework grounded in foundational social science research, which systematically dissects propaganda into techniques, arousal appeals, and underlying intent. PropaInsight offers a more granular understanding of how propaganda operates across different contexts. Additionally, we present PropaGaze, a novel dataset that combines human-annotated data with high-quality synthetic data generated through a meticulously designed pipeline.\nOur experiments show that off-the-shelf LLMS struggle with propaganda analysis, but training with PropaGaze significantly improves performance. Fine-tuned Llama-7B-Chat achieves 203.4% higher text span IoU in technique identification and 66.2% higher BertScore in appeal analysis compared to 1-shot GPT-4-Turbo. Moreover, PropaGaze complements limited human-annotated data in data-sparse and cross-domain scenarios, showing its potential for comprehensive and generalizable propaganda analysis.", "sections": [{"title": "Introduction", "content": "In an era of unbounded digital information, the deliberate dissemination of propaganda has proliferated, shaping public opinion and influencing political events (Stanley, 2015). Propaganda is also a key component of disinformation, where false information is intentionally crafted and distributed to deceive or mislead (Da San Martino et al., 2020). Detecting and analyzing propaganda is essential to maintain the integrity of public discourse and to ensure that individuals make informed, unbiased decisions (Da San Martino et al., 2020).\nMost current research on propaganda detection focuses on identifying and categorizing the specific techniques used to persuade the audience (Da San Martino et al., 2019; Martino et al., 2020a). However, simply recognizing these techniques does not fully capture the motives behind the propaganda or its broader impact. As noted by Hobbs (2020); Lord and Vogt (2021), understanding the sources and the intentions behind the information is critical to effectively combat disinformation. Therefore, there is a growing need for moving beyond simply recognizing techniques to deepen understanding of propaganda.\nMoreover, existing studies often rely on expert annotations because non-expert annotators struggle with separating personal biases from their assessments of propaganda (Da San Martino et al., 2019). This dependence on expert annotations leads to relatively small datasets, which may be insufficient for training large, generalizable models, and limits their applicability in cross-domain contexts where propaganda usage varies. For instance, strategies in military content can differ greatly from such in political content, highlighting the need for broader, more diverse datasets.\nTo address these challenges, we build on foundational social science research on propaganda (Nelson, 1997; Jowett and O'donnell, 2018; Ellul, 2021) and identify three key elements behind propaganda attempts: propaganda techniques, arousal appeals, and underlying intent. Consequently, we introduce a new conceptual framework, PropaInsight, that systematically analyzes these elements. Additionally, we leverage the strong context understanding ability of large language models (LLMs) to generate synthetic data, resulting in PropaGaze, a novel dataset for propaganda analysis. Our motivations are twofold: (1) to develop a comprehensive framework that goes beyond identifying techniques, and (2) to explore the use of synthetic data to supplement limited human-annotated data. Our contributions are as follows:\n1.  We propose PropaInsight, a conceptual framework for granular and comprehensive propaganda analysis that identifies propaganda techniques, arousal appeals, and underlying intent in news articles.\n2.  We introduce PropaGaze, a novel dataset for fine-grained propaganda analysis, consisting of a human-annotated news sub-dataset and two high-quality synthetic sub-datasets: one focused on the Russia-Ukraine conflict and one on the political domain.\n3.  We demonstrate that PropaGaze enhances LLMs' ability to analyze propaganda within the PropaInsight framework, paving the way for more nuanced and generalizable propaganda analysis methods."}, {"title": "PropaInsight: A Propaganda Analysis Framework", "content": "We introduce PropaInsight, a new conceptual framework for comprehensive propaganda analysis. In contrast to previous methods which ignore the underlying purposes and only focus on techniques, PropaInsight delves into the more subtle and hidden elements of propaganda. Drawing from foundational social science research on propaganda (Nelson, 1997; Jowett and O'donnell, 2018; Ellul, 2021), we identify three key elements of each propaganda attempt: propaganda techniques, arousal appeals, and underlying intent. As shown in Figure 1, for a given article, we first identify and classify the techniques used. We then infer the arousal appeals these techniques evoke, and we further deduce the underlying intent of the article. To ensure interpretability and consistency, we consolidate these elements into a clear, structured natural language paragraph using a descriptive template, as shown in Figure 1. Below, we provide a detailed explanation of each element of our proposed framework.\nPropaganda Techniques Propaganda techniques are systematic, deliberate strategies used to craft persuasive content (Jowett and O'Donnell, 2012). Domain experts typically define these techniques as pre-defined labels like 'loaded language'. While the specific techniques may vary across different shared tasks (Torok, 2015), we follow the set of propaganda techniques defined in (Da San Martino et al., 2019), where each technique can be evaluated intrinsically. The full list of the 16 propaganda techniques we use is provided in Appendix C.\nArousal Appeals Appeals directly influence a reader's emotions, opinions, and actions after consuming propagandistic content (Nelson, 1997; Jowett and O'Donnell, 2012). A common propaganda device is to evoke strong emotions, such as hate or fear, in readers (Miller, 1937). Another approach involves selectively presenting evidence and facts to shape the audience's perception (Walton, 1997; O'Shaughnessy, 2004). To capture these effects, we design three templates (detailed in Appendix C) that identify the emotions evoked and the aspects readers are guided toward or distracted from while reading an article.\nUnderlying Intent Intent represents the ideological, political, or other underlying goal the author seeks to convey or achieve. To handle diverse real-world scenarios, we frame intent prediction as a free-text generation task, similar to approaches used for open intent generation in dialogue systems (\u015eim\u015fek and Fensel, 2018; Wagner, 2022). The advantage of this novel formulation in propaganda intent analysis is its flexibility in capturing complex, nuanced intent that predefined labels cannot easily categorize, allowing greater freedom to generate more detailed and context-specific interpretations of intent.\nPropaganda Analysis Task The design of PropaInsight introduces a new propaganda analysis task: generating a descriptive natural language paragraph explaining the techniques used, the appeals aroused, and the underlying intent. To avoid overlooking individual elements and to simplify evaluation, we divide the task into three sub-tasks:\n1.  Propaganda Technique Identification: Detect the spans where propaganda techniques are applied and which specific technique(s) correspond to each span, following prior task settings (Martino et al., 2020a,b).\n2.  Appeal Analysis: Generate the descriptions of emotions and feelings evoked using a template-based approach (see Appendix C).\n3.  Intent Analysis: Generate a free-form explanation of the article's underlying intent."}, {"title": "PropaGaze: A Dataset for Systematically Analyzing Propaganda", "content": "Existing propaganda datasets (Martino et al., 2020a; Heppell et al., 2023) primarily focus on identifying propaganda techniques and their associated text spans, but lack insights into appeal and intent. We introduce PropaGaze, a new dataset specifically designed for comprehensive propaganda analysis, consisting of three sub-datasets: PTC-Gaze, RUWA-Gaze, and Politifact-Gaze."}, {"title": "PTC-Gaze: Human-Annotated Dataset", "content": "PTC-Gaze builds on the existing PTC dataset (Martino et al., 2020a), which includes human-written news articles annotated for propaganda techniques and spans. We reannotate this dataset by hiring human annotators to label appeals and intent independently. For appeals, annotators review propaganda-containing sentences along with their context and describe the feelings evoked. To reduce cognitive load, we provide GPT-4 generated candidate annotations for assistance. Annotators then evaluate whether the generated candidates accurately reflect their interpretations and reactions, and if not, they rewrite the descriptions based on the template in \u00a7 2 and Appendix C. For intent, annotators read the full article and infer its underlying intent in a single free-form sentence, and we leave the multi-intent scenarios for future work. As shown in Table 1, this annotated sub-dataset contains 79 articles, with an average of 12.77 propaganda techniques per article. Additional information, data examples, and analysis of the annotation quality are given in Appendix D."}, {"title": "RUWA-Gaze and Politifact-Gaze: Synthetic Datasets", "content": "One limitation of the fully human-annotated dataset is that its usualy expensive, due to the challenging nature of the annotation tasks. This makes it insufficient for training large, generalizable models and limits its cross-domain applicability. Sparse data is a common issue in propaganda analysis research. To address this, we leverage LLMs such as LLaMA (Touvron et al., 2023) and GPT (Ye et al., 2023; OpenAI, 2023) to synthesize data, using their strong prior knowledge and context understanding. These models have shown effectiveness in data augmentation for tasks like propaganda techniques identification, such as fallacy recognition (Alhindi et al., 2024). These synthetic datasets are created mainly for training and can also serve as silver-standard benchmarks for propaganda analysis.\nWe construct RUWA-Gaze and Politifact-Gaze using a partially controlled data generation pipeline, as illustrated in Figure 2. Specifically, RUWA-Gaze is built upon a dataset of real-world news articles focused on the Russia-Ukraine War (Khairova et al., 2023), while Politifact-Gaze is constructed using the PolitiFact partition of the FakeNewsNet dataset (Shu et al., 2020).\nData Generation Pipeline Figure 2 shows the data creation pipeline. Initially, we use GPT-3.5 to summarize human-written, published news articles and to identify key events and objective facts. These summaries are intended to be objective, as the original articles may reflect various biases that could influence the creation of new propaganda pieces. Following this, we use GPT-3.5 to extract all focal entities involved in the events. We then randomly select one entity's perspective and set an intent to guide the revision of the article. We also randomly choose a set of propaganda techniques to be inserted into the article, reshaping its narrative. Subsequently, we use GPT-4 as an intermediary author to craft intentional propaganda articles based on real-world events by injecting sampled propaganda techniques into an objective summary. We also ask the model to self-analyze the appeals the rewritten article might evoke to ensure alignment with the established intent. Human readers then verify the data quality for any obvious errors. The prompts for each step are provided in Appendix B.\nAs illustrated in Table 1, RUWA-Gaze consists of 497 articles, and Politifact-Gaze consists of 593 articles. While we generated moderate data due to the computational cost. We believe the data generation pipeline is generalizable. The language models can be replaced with cheaper or open-source LLMs to reduce costs and, in turn, generate larger-scale datasets. In addition, we identify that these two subsets come from different domains (Military & War and Politics), and they differ significantly in both content and the use of propaganda techniques."}, {"title": "Experiments", "content": "LLMs have strong prior knowledge and advanced context understanding, which makes them ideal for synthesizing propaganda-rich datasets and potentially effective for analyzing propaganda. In this section, we explore three research questions: (1) how off-the-shelf LLMs perform on propaganda analysis, (2) how much the PropaGaze dataset improves performance when used for training or fine-tuning, and (3) whether propaganda analysis is transferable across domains."}, {"title": "Experimental Setup", "content": "Sub-Tasks and Metrics As outlined in \u00a7 2, PropaInsight makes it possible to break the propaganda analysis task into three sub-tasks to ensure detailed evaluation and capture key elements:\n1.  Propaganda Techniques Identification: We use Intersection over Union (IoU) to measure the overlap between the identified text spans and the actual propaganda spans, and F1 scores to evaluate propaganda technique classification, following prior task settings (Martino et al., 2020a,b).\n2.  Appeals Analysis: We evaluate the quality of the generated responses using BertScore (Zhang et al., 2019) to measure semantic similarity.\n3.  Intent Analysis: Similarly, we use BertScore for this sub-task.\nModels We experiment with the following:\n1.  GPT-4-Turbo: One of the top-performing OpenAI models for various tasks. We use it in both zero-shot and few-shot prompting settings across all sub-tasks. The specific prompts used for each sub-task are given in Appendix C.\n2.  Llama-7B-Chat: A popular open-source LLM. Due to its smaller size and relatively worse performance compared to GPT-4-Turbo, we fine-tune it for our sub-tasks. Specifically, we instruction-tune it to predict whether each sentence contains propaganda, and if so, identify the techniques and the appeals used, and predict the article's intent. See Appendix C for the fine-tuning prompts.\n3.  Multi-Granularity Neural Networks (MGNN model) (Da San Martino et al., 2019): A benchmark method for the propaganda techniques identification sub-task. We train MGNN from scratch for this specific task, as it is not designed for text generation and cannot be applied to the other two sub-tasks.\nData-Rich and Data-Sparse Training Settings\nIn real-world scenarios, obtaining a large volume of well-annotated data for analyzing propaganda is challenging, as discussed in \u00a7 3. To simulate this limitation, we use both data-rich and data-sparse environments to evaluate the impact of larger, generalized synthetic datasets on model performance.\nFor all PropaGaze sub-datasets, we split the articles into training and testing sets using a 70:30 ratio. PTC-Gaze, with only 79 articles, represents a data-sparse condition. In contrast, the synthetic sub-datasets, RUWA-Gaze and Politifact-Gaze, contain a total of over 1,000 articles. To simulate data-sparse scenarios with these two sub-datasets, we sample subsets matching the size of the full PTC-Gaze training set. For data-rich conditions, we use the full training sets of RUWA-Gaze and Politifact-Gaze, reserving one-seventh as the validation set."}, {"title": "How Do Off-the-Shelf LLMs Perform on Propaganda Analysis Tasks?", "content": "As shown in Tables 2 and 3, zero-shot LLMs struggle with propaganda analysis. For example, in identifying propaganda techniques, zero-shot GPT-4-Turbo underperforms compared to the trained MGNN, even in data-sparse conditions, despite MGNN being much smaller in size. Zero-shot LLMs often struggle to pinpoint sentences containing propaganda. Similarly, in appeal analysis, zero-shot GPT-4-Turbo achieves relatively low BertScores. However, these models perform better at inferring intent, as shown by their stronger performance in the intent analysis sub-task (Table 3).\nFew-shot prompting improves LLM performance in analyzing propaganda elements. Specifically, in identifying propaganda techniques, one-shot GPT-4-Turbo shows an 80.8% improvement in average IoU on RUWA-Gaze, a 20.4% increase on Politifact-Gaze, and a 33.1% higher IoU on PTC-Gaze compared to a zero-shot setting. Similarly, in appeal analysis, one-shot GPT-4-Turbo achieves 14.9% higher BertScore on RUWA-Gaze, 15.8% higher on Politifact-Gaze, and 45.2% higher on PTC-Gaze. In intent analysis, zero-shot GPT-4-Turbo already performs well. The improvements compared to one-shot prompting are minor, with the highest increase being 3.5% on RUWA-Gaze."}, {"title": "How Much Does PropaGaze Enhance Model Performance?", "content": "PropaGaze substantially improves the overall propaganda analysis performance, especially in identifying propaganda techniques, under both data-sparse and data-rich training conditions. In the data-sparse setting, fine-tuned LLaMA-7B-Chat outperforms one-shot GPT-4-Turbo, achieving an average of 65.8% higher text span IoU and 33.7% higher technique identification F1 score, as shown in Table 2. In the data-rich setting, the performance increases even further, with LLaMA-7B-Chat showing 90.9% higher text span IoU and 125.1% higher F1 score compared to the data-sparse results.Table 3 shows similar improvements in appeals and intent analysis. For the appeals sub-task, data-rich fine-tuning leads to an average 70.1% increase in BertScore, while for intent analysis there is a smaller 8.5% gain compared to data-sparse training. This is likely due to the already high baseline performance. These results demonstrate that the synthetic sub-datasets effectively complement the limited human-annotated data, significantly improving the model's performance in analyzing propaganda elements.\nWe also compare the performance of LLaMA-7B-Chat with the baseline benchmark MGNN on propaganda identification. In the data-sparse setting, fine-tuned LLaMA-7B-Chat substantially outperforms trained MGNN, achieving 158.43% higher IoU on RUWA-Gaze and 58.1% higher IoU on PolitiFact-Gaze. However, in data-rich scenarios, MGNN, benefiting from the larger amount of training data, surpassing LLaMA-7B-Chat. This may be due to the fact that smaller models, such as MGNN, can overfit when exposed to excessive training data, while larger LLMs, such as LLaMA-7B-Chat, generalize better in data-sparse conditions. These findings suggest that LLMs are more suited for the task with limited training data, while smaller, dedicated models like MGNN could benefit more from the synthetic sub-datasets provided by PropaGaze in data-rich environments. This is consistent with the findings of Alhindi et al. (2024). Thus, with sufficient training data, we can implement a pipeline that first localizes and identifies propaganda techniques using MGNN, followed by appeals and intent analysis based on MGNN's output. This approach could potentially enhance the overall quality of the model's output for the entire propaganda analysis task."}, {"title": "Is Propaganda Analysis Transferable Across Domains?", "content": "In the real world, propaganda spans various domains, including military and war, politics, economics, science, environmental issues, and more. Although the specific use of propaganda may differ across these domains, we are particularly interested in determining whether the general patterns of propaganda are transferable between domains. Additionally, high-quality human-annotated data is scarce, prompting us to investigate whether leveraging data from other domains can improve propaganda analysis in a target domain.\nAs outlined in \u00a7 3, our dataset consists of three subsets: RUWA-Gaze (military and war), Politifact-Gaze (politics), and PTC-Gaze (general news). To explore cross-domain transferability, we perform additional training on each target sub-dataset using data from the other two sub-datasets after the in-domain training. For instance, in a data-sparse scenario, if RUWA-Gaze is the target, cross-domain training on Politifact-Gaze involves first training the model on the sparse RUWA-Gaze data, followed by further training with sparse Politifact-Gaze data. In a data-rich scenario, the model is trained on the full in-domain RUWA-Gaze data, then further trained on the entire Politifact-Gaze dataset. The results are presented in Tables 4 and 5.\nIn data-sparse settings, we observe that models benefit substantially from incorporating cross-domain data. As shown in Table 4, when evaluated on RUWA-Gaze, models trained on additional data from Politifact-Gaze and PTC-Gaze achieve higher performance than those trained solely on sparse in-domain data. Specifically, LLaMA-7B-Chat fine-tuned with additional Politifact-Gaze data achieves the highest text span IoU of 0.271, while MGNN trained with additional Politifact-Gaze data reaches the highest technique F1 score of 0.281. This pattern is consistent across other sub-datasets and holds true for appeal analysis as well, as shown in Table 5. This is expected, as models trained in data-sparse conditions tend to benefit from cross-domain data due to the need for a larger pool of training examples. Access to additional data from related domains enables models to learn generalized patterns of propaganda usage more effectively, leading to improved performance even on tasks outside of their original training domain.\nHowever, in data-rich scenarios, the benefit of cross-domain training diminishes. For example, as shown in Table 4, models trained on additional Politifact-Gaze data underperform those trained solely on in-domain data when evaluated on RUWA-Gaze. Similarly, when evaluated on Politifact-Gaze, adding RUWA-Gaze data sometimes leads to performance improvements, but the gains are relatively small. This holds for appeal analysis as well, as we can see in Table 5. These results suggest that when there is sufficient training data, the quality of the data has a greater impact on performance than its quantity. We further observe that training on both RUWA-Gaze and Politifact-Gaze improves the performance on the human-annotated PTC-Gaze across all sub-tasks. While this is partly due to the data-sparse nature of PTC-Gaze, making extra training samples valuable, it also highlights that our synthetic data effectively complements the limited human-annotated data."}, {"title": "Discussion", "content": "Discrepancy between Human-Annotated and Synthetic Datasets\nWe acknowledge the discrepancy between the synthetic sub-datasets and the human-annotated sub-dataset in PropaGaze. As shown in Table 1, the average number of propaganda techniques per article in PTC-Gaze is 12.77, which is about 3.7 times higher than in the synthetic RUWA-Gaze and Politifact-Gaze. This occurs due to the way we generate the synthetic data, where we inject three propaganda techniques per article, with GPT-4-Turbo sometimes reusing techniques. However, we believe this is less of an issue, as PTC-Gaze articles are on average 3.3 times longer than those in the other sub-datasets. Moreover, since we treat the injected techniques as silver labels, we have not yet checked whether other sentences in the articles also use propaganda techniques. See the Limitations section for more details. Finally, we note the inherent difference in writing styles between synthetic and human-written articles, which is a common challenge with synthetic datasets."}, {"title": "Further Challenges of Propaganda Analysis", "content": "We identified that accurately pinpointing the occurrence of propaganda is a major challenge in propaganda analysis. As highlighted in the case study (Appendix F), LLMs often misclassify non-propagandistic sentences as propagandistic, leading to a high false positives rate. This issue may be partially attributed to hallucination or failing to account for subtle contextual differences. Although less frequent, similar errors occur with MGNN, indicating that the problem lies not only in the models themselves, but also in the training methodologies and the underlying algorithms. This underscores the need for improvements in both model development and in the training approaches to better distinguish propagandistic content from neutral text."}, {"title": "Related Work", "content": "Propaganda Detection Propaganda detection has long been a focus in both Natural Language Processing, with most work focusing on identifying propaganda usage and specific techniques. Various learning-based approaches have improved performance (Da San Martino et al., 2019; Yoosuf and Yang, 2019; Li et al., 2019; Vorakitphan et al., 2022) and interpretability (Yu et al., 2021) in detecting propaganda in news articles (Vlad et al., 2019; Da San Martino et al., 2019; Gupta et al., 2019; Yu et al., 2021) and tweets (Vijayaraghavan and Vosoughi, 2022; Khanday et al., 2021; Guarino et al., 2020). Recent efforts have also applied LLMs to this task (Sprenkamp et al., 2023; Jones, 2024). While these studies focus on identifying propaganda techniques, further research is needed to understand the appeals and intent behind them.\nFollowing the escalation of the Russo-Ukrainian conflict in 2022, research has focused on analyzing propaganda campaigns, particularly from Russia. Chen and Ferrara (2023); Fung and Ji (2022) collected user content and opinions from social media platforms such as X and Weibo, while Golovchenko (2022) examined censorship of Ukrainian content on Russian platforms. Geissler et al. (2023) studied pro-Russian sentiment on social media and the role of bots, and Patrona (2022) explored intertextuality and rhetoric in political performances during the war. However, few studies developed frameworks to analyze the specific intent behind propagandistic efforts. Ai et al. (2024) examined two specific propaganda narrative intentions, but failed short of proposing a generalizable framework for propaganda analysis.\nPropaganda Generation Compared to propaganda detection, research on propaganda generation is sparse. Zellers et al. (2019) explores generating propaganda to spread targeted disinformation, while Huang et al. (2023) focuses on incorporating emotional and non-emotional propaganda techniques into generated articles. Goldstein et al. (2024) find that GPT-3 can generate highly persuasive propaganda. Our data generation pipeline goes further by allowing a broader range of propaganda techniques to be inserted into generated articles to evoke specific intent, while allowing for more granular analysis of the appeals behind their use.\nUser Intent Detection Previous methods on intent detection concentrated primarily on understanding user queries in human-machine dialogue systems (Brenes et al., 2009; Liu et al., 2019; Weld et al., 2022). This research facilitated the construction of more robust search engines and virtual assistants. The similarity of this task to ours is that both tasks require strong natural language understanding. However, detecting user query intent is relatively superficial compared to the intent behind a propaganda tactic, which could be highly concealed and hard to recognize (Jowett and O'Donnell, 2012)."}, {"title": "Conclusion and Future Work", "content": "We proposed a comprehensive approach to propaganda analysis that goes beyond simply identifying techniques and addresses the common challenge of obtaining high-quality human-annotated data. We further introduced PropaInsight, a conceptual framework for granular propaganda analysis that identifies propaganda techniques, arousal appeals, and underlying intent, grounded in foundational social science research. Moreover, we presented PropaGaze, a novel dataset for fine-grained propaganda analysis that includes both human-annotated and high-quality synthetic sub-datasets. Our experiments showed that models fine-tuned on PropaGaze outperform one-shot GPT-4-Turbo by a margin. PropaGaze proved highly beneficial in data-sparse and cross-domain scenarios, serving as an effective complement to limited human-annotated data.\nFurthermore, PropaInsight has broader implications beyond propaganda analysis. It enhances tasks such as disinformation detection (Guess and Lyons, 2020; Ai et al., 2021; Huang et al., 2023), sentiment analysis (Ahmad et al., 2019), narrative framing (Colley, 2019; Andersen and Sandberg, 2020), media bias analysis (Nakov and Da San Martino, 2021; Zollmann, 2019), and social media monitoring (Chaudhari and Pawar, 2021), offering deeper insights into manipulative content and coordinated disinformation campaigns, making the framework applicable to a wide range of areas.In the future, we plan to expand PropaGaze into more diverse domains and genres, which will further broaden the scope of propaganda analysis. We will also explore how PropaInsight can improve downstream applications and contribute to a deeper understanding of propaganda."}, {"title": "Limitations", "content": "We reflect on the limitations of our work below:\n1.  Although our dataset PropaGaze is novel and reliable, its size is relatively small due to the computational costs associated with GPT-4 and the high expense of human annotation. Consequently, we are uncertain about the dataset's ability to generalize across a broad range of domains when models are fine-tuned exclusively on it.\n2.  While we aimed to include diverse domains and construct a cross-domain dataset, the vast range of real-world scenarios exceeds what we could capture. The extent of the domain gap where propaganda thrives remains unclear, and therefore, the cross-domain performance we tested across our paper might not generalize well under varied conditions.\n3.  Despite our careful calibration of the proposed propaganda framework, the real-world responses such as reader engagement and ultimate impact can vary significantly. Personalized appeals may emerge, influencing the effectiveness of propaganda; however, our study did not account for these individual differences. We did not take this into consideration and leave this part for future work.\n4.  Although we use a partially controlled pipeline to generate synthetic data and have basic human reviewers skim the content to ensure its overall quality, a more fine-grained review is necessary. Specifically, we need to assess whether the sampled propaganda techniques are contextually appropriate within each article. Additionally, while we treat the injected techniques as silver labels for our experiments, we do not check whether other sentences in the article, beyond those explicitly marked, also employ propaganda techniques. This means that our synthetic sub-datasets have high precision in labeled techniques but have not been evaluated for recall. It is likely that in reshaping the articles, additional sentences may also use propaganda techniques not explicitly labeled. Further evaluation, potentially involving more comprehensive human annotation, is needed for a more granular assessment of the dataset's quality.\nBased on this, we propose several promising future directions to further push for the success of combating misinformation."}, {"title": "Ethical Considerations", "content": "Our paper introduces a pipeline capable of generating news articles with strong intent and the potential for propaganda use. While our primary goal is to leverage this synthetic data to combat misinformation, it is important to acknowledge that the same technology could be misused to produce high-quality deceptive news content that could mislead public opinion. Consequently, the use of this pipeline must be approached with caution and safeguarded to prevent exploitation by malicious actors."}, {"title": "Details of a Propaganda Frame", "content": "We list the closed set of propaganda techniques that are used in the paper in Table 6. We also included the full template that we used to describe appeals and intent. Note that (1) The set of propaganda techniques included here can be freely extended with any other techniques. (2) We made the templates for Appeals and intent with a valid rationale, as detailed in Section\u00a7 2. However, we are not claiming that this is the optimal template design among all possible designs. We believe that prompt engineering and further human assessment are necessary for discovering the optimal template for this task. And we leave this part for future work.\nAppeals Raised by Propaganda Usage\n1.  Make the readers feel [Emotion] about [Sth that is related]\n2.  Make the readers realize that [Sth that is related]\n3.  Make the readers ignore that [Sth that is related]\nUltimate Intent Generation\n[Anything that is applicable for explaining the intent]"}, {"title": "Data Generation Prompt Templates", "content": "Step1: News Summarization\nSystem Message: You are a helpful assistant.\nNews: {news}\nProvide an objective summary of the news article, ensuring to present information in neutral terms. Avoid using language or phrases that show bias towards either party involved.\nStep 2: Intent Creation\nSystem Message: You are a helpful assistant.\nArticle: {article}\nIdentify all parties mentioned in the article. Select one party randomly and create an intent narrative to potentially reshape the article.\nStep 4: Techniques Insertion\nSystem Message: You are a skilled journalist, proficient in composing short brief news pieces.\nArticle: {article}\nRewrite the article into a short news piece to {intent}. Convey the intent narrative effectively by applying the following rhetorical tactics, once or more as needed. The revision must be concise, with a clear emphasis on using these tactics to communicate the intended message. Avoid generating non-factual information.\n1.  {appeal tactic 1}\nExample: {appeal tactic 1 example}\n2.  {appeal tactic 2}\nExample: {appeal tactic 2 example}\n3.  {appeal tactic 3}\nExample: {appeal tactic 3 example}\nStep 4: Appeals Generation\nSystem Message: You are a helpful assistant that identifies how the writer of a news article wants the readers of the article to feel after reading some sentences.\nIn this task, the input will be a news article, then some sentence in the article will be provided and you need to identify how the specific sentence raise appeals among the readers, the propaganda tactics used in these sentences will also be proivded as a hint. Also remeber that your response should be aware of the main goal of the whole article. For each sentence, you only need to output a sentence describing the feelings in one of the following two templates:\nMake the readers feel [Positive & Negative Emotions] about [Something that is related]\nMake the readers realize/Ignore [Something that is related]\nHere is an example indicating the input and output format:\nInput: News article: {article}"}, {"title": "Templates and Prompts We Used for Propaganda Analysis", "content": "This section describes each generation and prompt used in this paper. While these prompts could be enhanced through prompt engineering and additional human evaluation", "task": "nThis article uses propaganda to further its author's ultimate intent of {The ultimate intent that is predicted by the model"}, ".", "Specifically, the author uses {The first identified propaganda technique} in the sentence: \"{The first sentence that is identified to use propaganda}\" to make the readers {The first appeal that is raised among the readers}. The author also uses {The second identified propaganda technique} in the sentence: \"{The second sentence that is identified to use propaganda}\" to make the readers {The second appeal that is raised among the readers}...\nPrompt Template for the Language Models to Analyze Propaganda in a Zero-shot Manner\nWe use the following prompt to encourage language models to correctly predict the elements within a propaganda frame, this prompt also enables simple parsing to obtain the results.\nNews article: {The news article that needs to be analyzed}\nGiven the news article above, you should detect the major intent of the article. The intent is conveyed by using certain tactics and raise appeals in some text spans. You are also going to output all the text spans and the corresponding tactics and appeals.\nThe tactics that maybe used are listed here: loaded language, flag waving, slogans, repetition, straw man, red herring, whataboutism, obfuscation, causal oversimplification, false dilemma, thought terminating cliche, appeal to authority, bandwagon, glittering generalities, name calling, doubt, smears, reducito ad hitlerum\nYou should also formulate the generated appeals in the following format, choose one of the following template to fill in the appeals:\nMake the readers feel [Some Emotion"]}