{"title": "Unconditional stability of a recurrent neural circuit implementing divisive normalization", "authors": ["Shivang Rawat", "David J. Heeger", "Stefano Martiniani"], "abstract": "Stability in recurrent neural models poses a significant challenge, particularly in developing biologically plausible neurodynamical models that can be seamlessly trained. Traditional cortical circuit models are notoriously difficult to train due to expansive nonlinearities in the dynamical system, leading to an optimization problem with nonlinear stability constraints that are difficult to impose. Conversely, recurrent neural networks (RNNs) excel in tasks involving sequential data but lack biological plausibility and interpretability. In this work, we address these challenges by linking dynamic divisive normalization (DN) to the stability of \"oscillatory recurrent gated neural integrator circuits\u201d (ORGANICs), a biologically plausible recurrent cortical circuit model that dynamically achieves DN and has been shown to simulate a wide range of neurophysiological phenomena. By using the indirect method of Lyapunov, we prove the remarkable property of unconditional local stability for an arbitrary-dimensional ORGANICs circuit when the recurrent weight matrix is the identity. We thus connect ORGANICs to a system of coupled damped harmonic oscillators, which enables us to derive the circuit's energy function, providing a normative principle of what the circuit, and individual neurons, aim to accomplish. Further, for a generic recurrent weight matrix, we prove the stability of the 2D model and demonstrate empirically that stability holds in higher dimensions. Finally, we show that ORGANICs can be trained by backpropagation through time without gradient clipping/scaling, thanks to its intrinsic stability property and adaptive time constants, which address the problems of exploding, vanishing, and oscillating gradients. By evaluating the model's performance on RNN benchmarks, we find that ORGANICS outperform alternative neurodynamical models on static image classification tasks and perform comparably to LSTMs on sequential tasks.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks (DNNs) have found widespread use in modeling tasks from experimental systems neuroscience. The allure of DNN-based models lies in their ease of training and the flexibility they offer in architecting systems with desired properties [1-3]. In contrast, neurodynamical models like the Wilson-Cowan [4] or the Stabilized Supralinear Network (SSN) [5] are more biologically plausible than DNNs, but these models confront considerable training challenges due to the lack of stability guarantees for high-dimensional problems. The primary reason training RNNs is more straightforward is because of specialized ad hoc implementation of regularization techniques like batch and layer normalization, and gradient clipping/scaling. These methods help stabilize training"}, {"title": "2 Related Work", "content": "Trainable Biologically Plausible Neurodynamical Models: There have been several attempts to develop neurodynamical models that mimic the function of biological circuits and that can be trained on cognitive tasks. Song et al. [54] incorporated Dale's law into the vanilla RNN architecture, which was successfully trained across a variety of cognitive tasks. Building on this, Soo et al. [55] developed a technique for such RNNs to learn long-term dependencies by using skip connections through time. ORGANICs is a model that is already built on biological principles, and can learn long-term dependencies intrinsically, therefore it does not require the method used in [55]. Soo et al. [53] introduced a novel training methodology (dynamics-neural growth) for SSNs and demonstrated its utility for tasks involving static (time-independent) stimuli. Contrastingly, This training approach is costly and difficult to scale (because SSNs, unlike ORGANICs, are not unconditionally stable), and its applicability on tasks with dynamically changing inputs remains unclear.\nDynamical Systems View of RNNs: The stability of continuous-time RNNs has been extensively studied and discussed in a comprehensive review by Zhang et al. [56]. Recent advancements have focused on designing architectures that address the issues of vanishing and exploding gradients, thereby enhancing trainability and performance. A central idea in these designs is to achieve better trainability and generalization by ensuring the dynamical stability of the network. Further to avoid the problem of vanishing gradients the key idea is to constrain the real part of the eigenvalues to be close to zero, which facilitates the propagation and retention of information over long durations of time. Chang et al. [57] and Erichson et al. [58] achieve this by imposing an antisymmetric constraint on the recurrent weight matrix. Meanwhile, Rusch et al. [59, 60] propose an architecture based on coupled damped harmonic oscillators, resulting in a second-order system of ordinary differential equations that behaves similarly to how ORGANICs behaves in the vicinity of the normalization fixed point, as we show in Section 4. Despite their impressive performance on various sequential data benchmarks, these models lack biological plausibility due to their use of saturating nonlinearities (instead of normalization) and unrealistic weight parametrizations (e.g., requiring strictly antisymmetric weight matrices)."}, {"title": "3 Model description", "content": "In its simplest form, the two-neuron-type ORGANICs model [46, 47] with n neurons of each each type can be written as,\n$\\begin{equation}\n\\begin{aligned}\n\\tau_y \\odot \\dot{y} &= -y + b \\odot z + (1 - \\sqrt{[a]}) \\odot W_r (\\sqrt{y^+} - \\sqrt{y^-})\\\\\n\\tau_a \\odot \\dot{a} &= - a + b_o \\odot \\sigma^2 + W ( (y^+ + y^-) a^{+2})\n\\end{aligned}\n\\end{equation}$$\nwhere $\\odot$ denotes element-wise multiplication of vectors, squaring/rectification/square-root/division are performed element-wise, and 1 is an n-dimensional vector with all entries equal to 1. Here, $z\\in \\mathbb{R}^n$ is the input drive to the circuit and is a weighted sum of the input ($x \\in \\mathbb{R}^m$), i.e., $z = W_{zx}x$; $y \\in \\mathbb{R}^n$ and $a \\in \\mathbb{R}^n$ are the membrane potentials (relative to an arbitrary threshold potential that we take to be 0) of the excitatory (E) and inhibitory (I) neurons, respectively, that evolve according to the dynamical equations defined above. Note also that the sign of the potential is arbitrary and depends on the sign of the input, as described below. $\\dot{y}$ and $\\dot{a}$ denotes the respective time derivatives. The firing rates of E and I neurons are $y^+ = [y]^2$ and $a^+ = \\sqrt{[a]}$, respectively, and they are found by applying rectification ($[\\cdot]$) followed by a power function. For the derivation of a general model with arbitrary power-law exponents, including the Eq. 1, see Appendix A. Note that the term $\\sqrt{y^+} - \\sqrt{y^-}$ serves the purpose of defining a mechanism for recovering the membrane potential (which can be negative) from the firing rates $y^\\pm$ that are strictly non-negative. $y^+$ and $y^-$ are the firing rates of neurons with complementary receptive fields such that they encode inputs with positive and negative signs, respectively. Note that only one of these neurons fires at a given time. In ORGaNICs, these neurons have a single dynamical equation for their membrane potentials, where the sign of y indicates which neuron is active. Neurons with such complementary (anti-phase) receptive fields are found adjacent to one another in the visual cortex [61], and we hypothesize that such complementary"}, {"title": "4 Stability analysis of high-dimensional model with identity recurrent weights", "content": "We consider the stability of the general high-dimensional ORGANICs circuit (Eq. 1) when the recurrent weight matrix is identity, $W_r = I$. We first simplify the dynamical system by noting that $\\sqrt{y^+} - \\sqrt{y^-} = y$ and $y^+ + y^- = y^2$ giving us the following equations,\n$\\begin{align}\n\\tau_y \\odot \\dot{y} &= -y + b \\odot z + (1 - \\sqrt{[a]}) y  \\\\\n\\tau_a \\odot \\dot{a} &= - a + b_o \\odot \\sigma^2 + W (y^2 [a])\n\\end{align}$$\nUnder these constraints, we have a unique fixed point, given by,\n$\\begin{equation}\ny_s = \\frac{b \\odot z}{\\sqrt{b_o \\odot \\sigma^2 + W (b^2 \\odot z^2)}}; \\quad\na_s = b_o \\odot \\sigma^2 + W (b^2 \\odot z^2)\n\\end{equation}$$\nSince the normalization weights in the matrix W are nonnegative, at steady-state we have $a_s > 0$, so that $[a_s] = \\sqrt{a_s}$, and the corresponding firing rates at steady state are\n$\\begin{equation}\ny^+ = \\frac{[b \\odot z]^2}{b_o \\odot \\sigma^2 + W (b^2 \\odot z^2)}; \\quad\na_s = b_o \\odot \\sigma^2 + W (b^2 \\odot z^2)\n\\end{equation}$$\nNote that we recover the normalization equation, Eq. 2, if $b = b_o$. Since the fixed points of the E and I neurons are known analytically, to prove that this fixed point is locally asymptotically stable, i.e., responses asymptotically converge to the fixed point, we apply the indirect method of Lyapunov to the system about this fixed point [62]. This method allows us to analyze the stability of the nonlinear system in the vicinity of the fixed point by studying the corresponding linearized system. The Jacobian matrix $J \\in \\mathbb{R}^{2n \\times 2n}$ about $(y_s, a_s)$, defining the linearized system, is given by,\n$\\begin{equation}\nJ = \\begin{bmatrix}\n-D(\\frac{1}{\\tau_y}) & -D(\\frac{2y_s}{\\sqrt{a_s} \\odot \\tau_y})\\\\\nD(\\frac{2[y]^2}{\\sqrt{a_s} \\odot \\tau_y}) WD(a_s \\odot y_s) & D(\\frac{1}{\\tau_a}) (-I + WD (y^2))\n\\end{bmatrix}\n\\end{equation}$$\nwhere $D(x)$ is a diagonal matrix of appropriate size with the element of vector x on the diagonal. A necessary and sufficient condition for local stability is that the real parts of all eigenvalues of this matrix are negative. We proceed by first computing the characteristic polynomial for the Jacobian. The roots of this polynomial found by setting $det(J - \\lambda I) = 0$ are the eigenvalues of the system. Consider the block matrix\n$\\begin{equation}\nJ - \\lambda I = \\begin{bmatrix}\nA_{11} & A_{12}\\\\\nA_{21} & A_{22}\n\\end{bmatrix} = \\begin{bmatrix}\n-D(\\frac{1}{\\tau_y}) - \\lambda I & -D(\\frac{2y_s}{\\sqrt{a_s} \\odot \\tau_y})\\\\\nD(\\frac{2[y]^2}{\\sqrt{a_s} \\odot \\tau_y}) WD(a_s \\odot y_s) & D(\\frac{1}{\\tau_a}) (-I + WD (y^2)) - \\lambda I\n\\end{bmatrix}\n\\end{equation}$$"}, {"title": "5 Stability analysis for arbitrary recurrent weights", "content": "Now, we allow the recurrent weight matrix to not be constrained to the identity matrix, $W_r \\neq I$ and see how the stability result changes. Relaxing this constraint gives us the following set of equations,\n$\\begin{align}\n\\tau_y \\odot \\dot{y} &= -y + b \\odot z + (1 - \\sqrt{[a]}) W_ry  \\\\\n\\tau_a \\odot \\dot{a} &= - a + b_o \\odot \\sigma^2 + W (y^2 [a])\n\\end{align}$$\nThe linear stability analysis becomes intractable for a general W, because we no longer have an analytical expression for the steady states of y and a. Additionally, the characteristic polynomial cannot be expressed in a way similar to Eq.8. Nevertheless, for a two-dimensional system, given by,\n$\\begin{align}\n\\tau_y\\dot{y} &= -y+bz + (1 - \\sqrt{[a]}) w_{ry}\\\\\n\\tau_a\\dot{a} &= -a + b_o \\sigma^2 + wy^2[a]\n\\end{align}$$\nwe can prove the following, with a detailed analysis provided in Appendix E,"}, {"title": "6 Experiments", "content": "We provide empirical evidence in support of the conjecture that ORGANICs is asymptotically stable by showing that we can train stable ORGANICs using na\u00efve BPTT on two different tasks: 1) Static input of MNIST handwritten dataset, 2) Sequential pixel-by-pixel MNIST trained as an RNN. Because these machine learning tasks have no relevance for neurobiological or cognitive processes, we relax one aspect of the biological plausibility of ORGANICs, specifically, allowing arbitrary (learned) non-negative values for the intrinsic time constants 1."}, {"title": "6.1 Static input classification task", "content": "We first show that we can train ORGANICS on MNIST handwritten digit dataset [70] presented to the circuit as a static input. This setting corresponds to evolving the responses of the neurons dynamically until they reach a fixed point solution and using the steady-state firing rates of the principal neurons to predict the labels, akin to deep equilibrium models [71]. While the fixed point of the circuit is known when $W_r = I$ (given by Eq. 88), we allow $W_r$ to be learnable and parameterized it to have a maximum singular value of 1. This constraint allows us to find the fixed point responses of all the neurons without simulation, using an iterative algorithm that converges with great accuracy in a few (less than 5) steps Fig. 4 & 5. We provide an intuition for why this algorithm works with empirical evidence of fast convergence in Appendix G. Constraining the maximum singular value to 1 yields a simpler iterative scheme given by Algorithm 1.\nWe trained ORGANICs on this task (details provided in Appendix I.1) and compare its performance to SSN trained by dynamics-neutral growth [53]. We found that ORGANICs performs better than SSN with the same model size and as well as an MLP [53]. We analyzed the eigenvalues of the Jacobian matrix of the learned circuit and found that the largest real part was consistently negative (Fig. 5), indicating stability and verified that stability is maintained during training (Fig. 6)."}, {"title": "6.2 Time varying input", "content": "We trained unconstrained ORGANICs by na\u00efve BPTT on a classification task of sequential MNIST (sMNIST), proposed by Le et al. [72]. This is a challenging task because it involves long-term dependencies and requires the architecture to maintain and integrate information over long timescales. Briefly, the task involves the presentation of pixels of MNSIT images sequentially (one pixel at a given timestep) in scanline order, and at the end of the input the model has to predict the digit that was presented. There is a more complicated version of this task, permuted sequential MNIST, in which the pixels of all images are permuted in some random order before being presented sequentially. We train ORGANICs with different hidden layer sizes (number of E neurons) on these two tasks by discretizing the rectified ORGaNICs with arbitrary recurrence, Eq. 86, which has all the properties that we have derived for the main model. The hidden states of the neurons are initialized with a uniform random distribution. For more details, see Appendix I.2. Since the presence of an unstable fixed point is undesirable in such a task because it can lead to exploding trajectories, we prefer the rectified model (Appendix F) for this task over the main model as we have proof that the 2D rectified ORGANICs (Eq. 101) does not exhibit an unstable fixed point for positive inputs Fig 1. Additionally, we make the input gains b and bo dynamical with their ODEs given by,\n$\\begin{align}\n\\tau_b \\odot \\dot{b} &= -b + f(W_{bx}x + W_{by}y + W_{ba}a)\\\\\n\\tau_{bo} \\odot \\dot{b_o} &= -b_o + f(W_{box}x + W_{boy}y + W_{boa}a)\n\\end{align}$$"}, {"title": "7 Discussion", "content": "Summary: While extensive research has been conducted to design highly performing RNN ar- chitectures that can model complex data, there has been little advancement in developing robust, biologically plausible recurrent neural circuits that are easy to train and perform comparably to their artificial counterparts. Regularization techniques such as batch, group, and layer normalization have been developed and are implemented as ad hoc add-ons making them biologically implausible. In this work, we bridge these gaps by leveraging the recently proposed ORGANICs model which implements divisive normalization (DN) dynamically in a recurrent circuit. We establish the unconditional stability of an arbitrary dimensional ORGANICs circuit with an identity recurrent weight matrix (W), with all of the other parameters and inputs unconstrained, and provide empirical evidence of stability for ORGaNICs with arbitrary Wr. Since ORGANICs remain stable for all parameter values and inputs, we do not need to resort to techniques that are restrictive in parameter space, or that require designing unrealistic structures for weight matrices. This allows for the use of vanilla BPTT without the need for gradient clipping, a common practice in training LSTMs, thereby avoiding the issue of exploding, and oscillating gradients. Moreover, ORGANICs effectively address the vanishing gradient problem often encountered in training RNNs by processing information across various timescales, resulting in a blend of lossy and non-lossy neurons while maintaining stability. The model's effectiveness in overcoming vanishing gradients is further evidenced by its competitive performance against architectures specifically designed to address this issue, such as LSTMs.\nDynamic normalization: Normalization techniques, such as batch and layer normalization, are fundamental in modern ML architectures significantly enhancing the training and performance of CNNs. However, a principled approach to incorporating normalization into RNNs has remained elusive. While layer normalization is commonly applied to RNNs to stabilize training, it does not influence the underlying circuit dynamics since it is applied a-posteriori to the output activations, leaving the stability of RNNs unaffected. Furthermore, DN has been shown to generalize batch and layer normalization [39], leading to improved performance [39\u201341]. ORGaNICs, unlike RNNs with layer normalization, implement DN dynamically within the circuit, marking the first instance of this concept being applied and analyzed in ML. Our work demonstrates that embedding DN within a circuit naturally leads to stability, which is greatly advantageous for trainability. This stability, a consequence of dynamic DN, sets ORGANICs apart from other RNNs by providing both output normalization and model robustness. As a result, ORGANICs can be trained using BPTT, achieving performance on par with LSTMs. The key insight is that the dynamic application of DN not only enhances training efficiency but also improves robustness. This illustrates how the incorporation of neurobiological principles can drive advances in ML.\nInterpretability: In the proof of stability, we establish a direct connection between ORGANICS and systems of coupled damped harmonic oscillators, which have long been studied in mechanics and control theory. This analogy not only enables us to derive an interpretable energy function for ORGANICs providing a normative principle of what the circuit aims to accomplish but also sheds light on why normalization is a canonical neural computation observed across different brain areas and species, due to its connection to stability. For a relevant ML task, having an analytical expression for the energy function allows us to quantify the relative contributions of the individual neurons in the trained model, offering more interpretability than other RNN architectures. For instance, Eq. 127 shows that the ratio of time constants for E-I neuron pairs determines how much weight a neuron assigns to divisive normalization relative to aligning its responses with the input drive z. This insight provides a clear functional role for each neuron in the trained model. Moreover, since ORGANICs are biologically plausible, we can understand how the various components of the dynamical system might be computed within a neural circuit bridging the gap between theoretical models and biological implementation, and offering a means to generate and test hypotheses about neural computation in real biological systems (which we will be reporting elsewhere).\nLimitations: Although the stability property pertains to a continuous-time system of nonlinear differential equations, typical implementations for tasks with sequential data involve an Euler dis- cretization of these equations for training purposes. This might lead to a stiff dynamical system, potentially causing numerical instabilities and explosive dynamics, highlighting the importance of carefully parameterizing time constants and choosing a small enough time step to maintain stable dynamics. The proof of unconditional stability is only tractable for the two-dimensional circuit and the high-dimensional circuit with W = I. Therefore, for the stability of ORGANICs with arbitrary Wr, we have to rely on these two special cases and empirical evidence. In the current form, the weight matrices of the input gain modulators, Wby, Wba, Wboy, and Woa, are each n \u00d7 n. As a result, the number of parameters grows more rapidly with the hidden state size compared to other RNNs. To mitigate this, we plan to explore using compact and/or convolutional weights to prevent a significant increase in the number of parameters as the hidden state size expands.\nAttention mechanisms in ORGANICS: ORGANICs have a built-in mechanism for attention: modu- lating the input gain b (e.g., as in Eq. 16), coupled with DN. This mechanism for attention matches experimental data on both increases in the gain of neural responses and improvements in behavioral performance Moreover, this mechanism performs a computation that is analogous to that of an attention head in ML systems (including transformers [2]) as both function through changing the gain over time. In ORGANICs, DN replaces the softmax operation typically used in an attention head.\nFuture work: This study has initially explored only a single layer of ORGANICs for the sequential tasks. Future work will examine how stacked layers with feedback connections, similar to those in the cortex, perform on benchmarks for sequential modeling and also on cognitive tasks with long-term dependencies. We have thus far shown that ORGANICs can address the problem of long-term dependencies by learning the intrinsic time constants. Future investigations will assess the performance of ORGANICs for tasks with long-term dependencies by learning to modulate the responses of the a and b neurons to control the effective time constant of the recurrent circuit (without changing the intrinsic time constants) i.e., implementing a working memory circuit capable of learning to maintain and manipulate information across various timescales."}, {"title": "A Derivation of the ORGANICs circuit", "content": "Here, we derive a generalized 2-neuron type (excitatory and inhibitory) ORGANICs model for a high dimensional input. The system presented in Eq. 1 is a special case of this generalized model where p = 2 and $a^+ = \\sqrt{[a]}$. Assuming W is the normalization weight matrix, and z is the input drive, we can write the normalization equations for principal neurons with complementary receptive fields as,\n$\\begin{equation}\ny_s = \\frac{[z]^p}{\\sigma^p + W ([z]^p + [-z]^p)}; \\quad\\frac{[-z]^p}{\\sigma^p + W ([z]^p + [-z]^p)}\n\\end{equation}$$\nNote that typically the exponent of the input p ~ 2 for cortical neurons. [z] and [-z]p represent the contribution of neurons with complementary receptive fields to the normalization pool. Mathematically, we have, $[z]^p + [-z]^p = |z|^p$.\nHere, we derive, for a general p, the dynamical equations that have the fixed point defined by the equation above. First, it is important to distinguish between the membrane potentials and the firing rates of neurons. The coarse (low-pass filtered) membrane potential of a given type of neuron is denoted by the vector, y, a, with the corresponding firing rates of y+(y), a+. The instantaneous firing rates of the neurons are related to the corresponding coarse membrane potential by applying rectification, denoted by [.], and a power law (sub/supra-linear) activation for different types of neurons Therefore, for a set of membrane potentials x the instantaneous firing rates x+ are given by [x]. Specifically for principal neurons, we have, $y^+ = [y]^2$ and $y^- = [-y]^2$. Combining the firing of principal neurons with the complementary receptive fields, $y^+$ and $y^-$, Eq. 17 can be alternatively written as,\n$\\begin{equation}\ny_s = y^+ + y^- = \\frac{|z|^p}{\\sigma^p + W |z|^p}\n\\end{equation}$$\nNow for the principal neuron yj receiving an input drive zj, we can rewrite the normalization equation for each neuron as,\n$\\begin{equation}\ny_j^p = \\frac{z_j^p}{\\sigma_j^p + \\sum_k W_{jk}z_k^p}\n\\end{equation}$$\nIn the ORGANICs paradigm the steady-state activity of the principal neurons (single equation for complementary receptive fields) is a weighted sum of input drive and recurrent drive,\n$\\begin{equation}\n\\tau_{y_j} \\frac{dy_j}{dt} = -y_j + \\underbrace{b_jz_j}_{Weighted input drive} + \\underbrace{(1 - \\sqrt{a_j}) \\sum_k W_{r_{jk}} ((\\sqrt{y_k})^+ - (\\sqrt{y_k})^- )}_{Weighted recurrent drive}\n\\end{equation}$$\nHere $w_{r_{jk}}$ are the weights of the recurrent weight matrix W encoding the recurrent/lateral connections between the principal neurons $y_j$; $y_k$ is the firing rate of the principal neuron k, given by applying the following nonlinearity on the membrane potential: $y_k = [y_k]^p$ and $y_k = [-y_k]^p$ is the firing rate of the complementary principal neuron k. $b_j$ is the gain to the input drive $z_j$ which simulates attention (realized via gain modulation). $(1 - \\sqrt{a_j})$ is the gain to the recurrent drive which controls the recurrent amplification.\nNow, we find the dynamics of the inhibitory neurons aj with firing rates a+, that yield stable dynamics with the fixed point given by Eq. 17. For simplicity, we assume that the recurrent weight matrix $W_I = I$. Also, note that the following identity holds: $(\\sqrt{y_k})^+ - (\\sqrt{y_k})^- = [y_k] - [-y_k] = y_k$. Therefore, Eq.20 can be simplified as,\n$\\begin{equation}\n\\tau_{y_j} \\frac{dy_j}{dt} = -y_j + b_jz_j + (1 - \\sqrt{a_j}) y_j\n\\end{equation}$$\nAt steady-state, the fixed-points (y, a), satisfy the following relationship,\n$\\begin{equation}\n\\sqrt{a_j}y = b_jz_j\n\\end{equation}$$\nTaking modulus and raising both sides to power p, we get,\n$\\begin{equation}\n|\\sqrt{a_j}y_j|^p = |b_jz_j|^p\n\\end{equation}$$"}, {"title": "B Stability theorem", "content": "Theorem B.1. Consider a system of linear differential equations with constant coefficients of the form,\n$\\begin{equation}\nI\\ddot{x} + B\\dot{x} + Kx = 0\n\\end{equation}$$\nwhere $B \\in \\mathbb{R}^{n\\times n}$ and $K \\in \\mathbb{R}^{n\\times n}$ is a positive diagonal matrix, therefore, $K > 0$. The dynamical system is globally asymptotically stable if B is Lyapunov diagonally stable.\nProof. The stability of the system is defined by solving the following associated quadratic eigenvalue problem,\n$\\begin{equation}\nL(\\lambda) = det(\\lambda^2 + B\\lambda + K)\n\\end{equation}$$\nThe spectrum of $L(\\lambda)$, i.e., {\u03bb\u2208 C : det(L(A)) = 0} are also known as the eigenvalues of the system. The system defined by Eq. 35 is globally asymptotically stable if all of the eigenvalues have negative real parts.\nWe take the direct Lyapunov approach to prove the stability of the linear system. We write Eq. 35 in the matrix form as $\\dot{z} = Jz$, where,\n$\\begin{equation}\nz = \\begin{bmatrix} x \\\\ \\dot{x} \\end{bmatrix} \\text{ and } J = \\begin{bmatrix}\n0 & I \\\\ -K & -B\n\\end{bmatrix}\n\\end{equation}$$\nWe will first prove the Lyapunov stability (Re(AJ) \u2264 0) of this system to find the appropriate block diagonal matrices and then we will prove global asymptotic stability. To prove Lyapunov stability (Appendix B.2), we propose a Lyapunov function $V(z) = z^TPz$, where P is a block positive definite matrix defined as follows,\n$\\begin{equation}\nP = \\begin{bmatrix}\nA & 0 \\\\ 0 & T\n\\end{bmatrix}\n\\end{equation}$$\nwhere $T \\in \\mathbb{R}^{n\\times n}$ is a positive diagonal matrix such that TB + BTT > 0 (notation for positive definite matrix) and $A \\in \\mathbb{R}^{n\\times n}$ a flexible symmetric positive definite matrix that we'll find using the second Lyapunov criteria. Note that such a matrix T exits since B is defined to be Lyapunov diagonally stable. It can be easily seen that P > 0 using the first criteria in Section B.1 since A > 0 and T 0. Additionally, since T > 0, it is invertible.\nNow, for Lyapunov stability, we need $\\dot{V}(z) = z^T (PJ + J^TP) z \u2264 0$. Therefore, we find A such that $Q = (PJ + J^TP)$ is positive semi-definite.\n$\\begin{equation}\nQ = (PJ + J^TP)\n\\begin{bmatrix}\nA & 0 \\\\ 0 & T\n\\end{bmatrix} \\begin{bmatrix}\n0 & I \\\\ -K & -B\n\\end{bmatrix} + \\begin{bmatrix}\n0 & -K \\\\ I & -B\n\\end{bmatrix} \\begin{bmatrix}\nA & 0 \\\\ 0 & T\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & A \\\\ -KT & -BT\n\\end{bmatrix} + \\begin{bmatrix}\n0 & -TK \\\\ A & -BT\n\\end{bmatrix}\n\\begin{bmatrix}\n0 & KT - A \\\\ TK - A & TB+BT\n\\end{bmatrix}\n\\end{equation}$$\nWe want to define A > 0, such that Q \u2265 0. Using the second criteria from Section B.1, we need TB + BT > 0 and \u2013(KT \u2013 A)(TB + BTT)=1(TK \u2013 A) ~ 0. The first condition is satisfied by the definition of T. For the second condition to be satisfied, an obvious candidate for A is TK. Note that both T and K are positive definite and diagonal, therefore they commute (KT = TK) and A is symmetric and positive definite. When A = TK, the LHS of the second condition becomes 00. Therefore, the system is Lyapunov stable.\nNow, we prove the global asymptotic stability of the system by again using the direct Lyapunov approach. We propose the same form of the Lyapunov function as before, i.e., V(z) = zTPz, where"}, {"title": "C Analytical eigenvalue for fully normalized circuit", "content": "Here we show that when all of the normalization weights in the system are equal, to value a, and the various parameters are scalars, i.e., $T_y = T_y1, T_a = T_a1, b_o = b_o1$ and $\\sigma = \\sigma_1$, we can derive an analytical expression for all of the eigenvalues. Considering these assumptions, we can break the determinant in Eq. 8 into a diagonal and non-diagonal part as\n$\\begin{equation}\ndet (J - \\lambda I) = det \\Bigg[\\lambda^2 I + \\lambda \\begin{bmatrix} \\frac{1}{\\tau_y} & 0 \\\\ 0 & \\frac{1}{\\tau_a} \\end{bmatrix}+ D \\bigg( \\frac{\\sqrt{a_s}}{\\tau_y}\\bigg) \\\\ WD (y_s^2)\\bigg", "Bigg": "n\\end{equation}$$\nConsider the non-diagonal part of the matrix in the determinant, (\u03bb/\u03c4\u03b1)WD (y). Since W is a matrix with all entries equal to a positive constant, a, it is rank 1. Therefore, it can be written as the following outer product,\n$\\begin{equation}\nW = \\alpha \\begin{bmatrix} 1\\\\1\\\\...\\\\1 \\end{bmatrix} [1 \\quad 1 \\quad ... \\quad 1", "1": "T and $v = (b^2 \\odot z^2) / (\\sigma^2 b_o^2 + W (b^2 \\odot z^2))$. We use the matrix determi- nant lemma which states that,\n$\\begin{equation}\ndet(A - \\gamma u v) = (1 - \\gamma v^T A^{-1}u) det(A)\n\\end{equation}$$\nThe matrix A is given by,\n$\\begin{align}\nA &= \\lambda^2 I + \\lambda \\begin{bmatrix} \\frac{1}{\\tau_y} & 0 \\\\ 0 & \\frac{1}{\\tau_a} \\end{bmatrix} \\\\\n&= \\lambda^2 I + \\lambda \\begin{bmatrix} D (\\frac{\\sqrt{a_s}}{\\tau_y}) & 0 \\\\ 0 & \\frac{1}{\\tau_a} \\end{bmatrix} +  D (\\frac{\\sqrt{\\sigma^2 b_o^2 + W (b^2 \\odot z^2)}}{\\tau_y T_a}}) D (\\frac{\\sqrt{\\sigma^2 b_o^2 + W (b^2 \\odot z^2)}}{\\tau_y T_a}})\n\\end{align}$$\nHere, ||x||, represents the Euclidean norm of x. Therefore, A = \u03b4I, where d is a quadratic scalar polynomial in \u5165. Now using Eq. 57, we can write\n$\\begin{equation}\ndet (J - \\lambda I) = \\Bigg(1 - \\frac{\\lambda \\alpha \\tau_y \\tau_a [T_y + T_a", "V[Iu": "Bigg)\\delta^{n}\n\\end{equation}$$\nSolving for the eigenvalues, det(J \u2013 XI) = 0, we get 2(n - 1) repeated solutions by equation $\\delta^{n-1} = 0$, where each solution is found by solving \u03b4 = 0,\n$\\begin{equation}\n\\lambda^2 + \\lambda [\\frac{1}{\\tau_y} + \\frac{\\sqrt{\\sigma^2 b_o^2 + \\alpha||[bz", "2}}{\\tau_a}": "frac{\\sqrt{\\sigma^2 b_o"}]}