{"title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity", "authors": ["Hongjie Wang", "Chih-Yao Ma", "Yen-Cheng Liu", "Ji Hou", "Tao Xu", "Jialiang Wang", "Felix Juefei-Xu", "Yaqiao Luo", "Peizhao Zhang", "Tingbo Hou", "Peter Vajda", "Niraj K. Jha", "Xiaoliang Dai"], "abstract": "Text-to-video generation enhances content creation but is highly computationally intensive: The computational cost of Diffusion Transformers (DiTs) scales quadratically in the number of pixels. This makes minute-length video generation extremely expensive, limiting most existing models to generating videos of only 10-20 seconds length. We propose a Linear-complexity text-to-video Generation (LinGen) framework whose cost scales linearly in the number of pixels. For the first time, LinGen enables high-resolution minute-length video generation on a single GPU without compromising quality. It replaces the computationally-dominant and quadratic-complexity block, self-attention, with a linear-complexity block called MATE, which consists of an MA-branch and a TE-branch. The MA-branch targets short-to-long-range correlations, combining a bidirectional Mamba2 block with our token rearrangement method, Rotary Major Scan, and our review tokens developed for long video generation. The TE-branch is a novel TEmporal Swin Attention block that focuses on temporal correlations between adjacent tokens and medium-range tokens. The MATE block addresses the adjacency preservation issue of Mamba and improves the consistency of generated videos significantly. Experimental results show that LinGen outperforms DiT (with a 75.6% win rate) in video quality with up to 15\u00d7 (11.5\u00d7) FLOPs (latency) reduction. Furthermore, both automatic metrics and human evaluation demonstrate our LinGen-4B yields comparable video quality to state-of-the-art models (with a 50.5%, 52.1%, 49.1% win rate with respect to Gen-3, LumaLabs, and Kling, respectively). This paves the way to hour-length movie generation and real-time interactive video generation. We provide 68s video generation results and more examples in our project website: https://lineargen.github.io/.", "sections": [{"title": "1. Introduction", "content": "Diffusion Models (DMs) [16, 54] have exhibited superior performance on various generative tasks, including image generation [5, 41, 46, 48], image editing [4, 22, 50, 70], 3D shape generation [34, 58], and video generation [1, 11, 42, 75]. Among them, high-resolution text-to-video generation is widely regarded as one of the most challenging tasks due to two key factors: (1) the immense complexity of predicting the values of hundreds of millions of pixels and (2) the human eye's acute sensitivity to inconsistencies across frames. Sora [1] and Movie Gen [42] achieve highly consistent video generation by scaling Diffusion Transformers (DiTs) [40] to tens of billions of parameters. However, the computational cost of DiTs scales quadratically in the resolution and length of generated videos, making it extremely expensive to generate long videos and limiting the raw video length of most existing models to 10-20 seconds.\nNumerous existing studies have focused on improving the efficiency of video generation. This can be categorized into two approaches: (1) sampling distillation [28, 63], which reduces the number of sampling steps, and (2) efficient architectural designs that lower the computational cost of each sampling step, which includes factorized attention [2, 62] and State Space Models (SSMs) [10, 37]. However, they either retain quadratic complexity or are restricted to generating low-resolution, short videos. It is challenging to perform high-resolution long video generation solely based on the linear-complexity SSMs like Mamba [12], due to its adjacency preservation issue [10]. Mamba was originally designed for language tasks, where the inputs are natively sequences. When it is adapted to the vision modality, rearranging 2D (images) or 3D (videos) tensors into a 1D sequence becomes a necessity. This rearrangement causes spatially and temporally adjacent tokens to become distant in the sequence. This significantly hurts the quality of generated images and videos [19] due to the inherent decay when Mamba calculates long-range correlations [12]. Although more sophisticated rearrangement methods [15, 19, 45] could alleviate this issue, they can hardly ensure consistency across frames when scaled to high-resolution long video generation.\nTo address the above challenge, we propose a Linear-complexity text-to-video Generation (LinGen) frame-"}, {"title": "2. Related Work", "content": "High-Quality Video Generation. Sora [1] was the first work to successfully produce high-resolution videos with exceptional consistency. It learns an encoded latent space and deploys a large-scale DiT embedded in it. Runway Gen3 [47], LumaLabs [33], and Kling [24] are subsequent works capable of generating highly consistent, high-resolution videos with high frame rates. MovieGen [42] generates photorealistic and highly consistent videos with all implementation details revealed. However, it scales the DiT to 30 billion parameters. Its quadratic complexity makes generating minute-length videos very difficult. Several open-source models [3, 62, 75] also aim to generate high-quality videos. However, the quality of their outputs still notably lags behind that of the aforementioned models. An alternative to DMs for video generation is the use of transformer-based language models, which auto-regressively generate video tokens [25, 38, 59, 69, 72]."}, {"title": "3. Methodology", "content": "The computational cost of self-attention scales quadratically with the number of tokens in the sequence, creating a bottleneck for DiT-based video generative models due to the extensive length of the encoded video token sequence [32, 64]. Such a quadratic complexity makes generating high-resolution minute-length videos extremely expensive. Therefore, we propose LinGen, a text-to-video generation framework that produces photorealistic videos with linear complexity, enabling high-resolution minute-length video generation at a low cost.\n3.1. Overview\nLinGen uses a Temporal AutoEncoder design that is similar to a prior work [42]. In the latent space, LinGen denoises tokens using Flow Matching [30] and the linear-quadratic t-schedule [42]. The denoising module of LinGen is shown in Fig. 2. We provide more implementation details in the Supplementary Material (Supp. Mat.) section. The cross-attention layer conditions on text embeddings projected by three encoders: UL2 [56], ByT5 [68], and MetaCLIP [67]. They take long prompts re-written"}, {"title": "3.2. MA-Branch: Targets Short-to-Long Range", "content": "Bidirectional Mamba2. Mamba2 [6] unifies SSMs and masked efficient attention by proposing a special SSM with an attention format (i.e., Structured State Space Duality). Compared to Mamba, Mamba2 is more hardware-friendly. Thus, we deploy the bidirectional version of Mamba2 in LinGen to obtain the complete correlation map, as shown in Fig. 3. The number of FLoating Point Operations (FLOPs) of this block is given by\n$C_{bimamba} = (6+\\frac{2}{d_h})ENd^2 + 4Ndsd + O(Nd)$,\nwhere E is the expansion factor, d is the dimension of token embedding vectors, N is the number of tokens, $d_s$ is the hidden state size, and $d_h$ is the head dimension of Mamba2, whose default value is 64. We provide the complete expression for $C_{bimamba}$ in Supp. Mat. This format shows that $C_{bimamba}$ scales linearly in N. The linear complexity of Mamba and Mamba2 makes them highly suitable for video generation, where latent space sequences often contain tens or even hundreds of thousands of tokens. However, videos generated by the native Mamba model exhibit high inconsistency, primarily due to the adjacency preservation issue when rearranging 3D tensor tokens into a sequence [10, 19]. Previous works have attempted to address this problem by mixing Mamba layers with global attention layers [10], thus"}, {"title": "3.3. TE-Branch: TEmporal Swin Attention", "content": "Besides the MA-branch, to further address the adjacency preservation issue and enhance video consistency, we propose TEmporal Swin Attention (TESA) to build the TE-branch, which gathers short-range information along the spatial dimension and medium-range information along the temporal dimension, as shown in Fig. 5. It is inspired by a prior window attention work [31], divides the 3D video token tensor into multiple windows, and calculates attention between tokens within the same window. Assuming the window size is $T_w \\times S_w \\times S_w$ and the video token tensor size is T \u00d7 H \u00d7 W, the FLOPs of TESA is given by\n$C_{TESA} = (8Nd^2 + 4N^2d) \\frac{T}{T_w} \\frac{H}{S_w} \\frac{W}{S_w}$,\nwhere $N_\\omega = T_w \\cdot S_w \\cdot S_w$ and d is the dimension of token embedding vectors. This equation indicates that $C_{TESA}$"}, {"title": "3.4. Training Recipe", "content": "Progressive Training. We use a progressive recipe (check details in Supp. Mat.) to pre-train our LinGen-4B model. We first pre-train our model on the text-to-image task at a 256p resolution, followed by text-to-video pre-training at progressively higher resolutions (256p to 512p) and longer video lengths (17s to 34s and then 68s).\nText-to-Image and Text-to-Video Hybrid Training. In the text-to-video pre-training stages, we incorporate text-image pairs into the pre-training dataset and perform text-to-image and text-to-video joint training in practice. We find such a hybrid training improves consistency of generated videos in some failure cases.\nQuality Tuning. Similar to the observation in prior works [5, 11], we find the quality of generated videos can be greatly enhanced by fine-tuning the model on a small set of high-quality videos. We select 3K high-quality videos from our pre-training dataset and fine-tune our model on them."}, {"title": "4. Experiments", "content": "In this section, we begin by describing the experimental settings in Sec. 4.1. We then illustrate the efficiency superiority of LinGen in Sec. 4.2. Next, we benchmark LinGen against state-of-the-art models in Sec. 4.3. In addition, we demonstrate rapid adaptation of LinGen to longer sequences in Sec. 4.4. Finally, in Sec. 4.5, we report on ablation studies that validate the effectiveness of individual modules and techniques incorporated into LinGen.\n4.1. Experimental Settings\nModels. (1) LinGen-4B. We build the denoising module of this model following the setting described in Sec. 3. We employ 32 layers with 20 heads in each, with the dimension of embedding vectors being 2560. (2) DiT-4B. We replace MATE blocks in LinGen-4B with global self-attention layers to build a standard DiT. Our DiT-4B has 32 layers with 24 heads in each, with the dimension of embedding vectors being 3072. (3) State-of-the-art models. We compare LinGen to state-of-the-art accessible commercial text-to-video generative models, including Runaway Gen3 [47], Kling [24], and LumaLabs [33], and a typical open-source"}, {"title": "4.2. Efficiency: Linear Computational Complexity", "content": "We compare the efficiency of DiT-4B and our proposed LinGen-4B in terms of FLOPs cost and latency. We show the results in Fig. 6. In terms of FLOPs, LinGen-4B achieves 5x, 8x, and 15\u00d7 speed-up relative to DiT-4B when generating 512p videos of 17s, 34s, and 68s length, respectively. In terms of latency, LinGen-4B achieves 2.0\u00d7 and 3.6\u00d7 speed-up relative to DiT-4B when generating 512p and 768p 17s videos on a single H100, respectively. LinGen-4B achieves 2.0\u00d7, 3.9\u00d7, and 11.5\u00d7 latency speed-up compared to DiT-4B when generating 512p videos of 17s, 34s, and 68s length, respectively. These results indicate that the cost of LinGen scales linearly in the number of pixels in generated videos, thus demonstrating huge efficiency and scalability superiority of LinGen."}, {"title": "4.3. Comparing Quality to State-of-the-Art Models", "content": "We evaluate the performance of our proposed LinGen-4B model and other text-to-video models in three ways: (1) Exhibit visual examples for eyeballing comparison, as shown in Fig. 7. We provide more examples in Supp. Mat. (2) Use human evaluation to perform A/B comparison and calculate win rates. (3) Use automatic quantitative metrics to compare LinGen with more existing text-to-video models. We use a standard video evaluation benchmark, VBench [20], to evaluate video quality and text-video faithfulness. VBench comprehensively evaluates text-to-video models using 16 disentangled dimensions. Each dimension is tailored to specific prompts and evaluation methods."}, {"title": "4.4. Adaptation to Longer Token Sequences", "content": "LinGen adapts to longer sequences of latent tokens more quickly than DiT. This could benefit from the strong adaptation ability of Mamba models to longer sequences, which has also been observed in language tasks [44]. We observe this phenomenon in the loss curves when transferring the model trained on 256p video generation to 512p generation in progressive training, as shown in Fig. 10 (a). We further conduct a human evaluation on the checkpoints at an early stage of 512p 17s video generation pre-training and 512p 34s video generation pre-training, as shown in Fig. 10 (b). The results validate our observation that LinGen adapts more quickly to longer sequences of latent tokens than DiT, which means better scalability for video generation at higher resolutions and longer lengths."}, {"title": "4.5. Ablation Experiments", "content": "For performance, we conduct ablation experiments on the 256p 17s video generation task in two ways: (1) Comparing loss curves. The prior work [42] has observed that the loss curve correlates well with visual quality evaluated by humans. Thus, we compare the loss curves under different training settings to validate their effectiveness, as shown in Fig. 11. (2) Performing human evaluations. We select corresponding checkpoints after 30K pre-training steps and perform A/B quality comparison between the default set-"}, {"title": "5. Conclusion", "content": "In the paper, we proposed LinGen, a linear-complexity text-to-video generation framework that enables high-resolution minute-length video generation on a single GPU. It replaces self-attention layers in DiTs with our novel MATE block, which inherits linear complexity from its two branches: MA-branch and TE-branch. Compared to the native Mamba block, MATE addresses its adjacency preservation issue and comprehensively enhances short-, medium-, and long-range correlations, improving the consistency and fidelity of generated videos significantly. Our experimental results show that LinGen achieves linear complexity and up to 11.5x speed-up in terms of latency, while maintaining the high quality of generated videos. LinGen presents a linear-complexity self-attention replacement, paving the way for broader adoption of this framework to hour-length video generation and real-time interactive video generation."}, {"title": "D.1. Backbone Details", "content": "LinGen learns a spatiotemporally compressed latent space using a Temporal AutoEncoder (TAE), designed similarly to the one in a prior work [42]. The TAE achieves a temporal compression rate of 8\u00d7 and a spatial compression rate of 8\u00d78, followed by a 2\u00d72\u00d71 patchification. LinGen uses a factorized learnable positional embedding [8] to enable arbitrary video size and length. We employ RMSNorm [73] and SwiGLU [49] in LinGen, with adaptive layer normalization conditioned on the time step [40].\nAfter completing architectural design exploration depicted in Fig. 20, we employ 32 layers with 20 heads in each, with the dimension of embedding vectors being 2560."}, {"title": "D.2. Mamba and Mamba2", "content": "SSMs have gained popularity in the field of natural language processing due to their high efficiency and strong performance in handling long sequences [13, 14]. Mamba [12], as a variant of SSM, enhances efficiency significantly by incorporating dynamic parameters into the SSM structure and developing algorithms optimized for better hardware compatibility. Based on this, Mamba2 [6] unifies SSMs and masks efficient attention by proposing a special SSM with an attention format (i.e., Structured State Space Duality). Mamba2 removes sequential linear projections that are used in Mamba and produces SSM parameters A, B, C in parallel. The normalization layer in Mamba2 is the same as that in [51]. It improves stability. As mentioned in our main paper, the FLOPs cost of a bidirectional Mamba2 module is given by\n$C_{bimamba} = (6+\\frac{2}{d_h})ENd^2 + 4Ndsd + O(Nd)$,\nwhere E is the expansion factor, d is the dimension of token embedding vectors, N is the number of tokens, ds is the hidden state size, and dh is the head dimension of Mamba2,\nwhose default value is 64. O(Nd) includes the FLOPs cost of 1D convolution and the SSM block in Mamba2:\n$C_{conv} = 2EK(N + K \u2212 1)d$\n$C_{SSM} = 4ENdsd + 2ENd$\nwhere K is the kernel size of 1D convolution. The above FLOPs should be doubled when the module is bidirectional.\nCompared to Mamba, Mamba2 (1) has an attention format and thus benefits from existing efficient attention kernels, such as FlashAttention [7] and xFormers [27], (2) supports much larger hidden state sizes with lower latency, and (3) has better support for tensor parallelism for upscaling of the model [60].\nAlthough Mamba2 compromises expressive power due to the simplification of the decay matrix in an SSM [6], it compensates for this using a much larger hidden state size. We set the hidden state size to 16 and 128 in LinGen w/ Mamba and LinGen w/ Mamba2, respectively, for both quality comparison and latency measurement, following their default values in the original design [6]."}, {"title": "D.3. Training Recipe Details", "content": "In this Section, we introduce our progressive training recipe in Sec. D.3.1. Then, we discuss our text-to-image and text-to-video hybrid training setting in Sec. D.3.2. We describe the details of our training datasets and quality-tuning design in Sec. D.3.3.\nD.3.1 Progressive Training Recipe\nWe use a progressive recipe to pre-train our LinGen-4B model. As shown in Table 7, we first pre-train our model on the text-to-image task at a 256p resolution, followed by text-to-video pre-training at progressively higher resolutions and longer video lengths. In this progressive training schedule, the token sequence length in the latent space gradually increases.\nD.3.2 Hybrid Training\nIn the text-to-video pre-training stages, we incorporate text-image pairs into the pre-training dataset and perform text-to-image and text-to-video joint training in practice. The sampling ratio of text-image pairs to text-video pairs is"}, {"title": "D.3.3 Quality Tuning and Datasets", "content": "We use a progressive training schedule to train our DiT-4B and LinGen-4B models. (1) Text-to-image pre-training at 256p resolution. We use the licensed ShutterStock [52] image dataset, which includes 300M text-image pairs, to train our models. (2) Text-to-video pre-training at 256p and 512p resolutions to generate 17s videos. We use the licensed ShutterStock video dataset, which includes 24M text-video pairs, to train our models. (3) Text-to-video pre-training at 512p resolution to generate 34s and 68s videos. We select 2.5M videos that are longer than 30 seconds from the licensed ShutterStock video dataset to train our models. (4) Text-to-video pre-training at 512p resolution to generate 68s videos. We select 145K videos that are longer than 60s from the licensed ShutterStock video dataset to train our models. (5) Text-to-video quality tuning at 512p resolution. For the 17s video generation, we select 3K videos with extremely high quality and good motions from the ShutterStock and RawFilm [43] video dataset to fine-tune our model. For 68s video generation, we select 300 minute-length videos with high quality and good motions from the ShutterStock video dataset to fine-tune our model.\nThe way that we select high-quality videos is similar to that in prior works [5, 42]. We first filter videos via automatic metrics, including aesthetic score and motion score."}]}