{"title": "Subword Embedding from Bytes Gains Privacy without Sacrificing Accuracy and Complexity", "authors": ["Mengjiao Zhang", "Jia Xu"], "abstract": "While NLP models significantly impact our lives, there are rising concerns about privacy invasion. Although federated learning enhances privacy, attackers may recover private training data by exploiting model parameters and gradients. Therefore, protecting against such embedding attacks remains an open challenge. To address this, we propose Subword Embedding from Bytes (SEB) and encode subwords to byte sequences using deep neural networks, making input text recovery harder. Importantly, our method requires a smaller memory with 256 bytes of vocabulary while keeping efficiency with the same input length. Thus, our solution outperforms conventional approaches by preserving privacy without sacrificing efficiency or accuracy. Our experiments show SEB can effectively protect against embedding-based attacks from recovering original sentences in federated learning. Meanwhile, we verify that SEB obtains comparable and even better results over standard subword embedding methods in machine translation, sentiment analysis, and language modeling with even lower time and space complexity.", "sections": [{"title": "1 Introduction", "content": "Advances in Natural Language Processing (NLP), such as Large Language Models (LLMs), have made noticeable advancements in performance over the last decades, partially attributed to the large datasets available. Since most data are from users, their privacy concerns play an increasingly critical role, which is essential to building user trust, encouraging the responsible use of language data, protecting personal information, ensuring ethical use, and avoiding potential harm to individuals.\nFederated learning (FL) enables training shared models across multiple clients without transferring the data to a central server to preserve user privacy. Although only the model updates are sent to the central server, adversaries can still use model updates to reconstruct the original data and leak sensitive information to compromise the user's privacy. In the illustrated example, the attacker extracts all candidate words in a batch of data from the embedding gradients and can easily reconstruct the text with beam search and reordering since one can perform straightforward lookups when a vector is updated due to the one-to-one mapping between word/subword tokens and embedding vectors.\nOur intuitive idea is to apply the byte embedding method because the same bytes are repeatedly used for multiple subwords. We aim to design a one-to-many mapping between words/subwords and embedding vectors to increase the difficulty of the simple lookup so that retrieving input subwords with the updated byte embeddings is harder, which makes the byte embedding in NLP models a potential defense. For example, in subword embedding, if the word \"good\" is updated, the attacker will only retrieve this word based on embedding updates. However, if we tokenize \u201cgood\u201d into four bytes, such as \u201c50, 10, 128, 32\", all subwords containing at least one of these bytes will be retrieved,"}, {"title": "2 Related Work", "content": "Attacks and defenses in language model Some recent works consider the reconstruction as an optimization task [32, 5, 2]. The attacker updates its dummy inputs and labels to minimize the distance between the gradients of the victim uploaded and the gradients the attacker calculated based on its dummy inputs and labels. [7] shows that the attackers can reconstruct a set of words with the embedding gradients, then apply beam search and reorder with a pretrained language model for input recovery. One defense described in [32, 5, 2] is to encrypt the gradients or make them not directly inferable. However, encryption requires special setups and could be costly to implement. Moreover, it does not provide effective protection against server-side privacy leakage [1, 8, 6]. Differential privacy is another defense strategy, but it can hurt model accuracy [32, 26, 29, 10]. While [30] proposed a secure federated learning framework that can prevent privacy leakage based on gradient reconstruction, it does not effectively address the retrieval of a bag of words from the embedding matrix gradients, as proposed in [7]."}, {"title": "3 Preliminaries", "content": "Subword-level and byte-level language models Subword tokenization such as BPE [19] has some limitations, despite the wide application. It cannot handle out-of-vocabulary subwords and requires language-specific tokenizers. Another challenge is the high space complexity of the embedding matrix when the vocabulary size is huge. Byte tokenization is a solution to address these issues [20, 31, 27]. UTF-8 can encode almost all languages. Therefore, there will be no out-of-vocabulary words and the language-specific tokenizer is unnecessary. In addition, as the total number of bytes in UTF-8 is 256, the embedding matrix for byte vocabulary is much smaller than most subword vocabularies, reducing the number of parameters in the embedding layer and saving memory space.\nSubword-level model with character- or byte-level fusion The character/byte-based models often result in longer input sequences and higher time complexity compared to the subword-based model. To make the model efficient, recent works have explored character/byte-level fusion. For example, [24] proposes CHARFORMER, using a soft gradient-based subword tokenization module to obtain \"subword tokens\". It generates and scores multiple subword blocks, aggregates them to obtain subword representation, and then performs downsampling to reduce the sequence length. Although CHARFORMER is faster than vanilla byte/character-based models, it does not maintain subword boundaries, limiting the model's interpretability. [23] proposes Local Bytes Fusion (LOBEF) to aggregate local semantic information and maintain the word boundary. However, it does not reduce the sequence length, making training and inference time-consuming."}, {"title": "3.1 Federated Learning", "content": "In federated learning (FL), multiple clients jointly train a model using their private data. Assume we have N clients, C = {C1, C2, . . ., CN}, and a server s, in an FL system. The jointly trained model is f with parameters \u03b8. The clients' private data are D1, D2, . . ., DN and the objective function is L. For easier illustration, we assume all clients participate in each communication and use FedSGD [13] to update the model parameters. In each communication round t, server s first sends the model parameters \u03b8t to all clients. Then each client ci compute \u2207\u03b8t L\u03b8t (Bi), the gradients of current model f\u03b8t, based on a randomly sampled data batch Bi \u2282 Di. After local computation, the clients send the gradients \u03941, \u03942, ..., \u0394N to server and server s aggregate all the gradients and update the model:\n\u03b8t+1 = \u03b8t - \u03b7 \\sum_{i=1}^{N} \u2207\u03b8t L\u03b8t (B_i)\nHere, Equation (1) is the gradient descent, and \u03b7 is the learning rate."}, {"title": "3.2 Threat Model", "content": "Adversary's capabilities and objective We follow the attack settings in [7]. The optimized model is a language model L, parameterized by \u03b8. This scenario makes the attacker white box access to the gradients \u2207\u03b8t L\u03b8t (Bi) sent by the victim client ci. \u03b8t is the model parameter that the server sends to the clients at any communication round t. From parameters \u03b8t and gradients \u2207\u03b8t L\u03b8t (Bi), the attacker can get the information of the vocabulary V and the embedding matrix W to retrieve which tokens are updated. The goal of the attacker is to recover at least one sentence from Bi, based on \u2207\u03b8t L\u03b8t (Bi) and \u03b8.\nAttack model This paper does not address the gradient leakage attack which aims to obtain private data by minimizing the difference between gradients derived from a dummy input and the actual gradients of the victim's data, because several methods have been proposed to mitigate this particular attack [32, 5, 26]. Instead, we focus on a specific attack model, FILM, introduced in [7], for which effective defenses have yet to be explored. In this model, the attacker attempts to reconstruct sentences from the victim's training batches as follows: (1) extracting candidate tokens from the gradients, (2) applying beam search with a pre-trained Language Model, such as GPT-2, to reconstruct the input sentence, and (3) reordering the subword tokens to achieve the best reconstruction."}, {"title": "4 Proposed Method", "content": "We propose Subword Embedding from Bytes (SEB) shown in Figure 2, including byte sequence for input text, byte embeddings, aggregation of byte embeddings, and a feed-forward network to output the subword embedding.\nWe aim to develop subword encoding using a smaller byte embedding matrix to save space and protect against attacks based on the embedding gradients while preserving subword boundaries to maintain the model's time efficiency. This raises two main challenges: 1) how to convert subwords into a byte sequence? and 2) how to obtain subword embeddings using byte representations?"}, {"title": "4.1 Subword to Byte Sequence Mapping", "content": "UTF-8 encoding results in different sequence lengths for subwords. In real practice, all byte sequences need to be padded to the same length, making the byte sequence of the subword even longer. Instead of using the existing byte encoding system, we define our subword to byte sequence mapping M : Vw \u2192 (Vb)n. Vw and Vb are subword and byte vocabularies with size of |Vw| and |Vb|, respectively. (Vb)n is a sequence of n bytes in Vb. Here the byte vocabulary size |Vb| and the number of bytes n to represent a subword are hyperparameters. In this way, every subword is represented with the same length, getting rid of the longer byte sequence with padding.\nTo construct the mapping, for every subword wi \u2208 Vw, we randomly sample n bytes with replacement from Vb to obtain the byte sequence (bi1, bi2, ..., bin). If the byte sequence already exists in M, we repeat the sampling until a unique byte sequence is obtained. For example, we set |Vb| = 64 and n = 4. A subword \u201cHello\u201d can be represented with (14, 3, 10, 4), shown in Figure 2(b).\nWe analyze the probability p that two subwords are mapped to the same byte sequence. With the byte vocabulary size |Vb| and the number of bytes per subword n, the probability p = 1/(|Vb|)n. For example, if |Vb| = 16 and n = 4 then p = 1.5 \u00d7 10\u22125. For |Vb| = 128 and n = 8 in our experiment, p = 1.39 \u00d7 10-17, which means there is almost no possibility to map two words into the same subword sequence. Therefore, SEB is highly expressive for representing subwords."}, {"title": "4.2 Subword Embedding from Bytes", "content": "Different from the byte-based models which have higher time complexity due to longer input sequences, our method tokenizes the text into a sequence of bytes while preserving the subword boundary. We first tokenize the original text into subwords using a common subword tokenization method such as BPE. Then, we token each subword into a byte sequence with the mapping we designed above and then aggregate byte representations back to subword embeddings. The two detailed algorithms are in Appendix, Algorithms 1 and 2.\nLet the byte embedding matrix be B \u2208 RVb\u00d7d, where d is the embedding size. Given a text S, we tokenize S into a subword sequence (w1,w2,..., wm), then further use the mapping M defined above to tokenize this sequence into a byte sequence (b11,..., b1n, ..., bm1,..., bmn) with mn bytes. We retrieve the byte embeddings E \u2208 Rmn\u00d7d for these bytes from B."}, {"title": "4.3 Complexity Analysis and Frequency Analysis", "content": "To demonstrate the efficiency of the proposed SEB, we summarize the space and time complexity of each embedding method in Table 1. Here, the column \"Memory\" represents the memory usage for each embedding, and the column \u201cTime\" shows the time complexity in Transformer attention. For simplicity, we let d' = d and use one linear layer as FFN in SEB which contains nd\u00b2 parameters.\nIn terms of space complexity, subword embeddings typically have an exponentially large vocabulary size Vw, exceeding 104, while the dictionary size of byte embeddings is no more than 256. For the proposed SEB, the number of parameters in embedding is O(nd\u00b2 + |Vb|d) = O((nd + |Vb|)d), including the FFN and byte embedding matrix. In practice, nd + |Vb| < Vw. As a result, both byte embeddings and our proposed SEB significantly reduce the memory cost required for embeddings.\nRegarding time complexity, we analyze the attention in the widely used Transformer [25]. Given the sequence length m, byte embedding is more time-consuming since the input length is c times longer than subword embedding. Here c is the average ratio between the lengths of byte and subword sequences. Based on the statistics [20], c is usually around 5. However, our proposed SEB maintains the same time efficiency as conventional subword embeddings because we preserve the subword sequence along with its boundaries.\nOne may consider the frequency analysis in cryptanalysis to get the original text if the attacker also has information about the tokens used for the text and the frequency of each byte."}, {"title": "5 Experiment", "content": "We conduct experiments to demonstrate the advantages of SEB in reducing space complexity, maintaining time efficiency, and preserving privacy for NLP models in federated learning. In all experiments, we set |Vb| = 256 and n = 8, which is sufficient to prevent encoding two subwords into the same byte sequences. We use a 2-layer FFN in the proposed SEB."}, {"title": "5.1 Experiments on Privacy Protection", "content": "Dataset, attack task, and evaluation metrics We followed the settings in the FILM attack [7]. The dataset is WikiText-103 [14]. For the attack task, we use GPT-2 base [18] with 117M parameters to recover the input batches. The ROUGE-1/2/L F-Scores [11] are used to evaluate the similarity between the recovered and original text.\nQuantitative analysis of defense We first show that it is difficult to retrieve a bag of candidate subwords in SEB with Figure 3 and 4. In Figure 3, we present the distributions of the subword number, unique subword number, and unique byte number in a client's batch of data. We observe that even a single sample contains over 120 unique bytes on average, while only having approximately 25 unique subwords. In Figure 4, we present the average coverage of subwords for a subset of bytes. Based on Figure 4, 120 bytes cover about 50K subwords. It means recovery is a random generation using almost the entire vocabulary.\nAdditionally, Figure 5 shows the FILM attack performances using various batches on WikiText-103, with subword embedding and SEB. As the candidate subwords are almost the whole vocabulary, beam search takes huge memory which is not executable on our device. To show the defense performance, we lose the constraints and randomly sample 7,000 subwords, combined with the subwords in the original text. We randomly select 5 tested batches for each batch size and take the average ROUGE F-Scores. When batch size is 1, ROUGE-1/2/L scores are close to 1 for attacks with subword embedding, indicating a nearly perfect recovery. However, these scores are quite low when using SEB, showing the effectiveness of SEB to defend the attacks based on embedding gradients.\nQualitative analysis of defense To intuitively show the difference between the recovered sentences of FILM using subword embedding and the proposed SEB, we select the best-recovered sentences of these two methods based on the ROUGE-L F-score and list the results in Table 2. In the recovered sentence with the subword embedding, all words are successfully retrieved and have a very close order to the original sentence. However, with SEB, only a few words are retrieved, and many of them are stop words. The results show that SEB can prevent the attacker from recovering private information in the original sentence even though the batch only contains one sentence.\nWe also compare our method with the defense method of gradient pruning in the FILM attack [7] for batch size = 8, 16, 32."}, {"title": "5.2 Experiment on Performance", "content": "To provide a more comprehensive assessment of our proposed technique's applicability and performance of federated learning and across different NLP tasks, we conduct experiments on machine translation, sentiment analysis, and Language Modeling (LM) tasks."}, {"title": "5.2.1 Translation", "content": "Dataset and evaluation metrics In the translation task, we consider two datasets, the medium-size dataset IWSLT14 [4] and the large-scale dataset WMT14 [3]. We follow the settings as prior work [20, 31] and translate German (de) to English (en) in IWSLT14 [4]. The translation of WMT is English (en) to German (de) and the preprocessing is the same as Fairseq [16]. We use SacreBLEU, case-sensitive, with the 13a tokenizer [17] as the evaluation metric.\nMain results For IWSLT14, we run 5 trials and report the average performance with the standard deviation. We show the translation results of Transformer with subword embedding and SEB. The hidden dimension of the two-layer FFN is 2048 for IWSLT because we try to keep the total parameters of SEBco the same as the original Transformer. For WMT, the hidden dimension of FFN is 4096. Here, we test three variants of SEB when aggregating the byte embedding back to subword embedding: added real-valued embedding (SEBar), concatenated real-valued embedding (SEBcr), and concatenated one-hot embedding (SEBco). In this experiment, the dimensions of real-valued and one-hot vectors are 512 and 256. Table 3 shows that SEBcr and SEBco can achieve better performances than subword embedding. Concatenating the one-hot vectors yields better results even with fewer model parameters than concatenating byte embedding. Therefore, we can conclude that SEB is a better alternative to using large subword embeddings. Additionally, based on the comparison between SEBar and SEBcr, we find that concatenation is better than the simple adding of byte embeddings. This is expected because adding does not consider the positional information of bytes. The result of WMT14 shows the same performance as the subword-based model but with a smaller size of embedding parameters. It is important to emphasize that while privacy is improved, our model achieves the same or better accuracy than the baselines.\nSensitivity analysis on |Vb| and n To investigate the impact of the byte vocabulary size |Vb| and number of bytes per subword n, we set |Vb| as 64, 128, 256 and n as 4, 8, 16. Based on the previous experiments, we set the hidden units in the 2-layer FFN to 1024 in SEBco, which provides good performance with a small scale of parameters. We first discuss the model size in terms of embedding parameter numbers in Table 4. All settings have smaller embedding parameter numbers than the original Transformer."}, {"title": "5.2.2 Sentiment Analysis", "content": "Dataset and evaluation metrics We use IMDb [12] and SST2 [22] datasets provided by Hugging Face. The detailed preprocessing of the dataset is shown in Appendix B.3. We use the accuracy for evaluation which is a routine in prior work [15, 28].\nMain results We compare the same BiLSTM models with subword embedding and SEBco. The classification accuracies are shown in Table 5. The results show that SEBco can replace the conventional subword embedding without hurting the model performance. For SST2, SEBco even has better performance. The reason for that is the parameters of the conventional subword embedding layer in BiLSTM take a large portion of the model parameters, making the model easily overfitting. In this experiment, SEBco has smaller embedding parameters, which can address overfitting. We show that SEB also learns the semantic meaning of subword in B.5."}, {"title": "5.2.3 Language Modeling", "content": "Dataset and evaluation metrics We use the same data as Fairseq did for the language modeling tasks. The dataset we use is WikiText-103. We use the same preprocessing and training settings as the official Fairseq does. The number of samples for training, testing, and validation are 1801350, 3760, and 4358 respectively. We evaluate the language modeling performance with perplexity."}, {"title": "5.2.4 Federated Learning", "content": "we experiment with federated learning on the SST2 sentiment analysis task using the FedAvg framework[13]. In this experiment, we have 20 clients and distribute training samples uniformly to these clients. In training, we sample a part of the clients with the ratio c in every communication round. The results are shown in the following table. We can see that even in the federated learning framework, our SEBmethod is still comparable to subword embedding and can achieve stable results when the number of clients in training varies."}, {"title": "6 Conclusion", "content": "This paper introduces SEB, Subword Embedding of Bytes, a novel subword embedding method that defends against privacy leakage attacks based on embedding gradients in federated learning. Different from traditional approaches that learn a large subword embedding matrix, SEB uses smaller byte embedding matrices or byte one-hot encoding and aggregates byte representations to obtain subword embeddings. With SEB, attackers cannot retrieve a small set of subwords and generate private text, even with a well-trained large language model. Additionally, we demonstrate that SEB makes it difficult for attackers to recover private text with embedding gradients in federated learning. Our extensive experiments show that SEB is effective for machine translation, sentiment analysis, and language modeling tasks without sacrificing model performance or efficiency."}, {"title": "7 Limitations and Broader impact", "content": "Large language model and more tasks Limited to the computation resources, we only experiment with small Transformer models and moderate-size datasets. Moreover, we consider three common tasks such as machine translation, sentiment analysis, and language Modeling to show the effectiveness of our proposed method. The efficiency and effectiveness of our proposed method on large language models as well as other natural language processing tasks still need exploration.\nPretraining exploration All models in the experiments are trained from scratch. We have not experimented with the pretraining and finetuing/prompting paradigm with SEB. However, our proposed SEB is an effective alternative to subword embedding, and we think that our method is generalizable to popular NLP models, tasks, and training paradigms. In the future, we will investigate the performance of our proposed method under the pretraining and finetuing/prompting paradigm.\nBroader impact In this work, we use the IWSLT14 [4], WMT14 [3], SST2 [22], IMDb [12], and WikiText-103 [14]. All these 5 datasets are widely used pubic datasets in NLP tasks, which do not have personal or sensitive information. For the first four datasets, they are freely used without any license. For the WikiText-103 dataset, it is used under the \"Creative Commons Attribution-ShareAlike License\". We believe our work does not bring more risk in training and using NLP models. Our intention is to provide practical privacy-preserving collaboration and build trust among clients while maintaining the model's performance and efficiency. We hope this work can motivate more effective defenses, encourage secure and privacy-preserving collaborations in practical applications and have a positive effect on popular large-scale language models."}, {"title": "A SEB Algorithm", "content": "Algorithm 1 illustrates the detailed steps of our proposed subword to byte sequence mapping in Section 4.1. Algorithm 2 gives the step-by-step operations to obtain the subword embedding with SEB based on bytes, given the mapping in Algorithm 1.\nAlgorithm 1: Construction of Subword to byte sequence mapping\nInput: Byte vocabulary Vb = {0, 1, . . ., |Vb| - 1};\nSubword vocabulary Vw = {w0, w1,...,w|Vw|-1};\nThe number of bytes per subword n\nOutput: Mapping: M : Vw \u2192 (Vb)n\n1 for wi \u2208 Vw do\n2 repeat\n3 for j \u2190 1 to n do\n4 | Sample bij ~ Uniform[0, Vb) // P(bij) = 1/Vb, bij \u2208 Vb\n5 end\n6 until (bi1, bi2,..., bin) \u2209 M\n7 Add wi\u2192 (bi1, bi2,..., bin) to M\n8 end\n9 return M\nAlgorithm 2: Subword embedding with SEB\nInput : Subword to byte sequence mapping M : Vw \u2192 (Vb)n;\nSubword sequence S = (w1, w2,..., wm);\nA feed-forward network FFN;\nEmbedding matrix is B \u2208 RVb\u00d7d;\nSubword embedding dimension d'\nOutput: Subword embeddings E' \u2208 Rm\u00d7d'\n1 for i \u2190 1 to m do\n2 | (bi1, bi2,..., bin) \u2190 M[wi]\n3 end\n4 Byte sequence for S : (b11, . . ., b1n,..., bm1,..., bmn)\n5 E \u2208 Rmn\u00d7d \u2190 retrieve (b11, . . ., b1n, ..., bm1, ..., bmn) from B\n6 \u1ebc \u2208 Rm\u00d7nd \u2190 reshape E (in a row-major order)\n7 E' = FFN(\u1ebc) \u2208 Rm\u00d7d'\n8 return E'"}, {"title": "B Experimental Details and More Results", "content": "B.1 Enviormental Settings\nAll the programs in our work are implemented using Python 3.9.0, Fairseq [16], PyTorch 1.13.0, and CUDA 11.7. For the hardware environment, we run all codes on a machine with Intel i7-11700K CPU, 64G memory, and NVIDIA GeForce RTX 3080 GPU.\nB.2 Dataset and evaluation metrics\nTranslation In the translation task, we consider two datasets, one is the medium-size IWSLT14 [4] dataset and a large-scale dataset WMT14 [3]. We follow the settings as prior work [20, 31] and translate German (de) to English (en) in IWSLT14 [4]. The translation of WMT is English (en) to German (de) and the preprocessing is the same as Fairseq [16]. We use SacreBLEU, case-sensitive, with the 13a tokenizer [17] as the evaluation metric.\nSentiment Analysis We use IMDb [12] and SST2 [22] datasets provided by Hugging Face. The detailed preprocessing of the dataset is shown in Appendix B.3. We use the accuracy for evaluation which is a routine in prior work [15, 28].\nLanguage modeling We use the same data as Fairseq did for the language modeling tasks. The dataset we use is WikiText-103. We use the same preprocessing and training settings as the official Fairseq does. The number of samples for training, testing, and validation are 1801350, 3760, and 4358 respectively. We evaluate the language modeling performance with perplexity.\nB.3 Preprocessing Details\nTranslation For IWSLT14, there are 166K sentence pairs for training and validation and 5.6K for testing. The vocabulary shared by the source and target languages is built by BPE [19] with 10K tokens.\nFor WMT14, en-de contains 4.5M sentence pairs. Newstest2013 is used for validation and new-stest2014 for testing respectively. The merge operation is 32K for BPE and the dictionary is shared by source and target.\nSentiment analysis There are 25000 training samples and 25000 testing samples for IMDb. We take 25% of the training data for validation and the rest for training. For SST2, The training, validation, and test examples in SST2 are 67349, 872, and 1821, respectively. The tokenizer is \u201cbasic_english\u201d in the TorchText package. The minimum frequency needed to include a token in the vocabulary is 5. The maximum length of the sentence is 256.\nB.4 Implementation Details\nTranslation The baseline we compare is the transformer with subword embedding [25]. Our proposed method only replaces the subword embedding with SEB.\nFor IWSLT14, the encoder and decoder layers are both 6 and have 4 attention heads. The hidden dimension of attention is 512 and the dimension of the feedforward layer is 1024. The optimizer is Adam [9] with an inverse square root learning rate scheduler, and warm up 4000 steps. The learning rate is 5 \u00d7 10-4. The total training epochs are 100, and we average the best 5 checkpoints for testing.\nFor WMT14, the encoder and decoder layers are both 6 and have 8 attention heads. The hidden dimension of attention is 512 and the dimension of the feedforward layer is 2048. The optimizer is Adam [9] with an inverse square root learning rate scheduler, and warm up 4000 steps. The learning rate is 5 \u00d7 10\u22124. The total training epochs are 100 and we use the early stop if the validation loss does not decrease in 5 epochs. We average the best 5 checkpoints for testing.\nSentiment Analysis We use 2-layer BiLSTMs for both IMDb and SST2 classification tasks. We keep all model architectures the same for the baseline models and models with our SEBco except for the embedding parts. The subword embedding dimension is 64 and 256 for IMDb and SST2. The hidden units are 64 and 300 for IMDb and SST2. The hidden dimension of 2-layer FFN in SEBco is 128 for both datasets. We optimize the model using Adam [9] and the learning rate is 5 \u00d7 10\u22124 for the baseline and our method on both datasets. The best model parameters evaluated on validation data are applied for testing.\nLanguage modeling In this experiment, we also use a two-layer FFN in SEB, which has 4096 hidden units. The architecture is transformer_lm in the Fairseq framework. We share the input and output embedding in the encoder and the other hyperparameters and settings are the same as Fairseq.\nB.5 Analysis for Semantic Meaning\nIn this section, we will analyze whether the derived subword embeddings from our method can truly encode the meaning of the words in the embedding space. To experiment on this aspect, we mainly calculate the cosine similarity between two word embeddings obtained based on our method SEB.\nB.6 Analysis for Space Complexity\nWe analyze the space complexity in the experiments. We mainly take the translation on IWSLT14 and sentiment analysis as examples. Transformer model for translation on IWSLT14 de-en with 256 hidden units in our method SEBas the translation results are close to the traditional subword embedding.\nB.7 Comparison with Gradient Prune Defense\nWe compare the defense method of gradient pruning in the FILM attack for batch size = 8, 16, 32.\nC Discussion of Frequency Analysis\nFrequency analysis is useful in cryptanalysis."}, {"title": "C Discussion of Frequency Analysis", "content": "Frequency analysis is useful in cryptanalysis. For simple substitution ciphers, there is a characteristic distribution of letters that is roughly the same for almost all samples of that language. Frequency analysis uses the characteristic distributions of the plaintext and ciphertext to guess the mapping between them.\nIn the scenario of the threat model and our proposed method, the attacker only knows the gradients, model parameters and the mapping from subword to byte sequence. Based on these, the attacker can only get the information about distinct byte candidates which are updated in training. The attacker cannot determine the frequencies of the bytes based on the gradients.\nTo demonstrate the effectiveness of our method, we further assume the attacker have the information about the frequency of each byte. The goal of the attacker is to get the plaintext, given the plaintext to ciphertext mapping, and the characteristic distributions of the ciphertext.\nTo infer the plaintext, the attacker needs to know the order for combining all of the byte candidates and then use the ciphertext to plaintext mapping to obtain the original text. However, the attacker only knows a bag of bytes without ordering. It is difficult to infer the correct combination of the bytes as possible combinations of bytes is extremely large."}]}