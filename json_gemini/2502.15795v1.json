{"title": "LEAN-ING ON QUALITY:\nHOW HIGH-QUALITY DATA BEATS DIVERSE MULTI-\nLINGUAL DATA IN AUTOFORMALIZATION", "authors": ["Willy Chan", "Michael Souliman", "Jakob Nordhagen", "Brando Miranda", "Elyas Obbad", "Kai Fronsdal", "Sanmi Koyejo"], "abstract": "Autoformalization, the process of transforming informal mathematical language\ninto formal specifications and proofs remains a difficult task for state-of-the-art\n(large) language models. Existing works point to competing explanations for the\nperformance gap. On one hand, large language models exhibit exceptional per-\nformance on translation tasks, suggesting their significant potential for autofor-\nmalization. On the other hand, the quantitative reasoning capabilities of standard\nlanguage models remain limited, leading to suboptimal performance on autofor-\nmalization and the subsequent task of formal theorem proving. To this end, we\nintroduce a novel methodology that leverages backtranslation with hand-curated\nprompts to enhance the mathematical capabilities of language models, particu-\nlarly addressing the challenge posed by the scarcity of labeled data. Specifically,\nwe evaluate three primary variations of this strategy: (1) on-the-fly (online) back-\ntranslation, (2) distilled (offline) backtranslation with few-shot amplification, and\n(3) line-by-line proof analysis integrated with proof state information. Each vari-\nant is designed to optimize data quality over quantity, focusing on the high fidelity\nof generated proofs rather than sheer data scale. Our findings provide evidence\nthat employing our proposed approaches to generate synthetic data, which priori-\ntizes quality over volume, improves the autoformalization performance of LLMS\nas measured by standard benchmarks such as ProofNet. Crucially, our approach\noutperforms pretrained models using a minimal number of tokens. We also show,\nthrough strategic prompting and backtranslation, that our approaches surpass the\nperformance of finetuning with extensive multilingual datasets such as MMA on\nProofNet with only 1/150th of the tokens. Taken together, our methods show a\npromising new approach to significantly reduce the resources required to formal-\nize proofs, thereby accelerating AI for math.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural machine translation has been a focal point of research since the early development of machine\nlearning (Dong et al., 2015) (Wu et al., 2016). Autoformalization can be viewed as a specialized\napplication of this approach, where the objective is to translate theorems from informal, human-\nreadable markup languages like LaTeX into formal languages such as Lean4 - a programming\nlanguage specifically designed to encode complex mathematical constructs (de Moura & Ullrich,\n2021). In principle, the development of an agent capable of precise autoformalization would sig-\nnificantly reduce the prohibitive costs of manually formalizing proofs. This would have profound\nimplications; such an advancement could render all mathematical knowledge, much of which is\npresently recorded in natural language, programmable: substantially enhancing the usability of in-\nteractive theorem proving systems and accelerating the expansion of human mathematical under-\nstanding (Klein et al., 2009).\nAutoformalization first gained traction in 2018 when researchers used long short-term memory net-\nworks to generate statements in Mizar (Wang et al., 2018). More recently, experiments demonstrated\npromising results using a naive few-shot learning approach to translate English into Isabelle code"}, {"title": "2 TRAINING DATA GENERATION VIA BACKTRANSLATION", "content": "Backtranslation has been a pivotal method in neural machine translation (NMT), with its demon-\nstrated utility for enhancing training efficacy in the absence of sufficient labeled data (Poncelas et al.,\n2018). This technique was further refined in transformer models (Han et al., 2021) and in pre-\nliminary experiments involving distilled backtranslation within the domain of autoformalization\n(Azerbayev et al., 2023a). We chose this methodology because it is one of the most prominent\nand validated methods for NMT tasks (Liu et al., 2021), and has been shown to be analytically and\nempirically suitable for neural model training (Jiang et al., 2023).\nOur approach is to start with data in formal language (FL) and translate it into informal language\n(IL). We refer to this process as informalization. We demonstrate that each of these three dataset\ngeneration methodologies can improve model performance:\n\u2022 On-The-Fly Backtranslation: This strategy for data augmentation utilized a unique back-\ntranslation approach integrated within a custom training loop. Specifically, the training loop\nitself has four main steps, as outlined in Figure 2: (1) translating a batch of FL examples to\nIL (2) translating the synthetic IL examples back to FL, using the (synthetic IL, FL) pairs\nas labeled training data (3) computing the loss of the generated FL translations compared\nto the original FL examples and (4) backpropogating to update the model weights.\nThis method diverged from traditional practices by not employing separate teacher and\nstudent models; instead, it trained a single model to simultaneously manage both directions\nof the translation task. This setup was chosen because it enabled the model to iteratively\ngenerate its own training data at each step, and this self-sufficient nature allowed us to avoid\nthe problem of limited labeled data without incurring large API costs.\nThe relatively small size of the model used in our experiments, GPT-2 with 124M param-\neters, suggested that it would not be very effective at generating high-quality synthetic\ninformalizations. Despite this, the on-the-fly backtranslation method led to a significant\nreduction in evaluation loss after just a few hundred training steps, eventually leading to\na substantial improvement compared to the baseline as seen in Table 2. Nevertheless, we\neventually reached a performance plateau due to the inherent limitations of the baseline\nmodel. Due to resource constraints, we were unable to validate this method with a larger,\nmore capable model such as Llemma-7B (Azerbayev et al., 2023b). We hypothesize that\nemploying a larger pretrained model could yield even more promising results. This method\nthus serves as a baseline for comparison, and can highlight the potential benefits of using\nmore powerful models in future work.\n\u2022 Distilled Backtranslation: Considering the limitations of our initial approach, we pivoted\nto a more robust technique known as distilled backtranslation. For this method, we utilized\na more powerful pretrained model, GPT-4 (OpenAI, 2023), as the teacher model by feeding\nit a dataset of FL examples and utilizing it to generate corresponding synthetic IL text, as"}, {"title": "3 AI4MATH DATASET OVERVIEW", "content": "The integration of data collected through these methodologies resulted in the AI4Math Dataset,\nwhich encompasses a broad spectrum of proof tactics and informal statements. The total number\nof tokens from each collection method is provided in Table 1. The following datasets are labeled\naccording to the composition of their contents:"}, {"title": "4 AUTOFORMALIZATION PERFORMANCE", "content": "To quantify the results of our methods, we evaluate our fine-tuned models for accuracy on ProofNet's\ntest dataset, with the goal of measuring the performance gain over their initial pretrained states. All"}, {"title": "5 RELATED WORK", "content": "The Multilingual Mathematical Autoformalization (MMA) dataset also uses powerful models GPT-4\nto translate from formal language (FL) to informal language (IL) via zero-shot instruction prompting\non a large, diverse corpus of mathlib4 data (Jiang et al., 2023). Our approach enhances the efficiency\nof this process by applying a more intense six-shot prompting strategy on mathlib4 statements; the\nfine-tuning performance of the resulting dataset not only surpasses that of the MMA dataset on the\nProofNet benchmark but does so using only 1/150th of the tokens: significantly optimizing resource\nuse while enhancing output quality. Utilizing a regex, we parsed Lean files from mathlib into over\n100,000 tactic proof scripts, and we call this methodology using the \u201cFull-Proof\u201d in pregenerating\ninformal-formal pairs for informalization via GPT-4. Despite the high number of parsed scripts, at\nan estimated $0.05 per informalization we were only able to informalize a subset of this collection.\nOur dataset also utilizes LeanDojo, an open-source platform that offers a comprehensive dataset of\nover 90,000 Lean theorems. Existing models like ReProver, a retrieval-augmented language model,\nhave demonstrated superior performance compared to established models such as Tidy and GPT-\n4 (Yang et al., 2023). However, while retrieval-augmented models like ReProver are promising in\nleveraging formal corpora to enhance LLMs, they rely heavily on automated theorem provers. These\nmodels often skip over the detailed intermediate steps that are critical for understanding complex\nmathematical logic, thereby restricting the systems' flexibility and explainability.\nMoreover, the prevalent use of first-order logic in these models constrains their capacity to articulate\nmore sophisticated proofs. Our approach in GPT-4 LeanDojo (Individual Tactics) addresses this lim-\nitation by directly modeling the incremental deduction characteristic of interactive theorem provers.\nWhile this approach involves higher computational costs, Table 2 demonstrates its effectiveness in\nsignificantly enhancing baseline performance. Our method achieves notable improvements over the\nbaseline with a very small number of tokens."}, {"title": "6 DISCUSSION", "content": "Our paper introduces a novel dataset that can be used for enhancing the autoformalization capabili-\nties of large language models. We demonstrate evidence that models can attain superior performance\non autoformalization benchmarks such as ProofNet by training on AI4Math, and our generation\nstrategies can significantly reduce the number of tokens required compared to MMA.\nWe also note several limitations with our method. Firstly, our fine-tuning was exclusively conducted\nusing GPT-2. In future research iterations, we anticipate that employing more powerful models\nsuch as Mistral 7B or Llemma 7B for fine-tuning could lead to even better performance. Our\n\"On-The-Fly\" backtranslation process was especially constrained by the smaller model size used,\nand using more powerful models for fine-tuning in future iterations could enhance the quality of\ninformalizations."}, {"title": "7 FUTURE WORK", "content": "In our current research, we concentrated on autoformalizing theorem statements rather than gener-\nating complete formal proofs. This focus was primarily driven by our goal to streamline the process\nof translating natural language into formal language theorems, which are integral to the groundwork\nof interactive theorem proving (ITP). However, a significant next step in our exploration involves as-\nsessing whether the entire proofs or tactic scripts, when generated by our model, can be successfully\ncompiled within the Lean4 environment.\nAdditionally, leveraging Interactive Theorem Provers (ITPs) to extract datasets of intermediate proof\nsteps presents a novel opportunity for advancing the task of autoformalization. By sequentially\ninformalizing intermediate statements within a proof, we can generate a dataset that captures the\nindividual steps undertaken by ITPs. This data can subsequently be used to train an LLM, effectively\nreplicating the sequential reasoning performed by ITPs. Since this approach looks to model the\nincremental deduction used by interactive theorem provers directly, we predict that this could enable\nlanguage models to develop richer capabilities for reasoning tasks and enable generalization beyond\ntheir training proofs.\nThe ability for a model to generate not just the theorem statements but also the accompanying proofs\nwould mark a substantial leap forward in automating the theorem proving process. It would not\nonly enhance the efficiency and accessibility of formal verification but also potentially transform\nhow complex mathematical and logical reasoning is conducted in computational settings. Thus, the\nnext phase of our research will be dedicated to testing the compatibility and correctness of model-\ngenerated proofs with Lean4, aiming to bridge the gap between automated theorem generation and\nits practical application in ITP systems. This endeavor will necessitate a deeper integration of our\nmodel's outputs with the stringent syntax and logical framework of Lean4, posing a challenging yet\nessential milestone towards fully automated theorem proving."}, {"title": "8 CONCLUSION", "content": "Our paper has several larger implications for the field of autoformalization. By focusing on aut-\noformalizing theorem statements, we streamline the process of translating natural language into\nformal language, which is critical for the groundwork of interactive theorem proving. Successfully\ntraining an agent for precise autoformalization could drastically reduce the resources needed to con-\nvert extensive natural-language mathematical proofs into formal language. This progress would\nenhance the accessibility and efficiency of automated theorem proving and potentially quicken ad-\nvancements in related fields like formal verification and program synthesis. Looking forward, a\nskilled autoformalizer could make all mathematical knowledge, currently documented primarily in\nnatural language, programmable significantly broadening the usability of interactive theorem\nproving systems and accelerating the growth of human mathematical knowledge."}, {"title": "A APPENDIX A: \"FULL-PROOF\" PROMPT", "content": "We used the following prompt for informalizing the MathLib4 theorems we extracted, with a six-\nshot example. A complete example can be found in Figure 7."}, {"title": "B APPENDIX B: REGEX-PARSED LEANDOJO PROOFS", "content": "We used the regular expressions outlined in Figure 4 to parse specific patterns from Lean code. This\nenabled us to format the parsed strings using the templates in Figure 6 to create a basic 'informaliza-\ntion' of the proofs. This is a tradeoff of quality for quantity, as the informalizations are rudimentary\nbut simple to create. Given the improvements over baseline performance, more complex regex-based\nsystems could contribute to increasing the volume of paired data."}, {"title": "C APPENDIX C: DATA APPENDIX", "content": "All of the models and datasets used for fine-tuning can be found on HuggingFace under the AI4M\nOrganization."}]}