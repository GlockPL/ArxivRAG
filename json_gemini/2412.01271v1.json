{"title": "MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages with Negligible Cost", "authors": ["Sen Xing", "Muyan Zhong", "Zeqiang Lai", "Liangchen Li", "Jiawen Liu", "Yaohui Wang", "Jifeng Dai", "Wenhai Wang"], "abstract": "In this work, we explore a cost-effective framework for multilingual image generation. We find that, unlike models tuned on high-quality images with multilingual annotations, leveraging text encoders pre-trained on widely available, noisy Internet image-text pairs significantly enhances data efficiency in text-to-image (T2I) generation across multiple languages. Based on this insight, we introduce MuLan, Multi-Language adapter,a lightweight language adapter with fewer than 20M parameters, trained alongside a frozen text encoder and image diffusion model. Compared to previous multilingual T2I models, this framework offers: (1) Cost efficiency. Using readily accessible English data and off-the-shelf multilingual text encoders minimizes the training cost; (2) High performance. Achieving comparable generation capabilities in over 110 languages with CLIP similarity scores nearly matching those in English (38.61 for English vs. 37.61 for other languages); and (3) Broad applicability. Seamlessly integrating with compatible community tools like LoRA, LCM, ControlNet, and IP-Adapter, expanding its potential use cases.", "sections": [{"title": "1. Introduction", "content": "Recent diffusion models [11, 18, 19, 40, 44, 48] for content generation have attained stunning advancements in terms of both aesthetic quality and text-content alignment. However, these models still face substantial limitations in multilingual support. For instance, one of the most popular image-generation models, Stable Diffusion [33], and its successors [11, 28] only supports English and a few Latin-based languages. This language barrier restricts the model's performance in multilingual contexts and hinders its applicability worldwide across diverse cultural and linguistic backgrounds.\nThe community primarily adopts two approaches to achieve multilingual text-to-image (T2I) generation. (1) The first is translation-based methods where input content is temporarily translated into English before generating the image. However, this approach often leads to inference delays, translation errors, and notable issues when handling slang or culturally nuanced content. (2) The other approach is native multilingual T2I models [1, 19, 29, 39, 40, 48], which is trained directly on high-quality images captioned in the target language. While this native approach improves image generation quality for non-English languages, it relies on extensive, carefully curated image-generation data in the target language, making it data- and resource-intensive. As a result, exploring a more efficient and generalizable approach to achieve strong multilingual generation capabilities remains challenging.\nOn the other hand, thanks to the development of computational power and dataset scale, many existing language models have achieved strong multilingual capabilities through training on large-scale internet data. For example, models trained on text data (e.g., BERT [8], the GPT series [2, 25], and LLaMA [9, 42]) and those trained on image-text pairs (e.g., CLIP [29], ALIGN [15], and InternVL [6]) demonstrate outstanding performance in multilingual understanding. Since the multilingual capability of T2I generation models is closely tied to their text encoders, it becomes essential to explore how these powerful multilingual text encoders can be leveraged to enable existing generative models to achieve multilingual capabilities more efficiently and effectively.\nIn this work, we deeply explore the application of multilingual semantic alignment in image generation from the perspective of language and image-text alignment. We also reveal that text encoders trained on large-scale multilingual image-text datasets with noisy data demonstrate remarkable data efficiency in multilingual image generation. Based on this insight, we introduce MuLan, a lightweight Multi-"}, {"title": "2. Related Work", "content": "Multilingual Diffusion Models. Recent advancements have seen the rise of diffusion-based models, which have significantly improved image generation quality and diversity. Popular models such as Stable Diffusion series [11, 28, 33], DALL-E [31, 32], Imagen [35] and Glide [24] demonstrate photorealistic generation capabilities, yet they are primarily trained on English data and thus struggle with multilingual image generation. Although diffusion models using CLIP text encoders can generalize somewhat to Romance languages (e.g., French), their performance significantly degrades for languages outside this family, particularly East Asian languages such as Chinese. This limitation arises from the models' reliance on English-centric text encoders, such as CLIP [29] and T5 [30], and the predominantly English training data.\nRecent efforts have explored multilingual image generation by incorporating multilingual text encoders and datasets to overcome the limitations of English-centric models. One approach involves building models entirely from scratch with non-English data. For instance, Hunyuan-DiT [19] and Kolors [40] incorporate Chinese text encoders and extensive Chinese datasets to enhance their support for culturally specific concepts and improve generation quality in Chinese. Alternatively, some methods attempt to adapt existing models by replacing or fine-tuning the text encoder. Models such as Taiyi [44], PanGu [22], and AltDiffusion [46] replace the text encoder in Stable Diffusion and then fine-tune it with multilingual data, thus reducing the overhead compared to training from scratch. Despite these efforts, these models still face computational challenges, as fine-tuning often requires updating the entire denoising diffusion UNet.\nMultilingual Language Model. Language models have evolved through various modeling approaches, reflecting a range of training objectives and capabilities. Early models, such as BERT [8], BART [17], focus on understanding sentence structure and contextual relationships by predicting masked tokens within a sentence. More recent large language models (LLMs), such as LLaMA series [9, 42], T5 [30] and GPT series [2, 25], build on this foundation with improved context understanding and generation abilities, allowing them to handle a wide range of tasks with nuanced language comprehension and open-ended text generation. Building on these foundational text encoders, recent works, such as CLIP [29], ALIGN [15], and InternVL [6], incorporate visual alignment through paired image-text data, creating joint representations that bridge language and vision for cross-modal tasks.\nPrevious works such as mBERT [8] and LLaMA3 [9] have shown strong multilingual capabilities by pre-training on large multilingual corpora, enabling these models to understand and generate text in a variety of languages. InternVL-LLaMA [6] achieves powerful multilingual cross-modal capabilities by aligning a text encoder with a vision transformer (ViT) on the multilingual image-text dataset LAION[36], enabling effective image-text contrastive learning. However, due to the scarcity of large-scale multilingual image-text datasets, some approaches [3, 5] have resorted to using translated datasets and distillation learning to align multilingual text features. Despite these advances, an open challenge remains inefficiently leveraging multilingual text encoders for text-to-image (T2I) generation. Previous works, such as Taiyi [44] and AltDiffusion [46], have adopted multilingual text encoders and fine-tuned them on multilingual image-text pairs for T2I tasks. In contrast, our method takes advantage of InternVL-LLaMA's multilingual capabilities, requiring only a small amount of English data and without the need to fine-tune the SD model weights, achieving state-of-the-art performance in multilingual T2I generation."}, {"title": "3. Proposed Method", "content": "In this section, we first revisit the mainstream text-to-image generation framework, analyzing its underlying structures and presently available data resources. Based on these insights, we introduce MuLan, a cost-effective multilingual generation framework designed to improve cross-language adaptability and generation quality."}, {"title": "3.1. Revisiting Text-to-Image Generation", "content": "Given an input text prompt x and ground-truth image y from the training dataset D, a mainstream text-to-image (T2I) generation model G(\u00b7), which consists of a language model L(\u00b7) and a visual generator V(\u00b7), can be defined as follows:\n$\\theta_l,\\theta_v = \\arg \\min_{\\theta_l,\\theta_v} E_{(x,y)\\sim D} [L(G(x; \\theta_l,\\theta_v), y)],$ (1)\nwhere $\\theta_l$ and $\\theta_v$ represent the parameters of the language model L(\u00b7) and the visual generator V(\u00b7), respectively, and L denotes the generation loss, * represents the optimal solution for function optimization. It can be observed that the primary components related to multilingual processing are the training dataset D and the language model L(\u00b7). Therefore, in the following sections, we focus on the two modules, exploring pathways for building an efficient text-to-image generation model.\nLanguage Model. Existing T2I generation models typically utilize pre-trained language models as text encoders. For instance, the Stable Diffusion series [11, 28, 33] utilizes CLIP text encoder [29] or T5 [30] as language encoders. Beyond these encoder-only and encoder-decoder language models, recent years have seen increased attention on decoder-only large language models (LLMs), such as the GPT series [2, 25] and LLaMA [9, 42]. These models are trained on pure text data by predicting the next word, supporting multiple languages, and outperforming traditional language models in NLP tasks. As shown in Table 1, a wide variety of language models are now available in the community; however, determining which models are suited for multilingual text-to-image generation remains an open question.\nDataset. As shown in Table 2, the current datasets available in the community including multilingual text corpora, multilingual text-image paired datasets, and high-quality text-to-image datasets, hold potential value for multilingual image generation tasks. For example, large-scale datasets like LAION-400M/5B [36] contain large-scale noisy and lower-quality data, they provide multilingual text-image pairs, which are valuable for supporting multiple languages. Additionally, multilingual text translation corpora (such as CC-Matrix [37]), while lacking corresponding image-text pair, offer useful cross-language correspondences. High-quality text-to-image datasets (such as JourneyDB [26]) provide high-resolution, high-quality images and are frequently used in text-image generation models; however, they primarily support English or some mainstream languages with limited coverage of other languages. Therefore, effectively utilizing low-cost, readily accessible internet-based text-image datasets and multilingual translation datasets to support multilingual text-to-image model training is a valuable area for further exploration."}, {"title": "3.2. MuLan: Toward Multilingual T2I Generation", "content": "Overall Architecture. To facilitate efficient and effective multilingual T2I generation, MuLan incorporates two key designs: (1) the multilingual semantic alignment through easily accessible large-scale data, and (2) a language adapter trained on a limited set of English T2I data. These designs enable MuLan to operate without the constraints of multilingual T2I data, allowing for more efficient training by leveraging existing models and datasets.\nMultilingual Semantic Alignment. While many existing language models demonstrate robust multilingual capabilities, not all are well-suited for the multilingual T2I generation task needed for the Mulan framework. In this work, we emphasize the importance of maintaining a consistent vector space across languages for multilingual T2I generation. Specifically, given two text prompts, $x_1$ and $x_2$, that share the same meaning but are in different languages, and a text encoder L(\u00b7), the representations of these prompts, L($x_1$) and L($x_2$), should closely align. This alignment ensures that the conditional inputs to the image decoder remain consistent across languages, thereby preserving consistent image generation quality. Here, we mainly consider two alignment approaches: (1) Image-centered alignment;(2) Language-centered alignment.\n(1) Language-Centered Alignment. A straightforward method that aligns multilingual semantics is to align multilingual semantics is to leverage a large set of translation data to align other languages' vector spaces with the well-supported English vector space. By conducting distillation training with translation data alone, this can be achieved: we designate the Stable Diffusion language encoder as the teacher encoder, indicated by $L_t(x)$ and any multilingual encoder $L_s(x, \\theta_s)$ as the student encoder, aligning their features using MSE Loss. This alignment can be written as:\n$\\theta_s = \\arg \\min_{\\theta_s} E_{(x,y)\\sim D_{tr}} [MSE(L_s(x_1, \\theta_s), L_t(x_2))]$ (2)\n(2) Image-Centered alignment. In the image-centered alignment approach, CLIP maximizes the similarity between positive text-image pairs and minimizes it for negative pairs through contrastive learning. This training uses text-image pairs, and when the text includes multiple languages, the language encoder aligns different languages in the vector space naturally around the image. In this case, the objective function can be written as:\n$\\theta_l = \\arg \\min_{\\theta_l} E_{(x,y)\\sim D} [cosine(L(x, \\theta_l), E_I)],$ (3)\nHere, $\\theta_l$ refers to the parameters of the language model L(\u00b7), $E_I$ refers to the image feature. This method, however, is resource-intensive and requires large multilingual text-image pairs, which are challenging to obtain and require substantial storage. Using pre-trained models can help reduce these costs.\nLanguage Adapter. After getting the aligned language model, to achieve cost-effective multilingual text-to-image generation, we propose MuLan. This model incorporates a lightweight language adapter L' that bridges a multilingual-aligned language model with a visual generator, enabling generalization to multiple languages after training on a small amount of English text-to-image generation data $D_{en}$. So the Eqn 1 could be rewritten as:\n$\\theta_v = \\arg \\min_{\\theta_v} E_{(x,y)\\sim D_{en}} [L(G(x; \\theta_l,v), y)] .$ (4)\nAfter aligning different languages using Eqn 2 or Eqn 3, we can achieve multilingual T2I generation by training only this adapter. The key to low-cost implementation lies in freezing the language model and visual generator while training only the language adapter $\\theta_v$ on small-scale English data $D_{en}$.\nIn this work, we consider two types of adapters: MLP and transformer. These adapters are used to re-project the high-dimensional representations of text prompts from different languages into a unified space. We adopt different adapter designs for different diffusion models, as shown in Fig. 2. In detail, we can use either MLP or transfromer for projecting prompt embeddings for Pixart-a [4], and both architectures achieve good results. However, we find a simple MLP could not properly deal with Stable Diffusion models [28, 33]. Instead, we choose to use one layer encoder-decoder transformer with a set of learnable queries for extracting embeddings from InternVL outputs. For SDXL [28], we use two transformers to project embeddings for two text encoders that SDXL [28] adopts, and one attention pooling layer for extracting pool embeddings."}, {"title": "4. Experiments", "content": "This section describes the datasets, implementation details, and evaluation metrics used to assess our model, InternVL-MuLan. We compare its performance with state-of-the-art methods, including translation-based Stable Diffusion 1.5 [33] and AltDiffusion-m18 [46], across benchmarks such as XM18 [41], DPG-Bench [14], and COCO2014 [20]. Additionally, we perform ablation studies on adapter architectures and text encoders and present qualitative results to demonstrate our model's capability to generate high-quality images across multiple languages."}, {"title": "4.1. Experimental setup", "content": "Datasets. We primarily use a subset of LAION-EN [36] with all samples that have aesthetic scores larger than 5.8 for training base models and PixArt [4] dataset for aesthetic models.\nImplementation details. We trained MuLan adapters for a variety of pre-trained text encoders. By default, we use a transformer-based adapter with one encoder layer and one decoder layer. The number of transformer queries is set to 77 to match the input of Stable Diffusion [33].\nAll MuLan adapters are trained with AdamW [21] optimizer and 128 batch size. We use constant learning rate 1e-5 for Stable Diffusion 1.5/2.1 [33], le-6 for SDXL [28], and 2e-5 for Pixart-a [4]. For Stable Diffusion 1.5 [33], we train the adapter for 50k steps at the resolution of 512\u00d7512, and for Stable Diffusion 2.1 [33] we adjust the resolution to 768x768. For SDXL [28], we first train the adapter for 100k steps at the resolution of 512\u00d7512 and finetune it for another 1k steps at the resolution of 1024x1024. For Pixart-a [4], we train the adapter for 118k steps at the resolution of 512\u00d7512. We randomly drop text conditions at the rate of 10% and use min-SNR [12] to accelerate training.\nThe training process was conducted on 8 NVIDIA A100-80G GPUs. For SD 1.5 and SD 2.1, training was completed within two days. For SDXL and Pixart-a, training was completed within four days.\nEvaluation Metrics. We use Crossmodal-3600 [41] to assess the model's capabilities across 18 mainstream languages (denoted as XM18 hereafter). To evaluate the model's generalization to additional languages, we tested multilingual versions of the COCO2014 [20] validation set and DPG-Bench [14]. We translated the prompts into 85 languages using Google Translate for these datasets. The model's performance was compared with an ad-hoc translation-based SD1.5 model and other multilingual T2I models. Regarding evaluation metrics, we employed the Aesthetic Score to assess the quality of the generated images. Additionally, we calculated the Cosine Similarity (CLIP Sim) score with InternVL-LLaMA [6]. These metrics provide complementary insights into the visual quality and semantic accuracy of the model's outputs."}, {"title": "4.2. Quantitive Results", "content": "Multilingual T2I Comparison. We integrated InternVL-LLaMA [6] into the adapter's model, which we call InternVL-MuLan. Specifically, for each version of Stable Diffusion, we train a separate adapter model, which we refer to as InternVL-MuLan-SD15, InternVL-MuLan-SD21, and InternVL-MuLan-SDXL.\nWe first compared the performance of InternVL-MuLan-SD15 with translation-based Stable Diffusion 1.5 [33]. The evaluation results on XM18 [41] are shown in Table 3. Our model significantly outperforms SD1.5 in terms of image aesthetic quality. For CLIP similarity scores, our model surpasses Stable Diffusion in most languages.\nNext, we compared the performance of our model with AltDiffusion [46] in both mainstream and minority languages. We used DPG-Bench [14] to assess instruction-following ability in 5 mainstream languages. As shown in Table 5, our model's instruction-following ability naturally extends to multiple languages, achieving results comparable to AltDiffusion [46]. To further evaluate our model's capability in the broader range of minority languages, we evaluated our model on the COCO2014 [20] validation set (85 languages) and computed CLIP similarity scores, as shown in Figure 3(the complete list of scores will be provided in the appendix). Our model achieved performance comparable to AltDiffusion [46] in mainstream languages while substantially surpassing AltDiffusion [46] in less common languages.\nOur model achieves state-of-the-art performance across multiple languages, reaching SOTA levels in mainstream languages and excelling in minority languages. Even though it was trained using only English text-image pairs, it outperforms certain specialized models in specific languages. While its performance in mainstream languages is slightly below that of AltDiffusion-m18 [46], the data and training costs we incurred are significantly lower than those of AltDiffusion. Specifically, our training on InternVL-MuLan-SD15 required only 0.5\u00d7 8 GPU-days, compared to the 19 \u00d7 64 GPU-days needed by AltDiffusion-m18, highlighting a substantial difference in resource consumption. This makes our method both highly efficient and effective."}, {"title": "4.3. Qualitative Results", "content": "Our model can generate high-quality images across multiple languages. It also supports multilingual mixed input and can recognize certain emojis. In addition to simple T2I tasks, there are numerous downstream applications of Stable Diffusion (SD) within the community, including LORA [13], LCM [23], and ControlNet [49]. These tools played crucial roles in enhancing model adaptability, control over outputs, and finetuning for specific tasks. Unlike AltDiffusion [46], our model does not require finetuning on SD, allowing it to seamlessly integrate and be compatible with these community-developed SD applications in a plug-and-play manner. We show some examples in Figure 4."}, {"title": "5. Conclusion", "content": "We introduce language adapter MuLan that could equip image/video/3D diffusion models with multilingual generation abilities. MuLan shows strong zero-shot capabilities for up to 110 different languages, even if the adapter is solely trained on English data. MuLan also can be trained with a frozen text encoder and diffusion denoising model, which makes it applicable for many downstream models, such as LoRA [13], ControlNet [49], LCM [23], and etc., without any additional finetuning. MuLan is currently trained with paired data, and it could inevitably bring in bias and cause a distribution shift of original models. A promising extension would be to alleviate the need for paired data and make original capabilities intact. Furthermore, MuLan currently focuses on improving multilingual generation capabilities, but it would be interesting to extend it to improving prompt understanding and following under a multilingual context."}]}