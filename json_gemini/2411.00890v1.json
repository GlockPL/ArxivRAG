{"title": "Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research", "authors": ["Marcello Carammia", "Stefano Maria Iacus", "Giuseppe Porro"], "abstract": "Large Language Models (LLMs) are distinguished by their architecture, which dictates their parameter size and performance capabilities. Social scientists have increasingly adopted LLMs for text classification tasks, which are difficult to scale with human coders. While very large, closed-source models often deliver superior performance, their use presents significant risks. These include lack of transparency, potential exposure of sensitive data, challenges to replicability, and dependence on proprietary systems. Additionally, their high costs make them impractical for large-scale research projects.\nIn contrast, open-source models, although available in various sizes, may underperform compared to commercial alternatives if used without further fine-tuning. However, open-source models offer distinct advantages: they can be run locally (ensuring data privacy), fine-tuned for specific tasks, shared within the research community, and integrated into reproducible workflows.\nThis study demonstrates that small, fine-tuned open-source LLMs can achieve equal or superior performance to models such as ChatGPT-4. We further explore the relationship between training set size and fine-tuning efficacy in open-source models. Finally, we propose a hybrid workflow that leverages the strengths of both open and closed models, offering a balanced approach to performance, transparency, and reproducibility.", "sections": [{"title": "Introduction", "content": "The widespread availability of generative AI, particularly large language models (LLMs), is transforming both society and science. These models are becoming increasingly easy to prompt, making them appealing to scientists across both the hard and social sciences for conducting text classification tasks in a wide range of disciplines, including finance (Loukas et al. 2023), health studies (Guo et al. 2024; Cao et al. 2024; Kiyasseh et al. 2024), environmental studies (Trajanov et al. 2023), geoscience (Maze et al. 2024), physics (Y.-Y. Li et al. 2024), chemistry (Liao et al. 2024) radiology (Nowak and Alois M Sprinkart 2024a), psychology (Abdurahman et al. 2024), sociology (Kozlowski, Kwon, and Evans 2024; M\u00fctzel and Ollion 2023; Chae and Davidson 2023), political science (M. Liu and Shi 2024), criminology (Nikolakopoulos et al. 2024), law studies (Wei et al. 2023), just to mention a few. Recent studies have sparked both enthusiastic (Gilardi, Alizadeh, and Kubli 2023; Rouzegar and Makrehchi 2024) and critical (Abdurahman et al. 2024) assessments of LLMs.\nA central debate in this field involves closed vs. open foundation models (G. Zhang et al. 2024; Nowak and Alois M Sprinkart 2024a; Abdurahman et al. 2024; Hanke et al. 2024; Chae and Davidson 2023). Proprietary models, such as those from OpenAI and Google, present several challenges: they can change versions without notice, their architectures are not fully disclosed, and they may be subject to content moderation such as filtering for hate speech or privacy concerns designed to protect companies from legal and reputational risks. While these safeguards are important, they fall outside the control of scientists."}, {"title": "Related Work", "content": "Numerous studies (Chae and Davidson 2023; Pangakis and Wolken 2024; Rouzegar and Makrehchi 2024; Yin et al. 2024; Gougherty and Clipp 2024) have shown that recent advancements in large language models (LLMs) have revolutionized so much the field of natural language processing that they now hold a significant potential for social science research. Since the introduction of pioneer LLMs like BERT (Devlin et al. 2019) (a model with \u201cjust\u201d 110 million parameters) up to XLM-ROBERTa (Conneau et al. 2020) (550 million parameters), the development of LLMs has progressed in the last few years through two key advancements: refined Transformer architectures (Vaswani et al. 2017) and optimized training techniques. These improvements, along with increased parameter sizes in models like GPT-3 and T5, have enabled LLMs to capture complex language patterns across diverse contexts, making them viable for varied social science tasks with minimal fine-tuning (Brown et al. 2020; Raffel et al. 2020).\nWith LLMs increasingly accessible as research tools, computational social science (Lazer et al. 2009) has started to leverage these models for large-scale text analysis beyond basic sentiment classification. This shift in scale has spurred numerous studies examining LLMs for annotation, classification, and knowledge extraction from text (Pangakis and Wolken 2024; Singh et al. 2024; Y. Zhang et al. 2024; Fechner and Dorpinghaus 2024; Rouzegar and Makrehchi 2024; Bisbee et al. 2024; Barrie, Palaiologou, and T\u00f6rnberg 2024). Recently, researchers have also explored the multi-modal capabilities of LLMs for combining text with other data types (Miah et al. 2024). Our work aligns with studies examining fine-tuning's impact on LLM"}, {"title": "Methods", "content": "Streamlining Labeled Dataset Creation for Fine-Tuning and Scaling\nThe traditional approach to text classification involves training human coders, assigning them random sets of documents to classify, and resolving disagreements among coders to generate a clean set of labeled data for training and validation. An empirical fact familiar to researchers is that human coders often disagree in 15-20% of cases. This disagreement arises from various factors, such as inflexible codebooks, cultural and political biases, coder fatigue, or the inherent uncertainty in capturing certain dimensions.\nAchieving high inter-coder agreement can be challenging, and surprisingly, coders often prefer LLM-generated annotations over expert answers in certain contexts (Goyal, J. J. Li, and Durrett 2023; Hagendorff, Fabi, and Kosinski 2023). In some studies, the correlation between annotations by platforms like Mechanical Turk and expert annotators can approach zero (Fabbri et al. 2021). Recent literature has also questioned the reliability of human classification, traditionally considered the \"gold standard\", suggesting that it may not be as accurate as commonly assumed (Clark et al. 2021; Y. Liu et al. 2023).\nAnother critical issue is scalability. Creating a sufficiently large set of classifications to cover both training and validation datasets is time-consuming and expensive. In cases like ours, which involve a high dimensionality of categories, even expert coders find the task challenging. Furthermore, using human cognitive resources for this task may not be the most efficient use of time and skill.\nOur proposed approach is to generate multiple classifications for an unlabeled set using potentially several LLMs, with humans focusing on rejecting incorrect classifications. It's faster and easier to spot what does not apply to a text\u2014especially when faced with a set of over 10 possible labels (our application uses 15, 20, and 46 labels) than to identify which set of labels apply.\nThere are several psychological reasons why it's often easier for humans to recognize errors rather than identify correct classifications. For example, error salience (Harsay et al. 2012) suggests that mistakes or anomalies tend to stand out more than correctly processed information, which blends into our expectations. Human cognitive biases, like being more sensitive to inconsistencies (an ancestral survival mechanism that makes us more sensitive to risks or issues), also play a role (Mobbs and Hagan 2015). Lastly, the negativity bias (Ito et al. 1998) makes humans pay more attention to negative or incorrect information than to positive or correct information.\nAll the above motivates our approach on how to accelerate the creation of a labeled set."}, {"title": "Evaluation Metrics", "content": "In this work, we employ various accuracy metrics depending on the specific classification problem being addressed. Let $n$ represent the total number of documents to classify, and let $M$ denote the number of possible labels in a given classification task.\nWe define $y_{ij}$ as an indicator function that takes the value 1 if document $i$, where $i = 1,..., n$, is assigned to class $j$, where $j = 1, ..., M$, and 0 if label $j$ does not apply to document $i$.\nFor mutually exclusive categories, each document $i$ can belong to at most one class, meaning there exists at most one index $j$ such that $y_{ij} = 1$. In such cases, we use the notation $y_{i} = j$ to indicate that document $i$ belongs to category $j$.\nWe now review a few basic metrics commonly used to assess the quality of a classifier in the context of mutually exclusive categories.\nAccuracy\nIf $y_{i}$ is the actual true category for document $i$ and $\\hat{y}_{i}$ is the category predicted by the classifier, then accuracy is defined as\n$Accuracy = \\frac{1}{n} \\sum_{i=1}^{n} 1(\\hat{y}_{i} = y_{i}),$\nwhere the function 1 (cond) = 1 if the condition 'cond' is true and 0 otherwise.\nThe accuracy is simply the sum of the diagonal elements of the $M \\times M$ confusion matrix $C = (C_{jk}, j, k = 1,..., M)$, where $C_{jk}$ counts the number of times a document with true label $j$ is assigned to category $k$ by the classifier. It is also worth to introduce other measures like Precision, Recall and Specificity. The Precision measure is defined as:\n$Precision(class = j) = \\frac{TP(class = j)}{TP(class = j) + FP(class = j)} = \\frac{\\sum_{i} 1((\\hat{y}_{i} = j) \\cap (y_{i} = j))}{\\sum_{i} 1(\\hat{y}_{i} = j)},$\nwhere TP refers to \u2018true positives\u2019 and FP refers to 'false positives.' Similarly, the Recall (or Sensitivity) for class $j$ is defined as:\n$Recall(class = j) = Sensitivity(class = j) = \\frac{TP(class = j)}{TP(class = j) + FN(class = j)} = \\frac{\\sum_{i} 1((\\hat{y}_{i} = j) \\cap (y_{i} = j))}{\\sum_{i} 1(y_{i} = j)}$"}, {"title": "F1 Score", "content": "where FN refers to 'false negatives.' Finally, the Specificity measures the proportion of actual negatives that are correctly identified by the classifier. For class j, Specificity is defined as:\n$Specificity (class = j) = \\frac{TN(class = j)}{TN(class = j) + FP(class = j)} = \\frac{\\sum_{i}1(y_{i} \\neq j \\cap \\hat{y}_{i} \\neq j)}{\\sum_{i} 1(y_{i} \\neq j)}$\nAnother important metric is the so-called Balanced accuracy, that is used when dealing with imbalanced datasets (like in our examples below). It aims to provide a better measure of performance when the classes in a classification problem are not equally represented. The main purpose of balanced accuracy is to address the limitations of standard accuracy in scenarios where the dataset has an unequal distribution of classes. It does this by taking into account both the Sensitivity (true positive rate) and the Specificity (true negative rate), giving equal weight to both. It is given by the formula:\n$Balanced Accuracy = \\frac{Sensitivity + Specificity}{2}$\nThe Macro Balance Accuracy is the simple average of the Balanced Accuracy by class.\nThe F1 score, originally introduced for a 2 \u00d7 2 table, is given by the formula:\n$F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}$\nThe F1 score for class $j$ is then computed as:\n$F1_{j} = 2 \\times \\frac{Precision(class = j) \\times Recall(class = j)}{Precision(class = j) + Recall(class = j)}$\nAs with accuracy, higher values of the F1 score indicate better classifier performance."}, {"title": "Hamming Loss", "content": "In multi-label classification, each document can be assigned multiple labels. In general, each document $i$ is associated with a vector $y_{i*} = (y_{i1}, y_{i2}, ..., y_{iM})$, which indicates the presence or absence of labels. Let $\\hat{y}_{i*}$ be the predicted vector of $y_{i*}$ for document $i$. The Hamming loss is a measure of how frequently a prediction is wrong. It is particularly useful in this context of multi-label classification. For a single document $i$, the Hamming Loss (Hamming 1950) is given by the formula:\n$HL_{i} = \\frac{1}{M} \\sum_{j=1}^{M} 1(y_{ij} \\neq \\hat{y}_{ij}).$\nThe overall Hamming Loss for the sample of documents is:\n$HL = \\frac{1}{n} \\sum_{i=1}^{n} HL_{i} = \\frac{1}{nM} \\sum_{i=1}^{n} \\sum_{j=1}^{M} 1(y_{ij} \\neq \\hat{y}_{ij}).$\nThe metric $HL_{i}$ is equal to 1 if all predicted $y_{ij}$ values are wrong, and 0 if all predicted $y_{ij}$ values are correct. The overall Hamming Loss, which is just the average of the $HL_{i}$'s, also ranges in [0,1]. When comparing two classifiers, the one with lower Hamming Loss is better."}, {"title": "Jaccard Index", "content": "Another metric that is popular in multi-label classification problems is the Jaccard Index (Gilbert 1884; Jaccard 1901). It measures the similarity between the predicted and true vectors of $y_{i*}$ by dividing the size of their intersection by the size of their union. The formula is as follows:\n$J = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{|y_{i*} \\cap \\hat{y}_{i*}|}{|y_{i*} \\cup \\hat{y}_{i*}|} = \\frac{1}{nM} \\sum_{i=1}^{n} y_{i*} \\cdot \\hat{y}_{i*},$"}, {"title": "Data and Problem Statements", "content": "We will consider three different datasets and classification problems which present a spectrum of challenges."}, {"title": "Classification of Tweets according to the Human Flourishing Program", "content": "The Human Flourishing Program (VanderWeele 2017) is a research initiative based at the Institute for Quantitative Social Science (IQSS) at Harvard University. Its goal is to study and promote human flourishing across a broad spectrum of life domains, integrating interdisciplinary research in the social sciences, philosophy, psychology, and other fields. The program aims to understand what constitutes human well-being or flourishing, which goes beyond mere happiness or economic success. It seeks to identify and analyze the factors that contribute to a flourishing life, including physical and mental health, happiness and life satisfaction, meaning and purpose, character and virtue, and close social relationships. Within this program, the Global Flourishing Study (GFS) is a five-year longitudinal data collection on approximately 200,000 participants from 20+ geographically and culturally diverse countries and territories, including Argentina, Australia, Brazil, China (Hong Kong), Egypt, Germany, India, Indonesia, Israel, Japan, Kenya, Mexico, Nigeria, the Philippines, Poland, South Africa, Spain, Sweden, Tanzania, Turkey, United Kingdom, and the United States. GFS measures global human flourishing in six areas: i) Happiness and life satisfaction; ii) Mental and physical health; iii) Meaning and purpose; iv) Character and virtue; v) Close social relationships and vi) Material and financial stability.\nEach of these human flourishing areas is investigated through several questions. For our study, we have selected 46 dimensions (see Supplementary Material) among all the dimensions identified by the GFS study, taking into account the fact that we cannot ask question but we should infer the presence or absence of each dimension in a particular tweet. The scope of our study, that goes much beyond the scope of the present work, it to analyze the entire Harvard's 10B Geo-Tweets archive (Lewis and Kakkar 2016) with an accurate and scalable solution. We will analyze the performance of open source models from the Meta-developed LLAMA family, testing base and fine-tuned versions of LLAMA-2, 3, 3.1 and 3.2 versions for different sizes of model parameters.\nFor this analysis we selected 10,000 tweets at random in English language. We then applied the workflow illustrated in Figure 1, letting four models (LLAMA-2 7B, 13B and 70B and ChatGPT4) produce the unsupervised codings of Step 2 of the workflow. Then, we trained a group master and doctoral students to verify (Step 3) the AI-classifications according to the codebook of the project (see Supplementary"}, {"title": "Policy Attribution of European Parliamentary Questions", "content": "The Comparative Agendas Project (CAP) is an international research initiative aimed at systematically tracking, coding, and comparing policy attention and issue agendas over time across various countries (Alexandrova et al. 2014; Jones et al. 2023). The project seeks to understand how government attention and public priorities shift across policy domains, such as healthcare, defense, education, and the environment.\nThe CAP accomplishes this by analyzing policy outputs such as laws, government speeches, media coverage, and legislative bills-coding them into standardized categories, and comparing them across time and countries. This approach enables researchers to examine how governments respond to public demands and to assess whether certain issues are prioritized over others across different contexts.\nWhile the exact number of agenda items or categories may vary depending on updates or extensions for specific countries, the core CAP codebook typically includes 19 major policy areas, subdivided into over 200 subcategories. These major areas include: Macroeconomics; Civil Rights, Minority Issues, and Civil Liberties; Health; Agriculture; Labor, Employment, and Immigration; Education; Environment; Energy; Transportation; Law, Crime, and Family Issues; Social Welfare; Community Development and Housing Issues; Banking, Finance, and Domestic Commerce; Defense; Space, Science, Technology, and Communications; Foreign Trade; International Affairs and Foreign Aid; Government Operations; and Public Lands, Water Management, and Territorial Issues.\nIn this study, we focus on the classification of European Parliament questions from 1994 to 2021. The dataset comprises 174,161 questions posed over seven legislative terms, covering 28 countries and eight European party families. These questions reflect various EU institutional configurations, ranging from Maastricht to Lisbon, and are all provided in English.\nAccording to the CAP framework, these questions can be classified into 19 broad policy areas. Certain categories, such as Defense, Foreign Trade, and Social Policy, are under-represented, while others, such as Macroeconomics, Agriculture, Civil Rights, and International Affairs and Foreign Aid, are over-represented. This imbalance poses challenges for classification. Nonetheless, the dataset is free from noise, as each parliamentary question is assigned to a specific policy area based on the CAP project's codebook guidelines.\nAlthough the CAP coding protocol assumes that each text should be classified into one of the 19 major policy areas, this task presents significant challenges for human coders. It is well-known among CAP project experts that inter-coder reliability typically around 70% and sate-of-the art models (Seb\u0151k et al. 2024) built on top of very large training-set data (1,147,783 observations) show weighted F1 score that varies from 0.71 to 0.9 according to different data sources (Media, Social Media, Parliamentary Speech, Legislative Executive Speech, Executive Order, Party Manifesto, Judiciary), signaling that the CAP data are not easy to classify. This variability highlights the inherent difficulty of consistently assigning policy areas, even with detailed coding guidelines. On this particular dataset (parliamentary questions) the inter coder reliability over size well trained coders on a subset of 134 manually coded parliamentary questions, the Cohen's kappa statistics (Fleiss et al. 1971) is equal to 66.4\nIn order to classify a preliminary set of data according to Step 2 of the workflow in Figure 1, we used three different strategies:\n\u2022 direct method: we prompted ChatGPT4 to select one of the 200 micro areas and then aggregated by macro areas;\n\u2022 zero shot method: we asked ChatGPT4 to choose one of the 19 categories;\n\u2022 iterative method:\nfor each of the 19 policy macro areas we prompted ChatGPT4 to choose only one among the corresponding subtopic areas or None;\nafter iterating the 19 policy macro areas, we are left with a subset of subtopics corresponding each to a different macro area;"}, {"title": "Classification of Harvard Dataverse datasets", "content": "Harvard Dataverse (https://dataverse.harvard.edu) is a generalist research data repository (King 2007) with highly curated metadata. Thousands of scholars across many disciplines have deposited data, along with their associated metadata, in this repository. To support researchers in depositing their data more easily, the Dataverse development team is experimenting to generate metadata suggestions based on data characteristics, with the use of large language models (LLMs). In this experiment, we explored the ability of a small LLAMA-7B fine-tuned model to predict one or more subject categories for datasets using only the dataset Title and Description. The outcome of this project will be also used to verify the correctness of metadata in already published datasets or to test their FAIR-ness (Wilkinson et al. 2016). Previous works (ICPSR 2023; Shigapov and Schumm 2024) tried to do this using ChatGPT but again the approach is not scalable and not fully under the control of the researcher.\nThe dataset collection contains 13 subject categories, inherited from the Revised Field of Science and Technology (FOS) Classification by OECD (Organisation for Economic Co-operation and Development (OECD) 2007), plus \u201cOther\u201d and \u201cN/A\" when information is missing, for a total of 15 labels to predict in this classification task. Since each dataset can belong to one or more subject categories, this is a multi-class classification problem. Notice that in this case a human supervised training set already exists, so there is no need to apply the workflow in Figure 1. The LLM is asked to return up to 3 subject categories per dataset. This choice is motivated by the application at hand in which the Dataverse system suggests the three most relevant subject categories that apply to the users that is submitting data to the archive without overwhelming them with too much choices. In this system the user is supposed to reject the wrong subject categories.\nFor simplicity, we report accuracy based on the number of times the LLM correctly predicts at least one subject category. The subject categories are: \"Agricultural Sciences\u201d, \u201cArts and Humanities\u201d, \u201cAstronomy and Astrophysics\u201d, \u201cBusiness and Management\u201d, \u201cChemistry\u201d, \u201cComputer and Information Science\", \"Earth and Environmental Sciences\", \"Engineering\", \"Law", "Mathematical Sciences": "Medicine, Health and Life Sciences\", \"Physics\", \"Social Sciences\", plus \"Other\" and \"N/A\".\nThe Harvard archive of datasets comprises 108,729 datasets, of which 76,110 (70%) were used as the training set, and 32,619 as the test set. These sets were selected randomly. We generated two fine-tuned versions of the LLAMA2-7B model, using respectively the entire training set and a random subset of 5,000 datasets from the larger training set. We denote with \"large\" and \"small\" the two fine-tuned model in the results. We then compared the classification performance of the two fine-tuned LLAMA2-7B models against the base versions of LLAMA2 at sizes 7B, 13B, and 70B."}, {"title": "Results", "content": "Tweets Classification according to the Human Flourishing Program\nAs explained in the above, we generated curated labeled data using the workflow in Figure 1. We split the data into a training and test set of 2,404 and 2,177 tweets respectively. Considering that each tweet can be classified along 46 dimensions simultaneously, the total number of codings to analyse are 110,584 and 100,142 respectively for the two sets, meaning that each tweet is replicated according to the number of the flourishing dimensions that apply. In this application it is important to correctly identify the dimensions that applies but also those that does not apply, i.e. both true positives and true negatives matter in the analysis. This is why for this analysis we will mainly rely on the Accuracy (in the sense of Jaccard Index) and the Hamming Loss.\nBefore going into the details, we can summarize the evidence as follows:\n\u2022 the larger the number of parameters, the better the performance of the model;\n\u2022 given the size of the model parameters, fine-tuning produces a significant increase in the performances;"}, {"title": "Attribution of European Parliamentary Questions to Policy Dimension", "content": "As it can be seen in Table 2, after human validation (Step 3 of workflow in Figure 1) the zero shot method for ChatGPT4 gives very low Accuracy: 36.6%. The base LLAMA2 7B, also with zero shot method, works as badly as ChatGPT4. The proposed iterative method for ChatGPT4 gets closer to human performance: 75.8%. This is considered a good result for the CAP project.\nMost surprisingly, after fine-tuning the LLAMA2-7B model, the Accuracy of the zero-shot method jumps from about 37% to about 75%. Figure 5 also reports the F1 score, Sensitivity, Specificity and the Balanced Accuracy for the LLAMA2-7B fine-tuned model on both the train and test sets.\nNevertheless, the Macro Balance Accuracy for the train and test set are, respectively, equal to 86.5% and 86.4% for LLAMA2-7B and 83.4% for ChatGPT4."}, {"title": "Harvard Dataverse Dataset", "content": "Table 4 reports the accuracy of the LLAMA2 family of models in this multi-label classification task. As it can be seen, a fine-tuned 7B model can do as good as a 70B parameter model with as little as 5,000 labeled sets, i.e. both reach an accuracy of about 84%. By extending the training set to its whole size of 76,110 records, the accuracy get as high as 94.6%. It is interesting to notice that, even though the LLM is asked to produce at most three subject categories, in a few cases it proposes more labels than those considered true. Table 5 shows the accuracy in predicting the correct number of categories: 90.3% of the times the fine-tuned (large) model predicts exactly the same categories as those existing, and almost symmetrically it misses or produce more entries. The table cuts out the rows for which the number of true categories is 5,6, 8 and 14 (a total of 34 datasets, 0.1% of the data) and the columns for which the predict number of categories is 5, 6, or 7 (27 datasets in total or 0.08% of the datasets)."}, {"title": "Discussion", "content": "In this study, we demonstrate that small, fine-tuned open-source language models, such as LLAMA2-7B, can achieve performance comparable or better than large commercial models like ChatGPT-4. Our findings underscore that, while very large, closed-source LLMs often excel in text classification tasks, their use comes with substantial drawbacks-particularly for large-scale social science applications. Closed-source models pose risks related to transparency, data security, and replicability and rely on proprietary systems, which can impede open scientific inquiry and drive up costs. In contrast, open-source models, although requiring more careful setup, offer distinct advantages by allowing local deployment for data privacy, task-specific fine-tuning, ease of sharing, and seamless integration into reproducible workflows.\nWe also highlight the efficacy of a hybrid approach to dataset creation that leverages the strengths of both LLMs and human oversight, thus bypassing some of the traditional limitations of manual text classification. The conventional process-relying on human coders to label documents-often involves high rates of coder disagreement (up to 15-20%) due to subjective biases, rigid codebooks, or coder fatigue. This method not only introduces variability but also slows down the labeling process significantly, which can become prohibitively expensive in high-dimensional classification tasks with numerous categories. Emerging research further questions the reliability of human annotations, which have traditionally been considered the \"gold standard\" for text classification.\nOur proposed workflow circumvents these issues by using multiple LLMs to provide initial classifications and then relying on human reviewers to reject only the labels that do not apply, rather than selecting the correct ones. This approach leverages cognitive strengths such as error salience and negativity bias-that make it easier for humans to detect errors rather than assign precise labels. As a result, this hybrid process is faster, more efficient, and reduces the demand on human coders, particularly in high-dimensional classification contexts where coders might otherwise face significant cognitive load.\nThis workflow comprises five key steps: (1) data normalization for a consistent tabular format, (2) AI-driven \"crowd\" classification using one or more LLMs to produce initial labels for each document, (3) human verification to filter out incorrect labels, (4) fine-tuning of a single LLM on the human-filtered labeled dataset, and (5) scaling to classify additional, previously unseen documents. This streamlined approach to labeled dataset creation makes it possible to fine-tune smaller models efficiently and expand the classification process to large datasets, ensuring high-quality, reproducible results without compromising data sensitivity.\nFurthermore, our results reveal that smaller, fine-tuned models like LLAMA2-7B not only reach but often match or surpass the performance of large models like ChatGPT-4 on specific tasks. While large models are robust across general applications due to extensive training on broad datasets, this generalist nature can make them less adaptable to domain-specific tasks, as their weights are less responsive to fine-tuning adjustments. In contrast, smaller models tailored with a focused training set can adapt more responsively to specialized tasks, balancing the need for precision and scalability in applied research contexts.\nIn conclusion, this study demonstrates that, given a robust training set and a streamlined hybrid workflow, smaller, fine-tuned open-source models are not only viable but advantageous for specific applications in social science research. Such models provide comparable performance to larger, closed models while offering significant benefits in transparency, adaptability, data sensitivity, and computational efficiency. This approach underscores the potential for smaller, open-source models to support high-performance, reproducible research, with a cost-effective and privacy-conscious framework that empowers researchers to conduct scalable, collaborative studies."}, {"title": "Additional information", "content": "The Supplementary Material will be available soon with prompts, scripts for fine-tuning, verification results, datasets, code-books, and fine-tuned models. A link to the repository will be included in the next revision of the manuscript."}]}