{"title": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs", "authors": ["Seong Yeub Chu", "JongWoo Kim", "Bryan Wong", "MunYong Yi"], "abstract": "Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays.", "sections": [{"title": "1 Introduction", "content": "Multi-trait essay scoring, which evaluates essays on multiple dimensions such as Content, Organization, and Style, rather than on a single holistic score, has recently become a central issue in automated essay scoring (AES). Extensive research in this area has primarily utilized BERT and trait-wise layers to predict scores for individual traits (Mathias and Bhattacharyya, 2020; Ridley et al., 2021; Kumar et al., 2021; Do et al., 2023). Notably, Do et al. (2024) proposed using an autoregressive pre-trained language model, T5 (Raffel et al., 2020), for greater computational efficiency. Despite these efforts, most studies have used essay texts alone to predict labels as represented in Figure 1 (A), rather than extracting aspects evaluated by rubric indicators from the essays and using them.\nWith the advent of LLMs, generating fine-grained rationales-explanations of how essays align with rubric criteria-has become feasible. As shown in Figure 1, incorporating rationales identifies relevant essay sections that demonstrate specific traits and links them directly to the rubric. This approach mirrors how human evaluators use rubrics to assess essays in real-world settings (Freeman and Miller, 2001). For instance, a rationale for Organization highlights transitions and structure, leading to more precise, rubric-aligned evaluations. Without rationales, the model may overlook key elements and score less accurately by focusing only on the semantic sequence.\nTo the best of our knowledge, few studies have attempted to use rationale-based evaluations derived from rubrics to assess essays (Lee et al., 2024; Li et al., 2023). Lee et al. (2024) used LLMs to predict holistic scores based on criteria synthesized from rubrics, but the models cannot be fine-tuned"}, {"title": "2 Related Work", "content": "2.1 Traditional and transformer-based automated essay scoring\nTraditional automated essay scoring (AES) focused on holistic scoring, predicting an overall score using handcrafted features and linear regression models (Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Cozma et al., 2018). Particularly, transformer-based models like BERT (Devlin et al., 2018) significantly improved AES by capturing detailed language information (Yang et al., 2020; Wang et al., 2022; Mayfield and Black, 2020). These models enhanced scoring accuracy but were primarily used for holistic scoring. Extending them to multi-trait scoring is inefficient due to the need for multiple models for different traits, increasing computational costs (Kumar et al., 2021; Do et al., 2024).\n2.2 Multi-trait essay scoring approaches\nMulti-trait AES evaluates essays across various dimensions with respect to different features existing in essays such as Content, Organization, and Conventions. Existing models used multiple linear layers or separate models for each trait, which require intensive resources (Mathias and Bhattacharyya, 2020; Ridley et al., 2021). Recent approaches introduced multi-task learning frameworks with shared models and trait-specific layers, improving efficiency (Kumar et al., 2021). However, handling trait dependencies and requiring specialized modules remains challenging. The autoregressive multi-trait scoring (ArTS) model addressed this by using a pre-trained T5 model to sequentially generate trait scores, leveraging inter-dependencies for better accuracy (Do et al., 2024). Yet, it still relied solely on essay texts alone for score prediction.\n2.3 Rubric-based essay scoring using large language model\nRecently, LLMs have been used to evaluate essays alongside assessment rubrics, showing competitive performance. For example, one study (Lee et al., 2024) divided criteria into multiple traits and generated sub-criteria for scoring, achieving moderate results. Another study (Li et al., 2023) used rubrics to score short answers and generated rationales, which were then used as labels to fine-tune the T5 model (Raffel et al., 2020) to produce both scores and rationales. However, this approach did not outperform fine-tuned models like BERT (De-"}, {"title": "3 RMTS", "content": "RMTS is a framework that enhances the multi-trait essay scoring capabilities of an S-LLM, a pre-trained sequence-to-sequence model, by incorporating rationales. The framework consists of two parts: (1) generating trait-specific rationales using an LLM-based system with GPT-3.5 Turbo (OpenAI, 2022)\u00b9 and Llama-3.1-8B-Instruct (Touvron et al., 2023)\u00b2 (referred to as GPT and Llama respectively), and (2) extracting representations from both the essay and rationale using a shared encoder of the S-LLM. This dual-process approach improves the reliability of the scoring model. The detailed procedure is shown in Figures 2 and 3.\n3.1 LLM-based trait-wise rationale generation system\nAs illustrated in Figure 2, individual trait-specific prompts are constructed using the essay and the rubric corresponding to each trait. Each trait-specific prompt is then provided to a separate LLM agent dedicated to that trait. This approach, referred to as the LLM-based trait-wise rationale generation system, relies on the LLM's demonstrated ability to effectively evaluate essays, as supported by prior research (Lee et al., 2024; Ho et al., 2022; Li et al., 2023). We have adopted the prompts used in Lee et al. (2024) as a basis for the task description and modified them to fit our context. We have also added trait-specific rubric to them. Our trait-wise LLM agents generate qualitative assessments based on the rubric, producing rationales in a text form. This method enables the generation of detailed, text-based rationales that are directly tied to the rubric, facilitating a subsequent S-LLM to decide the final numeric score in a more accurate manner.\nGiven that the decoder of the S-LLM used in RMTS predicts subsequent tokens based on previ-"}, {"title": "3.2 Representation extraction and scoring", "content": "In the current study, we utilize various pre-trained encoder-decoder S-LLMs for scoring multi-traits of essays. We include five widely used models\u2014T5, Flan-T5, BART, Pegasus, and LED (Longformer Encoder-Decoder model) (Raffel et al., 2020; Chung et al., 2024; Lewis et al., 2019; Zhang et al., 2019; Beltagy et al., 2020b)\u2014as the S-LLMs for essay scoring. Figure 3 shows the RMTS architecture. Each component in RMTS framework's essay scoring model corresponds to the respective component of the individual S-LLM.\nIn RMTS, both the essay and the generated rationales are fed into a single encoder to extract their respective representations, which means that the two texts share a common encoder, allowing their representations to be projected into the same vector space. Inspired by (Do et al., 2024), we add the prompt \"Score the essay of the prompt N\" to the essay text to improve model inference. Special tokens, such as \"<Essay>\" and \"<Rationale>,\" are inserted before the essay and rationale to help the tokenizer distinguish between the two. We also introduce tokens for multi-trait names (e.g., <Content>) to prevent them from being split into sub-words, preserving their meanings.\nThe encoder processes this combined input to generate dense representations, which are integrated into a unified feature vector by a linear layer for scoring. This vector is passed to a decoder, which predicts trait-specific scores. By leveraging both the essay and rationale, the model delivers detailed multi-trait scoring."}, {"title": "3.3 Score extraction", "content": "Since we use S-LLMs, which are sequence-to-sequence models, we predict and generate scores for multiple traits alongside their respective names from each essay one at a time, based on techniques from Do et al. (2024). The generated string of scores is transformed into a dictionary format, where trait names serve as keys to extract the scores. For accurate evaluation, we disregard predictions"}, {"title": "4 Experiment", "content": "In this study, we conducted extensive experiments to analyze the generated rationales and evaluate their effectiveness in scoring multiple essay traits, guided by the following research questions.\n\u2022 RQ1. What are the key findings from the analysis of LLM-generated rationales for essay evaluation?\n\u2022 RQ2. To what extent does incorporating rationales improve the reliability of multi-trait essay scoring using S-LLMs?"}, {"title": "4.1 Datasets", "content": "In our main experiment, we utilized the ASAP\u00b3 and ASAP++\u2074 (Mathias and Bhattacharyya, 2020) datasets, comprising English essays from American high school students (grades 7\u201310) across eight prompts. The ASAP dataset provides overall scores for all essays, but only prompts 7 and 8 have trait-specific scores. Thus, we included ASAP++ for rated trait scores on the remaining prompts, and this combined dataset will be referred to as \"ASAP/ASAP++\" throughout the paper. Additionally, the Feedback Prize dataset\u2075, which consists of argumentative essays written by American students (grades 6-12) and labeled with six traits, was used without distinguishing between prompts to examine the generalizability of the incremental effect of using essays and rationales together on vanilla S-LLMs. Due to space constraints, the dataset de-scriptions are provided in Table 1."}, {"title": "4.2 Rationale Analysis", "content": "To evaluate rationale quality, we performed various analyses. We evaluated the similarity of the generated rationales using ROUGE-L (Lin, 2004) on a sample of 100 essays to analyze the diver-sity in how LLMs generate them. Additionally, we measured the faithfulness of LLM-generated"}, {"title": "4.3 Baselines", "content": "To compare performance across the two datasets described earlier, we used five widely adopted vanilla S-LLMs, all encoder-decoder models (Raffel et al., 2020; Chung et al., 2024; Lewis et al., 2019; Zhang et al., 2019; Beltagy et al., 2020b) designed for text generation tasks. We also included baseline models with a string kernel based model and RNN-based architectures from the referenced papers: HISK (Cozma et al., 2018), STL-LSTM (Dong et al., 2017) and MTL-BiLSTM (Kumar et al., 2021), which align with our model's characteristics (see Appendix A for details on each baseline). For a fair comparison with the traditional benchmark dataset (ASAP/ASAP++), we used the performance data of the four baseline models\u2014HISK (Cozma et al., 2018), STL-LSTM (Dong et al., 2017), MTL-BiLSTM (Kumar et al., 2021), and ArTS (Do et al., 2024)\u2014as reported in (Do et al., 2024). ArTS is a model that employs the vanilla T5-base model for scoring multi-trait essays."}, {"title": "4.4 Experimental Settings", "content": "In this study, we employed GPT-3.5-Turbo\u2076 and Llama-3.1-8B-Instruct\u2077 for rationale generation based on a prompt-engineering technique, and fine-"}, {"title": "4.5 Evaluation", "content": "To ensure consistent evaluation, we utilized 5-fold cross-validation across all models, employing a 60/20/20 split for training, validation, and testing, following the methodology of Taghipour and Ng (2016) and Kumar et al. (2021) with the combined ASAP and ASAP++ dataset. For the Feedback Prize dataset, we applied the same 5-fold process but with stratified splitting based on label distribution. Assessment was conducted using quadratic weighted kappa (QWK) (Cohen, 1968), the dataset's designated metric, which effectively measures score disparities between human raters and model predictions. We chose the top two models from each fold and reported the highest QWK as the final result (Do et al., 2024)."}, {"title": "5 Results", "content": "5.1 Rationale Analysis (RQ1)\nWe focus on analyzing the rationales from the ASAP/ASAP++ and Feedback Prize datasets in terms of similarity and faithfulness."}, {"title": "5.1.1 Similarity of rationales", "content": "Figure 4 displays the similarity analysis results for rationales generated by two LLMs, presenting ROUGE scores for the ASAP/ASAP++ and Feedback Prize datasets. To evaluate consistency, we calculated ROUGE scores between rationales generated for the same essay across five iterations and averaged them, labeled as \"within\". In RMTS, we"}, {"title": "5.1.2 Faithfulness of rationales", "content": "Figures 5 and 6 compare the reliability (measured by QWK against human-labeled scores) of each model in predicting essay scores for the ASAP/ASAP++ dataset, using either the essays or the LLM-generated rationales (see Appendix D for the Feedback Prize dataset). Most models performed at over 80% of their essay-only performance in nearly entire traits when using the rationales, demonstrating that rationales make a meaningful contribution to S-LLMs' essay evaluations. Given that these rationales are qualitative free-text outputs, this also indicates that they provide partial explanations for the models' score predictions (Wiegreffe et al., 2020; Jain et al., 2020; Li et al., 2023). However, as shown in Figure 5, Pegasus achieved about 40% of its essay-only performance with GPT-generated rationales and 50% with Llama generated rationales in average, suggesting that it relies more on the intrinsic features of essays than on qualitative"}, {"title": "5.2 Performance Comparison (RQ2)", "content": "To address the second research question, we first evaluate RMTS against the baseline models and vanilla S-LLMs on the ASAP/ASAP++ dataset, the standard for essay scoring. We then assess RMTS against vanilla S-LLMs on the Feedback Prize dataset to demonstrate its broader applicability. Finally, to gain deeper insights into the role of rationales, we fine-tuned three S-LLMs with rationales and essays, removing one trait at a time during the process to observe performance varia-tions.\n5.2.1 Performance with ASAP/ASAP++\nSince our target is to predict individual trait scores, we will focus on trait scoring rather than Overall scores. Owing to space constraints, the results are presented in Table 2 and 3. Table 2 shows model performance on the ASAP/ASAP++ dataset. Using GPT-generated rationales, RMTS applied to each of the five S-LLMs outperforms their respective vanilla versions across nearly all traits, except for Style in T5 and Word Choice and Voice in Flan-T5. T5 model shows incremental improvements with rationales, ranking first or second in every trait, including Overall. Additionally, RMTS with T5, BART, and LED outperforms the best traditional models\u2014HISK, STL-LSTM, and MTL-BiLSTM-in every trait using GPT rationales. Although MTL-BiLSTM has a higher Overall score, the gap with RMTS-T5 is small, and RMTS fo-"}, {"title": "5.2.2 Performance with Feedback Prize Dataset", "content": "To assess the broader applicability of using rationales to enhance S-LLMs in essay scoring, we conducted additional experiments with the Feedback Prize dataset, as shown in Table 4. Despite the small dataset size of 2.3K samples\u2014about one-third of the ASAP/ASAP++ dataset\u2014rationales improve the performance of the four S-LLMs (T5, BART, Pegasus, and LED) across most traits. However, integrating rationales does not improve the vanilla Flan-T5 model. We attribute this to the model's inherent characteristics from instruction-fine-tuning, which may prevent it from effectively incorporating rationales with such a small dataset. Nevertheless, these results indicate that rationales generally enhance model performance, even in data-scarce environments."}, {"title": "5.3 Trait Rationale Ablation Study (RQ2)", "content": "To assess the effectiveness of trait-specific rationales, we conducted an ablation study by removing a trait commonly shared across prompts, as illustrated in Figure 7. We focused on traits present in at least four prompts. Specifically, Content is evaluated in all prompts, while Organization and Conventions are scored in Prompts 1, 2, 7, and 8. Prompt Adherence, Language, and Narrativity are assessed in Prompts 3 to 6. For comparison, we divided the prompts into two groups and excluded Word Choice, Sentence Fluency, Style, and Voice since they appear in fewer prompts. We used T5, Flan-T5, and BART in the experiments, fine-tuning each model by removing one trait at a time.\nA consistent decline in performance is observed across all traits when the corresponding rationale is removed, confirming that trait rationales significantly influence their respective assessments. For instance, removing the rationale for Conventions results in a performance drop for that trait, particularly when compared to RMTS, which utilizes rationales for all traits. Although the performance of models without a trait rationale lag behind RMTS, it still outperforms vanilla models without any rationale input. This suggests that trait rationales not only influence their own assessments but also interact with and affect the evaluation of other traits (Canale and Swain, 1980). For example, when the rationale for Conventions is removed, performance still surpass that of vanilla models.\nInterestingly, performance does not always drop"}, {"title": "6 Conclusion", "content": "This paper introduces RMTS, a framework that uses prompt-engineering-based LLMs to improve multi-trait essay scoring in S-LLMs by generating trait-specific rationales aligned with rubric guidelines and incorporating them into the scoring process. Our results show that RMTS with S-LLMs significantly improves the performance of each vanilla model, with RMTS using T5 even outperforming state-of-the-art baselines. Additionally, removing rationales negatively impacts performance. These study findings highlight the substantial benefits of utilizing trait-specific rationales generated by LLMs, which has been untapped by prior research. From this view point, RMTS can be seen as opening up new horizons for automated essay scoring with S-LLMs."}, {"title": "Limitations", "content": "In this study, we have identified two primary limitations. First, our model's performance could be affected by the sequence order of traits due to the use of autoregressive models like T5 and BART. Future research should explore models like XL-Net (Yang et al., 2019), which are better suited for handling sequence orders. Secondly, we focused exclusively on multi-trait scoring of English writing. To evaluate the scalability of our model for general language education, further studies are needed on languages other than English."}, {"title": "Ethical Statement", "content": "This study utilizes only publicly available benchmark datasets, including ASAP, ASAP++, and Feedback Prize."}, {"title": "Appendix", "content": "A Details of the baselines\n\u2022 HISK (Cozma et al., 2018): is a string kernel based on histogram intersection, used in combination with a support vector regressor.\n\u2022 STL-LSTM (Dong et al., 2017): uses a combination of LSTM and CNN to infer essay scores of every trait individually.\n\u2022 MTL-BiLSTM (Kumar et al., 2021): employs trait-specific BiLSTM layers to score multi-trait, ultimately predicting the overall score.\n\u2022 T5 (Raffel et al., 2020): is a transformer-based model that frames NLP tasks as a text-to-text problem. In this study, we used the \"google-t5/t5-base\" model.\n\u2022 Flan-T5 (Chung et al., 2024): builds upon T5 (Raffel et al., 2020) by introducing fine-tuning on instruction-based datasets. In this study, we used the \"google/flan-t5-base\" model.\n\u2022 BART (Lewis et al., 2019): is a sequence-to-sequence models trained by corrupting text and learning to reconstruct the original text. In this study, we used the \"facebook/bart-base\" model.\n\u2022 Pegasus (Zhang et al., 2019): is designed specifically for abstractive summarization tasks, focusing on predicting whole sentences that have been masked. In this study, we used \"google/pegasus-x-base\" model.\n\u2022 LED (Beltagy et al., 2020b): extends the transformer architecture to handle longer documents efficiently by using sparse attention mechanisms. In this study, we used \"allenai/led-large-16384\" model."}, {"title": "B Length Statistics of Rationales", "content": "The generated rationales were tokenized using each model's corresponding tokenizer. As shown in Figure 8, aside from the rationales from the Feedback dataset generated by Llama (which reached a maximum of 586 tokens when tokenized by the T5 tokenizer), the maximum number of tokens in the rationales produced by either GPT or Llama did not exceed 512. This is the typical limit that transformer-based language models can process. This suggests that the rationale lengths are manageable and should not impede the models' ability to capture contextual information. For Llama-generated rationales in the Feedback Prize dataset, any rationales exceeding 512 tokens were truncated to comply with the limit. Interestingly, Llama tended to generate longer rationales than GPT in the Feedback dataset."}, {"title": "C Faithfulness of rationales from ASAP/ASAP++ dataset", "content": "Figure 9 shows the QWK performance of three S-LLMs-Flan-T5, Pegasus, and LED\u2014on the ASAP/ASAP++ dataset using only LLM-generated rationales (see Figure 6 for T5 and BART)."}, {"title": "D Faithfulness of rationales from Feedback Prize dataset", "content": "Figure 10 and 11 show the model performance in QWK on the Feedback Prize dataset using only LLM-generated rationales."}, {"title": "E LLM Settings", "content": "For RMTS, we used GPT-3.5-Turbo and LLama3.1-8B-Instruct provided by OpenAI and Meta. GPT-3.5-Turbo was used in the form of API\u00b9\u2076, and LLama 3.1-8B-Instruct was employed by utilizing the official code shared by Meta\u00b9\u2077. Regarding GPT, we performed the experiments with gpt-3.5-turbo-0125. When this study was conducted, the cost for processing input tokens with the model was $0.5 per 1M tokens, while generating output tokens was priced at $1.5 per 1M tokens. We consistently used identical hyperparameters: a temperature of 0, frequency and presence penalties both set to 0, and a Top-p value of 1 for the cumulative probability cutoff in nucleus sampling. Given that the temperature hyperparameter is set to 0, we conducted the experiment a single time. For prompts 3 to 6 of ASAP++, excerpts were excluded in both LLMs."}, {"title": "F Prompts and System Message", "content": "Examples of prompts and system messages used by the LLMs to generate rationales can be found in Appendix G. We revised and supplemented (Lee et al., 2024) by adding trait-specific rubric and ad-"}, {"title": "G Examples of System Messages and Predefined Template", "content": "G.1 System Message\nThe system message corresponding to each agent used in our experiment are as follows.\nSystem message: You are a member of the English essay writing test evaluation committee. Please, evaluate given essay using following information.\nG.2 Predefined Template (Prompt 1, Content)\n[Prompt]\nMore and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.\n(end of [Prompt])\n[Trait-Specific Rubric Guidelines]\nThis property checks for the amount of content and ideas present in the essay.\nScore 6: The writing is exceptionally clear, focused, and interesting. It holds the reader's attention throughout. Main ideas stand out and are developed by strong support and rich details suitable to audience and purpose. The writing is characterized by\n\u2022 clarity, focus, and control.\n\u2022 main idea(s) that stand out.\n\u2022 supporting, relevant, carefully selected details; when appropriate, use of resources provides strong, accurate, credible support.\n\u2022 a thorough, balanced, in-depth explanation / exploration of the topic; the writing makes connections and shares insights.\n\u2022 content and selected details that are well-suited to audience and purpose.\nScore 5: The writing is clear, focused and interesting. It holds the reader's attention. Main ideas stand out and are developed by supporting details suitable to audience and purpose. The writing is characterized by\n\u2022 clarity, focus, and control.\n\u2022 main idea(s) that stand out.\n\u2022 supporting, relevant, carefully selected details; when appropriate, use of resources provides strong, accurate, credible support.\n\u2022 a thorough, balanced explanation / exploration of the topic; the writing makes connections and shares insights.\n\u2022 content and selected details that are well-suited to audience and purpose.\nScore 4: The writing is clear and focused. The reader can easily understand the main ideas. Support is present, although it may be limited or rather general. The writing is characterized by\n\u2022 an easily identifiable purpose.\n\u2022 clear main idea(s).\n\u2022 supporting details that are relevant, but may be overly general or limited in places; when appropriate, resources are used to provide accurate support.\n\u2022 a topic that is explored / explained, although developmental details may occasionally be out of balance with the main idea(s); some connections and insights may be present.\n\u2022 content and selected details that are relevant, but perhaps not consistently well-chosen for audience and purpose."}, {"title": "Score", "content": "Score 3: The reader can understand the main ideas, although they may be overly broad or simplistic, and the results may not be effective. Supporting detail is often limited, insubstantial, overly general, or occasionally slightly off-topic. The writing is characterized by\n\u2022 an easily identifiable purpose and main idea(s).\n\u2022 predictable or overly-obvious main ideas; or points that echo observations heard elsewhere; or a close retelling of another work.\n\u2022 support that is attempted, but developmental details are often limited, uneven, somewhat off-topic, predictable, or too general (e.g., a list of underdeveloped points).\n\u2022 details that may not be well-grounded in credible resources; they may be based on clich\u00e9s, stereotypes or questionable sources of information.\n\u2022 difficulties when moving from general observations to specifics.\nScore 2: Main ideas and purpose are somewhat unclear or development is attempted but minimal. The writing is characterized by\n\u2022 a purpose and main idea(s) that may require extensive inferences by the reader.\n\u2022 minimal development; insufficient details.\n\u2022 irrelevant details that clutter the text.\n\u2022 extensive repetition of detail.\nScore 1: The writing lacks a central idea or purpose. The writing is characterized by\n\u2022 ideas that are extremely limited or simply unclear.\n\u2022 attempts at development that are minimal or nonexistent; the paper is too short to demonstrate the development of an idea.\n(end of [Trait-Specific Rubric Guidelines])\nRefer to the provided [Prompt], and [Trait-Specific Rubric Guidelines] to evaluate the given essay.\n[Note]\nI have made an effort to remove personally identifying information from the essays using the Named Entity Recognizer (NER). The relevant entities are identified in the text and then replaced with a string such as \"@PERSON\", \"@ORGANIZATION\", \"@LOCATION\", \"@DATE\", \"@TIME\", \"@MONEY\", \"@PERCENT\", \"@CAPS\" (any capitalized word) and \"@NUM\" (any digits). Please do not penalize the essay because of the anonymizations.\n(end of [Note])\n[Essay]\n{essay}\n(end of [Essay])\nQ. List the quotations from the [Essay] that are relevant to \"content\" and evaluate whether each quotation is well-written or not. Provide a literal evaluation of the [Essay], returning text-based feedback rather than scores. Refrain from offering additional commentary. Please keep your response within 50 words."}, {"title": "H Examples of Rationales", "content": "H.1 A Rationale of an Essay for Prompt 1 of ASAP++\n\u2022 Essay: Dear @CAPS1 @CAPS2, I believe that using computers will benefit us in many ways like talking and becoming friends will others through websites like facebook and mysace. Using computers can help us find coordibates, locations, and able ourselfs to millions of information. Also computers will benefit us by helping with jobs as in planning a house plan and typing a @NUM1 page report for one of our jobs in less than writing it. Now lets go into the wonder world of technology. Using a computer will help us in life by talking or making friends on line. Many"}, {"title": "Score (GPT", "content": "Score (GPT-3.5-Turbo): [sentence fluency]: The essay lacks sentence fluency due to inconsistent structure, awkward phrasing, and errors in grammar and punctuation. The ideas are presented in a disjointed manner, making it challenging for the reader to follow smoothly. The essay would benefit from improved sentence variety and coherence.[word choice]: The essay lacks precision and variety in word choice. There are numerous spelling and grammar errors, misuse of words, and awkward phrasing. The vocabulary is limited, and the language is not engaging or effective in persuading the audience. The essay would benefit from significant revisions for clarity and impact.[conventions]: The essay demonstrates significant issues with standard writing conventions. There are frequent errors in punctuation, spelling, grammar, and capitalization, which impede readability and clarity. The lack of control over basic conventions hinders effective communication and requires substantial editing for improvement. [organization]: The essay lacks clear organization. Ideas are scattered and not well-developed, making it difficult to follow a logical flow. The structure of a letter to the editor is attempted but not effectively maintained. Improvement is needed in organizing paragraphs and connecting ideas cohesively. [content]: The essay discusses the benefits of using computers, such as online communication, access to information, and assistance with tasks like job applications. However, the ideas lack depth and coherence, with limited development and organization, impacting the overall clarity and effectiveness of the argument."}, {"title": "Rationale (Llama", "content": "Rationale (Llama-3.1-8B-Instruct): [sentence fluency]: The essay has a choppy flow with abrupt sentence transitions and inconsistent sentence structure. Many sentences are short and lack variety, resulting in a monotonous rhythm. The use of fragments and run-on sentences further disrupts the flow, making it difficult to follow at times. [word choice]: The essay demonstrates a limited vocabulary, relying heavily on simple and generic words. The writer uses colloquial expressions and slang (\"garrenteed,\" \"I hav an\") and misuses words (\"coordibates,\" \"able ourselfs\"). The text also features overused phrases and clich\u00e9s (\"the greatest invention since sliced bread\"). [conventions]: The essay demonstrates limited control of standard writing conventions, with numerous errors in punctuation, spelling, capitalization, and grammar. Internal punctuation is frequently incorrect, and"}, {"title": "Score (GPT", "content": "Score (GPT-3.5-Turbo): [conventions]: \"they wont have to change and get up early in the morning to shower and do there hair.\" - Spelling and grammar errors. \"most students usually take showers before school.\" - Spelling errors and lack of punctuation. [grammar]: \"they wont have to change and get up early in the morning to shower and do there hair.\" (Grammar errors: \"wont\" should be \"wont,\" \"there\" should be \"their\"). [phraseology]: \"get up early in the morning to shower and do there hair.\" - Incorrect grammar and spelling. \"you wake up go brush your teeth and go to your closet and look at your cloths.\" - Incorrect grammar and spelling. \"most students usually take showers before school.\" - Relevant to the topic but lacks variety in phrases.[vocabulary]: \"they wont have to change and get up early in the morning to shower and do there hair.\" - Incorrect word usage and spelling errors. \"most students usually take showers before school.\" - Simple vocabulary, lacks complexity. [syntax]: \"The hardest part of school is getting ready.\" - Well-written, simple sentence. \"most students usually take showers before school.\" - Minor errors in sentence structure. \"when your home your comfortable and you pay attention.\" - Basic errors in sentence structure. [cohesion]: \"taking only classes helps them because at there house they\u013al be pay more attention.\" (Poorly written - lacks proper grammar and cohesion) \"most students usually take showers before school.\" (Fairly written - lacks cohesion but clear in meaning) \"when your home your comfortable and you pay attention.\" (Poorly written - lacks proper grammar and cohesion)"}, {"title": "Rationale (Llama", "content": "Rationale (Llama-3.1-8B-Instruct): [cohesion]: The essay demonstrates a limited range of cohesive devices, primarily relying on repetition of ideas and phrases to connect sentences and paragraphs. There is a lack of transitional words and phrases, resulting in abrupt shifts between ideas, and some sentences seem disconnected from the rest of the text. [syntax]: The essay demonstrates a limited range of syntactic structures, primarily using simple sentences with some compound structures. There are occasional errors in sentence formation, such as incorrect subject-verb agreement (\"they wont"}]}