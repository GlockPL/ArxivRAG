{"title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "authors": ["Eli Sennesh", "Hao Wu", "Tommaso Salvatori"], "abstract": "Unexpected stimuli induce \u201cerror\u201d or \u201csurprise\" signals in the brain. The theory of predictive coding promises to explain these observations in terms of Bayesian inference by suggesting that the cortex implements variational inference in a probabilistic graphical model. However, when applied to machine learning tasks, this family of algorithms has yet to perform on par with other variational approaches in high-dimensional, structured inference problems. To address this, we introduce a novel predictive coding algorithm for structured generative models, that we call divide-and-conquer predictive coding (DCPC). DCPC differs from other formulations of predictive coding, as it respects the correlation structure of the generative model and provably performs maximum-likelihood updates of model parameters, all without sacrificing biological plausibility. Empirically, DCPC achieves better numerical performance than competing algorithms and provides accurate inference in a number of problems not previously addressed with predictive coding. We provide an open implementation of DCPC in Pyro on Github.", "sections": [{"title": "1 Introduction", "content": "In recent decades, the fields of cognitive science, machine learning, and theoretical neuroscience have borne witness to a flowering of successes in modeling intelligent behavior via statistical learning. Each of these fields has taken a different approach: cognitive science has studied probabilistic inverse inference [Chater et al., 2006, Pouget et al., 2013, Lake et al., 2017] in models of each task and environment, machine learning has employed the backpropagation of errors [Rumelhart et al., 1986, Lecun et al., 2015, Schmidhuber, 2015], and neuroscience has hypothesized that predictive coding [Srinivasan et al., 1982, Rao and Ballard, 1999, Friston, 2005, Bastos et al., 2012, Spratling, 2017, Hutchinson and Barrett, 2019, Millidge et al., 2021] (PC) may explain neural activity in perceptual tasks. These approaches share in common a commitment to \"deep\" models, in which task processing emerges from the composition of elementary units.\nAt the computational level, probabilistic theories of perception suggest that the brain is an hypotesis testing machine, where the world is perceived via Bayesian inference [Doya, 2007]. In the PC framework, hypothesis correspond to prediction signals that flow down the cortical hierarchy to inhibit the bottom-up processing of predictable (or irrelevant) stimuli. Combining these top-down predictions with a bottom-up stimulus signal generates prediction errors, defined as the (weighted) difference between predicted and actual signals [Hoemann et al., 2017, Barrett, 2022]. Algorithmically, PC implements variational inference [Friston et al., 2006]: under some specific assumptions, a prediction\nerror $\\varepsilon$ is the gradient of a variational free energy defined over a hierarchical Gaussian generative model, i.e., $\\varepsilon := \\nabla_{\\mu} \\log N (\\mu, \\tau) = \\tau(x - \\mu)$, with respect to the location parameter $\\mu$, of a Gaussian $x \\sim N(\\mu, \\tau)$ log-density parameterized by mean $\\mu$ and precision $\\tau$.\nIn machine learning, predictive coding algorithms have recently gained popularity, as they provide a more biologically plausible alternative to backpropagation for training neural networks [Salvatori et al., 2023, Song et al., 2024]. However, PC does not perform comparably in these tasks to backpropagation due to limitations in current formulations. First, predictive coding for gradient calculation typically models every node in the computation graph with a Gaussian, and hence fails to express many common generative models. Recent work on PC has addressed this by allowing approximating non-Gaussian energy functions with samples [Pinchetti et al., 2022]. Second, the Laplace approximation to the posterior infers only a maximum-a-posteriori (MAP) estimate and Gaussian covariance for each latent variable, keeping PC from capturing multimodal or correlated distributions. Third, this loose approximation to the posterior distribution results in inaccurate, high-variance updates to the generative model's parameters.\nIn this work we propose a new algorithm, divide-and-conquer predictive coding (DCPC), for approximating structured target distributions with populations of Monte Carlo samples. DCPC goes beyond Gaussian assumptions, and decomposes the problem of sampling from structured targets into local coordinate updates to individual random variables. These local updates are informed by unadjusted Langevin proposals parameterized in terms of biologically plausible prediction errors. Nesting the local updates within divide-and-conquer Sequential Monte Carlo [Lindsten et al., 2017, Kuntz et al., 2024] ensures that DCPC can target any statically structured graphical model, while Theorem 2 provides a locally factorized way to learn model parameters by maximum marginal likelihood.\nDCPC also provides a computational perspective on the canonical cortical microcircuit [Bastos et al., 2012, 2020, Campagnola et al., 2022] hypothesis in neuroscience. Experiments have suggested that deep laminar layers in the cortical microcircuit represent sensory imagery, while superficial laminar represent raw stimulus information [Bergmann et al., 2024]; experiments in a predictive coding paradigm specifically suggested that the deep layers represent \u201cpredictions\u201d while the shallow layers represent \"prediction errors\". This circuitry could provide the brain with its fast, scalable, generic Bayesian inference capabilities. Figure 1 compares the computational structure of DCPC with that of previous PC models. The following sections detail this work's contributions:\n\u2022 Section 3 defines the divide-and-conquer predictive coding algorithm and shows how to use it as a variational inference algorithm;\n\u2022 Section 4 examines under what assumptions the cortex could plausibly implement DCPC, proving two theorems that contribute to biological plausibility;\n\u2022 Section 5 demonstrates DCPC experimentally in head-to-head comparisons against recent generative models and inference algorithms from the predictive coding literature.\nSection 2 will review the background for Section 3's algorithm: the problem predictive coding aims to solve and a line of recent work adressing that problem from which this paper draws."}, {"title": "2 Background", "content": "This section reviews the background necessary to construct the divide-and-conquer predictive coding algorithm in Section 3. Let us assume we have a directed, acyclic graphical model with a joint density split into observations $x \\in \\mathcal{x}$ and latents $z \\in \\mathcal{z}$, parameterized by some $\\theta$ at each conditional density\n$p_\\theta(x,z) := \\prod_{x\\in \\mathcal{x}} p_\\theta(x | Pa(x)) \\prod_{z \\in \\mathcal{z}} p_\\theta(z | Pa(z)),$  (1)\nwhere $Pa(z)$ denotes the parents of the random variable $z \\in \\mathcal{z}$, while $Ch(z)$ denotes its children.\nEmpirical Bayes Empirical Bayes consists of jointly estimating, in light of the data, both the parameters $\\theta^*$ and the Bayesian posterior over the latent variables $z$, that is:\n$\\theta^* = \\arg \\max_\\theta p_\\theta(x) = \\arg \\max_\\theta \\int_{\\mathcal{z}} p_\\theta(x, z) dz, \\qquad p_{\\theta^*}(z | x) := \\frac{p_{\\theta^*}(x, z)}{p_{\\theta^*}(x)}.$\nTypically the marginal and posterior densities have no closed form, so learning and inference algorithms treat the joint distribution as a closed-form unnormalized density over the latent variables;"}, {"title": "3 Divide-and-Conquer Predictive Coding", "content": "The previous section provided a mathematical toolbox for constructing Monte Carlo algorithms based on gradient updates and a working definition of predictive coding. This section will combine those tools to generalize the above notion of predictive coding, yielding the novel divide-and-conquer predictive coding (DCPC) algorithm. Given a causal graphical model, DCPC will approximate the posterior with a population $q(z)$ of $K$ samples, while also learning $\\theta$ explaining the data. This will require deriving local coordinate updates and then parameterizing them in terms of prediction errors.\nLet us assume we again have a causal graphical model $p_\\theta(x, z)$ locally parameterized by $\\theta$ and factorized (as in Equation 1) into conditional densities for each $x \\in \\mathcal{x}$ and $z \\in \\mathcal{z}$. DCPC then requires two hyperparameters: a learning rate $\\eta \\in \\mathbb{R}^+$, and particle count $K \\in \\mathbb{N}^+$, and is initialized (at $t = 0$) via a population of predictions by ancestor sampling defined as $z^0 \\sim \\prod_{z\\in \\mathcal{z}} p_\\theta(z^0 | Pa(z^0))$.\nTo respect the graphical structure of the generative model with only local computations, DCPC recursively targets each variable's (unnormalized) complete conditional density:\n$\\gamma_\\theta(z; z_{\\backslash z}) = p_\\theta(z | Pa(z)) \\prod_{v\\in Ch(z)} p_\\theta(v | Pa(v)).$ (5)\nWe observe that the prediction errors $\\varepsilon_z$ in classical predictive coding, usually defined as the precision weighted difference between predicted and actual value of a variable, can be seen as the score function of a Gaussian, where the score is the gradient with respect to the parameter $z$ of the log-likelihood:\n$\\varepsilon_z := \\nabla_z \\log N(z, \\tau) = \\tau (x - z);$\nWhen given the ground-truth parameter $z$, the expected score function $\\mathbb{E}_{x\\sim p(x|z)} [\\nabla_z\\log p(x | z)] = 0$ under the likelihood becomes zero, making score functions a good candidate for implementing predictive coding. We therefore define $\\varepsilon_z$ in DCPC as the complete conditional's score function\n$\\varepsilon_z := \\nabla_z \\log \\gamma_\\theta (z; z_{\\backslash z}) = \\nabla_z \\log p_\\theta(z | Pa(z)) + \\sum_{v\\in Ch(z)} \\nabla_z \\log p_\\theta (v | Pa(v)).$  (6)\nThis gradient consists of a sum of local prediction-error terms: one for the local \"prior\" on $z$ and one for each local \"likelihood\" of a child variable. Defining the prediction error by a locally computable gradient lets us write Equation 3 in terms of $\\varepsilon_z$ (Equation 6):\n$q_\\eta(z^t | \\varepsilon, z^{t-1}) := N (z^{t-1} + \\eta \\varepsilon^t, 2\\eta \\mathbb{I}_z).$\nThe resulting proposal now targets the complete conditional density (Equation 5), simultaneously meeting the informal requirement of Definition 1 for purely local proposal computations while also \"dividing and conquering\" the sampling problem into lower-dimensional coordinate updates."}, {"title": "4 Biological plausibility", "content": "Different works in the literature consider different criteria for biological plausibility. This paper follows the non-spiking predictive coding literature and considers an algorithm biologically plausible if it performs only spatially local computations in a probabilistic graphical model [Whittington and Bogacz, 2017], without requiring a global control of computation. However, while in the standard literature locality is either directly defined in the objective function [Rao and Ballard, 1999], or derived from a mean-field approximation to the joint density [Friston, 2005], showing that the updates of the parameters of DCPC require only local information is not as trivial. To this end, in this section we first formally show that DCPC achieves decentralized inference of latent variables $z$ (Theorem 1), and then that also the parameters $\\theta$ are updated via local information (Theorem 2).\nTheorem 1 (DCPC coordinate updates sample from the true complete conditionals). Each DCPC coordinate update (Equation 7) for a latent $z \\in \\mathcal{z}$ samples from $z$'s complete conditional (the normalization of Equation 5). Formally, for every measurable $h : \\mathcal{Z} \\rightarrow \\mathbb{R}$, resampled expectations with respect to the DCPC coordinate update equal those with respect to the complete conditional\n$\\mathbb{E}_{z \\sim q_\\eta(z|z^{t-1},\\varepsilon_t)} \\Big[\\mathbb{E}_{u \\sim \\delta(u), z'\\sim RESAMPLE(z,u)}[h(z')]\\Big] = \\int_{\\mathcal{z}} h(z) \\pi_\\theta(z | z_{\\backslash z}) dz.$\nWe follow the canonical cortical microcircuit hypothesis of predictive coding [Bastos et al., 2012, Gillon et al., 2023] or predictive routing [Bastos et al., 2020]. Consider a cortical column representing $z \\in \\mathcal{z}$; the $\\theta, \\alpha/\\beta$, and $\\gamma$ frequency bands of neuronal oscillations [Buzs\u00e1ki and Draguhn, 2004] could synchronize parallelizations (known to exist for simple Gibbs sampling in a causal graphical model [Gonzalez et al., 2011]) of the loops in Algorithm 1. From the innermost to the outermost and following the neurophysiological findings of Bastos et al. [2015], Fries [2015], $\\gamma$-band oscillations could synchronize the bottom-up conveyance of prediction errors (lines 4-6) from L2/3 of lower cortical columns to L4 of higher columns, $\\beta$-band oscillations could synchronize the top-down conveyance of fresh predictions (implied in passing from s to s + 1 in the loop of lines 2-9) from L5/6 of higher columns to L1+L6 of lower columns, and $\\theta$-band oscillations could synchronize complete attention-directed sampling of stimulus representations (lines 1-11). Figure 5 in Appendix A visualizes these hypotheses for how neuronal areas and connections could implement DCPC.\nBiological neurons often spike to represent changes in their membrane voltage [Mainen and Sejnowski, 1995, Lundstrom et al., 2008, Forkosh, 2022], and some have even been tested and found to signal the temporal derivative of the logarithm of an underlying signal [Adler and Alon, 2018, Borba et al., 2021]. Theorists have also proposed models [Chavlis and Poirazi, 2021, Moldwin et al., 2021] under which single neurons could calculate gradients internally. In short, if neurons can represent probability densities, as many theoretical proposals and experiments suggest they can, then they can likely also calculate the prediction errors used in DCPC. Theorem 2 will demonstrate that given the \"factorization\" above, DCPC's model learning requires only local prediction errors.\nTheorem 2 (DCPC parameter learning requires only local gradients in a factorized generative model). Consider a graphical model factorized according to Equation 1, with the additional assumption that the model parameters $\\theta \\in \\Theta = \\prod_{x\\in \\mathcal{x}} \\Theta_x \\times \\prod_{z\\in \\mathcal{z}} \\Theta_z$ factorize disjointly. Then the gradient $\\nabla_\\theta F(\\theta, q)$ of DCPC's free energy similarly factorizes into a sum of local particle averages\n$\\nabla_\\theta F = \\mathbb{E}_q[-\\nabla_\\theta \\log p_\\theta (x, z)] \\approx - \\frac{1}{K} \\sum_{v \\in (\\mathcal{x},\\mathcal{z})} \\sum_{k=1}^{K} \\nabla_{\\theta_v} \\log p_{\\theta_v} (v^k | Pa(v)^k).$   (9)"}, {"title": "5 Experiments", "content": "Divide-and-conquer predictive coding is not the first predictive coding algorithm to incorporate sampling into the inference process, and certainly not the first variational inference algorithm for structured graphical models. This section therefore evaluates DCPC's performance against both models from the predictive coding literature and against a standard deep generative model. Each experiment holds the generative model, dataset, and hyperparameters constant except where noted.\nWe have implemented DCPC as a variational proposal or \u201cguide\u201d program in the deep probabilistic programming language Pyro [Bingham et al., 2019]; doing so enables us to compute free energy and prediction errors efficiently in graphical models involving neural networks. Since the experiments below involve minibatched subsampling of observations $x \\sim \\mathcal{B}$ from a dataset $\\mathcal{D} \\sim p(\\mathcal{D})$ of unknown distribution, we replace Equation 9 with a subsampled form (see Welling and Teh [2011] for derivation) of the variational Sequential Monte Carlo gradient estimator [Naesseth et al., 2018]\n$\\nabla_\\theta F \\approx \\mathbb{E}_{B \\sim p(\\mathcal{D})} \\Big[ \\frac{|\\mathcal{D}|}{|B|} \\sum_{x_b \\in B} \\mathbb{E}_{(\\mathcal{z},w)^{1:K}\\sim q} \\log \\Big[\\frac{w^{x_b, k}}{K} \\sum_{k=1}^{K} w^{x_b, k} \\Big] \\Big].$  (10)\nWe optimized the free energy in all experiments using Adam [Kingma and Ba, 2014], making sure to call detach() after every Pyro sample() operation to implement the purely local gradient calculations of Theorem 2 and Equation 10. The first experiment below considers a hierarchical Gaussian model on three simple datasets. The model consists of two latent codes above an observation.\nDeep latent Gaussian models with predictive coding Oliviers et al. [2024] brought together predictive coding with neural sampling hypotheses in a single model: Monte Carlo predictive coding (MCPC). Their inference algorithm functionally backpropagated the score function of a log-likelihood, applying Langevin proposals to sample latent variables from the posterior joint density along the way. They evaluated MCPC's performance on MNIST with a deep latent Gaussian model [Rezende et al., 2014] (DLGM). Their model's conditional densities consisted of nonlinearities followed by linear transformations to parameterize the mean of each Gaussian conditional, with learned covariances. Figure 2 shows that the DLGM structure already requires DCPC to respect hierarchical dependencies."}, {"title": "6 Related Work", "content": "Pinchetti et al. [2022] expanded predictive coding beyond Gaussian generative models for the first time, applying the resulting algorithm to train variational autoencoders by variational inference and transformer architectures by maximum likelihood. DCPC, in turn, broadens predictive coding to target arbitrary probabilistic graphical models, following the broadening in Salvatori et al. [2022] to arbitrary deterministic computation graphs. DCPC follows on incremental predictive coding [Salvatori et al., 2024] in quickly alternating between updates to random variables and model parameters, giving an incremental EM algorithm [Neal and Hinton, 1998]. Finally, Zahid et al. [2024] and Oliviers et al. [2024] also recognized the analogy between predictive coding's prediction errors and the score functions used in Langevin dynamics for continuous random variables."}, {"title": "7 Conclusion", "content": "This paper proposed divide-and-conquer predictive coding (DCPC), an algorithm that efficiently and scalably approximates Gibbs samplers by importance sampling; DCPC parameterizes efficient proposals for a model's complete conditional densities using local prediction errors. Section 4 showed how Monte Carlo sampling can implement a form of \u201cprospective configuration\" [Song et al., 2024], first inferring a sample from the joint posterior density (Theorem 1) and then updating the generative model without a global backpropagation pass ( Theorem 2). Experiments in Section 5 showed that DCPC outperforms the state of the art Monte Carlo Predictive Coding from computational neuroscience, head-to-head, on the simple generative models typically considered in theoretical neuroscience; DCPC also outperforms the particle gradient descent algorithm of Kuntz et al. [2023] while under the constraint of purely local computation. DCPC's Langevin proposals admit the same extension to constrained sample spaces as applied in Hamiltonian Monte Carlo [Brubaker et al., 2012]; our Pyro implementation includes this extension via Pyro's preexisting support for HMC.\nDCPC offers a number of ways forward. Particularly, this paper employed naive Langevin proposals, while Dong and Wu [2023], Zahid et al. [2024] applied momentum-based preconditioning to take advantage of the target's geometry. Yin and Ao [2006] demonstrated that gradient flows of this general kind can also provide more efficient samplers by breaking the detailed-balance condition necessary for the Metropolis-Hastings algorithm, motivating the choice of SMC over MCMC to correct proposal bias. Appendix C derives a mathematical background for an extension of DCPC to discrete random variables. Future work could follow Marino et al. [2018], Taniguchi et al. [2022] in using a neural network to iteratively map from particles and prediction errors to proposal parameters."}, {"title": "7.1 Limitations", "content": "DCPC's main limitations are its longer training time, and greater sensitivity to learning rates, than state-of-the-art amortized variational inference trained end-to-end. Such limitations occur frequently in the literature on neuroscience-inspired learning algorithms, as well as in the literature on particle-based algorithms with no parametric form. Scaling up neuroscience-inspired algorithms is an active area of research, and successes in this direction will naturally apply to DCPC, enabling the training of larger models on more complex datasets by predictive coding. This work has no singular ethical concerns specific only to DCPC, rather than the broader implications and responsibilities accompanying advancements in biologically plausible learning and Bayesian inference."}, {"title": "C Extension to discrete sample spaces", "content": "Contemporaneously to the work of Kuntz et al. [2023] on particle gradient descent, ? derived a novel Wasserstein gradient flow and corresponding descent algorithm for discrete distributions. In their setting, each Wasserstein gradient step constructs a D-dimensional, finitely supported distribution over the C-Hamming ball of the starting sample, such that the distribution has $D_C$ possible states in total. Let $z^{t+h} \\in \\mathcal{N}_C(z^t)$ denote the resulting discrete random variable in the C-neighborhood around $z^t$ with respect to the Hamming distance. The update rule relies on simulating the gradient flow for time h, sampling from a Markov jump process at time t+h\n$z^{t+h} \\sim \\prod_{d \\in [1...D]} q(z^{t+h}| z^t).$\nA rate matrix $Q_d(z^t)$ defined by the entire discrete variable $z$ parameterizes the proposal distribution\n$q_h(z^{t+h} | z^t) = \\exp (Q_d(z^t)h).$  (14)\nthe rate matrix will have nondiagonal entries at indices $i \\neq j \\in [1 . . . C]$ in the neighborhood $\\mathcal{N}_C(z^t)$,\n$Q_d(z^t)_{i,j} = w_{i,j} g(\\frac{\\pi_\\theta (z^\\prime_{d,j})}{\\pi_\\theta (z^\\prime_{d,i})}), \\qquad \\text{ where } z^\\prime_{d',j} := \\begin{cases} z^t & \\text{if } d' \\neq d \\cr z_{d,j} & \\text{if } d'=d. \\end{cases}$\nThe above equation requires that $\\forall i, j \\in [1... C], w_{i,j} = w_{j,i} \\in \\mathbb{R}$ and $g(\\alpha) = \\alpha g(\\frac{1}{\\alpha})$. The ratio of normalized target densities $\\pi$ will equal the ratio of unnormalized densities $\\gamma$\n$\\frac{\\pi_\\theta (z^\\prime_{d,j})}{\\pi_\\theta (z^\\prime_{d,i})} = g( \\frac{\\gamma_\\theta (z^\\prime_{d,j})}{\\gamma_\\theta (z^\\prime_{d,i})})\\text{, where } z^\\prime_{d,j} := \\begin{cases} z^t & \\text{if } d' \\neq d \\cr z_{d,j} & \\text{if } d'=d. \\end{cases}$\nBased on the experimental recommendations of ?, let $w_{i,j} = w_{j,i} = 1$ and $g(\\alpha) = \\sqrt{\\alpha}$. The rate matrix then simplifies to nondiagonal and diagonal terms\n$Q_d(z^t)_{i,j} = \\sqrt{\\frac{\\gamma_\\theta (z^\\prime_{d,j})}{\\gamma_\\theta (z^\\prime_{d,i})}}, \\qquad Q_d(z^t)_{i,i} = - \\sum_{j \\neq i} Q_d(z^t)_{i,j}.$ (15)\nEquations 14 and 15 give a distribution descending the Wasserstein gradient of the free energy with respect to a particle cloud in a discrete sample space. Applying Equation 15 to $\\gamma_\\theta (z; z_{\\backslash z})$ yields a factorization in log space\n$Q(z^t)_{i,j} = \\sqrt{\\frac{\\gamma_\\theta (z^t + i; z_{\\backslash z})}{\\gamma_\\theta (z^t + j; z_{\\backslash z})}}, \\qquad \\log Q(z^t)_{i,j} = \\frac{1}{2} \\big( \\log \\gamma_\\theta (z^t + i; z_{\\backslash z}) - \\log \\gamma_\\theta (z^t + j; z_{\\backslash z}) \\big).$\nThis difference can be written as a difference of differences\n$\\log \\gamma_\\theta (z^t + i; z_{\\backslash z}) - \\log \\gamma_\\theta (z^t + j; z_{\\backslash z}) =  \\big( \\log \\gamma_\\theta (z^t + i; z_{\\backslash z}) - \\log \\gamma_\\theta (z^t ; z_{\\backslash z}) \\big) - \\big( \\log \\gamma_\\theta (z^t + j; z_{\\backslash z}) - \\log \\gamma_\\theta (z^t ; z_{\\backslash z}) \\big).$ (16)\nRecent work on efficient sampling for discrete distributions has focused on approximating density ratios, such as the one in Equation 15, with series expansions parameterized by error vectors. When the underlying discrete densities consist of exponentiating a differentiable energy function, as in ?, these error vectors have taken the form of gradients and the finite-series expansions have been Taylor series. When they do not, ? showed how they take the form of finite differences and Newton's series\n$\\log \\gamma(z') - \\log \\gamma(z) \\approx \\triangle_1 \\big(\\log \\gamma(z) \\big) \\cdot (z' - z).$ (17)"}, {"title": null, "content": "Discrete DCPC would therefore use finite differences as discrete prediction errors, breaking each discrete $z \\in \\mathcal{z}$ into dimensions and incrementing each dimension separately to construct a vector\n$\\bigtriangleup_1 f(z) := \\big( f(z_1 + 1, z_{2:D}), \\dots, f(z_{1:i}, z_i + 1, z_{i+1:D}), \\dots, f(z_{1:D-1}, z_D + 1) \\big) \\ominus f(z),$  (18)\nwhere $\\ominus$ subtracts the scalar $f(z)$ from the vector elements and $f : \\mathbb{Z}^D \\rightarrow \\mathbb{R}$ is the target function. This would lead to defining the discrete prediction error as the finite difference\n$\\varepsilon_z := \\bigtriangleup_1 \\log \\gamma_\\theta (z; z_{\\backslash z}).$ (19)\nApplying Equation 17 to the two terms of Equation 16, we obtain the approximations\n$\\log \\gamma_\\theta (z^t + i; z_{\\backslash z}) - \\log \\gamma_\\theta (z^t; z_{\\backslash z}) \\approx \\bigtriangleup_1 \\big( \\log \\gamma_\\theta (z^t ; z_{\\backslash z}) \\big)  \\cdot ((z^t + i) - z^t) \\approx \\varepsilon_z(z^t) \\cdot i,$\n$\\log \\gamma_\\theta (z^t + j; z_{\\backslash z}) - \\log \\gamma_\\theta (z^t; z_{\\backslash z}) \\approx \\bigtriangleup_1 \\big( \\log \\gamma_\\theta (z^t ; z_{\\backslash z}) \\big)  \\cdot ((z^t + j) - z^t) \\approx \\varepsilon_z(z^t) \\cdot j,$\n$\\frac{1}{2} \\log Q(z^t)_{i,j} \\approx \\frac{1}{2} \\varepsilon_z(z^t)  \\cdot (i - j).$\nDiscrete DCPC would thus parameterize its discrete proposal (Equation 14) in terms of $\\varepsilon_z$ (Equation 19), so that Equation 15 comes out to the (matrix) exponential of the (elementwise) exponential\n$q_h(z^{t+h} | \\varepsilon_z) = \\exp (Q(\\varepsilon_z)h) \\qquad Q_d(\\varepsilon_z)_{i,j} = \\exp \\Big(\\frac{(\\varepsilon_z)_{I,j} (i_d - j_d)}{2} \\Big).$"}]}