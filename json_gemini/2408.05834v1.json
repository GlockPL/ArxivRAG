{"title": "Divide-and-Conquer Predictive Coding: a structured Bayesian inference algorithm", "authors": ["Eli Sennesh", "Hao Wu", "Tommaso Salvatori"], "abstract": "Unexpected stimuli induce \u201cerror\u201d or \u201csurprise\" signals in the brain. The theory\nof predictive coding promises to explain these observations in terms of Bayesian\ninference by suggesting that the cortex implements variational inference in a proba-\nbilistic graphical model. However, when applied to machine learning tasks, this\nfamily of algorithms has yet to perform on par with other variational approaches in\nhigh-dimensional, structured inference problems. To address this, we introduce a\nnovel predictive coding algorithm for structured generative models, that we call\ndivide-and-conquer predictive coding (DCPC). DCPC differs from other formula-\ntions of predictive coding, as it respects the correlation structure of the generative\nmodel and provably performs maximum-likelihood updates of model parameters,\nall without sacrificing biological plausibility. Empirically, DCPC achieves better\nnumerical performance than competing algorithms and provides accurate inference\nin a number of problems not previously addressed with predictive coding.\nWe provide an open implementation of DCPC in Pyro on Github.", "sections": [{"title": "1 Introduction", "content": "In recent decades, the fields of cognitive science, machine learning, and theoretical neuroscience have\nborne witness to a flowering of successes in modeling intelligent behavior via statistical learning.\nEach of these fields has taken a different approach: cognitive science has studied probabilistic\ninverse inference [Chater et al., 2006, Pouget et al., 2013, Lake et al., 2017] in models of each task\nand environment, machine learning has employed the backpropagation of errors [Rumelhart et al.,\n1986, Lecun et al., 2015, Schmidhuber, 2015], and neuroscience has hypothesized that predictive\ncoding [Srinivasan et al., 1982, Rao and Ballard, 1999, Friston, 2005, Bastos et al., 2012, Spratling,\n2017, Hutchinson and Barrett, 2019, Millidge et al., 2021] (PC) may explain neural activity in\nperceptual tasks. These approaches share in common a commitment to \"deep\" models, in which task\nprocessing emerges from the composition of elementary units.\nAt the computational level, probabilistic theories of perception suggest that the brain is an hypotesis\ntesting machine, where the world is perceived via Bayesian inference [Doya, 2007]. In the PC\nframework, hypothesis correspond to prediction signals that flow down the cortical hierarchy to inhibit\nthe bottom-up processing of predictable (or irrelevant) stimuli. Combining these top-down predictions\nwith a bottom-up stimulus signal generates prediction errors, defined as the (weighted) difference\nbetween predicted and actual signals [Hoemann et al., 2017, Barrett, 2022]. Algorithmically, PC\nimplements variational inference [Friston et al., 2006]: under some specific assumptions, a prediction"}, {"title": "2 Background", "content": "This section reviews the background necessary to construct the divide-and-conquer predictive coding\nalgorithm in Section 3. Let us assume we have a directed, acyclic graphical model with a joint density\nsplit into observations $x \\in x$ and latents $z \\in z$, parameterized by some $\\theta$ at each conditional density\n$$p_{\\theta}(x,z) := \\prod_{x\\in x} P_{\\theta}(x | Pa(x)) \\prod_{z\\in z} p_{\\theta}(z | Pa(z)),$$\nwhere $Pa(z)$ denotes the parents of the random variable $z \\in z$, while $Ch(z)$ denotes its children.\nEmpirical Bayes Empirical Bayes consists of jointly estimating, in light of the data, both the\nparameters $\\theta^*$ and the Bayesian posterior over the latent variables $z$, that is:\n$$\\theta^* = \\arg \\max_{\\theta} p_{\\theta}(x) = \\arg \\max_{\\theta} \\int_{z \\in z} p_{\\theta}(x, z) dz,$$ $$P_{\\theta^*}(z | x) := \\frac{P_{\\theta^*}(x, z)}{P_{\\theta^*}(x)}$$\nTypically the marginal and posterior densities have no closed form, so learning and inference\nalgorithms treat the joint distribution as a closed-form unnormalized density over the latent variables;"}, {"title": "3 Divide-and-Conquer Predictive Coding", "content": "The previous section provided a mathematical toolbox for constructing Monte Carlo algorithms based\non gradient updates and a working definition of predictive coding. This section will combine those\ntools to generalize the above notion of predictive coding, yielding the novel divide-and-conquer\npredictive coding (DCPC) algorithm. Given a causal graphical model, DCPC will approximate the\nposterior with a population $q(z)$ of $K$ samples, while also learning $\\theta$ explaining the data. This will\nrequire deriving local coordinate updates and then parameterizing them in terms of prediction errors.\nLet us assume we again have a causal graphical model $p_{\\theta}(x, z)$ locally parameterized by $\\theta$ and\nfactorized (as in Equation 1) into conditional densities for each $x \\in x$ and $z \\in z$. DCPC then requires\ntwo hyperparameters: a learning rate $\\eta \\in \\mathbb{R}+$, and particle count $K \\in \\mathbb{N}+$, and is initialized (at\n$t = 0$) via a population of predictions by ancestor sampling defined as $z^0 \\sim \\prod_{z \\in z} P_{\\theta}(z^0 | Pa(z^0))$.\nTo respect the graphical structure of the generative model with only local computations, DCPC\nrecursively targets each variable's (unnormalized) complete conditional density:\n$$\\gamma_{\\theta}(z; Z\\setminus z) = p_{\\theta}(z | Pa(z)) \\prod_{v \\in Ch(z)} p_{\\theta}(v | Pa(v)).$$\nWe observe that the prediction errors $\\varepsilon_z$ in classical predictive coding, usually defined as the precision\nweighted difference between predicted and actual value of a variable, can be seen as the score function\nof a Gaussian, where the score is the gradient with respect to the parameter $z$ of the log-likelihood:\n$$\\varepsilon_z := \\nabla_z log N(z, t) = t (x - z);$$\nWhen given the ground-truth parameter $z$, the expected score function $\\mathbb{E}_{x \\sim p(x | z)} [\\nabla_z log p(x | z)] =$\n0 under the likelihood becomes zero, making score functions a good candidate for implementing\npredictive coding. We therefore define $\\varepsilon_z$ in DCPC as the complete conditional's score function\n$$\\varepsilon_z := \\nabla_z log \\gamma_{\\theta} (z; z\\setminus z) = \\nabla_z log p_{\\theta}(z | Pa(z)) + \\sum_{v \\in Ch(z)} \\nabla_z log p_{\\theta} (v | Pa(v)).$$\nThis gradient consists of a sum of local prediction-error terms: one for the local \"prior\" on $z$ and one\nfor each local \"likelihood\" of a child variable. Defining the prediction error by a locally computable\ngradient lets us write Equation 3 in terms of $\\varepsilon_z$ (Equation 6):\n$$q_{\\eta}(z_t | \\epsilon, z_{t-1}) := \\mathcal{N} (z_{t-1} + \\eta \\epsilon, 2\\eta I_z).$$"}, {"title": "4 Biological plausibility", "content": "Different works in the literature consider different criteria for biological plausibility. This paper\nfollows the non-spiking predictive coding literature and considers an algorithm biologically plausible\nif it performs only spatially local computations in a probabilistic graphical model [Whittington and\nBogacz, 2017], without requiring a global control of computation. However, while in the standard\nliterature locality is either directly defined in the objective function [Rao and Ballard, 1999], or\nderived from a mean-field approximation to the joint density [Friston, 2005], showing that the updates\nof the parameters of DCPC require only local information is not as trivial. To this end, in this section\nwe first formally show that DCPC achieves decentralized inference of latent variables $z$ (Theorem 1),\nand then that also the parameters $\\theta$ are updated via local information (Theorem 2)."}, {"title": "5 Experiments", "content": "Divide-and-conquer predictive coding is not the first predictive coding algorithm to incorporate\nsampling into the inference process, and certainly not the first variational inference algorithm for\nstructured graphical models. This section therefore evaluates DCPC's performance against both\nmodels from the predictive coding literature and against a standard deep generative model. Each\nexperiment holds the generative model, dataset, and hyperparameters constant except where noted.\nWe have implemented DCPC as a variational proposal or \u201cguide\u201d program in the deep probabilistic\nprogramming language Pyro [Bingham et al., 2019]; doing so enables us to compute free energy and\nprediction errors efficiently in graphical models involving neural networks. Since the experiments\nbelow involve minibatched subsampling of observations $x \\sim B$ from a dataset $D \\sim p(D)$ of\nunknown distribution, we replace Equation 9 with a subsampled form (see Welling and Teh [2011]\nfor derivation) of the variational Sequential Monte Carlo gradient estimator [Naesseth et al., 2018]\n$$\\nabla_{\\theta} F \\approx \\mathbb{E}_{B \\sim p(D)} \\frac{1}{\\vert B \\vert} \\sum_{x_b \\in B} {\\mathbb{E}_{q(z,w)_{1:K}} log \\left[ \\frac{w_{k x_b}}{\\sum_{k=1}^{K} w_{k x_b}} \\right]},$$\nWe optimized the free energy in all experiments using Adam [Kingma and Ba, 2014], making\nsure to call detach() after every Pyro sample() operation to implement the purely local gradient\ncalculations of Theorem 2 and Equation 10. The first experiment below considers a hierarchical\nGaussian model on three simple datasets. The model consists of two latent codes above an observation."}, {"title": "6 Related Work", "content": "Pinchetti et al. [2022] expanded predictive coding beyond Gaussian generative models for the first\ntime, applying the resulting algorithm to train variational autoencoders by variational inference and\ntransformer architectures by maximum likelihood. DCPC, in turn, broadens predictive coding to target\narbitrary probabilistic graphical models, following the broadening in Salvatori et al. [2022] to arbitrary\ndeterministic computation graphs. DCPC follows on incremental predictive coding [Salvatori et al.,\n2024] in quickly alternating between updates to random variables and model parameters, giving\nan incremental EM algorithm [Neal and Hinton, 1998]. Finally, Zahid et al. [2024] and Oliviers\net al. [2024] also recognized the analogy between predictive coding's prediction errors and the score\nfunctions used in Langevin dynamics for continuous random variables."}, {"title": "7 Conclusion", "content": "This paper proposed divide-and-conquer predictive coding (DCPC), an algorithm that efficiently\nand scalably approximates Gibbs samplers by importance sampling; DCPC parameterizes efficient\nproposals for a model's complete conditional densities using local prediction errors. Section 4 showed\nhow Monte Carlo sampling can implement a form of \u201cprospective configuration\" [Song et al., 2024],\nfirst inferring a sample from the joint posterior density (Theorem 1) and then updating the generative\nmodel without a global backpropagation pass ( Theorem 2). Experiments in Section 5 showed\nthat DCPC outperforms the state of the art Monte Carlo Predictive Coding from computational\nneuroscience, head-to-head, on the simple generative models typically considered in theoretical\nneuroscience; DCPC also outperforms the particle gradient descent algorithm of Kuntz et al. [2023]\nwhile under the constraint of purely local computation. DCPC's Langevin proposals admit the same\nextension to constrained sample spaces as applied in Hamiltonian Monte Carlo [Brubaker et al.,\n2012]; our Pyro implementation includes this extension via Pyro's preexisting support for HMC.\nDCPC offers a number of ways forward. Particularly, this paper employed naive Langevin proposals,\nwhile Dong and Wu [2023], Zahid et al. [2024] applied momentum-based preconditioning to take\nadvantage of the target's geometry. Yin and Ao [2006] demonstrated that gradient flows of this\ngeneral kind can also provide more efficient samplers by breaking the detailed-balance condition\nnecessary for the Metropolis-Hastings algorithm, motivating the choice of SMC over MCMC to\ncorrect proposal bias. Appendix C derives a mathematical background for an extension of DCPC to\ndiscrete random variables. Future work could follow Marino et al. [2018], Taniguchi et al. [2022] in\nusing a neural network to iteratively map from particles and prediction errors to proposal parameters."}, {"title": "7.1 Limitations", "content": "DCPC's main limitations are its longer training time, and greater sensitivity to learning rates, than\nstate-of-the-art amortized variational inference trained end-to-end. Such limitations occur frequently\nin the literature on neuroscience-inspired learning algorithms, as well as in the literature on particle-\nbased algorithms with no parametric form. Scaling up neuroscience-inspired algorithms is an\nactive area of research, and successes in this direction will naturally apply to DCPC, enabling the\ntraining of larger models on more complex datasets by predictive coding. This work has no singular\nethical concerns specific only to DCPC, rather than the broader implications and responsibilities\naccompanying advancements in biologically plausible learning and Bayesian inference."}, {"title": "C Extension to discrete sample spaces", "content": "Contemporaneously to the work of Kuntz et al. [2023] on particle gradient descent, ? derived a novel\nWasserstein gradient flow and corresponding descent algorithm for discrete distributions. In their\nsetting, each Wasserstein gradient step constructs a D-dimensional, finitely supported distribution\nover the C-Hamming ball of the starting sample, such that the distribution has DC possible states\nin total. Let $z^{t+h} \\in N_C(z^t)$ denote the resulting discrete random variable in the C-neighborhood\naround $z^t$ with respect to the Hamming distance. The update rule relies on simulating the gradient\nflow for time h, sampling from a Markov jump process at time t+h\n$$z^{t+h} \\sim \\prod_{d \\in [1...D]} q(z_d^{t+h} | z_d^t).$$ \nA rate matrix $Q_d(z^t)$ defined by the entire discrete variable $z$ parameterizes the proposal distribution\n$$q_h(z_i^{t+h} | z_i^t) = exp \\left( Q_d(z^t)h \\right).$$ \nthe rate matrix will have nondiagonal entries at indices $i \\neq j \\in [1 . . . C]$ in the neighborhood $N_C(z^t)$,\n$$Q_d(z^t)_{i,j} = w_{i,j} g \\left( \\frac{\\pi_{\\theta}(z_{d,j} \\vert z_{-d})}{\\pi_{\\theta}(z_{d,i} \\vert z_{-d})} \\right).$$\nThe above equation requires that $\\forall i, j \\in [1 . . . C], w_{i,j} = w_{j,i} \\in \\mathbb{R}$ and $g(\\alpha) = \\sqrt{\\alpha}$. The ratio of\nnormalized target densities $\\pi$ will equal the ratio of unnormalized densities $\\gamma$\n$$\\frac{\\pi_{\\theta} (z_{d,j} \\vert z_{-d})}{\\pi_{\\theta} (z_{d,i} \\vert z_{-d})} = g \\left( \\frac{\\frac{\\gamma_{\\theta} (z_{d,j} \\vert z_{-d})}{Z_{\\theta} (z_{d,i} \\vert z_{-d})}}{\\frac{\\gamma_{\\theta} (z_{d,i} \\vert z_{-d})}{Z_{\\theta} (z_{d,i} \\vert z_{-d})}} \\right) = g \\left( \\frac{\\gamma_{\\theta} (z_{d,j} ; z\\setminus z)}{\\gamma_{\\theta} (z_{d,i} ; z\\setminus z)} \\right)$$\nBased on the experimental recommendations of ?, let $w_{i,j} = w_{j,i} = 1$ and $g(\\alpha) = \\sqrt{\\alpha}$. The rate\nmatrix then simplifies to nondiagonal and diagonal terms\n$$Q_d(z^t)_{i,j} = \\sqrt{\\frac{\\gamma_{\\theta} (z_{d,j} \\vert z_{-d})}{\\gamma_{\\theta} (z_{d,i} \\vert z_{-d})}},$$ $$Q_d(z^t)_{i,i} = - \\sum_{j \\neq i} Q_d(z^t)_{i,j}.$$ \nEquations 14 and 15 give a distribution descending the Wasserstein gradient of the free energy with\nrespect to a particle cloud in a discrete sample space. Applying Equation 15 to $\\gamma_{\\theta} (z; z\\setminus z)$ yields a\nfactorization in log space\n$$Q(z^t)_{i,j} = \\frac{\\gamma_{\\theta} (z^t + i; z\\setminus z)}{\\gamma_{\\theta} (z^t + j; z\\setminus z)}$$ $$\\\\ log Q(z^t)_{i,j} = \\frac{1}{2} \\left( log \\gamma_{\\theta} (z^t + i; z\\setminus z) - log \\gamma_{\\theta} (z^t + j; z\\setminus z) \\right).$$\nThis difference can be written as a difference of differences\n$$log \\gamma_{\\theta} (z^t + i; z\\setminus z) - log \\gamma_{\\theta} (z^t + j; z\\setminus z) = \\left( log \\gamma_{\\theta} (z^t + i; z\\setminus z) - log \\gamma_{\\theta} (z^t; z\\setminus z) \\right) - \\left( log \\gamma_{\\theta} (z^t + j; z\\setminus z) - log \\gamma_{\\theta} (z^t; z\\setminus z) \\right).$$\nRecent work on efficient sampling for discrete distributions has focused on approximating density\nratios, such as the one in Equation 15, with series expansions parameterized by error vectors. When\nthe underlying discrete densities consist of exponentiating a differentiable energy function, as in ?,\nthese error vectors have taken the form of gradients and the finite-series expansions have been Taylor\nseries. When they do not, ? showed how they take the form of finite differences and Newton's series\n$$log \\gamma(z') - log \\gamma(z) \\approx \\triangle_h \\left( log \\gamma(z) \\right) \\cdot (z' - z).$$\nDiscrete DCPC would therefore use finite differences as discrete prediction errors, breaking each\ndiscrete $z \\in z$ into dimensions and incrementing each dimension separately to construct a vector\n$$\\triangle_h f(z) := \\left( f(z_1 + h, z_{2:D}), . . ., f(z_{1:i}, z_i + h, z_{i+1:D}), ..., f(z_{1:D-1}, z_D + h) \\right) - f(z),$$\nwhere - subtracts the scalar $f(z)$ from the vector elements and $f : \\mathbb{Z}^D \\rightarrow \\mathbb{R}$ is the target function.\nThis would lead to defining the discrete prediction error as the finite difference\n$$\\epsilon_z := \\triangle_h log \\gamma_{\\theta} (z; z\\setminus z).$$ \nApplying Equation 17 to the two terms of Equation 16, we obtain the approximations\n$$log \\gamma_{\\theta} (z^t + i; z\\setminus z) - log \\gamma_{\\theta} (z^t; z\\setminus z) \\approx \\triangle_h \\left( log \\gamma_{\\theta} (z^t; z\\setminus z) \\right) \\cdot (z^t + i) - z^t)$$\n$$\\\\ \\approx \\epsilon_z(z^t) \\cdot i,$$\n$$log \\gamma_{\\theta} (z^t + j; z\\setminus z) - log \\gamma_{\\theta} (z^t; z\\setminus z) \\approx \\triangle_h \\left( log \\gamma_{\\theta} (z^t; z\\setminus z) \\right) \\cdot (z^t + j) - z^t)$$\n$$\\approx \\epsilon_z(z^t) \\cdot j,$$\n$$log Q(z^t)_{i,j} \\approx \\frac{1}{2} \\epsilon_z(z^t) (i - j).$$\nDiscrete DCPC would thus parameterize its discrete proposal (Equation 14) in terms of $\\epsilon_z$ (Equa-\ntion 19), so that Equation 15 comes out to the (matrix) exponential of the (elementwise) exponential\n$$q_h (z^{t+h} \\vert \\epsilon_z) = exp \\left( Q(\\epsilon_z) h \\right),$$ $$Q_d(\\epsilon_z)_{i,j} = exp \\left( \\frac{\\epsilon_z (i_d - j_d)}{2} \\right)$$"}]}