{"title": "CasFT: Future Trend Modeling for Information Popularity Prediction with Dynamic Cues-Driven Diffusion Models", "authors": ["Xin Jing", "Yichen Jing", "Yuhuan Lu", "Bangchao Deng", "Xueqin Chen", "Dingqi Yang"], "abstract": "The rapid spread of diverse information on online social platforms has prompted both academia and industry to realize the importance of predicting content popularity, which could benefit a wide range of applications, such as recommendation systems and strategic decision-making. Recent works mainly focused on extracting spatiotemporal patterns inherent in the information diffusion process within a given observation period so as to predict its popularity over a future period of time. However, these works often overlook the future popularity trend, as future popularity could either increase exponentially or stagnate, introducing uncertainties to the prediction performance. Additionally, how to transfer the preceding-term dynamics learned from the observed diffusion process into future-term trends remains an unexplored challenge. Against this background, we propose CasFT, which leverages observed information Cascades and dynamic cues extracted via neural ODEs as conditions to guide the generation of Future popularity-increasing Trends through a diffusion model. These generated trends are then combined with the spatiotemporal patterns in the observed information cascade to make the final popularity prediction. Extensive experiments conducted on three real-world datasets demonstrate that CasFT significantly improves the prediction accuracy, compared to state-of-the-art approaches, yielding 2.2%-19.3% improvement across different datasets.", "sections": [{"title": "Introduction", "content": "The advent of the digital age has led to the emergence of various online social platforms (OSNs), such as Twitter and Facebook, revolutionizing the way of sharing information among people. Users on OSNs can freely post and share their interests, which can then be followed or re-posted by others, resulting in the widespread dissemination of content in the form of information cascades (Cheng et al. 2014). In such an era of information explosion, understanding future trends in information - predicting its future popular- \\u2014 can aid social management (Shen et al. 2014) and benefit various applications, including recommendation systems (Wang et al. 2021), fake news detection (Lazer et al. 2018), and personalized user experiences (Li et al. 2014). Specifically, this prediction task, known as information popularity prediction, aims at forecasting the future increase in"}, {"title": "Related Work", "content": "In the current literature, cascade popularity prediction techniques can be roughly classified into three categories.\nFeature-based approaches: Feature-based approaches extract observed features manually which include information about the content, such as tags (Ma, Sun, and Cong 2013), topic features (Martin et al. 2016), and graph structures (Gao, Ma, and Chen 2014; Wang et al. 2016), temporal features such as publication time (Petrovic, Osborne, and Lavrenko 2011; Wu et al. 2016), observation time (Cheng et al. 2014; Yang and Leskovec 2011), first participation time (Zaman, Fox, and Bradlow 2014), etc. However, these works heavily rely on the quality of hand-crafted features and thus are dataset-specific and have limited generalizability.\nGenerative-based approaches: These methods use probabilistic models to describe the propagation, including epidemic models (Matsubara et al. 2012), survival analysis (Lee, Moon, and Salamatian 2012), and various stochastic point processes, e.g., Poisson process (Bao et al. 2015), Hawkes process (Zhao et al. 2015), (Zhang, Aravamudan, and Anagnostopoulos 2022). However, the selection of underlying probabilistic models can lead to significant differences in performance.\nDeep-learning-based approaches: The recent rapid development of deep learning has also spurred the emergence of numerous information cascade models based on deep neural networks. DeepHawkes (Cao et al. 2017) constructs an end-to-end network by Hawkes processes. DeepCas (Li et al. 2017) is the first cascade graph representation learning method, capturing both structural and temporal information. VaCas (Zhou et al. 2020a) and CasFlow (Xu et al. 2021) propose a hierarchical graph learning method using variational autoencoders or normalizing flows to model uncertainty in cascade graphs, and a bidirectional GRU to capture temporal dynamics. CasSeqGCN (Wang et al. 2022) aggregates cascade node representations using a dynamic routing mechanism. CTCP (Lu et al. 2023)integrates multiple cascades into a diffusion graph to further capture the correlation between cascades and users. However, existing methods only consider modeling cascades within observation. We argue that CasFT incorporates the future trends of popularity, thereby improving the prediction performance."}, {"title": "Preliminaries", "content": "In this section, we present the definitions of key concepts in our work and give a brief introduction to the concepts of neural ODEs and diffusion models."}, {"title": "Problem Definition", "content": "Suppose that a Twitter user uo posts a tweet I at time ts = 0 (we set the start time to 0) and other users can interact with tweet I through various actions such as liking, retweeting, and commenting. Here we focus on the \"retweet\\u201d behavior, while our work can also be generalized to other actions. Specifically, when a user u\u2081 retweets I posted by user uo at time t1, we can define a triplet (40, 41, t\u2081) to present the diffusion of information I from user uo to user u\u2081. Subsequently, a series of such triplets constitutes a retweet cascade C. Specifically, within a given observation time to, the retweet cascade C(to) can be defined as the set of all relevant triplets, i.e., C(to) = {(Ui1, Ui2, ti)}i\u2208N, where N represents the total number of triplets involved in the retweet process during the observation period and ti < to.\nCascade Graph: Given a tweet I and an observation time to, the cascade graph can be defined as G(to) = (Vc, Ec), where Ve represents the set of users involved in C(to), and Ec represents the set of edges, each denoting a retweet relationship between two users (nodes).\nCascade Sequence: Different from the cascade graph, the cascade sequence S(to) for cascade C(to) is a collection of users, each arranged in chronological order based on their timestamps, i.e., S(to) = {uo, u1,..., un}.\nGlobal Graph: Given all the retweet cascades under the observation time to, we define the global graph as G = {G1(to)|I \u2208 I}, where the edge in Eg represents the node relationship from cascading, such as the follower/followee relationship in the social network.\nCascade Popularity Prediction: Given a cascade C(to) and a duration to, we predict its incremental popularity P over the time interval from to to tp, where tp \u226b to, and tp is the prediction time. P represents the number of triplets occurring during the time period (to, tp)."}, {"title": "Neural Ordinary Differential Equations", "content": "A neural Ordinary Differential Equation (ODE) (Chen et al. 2018) describes the continuous-time evolution of variables. It represents a transformation of variables over time, where the initial state at time to, denoted as h(to), is integrated forward using an ODE to determine the transformed state at any subsequent time ti.\n$\\frac{dh(t)}{dt} = f(h(t), t;\\theta) \\text{ where } h(0) = h_0$\n(1)\n$h(t_i) = h(t_0) + \\int_{t_0}^{t_i} \\frac{dh(t)}{dt}dt$\n(2)\nwhere f denotes a neural network, such as a feed-forward or convolutional network."}, {"title": "Diffusion Models", "content": "Diffusion models (Ho, Jain, and Abbeel 2020) are used to generate high-quality samples from complex data distributions through a bi-directional process.\nThe forward process of diffusion model is a Markov Chain that gradually adds Gaussian noise to x\u00ba:\nq(x_{1:T}|x^0) = \\prod_{t=1}^{T} q(x_t|x_{t-1}),\n(3)\nq(x_t|x_{t-1}) = N(x_t; \\sqrt{1 - \\beta_t} x_{t-1}, \\beta_t I),\n(4)\nwhere T is the timesteps of adding noise and $ \\beta_t \\in (0,1)$ is the variance schedule.\nFor the denoising process, the goal is to learn a conditional probability distribution $p_\\theta(x_{t-1}|x_t)$ that reverses the diffusion process:\np_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_\\theta(x_t, t)I),\n(5)\nwhere the parameter @ can be optimized by minimizing the negative log-likelihood via an Evidence Lower Bound (ELBO):\n$\\min_\\theta E_{q(x_0)} \\le \\min_\\theta E_{q(x_{0:K})} L$\n(6)\n$L = -\\log p_\\theta(x_0|x_1) - \\sum_{k=1}^{K} \\log \\frac{p_\\theta(x_{k-1}|x_k)}{q(x_k|x_{k-1})}$\n(7)"}, {"title": "Our Method: CasFT", "content": "Our proposed CasFT can be summarized as a three-step model: the first step involves extracting spatiotemporal patterns from the observed cascade, the second step simulates the future trends of popularity increment, and the third step makes the final prediction. The overall framework is shown in Fig. 2. We now provide a detailed discussion of its main components."}, {"title": "Historical Spatiotemporal Modeling", "content": "Given an observed cascade C(to), CasFT first extracts the spatiotemporal features from it. Specifically, this step involves two components: (1) spatial structural pattern extraction from the observed cascade graph G(to) and the global graph G, and (2) temporal pattern extraction based on the cascade sequence. The details are outlined as follows."}, {"title": "Spatial Structural Pattern Extraction:", "content": "In light of the distinct characteristics between the cascade and global graphs, we employ two different graph embedding methods to extract unique structural patterns from each, respectively. For the cascade graph, we use GraphWave (Donnat et al. 2018) to capture local structural information, as referenced in previous studies (Zhou et al. 2020b; Xu et al. 2021). For the large-scale global graph, we employ NetSMF (Qiu et al. 2019), which is designed for large-scale networks and can quickly learn interactions between all users, efficiently embedding these global structural patterns into node embeddings. Specifically, given the observed cascade graph G(to) and the global graph G, we calculate the local-view and global-view representations of users as follows:\n$E_c = GraphWave(G(t_0)),$\n(8)\n$E_g = NetSMF(G),$\n(9)\nwhere Ec and Eg represent local-view and global-view representations for users in G(to) and G, respectively. For a specific user ui, we retrieve its representations from these two views according to its index, denoted as Ec(ui) and Eg(ui)."}, {"title": "Temporal Pattern Extraction:", "content": "In this phase, we split the cascade sequence S(to) into multiple sub-sequences in chronological order, represented as S = {Sj}jEN = {(\u03b9\u03bf), (\u03ba\u03bf, 41), ..., (uo, u1, ..., UN)}, and employ the self-attention mechanism (Vaswani et al. 2017) to each sub-sequence to capture its internal interdependencies, thereby accounting for the long-term history information. Firstly, for each timestamp ti, we define a temporal encoding procedure by trigonometric functions:\n$[z(t_i)]_j = \\begin{cases} \\cos(t_i/10000^j), & \\text{if } j \\text{ is odd} \\\\ \\sin(t_i/10000^j), & \\text{if } j \\text{ is even} \\end{cases}$\n(10)\nWe deterministically compute z(ti) \u2208 RB, where B is the dimension of encoding and get Zt which is the set of z(ti). The initial embedding of each sub-sequence Sj is specified by\nx(i) = z(ti) + Ec(uz),\n(11)\nE = {x(0), x(1), ..., x(i)},\n(12)\nE = { E_g(u_0), E_g(u_1), ..., E_g(u_i) },\n(13)\nwhere ui \u2208 Sj, and ti is its associated time stamp. We then encode E\u00bf and Ej via two independent self-attention layers. Specifically, the scaled dot-product attention (Vaswani et al. 2017) is defined as:\n$\\mathcal{S} = Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$\n(14)\nwhere Q, K, V represent queries, keys, and values. In our case, the self-attention operation takes E or Ej as input and converts them into different groups of Q, K, and V through separate linear transformations. By applying the above self-attention operation to all sub-sequences, we obtain sets of hidden representations sc = {s_0^c, s_1^c, ..., s_N^c} and sg = {s_0^g, s_1^g, ..., s_N^g} for the local-view and global-view, respectively. Finally, we concatenate these two views of hidden representations at the sub-sequence level to form the final historical spatiotemporal features of S, denoted by {S0, S1,..., SN}."}, {"title": "Interpretation of Future Trends", "content": "As we discussed in the Introduction section, the real dynamics of cascade evolution between the observation and prediction periods are invisible and difficult to depict due to their fluctuations. To address this challenge, we designed a dynamic cues-driven diffusion model to simulate future trends based solely on the observed cascades. Specifically, this model consists of two components: (1) a dynamic cues learning module and (2) a diffusion model-based future trends simulation module. To acquire the dynamic cues, we are inspired by ODEs, which are used to model quantity changes over time in dynamic systems. We introduce a neural ODEs to model the continuous-time growth rate during the observation period, capturing its evolving tendency. Afterward, the future growth rate between the observation and prediction periods can be autoregressively predicted using this neural ODEs. Finally, the dynamic cues of popularity increment are derived through an integration operation based on these predicted growth rates. However, one limitation of ODEs is that they are constrained by the initial state of the dynamic system, leading to popularity predictions based on the estimated growth rate that are biased and lack realistic variability. Accordingly, we further divide the estimated period into several time spans and employ diffusion models to generate incremental popularity in each time span, conditioned on the growth rate."}, {"title": "Dynamic Cues Learning with Neural ODEs:", "content": "We leverage neural ODEs to model the continuous-time growth rate dynamics with a vector representation ht\u2081 at every timestamp ti. Here we parameterize the growth rate using hidden state dynamics:\n\u03bb*(t) = f1 (ht),\n(15)\nwhere f is a linear neural network with the softplus function to ensure the output is positive. As for the continuous-time hidden state ht\u2082, we use a multi-layer fully connected neural network f2 to model the continuous change in the form of an ODE. When a new retweeting action occurs at time ti, we use a GRU function g to model instantaneous changes triggered by a newly observed event:\n$\\frac{dh_{t_i}}{dt} = f_2(t, h_{t-1}),$\n(16)\n$h = ODESolve(f_2, h_{t_{-1}}, (t_{i-1}, t_i)),$\n(17)\n$h_{t_i} = g(h_t, s_i),$\n(18)\nand the popularity at time ti can be calculated as:\n$P_{t_i} = \\int_{0}^{t_i} \\lambda^*(\\tau)d\\tau = \\Delta_{t_i}.$\n(19)\nThen we can get the hidden state, growth rate, and cumulative popularity at an arbitrary time by the above equations, even after observation time."}, {"title": "Future Trends Simulation with Diffusion Model:", "content": "We first divide the time between the observation and prediction time into l uniform time intervals, denoted as {(to, ts1), ..., (tsi-1, tsi), ..., (tst\u22121,tsi)}, where i \u2208 l. The start time is to and the end time tsi equals to tp. We aim to model the incremental popularity in each time interval and define Y = (P1, P2, ..., P\u2081) as the sequential increase process of popularity. Yi represents the popularity during the time interval (ti\u22121,ti).\nWe compute the hidden state ht at the exact observation time, and the dynamic cues {Ats1,..., Atsi} by using Eq. (16) and (19), respectively. We then concatenate them together to form a new guide vector c. Conditioned on c, we utilize the diffusion model to approximate the real distribution of the incremental popularity sequence Y. Specifically, we characterize the diffusion operation of Y as a Markov process (Y0, Y1, ..., YK) to model the potential changes of incremental popularity over the prediction window, with T denoting the number of diffusion steps. In the forward process, Gaussian noise is added to Yo step by step until it is corrupted into pure Gaussian noise, which is formulated as follows:\nq(Y_k|Y_{k-1}) = N(Y_k; \\sqrt{1 - \\beta_k} Y_{k-1}, \\beta_k I).\n(20)\nDuring the reverse diffusion process, we iteratively reconstruct the incremental popularity sequence Y\u00ba for the final prediction. Specifically, the denoising process does not only depend on the representation obtained in the previous step but also on the significant cue c serving as a condition, which is formulated as follows:\np_\\theta(Y_{0:K}|c) := p(Y_K) \\prod_{k=1}^{K} p_\\theta(Y_{k-1}|Y_k, c),\n(21)\nIn our implementation, we adopt a fully connected layer to parameterize the denoising process and use Denoising Diffusion Implicit Models (DDIM) (Song, Meng, and Ermon 2020) to generate segmented popularity sequence Y."}, {"title": "Prediction & Optimization", "content": "Prediction. After obtaining both the hidden state ht. at observation time to and the generated segmented popularity sequence Y\u00ba, we concatenate and feed them into an MLP to make the final cascade popularity prediction:\nP = Softplus(MLP([Y\u00ba, ht\uff61]))\n(22)\nwhere the softplus activation function is to ensure the predicted popularity is positive."}, {"title": "Conclusion", "content": "In this work, we propose CasFT, which leverages observed information Cascades and dynamic cues modeled via neural ODEs as conditions to guide the generation of Future popularity-increasing Trends through a diffusion model. The generated trends are integrated with the spatiotemporal patterns present in the observed information cascades to enhance the accuracy of popularity predictions. Experiments on three real-world information cascade datasets demonstrate the superior performance of CasFT compared to a sizeable collection of state-of-the-art baselines. CasFT significantly outperforms all the baselines, with 2.2%-19.3% improvement over the best-performing baseline methods across various datasets."}]}