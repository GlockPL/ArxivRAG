{"title": "Neural Horizon Model Predictive Control - Increasing Computational Efficiency with Neural Networks", "authors": ["H. Alsmeier", "A. Savchenko", "R. Findeisen"], "abstract": "The expansion in automation of increasingly fast applications and low-power edge devices poses a particular challenge for optimization based control algorithms, like model predictive control. Our proposed machine-learning supported approach addresses this by utilizing a feed-forward neural network to reduce the computation load of the online-optimization. We propose approximating part of the problem horizon, while maintaining safety guarantees \u2013 constraint satisfaction \u2013 via the remaining optimization part of the controller. The approach is validated in simulation, demonstrating an improvement in computational efficiency, while maintaining guarantees and near-optimal performance. The proposed MPC scheme can be applied to a wide range of applications, including those requiring a rapid control response, such as robotics and embedded applications with limited computational resources.", "sections": [{"title": "I. INTRODUCTION", "content": "From its roots in the process industry, model predictive control (MPC) has been established as a safe and reliable control algorithm for a wide range of systems, from au- tonomous driving and robotics to (bio)chemical plants. Its wide adoption has been driven by MPCs flexibility of formu- lating control problems, satisfying constraints, and enabling optimal operation [1], [2]. However, MPC can lose real-time feasibility in case the plant evolves faster than the algorithm can solve the underlying optimal control problem (OCP) [1], [3], e.g. if the system model consists of many states, or its prediction horizon is too long. Thus, the speedup of the OCP is a popular research topic for MPCs [4].\nOne method to accelerate computations is to solve the MPC problem to sub-optimality, as demonstrated, for exam- ple, by the real-time iteration approach outlined in [5], which linearizes the OCP to solve a quadratic problem.\nAnother approach is explicit MPC, which aims to deter- mine an analytic pre-computed MPC solution. It is, however, typically limited to linear systems with quadratic cost and small state dimensions, as the solution consists of numerous linearly affine control laws [6].\nMachine learning (ML) can be used to approximate the implicit MPC control law and replace the controller in the closed-loop with an approximation. In [7] the implicit control law of a MPC controller was learned from data samples of measured states and resulting MPC control actions. By avoiding online optimization such imitation controllers re-"}, {"title": "II. PROBLEM SETUP", "content": "We consider a discretized continuous-time nonlinear sys- tem:\n$$x(t_{i + 1}) = f(x(t_i), u(t_i)).$$\nHere $$x(t_i) \\in \\mathbb{R}^{n_x}$$ represents the system states, $$u(t_i) \\in \\mathbb{R}^{n_u}$$ are the applied inputs for the current time point $$t_i$$. The function $$f: \\mathbb{R}^{n_x} \\times \\mathbb{R}^{n_u} \\to \\mathbb{R}^{n_x}$$ denotes the state transition mapping.\nWe consider a model predictive controller (MPC), which works as follows [1], [2]. At each sampling point $$t_i$$ it solves an optimal control problem (OCP) and applies its first optimal input to the system (1) until the next sampling time $$t_{i+1}$$.\nBy repeating this process from the next sampling point"}, {"title": "III. NEURAL HORIZON MPC", "content": "We propose simplifying the OCP formulation (2) over $$k \\in [M+1,..., N]$$ for some $$M \\ll N$$. This procedure removes decision variables by replacing the state predictions with a computationally cheap approximated NN-based model. We denote these approximated sequences of states as $$\\lbrace x_k \\rbrace_{M+1}^N$$, and the corresponding mapping as\n$$\\lbrace x_k \\rbrace_{M+1}^N = f_s(x_M).$$\nThe function $$f_s(\\cdot)$$ is a neural network of the form (5), gen- erating an approximated optimal open-loop state sequence $$\\lbrace x_k \\rbrace_{M+1}^N$$ based on the state $$x_M$$, which remains a decision variable. $$\\lbrace x_k \\rbrace_{M+1}^N$$ replaces the tail of the optimal solution trajectory of the OCP (2). This sequence is then employed to calculate the cost function over the horizon $$k \\in [M + 1,..., N]$$, approximating the cost-to-go with optimal $$u_k$$ of the OCP (2) over the horizon $$M$$ to $$N$$. This approach removes dependencies of the states on inputs $$\\lbrace u_k \\rbrace_{M+1}^N$$, since we outright generate the optimal state sequence given this optimal input sequence. And as the neural network produces the entire sequence at once, fewer function evaluations of $$f_s(\\cdot)$$ are needed during optimization. The modified OCP, that we denote Neural Horizon MPC, we thus formulate as\n$$\\begin{aligned}\n\\arg \\min_{ \\lbrace x_k \\rbrace_{0}^M, \\lbrace u_k \\rbrace_{0}^{M-1}} & \\sum_{k=0}^{M-1} L( \\lbrace x_k \\rbrace_{0}^{M-1}, \\lbrace u_k \\rbrace_{0}^{M-1}) + \\sum_{k=M+1}^N L( \\lbrace \\tilde{x}_k \\rbrace_{M+1}^N) + V(\\tilde{x}_N) \\\\\ns.t. \\\\\nx_0 &= x_{init}, \\\\\nx_{k+1} &= f (x_k, u_k) \\quad \\forall k \\in [0,..., M - 1], \\\\\nx_k &\\in \\mathcal{X}_k \\quad \\forall k \\in [0,..., M], \\\\\nu_k &\\in \\mathcal{U}_k \\quad \\forall k \\in [0,..., M], \\\\\\lbrace \\tilde{x}_k \\rbrace_{M+1}^N &= f_s (x_M), \\\\\n\\tilde{x}_k &\\in \\mathcal{X}_k \\quad \\forall k \\in [M+1,..., N]\n\\end{aligned}$$\nIn contrast to (2), current formulation introduces the ap- proximated stage costs $$L(\\lbrace \\tilde{x}_k \\rbrace_{M+1}^N)$$, terminal cost $$V_f(\\tilde{x}_N)$$, state sequence $$\\lbrace \\tilde{x}_k \\rbrace_{M+1}^N = f_s(x_M)$$, and state constraints $$\\mathcal{X}_k$$. Since the state sequence is generated from a static mapping $$f_s(x_M)$$, it can be viewed as a more elaborate terminal cost and terminal constraint for the OCP (2) with a horizon $$M$$. For process safety, it is to note that we retain the ability to enforce constraints over the Neural Horizon $$\\tilde{x}_k \\in \\mathcal{X}_k$$. Furthermore, introduced cost functions can be separately tuned to compensate for possible state prediction errors.\nIn the simplest case, we can reuse all components of (2) for the Neural Horizon MPC, such as state constraints $$\\mathcal{X}_k = \\mathcal{X}_k$$ and the corresponding components of the cost function. However, as will be shown in Section III-B, some adjustments are needed for stability guarantees."}, {"title": "A. Cost Estimation of Neural Horizon", "content": "The mapping (6) converts a state of dimension $$n_x$$ to a state sequence of length N-M, i.e. $$f_s: \\mathbb{R}^{n_x} \\to \\mathbb{R}^{n_x \\times (N-M)}$$. Al- ternatively, we can consider approximating the cost function"}, {"title": "B. Recursive Feasibility and Constraint Satisfaction", "content": "To assess the recursive feasibility and constraint satis- faction of the presented Neural Horizon MPC design we introduce the following Assumption:\nAssumption 1. Given a continuously differentiable mapping $$\\Phi: A \\to B: a \\to b$$ and an error bound $$\\gamma > 0$$ there exists a representative dataset $$\\lbrace (a_i, b_i) \\rbrace_{i \\in I}$$ and a training procedure for the neural network of type (5), which results in NN(a) $$ \\in $$ B and $$||NN(a) - \\Phi(a)||_{\\infty} < \\gamma$$ for all $$a \\in A$$.\nAs the neural networks are general approximators, in our view it is reasonable to expect the properties in Assumption 1 to hold for sufficiently large networks and training sets, even if guaranteeing it is difficult.\nThis assumption can now be used to generate the training data in a way, that the predictions are guaranteed to satisfy given state constraints $$\\mathcal{X}$$. For that, we adapt the OCP (2) as follows:\n$$\\begin{aligned}\n\\Phi_\\epsilon(x_{init}) = & \\arg \\min_{ \\lbrace x_k \\rbrace_{0}^N, \\lbrace u_k \\rbrace_{0}^{N-1}} \\sum_{k=0}^{N-1} L( \\lbrace x_k \\rbrace_{0}^{N-1}, \\lbrace u_k \\rbrace_{0}^{N-1}) + V(x_N) \\\\\ns.t. & \\\\\nx_{k+1} = f (x_k, u_k) \\quad \\forall k \\in [0, ..., N - 1], \\\\\nx_k &\\in \\mathcal{X} \\quad \\forall k \\in [0,..., M], \\\\\nx_k &\\in \\mathcal{X} \\ominus B_\\epsilon \\quad \\forall k \\in [M+1, ..., N], \\\\\nu_k &\\in \\mathcal{U}_k \\quad \\forall k \\in [0,...,N-1], \\\\\nx_0 &= x_{init}.\n\\end{aligned}$$\nHere, $$B_\\epsilon$$ denotes the ball of size $$\\epsilon$$ for the infinity norm, and $$ \\ominus $$ is a Pontryagin set difference operator. It is easy to see, that if Assumption 1 is satisfied for some $$\\gamma > 0$$, then generating training data with features $$\\Phi_\\epsilon(\\cdot)$$ and labels $$\\Phi_\\gamma(\\cdot)|_{x_{M+1},...,x_N}$$, which yield the predictions $$x \\in \\mathcal{X}$$. This, in turn, implies that the Neural Horizon MPC formulation (7) with $$\\mathcal{X}_k = \\mathcal{X}_k = \\mathcal{X}$$ for every $$k \\in [0, ..., N]$$ will remain feasible for every admissible initial state of $$\\Phi(\\cdot)$$.\nFurthermore, the formulation (7) with a Neural Horizon constructed from such a $$\\Phi_\\gamma(\\cdot)$$, can be viewed as a simplified case of the MPC formulated in [22]. If Assumption 1 holds, the error bound over the Neural Horizon is constant and independent of the state, which can be used directly, without the construction of a tube-based MPC over the second horizon. Besides, as we do not need a projection at M in (7) to interface between the horizons, we satisfy all assumptions"}, {"title": "IV. SIMULATION RESULTS", "content": "We simulate the upswing of the pendulum with all con- trollers for 5s of simulation time. The comparison between the Short Horizon MPC, Baseline MPC, and Neural Horizon MPC (5 trained NNs) with explicit state bounds are shown in Fig. 3. Here, the Baseline MPC and all Neural Horizon MPCs manage to reach the reference and stabilize the system. The Short Horizon MPC fails to achieve the goal and becomes infeasible at 1.38 s of simulation time. The solver wall time of the Neural Horizon MPCs is lower than one of the Baseline MPC at every iteration, and is lower than the solver wall time of the Short Horizon on average (cf. also TABLE IV).\nA comparison between all approximation-based con- trollers is shown in Fig. 4. Here, the Cost-Estimation MPCs cannot stabilize the system and in some cases become infeasible (for 6 out of 10 trained NNs). The Neural Horizon MPCs without state bounds are slightly faster (cf. TABLE IV), but produce identical trajectories to the version with explicit state bounds. The Imitation Controllers have also stabilized the reference, though most of them have violated input constraints. Given that these controllers are static mappings, their computation time was also much lower than that of other considered controllers."}, {"title": "V. PERFORMANCE DISCUSSION", "content": "As illustrated in Fig. 2, the controller performance falls sharply for a horizon length lower than 30. However, we showed that the Neural Horizon recovers the missing infor- mation and can approximate it well enough to reduce the first horizon down to 8 steps. Despite the fact that mathematically the Cost-Estimation MPC version is very similar to the Neural Horizon MPC without explicit state bounds, we observe a much worse performance even for comparably trained neural networks (cf. TABLE III). A possible reason is the complex relation between the state $$x_M$$ and the remaining cost terms, that the Neural Horizon formulation regularizes by aggregating separate state predictions. However, a follow- up investigation is required."}, {"title": "VI. CONCLUSION AND OUTLOOK", "content": "We have demonstrated a method to successfully reduce the horizon length of the MPC algorithm by employing neural networks to construct an approximation of the horizon tail. We have further shown how these networks can be utilized inside the optimization problems, despite their adverse gradi- ent properties. Though in this work we have concentrated on the most direct comparison between the Baseline MPC and the proposed Neural Horizon MPC, there are steps to make this approach even more practical. As the training data is generated offline, we can use a controller with a much longer time horizon, in essence approximating an infinite-horizon MPC formulation. On the other hand, we can use any source of control actions for training, e.g. emulating a human driver to derive realistic control actions while retaining constraint satisfaction from the first horizon of the MPC.\nThough our simulation results indicate that explicit con- straints in the Neural Horizon are not required for maintain- ing close-to-optimal performance, further research is needed to confidently state that. A further investigation of the poor performance of the Cost-Estimation MPC is also needed to determine possible conditions for this approach to perform on par with the Neural Horizon variant."}]}