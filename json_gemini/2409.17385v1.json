{"title": "Data-efficient Trajectory Prediction via Coreset Selection", "authors": ["Ruining Yang", "Lili Su"], "abstract": "Modern vehicles are equipped with multiple information-collection devices such as sensors and cameras, continuously generating a large volume of raw data. Accurately predicting the trajectories of neighboring vehicles is a vital component in understanding the complex driving environment. Yet, training trajectory prediction models is challenging in two ways. Processing the large-scale data is computation-intensive. Moreover, easy-medium driving scenarios often overwhelmingly dominate the dataset, leaving challenging driving scenarios such as dense traffic under-represented. For example, in the Argoverse motion prediction dataset, there are very few instances with \u2265 50 agents, while scenarios with 10~20 agents are far more common. In this paper, to mitigate data redundancy in the over-represented driving scenarios and to reduce the bias rooted in the data scarcity of complex ones, we propose a novel data-efficient training method based on coreset selection. This method strategically selects a small but representative subset of data while balancing the proportions of different scenario difficulties. To the best of our knowledge, we are the first to introduce a method capable of effectively condensing large-scale trajectory dataset, while achieving a state-of-the-art compression ratio. Notably, even when using only 50% of the Argoverse dataset, the model can be trained with little to no decline in performance. Moreover, the selected coreset maintains excellent generalization ability.", "sections": [{"title": "I. INTRODUCTION", "content": "Reliably understanding the driving environment paves the way for the downstream safe navigation of autonomous vehicles. Modern autonomous vehicles are equipped with multiple information-collection devices such as sensors and cameras, continuously generating a large volume of raw data. Leveraging these large-scale datasets, significant attention has been devoted to predicting the trajectories of neighboring vehicles, with a focus on designing complex neural networks to capture both agent-agent and agent-infrastructure interac-tions [1], [2], [3], [4], [5], [6].\nThe practice of training trajectory prediction models is challenging in two ways. First, although known to be data-hungry, training complex deep neural networks on a large dataset consumes forbiddenly high computational resources. For example, training a SceneTransformer [7] model with 15.3 million parameters using a dataset containing over 0.2 million trajectory samples consumes thousands of GPU hours. This dilemma raises the following pivotal question:\nQ1: How can we identify and maintain the most critical data without significant accuracy degradation?\nThe second challenge lies in the fact that standard machine learning tasks often use the weighted average accuracy of all"}, {"title": "II. RELATED WORK", "content": "Early approaches [27], [28], [29] to trajectory prediction focused primarily on modeling the future path of a single agent without considering interactions with other agents in the environment. Recent trajectory prediction methods [30], [31] shifted to vectorization-based techniques to capture road geometry and contextual information more efficiently. Transformer-based models emerged as a powerful approach to capture complex multi-agent interactions in trajectory prediction [32], [33], [34]. mmTransformer [35] and scene-Transformer [7] fused information over time by combining road elements, agent interactions, and time steps through an attention mechanism. HiVT [1] extended these Transformer-based methods by leveraging multimodal attention to capture complex interactions between multiple agents, extracting local context while modeling global interactions for more accurate predictions. QCNet [36] introduced a query-centric approach that reused past map information to achieve faster inference by minimizing redundant processing of static fea-tures. The latest model, HPNet [37], contained a historical prediction attention module that improved predictions by considering correlations between consecutive time steps, thereby enhancing temporal consistency and accuracy."}, {"title": "B. Training Data Selecting", "content": "Training complex neural networks on large-scale datasets consumed extremely high computation resources. To address this, various data-efficient approaches were explored, includ-ing optimization algorithms and dataset distillation. Model optimization algorithms enhanced training efficiency through frequent parameter updates [38], reduced iterations [39], and adaptive learning rates [40], [41]. While optimization algo-rithms aimed to accelerate convergence speed and improve performance with limited data, data distillation methods [18] focused on directly reducing the size of the dataset. These methods aimed to distill the knowledge of a dataset into a smaller synthetic dataset while preserving the performance of the models trained on it.\nData distillation methods (e.g., the method proposed by [18]) introduced a bi-level learning framework to ensure that models trained on distilled samples perform well on real samples. Later methods, such as gradient matching [19], learn synthetic datasets by minimizing the difference between gradients computed from real and synthetic datasets. [42] proposed matching model training trajectories on real and synthetic data to capture long-term behavior. However, data distillation methods are computationally expensive, as they require synthesizing new data and repeatedly optimizing complex objectives [19]. In contrast, coreset selection has"}, {"title": "III. PROBLEM FORMULATION", "content": "The training dataset 9 = {$S$}$_{i=1}^{n}$ consists of n driving scenarios/scenes. Each driving scenario (i.e. each sample) is described by a triple S = (X,Y,M), where X and Y are the collections of observed and future trajectories of neighboring agents (an agent can be a vehicle or a pedestrian), and M is the map. Let m denote the number of agents in the scenario, then X and Y can be expressed as X = {$x_1$,...,$x_m$} and Y = {$y_1$,...,$y_m$}, where $x_i$ \u2208 $R^{2\u00d7T_{obs}}$ and $y_i$ \u2208 $R^{2\u00d7T_{pre}}$ are the two-dimensional observed and future trajectory coordinates of agent i, with lengths $T_{obs}$ and $T_{pre}$, respectively.\nWe quantify the difficulty of a scenario S by its traffic density (i.e., the number of agents involved), denoted as $m_j$. Let $m^*$ = max$_{j\u2208[n]}$$m_\u00cc$. Let L = {$l_1$,$l_2$,\u2026,I$_{max}$$} denote the user-specific difficulty fidelity. For example, one can set l\u2081 = {1,\u2026,10}, l\u2082 = {11,\u2026,20}, \u2026, I$_{max}$ = {$m^*$ \u2013 ($m^*$ mod 10)+1,\u2026\u2026,$m^*$}$^1$. In this example, class l\u2081 is the group of difficulties 1 ~ 10, and the same applies to other classes. With a bit of abuse of notation, let\n$L_k$ := {$S_\u0130 \u2286 D$ : $m_\u00b2$ \u2208 $l_\u2081$} for k = 1,\u2026,max,\nbe a partition of 9 for the given data difficulty fidelity L.\nOur goal is to efficiently and meticulously pinpoint a subset of data C C D such that (1) a model trained on C performs comparably to one trained on the entire dataset 9, and (2) the proportion of data in & across different difficulty levels L is as close to uniform as possible."}, {"title": "IV. METHOD", "content": "Unlike image data whose dimension is fixed in a dataset, the dimensions of trajectory prediction data vary depending on the number of agents in the scene. As shown in Fig.1, the dimension of a scene with 80 agents is eight times that of a scene with 10 agents. In addition, a given agent may be present or absent at different time frames. This is because that trajectory prediction data is temporally continuous, with agents moving at different speeds, and that the sensing range of information-collection devices (such as LiDAR or cameras) is limited. We address these challenges by preprocessing the trajectory data in two steps.\nFiltering: For each scene, we determine its difficulty m by ranking all involved agents in descending order based on their frequency of appearance in the scene duration, and removing agents that appear infrequently.\nCategorization: We partition the dataset into subsets based on user-specified difficulty fidelity L, and obtain $L_1$, $L_2$,\u2026,$L_{max}$."}, {"title": "B. Coreset Selection", "content": "We use HiVT-64 as the backbone model in assisting the coreset construction and employ a corresponding submodular function P to evaluate the contribution of each sample. A greedy algorithm is then utilized to construct the coreset, ensuring that the selected subset best represents the charac-teristics of the entire dataset. Notably, the identified coreset is not limited to the HiVT-64 model but can be effectively used to train other trajectory prediction models, as demonstrated in our experiments (see Section V, Table I). We propose two coreset constructions, which we refer to as fixed selection and balanced selection, respectively. Since they share a common structure, we describe them together in Algorithm 1. In line 7 of Algorithm 1, \u2206($S_\u0130$ | 6) := P(6\u222a {$S_j$}) \u2013 P(6) is the submodular gain of adding S to the current coreset C.\n1) Fixed Selection: This method uses a fixed sample ratio of data to include in the coreset. Let \u03b1 represent the fixed ratio (e.g., \u03b1 = 0.1 for 10%). The number of samples selected from each difficulty level $l_k$ \u2208 L is determined by $n_k$ = \u03b1 \u00d7 $\\left|L_k\\right|$, where $\\left|L_k\\right|$ is the total number of samples in $l_k$. To select the most important samples to iteratively build 6, we utilize the following submodular function\nP($S_i$) = $\u03a3_{S' i \u2208 b}$  $\\left| f(S') - f(s_i) \\right|^2  $\u2013 $\u03a3_{S'i \u2208 D \\backslash C}$  $\\left|f(S') - f(S_i) \\right|^2$,                                                                                    (1)\nwhere f(.) represents the objective function of HiVT-64, composed of Laplace Negative Log-Likelihood Loss and Soft Target Cross-Entropy Loss, C is the set of already selected samples, $S_i$ is the sample currently being considered for inclusion in the coreset, and $S_i$ represents the samples already in the coreset. The first term, $\u03a3_{s_i\u22086}$  $\\left|f(S_i) \u2212 f(S_i)\\right|^2 $, measures the similarity between the candidate sample $S_i$ and the samples S' in 6. This term assesses how well Si aligns"}, {"title": "V. EXPERIMENTS AND EVALUATIONS", "content": "We use the large-scale Argoverse Motion Forecasting Dataset 1.1 [8] as the core data source. This dataset is specif-ically designed for trajectory prediction tasks in autonomous driving and includes 323,557 real-world scenarios, covering the motion trajectories of various traffic participants and providing high-definition map information. The dataset is divided into a training set (205,942 samples), a validation set (39,472 samples), and a test set (78,143 samples). The test set only provides the first 2 seconds of trajectory data, requiring prediction for the subsequent 3 seconds. The map data in-cludes lane geometry, direction, connectivity, and traffic light locations, which helps the model understand the dynamic interactions between the environment and participants.\nWe use the standard evaluation metrics: minADE, minFDE, and MR to assess model performance, where mi-nADE measures the average distance between the predicted and ground truth trajectories, minFDE evaluates the devia-tion between the predicted and ground truth endpoints, and MR calculates the proportion of endpoints with a deviation exceeding 2 meters."}, {"title": "B. Implementation Details", "content": "To identify the most impactful coreset of Argoverse 1 [8], we used HiVT-64 [1] as the backbone"}, {"title": "VI. CONCLUSION", "content": "In this paper, we address two major challenges in tra-jectory prediction for autonomous vehicles: the need for data-efficient training and reducing bias due to imbalanced data distribution in different driving scenarios. To address these challenges, we propose a novel data-efficient training method based on coreset selection. Our method focuses on selecting a small but representative subset of trajectory data that contributes significantly to model performance while balancing the proportion of data with different difficulty levels. The fixed coreset selection method can effectively find data that contributes significantly to the model, thereby significantly reducing the amount of training data while maintaining model performance. The balanced coreset se-lection method specifically addresses the data imbalance problem by prioritizing more challenging scenarios with higher agent density, thereby achieving a more uniform data distribution across different scenario categories. At the same time, our selected coreset shows good generalization ability to other models."}]}