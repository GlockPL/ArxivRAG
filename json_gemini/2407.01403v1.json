{"title": "Optimization of Retrieval-Augmented Generation Context with Outlier Detection", "authors": ["Vitaly Bulgakov"], "abstract": "In this paper, we focus on methods to reduce the size and improve the quality of the prompt context required for question-answering systems. Attempts to increase the number of retrieved chunked documents and thereby enlarge the context related to the query can significantly complicate the processing and decrease the performance of a Large Language Model (LLM) when generating responses to queries. It is well known that a large set of documents retrieved from a database in response to a query may contain irrelevant information, which often leads to hallucinations in the resulting answers. Our goal is to select the most semantically relevant documents, treating the discarded ones as outliers. We propose and evaluate several methods for identifying outliers by creating features that utilize the distances of embedding vectors, retrieved from the vector database, to both the centroid and the query vectors. The methods were evaluated by comparing the similarities of the retrieved LLM responses to ground-truth answers obtained using the OpenAI GPT-40 model. It was found that the greatest improvements were achieved with increasing complexity of the questions and answers.", "sections": [{"title": "1. Introduction", "content": "Improving context retrieval in question-answering systems has garnered considerable attention over the past few years. For instance, [1] presents a comprehensive survey on the Retrieval-Augmented Generation (RAG) framework, detailing the involved retrievers and generators. It explores various enhancement methods for RAG, evaluates benchmark frameworks, and addresses current limitations and future directions, providing a solid foundation for understanding RAG techniques and their applications. In another study, [2] introduces xRAG, a method for extreme context compression that reinterprets document embeddings used in dense retrieval as features for language models. This approach aims to maintain the semantic coherence of retrieved documents while minimizing memory overhead and computational expense. xRAG employs techniques such\nas self-distillation and instruction tuning to enhance the use of contextual information. Also, [3]\npresents a novel in-context retrieval approach for RAG systems that avoids the traditional chunk-\ning process. It uses encoded hidden states of documents for retrieval, improving the fidelity and\naccuracy of the evidence text used for generating responses. This method ensures that relevant\nand precise context is maintained without disrupting the semantic coherence of the documents. [4]\ndiscusses the concept of re-ranking, where a re-ranking agent re-orders retrieved documents based\non additional factors such as user behavior, document popularity, or deeper semantic analysis, en-\nsuring the most useful and relevant documents appear at the top of search results. In [5] we explore\nthe benefits of reducing vector database dimensions with Fast Fourier Transform, with a focus on\ncomputational efficiency in processing context documents.\nIn the described approach, we propose a mechanism to improve the quality and reduce the size\nof the context based on retrieved documents for a given query. This mechanism identifies outliers\nthat are far from or irrelevant to the query and focuses on the portion of the context most related\nto the query. Distances are calculated for each retrieved document, represented as an embedding\nvector by a sentence transformer model. We utilize distances to the vectors' centroid and the query\nvector. These distances form features based on the following methods: concatenation, weighted\nsum, interaction, and polynomial, which will be described later. The constructed features provide a\nrich representation of the distances between embedding vectors, the centroid, and the query vector,\ncapturing various aspects of the relationship between the vectors and helping distinguish typical\nvectors from outliers. Outliers are determined with help of a Gaussian Mixture Model (GMM) and\nLog-Likelihood approach with selected percentile, [6] and [7].\nWe experimented with three categories of questions of increasing complexity and demonstrated\nthat the approach provides the most significant advantage with the most complex questions."}, {"title": "2. Theory and Methods", "content": "Our approach aims to identify outliers among a set of embedding vectors used in a context for\nRetrieval-Augmented Generation (RAG). The outliers are detected based on distances from a query\nvector and a centroid vector. Below is a detailed explanation of the methods implemented, along\nwith the corresponding formulas.\nDistance Calculation For each embedding vector in the set, the distances to a given query vector\nand a centroid vector are calculated:\nDistance to Centroid : \\(d_{centroid} = ||V_i - c||\\) \nDistance to Query Vector : \\(d_{query} = ||V_i - q||\\)\nWhere:\n\u2022\nvi is the i-th embedding vector,\n\u2022 c is the centroid vector,\n\u2022\nq is the query vector,"}, {"title": null, "content": "\u2022 || || denotes the Euclidean norm.\nWe use a weighting factor 0 \u2264 a \u2264 1 to balance these distances:\ndistance to centroid (weighted) = \\(d_{centroid} \u00d7 (1 \u2212 a)\\)\ndistance to query (weighted) = \\(d_{query} \u00d7 a\\)\nFeature Creation Methods We used multiple methods to combine these distances into features\n(see, e.g. [8] - [9]) for further analysis:\nConcatenate : \\(f_i = [d_{centroid}, d_{query}]\\) \nWeighted Sum : \\(f_i = ad_{query} + (1 \u2212 a)d_{centroid}\\)\nInteraction: \\(f_i =  [\\frac{d_{centroid}}{d_{centroid} + \\epsilon}, d_{query}, d_{centroid} \u00d7 d_{query}, \\frac{d_{centroid}}{d_{query} + \\epsilon}]\\)\nWhere epsilon is a small value to avoid division by zero.\nPolynomial : \\(f_i = PolynomialFeatures (d_{centroid}, d_{query})\\)\nThis expands the distance features to include polynomial combinations up to the specified degree.\nStandardization Standardization is applied to the feature vectors to ensure they have zero mean\nand unit variance:\n\\(f_{normalized} = \\frac{f_i - \\mu}{\\sigma}\\)\nWhere \u03bc is the mean and o is the standard deviation of the features.\nDimensionality Reduction If the number of features is more than two, we apply Principal Com-\nponent Analysis (PCA) to reduce the features to two dimensions for visualization. We also use\nvarious PCA dimensions for outliers detection which will be explained further:\n\\(f_{PCA} = PCA(f_{normalized})\\)\nOutlier Detection Outliers are identified based on a log-likelihood threshold. Points with a log-\nlikelihood below the threshold are considered outliers.\n1. Fit a Gaussian Mixture Model (GMM): The feature vectors are modeled using a GMM,\nwhich assumes that the data is generated from a mixture of several Gaussian distributions.\nThe GMM is parameterized by the mean vectors \u03bc\u03ba, covariance matrices \u03a3\u03ba, and mixture\nweights \u03c0\u03ba for each component k."}, {"title": null, "content": "2. Calculate Log-Likelihood: For each feature vector fi, compute the log-likelihood under the\nGMM:\n\\(log p(f_i) = log (\\sum_{k=1}^{K} \\pi_k N(f_i|\\mu_k, \\Sigma_k))\\)\nwhere N(fi|\u03bc\u03ba, \u03a3\u03ba) is the probability density function of the Gaussian distribution with\nmean \u03bc\u03b5 and covariance \u03a3\u03ba.\n3. Determine Outliers: Identify the outliers as those points whose log-likelihood is below a\ncertain threshold. This threshold can be defined as a certain percentile of the log-likelihood\nvalues. For example, consider the bottom 10\nthreshold = percentile({logp(fi)}=1,10)\nwhere N is the total number of feature vectors.\n4. Outlier Identification: Points with log-likelihood values less than the threshold are marked\nas outliers:\noutliers = {f; | log p(fi) < threshold}\nGMM provides a probability distribution over clusters for each data point. This means each\npoint can belong to multiple clusters with different probabilities. The choice of K (number\nof components) is crucial. It can be selected based on prior knowledge, or determined using\nmodel selection criteria like the Bayesian Information Criterion (BIC) or the Akaike Infor-\nmation Criterion (AIC), which balance model fit and complexity. Another approach would\nbe by varying a combinations of GMM components and PCA dimensions one can control\nand optimize outliers selection."}, {"title": "3. Numerical study", "content": "The purpose of the numerical study was to determine if better responses could be obtained for a\nquery using a filtered context (excluding outlier documents) compared to using the context based\nsolely on a set of documents retrieved from the retrieval language model. Our base text-to-text\nmodel was 'TinyLlama/TinyLlama-1.1B-Chat-v1.0,' which, although rather small, proved to be a\ngood choice for research and allowed us to conduct extensive experiments on a regular laptop. We\nalso tested the proposed method with the 'mistralai/Mistral-7B-Instruct-v0.2' model and obtained\nvery similar results but at a much higher cost.\nAs a document retriever, we used a popular sentence transformer model, \u2018sentence-transformers/all-\nmpnet-base-v2,' which can be used as a text to dense vectors converter. With this model we con-\nverted all chunked documents and stored them in a FAISS vector database developed by Facebook\nResearch [10]. All models we used are publicly available in Hugging Face hub [11].\nFor testing, we used various datasets. The results of this study are obtained with SQuAD2.0 - The\nStanford Question Answering Dataset, which includes a mixture of 35 topics covering political\naspects, geography, mathematics, chemistry, religion, etc., ranging from \u201cAmazon Rainforest\u201d to\n\"Yuan Dynasty,\u201d from \u201cPharmacy\u201d to \u201cPrime Numbers.\u201d To generate questions and corresponding"}, {"title": null, "content": "answers for a benchmark, we leveraged the extensive knowledge and capabilities of the OpenAI\nGPT-40 model with the uploaded dataset text. The following three categories of questions and\ncorresponding answers were generated:\n\"Simple\"\nQuestions that allow for a short simple answer, such as:\n\u2022 When did the 1973 oil crisis begin?\n\u2022 What did Nixon request from Congress on October 19, 1973?\n\"Broader\"\nQuestions that require to provide detail and insight, such as:\n\u2022 What allowed the tropical rainforest to spread out across the continent after the Creta-\nceous-Paleogene extinction event?\n\u2022 How did the construction of highways in the Amazon lead to deforestation?\n\"Double\"\nTwo questions of \u201cbroader\u201d type combined in one, such as:\n\u2022 What caused the recurrence of plague outbreaks in Europe until the 19th century and how\ndid the rise of the Andes Mountains create the Solim\u00f5es Basin?\n\u2022 What led to the creation of the National Energy Act of 1978 and what caused the Amazon\nrainforest to be considered unsustainable by 2100?\nWhen splitting text documents into chunks we tried to select a max chunk size so that the\nnumber of tokens from 20 \u2013 25 chunks, considered as documents, does not exceed the sequence\nlength of the model, which in case of TinyLlama-1.1B-Chat-v1.0 model is 2048.\nIn Outlier Detection section we described the method of selecting outliers from a given set\nof feature vectors. This selection depends on the number of GMM components (clusters). For\nexample in case of \"interaction\u201d method the size of the feature vector fi is 4. This is because the\nfeature vector is composed of four components:\n1. \\(d_{centroid}\\)\n2. \\(d_{query}\\)\n3. \\(d_{centroid} \u00d7 d_{query}\\)\n4. \\(\\frac{d_{centroid}}{d_{query} + \\epsilon}\\)"}, {"title": null, "content": "Each of these components represents a different feature derived from the distances \\(d_{centroid}\\) and\n\\(d_{query}\\). Then, by projecting these 4 features to principal components using PCA and selecting 2, 3,\nor 4 components, another leverage for controlling the clustering mechanism is introduced. We will\nuse the following notation in selecting outliers:\n\u2022 Number of clusters: [4, 5, 6]\n\u2022 PCA dimensions: [2, 3]\nBy running the double loop over the number of clusters and PCA dimensions we will be getting\ndifferent sets of outlier documents and some of them will be common. The final set of outliers\nwill be those that are common or those that that are common at least at certain given number of\noccurrences. We will call it \"min outlier frequency\u201d denoted by \"freq\".\nWe will use the following experiment setup:\n\u2022 Text-to-text model: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\nMax Sequence Length: 2048\n\u2022 Retrieval Model: \u2018sentence-transformers/all-mpnet-base-v2'\n\u2022\nmax_seq_length: 384\nsentence_embedding_dimension: 768\nmax_new_tokens: 1500\n\u2022 Number of Docs: 20\n\u2022 Threshold Percentile: 15%\n\u2022 Number of clusters: [4, 5, 6]\n\u2022 PCA dimensions: [2, 3]\nWe use the following simple template to form a query prompt to the model that includes a short\ninstruction, context and the query itself:\nformatted_prompt\n=\nf'''\nYou are a friendly chatbot who responds to the user's question by\nlooking into context.</s>\nContext:\n{context}\n</s>\nQuestion: {question}</s>\n'''"}, {"title": null, "content": "Context consists of documents retrieved on a query from the vector database. We call it a \"fil-\ntered prompt\" when all outliers are removed from the context. The response to a filtered prompt is\ncompared with the ground-truth response. The same is done with the \"original prompt,\u201d where the\ncontext consists of the same number of first documents retrieved from the database. For compari-\nson, we use two types of similarity metrics: cosine (dense vector) similarity and TFIDF similarity\nbased on tokens. Our purpose is to determine the improvement received when using \"filtered\nprompts\" over \"original prompts\u201d with respect to these similarities. For query Qi, let Rg; be the\nground truth response, Rf\u00bf be the filtered response, and Ro\u00bf be the original response. The improve-\nment in similarity can be expressed as the difference in similarities of Rf; to Rg, and Ro; to Rg\ndivided by the similarity of Roz to Rgi:\nImprovement; =\n\\( \\frac{Similarity (Rf_i, Rg_i) \u2013 Similarity (Ro_i, Rg_i)}{Similarity (Ro_i, Rg_i)}\\) \nWhere:\n\u2022 Similarity(Rf\u00bf, Rg\u2081) is the similarity between the filtered response and the ground truth re-\nsponse.\n\u2022 Similarity (Ro\u2082, Rg\u2081) is the similarity between the original response and the ground truth\nresponse.\nThe average improvement over N questions can be calculated as follows:\nAverage Improvement =\n\\( \\frac{1}{N}\\sum_{i=1}^{N} Improvement_i\\)\nWhere:\n\u2022 N is the total number of questions.\n\u2022 Improvement; is the improvement for question Qi."}, {"title": "4. Conclusion", "content": "In conclusion, this study demonstrates the efficacy of filtering outlier documents to improve the\nquality of responses in question-answering systems using retrieval-augmented generation (RAG).\nBy removing documents that are semantically irrelevant or far from the query, we have shown that\nthe context can be significantly refined, leading to more accurate and relevant answers. Besides, a\nreduced context length is beneficial it terms of computational cost required to produce the response\nby the model.\nOur experiments with different text-to-text models, including 'TinyLlama/TinyLlama-1.1B-\nChat-v1.0' and 'mistralai/Mistral-7B-Instruct-v0.2,' revealed that even smaller models benefit from\nthis approach, making it feasible to conduct extensive research on standard hardware. The use of\na sentence transformer model, 'sentence-transformers/all-mpnet-base-v2,' for document retrieval\nfurther reinforced the robustness of our method across diverse datasets, including the SQuAD2.0\ndataset.\nThe key observations from our experiments indicated that: 1. The \"interaction\u201d method pro-\nvided the most significant improvement in similarity metrics. 2. Higher \"min outlier frequency\"\nled to better similarity improvements, albeit at the cost of increased computational demands. 3.\nThe balance parameter a played a certain role in enhancing the feature representation for out-\nlier detection. 4. The complexity of questions had a profound impact on the effectiveness of the\nfiltering mechanism, with more complex queries benefiting the most from the refined context.\nOverall, this study highlights the potential for enhanced context retrieval methods to address\nchallenges in RAG systems, particularly when dealing with complex queries. Future work will\nfocus on further optimizing the outlier detection mechanisms and exploring their applicability to\nother datasets and use cases."}]}