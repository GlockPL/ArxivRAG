{"title": "DynSegNet:Dynamic Architecture Adjustment for Adversarial Learning\nin Segmenting Hemorrhagic Lesions from Fundus Images", "authors": ["Zesheng Li", "Minwen Liao", "Haoran Chen", "Yan Su", "Chengchang Pan", "Honggang Qi"], "abstract": "The hemorrhagic lesion segmentation\nplays a critical role in ophthalmic\ndiagnosis, directly influencing early\ndisease detection, treatment planning, and\ntherapeutic efficacy evaluation. However,\nthe task faces significant challenges due to\nlesion morphological variability, indistinct\nboundaries, and low contrast with\nbackground tissues. To improve\ndiagnostic accuracy and treatment\noutcomes, developing advanced\nsegmentation techniques remains\nimperative. This paper proposes an\nadversarial learning-based dynamic\narchitecture adjustment approach that\nintegrates hierarchical U-shaped encoder-\ndecoder, residual blocks, attention\nmechanisms, and ASPP modules. By\ndynamically optimizing feature fusion,\nour method enhances segmentation\nperformance. Experimental results\ndemonstrate a Dice coefficient of 0.6802,\nIoU of 0.5602, Recall of 0.766, Precision\nof 0.6525, and Accuracy of 0.9955,\neffectively addressing the challenges in\nfundus image hemorrhage segmentation.", "sections": [{"title": "Introduction", "content": "Fundus hemorrhage serves as a critical\npathological marker for ocular diseases such as\ndiabetic retinopathy, where precise segmentation\nholds vital clinical significance for early\ndiagnosis and treatment planning. By accurately\nidentifying the location and extent of\nhemorrhagic lesions, clinicians can develop more\neffective therapeutic strategies, thereby\nimproving patient outcomes and prognosis.\nHowever, despite significant advancements in\nmedical image segmentation, traditional methods\nlike Fully Convolutional Networks (FCN) and\nDeepLabV3+ still face formidable challenges in\nprocessing complex hemorrhagic lesions in\nfundus images.\nAtlas et al. (2017) demonstrated that traditional\nthreshold-based methods (e.g., ANFIS combined\nwith particle swarm optimization) could partially\nsegment regular lesions but exhibited\nsignificantly reduced sensitivity for polymorphic,\nlow-contrast hemorrhagic regions. These lesions\nare characterized by morphological diversity,\nblurred boundaries, and low contrast with\nsurrounding tissues. Wang S. et al. (2019) found\nthat boundary ambiguity could reduce\nsegmentation Dice coefficients by up to 12.7%,\nrendering accurate segmentation exceptionally\nchallenging. Furthermore, the substantial\nvariability in lesion size and morphology\nnecessitates effective fusion of multi-scale\nfeatures to comprehensively capture pathological\ninformation. For instance, Khojasteh P. et al.\n(2018) demonstrated that a ten-layer CNN\narchitecture with probabilistic multi-scale feature\nfusion significantly improved hemorrhage\ndetection rates, though its computational\ncomplexity hindered clinical adoption.\nTo address these challenges, this study\nproposes a novel fundus hemorrhage\nsegmentation framework integrating adversarial\nlearning with dynamic architectural adaptation.\nThe framework leverages a generator-\ndiscriminator interaction mechanism: the\ngenerator performs feature extraction and\npreliminary segmentation, while the discriminator\nevaluates and refines the segmentation outputs.\nThe generator employs a hierarchical U-shaped"}, {"title": "Related Work", "content": "The application of deep learning in medical\nimage analysis has become extremely widespread.\nWith continuous technological advancements and\nalgorithm optimization, deep learning techniques\nhave demonstrated increasingly significant roles\nin medical image recognition, classification,\nsegmentation, and disease diagnosis, leveraging\ntheir powerful data processing and pattern\nrecognition capabilities. A prime example is the\nU-Net segmentation network, an innovative\narchitecture proposed by Ronneberger et al.\n(2015). By constructing a U-shaped network\nstructure, it skillfully integrates semantic and\npositional information. The encoder-decoder\narchitecture, combined with skip connections,\nenables effective concatenation of features from\nthe downsampling process with those in the\ndecoder phase, thereby preserving finer details in\npixel-level classification tasks. This design has\nmade U-Net particularly outstanding in medical\nimage segmentation, especially in retinal vessel\nsegmentation.\nBuilding upon this foundation, researchers\nhave conducted numerous valuable explorations\nbased on CNNs and U-Net. For instance, Uysal et\nal. (2020) implemented a fully convolutional\nneural network to segment vessels from grayscale\nfundus images. Gu et al. (2019) proposed a\nContextual Encoder Network (CENET),\nachieving high-precision 2D vessel segmentation\nby constraining complex high-level information\nwhile retaining spatial details. Hu et al. (2018)\ncombined CNNS with fully connected\nConditional Random Fields (CRFs) to accurately\nsegment vessels from color fundus images,\nemploying a multi-scale CNN architecture and an\nenhanced cross-entropy loss function to drive\nrobust training. Additionally, Shin et al. (2019)\ninnovatively bridged CNN architectures with\nGraph Neural Networks (GNNs), proposing a\nVascular Graph Network (VGN), which opened\nnew avenues for retinal vessel segmentation."}, {"title": "GANs and Their Variants", "content": "In recent years, Generative Adversarial Networks\n(GANs) have sparked a surge of interest in\nmedical image analysis due to their unique\nadversarial mechanism. Emami et al. (2018)\nleveraged GANs to synthesize CT images from\nmagnetic resonance imaging (MRI),\ndemonstrating their immense potential for cross-\nmodal image generation. In the field of medical\nimage segmentation, GANs and their variants\nhave become a research hotspot through their\nintegration with U-Net architectures.\nBy leveraging the adversarial interplay\nbetween generators and discriminators, GANS\nexcel at producing high-quality segmentation\nresults, particularly in few-shot learning scenarios\nand data augmentation. For instance, Wu et al.\n(2019) proposed U-GAN, which ingeniously\nintegrated U-Net with attention mechanisms to\nenhance retinal vessel segmentation accuracy and\nnoise robustness through adversarial training.\nMeanwhile, Xue et al. (2017) introduced SegAN,\nemploying multi-scale L1 loss functions to\nimprove the stability and precision of GANs in\nmedical image segmentation, outperforming\ntraditional U-Net methods.\nResearchers have further explored GANs for\nspecialized segmentation tasks. Guo et al. (2020)\nachieved remarkable accuracy in pectoralis major\nsegmentation from mammograms by decoupling\nboundary detection and shape prediction,\nparticularly excelling in handling ambiguous\nboundaries. Fan et al. (2023) developed U-Patch\nGAN, combining U-Net with PatchGAN to fuse\nmultimodal brain images via self-supervised\nlearning. This approach not only enhanced fusion"}, {"title": "Methodology", "content": "This study proposes a segmentation framework\nbased on adversarial learning and dynamic\narchitectural adaptation (as illustrated in Figure 2).\nThe framework comprises three core components:\na Generator network, a Discriminator network,"}, {"title": "Generator Design", "content": "The generator, as the core component of the\nsegmentation framework, achieves precise\nlocalization and feature representation of lesion\nregions through multi-module collaboration. The\noverall architecture employs an improved\nHierarchical U-shaped encoder-decoder as the\nbackbone, integrates residual modules to enhance\ngradient propagation, incorporates a spatial\nattention mechanism to focus on critical areas,\nand introduces an Atrous Spatial Pyramid Pooling\n(ASPP) module to capture multi-scale contextual\ninformation. Through synergistic interactions\namong these modules, the framework effectively\naddresses challenges in medical segmentation\nsuch as diverse lesion morphology and blurred\nboundaries."}, {"title": "Attention Mechanism", "content": "The attention mechanism is a technique that\nmimics the selective perception of human vision.\nIts core concept lies in automatically focusing on\nregions most critical to the current task (e.g.,\nimage segmentation) by calculating the\nimportance weights of different regions or\nfeatures, while suppressing the influence of\nirrelevant or less relevant areas.\nSpecifically, the mechanism achieves this by\nintroducing attention coefficients $a_i$. These\ncoefficients, ranging between 0 and 1, quantify\nthe relative importance of different\nregions/features for the task. When $a_i$approaches\n1, it indicates high relevance of the corresponding\nregion, prompting the model to allocate stronger\nattention; conversely, values near 0 suggest low\nimportance, leading to attenuated focus.\nThe output of the attention mechanism is a\nweighted feature representation that intensifies\nfocus on task-critical regions/features, thereby\nenhancing the model's performance and accuracy.\nMathematically, this operation can be formulated\nas the element-wise multiplication between the\ninput feature map and attention coefficients:\n$X_{ic} = {\\{x^l_i a_i\\}}_{i=1}^{n}$\nWhere, $x = {\\{x^l_c\\}}_{i=1}^{n}$ is the feature map for\npixel i in layer l and class c.The specific\nimplementation of the attention gate architecture\nreferenced here is illustrated in Figure 2."}, {"title": "Atrous Spatial Pyramid Pooling", "content": "Atrous Spatial Pyramid Pooling (ASPP) is a\ntechnique in deep learning designed to capture\nmulti-scale contextual information. By integrating\nmultiple parallel dilated convolutions with\nvarying dilation rates, ASPP expands the\nreceptive field of convolutional kernels without\nincreasing computational complexity, thereby\neffectively capturing broader contextual\ninformation. The core principle of dilated\nconvolution lies in adjusting the dilation rate of\nthe kernel, enabling the model to simultaneously\nextract features at different scales. This capability"}, {"title": "Discriminator", "content": "In Generative Adversarial Networks (GANs), the\ndesign of the discriminator plays a pivotal role in\ndetermining model performance and training\nstability. The discriminator's primary objective is\nto distinguish real samples from generated ones,\nthus providing the generator with effective\ngradient signals for optimization. In this study, we\nadopt an enhanced PatchGAN architecture as the\ndiscriminator backbone, integrated with a\ndynamic spectral normalization framework to\nfurther improve model robustness and\nconvergence stability."}, {"title": "PatchGAN Architecture", "content": "The core advantage of PatchGAN lies in its\nability to capture localized patterns and textural\ndetails within images. Specifically, PatchGAN\noperates through a sliding window mechanism\nthat partitions the input image into multiple fixed-\nsize patches. Each patch undergoes independent\nclassification, with the final discrimination score\nobtained by averaging all patch-level predictions\nthrough global average pooling. This architectural\ndesign enables two critical benefits for high-\nresolution medical image processing: (1)\nMaintains fine-grained structural features (e.g.,\nlesion boundaries, tissue textures) through patch-\nwise analysis. (2) Reduces complexity from O(n\u00b2)\nto O(n) through parameter-sharing convolutional\nimplementation\nThe PatchGAN loss can be expressed as:\n$L_{PatchGAN} = \\frac{1}{N} \\sum_{i=1}^{N} [y_i log (D(x_i)) + (1 - y_i)log (1 \u2013 D(x_i))]$  \nWhere, N presents the number of patches, $y_i$\nis the true label(0 or 1), and D($x_i$) is the output\nof the discriminator for the i-th patch."}, {"title": "Dynamic Adaptation Framework", "content": "To further enhance model performance, we\nintegrate a dynamically adaptive network\narchitecture into the discriminator. This\nframework continuously regulates the weights of\ngenerator components and residual blocks\nthrough real-time adversarial loss feedback,\nthereby optimizing feature extraction capabilities.\nThe dynamic adaptation module is optimized\nvia backpropagation, with its weight update rule\nformulated as:\n$\\theta_{new} = \\theta_{old} \u2013 \\alpha \u00b7 \\nabla_{\\theta} L_{adv}$\nHere, $\\theta_{new}$ denotes the updated weights, $\\theta_{old}$\nrepresents the pre-updated weights, \u03b1 is the\nlearning rate, and $\u2207_\u03b8L_{adv}$ indicates the gradient of\nthe adversarial loss $L_{adv}$ with respect to the\nweights.\nIn this study, the learning rate \u03b7 is set to 0.01.\nThe dynamic adaptation module stabilizes the\nadversarial training between the generator and\ndiscriminator, enabling the generator to more"}, {"title": "Experiment", "content": "This section presents the experimental results of\nour model conducted on multiple public datasets.\nThe results demonstrate that our model exhibits\noutstanding performance in fundus image lesion\nsegmentation tasks, achieving precise\nsegmentation of pathological regions. This\nachievement provides strong and reliable support\nfor medical diagnosis, contributing to improved\ndiagnostic accuracy and efficiency."}, {"title": "Datasets and Preprocessing", "content": "This study employs four publicly available\nfundus image datasets for experimental validation,\nwith specific information as follows: (1) IDRID\ndataset (Porwal et al., 2018), containing fundus\nimages from 79 patients with diabetic retinopathy;\n(2) DDR dataset (Li et al., 2019), comprising 757\nsamples of diabetic retinopathy at different\nseverity levels; (3) Retinal-lesions dataset (Wei et\nal., 2020), providing 1,302 images with circular\napproximation annotations for hemorrhagic\nregions; (4) FGADR dataset (Zhou et al., 2021),\nincluding 1,842 high-resolution fundus images\nwith pixel-level annotations. These four datasets\nexhibit significant differences in sample size,\nlesion types, annotation granularity, and image\nquality, establishing a multi-dimensional\nvalidation benchmark for assessing the model's\ngeneralization capability under divergent labeling\nprotocols.\nDuring the data preprocessing stage, a stratified\nrandom sampling strategy was adopted to divide\neach dataset into training, test, and validation sets\nat a 7:2:1 ratio, ensuring balanced distribution of\neach category. All input images were uniformly\nresampled to 512\u00d7512 pixel resolution using a\nbilinear interpolation algorithm, and standardized\nprocessing (Normalization) was applied to\nlinearly map pixel values to the [0,1] range. This\npreprocessing pipeline effectively eliminated\ndomain shift issues caused by device\nheterogeneity while improving model training\nefficiency through reduced computational\nredundancy. To validate the effectiveness of data\naugmentation, dynamic Gaussian noise injection\nwas implemented during the training phase to\nenhance data augmentation and expand the\ndiversity of training samples."}, {"title": "Evaluation Metrics", "content": "In semantic segmentation tasks, to\ncomprehensively and accurately measure the\nperformance of a model, multiple evaluation\nmetrics are typically employed. These metrics\nreflect the model's segmentation effect on the\ntarget from different perspectives. The Dice\ncoefficient is a commonly used statistical metric\nfor measuring the similarity between two samples.\nIn semantic segmentation, it assesses the model's\nperformance by calculating the ratio of the\nintersection to the union between the predicted\nsegmentation result and the true annotation. The\nser its value is to 1, the more consistent the\nmodel's segmentation result is with the actual\nsituation. The calculation formula for the Dice\ncoefficient is:\n$Dice = \\frac{2 \u00d7 |P \u2229 G|}{|P| + |G|}$\nWhere, P represents the set of pixels in the\npredicted segmentation result, G represents the set\nof pixels in the ground truth annotation, |P \u2229 G|\nindicates the number of pixels in the intersection\nof the predicted result and the ground truth\nannotation, |P| and |G| respectively represent the\ntotal number of pixels in the predicted result and\nthe ground truth annotation.\nThe Intersection over Union (IoU) represents\nthe ratio of the number of pixels in the\nintersection of the predicted segmentation area\nand the true segmentation area to the number of\npixels in their union. A higher IoU value indicates\nthat the model is more accurate in localizing and\ncapturing the shape of the target during the\nsegmentation process, and can better reflect the\nmodel's coverage of the target area. The formula\nfor calculating IoU is as follows:\n$IoU = \\frac{|P \u2229 G|}{|P \u222a G|}$\nWhere, |P\u222a G| represents the number of pixels\nin the union of the predicted results and the\nground truth annotations.\nThe F1 Score is the harmonic mean of\nPrecision and Recall, which comprehensively\nconsiders the model's ability to correctly identify\npositive samples and the proportion of positive\nsamples that are correctly classified as such. The\nF1 Score strikes a balance between Precision and\nRecall, providing a comprehensive measure of the\noverall performance of the model. The formula\nfor calculating the F1 Score is:\n$F1 = 2 \u00d7 \\frac{Precision \u00d7 Recall}{Precision + Recall}$\nPrecision represents the proportion of actual\npositive samples among the results predicted as\npositive by the model. It focuses on the accuracy\nof the model's predictions, aiming to avoid\nexcessive false positives. The formula for\ncalculating Precision is:\n$Precision = \\frac{TP}{TP + FP}$\nRecall reflects the proportion of actual positive\nsamples that are correctly predicted as positive by\nthe model among all actual positive samples. It\nemphasizes the model's ability to\ncomprehensively identify positive samples,\nensuring that as many true target regions as\npossible are detected. The formula for calculating\nRecall is:\n$Precision = \\frac{TP}{TP + FN}$\nWhere TP (True Positive) represents the\nnumber of samples correctly predicted as positive\nby the model; FP (False Positive) represents the\nnumber of samples incorrectly predicted as\npositive by the model; FN (False Negative)\nrepresents the number of samples incorrectly\npredicted as negative by the model.\nPA (Pixel Accuracy) stands for the ratio of\ncorrectly classified pixels to the total number of\npixels. PA intuitively reflects the model's\nclassification accuracy at the pixel level.\nAlthough it can be influenced to some extent by\nbackground pixels, it remains a fundamental and\nimportant metric for evaluating the performance\nof semantic segmentation models. The formula\nfor calculating PA is:\n$PA = \\frac{TP + TN}{TP + TN + FP + FN}$\nWhere TN (True Negative) represents the\nnumber of es correctly predicted as negative by\nthe model."}, {"title": "Implementation Environment", "content": "The experimental environment was fully\nequipped with NVIDIA GeForce RTX 4090\nGPUs, deployed on an Ubuntu 18.04 system . The\noptimizer setup involved 150 training epochs,\nconfiguring the discriminator and generator\nlearning rates at le-4 and 7.5e-4 respectively. A\nconsistent adversarial loss weight of 0.5 was\nutilized. Additionally, the training protocol\nincorporated a mechanism for dynamic learning"}, {"title": "Experimental Results", "content": "This study evaluated 10 mainstream backbone\nnetworks. As shown in Table 1, ResNet50\ndemonstrated superior comprehensive\nperformance with a Dice coefficient of 0.682 and\nIoU of 0.5602, significantly outperforming other\narchitectures. Notably, while AlexNet achieved\nthe highest Precision of 0.7801, its limited Recall\nvalue (0.6136) constrained overall segmentation\nefficacy."}, {"title": "Conventional Model Comparison", "content": "Eight conventional models were systematically\nevaluated. The proposed method achieved a\nRecall value of 0.766, representing a 9.5%\nimprovement over the optimal baseline (Unet:\n0.6994) and 29.8% enhancement compared to\nUnet++ (0.59). With a Dice coefficient of 0.682,\nour approach outperformed traditional Unet (0.57)\nby 19.6%, exhibiting robust clinical applicability"}, {"title": "Discriminator Architecture Analysis", "content": "Replacing PatchGAN with ImageGAN in GAN\nframework revealed critical insights. The\nVGG+PatchGAN configuration dominated three\nkey metrics: Dice (0.664), IoU (0.5395), and\nRecall (0.664), surpassing ImageGAN\ncounterparts by 5.4% in Dice and 7.5% in IoU.\nSimilarly, ResNet18+PatchGAN excelled in four\nindicators Dice (0.64), IoU (0.512), Recall\n(0.724), and Precision (0.619), demonstrating\nPatchGAN's superior capability in medical image\nfeature discrimination."}, {"title": "Ablation Study", "content": "To validate the contributions of individual\nmodules to model performance, seven controlled\nexperiments were designed (as shown in the\nTable 4). The baseline model (A+E), comprising\na U-shaped generator architecture and a\nPatchGAN discriminator framework, achieved a\nDice coefficient of 0.5753 and a Recall value of\n0.5311 in retinal vessel segmentation tasks,\ndemonstrating the effectiveness of the\nfoundational framework. Upon integrating\nresidual blocks (B) into the A+B+E configuration,\nthe Dice coefficient increased by 2.74% to 0.5927,\nwith the IoU metric rising to 0.4921, indicating\nthat residual connections effectively enhanced the\ntransmission of deep features and alleviated\ngradient vanishing issues. Further incorporation\nof the attention mechanism (D) into the A+D+E\narchitecture resulted in a significant 4.25\npercentage point improvement in Precision to\n0.6725 while maintaining a high pixel accuracy,\nconfirming that the attention mechanism\nsuccessfully focused on critical regions such as\nvessel boundaries.\nBy integrating the multi-scale feature\nextraction module ASPP (C), the A+B+C+E\nconfiguration achieved a Dice coefficient\nexceeding 0.625 and a Recall value of 0.5722,\nattributed to the adaptive modeling of varying\nvessel thicknesses through dilated convolution\npyramids. When combining residual blocks and\nattention mechanisms in the A+B+D+E\narchitecture, the model achieved a breakthrough\nin vascular detail capture, with the Dice\ncoefficient reaching 0.6417 and IoU improving to\n0.5373, demonstrating that multi-module\ncollaboration significantly enhances feature\nrepresentation. Finally, the complete model\nA+B+C+D+E+F, optimized by the dynamic\nadjustment network (F), attained optimal\nperformance with a Dice coefficient of 0.682 and\nIoU of 0.5602. Its Precision of 0.766 highlights\nthe architecture's ability to accurately distinguish\nvessels from background noise. Although pixel\naccuracy slightly decreased by 0.9 percentage\npoints, all primary segmentation metrics showed\nsubstantial improvements, validating the efficacy\nof the integrated design."}, {"title": "Comparison with the SOTA methods", "content": "To quantitatively evaluate the advancement of our\nmodel, we compared the Dice coefficients of\nDynSegNet with mainstream SOTA methods"}, {"title": "Conclusion", "content": "This study focuses on the segmentation task of\nhemorrhagic lesions in fundus images, proposing\nan innovative adversarial learning-based dynamic\narchitecture adjustment method. The approach\ningeniously integrates Hierarchical U-shaped\nencoder-decoder, residual blocks, attention\nmechanisms, and Atrous Spatial Pyramid Pooling\n(ASPP) modules, significantly improving the\nmodel's performance across multiple key\nevaluation metrics. Experimental results\ndemonstrate that dynamically adjusting network\narchitectures can flexibly adapt the weights of\ngenerator components and residual blocks\naccording to adversarial loss, thereby enhancing\nthe model's adaptability to lesions with varying\nmorphologies and distributions, while improving\nsegmentation accuracy and robustness.\nFurthermore, comparative experiments and\nablation studies validate the model's effectiveness\nand reveal the critical contributions of each\nmodule to performance. Notably, the method\nexhibits substantial clinical application potential,\nenabling clinicians to formulate treatment plans\nmore accurately, enhance therapeutic outcomes, and\nsupport early diagnosis and disease\nprogression assessment in ophthalmology."}, {"title": "Limitation", "content": "First, the model's generalization capability\nrequires further improvement to adapt to\nvariations in imaging equipment, imaging\nconditions, and patient populations across\ndifferent hospitals. Future research should involve\nvalidation on larger-scale and more diverse\ndatasets. Second, the model's high computational\ncomplexity and extended training/inference time\nlimit its efficiency in practical applications.\nFinally, there remains room for enhancement in\ndata preprocessing and postprocessing procedures\nto accommodate images with varying resolutions\nand quality levels, while meeting diverse\nsegmentation requirements. In future work, we\nwill continue to optimize the model architecture\nto improve segmentation accuracy and efficiency,\nthereby providing more reliable technical support\nfor the diagnosis and treatment of ophthalmic\ndiseases."}]}