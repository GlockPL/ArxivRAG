{"title": "Gradient Harmonization in Unsupervised Domain Adaptation", "authors": ["Fuxiang Huang", "Suqi Song", "Lei Zhang"], "abstract": "Unsupervised domain adaptation (UDA) intends to transfer knowledge from a labeled source domain to an unlabeled target domain. Many current methods focus on learning feature representations that are both discriminative for classification and invariant across domains by simultaneously optimizing domain alignment and classification tasks. However, these methods often overlook a crucial challenge: the inherent conflict between these two tasks during gradient-based optimization. In this paper, we delve into this issue and introduce two effective solutions known as Gradient Harmonization, including GH and GH++, to mitigate the conflict between domain alignment and classification tasks. GH operates by altering the gradient angle between different tasks from an obtuse angle to an acute angle, thus resolving the conflict and trade-offing the two tasks in a coordinated manner. Yet, this would cause both tasks to deviate from their original optimization directions. We thus further propose an improved version, GH++, which adjusts the gradient angle between tasks from an obtuse angle to a vertical angle. This not only eliminates the conflict but also minimizes deviation from the original gradient directions. Finally, for optimization convenience and efficiency, we evolve the gradient harmonization strategies into a dynamically weighted loss function using an integral operator on the harmonized gradient. Notably, GH/GH++ are orthogonal to UDA and can be seamlessly integrated into most existing UDA models. Theoretical insights and experimental analyses demonstrate that the proposed approaches not only enhance popular UDA baselines but also improve recent state-of-the-art models.", "sections": [{"title": "INTRODUCTION", "content": "EEP convolutional neural networks and transformers, driven by extensive labeled samples, have achieved remarkable success in various computer vision tasks such as classification, semantic segmentation and object detection. However, these models often demonstrate high vulnerability when deployed in novel application scenarios due to data distribution discrepancies. The process of collecting and annotating data across various domains is expensive, labor-intensive and time-consuming. Consequently, Unsupervised Domain Adaptation (UDA) arises to transfer the knowledge from a labeled source domain to an unlabeled target domain [15], [37], [56], [65].\nIn recent years, UDA algorithms have made significant progress in enhancing classification performance [9], [43], [55], [59], [87], [94], [95]. The primary focus of these approaches is to acquire domain-invariant feature representations, thereby achieving domain alignment and narrowing the probability distributions across domains. Currently, domain alignment methods fall into two main categories: distance metrics-based methods [1], [28], [54], [57], [73], [76] and adversarial learning-based methods [4], [8], [14], [20], [44]. The distance metrics-based approaches align the source"}, {"title": "RELATED WORK", "content": "2.1 Unsupervised Domain Adaptation (UDA)\nUnsupervised Domain Adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to"}, {"title": "PROPOSED APPROACH", "content": "3.1 Problem Definition\nGiven a labeled source domain $D_s = \\{(x_i, y_i)\\}_{i=1}^{n_s}$ with $n_s$ samples and an unlabeled target domain $D_t = \\{x_i\\}_{i=n_s+1}^{n_t}$ with $n_t$ samples, where $y_i$ is the class label of the $i^{th}$ source sample $x_i$. $D_s$ and $D_t$ share the same feature space and category space, but have different data distributions. Our purpose is to utilize the labeled data $D_s$ and unlabeled data $D_t$ to learn a deep model, which can accurately predict the class label of samples in the target domain."}, {"title": "A General Framework of UDA", "content": "Adversarial learning has proven to be an effective method for domain alignment, starting from Domain Adversarial Neural Network (DANN) [25]. The basic idea is to trick the domain discriminator $D$ by generating features via a feature generator $G$. Then the domain discriminator predicts whether the generated feature by $G$ is from the source domain or the target domain. The training of domain alignment is achieved through the game between generator and domain discriminator. The parameter $\\theta_g$ of generator $G$ and the parameter $\\theta_d$ of domain discriminator $D$ are optimized by the following domain alignment objective function.\n$L_{dom} (\\theta_g, \\theta_d) = E_{x \\sim D_s}log[D(G(x))] + E_{x \\sim D_t}log[1 - D(G(x))]$.\n(1)\nIn order to improve the classification performance of the target domain samples, we must first ensure that the classifier $C$ can correctly classify the samples from the source domain. Thus, the supervised classification loss can be described as\n$L_{cls} (\\theta_g, \\theta_c) = \\frac{1}{N_s} \\sum_{i=1}^{N_s} L_{ce}(C(G(x_i; \\theta_g); \\theta_c), y_i)$,\n(2)\nwhere $L_{ce}$ is the standard cross-entropy loss function.\nDuring training stage, the existing methods usually jointly optimize the two objective functions ($L_{dom}$ and $L_{cls}$) to obtain domain-invariant and class-discriminant feature representation. The overall minimax objective function is\n$\\min_{\\theta_d, \\theta_c} \\max_{\\theta_g} L_{dom} + L_{cls}$,\n(3)\nwhere $\\theta_g, \\theta_d, \\theta_c$ denote the parameters of feature generator, domain discriminator and classifier, respectively."}, {"title": "Gradient Harmonization (GH)", "content": "Domain alignment and classification are two different tasks. Their optimal gradient descent directions may not be coordinated, which results in the optimization conflict of the two loss functions in the training process and deteriorates the final domain adaptation performance.\nIn order to ensure that the two target tasks can be optimized in a coordinated manner, we propose an idea of de-conflict on the gradients, which then formulates the proposed GH technique, i.e., altering the gradient angle between different tasks from an obtuse angle to an acute angle. The de-conflict process for the gradients (i.e., $g_1$ and $g_2$) is schematically shown in Fig. 4. Specifically, we first provide three Lemmas to support our idea. Then the proposed GH is summarized as Theorem 1. For convenience, we define $L_1(\\Theta)$ and $L_2(\\Theta)$ as the general form of any two conflicted loss functions. In UDA, $L_1(\\Theta)$ and $L_2(\\Theta)$ represent the domain alignment loss $L_{dom}(\\theta_g, \\theta_d)$ and the classification loss $L_{cls} (\\theta_g, \\theta_c)$, respectively. $\\Theta = \\{\\theta_g, \\theta_d, \\theta_c\\}$ indicates model parameters of generator, discriminator and classifier. In fact, the three Lemmas aim to deduce the gradient harmonization formula during model optimization (training) phase, by exhausted mathematical solving process.\nLemma 1. Given two objective functions $L_1(\\Theta)$ and $L_2(\\Theta)$, we define $g_1$ and $g_2$ as their gradient, respectively, and $\\tilde{g}_1$ is the result of harmonizing the gradient $g_1$. For minimizing the"}, {"title": "Improved Version: GH++", "content": "In this section, we propose an improved version called GH++, which aims to adjust the gradient angle between the two tasks from an obtuse angle to a vertical angle, i.e., making them orthogonal. This is to eliminate the conflict but simultaneously relieve the gradient deviation. Fig. 7 visually illustrates the concepts of the proposed GH and GH++, with a primary focus on scenarios where gradient conflict exists, i.e., when the angle between $g_1$ and $g_2$ is obtuse.\nGradient deviation. For clarity, we designate the original gradients as $g_1 = \\overrightarrow{OA}$ and $g_2 = \\overrightarrow{OB}$. Let $\\theta$ represent the angle between $g_1$ and $g_2$. As illustrated in Fig. 7 (a), we denote the gradients after applying GH as $\\tilde{g}_1 = \\overrightarrow{OC}$ and $\\tilde{g}_2 = \\overrightarrow{OD}$, where $\\overrightarrow{OC} \\perp \\overrightarrow{OB}$ and $\\overrightarrow{OD} \\perp \\overrightarrow{OA}$. Apparently, the sum of the angles deviated from the original directions is $2(\\theta - \\frac{\\pi}{2})$, i.e., $\\angle AOC + \\angle DOB$. In other words, although GH can promote the positive correlation between the two gradients, its optimization gradient is seriously deviated from the original gradient. Therefore, we propose an improved version, GH++, to eliminate the conflict and minimize the sum of the gradient deviations. As shown in Fig. 7 (b), GH++ adjusts the gradient angle between the two tasks from an obtuse angle to a vertical angle, i.e., $\\angle EOF$. The sum of the gradient deviation angles is $(\\theta - \\frac{\\pi}{2})$, i.e., $\\angle AOE + \\angle FOB$, which is half of the sum of the gradient deviations of GH.\nSpecifically, as shown in Fig. 7 (b), let denote the harmonized gradients of GH++ as $\\tilde{g}_1 = \\overrightarrow{OE}$ and $\\tilde{g}_2 = \\overrightarrow{OF}$. In order to resolve the conflict and relieve the deviation from the original gradient directions, we designate $\\overrightarrow{OE} \\perp \\overrightarrow{OF}$, i.e., $\\angle EOF = \\frac{\\pi}{2}$, where points E and F move along arcs AC and BD, respectively. Intuitively, $\\bigtriangleup EOF$ forms a rotatable right triangle. We define the direction of rotation from $g_1$ to $\\tilde{g}_1$ as positive. $\\beta$ represents the rotating angles from $g_1$ to $\\tilde{g}_1$, which is a positive angle and $\\bar{\\beta}$ represents the rotating angles from $g_2$ to $\\tilde{g}_2$, which is a negative angle. According to Fig. 7 (b), the harmonization gradients of GH++, i.e., $\\tilde{g}_1$ and $\\tilde{g}_2$, can be represented as\n$\\tilde{g}_1 = \\overrightarrow{OE} = \\overrightarrow{OA} + \\overrightarrow{AE} = g_1 + \\overrightarrow{AE}$,\n$\\tilde{g}_2 = \\overrightarrow{OF} = \\overrightarrow{OB} + \\overrightarrow{BF} = g_2 + \\overrightarrow{BF}$.\n(33)"}, {"title": "Equivalent Model of UDA with GH/GH++", "content": "By combining the general UDA model (Eq. (3), described in Section 3.2) with gradient harmonization strategies (Eq. (29)/Eq. (40), a well-balanced UDA model can be trained and implemented. However, the gradient aggregation operator in Eq. (29)/Eq. (40) is intricate and has an impact on optimization efficiency. Therefore, we introduce a computation-efficient alternative model that is functionally equivalent to UDA with GH/GH++. For convenience, we can express the proposed gradient harmonization approaches, i.e., Eq. (29)/Eq. (40), as follows:\n$g = \\tau_1 g_1 + \\tau_2 g_2$,\n(41)\nwhere $\\tau_1$ and $\\tau_2$ are constants that can be calculated by using the original gradients $g_1$ and $g_2$. Notably, the gradient $g_1$ of the original loss $L_1(\\Theta)$ and the gradient $g_2$ of the original loss $L_2(\\Theta)$ can be easily computed, only if the UDA model is fixed. Then, if GH is chosen to resolve the conflict, $\\tau_1$ and $\\tau_2$ can be calculated as follows.\n$\\tau_1 = 1 - \\delta(g_1^T g_2 < 0) \\frac{g_2^T g_1}{||g_1||^2}$,\n$\\tau_2 = 1 - \\delta(g_1^T g_2 < 0) \\frac{g_1^T g_2}{||g_2||^2}$.\n(42)\nIf GH++ is used to eliminate the conflict, $\\tau_1$ and $\\tau_2$ can be calculated as follows.\n$\\tau_1 = (1 + 2\\delta(g_1^T g_2 < 0) sin(\\frac{\\lambda (arccos \\frac{g_1^T g_2}{||g_1|| ||g_2||} - \\frac{\\pi}{2})}{2}))$,\n$\\tau_2 = (1 + 2\\delta(g_1^T g_2 < 0) sin(\\frac{(\\lambda - 1) (arccos \\frac{g_1^T g_2}{||g_1|| ||g_2||} - \\frac{\\pi}{2})}{2}))$.\n(43)\nUltimately, we can derive the equivalent UDA model embedded GH/GH++ by conducting the integral operation on the gradient in Eq. (41). The overall loss of UDA with GH/GH++ model can be represented as\n$\\int (\\tau_1 g_1 + \\tau_2 g_2) d\\Theta = \\tau_1 L_1(\\Theta) + \\tau_2 L_2(\\Theta)$,\n(44)\nwhere $L_1(\\Theta)$ and $L_2(\\Theta)$ can be the domain alignment loss and classification loss described in Eq. (1) and Eq. (2),"}, {"title": "EXPERIMENTS", "content": "4.1 Datasets\nOffice-31 [69] is a mainstream benchmark dataset for visual domain adaptation, which consists of three distinct domains: Amazon (A), DSLR (D), Webcam (W). It totally contains 4,652 images from 31 categories. We evaluate our method in all 6 different transfer tasks across domains.\nOffice-Home [79] is a more challenging and harder benchmark than Office-31. It contains 15.5K images across 65 object categories from 4 different domains: Artistic images (Ar), Clip Art (Cl), Product images (Pr), and Real-World images (Rw). We evaluate our method in all 12 different transfer tasks across domains.\nDigits Datasets. We mainly study three datasets: MNIST (M) [91], USPS (U) [36] and SVHN (S) [64]. MNIST and USPS are two general handwriting recognition datasets involving 10 categories. SVHN is obtained from house numbers in"}, {"title": "Implementation Details", "content": "We compare our method with the following state-of-the-art unsupervised domain adaptation methods: ResNet [32], DAN (Deep Adaptation Networks) [54], DANN (Domain-adversarial Neural Networks) [25], JAN (Joint Adaptation Networks) [58], DRCN (Deep Reconstruction-Classification Networks) [29], CoGAN (Coupled Generative Adversarial Networks) [52], ADDA (Adversarial Discriminative Domain Adaptation) [75], CyCADA (Cycle-consistent Adversarial Domain Adaptation) [33], CAT (Cluster Align-ment with a Teacher) [17], TPN (Transferrable prototypical networks) [66], LWC (Light-weight Calibrator) [92], ETD (Enhanced Transport Distance) [43], TAT (Transferable Adversarial Training) [49], TADA (Transferable Attention"}, {"title": "Results on UDA", "content": "Tables 1, 2, 3, 4 and 5 present evaluation results on Office-31, Office-Home, VisDA-2017, Digits and DomainNet, respectively. Generally, the transformer-based results, such as TVT and SSRT, are much better than CNN-based results, which has been validated in previous work. For TVT and SSRT, the ViT backbone pre-trained on ImageNet-1K is slightly inferior than ImageNet-21K (i.e., TVT* &TVT, SSRT*&SSRT). Considering that the CNNs are pre-trained on ImageNet-1K, we take SSRT* for fair analysis as default in the following."}, {"title": "MODEL ANALYSIS AND DISCUSSION", "content": "5.1 Feature Visualization\nFig. 8 describes the t-SNE [77] visualizations of features learned by MCD (baseline) and MCD+GH on the tasks of U \u2192 M and M \u2192 U. Fig. 8 (a) and (c) are visualization features generated by MCD. Fig. 8 (b) and (d) are visualization features generated by MCD+GH. It can be observed that both features learned by MCD and MCD+GH achieve well-performed global alignment effect with 10 clusters under two tasks. Further, the visualization feature distributions with GH deployed have better clustering effect and have fewer samples distributed across class boundaries, which intuitively boosts the feature discriminability. In addition,"}, {"title": "Convergence Analysis", "content": "We present the convergence curves of test error with respect to the number of iterations on tasks of U \u2192 M and Synthetic \u2192 Real as shown in Fig. 9. For each subfigure, the blue line represents the test error of different baselines, and the red line represents the test error for baseline+GH (e.g., CDAN+GH). Obviously, compared with baselines, the intro-duction of GH can further improve the test performance and convergence. This fully indicates that GH plays an active"}, {"title": "Confusion Matrix Visualization", "content": "Fig. 10 displays the visualizations of confusion matrix for the classifier trained by DWL and DWL+GH. DWL obtains several uncertain predictions with small values while DWL+GH obtains more confident predictions. Comparing with Fig. 10 (a) and (b), the confusing \u201cClass 1, 2, 7, 8, and 9\u201d are correctly recognized in DWL+GH. From Fig. 10 (c) and (d), the confusing \u201cClass 8\u201d is corrected in DWL+GH."}, {"title": "Balance Analysis of GH-based UDA", "content": "Fig. 11 shows MMD distance [3] and the max J(W) [10] values based on the feature representation learned by MCD+GH. The left vertical axis corresponding to the red curve represents the MMD distance used to measure the"}, {"title": "Training Speed and Accuracy", "content": "In order to observe the efficiency of the proposed equivalent model more clearly, we present the training speed and classification accuracy before and after applying GH. As shown in Table 6, for tasks M\u2192U and U\u2192M on digits datasets, the training speed of MCD+GH is 0.14254 s/epoch and 0.09809 s/epoch longer than MCD, respectively. In other words, MCD+GH takes less training time than MCD in training process, but can get 2.5%/2.7% classification accuracy gain. The computational cost of employing GH is quite low, and thus GH is a powerful and efficient auxiliary tool to facilitate those popular domain adaptation baselines towards more outstanding classification performance."}, {"title": "Gradient Inner Product Visualization", "content": "Fig. 12 presents inner product distributions of two gradients between domain alignment and classification tasks before and after applying Gradient Harmonization for MCD. From Fig. 12 (a), we observe the acute and obtuse angles between gradients of the two tasks before coordination. The obtuse angles account for about 40% of the total number, which exhibits the identical property as Fig. 2. In other words, there exists between-task conflict in the model optimization process but paid less attention. After Gradient Harmonization, as shown in Fig. 12 (b), the inner products of the two gradients are all positive. That is, the gradient angles between the two tasks are coordinated into acute angles. The proposed GH avoids the optimization conflict by separately adjusting the gradients of the two tasks to achieve the purpose of optimal coordination. Experimental results fully illustrate the effectiveness of the proposed GH."}, {"title": "Rationality and Comparison to Other Alternatives", "content": "The proposed GH/GH++ aims at altering the gradient angle between two different tasks from an obtuse angle"}, {"title": "Parameter Sensitivity Analysis", "content": "To investigate the effect of the parameter \u5165 in GH++, we conduct experiments on three tasks (i.e., M\u2192U, U\u2192M and S\u2192M) based on MCD and GVB by varying \u03bb\u2208 {0, 0.1, 0.3, 0.5, 0.7, 0.9, 1}. The results are presented in Fig. 14. We can observe GH++ is little sensitive to the scale variety of A, which indicates that GH++ is robust across different baselines and tasks. Besides, we observe that when X = 0.5, models generally achieve the best performance. In other words, when the gradient deviation of the two tasks is relieved, the performance can be largely improved."}, {"title": "Scalability to Other Multi-task Problems", "content": "To further demonstrate the universality and scalability of the proposed approaches, we evaluate GH/GH++ in the object detection and multi-modal interactive retrieval fields, which also involves optimization of multiple objectives.\nDataset. We select widely used benchmarks, i.e., PAS-CAL VOC 2007 [22] and CSS [80] for object detection and multi-modal interactive retrieval, respectively. PASCAL"}, {"title": "CONCLUSION", "content": "In this paper, we pay attention to the optimization conflict (i.e., imbalance or incoordination) problem between different tasks (i.e., the alignment task and the classification task) in alignment-based unsupervised domain adaptation models. To mitigate this problem, we propose two simple yet efficient Gradient Harmonization approaches, including GH and GH++, which take measures to de-conflict between the gradients of both tasks in optimization. Besides, to facilitate the harmonization during adaptation, we derive the equivalent but more efficient model of UDA with GH/GH++, which becomes a dynamically reweighted loss function of most existing unsupervised domain adaptation models. Further, the essence and insights of the proposed approaches are provided to indicate its rationality. Exhaustive experiments and model analyses demonstrate that the proposed approaches significantly improve the existing UDA models and contribute to achieving state-of-the-art results. In addition, we have verified that the proposed approaches can be adapted to other problems and areas, such as object detection and multi-modal retrieval, to de-conflict between the gradients of any two tasks in optimization and improve model performance."}]}