{"title": "Towards Adversarially Robust Vision-Language Models: Insights from Design Choices and Prompt Formatting Techniques", "authors": ["Rishika Bhagwatkar", "Shravan Nayak", "Reza Bayat", "Alexis Roger", "Daniel Z Kaplan", "Pouya Bashivan", "Irina Rish"], "abstract": "Vision-Language Models (VLMs) have witnessed a surge in both research and real-world applications. However, as they are becoming increasingly prevalent, ensuring their robustness against adversarial attacks is paramount. This work systematically investigates the impact of model design choices on the adversarial robustness of VLMs against image-based attacks. Additionally, we introduce novel, cost-effective approaches to enhance robustness through prompt formatting. By rephrasing questions and suggesting potential adversarial perturbations, we demonstrate substantial improvements in model robustness against strong image-based attacks such as Auto-PGD. Our findings provide important guidelines for developing more robust VLMs, particularly for deployment in safety-critical environments.", "sections": [{"title": "1. Introduction", "content": "VLMs process images in conjunction with text prompts, enabling them to perform a wide array of tasks, such as image captioning, visual question answering (VQA), and cross-modal retrieval (Liu et al., 2023; Laurenccon et al., 2023; Awadalla et al., 2023; Radford et al., 2021). While there is extensive research on advancing architecture and scaling, recent works demonstrate that VLMs are not immune to adversarial vulnerabilities subtle, intentionally crafted perturbations to input data that can lead to significant errors in the output (Schlarmann et al., 2024). These vulnerabilities can mislead users with harmful or toxic responses, undermining the models' robustness and integrity.\nWhite-box attacks are a common type of adversarial threat. These attacks assume complete access to a model's parameters, enabling attackers to exploit specific vulnerabilities. Since many VLMs are open-source, attackers can easily analyze and exploit them. Current VLMs have several design choices, including the vision encoder, large language model (LLM), mapping network, image resolution, and the training data (Liu et al., 2023; Laurenccon et al., 2023; Awadalla et al., 2023). Despite the importance of these factors, their impact on adversarial robustness is under-explored. In this study, we evaluate how these design choices during VLM training influence their susceptibility to white-box adversarial attacks on the input images.\nIn addition to design choices, the selection and quality of prompts can significantly impact the performance and robustness of VLMs (Awal et al., 2024). Effective prompts can enhance the models' understanding and response to inputs, affecting their robustness to adversarial attacks. Recent works have focused on adversarial training to increase robustness (Schlarmann et al., 2024; Mao et al., 2023), but it is resource-intensive and costly, often requiring millions of samples (Wang et al., 2023; Bartoldson et al., 2024). As a practical and cost-effective alternative, we investigate whether prompt formatting can enhance the adversarial robustness of VLMs. This approach explores if simple linguistic modifications can increase robustness, offering a low-cost alternative to adversarial training.\nThrough evaluating both design choices and prompt formatting, we aim to provide comprehensive insights into enhancing the adversarial robustness of VLMs. Our main contributions are summarized as follows:\n1. We provide an in-depth analysis of how various design choices of VLMs impact their robustness to white-box visual adversarial attacks.\n2. We investigate a novel approach to prompt formatting for enhancing the adversarial robustness of VLMs.\n3. To the best of our knowledge, we are the first to offer actionable insights and practical recommendations for using text prompting techniques to enhance the robustness of VLMs in deployment."}, {"title": "2. Related Works", "content": "Vision Language Models. VLMs traditionally align visual tokens from the vision encoder with the linguistic space of the language model using various mapping networks, such as the Q-former in BLIP2 (Li et al., 2023) and the multilayer perceptron in LLaVA (Liu et al., 2023). Recent studies investigate how choices like vision encoder type, language model, resolution of images, and training duration affect the accuracy on clean inputs (Karamcheti et al., 2024). In contrast, our study specifically aims to explore how these choices affect robust accuracy.\nAdversarial Robustness of VLMs. Research into the adversarial robustness of multi-modal foundation models like BLIP2 (Li et al., 2023), OpenFlamingo (Awadalla et al., 2023), CLIP (Radford et al., 2021), and LLaVA (Liu et al., 2023) has highlighted their susceptibility to both targeted and untargeted visual attacks (Cui et al., 2023a; Zhao et al., 2023). Studies also explore the potential of using pretrained VLMs to craft adversarial image and text perturbations that can compromise black-box models fine-tuned for various tasks (Zhao et al., 2023; Dong et al., 2023). Additionally, the transferability of these attacks is well-studied, with techniques developed to enhance efficacy using surrogate models (Yin et al., 2023).\nAdvancements in Defense Mechanisms. Many studies focusing on the adversarial robustness of VLMs using CLIP as a vision encoder have revealed its susceptibility to adversarial attacks (Fang et al., 2022; Tu et al., 2023; Nguyen et al., 2022). To counter this, TeCoA (Mao et al., 2023) proposes adversarial fine-tuning to maintain zero-shot capabilities. Further, RobustCLIP (Schlarmann et al., 2024) proposes an unsupervised method leveraging adversarial training on the ImageNet (Deng et al., 2009) dataset to improve robustness across vision-language tasks. Additionally, efforts to enhance robustness include prompt tuning, where one study suggests enriching prompts with contextual image-derived information for improving adversarial robustness (Cui et al., 2023b). Another approach optimizes prompts through adversarial fine-tuning on ImageNet with specific parameters (Zhang et al., 2023). Our research, however, focuses on analyzing the impact of prompt formatting on model performance without additional training or image-based information extraction."}, {"title": "3. Experiments", "content": "In this section, we outline the attack setups used during evaluations, the tasks assessed, and the specific models examined in our model design choice experiments."}, {"title": "3.1. Attack Setup", "content": "This work focuses on white-box gradient-based untargeted attacks on image inputs, where it is assumed that the attacker has complete knowledge of the model, including architecture and parameters. The objective in crafting adversarial samples under this scenario is to subtly perturb the input so that the model produces an incorrect output. Mathematically, it can be formulated as $\\max_{\\delta} L(f(x + \\delta), y)$ where $f$ is the model, $x$ is original input, $\\delta$ is the adversarial perturbation learnt within the $|\\delta|\\|_{\\infty} \\leq \\epsilon$ constraint and $y$ is the original label. Hence the goal is to find a perturbation $\\delta$ that maximizes the loss while respecting the perturbation bound.\nOur evaluation encompasses three gradient-based adversarial attacks, ordered in increasing complexity: Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014), Projected Gradient Descent (PGD) (Madry et al., 2017), and Auto-PGD (APGD) (Croce & Hein, 2020). We employ PGD and APGD attacks with 100 iterations while FGSM uses single iteration by design. Our evaluation focuses on $l_{\\infty}$ bounded perturbations, with the perturbation magnitudes $\\epsilon \\in \\{4/255, 8/255, 16/255\\}$. This range allows us to systematically assess the robustness of models against varying strengths of adversarial attacks."}, {"title": "3.2. Tasks", "content": "Our evaluation covers two primary tasks: Image Captioning and VQA. For image captioning, we use the validation splits of the COCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015) datasets to assess caption accuracy and relevance. In the VQA domain, we evaluate using the validation splits of VQAv2 (Antol et al., 2015), TextVQA (Singh et al., 2019), OK-VQA (Marino et al., 2019), and VizWiz (Gurari et al., 2018) datasets. We report the robust VQA accuracy for datasets associated with VQA tasks and robust CIDEr scores for the captioning datasets. Higher is better for both metrics. We randomly sample 1000 examples from the validation set of each task and use this for the adversarial evaluations of all models to ensure a fair comparison. The models selected for evaluating the impact of design choices on adversarial robustness are detailed in Table 1."}, {"title": "4. Results", "content": "In our analysis, we examine the impact of various model design choices on adversarial robustness. Specifically, we focus on: (a) the choice of vision encoder; (b) the input resolution used by the vision encoder; (c) the sizes of the language models; and (d) the ensemble use of multiple vision encoders. Each of these aspects is detailed in the sections below. We report results using $\\epsilon = 8/255$. Please check Appendix A for results with other values $\\epsilon = 4/255, 16/255$. We highlight the best robust FGSM, PGD, APGD accuracy across benchmarks for all attacks and models."}, {"title": "4.1. Model Design Choices", "content": "In our analysis, we examine the impact of various model design choices on adversarial robustness. Specifically, we focus on: (a) the choice of vision encoder; (b) the input resolution used by the vision encoder; (c) the sizes of the language models; and (d) the ensemble use of multiple vision encoders. Each of these aspects is detailed in the sections below. We report results using $e = 8/255$. Please check Appendix A for results with other values \u20ac = 4/255, 16/255. We highlight the best robust FGSM, PGD, APGD accuracy across benchmarks for all attacks and models."}, {"title": "4.1.1. IMPACT OF VISION ENCODER", "content": "We systematically evaluate the effects of employing different vision encoders, each trained under distinct conditions. We compare VLMs that use four different image encoders: CLIP (Radford et al., 2021), SigLIP (Zhai et al., 2023), DINOv2 (Oquab et al., 2023), and ImageNet (Dosovitskiy et al., 2020). As shown in Table 2, SigLIP slightly outperforms CLIP, with both noticeably surpassing DINOv2 and ImageNet VLMs on weaker attacks. However, the difference diminishes for stronger attacks. We hypothesize that the Vision Transformer (ViT) used in CLIP and SigLIP has been trained across a wide spectrum of internet-collected images and hence has seen many more distributions during training than DINOv2 and ImageNet. The results also resonate with the choice of vision encoders in recent state-of-the-art VLMs (Liu et al., 2023; Karamcheti et al., 2024)."}, {"title": "4.1.2. RESOLUTION OF VISION ENCODER", "content": "Generally, a higher input resolution improves the quality of visual representations, potentially boosting model performance (Karamcheti et al., 2024). Owing to the availability of high-resolution variants, we specifically evaluate models equipped with CLIP and SigLIP vision encoders at two distinct resolutions to thoroughly understand these effects. Based on Table 3, while increasing resolution enhances robustness against stronger attacks for high-resolution CLIP models (on most tasks), the effectiveness of the increased resolution in SigLIP models appears to be task-dependent. However, we observe that robust accuracy significantly deteriorates under APGD attacks in all cases except for VQAv2. For results on other e values, please refer to Appendix A."}, {"title": "4.1.3. SIZE OF LANGUAGE MODEL", "content": "We evaluate a series of VLMs utilizing the same vision encoder, and same LLM architecture, with the only difference being the language model's size. Specifically, we examine models equipped with the Vicuna language model (Zheng et al., 2024) in two sizes: 7B and 13B. According to the results in Table 4, the model's vulnerability to adversarial attacks and the significant drop in robust accuracy remain consistent, regardless of the model's scale. Hence, increasing the size of the language model does not seem to enhance robustness. One potential reason for this could be that adversarial attacks compromise the representations from the vision encoder. As a result, LLMs even at 13B scale may struggle to effectively interpret these flawed representations, making robustness to image-based attacks less sensitive to language model size. Therefore, enhancing the vision encoder's adversarial robustness is sufficient as shown in prior work (Schlarmann et al., 2024). Please check Appendix A for results on other e values."}, {"title": "4.1.4. ENSEMBLE OF VISION ENCODERS", "content": "We also explore the vulnerability of VLMs that employ an ensemble of vision encoders. Although recent studies suggest that multiple encoders can significantly improve performance (Karamcheti et al., 2024; Kar et al., 2024), our research aims to assess whether compromising just one encoder can affect the entire model. This approach allows us to analyze if knowledge of the weakest link is sufficient to compromise the entire model when an ensemble of encoders is used. We specifically examined models combining DINOv2 with either CLIP or SigLIP. In our experiments, we perturbed the images processed by DINOv2 while keeping inputs to the other encoder intact. Results in Table 5 show that attacking only DINOv2 is sufficient to compromise the model under stronger attacks, despite the other vision encoder providing clean inputs. This highlights a significant vulnerability in ensemble approaches: even with enhanced performance capabilities, the robustness of the entire system can be jeopardized by targeting a single encoder. Please check Appendix A for results on other e values."}, {"title": "4.2. Prompt Formatting", "content": "Considering that our adversarial examples are generated solely by perturbing visual inputs, we hypothesize that modifying the original prompts could be particularly effective in countering the effects of such perturbations. We test this hypothesis with the LLaVA 7B and LLaVA 13B models, employing different types of prompts for COCO and VQAv2. Our evaluation includes adversarial examples created using FGSM, PGD, and APGD attacks, with PGD, APGD based on 100 iterations. Specific details on the prompts used and the corresponding results are detailed in the subsequent subsections."}, {"title": "4.2.1. \u0421\u0410\u0420TIONING", "content": "Our experiments evaluated various prompt formatting strategies, including: (1) Original - using the original prompt; (2) Adversarial Certainty (AC) Prompt - explicitly informing the model that the image is adversarially perturbed; (3) Adversarial Possibility (AP) Prompt - suggesting the possibility that the image might be adversarially perturbed; and (4) Random - appending a random sentence or string at the beginning of the prompt. These are listed in Table 15 in Appendix B. From the results presented in Fig. 1 and Table 16 in Appendix C, it is evident that indicating the possibility of adversarial perturbations (AP prompt) assists the model significantly more than explicitly stating that the image is perturbed (AC prompt). Further, the improvements from simply adding a random string or sentence are substantial, even comparable to the effects observed with the AP prompt. This indicates that the models pay more attention to the inputs when they struggle to establish a clear relationship between them."}, {"title": "4.2.2. VISUAL QUESTION ANSWERING", "content": "Here, we explored four strategies: (1) Rephrase - rephrasing the original question to create a semantically similar question; (2) Expand - increasing the length of the questions; (3) Adversarial Certainty (AC) Prompt - explicitly informing the model that the image is adversarially perturbed; and (4) Adversarial Possibility (AP) Prompt - suggesting the possibility that the image might be adversarially perturbed. We utilize a finetuned model of the Mistral 7B LLM (Teknium, 2023) to generate questions according to the above-mentioned strategies. All the instructions used to obtain the modified questions are listed in Table 14 in Appendix B. According to the results presented in Fig. 3 and Table 17 in Appendix C, simply rephrasing the questions significantly improved performance compared to the other methods, such as extending the question length or explicitly warning about potential adversarial perturbations. Moreover, indicating the possibility of an adversarial perturbation yielded the best robustness performance, reinforcing our observations with the COCO dataset discussed earlier."}, {"title": "5. Discussion and Conclusion", "content": "Our evaluation highlighted critical insights into how various design elements affect the adversarial robustness of VLMs. First, we observed that vision encoders trained across diverse data distributions only improve resistance against simpler, less sophisticated attacks, demonstrating limited effectiveness against more complex threats. Additionally, increasing the resolution of image encoders did not correlate with enhanced adversarial robustness, suggesting that benefits seen in the clean accuracy do not extend to improved robustness. Similarly, scaling up the size of the language model did not increase the model's robustness to attacks, indicating that larger models are not inherently more robust. Most notably, our results revealed that using multiple vision encoders does not guarantee robustness; rather, knowledge about the most vulnerable encoder is enough to compromise the entire system.\nBuilding on our findings, we further explored the influence of prompt formatting on enhancing adversarial robustness. Our experiments revealed that even naively rephrasing the questions significantly improves robustness in VQA. Similarly, merely suggesting the possibility of an adversarial image during captioning led to a notable performance boost. More importantly, we found that we do not need to add additional context from the image or fine-tune additional tokens to make models adversarially robust, as opposed to prior work (Cui et al., 2023b; Zhang et al., 2023)."}, {"title": "6. Impact Statement", "content": "As VLMs see increased real-world deployment, ensuring their robustness against adversarial attacks is critical. Our research makes two key contributions: providing optimal model design choices for safe deployment and demonstrating how prompt formatting can enhance adversarial robustness. Our lightweight technique offers a practical alternative to computationally intensive adversarial training, reducing the computational footprint. While enhancing robustness against multimodal attacks using prompt formatting remains unexplored, our work addresses the crucial task of defending against strong image-based attacks that can lead to misinformation or harmful content generation. This research aims to support future advancements in the safe deployment of Al systems."}]}