{"title": "Automating Curriculum Learning for Reinforcement Learning using a Skill-Based Bayesian Network", "authors": ["Vincent Hsiao", "Mark Roberts", "Laura M. Hiatt", "George Konidaris", "Dana Nau"], "abstract": "A major challenge for reinforcement learning is automatically generating curricula to reduce training time or improve performance in some target task. We introduce SEBNs (Skill-Environment Bayesian Networks) which model a probabilistic relationship between a set of skills, a set of goals that relate to the reward structure, and a set of environment features to predict policy performance on (possibly unseen) tasks. We develop an algorithm that uses the inferred estimates of agent success from SEBN to weigh the possible next tasks by expected improvement. We evaluate the benefit of the resulting curriculum on three environments: a discrete gridworld, continuous control, and simulated robotics. The results show that curricula constructed using SEBN frequently outperform other baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Adapting skills to new or unseen tasks is a major challenge in Reinforcement Learning (RL). Curriculum Learning [5, 23], an approach for training agents using a sequence of increasingly difficult environments, often promotes the effective development of policies with more robust capabilities. However, customizing a curriculum to a particular student often requires substantial human insight and oversight. This is especially challenging for robotics, where the environment or tasks that need to be performed can change frequently. An ideal solution to this problem would be an automated curriculum that enables the robot to discern for itself when it needs to adapt, how long should train, and in what environments.\nPast work on automated curriculum generation such as [18, 31] has primarily focused on choosing what skills to train while holding the environment itself static. More recent approaches that build a curriculum over different environments such as [25] do not consider agent skill competencies. Furthermore, these environment-based approaches require explicit evaluation on an environment before being able to calculate an estimate of agent success or regret to add those environments to the curriculum.\nWe address the aforementioned issues by introducing Skill Environment Bayesian Networks (SEBNs) as a potential method for estimating agent competency level and selecting the most appropriate environments for training. SEBNs model a probabilistic relationship between these goals, (latent) competencies, and environment features using data from past rollouts. Using this model, we can estimate agent success rates on new (possibly unseen) environments. We use these estimates to select the next set of training tasks within a curriculum in what we call an SEBN-guided automated curriculum. Importantly, SEBN does not require explicit evaluation on each possible environment to estimate agent success.\nThe contributions of the paper include: (1) Introducing and formalizing the SEBN for skills, task features, and reward structure; (2) Providing an algorithm for constructing curricula using SEBNs; (3) Introducing Megagrid, a gridworld environment that simplifies generating partially-specified environments for transfer learning; (4) Assessing SEBN-based curricula on three distinct environments: a discrete gridworld (DoorKey), continuous control (Bipedal-Walker), and a difficult simulated robotics domain (robosuite); and (5) Demonstrating via experiments that SEBN curricula produce more robust policies that reach success more quickly than other curricula in the continuous control and robotics environments, and performed comparably in the gridworld environment."}, {"title": "2 BACKGROUND AND PRELIMINARIES", "content": "Bayesian statistics rely on some sort of informed prior, provided or learned, to estimate the future values. In an SEBN, part of this prior is provided in the form of the network and in the strength of relationships, and part of the prior is learned through the collection of samples from the environment. We next provide our motivation for this Bayesian approach (\u00a72.1) with some background on Bayesian Networks (\u00a72.2). We then formalize the curriculum learning problem (\u00a72.3), how we use task features to construct tasks (\u00a72.4), and an extension to the options framework (\u00a72.5)."}, {"title": "2.1 Evidence Centered Design", "content": "Our motivation for using a Bayesian Network to estimate learning proficiency comes from the method of Evidence Centered Design (ECD) [21], a technique used in human educational assessments. In Evidence-Centered Design, statistical models, such as Bayesian networks, are used to measure the proficiency levels of a given student. These proficiency measurements are then used to inform task and assessment creation. For example, ECD could be used to help model and analyze the performance of a tennis player. The Bayesian network in this domain can include nodes that represent latent competencies (e.g., mobility, footwork, dynamic vision, etc.) and nodes that represent observable metrics (e.g., number of successful serves, return rate, game score). The performance of a tennis player on the observable metrics is used to infer their latent capabilities. New training goals can then be set using these estimated capabilities. This technique is effective in human educational contexts, and we hypothesize that a similar approach could be applied to assist in designing a curriculum to improve learning in robotic agents."}, {"title": "2.2 Bayesian Networks (BNs)", "content": "Bayesian Networks (BNs) [28] are a type of graphical model that provide an efficient way to represent and reason about probabilistic relationships among a set of random variables. A Bayesian Network (X, D, \\Gamma) is defined by a set of variables X, their corresponding domains D, and a set of parent functions \\Gamma that specify the conditional probability distributions of each variable given its parents. When D is discrete, these parent functions are typically specified in a tabular format known as Conditional Probability Tables (CPTs).\nIt is common to use BNs to model relationships between latent and observable variables. Once constructed, the network can be used to infer latent values from observed values. New data can be entered in the form of evidence values for observed variables in a BN. The probabilities over other variables in the network are calculated by conditioning on this observed evidence, i.e., as a conditional probability: P(X1|X2, ..., XN). We will employ a standard bucket elimination algorithm (aka variable elimination) [7, 8] to perform inference. In this paper, the observable variables of the BN relate to the environment of an agent and a target performance it is attempting to achieve; both are modeled as a task in an MDP problem and defined in \u00a72.3. The unobservable variables will relate to a set of latent competencies that we define in \u00a73.1."}, {"title": "2.3 Curriculum Learning", "content": "We adapt the notation of Narvekar et al. [23] to describe a task as the interaction of an agent with its environment to meet some objective. A task, formalized as an episodic Markov Decision Process (MDP), is a tuple M = (S,A, p, r), where S is the set of states, A is the set of actions, p(s'|s, a) gives the probability of being in state s' after taking action a in state s, and r(s, a, s') is the reward function after taking action a in state s and transitioning to state s'. A solution to M is a policy \\pi that maximizes the cumulative sum of rewards for an episode of length T, i.e., \\sum_{t=1}^{T} Rt.\nLet T be a set of all tasks an agent could complete in M, where a task mi \u2208 T is a task-specific MDP mi = (Si, Ai, pi, ri). For all tasks in T, let DT be the set of all possible transition samples from T (see Narvekar et al. [23] for a complete definition). In their formalism, a Curriculum C = (V, E, \\theta, T) is a directed acyclic graph, where V is the set of vertices, & \u2286 {(x, y)|(x, y) \u2208 V \u00d7 V ^ x \u2260 y} is a set of directed edges, \\theta : V \u2192 P(DT) is a function that associates samples within each vertex, and P(DT) is the powerset of DT.\nIn this paper, we develop what Narvekar et al. [23] call a task-level curriculum, where each vertex v \u2208 V is associated with samples from a single task in T. That is, the mapping function for task m\u00a1 is \\theta : V \u2192 {D{ |m\u00a1 \u2208 T}. For convenience, we will refer to a task's available samples at vertex v as m\u00a1. In other words, a task descriptor \u03b8\u00a1 is used to construct task mi, and a curriculum is a sequence of tasks m1, m2, ..., mtarget up to some target task.\nBefore we describe how we construct this function using task features, we point out some deviations from the model just described. The curriculum being a DAG is a very strong assumption and is not true of the SEBN-guided curriculum. While the episodic MDP model of Narvekar et al. provides a more comprehensible model of curriculum learning, the RL algorithms of this paper actually learn with a discount factor \u03b3, and one could argue that the Partially Observable MDP might be more appropriate. Both changes would be extensions to the simplified MDP model presented here."}, {"title": "2.4 Task Descriptors (Env't+Target Features)", "content": "We will use task features to define \\theta for a task m. This is a common approach to quantify potential transfer between two tasks (e.g., [14, 16, 24, 29]). The notion is that two tasks that share similar features will exhibit better transfer. We adopt a task descriptor similar to Rostami et al. [29] and Narvekar et al. [24].\nSpecifically, we parameterize @ with a vector that consists of set of environment-specific features E and a set of one or more performance targets K. Thus, \\theta((Zo)|E|(Zo)|K|) will indicate the specific task m\u00a1. We will often omit the task descriptor for clarity and just reference mi. Note that the task descriptor is underspecified with respect to the environment, so one configuration of e represents a class of different environments an agent can encounter.\nExample Task Descriptors using Bipedal Walker. The Bipedal Walker (BPW) benchmark [35] involves two-legged agent moving through terrain in a 2D environment.  Here, E consists of five features that control the difficulty of the environment, and there is a single target of moving to the right by at least 30 steps (The dashed latent competencies are defined in \u00a73.1). The task descriptor for this BPW is (pit-gap, stump-height, stair-width, stair-steps, roughness, moved>30). A task me for BPW involves a particular setting of the parameters for \u03b8."}, {"title": "2.5 Extending Targets to Include Options", "content": "One could imagine a richer set of targets in K, even ones that are hierarchically organized with a natural decomposition of subtasks. The options framework [33] is a common model for such situations. Briefly, a subtask j for task mi is an option oj = (Sj, \u03c0j, TERMj), where Sj \u2286 Si are the starting states of the option, rj is used to take action while the option is enabled, and TERMj : Si \u2192 {0, 1} is a function that indicates the option has terminated.\nFor a specific task mi, a subtask m\u00b2 = (Sj, Ai, pi, rj) indicates that an option has a specific context: it works over a set of states S; that are a subset of the tasks states Si, it uses a specific reward independent of the task reward, and it has the same actions and transitions as the original task mi. Each option j is enabled as part of the feature parameters for \u03b8 (i.e., (Zo)|K|)).\nExample Task Descriptors using DoorKey. Suppose we want an agent to learn to navigate in a gridworld environment to a goal while opening locked doors. several possible environments for this agent and their corresponding environment feature vector. In the easiest environment (top left), the agent (the white arrow) starts very near the goal (\"A\") in an empty grid. Adding additional obstacles such as a wall, shown as chess rooks, or a locked door, shown as a lock, with a key to unlock it, adjusts the environmental features accordingly. The first three components of the task descriptor e indicate whether the distance (D) of agent starts near (within 2 squares) of any point of interest (key or goal), the presence of a wall (W), and the presence of a locked door (L).\nDoorKey also enables the use of options. the targets K for this problem, corresponding to ordered subtasks. This problem has three options (at(goal), opened(door), and has(key)), each with its own reward. A distinct policy is learned for each of these options. The last three components of e indicate which of these three subgoals are enabled for a task: getting a key (K), opening a door (O), or being at (A) a cell."}, {"title": "3 BAYESIAN CURRICULUM LEARNING", "content": "The key idea in this section is to use a BN to estimate performance on K over the environmental features from E plus a set of estimated proficiency \u03a8 on latent competencies, which are hidden or unobserved. Before we formally define the SEBN, we describe extend the DoorKey example to discuss this process.\nExample of Latent Competencies. Suppose we have collected past data of the agent's performance on different environments in E. For example, say we ran our agent on the empty-grid (D0 W0 L0), wall-only (D1 W1 L0), and door-only (D1 W0 L1) environments and recorded that the agent was successful on the empty-grid and door-only environments but not the wall-only environment. This failure might be due to the agent not yet knowing how to navigate around walls. We can think of this ability as a latent \"avoid wall\" capability that an agent needs to have mastered to solve tasks that require it. Furthermore, using the notion that there is this latent capability, we can easily predict that the agent will fail on the wall-and-door (D0 W1 L1) environment without having any data of the agent's performance on that specific type of environment.\nThe bottom row of Fig. 4 shows a set of latent competencies or capabilities \u03a8. In this example, we chose four latent competencies: (move, pick up, avoid wall, open) that we expect the agent to need to master to successfully solve different tasks. These latent variables are provided by an expert, similar to Abel et al. [1].\nWe can use the SEBN to predict two important quantities. First, when faced with a new (possibly unseen) task mi \u2208 T, we need to estimate the proficiency of each competency in \u03a8. This is important because competencies will advance at different rates and some tasks will require more proficiency than others for specific competencies.\nSecond, when faced with a new (possibly unseen) task, we need to be able to predict performance on kj \u2208 K given the current estimates of competency level (from the first step). This is important because it can be used to select from a set of candidate tasks for training in the next iteration of a curriculum.\nReturning to Fig. 4, suppose a new task is defined over E and K. The proficiency estimate(s) can be calculated using the links to the bottom row. Once these estimates are provided back to the network, the expected reward can be calculated in the target layer K."}, {"title": "3.1 Competencies (Latent or Explicit)", "content": "As with the \"avoid wall\" latent competency for DoorKey, we propose that there is some shared latent set of competencies of which mastery over can predict an agent's success rate on different metrics in different environments. More concretely, let ki \u2208 K be a set of observable metrics, which can be any measurable target (e.g., a standard reward function, a shaped or partitioned reward function, or the completion an option). For example, we can define a binary variable that is 1 if an agent received a reward of more than a threshold value, and 0 otherwise.\nDrawing inspiration from ECD, we propose that there exists a set of latent competencies \u03a8 that are not directly observable but can be inferred from the observed metrics. These competencies are such that the probability of success on a given observable metric ki of an agent on a specific task mi is dependent on sufficient mastery of the corresponding latent competencies. This relationship allows us estimate the impact of competencies on unseen tasks. A BN allows us to model this relationship, estimate competency levels from data, and subsequently estimate success rates on different environments.\nExplicit competencies can be derived from techniques that decompose a task into subtasks. For example, in hierarchical planning, a method decomposes an abstract task. A set of such methods could be used to construct the competencies. For example, recent work has used hierarchical goal networks to decompose tasks and train RL policies. [26, 27]. They call the resulting policies goal skills. For the SEBN defined in Fig. 4, all four of the competencies in \u03a8 could be defined as an explicit goal skills. In the case of this SEBN, the dependencies in the network are exactly the same as a corresponding hierarchical goal skill network. Consequently, we could take any problem with a heirarchical goal skill network, define environment-conditioned dependencies, and turn it into a SEBN.\nThe flexibility of letting competencies be latent or explicit allows us to model environments where intermediate decompositions may not be well-defined. In the absence of an easy way to check if an agent satisfies a goal for a given goal-skill, then it can be set to be a latent competency in the SEBN."}, {"title": "3.2 Skill Environment Bayesian Network", "content": "We can now define a Skill Environment Bayesian Network (SEBN) for estimating the competency level of an agent and modeling the relationship between agent competency levels, env features, and observable goals/metrics. The SEBN is a tuple {X, D, \\Gamma}, where the variable set X = E \u222a K \u222a \u03a8 is split into three sets of variables:\nEnvironment Variables. E is a set of variables that represent the features of an environment descriptor. Each variable in this set corresponds to a specific feature of the environment (and thus the domain of a given variable is the set of possible values the corresponding environment feature can take). In the gridworld example, E consists of features for wall, door, and distance.\nTarget Variables. K is set of variables that directly correspond one or more targets. If there is a single policy, then K will have a single node, as in Figure 2. But if there are options available to the agent, then each kj \u2208 K corresponds to the option for task m (cf. \u00a72.5). These variables then provide estimates of the value of executing that option in the current environment. For the purposes of this paper, that estimate is thresholded such that each variable returns a boolean value corresponding to whether its estimate meets a performance threshold (roughly corresponding to an estimate of whether the option will succeed or fail).\nCompetency Variables. \u03a8 contains the set of variables that represent the competency levels of an agent. Each variable in this set denotes the level of proficiency an agent has in a particular competency and takes values in a range from {0, 1, ..., N} where N is the highest proficiency level for a given competency. For this paper, we will define competency with two or three levels of proficiency. Competency levels are roughly ordered, as provided in a set of requirement specifications, by a human expert. The rationale for writing these specifications is to convey whether a given environment requires sufficient proficiency in several competencies. Specifications follow (roughly) ordered values of competency (e.g. a higher \"move\" competency should enable harder tasks). The competencies can be latent (e.g., capturing whether an agent avoids obstacles while moving) or explicitly learnable (cf. \u00a73.1).\nThe parent functions \u0393 provide the distribution of possible values of each variable conditioned on their parent variables. To construct these parent functions, we specify a list of competency requirements that is procedurally used to construct the corresponding CPTs. We provide specific detail about this process in \u00a73.3.\nOnce constructed, we can use the SEBN to estimate an agent's competency levels and determine what environments or competencies the agent should focus on learning next. To do this, we must estimate two quantities for a task mi \u2208 T: Competency Level: P(\u03a8 = \u03c8|ki = r, T = mi) - the probability that an agent has a competency level of \u03c8, given its prior performance of at least reward r on target ki for task mi. Expected task reward: P(ki = r|T = mi, \u03a8 = \u03c8) - the probability that an agent can achieve a reward of at least r for target K\u00a1 conditioned on task mi with a given competency level \u03c8.\nWe will estimate the competency levels using past rollouts. We will then apply the estimates of competency levels to estimate the expected task reward over a collection of candidate environments mi \u2208 T. These estimated probabilities will be used to determine which environments the agent should train on next."}, {"title": "3.3 Defining variable distributions", "content": "To complete our network definition, we need to define the distributions of each variable in the network. The distribution of variables in \u03a8 and K are defined using a hierarchical structure. We start by defining the leaves of this structure which are located in \u03a8.\nDefining \u03a8. Consider the \"move\" competency move \u2208 \u03a8 in the gridworld navigation environment. In Fig. 4, we define move as having three levels of proficiency {0, 1, 2}, associated with a corresponding parameter set \\theta_{move} = {0.8, 0.2, 0.0} such that the probability of the \"move\" at competency level j is given by: P(move = j) = \u03b8move (j). In this case, the parameters denote that currently the agent has a move competency of 80% probability for no mastery and a 20% probability of having level one mastery.\nMore generally, \u03a8 can have a hierarchical structure and there can be latent competencies within \u03a8 that depend on other latent competencies. To allow for these hierarchies in \u03a8, we define B to be a subset of \u03a8 which represents a base set of competencies (the leaves). The distribution of these base competencies is determined by a set of associated parameters \u03c6\u0432.\nDefining K. The variables in K represent the success of an agent on a given target. In an SEBN, we seek to model the relationship where the success probability of an agent on a variable ki \u2208 K is dependent on three types of parent variables: (1) the environment features ei \u2208 E relevant to ki (2) the agent's current competency levels \u03c8i \u2208 \u03a8 (3) other targets \u03ba \u2208 K This means that variables in K depend on other variables in \u03a8 and K as well as a set of variables in E. The success of a task depends on the agent's mastery of the competencies required for a given environment configuration and an independent failure rate \u03bb. A task succeeds with a rate of (1 \u2013 \u03bb), if all sub-requirements in K and \u03a8 are satisfied.\nMore concretely, for each relevant environment configuration ei in relation to a goal variable \u03ba\u012f \u2208 K, we define a set of competency level requirements R\u2081\u2081 that an agent must master to successfully complete the task of ki on the environment ei. For example, in Fig. 4, haskey depends on the distance environment feature and the latent move and pickup competencies. Suppose we define the following competency level requirements for the haskey node:\nhaskey: (distance=0 | move=1,pick up = 1)\nhaskey: (distance=1 | move=2, pick up = 1)\nThese state that if the key is close (distance=0), the agent needs a level of proficiency of 1 in the move and pick up competencies to successfully get the key. However, if the key is far (distance=1), the agent needs a higher level of proficiency in the move competency (move=2). The agent should have a high chance of success if it meets all necessary requirements and a high chance of failure otherwise. We directly translate these requirements into entries in the corresponding CPTs in the following way. For the first competency level (distance=0), we have that:\nP(haskey = 1 distance = 0, move = 0, pickup = 0) = \u03bb\nP(haskey = 1 distance = 0, move = 0, pickup = 1) = \u03bb\nP(haskey = 1 distance = 0, move = 1, pickup = 0) = \u03bb\nP(haskey = 1 distance = 0, move >= 1, pickup >= 1) = (1 \u2013 \u03bb)\nFor the next competency level (distance=1), we have that:\nP(haskey = 1 distance = 1, move = 0, pickup = 0) = \u03bb\nP(haskey = 1 distance = 1, move = 0, pickup = 1) = \u03bb\nP(haskey = 1 distance = 1, move = 1, pickup = 0) = \u03bb\nP(haskey = 1 distance = 1, move = 1, pickup = 1) = \u03bb\nP(haskey = 1 distance = 1, move >= 2, pickup >= 1) = (1 \u2013 \u03bb)\nThese state that the agent has a success rate of (1-\u03bb) for a given goal for an environment setting if it satisfies all necessary requirements and a success rate of \u03bb if there is one or more requirement missing.\nFor the environment features set E, the distribution of variables in this set is fully controlled for the purpose of curriculum generation. Therefore, the data (samples obtained from rollouts) can be used as the distribution for variables in E."}, {"title": "3.4 Curriculum through Inference", "content": "The general algorithm to generate an SEBN-guided automated curriculum is as follows. First we define a generation of the algorithm as L rollouts. Let T be the set of possible tasks and PT(mi)t be the probability that task mi = (ei, Ki) is chosen in the current generation t. We first initialize PT(mi) to some initial task distribution. This initial weighting can be biased towards easier tasks or can be set to a uniform distribution for better initial competency estimation. Let B = {..., B\u2081, ...} be the set of parameters associated with the base competencies B in the SEBN. For each generation, we take the following steps:\n(1) Sample L rollouts (mi = (ei, Ki) ~ PT, \u03b8i). For each rollout, we record a set of observable metrics k\u012f.\n(2) Solve the MLE problem:\n\u03a6 = arg \\max_{\\Phi_B} \\Pi_{i=1}^L P(E = e_i, K = k_i) \\label{eq:1}\nfor P, the values of the parameters for the base competencies, which is an estimate of agent proficiency level in those competencies.\n(3) Update the task distribution for the next generation: PT(mi) = F(Po (ki|mi)) where F is some function that maps the estimated success rate of an environment Po (ki|mi) to a probability distribution.\nFor our work, we define F using the following fitness function:\nF(m_i)_t = (P_\\theta (K_i | m_i)_t - P_\\theta (K_i | m_i)_{t-1})^2 \\label{eq:2}\nP_T(m_i)_{t+1} = 0.5 \\cdot \\frac{F(m_i)}{\\sum_{m_j} F(m_j)} + 0.5 \\cdot P_T(m_i)_t.\nIn [31], it was proposed that curricula should focus on problems where the agent improves the most or has the most expected improvement in competence. To simulate this in our approach, we choose a fitness function where the fitness of the environment is a function of the difference between the current estimated success rate and the estimated success rate on the last generation's SEBN. We also add a smoothing factor to improve learning stability.\nNote that our curriculum is agnostic to the choice of learning algorithm and policy which can be assumed to be black boxes. The curriculum only requires observations of rollouts and not the internal reward structure of a given policy."}, {"title": "3.5 Candidate Selection", "content": "On a domain such as Megagrid, we can evaluate Po (Kimi) for every combination of environmental features. However, calculating Po (Kimi) for all possible environments in BipedalWalker took too much time at the end of each rollout generation. In general, for domains with a large amount of environmental features, it is impractical to evaluate Po (Kimi) for every single task mi \u2208 T."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "To demonstrate the effect of our proposed curriculum learning approach, we evaluate the SEBN curriculum on three environments. DoorKey: A MiniGrid [10] inspired domain with explicit intermediate goal skills. In this domain, the agent must learn to navigate through a grid-based environment to reach a goal location, while also learning to achieve intermediate goals along the way. BipedalWalkerHardcore: a simulated bipedal robot must learn to walk forward as quickly as possible while maintaining balance. The bipedal robot can encounter a variety of obstacles such as rough terrain, stumps, pits and stairs that need robust policies. Robosuite: a robotic arm (Kinova Gen 3) must learn to open a door with different weight and latch settings.\nIn each of these domains, we compare the performance of reinforcement learning agents trained with and without our proposed SEBN-guided curriculum as well as two additional controls: Uniform curriculum (or Domain Randomization [34]): all environments have an equal probability of being selected. Anti-curriculum: the probability difference in Eq. 2 is replaced with (1 - difference) and we use a min priority queue for candidate selection during the sample-search algorithm.\nWe evaluate the agents on their ability to learn effective policies that can achieve high rewards in each domain, as well as their ability to generalize to new tasks and environments. Our results show that the SEBN-guided curriculum consistently improves the performance of reinforcement learning agents across all three domains.\nAll runs are performed on an AMD EPYC 7H12 64 core CPU with networks being handled on an A100 GPU.\nDoorKey. We start with a gridworld environment named Megagrid based on MiniGrid [10]; we reimplemented this standard gridworld environment to enable easier generation of environments using the task descriptor\u00b9. We evaluate on the simple DoorKey"}, {"title": "5 RELATED WORK", "content": "Automated Curriculum Generation. Several topics relate to ordering tasks to improve learning performance. A few approaches have considered the problem of estimating agent skill competencies. In the context of education, in addition to ECD [21], another approach close to ours is that of Green et al. [11], who used a BN to determine the next task for the human student. This approach is similar to [18], which considered an active learning problem in a robotics domain of choosing which skills to practice to maximize future task success, which involves estimating the competence of each skill and situating it in the task distribution through competence-aware planning. In contrast to our approach, they employ a simplified Bayesian time series model that does not relate environmental features with goal and skill competencies. This limits the applicability of their approach towards only choosing what skill to train and not the agent's environment. Similar to our selection process, Ballera et al. [4] used a roulette wheel selection of tasks.\nA related area is the literature on Unsupervised Environment Design (UED) [9] and other developed mechanisms for curating environments based on a regret heuristic [15]. In prior UED approaches such as PAIRED [9], the agent's curriculum is generated using a regret-based heuristic. The heuristic is typically an estimate of the true regret, since the optimal policy is unknown. In PAIRED, this heuristic is calculated by learning an antagonistic policy and evaluating the difference between it and the protagonist policy. In contrast, like ACCEL [25], our method does not need to learn a second antagonistic policy and instead uses rollouts from a single agent to compute the next part of the curriculum. In contrast to ACCEL, our curriculum does not rely on local changes and can incorporate larger jumps in environment selection. Furthermore, while it is necessary to use rollouts on each environment for ACCEL to obtain a regret estimate, we can estimate success rates on unseen environments by leveraging the relationships encoded between competencies and environmental features in our SEBN.\nTask Descriptors. As mentioned in \u00a72.4, grouping tasks using features, i.e., task descriptors, are a well understood technique for task creation ([14, 16, 24, 29]). The key idea in these works is to facilitate learning transfer by creating similar tasks that share common features. These features can leave certain variables free during task construction that enable a family of similar tasks. Our work supplements prior work by adding the target of the task to the task descriptor, allowing the curriculum to emphasize subtasks. To our knowledge, there has been limited previous work in integrating curriculum learning on both skills and environment features.\nHowever, it can be said that our research expands on the concept of using task descriptors in the creation of automated curricula. In [27], a task-graph curricula is used to generate a curriculum over tasks and environmental features. However, they employ a simple greedy best-first search on the task-graph to choose an order for their curriculum. This is different from our approach that updates a distribution over the task-graph and dynamically adjusts this distribution based on data from rollouts.\nHierarchical Goal Networks. The structure of our Skill Environment Bayesian network shares similarity to both goal skill networks and fault diagnosis networks. In fault diagnosis networks [6], BNs are used to model the relationship between a set of sensors and a set of faults. In our case, the sensors are analogous to an SEBN's observable goal metrics, and the faults are analogous to an SEBN's skills. An SEBN can then be seen as a fault diagnosis network where different roll-outs are independent tests that determine what latent competencies may have not been mastered.\nExpert guidance for RL training. One of the limitations of the SEBN is its reliance on the expert-provided competencies. As noted, these could be derived from hierarchical approaches. But providing domain knowledge is common in many hierarchical RL settings. Similar to our work, Patra et al. [26], provided a hierarchical learning structure. This kind of expert knowledge is common in imitation learning (e.g., [37] [13] [19]), where an expert human guides a learning agent. Providing expert guidance is also common in Hierarchical RL approaches (e.g., [2]) and in standard RL approaches (e.g., [3]). Finally, expert guidance was shown to be helpful for a sparse-reward task in an Object Oriented MDP setting [1]."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "We presented a novel method for generating curriculum over environmental features using a Skill-Environment Bayesian Network. This network is used to estimate agent competency level based off of past rollouts and can be used to infer estimated agent success rates on unseen environments. We demonstrate the effectiveness of this approach on a variety of domains.\nFor this work, we relied on a pre-defined set of skills and environmental features. In future work, we would like to extend the model to handle more open ended environments where we can add new environmental features or agent skills dynamically to the SEBN. It might be interesting to see if we can apply techniques such as GO-MTL [17] for learning a latent space over tasks and approaches for detecting critical regions [22] to learn new skills.\nThere has also been a great deal of interest in the use of Large Language Models (LLMs) in planning domains for the purpose of automatically generating planning models. As SEBNs can be built from a heirarchical goal network, it might stand to reason that LLMs could also be used to automatically generate SEBNs from domain documents. Since there is much more leeway in the skills required (since our method supports latent competency nodes), it may be easier to generate these SEBNs than equivalent goal networks."}]}