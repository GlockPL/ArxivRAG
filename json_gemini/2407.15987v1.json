{"title": "AI FOR HANDBALL: PREDICTING AND EXPLAINING THE 2024 OLYMPIC GAMES TOURNAMENT WITH DEEP LEARNING AND LARGE LANGUAGE MODELS", "authors": ["Florian Felice"], "abstract": "Over summer 2024, the world will be looking at Paris to encourage their favorite athletes win the Olympic gold medal. In handball, few nations will fight hard to win the precious metal with speculations predicting the victory for France or Denmark for men and France or Norway for women. However, there is so far no scientific method proposed to predict the final results of the competition. In this work, we leverage a deep learning model to predict the results of the handball tournament of the 2024 Olympic Games. This model, coupled with explainable AI (XAI) techniques, allows us to extract insightful information about the main factors influencing the outcome of each match. Notably, xAI helps sports experts understand how factors like match information or individual athlete performance contribute to the predictions. Furthermore, we integrate Large Language Models (LLMs) to generate human-friendly explanations that highlight the most important factors impacting the match results. By providing human-centric explanations, our approach offers a deeper understanding of the AI predictions, making them more actionable for coaches and analysts.", "sections": [{"title": "1 Introduction", "content": "The Olympic Games are one of the main sports events in the world with around 3,000 athletes from 80 countries competing in 40 different sports. One of the team-based sports is handball, for which the Olympic tournament is considered as the most prestigious competition.\nThe 2024 edition has a particular flavor for handball because they are hosted in Paris, France where the national teams for both men and women are considered as favorites. Indeed, both teams won the gold medal in the last 2020 occurrence in Tokyo, Japan. In this work, we propose a prediction model which leverages statistics, deep learning and large language models (LLM) to predict and explain the results of men and women handball tournaments. Previous work from [1] proposed to compare different approaches and concluded that tree-based models provide the best predictive performance. We however realize that all features are numeric which can make it challenging to interpret for sports experts. In this work, we will explore an alternative to better account for the composition of the teams by adding the lineup information. Furthermore, with the recent raise of artificial intelligence (AI), and LLMs at the forefront, more tools can assist sports experts interact with complex predictive solutions. We view AI as a new layer to act between our complex set of predictive, explainable solutions and sports experts such as coaches. By proposing an AI based solution, which better accounts for the composition of the teams and generate human-friendly outcomes and explanations, we aim to make these advanced predictive solutions accurate and actionable."}, {"title": "1.2 Literature review", "content": "To predict the results of handball matches, few different methods exist. [2] proposed a univariate approach by predicting the score difference between the two teams. They leverage statistical techniques (such as the Skellam distribution and copulas) to compensate for the challenge of non equi-dispersed target variables. Focusing on the prediction of the scores of both teams, [3] proposed a lasso regression with a constant and low variance to account for under-dispersion. With the same objective of predicting the score of the two teams, [1] proposed a Machine Learning (ML) approach accounting for different factors that can impact the score. In particular, the approach uses the estimation of teams' strengths modelled by a Conway-Maxwell-Poisson distribution [4]. In this work, we will extend this work by provided a deep learning based approach to better account for teams' compositions and provide the lineups as a categorical input feature.\nTo account for player's information, some approaches [5] use Natural Language Processing by leveraging Transformers [6] to model sequences of events in a match. This approach allows to learn a numerical representation of the players which can be assimilated as playing behavior.\nThe Transformer architecture has been key in driving the recent revolution of Artificial Intelligence lead by the major advances of LLMs. Large Language Models have drastically impacted the field of data science and now allow a wider audience to access and interact with AI systems.\nDespite their large success in the industry\u00b2, Large Language Models are not widely adopted in the field of sports analytics yet.\nTo date, in the domain of sports, Large Language Models mostly appear for benchmarking existing models on sports tasks [8, 9] and evaluate their reasoning and sports knowledge. Some benchmarks go further and are used to evaluate the capabilities to summarize play-by-play match events [9].\nThese models are also known for their summarization capabilities which can be leveraged to automatically summarize information from the news [10]. Building on this strength, [11] proposed a workflow to generate automatic journalistic insights from sports news. Such tool can act as a first step towards closing the gap between advanced analytics and sports fans and experts.\nIn this work, we leverage three core components of AI to generate handball match predictions with insightful comments. We use deep neural networks [12] to predict the score of two teams in a match based on several attributes. We then use explainability methods to augment the predictions with an understanding of how the different covariates lead to the predicted score. Finally, to make these predictions and explanations human-friendly, we leverage a Large Language Model\nThis document is therefore structured as follows. Section 2 details the data used and the core methodology composed of the three AI components. Section 3 presents the final results with the predictions of the 2024 Olympic tournament for men and women. It also illustrates the AI-generated explanations for an important match: the final of the tournament. In Section 4, we discuss the limitations, potential extensions of this work before concluding."}, {"title": "2 Materials and methods", "content": "Our approach aims to predict the number of goals scored by the home (denoted with the subscript h) and away (with subscript a) team. We design a Machine Learning model that learns from past matches to understand how the different factors will impact the score of the teams. In this section, we first present the data used for training our model and detail the different types of features it uses. Next, we present the learning approach. It is based on a neural network which uses information from another model trained on clubs' matches as pre-trained information for initialization. Last, we show how we can combine xAI techniques and LLMs to explain the predictions of our model in a human-friendly way."}, {"title": "2.1 Data", "content": "In our setup, we design a Machine Learning model for a multi-target regression which aims to predict two outcomes $(Y_h, Y_a) \\in \\mathbb{R}^2$. We denote $y_h$ the number of goals scored by the home team at the end of a match and $y_a$ is the score of the away team.\nWe build our dataset using the handball API from SportDevs. The dataset is multi-modal and combines numerical and textual data. We thus define four different feature types. The first three feature types for the numerical covariates are: match information, teams information and teams strengths. The last feature type is textual and contains the teams lineups.\nMatch information These numerical features aim to carry information about the match and its importance. It can help gather information such as potential stress of players.\n\u2022 Day of week: encoded day of the week for the start time of the match.\n\u2022 Hour: hour of the start time of the match. We can expect that matchs starting early in the day (e.g., morning) can be less important or teams may lack time for preparation.\n\u2022 Importance: carries the importance of the competition from the lowest (friendly games with value 4) to the highest importance (Olympic Games with value 10). Detailed values are presented in Table 4 in Appendix 5.2.\nTeams information We incorporate inherent information of the teams regarding their composition, homogeneity and propensity to fatigue.\n\u2022 Travel distance: distance in kilometers (as the crow flies) to travel for the home and away teams between the teams locations and the match location. This aims to capture the potential fatigue caused by the travelling distance.\n\u2022 Number of clubs: number of clubs in which each player of the team are playing. A low number suggests that majority of players are from the same clubs and are used to playing together.\nTeams strengths We add features that correspond to the teams' strengths as described in [4]. These covariates in the spirit of Statistically Enhanced Learning (SEL, [13]) aim to estimate the offensive and defensive strengths of both teams.\n\u2022 Attack strength: estimated strength in attack via SEL for home and away teams.\n\u2022 Defense strength: estimated strength in defense via SEL for home and away teams.\nTeams lineups The last feature type corresponds the textual covariate to incorporate the composition of the teams. We note that, depending on the competition, teams can include up to 16 players\u00b3 per match.\n\u2022 Home lineup: list of (up to 16) players for the home team present on the match report.\n\u2022 Away lineup: list of (up to 16) players for the away team present on the match report.\nThis feature requires to know the composition of the teams prior to a match. To be ingested by the model defined in Section 2.2, the list of players of the home and away teams are concatenated to produce one vector of up to 32 players. This vector needs to be pre-processed to generate a numerical vector composed of word tokens. Our tokenizer simply considers the full name of the player as one token.\nThe main challenge remains the amount and the quality of data. The lineups are not systematically recorded and made available in the API, therefore we use imputation tricks to backfill matches with missing lineups. In particular, for each team, we use the last lineup available to fill the missing lineups of the season. Furthermore, we note that the data for modelling club matches are filtered such that lineups are not empty. We can only enforce such a logic for clubs because of the amount of data available (e.g. for men the training data goes from 55,414 observations to 16,450 data points once filtered, meaning that 38,964 have no lineup information). For national teams, we allow empty lineups meaning that the model will ingest a vector of null values. The null value is already a token by itself (with value = 0), so the model will learn that a vector full of O's is a specific form of a team.\nWe observe the large difference of data available between national teams and clubs as presented in Table 1. We have much more data for clubs than for national teams with a scale for 6.1 times more for women to 8.9 times more for men. This large difference of available training data will impact the prediction performance. We will thus use transfer learning as a mitigation technique to benefit from the amount of data available for clubs."}, {"title": "2.2 Neural networks and transfer learning", "content": "Embedding Multi-Layered Perceptron Even though the deep learning approaches did not exhibit the best perfor- mance amongst the compared methods in [1], neural networks still benefit from an interesting capability we want to build upon. We want to use the capabilities of these models to handle multi-modal data for explainability purposes. Indeed, in [1], to exploit the information on the teams' compositions, the tree-based solutions need to ingest numerical features. The generated features thus included information such as the average and standard deviation of age or height of players per position. However, when it comes to interpreting the predictions, sports experts gave us the feedback that they had hard times finding a sports meaning from these numerical covariates. They would instead prefer to be able to identify the name of the players to understand their direct impact on the match.\nBuilding on this important feedback, we update the architecture of the neural network model exposed in [1] to better incorporate explainable teams' composition.\nAs we introduced in Section 2.1, we keep the main features types about match and teams information along with strengths. To these features, we now add the lineups via an embedding layer as depicted in Figure 1.\n\nThe embedding layer ingests the concatenated lineup vector $[h_1,\\ldots, h_j, a_1,\\ldots,a_j] \\in \\mathbb{N}^{2j}$ and maps the values to a continuous vector of fixed size in $\\mathbb{R}^m$. Considering the maximum number of players in different competitions, we set $j = 16$. After tuning hyper-parameters of the model, we set the output dimension $m = 25$, meaning that each player has a vector representation in $m = 25$ dimensions.\nThe weights of this layer are usually initialized with random values pulled from a standard normal distribution $\\mathcal{N}(0, 1)$ [14]. However, as we will see in the next paragraph, initializing with non-random weights from a pre-trained model via transfer learning can help the training process.\nDuring the training process, the embedding layer learns a numerical representation of each player that corresponds to their intrinsic impact on the match results. We note that the model does not have information about the player's performance itself (number of goals, etc.). Instead, it learns how the team performs when the player is present in the lineup. In other words, it learns a form of plus-minus rating of players [15] but in an implicit manner.\nTransfer Learning To mitigate the difference in data between clubs and national teams settings, we use a transfer learning approach. Given that players in national teams also necessarily have records in the clubs data set, we want to leverage the information learnt by the clubs model to support the training of the model for national teams. Moreover, since we have a larger data set for clubs, and potentially of better quality (since filtered), we expect that the transfer of information will help boost the performance of the model for national teams.\nWe use the approach called \"Network-based deep transfer learning\" [16]. When initializing the weights of our international model, the weights of the embedding layer are no longer pulled from a normal distribution as the default behavior. Instead, we set the weights with the values learnt from the pre-trained clubs model. Since players from national teams were also present in the data when pre-training the clubs model, we expect that their numerical representation will be similar in both models. The model will then adjust the values during the training process to account for the impact of players in international competitions.\nThis implies that some non-international players will still have a numerical representation, since available in the pre-training stage, but their weights will no longer be of interest. We also note that we cannot initialize all the weights of the model from the pre-trained one since the rest of the numerical features are not the same between clubs and international teams models. For instance, the clubs' model considers the share of international players that will be a constant equal to 100% for the international model. On the other hand, the international model considers the number of clubs that we be equal to 1 for the clubs model.\nThis approach aims to provide some initial knowledge to our model when starting the training so it can benefit from the information learnt by the pre-trained model and later exhibit better performance than with a random initialization."}, {"title": "2.3 Large Language Model for human readable explanations", "content": "The core objective of this work is not only to provide accurate predictions but also to suggest meaningful explanations of the match. We play with the gradient-based nature of neural network to use Integrated Gradients to generate local explanations [17]. This method is used for attributing a score on the model's input features based on their impact on a prediction. It works by accumulating the gradients interpolated along a straight-line path from a defined baseline input to the actual input value and integrates the accumulated values. The resulting attributions provide a measure of the feature's importance that sums to the difference between the model's output for the input and the baseline.\nThe explanations generated by this method can be hard to read and to interpret for humans. Additionally, the relative high cardinality the input features\u2074 can make this task even more difficult. To exploit such information, only experts in both handball and explainable Machine Learning would be able to derive meaningful information. In other words, the raw results without an interpretation layer are of little interest for coaches. It would require massive educational efforts to give them the required skills.\nTo mitigate this issue, we use a Large Language Model to extract meaningful information in a short and human-readable summary. In particular, we use the open-source model Mistral 7B instruct [18]. The model was trained on a large corpus of text available from the internet corresponding to approximately 600 billion words. This gives the ability to the model to reason on a large set of topics. To afford running a LLM on relatively small server (with 8GB of GPU memory), we use a quantized version of the model which allows to compress it and limit its memory footprint [19]. We leverage the reasoning and summarization capabilities of the model to generate a comprehensive summary of the match based on the predictions and explanations.\nTo generate the match summary, we structure the prompt in three part: inputs, examples and instructions. The final prompt template is available in Appendix 5.1.\nLLM inputs The output of the prediction model and of the xAI function become the input of our LLM. We first give contextual information to the model about the teams, date of the match, the type of competition and the predicted score. The LLM knows the expected outcome and can adjust its explanations based on the results and the overall match context. Then, we provide information about the covariates of the model. For each of the features, we provide a short description of what it would mean to have a positive attribution score. This gives more structure to the model and guides it derive meaningful conclusions. We finally add the generated feature attributions derived from the xAI method that will be the main source of information for the LLM to generate the match summary. To each player name, we add the position and the team of the player along with the estimated attribution.\nFew-shot prompting To ensure that the LLM sticks to the desired structure, we provide few examples in the prompt. This approach, known as in-context learning [20], uses examples in the context (prompt) to help the model learn the structure of the expected output and what type of reasoning is expected. We then take few examples of past matches and manually generate reports as we would expect the LLM to generate them. We ensure that the different examples we generate are consistent in the format and type of conclusions we want to derive so the model does not get confused. However, we try to make them diversified so it can learn the variety of output we expect and does not repeat non-informative patterns by just mimicking the examples.\nSummarization instructions The final important step is to provide the exact instructions to the LLM. While we already provided the different inputs and examples, we need to formalize and write down how we want the model to behave and what we want it to generate. We instruct the model on how to interact with the provided inputs and how to use the examples. We also specify what kind of conclusions and reasoning we expect. This step is crucial to limit the risk of hallucination and guarantee the accuracy of the response [21]."}, {"title": "3 Results", "content": "To illustrate the performance of our solution, we first evaluate the prediction model on a test set for international competitions and analyze the change in performance using transfer learning. We next present the expected results for the 2024 Olympic Games tournament using the presented model. Finally, we present an example of model explanation using the presented Large Language Model."}, {"title": "3.1 Prediction performance", "content": "First, we notice that the performance presented in Table 2 shows higher errors than what is presented in [1] because we now evaluate the performance for international competitions and not for clubs. This performance gap is simply explained by the lower volume of data for national teams, an issue for which the transfer learning approach aims to mitigate."}, {"title": "3.2 Paris 2024 predictions for handball tournament", "content": "For the 2024 Olympic Games, France is considered as the favorite team for both the men and women handball tournaments. Furthermore, the last international competitions before the 2024 Olympiads saw France get the gold medal. Indeed, for the men's European championships in January 2024, France defeated Denmark in final (33 - 31 after extra time) and became European champions. One month before, France became world champions after winning in front of Norway in the women's world championships (31 - 28).\nIt is worth mentioning that both French teams also got the gold medal at the 2020 Olympic Games in Tokyo, strengthen- ing the favorite status.\nWe report the final results from the semi-final and finals using our deep learning solution in Table 3. As expected, we can find the finalists of the last competitions on the podium of the competition.\nAnalyzing the ranking for the men's tournament in Table 3a, we expect to see a semi-final between the 2 leading teams France and Denmark, ending in the favor of the local host. The final will then see France face off against Croatia to end with a new Olympic title for the French team.\nOn the women's side, as we can observe in Table 3b, we expect the repetition of the last world championships where France will play against Norway which will also lead to a gold medal for France. On the bronze final, we expect that Sweden will take their revenge from the bronze final of the 2023 world championships and win against Denmark.\nWe present more detailed results in Appendix 5.3."}, {"title": "3.3 AI-generated explanations", "content": "To go beyond the predictions of the number goals scored by each team, we look at the drivers of the prediction. For the sake of illustration, we focus on the final match of the men's tournament between France and Croatia. We expect to see France win the goal medal with a score of 35-24 (see Figure 3).\nTo have a better understanding about the drivers of the prediction, we first generate the feature attributions using Integrated Gradients from the deep neural network as presented in Section 2.2. This local explainability method helps us understand how the model uses the different input features to predict the score. Focusing on one team, say France, we plot the generated attribution values per feature in Appendix 5.3.3. We can see that the relatively large amount of features make the plot interesting only in case we want to dive into more details and potentially compare one feature with another.\nTo generate a human-friendly summary of the match, we ask the LLM to take the expert translator role and summarize the attribution scores we extracted. We report the AI-generated output in Listing 1.\nWe can observe that the model correctly picks on the context of the match. It mentions the match in the Olympic Games between France and Croatia with a final score of 35-24 in favor of France. It does not mention the final because, to date, we do not have a way to include such information in a reliable way yet. Instead, it is able to understand the high importance that Olympic Games represent in handball, as the most prestigious competition. We can also notice that the model highlights facts related to Dika Mem and Elohim Prandi who are key actors of the French strategies. Additionally, the LLM mentions the name of the wing players for the ability to play fast breaks which aligned with the reputation of the team's dynamics.\nThe response finally highlights the impact of some Croatian players such as the center back Igor Karacic, who was part of the All-Star team of the 2020 European championships. Unsurprisingly, the conclusion is that the match will be rather in favor of France and the key element of the victory will be about the ability to capitalize on the strong defense."}, {"title": "4 Conclusion", "content": "In this work, we leveraged different AI tools to predict, explain and generate human-friendly explanations for the handball tournament at the 2024 Olympic Games. The prediction model is based on a deep neural network architecture and is articulated around transfer learning to benefit from the knowledge acquired by another model pre-trained on clubs data. Though we face challenges because of the volume of data available, this approach allows us to generate reasonable outcomes that are in line with sports experts expectations.\nTo further improve the performance of the prediction model, continuing the data collection to improve the quality as well as the quantity will be a key enabler. This will imply heavy manual data collection tasks or the identification of improved data sources, though the bottleneck is often coming directly from the handball federations which do not have a systematic reporting in some competitions (in particular friendly matches).\nAnother path for improvement would be to develop a dedicated model that will be used to feed the embedding of the final prediction model. In particular, if data allow, one could build a model that focuses on predicting the performance and statistics of the players during a match (e.g. goals scored and blocks). This model's embedding will be explicitly focused on players performance and can be another way of pre-training. As mentioned in the previous paragraph, this is dependent on the data quantity and quality made available by the different federations.\nFocusing on the improvements of the AI-generated explanations, another step would be to further explore the model selection. In this work, we leveraged a quantized version of Mistral 7B Instruct to lower the computational footprint. Indeed, in practice, only few clubs or federations would have the required budget to afford paying a dedicated API, fine-tuning their own model or even having the necessary compute resources. All of the experiments were made on a relatively small server for this exact consideration. Additional compute resources would allow bigger models, more detailed instructions in the prompt template or potentially fine-tune the model on sports related datasets.\nOur proposed solution leverages advanced AI technologies to generate accurate and meaningful predictions applied to handball at the 2024 Olympic Games. Not only it allows to generate accurate predictions about the final score of a match but it also comes up with a detailed explanations that can be shared with sports experts and coaches so they can better understand how they would need to adapt their strategy for the match."}, {"title": "Disclaimer", "content": "This work is not related to Amazon."}]}