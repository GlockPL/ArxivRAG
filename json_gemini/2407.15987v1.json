{"title": "AI FOR HANDBALL: PREDICTING AND EXPLAINING THE 2024\nOLYMPIC GAMES TOURNAMENT WITH DEEP LEARNING AND\nLARGE LANGUAGE MODELS", "authors": ["Florian Felice"], "abstract": "Over summer 2024, the world will be looking at Paris to encourage their favorite athletes win\nthe Olympic gold medal. In handball, few nations will fight hard to win the precious metal with\nspeculations predicting the victory for France or Denmark for men and France or Norway for women.\nHowever, there is so far no scientific method proposed to predict the final results of the competition.\nIn this work, we leverage a deep learning model to predict the results of the handball tournament\nof the 2024 Olympic Games. This model, coupled with explainable AI (XAI) techniques, allows\nus to extract insightful information about the main factors influencing the outcome of each match.\nNotably, xAI helps sports experts understand how factors like match information or individual athlete\nperformance contribute to the predictions. Furthermore, we integrate Large Language Models (LLMs)\nto generate human-friendly explanations that highlight the most important factors impacting the\nmatch results. By providing human-centric explanations, our approach offers a deeper understanding\nof the AI predictions, making them more actionable for coaches and analysts.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Motivation", "content": "The Olympic Games are one of the main sports events in the world with around 3,000 athletes from 80 countries\ncompeting in 40 different sports. One of the team-based sports is handball, for which the Olympic tournament is\nconsidered as the most prestigious competition.\nThe 2024 edition has a particular flavor for handball because they are hosted in Paris, France where the national teams\nfor both men and women are considered as favorites. Indeed, both teams won the gold medal in the last 2020 occurrence\nin Tokyo, Japan. In this work, we propose a prediction model which leverages statistics, deep learning and large\nlanguage models (LLM) to predict and explain the results of men and women handball tournaments. Previous work\nfrom [1] proposed to compare different approaches and concluded that tree-based models provide the best predictive\nperformance. We however realize that all features are numeric which can make it challenging to interpret for sports\nexperts. In this work, we will explore an alternative to better account for the composition of the teams by adding the\nlineup information. Furthermore, with the recent raise of artificial intelligence (AI), and LLMs at the forefront, more\ntools can assist sports experts interact with complex predictive solutions. We view AI as a new layer to act between our\ncomplex set of predictive, explainable solutions and sports experts such as coaches. By proposing an AI based solution,\nwhich better accounts for the composition of the teams and generate human-friendly outcomes and explanations, we\naim to make these advanced predictive solutions accurate and actionable."}, {"title": "1.2 Literature review", "content": "To predict the results of handball matches, few different methods exist. [2] proposed a univariate approach by predicting\nthe score difference between the two teams. They leverage statistical techniques (such as the Skellam distribution and\ncopulas) to compensate for the challenge of non equi-dispersed target variables. Focusing on the prediction of the\nscores of both teams, [3] proposed a lasso regression with a constant and low variance to account for under-dispersion.\nWith the same objective of predicting the score of the two teams, [1] proposed a Machine Learning (ML) approach\naccounting for different factors that can impact the score. In particular, the approach uses the estimation of teams'\nstrengths modelled by a Conway-Maxwell-Poisson distribution [4]. In this work, we will extend this work by provided\na deep learning based approach to better account for teams' compositions and provide the lineups as a categorical input\nfeature.\nTo account for player's information, some approaches [5] use Natural Language Processing by leveraging Transformers\n[6] to model sequences of events in a match. This approach allows to learn a numerical representation of the players\nwhich can be assimilated as playing behavior.\nThe Transformer architecture has been key in driving the recent revolution of Artificial Intelligence lead by the major\nadvances of LLMs. Large Language Models have drastically impacted the field of data science and now allow a wider\naudience to access and interact with AI systems.\nDespite their large success in the industry\u00b2, Large Language Models are not widely adopted in the field of sports\nanalytics yet.\nTo date, in the domain of sports, Large Language Models mostly appear for benchmarking existing models on sports\ntasks [8, 9] and evaluate their reasoning and sports knowledge. Some benchmarks go further and are used to evaluate\nthe capabilities to summarize play-by-play match events [9].\nThese models are also known for their summarization capabilities which can be leveraged to automatically summarize\ninformation from the news [10]. Building on this strength, [11] proposed a workflow to generate automatic journalistic\ninsights from sports news. Such tool can act as a first step towards closing the gap between advanced analytics and\nsports fans and experts.\nIn this work, we leverage three core components of AI to generate handball match predictions with insightful comments.\nWe use deep neural networks [12] to predict the score of two teams in a match based on several attributes. We then use\nexplainability methods to augment the predictions with an understanding of how the different covariates lead to the\npredicted score. Finally, to make these predictions and explanations human-friendly, we leverage a Large Language\nModel\nThis document is therefore structured as follows. Section 2 details the data used and the core methodology composed of\nthe three AI components. Section 3 presents the final results with the predictions of the 2024 Olympic tournament for\nmen and women. It also illustrates the AI-generated explanations for an important match: the final of the tournament.\nIn Section 4, we discuss the limitations, potential extensions of this work before concluding."}, {"title": "2 Materials and methods", "content": "Our approach aims to predict the number of goals scored by the home (denoted with the subscript h) and away (with\nsubscript a) team. We design a Machine Learning model that learns from past matches to understand how the different\nfactors will impact the score of the teams. In this section, we first present the data used for training our model and detail\nthe different types of features it uses. Next, we present the learning approach. It is based on a neural network which\nuses information from another model trained on clubs' matches as pre-trained information for initialization. Last, we\nshow how we can combine xAI techniques and LLMs to explain the predictions of our model in a human-friendly way."}, {"title": "2.1 Data", "content": "In our setup, we design a Machine Learning model for a multi-target regression which aims to predict two outcomes\n$(Y_h, Y_a) \\in \\mathbb{R}$. We denote $y_h$ the number of goals scored by the home team at the end of a match and $y_a$ is the score of\nthe away team.\nWe build our dataset using the handball API from SportDevs. The dataset is multi-modal and combines numerical\nand textual data. We thus define four different feature types. The first three feature types for the numerical covariates"}, {"title": "Match information", "content": "These numerical features aim to carry information about the match and its importance. It can\nhelp gather information such as potential stress of players.\n\u2022 Day of week: encoded day of the week for the start time of the match.\n\u2022 Hour: hour of the start time of the match. We can expect that matchs starting early in the day (e.g., morning)\ncan be less important or teams may lack time for preparation.\n\u2022 Importance: carries the importance of the competition from the lowest (friendly games with value 4) to the\nhighest importance (Olympic Games with value 10). Detailed values are presented in Table 4 in Appendix 5.2."}, {"title": "Teams information", "content": "We incorporate inherent information of the teams regarding their composition, homogeneity and\npropensity to fatigue.\n\u2022 Travel distance: distance in kilometers (as the crow flies) to travel for the home and away teams between\nthe teams locations and the match location. This aims to capture the potential fatigue caused by the travelling\ndistance.\n\u2022 Number of clubs: number of clubs in which each player of the team are playing. A low number suggests\nthat majority of players are from the same clubs and are used to playing together."}, {"title": "Teams strengths", "content": "We add features that correspond to the teams' strengths as described in [4]. These covariates in\nthe spirit of Statistically Enhanced Learning (SEL, [13]) aim to estimate the offensive and defensive strengths of both\nteams.\n\u2022 Attack strength: estimated strength in attack via SEL for home and away teams.\n\u2022 Defense strength: estimated strength in defense via SEL for home and away teams."}, {"title": "Teams lineups", "content": "The last feature type corresponds the textual covariate to incorporate the composition of the teams.\nWe note that, depending on the competition, teams can include up to 16 players\u00b3 per match.\n\u2022 Home lineup: list of (up to 16) players for the home team present on the match report.\n\u2022 Away lineup: list of (up to 16) players for the away team present on the match report.\nThis feature requires to know the composition of the teams prior to a match. To be ingested by the model defined in\nSection 2.2, the list of players of the home and away teams are concatenated to produce one vector of up to 32 players.\nThis vector needs to be pre-processed to generate a numerical vector composed of word tokens. Our tokenizer simply\nconsiders the full name of the player as one token.\nThe main challenge remains the amount and the quality of data. The lineups are not systematically recorded and made\navailable in the API, therefore we use imputation tricks to backfill matches with missing lineups. In particular, for each\nteam, we use the last lineup available to fill the missing lineups of the season. Furthermore, we note that the data for\nmodelling club matches are filtered such that lineups are not empty. We can only enforce such a logic for clubs because\nof the amount of data available (e.g. for men the training data goes from 55,414 observations to 16,450 data points once\nfiltered, meaning that 38,964 have no lineup information). For national teams, we allow empty lineups meaning that the\nmodel will ingest a vector of null values. The null value is already a token by itself (with value = 0), so the model will\nlearn that a vector full of O's is a specific form of a team.\nWe observe the large difference of data available between national teams and clubs as presented in Table 1. We have\nmuch more data for clubs than for national teams with a scale for 6.1 times more for women to 8.9 times more for\nmen. This large difference of available training data will impact the prediction performance. We will thus use transfer\nlearning as a mitigation technique to benefit from the amount of data available for clubs."}, {"title": "2.2 Neural networks and transfer learning", "content": ""}, {"title": "Embedding Multi-Layered Perceptron", "content": "Even though the deep learning approaches did not exhibit the best perfor-\nmance amongst the compared methods in [1], neural networks still benefit from an interesting capability we want to\nbuild upon. We want to use the capabilities of these models to handle multi-modal data for explainability purposes.\nIndeed, in [1], to exploit the information on the teams' compositions, the tree-based solutions need to ingest numerical\nfeatures. The generated features thus included information such as the average and standard deviation of age or height\nof players per position. However, when it comes to interpreting the predictions, sports experts gave us the feedback that\nthey had hard times finding a sports meaning from these numerical covariates. They would instead prefer to be able to\nidentify the name of the players to understand their direct impact on the match.\nBuilding on this important feedback, we update the architecture of the neural network model exposed in [1] to better\nincorporate explainable teams' composition.\nAs we introduced in Section 2.1, we keep the main features types about match and teams information along with\nstrengths. To these features, we now add the lineups via an embedding layer as depicted in Figure 1."}, {"title": "Transfer Learning", "content": "To mitigate the difference in data between clubs and national teams settings, we use a transfer\nlearning approach. Given that players in national teams also necessarily have records in the clubs data set, we want to\nleverage the information learnt by the clubs model to support the training of the model for national teams. Moreover,\nsince we have a larger data set for clubs, and potentially of better quality (since filtered), we expect that the transfer of\ninformation will help boost the performance of the model for national teams.\nWe use the approach called \"Network-based deep transfer learning\" [16]. When initializing the weights of our\ninternational model, the weights of the embedding layer are no longer pulled from a normal distribution as the default\nbehavior. Instead, we set the weights with the values learnt from the pre-trained clubs model. Since players from national\nteams were also present in the data when pre-training the clubs model, we expect that their numerical representation\nwill be similar in both models. The model will then adjust the values during the training process to account for the\nimpact of players in international competitions.\nThis implies that some non-international players will still have a numerical representation, since available in the\npre-training stage, but their weights will no longer be of interest. We also note that we cannot initialize all the weights\nof the model from the pre-trained one since the rest of the numerical features are not the same between clubs and\ninternational teams models. For instance, the clubs' model considers the share of international players that will be a\nconstant equal to 100% for the international model. On the other hand, the international model considers the number of\nclubs that we be equal to 1 for the clubs model.\nThis approach aims to provide some initial knowledge to our model when starting the training so it can benefit from the\ninformation learnt by the pre-trained model and later exhibit better performance than with a random initialization."}, {"title": "2.3 Large Language Model for human readable explanations", "content": "The core objective of this work is not only to provide accurate predictions but also to suggest meaningful explanations\nof the match. We play with the gradient-based nature of neural network to use Integrated Gradients to generate local\nexplanations [17]. This method is used for attributing a score on the model's input features based on their impact on a\nprediction. It works by accumulating the gradients interpolated along a straight-line path from a defined baseline input\nto the actual input value and integrates the accumulated values. The resulting attributions provide a measure of the\nfeature's importance that sums to the difference between the model's output for the input and the baseline.\nThe explanations generated by this method can be hard to read and to interpret for humans. Additionally, the relative\nhigh cardinality the input features\u2074 can make this task even more difficult. To exploit such information, only experts in\nboth handball and explainable Machine Learning would be able to derive meaningful information. In other words, the\nraw results without an interpretation layer are of little interest for coaches. It would require massive educational efforts\nto give them the required skills.\nTo mitigate this issue, we use a Large Language Model to extract meaningful information in a short and human-readable\nsummary. In particular, we use the open-source model Mistral 7B instruct [18]. The model was trained on a large\ncorpus of text available from the internet corresponding to approximately 600 billion words. This gives the ability to\nthe model to reason on a large set of topics. To afford running a LLM on relatively small server (with 8GB of GPU\nmemory), we use a quantized version of the model which allows to compress it and limit its memory footprint [19]. We\nleverage the reasoning and summarization capabilities of the model to generate a comprehensive summary of the match\nbased on the predictions and explanations.\nTo generate the match summary, we structure the prompt in three part: inputs, examples and instructions. The final\nprompt template is available in Appendix 5.1."}, {"title": "LLM inputs", "content": "The output of the prediction model and of the xAI function become the input of our LLM. We first\ngive contextual information to the model about the teams, date of the match, the type of competition and the predicted\nscore. The LLM knows the expected outcome and can adjust its explanations based on the results and the overall match\ncontext. Then, we provide information about the covariates of the model. For each of the features, we provide a short\ndescription of what it would mean to have a positive attribution score. This gives more structure to the model and guides\nit derive meaningful conclusions. We finally add the generated feature attributions derived from the xAI method that\nwill be the main source of information for the LLM to generate the match summary. To each player name, we add the\nposition and the team of the player along with the estimated attribution."}, {"title": "Few-shot prompting", "content": "To ensure that the LLM sticks to the desired structure, we provide few examples in the prompt.\nThis approach, known as in-context learning [20], uses examples in the context (prompt) to help the model learn\nthe structure of the expected output and what type of reasoning is expected. We then take few examples of past\nmatches and manually generate reports as we would expect the LLM to generate them. We ensure that the different\nexamples we generate are consistent in the format and type of conclusions we want to derive so the model does not get\nconfused. However, we try to make them diversified so it can learn the variety of output we expect and does not repeat\nnon-informative patterns by just mimicking the examples."}, {"title": "Summarization instructions", "content": "The final important step is to provide the exact instructions to the LLM. While we\nalready provided the different inputs and examples, we need to formalize and write down how we want the model to\nbehave and what we want it to generate. We instruct the model on how to interact with the provided inputs and how to\nuse the examples. We also specify what kind of conclusions and reasoning we expect. This step is crucial to limit the\nrisk of hallucination and guarantee the accuracy of the response [21]."}, {"title": "3 Results", "content": "To illustrate the performance of our solution, we first evaluate the prediction model on a test set for international\ncompetitions and analyze the change in performance using transfer learning. We next present the expected results for\nthe 2024 Olympic Games tournament using the presented model. Finally, we present an example of model explanation\nusing the presented Large Language Model."}, {"title": "3.1 Prediction performance", "content": "First, we notice that the performance presented in Table 2 shows higher errors than what is presented in [1] because\nwe now evaluate the performance for international competitions and not for clubs. This performance gap is simply\nexplained by the lower volume of data for national teams, an issue for which the transfer learning approach aims to\nmitigate."}, {"title": "3.2 Paris 2024 predictions for handball tournament", "content": "For the 2024 Olympic Games, France is considered as the favorite team for both the men and women handball\ntournaments. Furthermore, the last international competitions before the 2024 Olympiads saw France get the gold"}, {"title": "3.3 AI-generated explanations", "content": "To go beyond the predictions of the number goals scored by each team, we look at the drivers of the prediction. For the\nsake of illustration, we focus on the final match of the men's tournament between France and Croatia. We expect to see\nFrance win the goal medal with a score of 35-24 (see Figure 3)."}, {"title": "4 Conclusion", "content": "In this work, we leveraged different AI tools to predict, explain and generate human-friendly explanations for the\nhandball tournament at the 2024 Olympic Games. The prediction model is based on a deep neural network architecture\nand is articulated around transfer learning to benefit from the knowledge acquired by another model pre-trained on\nclubs data. Though we face challenges because of the volume of data available, this approach allows us to generate\nreasonable outcomes that are in line with sports experts expectations.\nTo further improve the performance of the prediction model, continuing the data collection to improve the quality as\nwell as the quantity will be a key enabler. This will imply heavy manual data collection tasks or the identification of\nimproved data sources, though the bottleneck is often coming directly from the handball federations which do not have\na systematic reporting in some competitions (in particular friendly matches).\nAnother path for improvement would be to develop a dedicated model that will be used to feed the embedding of the\nfinal prediction model. In particular, if data allow, one could build a model that focuses on predicting the performance\nand statistics of the players during a match (e.g. goals scored and blocks). This model's embedding will be explicitly"}, {"title": "Disclaimer", "content": "This work is not related to Amazon."}, {"title": "5 Appendix", "content": ""}, {"title": "5.1 Prompt template", "content": ""}]}