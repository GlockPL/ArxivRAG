{"title": "Towards High-Fidelity 3D Portrait Generation with Rich Details by Cross-View Prior-Aware Diffusion", "authors": ["Haoran Wei", "Wencheng Han", "Xingping Dong", "Jianbing Shen"], "abstract": "Recent diffusion-based Single-image 3D portrait generation methods typically employ 2D diffusion models to provide multi-view knowledge, which is then distilled into 3D representations. However, these methods usually struggle to produce high-fidelity 3D models, frequently yielding excessively blurred textures. We attribute this issue to the insufficient consideration of cross-view consistency during the diffusion process, resulting in significant disparities between different views and ultimately leading to blurred 3D representations. In this paper, we address this issue by comprehensively exploiting multi-view priors in both the conditioning and diffusion procedures to produce consistent, detail-rich portraits. From the conditioning standpoint, we propose a Hybrid Priors Diffsion model, which explicitly and implicitly incorporates multi-view priors as conditions to enhance the status consistency of the generated multi-view portraits. From the diffusion perspective, considering the significant impact of the diffusion noise distribution on detailed texture generation, we propose a Multi-View Noise Resamplig Strategy integrated within the optimization process leveraging cross-view priors to enhance representation consistency. Extensive experiments demonstrate that our method can produce 3D portraits with accurate geometry and rich details from a single image. The project page is at https://haoran-wei.github.io/Portrait-Diffusion.", "sections": [{"title": "1. Introduction", "content": "The generation of realistic 3D portraits from a single image [7, 10, 20, 35, 39] has become an important focus in computer vision and graphics, with broad applications in augmented reality, virtual reality, video conferencing, and gaming[14, 18, 45]. The most straightforward approach involves training GAN models [1, 47] on extensive portrait datasets to directly produce 3D representations. However, acquiring such training data can be costly and technically challenging, leading to failures in generating high-fidelity 360\u00b0 full-head portraits [7, 8] and often resulting in a lack of diversity in the outputs.\nTo address these limitations, recent developments [24, 26, 30, 31, 46] leverage text-to-image diffusion priors [5, 41, 44], which exhibit stronger generalization capabilities and higher generation quality, to produce novel perspectives. Most approaches incorporate additional priors, such as reference image latents [42, 50], ID features [13, 28], and view embeddings [28], to enhance the consistency between new perspectives and the primary viewpoint. Subsequently, they commonly employ Score Distilling Sampling (SDS) loss [25] to distill these 2D priors into 3D representations, ensuring consistent 3D generation.\nHowever, in single-image 3D portrait generation, these methods still face challenges: generated portraits often appear over-smoothed and fail to capture detailed textures like hair strands, as illustrated in Fig. 1, limiting their practical applications. We attribute this issue to the insufficient consideration of cross-view consistency during the diffusion process, resulting in significant disparities between different views. This 2D inconsistency results in blurred 3D output by SDS optimization. Although these methods attempt to improve consistency by incorporating additional priors, they rely solely on diffusion attentions to implicitly convey these priors. This reliance results in a lack of explict constraints, leading to inconsistent status across different viewpoints. Moreover, the diffusion procedure is inherently stochastic; even with the same conditions, a diffusion model can generate varied representations due to randomly sampled noises. By using view-independent procedures with purely random noise in diffusion, these methods overlook the impact of stochasticity on representation consistency. Consequently, these inconsistencies in status and representation jointly result in over-smoothed 3D models when optimized under the SDS loss, which enforces 3D consistency and continuity in sacrifice of texture details.\nTo address these issues, we propose fully exploiting cross-view priors in both the conditioning and diffusion procedures to enhance multi-view consistency, thus yielding detail-rich 3D portraits, as showcased in Fig. 1. From a conditioning perspective, we propose Hybrid Priors Diffusion Model (HPDM). Our approach seeks to transfer and utilize cross-view prior information in both explicit and implicit ways to control the novel view generation. In an explicit manner, we begin by employing geometric priors to map pixels from the current view to the next, providing an explicit reference to dominate the generation process. Given that this reference encompasses only a limited overlapping region and contains artifacts introduced through perspective transformations, we further propose to utilize the robust modeling capabilities of attention mechanisms to mitigate these deficiencies. These mechanisms capture finer texture and geometry priors and implicitly transfer these priors into the control conditions, ensuring a more comprehensive and precise guidance for the portrait status of novel viewpoint.\nFrom a diffusion procedure perspective, our goal is to manage randomness in adjacent viewpoints so that they can share detailed, consistent representations. To achieve this, we introduce a Multi-View Noise Resampling Strategy (MV-NRS) integrated into the SDS loss, which manages each view's noise distribution by passing cross-view priors. MV-NRS consists of two main components: first, a shared anchor noise initialization that leverages geometric priors to establish a preliminary representation; and second, an anchor noise optimization phase, where we resample and update the anchor noise based on denoising gradient consistency prior to progressively align the representations during the SDS optimization.\nTo summarize, our main contributions are as follows:\n\u2022 We developed a Portrait Diffusion pipeline consisting of GAN-prior Initialization, Portrait Geometry Restoration, and Multi-view Diffusion Refinement modules to generate rich-detail 3D portraits.\n\u2022 We designed a Hybrid Priors Diffusion Model that emphasizes both explicit and implicit integration of multi-view priors to impose conditions, aiming to enhance the consistency of multi-view status.\n\u2022 We introduced a Multi-View Noise Resampling Strategy integrated within the SDS loss to manage randomness across different views through the transmission of cross-view priors, thereby achieving fine-grained consistent representations.\n\u2022 Through extensive experiments, we show that our proposed pipeline successfully achieves high-fidelity 3D full portrait generation with rich details."}, {"title": "2. Related Work", "content": "One-shot 3D Generation 3D GANs [4, 11, 27, 40, 43, 48] have made significant strides in advancing one-shot 3D object generation by enhancing both quality and efficiency. GRAM [6] enhanced efficiency through point sampling on 2D manifolds, and GET3D [12] integrated differentiable rendering with 2D GANs to efficiently generate detailed 3D meshes. For improving 3D consistency, Geometry-aware 3D GAN [3] used a hybrid architecture to maintain multi-view consistency, while GRAM-HD [37] employed super-resolution techniques to address inconsistency issues. Despite these advances, limited datasets constrain the prior distribution, and acquiring high-quality data remains costly.\nRecently, methods leveraging 2D diffusion prior [2, 21, 21-23, 33, 34, 49] for generating 3D objects have gained traction [9, 16, 26, 32, 38, 46]. Dreamfusion [24] introduces a loss mechanism based on probability density distillation for optimizing parametric image generators. DreamCraft3D [29] employs view-dependent diffusion models for coherent 3D generation, using Bootstrapped Score Distillation to enhance textures. Make-It-3D [30] uses 2D diffusion models as perceptual supervision in a two-stage process, enhancing textures with reference images. Make-it-Vivid [31] focuses on automatic texture generation from text instructions, achieving quality outputs in UV space. These advancements underscore the promise of diffusion priors in achieving multi-view consistency in 3D object generation.\nOne-shot 3D Portrait Generation In 3D portrait synthesis, Yin et al. [47] enhanced 3D GAN inversion using facial symmetry and depth-guided pseudo labels for better structural consistency and texture fidelity. PanoHead [1] creates 360\u00b0 portraits with a two-stage registration process using tri-mesh neural volumetric representation.\nBenefiting from diffusion priors, diffusion models significantly enhance 3D portrait synthesis by enabling detailed zero-shot full head generation. Portrait3D [36] uses 3DPortraitGAN to produce 360\u00b0 canonical portraits, addressing \"grid-like\" artifacts with a pyramidal tri-grid representation and improving details through diffusion model fractional distillation sampling. DiffusionAvatars [17] combine a diffusion-based renderer with a neural head model, using cross-attention for consistent expressions across angles. Another Portrait3D framework [13] by Hao et al. emphasizes identity preservation in avatars across three phases: geometry initialization, sculpting, and texture generation, employing ID-aware techniques. While many of these methods utilize SDS and incorporate ID and normal information for enhanced representation, they often struggle to fully utilize multiple priors across viewpoints, leading to texture issues like over-smoothing or artifacts."}, {"title": "3. Methods", "content": "In this section, we first analyze the limitations of existing methods and give our motivations (Sec. 3.1). Next, we provide an overview of our pipeline, including GAN Prior Initialization Module, Portrait Geometry Restoration Module and Multi-view Diffusion Texture Refinement Module (Sec. 3.2). We then focus on the Multi-view Diffusion Texture Refinement Module, emphasizing both Multi-view Status Consistency (Sec. 3.3) and Multi-view Representation Consistency (Sec. 3.4) to achieve consistent multi-view generation achieving fine texture fidelity in 3D portrait."}, {"title": "3.1. Preliminary", "content": "Existing diffusion-based methods for generating 3D objects predominantly utilize Score Distillation Sampling (SDS) loss [25] to distill 2D diffusion priors into 3D representations. This process can be formulated as follows:\n$\\Phi^* = arg \\min_{\\Phi} (\\mathcal{L}_{SDS}(\\Phi; \\theta) + \\mathcal{L}_{ref}(\\Phi; I_{ref}))$\nwhere $\\Phi$ denotes the parameters of the 3D model, $\\mathcal{L}_{SDS} (\\Phi; \\theta)$ represents the SDS loss using a diffusion model paramterized by $\\theta$, and $\\mathcal{L}_{ref}(\\Phi; I_{ref})$ is a loss computed from reference image $I_{ref}$. The SDS loss can be formulated as:\n$\\nabla \\mathcal{L}_{SDS} = \\mathbb{E}_{t,v,\\epsilon} [w_t (\\epsilon_{\\theta}(z_{t,v}, t, c) - \\epsilon) \\cdot \\nabla R(v)]$\nwhere $z_{t,v} = \\sqrt{\\alpha_t} z_v(\\Phi) + \\sqrt{1 - \\alpha_t} \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0, I)$, where $z_{t,v}$ is a noisy latent representation obtained by combining the image latents $z_v (\\Phi)$, which is rendered from viewpoint $v$ by $\\Phi$, with random noise $\\epsilon$; $\\epsilon_{\\theta}(z_{t,v}, t, c)$ is a diffusion UNet model that predicts the noise component at each time step t, conditioned on $c$. $w_t$ and $\\alpha_t$ are weitghts, and $R$ is rendering function.\nFrom (2), the SDS loss aggregates the denoising gradients from all $v$ to the 3D model parameters $\\Phi$. When the denosing distributions across viewpoints are inconsistent, the SDS loss will produce over-smoothed representations to minimize the overall loss by averaging conflicting gradients, sacrificing the details of each perspective. The denoising function $\\epsilon_\\theta$ is influenced by both the conditions c"}, {"title": "3.3. Multi-view Status Consistency", "content": "Fig. 3 (a) presents our Hybrid Prior Diffusion Model (HPDM), which focuses on leveraging multi-view priors in a hybrid manner to condition the novel view synthesis process for more consistent portrait status. Initially, we leverage explicit priors by providing reference images to dominate the generation process, offering direct control and constraints. Following inpainting tasks, we introduce our Explicit-Branch. This branch takes the image projected from the driving view to the target view as an explicit reference and extends it to fill in the invisible areas.\nTo generate this reference, we convert the driven view image $I_{v_i}$ into a colored 3D point cloud $P_{v_i}$ using the NeRF-rendered depth map $D_{v_i}$. Then, a reference image of target view is rendered from this colored point cloud:\n$I_{v_{i+1}}^{Proj} = R_{P_{v_i}} (v_{i+1}, I_{v_i})$\nBesides, the segmentation mask $S_{v_i}$ is similarly rendered onto the target view as an auxiliary mask condition $S_{v_{i+1}}^{Proj}$. The $z_{v_{i+1}}^{Proj}$, obtained by encoding $I_{v_{i+1}}^{Proj}$ from an VAE, along with the $S_{v_{i+1}}^{Proj}$ are fed into the diffusion UNet.\nThe diffusion UNet, following the inpainting method [15], is a adapted version of a pretrained diffusion UNet. In this adaptation, the cross-attention components are removed to to focus entirely on the reference. Features from this UNet are injected into the frozen layers of the original diffusion UNet layer by layer with zero convs, allowing for dense, pixel-level control over the generation process:\n$\\epsilon_{\\rho}(z_t, t, y) = \\epsilon_{\\theta}(z_t, t, y) \\mathbb{I} + w_{Ex} Z(\\epsilon_{Ex}([z_{v_i+1}^{Proj}, S_{v_i+1}^{Proj}, z_t], t) \\mathbb{I})$\nwhere $\\mathbb{I}$ denotes the $l$ layer of the UNet, $Z$ denotes zero conv and $w_{Ex}$ denotes control weight.\nHowever, since the reference cannot guide all areas and the degraded priors during the view transformation, relying solely on such explicit priors transfer would introduce noises into control signals.\nTo address this, we aim to implicitly leverage priors to compensate for these deficiencies. To enhance texture priors, we integrated a second branch, Implicit-Branch, for lossless texture understanding, and designed a res-block to semi-explicitly pass this understanding to the Explicit-Branch. In detail, this branch is a copy of a Explicit-Branch that directly takes the driving image $I_{v_{i-1}}$ as input. To ensure effective transfer of these priors to the Explict-Branch, we first explicitly rendering Implicit-Branch latents into the target view and then implicitly integrating them into the Explicit-Branch through res-blocks with zero-convs. We opt for simple res-blocks rather than complex transformers, benefiting from the spatial prior alignment provided by explicit geometric projection. The design of the res-blocks is detailed in the Sec. 6. This process can be expressed as:\n$\\epsilon_{Ex}([z_{v_i+1}^{Proj}, S_{v_i+1}^{Proj}, z_t, v_i], t) \\mathbb{I} = Res [\\epsilon_{Ex}([z_{v_i+1}^{Proj}, S_{v_i+1}^{Proj}, z_t, v_i], t) \\mathbb{I}, R_{P_{v_{i-1}}}(\\epsilon_{Im}([I_{v_{i-1}}, z_t, v_i], t) \\mathbb{I})]$"}, {"title": "3.4. Multi-view Representation Consisency", "content": "Although the aforementioned conditions can enhance the consistency of the generation status, considering the stochastic nature of diffusion process, it is crucial to align the generated representations by controlling the noise sampling distribution for each viewpoint. Therefore, we propose utilizing cross-view priors to identify the optimal noise distribution for each perspective, ensuring that multi-view representations are well-aligned and remain clear. To this end, we develop a Multi-View Noise Resampling Strategy (MV-NRS) within the SDS Loss. MV-NRS consists of two steps: anchor noise initialization and anchor noise optimization, as shown in Fig. 3 (b).\nTo identify these noise distributions, we begin by pinpointing a specific set of multi-view noise, denoted as anchor noise $\\epsilon_{UN}$, to ensure the generated images under these noises are initially more closely aligned. Subsequently, we perform resampling based on this anchor noise set, facilitating an initial alignment of the generated distributions. The resampled $\\epsilon_{Rs}$, with a small variance $\\sigma^2$, follows the distribution:\n$\\epsilon_{Rs} \\sim N(\\sqrt{1 - \\sigma^2} \\epsilon_{UN}, \\sigma^2 I)$\nWe recognize that the output of the 2D diffusion model demonstrates both invariance to linear transformations and robustness to small-scale nonlinear transformations. Consequently, it exhibits a degree of invariance to small-range viewpoint changes, which can be considered as local linear transformations. Therefor, by aligning the inputs according to the viewpoints, we can ensure that the outputs align as well. Since the input of the UNet consists of a combination of rendered image latents and noises, we only need to align the noises. To achieve this, we just lift the driven view noise into a point cloud and render it onto the target views:\n$\\epsilon_{0_{v_{i+1}, 0}} = R_{P_{v_i}}(\\epsilon_{0_{v_i, 0}}, v_{i+1}) + \\epsilon_{rand}$\n$\\epsilon_{0_{v_i, 0}} \\sim \\mathcal{N}(0, I), \\epsilon_{rand} \\sim \\mathcal{N}(0, I)$,\nwhere $s=0$ denotes the initial training iteration and M is a mask that indicates the locations of voids in the rendered noise. Compared to random noise initialization, this method uses cross-view priors to build noise, enabling the capture of some small-scale noise distributions that are almost impossible to obtain through pure random sampling, particularly when the multi-view generative distributions are far apart.\nNext, since the initial representations may not be perfectly aligned, we utilize multi-view gradient consistency to gradually finetune the anchor noises during the SDS training. Specifically, we have designed a Resampling Retention Strategy:\nIn each training iteration $s$, we first resampled a noise $\\epsilon_{vi,s}^{Rs}$ according to (12). Then, we decide whether to keep the resampled noise for updating the anchor noise by utilizing the multi-view gradient consistency score. The key idea is compute the gradients obtained from both the resampled noise and the anchor noise, and then assess their similarity with the gradients from the driven viewpoint. By comparing these similarities, we can determine whether to retain the resampled noise.\nIn detail, for $\\epsilon_{vi,s}^{Rs}$, we first compute the loss between a rendered image $I_{v_i}^R$ and denoised image $I_{v_i}^{SID,s}$ from $\\epsilon_{vi,s}^{Rs}$, then backpropagate it to get the gradient $grad_{v_i,s}^{Ds}$:\n$\\mathcal{L}_{Dis} = \\mathcal{L}_I(I_{v_i}^R, I_{v_i}^{SID,s})$\n$grad_{v_i,s}^{Ds} = BP_{v_i}^{-1}(\\mathcal{L}_{Dis})$\nThe cosine similarity is used to evaluate the consistency between this gradient and the applied gradient of the driven view $grad_{v_{i-1},s}$:\n$S_{vi,s}^D = \\frac{grad_{v_i,s}^{Ds} \\cdot grad_{v_{i-1},s}}{\\|grad_{v_i,s}^{Ds}\\| \\|grad_{v_{i-1},s}\\|}$\nSimilarly, for anchor noise $\\epsilon_{vi,s-1}^{Ac}$, we direct use the $I_{v_i,s-1}$ of previous training iteration to compute the gradient $grad_{v_i,s}^{Ac}$ and corresponding score $S_{vi,s}^{Ac}$.\nIf $S_{vi,s}^D > S_{vi,s}^{Ac}$, the resampled noise is superior over the anchor noise in terms of gradient consistency. Therefore, we update the anchor noise and treat $I_{v_i}^{ID}$ as the current target image, $grad_{v_i,s}^{Ds}$ as the current applied gradient. Conversely, we retain the anchor noise and target image, using the corresponding gradient instead:\n$\\begin{cases}I_{v_i,s}, \\epsilon_{v_i,s}, grad_{v_i,s} = I_{v_i}^{ID,s}, \\epsilon_{vi,s}^{Rs}, grad_{v_i,s}^{Ds} & \\text{if } S_{vi,s}^D > S_{vi,s}^{Ac} \\\\I_{v_i,s-1}, \\epsilon_{v_i,s-1}, grad_{v_i,s}^{Ds} & \\text{otherwise}\\end{cases}$\nAfter calculations for all viewpoints, we aggregate the gradients across all views, denoted as $Grad_s$. Finally, we update the 3D model in the current training iteration:\n$Grad_s = \\sum_{v} grad_{v_i,s}$\n$\\Psi_s = \\Psi_{s-1} + \\alpha_s \\cdot Grad_s$"}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nWe employ triplane features with a resolution of 128x128 and a total of 96 channels, resulting in a rendered image resolution of 512x512. During the geometry restoration phase, we randomly sampling viewpoints across a full 360\u00b0 azimuth and a pitch angle ranging from -30\u00b0 to 30\u00b0 for SDS training. In the texture restoration phase, we reconstruct images using 13 fixed viewpoints. The training of our HPDM uses synthetic dataset from GANs methods. In addition to these fixed perspectives, we randomly sample additional viewpoints and apply a multi-step denosing SDS loss (image-supervised) to improve rendering quality between the fixed viewpoints. All our experiments were conducted on a single A100 GPU. For more hyperparameters and training details, please refer to the Sec. 7.\n4.2. Qualitative Results\nWe compare our results with those from open-sourced SOTA approaches: Portrait3D [36], DreamCraft3D [29], and Wonder3D [19]. Notably, Portrait3D is a text-to-3D method; in our case, we bypass the text-to-image step by directly providing the reference image.\nFrom Fig. 4, it can be observed that the models generated by Wonder3D exhibit excessive smoothness and show a distinctly toy-like texture. While DreamCraft3D demonstrates some improvements, it completely loses reasonable texture in the profile view. Compared to the previous methods, Portrait3D can produce visually appealing models; however, it experiences identity variation issues and exhibits overly blurred textures and some artifacts in regions with complex hair patterns (e.g. the hair ends in the second row). In contrast, our model demonstrates the strongest fidelity and texture quality. We achieve an identity that is highly consistent with the reference and generating hair texture style that align appropriately with the reference. Furthermore, we can distinctly showcase the texture of nearly every strand of hair. Therefore, our propsed Portrait Diffusion surpasses SOTA methods and achieves the most detailed textures in 3D portraits.\n4.3. Quantitative Evaluation\nTo comprehensively evaluate our generated head models, we employed three complementary metrics: CLIP-I for overall structural consistency, LPIPS for perceptual similar-"}, {"title": "4.4. Ablation Study", "content": "Effectiveness of the Hybrid Priors Diffusion Model. The Fig. 5 illustrates the visual results of the ablation study conducted to evaluate the effectiveness of the Hybrid Priors Diffusion Model. Panel (a) displays results of conditioning only with Explicit-Branch, which aligns corresponding areas in the reference but reveals numerous deficiencies in both structure and texture. Panel (b) demonstrates the results achieved by incorporating geometric priors into the model. This addition fosters a more harmonious and unified geometry, significantly improving the overall shape fidelity of the 3D portrait. However, there are still texture striped artifacts present. Panel (c) further incorporates texture priors (Implicit-Branch), effectively eliminating all striped texture artifacts and showcasing exquisite details. This validates the effectiveness of our approach in leveraging multiple types of priors.\nEffectiveness of the Multi-View Noise Resampling Strat-"}, {"title": "5. Conclusion", "content": "We introduced a Portrait Diffusion pipeline that generates detail-rich 3D full portraits. This pipeline consists of three main modules, including a GAN-prior Initialazation module, a Portrait Geometric Restoration module and a Mult-view Diffusion Refinement Module. Our Mult-view Diffusion Refinement Module incorporates a Hybrid Priors Diffusion model that effectively leverage multi-view priors for consistent status, and a Multi-View Noise Resampling Strategy to ensure consistent representations during the optimization. Qualitative and Quantitative assessments have shown that portraits produced by our pipeline exhibit superior detail and realism compared to state-of-the-art alternatives. Our Portrait Diffusion pipeline sets a new standard in 3D portrait generation, offering unparalleled texture detail and fidelity, and paving the way for future developments in computer vision, graphics, and digital art."}]}