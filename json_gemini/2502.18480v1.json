{"title": "QExplorer: Large Language Model Based Query Extraction for Toxic Content Exploration", "authors": ["Shaola Ren", "Li Ke", "Longtao Huang", "Dehong Gao", "Hui Xue"], "abstract": "Automatically extracting effective queries is challenging in information retrieval, especially in toxic content exploration, as such content is likely to be disguised. With the recent achievements in generative Large Language Model (LLM), we are able to leverage the capabilities of LLMs to extract effective queries for similar content exploration directly. This study proposes QExplorer, an approach of large language model based Query Extraction for toxic content Exploration. The QExplorer approach involves a 2-stage training process: instruction Supervised FineTuning (SFT) and preference alignment using Direct Preference Optimization (DPO), as well as the datasets construction with feedback of search system. To verify the effectiveness of QExplorer, a series of offline and online experiments are conducted on our real-world system. The offline empirical results demonstrate that the performance of our automatic query extraction outperforms that of several LLMs and humans. The online deployment shows a significant increase in the detection of toxic items.", "sections": [{"title": "1 Introduction", "content": "The content on widely used social media or e-commerce platforms, such as Twitter, Amazon and Taobao, is created by millions of users. It is inevitable that a small portion of toxic content such as offensive, sexual, violent, and fraudulent language will appear. Therefore, the detection and exploration of such toxic content become important for maintaining a healthy platform. This toxic content can appear in the form of text, image, audio, or video. This study focuses on the item's textual information of Xianyu which is the largest Chinese second-hand trading platform.\nTo recognize toxic content, a general solution is to train classification models to process all the information that is concerned. However, these models cannot identify all toxic items. Figure 1 illustrates the framework of our toxic content detection system, which contains strategies, classification models, a risk understanding module, and a search system. In our case, some toxic items overlooked by these models are reported by the users of the trading platform. We aim to explore all the similar toxic content when such reports occur. Since toxic content is often camouflaged within normal language expressions, we incorporate a risk understanding module before the search process, as shown in Figure 1. The risk understanding module essentially generates queries based on the reported toxic content. Prior to the implementation detailed in this work, only manually annotated queries were used for exploration.\nWhen a suspicious toxic content is reported, the auditor first analyzes the content and extracts keywords or images that need to be explored. The queries are then used to search within a search system comprised of two phases: retrieval and ranking. During the retrieval phase, methods such as inverted index based exact keyword matching, embedding based semantic retrieval, and graph based retrieval are employed. Generally speaking, the inverted index based keyword exact matching is the most effective method for toxic content exploration. In the ranking phase, all retrieved items are ranked according to their risk probabilities, which are calculated by the ranking service.\nIn this toxic content exploration system shown in Figure 1, the auditor risk understanding component serves as the foundation of the entire process. An effective query can result in an efficient toxic content exploration by identifying most of the similar toxic items in the search system. Our auditors are asked to mark out the part related to risk. However, an effective query does not always focus solely on hazardous parts of the content; sometimes specific language patterns are also helpful for exploration, as different expressions with similar meanings are often employed. Table 1 shows some typical examples, with italicized or highlighted in red parts indicating the human annotated phrases that are deemed improper or suspicious according to the auditors' judgment. Those annotated phrases are then used for exploring similar toxic content. As observed, not all the annotated phrases are suitable for an efficient search. For example, the phrase \"Transferable Commercial Performance License (2+ Years) and Radio/TV Program Production License\" is overly verbose and may not yield any results in a search system. Meanwhile, the term \"companionship\u201d is too broad and may result in too many irrelevant items.\nAssistance from models can alleviate the psychological burden on auditors who have been exposed to negative content for a long period. To reduce human dependence in the risk understanding phase and enhance the efficiency in toxic content exploration, we introduce the following LLM based automatic query extraction approach.\nGenerally, existing methods for keyphrase extracting can be divided into three categories: (1) 2-step keyphrase extraction ([4, 10, 20, 32]) which involves generating a set of phrase candidates using various methods and subsequently ranking or classifying these candidates. (2) Sequence labeling ([1, 9, 19, 31]) which predicts the likelihood that a token should be included in the extracted keyphrase. (3) Sequence-to-sequence generation ([5, 18, 28]) which is based on pretrained models such as Bert, Bart or T5.\nThe way humans work is fundamentally different from the first two methods. An experienced content safety expert typically has an intuitive sense of which parts of toxic content is efficient for further exploration. This is the intuition of our proposed method.\nRecent achievements in LLMs demonstrate their capability to comprehend natural language text and address even more complex real world scenarios. Proprietary LLMs like GPT3.5 or GPT4 have shown impressive performance in query extraction when assisted by carefully crafted prompts. This encourages us to leverage LLMs for query extraction. Furthermore, the availability of open-source LLMs, such as the series of Llama ([25, 26]), ChatGLM ([7, 8]), Qwen ([2, 30]), allows us to conduct this study in a practical manner. When given a piece of toxic content, the useful parts for exploration may appear in various forms, such as phrases, fragments of sentences, emojis, or prompts. We expect that a finetuned LLM can directly identify and extract these components.\nIn this study, we propose an approach of LLM based query extraction aimed at toxic content exploration (QExplorer), which can compete with humans in exploration efficiency. It can be classified as a sequence-to-sequence generation method but differs in design from previous methods. The QExplorer involves a 2-stage training process: instruction Supervised Fine-Tuning (SFT) and preference alignment using Direct Preference Optimization (DPO). Feedback of the search system is used to construct preference data. The model training objective is aligned with the goal of making the extracted queries useful for toxic content exploration.\nThe contributions of this work are in the following aspects:\n(1) We formulate the query extraction as a 2-stage LLM alignment problem innovatively. The preference alignment enables further adaptation of the query extraction model, achieving improved performance when using Qwen1.5-7B-Chat and ChatGLM3-6B as base models.\n(2) We propose a finetuning framework named QExplorer for query extraction aimed at toxic content exploration. This framework seeks to extract more effective queries to look for toxic items in search system by integrating feedback from the search system into the training process.\n(3) Offline experiments demonstrate that our method outperforms baseline models and humans in extracting effective queries. The Online deployment shows a significant increase in toxic items detection."}, {"title": "2 Related Works", "content": "Keyphrase Extraction\nKeyphrase extraction approaches can be broadly categorized into three main types. Early work in this area follows a pipeline approach, often employing a 2-step method ([4, 10, 20, 32]). Initially, a set of candidate phrases is generated using heuristics, part-of-speech tags, n-grams, or other techniques. Subsequently, a ranking model is trained to order the candidates, with the top n phrases being selected as the extracted keyphrases. Sequence labeling, a technique widely used in Named Entity Recognition (NER) ([3, 16]), has been adapted for keyphrase extraction over the past decade ([1, 9, 19, 31]). In sequence labeling training, designed labels are assigned to each token to indicate whether it is part of a keyphrase. The model is then trained to predict the probability of each token should be part of the expected keyphrase. In recent years, sequence-to-sequence model have been employed to extract keyphrases ([5, 18, 28]). These models are typically built on pretrained language models such as Bert, Bart, T5. Our proposed approach can be considered as a sequence-to-sequence model, but it features a novel training design.\nLLM Alignment\nSince the launch of ChatGPT, the impact of LLM has been increasingly recognized. In addition to the extensive data used for training, two other valuable techniques for addressing various tasks are instruction SFT and preference alignment. A well pretrained LLM can be adapted to various downstream tasks via instruction SFT. These studies ([21, 27]) have shown the zero-shot ability and generalization of instruction finetuned language models. This study ([21]) observes that multi-task instruction finetuned language models exhibit cross-task generalization, with finetuned models showing improved performance on unseen tasks. This study ([6]) demonstrates that using a series of designed instructions allows finetuned language models to achieve significant improvements. The study ([22]) by OpenAI combines instruction finetuning and reinforcement learning from human feedback (RLHF) and shows an impacted improvement in the performance of LLM. Additionally, subsequent LLMs ([2, 23, 26]) have demonstrated the effectiveness of instruction finetuned LLM in query reformulation for search purposes.\nThe studies ([22, 26]) demonstrate that implementing RLHF after instruction SFT can greatly increase the win rate of LLM in generating tasks. Specific comparisons of the performance between LLMs with RLHF and without RLHF are presented in these studies. However, RLHF requires a reward model to be trained first, and the training of RLHF is relatively unstable. Alternative methods like DPO ([24]) and odds ratio preference optimization (ORPO) ([13]) have been proposed. These methods do not require an explicit reward model; instead, the preference alignment is trained using pairs of preference data. This study ([14]) focuses on alignment of LLMs with human feedback, employing both DPO and PPO. DPO exhibits a promising performance in preference alignment, benefiting from a simpler design and requiring fewer resources during training.\nFully tuning the parameters of an LLM is expensive because it involves training all of the model's parameters. Methods such as LORA ([15]), LORA+([11]) make the adaptation of LLM feasible with relatively low resources. During training, the parameters of the base model are frozen, and trainable rank decomposition matrices are introduced to modify each layer of the transformer. These methods enable finetuning an LLM with a size of 7B or 14B using a single A100 GPU."}, {"title": "3 Methodology", "content": "Large Language Model based query extraction for toxic content exploration aims to get the key components within the given piece of content. These key components may include toxic expressions, suspicious prompts, or combinations of several rare co-occurrence phrases. The proposed approach intends to leverage the comprehension and reasoning abilities of LLMs. We propose a two-stage method, which includes instruction Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). In this study, we focus exclusively on the textual information of the reported toxic items. Figure 2 illustrates the diagram of our method.\nInstruction SFT\nPublicly available LLMs tend to refuse to engage with sensible content that involves offensive, sexual, violent or fraudulent languages. And they are not designed for extracting effective queries for further search. In the query extraction task, these models tend to list out all the possible phrases. On the other hand, we cannot use a third party API due to data security and cost concerns. Therefore, we create training datasets from the logs of our real production system and fine-tune LLMs to enhance their ability to extract effective queries from toxic content.\nAs shown in Figure 1, the auditors analyze the reported toxic content and mark out the improper parts during the risk understanding phase. Most of the annotated phrases are used to search for similar toxic content. The searched keywords, exposed items and detected toxic items are stored in the system log. Some typical examples are provided in Table 1. Not all annotated data is suitable for creating a training dataset. We select the annotated data with non-zero amount of exposed items or a length no more than 10. In this step, we include the annotated keywords regardless of whether they can lead to toxic items. This is because some keywords that do not yield toxic items merely indicate the absence of toxic items in the indices at the searching time, rather than being useless. We removed data where the annotated phrase length is less than 2 or greater than 10; these values are based on empirical observation. We denote the reported toxic content as C, the annotated keywords set as K, the number of exposed items caused by keyword ki in our search system as hit(ki), and the length of keyword ki as len(ki).\nThe dataset used to for the instruction SFT can be formulated as follows:\n$D = \\{(c, k_i) | \\frac{hit(k_i | S)}{len(k_i)} > 0 \\text{ or } len(k_i) \\leq 10, c \\sim p(x)\\}_{i=1}^{N}, len(k_i) > 1$ (1)\nwhere p(x) denotes the toxic content distribution in Xianyu trading platform, S denotes the Xianyu search engine.\nIn order to construct some long context data, based on the above dataset, we cluster the data into thousands of groups according to toxic content category and similarity. This data is used to construct the preference data and in the ablation study. For each group we sample up to 20 different items and concatenate the text information together using comma to form content C, and similarly, concatenate the qualified keywords using commas. This process can be formulated as follows:\n$D_{cat} = \\{(\\text{con}(c), \\text{con}(k)) | (c, k) \\in G_i, D = \\bigcup_{i=1}^{N_c,s=n} G_i\\}$ (2)\nwhere $G_i$ denoted the clustered content, con(\u00b7) represents the concatenation of the given text. This dataset $D_{cat}$ is further used to construct the preference alignment data.\nWe construct the final instruction SFT dataset using the above datasets and task specific prompts. An example is shown in Table 2. As illustrated in this example, the content C is fed into the input and the annotated keywords K are fed into the output part.\nIn the supervised finetuning process, the concatenation of the instruction and input in the prompt serves as the input x, whereas the output in the prompt serves as label y. The objective of our query extraction model is to maximize the conditional probability p(y|x). The loss can be depicted as follows:\n$L_{SFT}(\\theta) = -\\sum_{(x,y) \\in D_{SFT}} \\sum_{t=1}^{T} \\log P(y_t | x, y_{<t}, \\theta)$ (3)\nwhere DSFT represents the dataset for instruction finetuning, \u03b8 denotes the model parameters, and T denotes the length of the label sequence y.\nWe apply LORA (Low-Rank Adaptation of Large Language Models) to finetune the LLMs.\nPreference Alignment\nThis preference alignment aims to align LLM to the objective of identifying more similar toxic items. For a given reported toxic content, we assume the keywords leading to toxic items in a search engine are more effective than those that only retrieve normal items. To incorporate search engine feedback into training process simply, we employ the DPO (Direct Preference Optimization) algorithm. For each reported toxic content, it is necessary to construct two distinct sets of keywords, organized in a partial order. The DPO algorithm is designed to adapt the LLM parameters such that the likelihood of the preferred response is higher than that of the dispreferred response. Practically, there is usually one annotated query for a given reported toxic content. To obtain a pair of partially ordered keyword sets, we concatenate similar toxic content into a single, extended context paragraph, ensuring enough keyword candidates for ordering. Preference alignment data is constructed based on the dataset Dcat. The data construction process in this step could be formulated as follows:\n$D_{comp} = \\{(\\text{con}(c), \\text{con}(k^s_p), \\text{con}(k^s_d)) | (c, k^s_p, k^s_d) \\in G_i, D = \\bigcup_{i=1}^{N_{c,s=n}} G_i, p(k^s_p | S) > 0.05, p(k^s_d | S) \\leq 0.05\\}$ (4)\nwhere con(\u00b7) denotes the concatenation of the given text, $k^s_p$ represents the preferred keyword and $k^s_d$ represents the dispreferred keyword. p(\u00b7) denotes the percentage of toxic items can be found among the logged exposed items by the given keyword on the search platform. S denotes the search engine. Empirically, the threshold of p(\u00b7) is set to 0.05. In the subsequent ablation study, we analyze the model's performance using different thresholds. Table 3 illustrates a prompt example at this stage. A complete example is shown in table 8 in the Appendix.\nIn the preference alignment training, the concatenation of the content of the system and the question within the prompt constitutes the input x. The preferred answer is denoted as the label yp, while the dispreferred answer is denoted as the label yd. The initialized LLM acts as a reference model with fixed parameters \u03b8, during training. As derived in [24], the loss for DPO can be expressed as follows:\n$L_{DPO}(\\theta) = -\\beta \\mathbb{E}_{(x,y_p,y_d) \\in D_{comp}} [\\log \\sigma (\\beta \\log \\frac{P(y_p|x,\\theta)}{P(y_d|x,\\theta)} - \\log \\frac{P(y_p|x,\\theta_r)}{P(y_d|x,\\theta_r)})]$ (5)\nwhere P(\u00b7) represents the likelihood of the label with the given condition. The variable \u03b8 denotes the trainable LLM parameters, while \u03b8r denotes the fixed reference LLM parameters. \u03b2 is a hyper parameter that controlls the deviation from the reference policy as described in [24]. To maintain the ability of generating outputs as an LLM, the loss function for preference alignment is formed as follows:\n$L(\\theta) = \\gamma L_{SFT}(\\theta) + L_{DPO}(\\theta)$ (6)\nwhere LSFT is as equation (3), LDP O is as equation (5), \u03b3 is a hyper parameter.\nWe also utilize LoRA to perform finetuning for DPO.\nOnline Service\nThe framework of our toxic content exploration system is illustrated in Figure 1. The aligned LLM is utilized to perform query extraction in the risk understanding phase, as shown in Figure 3. This model complements human analysis. We deploy a data process pipeline on our Dataworks platform, which operates on a scheduled mode. Each day, the system receives a bunch of reported toxic items, and the textual information is fed into the finetuned LLM for inference. These reported toxic items are also analyzed by auditors. Both human annotated queries and LLM extracted queries are then used to create search tasks for further exploration. Each exploration task includes dozens of queries, with semantically similar queries are clustered into a single task. Whether a query originates from human annotation or LLM suggestion is logged so that we can assess the effectiveness of the proposed model. If an LLM suggested query matches a human annotated query, it is labeled as provided by a human. The online performance is evaluated in this implementation."}, {"title": "4 Experiments", "content": "Datasets\nThe data for training is processed from half year logs from our toxic content exploration system, covering the period of [11.19. 2023, 05.19.2024]. We construct 25772 pairs of for dataset D, 2378 pairs long context samples for Dcat. Both D and Dcat are described in Section 3. The long context dataset Dcat has an average length of 402 characters, while dataset D has an average length of 154 characters. During training, the cutoff length of the input token sequence is set to 2048. For instruction SFT, we constructed 3 datasets using D, Dcat and D + Dcat respectively to study the effectiveness of different combination.\nFor preference alignment, using the method described in Section 3, we constructed about 600 preference samples.\nTable 4 shows the statistics of the datasets used in this study. For the offline evaluation, we utilize the logged reported toxic items in our system after 05.19.2024. Using the same preprocessing as for training, we feed the text data into the finetuned LLM for inference. The offline test data presented in this study is derived from the reported toxic items of 10.20.2024.\nOffline Experiment Settings\nBaselines. We compare QExplorer with typical methods used for keyword extraction, some LLMs and humans. The methods used for comparison are listed below.\nTF-IDF is a simple method for selecting top N terms according to TF-IDF scores. Jieba segmentation is used in this study. N is set to equal the number of test samples.\nBi-LSTM-CRF([1]) is a sequence labeling method. In our implementation, BIO tagging is used, where B and I tags are assigned to keywords, and words out of keywords are labeled to O. The Bi-LSTM-CRF model is trained on dataset D for 500 epochs. Both the embedding size and hidden size are set to 256. The training data used in [1] consists more than 0.5 million documents, which is larger than the size of our SFT dataset D.\nBART([5, 17]) is a pretrained language model with encoder-decoder architecture for generation, translation, and comprehension tasks. It can be easily adapted to extract keyword and demonstrates competitive performance with state-of-the-art keyphrase generation on several benchmarks. In this study, bart-large is supervised finetuned to extract query with dataset D for 5 epochs. Since BART cannot handle data more than 1024 tokens, the cutoff of input sequence is set to 1024.\nBase LLMs A powerful LLM demonstrates impressive zero-shot capabilities across various tasks. However, in query extraction, a base LLM without SFT tends to list all the possible keywords. To ensure a fair comparison, we use a slightly modified prompt, as shown in Table 2, for LLMs without SFT. In the prompt, we instruct the LLM to arrange the keywords in descending order of risk, then select the top 1 keyword for evaluation. Qwen2.5-72B-Instruct and GPT4 Turbo 128k are evaluated through local deployment, while GPT4 Turbo 128k is evaluated via the Azure OpenAI API. Other base LLMs used in this study, such as Qwen1.5-7B-Chat and ChatGLM3-6B, cannot follow the instructions well without SFT.\nSearch Engine and Retrieval Setup. We do not build an isolated search engine for the offline evaluation, the online search engine is used. For a given set of reported toxic content, we create a series of retrieval tasks that includes all the queries extracted by LLMs or annotated by humans. The data retrieved from the search system is collected before the auditors start to explore. Once auditors begin their exploration, the search system becomes non-stationary because the toxic items are removed as soon as they are detected. The retrieval data collection takes about 1 to 2 hours, during which the search system can be considered approximately static. As the detection system depicted in Figure 1 runs for 1 to 2 days after the retrieval data collection is completed, most of the toxic items in the system are labeled by auditors, we then use these toxic labels to calculate the offline metrics.\nEvaluation Tasks and Metrics\nIt is hard to set a gold standard for the query extraction for toxic content exploration. For a given reported toxic content, the most effective query for exploration depends on the items' distribution of the search engine at a certain moment. Even an auditor may be unable to determine which query is optimal without conducting a search. For a given reported toxic item, an auditor can only assess whether the extracted query is acceptable based on their understanding. When an item is detected as toxic, it is removed from the search system. And in the online application, the human annotated queries have higher priority to search. Thus, we ave designed the following five criteria to evaluate the quality of the extracted queries.\nQuery hit rate, the percent of queries that could lead to the discovery of toxic items. This metric measures the model's ability to extract effective queries for toxic content exploration and is evaluated offline.\nNumber of hit@100, the number of the toxic items found among the top 100 retrieved items in the search system using a given query. This metric is proportional to recall and measures the retrieval effectiveness of queries suggested by models or humans, and is evaluated offline.\nQuery acceptance rate, depicts the chosen probability of the extracted queries by an auditor in online application, and is evaluated online. This metric is related to the offline query hit rate.\nToxic item detection increment, depicts the additional toxic items detected through the LLM suggested queries in an online application. This is evaluated online and relates to the offline number of hit@100.\nHit query increment, depicts the additional queries that can retrieve toxic items compared to human, evaluated online.\nImplementation Details\nWe utilize the open-source framework LLaMa-Factory ([33]) for training. Qwen1.5-7b-chat ([2]) and ChatGLM3-6B ([8]) are used as the base models for finetuning. During the instruction SFT stage, the model is trained for 2 epochs with a learning rate of 3e-5. In the preference alignment stage, the finetuned model from the instruction SFT stage undergoes further training for 5 epochs with the same learning rate of 3e-5. The maximum length of the input token sequence is set to 2048. The LoRA method is used in the both stages, with the rank decomposition matrices employed for all the linear modules of a specific LLM. The optimizer is AdamW. The parameter \u03b2 in equation (5) is set to 0.1. The parameter \u03b3 in equation (6) is set to 1.0.\nOffline Effectiveness Analysis\nTable 5 presents the main results of human, baseline models and QExplorer in metrics of Number of hit@100 and Query Hit Rate. The proposed method Qwen1.5-7B-Chat (QExplorer, LoRA) outperforms all other models. Qwen1.5-7B-Chat (SFT, LoRA) is trained on dataset D. Qwen1.5-7B-Chat (QExplorer, LoRA) is further trained on the preference dataset Dcomp on base of Qwen1.5-7B-Chat (SFT, LoRA). The Query Hit Rate of Qwen1.5-7B-Chat (QExplorer, LoRA) is 0.561, which is higher than human labeled data by 14.7%. It suggests that QExplorer can give more effective queries for toxic content exploration than humans. In metric Number of hit@100, it is observed that queries suggested by QExplorer can detect more toxic items than queries given by human.\nLarge pretrained generative models outperforms TF-IDF and Bi-LSTM-CRF in query extraction. The TF-IDF and Bi-LSTM-CRF methods are less efficient than these generative models. TF-IDF tends to yield short queries, the efficiency of each query is not as high as other methods. Since the boundaries of queries in our situation are not strict, Bi-LSTM-CRF tends to assign more \"O\" tags to words within a sentence, resulting in fewer queries extracted compared to other methods. Bart is a powerful pretrained model; considering both Number of hit@100 and Query Hit Rate, BART (SFT) performs better than TF-IDF and Bi-LSTM-CRF. The results given by BART (SFT) is a strong baseline. The results given by GPT4 Turbo 128k and Qwen2.5-72B-Instruct are strong baselines too. Notably, GPT4 Turbo 128k surpasses human performance in this evaluation. However, GPT4 Turbo 128k suggests fewer queries than most of other methods, as it refuses to respond to questions involving offensive, sexual, violent content, etc. Nevertheless, GPT4 Turbo 128k does not surpass the finetuned Qwen1.5-7B-Chat. This observation indicates the necessity of aligning an LLM to a specific domain using customized data in practice.\nThe comparison between Qwen1.5-7B-Chat (QExplorer, LoRA) and Qwen1.5-7B-Chat (SFT, LoRA) demonstrates that further search system preference alignment improves the Query Hit Rate by 10.4% (0.508 v.s. 0.561), enabling the model to extract more effective queries. The objective of preference alignment is to generative queries that can retrieve toxic items, rather than merely producing queries that agree with human annotations, which is the goal of the instruction SFT.\nAblation Study\nIn this section, we evaluate the performance of QExplorer by varying base LLMs and training data as shown in Table 6.\nDifferent Base LLMs. ChatGLM3-6B has a different model architecture and uses different pretraining data compared to Qwen1.5-7B-Chat. The two models' size are comparable. As shown in Table 6, the ChatGLM3-6B's performance is not as good as that of Qwen1.5-7B-Chat on this test dataset, but both LLMs demonstrate that further alignment with preference data results in a higher Query Hit Rate.\nDifferent SFT Data. Table 6 shows that LLMs aligned within the QExplorer framework performs better than those aligned only through SFT. To determine whether improved performance is due to the long context samples not included in dataset D, we conducted experiments by varying the SFT data from D to D + Dcat. Although Qwen1.5-7B-Chat (SFT, LORA, D+Dcat) and ChatGLM3-6B (SFT, LORA, D+Dcat) outperform their respective LLMs finetuned with dataset D, they still cannot surpass the performance of the corresponding LLMs aligned using the QExplorer framework (dataset D is used in the instruction SFT stage). Further preference alignment improves the performance of both Qwen1.5-7B-Chat (SFT, LORA, D+Dcat) and ChatGLM3-6B (SFT, LORA, D+Dcat). We believe the QExplorer framework works in Qwen1.5-7B-Chat and ChatGLM3-6B when using this data recipe. We observe that the improvement provided by QExplorer on Qwen1.5-7B-Chat is more pronounced than on ChatGLM3-6B. This indicates that the ideal alignment recipe for various pretrained models may differ due to differences in model architecture, pretraining data, and training process. This comprehensive comparison was conducted after several months of online implementation. Although Qwen1.5-7B-Chat (QExplorer, LoRA, D+Dcat) has the best performance, our online service uses Qwen1.5-7B-Chat (QExplorer, LoRA) at the time of writing.\nDifferent Preference Data. In Section 3.2, the dataset Dcomp is constructed with a threshold of p() = 0.05. To assess the impact of the threshold for selecting preferred keywords, we constructed another dataset Dcomp2, where p(\u00b7) = 0. In dataset Dcomp2, a larger number of less efficient queries are selected as the preferred keywords. In Table 6, the models in the \"Different Preference Data\" group use dataset D for instruction SFT. We observe that dataset Dcomp2 can still improve the corresponding models' performance in Query Hit Rate, but not as effectively as dataset Dcomp. In term of Number of hit@100, the dataset Dcomp2 fails to improve the performance of the model in the instruction SFT stage.\nPreference Alignment Analysis. Table 6 shows that for the comparison between LLMs trained with SFT and the QExplorer framework, the LLM trained with our QExplorer framework performs better. This indicates that preference alignment can further improve the performance of LLMs in this query extraction task within the above experimental settings. However, not all DPO processes can enhance a model's performance; this study ([29]) shows that DPO cannot surpass the SFT model performance on the code generation benchmark APPS ([12]). In this query extraction task, LLMs trained through SFT tend to generate a wider variety of queries compared to those trained using the QExplorer framework. This suggests that preference alignment reduces the diversity of the generated queries, which is expected, as queries differing only by auxiliary tokens are not helpful for exploration."}, {"title": "4.7 Online Performance", "content": "Following the offline validation of the effectiveness of the Qwen1.5-7B-Chat (QExplorer), we have been deploying this version of LLM in our online service since June 17th, 2024. The implementation details are in Section 3.3. The online performance demonstrated in this study is assessed using data collected over a period of 20 weeks (from 06.17.2024 to 11.02.2024). The results are shown in Table 7 and Figure 4. The LLM suggested queries have an average acceptance rate of 61.6%, which is competitive with that of queries given by humans, especially considering that queries overlapping between the LLM and humans are classified as human annotations. Approximately 20% of the queries suggested by the LLM coincide with human annotations. The average toxic item detection increment is 59.9%, indicating that the queries suggested by the LLM enable auditors to identify 59.9% more toxic items. The average hit query increment is 39.9%.\nConclusion and Discussion\nIn the content safety area, one challenging issue is how to detect all the suspicious content with a limit amount of given toxic content. Xianyu is a second-hand trading platform, and each day a small number of toxic items are reported to our system. To maintain Xianyu as a healthy community, we need to address all these reported toxic items. Previously, the toxic content analysis was performed by auditors. In this study, we propose a method QExplorer to assist auditors to explore toxic items more efficiently. Firstly, we formulate this issue as a query extraction problem using LLM. Next, we construct two datasets from the logs of the online system with well-designed restrictions. To construct long context samples, we cluster the toxic items according to their category and similarity, then concatenate the similar content to form a reasonable long context. For preference alignment, we use the search engine as a reward model to select queries that are more effective. DPO is used to align the LLM with search system preferences. Both the instruction SFT and preference alignment are trained using LoRA. We introduce several metrics to evaluate the effectiveness of the proposed method, QExplorer. Results of offline experiments show that the queries suggested by Qwen1.5-7B-Chat (QExplorer) outperform the queries crafted by human. Online experiments reveal that Qwen1.5-7B-Chat (QExplorer) can suggest a substantial percentage of high quality queries and significantly enhance toxic item detection.\nOur future work will focus on the following aspects: In this study, the data for the preference alignment is scarce. We can use beam search of LLM or results of different version of LLM, along with the feedback from search engine, to construct more data. Currently, we focus only on textual information for toxic content exploration. Visual information is also important, and multimodal finetuning of LLM can be considered in future."}, {"title": "A Data Examples", "content": "A sample from our preference alignment dataset is shown in table 8."}]}