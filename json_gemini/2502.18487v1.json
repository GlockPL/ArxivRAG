{"title": "AuPair: Golden Example Pairs for Code Repair", "authors": ["Aditi Mavalankar", "Hassan Mansoor", "Zita Marinho", "Masha Samsikova", "Tom Schaul"], "abstract": "Scaling up inference-time compute has proven to be a valuable strategy in improving the performance of Large Language Models (LLMs) without fine-tuning. An important task that can benefit from additional inference-time compute is self-repair; given an initial flawed response, or guess, the LLM corrects its own mistake and produces an improved response, or fix. We leverage the in-context learning ability of LLMs to perform self-repair in the coding domain. The key contribution of our paper is an approach that synthesises and selects an ordered set of golden example pairs, or AuPairs, of these initial guesses and subsequent fixes for the corresponding problems. Each such AuPair is provided as a single in-context example at inference time to generate a repaired solution. For an inference-time compute budget of N LLM calls per problem, N AuPairs are used to generate N repaired solutions, out of which the highest-scoring solution is selected as the final answer. The underlying intuition is that if the LLM is given a different example of fixing an incorrect guess each time, it can subsequently generate a diverse set of repaired solutions. Our algorithm selects these AuPairs in a manner that maximises complementarity and usefulness. We demonstrate the results of our algorithm on 5 LLMs across 7 competitive programming datasets for the code repair task. Our algorithm yields a significant boost in performance compared to best-of-N and self-repair, and also exhibits strong generalisation across datasets and models. Moreover, our approach shows significantly stronger scaling with inference-time compute budget compared to baselines.", "sections": [{"title": "1. Introduction", "content": "Recent progress in the field of Large Language Models (LLMs) has resulted in models that keep getting better at generating responses to user queries. When providing these already powerful models with more inference-time compute\u2014increasing number of LLM calls\u2014methods that sample different responses and then select the best among them, such as best-of-N or self-consistency, have shown clear benefits. While these approaches are more breadth-focused, another way to leverage inference time compute is to improve or repair the LLM's initial guesses by generating better fixes (Olausson et al., 2024). We propose combining the benefits of both these approaches to generate a wide set of repaired solutions for poor initial LLM responses, out of which the highest-scoring solution is the final answer.\nTo generate a wide range of repaired solutions for each initial LLM response, we exploit the in-context learning capability exhibited by LLMs. The main contribution of our paper is an algorithm that, given an inference-time compute budget of N LLM calls, produces an ordered set of up to N golden example pairs, or AuPairs\u00b9. Each such AuPair consists of the initial guess and the consequent fix for the corresponding coding problem, along with their respective unit test scores. An example AuPair is illustrated in Fig. 1. At inference time, the contents of an AuPair are concatenated as described in \u00a7 A.7, and provided as a 1-shot example to generate an improved solution or fix for the test problem. This is done for each of the N AuPairs; of all the fixes generated, the one that gets the highest score on the unit tests is selected."}, {"title": "2. Approach", "content": "The goal of our proposed approach in the coding domain is to improve code repair performance on unit tests at inference time, by curating a list of pairs that can be provided as in context examples. The code repair prompt includes an in-context example, followed by a text description of the problem to solve and the initial guess generated by the LLM. The LLM then generates a revision, or a fix that improves performance on the unit tests for that problem. In the prompt, we also include the scores achieved by the guess and fix on the unit tests, but no additional execution feedback.\nIn order to disentangle repair performance from the quality of initial guesses, we first curate composite datasets consisting of initial guesses for all the coding problems. Given a dataset consisting of problems and their corresponding tests, we generate an initial guess for each problem and compute"}, {"title": "2.1. Phase 1: Pair Generation", "content": "In this phase, we generate a large set C of pairs that are potential candidates for our final set of AuPairs. This is done in the following manner: a problem along with its initial guess is sampled from the training dataset D. The LLM generates a fix for this guess. If this generated fix has a higher score on the unit tests for that problem than the initial guess, this guess-fix pair is added to C. Furthermore, if this fix is imperfect, i.e. it does not pass all the unit tests, it becomes a potential guess with further scope for improvement, so it is added as a new guess to the training dataset D. This is repeated several times to collect a large set of such candidate pairs. A visual illustration of this phase is provided in Fig. 2.\nWhile we include the pair generation phase for completeness, it is important to note that in several other domains, paired data may already be available. In such cases, the set of candidate pairs C that we curate in this step, can simply be replaced by the given paired data."}, {"title": "2.2. Phase 2: AuPair Extraction", "content": "Now that we have a large set C of candidate pairs, the next step is to determine which of these will actually help boost performance, i.e., which of these are AuPairs. We do this in a submodular fashion by making use of the validation dataset Dval. For each pair-problem combination $(c_i, x_j) \\in C \\times D_{val}$, we build a 1-shot prompt p using the prompting strategy described in A.7. This 1-shot prompt p is given as input to the LLM, which generates a fix for the given problem $x_j$. The fix generated by the LLM is then evaluated on the unit tests and stored in the fix quality matrix $M \\in \\mathbb{R}^{|C| \\times |D_{val}|}$ at index (i, j). This part of AuPair extraction is outlined in Algorithm 1."}, {"title": "3. Experiments", "content": "Datasets: We use 7 datasets that contain problems and test cases from competitive programming contests: 1) CodeForces (8.8k problems), 2) AtCoder (1.3k problems), 3) HackerEarth (1.2k problems), 4) CodeChef (768 problems), 5) LiveCodeBench (400 problems), 6) CodeJam (180 problems), and 7) Aizu (2.2k problems) We choose CodeForces and AtCoder, separately, for in-distribution testing, and use the rest exclusively for out-of-distribution"}, {"title": "3.1. Significantly Boosted Code Repair Performance", "content": "The first step to assess code repair performance is to measure in-distribution performance; namely generating and selecting AuPairs on the training and validation sets that match the test dataset, and using the same model for evaluation as AuPair construction. We do this for 2 datasets (CodeForces and AtCoder) and all 5 models. Fig. 4 shows the resulting comparison between the best-of-N and self-repair baselines and AuPair, for a budget of N = 32 LLM calls at inference time. AuPair is clearly superior to best-of-N and self-repair (matching in a few cases) on all models and datasets, sometimes by wide margins. This clearly establishes that our proposal of providing a different in-context example of code repair in each LLM call can significantly boost performance.\nAn interesting side-result is visible in initial performance, i.e., the performance of the initial responses of the LLMs to the problems, which have to then be repaired. Gemini-1.5-Pro, despite being a superior model to Gemini-1.5-Flash, shows worse initial performance. Since the code generated has certain conditions that allow successful execution, we observe that many initial guesses of generated code fail because they do not obey these conditions (see Appendix \u00a7A.8). In such cases, code repair with either best-of-N or self-repair is unlikely to give us high boost in performance since the initial solution is badly formatted. This is one clear case where having an AuPair in context significantly improves performance. As a result, using AuPairs in conjunction with high performing models leads to large performance improvements despite poor initial performance, as we can see for both CodeForces and AtCoder with the Gemini-1.5-Pro model in Fig. 4. This also mitigates the need for more sophisticated prompt engineering to a large extent."}, {"title": "3.2. Selection Matters: AuPairs are More Effective than Random Pairs", "content": "We design an ablation to disentangle the two possible sources of improvement that our approach demonstrates, namely 1) in-context learning and 2) the choice of AuPairs. It is not implausible for"}, {"title": "3.3. Better Scaling with Inference-Time Compute", "content": "At a fixed budget of N = 32 LLM calls, our results look promising. In this section, we investigate whether and how performance scales with the compute budget N. Fig. 5(b) plots the score as a function of N using Gemini-1.5-Pro on the CodeForces dataset (additional scaling curves for Gemini-1.5-Flash and strict accuracy metric in the Appendix, see Figs. 11 and 12). For each additional LLM call, we use the next best AuPair produced by the algorithm and provide it in context to generate the LLM response. Our algorithm produces 144 AuPairs for the CodeForces dataset using Gemini-1.5-Pro, and achieves a test pass rate of 51.32% and strict accuracy of 39.73% (see \u00a7A.3) at 144 LLM calls. The results shows a clear scaling trend with a consistent log-linear performance increase as a function of compute, without any sign of a plateau. More importantly, the increase is substantially steeper than the best-of-N and self-repair baselines (which achieve test pass rate of 16.05% and 15.79% and strict accuracy 12.04% and 12.23% respectively); in other words, prompting with in-context complementary AuPairs makes more efficient use of compute than either repeated sampling given a fixed repair prompt, or repair with model-generated verbal feedback."}, {"title": "3.4. Strong Generalisation to Out-of-distribution Datasets", "content": "The aim of this set of experiments is to determine whether our approach exhibits out-of-distribution generalisation, i.e., given AuPairs collected on a different dataset, see if we can retain the performance"}, {"title": "3.5. Decent Cross-Model Transfer", "content": "Now that we have seen that our approach can exhibit very good out-of-distribution generalisation along the data axis, we evaluate it on its ability to generalise on the model axis, i.e., we look at the performance of AuPairs collected using a different model. We evaluate this cross-model transfer capability for several model combinations on CodeForces. The resulting 16 ablations are shown in Fig. 7(a), and help disentangle the impact of the AuPairs versus the code repair capabilities of the inference model. A key takeaway is that the Gemma models exhibit worse performance, regardless of the quality of AuPairs used at inference time, indicating that they are inferior at the capability of code repair. Gemini-1.5-Flash performs much better at code repair, and its sensitivity to the source of AuPairs is negligible: it is equally performant for each source. Gemini-1.5-Pro, on the other hand, is sensitive to the source of AuPairs; in particular, when Gemini-1.5-Pro uses AuPairs collected by the same model, it achieves the best performance by a large margin. With AuPairs selected using other models, Gemini-1.5-Pro achieves comparable performance to Gemini-1.5-Flash. One reason for the standout performance when using Gemini-1.5-Pro AuPairs seems that those examples result in substantially more diverse generations, as shown in \u00a73.6. However, Fig. 7(a) as a whole suggests that there is an ordering in terms of performance: 1) the model used at inference time has to have good code repair capabilities, and 2) the stronger the model is at code repair, the more improvement we can expect from it with a higher quality of AuPairs."}, {"title": "3.6. High Code-specific Diversity", "content": "We dive a bit deeper into the nature of fixes generated using different AuPairs. There are several ways to analyse code; we choose Abstract Syntax Trees (ASTs) since they mostly capture the structure of changes. More concretely, since we have N fixes for each problem (here N = 32), we measure the diversity per problem as the number of unique changes made to the guess over all N fixes for that problem. The diversity score is calculated as the average number of unique abstract syntactic subtrees generated per problem. More concretely, we perform the set difference of all subtrees in the fix AST that are not in the guess AST and normalise with the maximum number of subtrees. We plot this diversity metric against the score in Fig. 7(b) to get a sense of how diverse and useful the AuPairs are. We also include diversity results of the best-of-N baseline, see A.6 for further details on the diversity score computation. The results show that while AuPairs always increase performance, they result in higher diversity of fixes when given to the more competent models (Gemini-1.5-Pro and -Flash), and lower diversity for Gemma models. It is worth highlighting that the exceptional performance of AuPairs produced and used by Gemini-Pro (Fig. 7(a), bottom right) corresponds to highly diverse fixes (Fig. 7(a), top right)."}, {"title": "3.7. Improvement on All Difficulty Levels", "content": "Coding datasets have heterogeneous difficulty. As a sanity check, we conduct additional analysis to determine which problem levels are most helped by AuPair, compared to the quality of initial guesses. Table 1 shows the absolute improvement in test pass rate, i.e., the increase in this score achieved by AuPair for 4 models on CodeForces. The two key observations are (a) AuPair helps significantly at all difficulty levels across models, and (b) there are larger improvements on easier levels, and this trend is consistent across models. Note that the initial performance of Gemini-1.5-Pro is low because the initial guesses generated do not adhere to the instruction (elaborated in Appendix \u00a7A.8); however since this is the strongest model and shows the best overall performance across difficulty levels, the increases in score that we see are significantly higher than the other models."}, {"title": "3.8. Coverage of Problem Categories is Preserved", "content": "The CodeForces dataset is richly annotated with category labels for each problem. A problem may have multiple tags, for instance, strings and two pointers. We use these fine-grained tags to study how the problem distribution is affected by Phase 1 and Phase 2 of our method, separately. Fig. 8 shows the proportions of these categories observed in the initial dataset, the full set of pairs generated during Phase 1, and the final AuPairs. The high-level result is encouraging, namely that the starting diversity is approximately preserved. Phase 1 yields pairs for every single category, even those that lie at the tail. Furthermore, the (sparser) distribution over categories for the AuPairs after Phase 2 still shows several problems from rare categories. This additional result consolidates our insight that AuPairs are highly diverse, also in the types of problems they contain."}, {"title": "4. Related Work", "content": "Automatic Program Repair (APR) has been a longstanding research area in the field of machine learning Most methodologies rely on supervised finetuning to adapt LLMs to the task of code generation using labeled pairs of broken / fixed code pairs, which is costly to obtain and often task- and problem-specific. On the other hand, unsupervised APR is challenging since it requires syntactic and semantic understanding of code, and most automatic code breaking approaches tend to be out-of distribution with real samples. Our work uses partial fixes generated by the model as the initial broken code to be fixed iteratively.\nMore recently, a few unsupervised approaches have been proposed based on the capability of LLMs to generate code. The APR task still remains challenging, even though models are better at generating code. Zhao et al. (2024) use a step-by-step method to repair code using a reward model as a critic, providing feedback to finetune an LLM. Shypula et al. (2024) propose a retrieval based few-shot prompting approach with Chain-of-Thought (CoT) reasoning traces, and use supervised fine-tuning (SFT) to finetune a model using self-play.\nThe main disadvantage of using SFT approaches comes from the need to finetune the model to the task, which becomes much more costly with ever-growing model sizes. In recent years the in-context learning (ICL) paradigm has been shown to be a flexible and compute-efficient adaptation approach to new tasks. Le et al. (2022) use"}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Pair Generation", "content": "In this section, we discuss the specifics of the pair generation phase and provide results pertaining to this phase. The approach that we use for pair generation is provided in Algorithm 3. Note that this is one way to generate pairs; they can be generated in other ways, or be available beforehand. Studying the impact of using pre-generated pairs for extracting AuPairs could be an interesting avenue for future work."}, {"title": "A.2. Measuring correctness in terms of solved problems", "content": "In addition to pass rate of unit tests, we also report the percentage of fully solved problems, for which the generated code passes all test cases.\nstrict accuracy $ =  \\frac{1}{P}\\sum_{p=1}^P max_{i \\in \\{1,...,N\\}} \\frac{1}{T_p}\\sum_{j=1}^{T_p}  1\\{eval (codep,i, Tp,j) == pass\\}$\nwhere Tp refers to the unit tests for problem p, and codep,i is the code generated by the LLM for problem p in the ith LLM call. Here, the innermost loop, like test pass rate, computes the percentage of unit tests passed by the LLM output codep,i. Following this, we select the code output that passes all tests (max over binary values yields 1 if any such output exists, otherwise 0). The outermost loop averages this across all the problems in the test dataset.\nWe see that AuPair outperforms all other baselines on all models across the board, with results for CodeForces and AtCoder shown in Fig. 9. We also show the results for out-of-distribution generalisation on this strict accuracy metric in Fig. 10; again, the results clearly indicate that AuPair outperforms all baselines on this metric as well across all datasets. Furthermore, the scaling results in \u00a7A.3 also indicate that the scaling trends for the strict accuracy metric are consistent with those of the test pass rate metric."}, {"title": "A.3. Scaling Inference Compute", "content": "In addition to the scaling experiment we performed using Gemini-1.5-Pro (results in Fig. 5(b)), we also perform the same scaling experiment using Gemini-1.5-Flash and show the results in Fig. 11. Moreover, we report the results of the same scaling experiment on the strict accuracy metric in Fig. 12. The trend is similar to what we observed before: best-of-N plateaus after a certain number of LLM calls, while our approach scales as the compute budget increases, delivering an improvement in performance for each newly included AuPair. The self-repair baseline performs better with the Gemini-1.5-Flash model than with the Pro model; our hypothesis is that since the initial guesses for the Pro model were worse because of formatting issues, self-repair did not yield significant improvements. However, when the initial guesses are better, the self-repair baseline shows a stronger scaling result. Our algorithm yields 110 AuPairs and achieves a final test pass rate of 37.83% and strict accuracy 24.14%. Best-of-N, on the other hand, given the same budget of 110 LLM calls, has a test pass rate of 21.8% and strict accuracy 11.93%. Self-repair with the same compute budget has a final test pass rate of 34.1% and strict accuracy 22.39%. Since our AuPairs are selected submodularly, the initial pairs yield high returns in performance and these returns start diminishing slowly, but notably, performance does not plateau yet. Thus, it is abundantly clear that using AuPairs has a distinct advantage over currently used approaches like best-of-N and self-repair in improving performance at inference time as compute budget increases."}, {"title": "A.4. Code Repair with LiveCodeBench", "content": "Generalisation of AuPair prompting is important to improve code repair of smaller datasets. We posit that the AuPairs contain diverse code changes that transfer meaningfully across datasets, which may be important to those with scarce data, since out-of-distribution generalisation becomes especially relevant when we have small datasets, on which it can be quite difficult to obtain many different AuPairs.\nWe now show the results obtained for a smaller dataset (400 problems) LiveCodeBench (LCB) We generate the same train/val/test split (37.5/12.5/50%) over 400 problems and apply our AuPair approach to obtain in distribution AuPairs for LCB.\nshows that even with smaller number of selected AuPairs we still obtain a gain over best-of-N prompting. We obtained 5 AuPairs with the submodular extraction in Algorithm 2 for all the models except Gemma-9B which obtained only 3 AuPairs. Given the difference in dataset size these values are larger in proportion to the ones obtained from a larger dataset CodeForces (8.8k problems, 144 extracted AuPairs).\nAnother interesting result in Fig. 13 is that both metrics, the test pass rate and strict accuracy, are comparable when using in-distribution AuPairs from LiveCodeBench and out-of-distribution AuPairs"}, {"title": "A.5. Lineage", "content": "Here we look at the lineage of each pair generated during phase 1 of our algorithm, pair generation. The key idea here is to see if the set of all pairs collected during the pair generation phase are deeper i.e., they generate iteratively better solutions for a smaller set of problems, or broader i.e., they generate solutions for a larger set of problems but those solutions may not necessarily be perfect. The last plot in Fig. 14 (pairs generated on the CodeForces dataset using Gemini-Pro-1.5) indicates that the pairs collected have shallow lineage: a large proportion of guesses that had a score of 0 had corresponding fixes with perfect score at depth 1. We also see that the number of fixes decreases as depth increases (as seen from the size of the circles), indicating that several problems could not be improved beyond a certain point, or that they were not resampled during the pair generation phase. In both these cases, one solution is to allow more LLM calls during phase 1 to allow each problem to be sampled for repair more times. The takeaway here is that more sophisticated fixes for difficult problems can be discovered as we increase the budget of LLM calls during the pair generation phase. The entire evolution of this lineage at different points during pair generation is illustrated in Fig. 14."}, {"title": "A.6. Code Diversity", "content": "We compute the code diversity score in Fig. 7(b) based on the number of different abstract syntax subtrees that each code instance produces. Algorithm 4 describes how this diversity score is computed. As a first step, for each guess in the test dataset and its corresponding fix generated by the LLM, we compute the respective abstract syntax subtrees. Next, we compute their corresponding set difference to get the unique subtrees for each code diff. This is done N times for a compute budget of N and the number of code diff subtrees across pairs and problems is averaged and normalised in the following manner to yield the diversity score 8:\n$\\delta = \\frac{1}{N|D_{test}|S_{max}} \\sum_{i=1}^{|D_{test}|} |S_i^{diff}| $   (1)\nwhere $S_i^{diff}$ is the set of all code diff subtrees generated with compute budget N for problem i, and the normalising factor $S_{max}$ corresponds to the highest number of subtrees that are present in any such set of code diff subtrees."}, {"title": "A.7. Prompting", "content": "There are 2 types of prompts that we use: 1) guess generation prompt, and 2) repair prompt. The guess generation prompt is used during dataset creation, for obtaining the initial guesses for all problems in the dataset. The repair prompt is used throughout the rest of the paper: in the Pair Generation (Phase 1, \u00a72.1 with k = 32 random examples) and in the AuPair Extraction (Phase 2, \u00a72.2) and during inference, with k = 1. The function signature indicates that the function expects a string as an input. The instruction specifies that the final answer is meant to be printed inside the function, and that the main function is not meant to be written.\nThe structure of our repair prompt is as follows: there is an instruction at the top, followed by the"}, {"title": "A.8. Code Execution", "content": "When the LLM generates a fix for any problem, we call the solve() function for each test case associated with that problem. We then compare the output with the ground truth and give a partial score corresponding to the proportion of test cases passed by this fix.\nAn important point to note is that the solve() function has to take as input a string, which is then parsed into the correct variables. This formatting requirement is a key reason for the poor initial performance of Gemini-1.5-Pro in Fig. 4. Since the instruction for generating the initial guess is not correctly followed by the model, a lot of guesses end up invariably having incorrect parsing of the input, leading to low scores. A lot of AuPairs extracted using these models, as a result, contain this formatting fix, as we will see in \u00a7A.9."}, {"title": "A.9. Types of Fixes in AuPairs", "content": "We now show some examples of AuPairs and highlight the differences between the guess and fix for each pair. These are a mix of CodeForces pairs collected using different models. The scores achieved by the guess and fix on the corresponding problem's test cases are specified at the top right corner for each example in Fig. 15. We also provide a short description for each type of fix in the caption for ease of understanding. The types of pairs discovered using our algorithm cover a large area of potential fixes that can be made to an initial buggy piece of code: from smaller ones like parsing, fixing logical bugs pertaining to indexing errors, variable initialisations, etc., to larger changes like rewriting parts of the code, or even suggesting alternate routes to solve the same problem."}, {"title": "5. Conclusions and Future Work", "content": "We propose a novel algorithm, which produces an ordered set of AuPairs, each of which can be provided as an in-context example using 1-shot prompting with an inference compute budget of N LLM calls to improve code repair performance at inference time. Our approach is highly scalable, showing significantly better outcomes than best-of-N and self-repair, both of which are known to improve performance as inference compute is scaled up. In addition to this, the AuPairs generated"}]}