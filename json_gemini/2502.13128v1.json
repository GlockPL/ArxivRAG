{"title": "SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation", "authors": ["Zihan Liu", "Shuangrui Ding", "Zhixiong Zhang", "Xiaoyi Dong", "Pan Zhang", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "abstract": "Text-to-song generation, the task of creating vocals and accompaniment from textual inputs, poses significant challenges due to domain complexity and data scarcity. Existing approaches often employ multi-stage generation procedures, resulting in cumbersome training and inference pipelines. In this paper, we propose SongGen, a fully open-source, single-stage auto-regressive transformer designed for controllable song generation. The proposed model facilitates fine-grained control over diverse musical attributes, including lyrics and textual descriptions of instrumentation, genre, mood, and timbre, while also offering an optional three-second reference clip for voice cloning. Within a unified auto-regressive framework, SongGen supports two output modes: mixed mode, which generates a mixture of vocals and accompaniment directly, and dual-track mode, which synthesizes them separately for greater flexibility in downstream applications. We explore diverse token pattern strategies for each mode, leading to notable improvements and valuable insights. Furthermore, we design an automated data preprocessing pipeline with effective quality control. To foster community engagement and future research, we will release our model weights, training code, annotated data, and preprocessing pipeline. The generated samples are showcased on our project page at https://liuzh-19.github.io/SongGen/, and the code will be available at https://github.com/LiuZH-19/SongGen.", "sections": [{"title": "1. Introduction", "content": "Songs, blending vocals with instrumental accompaniment, are a cornerstone of musical expression. Unlike purely instrumental music, songs uniquely capture human emotions through emotive lyrics and diverse melodies. However, creating a song is a complex, multi-stage process involving composition, instrumental arrangement, vocal performance, and more. This process requires substantial time and expertise, making it challenging for most individuals. With the rise of AI Generated Content (AIGC), creative fields have been revolutionized, extending from text and image generation to sophisticated artistic domains like music. Building on these advancements, text-to-song generative models aim to transform natural language descriptions into full-song audio, making music creation more accessible and efficient.\nSong generation presents greater complexity than speech or instrumental music generation, and the scarcity of open-source data further limits research in this area. Current approaches tackle this challenge by separating vocals and accompaniment into distinct tracks, relying on multi-stage generation processes. As illustrated in Figure 1, these models first generate the vocal track from lyrics, then produce the accompaniment using natural language prompts alongside the generated vocals. However, multi-stage generation results in cumbersome training and inference pipelines while lacking unified control over both vocals and accompaniment. To improve pipeline simplicity and control flexibility, an important question arises: Is it possible for a single-stage model to achieve effective text-to-song generation?\nIn this paper, we introduce SongGen, a fully open-source, single-stage text-to-song generation model based on an auto-regressive transformer architecture. SongGen transforms lyrics and descriptive text into songs with harmonized vocals and accompaniment, allowing fine-grained control over instruments, genre, mood, timbre, and other musical elements. With a three-second reference vocal clip, it also supports zero-shot voice cloning. These user-defined controls are incorporated through modal-specific encoders, learnable projectors, and cross-attention mechanisms. SongGen offers two flexible generation modes: mixed mode, which blends vocals and accompaniment into a single output, and dual-track mode, which synthesizes them separately to facilitate professional post-production editing.\nHowever, due to the sophisticated relationship between vocals and accompaniment in a song, jointly predicting them with natural expressiveness is a non-trivial task. To this end, we perform extensive explorations into output token patterns, yielding valuable insights. Specifically, (1) in mixed mode, while the model generates high-quality accompaniment, it struggles with natural-sounding vocals. Accompaniment, with higher energy and stable spectral distribution, is easier to produce, whereas vocals, with higher semantic density and a lower signal-to-noise ratio due to overlap, pose a greater challenge. This learning bias makes it difficult to generate vocals with clear lyrics, a problem typically addressed by decoupling and multi-stage methods. To mitigate this issue, we introduce an auxiliary vocal token prediction target, enhancing the model's focus on vocal features and significantly improving vocal clarity in mixed-token outputs. (2) In dual-track mode, vocals and accompaniment are treated as distinct yet interconnected sequences, generated in sync by a single transformer decoder. We explore various track combination patterns to maintain precise frame-level alignment. Experimental results indicate that the optimal pattern yields well-coordinated vocals and accompaniment, achieving quality on par with mixed-mode generation.\nMoreover, the text-to-song generation community has long been constrained by data scarcity. To the best of our knowledge, no publicly available dataset currently includes paired audio, lyrics, and captions. To bridge this gap, we develop an automated pipeline for data cleaning, processing, and quality filtering, resulting in a high-quality dataset of 540K song clips spanning over 2,000 hours of audio.\nTo evaluate the effectiveness of the proposed SongGen framework, we conduct extensive experiments on the MusicCaps test set, using both objective and subjective evaluations. The results demonstrate that SongGen generates songs with excellent musicality and vocal-instrument harmony, achieving performance that is competitive with the ground truth. Surprisingly, the generated songs feature expressive vocal techniques, such as vibrato, enhancing naturalness and authenticity. Our contributions can be summarized as follows:\n\u2022 We introduce SongGen, a single-stage auto-regressive transformer for text-to-song generation, offering versatile control via lyrics, descriptive text, and an optional reference voice.\n\u2022 SongGen supports both mixed and dual-track mode to accommodate diverse requirements. Our experiments provide valuable insights for optimizing both modes.\n\u2022 By releasing the model weights, code, annotated data, and preprocessing pipeline, we aim to establish a simple yet effective baseline for future song generation research."}, {"title": "2. Related Work", "content": "2.1. Text-to-Music Generation\nIn recent years, significant progress has been made in text-to-music generation models, which use descriptive text as a condition for controllable music generation. Several works employ transformer-based language models (LMs) to model sequences of discrete tokens derived from audio codecs. Diffusion models, another competitive class of generative models, have also attained impressive results in music generation. However, although all the models discussed above excel at generating high-quality instrumental music, they face significant challenges in producing realistic vocals.\n2.2. Song Generation\nRecently, a few studies have begun exploring song generation, a task that involves vocal composition, instrumental arrangement, and harmonious generation. One of the pioneering efforts is Jukebox, which employs a multi-scale VQ-VAE to compress audio into discrete codes and models them using a cascade of transformer models. However, Jukebox offers limited style control, relying solely on genre tags and artist names, and suffers from long inference times. Recently, models like Melodist and MelodyLM have adopted multi-stage approaches to address the challenges of text-to-song generation. Melodist integrates singing voice synthesis with vocal-to-accompaniment (V2A) techniques, while MelodyLM improves upon Melodist by overcoming its reliance on music scores through a three-stage process: text-to-MIDI, text-to-vocal, and V2A. However, both approaches result in cumbersome training and inference procedures, and their corpus is limited to Mandarin pop songs, lacking diversity. Another model, Song Creator, utilizes a dual-sequence language model to capture the relationship between vocals and accompaniment. However, it lacks text-based control and produces vocals with limited clarity. Freestyle focuses on generating rapping vocals from lyrics and accompaniment inputs but is constrained to a single musical style, with rap typically featuring simpler melodies. Although industry tools like Suno and Udio have recently emerged for song generation, neither has disclosed their methodologies or expanded into broader controllable generation tasks. SeedMusic leverages both auto-regressive language modeling and diffusion approaches to support song generation. However, SeedMusic is not open-source and relies on a large proprietary dataset, making a fair comparison with our fully open model unfeasible."}, {"title": "3. Methodology", "content": "3.1. Overview\nThe objective of this paper is to guide the generation of a song using a text description, lyrics, and an optional reference voice. As illustrated in Figure 2, SongGen is composed of an auto-regressive transformer decoder with an off-the-shelf neural audio codec. The transformer decoder predicts a sequence of audio tokens, allowing control through user inputs via cross-attention. The final song is synthesized from these tokens using the codec decoder. In the subsequent section, we will elaborate on the details of SongGen. Section 3.2 will introduce the two generation modes supported by our unified framework: mixed mode and dual-track mode. Section 3.3 will discuss the lyric, voice, and text conditions. Section 3.4 will outline our data processing pipeline and quality filtering metrics. Section 3.5 will present our training scheme for progressively enhancing model performance.\n3.2. Auto-regressive Codec Language Modeling\n3.2.1. AUDIO TOKENIZATION\nThe effectiveness of the audio tokenizer is critical to the success of transformer-based song generation. Our framework is compatible with mainstream Codec designs. In experiments, we employ X-Codec, an audio codec based on Residual Vector Quantizer (RVQ), to produce discrete audio tokens. It utilizes $N_q = 8$ codebooks, each with a codebook size of $K = 1024$. Given an audio signal $X \\in R^{d \\cdot f_s}$, where $d$ is the audio duration and $f_s = 16$ kHz is the sampling rate, X-Codec encodes and quantizes $X$ into a sequence of token vectors $S = [s_1, s_2, ..., s_T] \\in R^{N_q \\times T}$, where $T = \\frac{d}{f_r}$ and $f_r = 50$ HZ is the frame rate. Each vector $s_t = [s_{1,t}, s_{2,t}, ..., s_{N_q,t}]$ consists of $N_q$ codes, with $s_{k,t}$ taking integer values from 0 to $K - 1$ for $k \\in [1, N_q]$. We apply the codebook-delay pattern to handle the multiple code sequences within a single transformer decoder architecture. Figure 3 at the top-right corner illustrates this process for the case of $N_q = 3$, where a one-step delay is maintained between adjacent sequences from different codebooks. After applying the delay pattern,"}, {"title": "3.2.2. \u039c\u0399\u03a7XED MODE", "content": "In mixed mode generation, we directly use the mixed audio tokens $\\hat{S}_{mixed}$, which are encoded by X-Codec from mixed audio (i.e. raw audio), as the output target. For each step, the vector of audio tokens from $N_q$ codebooks are embedded using a group of $N_q$ learnable embedding matrices, and then summed up to form the decoder input. Additionally, a sinusoidal positional embedding is added at each step. The last hidden state of decoder is passed to a group of $N_q$ linear heads, with each head predicting the logits corresponding to its respective codebook.\nDuring training, we employ the teacher-forcing scheme. Since each quantizer in the RVQ encodes the quantization error from the previous quantizer, earlier codebooks are more critical. Therefore, we compute a weighted sum of the losses from different codebooks, assigning higher importance to the losses from earlier codebooks:\n$\\mathcal{L}_{mixed} = \\sum_{k=1}^{N_g} w_k \\cdot \\mathcal{L}_{k}^{mixed},$  (1)\nwhere $k$ denotes the codebook index, and $w_k$ represents the weight, satisfying $w_k \\leq w_j$ for $k < j$ and $\\sum_{k=1}^{N_g} w_k = 1$. $\\mathcal{L}_{k}^{mixed}$ is the cross-entropy loss for the k-th codebook.\nHowever, this basic approach, referred to as \"Mixed\", presents challenges in producing coherent and clear vocals. In mixed audio, vocals suffer from a low signal-to-noise ratio because of overlap with the accompaniment. While the accompaniment typically exhibits higher energy and a more stable spectral distribution, the vocals tend to be sparser, more irregular, and prone to greater instantaneous frequency fluctuations. For example, vocals often feature rapid pitch changes to perform various singing techniques. Moreover, vocals carry more semantic meaning from the lyrics. When mixed audio is used as the training target, the model tends to prioritize the more predictable accompaniment, often neglecting the vocal features. Nevertheless, human perception is sensitive to the naturalness and clarity of vocals, making these aspects critically important in song generation.\nBuilding on this, we propose a method called \"Mixed Pro\" that emphasizes vocal learning by introducing an auxiliary vocal token prediction target. As depicted in Figure 3 (a), we incorporate a dedicated group of linear heads to predict logits for vocal tokens. These tokens, encoded by X-Codec from the vocal track, are aligned frame-by-frame with the mixed tokens. The overall loss function is formulated as:\n$\\mathcal{L}_{mixed-pro} = \\mathcal{L}_{mixed} + \\lambda \\mathcal{L}_{vocal},$ (2)\nwhere $\\lambda$ controls the contribution of the vocal loss to the total loss. It is important to note that these newly introduced vocal heads are used only during training to compute the auxiliary loss and do not affect inference."}, {"title": "3.2.3. DUAL-TRACK MODE", "content": "In dual-track generation mode, the two key components of a song\u2014the vocal and the accompaniment\u2014are separated, and SongGen synchronously generates both tracks within this unified framework. Considering the importance of harmony between vocals and accompaniment, we introduce two combination patterns, namely Parallel and Interleaving, to ensure frame-level alignment across the two tracks.\nParallel: Inspired by the stereo channel modeling of MusicGen , which simultaneously outputs audio tokens for two channels, we design a parallel pattern. As shown in Figure 3 (b), the accompaniment and vocal audio tokens are concatenated along the codebook dimension, with each step containing $N_q$ vocal tokens and $N_a$ accompaniment tokens. On the temporal dimension, we introduce three variants. In the \"Standard\" variant, the audio tokens for both tracks are strictly aligned frame by frame. The \"Parallel (A-V)\" variant delays the vocal tokens by one step relative to the accompaniment tokens. Thus, the vocal token prediction at each frame considers both the previous vocal token and the accompaniment token at the current frame. Conversely, in the \"Parallel (V-A)\" variant, the accompaniment tokens are delayed by one step relative to the vocal tokens. Two groups of code embeddings are used to separately embed the audio tokens for the two tracks. All embeddings are then averaged to form a combined input. Two groups of linear heads are employed to predict the audio tokens for each track. The training loss is defined as:\n$\\mathcal{L}_{parallel} = \\frac{1}{2} (\\mathcal{L}_{vocal} + \\mathcal{L}_{acc}),$ (3)\nwhere $\\mathcal{L}_{vocal}$ and $\\mathcal{L}_{acc}$ represent the individual losses for the vocal and accompaniment tracks, respectively. The calculation method is the same as in Equation 1.\nInterleaving: In this pattern, the audio tokens of the two tracks are interleaved along the temporal dimension, as illustrated in Figure 3 (c). There are two variants: \"Interleaving (A-V)\", where the accompaniment tokens precede the vocal tokens at each frame; and \"Interleaving (V-A)\", where the vocal tokens precede the accompaniment tokens. In the \"Interleaving (A-V)\" variant, each vocal token prediction at a given frame considers both the previous vocal token and the accompaniment token from the same frame, with the reverse for the \"Interleaving (V-A)\" variant. In this pattern, only a single group of code embeddings and one group of heads are used. The training loss is calculated in the same way as in Equation 3.\nAlthough the interleaving pattern requires longer sequence lengths than the parallel pattern, it provides a more effective approach to modeling the relationship between vocals and accompaniment. In the lower layers of the transformer, the interleaving pattern facilitates learning the interactions between the vocal and accompaniment tracks, while the higher layers focus on refining the distinct characteristics of each track. The attention visualizations in Figure 5 provide additional evidence for this. In contrast, the parallel pattern is unable to decouple the vocal and accompaniment information before reaching the heads."}, {"title": "3.3. Model Conditioning", "content": "Lyrics Conditioning. To address the challenge of data scarcity, we apply a 6681-token voice Byte-Pair Encoding (VoiceBPE) tokenizer to convert the lyrics $C_{lyrics}$ into a sequence of phoneme-like tokens. Word-level tokenizers, like the T5 tokenizer, lead to sparse training samples for each token embedding. In contrast, VoiceBPE not only enhances the model's ability to generalize to unseen words but also adapts more effectively to the variations in phoneme duration and pitch range inherent in sung vocals. Subsequently, the lyrics embedding $E_{lyrics} \\in \\mathbb{R}^{T_l \\times F_l}$ is obtained by passing the lyric tokens through a small transformer-based encoder (i.e., Lyrics Encoder) to extract critical pronunciation-related information. Here, $T_l$ denotes the length of the lyric tokens, and $F_l$ represents the dimensionality of the embedding.\nVoice Conditioning. As demonstrated by the Marble benchmark, MERT , a music representation model, consistently achieves state-of-the-art performance in vocal technique detection and singer identification tasks. Consequently, we employ a frozen MERT encoder to generate robust voice feature embeddings, enabling control over vocal timbre and singing techniques. Specifically, we randomly select 3-second clips from vocal segments to serve as the voice condition input, denoted as $C_{voice}$. The outputs from MERT's 24 hidden layers and 1 output layer are aggregated via a 1D convolutional layer, yielding the voice embedding $E_{voice} \\in \\mathbb{R}^{T \\times F_v}$, where T denotes the temporal length and $F_v$ represents the feature dimensionality of the embedding.\nText Conditioning. Our text descriptions cover a wide range of musical attributes, including but not limited to the instruments used, musical emotion, tempo, genre, and the singer's gender, offering more depth than simple tags or short phrases. Given a description $C_{text}$ matching the song, we apply a frozen FLAN-T5 encoder to obtain the text embedding, denoted as $E_{text} \\in \\mathbb{R}^{T_t \\times F_t}$.\nThe above three condition embeddings-$E_{lyrics}$, $E_{voice}$, and $E_{text}$-are each passed through their respective projection layers to obtain transformed embeddings, $\\hat{E}_{lyrics}$, $\\hat{E}_{voice}$, and $\\hat{E}_{text}$. These embeddings are then concatenated along the temporal dimension:\n$E_{cond} = E_{voice} \\oplus E_{text} \\oplus E_{lyrics} \\in \\mathbb{R}^{(T_v+T_t+T_l) \\times D},$ (4)\nwhere D denotes the dimension of the decoder hidden states. This concatenated embedding $E_{cond}$ is used to control song generation via cross attention."}, {"title": "3.4. Automated Data Preprocessing Pipeline", "content": "To the best of our knowledge, there is currently no publicly available dataset for text-to-song generation that includes paired audio, lyrics, and captions. To address this gap, we develop an automated data annotation pipeline that incorporates several filtering strategies to ensure high-quality data. (1) Data Source: We collect 8,000 hours of audio from Million Song Dataset (MSD), Free Music Archive (FMA) and MTG-Jamendo Dataset. (2) Source Separation: We utilize Demucs to separate vocals and accompaniment from the original audio. (3) Segmentation: We employ a voice activity detection (VAD) tool to detect voiced segments in the separate vocal tracks. Vocal, accompaniment, and mixed tracks are then sliced according to the VAD results, with an average clip duration of 15 seconds. Additionally, the energy of each clip is calculated as the sum of the squared amplitude over time, providing a measure of loudness. Clips with low energy in either the accompaniment or vocals are discarded. (4) Lyric Recognition: Lyric recognition accuracy is crucial for song generation, but it is challenging. Existing Automatic Speech Recognition (ASR) models, trained on speech data, struggle with the complexity and variability of sung vocals. Errors arise from two main factors: ASR limitations (misrecognitions and hallucinations); and inherently unclear vocal data, such as noise or genre-specific characteristics like those in rock music. To tackle this issue, we apply two ASR models, Whisper-large-v2 and Whisper-larger-v3, to automatically transcribe the vocals and generate two lyric transcriptions. We compute the edit distance between them to assess quality, excluding clips with an edit distance greater than 20%, and retaining only those with relatively clearer vocals and higher recognition confidence. (5) Captioning: We use LP-MusicCaps-MSD for MSD captions. For song clips without captions, we generate pseudo-captions using a music captioning model. The accuracy of the captions is evaluated by CLAP Score, which measures the alignment between audio and text with the official CLAP model. Samples with low CLAP scores are discarded, and any available original tags are added as a supplement. After preprocessing, the training dataset contains about 540K English-voiced clips, totaling around 2K hours of audio."}, {"title": "3.5. Training Scheme", "content": "Mixed Mode Training. Our mixed mode training consists of three key steps, aimed at progressively boost model performance. Step 1: Modality Alignment. We train the entire model using total paired data to align the modalities between the various conditioning inputs and the audio output. Step 2: Voice-Free Support. To enable the model to function without a reference voice, we apply a 50% random drop to the reference voice input. To maintain the model's original capabilities, we freeze all modules related to user inputs and fine-tune only the transformer decoder. Once the decoder adapts, we unfreeze the entire model and fine-tune all parameters to optimize performance. Step 3: High-Quality Fine-tuning. The final stage refines the model using a carefully selected subset of data filtered by these quality metrics: edit_distance < 5%, CLAPsrc \u2265 25%, energy > 1000. This yields 100K high-quality pairs for fine-tuning, enabling the model to enhance the quality of audio by learning from cleaner, more relevant data.\nDual-track Mode Training. Our experiments revealed that training the dual-track mode from scratch is challenging. To address this, we initialize the dual-track model with the pre-trained mixed mode model after Step 1. Step 1.5: Dual-Track Mode Adaptation. After initialization, we freeze user input modules and fine-tune only the transformer decoder to adapt it to the new token pattern. Once the adaptation is complete, we unfreeze all model weights and proceed to fine-tune the entire model. The subsequent training steps mirror those of Steps 2 and 3 in the mixed mode.\nCurriculum Learning for Codebook Loss Weight Adjustment. We propose a curriculum learning strategy to adjust the weights of codebook losses during training. Initially, the first three codebooks have weights of 0.25, while the rest are set to 0.05. This encourages the model to focus on the most important components first. As training progresses, the weights are gradually balanced, enabling the model to capture finer audio details step by step."}, {"title": "4. Experiments", "content": "4.1. Experimental setup\nBaselines. To the best of our knowledge, no open-source text-to-song model is currently available. We use two state-of-the-art text-to-music models as baselines: Stable Audio Open and MusicGen, both of which generate instrumental music from text. Additionally, we fine-tune Parler-tts, a text-to-speech model that generates speech from both transcript and description texts, using our own training data. We also compare our model with Suno, a commercial product, using subjective evaluations.\nEvaluation dataset and metrics. For the evaluation dataset, we filter the English-voiced song samples from MusicCaps benchmark, yielding a test set of 326 samples, with the lyrics annotated by our preprocessing pipeline. We conduct both objective and subjective evaluations. For objective evaluations, Frechet Audio Distance (FAD) measures the generation fidelity; Kullback-Leibler Divergence (KL) evaluates conceptual similarity with the target audio; CLAP Score measures the alignment between the audio and the text description; Speaker Embedding Cosine Similarity (SECS) assesses the similarity of speaker identity; Phoneme Error Rate (PER) gauges adherence to the provided lyrics. Note that due to limitations in the ASR model, the PER values are higher than the actual error rate, but the relative differences remain meaningful. For each method, we generate the audio five times with different random seeds and report the average metric. For subjective evaluations, we employ Mean Opinion Score (MOS) tests, assessing five key aspects: overall quality (OVL.), focusing on musicality and naturalness; relevance to the text description (REL.); vocal quality, with an emphasis on clarity and intelligibility (VQ.); harmony between vocals and accompaniment (HAM.); and similarity to the original singer (SS.). The appendix C shows details of the evaluations.\n4.2. Results of Text-to-song Generation\nAs shown in Table 1, we compare our models, including mixed mode and dual-track mode, with several baselines. For all our models in this table, we use the first 3-second vocal clip of the ground truth as the reference voice.\nComparison with Baselines. SongGen significantly outperforms Stable Audio Open, MusicGen, and Parler-tts across both subjective and objective metrics. The test set contains many voice-related descriptions, such as \u201cA group of female vocalists sings this energetic swing song.\u201d However, MusicGen generates pure music, lacking any vocal signals, which results in exceptionally low CLAP scores. Stable Audio Open generates some vocal signals based on the input text, but these signals do not form recognizable words. Although Parler-tts has achieved remarkable success in controllable text-to-speech tasks, fine-tuning it for the text-to-song task proves to be ineffective. This highlights the greater complexity of the text-to-song task compared to text-to-speech.\nAlthough SongGen shows some gaps when compared to Ground Truth and Suno, it is important to note that we use only 2k hours of labeled data, sourced from publicly available datasets. Despite the limited data, SongGen achieves competitive performance on an out-of-domain test dataset. Figure 4 shows a mel-spectrogram of our generated songs, demonstrating that SongGen produces songs with various singing techniques like vibrato. Compared to Suno, a commercial product, SongGen outperforms in terms of text relevance and vocal control. Suno struggles to adhere to the highly detailed textual descriptions in MusicCaps (as shown by the REL. metric) and lacks voice cloning support, which gives our model a clear advantage in these aspects.\nMixed Mode and Dual-Track Mode. We further analyze the performance of the mixed mode and dual-track mode of our framework. In mixed mode generation, the \u201cMixed Pro\u201d approach outperforms the basic \u201cMixed\u201d model across all metrics, particularly in vocal quality (as indicated by the PER and VQ.). It indicates that by incorporating an auxiliary vocal token prediction target, the learning biases in mixed mode are effectively mitigated.\nIn dual-track mode, the \u201cInterleaving (A-V)\u201d pattern obtains the best performance. Although the parallel pattern is more computationally efficient, its performance lags behind the interleaving pattern. This is likely because, in parallel mode, each hidden state mixes vocals and accompaniment, making separation difficult with only two linear heads. Interestingly, regardless of the pattern (parallel or interleaving), placing the accompaniment before the vocals leads to better results than the reverse order.\nCompared to \u201cMixed pro\u201d, \u201cInterleaving (A-V)\u201d shows competitive performance, with only slightly worse result in FAD. Further comparison reveals that \"Interleaving (A-V)\" achieves better vocal quality (VQ.), but its harmony (HAM.) is slightly inferior to that of the \u201cMixed pro\u201d. This highlights the distinct advantages and challenges of each generation mode. We further visualize the attention scores in the decoder to explore the internal mechanisms of the transformer in both modes. Figures 5 (a), (b), and (c) show self-attention over 500 steps in layer 18 for the \u201cmixed pro", "interleaving (A-V)": "attern. From (a) and (d), we observe evenly spaced parallel lines along the diagonal. Since songs typically have repetitive structures, this attention pattern suggests that our model has effectively learned the underlying structure of music. Interestingly, in (f), the attention follows a checkerboard pattern, where attention scores for odd steps are strong with other odd steps and similarly, for even steps. This indicates that in the", "A-V)": "ode, higher layers focus more on learning intra-track relationships, while lower layers capture inter-track interactions.\nWithout a reference voice. We explore the song generation capability of SongGen without a reference voice. Table 2 shows that performance declines slightly. However, the listening test results demonstrate that the model continues to produce enjoyable songs with coherent vocals."}, {"title": "4.3. Ablation Studies", "content": "In this section, we conduct extensive ablation studies. Since both mode are based on a unified framework, we present results from the mixed mode setting due to space limitations.\nEffect of training strategy. In Table 3, we evaluate the effectiveness of our High-Quality Finetuning (HQFT) and curriculum learning (CL) strategy for codebook loss weights. HQFT improves all metrics, confirming the effectiveness of our quality filtering criteria. Compared to the \"w/o CL\" variant, where each codebook's loss weight is fixed and equal, our CL strategy improves performance. This demonstrates that prioritizing the most important tasks first and then progressively refining the details is effective.\nEffect of Lyrics Module Design. We further investigate the impact of different lyric integration methods, including the choice of tokenizer (VoiceBPE vs. T5), the use of a lyrics encoder, and the integration approach (pre-pending vs. cross-attention). Figure 4 shows the results after Step 1 training for each variant. Our design (VoiceBPE, w/ lyrics encoder, cross-attention) achieves the best results across all metrics, validating the effectiveness. Unlike most TTS works, which prepend transcripts before audio tokens, we find that the cross-attention approach is more effective and stable. This may be because cross-attention allows the decoder to focus solely on generating the audio modality. Additionally, phoneme-like tokenizer (VoiceBPE) is more suitable than the word-level tokenizer (T5) for song lyric tokenization. Under this mechanism, the lyrics encoder can capture the relationships between lyric tokens, learning pronunciation patterns from different token combinations, and thus alleviating the burden of modality alignment on the decoder."}, {"title": "5. Conclusion", "content": "In this paper, we introduced SongGen, a fully open-source, single-stage auto-regressive transformer for text-to-song generation. Operating within a unified framework, we devised a variety of token patterns. These patterns endow SongGen with the ability to support two distinct generation modes: the mixed mode and the dual-track mode. Experimental outcomes convincingly demonstrate the efficacy of our token pattern design. Moreover, they showcase the strong song generation capabilities of SongGen in both the mixed mode and the dual-track mode."}, {"title": "Impact Statement", "content": "The proposed work, SongGen, a controllable text-to-song generation model, has the potential to impact various aspects of society. On the positive side, SongGen enables both content creators and novices to effortlessly express their creativity with a low entry barrier, while also streamlining the workflow for experienced music producers.\nHowever, since SongGen autonomously generates songs and supports voice cloning, there are risks of copyright infringement, intellectual property misuse, and the creation of deepfake audio. Proper constraints are needed to ensure the model is not misused in illegal or unethical ways.\nIn conclusion, while SongGen presents exciting possibilities for the music industry and creative expression, its development should be accompanied by careful consideration of its ethical and societal implications"}, {"title": "A. Limitations and Future Work", "content": "We acknowledge the limitations of our proposed SongGen model. Due to the scarcity of open-source song data, the current model can only generate songs up to 30 seconds in length, which is insufficient for producing songs with complete structures. Additionally, the current audio codec, X-Codec, operates at a sampling rate of 16kHz. To improve fidelity, our future work will involve training a renderer to upsample the audio for higher quality output."}, {"title": "B. Implementation and Training Details", "content": "In SongGen, the lyrics encoder is a 6-layer transformer with a hidden size of 1024. The transformer decoder, consisting of 24 layers with 1024 hidden size, includes both causal self-attention and cross-attention blocks in each layer. In \"Mixed Pro\" Mode, the vocal loss weight $\\lambda$ is set to 0.1. The model is trained for approximately 400K steps using 16 Nvidia A100 (80GB) GPUs, with a batch size of 16 per GPU. For optimization, we employ the AdamW optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.99$, and a weight decay of 10-4. During training step 1, the learning rate is set to $10^{-4}$, while for the subsequent fine-tuning steps, the learning rate is reduced to $5 \\times 10^{-5}$. A cosine learning rate schedule is applied for all traning steps. To facilitate reproducibility, we will make our training configurations publicly available."}, {"title": "C. Details in Evaluations", "content": "For evaluation", "metrics": "n\u2022 Frechet Audio Distance (FAD) : Evaluates the fidelity of generated songs by calculating the distribution distance between features of the target and generated audio", "KL)": "Measures the similarity between the generated and target audio with the label calculated by the audio tagging model. A lower KL suggests that the generated music shares similar concepts with the reference.\n\u2022 CLAP Score: Evaluates the alignment between generated audio and the given text prompt using the official CLAP model.\n\u2022 Phoneme Error Rate (PER): Assesses the adherence of the generated audio to the provided lyrics by transcribing the audio using Distill Whisper and computing the phoneme error rate against the reference lyrics. However", "SECS)": "Assesses the similarity of speaker identity using the Resemblyzer speaker encoder to compute the SECS between reference 3-second vocal clips and generated audio.\nFor the subjective evaluations, we randomly select 36 audio samples generated by our models, and each sample is evaluated by 20 listeners. We conduct the commonly used MOS (Mean Opinion Score) tests across five aspects. The rating scale ranges from 1 to 5, with higher scores indicating better performance. For the Overall Quality (OVL.) evaluation, we instruct the raters to focus on musicality and naturalness, while ignoring style differences. For the Relevance to Text Description (REL.) evaluation, we ask the raters to score based on the proportion of key points from the text description that are reflected in the generated song. For the Vocal Quality (VQ.) evaluation, we emphasize the importance of clarity, lyric accuracy, and the naturalness and coherence of the vocals in the ratings. For Harmony (HAM.), we ask the raters to pay particular attention to the temporal correspondence between"}]}