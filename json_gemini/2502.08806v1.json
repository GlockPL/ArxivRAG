{"title": "CLOVER: A Test Case Generation Benchmark with Coverage, Long-Context, and Verification", "authors": ["Jiacheng Xu", "Bo Pang", "Jin Qu", "Hiroaki Hayashi", "Caiming Xiong", "Yingbo Zhou"], "abstract": "Software testing is a critical aspect of software development, yet generating test cases remains a routine task for engineers. This paper presents a benchmark, CLOVER, to evaluate models' capabilities in generating and completing test cases under specific conditions. Spanning from simple assertion completions to writing test cases that cover specific code blocks across multiple files, these tasks are based on 12 python repositories, analyzing 845 problems with context lengths ranging from 4k to 128k tokens. Utilizing code testing frameworks, we propose a method to construct retrieval contexts using coverage information. While models exhibit comparable performance with short contexts, notable differences emerge with 16k contexts. Notably, models like GPT-40 and Claude 3.5 can effectively leverage relevant snippets; however, all models score below 35% on the complex Task III, even with the oracle context provided, underscoring the benchmark's significance and the potential for model improvement. The benchmark is containerized for code execution across tasks, and we will release the code, data, and construction methodologies.", "sections": [{"title": "1. Introduction", "content": "Software testing is integral to the software development lifecycle (Yoo & Harman, 2012; Wang et al., 2024a; Alshahwan et al., 2024). From test-driven development (Mathews & Nagappan, 2024) to program repair (Yasunaga & Liang, 2021; Jimenez et al., 2024), crafting efficient and high-quality test cases is a routine task. Recently, large language models (LLMs) have gained attention for their potential in code and software testing enhancements. These models utilize context, user prompts, history, and code prefixes for code suggestions (Nijkamp et al., 2023; 01.AI, 2024; Roziere et al., 2023; Yang et al., 2024). To evaluate models' capability in writing code, many benchmarks have been proposed in the past few years. These benchmarks vary in focus, tackling areas such as basic coding problems (Austin et al., 2021b; Chen et al., 2021), data science tasks (Lai et al., 2022), reasoning challenges (Gu et al., 2024), and issue resolution (Jimenez et al., 2024). We summarize the most relevant work in Table 1.\nCan LLMs write executable test cases with specific requirements in a realistic setup? To address this question, we create a benchmark, CLoVer, focusing on automatic evaluation of unit test case generated by LLMs. We create an automatic pipeline with little human intervention to scrape permissive repositories from GitHub and configure the execution environment. We identify potential problems by extracting and verifying test cases from the existing codebase. After this step, we take these problems and structure three challenging tasks: (1) Task I simulates a code completion tool by focusing on cloze-filling style questions; (2) Task II addresses scenarios requiring coverage and testing of specific methods or classes within the source code; (3) Task III involves improving code coverage, where models are challenged to cover certain code blocks within the source. The selection of example is driven by AST parser and code coverage results. We evaluate model performance by executing the generated code and capturing line coverage, offering a tangible measure of their effectiveness.\nIn practical software testing, leveraging a comprehensive context window is crucial, encompassing dependencies and their antecedents. To evaluate models in a realistic context-aware fashion, we construct oracle context via test coverage for each example. We assess model performance across three tasks with context lengths spanning 4k to 128k tokens and introduce context utilization as a metric to assess how effectively models leverage extended contexts, independent of their absolute performance.\nOur evaluation includes 10 open-source and 4 proprietary models. In Task I, many open-source models, such as MISTRAL-7B and QWEN 2.5CI-14B, underperform with longer contexts, indicating a decline in response quality despite their technical capacity to handle such lengths. In Tasks II and III, all models encounter difficulties in generating executable code, even when provided with oracle context. A notable trend is the sharp performance drop among open-source models starting at a 16k context window. The highest performance across all tasks is demonstrated by CLAUDE 3.5-S and GPT-40, with GPT-40 achieving a 32.7% success rate on the most demanding task, Task III. We identified a significant disparity in context utilization and long-context instruction-following capabilities between leading proprietary models and others. Our data pipeline and evaluation sandbox are designed for scalability. We plan to release the code, benchmark, Dockerized environment, and recipes to enable the community to use these resources for further development and training. The benchmark also supports code agent by providing APIs and task instructions."}, {"title": "2. Data & Sandbox Construction", "content": "In Figure 1, we describe the overall pipeline from data collection to final evaluation. In this section, we will primarily focus on data collection and environment setup."}, {"title": "2.1. Data Collection", "content": "Source identification Following (Jimenez et al., 2024), we began by scraping 42 new python repositories and ultimately narrowed it down to 12 repositories for actual use. Details and reasons for exclusions are provided in Appendix A. In our methodology, we identified folders containing source and test code by matching filenames with the pattern test_.py. For eleven repositories, we could not extract test suites. This process resulted in the identification of test modules, each comprising at least one test function.\nProblem extraction from file Test cases are extracted from modules by parsing Python files using the AST tool to identify test functions. Using heuristics, we isolate setup code s to remove unrelated test functions. In Figure 1, test functions test_simple and test-iter are preserved with the necessary setup code, resulting in self-contained problems named tmp_test_lexnparse_[*].py. We maintain the original structure and path of test modules.\nVerification API verify Executing new unit tests requires careful design. During the design and testing of our verify API, we considered several points: (1) Consistency check. Evaluate model-generated implementations against ground-truth code to identify issues from extraction, heuristics, or system conditions such as caching; (2) Batchify operations. Enable batch evaluation of test cases to decrease overhead from test framework executions and setups; (3) Timeout management. Prevent infinite loops in model-generated code; (4) Error handling and logging; (5) Repository restoration. Ensure repository state is reset before and after each use. We wrap this verification process to an API verify(case) \u2192 {true, false} where the output indicates whether the case can execute successfully.\nCoverage API cov The coverage API provides line coverage metrics for a test case across the entire repository. Utilizing pytest-cov, it reports hit and missed lines, and computes a file-level coverage rate, even if execution fails. Unlike verify, cov cannot be parallelized due to shared cache dependencies but can still deliver coverage reports on failed tests."}, {"title": "2.2. Sandbox Construction", "content": "To run test programs across different repositories, we create sandboxes and package them in a Docker image, maintaining minimal intervention to ensure the process is scalable to a larger number of repositories.\nProcedure First, we create a conda virtual environment with Python version set to 3.10. Then we install packages including poetry\u00b9 and tox\u00b2. We exhaustively search for txt files, and try to pip install those files. Then, git submodule related operations will handle submodules under the project if any. After this step, we try to install the package from the current project directory with pip, poetry and tox. After all the steps, we run pytest to check if we can find a significant number of passed test cases. In practice, the procedure above can automatically configure the environment of 25 out of 42 (59.5%) repositories. We describe more detail about construction failure in Section A.\nEfficiency Tasks are evaluated sequentially, while evaluations within each task run concurrently across different repositories. The longest-running repository determines the evaluation's time bottleneck. To limit evaluation to 2 hours per model on a CPU Linux machine, we capped the maximum number of examples per repository: 50 for Task I, and 25 each for Task II and III."}, {"title": "2.3. Evaluated Models", "content": "We utilized vLLM (Kwon et al., 2023) for model inference with temperature and top-p set to 0.2 and 1.0. Maximum output lengths were 200, 4,000, and 4,000 for Tasks I, II, and III, respectively. To accommodate output tokens without exceeding model length limits, we adjusted the maximum sequence length by the output length during data preparation. The tokenizer from MISTRAL-7B was used in this process. We evaluated open-source models including CODEGEMMA-7B (Team et al., 2024), MAGICODER 6.7B (Wei et al., 2024), QWEN 2.5CI-14B (Coder-Instruct) (Yang et al., 2024), YI-CODER-9B (01.AI, 2024), STARCODER2-15B (Lozhkov et al., 2024), CODELLAMA-13B (Roziere et al., 2023), LLAMA 3.1-8B, LLAMA 3.1-70B (Dubey et al., 2024), CODESTRAL-22B, and MISTRAL-7B (Jiang et al., 2023). For proprietary models, we evaluated CLAUDE 3.5-S(onnet), GEMINI 1.5-F(lash), GPT-40 (2024-08-06), and GPT-40-MINI (2024-07-18)."}, {"title": "3. Construction of Oracle Retrieval", "content": "To write or complete test cases, models need access to the related source code. To offer a simplified but realistic evaluation setting without using agents or retrievers, we provide oracle retrieval code in this benchmark. This leverages our executable environment and the coverage API for detailed coverage information. This setup aims to: (1) explore models' near-upper bound performance, (2) and test models in long-context scenarios. Our approach constructs long-contexts naturally and demands a multi-hop understanding of code and effective information use. Our setup is also perfect for software agent development.\nMotivation Files such as __init__.py are often highly covered by most test cases, but they contribute little value in terms of addressing specific problems, as per information theory. These files can quickly deplete the context budget due to their high coverage rates. Hence, we need to calibrate the coverage information to reflect the importance of certain informative files."}, {"title": "3.1. Calibration of Coverage", "content": "Within a file, we represent the test cases as $Y = {\u0423_1, \u0423_2, ..., \u0423_T}$. Typically, these cases share some setup code and are organized under the same testing topic. The collection of all source files is denoted as $X = {X_1,X_2,...,X_F}$. When using the verify API on T, we get a coverage tensor $C \\in {1,0}^{T \\times F \\times L}$, where $C_{t,f,l} = 1$ indicates test case $y_t$ covers the l-th line of file $x_f$, and $C_{t,f,l} = 0$ otherwise. $T = |Y|$ represents the total number of test cases in this file, $F = |X|$ is the total number of source files, and $L$ is the max number of lines in src. We run pytest-cov two times for each test case $y_t$:\n\u2022 a regular run $C^{base}$. This will return the regular coverage report of $y_t$ over X.\n\u2022 empty run $C^{empty}$. In this setting, we replace the code with an empty test statement: def test(): assert True and it will be deployed to the same location of $y_t$. For instance, if test_iter was implemented in tests/util, we will deploy the empty test to that directory as well.\nRepository Baseline We propose a repository baseline is established by comparing $C^{base}$ and $C^{empty}$.\n$Q^{repo} = {x_f | \\nexists t  argf[1(C^{base}_{t,f,l} = C^{empty}_{t,f,l})] }$\nwhere $1(\\cdot)$ is the indicator function. The set $Q^{repo}$ comprises the files $x_f$ for which, for any test case index t, the coverage remains unchanged after executing the actual test case. This implies that the files in $Q^{repo}$ offer minimal information gain in terms of entropy for generating test case $y_t$.\nPeer Baseline To uniquely identify each test case, we set a Peer Baseline. The aim is to identify the most distinctive information across test cases. For a particular test case $y_t$, the Peer Baseline is defined as follows\n$Q^{peer} = { \\,  x_f | \\nexists t  argf[1(C^{base}_{t, f, l} = \\sum^{T}_{t'=1}1(C^{base}_{t', f, l} = 1))} $\nwhere $1(\\cdot)$ is the indicator function. $\\sum^{T}_{t'=1}1(C^{base}_{t', f, l} = 1)$ ensures that the line l is covered by exactly one test case (test case $y_t$), meaning it's only covered by the test case $y_t$. Next, we define Q', which is the set not meeting the criteria for either $Q^{peer}$ or $Q^{repo}$: $Q' = F \\setminus (Q^{repo} \\cup Q^{peer})$. We regard the value of files in Q' as lower than those in $Q^{peer}$ but higher than the repository baseline $Q^{repo}$.\nCalibration of Coverage Source files are classified into three categories: $Q^{repo}$, $Q^{peer}$, and Q'. The approach for assembling context for test case t gives precedence to files in the order of $Q^{peer}$, followed by Q', and ultimately $Q^{repo}$. Within each category, we randomly select files if the context budget does not permit using them all."}, {"title": "3.2. Task Setup", "content": "Before diving into these three tasks, we define some terminologies which share across these tasks. For one task instance, we provide three categories of contents:\n\u2022 Task instruction. We show examples in Fig 1 and 2.\n\u2022 In-file code, including setup code s and function declaration f. Setup s prepares necessary components, such as imports, fixtures, and any initial configurations, required for the test. Function declaration f specifies the function's name, arguments, and any return types, if applicable. In Task I, we also provide code prefix, which will be discussed later.\n\u2022 Source files per task requirement and from oracle retrieval. Files required by task are guaranteed to be provided unless in the Problem Only setting. It also has higher priority compared to oracle retrieval when we try to fill the context budget.\nSetting We introduced two settings across three tasks, Problem Only (PO) and Contextual. In Problem Only setting, we only provide the Task instruction and in-file code. In contextual setting, we provide code snippets capped by context budget."}, {"title": "4. Task I: Mask Prediction in Assertion Statements", "content": "This task challenges the model to predict the missing element in an assertion statement within a test case, addressing the code completion feature offered by coding companions.\nProblem Formulation For each problem x, it has following elements in the prompt of the task [s, f, p, q, ref] in the Problem-Only setting:\n\u2022 Prefix (p) refers to the existing code within the test function, serving as the context for solving assertion statements.\n\u2022 Assertion statement with a MASK (q) represents the task for models to complete. Based on the surrounding code and any referenced materials, the model is expected to fill the gap and complete the assertion statements. q contains exactly one MASK.\n\u2022 Reference answer (ref) is the original answer from the code. Any valid answer passing RER, a metric defined next, is acceptable as correct.\nThe model relies solely on the problem details, without extensive information about the method under test."}, {"title": "Cloze construction", "content": "AST tools identify all possible assertion statements, including unary (e.g., variables, functions) and binary operations (comparisons). For binary, either operand can be masked. The suffix of q is removed to avoid hints, ensuring q is the last code line.\nExample selection Preliminary study find that within each repository, there exists certain high frequent assertion statements, which provides unwanted hint to models. For instance, \"200\" (string literal) and 200 (number) are the most frequent candidates for the MASK. So we filter out problems with common ref. The chosen probability of of a problem $x_i$ is defined as:\n$p(x_i) = \\begin{cases}\n0, & \\text{if } \\frac{\\text{count}(ref_i)}{N \\cdot \\text{len}(ref_i)} > 0.01 \\\\\n\\frac{\\text{len}(ref_i)}{\\sum_{l=1}^{k} \\text{len}(ref_i)}, & \\text{otherwise}\n\\end{cases}$\nwhere N is the total number of problems in one repository. We downsample to 50 problems per repository to maintain a diverse set of problems.\nPrompt Template We explore two elicitation methods: (1) answer only (pred): the model yields the answer directly in a code block; (2) assertion statement with answer filled q.replace(MASK, pred): the model returns the line with blank filled. Our studies with CODELLAMA-13B, MISTRAL-7B, CODEGEMMA-7B, and CODESTRAL-22B show the filled assertion method improves execution rate by at least 6.0%, thus we use it for Task I experiments.\nMetrics & Verification Let the model prediction for MASK be pred. We implement three evaluation metrics for this task: (1) Exact match is defined as EM = 1(ref = pred); (2) Execution Rate (ER) indicates the execution result of the assertion statements filled with pred; (3) Refined execution rate (RER) is based on ER but we applied post-processing steps to remove trivial assertions and prohibit the copy of an existing assertion from the context.\nPost-processing discards the following invalid scenarios: (1) pred is a constant in a unary operation; (2) pred is a constant in a binary operation where the other operand is also a constant; (3) in an equation comparison, pred matches the operand on the opposite side. We follow the definition of constant in AST. Since the problems are selected given its surface length, as a proxy to its difficulty, the false negative ratio is considered low."}, {"title": "4.1. Results in the Problem Only setting", "content": "In this setting, we only provide the problem itself, excluding external context like the code of MUT. We present the result in Table 3. The best open-source model in this setting is QWEN 2.5CI-14B, achieving comparable performance compared to proprietary models. CLAUDE 3.5-S performs the best in all metrics."}, {"title": "4.2. Results in Contextual Settings", "content": "We present the result in Table 4. The best overall performance is achieved by CLAUDE 3.5-S in both settings, with a 3.0% gain from 72.6% to 75.6%. In the contextual setting, we found there is a sharp decrease after 8k max length in most open source models including CODELLAMA-13B, STARCODER2-15B, MISTRAL-7B, QWEN 2.5CI-14B, and CODESTRAL-22B. We examined the model response in these cases and we find that the chance of getting gibberish output increases along with the increase of input length. Note that the prompt template remains the same for context free and contextual setting where the only change applied is the additional code snippets.\nContext Utilization A We introduce a novel metric to measure models' capability in effectively utilizing the context. On the performance gain side, we define it as Amax = max(r4k, r8k,..., rmaxLen) - r0 where r0 is the context free baseline performance. Since we provide oracle context to the model, shorter context carrying strong hint could be sufficient and even better than longer sequence. Amax measures the best possible gain a model could get. Vice versa, we define Amin = min(r4k, r8k,..., rmaxLen) - r0. This set of metrics focuses on the relative performance change given longer context. The ideal value, if context provides good source of information, for this metric follows this equation Amax > Amin > 0."}, {"title": "5. Task II: Targeted Test Implementation", "content": "In Task II and Task III, we will shift from code completion to open-ended code generation, which is more challenging and requires longer context. In Task II, given a python class or function from the source code, the model needs to complete the test code by using the target.\nProblem Formulation For each problem, we provide setup s, function declaration f and a specification to use the target. We show the specification template and an example in Figure 2. The \"target_name\" here is the name of the class or function. The \"type\" is either \u201cclass\u201d or \u201cfunction\u201d. \"file_name\" is where the target object was implemented.\nData Construction We use AST tools to parse the code and identify all Attribute type nodes through a recursive walk to find suitable targets. These identified classes and functions become potential targets. They are then matched with those covered by this case in Chase. A random target is selected as the requirement. In settings ranging from 8k to 64k, we ensure the inclusion of the necessary file as specified. The maximum length constraint is 8k and the output length is 4k, thus the combined length of instructions and the required file must not exceed 4k. Any cases exceeding this limit are discarded. By setting a single target, we maximize the inclusion of examples.\nAnswer Format In this task, the generated code is the completion of the function declaration f. We designed two prompt templates to capture the output, one with the completion part only (promptpart), and one with the full code block (s, f) along with the completion prompt full.\nMetrics We define two metrics for Task II and Task III. Execution Rate measures if the generated code can be captured, and executed successfully. Any test failures, exceptions and timeout will count as execution failure. Success Rate Besides the execution rate, we also check whether the specification was satisfied. For Task II, we check whether the required target was in the generated code. For Task III, we check for code coverage.\nResult We report the Success Rate using prompt full in Table 5 and promptpart in Table 7. In the context free setting of this task, it provides no context to the model, not even the \"file_name\" required to complete the task. For most of the models, there is a significant performance boost from context-free to 8k. With only 8k context length, CLAUDE 3.5-S achieves surprisingly high performance (46.2%), 9.0% ahead of the second best model GPT-40. Some models, however, remain the same or even get slightly worse performance, including CODEGemma-7B, CODELLAMA-13B, LLAMA 3.1-8B, and GEMINI 1.5-F. The best performance is achieved by CLAUDE 3.5-S and GPT-40 at 32k and 64k respectively. Starting at 16k, we have seen a sudden performance drop on CODELLAMA-13B, MISTRAL-7B, QWEN 2.5CI-14B, and CODESTRAL-22B."}, {"title": "6. Task III: Coverage-Oriented Test Implementation", "content": "In this task, given some code blocks from source code, the model needs to complete the test code and cover those target blocks. This task shares a lot of similarity with Task II, so we will focus on the different part.\nProblem Formulation For each problem, we provide [s, f] and a specification to cover up to 10 code blocks from the source code. We provide the full code snippet in the \"Retrieved Code Snippets for Reference\u201d section of the prompt along with other oracle retrieval files. In the specification prompt, we provide the file name, the code blocks to cover, and the starting and ending line number for these code blocks in the original file.\nData Construction We took a deterministic approach to select code blocks rather than randomly choosing code spans. We use the $Q^{peer}$ to guide the selection of code blocks to cover. For a case $y_t$, we check if there is some code blocks only covered by it, not any other peer cases. Typically it's the case where a conditional branch or a function is hit by only one case. We also filter out code blocks with fewer than 5 lines as we do not want to include many code fragments. The max number of files to cover is set at 10. With this approach, we can guide the model with a feasible and reasonable coverage requirement which also aligns with the function name and arguments."}, {"title": "Limitation & Conclusion", "content": "Our study is confined to Python and specific pytest tools. We didn't explore using code agents to tackle problems in this benchmark. Preliminary findings indicate that code agents, such as those by Wang et al. (2024c), are costly to run due to the need to explore entire repositories, primarily because of the overhead from reading large files.\nIn this study, we introduce a benchmark designed for multiple real-world software testing scenarios. We identified significant gaps in long-context handling, context utilization, and instruction-following capabilities between open-source and closed-source models, pointing to substantial opportunities for improvement. The coverage-driven oracle context could advance research on long-context evaluation in (code) LLMs. Researchers can use the API for real-time feedback to enhance models' coding and reasoning skills. Additionally, the data pipeline can be adapted to create high-quality training datasets."}, {"title": "Impact Statements", "content": "This paper presents research aimed at advancing the field of Machine Learning. There are a few potential societal impacts stemming from our work:\nSafety Considerations While it is possible, though unlikely unless intentionally prompted, for LLMs to generate malicious or corrupted code, we disclaim responsibility for any consequences resulting from executing such code on our benchmark.\nWe are committed to providing a safe and controlled evaluation environment by encapsulating our framework within a Docker container. We have implemented extensive precautionary measures to achieve this goal. However, in rare cases involving specific repositories that have been excluded from our benchmark, we have observed instances where the Docker container may exceed system memory limitations, potentially causing the host server to restart.\nWe advise users and researchers to carefully consider system configurations and setup when deploying any LLM-generated code directly on their machines.\nLegal Compliance The usage of all repositories referenced in this paper is approved by the authors' organization following a thorough license check. The licenses include BSD-3-Clause, Apache-2.0, MIT, and LGPL-2.1."}]}