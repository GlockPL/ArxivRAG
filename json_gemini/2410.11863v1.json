{"title": "ChatVis: Automating Scientific Visualization with a Large Language Model", "authors": ["Tanwi Mallick", "Orcun Yildiz", "David Lenz", "Tom Peterka"], "abstract": "We develop an iterative assistant we call \"ChatVis\" that can synthetically generate Python scripts for data analysis and visualization using a large language model (LLM). The assistant allows a user to specify the operations in natural language, attempting to generate a Python script for the desired operations, prompting the LLM to revise the script as needed until it executes correctly. The iterations include an error detection and correction mechanism that extracts error messages from the execution of the script and subsequently prompts LLM to correct the error. Our method demonstrates correct execution on five canonical visualization scenarios, comparing results with ground truth. We also compared our results with scripts generated by several other LLMs without any assistance. In every instance, ChatVis successfully generated the correct script, whereas the unassisted LLMs failed to do so. The code is available on GitHub: https://github.com/tanwimallick/ChatVis/.", "sections": [{"title": "I. INTRODUCTION", "content": "Scientific visualization is typically performed one of two ways: interacting through a graphical user interface with a visualization tool such as ParaView [2] or VisIt [5], or writing an offline script-usually in Python for the same tool. Either way, the visualization is created manually, through trial and error, one step at a time. Expert knowledge of data analysis and visualization is required, and the resulting visualizations are time-consuming to create and difficult to reproduce.\nWe propose a new approach to address the problems of productivity and usability of creating data analysis and visualization through synthetic software generation using an LLM. We develop an iterative prompt assistant, ChatVis, that allows the user to specify a chain of analysis/visualization operations in natural language. The assistant attempts to generate a Python script for the desired operations and iterates until the script executes correctly, prompting the LLM to revise the script as needed. We use ParaView as the visualization tool for this study, although we believe that VisIt or other tools that have offline Python scripting capability could work equally well. We use the Python API of OpenAI GPT-4 as the LLM.\nBecause GPT-4 is not trained on the intricacies of data analysis and visualization tool chains, we find that simply invoking the chat interface to GPT-4 does not produce correct code. As such, additional development was needed to improve the resulting code quality. In this paper, we describe the development of ChatVis and demonstrate its use in generating several canonical visualization pipelines. The contributions of this paper are:\n\u2022 A natural language assistant to generate and execute Python scripts for scientific visualization using few-shot prompting\n\u2022 An error detection and correction loop that iteratively extracts error messages from executions of the generated script and generates subsequent prompts to the LLM to correct the specified error\n\u2022 Automatically generated visualizations from the synthetically generated scripts\n\u2022 Comparison of visualizations against ground truth and attempts to generate the same results with GPT-4 without our agent, for five canonical visualization pipelines of varying complexity\nThe rest of this paper is organized as follows. Section II presents related work in coding assistants and an overview of the scientific visualization tools and tasks used in this study. Section III describes the development of our code assistant agent. Section IV details the visualization scripts that were generated by our agent. Section V concludes the paper with a summary and outlook to the future of code generation for scientific visualization."}, {"title": "II. RELATED WORK", "content": "In this section, we review the relevant literature on using LLMs to generate scripts for scientific visualization. We have organized the literature into two categories: AI coding assistants for applications other than scientific visualization, and a very brief overview of the scientific visualization tools and techniques used in this paper.\n\nA. Coding Assistants\nMyCrunchGPT [10] is an assistant for scientific machine learning (SciML) tasks. Users interact with MyCrunchGPT over a web-based GUI for various SciML tasks such as training a physics-informed neural network to solve a partial differential equation. The authors use prompting techniques to achieve this. The LM4HPC [4] framework aims to facilitate the use of language models for HPC specific tasks. It is built on top of several components in the ML software stack with a HuggingFace compatible API. It uses LangChain to improve model performance by integrating new HPC-specific data. It also uses its specific tokenizer, LM4HPC tokenizer, to tackle the input size limit for tokens as HPC tasks can involve scientific codes with large codebases. HPC-GPT [6] is based on LLaMA and fine-tuned specifically for the HPC domain using generated question-and-answer instances. It has been used for two HPC tasks: for retrieving models and datasets for HPC, and for data race detection in OpenMP programs. Finally, HPC-Coder [15] is an LLM fine-tuned to model HPC and scientific codes and used for OpenMP pragma labeling and code generation. In particular, the authors use the HPC source code available on GitHub repositories to fine-tune their model.\n\nB. Scientific Visualization\nAlthough hand-drawn visualizations of scientific data pre-date computer graphics by at least 100 years, modern scientific visualization as a computer science discipline is arguably 37 years old, beginning in 1987 with the seminal report from a workshop of the National Science Foundation [14]. Many textbooks provide thorough coverage of the main visualization algorithms [3], [7], [16]. Over the years, mature visualization tools-both open-source and commercial-with sophisticated user interfaces have evolved to enable the visualization of scientific datasets. In the open-source domain, ParaView [2] and VisIt [5] are commonly used. We used ParaView for this research; VisIt has different syntax for its scripting and user interfaces but otherwise provides similar overall functionality. These tools allow the user to chain together various visual-ization \u201cfilters,\u201d where each filter corresponds to a different analysis or visualization operation. The filters are implemented in VTK [18], an open-source software library containing many of the visualization algorithms developed by the scientific visualization community over the last three decades.\nThe following filters are featured in our experiments. (1) Contouring is used to extract regions of data with the same scalar value. In 3-d, contouring is called isosurfacing. Contouring and isosurfacing are performed with the marching squares algorithm in 2-d and marching cubes algorithm in 3-d [12]. (2) Slicing and clipping are used to extract regions of data on a plane (slicing) or to one side of a plane (clipping). A variation of marching cubes is used for these operations. (3) Volume rendering generates a rendering of a 3-d dataset with color and opacity custom-tailored to the data values, allowing the interior regions of a volume to become visible. Much literature has been published on volume rendering algorithms [9]. (4) Delaunay triangulation is used to convert a set of unstructured points into a simplicial mesh, with a number of guaranteed properties of the shape of the resulting simplices [11]. (5) Streamline tracing is one of several algorithms for visualizing a field of vector-valued data, commonly called flow visualization [22]. In addition to the filters described above, no visualization would be complete without defining view parameters that describe the position of the viewer, the direction being viewed, and the angle of the field of view. The view parameters are used to control the projection of 3-d data into a 2-d image."}, {"title": "III. METHODOLOGY", "content": "In this section we describe our methodology to generate an accurate ParaView Python script from user input written in natural language. We propose an LLM-based method as outlined in Figure 1. Initially, the user describes their visu-alization needs in natural language, and an LLM processes this input to generate a more effective prompt. This prompt, combined with multiple example code snippets, is used by the LLM to create a Python script. We then execute this script using ParaView's PvPython API. If no errors are detected, the script produces a 2D screenshot. If errors occur during script execution, the error messages are fed back to the LLM for corrections, creating a feedback loop that continuously refines the script. This iterative process allows for ongoing improvements based on the error messages until an error-free script is achieved. Upon successful execution, this refinement process generates an error-free Python script and a screenshot of the visualization for evaluation.\n\nA. User input and prompt generation\nInitially, users provide their visualization requirements in natural language, describing what they wish to visualize along with any specific directives or desired features. Upon receiving this input, we employ a language model to process the information and generate a prompt more suitable for scripting with ParaView that maintains the user's specifications. Our methodology involves feeding the LLM both the user's input and a previously crafted example prompt. By analyzing the structure and detailed requirements of the example input and its corresponding prompt, the LLM generates a new prompt that identifies the operations mentioned by the user and ar-ranges them in a step-by-step manner that helps to break down the complex request into smaller, sequential steps to generate high-quality code. This generated prompt includes tasks such as file reading, filter operations, element rendering, setting the camera position, and capturing screenshots at specified resolutions.\n\nB. Script generation using few-shot prompting\nAfter generating the prompt with step-by-step instructions for creating the Python script, we feed the generated prompt along with a set of example function calls for various op-erations into the LLM to generate the Python script. We utilize chain-of-thought [21] prompting, where the LLM is guided through the logical progression needed to address a complex task. This step-by-step logical breakdown approach helps mitigate common issues where LLMs generate function calls in random order instead of executing tasks logically according to user requirements. Providing examples also helps the LLM avoid generating function calls that do not exist in the ParaView library, thereby preventing syntax errors. With this structured input and examples, the language model can successfully generate a correct and functional script for the intended visualization tasks. These examples encompass reading input data and configuring visualization filters like slices, contours, clips, glyphs, tubes, and stream tracers. The scripts also detail how to manage render views by setting view sizes and directions, applying isometric views, creating layouts, displaying visualization data, and saving screenshots. These steps are crucial for generating a correct script.\n\nC. Error detection and correction loop\nAfter we generate the ParaView Python script, we execute it using ParaView's PvPython API. However, if there are any errors during execution, ParaView will fail to produce a visualization. To handle this, we first developed a tool to detect and extract error messages from the PvPython output, which may include warnings and errors, among other system messages. This tool operates by first splitting the output into individual lines and initializing a list to store these messages. It then identifies tracebacks, which typically start with File, and gathers subsequent lines until it encounters specific errors, such as AttributeError. Once all relevant lines are col-lected, the function compiles these into a list and returns the error messages. The extracted error messages are sent to the language model with a prompt to fix the code and generate the visualization. The LLM uses this feedback to modify the script. The revised script then undergoes the same cycle of execution, error message extraction, and script modification using the LLM until an error-free version is achieved. This iterative process ensures that the generated ParaView Python script is error-free.\n\nD. Visualization output and evaluation\nWe execute the final version of the script using PvPython and generate a screenshot to visualize the output. For eval-uation, we also manually perform the same operations using the ParaView GUI, saving both the final Python script and a screenshot. This facilitates a thorough comparison between the LLM-generated script and the manually created script. We assess both the scripts and the screenshots to ensure that the automated script faithfully replicates the manual process in terms of accuracy and visual fidelity. This comparison is crucial as it verifies that the LLM has correctly interpreted the visualization requirements and translated them into a script that produces the desired output."}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "We evaluate our approach through a series of common visualization tasks including isosurfacing, slicing, volume ren-dering, triangulation, streamline tracing, and view rendering. We conducted our experiments on a workstation equipped with an Apple M2 Max chip with 12-core CPU, 30-core GPU, and 96 GB unified memory. Our iterative few shot prompting-based assistant uses the GPT-4 language model from OpenAI [1], where the GPT-4 engine is the latest version of ChatGPT with 1.7 trillion parameters. We compare the images and scripts produced by ChatVis against those produced by manual construction and unassisted prompts to GPT-4. We use PvPython version 5.12.0 to generate these images.\n\nA. Isosurfacing\nThe first visualization task we consider is to generate an isosurface of a 3-d volume. We use the synthetic Marschner-Lobb benchmark dataset [13] as the input data. The set of operations consists of reading in the dataset, creating an isosurface where the data have a given value, and saving an image of the isosurface. The prompt provided by the user is shown below.\n\nUser prompt\nPlease generate a ParaView Python script for the following operations. Read in the file named ml-100.vtk. Generate an isosurface of the variable var at value 0.5. Save a screenshot of the result in the filename ml-iso-screenshot.png. The rendered view and saved screenshot should be 1920 x 1080 pixels.\n\nGiven an example and the user prompt, we use an LLM to generate a step-by-step instruction prompt for creating the final script. These step-by-step instructions help to outline the operations to be performed sequentially, which aids the LLM in generating the final script accurately. We follow this procedure for all our experiments, but due to space constraints, we report only one example. The LLM-generated prompt corresponding to the user prompt is shown below.\n\nGenerated prompt\nGenerate a Python script using ParaView for performing visualization tasks based on the provided steps. This script utilizes ParaView to visualize an isosurface from the ml-100.vtk file. Operations include reading the file, generating an isosurface, setting the view resolution, and saving a screenshot. Requirements step-by-step: leftmargin=*\nRead the file ml-100.vtk given the path.\nGenerate an isosurface of the variable var at value 0.5.\nConfigure the rendered view resolution to 1920 x 1080 pixels.\nSave a screenshot of the rendered view to ml-iso.png.\nFigure 2 displays the generated images for the isosurfacing task. Figure 2(a) is generated through manual use of the ParaView GUI and represents our \u201cground truth,\" while Figure 2(b) is generated by ChatVis, and Figure 2(c) is generated by GPT-4. We observe that both ChatVis and GPT-4 can correctly perform the requested operations. This is the only example where GPT-4 produces a correct image as this is a relatively simple task. GPT-4 created a gray background, which is different from the default ParaView script. Meanwhile, the ChatVis background matched the ground truth because ChatVis learned to specify the background color to be white. The camera position was also not specified by the user, leading to slightly different default zoom levels. Later experiments specify camera parameters.\n\nB. Slicing followed by contouring\nThe second visualization task takes a slice of the same volumetric dataset as the previous task, and follows with an isocontour at a given value. This demonstrates the ability to link filters together in a pipeline, such that the output of one filter becomes the input to the next. Color mapping is specified, and the view direction is also rotated from the default view, demonstrating simple view manipulation. The prompt provided by the user is shown below.\n\nUser prompt\nPlease generate a ParaView Python script for the following operations. Read in the file named 'ml-100.vtk'. Slice the volume in a plane parallel to the y-z plane at x=0. Take a contour through the slice at the value 0.5. Color the contour red. Rotate the view to look at the +x direction. Save a screenshot of the result in the filename 'ml-slice-iso-screenshot.png'. The rendered view and saved screenshot should be 1920 x 1080 pixels.\nFigure 3 compares the ground truth screenshot with the one generated by ChatVis. ChatVis executed all operations correctly and produced a screenshot identical to the ground truth. On the other hand, the code generated by GPT-4 encountered syntax errors because the script attempted to access non-existent attributes. Specifically, it attempted to access the UseSeparateColorMap attribute of the Contour class by calling ColorBy(contour, None) and to set the ViewUp attribute on the RenderView class using the code view.ViewUp = [0.0, 1.0, 0.0].\n\nC. Volume rendering\nThe third visualization task performs volume rendering on the same dataset as the previous task. This example demon-strates the ability to generate a default color and opacity transfer function based on the extent of the values in the data. The view direction is also rotated to an isometric view, demonstrating more complex view manipulation. The prompt provided by the user is shown below.\n\nUser prompt\nPlease generate a ParaView Python script for the following operations. Read in the file named 'ml-100.vtk'. Generate a volume rendering using the default transfer function. Rotate the view to an isometric direction. Save a screenshot of the result in the filename 'ml-dvr-screenshot.png'. The rendered view and saved screenshot should be 1920 x 1080 pixels.\nFigure 4 compares the ground truth screenshot with the one generated by ChatVis. ChatVis executed all operations correctly and produced a screenshot identical to the ground truth, except for a different color palette because the user prompt did not specify one. Conversely, the code generated by GPT-4 did not create a volume rendering of the data. Although the code did not produce any errors, it resulted in a blank screenshot because the generated script did not include a volume rendering command.\n\nD. Delaunay triangulation\nThe fourth visualization task converts a point dataset to a surface through Delaunay triangulation. The input dataset is a point cloud extracted manually from sample data provided with ParaView. This point cloud is read and triangulated. The mesh is then clipped by a plane, keeping only one half of the resulting triangles, further demonstrating the linking of filters together. Rendering is performed in wireframe style as an additional test of various rendering options.\n\nUser prompt\nPlease generate a ParaView Python script for the following operations. Read in the file named 'can_points.ex2'. Generate a 3d Delaunay triangulation of the dataset. Clip the data with a y-z plane at x=0, keeping the -x half of the data and removing the +x half. Render the image as a wireframe. View the result in an isometric view. Save a screenshot of the result in the filename 'points-surf-clip-screenshot.png'. The rendered view and saved screenshot should be 1920 x 1080 pixels.\nFigure 5 shows the generated images for the Delaunay triangulation task, where Figure 5(a) is the ground truth, and Figure 5(b) is generated by ChatVis. When we compare the ground truth and the image ChatVis generated, we can see that that we were able to correctly perform Delaunay triangulation. GPT-4 failed to generate an image because the script incorrectly assigned the InsideOut attribute to clipFilter, which does not have an InsideOut method.\n\nE. Streamline tracing\nThe fifth visualization task generates a set of streamlines from a vector-valued flow field. The input dataset is taken from sample data provided with ParaView. The flow velocity field and other scalar fields are read in and streamlines are traced through the velocity data. The streamlines originate at default seed positions. The streamlines are rendered using 3d tubes, and arrow-like glyphs are added to indicate flow direction. Streamlines and glyphs are color-mapped to a scalar data field, in this case temperature.\n\nUser prompt\nPlease generate a ParaView Python script for the following operations. Read in the file named 'disk.ex2'. Trace streamlines of the V data array seeded from a default point cloud. Render the streamlines with tubes. Add cone glyphs to the streamlines. Color the streamlines and glyphs by the Temp data array. View the result in the +X direction. Save a screenshot of the result in the filename 'stream-glyph-screenshot.png'. The rendered view and saved screenshot should be 1920 x 1080 pixels.\nFigure 6 shows the generated images for the streamline tracing task, where Figure 6(a) is the ground truth, and Figure 6(b) is generated by ChatVis. Upon comparing the ground truth image and the image ChatVis generated, we are unable to detect any differences, indicating that ChatVis successfully generated an accurate script.\nFor this example, we also report the generated Python scripts created by ChatVis and GPT-4. Although GPT-4 provides a similar response to our approach, it generates hallucinations in several places, due to the lack of knowledge for this particular visualization task. For instance, an error arises when the script attempts to set the Scalars and Vectors attribute on a Glyph object, which according to the error message, does not exist: 'AttributeError: type object 'Glyph' has no attribute Scalars and Vectors. Moreover, it used RenderView1 on Line 28 before this view was created. Additionally, the camera view and position result in cropped screenshots from different viewing angles. The code generated by ChatVis in lines 39 and 40 effectively resets the camera view and orientation, ensuring that the entire object is captured in the screenshot. Overall, our proposed methodology of breaking down the problem step-by-step and providing ParaView example code snippets helped to ensure the correct function calls were made in the proper order.\n\nF. Comparisons with other LLM models\nIn addition to GPT-4, we also compared the performance of ChatVis with other state-of-the-art LLMs such as GPT-3.5-turbo, LLaMA-3.8B [20], and other models specifically designed for code generation, such as Code LLaMA [17] and Codegemma [19]. The comparison is based on two criteria: (1) whether the model can generate scripts without syntax errors, and (2) whether the scripts, once executed, can successfully produce a screenshot. In sum, every model except GPT-4 failed to generate scripts without syntax errors, and therefore could not produce screenshots for any of the visualization tasks. As we report above, GPT-4 was only able to generate a correct isosurfacing screenshot, and a script without syntax errors for volume rendering; however, since the script did not perform volume rendering correctly, it could not generate the correct screenshot. This demonstrates that while LLMS excel at creating basic Python scripts for general tasks [8], they often fail to generate accurate scripts for specialized tasks such as scientific visualization. However, among all the LLMs evaluated, GPT-4 stood out by generating the least hallucinated code and accurately executing the isosurfacing task. Ultimately, this was the reason we selected GPT-4 as the base comparison model for ChatVis."}, {"title": "V. CONCLUSION", "content": "In this paper, we describe a methodology for generating accurate scientific visualization scripts from natural language inputs. We developed ChatVis, which first processes these inputs to create effective prompts. These prompts, supplemented with example code snippets, guide the LLM in scripting tasks that are compiled using ParaView's PvPython API. The process incorporates a feedback loop for error correction, enhancing script accuracy through iterative refinements until an error-free script is achieved. This method not only successfully generates both Python scripts and visualization screenshots but also demonstrates the effectiveness of integrating LLMs with domain-specific scripting tasks. On the other hand, the scripts generated by GPT-4 and other LLM models using basic user prompts often hallucinate, creating scripts with non-existent function calls, leading to compilation failures. In conclusion, ChatVis shows significant potential in accurately automating the generation of complex visualization scripts. This research sets the groundwork for future enhancements and broader applications of machine learning in scientific visualization. In the future, we plan to refine the capabilities of ChatVis by fine-tuning it with function calls from ParaView's source code. This targeted approach will improve the accuracy and reliability of script generation for specialized visualization tasks. Moreover, we will implement automated script evaluation, focusing on assessing the accuracy of the generated code, even without visual output. By systematically analyzing how closely the code matches expected outputs, we can effectively gauge script performance and conduct large-scale evaluations."}]}