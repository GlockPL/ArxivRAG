{"title": "Autotelic Reinforcement Learning: Exploring Intrinsic Motivations for Skill Acquisition in Open-Ended Environments", "authors": ["Prakhar Srivastava", "Jasmeet Singh"], "abstract": "This paper presents a comprehensive overview of autotelic Reinforcement Learning (RL), emphasizing the role of intrinsic motivations in the open-ended formation of skill repertoires. We delineate the distinctions between knowledge-based and competence-based intrinsic motivations, illustrating how these concepts inform the development of autonomous agents capable of generating and pursuing self-defined goals. The typology of Intrinsically Motivated Goal Exploration Processes (IMGEPs) is explored, with a focus on the implications for multi-goal RL and developmental robotics. The autotelic learning problem is framed within a reward-free Markov Decision Process (MDP), WHERE agents must autonomously represent, generate, and master their own goals. We address the unique challenges in evaluating such agents, proposing various metrics for measuring exploration, generalization, and robustness in complex environments. This work aims to advance the understanding of autotelic RL agents and their potential for enhancing skill acquisition in a diverse and dynamic setting.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) aims to create autonomous agents that can operate across diverse environments and complete a wide range of tasks. Researchers pursue different approaches, each focusing on specific drivers of learning. In Reinforcement Learning (RL) [1], agents learn by exploring their environment and using their experience to solve tasks. Imitation Learning (IL) [2] involves agents learning from expert demonstrations, while Multi-Agent Reinforcement Learning (MARL) [3] emphasizes cooperation among agents to solve collaborative tasks. Recent advancements in RL have demonstrated success in varied domains, such as playing Atari games [4], mastering chess and Go [5], and controlling stratospheric balloons [6]. IL, combined with transformers [7], has enabled generalist agents to be trained on diverse datasets and to perform in-context reinforcement learning via algorithm distillation. However, these algorithms remain sample-inefficient and struggle with generalization, creativity, and tackling novel tasks, largely because they rely on isolated learning signals. This research explores sociocultural interactions as a new avenue for Al learning inspired by human development. By immersing artificial agents in social contexts, we investigate how sociocultural dynamics impact learning. The first part of this study examines the formation of cultural conventions among agents, while the second part introduces a framework called Vygotskian Autotelic Artificial Intelligence, which leverages sociocultural interactions to enhance open-ended skill acquisition."}, {"title": "2. Humans are Goal-Directed Social Learners", "content": "Humans are an extraordinary inspiration for Artificial Intelligence (AI), as they are the fastest learning system observed. Within just a few years, children learn to crawl, navigate their surroundings, identify and manipulate objects, and even communicate with others. A key element of human development is the concept of goals. As defined by [8], a goal is \u201ca cognitive representation of a future object that the organism is committed to approach or avoid,\u201d influencing behaviors significantly. Children's exploratory play is often driven by intrinsically motivated processes that lead them to invent and pursue self-generated goals.\nTheir exploration is motivated by curiosity and a desire to experience interesting situations, evaluated in terms of optimal incongruity [9]. Moreover, [10] argues that for humans to experience pleasure during learning, they should engage in tasks with the optimal challenge, coining the term autotelic to describe intrinsically motivated individuals in a state of flow."}, {"title": "3. Towards Interactive Social Autonomous Agents", "content": "The present research seeks to bridge developmental psychology and modern Al methods to design embodied artificial agents, focusing on \"autotelic\" and \"cultural convention\" concepts. Our aim is to create interactive social autotelic agents by immersing them in social contexts and equipping them with mechanisms to construct or exploit cultural conventions. The groundwork for this is built on prior AI paradigms incorporating social elements, such as language-based Reinforcement Learning (RL) [11], instruction-based Imitation Learning (IL) and Multi-Agent RL (MARL) frameworks [12].\nOur work contributes by demonstrating that agents can use language as a cognitive tool for goal imagination. This research introduces two experimental contributions, focusing on the self-organization of cultural conventions. First, it explores the role of sensorimotor constraints in forming a graphical language using multi-modal contrastive learning in the context of Language Games [13]. Second, Chapter XVI investigates agent collaboration in the \"Architect-Builder Problem,\" where agents use shared intentionality and pragmatic frames to solve tasks through cultural conventions."}, {"title": "4. Background: Standard AI Paradigms", "content": "Our contributions bridge standard AI paradigms and developmental psychology to investigate two fundamental research questions (1) the language acquisition problem (self-organization of cultural conventions) and (2) the open-ended skill acquisition problem (self-organization of trajectories). In this chapter, we will first present standard Al problems and their associated families of algorithmic solutions before getting into the specifications of the two problems we investigate."}, {"title": "5. Reinforcement Learning", "content": "Reinforcement Learning (RL) involves an agent interacting with an environment to maximize cumulative rewards [1]. Formally, RL is modeled as a Markov Decision Process (MDP), with state space S, action space A, transition function T, initial state distribution po, and reward function R.\nAt each time step t, the agent selects an action $a_t \\in A$, receives a reward $r_{t+1}$, and observes the next state $S_{t+1} \\sim T (s'|st,at)$. The agent's goal is to learn an optimal policy $\\pi^*$ that maximizes expected return:\n$\\pi^* = argmax_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{T} \\gamma^t R(S_t, a_t) \\right]$"}, {"title": "5.1. Problem Definition", "content": "Reinforcement Learning (RL) involves an agent interacting with an environment to maximize cumulative rewards [1]. Formally, RL is modeled as a Markov Decision Process (MDP), with state space S, action space A, transition function T, initial state distribution po, and reward function R.\nAt each time step t, the agent selects an action $a_t \\in A$, receives a reward $r_{t+1}$, and observes the next state $S_{t+1} \\sim T (s'|st,at)$. The agent's goal is to learn an optimal policy $\\pi^*$ that maximizes expected return:\n$\\pi^* = argmax_{\\pi} \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^{T} \\gamma^t R(S_t, a_t) \\right]$"}, {"title": "5.2. Value Functions", "content": "Value functions estimate the expected reward for a state or state-action pair. The state-value function $V_\\pi(s)$ and action-value function $Q_\\pi(s,a)$ for policy $\\pi$ are defined by the Bellman expectation equations:\n$V_\\pi(s) = \\mathbb{E}_{a \\sim \\pi, s' \\sim T} [R(s,a) + \\gamma V_\\pi(s')]$,\n$Q_\\pi(s,a) = \\mathbb{E}_{s' \\sim T} [R(s,a) + \\gamma Q_\\pi(s',\\pi(s'))]$.\nFor the optimal policy, the Bellman optimality equations are:\n$V^*(s) = max_a \\mathbb{E}_{s' \\sim T} [R(s,a) + \\gamma V^*(s')]$,\n$Q^*(s,a) = \\mathbb{E}_{s' \\sim T} [R(s,a) + \\gamma max_{a'} Q^*(s,a')]$"}, {"title": "6. Imitation Learning", "content": "Imitation Learning (IL) [14-16] focuses on agents learning in a Markov Decision Process (MDP) without an explicitly defined reward function, instead relying on demonstrations of the task. This approach is particularly beneficial when designing a task-specific reward is challenging. A notable example is self-driving cars, where the complexity of driving makes it impractical to define a reward function, but ample video footage of human drivers is available for training. The typical formalization of the IL problem involves finding a policy that minimizes the divergence between the expert's feature distribution $q_{\\pi^*}(\\phi)$ and the learner's feature distribution $p_{\\pi}(\\phi)$, expressed as:\n$\\pi^* = argmin_\\pi D(q_{\\pi^*}(\\phi), p_{\\pi}(\\phi))$,\nwhere D is a measure of difference, such as the KullbackLeibler (KL) divergence. A common method to address the IL problem is Behavioral Cloning (BC), which treats imitation learning as a supervised learning task. Given a dataset of trajectories $D = \\{(T_i)\\}_{i=1}^{N}$ with $[(s_0,a_0),...,(s_T,a_T)]$, the objective is to minimize the cross-entropy loss:\n$L_n= - \\mathbb{E}_{ (s,a) \\sim D} [log \\pi(a|s)]$,\nMinimizing this loss is equivalent to minimizing the KL-divergence between the expert's trajectory distribution $P(\\tau | \\pi^*)$ and the learner's trajectory distribution $P(\\tau|\\pi)$. However, simple BC may suffer from distributional mismatch, where the learner's policy deviates from the expert's when operating outside the demonstrated state space. To address this, iteratively collecting new expert data is proposed.\nFurthermore, BC can only produce a policy that performs at best as well as the expert, which can be limiting if optimal expert trajectories are unattainable, prompting the exploration of Inverse Reinforcement Learning (IRL). IRL seeks to recover an expert's reward function based on observed trajectories, allowing for the development of an optimal policy through RL.\nThis process, termed Apprenticeship Learning, guarantees that the learned policy is consistent with a learned value function. Various strategies exist for deriving policies in IRL, including feature expectation-based methods, margin-maximization-based methods, and parameterization of the policy by the reward.\nRecent approaches have incorporated techniques akin to Generative Adversarial Networks (GAN) to emulate complex behaviors in high-dimensional environments. Although our contributions do not leverage IRL, it remains a significant area of study for surpassing demonstrator performance through advanced ranking methods."}, {"title": "7. Multi-Goal Reinforcement Learning", "content": "Multi-Goal Reinforcement Learning (MG-RL) extends standard RL by allowing agents to pursue multiple goals framed as constraints over one or more states. Goals can range from specific points to broader subspaces and even language-based goals (e.g., 'find a red object'). A goal-driven agent learns a goal-conditioned policy that generates actions based on the current state and goal, formalized as $a_t \\sim \\pi(\\cdot | S_t, z_g)$. In MG-RL, the problem is defined as an MDP with multiple reward functions, $R_G$, where the agent's behavior adapts depending on the goal pursued. Early work on MG-RL led to the development of Universal Value Function Approximators (UVFAs), where a single value function is learned for multiple goals, enabling efficient transfer learning across goals. Techniques like hindsight learning further improve sample efficiency by retrospectively using failed trajectories for goal learning."}, {"title": "7.1. Typology of Goal Representations", "content": "Goal representations vary across tasks. Common approaches include:\n*   Multiple Targets: One-hot encoded goals with distinct reward functions.\n*   State Features: Goals defined as target features (e.g., block coordinates, positions) with dense or sparse rewards.\n*   Dynamic Constraints: Language-based predicates representing constraints to be satisfied.\nMG-RL provides a framework for training versatile agents capable of pursuing a diverse set of goals by leveraging shared knowledge across tasks."}, {"title": "8. Problem Definition: Formative AI", "content": "People possess the ability to surprise, educate, and learn from one another, enabling the transfer and refinement of knowledge across generations. Even without a shared language or prior understanding, such as a parent teaching a child to stack blocks, humans can teach and learn through indirect signals and interactions. Experimental Semiotics studies the forms of communication that emerge when pre-established ones cannot be used, showing that humans can teach and learn without explicit demonstrations or shared protocols. For instance, a CoCo game explored a scenario where an architect guides a builder to construct a structure using arbitrary signals. This raises the question: can artificial agents develop such social conventions?"}, {"title": "9. Self-Organization Theory", "content": "The concept of emerging order stems from chaos theory and describes thermodynamic systems that self-organize from complex interactions. Formalized by cybernetician Ashby, self-organization refers to complex dynamical systems organizing around stable points called 'attractors'. An example is seen in the visual illusion, where perception shifts between two attractors: a young woman or an old woman. Self-organization is evident in nature, such as the formation of sand dunes and snowflakes in physical systems or bee hives and fish schools in biological systems. In technology, this principle enables innovations like adaptive traffic lights."}, {"title": "9.1. Self-Organization in Developmental AI", "content": "Developmental AI can be framed as adaptive systems where agents and their environment form coupled dynamical systems. This research formalizes two key problems using self-organization:\n(1) Language community formation among agents, seen as the self-organization of cultural conventions.\n(2) Autonomous skill acquisition as agents self-organize their behavior through internal drivers, leading to developmental trajectories. The autotelic approach integrates social interactions. These problems sit at the intersection of standard and developmental Al, studied further through language formation and the autotelic RL problem."}, {"title": "10. Language Game Interactions", "content": "Early solutions to the language game involve scoring tables that associate referents with utterances. Agents adjust scores based on communicative success, as illustrated in Figure 4. If predefined categories are unavailable, mechanisms to map visual inputs to object categories have been proposed. The Talking Head experiments adapt language games to dynamic word and meaning inventories. Inspired by the success of Convolutional Neural Networks, extended language games have been extended to image referents, where agents use neural networks for communication. In this framework, a speaker generates an utterance from two images, while a listener selects the target based on the utterance, as shown in Figure 5. Training occurs via Reinforcement Learning (RL) with a reward function reflecting communicative success. Beyond visual referents, researchers analyze the emergence of communication using neural agents. Sequences of symbols can name composite referents, and distinctions between agents' compositional capabilities and the properties of the communication code have been highlighted. Environmental factors influencing compositionality have also been investigated. While guessing interactions serve as an experimental foundation for language formation, human communication encompasses diverse purposes. Thus, AI researchers have examined communication in collaborative tasks using Multi-Agent Reinforcement Learning (MARL). For example, agents have been tasked with car coordination at traffic junctions to avoid collisions."}, {"title": "10.1. Emergence of Graphical Sensory-Motor Communication", "content": "Our first contribution extends the neural communicating agent framework to visual language games via a sensory-motor channel. Unlike prior approaches, which relied on idealized communication channels, we explore whether agents can develop a shared language within a sensory-motor framework. We introduce the Graphical Referential Game (GREG), where a speaker produces graphical utterances to identify visual referents among distractors, illustrated in Figure 7. The utterances are generated using dynamic motor primitives and a sketching library, with referents drawn from the MNIST dataset [17]. Through GREG, we investigate whether agents can self-organise a shared lexicon under sensory-motor constraints, assessing the coherence and compositionality of emerging signals."}, {"title": "10.2. The Architect-Builder Problem", "content": "Our second contribution introduces the Architect-Builder Problem (ABP), a novel paradigm examining goal-directed communication where the reward function is not accessible to all agents. In this setup, the architect knows the goal and receives rewards but cannot act, while the builder can act but lacks knowledge of the goal. The architect communicates with the builder solely through signals. The ABP addresses gaps in the existing literature by providing a new lens through which to study the emergence of communication in neural agents."}, {"title": "11. Self-Organization of Trajectories: The Open-Ended Skill Acquisition Problem", "content": "This section examines the self-organization of trajectories in open-ended skill acquisition, where autonomous agents refine skills through environmental interaction. Agents navigate complex action spaces, learning from experiences to improve task performance.\n*   Learning Objective Distributions: Agents must identify the support of objective distributions\u2014valid goal embeddings. Some methods operate within predefined spaces, while others leverage past representations to shape this distribution using generative models for image-based objectives.\n*   Goal Selection: After establishing an objective space, agents require strategies for goal selection. Automatic Curriculum Learning (ACL) helps in this regard, coordinating goal sampling for long-term performance improvement. Hierarchical Reinforcement Learning (HRL) sequences goals for lower-level policies, enabling task decomposition.\n*   Summary: We introduced the autotelic RL framework that fosters intrinsically motivated agents capable of open-ended goal generation.\n*   Scope: Previous studies using sensory-motor lacked the referential capabilities needed for generating meaningful communication. Our approach, distinguished by stochastic expressions and a decentralized structure, differs significantly in its methods and focus. We investigate factors promoting compositionality in emergent languages, measuring its effectiveness through communicative performance on hidden referents and geometric similarities."}, {"title": "11.1. Contributions", "content": "This section presents\n*   The Graphical Referential Game (GREG) for studying sign emergence in a graphical sensory motor framework.\n*   CURVES: a contrastive multimodal encoder with a generative model for graphical language emergence.\n*   Performance evaluation of CURVES on unseen structures in various settings.\n*   Comparative analysis of emerging language structure, focusing on vocabulary stability and compositionality scores via Hausdorff distance."}, {"title": "12. Graphical Referential Games", "content": "We concentrate on referential games, including a speaker (S) and an audience (L). Each game begins with a set R of n objects (referents) and an objective $r^* \\in R$. The speaker delivers an articulation u, and the audience chooses $\\hat{r} \\in R$. The game succeeds if $\\hat{r} = r^*$."}, {"title": "12.1. SetupReferents", "content": "Referents are balanced vector highlights (one-hot vectors). For m highlights $F_m$, the referent set is $R_m= \\{ P_f | S \\subseteq F_m \\}$, with subsets of k highlights $R_{hm} = \\{ \\sum_{f \\in S} f ||S| = k\\}$\nHere, m = 5."}, {"title": "12.1.1. Configurations", "content": "*   One-hot: $r \\in R_m$.\n*   Visual-shared: $r \\in R_m, R_s = R_L$.\n*   Visual-unshared: $r \\in R_m, R_s \\neq R_L$."}, {"title": "12.1.2. Sensory-motor Drawing System", "content": "Articulations are produced by $M : R^m \\rightarrow U \\subset \\mathbb{R}^{D \\times D}$ utilizing Dynamical Development Natives (DMPs), defined by $c \\in \\mathbb{R}^{20}$. Smooth directions $T = \\{v_i\\}$ are rendered into $D \\times D$ images (D = 52) by means of Differentiable Rendering."}, {"title": "12.2. Objectives We Address", "content": "*   Can the agent solve the game and generalize to compositional referents?\n*   Are the articulations interpretable and consistent?\n*   Do articulations display compositional principles?"}, {"title": "13. CURVES: Contrastive Articulation Referent Agreeable Scoring", "content": "It is an energy-based approach with two parts:\n*   Contrastive learning of an energy scene E(r,u) by means of cosine comparability.\n*   Articulation age amplifying energy for $r_s^*$."}, {"title": "13.1. Agents and Collaboration", "content": "Specialists $A \\in \\{A_1, A_2\\}$ utilize unmistakable CNN encoders $f_A$ (referents) and $g_A$ (articulations), planning to a common d-layered space: $z_{ra}=f_A(r)$, $z_{ua}= g_A(u)$. Energy scene: $E_A(r,u) = cos(f_A(r),g_A(u))$.\nThe result is o = 1[$r^=r^* - b, where b is the benchmark achievement rate.\nContrastive Learning: Specialists register closeness grids $\\Sigma_A$:\n$(\\Sigma_A)_{i,j} = E_A(r_i, u^j)$,\nwith the goal:\n$J_A(\\Sigma_A, I) = \\frac{CE((\\Sigma_A)_{1:n, ei}) + CE((\\Sigma_A)_{1:n,i, C_i})}{2}$\nWhere $e_i$ is a one-hot vector. Speaker and audience misfortunes:\n$min_{Ofs} \\sum_{s} o_i J_s (\\Sigma_S, I)$, $min_{OfL} \\sum_{L} o_i J_L (\\Sigma_L, I)$"}, {"title": "13.2. Expression Generation: Two Strategies", "content": "*   Distinct: Boost cosine comparability for $r_s^*$: $c^*= argmax_{c \\in R} E(r_{s^*},M(c))$.\n*   Discriminative: Limit cross-entropy for $r_s^*$"}, {"title": "14. Experiments", "content": "Specialists made close amazing preparation progress rates (SR) across every game setting (one-hot, visual-shared, and visual-unshared)."}, {"title": "14.1. Communicative Performance", "content": "Specialists made close amazing preparation progress rates (SR) across every game setting (one-hot, visual-shared, and visual-unshared)."}, {"title": "14.1.1. Generalization to Compositional Referents", "content": "Table 1 shows speculation execution on compositional referents ($r \\in R^i$) inside thorough settings ($|R| = 10$). Standard correlations incorporate a random technique ($SR_{random} = 0.1$) and a single-feature procedure ($SR_{1-feature} = 0.25$).\nSpecialists performed well across referent sorts, with one-hot settings yielding the most elevated SR, while execution in visual settings declined because of added intricacy according to point-of-view shifts. Curiously, enlightening and discriminative expressions made comparable progress rates, demonstrating that limiting uncertainty in articulations doesn't essentially support speculation execution."}, {"title": "14.2. Emergent Language Structure", "content": "Specialists exhibited expanding between specialist, between the point of view and between referent lucidness during preparation. Higher intelligence scores compared to merging correspondence conventions. For visual referents, particular signs arose, showing specialists' capacity to really separate referents."}, {"title": "14.2.1. Coherence", "content": "Specialists exhibited expanding between specialist, between the point of view and between referent lucidness during preparation. Higher intelligence scores compared to merging correspondence conventions. For visual referents, particular signs arose, showing specialists' capacity to really separate referents."}, {"title": "14.2.2. Compositionality", "content": "While specialists accomplished high SR for one-hot compositional referents, the produced articulations needed clear compositional designs. Examinations utilizing mathematical mappings, like distance measurements, showed a restricted vicinity between signals for compositional and individual referents."}, {"title": "14.2.3. Conclusion", "content": "Regardless of the shortfall of unequivocal compositional designs in created articulations, inward portrayals recommend specialists use compositional procedures. Compelled test arrangements are fundamental for additional investigation of rising language properties."}, {"title": "15. Experimental Design and Evaluation Metrics", "content": "To evaluate the efficacy of autotelic Reinforcement Learning (RL), a structured experimental setup was devised. The primary objective was to assess the agents' ability to autonomously generate and master diverse goals in dynamic environments. The experimental design encompassed the following key aspects:\nEnvironment Configuration: Agents were trained in simulated environments characterized by varying complexity levels. These environments included tasks requiring object manipulation, maze navigation, and abstract problem-solving, ensuring a comprehensive evaluation of agent capabilities.\n*   Evaluation Metrics:\n*   Exploration: Quantified using diversity measures that assess the range of goals generated and achieved by the agent.\n*   Generalization: Evaluated by testing agents on unseen goals and scenarios, measuring success rates compared to training tasks.\n*   Robustness: Assessed through perturbation tests, where environmental variables were systematically altered to determine the agent's adaptability.\nImplementation Details: The experimental framework incorporated various RL algorithms as baselines, enabling a comparative analysis of the autotelic framework against traditional approaches. Hyperparameter settings, training iterations, and computational resources were standardized across all experiments for consistency. This detailed experimental framework ensures that the proposed metrics capture the multifaceted performance of autotelic RL agents, providing a robust foundation for assessing their capabilities."}, {"title": "16. Discussion", "content": "In this part we formalized GREG: another biological referential game where two specialists should convey by means of a persistent tangible engine framework emulating a mechanical arm drawing outlines. To handle GREG, we propose CURVES: a contrastive portrayal of learning calculation enlivened by early language game contrastive execution that scales to high-layered signals. CURVES permits a gathering of two specialists two combine on a common graphical language in settings where referents are one-hot vectors or pictures of MNIST digits. The portrayals that specialists learn to empower them to convey compositional referents never experienced during preparation. Assuming the Haussdorf distance shows that rising signs are intelligent, it doesn't catch compositionality among them. Future work might use our natural arrangement and algorithmic answer to try different things with and test various theories that impact structures in self-coordinating using frameworks. An examination of the effect of the tangible engine imperatives on the geography of graphical signs could, for example, give a significant understanding of the natural elements working with the development of a compositional graphical language. Motivated by work on the social development of language, our arrangement can likewise act as a premise to explore and imagine the effect of different factors like populace dynamics or the mental capacities of specialists (with fluctuating memory or perceptual frameworks). At long last, CURVES is rationalist to the methodology used to address expressions. All things considered, it could handle other tactile engine frameworks. The focal component of CURVES lies in the contrastive learning of expression referent affiliations. In our execution, we improve expressions by boosting this energy through angle climb. Similar to the Clasp that opened numerous roads for multi-modular age, we could connect more intricate generative techniques like dispersion models."}, {"title": "17. Learning to Guide and to be Directed in the Draftsman Developer Problem", "content": "In this section, we explore the emergence of goal-directed communication between artificial agents in a novel setting, contrasting with the classical referential game where agents share the reward function. Specifically, we study collaboration between a builder - who performs actions but lacks access to rewards and an architect - who guides the builder towards the task's goal. This scenario requires agents to learn a task while simultaneously developing a communication protocol without predefined meanings. Drawing inspiration from Experimental Semiotics, we introduce the Architect-Builder Problem (ABP), where the architect knows the goal but can only send messages, while the builder acts without knowing the task, relying on the architect's guidance. We propose Architect Builder Iterated Guiding (ABIG), where the architect uses a learned model of the builder to guide it, and the builder employs self-imitation learning to reinforce its behavior. ABIG organizes interactions into structured frames, enabling agents to develop a reusable communication protocol, tested in a 2D environment with tasks like grasping cubes and building shapes, demonstrating effective generalization to unseen tasks."}, {"title": "17.1. Sociocultural Dynamics in Autotelic RL", "content": "Socio-cultural dynamics underpinning autotelic RL draw inspiration from human learning, where interactions with peers and the environment significantly influence skill acquisition. This study explores the integration of these dynamics into artificial agents, leveraging frameworks from developmental psychology and cognitive science.\n*   Theoretical Foundations: Sociocultural theories, such as Vygotsky's concept of the Zone of Proximal Development (ZPD), highlight how collaboration and shared knowledge accelerate learning\n*   By embedding similar principles into RL agents, the study aims to foster goal imagination and refinement through simulated peer interactions.\n*   Agent Interactions: Agents were immersed in environments where they could interact with other agents or simulated humans, exchanging information and adopting sociocultural conventions. For example, agents used language-like constructs to negotiate tasks and shared strategies for goal achievement.\n*   Implications for Learning: Sociocultural interactions enhanced the agents' ability to:\nGenerate novel goals inspired by peer actions."}, {"title": "18. Motivations", "content": "People possess the ability to surprise, educate, and learn from one another, enabling the transfer and refinement of knowledge across generations. Even without a shared language or prior understanding, such as a parent teaching a child to stack blocks, humans can teach and learn through indirect signals and interactions. Experimental Semiotics studies the forms of communication that emerge when pre-established ones cannot be used, showing that humans can teach and learn without explicit demonstrations or shared protocols. For instance, explored a CoCo game where an architect guides a builder to construct a structure using arbitrary signals. This raises the question: can artificial agents develop such social conventions?"}, {"title": "19. Designer Developer Problem", "content": "We consider a multispecialist arrangement made out of two specialists: a planner and a developer. The two specialists notice the climate state s, yet only the planner knows the objective within reach. The engineer can't make moves in the climate yet gets the natural prize r, though the manufacturer gets no award and has, in this manner, no information about the job that needs to be done. In this uneven arrangement, the modeler can collaborate with the developer through a correspondence signal m examined from its strategy \u03c0(m/s). These messages, which have no deduced implications, are gotten by the developer, which acts as per its approach \u03c0(\u03b1\u03c2,m). This makes the climate change to another state s' tested from PE(s's,a), and the designer gets reward r'. Messages are sent at each time step. The CoCo game that propelled ABP is outlined in Figure 11(a), while the general designer developer climate connection chart is given in Figure 11(b). The distinctions between the ABP setting and the MARL and IRL settings are represented. The engineer and the developer ought to team up to assemble the development focus while situated in various rooms. The engineering has an image of the objective while the manufacturer approaches the blocks.\nBuildWorld: We direct our trials in BuildWorld. BuildWorld is a 2D development framework universe of size (w \u00d7 h). Toward the start of an episode, the specialist and b blocks are produced at various arbitrary areas. The specialist can explore this world and handle blocks by enacting its gripper while on a block. The activity space A is discrete and incorporates a \"sit idle\" activity (|A| = 6). At each time step, the specialist notices its situation in the framework, its gripper state, as well as the place of the relative multitude of blocks and on the off chance that they have gotten a handle on (|S| = 3 + 3Nb). Tasks. BuildWorld contains 4 different preparation errands: 'Handle': The specialist should get a handle on any of the blocks; 'Spot': The specialist should put any block at a predefined area in the lattice; 'H-Line': The specialist should put every one of the blocks in a flat line design; 'V-Line':The specialist should put every one of the blocks in an upward line design. BuildWorld likewise has a harder fifth testing task, '6-blocksshapes', that comprises additional mind-boggling designs, and that is utilized to challenge a calculation's exchange capacities. For all assignments, rewards are inadequate and possibly given when the undertaking is finished. This climate typifies the intuitive learning challenge"}, {"title": "19.1. Planner Developer Issue", "content": "We consider a multispecialist arrangement made out of two specialists: a planner and a developer. The two specialists notice the climate state s, yet only the planner knows the objective within reach. The engineer can't make moves in the climate yet gets the natural prize r, though the manufacturer gets no award and has, in this manner, no information about the job that needs to be done. In this uneven arrangement, the modeler can collaborate with the developer through a correspondence signal m examined from its strategy \u03c0(m/s). These messages, which have no deduced implications, are gotten by the developer, which acts as per its approach \u03c0(\u03b1\u03c2,m). This makes the climate change to another state s' tested from PE(s's,a), and the designer gets reward r'. Messages are sent at each time step. The CoCo game that propelled ABP is outlined in Figure 11(a), while the general designer developer climate connection chart is given in Figure 11(b). The distinctions between the ABP setting and the MARL and IRL settings are represented. The engineer and the developer ought to team up to assemble the development focus while situated in various rooms. The engineering has an image of the objective while the manufacturer approaches the blocks.\nBuildWorld: We direct our trials in BuildWorld. BuildWorld is a 2D development framework universe of size (w \u00d7 h). Toward the start of an episode, the specialist and b blocks are produced at various arbitrary areas. The specialist can explore this world and handle blocks by enacting its gripper while on a block. The activity space A is discrete and incorporates a \"sit idle\" activity (|A| = 6). At each time step, the specialist notices its situation in the framework, its gripper state, as well as the place of the relative multitude of blocks and on the off chance that they have gotten a handle on (|S| = 3 + 3Nb). Tasks. BuildWorld contains 4 different preparation errands: 'Handle': The specialist should get a handle on any of the blocks; 'Spot': The specialist should put any block at a predefined area in the lattice; 'H-Line': The specialist should put every one of the blocks in a flat line design; 'V-Line':The specialist should put every one of the blocks in an upward line design. BuildWorld likewise has a harder fifth testing task, '6-blocksshapes', that comprises additional mind-boggling designs, and that is utilized to challenge a calculation's exchange capacities. For all assignments, rewards are inadequate and possibly given when the undertaking is finished. This climate typifies the intuitive learning challenge"}, {"title": "19.2. Communication", "content": "The engineer directs the developer by sending messages m which are one-hot vectors of size |V| going from 2 to 72."}, {"title": "19.3. Additional Suppositions", "content": "To zero in on the planner developer cooperations and the learning of a common correspondence convention, the draftsman approaches PE(s\\s,a) and to the prize capability r(s,a) of the current objective. This is expected to be that, assuming the planner were to act in the climate rather than the manufacturer, it would have the option to rapidly sort out some way to address the errand. This supposition is viable with the CoCo game trial where people members, and"}]}