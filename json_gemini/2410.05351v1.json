{"title": "Towards the generation of hierarchical attack models from cybersecurity vulnerabilities using language models", "authors": ["Kacper Sowka", "Vasile Palade", "Xiaorui Jiang", "Hesam Jadidbonab"], "abstract": "This paper investigates the use of a pre-trained language model and siamese network to discern sibling relationships\nbetween text-based cybersecurity vulnerability data. The ultimate purpose of the approach presented in this paper\nis towards the construction of hierarchical attack models based on a set of text descriptions characterising poten-\ntial/observed vulnerabilities in a given system. Due to the nature of the data, and the uncertainty sensitive environment\nin which the problem is presented, a practically oriented soft computing approach is necessary. Therefore, a key focus\nof this work is to investigate practical questions surrounding the reliability of predicted links towards the construction\nof such models, to which end conceptual and practical challenges and solutions associated with the proposed approach\nare outlined, such as dataset complexity and stability of predictions. Accordingly, the contributions of this paper fo-\ncus on producing neural networks using a pre-trained language model for predicting sibling relationships between\ncybersecurity vulnerabilities, then outlining how to apply this capability towards the generation of hierarchical attack\nmodels. In addition, two data sampling mechanisms for tackling data complexity, and a consensus mechanism for\nreducing the amount of false positive predictions are outlined. Each of these approaches is compared and contrasted\nusing empirical results from three sets of cybersecurity data to determine their effectiveness.", "sections": [{"title": "1. Introduction", "content": "Within computer-based systems, the emergence of\ncybersecurity vulnerabilities remains a key concern, as\ntheir presence may allow attackers unwarranted con-\ntrol over critical functionality and access to sensitive\ndata. However, given the sheer scale and complex-\nity of modern day systems, it is impossible to antici-\npate every potential vulnerability and it is thus infea-\nsible to completely secure any system against an at-\ntack. Many approaches for dealing with this intractabil-\nity exist, ranging from security oriented development\nlife-cycles, which aim to minimise the severity and like-\nlihood of emergent vulnerabilities [1], to strategies for\neffectively responding to a newly discovered vulnerabil-\nity in the event that it is not discovered prior to release.\nIn the latter case, when cybersecurity experts discover a\nvulnerability in a system, it is customary to inform the\ndeveloper and then publish information on the vulner-\nability publicly, in order to aid the mitigation of future\ncyberattacks and allow a response (such as a security\nupdate) to be mounted. One of the most popular medi-\nums through which this is done is the Common Vul-\nnerabilities and Exposures (CVE) database maintained\nby the MITRE corporation. As a primarily community\nrun effort, CVE entries are mainly characterised by text\ndescriptions written by volunteers in an informal style\ndescribing the nature and occurrence of the given vul-\nnerability.\nAlthough there are a wide variety of approaches ap-\nplied throughout the lifecycle of a software or hardware\nsystem, one common approach is the utilisation of mod-\nels for assessing cybersecurity risks [2, 3] or enumerat-\ning test cases [1, 4]. In particular, attack models are\noften used as a graphical representation of possible at-\ntacks within a given system, which can enable cyber-\nsecurity analysts to more intuitively determine areas of\nhigh risk. Within this paper, a specific type of attack"}, {"title": "2. Background and related work", "content": "There exists a notable selection of cybersecurity\ndatabases, each offering different types of data with dif-\nferent contextualising information at different levels of\nabstraction, in terms of the types of cybersecurity con-\ncept each dataset describes. These include, from most\ntangible to most abstract: ExploitDB\u00b9, Common Vul-"}, {"title": "2.1. Cybersecurity datasets", "content": "line is for networks to demonstrate the ability to deter-\nmine relationships between newly added vulnerabilities.\nThis remains true for privately maintained vulnerability\ndatasets, since as more information is gathered and new\nattacks are discovered, new entries are inevitable and the\ndeployment of a neural network capable of determining\nthe relationship of one vulnerability to others can ex-\npedite the analysis of how a new vulnerability affects\nthe security of a given system. A key characteristic of\nthe dataset chosen for this paper is that certain groups\nof vulnerabilities have many more entries than others.\nThus, the ability to generalise in a manner that does not\n\"lose\" these smaller groups is also explored as in the real\nworld, different types of vulnerabilities will be more or\nless common, but all equally important to evaluate."}, {"title": "2.2. Language models for cybersecurity tasks", "content": "Previous work has shown that using pre-trained lan-\nguage models, such as BERT [11], can be used for pre-\ndicting relationships between various MITRE cyberse-\ncurity datasets such as: CVEs and the ATT&CK ma-\ntrix [12], CVES and CAPEC entries [13], CVEs and\nCWE entries [14], predicting the exploitability rating\nof CVEs [15] and generating CVE descriptions from\nExploitDB entries [16]. Furthermore, there is work on\n\"fine-tuning\" the BERT model to cybersecurity domain\ntasks [17, 14, 15], which aims to improve the perfor-\nmance of downstream tasks by allowing the encoder to\nlearn domain specific language features. One issue with\ndeploying a BERT model is its large amount of parame-\nters, as the complexity of the network makes fine-tuning\nand application resource intensive. Thus, extensions of\nthe BERT model focusing on reducing the model com-\nplexity such as \"A lite BERT\" (ALBERT) [18] and Dis-\ntilBERT [19] have been proposed, which use parameter-\nreduction techniques and knowledge distillation respec-\ntively, to increase the efficiency of the BERT model.\nOf particular interest in this paper is the way in which\nseveral vulnerabilities (e.g. CVE entries) can be as-\nsigned to a group (e.g. a CWE), which offers additional\nhigh-level insight into how individual vulnerabilities re-\nlate to the bigger picture. While CVE entries consist of\nonly a text description in their original source, with ad-\nditional information like severity being provided via the\nNational Institute of Standards and Technology man-\naged National Vulnerability Database 7, CWE entries\noffer a gateway into relationships with other datasets,\nbackground details, applicable languages, common con-\nsequences, examples, and more."}, {"title": "2.3. Generating hierarchical attack models", "content": "A common approach towards managing cybersecu-\nrity is using attack models to emulate the behaviour\nof a given system whilst under cyberattack. This can\nbe approached in various ways, with graphical mod-\nels proving a popular method across various domains\n[21, 2, 22, 20, 9]. In essence, the premise is that the sys-\ntem and/or possible attacks are formally modelled using\ntechniques like graphical and mathematical representa-\ntions. As mentioned previously, of particular interest in\nthis paper are hierarchical attack models, which are de-\nfined here as a broad class of attack models which are\nstructured in a hierarchy, separating different levels of\nabstraction between individual attack actions and high-\nlevel goals.\nIn terms of what is actually being modelled, ap-\nproaches such as the attack tree [5, 2] focus on just\nthe attack model itself, with no explicit modelling of\nthe system itself beyond mentions to what a particular\nvulnerability targets. Meanwhile, attack graphs gener-\nally focus on modeling the physical system such as net-\nwork connections [23, 22] more explicitly and includ-\ning them within the graph itself, such that the behaviour\nof the system being targeted is modeled. Hong et al\n[21] introduce the concept of \"Hierarchical Attack Rep-\nresentation Models\", not to be confused with the gen-\neral class of hierarchical attack models, which separate\nsystem modelling into the \"top level\" of a hierarchy and\nthe \"bottom level\" made up of individual vulnerabilities,\nthus explicitly blending both approaches.\nWhile generation strategies exist for all the above ap-\nproaches [24, 23, 20, 9], the focus of this paper is on\nthe generation of hierarchical models in the style of at-\ntack trees, particularly from a \"bottom-up\" or \"vulner-\nability first\" perspective, focusing on the lowest level\nleaf nodes first. While there is a wide variety of ap-\nproaches in the literature for generating attack trees, in-\ncluding the use of process calculus to represent commu-\nnications within a system [25], using a formal enterprise\nmodel to derive an attack tree [26], hierarchies of ac-\ntions sourced from system models or explicitly defined\nrelationships [27] and using a graph based system model\nto follow the flow of data in a network and derive attack\npaths [28]. All of these focus on formally defined algo-\nrithms with pre-defined relationships in the data [8, 9],\nwhich do not incorporate advances in machine learning\nthat can open the way for inferring relationships from\nvulnerability data directly without the need for explicit\nmodels of the underlying system and its relationship to\nthe vulnerabilities.\nAttack tree generation methods can utilise different\ninformation sources to drive its generation, with the use\nof system models proving initially popular there has\nbeen increasing interest in using more generic attack\nlibraries to aid generation, with \"model-free\" methods\nthat do not necessitate the use of a system model also\nbeing proposed [8, 29]. Of these, the work of Falco et al\n[6] and Jhawar et al [7] make use of MITRE datasets"}, {"title": "3. Sibling prediction of CVES", "content": "In the context of relating CVEs to CWEs, a vital ques-\ntion must be answered on how the emergent CVE to\nCVE relationships are to be interpreted. Namely, if two\nCVEs are said to be linked to the same CWE, what does\nthat mean on a practical cybersecurity analysis level?\nSince CWEs represent high-level categories of weak-\nnesses, while CVEs represent individual low-level vul-\nnerabilities, the relationship from CWE to CVE can be"}, {"title": "4. Negative link problem", "content": "One factor which must be considered is how positive\nand negative links are formed, as each CVE will possess\na CWE \"parent\". Notably, the interpretation used by\nthis paper is that if two CVEs share a CWE parent, they\nare considered related (and thus form a positive link),\nwhile any two CVEs which do not share a parent are\nconsidered unrelated (negative link). A visualisation of\nthis can be seen in Figure 3, where fictional CWE 1 has\nchildren CVE 1 and 2, while CWE 2 has children CVE\n3,4 and 5, with an example positive and negative link\nbeing demonstrated.\nA major complication with this comes in the mathe-"}, {"title": "4.1. Clique-based sampling", "content": "One approach to mitigating the negative link problem\nis to define boundaries between cliques of CWEs within\nwhich CVEs are permitted to form negative links. This\ncould take the form of clustering together CWEs based\non some characteristics, such as the amount of children,\nor with a specific goal in mind, such as minimising the\namount of negative links. However, these are rather\narbitrary from the point of view of the data, meaning"}, {"title": "4.2. Weighted random sampling", "content": "An alternative solution is to only utilise a subset of\nall the CVE children to form negative links with, as il-\nlustrated in Figure 5, based on a weight inversely pro-\nportional to the relative size of the CWE. This sampling\nis controlled by Equation 1, which determines $N_i$ as the\npercentage of CVEs to sample from the given $CWE_i$.\n$N_i = \\frac{\\sum(C) - c_i * p}{\\sum(C)}$\nWith $c_i$ being the cardinality of the CWE for which $N_i$\nis being determined, $\\sum(C)$ being the sum of all CWE\ncardinalities and p being a parameter determining the\ndegree by which larger sets produce smaller samples.\nLogically, this equation assigns N such that the smaller\nthe cardinality $c_i$ with respect to all of C, more of its\nmembers will be sampled. This is to ensure that rel-\natively smaller CWEs are not overshadowed by larger\nones, and is based on the assumption that over repre-\nsented CWEs will be less affected by the culling of their\nnegative links. Empirically, the effectiveness of this"}, {"title": "5. Towards building hierarchical attack models", "content": "This section investigates how the neural networks de-\nscribed in previous sections can be utilised towards pro-\nducing hierarchical attack models. Firstly, let each pair-\nwise prediction between every vulnerability in a given\ninput set be structured into a \"prediction matrix\" P,\nwhere entry $P_{i,j}$ stores the predicted relationship be-\ntween vulnerabilities associated with indexes i and j re-\nspectively."}, {"title": "5.1. Algorithm for grouping vulnerabilities", "content": "arate. To illustrate, in the middle example in Figure 6\nnodes A and B are considered related to C, but not to\neach other, and so the resultant grouping results in 2 sep-\narate copies of C existing in two groups so that A and B\ndo not have to share a group with each other while each\ngetting to share a group with C.\nWhile not an attack model generator in itself, Algo-\nrithm 1 could be used in a layer-wise manner to produce\na model like an attack tree [2, 5, 3], which structures\nvulnerabilities in a hierarchy based on logical operators\nas parents (e.g. to enter a user account you must ob-\ntain the username AND password). This would require\ntwo additional functionalities: a multi class prediction\nmodel incorporating logical operators (OR, AND) and a\nway to incorporate information from the vulnerabilities\ninto upper layers. Meaning, that the sibling prediction\nnetwork should predict the type of logical relationship\nconnecting two vulnerabilities rather than simply deter-\nmining if there is one, and there should be some mech-\nanism for the prediction network to understand newly\ncreated \"parents\" alongside the NLP encoded vulnera-\nbilities to allow the top side of the structure to be created\nmingling both \"leaf\" and \"inner\" nodes together."}, {"title": "5.2. Consensus mechanism", "content": "If we contrast the task being learned by the neural net-"}, {"title": "6. Experimental results", "content": "Within this work, the key intention is to explore the\nability of neural networks using DistilBERT encoding\nof vulnerability descriptions to learn to predict previ-\nously unseen pairs of vulnerabilities. This means that in\nthe following experiments, the primary objective is to\ndetermine if these networks can successfully predict re-\nlationships between vulnerabilities that have been seen\nduring training, but with unseen pairings of these known\nvulnerabilities. An additional dimension to this is the\npotential for brand new vulnerabilities to be evaluated\nusing these same networks. In short, there are 2 sep-\narate levels of generalisation considered here: general-\nising to unseen links between previously seen vulnera-\nbilities, generalising to unseen links by virtue of show-\ning completely unseen vulnerabilities. Finally, these\ndifferent abilities to generalise can be shown using 3\nexperimental environments: testing unseen pairings of\nknown CVEs (old-to-old), testing known CVEs against\nunknown CVEs (old-to-new), and testing all pairings of\nunseen CVEs (new-to-new).\nNote that at the present, the ability to generalise this\nmethod to pairing new CVEs with new CVEs is out of\nscope, but the eventual end goal of a vulnerability-to-\nvulnerability prediction scheme is to enable new to new\nvulnerability prediction. To that end, the following ex-\nperiments aim to gauge this ability on 3 separate col-\nlections of CWEs, with an additional 2 modifications\nbeing introduced for the express purpose of evaluating\nthe ability to generalise to unseen CVEs. These data\ncollections have been assembled using the 2 data sam-\npling mechanisms introduced above. Further, these ex-\nperiments are also designed to take into account differ-\nent scales, with the amount of CWEs and their respec-\ntive cardinalities being a key variable, and the intended\neffect is to observe just how much smaller CWEs get\n\"lost\" in the training data. This is done by noting the\naverage, maximum and minimum sizes of CWE groups,\nand producing a correlation coefficient between the size\nof the group and its accuracy and F1 score for fine-tuned\nand base data. Another metric aiming to highlight the\nperformance of low size CWE groups is taking the accu-\nracy and F1 score of each CWE in isolation, then taking\nan average where every group has equal weight. In prac-\ntice, this results in the more common low-size CWEs\nbringing down the average metric to demonstrate their\nrelative poor performance."}, {"title": "6.1. Dataset 1", "content": "Dataset 1 is the largest, incorporated 3 cliques of\n5 CWEs each, whose breakdown can be seen in\nAppendix C. This set was chosen at random to contain\nrelatively low-cardinality CWEs. With a grand total of\n2283 unique CVEs and a negative to positive ratio of\nroughly 33:50. A learning rate of 5e - 7 and batch\nsize of 32 was used. Overall results for Dataset 1 can be\nseen in Table 1.\nTaken at a glance, these results do not tell the full\nstory however, as there are 15 CWE groups with vary-\ning sizes being flattened into 4 overall metrics. Overall,\nthe 3 cliques had an average CWE cardinality of 151,\nwith a maximum of 868 and minimum of 2. On the ba-\nsis of results obtained from testing on individual CWE\ngroups, 3 CWEs had not a single positive example in the\ntest set due to its small size, with another having only a\nsingle incorrectly classified positive sample. Using the\nPearson correlation coefficient, in the fine-tuned data a\npositive correlation of 0.29 can be computed between\nthe size of the CWE group and the accuracy of its pre-\ndiction, and a much stronger positive correlation of 0.6\ncan be drawn between the size of the group and the F1\nscore. In addition, the base sample had a 0.14 correla-\ntion between size of CWE and accuracy, and a 0.67 be-\ntween size and F1 score. When weighed equally, the av-\nerage accuracy of all CWEs taken individually is 99.84\nfor fine tuned and 99.36 for base, however, the average\nF1 score is 53.3 for fine-tuned and 44.34 for base."}, {"title": "6.2. Dataset 2", "content": "Details of the composition of Dataset 2 can be found\nin Appendix C. For this set, 8 CWEs and p = 2 was\nchosen to achieve a ratio of 2:3 between negative and\npositive links. This experiment utilised a learning rate\nof 5e-7 with a batch size of 32, the average CWE cardi-\nnality was 316.75, with a maximum of 1091 and a mini-\nmum of 12. Results for Dataset 2 can be seen in Table 2.\nThis time, interestingly, the Pearson correlation coeffi-\ncient between accuracy and size of CWE was -0.77 for"}, {"title": "6.3. Dataset 3", "content": "Details of the composition of Dataset 3 can be found\nin Appendix C, the design of this data sample was to\ninclude more high cardinality CWEs while maintaining\nthe same hyperparameters. For this set of CWEs the\nvalue of p = 1 was chosen to achieve a better balance of\nnegative and positive links, leading to a ratio of roughly\n33:50 negative to positive links. Same hyperparameters\nwere used as in Dataset 2, but with a larger mean CWE\ncardinality of 573.25, from a maximum of 2789 and\nminimum of 44. Table 3 shows the results for Dataset 3.\nOnce again, a negative correlation coefficient between\naccuracy and size of clique was computed, with -0.64\nfor fine-tuned and -0.63 for base. For F1 score, the cor-\nrelation was 0.65 for fine-tuned and 0.6 for base. When\ntaken individually, the average accuracy per CWE was\n88.87 for fine-tuned and 89.49 for base, with the aver-\nage F1 score being 36.33 for fine-tuned and 39.73 for\nbase."}, {"title": "6.4. Generation experiment", "content": "In order to gauge the effectiveness of the consensus\nmechanism, while also demonstrating the ability of the\nnetworks trained on weighted random data to generalise\nto mainly unseen negative links, this experiment con-\ncerns groups created using Algorithm 1 on the test sets\nof Dataset 2 and 3, with and without using the con-\nsensus mechanism outlined in Section 5.2. During pre-\ndiction, the best performing network was used in each\ncase, meaning that the fine-tuned encoder was used for\nDataset 2 predictions, and the base encoder was used for\nDataset 3 predictions.\nFirstly, for each Dataset, 100 CVE sets were pre-\npared at random from the two, three and four largest\nWEs such that the node selection produced pairings\nwhich never appear in training data. Using the largest\nCWEs means that there is a minimal amount of poten-\ntial negative links in the training data; while maximising\nthe potential pool of vulnerabilities, making the sam-\npling easier, with each CWE contributing a maximum\nof 10 CVEs. Following this, a prediction matrix P was\nprepared for each set using the relevant neural network\nalongside a consensus matrix C using Equations 3 and\n4, after which Algorithm 1 was used to generate groups\nfor both matrices.\nResults for this experiment on unseen pairings of old\nCVEs can be seen in Table 4, with the columns ex-\nplained as such: \"Dataset\" is the dataset from which\nentries were sampled, \"#\" being the amount of CWEs\nincluded in the sampling process (with each only be-\ning allowed to contribute a max of 10 CVEs), \"Direct\"\ndisplaying results using just matrix P and \"Consensus\"\nshowing results with the consensus mechanism via ma-\ntrix C. This is computed as the average similarity of\nthe generated group to the real CWE group it most re-\nsembles. This similarity metric is obtained using the\nJaccard similarity coefficient, which was also used by\nGadyatskaya et al. for gauging the similarity of the\n\"ground truth\" trees in literature to the ones generated\nby ChatGPT [30]."}, {"title": "7. Discussion & future work", "content": "It can be clearly seen that as long as both CVEs have\nbeen previously seen in some form, any of the 3 trained\nnetworks can correctly determine a previously unseen\ncombination of these with a very high accuracy of about\n93% to 99%. When it comes to determining links be-\ntween a previously seen CVE and a brand new CVE,\nthe results are much poorer. Although an average ac-\ncuracy of roughly 80% is achieved by all 3 networks,\nthis is coupled with a poor F1 score around 60%, con-\ntrasted with an F1 score of 96% to 99% for previously\nseen CVEs. Predictably, the accuracy and F1 scores for\npredicting links between completely new CVEs drops\nfurther, to levels which have questionable utility in a\npractical application at this stage.\nThis offers a variety of insights, as further analysis re-\nveals that a bulk of the inaccuracies stems from the less\nrepresented (aka smaller) CWE groups, an issue also\nplaguing the work of Das et al., for which the authors\nsuccessfully employ a \"reconstruction decoder\" to de-\ncrease the bias towards larger groups [14]. Employing\nregularization techniques such as this, alongside adjust-\ning the selection of cliques and values of p may yield\nmore promising results when applied towards the new-\nto-new prediction problem. Interestingly, while Dataset\n1 shows a positive correlation between size of CWE and\naccuracy, the other 2 experiments show a strong neg-\native correlation. This could be due to the sampling\nmechanism, as the other 2 utilise random weighted sam-\npling, but is more likely to do with the fact Dataset 1\ncontains more very small CWEs with a deceptively high\naccuracy, due to less of their positive examples appear-\ning in the test set. F1 score remains positively correlated\nwith CWE size in all experiments, which makes intu-\nitive sense as more represented CWE groups are more\nlikely to be remembered by the network."}, {"title": "7.1. Discussion of empirical results", "content": "In terms of comparison between the fine-tuned and\nbase DistilBERT encoder, some remarkable results can\nbe seen. Counter-intuitively, the fine-tuning process\ndoes not seem to have induced consistent improvement\nin performance, with the base encoder measurably out-\nperforming the fine-tuned encoder in Dataset 3.\nIn Dataset 1, as seen in Table 1, when determin-\ning links in previously seen CVEs, the fine-tuned en-\ncoder only slightly outperforms the base DistilBERT\nencoder. However, a more dramatic improvement is\nseen when contrasting the old-to-new and new-to-new\nperformance. In most cases, all metrics are boosted by\na noticeable margin, which becomes more dramatic in\nnew-to-new performance.\nIn Dataset 2, as seen in Table 2, there is also a signif-\nicant improvement when using the fine-tuned encoder,\nthough the scale of this improvement is more visible\nin old-to-old experiments than in the Dataset 1 exper-\niment. Measurable improvement is also seen in old-to-\nnew and new-to-new prediction, though the F1 score re-\nmains poor.\nDataset 3, as seen in Table 3, shows the most counter-\nintuitive results. In contrast to the previous 2 experi-\nments, the base encoder shows slight improvement in\npredicting old-to-old and old-to-new CVE relationships,\nhowever this improvement becomes significant when\npredicting new-to-new CVEs."}, {"title": "7.1.1. Performance of neural networks", "content": "In terms of comparison between the fine-tuned and\nbase DistilBERT encoder, some remarkable results can\nbe seen. Counter-intuitively, the fine-tuning process\ndoes not seem to have induced consistent improvement\nin performance, with the base encoder measurably out-\nperforming the fine-tuned encoder in Dataset 3.\nIn Dataset 1, as seen in Table 1, when determin-\ning links in previously seen CVEs, the fine-tuned en-\ncoder only slightly outperforms the base DistilBERT\nencoder. However, a more dramatic improvement is\nseen when contrasting the old-to-new and new-to-new\nperformance. In most cases, all metrics are boosted by\na noticeable margin, which becomes more dramatic in\nnew-to-new performance.\nIn Dataset 2, as seen in Table 2, there is also a signif-\nicant improvement when using the fine-tuned encoder,\nthough the scale of this improvement is more visible\nin old-to-old experiments than in the Dataset 1 exper-\niment. Measurable improvement is also seen in old-to-\nnew and new-to-new prediction, though the F1 score re-\nmains poor.\nDataset 3, as seen in Table 3, shows the most counter-\nintuitive results. In contrast to the previous 2 experi-\nments, the base encoder shows slight improvement in\npredicting old-to-old and old-to-new CVE relationships,\nhowever this improvement becomes significant when\npredicting new-to-new CVEs.\nWhy this is the case remains rather unclear. In-\ntuitively, this phenomenon could be explained by the\nfact that the prediction network might not use the more\nsalient data obtained with the fine-tuned encoder when\nfaced with CVEs it has previously examined, as it can\nlearn more specific features of these CVEs (arguably,\noverfit to them), while when faced with unseen CVEs,\nit instead is forced to rely on more salient language fea-\ntures obtained using the fine-tuned encoder. However,\nthis does not account for the fact that the fine-tuned\nencoder shows improvement in all data sampled but 1.\nThis would most likely be explained by some other pa-\nrameter of the data, such as the selection of CWEs and\nthe language used within the description of its CVEs, or\ndifference in the data distribution."}, {"title": "7.1.2. Fine-tuned vs base DistilBERT encoder", "content": "Why this is the case remains rather unclear. In-\ntuitively, this phenomenon could be explained by the\nfact that the prediction network might not use the more\nsalient data obtained with the fine-tuned encoder when\nfaced with CVEs it has previously examined, as it can\nlearn more specific features of these CVEs (arguably,\noverfit to them), while when faced with unseen CVEs,\nit instead is forced to rely on more salient language fea-\ntures obtained using the fine-tuned encoder. However,\nthis does not account for the fact that the fine-tuned\nencoder shows improvement in all data sampled but 1.\nThis would most likely be explained by some other pa-\nrameter of the data, such as the selection of CWEs and\nthe language used within the description of its CVEs, or\ndifference in the data distribution."}, {"title": "7.1.3. Generation experiments and Consensus mecha-nism", "content": "With respect to the experiments performed with Al-\ngorithm 1 and the consensus mechanism, it is clear that\nin all cases the consensus mechanism performs very\nwell in minimising the impact of false positive and neg-\native predictions on the stability of the generation algo-\nrithm. However, despite the networks achieving a sig-"}, {"title": "7.2. Practical viability", "content": "Undoubtedly, the biggest limitation of the work pre-\nsented thus far is the difficulty to meaningfully gener-\nalise to brand new CVEs. While not utilised in this\nwork, approaches for better mitigating the imbalance\nof certain groups over others do exist, most relevant to\nthis domain being the use of a reconstruction decoder\nby Das et al. [14]. It is clear from the results that in-\ncluding too many CWEs in the underlying training data"}, {"title": "7.3. Future work", "content": "Principally, this paper has set out to open up the pos-\nsibility of utilising machine learning based predictive\nmodels for constructing hierarchical attack models in\nthe style of attack trees. Although many questions have\nbeen addressed within this work, many remain to be in-\nvestigated.\nIn terms of practical viability, work should be done\non how viable vulnerability-to-vulnerability prediction\nis when using vulnerabilities grouped based on charac-\nteristics not described by CWE membership. As men-\ntioned above, if a penetration test or an analysis of the\ndata determined different groups of vulnerabilities, a\nmethod such as this should be deployed to determine\nhow well a network can learn to associate future vulner-\nabilities within these groups.\nIn addition, more work needs to be done on the way\npositive and negative links are sampled between vul-\nnerabilities, and a mechanism is needed to account for\noverlapping interpretations of group membership (e.g.\nvulnerabilities appearing in multiple groups) and under-\nlying relationships in the data (e.g. CVEs belonging to\nCWEs that are linked to one another).\nAlso worth noting is the fact that beyond just CVE\nand CWE, there is a wide range of sources for hier-\narchical relationships between more abstract MITRE\ndatasets, with there being at least 2 additional datasets\nplaced above CWE [10], which could provide a source"}, {"title": "8. Conclusion", "content": "This paper has proposed a method for the prediction\nof sibling-level relationships between vulnerability data\nin the form of CVE entries grouped together using their\nassociations to CWE entries. Having identified the chal-\nlenges in terms of data complexity, particularly with\ndata imbalance, two strategies for reducing class im-"}, {"title": "CRediT authorship contribution statement", "content": "Kacper Sowka: Conceptualisation, Investigation,\nData curation, Methodology, Software, Visualisation,\nValidation, Writing \u2013 original draft. Vasile Palade: Su-\npervision, Methodology, Conceptualisation, Writing\nreview & editing, Resources. Hesam Jadidbonab: Su-\npervision, Conceptualisation, Writing \u2013 review & edit-\ning. Xiaorui Jiang: Supervision, Conceptualisation,\nWriting - review & editing."}, {"title": "Declaration of competing interests", "content": "The authors declare that beyond the co-funding of\nthis research by Coventry University and HORIBA\nMIRA, as part of a PhD studentship, they have no\nknown competing financial interests or personal rela-\ntionships that could have appeared to influence the work\nreported in this paper. While the deliverables of the\nproject (code, prototype, trained models etc) are the in-\ntellectual property of HORIBA MIRA held for the po-\ntential development of future services or products, this\ndid not directly influence the contents of this paper."}, {"title": "Data availability", "content": "While the code and trained models used to derive\nthese results cannot be provided due to prior agreements\nin relation to funding, the data used was acquired from\na publicly available instantiation of the BRON dataset\nand can be further distributed based on the MIT license\nwhich applies to BRON10. This selection can be found\nin JSON format using the DOI 10.17632/s2sw4ck42n.1."}]}