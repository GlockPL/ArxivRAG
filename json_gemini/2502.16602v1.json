{"title": "VidLBEval: Benchmarking and Mitigating Language Bias in Video-Involved LVLMS", "authors": ["Yiming Yang", "Yangyang Guo", "Hui Lu", "Yan Wang"], "abstract": "Recently, Large Vision-Language Models (LVLMs) have made significant strides across diverse multimodal tasks and benchmarks. This paper reveals a largely under-explored problem from existing video-involved LVLMs - language bias, where models tend to prioritize language over video and thus result in incorrect responses. To address this research gap, we first collect a Video Language Bias Evaluation Benchmark, which is specifically designed to assess the language bias in video-involved LVLMs through two key tasks: ambiguous video contrast and interrogative question probing. Accordingly, we design ac-companied evaluation metrics that aim to penalize LVLMs being biased by language. In addition, we also propose Multi-branch Contrastive Decoding (MCD), introducing two expert branches to si-multaneously counteract language bias potentially generated by the amateur text-only branch. Our experiments demonstrate that i) existing video-involved LVLMs, including both proprietary and open-sourced, are largely limited by the language bias problem; ii) our MCD can effectively mitigate this issue and maintain general-purpose capabil-ities in various video-involved LVLMS without any additional retraining or alteration to model architectures.", "sections": [{"title": "1 Introduction", "content": "Building on the significant advancements of Large Language Models (LLMs) [Achiam et al., 2023; Dubey et al., 2024; Yang et al., 2024], Large Vision-Language Models (LVLMs) have recently garnered considerable attention [Li et al., 2023a; Zhu et al., 2024; Chen et al., 2024b]. Representative models such as LLaVA [Liu et al., 2024b] and Video-ChatGPT [Maaz et al., 2024b] exhibit impressive capabilities across a variety of multimodal tasks and their associated benchmarks. However, despite their potential, LVLMs have suffered from a language bias problem, which often manifests as skewed shortcuts between questions and responses. Some previous studies attribute the cause of this issue to mismatched model sizes between the base LLM and vision encoder within LVLMs [Rohrbach et al., 2018; Chen et al., 2024b]. In particular, the involved language model size is often ten times larger than the vision encoder, leading to a tendency to prioritize language over vision [Guan et al., 2024; Leng et al., 2024; Liu et al., 2024a]. To expose this problem in image-based LVLMs, HallusionBench [Guan et al., 2024] and AutoHallusion [Wu et al., 2024] perturb each input instance by removing or editing the given image, and then probe the potentially contradictory responses of <original image, perturbed image> pairs. In addition, some methods aim to address this issue by contrasting distorted visual inputs [Leng et al., 2024] or deliberately increasing the attention weights assigned to image tokens [Liu et al., 2024d]. We note that current studies primarily focus on language bias in image-only LVLMs. This problem, however, has been largely ignored by the existing literature within the video-involved LVLMs domain. As a result, the practical video-centric applications of LVLMs, such as autonomous driving and security surveillance, are significantly compromised. To address this research gap, we first collect a Video Language Bias Evaluation Benchmark (VidLBEval) to evaluate the language bias problem in video-involved LVLMs. Our VidLBEval involves two evaluation tasks: Ambiguous Video Contrast (AVC) and Interrogative Question Probing (IQP). For the former, we pair each original video with either 1) another relevant video or 2) its distorted counterpart. These instances are maintained with distinct answers, which will penalize these models that consistently respond with the same answer for the same query. For the latter, we curate follow-up questions beyond the original query to challenge the model's prediction confidence. These newly generated questions are highly grounded in the joint understanding of one original answer option and the given video (see Figure 1 for the dataset examples). Our second contribution in this work is a multi-branch contrastive decoding method. To this end, we introduce two expert branches to simultaneously counteract the language bias potentially generated by the amateur text-only branch. Specifically, beyond the weak expert inheriting the original model process, we design a strong expert branch to lay more attention on video features, prioritizing the reasoning over video content. We then apply this method to three state-of-the-art video-involved LVLMs-VideoLLaVA [Lin et al., 2024], VideoLLaMA2 [Cheng et al., 2024], and"}, {"title": "2 Related Work", "content": "Language Bias in VQA. Language bias has long been recognized as a challenging problem for conventional visual question answering (VQA). Previous methods in alleviating this problem can be roughly categorized into three groups: ensemble learning, contrastive learning, and loss rescaling. Approaches in the first group [Cad\u00e8ne et al., 2019; Clark et al., 2019] introduce an additional bias branch which is trained with the original input in an ensemble manner. Contrastive learning-based debiasing methods [Liang et al., 2020; Si et al., 2022] first generate positive and negative samples using data augmentation techniques. These samples are then utilized to jointly optimize the model with a contrastive learning loss alongside the original classification loss. The last group methods [Guo et al., 2021; Wu and Mooney, 2019] address this problem with inspiration from class-imbalance mechanisms. To this end, each instance-aware loss is re-weighted based on training data statistics to achieve fair training. Hallucination in LVLMs. Hallucination in LVLMs often refers that the generated textual responses are plausible but contradictory to the associated visual content [Zhou et al., 2024a; Zhou et al., 2024b]. Some initial efforts have been devoted to building benchmarks to probe the hallucinatory level of LVLMs. For instance, CHAIR [Rohrbach et al., 2018] and GAVIE [Liu et al., 2024a] instruct models to generate a free-form caption to reveal their exposure to errors, POPE [Li et al., 2023d], HallusionBench [Guan et al., 2024] and AutoHallusion [Wu et al., 2024] query models in terms of visual reasoning aspects with binary questions. Besides, hallucination mitigation has also attracted extensive interest recently. Some data augmentation methods like LRV-Instruction [Liu et al., 2024a] and HalluciDoctor [Yu et al., 2024a] introduce additional negative and counterfactual data to fine-tune LVLMs. Other approaches propose to leverage contrastive decoding [Leng et al., 2024; Liu et al., 2024d; Kim et al., 2024] or reinforcement learning from human feedback [Gunjal et al., 2024; Yu et al., 2024b] to address this problem. Overall, hallucination in LVLMs often manifests with multiple dimensions, wherein language bias contributes a significant factor. As a result, performing language debiasing greatly assists the reduction in hallucination, therefore improving the reliability of LVLMs. Benchmarks for Video-Involved LVLMs. The pervasiveness of LVLMs is accompanied by continual development in video-involved benchmarks. SEEDBench [Li et al., 2024a] and Video-Bench [Ning et al., 2023] cover a wide variety of video-centric tasks and aim to provide a comprehensive evaluation for video understanding capabilities. However, some studies find that these general benchmarks suffer from the static spatial bias from single frames [Lei et al., 2023]. To approach this, MVBench [Li et al., 2024b] and Tempcompass [Liu et al., 2024e] curate video instances covering more temporal aspects such as speed, moving direction, attribute"}, {"title": "3 VidLBEval Dataset Collection", "content": "Our benchmark dataset is built upon four publicly available video QA datasets: Next-OOD-QA [Zhang et al., 2024], Causal-VidQA [Li et al., 2022], CLEVRER-QA [Yi et al., 2020], and STAR-QA [Wu et al., 2021]. We source these datasets hinging on two criteria: 1) The validation or test sets have rarely been used in LVLM pre-training, which prevents the potential data leakage problem. 2) The datasets are enabled to cover a broad range of video concepts such as movement direction and scenarios like action count. With these anchor <video, question, answer options> candidates, we then construct our VidLBEval dataset."}, {"title": "3.1 Ambiguous Video Contrast (AVC)", "content": "Our first evaluation task queries LVLMs with the same question and different videos sharing similar semantics, where the answers are distinct. The motivation is that a highly biased model is prone to predict the same answer for the same question irrespective of the video context. We implement this idea with a two-stage data construction pipeline. To facilitate the evaluation as well as increase the task difficulty, we first employ the VideoMAE [Tong et al., 2022] model to extract visual features of all videos, based on which the pair-wise cosine similarity among videos is calculated. For each original video, we retrieve the most similar video that resembles highly in semantic content. In addition, we obtain another distorted video as a complement with the aid of applying Gaussian noise. In the second stage, we leverage GPT-4V [OpenAI, 2023] to generate detailed video descriptions. By the combination of the descriptions of the two paired videos, we then instruct GPT-4 to formulate a question that applies to both videos\u00b2. In particular, the ground-truth answer to each given question is complemented with multiple distracted answers. These distractors are ensured to maintain a high similarity with the ground-truth one in semantics."}, {"title": "3.2 Interrogative Question Probing (IQP)", "content": "In addition to the first task using distracted videos, we also explore complementing the original question with follow-up questions. Our intention for this evaluation task is two-fold: 1) the newly introduced questions require joint video-text understanding and 2) the models are expected to maintain great consistency of the original answering and its follow-up process. To this end, we first concatenate the original question with both the ground-truth answer and other candidate options. The combined text is then prompted to GPT-4 for generating follow-up questions with binary answers. These questions are crafted to introduce misleading information to challenge the model's consistency. We follow existing studies [Zhang et al., 2016] to focus on binary questions for two specific reasons. First, answering binary questions is generally easier than open-ended ones, and can be seen as a second visual concept perception verification. Second, existing LVLMs are shown to deliver affirmative responses regardless of the visual context [Li et al., 2023d]. We therefore, keep a balanced distribution of yes and no answers to eliminate such a visual priming bias."}, {"title": "3.3 Quality Control & Data Statistics", "content": "We show the quality control pipeline and the output from each step in Figure 2. Language-only Filtering. We expect the collected VidL-BEval dataset to require grounded visual reasoning, which cannot be addressed with the question only [Chen et al., 2024a]. To this end, we input the generated questions into three powerful LLMs, including Llama3 [Dubey et al., 2024], Qwen2 [Yang et al., 2024], and Phi3.5 [Abdin et al., 2024]. The questions which can be blindly answered without looking at the associated video are thereafter filtered out. External Tool Screening. We then utilize the Perspective API\u00b3 to assess the potential negative impacts of the generated sentences, such as rudeness and toxic content. In addition, GPT-4V serves as an expert for the AVC task to avoid the cases that some questions become unanswerable [Guo et al., 2024]. GPT-4o, in the IQP task, evaluates the generated questions on various aspects such as logical coherence and lexical precision, with samples passing the threshold kept. Human Verification. We finally involve further human verification, which results in our VidLBEval dataset with 521 and"}, {"title": "3.4 Evaluation Metrics", "content": "In the first AVC task, since the instances are maintained with distinct answers, for the same question, we aim to penalize LVLMs that consistently provide the same response. In particular, for each question, we consider the pairwise prediction consistency between the original video and the related video or the distorted video. Under this context, we compute the paired instances that result in the same prediction yet at least one answer prediction is incorrect. It is because we care more about the language bias effect than the answering accuracy. Recall the left example of Figure 1, we consider the model biased when it consistently responds with toy rocket across the two given videos. In this way, we design a Biased Visual Consistency (BVC) metric that accumulates these pairs over the whole dataset. We further divide the BVC into two categories: 1) BVCrel for the relevant video and 2) BVCdis for the distorted counterpart. A lower BVC corresponds to a better model that bears less language bias. Moreover, for the second IQP task, we aim to evaluate the logical consistency of model outputs and ensure that questions are not answered through random guessing. The follow-up questions are designed to be logically correlated over a video. Based on this intuition, we introduce Text Consistency Rate (TCR) and Robust Accuracy (RA) metrics. We first classify the answering prediction attributes in Figure 3 and then use N as a general notation for the sample count in the respective category. Based on this figure, the TCR is defined as TCR = NCR/(NCR + NPR), while the RA is given by RA = NCR/(NCR + NPR + NPV + Ncv). Beyond conventional accuracy metrics, our novel evaluation strategy provides a more effective mechanism for language bias probing while eliminating dependency on external LLMs. Specifically, LLM assessment introduces additional costs and variability, as open-source models may produce inconsistent outputs across different versions for the same input, such as GPT-4 and GPT-4V-Turbo used by [Guan et al., 2024] and [Wu et al., 2024], respectively. In contrast, our metrics are designed to be both stable and deterministic, ensuring a consistent and cost-efficient evaluation protocol."}, {"title": "4 Method", "content": "4.1 Preliminaries and Motivation\nWe consider a video-involved LVLM, parameterized by \u03b8, typically designed to generate the response y given a video v and a textual query x. The operation starts by passing the video v through a vision encoder, followed by a projector that maps it into a set of visual tokens. These visual tokens are then concatenated with the text tokens to serve as the input for the language model in LVLM. The response y is auto-regressively generated from the probability distribution conditioned on the query x, the video v, and the generated tokens y<t up to the time step t \u2212 1,\n\\(y_t \\sim P_{\\theta}(y_t | v, x, y_{<t}) \\approx logit_{\\theta}(y_t | v, x, y_{<t}),\\) (1)\nwhere yt represents the token sampled at the t-th time step, and logit refers to the predicted logits after the softmax function by model \u03b8.\nBuilding on the foundational advancements in LLMs [Li et al., 2023c], recent studies [Leng et al., 2024; Liu et al., 2024d; Kim et al., 2024] have introduced the Visual Contrastive Decoding (VCD) mechanism to enhance the visual understanding capability, thereby reducing the hallucination problem in image-based LVLMs. The next-token probability Pvcd in VCD is generally expressed as:\n\\(P_{vcd} = (1 + \\gamma)P_{\\theta}(y_t | v, x, y_{<t}) - P_{\\theta}(y_t | x, y_{<t}),\\) (2)\nwhere p\u03b8(yt|x, y<t) represents the amateur branch with pure textual inputs, and \u03b3 controls the penalty extent. It is worth noting that we do not consider other alternatives for the amateur branch, such as the same model architecture with different parameters or other inputs.\nMotivation. Our motivation for this method is two-fold. First, to the best of our knowledge, the VCD algorithm has been rarely studied in video-involved LVLMs. As such, we intend to explore its effectiveness within this specific domain. Second, we believe that video understanding requires a more nuanced approach compared to image understanding. It is because that videos inherently exhibit richer temporal dynamics than images, making it crucial to focus more on video frames."}, {"title": "4.2 Multi-branch Contrastive Decoding", "content": "We propose a Multi-branch Contrastive Decoding (MCD) framework, as shown in Figure 4,\n\\(P_{mcd} = (1 + \\gamma)p_{\\theta}(y_t | v, x, y_{<t}) - p_{\\theta}(y_t | x, y_{<t}),\\)\n\\(where\\ p = \\lambda p^w_{\\theta}(y_t | v, x, y_{<t}) + (1 - \\lambda) p^s_{\\theta}(y_t | v, x, y_{<t}).\\) (3)\nHere, \\(p^w_{\\theta}(y_t | v, x, y_{<t})\\) and \\(p^s_{\\theta}(y_t | v, x, y_{<t})\\) represent the weak expert and video-enhanced strong expert branches, respectively. The integrated expert, denoted by p', incorporates a weighting factor \u03bb \u2208 [0, 1] to balance the contributions of the two experts. In addition to the original weak expert with multimodal input used in previous VCD methods, we introduce the video-enhanced branch as the strong expert. The new branch places greater emphasis on video content, thereby allowing visual features to be interacted more with response generation.\nThe MCD objective rewards text patterns preferred by the multimodal expert branch while penalizing those favored by the amateur branch. However, this can lead to the over-penalization of text-based outputs that still align with linguistic norms and common sense. To address this, we follow [Li et al., 2023c] to introduce an adaptive plausibility constraint based on the confidence level of the output distribution:\n\\(V_{head}(y_{<t}) = \\{y_t \\in V :\\ P_{\\theta}(y_t | v, x, y_{<t}) \\geq \\beta\\ max\\ P_{\\theta}(w | v, x, y_{<t})\\},\\)\n\\(P_{mcd}(y_t | v, x, y_{<t}) = 0, if\\ y_t \\notin V_{head}(y_{<t}),\\) (4)\nwhere V refers to the token vocabulary and \u03b2 controls the truncation of the next token distribution, with only tokens in Vhead being considered for potential candidates. This method refines the candidate pool, effectively preventing the generation of implausible tokens and preserving the quality of the generated content."}, {"title": "4.3 Video-Enhanced Branch Design", "content": "To construct the strong expert branch, we propose to increase the attention weights by updating the self-attention matrices. Specifically, we first locate the attention positions of the video tokens for the currently generated token from the attention weights A \u2208 Rnxn before the softmax operation, where n is the sequence length. An amplification coefficient \u03b1 >= 0 is then applied to the video tokens to control the step size for generation intervention. We formulate this operation as:\n\\(A_i = A_i + \\alpha [A_i], where\\ i \\in \\{n_k + 1, ..., n_k + n_v\\},\\) (5)\nwhere nk and n\u03c5 indicate the number of query tokens preceding the video token and video tokens, respectively. Subsequently, a softmax function redistributes the attention values across all tokens. This encourages the attention mechanism to concentrate more on video information, thereby making the constructed strong expert branch video-enhanced.\nIt is worth noting that the attention weights are automatically redistributed during inference, without any additional retraining or alteration to model architectures. Moreover, we maintain the same parameters for all the three branches and do not introduce any additional parameters to the LVLMs, enabling our method to be seamlessly integrated into different models without many bells and whistles."}, {"title": "5 Experiments", "content": "5.1 Experimental Settings\nBaselines. We first benchmarked various video-involved LVLMs on VidLBEval: VideoChat (7B) [Li et al., 2023b],"}, {"title": "5.2 Experimental Results", "content": "Benchmark Results. We summarize the overall benchmark results across eight video-involved LVLMs on VidLBEval in Table 2. Our observations are three-fold: 1) All the evaluated models consistently demonstrate severe language bias. For example, these models display a clear weak logical consistency, as evidenced by the TCR and RA remaining below one-third and one-fourth, respectively. On the other hand, the majority of BVCrel exceed 30%, implying that current video-involved LVLMs confuse with similar videos. 2) Proprietary GPT-4V shows superior results than open-source models. This discrepancy is evidently demonstrated by BVCdis, with open-source models highlighting a marked gap at nearly 70%. 3) IQP poses greater challenges compared to AVC. Specifically, even the best-performing models, such as GPT-4V and VideoGPT+, yield suboptimal results. MCD Performance on VidLBEval. We present language bias results across three state-of-the-art video-involved LVLMs, as shown in Table 3. In summary, there is a notable improvement after incorporating MCD. Specifically, across various decoding settings, our MCD method consistently exceeds the baseline results by large margins. This highlights its critical role in enhancing video-focused understanding, thereby reducing instances of language bias."}, {"title": "5.3 In-depth Analysis on MCD", "content": "Effect of Different Branches. In our proposed MCD, we introduce the video-enhanced branch, which is integrated with the original branch to generate a more robust prediction. We then evaluate the effectiveness of each respective branch, compare it against the vanilla greedy decoding, and present the results in Table 4. One can see that each branch positively contributes to the reduction of language bias. Integrating the two expert branches together delivers the best results across different models. Case Study on VideoLLaVA. Figure 6 demonstrates two cases on how regular decoding can yield language bias. In the first case, the model consistently responds with shiny metal ball, despite green cube being the first object presented in the other video. In the second case, the model gives the affirmative answer yes, disregarding the fact that the boy kicked the ball after it fell out of the hoop. In contrast, our MCD emphasizes the video information by significantly increasing the attention weights of video tokens, thereby effectively mitigating language bias."}, {"title": "6 Conclusion and Discussion", "content": "In this paper, we address the research gap concerning language bias in video-involved LVLMs. We first introduce the VidLBEval benchmark to evaluate language bias in video-involved LVLMs, making it distinguished from existing benchmarks. Our initial findings reveal that current models"}]}