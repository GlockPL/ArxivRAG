{"title": "Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics", "authors": ["Ruining Li", "Chuanxia Zheng", "Christian Rupprecht", "Andrea Vedaldi"], "abstract": "We present Puppet-Master, an interactive video generative model that can serve as a motion prior for part-level dynamics. At test time, given a single image and a sparse set of motion trajectories (i.e., drags), Puppet-Master can synthesize a video depicting realistic part-level motion faithful to the given drag interactions. This is achieved by fine-tuning a large-scale pre-trained video diffusion model, for which we propose a new conditioning architecture to inject the dragging control effectively. More importantly, we introduce the all-to-first attention mechanism, a drop-in replacement for the widely adopted spatial attention modules, which significantly improves generation quality by addressing the appearance and background issues in existing models. Unlike other motion-conditioned video generators that are trained on in-the-wild videos and mostly move an entire object, Puppet-Master is learned from Objaverse-Animation-HQ, a new dataset of curated part-level motion clips. We propose a strategy to automatically filter out sub-optimal animations and augment the synthetic renderings with meaningful motion trajectories. Puppet-Master generalizes well to real images across various categories and outperforms existing methods in a zero-shot manner on a real-world benchmark. See our project page for more results: vgg-puppetmaster.github.io.", "sections": [{"title": "1 Introduction", "content": "We consider learning an open-ended model of the motion of natural objects, which can understand their internal dynamics. Most models of dynamic objects are ad-hoc and only work for a specific family of related objects, such as humans or quadrupeds [1, 2], severely limiting their generality. More open-ended models like [3] do not use such constrained shape priors but are difficult to scale due to the lack of suitable training data (i.e., vertex-aligned 3D meshes). Therefore, we require a more general framework to learn a universal model of motion. This framework must be flexible enough to model very different types of internal dynamics (e.g., part articulation, sliding of parts, and soft deformations). Furthermore, it must be able to tap substantial quantities of training data.\nRecently, video generators learned from millions of videos have been proposed as proxies of world models, i.e., models of any kind of natural phenomena, including motion. Such models may implicitly understand object dynamics; however, generating videos is insufficient: a useful model of object dynamics must be able to make predictions about the motion of given objects.\nInspired by DragAPart [4] and [5], we thus consider performing such predictions by learning a conditional video generator. This generator takes as input a single image of an object and one or more drags which specify the motion of selected physical points of the object; it then outputs a plausible video of the entire object motion consistent with the drags (Fig. 1).\nSeveral authors have already considered incorporating drag-like motion prompts in image or video generation [6\u201318]. Many such works utilize techniques like ControlNet [19] to inject motion control in a pre-trained generator. However, these models tend to respond to drags by shifting or scaling an"}, {"title": "2 Related Work", "content": "Generative models. Recent advances in generative models, largely powered by diffusion mod- els [23\u201325], have enabled photo-realistic synthesis of images [26\u201328] and videos [29\u201331, 20], and been extended to various other modalities [32, 33]. The generation is mainly controlled by a text or image prompt. Recent works have explored ways to leverage these models' prior knowledge, via either score distillation sampling [34\u201337] or fine-tuning on specialized data for downstream applications, such as multi-view images for 3D asset generation [38\u201343].\nVideo generation for motion. Attempts to model object motion often resort to pre-defined shape models, e.g., SMPL [1] for humans and SMAL [2] for quadrupeds, which are constrained to a single or only a few categories. Videos have been considered as a unified representation that can capture general object dynamics [5]. However, existing video generators pre-trained on Internet videos often suffer from incoherent or minimal motion. Researchers have considered explicitly controlling video generation with motion trajectories. Drag-A-Video [44] extends the framework proposed by DragGAN [8] to videos. This method is training-free, relying on the motion prior captured by the pre-trained video generator, which is often not strong enough to produce high-quality videos. Hence, other works focus on training-based methods, which learn drag-based control using ad-hoc training data for this task. Early efforts such as iPoke [6] and YODA [45] train variational autoencoders or diffusion models to synthesize videos with objects in motion, conditioned on sparse motion trajectories sampled from optical flow. Generative Image Dynamics [10] uses a Fourier-based motion representation suitable for natural, oscillatory dynamics such as those of trees and candles, and generates motion for these categories with a diffusion model. DragNUWA [9] and others [11, 16\u201318] fine-tune pre-trained video generators on large-scale curated datasets, enabling drag-based control in open-domain video generation. However, these methods do not allow controlling motion at the level of object parts, as their training data entangles multiple factors, including camera viewpoint and object scaling and re-positioning, making it hard to obtain a model of part-level motion. Concurrent works leverage the motion prior captured by video generative models for the related 4D generation task [46\u201349]. These models, however, lack the capability of explicit dragging control, which we tackle in this work."}, {"title": "3 Method", "content": "Given a single image y of an object and one or more drags D = {dk}Kk=1, our goal is to synthesize a video X = {x}1N sampled from the distribution\n\nX ~ P(x1, x2,..., xN|y, D)\n\nwhere N is the number of video frames. The distribution P should reflect physics and generate a part-level animation of the object that responds to the drags. To learn it, we capitalize on a pre-trained video generator, i.e., Stable Video Diffusion (SVD, Section 3.1) [20]. Such video generators are expected to acquire an implicit, general-purpose understanding of motion through their pre-training"}, {"title": "3.1 Preliminaries: Stable Video Diffusion", "content": "SVD is an image-conditioned video generator based on diffusion, implementing a denoising process in latent space. This utilizes a variational autoencoder (VAE) (E, D), where the encoder E maps the video frames to the latent space, and the decoder D reconstructs the video from the latent codes. During training, given a pair (X = x1:N, y) formed by a video and the corresponding image prompt, one first obtains the latent code as z]:N = E(x1:N), and then adds to the latter Gaussian noise\n\n\u20ac ~ N(0, I), obtaining the progressively more noised codes\n\nz1N = \u221aatz:N + \u221a1 \u2013 \u0101t\u00a21:N, t = 1,...,T.\n\nThis uses a pre-defined noising schedule \u0101o = 1, ..., \u1fb6\u03c4 = 0. The denoising network ee is trained to reverse this noising process by optimizing the objective function:\n\nmin E (21:N,y),t,e1:N~N(0,1) [||e1:N \u2212 60(z\u0142;N, t, y)||2] .\n\nHere, e uses the same U-Net architecture of VideoLDM [30], inserting temporal convolution and temporal attention modules after the spatial modules used in Stable Diffusion [27]. The image conditioning is achieved via (1) cross attention with the CLIP [50] embedding of the reference frame y; and (2) concatenating the encoded reference image E(y) channel-wise to z1: as the input of the network. After e\u0473 is trained, the model generates a video & prompted by y via iterative denoising from pure Gaussian noise z1N ~ N(0, I), followed by VAE decoding X = x1:N = D(z]:N)."}, {"title": "3.2 Adding Drag Control to Video Diffusion Models", "content": "Next, we show how to add the drags D as an additional input to the denoiser e\u0189 for motion control. We do so by introducing an encoding function for the drags D and by extending the SVD architecture to inject the resulting code into the network. The model is then fine-tuned using videos combined with corresponding drag prompts in the form of training triplets (X, y, D). We summarize the key components of the model below and refer the reader to Appendix A for more details.\nDrag encoding. Let & be the spatial grid {1, . . ., H}\u00d7 {1, ..., W} where H \u00d7 W is the resolution of a video. A drag dk is a tuple (uk, v:N) 1:N) specifying that the drag starts at location uk \u2208 \u03a9 in the reference image y and lands at locations v \u2208 \u03a9 in subsequent frames. To encode a set of K < Kmax = 5 drags D = {dk}k=1 we use the multi-resolution encoding of [4]. Each drag dk\u00b9,\nis input to a hand-crafted encoding function enc(., s) : \u1f6f\u00d1 \u21a6 R\u00d1\u00d78\u00d78\u00d7c, where s is the desired encoding resolution. The encoding function captures the state of the drag in each frame; specifically, each slice enc(dk, s)[n] encodes (1) the drag's starting location uk in the reference image, (2) its intermediate location ve in the n-th frame, and (3) its final location v in the final frame. The\ns \u00d7 s map enc(dk, s)[n] is filled with values \u20131 except in correspondence of the 3 locations, where\nwe store uk, v and \u03c5\u03ba vid respectively, utilizing c = 6 channels. Finally, we obtain the encoding\nDonc \u2208 RNXsXsXcKmax of D by concatenating the encodings of the K individual drags, filling extra\nchannels with value \u20131 if K < Kmax. The encoding function is further detailed in Appendix A.\nDrag modulation. The SVD denoiser comprises a sequence of U-Net blocks operating at different resolutions s. We inject the drag encoding Donc in each block, matching the block's resolution s. We do so via modulation using an adaptive normalization layer [21, 51\u201356]. Namely,\n\nfs fs (1+Ys) + \u03b2s,\n\nwhere fs \u2208 RB\u00d7N\u00d7s\u00d7s\u00d7C is the U-Net features of resolution s, and \u2297 denotes element-wise multiplication. Ys,\u1e9es \u2208 RBXNXs\u00d7s\u00d7C are the scale and shift terms regressed from the drag encoding Danc. We use convolutional layers to embed Donc from the dimension cKmax to the target dimension C. We empirically find that this mechanism provides better conditioning than using only a single shift term with no scaling as in [4].\nDrag tokens. In addition to conditioning the network via drag modulation, we also do so via cross- attention by exploiting SVD's cross-attention modules. These modules attend a single key-value obtained from the CLIP [50] encoding of the reference image y. Thus, they degenerate to a global bias term with no spatial awareness [57]. In contrast, we concatenate to the CLIP token additional drag tokens so that cross-attention is non-trivial. We use multi-layer perceptrons (MLPs) to regress an additional key-value pair from each drag dk. The MLPs take the origin uk and terminations vr and vid of de along with the internal diffusion features sampled at these locations, which are shown to contain semantic information [58], as inputs. Overall, the cross-attention modules have 1 + Kmax key-value pairs (1 is the original image CLIP embedding), with extra pairs set to 0 if K < Kmax."}, {"title": "3.3 Attention with the Reference Image Comes to Rescue", "content": "In preliminary experiments utilizing the Drag-a-Move [4] dataset, we noted that the generated videos tend to have cluttered/gray backgrounds. Instant3D [39] reported a similar problem when generating multiple views of a 3D object, which they addressed via careful noise initialization. VideoMV [59] and Vivid-ZOO [60] directly constructed training videos with a gray background, which might help them offset a similar problem.\nThe culprit is that SVD, which was trained on 576 \u00d7 320 videos, fails to generalize to very different resolutions. Indeed, when prompted by a 256 \u00d7 256 image, SVD cannot generate reasonable videos.\nAs a consequence, fine-tuning SVD on 256 \u00d7 256 videos (as we do for Puppet-Master) is prone to local optima, yielding sub-optimal appearance details. Importantly, we noticed that the first frame of each generated video is spared from the appearance degradation (Fig. 6), as the model learns to directly copy the reference image. Inspired by this, we propose to create a \"shortcut\" from each noised frame to the first frame with all-to-first spatial attention, which significantly mitigates, if not completely resolves, the problem."}, {"title": "4 Curating Data to Learn Part-Level Object Motion", "content": "To train our model we require a video dataset that captures the motion of objects at the level of parts. Creating such a dataset in the real world means capturing a large number of videos of moving objects while controlling for camera and background motion. This is difficult to do for many categories (e.g., animals) and unfeasible at scale. DragAPart [4] proposed to use instead renderings of synthetic 3D objects, and their corresponding part annotations, obtained from GAPartNet [64]. Unfortunately, this dataset still requires to manually annotate and animate 3D object parts semi-manually, which limits its scale. We instead turn to Objaverse [22], a large-scale 3D dataset of 800k models created by 3D artists, among which about 40k are animated. In this section, we introduce a pipeline to extract suitable training videos from these animated 3D assets, together with corresponding drags D.\nIdentifying animations. While Objaverse [22] has more than 40k assets labeled as animated, not all animations are useful for our purposes (Fig. 3). Notably, some are \u201cfake\u201d, with the objects remaining static throughout the sequence, while others feature drastic changes in the objects' positions or even"}, {"title": "5 Experiments", "content": "The final model, Puppet-Master, is trained on a combined dataset of Drag-a-Move [4] and Objaverse- Animation-HQ (Section 4). We evaluate the performance of the final checkpoint on multiple bench- marks, including the test split of Drag-a-Move and real-world cases from Human3.6M [66], Amazon- Berkeley Objects [67], Fauna Dataset [68, 69], and CC-licensed web images in a zero-shot manner, demonstrating qualitative and quantitative improvements over prior works and excellent general- ization to real cases (Section 5.1). The design choices that led to Puppet-Master are ablated and discussed further in Section 5.2. In Section 5.3, we show the effectiveness of our data curation strategy (Section 4). We refer the reader to Appendix C for the implementation details."}, {"title": "5.1 Main Results", "content": "Quantitative comparison. We compare Puppet-Master to DragNUWA [9] and DragAnything [16], both of which are trained on real-world videos to support open-domain motion control, on the part-level motion-conditioned video synthesis task in Table 1. On the in-domain test set (i.e., Drag- a-Move), Puppet-Master outperforms both methods on all standard metrics, including pixel-level PSNR, patch-level SSIM, and feature-level LPIPS and FVD, by a significant margin.\nAdditionally, to better demonstrate our model's superiority in generating part-level object dynamics, we introduce a flow-based metric dubbed flow error. Specifically, we first track points on the object throughout the generated and ground-truth videos using CoTracker [70], and then compute flow error"}, {"title": "5.2 Ablations", "content": "We conduct several ablation studies to analyze the introduced components of Puppet-Master. For each design choice, we train a model using the training split of the Drag-a-Move [4] dataset with batch size 8 for 30, 000 iterations and evaluate on 100 videos from its test split without classifier-free guidance [73]. Results are shown in Table 2 and Fig. 6 and discussed in detail next."}, {"title": "5.3 Less is More: Data Curation Helps at Scale", "content": "To verify that our data curation strategy from Section 4 is effective, we compare two models trained on Objaverse-Animation and Objaverse-Animation-HQ respectively under the same hyper-parameter setting. The training dynamics are visualized in Fig. 7. The optimization collapses towards 7k iterations when the model is trained on a less curated dataset, resulting in much lower-quality video samples (Table 3). This suggests that the data's quality matters more than quantity at scale."}, {"title": "6 Conclusion", "content": "We have introduced Puppet-Master, a model that can synthesize nuanced part-level motion in the form of a video, conditioned on sparse motion trajectories or drags. Fine-tuned from a large-scale pre-trained video generator on a carefully curated synthetic part-level motion dataset Objaverse- Animation-HQ, which we have contributed, our model demonstrates excellent zero-shot generalization to real-world cases. Thanks to the proposed adaptive layer normalization modules, the cross-attention modules with drag tokens and, perhaps more importantly, the all-to-first spatial attention modules, we have shown superior results compared to previous works on multiple benchmarks. Ablation studies verify the importance of the various components that contributed to this improvement."}, {"title": "A Additional Details of the Drag Encoding", "content": "Here, we give a formal definition of enc(., s) introduced in Section 3.2. Recall that enc(., s) en- codes each drag dk := (uk, v:N) into an embedding of shape N \u00d7 s \u00d7 s \u00d7 6. For each frame n, the first, middle, and last two channels (of the c = 6 in total) encode the spatial location of uk, vr and v respectively. Formally, enc(dk, s)[n, :, :, : 2] is a tensor of all negative ones except for enc(dk, s)[n, ,s)[n, [], [],:2] = (\uc55e - [\u7826\u300d,W - [W]) where uk = (h,w) \u0395\u03a9\n{1,\uff65\uff65\uff65, H} \u00d7 {1,\u2026, W}. The other 4 channels are defined similarly with uk replaced by v and\n\u03c5."}, {"title": "B Additional Details of Data Curation", "content": "We use the categorization provided by [75] and exclude the 3D models classified as 'Poor-Quality' as a pre-filtering step prior to our proposed filtering pipelines (Section 4).\nWhen using GPT-4V to filter Objaverse-Animation into Objaverse-Animation-HQ, we design the following prompt to cover a wide range of cases to be excluded:\nSystem: You are a 3D artist, and now you are being shown some animation videos depicting\nan animated 3D asset. You are asked to filter out some animations.\nYou should filter out the animations that:\n1) have trivial or no motion, i.e., the object is simply scaling, rotating, or moving as a whole\nwithout part-level dynamics;\nor 2) depict a scene and only a small component in the scene is moving;\nor 3) have motion that is imaginary, i.e., the motion is not the usual way of how the object\nmoves and it's hard for humans to anticipate;\nor 4) have very large global motion so that the object exits the frame partially or fully in one\nof the frames;\nor 5) have changes in object color that are not due to lighting changes;\nor 6) have motion that causes different parts of the same object to disconnect, overlap in an\nunnatural way, or disappear;\nor 7) have motion that is very chaotic, for example objects exploding or bursting apart.\nUser: For the following animation (as frames of a video), frame1, frame2, frame3, frame4,\ntell me, in a single word 'Yes' or 'No', whether the video should be filtered out or not.\nThe cost of GPT-4V data filtering is estimated to be $500."}, {"title": "C Additional Experiment Details", "content": "Data. Our final model is fine-tuned on the combined dataset of Drag-a-Move [4] and Objaverse- Animation-HQ (Section 4). During training, we balance over various types of part-level dy- namics to control the data distribution. We achieve this by leveraging the categorization pro- vided by [75] and sampling individual data points with the following hand-crafted distribu- tion: p(Drag-a-Move) = 0.3, p(Objaverse-Animation-HQ, category 'Human-Shape') = 0.25,\np(Objaverse-Animation-HQ, category \u2018Animals\u2019) = 0.25, p(Objaverse-Animation-HQ, category\n\u2018Daily-Used\u2019) = 0.05, p(Objaverse-Animation-HQ, other categories) = 0.15.\nArchitecture. We zero-initialize the final convolutional layer of each adaptive normalization module before fine-tuning. With our introduced modules, the parameter count is pumped to 1.68B from the original 1.5B SVD.\nTraining. We fine-tune the base SVD on videos of 256 \u00d7 256 resolution and N = 14 frames with batch size 64 for 12, 500 iterations. We adopt SVD's continuous-time noise scheduler, shifting the noise distribution towards more noise with logo ~ N(0.7, 1.62), where o is the continuous noise level following the presentation in [20]. The training takes roughly 10 days on a single Nvidia A6000 GPU where we accumulate gradient for 64 steps. We enable classifier-free guidance (CFG) [73] by"}]}