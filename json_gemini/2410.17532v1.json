{"title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact", "authors": ["Junhua Liu", "Bin Fu"], "abstract": "Multilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Multilingual large language models (MLLMs) aim to address linguistic diversity in Artificial Intelligence (AI), acknowledging that over 7,000 languages are spoken worldwide [1], with hundreds of them having millions of speakers.\nThanks to the recent growth with the advent of large language models (LLMs), the Natural Language Processing (NLP) research community has been enabling more advanced applications in text understanding and generation. However, much of this progress has been concentrated on high-resource languages, particularly English.\nDeveloping truly multilingual systems remains a significant challenge, as many languages are underrepresented in current Al technologies. Research and development in this area are critical to ensuring equitable access to technological advancements for speakers of all languages.\nThis study addresses the lack of multilingual support and linguistic inclusivity in state-of-the-art LLMs. We present emerging trends and promising directions in the development of MLLMs that can better address linguistic diversity and representation challenges.\nOur findings contrast with previous efforts that have mainly focused on a few dominant languages. We offer new insights into technical strategies that can support more equitable language processing across diverse linguistic groups.\nThis work highlights the importance of addressing linguistic inclusion in AI and suggests a path toward more comprehensive and inclusive NLP systems. Understanding these advancements will have broad implications for global AI accessibility and for bridging language-based technological divides [2]."}, {"title": "A. Contributions", "content": "Compared to existing MLLM surveys [3], [4], [5], this work emphasizes the practical aspects of MLLMs from development to production, based on empirical experiences. Specifically, the three primary contributions are summarised as follows:\n1) MLLM Production Framework: We present a comprehensive end-to-end framework for developing and deploying MLLMs, bridging the gap between theoretical understanding and practical implementation. Unlike previous surveys that focus on specific aspects, our work provides an integrated view of the complete MLLM life cycle - from data collection and pre-processing through model development to industrial applications. This holistic approach offers concrete guidance for both researchers and practitioners working to develop or improve multilingual models.\n2) Optimization Strategies: We provide detailed, practical optimization strategies for multilingual models, using Llama2 as a concrete case study. We analyze specific techniques for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, optimization of tokenization strategies, and effective sampling methods. This practical focus fills a crucial gap in existing literature, offering actionable insights for improving MLLM performance across diverse languages.\n3) Interdisciplinary Considerations: We take an interdisciplinary approach that integrates technical, linguistic, and"}, {"title": "II. OVERVIEW OF MULTILINGUAL RESEARCH", "content": "In today's global landscape, there are over 7,000 languages in use worldwide, with more than 400 languages having over one million speakers each, and approximately 1,200 languages having over 100,000 speakers. This section will focus on examining the critical questions of whether multilingual research and language preservation are necessary, and whether it is essential to ensure that speakers of all languages can equitably benefit from the technological advances brought by large language models. Additionally, this section will provide readers with a concise overview of the development trends and technical directions in MLLMs."}, {"title": "A. Multilingual Research", "content": "Multilingual research in Natural Language Processing (NLP) is crucial for several reasons:\n\u2022 Linguistic Diversity: With over 7,000 languages spoken worldwide [1], focusing solely on high-resource languages like English excludes a vast majority of the world's linguistic diversity.\n\u2022 Digital Inclusion: As shown in Table I, there's a significant disparity in internet penetration and usage across languages. Multilingual NLP can help bridge this digital divide and provide equal access to information and technology.\n\u2022 Cultural Preservation: Many low-resource languages are at risk of digital extinction. Developing NLP tools for these languages can aid in their preservation and revitalization.\n\u2022 Economic Opportunity: Enabling NLP capabilities in more languages can open up new markets and economic opportunities for speakers of those languages.\n\u2022 Fairness and Representation: Focusing on multiple languages helps reduce bias in AI systems and ensures fair representation of diverse linguistic communities.\nDespite the clear importance, developing truly multilingual NLP systems remains challenging due to resource constraints, linguistic complexity, and technical limitations. Addressing these challenges is essential for creating more inclusive and globally accessible AI technologies.\nWe will discuss the importance of multilingual research from four aspects:\n1) Social Aspect: The language people use determines the education, knowledge, and social networks they can access. Although the internet is open, there still exists a Digital Language Divide between mainstream languages (Chinese, English, and other major Western languages) and other languages. As shown in Table I, only a few languages appear frequently on the internet, which greatly increases the barrier for speakers of minority languages to access information.\nThe existence of the digital language divide affects the development of NLP technology at various levels. For example, most social apps (like WeChat) currently use informal input forms (such as abbreviations or slang) for commonly used languages. However, the digital language divide means that speakers of non-mainstream languages cannot get good keyboard input support and spelling correction services. This sometimes leads to bias and discrimination in NLP algorithms"}, {"title": "2) Linguistic Aspect:", "content": "When training large language models, we always hope to train a language-independent, more general large model. However, most of the time, limited by corpus resources, the models we train are only proficient in single-language tasks, such as Chinese or English tasks. Table II lists single-language BERT models for different languages and language families. Single-language pre-trained large models in Chinese, English, and some high-resource languages cannot represent other languages in the world.\nTaking Chinese as an example, it belongs to the Sino-Tibetan language family and is a morphologically poor language that focuses more on expressing meaning through syntax, i.e., emphasizing word order (such as subject-verb-object order). It needs to use different words rather than just word changes to express tense, singular/plural, and gender [16]. For example, in Manipuri (Meitei), te/de at the end of a sentence is used to express negation: thak-ke translates to \"I drank,\" while thak-de translates to \"I did not drink\" [17].\nApart from these lexical and syntactic differences, we can also look at the different characteristics of various languages from a more comprehensive perspective. The World Atlas of Language Structure [18] categorizes 192 features, with subject-verb-object (SVO) order being one of them. Each feature contains an average of 5.93 categories. For example, the SVO order feature can have different categories such as SOV, SVO, VSO, etc. Research has found that 48% of the features only appear in low-resource languages, which are groups 0-2 in Figure 1 of the World Atlas of Language Structure, and do not appear in groups 3-5 [19]. If we don't use these features when training or fine-tuning large models, we may miss some valuable information for model generalization. Aiming to train better MLLMs can help us better understand"}, {"title": "3) Cultural and Ethical Aspect:", "content": "The data we use to train large models not only reflects the characteristics of the corresponding language but also reveals cultural identity and common sense through the model's responses. However, different cultures have different common sense, such as alcohol culture not appearing in Arab countries. Some smaller large models, such as 13B or 7B models, if only fine-tuned for dialogue in one language, such as Chinese, will give reasonable answers in Chinese dialogue, but will exhibit hallucination phenomena common to large models for other languages, giving answers that are inconsistent with facts or violate ethical norms.\nLarge models are now being used for various complex generation tasks rather than simple classification tasks. Therefore, how to use multilingual resources to train large models that conform to the cultural and ethical norms of speakers of all languages in the world, and promote the development of Responsible AI (RAI) in different languages, has become an important topic."}, {"title": "4) Model Aspect:", "content": "When we train large models, they develop inductive biases towards languages that occupy a larger proportion of the training corpus, even if we don't explicitly encode language information in neural networks and only use N-grams to build language models. Research shows that their performance also significantly decreases on morphologically rich languages [23].\nSimilarly, Transformer-based models also ignore the complexity of morphologically rich languages [24]: subword tokenizers perform poorly on languages with reduplication [25]; BPE algorithms cannot align morphological information well; although large models have shown zero-shot learning ability in monolingual and cross-lingual tasks, their text generation and classification performance decline in low-resource languages"}, {"title": "B. Challenges in Multilingual NLP", "content": "After discussing the necessity of multilingual research, we look at the current multilingual research communities, challenges, and research directions."}, {"title": "1) Current Trends:", "content": "There are now many research institutions dedicated to multilingual research, covering both languages that cover large populations like Chinese, Japanese, Turkish, and Hindi, as well as languages that cover small populations like Irish. In recent years, some NLP communities have also appeared that specifically research under-represented languages or language families, and more NLP communities focus on regional languages. For example, Masakhane researches African languages, AmericasNLP researches Native American languages, and IndoNLP researches Indonesian languages. At the same time, there are also dedicated long-term Workshops and Events for non-English research, such as the regular conferences organized by the Chinese Information Processing Society of China (such as CCL, ACL), and special interest groups set up for linguistic typology (such as SIGTYP, AfricaNLP, ArabicNLP, and ComputeEL).\nMeanwhile, some communities focus on broader languages and work, such as ML Collective and Big Science. Big Science is a community dedicated to serving multilingual AI, which released the BLOOM model [28] and performed multi-task prompt fine-tuning on BLOOM and mT5 [29], two MLLMs, to produce BLOOMZ and mTO with strong cross-lingual capabilities [27].\nTo highlight the importance of multilingualism, ACL not only established SIGTYP but also set up a Special Theme Track in 2022, aiming to make scientific papers accessible to more people through the following efforts:\n\u2022 Translating the ACL Anthology into 60 languages.\n\u2022 Dubbing and adding subtitles to all conferences in 10 languages.\n\u2022 Translating a comprehensive and standard set of NLP terminology into 60 languages.\nThese resources and terminology lists can allow people from different regions to discuss NLP technology in their own languages."}, {"title": "2) Challenges:", "content": "Here we introduce two typical multilingual development challenges:\nCurse of Multilinguality. Why can current MLLMs cover at most over 100 languages? Apart from resource issues, another reason is the Curse of Multilinguality [30]. Similar to training large models on multiple tasks, the more languages used to train large models, the harder it becomes for the model to learn the representation information of each language due to the limited model capacity (a few hundred MB). The emergence of MLLMs has broken this bottleneck, with parameter scales of over 10B allowing large models to better learn the representation information of each language [31].\nLow-resource Problem. A primary issue in the development of MLLMs is that the available corpus resources show a long-tail distribution. The high-resource corpora we often talk about are heavily biased towards Indo-European languages represented by English and Chinese, Japanese, Korean, etc. These head languages have enormous quantities in both labeled and unlabeled data. We have divided the world's 7000+ languages into 6 categories from 0 to 5 based on two dimensions [19]: labeled data and unlabeled data, respectively representing the difficulty for large models to"}, {"title": "Category 0 (The Left-Behinds):", "content": "NLP technology has consistently ignored Category 0 languages (as shown in Class 0 in Table III. Due to extremely limited corpus resources, these languages will gradually become historical artifacts, making it difficult to connect them with digital transformation. Even using unsupervised learning methods would only make things worse, as there is essentially no unlabeled data available."}, {"title": "Category 1 (The Scraping-Bys):", "content": "Category 1 languages (such as Greenlandic) have some unlabeled data of certain scale, making it possible for them to receive more attention from researchers in multilingual research in the coming years. However, this requires researchers to systematically and continuously promote tasks involving these languages, get more people to pay attention to them, and be willing to collect more labeled data for them. Currently, these languages have almost zero labeled data."}, {"title": "Category 2 (The Hopefuls):", "content": "Category 2 languages (such as Irish) are experiencing darkness before dawn in the NLP field but continue to make progress. These languages have accumulated some labeled data, albeit in small quantities, indicating that there is a group of active researchers working on digital transformation of these languages. They are expected to develop some promising NLP tools for these languages in the coming years."}, {"title": "Category 3 (The Rising Stars):", "content": "From Category 3 onwards, we're basically talking about what we commonly refer to as high-resource languages. Unsupervised learning has greatly accelerated the influence of these languages (such as Indonesian) in the NLP field. Because these languages have more users on the internet, there are some flourishing NLP community researchers dedicated to studying these languages, although their research is also affected by insufficient labeled data. These researchers should leverage large model pre-training and parameter-efficient fine-tuning techniques [32] to compensate for the lack of labeled data."}, {"title": "Category 4 (The Underdogs):", "content": "Category 4 languages (such as Vietnamese) are like sparks in the NLP field, with huge development potential. They have accumulated a large amount of unlabeled data, and in terms of labeled data, they are only one order of magnitude behind Category 5. Experienced community researchers are dedicated to studying these languages. These languages have great potential to become Category 5 languages and are among those that can enjoy the superiority of digital transformation."}, {"title": "Category 5 (The Winners):", "content": "Category 5 languages (such as Chinese and English) have developed rapidly in the NLP field and have always been in a leading position, with research time longer than languages in previous categories. Because these languages have a dominant position on the network, numerous enterprises and government institutions invest in their NLP resources and technological development. They are absolutely high-resource languages, and their users enjoy the benefits brought by the most advanced achievements and technological breakthroughs in NLP. We selected Category 5 languages and placed them in Table III. From Table III, we can see that 88.38% of languages involving over 1 billion people have not enjoyed the convenience brought by NLP technological development."}, {"title": "3) Multilingual Models:", "content": "The classification of languages also determines the research directions for different languages. For example, for Category 3 and Category 4 languages, due to the lack of sufficient labeled data, unsupervised learning methods can be used to compensate. In recent years, with the development of large models, related research mainly uses pre-training technology to learn language features from unlabeled data, achieving zero-shot or few-shot learning capabilities for target languages. Currently, there are related research results published in academia, mainly focusing on Southeast Asian languages such as Indonesian [34], Vietnamese [35], and Thai [36].\nOf course, relying solely on unlabeled data to improve the performance of Category 3 and Category 4 language tasks is not enough. A natural idea is to transfer language features from high-resource languages to low-resource languages, known as cross-lingual transfer learning and multilingual learning. The former refers to transfer from one source language to one target language (one-to-one), while the latter refers to transfer from multiple source languages to one target language. After"}, {"title": "1) Constructing Prompts for ICL:", "content": "This technical direction uses prompts as input but does not fine-tune model parameters. On the multilingual side, it roughly divides into cross-lingual prompts and cross-lingual chain-of-thought prompts. Both use English as the main language for prompts but use target languages for test samples. These methods have been proven to achieve better results and have similar effects to translating test samples into English [37]."}, {"title": "2) Parameter Fine-tuning Using Multilingual Prompts:", "content": "Experimental results show that using English as prompts for fine-tuning under multilingual tasks can achieve State-of-the-Art (SOTA) results on zero-shot learning tasks in both English and non-English task sets. At the same time, using machine-translated multilingual prompts for fine-tuning can achieve better results in some languages than prompts fine-tuned with human translation [27]."}, {"title": "III. MULTILINGUAL LARGE LANGUAGE MODELS", "content": "Data resources are as important in this era of large language models (LLMs) as oil and coal were in the industrial age. This section will summarize the mainstream corpora used in training MLLMs, and how to process and refine these corpora to better help researchers improve the effectiveness of MLLMs."}, {"title": "A. Pre-training Resources", "content": "Unlike previous pre-training model technologies, large language models require massive training corpora to learn more comprehensive knowledge and content. Therefore, more and more open-source training corpora are being used to train large models. In this section, we briefly categorize the currently widely used corpus resources according to their content type into books resources, web resources, Reddit resources, Wikipedia, codes, and others [38], as shown in Table IV."}, {"title": "1) Book Resources:", "content": "BookCorpus [39] is a dataset frequently used by previous pre-training models (such as GPT/GPT-2), including more than 11,000 books covering a wide range of categories (such as novels and biographies). Currently, a larger scale book resource is Project Gutenberg [40], which contains over 70,000 books and is also used to train Llama and Llama2 models."}, {"title": "2) Web Resources:", "content": "CommonCrawl, as one of the largest open-source web crawl databases (PB-level), is widely used in the pre-training of large language models. Due to its rich corpus resources, current language models only use subsets of data from certain time periods for training. Web crawl data contains a lot of noise and low-quality data, so it needs to be cleaned and pre-processed before use. Currently, there are several cleaned multilingual datasets available for selection: C4 [41], OSCAR [42], CCAligned [43]."}, {"title": "3) Reddit Resources:", "content": "Reddit is a social networking platform similar to an online forum where users can post and answer topics they want to discuss, and upvote or downvote answers. Those high-voted resources are valuable. OpenWebtext is a dataset formed by crawling this data. Another dataset is PushShift.io [44], which is a real-time updated dataset that supports users to search, summarize, and other operations in the entire dataset, making it convenient for users to use and process the data."}, {"title": "4) Wikipedia:", "content": "Wikipedia has a large number of high-quality articles in different domains and covers multiple languages. The English version of Wikipedia has been used by Llama, GPT-3, and LaMDA, while the multilingual version has been used by mBERT and XLM-100. However, because the number of documents in the multilingual version is much less than the CommonCrawl dataset, it has not been widely used by MLLMs. WikiMatrix [45] is a parallel corpus extracted from Wikipedia data, containing 135 million parallel corpora, covering 1620 language pairs and 85 languages, mainly used for machine translation."}, {"title": "5) Code:", "content": "For code datasets, existing work mainly focuses on crawling code with open-source licenses from the internet. Code mainly has two crawling sources: the first is public code repositories, such as GitHub; the second is code-related Q&A platforms, such as Stack Overflow, CSDN. Google has open-sourced BigQuery [46], which includes a certain amount of open-source code in different programming languages. As a code large model, CodeGen [47] used BigQuery for training, and the multilingual version of BigQuery was also used to"}, {"title": "7) Quality of Pre-training Resources:", "content": "Data quality is key to training MLLMs with excellent performance. Robust MLLMS can perform better on cross-lingual downstream tasks [48], [49]. The multilingual pre-training datasets introduced above are mostly automatically mined from the internet, and their data quality cannot be well guaranteed. Therefore, it is necessary to evaluate these corpora mined from the internet using different dimensions.\nIn the ACL 2023 Tutorial on MLLMS, Microsoft's researchers proposed to evaluate the quality of multilingual pre-training datasets from four aspects: multilingual distribution, data quality, source, and governance. In terms of multilingual distribution, researchers used the CommonCrawl dataset as an example, pointing out that although it includes more than 100 languages, 57 languages account for less than 0.001%, so"}, {"title": "B. Evaluation Tasks", "content": "The datasets introduced in Section III-A mainly focus on unsupervised data. This section will introduce benchmarks for evaluating MLLMs on multilingual supervised datasets. The tasks in the benchmark mainly include text classification tasks (single sentence or sentence pair), QA tasks (Text-span Prediction), sequence labeling tasks, and text generation tasks, as shown in Figure 2."}, {"title": "1) Datasets:", "content": "Constructing robust and comprehensive evaluation tasks can help us better understand the effectiveness of large models. This type of evaluation is a very active research area in English, such as the GLUE and the more difficult SuperGLUE evaluation dataset benchmark, as well as the recently established multilingual evaluation datasets"}, {"title": "2) Methods:", "content": "Traditional model evaluation methods (Evaluation Methodologies) use supervised training corpora to fine-tune pre-trained models, with the pipeline being pre-training + fine-tuning. MLLMs have inherent zero-shot and few-shot learning capabilities, and their evaluation methods can be divided into two categories: one uses the original evaluation method for task-related fine-tuning, and the other is based on prompt in-context learning (ICL), with the pipeline changing to pre-training + prompting + prediction. Task-related fine-tuning requires using training corpora to update model parameters, while ICL does not need to update model parameters, only requiring the design of different prompts for the model to return corresponding results. The evaluation methods for MLLMs are shown in Figure 3."}, {"title": "Task-Specific Fine-tuning:", "content": "\u2022 Zero-Shot Cross Lingual Transfer: First perform task-related fine-tuning on one language, then evaluate using the test set of another language.\n\u2022 Few-Shot Cross Lingual Transfer: First fine-tune parameters on English and a small amount of target language, then evaluate on the target language test set.\n\u2022 Monolingual Fine-tuning: Only fine-tune parameters on the full target language.\n\u2022 Translate-Train: Use machine-translated target language for parameter fine-tuning."}, {"title": "Prompt-based In-Context Learning:", "content": "This evaluation method hopes to construct appropriate prompts to activate the capabilities of large models to help people solve current tasks. It mainly consists of different dimensions: input prompts, task-related few-shot examples generalized through templates, and answers. Table V shows prompts and few-shot data templates for different tasks.\n\u2022 Monolingual Prompting: Prompts, few-shot inputs, test data, and answers are all in the target language.\n\u2022 Cross Lingual Prompting: Prompts and few-shot inputs are in the source language, test data and answers are in the target language.\n\u2022 Chain-of-Thought Prompting: For problems that focus on multi-step logical reasoning, adding \"Step-by-Step\" in the prompt helps improve the accuracy of large model answers. In this type of prompting technique, prompts, few-shot inputs, test data, and responses are all in the target language.\n\u2022 Cross-Thought Prompting: This technique takes advantage of large models' proficiency in English. The input uses the target language, first letting the multilingual large model rephrase the question in English in the prompt, then using English for CoT reasoning, and the final response is also in English. Experiments have shown that this method performs better in cross-lingual reasoning than using only English input or translating the target language into English."}, {"title": "IV. OPTIMIZING MLLMS", "content": "After introducing the pre-training data and evaluation tasks for MLLMs, this section will discuss how to pre-process the pre-training data and tokenize it, as well as the structure and effectiveness of various MLLMs."}, {"title": "A. Data Pre-processing Flow", "content": "The pre-processing of pre-training data for MLLMs is not significantly different from that for monolingual large models. As shown in Figure 4, they all go through steps such as data collection, initial data cleaning (language detection), document de-duplication, and quality filtering (privacy filtering). The only difference is that during the initial data cleaning process, appropriate thresholds need to be set to identify the languages we need and retain a certain proportion of multilingual documents for model training."}, {"title": "B. Tokenizer", "content": "After the corpus has gone through data pre-processing, it needs to be processed by a tokenizer before it can be converted into a data format directly usable by MLLMs. Below, we will discuss commonly used tokenizers and their potential issues in multilingual scenarios."}, {"title": "1) Tokenizer Selection:", "content": "There are mainly two types of mainstream tokenizers, one is subword-based tokenizers, of which the following two are representative:\nBPE algorithm [54]: learining learns how to merge the two most common and consecutive tokens in the training dictionary into a new token and add it to the dictionary until the dictionary capacity is met. This method is also used by GPT, BLOOM, and LLaMA series.\nSentencePiece [55]: an open-source code library that implements tokenization algorithms, such as BPE and Unigram Tokenization, and also non-subword tokenizers, such as character-based and letter-based tokenizers. Large models such as XGLM, mT5, and PaLM series all use this library as a tokenizer.\nThe other type is byte-based tokenizers, BBPE [56]. When our dictionary capacity is limited, tokenizing with bytes as the smallest token can encode all possible sequence information and has good fault tolerance for spelling. Research has trained a multilingual large model mT5 without subwords using UTF-8 encoding, showing good results on multilingual test sets. This method can be used to train large language models without subwords (Latin-based languages) because UTF-8 encoding of Latin-based languages will be smaller"}, {"title": "2) Potential Issues with Tokenizers:", "content": "Compared to monolingual LMs, MLLMs pay more attention to the quality of tokenizers because MLLMs need to use tokens to represent more language sequences with a limited vocabulary. Ahia et. al. [57] has conducted quantitative analysis from the perspective of subword fertility. The definition and calculation method of subword fertility are shown in Figure 5a. Fertility is mainly used to calculate the average length of a natural word being split into subwords, with the minimum fertility being 1, indicating that each natural word is a subword. As can be seen from Figure 5b, the fertility of the multilingual large model mBERT in English (EN), Chinese (ZH), Indonesian (ID), and Japanese (JA) is basically the same as that of monolingual large models, but it is significantly higher than that of their respective monolingual large models in Korean (KO), Arabic (AR), Finnish (FI), Russian (RU), and Turkish (TR). This is"}, {"title": "C. Training Data Format and Sampling Strategy", "content": "After the corpus has gone through tokenizer processing, before it can be fed into MLLMs, the models need to con-"}, {"title": "General Trend of Performance Increase (within a model class type)", "content": "Fig. 7: Comparison of English-centric parallel sentence training data formats and models"}, {"title": "Fig. 8: Training objectives of different language models", "content": "sider how to organize multilingual samples and the sampling strategy for different language samples. This section will discuss monolingual samples, English-centric parallel sentence pairs, and multilingual parallel sentence pairs. In general, using multilingual parallel sentence pairs for training will yield better results compared to using monolingual samples or English-centric parallel sentence pair samples. The reason for sampling languages is to better balance the impact of model scale and language scale on the effectiveness of MLLMs, avoiding the curse of multilinguality phenomenon mentioned in Section 1."}, {"title": "1) Monolingual Samples:", "content": "Monolingual sampling refers to the strategy of treating the training data from each language as an independent pre-training task sample during the training process. Different sampling strategies are employed to sample from different languages in the corpus to form a batch for alternative training. This is the training data format used by models like GPT-3/LLaMA series/BLOOM. The reason for sampling across languages is that there are three multilingual sampling strategies."}, {"title": "Fig. 9: Different Structures of Language Models", "content": "the output sequence uses an autoregressive method for one-by-one prediction, and shares parameters in both encoding and decoding processes. Compared with the previously mentioned encoder-decoder structure, this structure delegates the encoding work of the encoder to the decoder, saving 2 times the parameters and memory while ensuring similar computational complexity. Generally, we won't use this structure to train MLLMs from scratch, but will first train a decoder model, then transform the task into a corpus format that matches the input format of this model, and perform secondary training on the model to accelerate convergence speed. Common prefix language models include Llama2-chat, GLM-130B, and U-PaLM."}, {"title": "Fig. 10: Proportion of each language in Llama2's training corpus", "content": "This section will use Llama2 as an example to comprehensively explain how to use Llama2 to train a large model proficient in multilingual tasks from three aspects: pre-training corpus preparation, training data sampling, and training objectives and model fine-tuning, based on the technical directions introduced earlier."}, {"title": "Fig. 11: Proportion of main languages in common MLLMs", "content": "Monolingual training corpus enhancement: Generally, if we want to improve Llama2's performance on tasks in"}, {"title": "Fig. 12: Comparison of perplexity between common MLLMs in PolyLM technical report and XLM-R", "content": "languages other than English, we need to collect corpora for specific languages and retrain Llama2 (such as Chinese Llama2). In addition to collecting corpora equivalent to English, we can also use the collected corpora to generate new tokens using encoding algorithms like BPE, supplement them to the original Llama2 dictionary, and then start model retraining. This can improve the model's performance on tasks in corresponding languages."}, {"title": "Multilingual training corpus enhancement:", "content": "The model enhanced with monolingual training corpus mentioned above, although it can improve capabilities in specific languages, still lacks multilingual capabilities and cannot solve the model's bias towards high-resource languages. A typical application of multilingual capability is machine translation. When we want to train a multilingual large model capable of various language translations, we still need to collect sufficient multilingual training corpora. Currently public multilingual datasets such as mC4, ROOT, and OPUS-100 [70] can all be used as enhancement corpora for large models.\nAs mentioned in Section IV-C3, large models trained with multilingual parallel sentence pairs generally perform better on multilingual downstream tasks compared to models trained only with monolingual or English-centric parallel sentence pairs. Therefore, when constructing multilingual training corpora, we recommend building multilingual parallel corpora and utilizing data augmentation to balance the number of mutual translation pairs, making them as similar as possible. For example, the amount of English-to-Chinese translation corpus should be similar to the Chinese-to-English translation corpus.\nAfter completing the enhancement of training corpora, if we want to continue using new token enhancement or training our own tokenizer, it is recommended to use XLM-R's tokenizer as a benchmark and evaluate our tokenizer's fertility rate across different languages. XLM-R's dictionary deliberately includes support for low-resource language tokens, effectively addressing the high-resource language bias problem during pre-training model training. Therefore, if your tokenizer's fertility rate for different languages is similar to or lower than XLM-R's, it indicates that your model can effectively parse these language features and improve efficiency during inference and training."}, {"title": "Fig. 13: Zero-shot multilingual task performance chart (all prompts in English)", "content": "all context information in the Transformer layer."}, {"title": "4) Fine-tuning MLLMs:", "content": "When we have clarified our goals (retraining or fine-tuning goals)", "retraining": "Generally", "72": "methods are usually adopted for multilingual large model pre-training. This approach aims to transfer some general knowledge from high-resource languages to low-resource languages while retaining the model's excellent capabilities in high-resource languages. The general approach is to divide the training into two stages: the first stage focuses on high-resource training corpora, trying to let the model learn more general language knowledge; the second stage expands the proportion of low-resource training corpora, hoping the model can enhance multilingual capabilities. For instance, past work proposed curriculum learning"}]}