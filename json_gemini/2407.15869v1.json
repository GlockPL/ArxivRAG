{"title": "Long Input Sequence Network for Long Time Series Forecasting", "authors": ["Chao Ma", "Yikai Hou", "Xiang Li", "Yinggang Sun", "Haining Yu"], "abstract": "Short fixed-length inputs are the main bottleneck of deep learning methods in long time-series forecasting tasks. Prolonging input length causes overfitting, rapidly deteriorating accuracy. Our research indicates that the overfitting is a combination reaction of the multi-scale pattern coupling in time series and the fixed focusing scale of current models. First, we find that the patterns exhibited by a time series across various scales are reflective of its multi-periodic nature, where each scale corresponds to specific period length. Second, We find that the token size predominantly dictates model behavior, as it determines the scale at which the model focuses and the context size it can accommodate. Our idea is to decouple the multi-scale temporal patterns of time series and to model each pattern with its corresponding period length as token size. We introduced a novel series-decomposition module(MPSD), and a Multi-Token Pattern Recognition neural network(MTPR), enabling the model to handle inputs up to 10\u00d7 longer. Sufficient context enhances performance(38% maximum precision improvement), and the decoupling approach offers Low complexity(0.22\u00d7 cost) and high interpretability.", "sections": [{"title": "1 Introduction", "content": "Accurate long-term time series forecasting is essential for facilitating effective long-term planning and optimizing returns across diverse domains, such as finance [17], energy [7, 25], transportation[20]. Recent advancements in deep neural network methods show promise, particularly in leveraging time series properties to design more suitable neural network architectures. These properties include Multi-Periodicity [23], Seasonality [24, 29], Multi-Scale [21, 1], Segmented[27, 13] and non-Stationarity[14, 12, 8].\nHowever, this advancement has encountered a bottleneck: the best-performing context window size(input sequence length) of these methods is often significantly shorter compared to the prediction length[22]. We tested several current mainstream methods for prolonging input sequences on the ETTm2 dataset, as illustrated in Figure 1(left). These methods fail to benefit from long context window and exhibit performance degradation. According to observations during the experiments, the primary reason for the performance decline is overfitting caused by long context window. This results in insufficient context for the model to effectively learn and generalize.\nOur research indicates that the overfitting are the combined result of multi-scale pattern coupling within sequences and the fixed focusing scale of the model. In sequence perspective, distinct patterns manifest at varying scales. Two relevant sequence properties are multi-scale variations and multi-periodicity. Previous work[21, 23] have leveraged these properties by incorporating down-sampling and multi-period information, thereby enhanced representation capability. However, they failed to decouple the overlapping multi-scale patterns, which led to the small-scale and short-period sequence in the enhanced representation constitutes a short board for model to handle larger context. In model perspective, Transformer-based approaches operate on each segments of inputs, i.e. tokens[2, 3, 26]."}, {"title": "2 Related Work", "content": "Current mainstream methods, mostly based on MLP and Transformer, have addressed the issue of error accumulation inherent in autoregressive prediction approaches, particularly prevalent in recurrent neural networks[18, 16]. Early Transformer variants primarily focused on reducing attention complexity, exemplified by approaches like Autoformer[24] and FEDformer[29], Informer[28] which introduced Fourier analysis and sequence decomposition methods into time series forecasting. The latest Transformer method, PatchTST[13], adopts strategies such as channel-wise independence and Patch embedding methods, while employing a simple linear head as a predictor or decoder, effectively enhancing prediction accuracy. Analogous to the Vision Transformer (ViT[3]) paradigm in computer vision, PatchTST delineates the Transformer paradigm in the domain of time series prediction. MLP-based techniques often leverage inductive biases derived from time series analysis, as seen in methods like N-BEATS[15] and N-HITS[1], which employ ensemble methods and hierarchical pooling techniques. Recent advancements such as Non-stationary [12], RevIN[8], TimesNet[23], TSMixer[4] and TimeMixer[21] utilize the periodicity, multivariate and multiscale characteristics of time series, respectively, to increase expressiveness. However, the best-performing context window size of these methods is often significantly shorter compared to the prediction, which has become a major bottleneck for long-term time series forecasting[22].\nThis prompts a question: What controls the input size of neural networks for time series fore- casting? Through a comparative analysis of the design architectures and basic components of current mainstream methods, we find a positive correlation between the size of token(the minimum processing units of the model) and the input length. Models with longer tokens can handle longer input sequences. For instance, Autoformer [24] and FEDformer[29] treat each individual time point as a token, and their optimal context window size is 96. PatchTST[13] and Crossformer[27] utilize fragments ranging in size from 8 to 16 as tokens, extending their optimal context window to 336. When the entire sequence is processed using an MLP, such as TSMixer[4], Nbeat[15], and Nhits[1], the context window can expand to 512. Another important finding is that token size also affects, dominant at some extent, the behavior of the model. Models with smaller tokens [10, 9, 28, 29, 13] fit local variations more accurately, such as subtle periodic fluctuations, while models with larger tokens ignore subtle fluctuations and fit better to overall distribution features, such as mean and variance [15, 1, 4]. Based on the above findings, we arrive at the answer to the initial question: Token size controls the input size that the model can accommodate. However, long tokens may lead the model to overlook local variations. A model that simultaneously processes tokens of multiple lengths to attend to variations of different scales might be the optimal solution.\nHow should the token size be determined? In the research of self-supervised learning for natural language processing[19, 2] and computer vision [3, 6], a patch or token should contain semantic information, which naturally leads us to the periodicity of time series. We initiate our analysis by examining the Multi-Periodicity of the time series, a concept previously explored in TimesNet[23]. In this framework, sequences can be understood as overlapping periods with different scales, where the scale refers to the intervals at which events recur, such as hourly, daily, or weekly periods. Real-world phenomena are influenced by these scales, with distinct patterns emerging at different scales. It uses Fourier analysis to find the potential period lengths of the sequence, which can be used to determine the token size of our model."}, {"title": "3 Methodologies", "content": "Preliminaries. In a Deep learning paradigm for time series forecasting[5], given a multivariate historical sequence $x \\in \\mathbb{R}^{M \\times L}$, where L is the context windows size (or input sequence length), the objective of long time series forecasting is to predict a future sequence $y \\in \\mathbb{R}^{M \\times H}$, where H is the predict length. The multivariate time series comprises multiple dimensions, where each dimension $i \\in \\{1, 2, ..., M\\}$ represents a separate time series $x^{(i)} \\in \\mathbb{R}^{L}$, which was referred as a channel. our MTE handles multiple channels simultaneously, but ignores potential correlations between channels, since cross-dimension dependency [4, 27] is not the focus of this paper.\nOverview. our MTE comprises four steps: (3.1)Employing Fourier analysis, the top k period lengths are identified based on the multi-periodicity of the sequence. (3.2)Recursively extracting different- scale periodic patterns from the sequence, from short to long, to form separate period sequences. (3.3)For each period sequence, its period length is used as the token size to specify an individual module for pattern recognition. (3.4)Employing separate predictors for each pattern, prediction and interpolation(corresponding pooling) are conducted, then combined to form the result. The overall framework is depicted in the Figure 2."}, {"title": "3.1 Multi-Periodicity Analysis", "content": "Time series exhibit multi-periodicity, as illustrated in Figure 2. The first step of our model involves identifying the different period lengths within the sequence, a task conventionally accomplished through frequency domain analysis. Initially, we apply the Fast Fourier Transform (FFT) to convert the time series from the time domain to the frequency domain, aiming to extract frequencies that exhibit significant periodicity. Due to the sparsity of the frequency domain and the potential noise introduced by high-frequency components, we select only the top k frequencies$\\left\\{f_{1}, f_{2}, \\ldots, f_{k}\\right\\}$ with the highest amplitude values A. Given the conjugate nature of the frequency domain, we focus on frequencies below $[L / 2]$, where L denotes the length of the input sequence. In this context, the period is defined as the reciprocal of the frequency. Specifically, we utilize the Period module in TimesNet[23] to select the periods $\\left\\{p_{1}, \\ldots, p_{k}\\right\\}$ corresponding to the reciprocals of the top k amplitudes(The Period module is not mandatory as, in fact, it is unstable. We recommend presetting hyperparameters.\n$\\mathrm{A},\\left\\{f_{1}, \\ldots, f_{k}\\right\\},\\left\\{p_{1}, \\ldots, p_{k}\\right\\}=\\operatorname{Period}(x)$.\n(1)"}, {"title": "3.2 Multi-Periodic Series-Decomposition", "content": "To enable the model focusing simultaneously on multiple periodic patterns at different scale, it is imperative to disentangle the overlapping multi-periodic patterns within the sequence. Multi-Periodic Series-Decomposition module recursive extracts periodic patterns in a sequence base on Multi- Periodicity Analysis results $\\left\\{p_{1}, \\cdots, p_{k}\\right\\}$. According to the nature of the sequence decomposition algorithm, the reasonable extraction order is based on the period lengths from short to long. Therefore, we first sort the periods:\n$\\left\\{p_{1}, \\cdots, p_{k}\\right\\}=\\operatorname{Asc}\\left(\\left\\{p_{1}, \\ldots, p_{k}\\right\\}\\right)$,\n(2)\nHere, Asc() denote the ascending ordering. Base on the period lengths $\\left\\{p_{1}, \\cdots, p_{k}\\right\\}$, the Series- Decomposition module [24] is used to extract larger-scale period features from the trend components decomposed from preceding round, in a recursive manner, from short to long period:\n$s_{j+1}, t_{j+1}=\\operatorname{SeriesDecomp}_{w=p_{j+1}, v=1}\\left(t_{j}\\right), j \\in\\{0, \\ldots \\ldots, k-1\\}, t_{0}=x$,\n(3)\n$s_{j} \\in \\mathbb{R}^{M \\times L}$ and $t_{j} \\in \\mathbb{R}^{M \\times L}$ are the season and trend component of round j respectively, where in the first round the trend components t is just input sequence x. $\\operatorname{SeriesDecomp}_{w, v}(\\cdot)$ denote the Series-Decomposition module and its subscripts w, v represent the sliding window size and step size, respectively. Given that long period features typically entail less information, average pooling, $\\operatorname{AvgPool}_{w, v}(\\cdot)$, is applied, wherein the period length of preceding round $p_{j-1}$ are used as parameters to w and v, thereby mitigating redundancy:\n$\\begin{aligned} &\\tilde{s}_{j}=\\operatorname{AvgPool}_{w=p_{j-1} / 2, v=p_{j-1} / 2}\\left(s_{j}\\right), j \\in\\{1, \\ldots, k\\}, p_{0}=2, \\\\ &s_{k+1}=t_{k}=\\operatorname{AvgPool}_{w=p_{k} / 2, v=p_{k} / 2}\\left(t_{k}\\right), \\end{aligned}$\n(4)\nIn addition to the season component $\\tilde{s}_{j} \\in \\mathbb{R}^{M \\times L_{j}}$ for each round, the trend component $t_{k}$ for the last round is also retained as $s_{k+1}$. We have a series group $\\left\\{\\tilde{s}_{1}, \\ldots, s_{k+1}\\right\\}$ as the final result of this module, and each size is $L_{j}=\\left[\\frac{L}{p_{j}}\\right]+1$, and their corresponding period length is $\\tau_{j}=p_{j}$ and"}, {"title": "3.3 Multi-Token Pattern Recognition", "content": "As shown in Figure 2, we organize multiple Periodic Pattern Recognition(PPR) modules in parallel for k + 1 period sequences. Each module uses a set of parameters $(\\mathcal{l}_{j}, N_{j}, \\mathcal{T}_{j})$ to define for an input sequence $\\tilde{s}_{j}$, which can be formalized as:\n$z_{j}=\\operatorname{PPR}_{\\left(\\mathcal{l}_{j}, N_{j}, \\mathcal{T}_{j}\\right)}\\left(\\tilde{s}_{j}\\right), j \\in\\{1, \\ldots, k+1\\}$,\n(6)\nwhere $\\operatorname{PPR}_{(\\mathcal{l}, N, \\mathcal{T})}(\\cdot)$ use $\\mathcal{l}_{j}, N_{j}$ as input and output length, and $\\mathcal{T}_{j}$ as token size, containing two main modules, Period & Phase Embedding and Two-stage Attention Encoding Layer.\nPeriod & Phase Embedding. For each sequence $\\tilde{s}_{j}$, we use its period length $\\mathcal{T}_{j}$ as the token size for embedding. We first truncate $\\tilde{s}_{j}$ to length p$\\mathcal{T}_{j}$, where p is the preset maximum number of periods, to prevent small-scale se- quences from becoming a bottleneck. To allow focusing on the changes between different periods, namely inter- period and the changes between different phases of each period, namely intra-period, we need to embed each phase and period adequately. Firstly, we fold the sequence $\\tilde{s}_{j}$ according to the parameter $\\mathcal{T}_{j}$ :\n$S_{j}=\\operatorname{Fold}_{w=\\mathcal{T}_{j}, v=\\mathcal{T}_{j}}\\left(\\operatorname{Padding}\\left(\\tilde{s}_{j}\\right)\\right)$,\n(7)\nwhere Padding($\\cdot$) is to lineup the sequence for fold- ing $\\operatorname{Fold}_{w,}(\\cdot) . S_{j} \\in \\mathbb{R}^{M \\times\\left(\\mathcal{L}_{j} / \\mathcal{T}_{j}\\right) \\times \\mathcal{T}_{j}}$ is 2D form of original 1D sequence, where each row is a period, and each column is a phase. Afterwards, linear mapping $W^{\\text {intra }} \\in \\mathbb{R}^{d \\times\\left(\\mathcal{L}_{j} / \\mathcal{T}_{j}\\right)}, W^{\\text {inter }} \\in \\mathbb{R}^{d \\times \\mathcal{T}_{j}}$ is applied to each period and phase separately, and position encod- ing $W^{(p o s)} \\in \\mathbb{R}^{T_{j}}, W^{(p o s)} \\in \\mathbb{R}^{\\left(\\mathcal{L}_{j} / \\mathcal{T}_{j}\\right)}$ is added to obtain the embedding of phases $\\sigma_{i} \\in \\mathbb{R}^{M \\times \\mathcal{L}_{j} \\times d}$, and periods $\\mu_{i} \\in \\mathbb{R}^{M \\times\\left(\\mathcal{L}_{j} / \\mathcal{T}_{j}\\right) \\times d}$.\n$\\begin{aligned} &\\sigma_{i}=W^{\\text {intra }} S_{j}+W^{(p o s)}, \\\\ &\\mu_{i}=W^{\\text {inter }} S_{j}^{T}+W^{(p o s)} . \\end{aligned}$\n(8)\nTwo-stage attention encoding layer. Transformer en- coders have been widely acknowledged as versatile seq2seq approximators [26]. To capture both inter-period dependencies and intra-period dependencies effectively, we adopt a two-stage methodology. In the initial stage, self- attention encoders are deployed to independently model two dependencies:\n$\\begin{aligned} &\\hat{\\sigma}_{j}=\\operatorname{MLP}^{\\text {intra }}\\left(\\operatorname{MSA}^{\\text {intra }}\\left(\\sigma_{j}, \\sigma_{j}, \\sigma_{j}\\right)\\right), \\\\ &\\hat{\\mu}_{j}=\\operatorname{MLP}^{\\text {inter }}\\left(\\operatorname{MSA}^{\\text {inter }}\\left(\\mu_{j}, \\mu_{j}, \\mu_{j}\\right)\\right) . \\end{aligned}$\n(9)\nIn the second stage, cross-attention encoders are employed to integrate these two types of dependen- cies:\n$\\begin{aligned} &\\sigma_{j}^{(\\text {mix })}=\\operatorname{MLP}^{\\text {cross }}\\left(\\operatorname{MSA}^{\\text {cross }}\\left(\\hat{\\sigma}_{j}, \\hat{\\mu}_{j}, \\hat{\\mu}_{j}\\right)\\right), \\\\ &\\mu_{j}^{(\\text {mix })}=\\operatorname{MLP}^{\\text {cross }}\\left(\\operatorname{MSA}^{\\text {cross }}\\left(\\hat{\\mu}_{j}, \\hat{\\sigma}_{j}, \\hat{\\sigma}_{j}\\right)\\right), \\end{aligned}$\n(10)\nwhere MSA($\\cdot$) is Multi-head dot-product attention layer and MLP($\\cdot$) is the feed-forward layer(identical to MLP). For clarity, we omitted the residual connections and normalization lay- ers between each layers. We will concatenate the output of the last layer for final encoding:\n$z_{j}=\\left[\\sigma_{j}^{(\\text {mix })}, \\mu_{j}^{(\\text {mix })}\\right]$\n(11)"}, {"title": "3.4 Multi-Predictor Mixing", "content": "For each period sequence $\\tilde{s}_{j}$, we obtained a unique encoding $z_{j} \\in \\mathbb{R}^{M \\times\\left(\\mathcal{L}_{j} / \\mathcal{T}_{j}+L_{j}\\right) \\times d}$.\n$z_{j}=\\operatorname{PPR}\\left(\\tilde{s}_{j}\\right), j \\in\\{1, \\ldots, k+1\\}$,\n(12)\nConsidering the distinction of each patterns, we use separate predictor for each encoding. Each predictor is linear layer W $\\in \\mathbb{R}^{\\left(\\mathcal{L}_{j} / \\mathcal{T}_{j}+L_{j}\\right) \\times n_{j}}$.Then let each predict shorter sequences $y_{j} \\in \\mathbb{R}^{M \\times n_{j}}$ then use interpolation to complete them, to induce multi-scale hierarchical time series forecasts[1]. We get the final prediction y as following:\n$\\begin{aligned} &y_{j}=\\operatorname{Predictor}_{j}\\left(z_{j}\\right), j \\in\\{1, \\ldots, k+1\\}, \\\\ &y=\\sum_{j=0}^{k+1} \\operatorname{Interpolate}_{\\tau_{j} \\rightarrow L}\\left(y_{j}\\right), \\end{aligned}$\n(13)"}, {"title": "4 Experiments", "content": "Benchmarks. We conduct experiments of our model on eight main-stream benchmarks, including Weather, Traffic, Electricity, Solar-energy and 4 ETT datasets(ETTh1, ETTh2, ETTm1, ETTm2), which have been extensively used in previous works [28, 24, 29, 13, 11] and publicly available at [24]. Training/Validation/Test sets are zero-mean normalized with the mean and std of Training set. The Statistics of all benchmarks are gathered in table 1.\nBaselines. We selected eight popular State of The Art(SoTA) models as baselines, including FED- former[29], Autoformer[24], Informer[28], Non-Stationary[12], TimesNet[23], TimesMixer[21], Crossformer[27] and PatchTST[13]. TimesMixer(MLP-based) is current SoTA baseline, and PatchTST is Transformer-based SoTA baseline.\nSetup. We follow the experimental setup of mainstream methods [13]. The input length is set to 960 or 1680 most case, sometimes 336 for short-term forecasting, according to the multi-periodicity. And the prediction length is varied with H = {96, 192, 336, 720}. We utilize the Adam optimizer with Mean Squared Error(MSE = $\\frac{1}{n} \\sum_{i=1}^{n}(y-\\hat{y})^{2}$) as the loss function and evaluate using Mean Absolute Error(MAE = $\\sum_{i=1}^{n}|y-\\hat{y}|$) and MSE as metrics."}, {"title": "4.1 Main Results", "content": "All results are shown in Table 2. Other methods result employ a context window size of 96, and its cited from previous work[21]. our MTE utilizes an optimal window size ranging from 960 to 1680(some times 336), determined by the sequence's periodicity. Certain methods may exhibit better performance with larger context window sizes, such as PatchTST. However, this enhancement comes at the cost of a quadratic increase in complexity as the window size expands (see Picture 4).\nour MTE performs outstandingly in most cases, with an improvement of 11.43% compared to the MLP baseline TimeMixer and approximately 20.17% compared to the Transformer baseline PatchTST, in MSE. our MTE performs particularly well in long-term forecasting scenarios. For example, on large time series datasets such as Traffic, ECL, and Solar, when the prediction amplitude is 336 or above, the average improvement compared to MLP baseline is 15.13%, and the average improvement compared to Transformer baseline is 27.56%, and it shows an upward trend. The driving force for improvement lies in a larger context. In ETTm, Weather, and Solar Energy datasets, we used a context window of 960(10 days), while in ETTh, Traffic, and Electricity, the context window was 1680(10 weeks), which is about 10x to 20\u00d7 more than previous work."}, {"title": "4.2 Ablation", "content": "We conducted detailed ablation study on all modules from the model, the main results are shown in Table 3. From aba3, it can be seen that the Intra-period module works fine on its own when in short-term forecasting scenario. This confirms the previous argument that the small token model focuses more on local variations, since each token in the Intra-period module is actually a time point. However, in long-term forecasting scenarios, it cannot continueto be competent, which led us to the inter-period module. From aba4, the cross-period module itself performs mediocrely. The fact that Inter-period module is actually a standard paradigm of Transformer, encourages us to explore innovative structures beyond conventional framework. On the other hand, from aba5, it can be seen that the decomposition module itself can significantly improve the performance of long-term forecasting. Simply replacing the subsequent PPR module, it can be used for any method. Lastly, aba2 and aba3 prove that complex neural networks cannot function properly without the guidance of a coherent data flow structure."}, {"title": "4.3 Efficiency Analysis", "content": "In long input scenarios, the most concerning issue is perhaps the efficiency of the model. Therefore, we conducted a detailed analysis of usability to the model by examining both its computational complexity and its real-world time and memory cost.\nComplexity. We also analyzed the computational complexity, as detailed in Table 4. As a bench- mark, the computational complexity of a Transformer scales quadratically with the input length L. Early methods, such as Informer, Autoformer, and Fedformer, process all sequence dimensions simultaneously within a single token, making their complexity unaffected by the input sequence dimension M. This approach neglects inter-channel differences, leading to suboptimal performance. In contrast, similar to other recent methods, our MTE adopts a channel-independent strategy to handle multivariate sequences.\nFurthermore, we observed that FEDformer exhibits a very low complexity, which scale linearly with the input length. However, it introduces FFT and RFFT at each layer to achieve this reduction in complexity, which significantly slows down the actual processing speed(we only compute FFT once at the beginning). Crossformer and PatchTST employ segmented embedding, resulting in a complexity that is quadratic with respect to the number of tokens ($\\frac{L}{\\text {seg}}$, where p is the token size and s is the stride), which is significantly lower than the quadratic complexity relative to the input length. our MTE broadly classified to this approach but is correlated to the maximum number of periods p or the maximum number of period length $P_{\\max }$. Due to pooling and truncation, these values are relatively small and do not increase significantly with the input length, thereby keeping the overall cost manageable."}, {"title": "Efficiency.", "content": "Based on the above analysis, lower complexity on paper may not necessarily lead to proportionally lower costs. As a supplementary analysis to the complexity assessment, we conducted experiments to measure the training time and memory usage(nvidia-smi) of different methods. Using the ETTm2 dataset with a fixed prediction length of 96, the results are depicted in Figure 4. our MTE demonstrates lower time and memory costs compared to Transformer-based approaches, and is approximately on par with linear and MLP models. This indicates that our MTE remains more practical compared to other Transformer models even in the case of 10 times increased the input sizes."}, {"title": "Hyper-parameter Sensitivity Analysis.", "content": "our MTE sometimes encounters high errors with long inputs (Table 5 ETTh2-720), and we will carefully analyze the reasons for this phenomenon in Section 4.4. The sensitivity of the maximum number of periods p was tested with a fixed input length of 1680, showing no significant impact on performance, which is typically set to 16."}, {"title": "4.4 Model Interpretation Analysis", "content": "Output of our model can be expanded to observe the prediction results of each period component, providing a way to understand the behavior patterns of the model, as pattern of each component exhibits sufficient regularity. As shown in Figure 5a, when there are no outliers in the context, everything works ideally. However, our MTE is sensitive to outliers, and a long context window further exacerbates this issue (Picture 5b). In the prediction of period 168 of the bad case, the variance of the predicted values did not follow the expected decreasing trend but remained relatively high, which we refer to as variance shift. Similarly, in the prediction of period 1680, mean shift was observed. These issues reflect the same problem in current deep learning-based time series forecasting methods, which focus on overall statistical significance patterns rather than temporal variation. Maybe introducing higher order derivatives pattern recognition may help this problem, which will leaving to the future exploration."}, {"title": "5 Conclusion", "content": "Neural networks with a fixed token size tend to focus on temporal pattern at a dominant scale, which is usually smaller. Focusing on minor scale changes further leads to a smaller context window to prevent overfitting. Since the token size essentially determines the scale to which the model is focusing, the design of the embedding layer is crucial. Therefore, unlike previous work, the decoupling method and multi-scale recognition module proposed in this paper revolve around how to reasonably embed the time series, rather than focusing on the encoding layers. On the other hand, we have noticed that in the self-supervised learning methods of Transformers for natural language processing and image recognition tasks, a patch or token should have some semantic meaning. So in time series, we naturally thought of periodicity and used the pattern of a period as a token. By combining Fourier analysis of the multi-periodicity of sequences, we have implemented a neural network that processes multiple tokens in parallel, thereby enabling multi-scale processing also long context.\nOur work altered the behavioral pattern of time series forecasting methods enabling the forecasted results to exhibit rich temporal patterns rather than a simple repetition of a specific scale variation. We hope these findings will inspire future work."}]}