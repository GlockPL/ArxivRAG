{"title": "Unveiling the Mystery of Weight in Large Foundation Models: Gaussian Distribution Never Fades", "authors": ["Chongjie Si", "Jingjing Jiang", "Wei Shen"], "abstract": "This paper presents a pioneering exploration of the mechanisms underlying large foundation models' (LFMs) weights, aiming to simplify AI research. Through extensive observation and analysis on prevailing LFMs, we find that regardless of initialization strategies, their weights predominantly follow a Gaussian distribution, with occasional sharp, inverted T-shaped, or linear patterns. We further discover that the weights share the i.i.d. properties of Gaussian noise, and explore their direct relationship. We find that transformation weights\u00b9 can be derived from Gaussian noise, and they primarily serve to increase the standard deviation of pre-trained weights, with their standard deviation growing with layer depth. In other words, transformation weights broaden the acceptable deviation from the optimal weights, facilitating adaptation to downstream tasks. Building upon the above conclusions, we thoroughly discussed the nature of optimal weights, ultimately concluding that they should exhibit zero-mean, symmetry, and sparsity, with the sparse values being a truncated Gaussian distribution and a few outliers. Our experiments in LFM adaptation and editing demonstrate the effectiveness of these insights. We hope these findings can provide a foundational understanding to pave the way for future advancements in the LFM community.", "sections": [{"title": "1. Introduction", "content": "Large foundation models (LFMs) (Qin et al., 2023; Touvron et al., 2023a; Devlin et al., 2018; Kirillov et al., 2023) have exhibited remarkable performance across a diverse range of tasks (Sap et al., 2020; Ma et al., 2024; Rombach et al., 2022), and are widely regarded as a key pathway toward achieving Artificial General Intelligence (Feng et al., 2024). However, their applications, including pretraining (Guu et al., 2020), adaptation (Si et al., 2024c), editing (Ilharco et al., 2022), and compression (Zhu et al., 2023), are fraught with intricate engineering challenges and resource limitations, making AI research challenging and thorny. This leads us to ponder: could we find a way to explore the properties or principles of optimal weights, radically simplify the landscape of AI research?\nThis paper represents the first exploration into the intrinsic mechanisms of LFMs' weights, aiming to offer profound insights into their behavior. Through extensive observation and analysis of the weights across prevailing LFMs in natural language processing (NLP), computer vision (CV), and multi-modal (MM), we arrive at a series of surprising and illuminating conclusions as follows:\n\u2022 Regardless of the initialization strategies, nearly all pre-trained weights conform to a Gaussian distribution, with a rare subset displaying sharp, inverted T-shaped, or even linear patterns.\n\u2022 The weights exhibit i.i.d. properties similar to Gaussian noise, and transformation weights can be directly derived from Gaussian noise.\n\u2022 Transformation weights primarily increase the standard deviation of the pre-trained weights, with their standard deviation growing with layer depth. In essence, they increase the acceptable variance from the optimal weights to facilitate adaptation to downstream tasks.\n\u2022 The smaller differences in their standard deviations correlate with more similar model performance2, highlighting the potential to develop evaluation beyond only test data reliance.\nBuilding upon the aforementioned conclusions, we conducted an in-depth exploration of optimal weights and ultimately concluded that optimal weights should exhibit zero-mean, symmetry, and sparsity, with sparse values"}, {"title": "2. Analysis on Weights Distribution", "content": "The prevailing consensus in most studies is that model weights encapsulate the knowledge learned from the data (Eilertsen et al., 2020; Sch\u00fcrholt et al., 2024). Consequently, these weights exhibit distinct structural properties that reflect the underlying patterns captured by the model (Monasson & Zecchina, 1995; Lee & Kim, 2023). Intuitively, it is widely accepted that the weights should be precise and resilient to perturbations, as they directly influence the model's performance and generalization ability. However, what exactly do these weights look like?"}, {"title": "2.1. Observation", "content": "In the broader LFM community, the weights available for analysis can primarily be categorized into two types: pre-trained weights and fine-tuned weights obtained from adapting LFMs for downstream tasks. To explore the properties of weights, we first examine the distributions of these two categories. Since the latter can be viewed as a specific case of the"}, {"title": "2.2. Validation", "content": "To determine whether the weight distributions indeed resemble a Gaussian distribution, we adopt a multi-step statistical analysis. First, for the elements in each matrix, we calculated their mean (\u03bc) and standard deviation (\u03c3). \u03a4\u03bf mitigate"}, {"title": "3. Derivation from Gaussian Noise", "content": "Moreover, some studies (Thamm et al., 2022; Anonymous, 2024; Yang, 2020; Lee et al., 2017) suggest that the individual elements of a weight matrix, denoted as {wi}=1, are asymptotically independent. Assume that each element wi follows a certain distribution Pi(w). The overall distribution of the weights is then determined by the specific forms of these individual distributions. If the distribution of each weight element wi is distinct, that is, Pi(w) \u2260 Pj(w) for some i \u2260 j, then the combined distribution of the weights would not form a single Gaussian distribution. However, based on our exploration in the previous section, we observed that the overall weight distributions in LFMs are Gaussian distributions. Given that wi are independent, their joint distribution can be expressed as the product of their marginal distributions, i.e.,\n$P(\\lbrace w_i \\rbrace_{i=1}^n) = \\prod_{i=1}^n P_i(w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}^n}  \\exp(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (w_i - \\mu)^2)  = \\frac{1}{\\prod_{i=1}^n \\sqrt{2\\pi\\sigma_i^2}}  \\exp(-\\frac{1}{2} \\sum_{i=1}^n \\frac{(w_i - \\mu_i)^2}{\\sigma_i^2}) \\newline = \\prod_{i=1}^n P_i(w).$  (1)\nThis indicates that the independent weight elements must share the same underlying distribution, i.e.,\n$w_i \\sim P_i(w) = N(\\mu, \\sigma^2), \\forall i$.  (2)\nTherefore, the weight elements are asymptotically independent and identically distributed (i.i.d.).\nEach element being independently and identically distributed, combined with the fact that the overall distribution adhering to a Gaussian distribution, inevitably brings"}, {"title": "3.1. I.i.d Properties of Weights", "content": "Moreover, some studies (Thamm et al., 2022; Anonymous, 2024; Yang, 2020; Lee et al., 2017) suggest that the individual elements of a weight matrix, denoted as {wi}_1, are asymptotically independent. Assume that each element wi follows a certain distribution Pi(w). The overall distribution of the weights is then determined by the specific forms of these individual distributions. If the distribution of each weight element w\u2081 is distinct, that is, Pi(w) \u2260 Pj(w) for some i \u2260 j, then the combined distribution of the weights would not form a single Gaussian distribution, as illustrated in Fig. 3. However, based on our exploration in the previous section, we observed that the overall weight distributions in LFMs are Gaussian distributions. Given that wi are independent, their joint distribution can be expressed as the product of their marginal distributions, i.e.,\n$P(\\lbrace w_i\\rbrace_{i=1}) = \\prod_{i=1} P_i(w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}  \\exp \\newline = \\prod_{i=1} P_i(w).$  (1)\nThis indicates that the independent weight elements must share the same underlying distribution, i.e.,\n$w_i \\sim P_i(w) = N(\\mu, \\sigma^2), \\forall i.$  (2)\nTherefore, the weight elements are asymptotically independent and identically distributed (i.i.d.).\nEach element being independently and identically distributed, combined with the fact that the overall distribution adhering to a Gaussian distribution, inevitably brings"}, {"title": "3.2. Validation", "content": "In a statistical sense, noise is a relative concept, referring to information that is not helpful for the current task. Notably, what may be perceived as noise for one task could be crucial for another, effectively becoming \u201cweights\" in that context. This duality leads us to believe that weights and noise are fundamentally indistinguishable in their structure and behavior. To validate this hypothesis, we begin with transformation weights, as they are computationally inexpensive to train and readily accessible.\nWe aim to achieve adaptation of the pre-trained model DeBERTaV3-base (He et al., 2021) on the GLUE benchmark (Wang et al., 2018), which consists of eight datasets for natural language understanding (NLU) tasks. For a pre-trained weight matrix W, we aim to learn the transformation matrix \u25b3W that represents the updates required for adaptation (Hu et al., 2021; Si et al., 2024b). To significantly reduce the number of trainable parameters, we adopt a \"cheating\" approach by leveraging auxiliary information (e.g., information of fully fine-tuned weights) to pre-construct \u25b3W. During the training phase, we only train a scalar coefficient s while keeping W and \u25b3W frozen, such that the fine-tuned weight matrix W' is represented as:\nW' = W + s\u25b3W.  (3)\nThe results are presented in Table 2. For comparison, we include the results of training only the head (row 1) and fully fine-tuning the model (row 2). We also include the results of a efficient adaptation method SVDiff (Han et al., 2023). First, comparing the results in rows 1 and 4 demonstrates that by training only 72 parameters, we achieve performance that significantly surpasses the Head baseline. When comparing rows 3 and 4, it is obvious that we train only a tiny fraction of the parameters in SVDiff (over 700 times fewer), yet achieve superior performance. This indicates that the learned parameters significantly capture more task-specific knowledge. Moreover, when comparing rows 2 and 4, on certain datasets, the results obtained by training these 72 parameters (a mere 0.00004% of the total parameters) even approach the performance of fully fine-tuning. Considering the extremely small number of trainable parameters and the remarkable performance achieved, it is reasonable to attribute this success to the vast amount of task-specific knowledge embedded in the pre-constructed \u25b3W, which indeed should not be available during standard training.\nHowever, what if we told you that these carefully crafted \u25b3W matrices were, in fact, randomly matrices using standard Gaussian noise?"}, {"title": "3.3. Revisiting the Transformation Weight Distribution", "content": "As Isaac Newton once remarked, \"Nature is pleased with simplicity, and affects not the pomp of superfluous causes\" (Newton, 1687), we believe that the oracle weights should adhere to this principle of simplicity. Consider an optimal weight matrix W* derived from an immensely large dataset D. To simplify notation, we treat the matrix as being flattened. During the pre-training phase10, we sample n examples from D and train the model by optimizing a loss function to obtain the weight matrix W. Consequently, W can be viewed as an M-estimator of W* (Van der Vaart, 2000; Peracchi, 1990). Under fairly general conditions, the difference W \u2013 W* is known to be asymptotically normal (Yohai & Maronna, 1979; Hoeffding, 1992), i.e.,\n$\\sqrt{n}(W - W*) \\thicksim N(0,\\sigma^2I)$.  (4)\nIn other words,\nW = W*+  N(0, \\frac{\\sigma^2}{n}I) + o(\\frac{1}{\\sqrt{n}}).  (5)\nIn practice, the third term can be assumed to be negligible when the number of training samples is sufficiently large. The second term represents the noises arisen during training (Zhou et al., 2019; Smith et al., 2020). This indicates that the obtained weight matrix W can be interpreted as a sample drawn from a Gaussian distribution, where the mean is the"}, {"title": "4. Unveiling the Mystery of Weight", "content": "Our observations reveal that the pre-trained weights follow a zero-mean Gaussian distribution. Based on Eq. (5), it is straightforward to deduce that the optimal weights are also a zero-mean Gaussian. Philosophically, we posit that W* embodies simplicity and highly compressibility. A Gaussian distribution indeed appears to satisfy these assumptions.\nTo validate this, we initialized DeBERTaV3-base with a standard Gaussian distribution and subsequently trained only its corresponding standard deviation. As a result, the fine-tuned weights obtained at the end strictly follow a Gaussian distribution. However, as shown in row 5 in Table 2, the model's performance is only about half of that achieved with full training. This comes to the conflict: if the optimal weights were indeed a Gaussian distribution, then the model results should, as expected, be superior.\nWhen theory and experiments conflict, we need to revisit our entire line of reasoning. Eventually, we identified a critical assumption: the Gaussian distribution of W was derived by focusing exclusively on statistical values within the 3\u03c3 range, discarding outliers with exceptionally large magnitudes. We then conducted extensive literature research on outliers and discovered that some studies highlight how outliers can influence model outputs (Yin et al., 2023; Kovaleva et al., 2021; Puccetti et al., 2022). (Yadav et al., 2024) even demonstrates that retaining only the outliers in the weights can achieve more than 70% of the model's original performance. This motivates us to rethink the distribution of weights, which leads to the following conclusions:\nWe hypothesize that the optimal weight W* is a sparse matrix containing a few values (e.g., outliers, Gaussian signals, etc). Under current pre-training techniques, we obtain its M-estimator, the pre-trained weights W. Due to dataset limitations, W includes some values in W* that are closely related to the pre-training dataset while neglecting others. Combined with Gaussian noise introduced during training,"}, {"title": "5. Application", "content": "One might wonder about the potential value and practical application of our exploration of weight matrices. First, our observations provide a solid explanation for the rationale behind many existing approaches in various fields, which we believe is a significant contribution. Moreover, based on our detailed exploration of \u25b3W, without preamble, we turn to two representative applications: Parameter-Efficient Fine-Tuning (adaptation), and Model Merging (editing)."}, {"title": "5.1. Parameter Efficient Fine-tuning", "content": "Parameter-efficient fine-tuning (PEFT) aims to learn fewer parameters while achieving comparable performance compared to fully fine-tuning. In the previous sections, we established the intrinsic relationship between Gaussian noise and the transformation matrix during training. Building upon this insight, we propose a plug-and-play augmentation method applicable to any existing approach: augment the pre-trained weight matrix W with a randomly initialized Gaussian noise term, while learning the standard deviation of this noise during training, as described in Eq. (3). It not only effectively increases the standard deviation of Gaussian noise in the pre-trained weights but also simplifies the learning process for other methods, allowing them to focus on identifying and refining better outliers.\nFurthermore, since the pre-trained weights W themselves follow a Gaussian distribution, they can be used as an initialization for Gaussian noise. We can directly initialize the transformation matrix using the pre-trained W. By doing so, the task reduces to learning the scaling factor s for W, thereby streamlining and optimizing the training process. Taking a prevailing PEFT method LoRA (Hu et al., 2021) as an instance, the weight update can be redefined as:\nW' = (s + 1)W + AB,  (6)\nwhere A and B are two low-rank matrices introduced by LORA. For more details on PEFT and related methods like LORA, please refer to Appendix E.3.\nThe experimental results, as presented in the Table. 3, clearly demonstrate that LoRA combined with specific Gaussian noise significantly outperforms standalone LoRA. Notably, this improvement is observed under both parameter configurations and for all three LLaMA variants. By leveraging the duality of Gaussian noise, we provide a novel perspective for designing methods in PEFT, which further validates the reliability of our observations."}, {"title": "5.2. Model Merging", "content": "Model merging techniques aim to integrate multiple task-specific models into a single, cohesive model, retaining the strengths of each individual model while eliminating the need for access to the original training data. Despite the various methods proposed for model merging, the dominant approach in large model technical reports remains averaging the weights of different models (He et al., 2024; Team et al., 2024; Baichuan, 2023), due to its simplicity and effectiveness. Please refer to Appendix E.4 for more details.\nBased on our observations, we believe that the averaging process essentially performs a form of \u201csmoothing\" on the weights. However, this averaging can also cause outliers to diminish, potentially reducing them to the same level as Gaussian signals. Therefore, we propose an intuitive approach: to amplify the outliers while averaging the others.\nFirstly, to merge n task-specific models, following (Yadav et al., 2024; Ilharco et al., 2022), we begin by obtaining the change in weights {Wk} for each trained task-specific model relative to the pre-trained weights (i.e., the task vectors (Ilharco et al., 2022)). Here, p denotes the number of distinct parameter groups (i.e., a matrix or vector) in the model, and AW represents the k-th group of weight changes for the i-th task-specific model. Next, considering that a value which appears as noise in one model might be an outlier in another, for each parameter group k, we first compute the standard deviation of \u03c3 of the k-th parameter group for each of the n models. Then, we determine the minimum standard deviation across all models for the k-th parameter group, defined as ok = min{\u03c3,\u03c3,\u2026\u2026,\u03c3}. Subsequently, for each AW, we classify its values into two categories: outliers and noises, based on a threshold tok (t \u2208 {2,3}). Specifically, values outside the range [-tok, tok] are treated as outliers and remain unchanged. Values within this range are treated as noises and are averaged across the n task-specific models. Finally, the processed weight changes {AW} are summed and added back to the pre-trained weights to obtain the final merged model. The pseudo codes are shown in Alg. 2."}, {"title": "6. Conclusion and Future Work", "content": "In this work, we explored the underlying mechanisms of LFM weights, offering a deeper understanding of their structure and dynamics. Through comprehensive analysis and observation of existing LFMs, we identify that the weights primarily conform to a Gaussian distribution, though some exhibit atypical patterns such as sharp, inverted T-shaped, or linear forms. We further reveal that these weights exhibit the i.i.d. properties of Gaussian noise, and explore the direct correlation between them. Transformation weights, which can be derived from Gaussian noise, play a key role in increasing the standard deviation of the pre-trained weights, with their standard deviation increasing as layer depth grows. Building on these findings, we conduct an in-depth examination of optimal weights and conclude that they should possess zero mean, symmetry, and sparsity, with sparse values likely following a truncated Gaussian distribution and featuring a few outliers. Furthermore, we hypothesize and preliminarily validate that the closer the standard deviations of transformation weights, the more analogous the model performance becomes. Our experiments in LFM adaptation and editing validate the potential of these findings.\nLooking forward, several directions emerge from this work. Further exploration of outlier distributions and properties could deepen our understanding of their impact on model performance. Besides, as LFMs continue to scale, future research should focus on leveraging these insights to enhance scalability and efficiency without sacrificing performance."}, {"title": "Impact Statement", "content": "This work offers a novel perspective on the underlying mechanisms of large foundation models (LFMs), with the potential to simplify and refine AI research. By investigating the intrinsic properties of LFM weights, we provide insights that could contribute to more efficient model adaptation, editing, and compression. Our findings propose a shift towards a more principled, physics-inspired approach, offering new avenues for understanding and improving AI models in a more systematic way.\nWhile this research focuses primarily on LFMs, we believe the methodologies and insights presented here could have broader implications for the design and optimization of machine learning systems. The exploration of weight properties could lead to improvements in model robustness, interpretability, and training efficiency-key challenges in advancing AI. Additionally, the findings may help refine evaluation frameworks, providing a more holistic understanding of model performance beyond traditional test data.\nWe hope this work will contribute to a broader discourse in AI research, providing a foundation for further developments that could eventually lead to more transparent, efficient, and powerful AI systems."}, {"title": "A. Further Exploration of Weight Distribution", "content": "We here delve deeper into the distribution of the pre-trained weights. While the majority of the weights follow a Gaussian distribution, certain weights occasionally exhibit peculiar distributions that appear unusually sharp, as shown in the first few rows of Figs. 10, 13 and 12. These sharp distributions are primarily observed in the shallower layers of the model. Further analysis reveals that this sharpness arises due to a significant presence of extremely small values (magnitude smaller than 0.001) in the weight. We here take the weights from the 3rd-8th rows of ConvNext (stage 2, layers 0 to 17, pwconv1 and pwconv2) as an example, since they exhibit the most pronounced sharpness. We first validate that, disregarding the impact of very small values, the overall distribution of the weights remains Gaussian. Since the skewness values of these weights are still zeros, we only present the kurtosis values for these 36 layers after applying a 3\u03c3 filter, and compare them with those obtained after further filtering out values whose magnitudes are smaller than 10-3. It can be observed that after removing the extreme small values, the kurtosis of the weights becomes closer to 3, indicating that these elements also follow a Gaussian distribution. We also present the distribution plots for these weights in Fig. 4."}, {"title": "A.1. Remarks on Pre-trained Weight Distribution", "content": "We here delve deeper into the distribution of the pre-trained weights. While the majority of the weights follow a Gaussian distribution, certain weights occasionally exhibit peculiar distributions that appear unusually sharp, as shown in the first few rows of Figs. 10, 13 and 12. These sharp distributions are primarily observed in the shallower layers of the model. Further analysis reveals that this sharpness arises due to a significant presence of extremely small values (magnitude smaller than 0.001) in the weight. We here take the weights from the 3rd-8th rows of ConvNext (stage 2, layers 0 to 17, pwconv1 and pwconv2) as an example, since they exhibit the most pronounced sharpness. We first validate that, disregarding the impact of very small values, the overall distribution of the weights remains Gaussian. Since the skewness values of these weights are still zeros, we only present the kurtosis values for these 36 layers after applying a 3\u03c3 filter, and compare them with those obtained after further filtering out values whose magnitudes are smaller than 10-3. The results are shown in Table. 5. It can be observed that after removing the extreme small values, the kurtosis of the weights becomes closer to 3, indicating that these elements also follow a Gaussian distribution. We also present the distribution plots for these weights in Fig. 4."}, {"title": "A.2. Remarks on Transformation Weight Distribution", "content": "1. Transformation Weights Follow a Gaussian Distribution:"}, {"title": "3.1.4  Average skewness and Kurtosis of the transformation weights in different settings.", "content": ""}, {"title": "Average skewness and Kurtosis of the transformation weights in different settings.", "content": ""}, {"title": "2. Gaussian Noise in Transformation Weights Complements the Standard Deviation of That in Pre-training:", "content": "Additionally, in the paper, we mention that the Gaussian noise in AW serves to complement the standard deviation of the Gaussian noise in pre-training. Here, we also validate AW obtained from LoRA-Dash and DoRA. We first present the results of these two methods, as shown in Table. 7, rows 1-2. The best performance of each method is achieved when r = 32; coincidentally, their results are identical. Therefore, we choose \u25b3Wr=32 of these two methods.\nWe observed that the absolute difference in standard deviations of AW obtained from LoRA-Dash and DoRA across all layers is less than 0.0005, with an average difference of 0.0002. This shows that the standard deviations across all layers are nearly identical. This further supports the idea that the Gaussian noise in AW complements the standard deviation of the Gaussian noise in pre-training. It is noteworthy that these are two entirely different methods, and we are unaware of their training techniques and parameters. However, they exhibit phenomena that are consistent with our observations in another model, which is quite intriguing. Moreover, their weights are publicly available, allowing interested researchers to replicate the experiments."}, {"title": "3.  The Closer the Standard Deviations of Two \u25b3W, the More Similar the Corresponding Model Performance Becomes:", "content": "Furthermore, we compared the differences in the standard deviations of AW obtained from LoRA-Dash and DoRA for the same rank. The results are shown in the Table. 7, row 3. Interestingly, we found that, overall, the greater the performance difference between the AW of the same rank, the larger the difference in their corresponding standard deviations. We thus can't help but make an intriguing hypothesis: The closer the standard deviations of AW, the more similar the corresponding model performance becomes.\nTo validate this hypothesis, we take \u25b3Wr=32 from both LoRA-Dash and DoRA as baselines, and compare the performance differences and standard deviation discrepancies with other settings17. The experimental results are presented in Fig. 5, where we clearly observe that, in general, when the performance difference is smaller, the discrepancy in \u03c3 is also smaller18."}, {"title": "4.  The Standard Deviation of \u25b3W Increases with the Layer Depth:", "content": "As will be shown in Appendix. B, the Gaussian noise introduced during the training of each layer increases as the layer depth"}, {"title": "B. Open Exploration on the Optimal Weights W*", "content": "With a sense of both apprehension and excitement, we now begin to explore matters related to W*. It is important to note that since the optimal W* is unknown\u2014and, indeed, remains unknown to the broader scientific community, obtaining and even verifying W* is impossible. Consequently, the discussion presented here is more akin to a \u201cphysical experiment\": based on observed phenomena, we propose plausible hypotheses and aim to validate them through a broad range of methods.\nWe will first propose reasonable inferences about W* based on our existing observations and supports from various works in the community, and hypothesize the distribution that W* should follow. Then, we will use the inferred distribution of W* to conduct the following validations: (1) examining the causes of Gaussian and sharp distributions in the observed pretrained weights; (2) comparing with existing analyses of weight properties in the literature; and (3) evaluating the rationality of methods used in different works."}, {"title": "B.1. Inferences and Conjectures Based on Existing Evidence", "content": "First, we argue that the training process inevitably introduces zero-mean Gaussian noise. The larger the noise's standard deviation, the greater its impact; conversely, a smaller standard deviation results in a lesser impact. When the standard deviation is zero, no Gaussian noise exists. Many studies have indicated that, compared to deeper layers, the weights of shallow layers are more easily trained and thus more likely to approach the optimal weights. This suggests that the standard deviation of Gaussian noise in shallow layers is smaller than that in deeper layers, which is consistent with our observation: the standard deviation of the Gaussian distribution tends to increase across layers within the same module, as demonstrated in Fig. 8.\nAdditionally, we find that the sharper distributions in the observed weights occur in the shallow layers\u2014those that are well-trained and closer to the optimal weights W*. We hypothesize that when the noise's standard deviation is large, the weight distribution resembles the noise distribution, i.e., a Gaussian distribution. Conversely, when the noise's standard deviation is small, the corresponding Gaussian distribution becomes very \u201cvertical\u201d, and the weight distribution begins to reveal more information about the distribution of W*. Therefore, we will use the sharper distributions as a starting point to analyze and explore the properties of W*.\nFirst, we hypothesize that W* is a sparse matrix. In the experiment presented in Appendix A.1, we observe that the sharp distribution is primarily caused by the presence of numerous small values (with magnitudes less than 10-3). These small values can account for up to 35% of the total weights (e.g., in the ConvNext-xlarge, Stage 2, Layer 10). Through our review in related literature, we have noted that many approaches confirmed such small values contribute negligibly to model performance, which can safely be set to zeros (Yadav et al., 2024; Deep et al., 2024; Yin et al., 2023; Han et al., 2015b). Based on this, we infer that these small values may stem from Gaussian noise and can be directly discarded. Considering that there may be many other elements that can also be set to zero, we believe that the optimal weights W* should contain a significant number of zero elements, and therefore, W* is a sparse matrix. It is worth noting that the sparsity of W* is consistent with findings from biological studies of brain.\nSecond, we propose that some of the sparse values in W* are outliers, i.e., elements with magnitudes significantly larger than the overall distribution. This observation is both evident and direct, as we have detected outliers in all the weight distributions we have analyzed. Moreover, many studies have emphasized the importance of outliers for model performance, suggesting that W* indeed contains outliers. It is also important to note that outliers could also be caused by Gaussian noise. In this context, we refer specifically to the outliers that exist in W*.\nThird, the sparse values in W* follow a symmetric distribution with a zero mean. We first explain why there are values other than outliers. We performed post-processing on the weights of LLaVA-v1.5-13B, and when we retained only the outliers beyond 3\u03c3 or even 2\u03c3, the model failed to function properly. It produced only end-of-sequence tokens and was unable to understand context or follow instructions. This indicates that some important values lie within the Gaussian distribution that we filtered out. Furthermore, based on the sharp distribution, we know that part of this distribution follows a Gaussian form, which contributes to the sharpness of the distribution. Thus, when the Gaussian values are filtered out, some other values remain. Therefore, we believe that W* contains values beyond just the outliers."}, {"title": "B.2. Validation: Derivation of Weight Distribution", "content": "Based on our hypothesis about W*, an important question is to explain why we can observe that most weights follow a Gaussian distribution, and some follow sharp or even extreme distributions (e.g., in the Qwen2.5 model, Layer 2 MLP's Down Projection).\nTo investigate this phenomenon further, we first simulate W*. We randomly fit a sparse matrix and assign its values a truncated Gaussian distribution. Specifically, we set the standard deviation of the Gaussian distribution to 0.1, and then filter out values whose absolute values are less than 0.001 or greater than 0.5, resulting in a truncated Gaussian distribution. Next, we simulate the training process by adding different levels of Gaussian noise to W*.\nWe present the weight distributions after adding noise at varying levels, as shown in Fig. 7. Through this figure, we gain insights into the generation of weight distributions observed in all the LFMs experiments. We can clearly observe that\n\u2022 When the standard deviation of the Gaussian noise is very small (0.001, 0.005), the resulting weight distribution appears as a single line, similar to the distributions observed in Qwen2.5 Layer 1 MLP's Up Projection, Layer 2 MLP's Up Projection and Down projection etc.\n\u2022 When the standard deviation of the Gaussian noise is relatively small (0.01), the weight distribution resembles an inverted T-shape, similar to the distributions observed in Qwen2.5 Layer 4 MLP's Up Projection, Layer 5 MLP's Up Projection, and most of the unusual distributions in ConvNext.\n\u2022 When the standard deviation of the Gaussian noise becomes non-negligible (0.03, 0.05), the weight distribution exhibits a sharp distribution, similar to the distributions observed in Qwen2.5 Layer 2 Attn's V Projection, Layer 3 Attn's K Projection, and sharp distributions in other models.\n\u2022 When the standard deviation of the noise is equal to or greater than the standard deviation of the optimal weights (0.1, 0.2, 0.3), the weight distribution closely resembles the Gaussian noise distribution. In this case, it aligns with the distribution observed in the vast majority of the weights.\nTherefore, the Gaussian distribution observed in the vast majority of the weights is not the true distribution of the weights themselves, but rather the distribution of the noise. Due to the large standard deviation of the noise, the true distribution of the matrix is overshadowed. This also explains why so many of the weights in LFM exhibit a Gaussian distribution. When the standard deviation of the Gaussian noise is small, the true distribution of the weights begins to gradually emerge, revealing sharp distributions. Such distributions require a small noise standard deviation, indicating that the weights have"}, {"title": "B.3. Validation: Support from Broader Community", "content": "We here present several works from the broader community which can be further explained by the properties of W*. To begin with, we first point out several properties of W*: zero-mean, symmetry, and sparsity. Next, we will explore how these properties manifest in the broader community's methods and how they indirectly support the concept of W*. While these"}, {"title": "B.3.1. ZERO-MEAN PROPERTY", "content": "The property of zero mean in neural network weights, especially in deep learning models, plays a fundamental role in weight initialization, training dynamics, and generalization (Pascanu, 2013). It has been recognized that initializing weights with a mean of zero helps avoid symmetry-breaking issues and promotes faster convergence during training (Kingma, 2014). Zero mean initialization is typically used in conjunction with techniques such as Gaussian distributions or uniform distributions, which ensure that the weights do not have any initial bias that could lead to skewed learning trajectories (Sutskever, 2014).\nXavier initialization (Glorot & Bengio, 2010) is a well-known method that sets the weights to have a zero mean and variance scaled according to the number of units in the layer, helping to maintain gradient flow and prevent vanishing or exploding gradients during training. This method ensures that, on average, the output of each neuron will not deviate significantly from zero, allowing the neural network to learn effectively from the start. Similarly, He initialization (He et"}]}