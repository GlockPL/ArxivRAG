{"title": "SUBTLE Errors MaTTER: PREFERENCE LEARNING VIA ERROR-INJECTED SELF-EDITING", "authors": ["Kaishuai Xu", "Tiezheng Yu", "Wenjun Hou", "Yi Cheng", "Chak Tou Leong", "Liangyou Li", "Xin Jiang", "Lifeng Shang", "Qun Liu", "Wenjie Li"], "abstract": "Large Language Models (LLMs) have exhibited strong mathematical reasoning and computational prowess, tackling tasks ranging from basic arithmetic to advanced competition-level problems. However, frequently occurring subtle errors, such as miscalculations or incorrect substitutions, limit the models' full mathematical potential. Existing studies to improve mathematical ability typically involve distilling reasoning skills from stronger LLMs or applying preference learning to step-wise response pairs. Although these methods leverage samples of varying granularity to mitigate reasoning errors, they overlook the frequently occurring subtle errors. A major reason is that sampled preference pairs involve differences unrelated to the errors, which may distract the model from focusing on subtle errors. In this work, we propose a novel preference learning framework called eRrror-Injected Self-Editing (RISE), which injects predefined subtle errors into partial tokens of correct solutions to construct hard pairs for error mitigation. In detail, RISE uses the model itself to edit a small number of tokens in the solution, injecting designed subtle errors. Then, pairs composed of self-edited solutions and their corresponding correct ones, along with pairs of correct and incorrect solutions obtained through sampling, are used together for subtle error-aware DPO training. Compared with other preference learning methods, RISE further refines the training objective to focus on predefined errors and their tokens, without requiring fine-grained sampling or preference annotation. Extensive experiments validate the effectiveness of RISE, with preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on MATH.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated advanced logical reasoning and computational capabilities, enabling them to tackle a diverse range of problems from elementary mathematics to complex, competition-level questions (Yue et al., 2024; Gou et al., 2024; Yu et al., 2024). Nevertheless, frequently occurring subtle errors remain one of the primary obstacles preventing these models from consistently realizing their full mathematical potential (Li et al., 2024). These errors, such as miscalculations, incorrect substitutions, and omission of calculation terms, account for approximately 75% of the total errors as shown in Figure 3.\nMany existing works have attempted to improve mathematical ability by distilling reasoning skills from stronger LLMs using synthetic datasets (Shao et al., 2024; Yue et al., 2024; Azerbayev et al., 2024). While distilling helps the model acquire expanded reasoning patterns, it does not directly enable it to reduce the occurrence of errors (Hong et al., 2024). Recently, a growing body of research has shifted its focus toward preference optimization with fine-grained preference pairs to mitigate mathematical errors (Lai et al., 2024; Chen et al., 2024; Setlur et al., 2024). This method leverages Direct Preference Optimization (DPO) to reduce the likelihood of generating the error step in a pair of steps with the same context (Rafailov et al., 2023). However, learning with these fine-grained preference pairs still struggles to focus on subtle errors, as the aforementioned pairs involve some"}, {"title": "2 METHOD", "content": "In this section, we introduce our RISE framework, which achieves hard negative preference pair construction through error-injected self-editing and enhances the mathematical reasoning capability"}, {"title": "2.1 DATASET CONSTRUCTION VIA ERROR-INJECTED SELF-EDITING", "content": "To create hard preference pairs, we employ an editing approach to modify certain tokens in the correct solution to introduce predefined errors. We focus on the error injection of each step in one solution. Compared with previous step-wise preference learning (Lai et al., 2024), our approach further refines the objective of preference learning by specifically targeting a few error tokens, enhancing the ability of LLMs to avoid subtle errors.\nError Types. As we aim to optimize the model to avoid subtle errors, the main types of errors we concentrate on are as follows: (1) Numerical calculation errors; (2) Numerical or symbolic substitution errors; (3) Omission of calculation terms. These errors frequently occur in most solutions and typically involve only a small amount of tokens. We also include two relatively less frequent errors: (4) Errors in the calculation order and (5) Errors in the use of calculation symbols. We extract and summarize these errors from the solutions generated by the models in our experiments.\nSelf-editing. We design appropriate prompts and utilize the model itself to edit the generated correct step \u0177. As most of the steps in the correct solutions are accurate (Lu et al., 2024b), we use all the steps from the correct ones without applying any additional filtering. Since solutions to mathematical problems are highly error-sensitive, any modification of numerical values, symbols, or other mathematical elements disrupts the original correct procedure. Thus, even small language models can be prompted to almost certainly inject errors. The error-injected step \u0177 is generated using an edit prompt shown on the left of Figure 2. This edit prompt contains the problem x, the concatenation of previous steps \u0177, the correct step \u0177, and the edit type e. We mainly use three types of editing operations: \u201cREPLACE\u201d, \u201cSWAP\u201d, and \u201cDELETE\" (Mallinson et al., 2022). Some error injection examples are shown on the right of Figure 2. For each step, only around ten tokens will be modified. We collect the error-injected steps and the correct steps to construct a paired edited"}, {"title": "2.2 SUBTLE ERROR-AWARE DPO TRAINING", "content": "Given all self-edited pairs and full-solution pairs, we apply DPO to optimize the model (Rafailov et al., 2023). Specifically, we first randomly choose N pairs from all self-edited pairs for each problem and combine them with the full-solution pair, which will be N + 1 pairs in total as the training set. To effectively learn subtle errors in each step, we adopt the step-wise DPO loss (Lai et al., 2024) for the self-edited pairs as it can focus on fine-grained preference learning, and general DPO loss for the full-solution pairs. Two loss functions are defined as follows:\n$\\mathcal{L}_{DPO-Edit} = -E_{(y_i^+, y_i^-)\\sim D_{Edit}} [log \\sigma(\\beta log \\frac{\\pi_{\\theta}(x; y_i^+)}{\\pi_M (y_i^+|x; y_{<i})}) - \\beta log \\frac{\\pi_{\\theta}(y_i^-|x; y_{<i})}{\\pi_M (y_i^-|x; y_{<i})}) ]$,\n$\\mathcal{L}_{DPO-Full} = -E_{(\\hat{y}^+,\\hat{y}^-)\\sim D_{Full}} [log \\sigma(\\beta log \\frac{\\pi_{\\theta}(\\hat{y}^+|x)}{\\pi_M (\\hat{y}^+|x)}) - \\beta log \\frac{\\pi_{\\theta}(\\hat{y}^-|x)}{\\pi_M (\\hat{y}^-|x)}) ]$,\nwhere $\\pi_{\\theta}$ is the policy model and \u3160M is the reference model. To mitigate the risk of optimization failure caused by the high similarity between paired samples (Pal et al., 2024), we additionally introduce a negative log-likelihood loss for the correct samples. This loss can help prevent the collapse of the probability of generating correct samples and is defined as LNLL. We present our subtle error-aware DPO loss that contains the above two objectives as follows:\n$\\mathcal{L} = \\mathcal{L}_{DPO-Edit} + \\mathcal{L}_{DPO-Full} + \\lambda \\mathbb{1}[r<0]L_{NLL}$,\n$r = log \\frac{\\pi_{\\theta}(x; y_i^-)}{\\pi_{\\theta}(y^+|x)} or log \\frac{\\pi_M (\\hat{y}^-|x)}{\\pi_M (\\hat{y}^+|x)}$,\nwhere X is the weight to control the balance of two objectives, r signals when to apply the NLL loss. Inspired by Pal et al. (2024), we design indicator r to represent whether the generation probability of the policy model is lower than that of the reference model. We outline the full algorithm of our Preference Learning via Error-injected Self-editing in Algorithm 1."}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 EXPERIMENTAL SETUP", "content": "Evaluation Datasets. We evaluate our framework on three in-domain datasets, GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and AQuA (Ling et al., 2017), along with three out-of-domain datasets, SVAMP (Patel et al., 2021), AIME24 (MAA, 2024), and Odyssey-MATH (Netmind.AI, 2024). These six datasets span a broad spectrum of mathematical problems, ranging from basic arithmetic to advanced competition-level problems. The problems in these datasets, including tabular, free-form, and multiple-choice formats, ensure a robust evaluation of the model's mathematical reasoning ability. The detail of all datasets is described in Table 5 in the Appendix A.\nBaselines. Our framework is compared with several LLMs performing well in mathematical reasoning. Two closed-source state-of-the-art LLMs: OpenAI's GPT-40 (OpenAI, 2024) and Anthropic's Claude-3.5-Sonnet (Anthropic, 2024). Three open-sourced general instruction-tuned LLMs: Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), Qwen2 series (Yang et al., 2024a), and Llama-3.1 series (Meta AI, 2024). Five mathematically enhanced LLMs: DeepSeekMath-RL (Shao et al., 2024), Llemma (Azerbayev et al., 2024), TORA (Gou et al., 2024), MAmmoTH (Yue et al., 2024), and MathGenieLM (Lu et al., 2024a). Four additional LLMs under step-wise DPO optimization are also included: Step-DPO series (Lai et al., 2024), SVPO (Chen et al., 2024), MCTS-DPO (Xie et al., 2024), and SCDPO (Lu et al., 2024b). We display results with CoT prompts for most baseline LLMs. ToRA and MAmmoTH are designed with built-in capabilities for tool usage, so we report the best results with tools.\nTraining Details. We adopt mathematical problems used by Lai et al. (2024) for preference learning. The training dataset contains around 9K problems with corresponding correct step-by-step solutions. We discard these solutions and use only the problems to construct our training set. The problems are mainly from MetaMath (Yu et al., 2024) and AQUA Ling et al. (2017). Details are presented in Table 6. We select two open-source LLM series, Qwen2 (Yang et al., 2024a) and Llama-3.1 (Meta AI, 2024) as our base LLMs. We apply the instruction-tuned version of these models to sample solutions and meanwhile edit each step of the solution. The number of sampling attempts is set to 5, and the number of self-edited pairs is set to 1 for the Qwen2 series and 3 for the Llama-3.1 series. Since some problems cannot yield correct and incorrect preference pairs within a limited number of sampling attempts, we ultimately use only about 4.5K problems to construct preference pair combinations for subtle error-aware DPO training. Implementation details are presented in the Appendix B."}, {"title": "3.2 MAIN RESULTS", "content": "We report the main results on different mathematical datasets shown in Table 1 and Table 2. The former is from commonly used mathematical datasets published earlier, consisting of three in-domain and one out-of-domain datasets. The latter is from the recent competition-level datasets and both datasets are out-of-domain. Overall, we can see that RISE outperforms the SOTA model at different scales. These results highlight the potential of our framework to help the general LLM to be a mathematical generalist. On several datasets, RISE-QWEN2-72B and RISE-LLAMA-3.1-70B even outperform some closed-source LLMs.\nTable 1 presents the results on the GSM8K, MATH, AQUA, and SVAMP datasets. Our framework enables the LLM to achieve noticeable improvements in mathematical reasoning compared to the corresponding backbone. Especially on MATH and AQUA, RISE-QWEN2-7B obtains 7.9% and 2.7% accuracy gain, and RISE-LLAMA-3.1-8B obtains 3.2% and 5.5%. RISE-QWEN2-7B outperforms other popular mathematical LLMs on all four datasets. Additionally, RISE performs better than the SOTA step-wise preference learning frameworks. Compared to Step-DPO, which shares the same backbone LLM and requires GPT-4-based annotations, RISE obtains better results without annotations. In detail, it achieves 4.1% higher accuracy on MATH, 6.7% higher on AQUA, and 2.9% higher on SVAMP. We scale our experiments on 70B/72B models and also observe around 1.0% accuracy gain on some evaluation datasets such as MATH and AQUA."}, {"title": "3.3 ABLATION STUDY", "content": "We demonstrate the effectiveness of our framework through different training settings as detailed below: (1) w/o self-edited pairs, which removes the supplemented edited pairs and trains the model with full-solution pairs. (2) w/o full-solution pairs, which trains the model with the edited pairs only. (3) w/o NLL loss, which removes the loss used for stabilizing training. Table 3 shows the results of different settings."}, {"title": "3.4 SUBTLE ERROR ANALYSIS", "content": "To analyze the effect of our framework on specific error mitigation, we counted the number of errors generated by different models on the MATH dataset. In detail, given the problem, the generated solution, and the reference answer, we prompt GPT-40 to detect the first error in any solution and output the error type in the final. To verify GPT-40's accuracy in detecting errors, we manually selected 50 random samples and checked for consistency in the identified errors. 46 (92%) of the samples were accurately detected with their error types, which is acceptable for conducting the complete analysis. We display the number of different errors made by the Qwen2-7B series in Figure 3. We can observe that numerical calculation errors account for approximately 60% of the total errors and subtle errors we defined for 75%. Compared with the standard DPO, our framework additionally reduces the number of predefined errors. Especially for numeric or symbolic substitution errors and omission of calculation terms, RISE reduces the number of errors, whereas standard DPO does not achieve this. In addition, other errors, mainly misunderstanding of problems or concepts, increase due to preference learning, but RISE still performs slightly better than standard DPO."}, {"title": "3.5 EFFECT OF SELF-EDITED PAIRS", "content": "Self-edited pairs are essential for fine-grained preference learning, particularly in mitigating subtle errors. To comprehensively explore the effect of self-edited pairs, we conduct experiments optimizing the model using different numbers of self-edited pairs (i.e., N in Sec. 2.2). Figure 4 shows the results for N = 1,2,3,4, and \"All\" self-edited pairs. For RISE-QWEN2-7B, the accuracies on GSM8K and MATH both decrease with more self-edited pairs. For RISE-LLAMA-3.1-8B, the accuracies reach a relative peak when using three self-edited pairs for each problem. This figure indicates that using more self-edited pairs is not always the better option, considering both the accuracy and the training cost of using additional samples. Additionally, RISE-LLAMA-3.1-8B prefers more self-edited pairs, which is consistent with the characteristics of Llama-3.1-8B-Instruct, as its full solutions contain around three more steps than those of Qwen2-7B-Instruct. More step-wise self-edited pairs help RISE-LLAMA-3.1-8B further avoid subtle errors."}, {"title": "3.6 EFFECT OF SAMPLING ATTEMPTS", "content": "We further explore the effect of sampling attempts, which directly determine the number of problems involved in preference learning. As shown in Figure 5, as the number of sampling attempts increases, the number of problems corresponding to paired correct and incorrect full solutions also increases. Although more problems are involved in preference learning, the final results show that pairs corresponding to a larger number of problems actually reduce learning performance. It may be because more sampling attempts yield more samples of \"extreme\" problems. For these problems, the LLM tends to consistently answer either correctly or incorrectly. Training the LLM with these samples may not only be futile but could also lead to performance degradation. We observe this phenomenon on both RISE-QWEN2-7B and RISE-LLAMA-3.1-8B."}, {"title": "3.7 EFFECT OF DIFFERENT ERROR-INJECTION COMBINATIONS", "content": "Predefined injected errors are essential for the effectiveness of our framework. We investigate the impact of different combinations of injected errors on the model's mathematical performance. Three types of errors that occur most frequently in solutions generated by our method are selected for analysis. Since not all samples are applicable to these three types of errors, we focus on one primary error type (i.e., \u201cDominate\u201d), supplemented by a small number of other errors. For example, some samples do not contain numerical values or calculation symbols, and thus cannot be injected with calculation errors. The comparison results are shown in Table 4. We can observe that all these combinations contribute to preference learning and a random combination yields the best performance. It indicates that samples with diverse predefined errors are more likely to help the LLM learn to avoid subtle errors."}, {"title": "4 RELATED WORK", "content": ""}, {"title": "4.1 LLM FOR MATHEMATICAL REASONING", "content": "Large language models (LLMs) have shown remarkable proficiency in mathematical reasoning, excelling in tasks ranging from basic arithmetic questions to complex mathematical Olympiad problems (Team et al., 2023; Jiang et al., 2024; Dubey et al., 2024; OpenAI, 2024; Huang et al., 2024). Furthermore, various approaches have been investigated to enhance the mathematical reasoning capabilities of LLMs. Methods such as Llemma Azerbayev et al. (2024), DeepseekMath Shao et al. (2024), and Qwen2.5-Math Yang et al. (2024b) have focused on collecting vast amounts of math-related data for continued pretraining. Recent efforts have also focused on designing more"}, {"title": "4.2 STEP-WISE PREFERENCE LEARNING", "content": "In addition to pre-training and instruction fine-tuning, step-wise preference learning\u2014particularly methods related to Proximal Policy Optimization (PPO)\u2014has been widely explored to enhance the mathematical capabilities of LLMs (Lightman et al., 2024; Luo et al., 2023; Shao et al., 2024). However, the final performance is highly dependent on the quality of the process-supervised reward model (PRM) (Uesato et al., 2022; Wang et al., 2024b), and the training process of PPO or its variants is notably complex. Recently, Direct Preference Optimization (DPO) (Rafailov et al., 2023) simplified this process by directly leveraging pair-wise data for preference learning. Many works extend DPO, enabling it to perform step-wise preference learning to improve the model's multi-step mathematical problem-solving ability (Lu et al., 2024b; Xie et al., 2024; Setlur et al., 2024; Lai et al., 2024). To the best of our knowledge, we are the first to tackle subtle errors in mathematical reasoning via step-wise preference learning especially on error tokens."}, {"title": "5 CONCLUSION", "content": "In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which constructs hard pairs through self-editing to mitigate predefined subtle errors. Compared to other fine-grained, step-wise preference learning methods, our framework further refines the training objective to target subtle error tokens, without requiring LLM-based or estimation-based preference annotations. To avoid optimization failure caused by overly similar preference pairs, we additionally introduce a negative log-likelihood loss, controlled by the generation probability of the correct samples. The effectiveness of our framework is demonstrated in two LLM series: Qwen2 and Llama-3.1. Results across multiple mathematical datasets demonstrate that our method can unlock the model's potential in mathematical reasoning."}, {"title": "6 REPRODUCIBILITY STATEMENT", "content": "We provide all codes and data used in this study in supplementary material. The evaluation details and implementation details are illustrated in Appendix A and Appendix B."}, {"title": "Problem 1:", "content": "What is the maximum value of 4(x + 7) (2 \u2013 x), over all real numbers x?\nReference Answer:\n81\nRISE-QWEN2-7B:\nTo find the maximum value of the function f(x) = 4(x + 7)(2 \u2013 x), we can start by expanding the function and then completing the square or using calculus to find the critical points.\n### Step 1: Expand the function\nf(x) = 4(x + 7)(2 \u2013 x)\n= 4(2x - x\u00b2 + 14 - 7x)\n= 4(-x2 \u2013 5x + 14)\n= -4x2\n20x + 56\n### Step 2: Find the critical points To find the maximum value, we can use calculus. We'll find the derivative of f (x) and set it to zero to find critical points.\nf'(x) = $\\frac{d}{dx}$(-4x\u00b2 - 20x + 56)\n=-8x20\nSetting the derivative equal to zero to find critical points:\n-8x20=0\n8x = -20\nx = $\\frac{20}{8} = \\frac{5}{2}$\n### Step 3: Determine the nature of the critical point To determine if this point is a maximum, we can use the second derivative test or simply recognize that since the coefficient of x\u00b2 in the original function is negative, the parabola opens downwards, indicating that the critical point is indeed a maximum.\n### Step 4: Find the maximum value Substitute x = - back into the original function to find the maximum value:\nf(-)=-4(-)-20(-)+56\n= $-4 \\frac{25}{4}$ +50+56\n=-25+50+56\n= 81\nTherefore, the maximum value of 4(x + 7) (2 \u2013 x) over all real numbers x is 81"}, {"title": "C.2 CASE STUDY ON LLAMA-3.1-8B SERIES MODELS", "content": "Table 9 and Table 10 shows a generated solution by RISE-LLAMA-3.1-8B and Llama-3.1-8B-Instruct. We can observe that the solution by Llama-3.1-8B-Instruct makes a mistake with numeraical substitution, where the numerator should be \u221a2 rather than 2\u221a2. RISE-LLAMA-3.1-8B successfully avoid this subtle error."}, {"title": "D EDIT PROMPT SET", "content": "We show prompts for the aforementioned five types of subtle errors in Table 11, Table 12, Table 13, Table 14, and Table 15."}, {"title": "Problem 2:", "content": "If we write \u221a2++\u221a3+ = $\\frac{a\\sqrt{2}+b\\sqrt{3}}{c}$ in the form  such that a, b, and c are positive integers and c is as small as possible, then what is a + b + c?\nReference Answer:\n23\nRISE-LLAMA-3.1-8B:\nTo solve this problem, we need to rationalize the given expression and then simplify it.\nStep 1: Rationalize the expression \u221a2+/+\u221a3+\nTo rationalize $\\frac{\\sqrt{2}}{2}$ and $\\frac{\\sqrt{3}}{3}$, we multiply each fraction by its conjugate over itself to eliminate the square root in the denominator.\n$\\frac{\\sqrt{2}}{2} = \\frac{\\sqrt{2}}{\\sqrt{2}} \\times \\frac{\\sqrt{2}}{\\sqrt{2}} = \\frac{2}{2}$\n$\\frac{\\sqrt{3}}{3} = \\frac{\\sqrt{3}}{\\sqrt{3}} \\times \\frac{\\sqrt{3}}{\\sqrt{3}} = \\frac{3}{3}$\nStep 2: Substitute the rationalized fractions back into the original expression.\n\u221a2++\u221a3+ = \u221a2+\u221a3\nStep 3: Combine like terms.\n\u221a2 + \u221a2 = $\\frac{2\\sqrt{2}}{\\sqrt{2}}+\\frac{\\sqrt{2}}{\\sqrt{2}} = \\frac{3\\sqrt{2}}{\\sqrt{2}}$\n\u221a3+\u221a3 = $\\frac{3\\sqrt{3}}{\\sqrt{3}}+\\frac{\\sqrt{3}}{\\sqrt{3}} = \\frac{4\\sqrt{3}}{\\sqrt{3}}$\nSo the expression becomes $\\frac{3\\sqrt{2}}{\\sqrt{2}} + \\frac{4\\sqrt{3}}{\\sqrt{3}}$.\nStep 4: To combine these two fractions into one fraction with a common denominator, we need to find the least common multiple (LCM) of 2 and 3, which is 6.\n3\u221a2 = $\\frac{3\\sqrt{2} x 3}{\\sqrt{2}} = \\frac{9\\sqrt{2}}{6}$\n$\\frac{4\\sqrt{3}}{\\sqrt{3}}$ = $\\frac{4\\sqrt{3} x 2}{3} = \\frac{8\\sqrt{3}}{6}$\nStep 5: Combine the two fractions.\n$\\{6}\\sqrt{2}$ + \u221a$\\{6}\\sqrt{3}$ = $\\frac{9\\sqrt{2}+8\\sqrt{3}}{6}$\nNow we have the expression in the desired form $\\frac{a\\sqrt{2}+b\\sqrt{3}}{C}$\nStep 6: Identify the values of a, b, and c.\na = 9, b = 8, and c = 6.\nStep 7: Calculate the sum of a, b, and c.\na+b+c=9+8+6= 23\nThe final answer is: 23"}, {"title": "Llama-3.1-8B-Instruct:", "content": "To simplify the given expression, we can start by rationalizing the denominators of the fractions.\n\u221a2++\u221a3+\nFirst, we rationalize the denominators of the fractions by multiplying each fraction by the conjugate of its denominator over itself.\n\u221a2++\u221a3+= \u221a2++\u221a3+\nNow, we can combine like terms.\n\u221a2\u221a2 + 2\u221a2 \u221a2 = 4\u221a2 + = 3\u221a2\u221a3 + +2 +3 + 3 = 1\u221a3\nCombine the like terms.\n42 + 2\u221a2 + \u221a3 + = 2 +4\u221a3\nSimplify the fractions.\n6\u221a22 + 3 = 3\u221a2 + \u221a3\nTo get a common denominator, we multiply the first term by.\n3\u221a2 + 3 = 9\u221a2 + \u221a3\nNow we can combine the fractions.\n9\u221a23 + \u221a3 = 9\u221a2+\u221a3\nNow we have the expression in the desired form  =  where a = 9, b = 4, and c = 3.\nTherefore, a +b+c=9+4+3 = 16.\nThe final answer is: 16"}, {"title": "Prompt for Numerical Calculation Errors:", "content": "Question:\n{question}\nInitial Answer:\n{answer}\nCurrent Step:\n{text}\nEdit a numerical value or a series of related values in the current step to make a wrong calculation.\nDo not state that errors have been made."}, {"title": "Prompt for Numeric or Symbolic Substitution Errors:", "content": "Question:\n{question}\nInitial Answer:\n{answer}\nCurrent Step:\n{text}\nEdit a value or symbol in the current step to make a wrong substitution. Do not state that errors have been made."}, {"title": "Prompt for Omission of Calculation Terms:", "content": "Question:\n{question}\nInitial Answer:\n{answer}\nCurrent Step:\n{text}\nDelete a calculation term in the current step to make a wrong calculation. Do not state that errors have been made."}, {"title": "Prompt for Errors in the Calculation Order:", "content": "Question:\n{question}\nInitial Answer:\n{answer}\nCurrent Step:\n{text}\nSwap two calculation terms in the current step to make a wrong calculation. Do not state that errors have been made."}, {"title": "Prompt for Errors in the Use of Calculation Symbols:", "content": "Question:\n{question}\nInitial Answer:\n{answer}\nCurrent Step:\n{text}\nEdit a calculation symbol (e.g., +-*/, etc.) in the current step to make a wrong calculation."}]}