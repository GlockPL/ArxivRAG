{"title": "Image-Feature Weak-to-Strong Consistency: An Enhanced Paradigm for Semi-Supervised Learning", "authors": ["Zhiyu Wu", "Jinshi Cui"], "abstract": "Image-level weak-to-strong consistency serves as the predominant paradigm in semi-supervised learning (SSL) due to its simplicity and impressive performance. Nonetheless, this approach confines all perturbations to the image level and suffers from the excessive presence of naive samples, thus necessitating further improvement. In this paper, we introduce feature-level perturbation with varying intensities and forms to expand the augmentation space, establishing the image-feature weak-to-strong consistency paradigm. Furthermore, our paradigm develops a triple-branch structure, which facilitates interactions between both types of perturbations within one branch to boost their synergy. Additionally, we present a confidence-based identification strategy to distinguish between naive and challenging samples, thus introducing additional challenges exclusively for naive samples. Notably, our paradigm can seamlessly integrate with existing SSL methods. We apply the proposed paradigm to several representative algorithms and conduct experiments on multiple benchmarks, including both balanced and imbalanced distributions for labeled samples. The results demonstrate a significant enhancement in the performance of existing SSL algorithms.", "sections": [{"title": "1 Introduction", "content": "Semi-supervised learning (SSL) [4,30,32,46], a research topic that aims to reduce manual labeling costs by capitalizing on unlabeled data, has received considerable attention in recent years. Among the proposed methods, the image-level weak-to-strong consistency paradigm introduced by FixMatch [32] has proven highly effective in leveraging the potential of unlabeled data. Specifically, FixMatch applies varying intensities of image-level perturbations to raw samples and encourages consistent predictions between corresponding weakly and strongly augmented views. Subsequent studies have refined this paradigm primarily in two aspects: (1) improving the utilization ratio of unlabeled data by replacing the constant threshold in FixMatch with a dynamic threshold [35,42] and (2) facilitating balanced predictions across different classes via refining pseudo-labels based on historical predictions [3,5]. Extensive experiments have demonstrated the effectiveness of these methods in enhancing the image-level weak-to-strong consistency paradigm.\nDespite the success of the established paradigm, two challenges persist. Firstly, the existing paradigm confines all perturbations to the image level, thus impeding the exploration of a broader augmentation space and consistency at non-image levels. Secondly, as revealed in [12], a notable proportion of samples, even undergoing strong image-level perturbation, continues to be accurately classified with high confidence, resulting in a loss close to zero. These instances (some examples are shown in Fig. 5), referred to as naive samples, are well-learned and thus fail to boost the model's performance. Therefore, relying solely on image-level perturbation proves insufficient to fully exploit the potential of unlabeled data.\nTo address the aforementioned challenges, we introduce a novel SSL paradigm named Image-Feature Weak-to-Strong Consistency (IFMatch). An overview of the conventional and proposed approaches is shown in Fig. 1. The primary innovation of our method lies in feature-level perturbation, which is motivated by earlier findings that regularizers effective in the input space can be extended to hidden representations [14,15,34]. Specifically, feature-level perturbation randomly alters intermediate features in the backbone, establishing the basis for feature-level consistency regulation. Furthermore, to achieve effective feature-level perturbation, we present refined designs focusing on position and strategy. For the perturbation position, we identify two positions to induce weak and strong feature-level perturbations, respectively. Regarding the perturbation strategy, we develop a series of approaches from three perspectives ('movement', 'dropout', and 'value'), thereby comprehensively exploring the feature augmentation space. Note that the proposed feature-level perturbation is sample-agnostic and thus differs from previous class-based feature augmentation techniques [20,36]."}, {"title": "2 Related Work", "content": "Consistency regulation [2] represents a fundamental approach to leveraging the potential of unlabeled data. Its core objective is to ensure consistent predictions across diverse perturbed views of the same sample. Various perturbation methods have been explored, such as stochastic augmentation [22,31], adversarial attack [25], and mixup [43]. Additionally, FixMatch [32] applies strong data augmentation techniques [7] to raw images, establishing the image-level weak-to-strong consistency paradigm. This paradigm significantly streamlines the framework and marks a pivotal milestone in semi-supervised learning.\nFollow-up studies refine FixMatch along two dimensions: the utilization ratio of unlabeled data and fairness considerations. Regarding the former, numerous dynamic threshold strategies have been proposed. For example, FlexMatch [42] maps the predefined threshold to class-specific thresholds based on class-wise learning status. SoftMatch [5] models sample weights by a dynamic Gaussian function, maintaining a soft margin between unlabeled samples of different confidence levels. FreeMatch [35] utilizes the confidence on unlabeled data as the adaptive threshold. Moreover, the concept of fairness stems from entropy-based regulation [1,11,16,18,44]. The most representative strategy in this context, Distribution Alignment [3], encourages equal-frequency predictions across classes by refining pseudo-labels based on overall predictions within the unlabeled set.\nConcurrently, several studies express similar concerns to the old paradigm. FeatMatch [20] and ISDA [36] prevent the exclusive reliance on image-level perturbation by devising class-based feature augmentation approaches. In contrast, our proposed feature-level perturbation is sample-agnostic, circumventing harmful perturbations derived from erroneous pseudo-labels. Additionally, UniMatch [40] uses channel-wise dropout to establish an auxiliary branch for feature-level perturbation. In comparison, we present feature-level perturbations with varying intensities and forms, further combining them with image-level perturbations within each branch to boost their synergy. Besides, [12] observes excessive naive samples and applies more aggressive image-level augmentation to these samples. To address this issue, we propose a confidence-based identification strategy to effectively distinguish between naive and challenging samples and introduce weak feature-level perturbation exclusively for naive samples."}, {"title": "3 Methodology", "content": "We begin by reviewing the image-level weak-to-strong consistency paradigm, as depicted in the left part of Fig. 1. Let \\(D_{l} = \\{(x_i, y_i)\\}_{i=1}^{N_l}\\) and \\(D_{u} = \\{u_i\\}_{i=1}^{N_u}\\) represent the labeled and unlabeled datasets, respectively. Here, \\(x_i\\) and \\(u_i\\) denote the labeled and unlabeled training samples, and \\(y_i\\) refers to the one-hot label for the labeled sample \\(x_i\\). We denote the prediction of sample \\(x\\) as \\(p(y|x)\\). Given a batch of labeled and unlabeled data, the model is optimized with the objective \\(\\mathcal{L} = \\mathcal{L}_s + \\lambda_u\\mathcal{L}_u\\). Specifically, \\(\\mathcal{L}_s\\) indicates the cross-entropy loss (H) for the labeled batch of size \\(B_L\\).\n\nOn the other hand, \\(\\mathcal{L}_u\\) signifies the image-level consistency regulation between the prediction of the strongly augmented view \\(A_s(u)\\) and the pseudo-label sourced from the corresponding weakly augmented view \\(A_w(u)\\). To mitigate incorrect pseudo-labels, FixMatch [32] introduces a constant threshold, and subsequent studies propose various dynamic threshold strategies. Without loss of generality, we denote the threshold as \\(\\tau_t\\) and define \\(\\mathcal{L}_u\\) as follows.\n\nHere, \\(p_w\\) is the abbreviation of \\(\\text{DA}(p(y|A_w(u_i)))\\), where DA indicates the distribution alignment strategy [3]. \\(p^*\\) denotes the one-hot pseudo-label obtained from \\(\\arg \\max(p_w)\\). Moreover, \\(p_s\\) stands for \\(p(y|A_s(u_i))\\). Lastly, \\(B_u\\) corresponds to the batch size of unlabeled data.", "3.1 Image-Level Weak-to-Strong Consistency": null}, {"title": "3.2 Feature-Level Perturbation", "content": "To avoid the exclusive reliance on image-level perturbation, this paper introduces feature-level perturbation varied in intensity and form, thereby expanding the augmentation space. In the following parts, we will elaborate on the implementation of feature-level perturbation regarding its position and strategy.\nPosition refers to where the feature-level perturbation is intro-duced. Given that WideResNet [41] is the prevailing backbone in SSL, we employ its basic component, the residual block [13] with pre-activation, as an illustrative example. Building upon previous studies that exploit feature-level regularizers directly after the convolution [14,15,34], we identify two positions for introducing feature-level perturbation, as depicted in Fig. 2. In position A, the method perturbs the summation of the residual and identity connections, i.e., the output of the residual block. As position A acts as the bottleneck for feedforward, perturbation at position A exhibits high intensity and is therefore regarded as strong feature-level perturbation \\(A_F^s\\). Additionally, position B perturbs the output of a random convolution within the residual component, which is relatively milder than position A. Therefore, perturbation at position B is considered as weak feature-level perturbation \\(A_F^w\\). It is worth noting that both position choices are applicable to all residual blocks, and the algorithm randomly selects one position at each iteration to insert feature-level perturbation.\nStrategy denotes the operation of feature-level perturbation. We first review strong image-level perturbation strategies \\(A^s\\) [7], which can be roughly divided into three groups: 'movement', 'dropout', and 'value'. Specifically, 'movement' comprises spatial perturbations like translation and shearing, 'dropout' corresponds to the Cutout technique [9], and 'value' represents operations that modify pixel values while preserving pixel relationships, such as adjusting contrast, brightness, and histogram. Considering the effectiveness of \\(A^s\\), we develop feature-level perturbation strategies from the same perspectives. For 'movement', we implement feature map translation and shearing along the X-axis and Y-axis. Regarding 'dropout', we apply channel-wise and spatial-wise dropout on hidden representations. Concerning 'value', we employ random-sized convolutions to achieve local smoothing, subsequently using the weighted sum of smoothed and input feature maps as the output. During training, the model randomly selects one candidate strategy at each iteration to perturb hidden representations at the selected position. For the detailed implementation of feature-level perturbation strategies, please refer to Appendix Sec.1."}, {"title": "3.3 Image-Feature Weak-to-Strong Consistency", "content": "Building upon well-designed image-level and feature-level perturbations, we can integrate them to achieve consistency regulation in both dimensions. Intuitively, a trivial approach is combining strong image-level and feature-level perturbations in one branch to construct a dual-branch structure similar to the old paradigm. However, such a toy paradigm engenders destructive perturbations according to our pilot study (Tab. 3). Accordingly, we integrate these two forms of perturbations in a more moderate manner, developing the image-feature weak-to-strong consistency paradigm (IFMatch) depicted in the right part of Fig. 1.\nOur paradigm retains the traditional teacher branch, utilizing predictions \\(p_w\\) of samples subjected to weak image-level perturbation as pseudo-labels. Furthermore, we introduce two student branches to integrate both types of perturbations. Mathematically, given a batch of labeled and unlabeled data, the model is optimized using the objective \\(\\mathcal{L} = \\mathcal{L}_s + \\lambda_u(\\mathcal{L}_{u1} + \\mathcal{L}_{u2})\\), where \\(\\mathcal{L}_s\\) denotes the cross-entropy loss for labeled data, and \\(\\mathcal{L}_{u1}\\) and \\(\\mathcal{L}_{u2}\\) represent the consistency regulation for unlabeled data in the two student branches respectively.\nIn the first student branch, we integrate weak image-level perturbation \\(A^w\\) and strong feature-level perturbation \\(A_F^s\\) to comprehensively explore the feature perturbation space. Furthermore, we employ the constant threshold \\(\\tau\\) adopted in FixMatch to filter out incorrect pseudo-labels. This choice arises from the observation that existing dynamic threshold mechanisms are less suitable for \\(A_F^s\\), especially in challenging tasks. Thus, the unsupervised loss \\(\\mathcal{L}_{u1}\\) in the first student branch can be expressed as follows.\nIn the second student branch, we combine strong image-level perturbation \\(A^s\\) and weak feature-level perturbation \\(A_F^w\\) to provide additional challenges in recognizing naive samples. However, a direct fusion of \\(A^s\\) and \\(A_F^w\\) poses obstacles in classifying hard samples. Accordingly, we present a confidence-based identification strategy to distinguish between naive and challenging samples, thus introducing extra challenges exclusively for naive samples, which will be detailed in Sec. 3.4. Moreover, given the central role of \\(A^s\\), we leverage existing threshold mechanisms \\(\\tau_t\\) to exclude misleading pseudo-labels, facilitating seamless integration with previous algorithms. Consequently, the unsupervised loss \\(\\mathcal{L}_{u2}\\) in the second branch can be defined as follows.\nwhere \\(p_s\\) represents the prediction of sample \\(u_i\\) in the second branch. Particularly, the use of distinct thresholds in the two student branches is motivated not only by experiments (Tab. 5) but also by an inherent rationale. Specifically, the threshold serves to balance the utilization ratio of unlabeled data and pseudo-label accuracy [5,35]. Furthermore, \\(A_F^s\\) and \\(A^s\\) act as the central driving forces behind the respective student branch. Due to the unique characteristics of \\(A_F^s\\) and \\(A^s\\), they exhibit varying preferences in the trade-off between the utilization and pseudo-label accuracy, resulting in the different desired threshold mechanisms in the two student branches. For a further understanding of our paradigm, please refer to Algorithm 1."}, {"title": "3.4 Confidence-Based Identification", "content": "As analyzed in Sec. 3.3, the combination of strong image-level perturbation \\(A^s\\) and weak feature-level perturbation \\(A_F^w\\) poses supplementary challenges for naive samples yet incurs exceeding difficulty for hard ones. Likewise, previous research has encountered similar challenges in expanding the augmentation space. For example, UniMatch [40] observes a performance degradation caused by the concatenation of channel-wise dropout and \\(A^s\\) and finally denies the feasibility of combining \\(A^s\\) and \\(A_F^w\\) within one branch. Besides, [12] imposes more aggressive image-level perturbations exclusively on naive samples. To distinguish between naive and challenging samples, their approach records sample-wise loss and utilizes the OTSU [28] method for distinction. However, OTSU is conventionally applied to the segmentation of bimodal gray-scale images, whereas the histogram of loss values typically exhibits a descending trend. As shown in Fig. 4, their approach tends to produce identification biased toward naive samples.\nTo effectively identify the naive samples, we propose a confidence-based identification strategy, which is depicted in Fig. 3. Similar to the threshold-based rule for pseudo-label filtering that only predictions exceeding the confidence threshold qualify as pseudo-labels, the proposed strategy can be succinctly concluded as: \u2018only strongly augmented samples \\(A^s(u)\\) with target confidence exceeding the threshold are considered naive and necessitate additional weak feature-level perturbation \\(A_F^w\\)\u2019. Specifically, the approach comprises two steps. Firstly, we record the confidence linked to the pseudo-label within the predicted distribution of the second branch. Mathematically, the target confidence \\(h_i\\) for sample \\(u_i\\) in the second student branch can be obtained as follows.\nIt is important to clarify that \\(p_s\\) solely represents the prediction of sample \\(u_i\\) in the second branch and has nothing to do with the introduction of \\(A_F^w\\) for \\(u_i\\). Secondly, the algorithm generates a binary mask for each sample to determine whether to introduce additional \\(A_F^w\\) by comparing the target confidence with the threshold \\(\\tau_t\\) in the second student branch. Consequently, the mask for sample \\(u_i\\), denoted as \\(M_i\\), and the perturbations applied to \\(u_i\\) in the second student branch can be expressed as follows.\n\nThe approach dynamically updates \\(M_i\\) throughout the training process, thereby considering both the learning status of individual samples and model performance when introducing \\(A_F^w\\)."}, {"title": "4 Experiments", "content": "Settings. We conduct experiments on multiple benchmarks, including CIFAR-10/100 [19], SVHN [26], STL-10 [6], and ImageNet [8], with various numbers of labeled samples. In the context of balanced SSL, we maintain a balanced distribution for labeled samples. The choice of backbone architecture adheres to established practices, employing specific models for different datasets: WRN-28-2 [41] for CIFAR-10 and SVHN, WRN-28-8 for CIFAR-100, WRN-37-2 [45] for STL-10, and ResNet-50 [13] for ImageNet. As for training parameters, we follow the unified codebase TorchSSL [42] to ensure fair comparisons. Specifically, batch sizes \\(B_L\\) and \\(B_u\\) are set to 128 and 128 on ImageNet and 64 and 448 on the remaining datasets. Moreover, the model is trained using the SGD optimizer with an initial learning rate of 0.03 and a momentum decay of 0.9. The learning rate is adjusted by a cosine decay scheduler over a total of \\(2^{20}\\) steps. Besides, we train the EMA model with a momentum decay of 0.999 for inference. The fixed threshold \\(\\tau\\) in the first branch is set to 0.95. For SVHN, we follow previous studies [35,39] and constrain the dynamic threshold within the range of [0.9, 1.0]. To suppress randomness, we repeat each experiment three times and report the average accuracy in Tab. 1. We omit the standard deviation given the stable performance of the proposed paradigm across different random seeds.\nPerformance. We apply the proposed paradigm (IFMatch) to several algorithms and compare their performance with existing methods on multiple benchmarks. The results presented in Tab. 1 underscore the significant contribution of our paradigm to promoting the performance of existing SSL algorithms. Specifically, the four algorithms involved achieve the average improvement of 4.05%, 3.22%, 1.62%, and 1.39% across all datasets. Moreover, previous methods struggle to enhance performance on less challenging tasks, such as CIFAR-10. In contrast, when integrated with the proposed paradigm, IFMatch (Fix) outperforms FixMatch by 2.29%/0.83%/0.70% on CIFAR-10 with 40/250/4000 labeled samples, respectively. Furthermore, our approach effectively addresses the challenges posed by extremely limited labeled samples, with IFMatch (Fix) achieving enhancements of 14.51% and 12.68% on STL-10 with 40 labels and CIFAR-100 with 400 labels, respectively, compared to FixMatch. Moreover, the four algorithms achieve an average promotion of 3.69%/2.30% on ImageNet with 100k/400k labels when following the proposed paradigm, indicating its effectiveness on large-scale datasets. Additionally, when employing our paradigm, the performance gap between dynamic and constant thresholds considerably narrows on challenging tasks, while IFMatch (Fix) even emerges as the preferred method on some less demanding tasks. The shifted performance gap implies a reduced reliance on complex threshold strategies of our paradigm, which primarily benefits from the expanded augmentation space. In summary, the impressive performance observed in Tab. 1 confirms the effectiveness of the proposed paradigm.", "4.1 Balanced Semi-Supervised Learning": null}, {"title": "4.2 Imbalanced Semi-Supervised Learning", "content": "Settings. We evaluate the proposed paradigm in the context of imbalanced SSL, where both labeled and unlabeled data exhibit a long-tailed distribution. Consistent with prior studies [5,10,21,23,27,37], we construct labeled and unlabeled sets using the configurations of \\(N_c = N_1 \\cdot \\gamma^{-\\frac{i}{y}}\\) and \\(M_c = M_1 \\cdot \\gamma^{-\\frac{i}{y}}\\). Specifically, for CIFAR-10-LT, we set \\(N_1\\) to 1500, \\(M_1\\) to 3000, and \\(\\gamma\\) to range from 50 to 150. Moreover, for CIFAR-100-LT, we set \\(N_1\\) to 150, \\(M_1\\) to 300, and \\(\\gamma\\) to span from 20 to 100. In all experiments, we employ WRN-28-2 as the backbone and utilize the Adam optimizer [17] with a weight decay of 4e-5. Besides, batch sizes \\(B_L\\) and \\(B_u\\) are fixed as 64 and 128 respectively. Additionally, the learning rate is initially set to 2e-3 and adjusted by a cosine decay scheduler during training. We set the fixed threshold \\(\\tau\\) in the first branch to 0.95. To account for randomness, we report the mean and standard deviation of the performance across three different random seeds.\nPerformance. We apply the proposed paradigm to numerous algorithms and compare the performance with their counterparts following the old paradigm. The results in Tab. 2 demonstrate that our paradigm substantially improves existing models, enabling them to achieve state-of-the-art performance. Specifically, when integrated with the enhanced paradigm, the four algorithms achieve the average promotion of 3.72%, 2.03%, 2.16%, and 1.92% across all benchmarks. Notably, IFMatch (Fix) outperforms FixMatch by 5.21% and 2.32% on CIFAR-10-LT (\u03b3=150) and CIFAR-100-LT (\u03b3=100), highlighting the robustness of the proposed paradigm when facing severe imbalance. Moreover, the performance gap between dynamic-threshold-based methods and FixMatch substantially narrows within our paradigm. Accordingly, the proposed approach promotes the utilization of unlabeled data, reducing reliance on complex threshold mechanisms. Overall, the impressive performance observed in imbalanced SSL suggests the potential of our approach to be deployed in real-world applications."}, {"title": "4.3 Component Analysis", "content": "This section presents the contribution of each component within the proposed paradigm. Unless otherwise stated, we conduct experiments on IFMatch (Fix). Additionally, we provide running speed analysis, feature visualization results [24], and performance on semi-supervised semantic segmentation in the Appendix. The subsequent analysis presents the revealed findings.\nThe proposed approach to integrating image-level and feature-level perturbations yields optimal performance. As the simplest idea, we can integrate \\(A_F^s\\) and \\(A^s\\) within one branch to establish a dual-branch structure similar to the old paradigm. Nonetheless, such a toy design engenders disruptive perturbations that exceed acceptable augmentation levels for consistency regulation, resulting in the performance degradation presented in lines 1-2 of Tab. 3. An alternative approach, proposed by UniMatch [40], combines \\(A_F^s\\) and \\(A^s\\) in separate branches, each employing a distinct type of perturbation (line 3). Despite its effectiveness, this method fails to boost interactions between the two types of perturbations. Accordingly, we attempt to achieve the co-occurrence of \\(A_F^s\\) and \\(A^s\\) within one branch, thereby constructing a more comprehensive augmentation space. The proposed paradigm adopts a milder combination strategy, specifically using \\(A_F^s + A_l^w\\) and \\(A_F^w + A_l^s\\) in the two student branches respectively. However, as shown in line 4 of Tab. 3, the introduction of \\(A_F^w\\) leads to a performance drop compared to UniMatch, probably due to the derived difficulties in recognizing hard samples. To address this issue, we devise a confidence-based identification strategy to distinguish between naive and challenging samples, posing additional challenges exclusively for naive samples. Finally, the proposed paradigm (IFMatch), as presented in line 5, achieves the optimal performance among all candidate designs for integrating \\(A_F^s\\) and \\(A^s\\), demonstrating its effectiveness and advantages.\nAll feature-level perturbation strategies contribute to the overall performance. As shown in Tab. 4, we conduct an ablation study on feature-level perturbation strategies. The baseline model in line 1 integrates all proposed strategies and achieves state-of-the-art performance. In subsequent configurations (line 2-6), we sequentially remove each strategy. Compared to the baseline, the absence of any proposed perturbation technique leads to a decline in performance on both datasets. Therefore, the proposed perturbation strategies exhibit a synergistic effect, collaboratively exploring the feature perturbation space.\nFeature-level perturbation \\(A_F\\) favors a high constant threshold over a dynamic one. As detailed in Sec. 3.3, \\(A_F\\) and \\(A^s\\) may require different threshold mechanisms due to their distinct characteristics. Considering the central role of \\(A_F\\) (\\(A_F^s\\)) in the first student branch, we further delve into the threshold strategy best suited for it. In general, existing threshold mechanisms can be categorized into constant thresholds and dynamic thresholds. The former prioritizes high pseudo-label accuracy at the expense of diminished utilization of unlabeled data, while the latter performs conversely. Accordingly, we determine the optimal threshold for the first branch from two candidates: a dynamic threshold mirroring the one used in the second branch and the constant threshold used in FixMatch. Combining Tab. 5 and Tab. 1, the proposed paradigm consistently outperforms the old paradigm regardless of the threshold adopted in the first branch. Additionally, the constant threshold beats all dynamic threshold mechanisms, suggesting that feature-level perturbation \\(A_F\\) is better suited to a constant threshold of 0.95, rather than existing dynamic threshold mechanisms.\nIFMatch outperforms previous methods from the perspective of the naive sample ratio. Firstly, as shown in the left part of Fig. 4, our approach (orange) achieves a lower naive sample ratio than UniMatch (green). This suggests more effective utilization of unlabeled data in our paradigm, primarily benefiting from combining both types of perturbation within each branch. Secondly, we compare the confidence-based identification strategy and SAA [12] within our paradigm to discern a better naive sample identification method. Specifically, SAA utilizes loss values as the criteria, recording historical loss for each sample and performing naive-challenging division by OTSU [28]. As shown in Tab. 6, the proposed module consistently outperforms SAA. Additionally, the naive sample ratio presented in the left part of Fig. 4 indicates that SAA (blue) tends to produce identification biased toward naive samples (e.g. starting with a ratio of 0.5 in the first iteration). Furthermore, we analyze the identification process of both strategies in the later training stages, as shown in the right part of Fig. 4. Notably, when employing the cross-entropy loss, the loss threshold of 1.79 in SAA corresponds to a confidence threshold of 0.17 (computed as e-1.79), which is a detrimental configuration. The limitations of SAA can be ascribed to the ill-suited nature of the OTSU technique for segmenting a histogram with a monotonically decreasing trend. In contrast, our strategy remains effective in distinguishing between naive and challenging samples even if the unlabeled set is predominantly composed of naive samples.\nVisualization analysis. We present the visualization for image-level and feature-level perturbations in Fig. 5, where the four parts (from left to right) correspond to the raw images, the images subjected to \\(A^s\\), and the feature maps that undergo \\(A_F^w \\circ A_l^s\\) and \\(A_F^s \\circ A_l^w\\). We can observe that the exclusive reliance on \\(A^s\\) engenders easy-to-recognize (naive) samples. In contrast, the proposed paradigm effectively combines \\(A^s\\) and \\(A_F\\) to expand the augmentation space. For additional visualization results, please refer to Appendix Sec.2."}, {"title": "5 Conclusion", "content": "This paper introduces feature-level perturbation to expand the augmentation space of the conventional semi-supervised learning framework, establishing the image-feature weak-to-strong consistency paradigm (IFMatch). To achieve effective feature-level perturbation, we propose refined designs that consider both perturbation position and strategy, facilitating diversity in terms of intensity and form. Furthermore, our paradigm develops two parallel student branches to seamlessly integrate the two types of perturbations and comprehensively explore the augmentation space. Additionally, we devise a confidence-based identification strategy to distinguish between naive and challenging samples, posing additional challenges exclusively for naive samples. Extensive experiments are conducted on multiple benchmarks, including both balanced and imbalanced label distributions. The experimental results demonstrate the effectiveness of our approach."}, {"title": "A Feature-Level Perturbation Strategies", "content": "As mentioned in Sec.3.2 of the main paper, we develop a series of feature-level perturbation strategies from different perspectives, including 'dropout' (comprising channel-wise dropout and spatial-wise dropout), \u2018movement' (encompassing translation and shearing in both the X-axis and Y-axis), and 'value' (involving the weighted sum of the smoothed and input feature maps). In this section, we will elaborate on the implementation of these strategies. For the sake of simplicity, we maintain a consistent notation for all perturbation strategies.\nwhere \\(f^{in}\\) and \\(f^{out}\\) denote the input and output feature maps, \\(\\Gamma\\) stands for the perturbation operation, C represents the number of channels, H and W signifies the height and width of the feature maps.", "A.1 Channel-Wise Dropout": "This perturbation strategy applies channel-wise dropout with a drop probability of 0.5 to hidden representations. Mathematically, the feature-level perturbation using channel-wise dropout can be defined as follows.", "A.2 Spatial-Wise Dropout": "Spatial-wise dropout discards a randomly selected rectangle region across all channels, similar to the Cutout operation applied to raw images. To determine the dropped spatial region, the strategy performs the following steps. Firstly, we compute the height \\(h_a\\) and width \\(w_a\\) of the dropped region by rescaling the size of input feature maps. Subsequently, we randomly select the upper left point (x, y) of the dropped area. With the size and the location of its top-left point in place, we can uniquely define the dropped area. Mathematically speaking, the above process can be expressed as follows.\n\nIn all experiments, we set a to 0.5. Once the dropped spatial region is determined, we generate a 0-1 mask denoted as \\(m \\in \\mathbb{R}^{H \\times W}\\) and implement spatial-wise dropout by multiplying the mask with the input feature maps. Formally, the output feature maps can be computed as follows.", "A.3 Translation": "The feature-level translation perturbation randomly selects a direction from the candidate set (up, down, left, and right) with equal probability and translates the input feature map along the determined direction. The length of the translation path denoted as \\(l\\) can be computed by rescaling the size of the input feature maps. Specifically, we first draw a random factor \\(a\\) from a uniform distribution ranging from 0 to \\(a_{max}\\) and then compute the translation distance. Taking the translation along the X-axis as an example, the above process can be mathematically described as follows.\n\nThroughout all experiments, we set \\(a_{max}\\) to 0.5. Given the translation direction and distance, the strategy assigns the corresponding value from the input feature maps to the output feature maps. Furthermore, to address the areas left vacant due to the translation operation, we utilize the average value outside the legal region as padding, thereby avoiding significant information loss and ensuring numerical stability. For a more intuitive understanding of the translation operation, please refer to the toy example presented in Fig. 6.", "A.4 Shearing": "Similar to translation, the shearing operation randomly selects one direction from a set of candidate directions, each with equal probability. The feature map is then sheared along the chosen direction. The length of the shearing path denoted as \\(l\\) can be calculated by rescaling the size of the input feature maps, and intermediate distances can be obtained by linear interpolation. Specifically, we first draw a random factor \\(a\\) from a uniform distribution between 0 and \\(a_{max}\\) and then compute the shearing distance. Taking the shearing operation in the X-axis as an illustrative example, the above process can be expressed as follows.\n\nIn all experiments, we set \\(a_{max}\\) to 1.0. Subsequently, the strategy assigns corresponding values from the input feature maps to the output feature maps and employs the average value of the illegal region to pad the missing areas. Fig. 7 provides an example of the shearing operation.", "A.5 Value Modification": "The value modification operation aims to change the feature value while preserving the relationship within each feature map. To achieve this goal, we first employ a random-sized group convolution to facilitate local smoothing in each feature map. The random-sized configuration allows for various degrees of smoothness, thus comprehensively exploring the feature perturbation space. The size of the kernel k is randomly sampled from the set of all odd numbers within the range of 3 to min(H, W) with equal probability. Subsequently, we adopt the average smoothing convolution to encode the input feature maps. Finally, we draw a random factor a from a uniform distribution between \\(a_{min}\\) and \\(a_{max}\\) and use the weighted sum between the input and smoothed feature maps as the perturbation results. Mathematically speaking, the above process can be expressed as follows.\n\nwhere * denotes the convolution operation. In all experiments, we set \\(a_{min}\\) and \\(a_{max}\\) to 0.50 and 0.95, respectively.", "A.6 Conclusion for Strategies": "The aforementioned perturbation strategies consider three distinct perspectives ('dropout', 'movement', and 'value'), thereby achieving diverse perturbation forms and providing a collective exploration of the feature perturbation space. Moreover, the parameter settings for each perturbation strategy are relatively simple. For example, the first four perturbation strategies all involve one single hyperparameter, and we set it to 0.5 on the first three, indicating satisfactory consistency. Note that the configuration setting \\(a_{max}\\) to 1.0 in the shearing operation leads to the same upper bound (half the size of the feature map) for the missing area as the translation operation. Additionally, the value modification operation introduces two hyperparameters, \\(a_{min}\\) and \\(a_{max}\\), and our settings keep consistent with the counterparts in strong image-level perturbation."}, {"title": "B Visualization Analysis", "content": "To diagnose the proposed paradigm in a more intuitive manner, we visualize the high-dimensional features on STL-10 with 40 labeled samples using T-SNE. The results are presented in Fig. 8, wherein the first two and last two columns respectively depict the performance of algorithms following the old and proposed paradigms. To elaborate, algorithms adhering to the conventional approach typically generate loose and adjacent feature clusters. In contrast, the proposed paradigm achieves more separable and tightly clustered features, indicating the effectiveness of feature-level perturbation. Moreover, traditional algorithms often overfit noisy pseudo-labels, while the proposed approach positions most ambiguous samples near the decision boundary, minimizing their impacts on the model. Overall, the proposed image-level weak-to-strong consistency paradigm (IFMatch) generates easy-to-distinguish features, laying a solid foundation for a robust classifier.", "B.1 High-Dimensional Features": null, "B.2 Image-Level Perturbation and Feature-Level Perturbation": "We present the visualization for image"}]}