{"title": "Image-Feature Weak-to-Strong Consistency: An Enhanced Paradigm for Semi-Supervised Learning", "authors": ["Zhiyu Wu", "Jinshi Cui"], "abstract": "Image-level weak-to-strong consistency serves as the predominant paradigm in semi-supervised learning (SSL) due to its simplicity and impressive performance. Nonetheless, this approach confines all perturbations to the image level and suffers from the excessive presence of naive samples, thus necessitating further improvement. In this paper, we introduce feature-level perturbation with varying intensities and forms to expand the augmentation space, establishing the image-feature weak-to-strong consistency paradigm. Furthermore, our paradigm develops a triple-branch structure, which facilitates interactions between both types of perturbations within one branch to boost their synergy. Additionally, we present a confidence-based identification strategy to distinguish between naive and challenging samples, thus introducing additional challenges exclusively for naive samples. Notably, our paradigm can seamlessly integrate with existing SSL methods. We apply the proposed paradigm to several representative algorithms and conduct experiments on multiple benchmarks, including both balanced and imbalanced distributions for labeled samples. The results demonstrate a significant enhancement in the performance of existing SSL algorithms.", "sections": [{"title": "1 Introduction", "content": "Semi-supervised learning (SSL) [4,30,32,46], a research topic that aims to reduce manual labeling costs by capitalizing on unlabeled data, has received considerable attention in recent years. Among the proposed methods, the image-level weak-to-strong consistency paradigm introduced by FixMatch [32] has proven highly effective in leveraging the potential of unlabeled data. Specifically, FixMatch applies varying intensities of image-level perturbations to raw samples and encourages consistent predictions between corresponding weakly and strongly augmented views. Subsequent studies have refined this paradigm primarily in two aspects: (1) improving the utilization ratio of unlabeled data by replacing"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Image-Level Weak-to-Strong Consistency", "content": "We begin by reviewing the image-level weak-to-strong consistency paradigm, as depicted in the left part of Fig. 1. Let $D_l = \\{(x_i, y_i)\\}_{i=1}^{N_l}$ and $D_u = \\{u_i\\}_{i=1}^{N_u}$ represent the labeled and unlabeled datasets, respectively. Here, $x_i$ and $u_i$ denote the labeled and unlabeled training samples, and $y_i$ refers to the one-hot label for the labeled sample $x_i$. We denote the prediction of sample $x$ as $p(y|x)$. Given a batch of labeled and unlabeled data, the model is optimized with the objective $L = L_s + \\lambda_uL_u$. Specifically, $L_s$ indicates the cross-entropy loss ($H$) for the labeled batch of size $B_L$.\n\n$L_s = \\frac{1}{B_L} \\sum_{i=1}^{B_L} H(y_i, p(y|x_i))$\n\nOn the other hand, $L_u$ signifies the image-level consistency regulation between the prediction of the strongly augmented view $A^s(u)$ and the pseudo-label"}, {"title": "3.2 Feature-Level Perturbation", "content": "To avoid the exclusive reliance on image-level perturbation, this paper introduces feature-level perturbation varied in intensity and form, thereby expanding the augmentation space. In the following parts, we will elaborate on the implementation of feature-level perturbation regarding its position and strategy.\nPosition refers to where the feature-level perturbation is intro-duced. Given that WideResNet [41] is the prevailing backbone in SSL, we employ its basic component, the residual block [13] with pre-activation, as an illustrative example. Building upon previous studies that exploit feature-level regularizers directly after the convolution [14,15,34], we identify two positions for introducing feature-level perturbation, as depicted in Fig. 2. In position A, the method perturbs the summation of the residual and identity connections, i.e., the output of the residual block. As position A acts as the bottleneck for feedforward, perturbation at position A exhibits high intensity and is therefore regarded as strong feature-level perturbation $AF_s$. Additionally, position B perturbs the output of a random convolution within the residual component, which is relatively milder than position A. Therefore, perturbation at position B is considered as weak feature-level perturbation $AF_w$. It is worth noting that both"}, {"title": "3.3 Image-Feature Weak-to-Strong Consistency", "content": "Building upon well-designed image-level and feature-level perturbations, we can integrate them to achieve consistency regulation in both dimensions. Intuitively, a trivial approach is combining strong image-level and feature-level perturbations in one branch to construct a dual-branch structure similar to the old paradigm. However, such a toy paradigm engenders destructive perturbations according to our pilot study (Tab. 3). Accordingly, we integrate these two forms of perturbations in a more moderate manner, developing the image-feature weak-to-strong consistency paradigm (IFMatch) depicted in the right part of Fig. 1.\nOur paradigm retains the traditional teacher branch, utilizing predictions $p^w$ of samples subjected to weak image-level perturbation as pseudo-labels. Furthermore, we introduce two student branches to integrate both types of perturbations. Mathematically, given a batch of labeled and unlabeled data, the model is optimized using the objective $L = L_s + \\lambda_u(L_{u1} + L_{u2})$, where $L_s$ denotes the cross-entropy loss for labeled data, and $L_{u1}$ and $L_{u2}$ represent the consistency regulation for unlabeled data in the two student branches respectively.\nIn the first student branch, we integrate weak image-level perturbation $A^w$ and strong feature-level perturbation $AF_s$ to comprehensively explore the feature perturbation space. Furthermore, we employ the constant threshold $\\tau$ adopted in FixMatch to filter out incorrect pseudo-labels. This choice arises from the observation that existing dynamic threshold mechanisms are less suitable for $AF_s$, especially in challenging tasks. Thus, the unsupervised loss $L_{u1}$ in the first student branch can be expressed as follows.\n\n$L_{u1} = \\frac{1}{B_u} \\sum_{i=1}^{B_u} \\mathbb{1}(\\max(p_i^w) \\ge \\tau) H(p_i^s, p_i^{w, F_s})$"}, {"title": "3.4 Confidence-Based Identification", "content": "As analyzed in Sec. 3.3, the combination of strong image-level perturbation $A^s$ and weak feature-level perturbation $AF_w$ poses supplementary challenges for naive samples yet incurs exceeding difficulty for hard ones. Likewise, previous research has encountered similar challenges in expanding the augmentation space. For example, UniMatch [40] observes a performance degradation caused by the concatenation of channel-wise dropout and $A^s$ and finally denies the feasibility of combining $A^s$ and $AF_w$ within one branch. Besides, [12] imposes more aggressive image-level perturbations exclusively on naive samples. To distinguish between naive and challenging samples, their approach records sample-wise loss and utilizes the OTSU [28] method for distinction. However, OTSU is conventionally applied to the segmentation of bimodal gray-scale images, whereas the histogram of loss values typically exhibits a descending trend. As shown in Fig. 4, their approach tends to produce identification biased toward naive samples.\nTo effectively identify the naive samples, we propose a confidence-based identification strategy, which is depicted in Fig. 3. Similar to the threshold-based rule for pseudo-label filtering that only predictions exceeding the confidence threshold qualify as pseudo-labels, the proposed strategy can be succinctly concluded as: \u2018only strongly augmented samples $A^s(u)$ with target confidence exceeding the threshold are considered naive and necessitate additional weak feature-level perturbation $AF_w$\u2019. Specifically, the approach comprises two steps. Firstly, we record the confidence linked to the pseudo-label within the predicted distribution of the second branch. Mathematically, the target confidence $h_i$ for sample $u_i$ in the second student branch can be obtained as follows.\n\n$h_i = p_i^{s, F_w} (j = p_i^w)$\n\nIt is important to clarify that $p_i^{s, F_w}$ solely represents the prediction of sample $u_i$ in the second branch and has nothing to do with the introduction of $AF_w$ for $u_i$. Secondly, the algorithm generates a binary mask for each sample to determine whether to introduce additional $AF_w$ by comparing the target confidence with the threshold $\\tau_t$ in the second student branch. Consequently, the mask for sample $u_i$, denoted as $M_i$, and the perturbations applied to $u_i$ in the second student"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Balanced Semi-Supervised Learning", "content": "Settings. We conduct experiments on multiple benchmarks, including CIFAR-10/100 [19], SVHN [26], STL-10 [6], and ImageNet [8], with various numbers of labeled samples. In the context of balanced SSL, we maintain a balanced distribution for labeled samples. The choice of backbone architecture adheres to established practices, employing specific models for different datasets: WRN-28-2 [41] for CIFAR-10 and SVHN, WRN-28-8 for CIFAR-100, WRN-37-2 [45] for STL-10, and ResNet-50 [13] for ImageNet. As for training parameters, we follow the unified codebase TorchSSL [42] to ensure fair comparisons. Specifically, batch"}, {"title": "4.2 Imbalanced Semi-Supervised Learning", "content": "Settings. We evaluate the proposed paradigm in the context of imbalanced SSL, where both labeled and unlabeled data exhibit a long-tailed distribution. Consistent with prior studies [5,10,21,23,27,37], we construct labeled and unlabeled sets using the configurations of $N_c = N_1 \\cdot \\gamma^{-\\frac{i}{I}}$ and $M_c = M_1 \\cdot \\gamma^{-\\frac{i}{I}}$. Specifically, for CIFAR-10-LT, we set $N_1$ to 1500, $M_1$ to 3000, and $\\gamma$ to range from 50 to 150. Moreover, for CIFAR-100-LT, we set $N_1$ to 150, $M_1$ to 300, and $\\gamma$ to span from 20 to 100. In all experiments, we employ WRN-28-2 as the backbone and utilize the Adam optimizer [17] with a weight decay of 4e-5. Besides, batch sizes $B_L$ and $B_u$ are fixed as 64 and 128 respectively. Additionally, the learn-ing rate is initially set to 2e-3 and adjusted by a cosine decay scheduler during"}, {"title": "4.3 Component Analysis", "content": "This section presents the contribution of each component within the proposed paradigm. Unless otherwise stated, we conduct experiments on IFMatch (Fix). Additionally, we provide running speed analysis, feature visualization results [24], and performance on semi-supervised semantic segmentation in the Appendix.\nThe subsequent analysis presents the revealed findings.\nThe proposed approach to integrating image-level and feature-level perturbations yields optimal performance. As the simplest idea, we can integrate $AF_s$ and $A^s$ within one branch to establish a dual-branch structure similar to the old paradigm. Nonetheless, such a toy design engenders disruptive perturbations that exceed acceptable augmentation levels for consistency regulation, resulting in the performance degradation presented in lines 1-2 of Tab. 3. An alternative approach, proposed by UniMatch [40], combines $AF_s$ and $A^s$ in separate branches, each employing a distinct type of perturbation (line 3)."}, {"title": "5 Conclusion", "content": "This paper introduces feature-level perturbation to expand the augmentation space of the conventional semi-supervised learning framework, establishing the image-feature weak-to-strong consistency paradigm (IFMatch). To achieve effective feature-level perturbation, we propose refined designs that consider both perturbation position and strategy, facilitating diversity in terms of intensity and form. Furthermore, our paradigm develops two parallel student branches to seamlessly integrate the two types of perturbations and comprehensively explore the augmentation space. Additionally, we devise a confidence-based identification strategy to distinguish between naive and challenging samples, posing additional challenges exclusively for naive samples. Extensive experiments are conducted on multiple benchmarks, including both balanced and imbalanced label distributions. The experimental results demonstrate the effectiveness of our approach."}, {"title": "A Feature-Level Perturbation Strategies", "content": "As mentioned in Sec.3.2 of the main paper, we develop a series of feature-level perturbation strategies from different perspectives, including 'dropout' (comprising channel-wise dropout and spatial-wise dropout), \u2018movement' (encompassing translation and shearing in both the X-axis and Y-axis), and 'value' (involving the weighted sum of the smoothed and input feature maps). In this section, we will elaborate on the implementation of these strategies. For the sake of simplicity, we maintain a consistent notation for all perturbation strategies.\n\n$f^{out} = \\Phi(f^{in})$\n\n$f^{out}, f^{in} \\in \\mathbb{R}^{C \\times H \\times W}$\n\nwhere $f^{in}$ and $f^{out}$ denote the input and output feature maps, $\\Phi$ stands for the perturbation operation, $C$ represents the number of channels, $H$ and $W$ signifies the height and width of the feature maps."}, {"title": "A.1 Channel-Wise Dropout", "content": "This perturbation strategy applies channel-wise dropout with a drop probability of 0.5 to hidden representations. Mathematically, the feature-level perturbation using channel-wise dropout can be defined as follows.\n\n$f^{out} = torch.nn.Dropout2d(p = 0.5)(f^{in})$"}, {"title": "A.2 Spatial-Wise Dropout", "content": "Spatial-wise dropout discards a randomly selected rectangle region across all channels, similar to the Cutout operation applied to raw images. To determine the dropped spatial region, the strategy performs the following steps. Firstly, we compute the height $h_a$ and width $w_a$ of the dropped region by rescaling the size of input feature maps. Subsequently, we randomly select the upper left point (x, y) of the dropped area. With the size and the location of its top-left point in place, we can uniquely define the dropped area. Mathematically speaking, the above process can be expressed as follows.\n\n$h, w = int(a \\times H), int(a \\times W)$\n\n$x, y = U[0, H \u2013 h], U[0, W \u2013 w]$\n\nIn all experiments, we set a to 0.5. Once the dropped spatial region is determined, we generate a 0-1 mask denoted as $m \\in \\mathbb{R}^{H \\times W}$ and implement spatial-wise dropout by multiplying the mask with the input feature maps. Formally, the output feature maps can be computed as follows.\n\n$\\mathbb{0} if (i, j) \\in Rectangle(x, y, h, w)$\nm(i, j)=\n$1 otherwise$\n\n$f^{out} = f^{in} \\odot m$"}, {"title": "A.3 Translation", "content": "The feature-level translation perturbation randomly selects a direction from the candidate set (up, down, left, and right) with equal probability and translates the input feature map along the determined direction. The length of the translation path denoted as $l$ can be computed by rescaling the size of the input feature maps. Specifically, we first draw a random factor $a$ from a uniform distribution ranging from 0 to $a_{max}$ and then compute the translation distance. Taking the translation along the X-axis as an example, the above process can be mathematically described as follows.\n\n$a \\sim U[0, a_{max}]$\n\n$l = int(a \\times W)$\n\nThroughout all experiments, we set $a_{max}$ to 0.5. Given the translation direction and distance, the strategy assigns the corresponding value from the input fea-ture maps to the output feature maps. Furthermore, to address the areas left vacant due to the translation operation, we utilize the average value outside the legal region as padding, thereby avoiding significant information loss and en-suring numerical stability. For a more intuitive understanding of the translation operation, please refer to the toy example presented in Fig. 6."}, {"title": "A.4 Shearing", "content": "Similar to translation, the shearing operation randomly selects one direction from a set of candidate directions, each with equal probability. The feature map is then sheared along the chosen direction. The length of the shearing path denoted as $l$ can be calculated by rescaling the size of the input feature maps, and intermediate distances can be obtained by linear interpolation. Specifically, we first draw a random factor $a$ from a uniform distribution between 0 and $a_{max}$ and then compute the shearing distance. Taking the shearing operation in the X-axis as an illustrative example, the above process can be expressed as follows.\n\n$a \\sim U[0, a_{max}]$\n\n$l = int(a \\times W)$\n\n$[l_1, ..., l_w] = torch.linspace(0, l, W)$\n\nIn all experiments, we set $a_{max}$ to 1.0. Subsequently, the strategy assigns corresponding values from the input feature maps to the output feature maps and employs the average value of the illegal region to pad the missing areas. Fig. 7 provides an example of the shearing operation."}, {"title": "A.5 Value Modification", "content": "The value modification operation aims to change the feature value while preserving the relationship within each feature map. To achieve this goal, we first employ a random-sized group convolution to facilitate local smoothing in each feature map. The random-sized configuration allows for various degrees of smoothness, thus comprehensively exploring the feature perturbation space. The size of the kernel $k$ is randomly sampled from the set of all odd numbers within the range of 3 to min(H, W) with equal probability. Subsequently, we adopt the average smoothing convolution to encode the input feature maps. Finally, we draw a random factor $a$ from a uniform distribution between $a_{min}$ and $a_{max}$ and use the weighted sum between the input and smoothed feature maps as the perturbation results. Mathematically speaking, the above process can be expressed as follows.\n\n$k \\sim \\{U[3, min(H, W)], odd number\\}$\n\n$K[i, j] = \\frac{1}{k^2}$\n\n$a \\sim U[a_{min}, a_{max}]$\n\n$f^{out} = a \\times (K * f^{in}) + (1 \u2212 a) \\times f^{in}$\n\nwhere * denotes the convolution operation. In all experiments, we set $a_{min}$ and $a_{max}$ to 0.50 and 0.95, respectively."}, {"title": "A.6 Conclusion for Strategies", "content": "The aforementioned perturbation strategies consider three distinct perspectives ('dropout', 'movement', and 'value'), thereby achieving diverse perturbation forms and providing a collective exploration of the feature perturbation space. Moreover, the parameter settings for each perturbation strategy are relatively simple. For example, the first four perturbation strategies all involve one single hyperparameter, and we set it to 0.5 on the first three, indicating satisfactory consistency. Note that the configuration setting amax to 1.0 in the shearing operation leads to the same upper bound (half the size of the feature map) for the missing area as the translation operation. Additionally, the value modification operation introduces two hyperparameters, amin and amax, and our settings keep consistent with the counterparts in strong image-level perturbation."}, {"title": "B Visualization Analysis", "content": ""}, {"title": "B.1 High-Dimensional Features", "content": "To diagnose the proposed paradigm in a more intuitive manner, we visualize the high-dimensional features on STL-10 with 40 labeled samples using T-SNE. The results are presented in Fig. 8, wherein the first two and last two columns respectively depict the performance of algorithms following the old and proposed paradigms. To elaborate, algorithms adhering to the conventional approach typically generate loose and adjacent feature clusters. In contrast, the proposed paradigm achieves more separable and tightly clustered features, indicating the effectiveness of feature-level perturbation. Moreover, traditional algorithms often overfit noisy pseudo-labels, while the proposed approach positions most ambiguous samples near the decision boundary, minimizing their impacts on the model. Overall, the proposed image-level weak-to-strong consistency paradigm (IFMatch) generates easy-to-distinguish features, laying a solid foundation for a robust classifier."}, {"title": "B.2 Image-Level Perturbation and Feature-Level Perturbation", "content": "We present the visualization for image-level and feature-level perturbations in Fig. 9, which serves as the supplement for the counterpart in the main paper. The four parts (from left to right) in Fig. 9 correspond to the raw images, the images subjected to $A^s$, and the feature maps that undergo $AF_w \\circ A^s$ and $AF_s \\circ A^w$, respectively. As we can observe, a significant proportion of samples in the second part can be effortlessly classified, indicating the naive sample issue caused by the exclusive reliance on $A^s$. In contrast, the combination of $A^s$ and $AF$ proposed by our paradigm effectively expand the perturbation space, thereby boosting the utilization of unlabeled samples."}, {"title": "C Running Speed Analysis", "content": "The proposed paradigm demonstrates substantial performance promotion when compared to the traditional approach. However, the obtained improvement is accompanied by feature-level perturbation and a triple-branch structure, inevitably resulting in a reduction in training speed. Consequently, we conduct a running"}, {"title": "D Performance on Semi-Supervised Semantic Segmentation", "content": "In addition to semi-supervised classification, we also test the performance of the proposed paradigm on the semi-supervised semantic segmentation task. As shown in Tab. 8, IFMatch (Fix) consistently outperforms U2PL and UniMatch with varying numbers of labeled samples. The impressive performance on different tasks demonstrates the effectiveness and generalization capability of our approach."}, {"title": "E Implementation Details", "content": "The detailed training settings for balanced and imbalanced semi-supervised learning are presented in Table 9 and Table 10, respectively."}]}