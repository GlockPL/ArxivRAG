{"title": "SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies\nUsing Gaussian Splatting", "authors": ["M. Nomaan Qureshi", "Sparsh Garg", "Francisco Yandun", "David Held", "George Kantor", "Abhisesh Silwal"], "abstract": "Abstract\u2014Sim2Real transfer, particularly for manipulation\npolicies relying on RGB images, remains a critical challenge in\nrobotics due to the significant domain shift between synthetic\nand real-world visual data. In this paper, we propose SplatSim,\na novel framework that leverages Gaussian Splatting as the pri-\nmary rendering primitive to reduce the Sim2Real gap for RGB-\nbased manipulation policies. By replacing traditional mesh\nrepresentations with Gaussian Splats in simulators, SplatSim\nproduces highly photorealistic synthetic data while maintaining\nthe scalability and cost-efficiency of simulation. We demonstrate\nthe effectiveness of our framework by training manipulation\npolicies within SplatSim and deploying them in the real world\nin a zero-shot manner, achieving an average success rate of\n86.25%, compared to 97.5% for policies trained on real-world\ndata.", "sections": [{"title": "I. INTRODUCTION", "content": "The Sim2Real problem, a focal challenge in robotics, per-\ntains to the transfer of control policies learned in simulated\nenvironments to real world settings. Recently, significant\nprogress has been made in deploying controllers trained in\nsimulation to the real world in a zero-shot manner. Robots\nhave demonstrated the ability to walk on rough terrains [1],\n[2], perform in-hand object rotation [3], [4], [5], [6], and\ngrasp previously unseen objects [7]. Notably, all of these\nmethods rely on perception modalities like depth, tactile\nsensing, or point cloud inputs, which have gained significant\nattention due to the relatively small Sim2Real gap they offer.\nThe reduced discrepancy between simulated and real-world\ndata in these modalities has led to remarkable progress,\nreinforcing the idea that modalities that can be simulated\nwell, can be transferred well.\nIn contrast, RGB images are rarely used as the primary\nsensing modality in robot learning applications. over other\ncommonly used modalities in Sim2Real transfer. They cap-\nture crucial visual details such as color, texture, lighting,\nand surface reflectivity, to name a few, which are essential\nfor understanding complex environments. For instance, in a\nsimple task of plucking ripe fruits, color is a key feature\nfor determining ripeness an inference straightforward in\nRGB space but difficult and impractical with depth or tactile\ninputs. Additionally, RGB images are easy to acquire in\nreal-world environments with cameras and align closely with\nhuman perception, making them well-suited for interpreting\nintricate details in dynamic and complex scenes.\nBut why has it been difficult to deploy policies trained\nin simulation with RGB information to the real world? The\nproblem lies in the fact that the distribution of images the\nrobot observes in the simulator is very different from the\ndistribution of images it would see in the real world. This\nmakes \"vision Sim2Real an out-of-domain generalization\nproblem\" [9], a fundamental challenge in machine learning\nthat is still unsolved. For this reason, policies trained on\nsimulated images often struggle when applied to distributions\nof real-world images.\nIn this paper, we propose a systematic and novel method\nto reduce the Sim2Real gap for RGB images, by leveraging\nGaussian Splatting [8] as a photorealistic render, using\nexisting simulators as the physics backbone. We propose\nutilizing Gaussian Splatting [8] as the primary rendering\nprimitive, replacing traditional mesh-based representations\nin existing simulators, to significantly improve the photo-\nrealism of rendered scenes. By integrating these renderings\nof simulated demonstrations with state-of-the-art behavior\ncloning techniques, we introduce a framework for zero-\nshot transfer of manipulation policies trained entirely on\nsimulation data, to the real world. Our key contributions are\nas follows:\n\u2022\n\u2022\nWe propose a novel and scalable data generation frame-\nwork, \"SplatSim\" for manipulation tasks. SplatSim is\nfocused predominantly on bridging the vision Sim2Real\ngap by leveraging photorealistic renderings generated\nthrough Gaussian Splatting, replacing traditional mesh\nrepresentation in the rendering pipeline of the simulator.\nWe show how to leverage Robot Splat Models and Ob-"}, {"title": "II. RELATED WORKS", "content": "A. Sim2real\nRobotics simulation tools like [10], [11], [12], [13], [14],\n[15] have become invaluable in scaling up robot learning\ndue to several advantages including parallelization, cost and\ntime efficiency, and safety. Recent advancements in trans-\nferring learned policies from simulation to the real world\nhave demonstrated impressive results, particularly in domains\nthat leverage modalities with a low Sim2Real gap, such\nas depth, point cloud, proprioception, or tactile feedback.\nThese modalities have enabled robots to perform contact-rich\ntasks like quadruped locomotion [1], [16], [2], [17], [18] and\nbipedal locomotion [19], [20], dexterous manipulation [3],\n[4], [5], [6], [7], manipulation of articulated objects [21],\n[22], among others [23], [24], [25], [26].\nHowever, Sim2Real transfer for RGB-based manipulation\npolicies remains challenging. Previous attempts in this do-\nmain have primarily focused on tasks like navigation [27],\nwhere high-fidelity collision meshes from scans [28], [29]\ncan enhance visual realism but fail to capture dynamic\ninteractions with the environment. Furthermore, existing ap-\nproaches such as domain adaptation [30], often rely on exten-\nsive offline data collection of real-world object interactions.\nIn this work, we address the challenge of transferring\nRGB-based policies for manipulation tasks, which require\nrendering complex interactions between objects and the\nrobot. Our method requires only an initial video of the static\nscene without the need for additional real-world data collec-\ntion. A work notably related to ours is RialTo [31] which\nuses a Real2Sim2Real approach similar to ours. However,\ntheir policy is still trained on point clouds, which requires\ndepth during execution time. In contrast, SplatSim only uses\nRGB images for learning and policy deployment. Another\nrecent work Maniwhere [32] does large-scale reinforcement\nlearning in simulation and shows generalization to the real\nworld, however, their method still requires depth at test time\nand cannot work with just RGB images in the real world.\nB. Gaussian Splatting for Robotics\nGaussian Splatting [8] is a state-of-the-art rendering tech-\nnique that models scenes using 3D Gaussian primitives,\noffering an efficient and photorealistic representation of"}, {"title": "III. PRELIMINARY", "content": "A. Rigid Body Transformations in Gaussian Splatting\nIn Gaussian Splatting, segmented objects within a scene\ncan undergo rigid body transformations, such as translation\nand rotation, while still maintaining high-quality renderings.\nEach object, represented by a set of 3D Gaussians, can\nbe transformed using a homogeneous transformation matrix\nT, defined by the rotation R and translation t. For a 3D\nGaussian with mean position \u03bc and covariance matrix \u03a3,\nthe transformed position \u03bc' and covariance \u03a3' under the rigid\ntransformation are given by:\n\u03bc' = R\u03bc + t\n\u03a3' = R\u03a3RT\nApplying these transformations updates the position and\norientation of the 3D Gaussians of the object while preserv-\ning the corresponding geometric properties. Despite making\nthese changes in the object's configuration, the Gaussian\nrepresentation enables smooth and accurate renderings."}, {"title": "IV. METHOD", "content": "The key premise of our method is that if each rigid\nbody in the Gaussian Splat representation of the real-world\nscene can be accurately segmented, and its corresponding\nhomogeneous transformation relative to the simulator is\nidentified, then it becomes feasible to render the rigid body\nin novel poses. The rigid bodies can include links of the\nrobot, links of the gripper, articulated objects, or simple non-\ndeformable objects. By applying this process to all rigid\nbodies interacting with the robot in simulation, we can\ngenerate photorealistic renderings for an entire demonstration\ntrajectory. This approach is analogous to traditional rendering\nin simulators; however, instead of using mesh primitives, we\nutilize Gaussian Splats as the underlying representation. This"}, {"title": "A. Problem Statement", "content": "We define Sreal as the Gaussian Splat of a real-world\nscene, captured from multiple RGB viewpoints, including\nthe robot. We also define Sob; as the splat of the k-th object\nin the scene, captured from multiple viewpoints. Our goal\nis to use Sreal for generating photorealistic renderings Isim\nof a robot operating in any simulator (e.g., PyBullet). Then,\nwe can leverage this representation to collect demonstrations\nusing the expert & for training RGB-based policies.\nThe expert & generates a trajectory \u03c4\u03b5 consisting of state-\naction pairs {($1,a1),...,(st,\u0430\u0442)} for a full episode. The\nstate at each time step t is defined as st = (qt, x1,...,xx),\nwhere qt \u2208 Rm denotes the robot's joint angles and xk\n(p, R) represents the position p\u2208 R\u00b3 and orientation\nR\u2208 SO(3) of the k-th object in the scene. The corre-\nsponding action at = (p, R) refers to the end effector's\nposition p\u00e5 \u2208 R\u00b3 and orientation R \u2208 SO(3).\nThe renderings Isim, derived from these simulated states"}, {"title": "B. Definitions of Coordinate Frames and Transformations", "content": "We define several coordinate frames to clarify the relation-\nships between the real-world scene, the simulator, and the\nsplat point clouds. The real-world coordinate frame, denoted\nas Freal, serves as the primary reference frame. Both the\nsimulator coordinate frame, Fsim, and the real-world robot\nframe, Frobot, are aligned with Freal. This alignment ensures\nthat the robot's base in the simulator and the real world share\nthe same coordinate system.\nAdditionally, the splat coordinate frame, denoted as Fsplat,\nrepresents the frame of the base of the robot in the Gaussian\nSplat of the scene Sreal. The robot base in the splat point\ncloud has a different frame from Freal, and we account for\nthis difference by using the transformation matrix TFsplat\nFrobot\nWe also define the k-th object frame in the simulator,\nFk-obj,sim, where objects are initialized in SIM at the\norigin with no rotation. The k-th object frame in the splat,\nFk-obj,splat, represents the object's position and orientation\nin its Gaussian splat Sobj. The object frames Fk-obj,sim and\nFk-obj,splat are later aligned during the simulation and splat\nprocess using the transformation matrix TFk-obj, sim"}, {"title": "C. Robot Splat Models", "content": "Our method for obtaining robot renderings at novel joint\nposes is summarized in Fig. 3. It follows a three-step\napproach:"}, {"title": "1) Alignment of Gaussian Splat Robot Frame to the\nSimulator Frame", "content": "In order to combine the Gaussian Splat\nrepresentation Sreal with the simulator, we first manually\nsegment out the 3D Gaussians associated with the robot. The\nmeans of these 3D Gaussians form a point cloud which is\naligned with the ground truth point cloud obtained from the\nsimulator. To achieve this, we use the Iterative Closest Point\n(ICP) algorithm, which produces the desired transformation\nTFsplat"}, {"title": "2) Segmentation of the Robot Links", "content": "To associate the 3D\nGaussians with their respective links in Sreal, we leverage\nthe ground truth bounding boxes of the robot's links, pro-\nvided by its CAD model. This method allows us to isolate\nthe 3D Gaussians corresponding to each link in the real-\nworld scene, denoted as Sreal, where l refers to the l-th link\nof the robot."}, {"title": "3) Forward Kinematics Transformation", "content": "Once we have\nthe 3D Gaussians for individual links and the frames aligned,\nwe can use the robot's forward kinematics to get the robot\npose at arbitrary joint angles qt \u2208 st, given by the simulator.\nIn this work, we use the forward kinematics routine from\nPyBullet to get the Transformation Tk for link l in the\nrobot's canonical frame Frobot. The transformation of the 3D\nGaussians can be calculated as :\nTl= (TFsplat)-1.Tk TFsplat\nwhere TFsplat is the transformation matrix to get the robot\nfrom splat frame to the simulation frame. Once the transfor-\nmation for each link is calculated, we use Eq. 1 and Eq. 2\nto transform the 3D Gaussians related to individual links of\nthe robot. The robot at novel poses is then rendered by the\nstandard Gaussian Splatting rendering framework [8]."}, {"title": "D. Object Splat Models", "content": "Similar to the robot rendering, we use ICP to align\neach object's 3D Gaussians Sob to its simulated ground\ntruth point cloud. In this way, we get the transformation\nT-k-obj, splat, which transforms the splat in Sob frame to\nsimulator frame. Given the position pr\u2208 st and orientation\nR\u2208 st can be used to calculate the transformation of object\nTk-obj from its original simulator frame Fk-obj,sim. Using"}, {"title": "E. Articulated Object", "content": "While CAD axis-aligned bounding boxes allow straight-\nforward segmentation of robot links, certain objects, such\nas parallel jaw grippers, present challenges due to their\nmisalignment with standard axes, that is, the gripper links\nare not neatly segmented out by just using bounding boxes\nin the 3D space. To address this, we employ a ground truth\nK-Nearest Neighbour (KNN) classifier trained on labeled\nsimulator point clouds as in Fig. 4 (a), which enables\ninference of the link class for each 3D Gaussian in the\naligned splat as shown in Fig. 4 (b)."}, {"title": "F. Rendering Simulated Trajectories using SplatSim", "content": "Now that we are able to render individual rigid bodies\nin the scene, we can use this to represent any simulated\ntrajectory \u03c4\u03b5 with photorealistic accuracy. We use these\nstate-based transformations along with methods described\nin Sec. IV-C, IV-D to get the demonstration for our policy\nto learn from tg = {(I\u015fim, a1), (I\u015fim, a2), . . ., (I\u015fim,\u0430\u0442)}.\nThis data is used by policy to predict actions from the\nsynthetically generated images."}, {"title": "G. Policy Training and Deployment", "content": "For learning from the generated demonstrations TG in the\nsimulator, we employ Diffusion Policy [49], [50], which is\nthe state of the art for behavior cloning. Although our method\nsignificantly mitigates the vision Sim2Real gap, discrep-\ncies between the simulated and real-world environments\nremain. For instance, simulated scenes lack shadows, and\nrigid body assumptions can lead to improper rendering of\nflexible components such as robot cables. To address these\nissues, we incorporate image augmentations similar to [51]\nduring policy training, which includes adding gaussian noise,\nrandom erasing and adjusting brightness and contrast of the\nimage. These augmentations notably enhance the robustness\nof the policy and improve its performance during real-world\ndeployment."}, {"title": "V. EXPERIMENTS", "content": "To evaluate the effectiveness of our framework in bridg-\ning the Sim2Real gap for RGB-based manipulation tasks,\nwe conducted extensive experiments across four real-world\nmanipulation tasks. We begin by detailing the data collection\nprocess in both the simulator and real-world environments.\nWe then compare the performance of policies trained on\nour synthetic data with Real2Real policies-those trained\non real-world data and deployed in real-world environ-\nments. This comparison demonstrates the high fidelity of\nour synthetic data, showing that policies trained within our\nframework can be deployed to real-world tasks without fine-\ntuning. Additionally, we assess Sim2Sim performance by\ntraining and evaluating policies entirely within the SplatSim,\nallowing us to quantify the degradation in performance\nduring Sim2Real transfer. Lastly, we investigate the effects\nof data augmentation on the transfer process and evaluate\nthe visual fidelity of the photorealistic renderings generated\nby the SplatSim framework."}, {"title": "A. Demonstrations in the Real World and Simulation", "content": "In the real world, demonstrations for each task were\nmanually collected by a human expert. In contrast, the\nsimulator streamlines this process by employing privileged\ninformation-based motion planners, which automatically\ngenerate data using privileged information, such as the posi-\ntion and orientation of each rigid body in the scene. The sim-\nulator not only reduces effort by automating resets between\ndemonstrations when a human expert is involved but more\nimportantly, it leverages motion planners that eliminate the\nneed for human intervention entirely. This enables the gener-\nation of large-scale, high-quality demonstration datasets with\nminimal manual input. As a result, the simulator drastically\nreduces the time and effort required for data collection. As\nshown in Table I, while real-world demonstration collection\nrequired about 20.5 hours, the same tasks were completed\nin just 3 hours in the simulator, underscoring the efficiency\nand scalability of our approach."}, {"title": "B. Zero-Shot Policy Deployment Results", "content": "We evaluate the zero-shot deployment of our policies\nacross four contact-rich real-world tasks, using task success\nrate as the primary metric. As shown in Table I, our method\nachieves an average success rate of 86.25% for zero-shot\nSim2Real transfer, compared to 97.5% for policies trained\ndirectly on real-world data, highlighting the effectiveness\nof our approach. All experiments were conducted using a\nUR5 robot equipped with a Robotiq 2F-85 gripper and 2\nIntel Realsense D455 cameras [52] with deployment on an\nNVIDIA RTX 3080Ti GPU for the Diffusion Policy [49].\n1) T-Push Task: The T-Push task, popularized by Diffu-\nsion Policy [49], captures the dynamics of non-prehensile\nmanipulation, which involves controlling both object motion\nand contact forces. For training, a human expert collected\n160 demonstrations in simulation using the Gello teleoper-\nation [48]. While testing, the robot started from a random\nlocation and achieved a 90% success rate (36/40 trials)\nin zero-shot Sim2Real transfer as shown in Table I. This\nresult shows the effectiveness of our framework in handling\nthe dynamics of pushing without fine-tuning on real-world\ndemonstrations. Additionally, the performance of our method\nis comparable to Real2Real (40/40) and Sim2Sim (40/40).\n2) Pick-Up-Apple Task: The Pick-Up-Apple task involves\ngrasping and manipulating the full pose of an object (i.e.,\nposition and orientation) in 3D. This task was designed\nto evaluate the robot's grasping capabilities when trained\nusing our simulated renderings. A motion planner, leveraging\nprivileged state information from the simulator (accurate\nposition and orientation of each rigid body in the scene),\ngenerated 400 demonstrations with randomized end-effector\npositions and orientations. During real-world trials, our pol-\nicy achieved a 95% success rate (38/40 trials) in zero-shot\nSim2Real transfer, as shown in Table I.\n3) Orange on Plate Task: In this task the robot has to pick\nup an orange and place it on a plate. In simulation, a motion\nplanner with access to privileged information, generated 400\ndemonstrations. The end-effector position and initial gripper\nstate were randomized during training. During testing, the\nrobot always started from a home position. We achieved\na 90% success rate (36/40 trials) in zero-shot Sim2Real\ntransfer.\n4) Assembly Task: In this task the robot has to put a\ncuboid block on top of another cuboid. The robot starts at the\nhome position with the green cube already grasped and has to\nplace it on top of the red cube. The task is particularly tough\nsince the robot has to make a precise placement otherwise the"}, {"title": "C. Quantifying Robot Renderings", "content": "We quantitatively evaluate the accuracy of rendered robot\nimages at various joint configurations by comparing them\nwith the real-world images. We assess the quality of the\nrobot's renderings across 300 different robot joint angles. To\nmeasure the similarity between the rendered and real-world\nimages, we employ two metrics commonly used in image\nrendering assessment: Peak Signal-to-Noise Ratio (PSNR)\nand Structural Similarity Index Measure (SSIM). Despite the\nvariations in joint configurations, the renderings achieve an\naverage PSNR of 22.62 and an SSIM of 0.7845, indicating\nthat the simulated images closely approximates the visual\nquality of the real-world RGB observations."}, {"title": "D. Effect of Augmentations", "content": "To quantify the impact of data augmentations on the\nSim2Real performance of our policy, we conducted exper-\niments comparing policies trained with and without aug-\nmentations. While the Diffusion-Policy performs effectively\nwithout augmentations in consistent environments (e.g.,\nSim2Sim or Real2Real scenarios), transferring a policy\ntrained in simulation to the real world introduces domain\nshifts that necessitate additional robustness as the renderings\ncan't capture dynamic details like changing reflections and\nshadows. We incorporated augmentations such as random\nnoise addition, Color Jitter, and random erasing during\ntraining to address these shifts. These augmentations improve\nthe performance of the policy from 21% to 86.25% across\nfour tasks in Sec. V-B."}, {"title": "VI. CONCLUSION", "content": "In this work, we tackled the challenge of reducing the\nSim2Real gap for RGB-based manipulation policies by\nleveraging Gaussian Splatting as a photorealistic rendering\ntechnique, integrated with existing simulators for physics-\nbased interactions. Our framework enables zero-shot transfer\nof RGB-based manipulation policies trained in simulation\nto real-world environments. While our framework advances\nthe current state-of-the-art, it is still limited to rigid body\nmanipulation and cannot handle complex objects such as\ncloth, liquids, or plants. In the future, our plan is to combine\nour framework with reinforcement learning-based methods to\nacquire more dynamic skills. We will also further improve\nour system to train and deploy robots in highly complex and\ncontact-rich tasks in the real world. Specifically, agricultural\ntasks such as pruning and harvesting, which require data that\nis challenging to obtain under field conditions, could greatly\nbenefit from our proposed method."}]}