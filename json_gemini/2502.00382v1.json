{"title": "Masked Generative Nested Transformers with Decode Time Scaling", "authors": ["Sahil Goyal", "Debapriya Tula", "Gagan Jain", "Pradeep Shenoy", "Prateek Jain", "Sujoy Paul"], "abstract": "Recent advances in visual generation have made significant strides in producing content of exceptional quality. However, most methods suffer from a fundamental problem - a bottleneck of inference computational efficiency. Most of these algorithms involve multiple passes over a transformer model to generate tokens or denoise inputs. However, the model size is kept consistent throughout all iterations, which makes it computationally expensive. In this work, we aim to address this issue primarily through two key ideas - (a) not all parts of the generation process need equal compute, and we design a decode time model scaling schedule to utilize compute effectively, and (b) we can cache and reuse some of the computation. Combining these two ideas leads to using smaller models to process more tokens while large models process fewer tokens. These different-sized models do not increase the parameter size, as they share parameters. We rigorously experiment with ImageNet256\u00d7256, UCF101, and Kinetics600 to showcase the efficacy of the proposed method for image/video generation and frame prediction. Our experiments show that with almost 3x less compute than baseline, our model obtains competitive performance.", "sections": [{"title": "1. Introduction", "content": "The last decade has witnessed tremendous progress in image andvideo generation, under diverse paradigms - Generative Adversarial Networks (Brock, 2018; Sauer et al., 2022), denoising processes such as diffusion models (Ho et al., 2020; 2022b; Dhariwal & Nichol, 2021; Rombach et al., 2022; Gu et al., 2022), image generation via vector quantized tokenization (Razavi et al., 2019; Esser et al., 2021; Ge et al., 2022; Van Den Oord et al., 2017), and so on. In recent years, diffusion models and modeling visual tokens as language have been the de-facto processes used to generate high-quality images. While initially proposed with a CNN or U-Net based architectures (Rombach et al., 2022; Saharia et al., 2022), transformer models have become the norm now for these methods (Peebles & Xie, 2023; Yu et al., 2023a).\nThe recent advancements in visual generation can be categorized along two axes \u2013 (a) different types of denoising processes in the continuous latent space (Ho et al., 2020; Nichol & Dhariwal, 2021b), discrete space (Gu et al., 2022; Lou et al.) or masking in the discrete space (Yu et al., 2023a; Chang et al., 2022), continuous space (Li et al., 2024a) (b) modeling tokens either auto-regressively (Kondratyuk et al., 2024; Esser et al., 2021; Yu et al., 2021) with causal attention or parallel decoding with bi-directional attention (Gu et al., 2022; Yu et al., 2023a; Chang et al., 2022; Zheng et al., 2022). To achieve a high synthesis fidelity, both, denoising in diffusion models, and raster scan based auto-regressive token modeling require several iterations.\nRecently, parallel decoding of discrete tokens have shown promise in generating high quality images with few iterations - MaskGIT (Chang et al., 2022), MAGVIT (Yu et al., 2023a), MUSE (Chang et al., 2023), MaskBIT (Weber et al., 2024), TiTok (Yu et al., 2024b). These models are trained with Masked Language Modeling (MLM) type losses, and the generation process involves unmasking a few confident tokens every decoding iteration, starting from all masked tokens. They can even surpass diffusion models, given a good visual tokenizer (Yu et al., 2023b; Weber et al., 2024).\nAlthough MaskGIT reduces decode complexity significantly, parallel decoding still includes several redundant computations. First, the need for same capacity model for all steps needs to be investigated. Second, unlike auto-regressive models, which cache its computation in all steps, parallel decoding models perform re-computation for all tokens. We empirically find that a smaller model can generate good-quality images but its performance saturates after a point with more decoding iterations. A bigger model can perform finer refinement and generate better-quality images.\nMotivated by these observations, we present Masked Generate Nested Transformers with Decode Time Scaling (MaGNETS). We design a model size curriculum over the decoding process, which efficiently utilize compute. MaG-NeTS gradually scales the model size up to the full model"}, {"title": "2. Related Work", "content": "Efficient Visual Generation. Image generation literature has seen significant improvements in the past years - Generative Adversarial Networks (Brock, 2018; Sauer et al., 2022), discrete token based models (Chang et al., 2022; Yu et al., 2023a), diffusion-based models (Kingma & Gao, 2023; Hoogeboom et al., 2023), and more recently hybrid models (Peebles & Xie, 2023; Yu et al., 2024c), but they often guzzle computing power. Researchers tackle this bottleneck of computational costs with efficient model architectures and smarter sampling strategies.\nIn diffusion model literature, there have been some work to reduce the number of sampling steps, by treating the sampling process like ordinary differential equations (Song et al., 2022; Lu et al., 2022; Liu et al., 2022), incorporating additional training process (Kong & Ping, 2021; Nichol & Dhariwal, 2021a; Salimans & Ho, 2022; Song et al., 2023), sampling step distillation (Salimans & Ho, 2022;"}, {"title": "3. Preliminaries", "content": "Parallel Decoding for Image Generation. Masked Generative Image Transformer (MaskGIT) (Chang et al., 2022) introduces a novel approach to image generation that significantly differs from traditional autoregressive models. In autoregressive decoding, images are generated sequentially, one pixel/token at a time, following a raster scan order (Esser et al., 2021; Kondratyuk et al., 2024; Wang et al., 2024; Yu et al., 2024a; Li et al., 2024b). This sequential approach can be computationally inefficient, as each token is conditioned only on the previously generated tokens, leading to a bottleneck in processing time. MaskGIT generates all tokens of an image simultaneously and then iteratively refines them. This method enables significant acceleration in the decoding process. The tokens are discrete and obtained using Vector Quantized (VQ) autoencoders, learned with self-reconstruction and photo-realism losses (Yu et al., 2023a). The iterative parallel decoding process can be represented as:\n\\begin{equation}\nX_k \\leftarrow \\text{Mask Sample}(M(X_{k-1},c),k)\n\\end{equation}\nwhere $X \\in \\mathbb{Z}^{N \\times 20}$, are the input tokens, N is the number of tokens, $k \\in [1, K]$ denote the iteration number, with K being the total number of iterations, $X_0$ is either completely masked for full generation, and partially masked for conditional generation tasks like frame prediction, c is the category of image/video under generation. The Sample function utilizes logits predicted by the model M(.), introduces certain randomness, and sorts them by confidence, unmasking only top-k tokens while masking the rest. We follow this process as in (Chang et al., 2022; Yu et al., 2023a).\nNested Models. The core of our algorithm for inference-efficient decoding relies on variable-sized nested models for efficient parameter-sharing and hence feature space. We use MatFormer's (Kudugunta et al., 2023) modeling approach to extract multiple nested models, from a single model, without increasing the total parameter count. Given a full transformer model M, MatFormer defines nested models ${m_1,...,m_c}$, such that $m_1 < m_2 \\dots< m_c = M$. Each $m_i$ has fewer parameters and reduced compute. The core idea of extracting nested models is that in a transformer block, a reduced computation using a parameter subspace can be performed via a sliced matrix multiplication. Assuming a parameter matrix $W \\in \\mathbb{R}^{d'\\times d}$ and feature vector $x \\in \\mathbb{R}^d$, then the computation $y = Wx$ can be partially obtained by computing $y[:] = W[::]x$, if y is desired to be partial and $y = W[:,:14]*[:] $, if input x is partial. Nested models can be obtained via partial computations throughout the network.\nWhile MatFormer (Kudugunta et al., 2023) obtained sub-models with partial computation only in the MLP layer, we also do it in the Self-Attention layer, specifically in obtaining the Q, K, V features. These features are of dimension $n_h \\times d_h$, where $n_h$ is the number of attention heads, $d_h$ is the head feature dimension, and p is the model downscaling"}, {"title": "4. Method", "content": "Given the preliminaries, here we introduce the core algorithm. We first discuss the idea of scheduling models of different sizes over decode iters of MaskGIT. Then, we discuss the process of caching key-value in parallel decoding, followed by how to refresh them to improve performance. We finally discuss the nested model training method. A pictorial overview of our method is presented in Figure 2.\nDecode Time Model Schedule. In iterative parallel decoding (Chang et al., 2022; Yu et al., 2023a), the same-sized model is used for all steps, starting with all tokens being masked. However, we hypothesize that certain stages of the generation process might be easier than others. For example, in the initial steps, the model only needs to capture coarse global structures, which can be achieved efficiently using smaller models. In the later steps, the model must refine finer details, which requires larger models. This hypothesis is bolstered with Figure 3, which shows that the generation process starts unmasking tokens from the background and shifts to the middle of the image in the later iterations (more categorical examples in Appendix Figure 8).\nOur hypothesis is further motivated by Figure 4, which presents the generation quality (FID) over iterations of parallel decoding for different-sized models. The smallest model reaches a reasonably good FID score with very low FLOPs compared to the biggest model. However, it saturates after a point, and the larger models surpasses the smaller ones"}, {"title": "5. Experiments and Results", "content": "in performance, demonstrating their ability to capture finer details and generate higher-quality images when provided with sufficient compute. This trend suggests that dynamically scaling the model size during decoding can exploit the varying task difficulty and achieve compute efficiency.\nWe use nested models to extract multiple models rather than using models with disjoint parameters. Nested models do not increase the parameter count and it also helps in better alignment of hypothesis when we shift model size over decode steps. The decode time model schedules can be generalized and represented as making the model choice in Equation (1) dependent on the iteration index as follows:\n\\begin{equation}\nX_k \\leftarrow \\text{Mask Sample} (M_k (X_{k-1},c), k)\n\\end{equation}\n\\begin{equation}\nM = \\{(m_{p_1})_{k_1}, (m_{p_2})_{k_2},..., (m_{p_n})_{k_n}\\}, s.t. \\sum_i k_i = K\n\\end{equation}\nwhere $p_1, p_2,..., p_n$ denoting the downscaling factors of the corresponding nested models, and $(m_p)_k$ denotes that model m will be executed for k iterations. K represents the total number of iterations. We can think of different model schedules - downscaling (starting with the full model and then gradually moving to the smallest model), upscaling, intermittently switching among a few models, and so on. We can also modify the integers $k_i$ to choose the number of times we stick to a model before switching. However, as intuitively discussed before, we empirically validate that gradually upscaling the model size gets the best trade-off between the compute and generation quality.\nCached Parallel Decoding. Inspired by caching key-value pairs in auto-regressive models, we explore caching in parallel decoding, which retains relevant computations and enhances efficiency. In auto-regressive models, caching progressively happens in one fixed direction. However, in parallel decoding, caching must depend on which tokens are unmasked over the iterations.\nConcretely, starting from an empty cached set, we keep adding keys and values to the set for the tokens that are unmasked after the Mask Sample steps (see Section 3). We do not update the predicted token indices for these unmasked tokens. Hence, the cached key and values for the unmasked tokens are the only features the other masked tokens need; hence, we do not need any further computation. In every decoding iteration, we can categorize tokens into three main categories: unmasked tokens (for which we have cached KV), tokens that can be unmasked during the current iteration, and the rest of the tokens. Note that the KV cache for the second category tokens cannot be used in the next iteration but only in the iteration after that once we know their token indices after the forward pass. We cache them in the next iteration for use in the immediately next iteration.\nCaching is even more useful for decode time model schedules. For a schedule that progressively scales up the model"}, {"title": "Intermittent Cache Refresh", "content": "Caching the key-value pairs for the unmasked tokens helps reduce computation, but it can slightly degrade performance. This happens because - (a) when we cache, the unmasked tokens are not updated in the subsequent iterations. (b) when we shift model size during generation, in the attention layer, the query size differs from the cached KV (see Section 3). While technically, we can zero-pad the KV to be compatible with the current model's query dimension, the model remains unfamiliar of such feature discrepancies between query and key-value.\nTo remedy this, we strategically refresh the cache while changing the model size. Refreshing involves discarding the cached KV for that iteration and caching a newly computed KV for the immediate next iteration. We empirically find that it bridges the performance gap that arises due to caching. The proposed decode time model scaling algorithm is presented in Algorithm 1, which uses MaskGIT's sampling strategy (Chang et al., 2022; Yu et al., 2023a) to sample tokens from logits predicted by the network."}, {"title": "Training Nested Models", "content": "MatFormer (Kudugunta et al., 2023) opts for a joint optimization of losses w.r.t. ground-truth from all models with equal weights. While this mode of training works for a small range of model downscaling, we found it to hurt performance with larger downscaling factors p. We introduce a combination of ground truth and distillation loss to address this issue. We perform online distillation progressively, where the teacher for model $m_i$ is model $m_{i+1}$. The full model $m_N (= M)$ is trained with only ground truth loss. This provides a simpler optimization for the smaller nested models while maintaining the overall objective. Progressive distillation also reduces the teacher-student size gap, which can otherwise hurt distillation performance (Stanton et al., 2021; Beyer et al., 2022; Mirzadeh et al., 2019). Given input X, ground truth label Y"}, {"title": "Evaluation Metrics", "content": "Following previous baselines, we use Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017; Dhariwal & Nichol, 2021) for image generation, Fr\u00e9chet Video Distance (FVD) (Unterthiner et al., 2019) for the video generation tasks, Inception Score (Salimans et al., 2016) for both tasks, as well as precision and recall for image generation. We compare algorithms using inference-time GFLOPs. Refer Appendix D for GFLOPs computation details."}, {"title": "5.1. Image Generation", "content": "Comparison with Baselines. In this section, we compare MaGNETS with state-of-the-art methods in the literature for image generation. We list the results in Table 1 for 256 \u00d7 256 image generation on ImageNet-1k. Table 1 shows that MaGNETS can speed up the generation process by 2.65-3x (depending on total step count), with a negligible drop in FID. Refer Appendix D for real-time gains. Figure 5 illustrates that MaGNeTS significantly accelerates parallel decoding, which gets more pronounced as image resolution grows. Figure 1 and Figure 10 show generated images from MaskGIT++ and MaGNETS (ours). As shown before in Yu et al. (2023b); Weber et al. (2024), using a superior tokenizer can further boost MaGNETS's performance. Note that several recent diffusion-based works only report results on the low-resolution of ImageNet (typically 64\u00d764), and therefore a direct comparison is not possible.\nScaling Analysis. To understand the scaling properties of MaGNETS we train models of different sizes - S (22M), B (86M), L (303M) and XL (450M) for both the baseline as well as nested models needed for our algorithm. We use the same hyper-parameters for all, such as learning rate, epochs, weight decay, etc. We present the results in Figure 6. It shows the compute vs performance of different models, with the blob size denoting the model size. For a certain parameter count, the baseline uses the full model for all 12 decoding steps, while the scheduled routines use a sequence of nested models with downsampling factors p = 8,4,2,1 for 3 steps each. It can be seen that scaling up model size lead to much cheaper compute scaling of MaGNETS than the baseline, with almost 3\u00d7 compute reduction."}, {"title": "5.2. Video Generation", "content": "We use the MAGVIT (Yu et al., 2023a) framework to train parallel decoding based video generation and frame prediction models. Figure 11 shows generated videos of UCF101. We summarize the results for class-conditional video generation on UCF101 in Table 2 and for frame prediction on Kinetics600 in Table 3. Despite the challenging nature of video generation relative to image generation, results indi-"}, {"title": "5.3. Ablation Studies", "content": "Impact of Decode Time Model Schedule. We study the effect of different model scheduling choices. As discussed previously, we can think of different model schedules - scaling up model size, scaling down, periodic scaling up and down, and so on. For this analysis, we consider the L-sized model, with three nested models within it with parameter reduction by roughly 1. We can denote the number of times these four models are called during decoding as $(k_1,k_2, k_3, k_4)$, s.t., $\\sum_{i=1}^4 ki = 12$. We drop the model notation of $m_p$ in Equation (2) for simplicity and explicitly mention the model names in the text, as discussed next.\nFirst, we evaluate all combinations of $k_i$ for which we always scale up in Figure 7a in red and scale down in Figure 7b in blue. The green curve shows the performance of the individual nested models. We have the following observations (1) for a certain compute budget, the scheduling of models over generation iterations (red dots) can offer better performance than using a single nested models (green curve) for all steps. (2) Models that have smoother transitions in nested"}, {"title": "Impact of Caching and Refresh", "content": "We now discuss the impact of caching and its refresh. For this analysis, we use a uniform model schedule: $k_1 = k_2 = k_3 = k_4 = 3$. We also perform caching and refresh on the baseline model, which has not been trained with any nesting and has the same model applied for all iterations. We also refresh the cache at exactly the same steps as the scheduled model for the baseline. We present the results in Table 4. The columns \"Baseline\u201d and \u201cScheduled\" do not involve any cache. While caching degrades the performance a bit, refreshing it intermittently can avoid the degradation. While refresh does have some compute overhead, it does help significantly. Scheduling of models with caching and refresh has the best compute-performance trade-off."}, {"title": "The efficiency of using nested models", "content": "In MaGNETS we use nested models instead of separately trained smaller sized models. This has two advantages - (a) parameter sharing, which limits the number of parameters to just that of the full model, compared to $1.875 \\times (= 1 + 1/2 + 1/4 + 1/8)$"}, {"title": "6. Conclusion", "content": "In this paper, we propose MaGNETS, a novel approach for allocating different compute to different steps of the image/video generation process. We show that instead of always using the same sized transformer model for all decoding steps, we can start from a model which is nested and fraction of its full size, and then gradually increase model size. This along with key-value caching in the parallel decoding paradigm obtains significant compute gains. We believe that our exploration of dynamic compute opens exciting new directions for research in efficient and scalable generative models. In future works, we plan to explore token-dependent model schedules for further compute gains."}, {"title": "A. Motivation for Decode Time Model Scaling", "content": "Our visualization of token density averaged across 50k ImageNet samples reveals a dynamic pattern - initial decoding iterations prioritize background regions. In contrast, later iterations focus on the center where foreground objects or region of interest typically reside. This highlights the need to allocate resources efficiently during generation. To further investigate this behavior, we examine token density across various ImageNet categories (refer Figure 8). This category-wise analysis further motivates our focus on decode time scaling. Figure 10 shows more qualitative results on ImageNet256 \u00d7 256 and Figure 11 shows samples on UCF101."}, {"title": "B. Hyper-parameter Details", "content": "The MaskGIT algorithm has the following hyper-parameters which we discuss next.\nGuidance Scale (gs). It is used in classifier-free guidance (Ho & Salimans, 2022) and governs the calculation of final logits during inference as shown in Equation (4).\n\\begin{equation}\n\\text{logitsfinal} = \\text{logitscond} + gs. (\\text{logitscond}- \\text{logitsuncond})\n\\end{equation}\nwhere $\\text{logitscond}$ are from class-conditional input, $\\text{logitsuncond}$ are from unconditional input, and A depends on the mask-ratio of the current decoding iteration.\nFigure 8 shows that the initial decoding iterations of parallel decoding focus on the background region, and focus gradually shifts to the main object/region in the final decoding iterations. Motivated by this, we experimented with applying guidance to only few final decoding iterations and present our findings in Figure 9b. As we can see, most of the decoding iterations do not require guidance. We use guidance only for final few decoding iterations for class-conditional generation in ImageNet256\u00d7256 and frame prediction in Kinetics600. Following MAGVIT (Yu et al., 2023a), for class-conditional generation in UCF101 we do not use classifier-free guidance.\nMask Temperature (MTemp). It controls the randomness introduced on top of the token predictions to mask tokens.\nSampling Temperature (STemp). It controls the randomness of the sampling from the categorical distribution of logits. Tokens are sampled from logits/STemp. STemp is calculated by Equation (5).\n\\begin{equation}\n\\text{STemp} = bias + scale \\cdot (1 - (k + 1)/K)\n\\end{equation}\nwhere bias and scale are hyperparameters (see Table 6), k"}, {"title": "C. Additional Ablations", "content": "Impact of Distillation. We use two types of losses to train the nested sub-models - loss w.r.t the ground-truth tokens and distillation loss using the progressively bigger model as the teacher. The weight between the two losses is also linearly interpolated from the former to the latter. We compare this training strategy with the two extremes \u2013 only ground truth loss and only distillation loss and present the results in Table 7. As we can see, using only distillation loss results in divergence. Using ground-truth loss is also inferior to linearly annealing on UCF101 and for the smallest model in ImageNet."}, {"title": "Nested Attention Heads", "content": "We also investigate nesting along the number of attention heads (nh), applying the same partial computation strategy as discussed before. However, this generally performed worse than nesting along the head feature dimension in attention, which is what we use for this work."}, {"title": "D. Compute Gains", "content": "Per-step FLOPs. Figure 9a illustrates the inference-time"}, {"title": "Calculation of GFLOPs", "content": "We illustrate the calculation of inference GFLOPs via Python pseudo-code in Table 9. We double the GFLOPs in decoding iterations where classifier-free guidance (Ho & Salimans, 2022) is used. Note that we always use a cosine schedule to determine the number of tokens to be unmasked in every step."}, {"title": "Real-Time Inference Benefits", "content": "In addition to the theoretical FLOP gains offered by MaGNETS, here we want to analyze the real-time gains that it offers. We implement MaGNETS on a single TPUv5 chip and present the results in Table 8."}, {"title": "E. Limitations", "content": "While our approach demonstrates strong performance in image and video generation, we acknowledge certain limitations. Some artifacts inherent to MaskGIT++ may also appear in our generated outputs (see Figure 12 for examples on ImageNet256 \u00d7 256). Such artifacts are common in models trained on controlled datasets like ImageNet. Moreover, the quality of the pretrained tokenizers (Yu et al., 2023b; Weber et al., 2024) directly impacts our method's effectiveness; however, improving these tokenizers is beyond the scope of this work. Although, use of nesting and decode time scaling does not have any specific requirement for model architecture and sampling scheme, KV caching requires discrete tokens."}]}