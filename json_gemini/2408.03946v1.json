{"title": "PROMPTING FOR PRODUCTS: INVESTIGATING DESIGN SPACE EXPLORATION STRATEGIES FOR\nTEXT-TO-IMAGE GENERATIVE MODELS", "authors": ["Leah Chong", "I-Ping Lo", "Jude Rayan", "Steven Dow", "Faez Ahmed", "Ioanna Lykourentzou"], "abstract": "Text-to-image models are enabling efficient design space ex-\nploration, rapidly generating images from text prompts. However,\nmany generative AI tools are imperfect for product design appli-\ncations as they are not built for the goals and requirements of\nproduct design. The unclear link between text input and image\noutput further complicates their application. This work empir-\nically investigates design space exploration strategies that can\nsuccessfully yield product images that are feasible, novel, and\naesthetic - three common goals in product design. Specifically,\nusers' actions within the global and local editing modes, in-\ncluding their time spent, prompt length, mono vs. multi-criteria\nprompts, and goal orientation of prompts, are analyzed. Key\nfindings reveal the pivotal role of mono vs. multi-criteria and\ngoal orientation of prompts in achieving specific design goals\nover time and prompt length. The study recommends prioritizing\nthe use of multi-criteria prompts for feasibility and novelty during\nglobal editing, while favoring mono-criteria prompts for aesthet-\nics during local editing. Overall, this paper underscores the\nnuanced relationship between the AI-driven text-to-image mod-\nels and their effectiveness in product design, urging designers\nto carefully structure prompts during different editing modes to\nbetter meet the unique demands of product design.", "sections": [{"title": "1. INTRODUCTION", "content": "Rapid advancements in generative artificial intelligence\n(GenAI) have enabled the generation of novel and innovative\ncontent, such as texts and images, from simple text prompts. In\nproduct design applications, text-to-image models can produce\nimages of designs from text prompts, enabling the exploration of\nmultiple designs in shorter spans of time compared to the tradi-\ntional method of manually rendering new designs. This function-\nality holds great potential for streamlining the iterative creative\nprocess in product design, particularly by facilitating design space\nexploration (DSE).\nWhile text-to-image GenAI can enable the rapid exploration\nof diverse product design concepts, most existing tools are not\nengineered to account for the multifaceted goals and requirements\nof product design, such as feasibility and aesthetics. For example,\ncurrent GenAI tools can generate a large number of designs,\nmany of which are infeasible [1-3]. Chong and Yang presented\na list of 16 different design objectives that are prevalent in design\nresearch and practice [4]. This long \u2013 and yet non-exhaustive\nlist, underscores the complexity of parameters essential for\ndesigning a successful product. Unfortunately, current GenAI\npossesses Al's inherent vagueness in the relationship between\nthe input (i.e., communicated goal) and the generated output\n(i.e., images of designs), a property that renders GenAI tools\ninsufficient for generating reliable product designs. For example,\nwhen the text prompt is \"design of a mug that is ergonomic,\nsleek, and modern\", it is unclear how the GenAI understands and\nmaps the meaning of \u201cergonomic\u201d, \u201csleek\u201d, and \u201cmodern\u201d onto\nthe generated images. While one way to address this problem\nis to develop new models specifically trained for product design,\ncurrent models possess a creative advantage, given the vast range\nof available image datasets compared to design-specific datasets\nlike computer-aided design files. Therefore, this work aims to\nunderstand how off-the-shelf, promising yet imperfect text-to-\nimage GenAI tools can be used to explore and refine product\ndesigns that are novel, aesthetically pleasing, and feasible.\nThis work conducts a controlled human subject experiment\nin which participants are asked to design bikes that are feasible,\nnovel, and aesthetic at the same time using Stable Diffusion 1.5\non an online platform called Leonardo.AI. At the time of running\nthe experiment, Leonardo.AI was one of the few interactive tools\nthat allowed the participants to easily use text-to-image genera-\ntive models, such as various versions of Stable Diffusion, without\nthe need to run custom Python scripts. The evaluations of the\nfeasibility, novelty, and aesthetics of the generated designs are\ncollected via crowdsourcing. The relationship between the par-\nticipants' DSE strategies when using Stable Diffusion and the\nevaluation scores of their generated designs are analyzed. Key\nresults from this study show that the goal orientation and the"}, {"title": "2. BACKGROUND", "content": "DSE is a crucial step in the product design process, dur-\ning which designers explore a wide range of potential designs\n(divergence) and select and refine a fewer selection of designs\n(convergence) [5, 41]. Divergence is important for successful de-\nsign as it expands designers' creativity and increases the novelty\nand quality of their designs [44]. It aims to prevent designers\nfrom limiting themselves to one or few viable solutions too early\nby encouraging the formulation of a variety of potential solutions.\nAlong with divergence, convergence is an equally important as-\npect of DSE. Once designers have explored enough, they must\nevaluate, select, and refine the final solution(s) based on various\ndesign requirements, goals, and preferences.\nDuring DSE, designers engage in divergent thinking through\nvarious methods like problem reframing [33, 34] and analogies\n[45]. Generation and consideration of a large number of design\noptions can be encouraged through the manipulation of a vari-\nety of characteristics, such as flexibility and imagery [36]. This\nprocess not only allows designers to maximize their creative out-\nput, but also provides an opportunity to gain more insights into\nthe problem and the design space. Prior research has attempted\nto find effective inspirations and methods to assist designers'\ndivergent thinking using non-AI-based methods. For example,\nThinklets [39] is a creativity support tool that guides designers\nto think of ideas from multiple angles through open-ended ques-\ntions. Ideation Decks [37] is another example, a set of cards that\nprompts designers to think about their design space from differ-\nent angles. While these tools have shown some effectiveness,\ntext-to-image GenAI can enable a much more efficient DSE and\nsignificantly reduce designers' cognitive load by quickly generat-\ning multiple designs from text prompts. However, adopting this\nmethodology means that designers must sacrifice some level of\ncontrol in the design generation process (between text prompt and\ngenerated image).\nConvergence is also a crucial aspect of DSE, during which\ndesigners make selections and/or mark preferences for certain\naspects of generated designs [45]. Often informed by data, de-\nsigners choose specific design directions and make refinements"}, {"title": "2.1 Design Space Exploration", "content": "With the recent advances in GenAI, there is great potential for\nthese tools to effectively support creative processes. GenAI is a\nrapidly evolving field that involves the creation of algorithms and\nmodels capable of generating novel content in various domains,\nsuch as images, text, and music. Its primary goal is to imitate the\nintricate creative process by leveraging existing datasets to iden-\ntify underlying patterns and yield outputs that closely resemble\nthe characteristics of the training examples. Since 2020, discus-\nsions of various applications of GenAI, such as human resources,\nliterature, and art, have emerged. Specifically in product design,\nthe potential of image GenAI as a tool for the early stages of the\ndesign process has been explored in some recent design litera-\nture [47]. There are primarily two types of models employed\nfor image GenAI: Generative Adversarial Networks (GANs) and\ndiffusion models.\nGANs were introduced by Goodfellow in 2014 in the field of\nmachine learning (ML). GANs are built using a pair of neural net-\nworks: a generator and a discriminator, operating on the principle\nthat one network's gain is another network's loss. The generator\nis trained to generate new data samples, while the discriminator\ndetermines whether these samples are real or generated. Training\ncontinues until the discriminator's performance is above a certain\nthreshold. Over the years, GANs have undergone significant re-\nfinement, incorporating methods such as injecting noise into the\ngenerator's input [7], employing diverse loss functions [8], and\napplying regularization methods [9], to promote the diversity of\nthe generated data and improve the overall quality of the model\noutputs. With this refinement, the practical applications of GANS\nhave expanded, serving as an effective model for image-to-image\nGenAI for DSE by rapidly creating a large number of possible\ndesigns [10, 11].\nThe diffusion model was introduced by Sohl-Dickstein, et al.\nin 2015 as an alternative paradigm for GenAI [15]. The diffu-\nsion model works by adding small random noise to the training\ndata over multiple steps to produce a sequence of samples, then\nlearning to recover the data by reversing this process. The per-\nformance of the model has been advanced continuously, giving\nrise to a flow-based generative model employing invertible trans-\nformations [16], a continuous-time diffusion process called the\nFree-form Jacobian of Reversible Dynamics (FFJORD) model ca-\npable of generating high-quality samples with efficient inference"}, {"title": "2.2 Image Generative Al", "content": "[17", "18": ".", "19": "."}]}