{"title": "Refusing Safe Prompts for Multi-modal Large Language Models", "authors": ["Zedian Shao", "Hongbin Liu", "Yuepeng Hu", "Neil Zhenqiang Gong"], "abstract": "Multimodal large language models (MLLMs) have become the cornerstone of today's generative\nAl ecosystem, sparking intense competition among tech giants and startups. In particular, an\nMLLM generates a text response given a prompt consisting of an image and a question. While\nstate-of-the-art MLLMs use safety filters and alignment techniques to refuse unsafe prompts, in\nthis work, we introduce MLLM-Refusal, the first method that induces refusals for safe prompts.\nIn particular, our MLLM-Refusal optimizes a nearly-imperceptible refusal perturbation and adds\nit to an image, causing target MLLMs to likely refuse a safe prompt containing the perturbed\nimage and a safe question. Specifically, we formulate MLLM-Refusal as a constrained optimization\nproblem and propose an algorithm to solve it. Our method offers competitive advantages for MLLM\nmodel providers by potentially disrupting user experiences of competing MLLMs. Since competing\nMLLM's users will receive unexpected refusals when they unwittingly use these perturbed images\nin their prompts. We evaluate MLLM-Refusal on four MLLMs across four datasets, demonstrating\nits effectiveness in causing competing MLLMs to refuse safe prompts while not affecting non-\ncompeting MLLMs. Furthermore, we explore three potential countermeasures-adding Gaussian\nnoise, DiffPure, and adversarial training. Our results show that they are insufficient: though they can\nmitigate MLLM-Refusal's effectiveness, they also sacrifice the accuracy and/or efficiency of the\ncompeting MLLM. The code is available at https://github.com/Sadcardation/MLLM-Refusal.", "sections": [{"title": "1 Introduction", "content": "Multimodal large language models (MLLMs) [40, 46, 30, 58, 14, 6] have emerged as a groundbreaking foundation for various generative AI applications such as visual question answering [30], image captioning [25], and embodied AI [16]. MLLMs are typically trained and deployed as web chatbots or cloud API services by resourceful entities, including major technology companies and startups. These companies are fiercely competing in the development of MLLMs, exemplified by Google's Gemini Pro [46] and OpenAI's GPT-4 [1]. An MLLM generally comprises three key components: a vision encoder, a vision-language projector, and a large language model (LLM). The vision encoder generates an embedding vector for an image, which the vision-language projector converts into tokens in the LLM's input token space. These tokens are concatenated with the question's tokens and fed into the LLM, producing the final text response.\nDespite rapid progress in MLLMs' capabilities, their safety has garnered increasing attention. A recent U.S. Executive Order [53] underscores the commitments from AI companies to ensure the safety and security of advanced AI systems. A safeguarded MLLM typically prevents generating harmful outputs by refusing unsafe prompts [23, 38, 42, 45]. Specifically, if either the prompt's image or question is unsafe, i.e., containing harmful content, the prompt is considered unsafe. For example, the first two rows in Figure 1 illustrate two types of unsafe prompts and the corresponding refusals from an MLLM. MLLMs can achieve this through safety filters [23, 56] and alignment techniques [42, 45, 13]."}, {"title": "Our Work:", "content": "In this work, we introduce a novel perspective on refusal: the refusal against safe prompts by MLLMs. A prompt is considered safe only if both the image and the question are devoid of harmful content. The third row in Figure 1 illustrates this concept. We explore scenarios where an image, seemingly benign, is subtly perturbed with a nearly-imperceptible perturbation named refusal perturbation, while the question remains safe. The image and question form a safe prompt that is likely to provoke an unwarranted refusal response from the target MLLM.\nWe consider three key roles in our setting: an MLLM model provider, its competitors, and normal users. The competitors are also MLLM model providers who develop competing MLLMs. Normal users query MLLMs with safe prompts. We consider the MLLM model provider is the attacker who aims to gain competitive advantages by utilizing effective refusal perturbations to cause competing MLLMs to refuse safe prompts. The model provider perturbs images and publishes them on the Internet, e.g., social media. Normal users of competing MLLMs would experience unexpected refusals when they unwittingly use these perturbed images in their prompts, leading to frustration and a decline in user satisfaction. Such new angle of refusal opens avenues for competitive differentiation in the rapidly evolving field of MLLMs."}, {"title": "MLLM-Refusal.", "content": "We propose MLLM-Refusal, which optimizes a refusal perturbation to make competing MLLMs refuse safe prompts. Specifically, we consider three primary goals when crafting a refusal perturbation: effectiveness, locality, and stealthiness. Roughly speaking, the effectiveness goal ensures that refusal perturbations cause competing MLLMs to refuse safe prompts containing the refusal perturbation. The locality goal ensures that refusal perturbations are effective only against competing MLLMs, while being ineffective against the model provider's own MLLM. The stealthiness goal ensures that refusal perturbations are nearly-imperceptible.\nTo achieve the three goals, our MLLM-Refusal uses a set of shadow questions to mimic the actual questions of normal users. Given an image, a set of shadow questions, and some competing MLLMs, MLLM-Refusal optimizes a refusal perturbation so that competing MLLMs are likely to refuse safe prompts containing any shadow question and the image embedded with the refusal perturbation. To ensure stealthiness, MLLM-Refusal applies an $l_\\infty$-norm constraint on the refusal perturbation during optimization. Formally, we formulate this as a constrained optimization problem and solve it via a gradient-based method. Figure 2 shows several examples of images with refusal perturbations. Our intuition is that competing MLLMs are likely to refuse users' safe prompts even when user questions differ from shadow questions, similar to the transferability of adversarial examples [51]. Because different MLLMs have unique vision-language projectors and the refusal perturbation is not optimized for the model provider's own MLLM, it does not affect the model provider's own MLLM, thus achieving the locality goal."}, {"title": "Evaluations.", "content": "Our evaluation of MLLM-Refusal involves using image-question pairs from VQAv2 [3], GQA [21], TextVQA [49], and an extended CelebA [33] dataset. We evaluate MLLM-Refusal's performance on four open-source MLLMs: LLaVA-1.5 [30], MiniGPT-4 [58], Qwen-VL-Chat [6], and InstructBLIP [14]. We use the refusal rate as our evaluation metric, defined as the proportion of prompts that result in refusals when perturbed images are presented to the MLLMs. MLLM-Refusal applies $l_\\infty$-norm constraint $\\epsilon = 8/255$ and thus achieves the stealthiness goal since such constraint is considered visually stealthy in previous work [43, 34, 7]. MLLM-Refusal achieves the effectiveness goal across all MLLMs and datasets. For example, MLLM-Refusal achieves over 0.88 refusal rates with LLaVA-1.5 on VQAv2 with three types of shadow questions. Moreover, our MLLM-Refusal maintains near-zero refusal rates on non-competing MLLMs, thereby achieving the locality goal."}, {"title": "Countermeasures.", "content": "To counter the refusal perturbations created by MLLM-Refusal, we evaluate three countermeasures: Gaussian noise, DiffPure [39], and adversarial training. Gaussian noise reduces MLLM-Refusal's effectiveness but significantly lowers MLLM accuracy, from 0.92 to around 0.80 with standard deviation \u03c3=0.02. DiffPure uses a diffusion model to purify images, reducing MLLM-Refusal's refusal rates but also dropping accuracy from 0.92 to 0.78 and increasing inference time by up to 13.07%. Adversarial training, which involves fine-tuning an MLLM on images with refusal perturbations, reduces refusal rates to around 60% but also significantly decreases MLLM accuracy and requires substantial computational resources. These findings indicate that while these countermeasures can mitigate MLLM-Refusal, they also sacrifice MLLM utility and efficiency.\nIn summary, we make the following key contributions:"}, {"title": "Related Work", "content": ""}, {"title": "2.1 MLLMS", "content": "Generally speaking, MLLMs[30, 58, 14, 6] are LLMs extended with the ability to deal with visual input. Specifically, an MLLM generates a text response to a prompt containing an image and a text question. An MLLM typically comprises three main components: a vision encoder, a vision-language projector, and an LLM.\nVision encoder: Given an image input, an MLLM uses a vision encoder to produce an image embedding vector. Vision encoders are often pre-trained on large datasets of unlabeled images or image-text pairs through self-supervised learning [41, 10, 44]. State-of-the-art vision encoders typically utilize convolutional neural networks or vision transformers (ViT). In particular, CLIP's vision encoders[44] are commonly used in many MLLMs [30, 31].\nVision-language projector: Since the output space of the vision encoder and the input space of the LLM are different, an MLLM uses a vision-language projector to align the image embedding vector from the vision encoder to the input token space of the LLM. MLLMs typically use cross-attention layers [29] or feed-forward networks (FFNs) as the vision-language projector.\nLLM: In an MLLM, the LLM takes the output of the vision-language projector, concatenates it with the question's tokens, and generates a text response. LLMs are typically based on the transformer architecture [55]. The self-attention mechanism in transformers captures long-range dependencies and contextual information, making them highly effective for tasks such as language modeling and question answering."}, {"title": "2.2 Adversarial Examples", "content": "Adversarial examples are intentionally crafted or perturbed inputs causing a machine learning model to make incorrect predictions [51]. Adversarial perturbations are often nearly-imperceptible to humans but can significantly affect a model's behavior. For MLLMs, adversarial examples can be applied to the image [47, 4, 43, 34, 7, 9, 20] and/or question [2, 24] in a prompt. In this work, we focus on perturbations that are added to an image in a normal user's prompt to an MLLM. This is because a user may obtain the image from an untrusted source (e.g., Internet), in which an attacker may add a perturbation to it. Additionally, it is more challenging for an attacker to perturb the question in a normal user's prompt since it is often came up by the user himself/herself.\nOne type of image adversarial example for MLLMs is called jailbreaking [43, 9, 35], which aims to bypass an MLLM's safety guardrails, causing it to generate responses containing harmful content. For instance, Qi et al. [43] propose to optimize a universal image adversarial example that causes an MLLM to generate responses containing harmful content when including the image adversarial example with an"}, {"title": "3 Problem Definition", "content": ""}, {"title": "3.1 System Setup", "content": "Our system setup involves three key roles: a model provider, competitors, and normal users. The model provider and competitors are resourceful companies, such as Meta, Google, and OpenAI, which invest substantially in training and deploying their own MLLMs as online chatbots or APIs. These companies fiercely compete for MLLM market dominance, continually enhancing their models' capabilities and user experiences. We refer to a competitor's MLLM as a competing MLLM and assume normal users query MLLMs with safe prompts."}, {"title": "Safe/Unsafe prompt:", "content": "A prompt for an MLLM consists of an image and a question. We define a prompt as safe if and only if both the image and the question do not contain any harmful and inappropriate content. If either the image or the question contains harmful content, the prompt is considered as unsafe."}, {"title": "3.2 Threat Model", "content": "Model provider's goal: Model provider is an attacker who aims to gain a competitive advantage in the MLLM \u201carms race\" by leveraging effective refusal perturbations. In particular, the model provider aims to craft effective refusal perturbations as shown in the third row of Figure 1. The model provider then publishes these perturbed images on the Internet, e.g., via social media. When normal users use a competing MLLM to answer questions about these perturbed images, the competing MLLM generates refusal responses. The model provider has the following three goals when crafting refusal perturbations: effectiveness, locality, and stealthiness."}, {"title": "4 Our MLLM-Refusal", "content": ""}, {"title": "4.1 Overview", "content": "Figure 3 is an overview of our MLLM-Refusal. Generally speaking, given an image, our MLLM-Refusal aims to optimize a refusal perturbation that achieves all effectiveness, locality, and stealthiness goals when added to the image. First, we construct a set of shadow questions using an LLM such as"}, {"title": "4.2 Constructing Shadow Questions", "content": "Exact user questions: When the model provider knows the exact user questions, such as \u201cWho is the artist of this image?\", shadow questions can be constructed to directly match these user questions.\nSimilar user questions: When the model provider understands the main ideas of potential user questions, it can create an example question and then generate a set of similar user questions via an LLM. In particular, our MLLM-Refusal uses the prompt shown in Figure 4 to generate these similar user questions. An example prompt and its resulting questions are provided in Figure 22 in the Appendix.\nGeneral user questions: In cases the model provider lacks specific information about user questions, an LLM can be utilized to generate general user questions that simulate normal user's questions about any image. In particular, our MLLM-Refusal uses the prompt shown in Figure 5 for this purpose. An example prompt and the resulting general user questions are illustrated in Figure 21 in the Appendix.\""}, {"title": "4.3 Formalizing the Model Provider's Goals", "content": "We denote a set of competing MLLMs as M and the constructed set of shadow questions as Qs. Given an image x1, the model provider aims to find a refusal perturbation \u03b4R, such that each competing MLLM"}, {"title": "4.4 Solving the Optimization Problem", "content": "Our MLLM-Refusal solves the optimization problem in Equation 3 via a gradient-based method called basic iterative method (BIM) [26]. Specifically, we initialize the refusal perturbation as a zero tensor that matches the dimensions of x1. In each iteration, we randomly select a mini-batch of shadow questions QB from the set of shadow questions Qs, i.e., QB \u2286 Qs. We then compute the gradient g for the average cross-entropy loss, i.e., g = \u2207\u03b4RL(M, R, x1 + \u03b4R, QB). Our MLLM-Refusal then updates \u03b4R as follows:\n\u03b4R = \u03b4R - \u03b1 \u00b7 sign(g),   (4)\nwhere \u03b1 is the step size and sign(\u00b7) is the sign function. At the end of each iteration, we project \u03b4R = proj(\u03b4R, \u03f5) to satisfy the constraint such that ||\u03b4R||\u221e\u2264 \u03f5. We repeat this process for max_iter iterations. Algorithm 1 summarizes our MLLM-Refusal. In Section 5.2, we show that when our MLLM-Refusal uses another popular method called projected gradient descent (PGD) [37], it can achieve comparable effectiveness but is less efficient."}, {"title": "5 Evaluations", "content": ""}, {"title": "5.1 Experimental Setup", "content": ""}, {"title": "5.1.1 Datasets", "content": "To evaluate the refusal of MLLMs, we need to use image-question pairs to simulate user prompts to MLLMs. Therefore, we use image-question pairs from three popular visual question answering datasets, VQAv2 [3], GQA [21], and TextVQA [49]. We also extend the CelebA [33] image dataset into a visual question dataset to represent common queries about celebrity facial images. The process of generating relevant questions for CelebA is detailed in Section A of the Appendix. Table 1 summarizes the key statistics of these datasets. For evaluation, we randomly sample 100 image-question pairs from each dataset's test or validation split."}, {"title": "5.1.2 User Questions", "content": "To mimic practical usage by normal users, we consider both image-relevant and image-irrelevant user questions. Image-relevant questions, directly related to the input image, represent the primary use case for MLLM users. Specifically, we use the questions associated with images in each dataset as image-relevant questions. Moreover, we also consider image-irrelevant questions. Considering the following scenario, a user might query an MLLM with questions about an image in the initial rounds, then continue with unrelated questions without starting a new chat session. Since the image remains in the MLLM's context, it may still influence subsequent queries. For image-irrelevant questions, we use the CommonsenseQA [52] dataset, which contains various questions unrelated to any image. For example, the question \"What is a likely consequence of ignorance of rules?\" is an image-irrelevant question in this dataset. We randomly sample 100 questions from this dataset and pair them with images from our image-question datasets to construct the prompts."}, {"title": "5.1.3 MLLMS", "content": "We evaluate four popular open-source MLLMs: LLaVA-1.5 [30], MiniGPT-4 [58], Qwen-VL-Chat [6], and InstructBLIP [14]. These MLLMs use various configurations of vision encoders, LLMs, and vision-language projectors, as summarized in Table 2. All MLLMs use CLIP [11] family vision encoders with varying sizes ranging from 428M to 2B parameters. The LLMs in these MLLMs consistently have 7B parameters, though they differ in pre-training algorithms and data. Notably, each MLLM implements a unique vision-language projector architecture. For consistency in evaluation, all image inputs are resized to a uniform resolution of 224\u00d7224 pixels across all MLLMs."}, {"title": "5.1.4 Evaluation Metrics", "content": "To evaluate the effectiveness and locality of our MLLM-Refusal, we use the refusal rate as our evaluation metric. For an MLLM M, given a dataset of N image-question pairs and corresponding refusal perturbations crafted by our MLLM-Refusal, the refusal rate is defined as $\\frac{N_R}{N}$, where NR is the number of refusals of the MLLM M when given N perturbed image-question pairs. Considering the intrinsic randomness in MLLM response generation due to sampling and temperature in LLM decoding strategies, we query each MLLM three times per image-question pair by default. The resulting refusal rates are then averaged to mitigate the effects of this randomness.\nTo evaluate whether an MLLM's response is a refusal, we use an LLM as the refusal judge. Specifically, we use GPT-4 [1] as the refusal judge LLM due to its recognized capability. We query the refusal judge LLM with the MLLM's response and the prompt in Figure 7 to determine if the response is a refusal."}, {"title": "5.1.5 Compared Methods", "content": "We extend two existing image adversarial examples to MLLMs [4, 43] to our scenario. In our context, these perturbations are repurposed to induce MLLMs' refusals of safe prompts. We also consider a variant of our MLLM-Refusal. Specifically, we consider the following three compared methods:\n\u2022 Qi et al.[43]: This method adds perturbations to the image input to elicit toxic responses from the MLLM when the set of shadow questions is empty. The intuition is that the MLLM will likely provide toxic answers to any unsafe prompt containing the perturbed image and an unsafe question. In our extension, we maintain an empty set of shadow questions during the optimization of the refusal perturbation.\n\u2022 Bagdasaryan et al. [4]: This approach optimizes the perturbation token by token. During optimization, a possible refusal R may contain r tokens. While our MLLM-Refusal optimizes the perturbation to increase the probability of the entire sequence of r tokens (as shown in Equation 2), their method optimizes the perturbation to increase the probability of each desired next token given its prefix. Consequently, for a possible refusal R with r tokens, our MLLM-Refusal requires one optimization step, whereas theirs requires r steps.\n\u2022 MLLM-Refusal + PGD: This variant of MLLM-Refusal replaces the basic iterative method (BIM) with projected gradient descent (PGD) [37] to optimize the refusal perturbation. Specifically, PGD uses the exact gradient values rather than the sign of the gradient for updating the refusal perturbation. We use a learning rate of 0.3 and a maximum of 1500 iterations when shadow questions are exact user questions, and a learning rate of 0.4 with a maximum of 2000 iterations when shadow questions are similar or general user questions."}, {"title": "5.1.6 Parameter Setting", "content": "Unless otherwise mentioned, we consider one competing MLLM and image-relevant questions. In terms of the set of shadow questions, we use one exact user question, ten similar user questions and fifty general user questions, respectively. In Section 5.2, we will show the impact of the number of shadow questions when the model provider knows similar or general user questions. To achieve the stealthiness goal, we constrain the refusal perturbation using an $l_\\infty$-norm bound of 8/255, which is considered stealthy in previous works [43, 34, 7]. We conduct a grid search for key hyperparameters in our MLLM-Refusal (Algorithm 1): step size \u03b1, maximum number of iterations, and mini-batch size of shadow questions. This search is performed separately for the model provider's different background knowledges of exact, similar, or general user questions. Section 5.2 shows the impact of different hyperparameter settings. To prevent overfitting of optimized refusal perturbations to shadow questions when using similar and general user questions, we implement early stopping in the optimization process if the loss in Equation 3 remains below 0.001 for 30 consecutive iterations."}, {"title": "5.2 Experimental Results", "content": "MLLM-Refusal outperforms compared methods: Table 3 shows the refusal rates of compared methods and our MLLM-Refusal using three types of shadow questions, with LLaVA-1.5 as the competing MLLM on the VQAv2 dataset. We make four key observations. First, MLLM-Refusal consistently achieves the highest refusal rates across all shadow question types. Specifically, MLLM-Refusal achieves a refusal rate of 0.88, while Bagdasaryan et al. achieve a refusal rate of only 0.51 when the shadow questions are general user questions. Second, Qi et al. [43] is ineffective when extended to our scenarios, achieving near-zero refusal rates. This ineffectiveness arises because their method uses only images and does not incorporate shadow questions. Consequently, the refusal perturbations optimized with an empty set of shadow questions are unlikely to cause refusals for the competing MLLM when users query with actual non-empty questions. This indicates the necessity of constructing a set of shadow questions to optimize effective refusal perturbations. Third, the refusal rates are zero when no perturbations are added to images, demonstrating that the image-question pairs in the VQAv2 dataset"}, {"title": "5. MLLM-Refusal achieves the locality goal:", "content": "Figure 8 shows the refusal rates of our MLLM-Refusal among four competing MLLMs for user questions that are either image-relevant or image-irrelevant. In both subfigures, diagonal values represent the refusal rates on each competing MLLM, while off-diagonal values represent the refusal rates on non-competing MLLMs. Our MLLM-Refusal demonstrates high refusal rates on the competing MLLM and near-zero refusal rates on non-competing MLLMs. This indicates that our MLLM-Refusal achieves the locality goal. Additionally, we find that MLLM-Refusal achieves slightly higher refusal rates on Qwen-VL-Chat compared to other non-competing MLLMs when Qwen-VL-Chat is not the competing MLLM. For instance, when LLaVA-1.5 is the competing MLLM with image-irrelevant user questions, MLLM-Refusal achieves a 0.08 refusal rate on Qwen-VL-Chat while achieving 0.01 refusal rates on both MiniGPT-4 and InstructBLIP."}, {"title": "Multiple competing MLLMs:", "content": "Figure 9 shows the refusal rates of MLLM-Refusal with multiple competing MLLMs. Since all competing MLLMs need to be loaded on GPUs, the total amount"}, {"title": "5. Impact of step size \u03b1:", "content": "The step size \u03b1 in our MLLM-Refusal (Algorithm 1) determines the magnitude of refusal perturbation change per iteration after gradient calculation. Figure 10 shows the impact of different step sizes on the refusal rates of MLLM-Refusal with three types of shadow questions. Figure 10 illustrates the impact of varying step sizes on MLLM-Refusal's refusal rates for three types of shadow questions. Figure 10a reveals a significant improvement in the refusal rate as the step size increases from 0.006 to 0.007, with 0.007 being optimal for exact user questions. In contrast, when using similar and general user questions as shadow questions (Figures 10b and 10c, respectively), refusal rates are less sensitive to varying step sizes, with 0.005 being optimal. These findings suggest that the optimal step size depends on the shadow questions' type: higher values (around 0.007) are more effective for"}, {"title": "Impact of the perturbation constraint:", "content": "Recall that our MLLM-Refusal applies l\u221e-norm constraint \u03f5 to refusal perturbations to achieve the stealthiness goal. Following previous work [43, 34, 7], we choose an l-norm perturbation constraint \u03f5 smaller than 16/255, which is considered stealthy. Figure 12 shows the results of MLLM-Refusal when varying the l\u221e-norm perturbation constraint \u03f5. We observe that when \u03f5 = 8/255, MLLM-Refusal achieves the highest refusal rate. The refusal rate then decreases as \u03f5 increases further. This suggests that a larger \u03f5 may lead to overfitting of refusal perturbations on shadow questions, causing the image swith refusal perturbations to less likely to cause refusal when prompting with actual user questions. This trend is also observed in previous work [43] on adversarial examples for image inputs of MLLMs. If \u03f5 is too small, e.g., 4/255, the refusal perturbations may be underfitted due to the overly constrained search space for refusal perturbations."}, {"title": "Impact of the mini-batch size of shadow questions:", "content": "Our MLLM-Refusal random samples a mini-batch from shadow questions to optimize the refusal perturbations every iteration. Figure 13 shows the refusal rates of MLLM-Refusal when varying the mini-batch sizes of shadow questions. We observe that the refusal rate of MLLM-Refusal increases from 0.82 as the mini-batch size of shadow questions increases from 1, and then converges at approximately 0.86 when the mini-batch size exceeds 3. This trend indicates that a mini-batch size of shadow questions below 3 may be suboptimal for MLLM-Refusal."}, {"title": "Impact of the size of shadow questions:", "content": "Recall that our MLLM-Refusal uses a set of shadow questions to mimic the normal users' questions to an image. Figure 14 shows the refusal rates of MLLM-Refusal when varying the sizes of the shadow questions set. The refusal rate increases from approximately 0.86 as the set size grows from 20, converging at around 0.88 when the size exceeds 40. This trend suggests that a larger shadow questions set, with at least 40 questions, enhances MLLM-Refusal's refusal rate on competing MLLM."}, {"title": "Impact of the temperature of competing MLLM:", "content": "The temperature in an MLLM determines the randomness and diversity of the model's answers, controlling how deterministic or exploratory the model behaves. A lower temperature results in more deterministic answers, while a higher temperature increases answers' randomness and diversity. We study the impact of the temperature of the competing MLLM on our MLLM-Refusal and show the results in Figure 15. We observe that our MLLM-Refusal consistently achieves high refusal rates between 0.86 and 0.89 across various temperatures. This indicates that the"}, {"title": "Effect of experiment trials:", "content": "Recall that we repeat each query for each competing MLLM three trials and average the refusal rates to mitigate the randomness of MLLM's decoding strategies such as temperature setting and sampling. Figure 16 shows the impact of the number of trials on our MLLM-Refusal. We observe that the refusal rate of MLLM-Refusal is not sensitive to the number of trials. This is likely because, although the refusal responses may vary, as long as they are refusal responses, our refusal judge LLM will classify them as refusals."}, {"title": "6 Countermeasures", "content": "Images with refusal perturbation crafted by MLLM-Refusal can be considered as a type of adversarial example. Various countermeasures [39, 18, 8, 32] have been proposed to defend against adversarial examples. We categorize these countermeasures into testing-time and training-time countermeasures. Our MLLM-Refusal is evaluated against two popular testing-time countermeasures: Gaussian noise and DiffPure [39], as well as one training-time countermeasure: adversarial training [18].\nIn addition to refusal rate to evaluate the effectiveness of MLLM-Refusal, we use accuracy to evaluate the utility of the competing MLLM. Specifically, accuracy is the fraction of correctly answered image-question pairs when applying the countermeasure to clean images without refusal perturbations"}, {"title": "Gaussian noise:", "content": "To counter the refusal perturbation, a competing MLLM can add Gaussian noise N(0, \u03c3) to the image input, where \u03c3 represents the standard deviation. Higher \u03c3 values result in more visually prominent noise.\nFigure 17 shows the accuracy of the competing MLLM on visual question answering and the refusal rates of MLLM-Refusal when adding Gaussian noise as the countermeasure. The competing MLLM is LLaVA-1.5, and the dataset is VQAv2. We have two main observations. First, larger noise (i.e., larger \u03c3) added to the image inputs is more effective at mitigating the impact of MLLM-Refusal. For example, without Gaussian noise (i.e., \u03c3 = 0), the refusal rates of MLLM-Refusal with three types of shadow questions are all higher than 0.90. When \u03c3 = 0.02, the refusal rates of MLLM-Refusal with three types of shadow questions are nearly zero. Second, adding Gaussian noise compromises the accuracy of the competing MLLM on visual question answering. Specifically, the accuracy drops significantly from 0.92 to around 0.80 when adding Gaussian noise with \u03c3 = 0.02. Therefore, adding Gaussian noise as a countermeasure is insufficient."}, {"title": "DiffPure [39]:", "content": "DiffPure can purify images with refusal perturbations by utilizing a diffusion model. Through iterative steps, DiffPure first adds Gaussian noise adaptively to the image input. The noised im-"}, {"title": "Adversarial training [18]:", "content": "We apply adversarial training to improve the robustness of the competing MLLM against refusal perturbations crafted by our MLLM-Refusal. In this scenario, we assume that the competing MLLM's model provider has detected and collected some image inputs with refusal perturbations crafted by MLLM-Refusal. Specifically, we randomly split the 100 image-question pairs with refusal perturbations into two equal parts. The first half is used as training data for adversarial training, while the second half serves as testing data. Following LLaVA-1.5 [30], we fine-tune both the vision-language projector and the LLM in LLaVA-1.5 on our training data. To reduce the computational cost of fine-tuning, we use a parameter-efficient method LoRA [19]. All training parameters follow the default settings in LLaVA-1.5.\nFigure 19 shows the accuracy of the competing MLLM and the refusal rates of MLLM-Refusal when using adversarial training with different training epochs. We use three types of shadow questions with LLaVA-1.5 on VQAv2. We observe that refusal rates of MLLM-Refusal remain around 60% even after three training epochs. However, the accuracy of the competing MLLM significantly decreases after adversarial training. Moreover, adversarial training requires substantially more computational resources. Note that the detection and collection of image inputs with refusal perturbations during testing is also challenging since refusal perturbations crafted by MLLM-Refusal are stealthy."}, {"title": "7 Discussion and Limitations", "content": ""}, {"title": "Multi-round visual question answering:", "content": "As the capability to process increasingly lengthy contexts becomes critical for MLLMs, users can pose a series of successive questions regarding the initial image in multi-round question answer with an MLLM. In this case, we consider one multi-round question answering session where only initial prompt containing an image with refusal perturbation, the following question answering takes previous question-answer history in context. We want to study how many round an image with refusal perturbation can impact during multi-round question answering. Figure 20 shows the refusal rate in multi-round question answering for different types of shadow questions. For the reason of simplicity, the same user question is used in all chatting rounds. We observe that when the shadow questions use exact or similar user questions\u2014the refusal rate tends to stabilize at a relatively high level as the number of chatting rounds increases, with a reduction in refusal rate of less than 5%. Conversely, longer context lengths"}]}