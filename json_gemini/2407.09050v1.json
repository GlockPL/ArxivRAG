{"title": "Refusing Safe Prompts for Multi-modal Large Language Models", "authors": ["Zedian Shao", "Hongbin Liu", "Yuepeng Hu", "Neil Zhenqiang Gong"], "abstract": "Multimodal large language models (MLLMs) have become the cornerstone of today's generative\nAl ecosystem, sparking intense competition among tech giants and startups. In particular, an\nMLLM generates a text response given a prompt consisting of an image and a question. While\nstate-of-the-art MLLMs use safety filters and alignment techniques to refuse unsafe prompts, in\nthis work, we introduce MLLM-Refusal, the first method that induces refusals for safe prompts.\nIn particular, our MLLM-Refusal optimizes a nearly-imperceptible refusal perturbation and adds\nit to an image, causing target MLLMs to likely refuse a safe prompt containing the perturbed\nimage and a safe question. Specifically, we formulate MLLM-Refusal as a constrained optimization\nproblem and propose an algorithm to solve it. Our method offers competitive advantages for MLLM\nmodel providers by potentially disrupting user experiences of competing MLLMs. Since competing\nMLLM's users will receive unexpected refusals when they unwittingly use these perturbed images\nin their prompts. We evaluate MLLM-Refusal on four MLLMs across four datasets, demonstrating\nits effectiveness in causing competing MLLMs to refuse safe prompts while not affecting non-\ncompeting MLLMs. Furthermore, we explore three potential countermeasures-adding Gaussian\nnoise, DiffPure, and adversarial training. Our results show that they are insufficient: though they can\nmitigate MLLM-Refusal's effectiveness, they also sacrifice the accuracy and/or efficiency of the\ncompeting MLLM. The code is available at https://github.com/Sadcardation/MLLM-Refusal.", "sections": [{"title": "1 Introduction", "content": "Multimodal large language models (MLLMs) [40, 46, 30, 58, 14, 6] have emerged as a groundbreak-\ning foundation for various generative AI applications such as visual question answering [30], image\ncaptioning [25], and embodied AI [16]. MLLMs are typically trained and deployed as web chatbots or\ncloud API services by resourceful entities, including major technology companies and startups. These\ncompanies are fiercely competing in the development of MLLMs, exemplified by Google's Gemini\nPro [46] and OpenAI's GPT-4 [1]. An MLLM generally comprises three key components: a vision\nencoder, a vision-language projector, and a large language model (LLM). The vision encoder generates\nan embedding vector for an image, which the vision-language projector converts into tokens in the\nLLM's input token space. These tokens are concatenated with the question's tokens and fed into the\nLLM, producing the final text response.\nDespite rapid progress in MLLMs' capabilities, their safety has garnered increasing attention. A\nrecent U.S. Executive Order [53] underscores the commitments from AI companies to ensure the safety\nand security of advanced AI systems. A safeguarded MLLM typically prevents generating harmful\noutputs by refusing unsafe prompts [23, 38, 42, 45]. Specifically, if either the prompt's image or question\nis unsafe, i.e., containing harmful content, the prompt is considered unsafe. For example, the first two\nrows in Figure 1 illustrate two types of unsafe prompts and the corresponding refusals from an MLLM.\nMLLMs can achieve this through safety filters [23, 56] and alignment techniques [42, 45, 13]."}, {"title": "Related Work", "content": ""}, {"title": "2.1 MLLMS", "content": "Generally speaking, MLLMs[30, 58, 14, 6] are LLMs extended with the ability to deal with visual input.\nSpecifically, an MLLM generates a text response to a prompt containing an image and a text question.\nAn MLLM typically comprises three main components: a vision encoder, a vision-language projector,\nand an LLM.\nVision encoder: Given an image input, an MLLM uses a vision encoder to produce an image embedding\nvector. Vision encoders are often pre-trained on large datasets of unlabeled images or image-text\npairs through self-supervised learning [41, 10, 44]. State-of-the-art vision encoders typically utilize\nconvolutional neural networks or vision transformers (ViT). In particular, CLIP's vision encoders[44]\nare commonly used in many MLLMs [30, 31].\nVision-language projector: Since the output space of the vision encoder and the input space of the\nLLM are different, an MLLM uses a vision-language projector to align the image embedding vector\nfrom the vision encoder to the input token space of the LLM. MLLMs typically use cross-attention\nlayers [29] or feed-forward networks (FFNs) as the vision-language projector.\nLLM: In an MLLM, the LLM takes the output of the vision-language projector, concatenates it with\nthe question's tokens, and generates a text response. LLMs are typically based on the transformer\narchitecture [55]. The self-attention mechanism in transformers captures long-range dependencies and\ncontextual information, making them highly effective for tasks such as language modeling and question\nanswering."}, {"title": "2.2 Adversarial Examples", "content": "Adversarial examples are intentionally crafted or perturbed inputs causing a machine learning model to\nmake incorrect predictions [51]. Adversarial perturbations are often nearly-imperceptible to humans\nbut can significantly affect a model's behavior. For MLLMs, adversarial examples can be applied\nto the image [47, 4, 43, 34, 7, 9, 20] and/or question [2, 24] in a prompt. In this work, we focus on\nperturbations that are added to an image in a normal user's prompt to an MLLM. This is because a\nuser may obtain the image from an untrusted source (e.g., Internet), in which an attacker may add a\nperturbation to it. Additionally, it is more challenging for an attacker to perturb the question in a normal\nuser's prompt since it is often came up by the user himself/herself.\nOne type of image adversarial example for MLLMs is called jailbreaking [43, 9, 35], which aims to\nbypass an MLLM's safety guardrails, causing it to generate responses containing harmful content. For\ninstance, Qi et al. [43] propose to optimize a universal image adversarial example that causes an MLLM\nto generate responses containing harmful content when including the image adversarial example with an"}, {"title": "3 Problem Definition", "content": ""}, {"title": "3.1 System Setup", "content": "Our system setup involves three key roles: a model provider, competitors, and normal users. The model\nprovider and competitors are resourceful companies, such as Meta, Google, and OpenAI, which invest\nsubstantially in training and deploying their own MLLMs as online chatbots or APIs. These companies\nfiercely compete for MLLM market dominance, continually enhancing their models' capabilities and\nuser experiences. We refer to a competitor's MLLM as a competing MLLM and assume normal users\nquery MLLMs with safe prompts.\nSafe/Unsafe prompt: A prompt for an MLLM consists of an image and a question. We define a prompt\nas safe if and only if both the image and the question do not contain any harmful and inappropriate\ncontent. If either the image or the question contains harmful content, the prompt is considered as unsafe."}, {"title": "3.2 Threat Model", "content": "Model provider's goal: Model provider is an attacker who aims to gain a competitive advantage in\nthe MLLM \u201carms race\" by leveraging effective refusal perturbations. In particular, the model provider\naims to craft effective refusal perturbations as shown in the third row of Figure 1. The model provider\nthen publishes these perturbed images on the Internet, e.g., via social media. When normal users use a\ncompeting MLLM to answer questions about these perturbed images, the competing MLLM generates\nrefusal responses. The model provider has the following three goals when crafting refusal perturbations:\neffectiveness, locality, and stealthiness.\n\u2022 Effectiveness goal. This goal means that refusal perturbations can trigger refusals from competing\nMLLMs when a user prompt's image is embedded with a refusal perturbation. Specifically, when\nusers query competing MLLMs with safe prompts containing images with refusal perturbations,\nthese MLLMs should be likely to respond with refusals.\n\u2022 Locality goal. The refusal perturbation should be effective against competing MLLMs but\nineffective against non-competing MLLMs including the model provider's own MLLM. When\nusers query the model provider's MLLM with the same prompts, it should generate normal,\nappropriate responses. This stark performance difference aims to showcase the apparent superiority\nof the model provider's MLLM, potentially attracting more users and conferring a significant\ncompetitive advantage.\""}, {"title": "4 Our MLLM-Refusal", "content": ""}, {"title": "4.1 Overview", "content": "Figure 3 is an overview of our MLLM-Refusal. Generally speaking, given an image, our MLLM-\nRefusal aims to optimize a refusal perturbation that achieves all effectiveness, locality, and stealthiness\ngoals when added to the image. First, we construct a set of shadow questions using an LLM such as"}, {"title": "4.2 Constructing Shadow Questions", "content": "Exact user questions: When the model provider knows the exact user questions, such as \u201cWho is the\nartist of this image?\", shadow questions can be constructed to directly match these user questions.\nSimilar user questions: When the model provider understands the main ideas of potential user\nquestions, it can create an example question and then generate a set of similar user questions via an\nLLM. In particular, our MLLM-Refusal uses the prompt shown in Figure 4 to generate these similar user\nquestions. An example prompt and its resulting questions are provided in Figure 22 in the Appendix.\nGeneral user questions: In cases the model provider lacks specific information about user questions,\nan LLM can be utilized to generate general user questions that simulate normal user's questions about\nany image. In particular, our MLLM-Refusal uses the prompt shown in Figure 5 for this purpose. An\nexample prompt and the resulting general user questions are illustrated in Figure 21 in the Appendix.\""}, {"title": "4.3 Formalizing the Model Provider's Goals", "content": "We denote a set of competing MLLMs as M and the constructed set of shadow questions as Qs. Given\nan image x1, the model provider aims to find a refusal perturbation \u03b4R, such that each competing MLLM"}, {"title": "4.4 Solving the Optimization Problem", "content": "Our MLLM-Refusal solves the optimization problem in Equation 3 via a gradient-based method called\nbasic iterative method (BIM) [26]. Specifically, we initialize the refusal perturbation as a zero tensor that\nmatches the dimensions of x1. In each iteration, we randomly select a mini-batch of shadow questions\nQB from the set of shadow questions Qs, i.e., QB \u2286 Qs. We then compute the gradient g for the\naverage cross-entropy loss, i.e., g = \u2207\u03b4RL(M, R, x1 + \u03b4R, QB). Our MLLM-Refusal then updates \u03b4R\nas follows:\n\u03b4R = \u03b4R - \u03b1 \u00b7 sign(g),\nwhere \u03b1 is the step size and sign(\u00b7) is the sign function. At the end of each iteration, we project\n\u03b4R = proj(\u03b4R, \u03f5) to satisfy the constraint such that ||\u03b4R||\u221e \u2264 \u03f5. We repeat this process for max_iter\niterations. Algorithm 1 summarizes our MLLM-Refusal. In Section 5.2, we show that when our\nMLLM-Refusal uses another popular method called projected gradient descent (PGD) [37], it can\nachieve comparable effectiveness but is less efficient."}, {"title": "5 Evaluations", "content": ""}, {"title": "5.1 Experimental Setup", "content": ""}, {"title": "5.1.1 Datasets", "content": "To evaluate the refusal of MLLMs, we need to use image-question pairs to simulate user prompts to\nMLLMs. Therefore, we use image-question pairs from three popular visual question answering datasets,\nVQAv2 [3], GQA [21], and TextVQA [49]. We also extend the CelebA [33] image dataset into a visual\nquestion dataset to represent common queries about celebrity facial images. The process of generating\nrelevant questions for CelebA is detailed in Section A of the Appendix. Table 1 summarizes the key\nstatistics of these datasets. For evaluation, we randomly sample 100 image-question pairs from each\ndataset's test or validation split."}, {"title": "5.1.2 User Questions", "content": "To mimic practical usage by normal users, we consider both image-relevant and image-irrelevant user\nquestions. Image-relevant questions, directly related to the input image, represent the primary use\ncase for MLLM users. Specifically, we use the questions associated with images in each dataset as\nimage-relevant questions. Moreover, we also consider image-irrelevant questions. Considering the\nfollowing scenario, a user might query an MLLM with questions about an image in the initial rounds,\nthen continue with unrelated questions without starting a new chat session. Since the image remains\nin the MLLM's context, it may still influence subsequent queries. For image-irrelevant questions, we\nuse the CommonsenseQA [52] dataset, which contains various questions unrelated to any image. For\nexample, the question \"What is a likely consequence of ignorance of rules?\" is an image-irrelevant\nquestion in this dataset. We randomly sample 100 questions from this dataset and pair them with images\nfrom our image-question datasets to construct the prompts."}, {"title": "5.1.3 MLLMS", "content": "We evaluate four popular open-source MLLMs: LLaVA-1.5 [30], MiniGPT-4 [58], Qwen-VL-Chat [6],\nand InstructBLIP [14]. These MLLMs use various configurations of vision encoders, LLMs, and vision-\nlanguage projectors, as summarized in Table 2. All MLLMs use CLIP [11] family vision encoders with\nvarying sizes ranging from 428M to 2B parameters. The LLMs in these MLLMs consistently have 7B\nparameters, though they differ in pre-training algorithms and data. Notably, each MLLM implements a\nunique vision-language projector architecture. For consistency in evaluation, all image inputs are resized\nto a uniform resolution of 224\u00d7224 pixels across all MLLMs."}, {"title": "5.1.4 Evaluation Metrics", "content": "To evaluate the effectiveness and locality of our MLLM-Refusal, we use the refusal rate as our evaluation\nmetric. For an MLLM M, given a dataset of N image-question pairs and corresponding refusal\nperturbations crafted by our MLLM-Refusal, the refusal rate is defined as NR/N, where NR is the number\nof refusals of the MLLM M when given N perturbed image-question pairs. Considering the intrinsic\nrandomness in MLLM response generation due to sampling and temperature in LLM decoding strategies,\nwe query each MLLM three times per image-question pair by default. The resulting refusal rates are\nthen averaged to mitigate the effects of this randomness.\nTo evaluate whether an MLLM's response is a refusal, we use an LLM as the refusal judge.\nSpecifically, we use GPT-4 [1] as the refusal judge LLM due to its recognized capability. We query the\nrefusal judge LLM with the MLLM's response and the prompt in Figure 7 to determine if the response\nis a refusal."}, {"title": "5.1.5 Compared Methods", "content": "We extend two existing image adversarial examples to MLLMs [4, 43] to our scenario. In our context,\nthese perturbations are repurposed to induce MLLMs' refusals of safe prompts. We also consider a\nvariant of our MLLM-Refusal. Specifically, we consider the following three compared methods:\n\u2022 Qi et al.[43]: This method adds perturbations to the image input to elicit toxic responses from\nthe MLLM when the set of shadow questions is empty. The intuition is that the MLLM will\nlikely provide toxic answers to any unsafe prompt containing the perturbed image and an unsafe\nquestion. In our extension, we maintain an empty set of shadow questions during the optimization\nof the refusal perturbation.\n\u2022 Bagdasaryan et al. [4]: This approach optimizes the perturbation token by token. During\noptimization, a possible refusal R may contain r tokens. While our MLLM-Refusal optimizes the\nperturbation to increase the probability of the entire sequence of r tokens (as shown in Equation 2),\ntheir method optimizes the perturbation to increase the probability of each desired next token\ngiven its prefix. Consequently, for a possible refusal R with r tokens, our MLLM-Refusal requires\none optimization step, whereas theirs requires r steps.\n\u2022 MLLM-Refusal + PGD: This variant of MLLM-Refusal replaces the basic iterative method (BIM)\nwith projected gradient descent (PGD) [37] to optimize the refusal perturbation. Specifically,\nPGD uses the exact gradient values rather than the sign of the gradient for updating the refusal\nperturbation. We use a learning rate of 0.3 and a maximum of 1500 iterations when shadow\nquestions are exact user questions, and a learning rate of 0.4 with a maximum of 2000 iterations\nwhen shadow questions are similar or general user questions."}, {"title": "5.1.6 Parameter Setting", "content": "Unless otherwise mentioned, we consider one competing MLLM and image-relevant questions. In\nterms of the set of shadow questions, we use one exact user question, ten similar user questions and\nfifty general user questions, respectively. In Section 5.2, we will show the impact of the number of\nshadow questions when the model provider knows similar or general user questions. To achieve the\nstealthiness goal, we constrain the refusal perturbation using an l\u221e-norm bound of 8/255, which is\nconsidered stealthy in previous works [43, 34, 7]. We conduct a grid search for key hyperparameters in\nour MLLM-Refusal (Algorithm 1): step size \u03b1, maximum number of iterations, and mini-batch size of\nshadow questions. This search is performed separately for the model provider's different background\nknowledges of exact, similar, or general user questions. Section 5.2 shows the impact of different\nhyperparameter settings. To prevent overfitting of optimized refusal perturbations to shadow questions\nwhen using similar and general user questions, we implement early stopping in the optimization process\nif the loss in Equation 3 remains below 0.001 for 30 consecutive iterations."}, {"title": "5.2 Experimental Results", "content": "MLLM-Refusal outperforms compared methods: Table 3 shows the refusal rates of compared\nmethods and our MLLM-Refusal using three types of shadow questions, with LLaVA-1.5 as the\ncompeting MLLM on the VQAv2 dataset. We make four key observations. First, MLLM-Refusal\nconsistently achieves the highest refusal rates across all shadow question types. Specifically, MLLM-\nRefusal achieves a refusal rate of 0.88, while Bagdasaryan et al. achieve a refusal rate of only 0.51 when\nthe shadow questions are general user questions. Second, Qi et al. [43] is ineffective when extended\nto our scenarios, achieving near-zero refusal rates. This ineffectiveness arises because their method\nuses only images and does not incorporate shadow questions. Consequently, the refusal perturbations\noptimized with an empty set of shadow questions are unlikely to cause refusals for the competing MLLM\nwhen users query with actual non-empty questions. This indicates the necessity of constructing a set of\nshadow questions to optimize effective refusal perturbations. Third, the refusal rates are zero when no\nperturbations are added to images, demonstrating that the image-question pairs in the VQAv2 dataset"}, {"title": "6 Countermeasures", "content": "Images with refusal perturbation crafted by MLLM-Refusal can be considered as a type of adversarial\nexample. Various countermeasures [39, 18, 8, 32] have been proposed to defend against adversarial\nexamples. We categorize these countermeasures into testing-time and training-time countermeasures.\nOur MLLM-Refusal is evaluated against two popular testing-time countermeasures: Gaussian noise and\nDiffPure [39], as well as one training-time countermeasure: adversarial training [18].\nIn addition to refusal rate to evaluate the effectiveness of MLLM-Refusal, we use accuracy to\nevaluate the utility of the competing MLLM. Specifically, accuracy is the fraction of correctly answered\nimage-question pairs when applying the countermeasure to clean images without refusal perturbations"}, {"title": "7 Discussion and Limitations", "content": "Multi-round visual question answering: As the capability to process increasingly lengthy contexts\nbecomes critical for MLLMs, users can pose a series of successive questions regarding the initial image\nin multi-round question answer with an MLLM. In this case, we consider one multi-round question\nanswering session where only initial prompt containing an image with refusal perturbation, the following\nquestion answering takes previous question-answer history in context. We want to study how many\nround an image with refusal perturbation can impact during multi-round question answering. Figure 20\nshows the refusal rate in multi-round question answering for different types of shadow questions. For\nthe reason of simplicity, the same user question is used in all chatting rounds. We observe that when the\nshadow questions use exact or similar user questions\u2014the refusal rate tends to stabilize at a relatively\nhigh level as the number of chatting rounds increases, with a reduction in refusal rate of less than\n5%. Conversely, longer context lengths reduce the effectiveness of added perturbations when shadow\nquestions are general user questions, causing the refusal rate to decrease from 88% to around 65%.\nAn interesting future work is to enhance the robustness of our MLLM-Refusal for multi-round visual\nquestion answering under different types of shadow questions. For instance, we could incorporate\nhistorical context into the optimization process of perturbation and expand our shadow questions set for\nbetter generalization to user questions.\nMore modalities: MLLMs are expanding to incorporate more modalities, such as audio [46, 40] and\nvideo [46, 40, 28, 57, 36], alongside text and image. As MLLMs become increasingly sophisticated,\nhandling ever more complex input modalities, the potential vulnerabilities for refusing safe prompts\nacross these new modalities also grow. An interesting future work is to extend our MLLM-Refusal to\nthese additional modalities. For instance, we could explore adding nearly-imperceptible perturbations to\naudio waveforms or specific frames within videos, causing advanced MLLMs to refuse safe prompts\njust as effectively as with images in this work."}, {"title": "8 Conclusion and Future Work", "content": "In this work, we introduce MLLM-Refusal, the first method to induce refusals for safe prompts in\nMLLMs. Our method optimizes a nearly-imperceptible refusal perturbation that, when added to an\nimage, causes competing MLLMs to refuse safe prompts while not affecting non-competing models. We\ndemonstrate MLLM-Refusal's effectiveness and locality across four MLLMs and datasets, highlighting\nits potential to gain competitive advantages for the model provider via disrupting user experiences"}]}