{"title": "Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers", "authors": ["Mohammad Raza", "Natasa Milic-Frayling"], "abstract": "Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver. SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose such near-certain reasoning as a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems.", "sections": [{"title": "Introduction", "content": "Logical reasoning remains a persistent challenge for large language models (LLMs). Although these models demonstrate reasoning capabilities across various domains, their reasoning often lacks robustness and becomes increasingly error-prone as task complexity increases. Many recent approaches have made notable advancements in this active area of research. Chain-of-thought (CoT) prompting has demonstrated how the quality of reasoning can be improved by prompting the model to explicitly generate the steps of reasoning in natural language before arriving at the final answer [Wei et al., 2022]. Variants of CoT and other related prompting and fine-tuning approaches have shown further improvements [Zhou et al., 2023; Wang et al., 2023; Yu et al., 2024; Weng et al., 2023; Creswell et al., 2023]. To address the logical inconsistencies that can arise in such natural language approaches, another interesting direction is to incorporate LLMs with logical solvers or automated reasoning tools [Pan et al., 2023; Ye et al., 2023]. Rather than directly attempting reasoning with the LLM, these approaches use the LLM to infer a formal representation of the problem as a program that can be executed by the solver, as such automated reasoning tools guarantee logically sound inference by construction.\nWhile these approaches have demonstrated relative improvements in accuracy, we are still far from achieving robustness and reliability of reasoning. For instance, Figure 1 shows an example reasoning problem from the Law School Admissions Test on analytical reasoning [Zhong et al., 2022]. On tasks of such complexity, the best reported accuracy, achieved by a solver-augmented system, is only 43% [Pan et al., 2023]. Such lack of reliability especially hinders the practical usability of existing approaches: for example, if a system demonstrates 70% accuracy on benchmarks, then in practice the user can only be 70% confident that the answer is correct on an arbitrary new task. Hence the burden of verifying correctness is always on the user, which can be especially difficult and error-prone for complex reasoning tasks. Therefore, having a reliable signal of correctness with high confidence can be hugely beneficial to help reduce the overall manual effort and cost of verification.\nIn this work, we propose a new approach to correctly formalizing reasoning problems called Semantic Self-Verification (SSV), which offers two key benefits: (1) it improves the overall accuracy of reasoning significantly over SOTA, and (2) it provides a novel feature of verification that has near-perfect precision. In our problem formulation, in addition to producing an answer to a given question, the system also indicates if it was able to verify the correctness of the answer: Question \u2192 (Answer, isVerified). This problem formulation is similar to confidence estimation in machine learning, where the system provides a score of confidence in addition to the answer. However, similar to selective classification [Chow, 1970], in our case the isVerified indicator is a boolean rather than continuous value: if true, it indicates a \"near certain\" confidence in the correctness of the answer, and otherwise there is no specific indication of confidence. The goal is to provide a high-confidence verification mechanism that can be used to reduce the need for manual checking in the cases where verification succeeds.\nAt its core, our approach addresses the key challenge in combining LLMs with the robust reasoning of logical solvers: the formulation of a problem from informal natural language (NL) to the formal representation that is a program executable"}, {"title": "Motivating Example", "content": "We consider the third constraint from the technicians problem in Figure 2, which requires that \"Stacy does not repair any type of machine that Yolanda repairs\". Figure 4 illustrates how the SSV approach works in this case. A direct translation using the LLM may produce an incorrect abstract formalization of this constraint as shown in Figure 4a, where the constraint is asserted only for some machine rather than for all machines because the Exists quantifier is incorrectly used. However, in the SSV approach, we use the LLM to also infer simple concrete instantiations, or examples, of the general NL constraint. For instance, a concrete positive example is that Stacy repairs radios and Yolanda repairs TVs. A concrete negative example is that Stacy and Yolanda cannot both repair TVs. After inferring these examples in NL, we also use the LLM to translate them to formal expressions in the language of the solver. We then use the solver to check that each of these expressions is satisfiable under the abstract formalization. In Figure 4a we see that the second instantiation fails verification because the abstract formalization does not assert the condition for all machine types, so it still allows for the possibility that Stacy and Yolanda can both repair TVs. However, with the correct formalization in Figure 4b that uses the ForAll quantifier, we see that both instantiations pass the solver verification, since the abstract formalization correctly disallows that any machine can be repaired by both Stacy and Yolanda. In the same way, SSV verifies all of the constraints identified in the full program by inferring concrete instantiations for them using the LLM. For instance, for the first constraint in Figure 2 it may infer a positive example that Xena, Urma, Wim and Stacy repair radios, and a negative example that only Xena and Urma repair radios."}, {"title": "Semantic Self-Verification", "content": "This section describes the semantic self-verification approach for reasoning problems, which generates programs verified and refined by concrete instantiations. Figure 5 presents the main algorithm, illustrating the top-level flow and key components. As formulated, the algorithm takes a question (Q), such as the technicians problem in Figure 1, and outputs an answer along with an indication of verification success. Figure 5 also details the algorithm's configuration parameters: the chosen LLM and solver, LLM temperature values, and the maximum repair attempts. We first outline the general algorithm before discussing its key phases in detail. For each temperature value to be explored, the algorithm first uses the LLM to infer a program P that the solver executes to answer the question Q, such as the program from Figure 2. If an executable program is generated (P \u2260 \u00d8), the verification loop begins (line 4). The solver first executes P to obtain an answer. Then, for verification, we infer concrete instantiations I, which are test cases for the program's constraints and options, such as the six constraints and five options in Figure 2. The solver attempts to verify that each instantiation is formally satisfiable and returns any failing instantiation Ifail. For example, for the third constraint in the technicians program, inferred instantiations (Figure 4a) may yield the failing case: \"Stacy and Yolanda cannot both repair TVs.\" If no failing instantiation is found (as in Figure 4b) and P satisfies general well-formedness properties, the algorithm returns its answer A along with verification success (line 12).\nIf verification fails, we attempt to repair the program P using the LLM and any failing instantiation, which provides insight into potential constraint implementation errors. For example, the failing instantiation in Figure 4a may guide the LLM to assert the condition for all machine types using the forall quantifier, as shown in Figure 4b. After obtaining the repaired program, we repeat the verification loop. If no answer is verified across all temperatures and repair attempts, we exit the outer loop (line 16). If no executable program was inferred, we fall back to direct inference using the LLM with a chain-of-thought prompt, as in prior work [Pan et al., 2023]. Otherwise, we return the best answer with verification failure. We next discuss key algorithm phases in more detail.\nProgram generation. The GenProgram function in Figure 5 uses the LLM to generate a solver-executable program for the given problem. A basic implementation relies on a direct LLM prompt, but we incorporate techniques from the code generation literature to improve quality. First, we use error-based refinement: syntax or execution errors in the generated program are fed back to the LLM for repair, a common approach in LLM-based code generation/reasoning domains [Chen et al., 2024; Pan et al., 2023]. Second, if direct code generation fails, we employ a compositional approach [Khot et al., 2023; Pourreza and Rafiei, 2024], generating the program incrementally for each identified constraint. This improves code quality compared to direct prompting, which often produces syntax errors. Our compositional code generation and refinement prompts are detailed in Appendix A.\nSemantic verification. While code generation ensures an executable solver program, it does not address semantic correctness-whether the program accurately implements the problem's intended constraints. SSV addresses this by generating concrete instantiations of the constraints and verifying their satisfaction in the generated program. The GenInstantiations function first parses the program P to extract constraints and their NL descriptions. Our program generation phase structures programs in segments of the form $P_{init} + C_1 + ... + C_N + O_1 + ... + O_M$, where $P_{init}$ contains initial definitions, followed by explicitly segmented constraints and options, each annotated with NL comments (e.g. see \"#CONSTRAINT:\" and \"#OPTION:\" segments in Figure 2). This structure allows parsing constraints along with their descriptions. For each constraint's NL description, the LLM infers concrete instantiations. While arbitrary instantiations can be generated, our implementation prompts the LLM for one positive (satisfied) and one negative (violated) example per constraint, both translated into solver expressions (Figure 4). The instantiation prompt is provided in Appendix B.\nOnce all instantiations I are obtained, we verify whether each is consistent with its respective constraint. Given the program's initial definitions $P_{init}$, constraint code C, and instantiation expression I, the Verify function constructs and executes the solver program $P_{init} + C + I$ to check logical satisfiability. If verification fails, it returns the first failing instantiation $I_{fail} \\in I$. Beyond verifying concrete instantiations, we also check general logical well-formedness properties using the IsWellFormed function, which ensures (1) the program follows the specified structure, (2) it returns a single answer, and (3) it avoids degenerate expressions-tautologies or vacuous implications that introduce redundancies or over-simplifications in the problem formalization.\nSemantic program repair. If semantic verification fails and a failing instantiation $I_{fail}$ is found, the RepairProgram function attempts to repair the original program P using the LLM, provided no answer has been found. Unlike error-based program repair, this is a semantic repair based on an instantiation inferred by the LLM rather than a syntactic or execution error. In our repair prompt, we supply the initial definitions code, the constraint code with its NL description, and the failing instantiation expression. The LLM is prompted to first analyze whether the error lies in the initial definitions, the constraint code, or the instantiation itself (using a chain-of-thought approach) before inferring the corrected code. The semantic repair prompt is detailed in Appendix C."}, {"title": "Evaluation", "content": "We evaluate our SSV technique on open benchmarks for logical reasoning, focusing on two key aspects: (1) improving the general accuracy of reasoning over existing baselines and (2) assessing verification quality in terms of both precision (correctness) and coverage (proportion of verified cases).\nDatasets. We use five common datasets for logical reasoning. These are the same datasets as [Pan et al., 2023] to help direct comparison with relevant baselines. All datasets follow a multiple-choice format, where each task includes a problem statement, a question, and answer options (e.g., Figure 1). PrOntoQA is a synthetic deductive reasoning dataset for LLM evaluation [Saparov and He, 2023]. We use its most challenging subset-fictional character tasks requiring 5 reasoning hops-comprising 500 test examples with 2 answer options (True/False). ProofWriter is a widely used logical reasoning dataset [Tafjord et al., 2021]. We use its open-world assumption subset with 5-hop reasoning tasks, following [Pan et al., 2023], with 600 test examples and 3 answer options (True/False/Unknown). FOLIO is an expert-crafted dataset for logical reasoning [Han et al., 2022], featuring real-world knowledge problems phrased in natural language and requiring complex first-order logic. We evaluate on its full test set of 204 examples, each with 3 answer options (True/False/Unknown). LogDeduction is a dataset from the Big-Bench benchmark [Srivastava et al., 2023] involving object sequence ordering based on given conditions. The full test set contains 300 tasks with 3, 5, or 7 answer options. AR-LSAT consists of all analytical reasoning questions from LSAT exams from 1991\u20132016 [Zhong et al., 2022]. This highly challenging dataset has seen only marginally better-than-random performance from state-of-the-art models [Pan et al., 2023; Liang et al., 2023]. The test set has 230 questions, each with 5 answer options.\nBaselines. We compare our technique against three baselines, which represent approaches of reasoning using the LLM alone, as well as the combination of formal logical solvers with LLMs. Each of these baselines and our own system is parametric in the LLM used, and in our experiments we investigate all systems with both the GPT-4 model (a current best general LLM for reasoning) as well as the weaker GPT-3.5 model from Open AI. We use the baselines and their results for these models as reported in [Pan et al., 2023]. The baselines are as follows. Standard is the direct approach of prompting the LLM, leveraging in-context learning to answer the question. CoT (Chain-of-Thought) [Wei et al., 2022] follows a step-by-step reasoning process, generating explanations before the final answer. Logic-LM is a state-of-the-art method that integrates LLMs with solvers for formal reasoning [Pan et al., 2023], where the LLM is prompted to generate a solver program to solve the task. SSV is our semantic self-verification technique (Figure 5). Our implementation uses the Z3 SMT solver [de Moura and Bj\u00f8rner, 2008] and applies identical prompts for both models, with 1-4 few-shot examples drawn from training datasets (detailed in the Appendices). The full SSV implementation sets MaxRepairs = 2 and Temperatures = [0, 0.3, 0.4, 0.5] (covering low to mid-range values), with parameter variations explored in the ablation analysis."}, {"title": "Results", "content": "Main results Figure 6 presents the main results, with all systems evaluated using GPT-4 as the underlying LLM. The figure reports general accuracy as well as the precision and coverage of SSV verification. General accuracy represents the percentage of correct answers across the dataset. For SSV, precision denotes the percentage of correct answers among those flagged as verified, while coverage indicates the percentage of verified cases relative to the entire dataset. The key observations are as follows:\n1. SSV outperforms all baselines in general accuracy. Our technique achieves a higher general accuracy over all baseline systems across all datasets. We especially note the drastic increase of 28.3% over the current best Logic-LM system on the most difficult AR-LSAT dataset. This shows the strong effectiveness of our technique in producing robust problem formalizations in contrast to just a direct LLM translation from the natural language description to the solver program.\n2. SSV verification has perfect precision across all datasets. With GPT-4 as base model, SSV achieves 100% verification precision on all datasets. Notably, on AR-LSAT, FOLIO, and ProofWriter, our verification mechanism identified erroneous cases where the datasets contained incorrect answers. However, for comparison with baselines, in Figure 6 we also report results based on the original datasets (showing slightly lower precision due to mislabelled cases). Appendix D details these corrections, and for AR-LSAT cases we also verified our corrections against the original test answers\u00b9. This empirically perfect precision highlights SSV's strong reliability for complex reasoning tasks.\n3. SSV verification has significant coverage on all datasets. Although the precision is very high, we know that SSV verification does not always succeed. However, we find that the coverage is significant across all datasets, with the lowest coverage of 21.7% on the most difficult AR-LSAT dataset. As expected, we find the coverage increases on the relatively easier datasets, with a verification coverage of up to 75.2% on ProofWriter. This significant coverage of verification shows that the SSV approach can help in avoiding manual human verification in a significant proportion of cases to reduce overall cost and effort.\nEffect of semantic repair and temperature exploration. Figure 7 shows the impact of varying semantic repair attempts (MaxRepairs) and temperatures (Temperatures) on the AR-LSAT dataset. We analyze overall accuracy, program accuracy (how often program generation succeeds rather than direct LLM answers), and verification coverage. Semantic repair improves accuracy by 6.1%, while temperature exploration increases it by 10.0%. Verification coverage gains 5.2% with repair and more than doubles with temperature exploration, rising 12.2% above an initial 10.9%. Repair attempts yield diminishing returns and cease to improve any metric beyond three attempts, while temperature exploration continues to show some gains up to 0.6. Additionally, the gap between program accuracy and overall accuracy narrows (from 9.8% to 5.2% on average), indicating greater reliance on program generation with these enhancements.\nEvaluation on GPT-3.5. We also evaluated our system and all baselines using GPT-3.5 as the underlying LLM. The results are shown in Figure 8. Firstly, we note that while the general accuracy of all systems drops significantly with this weaker model, our SSV system still performs best overall, with an average accuracy of 56.2%. However, Logic-LM performs better than SSV on FOLIO and LogDeduction (this could be partly due to differences in the code generation quality for the different solver languages that Logic-LM uses for these datasets). Secondly, we observe that while the coverage of SSV verification also drops significantly, with two of the more difficult datasets (AR-LSAT and LogDeduction) having no coverage at all, the precision of SSV is very minimally affected. On the three datasets where there is coverage, we still see an average precision of 97%. This demonstrates an important property of reliability of SSV verification: even for weaker models, if verification succeeds then it is still very reliable (and much more reliable than general accuracy), though it may succeed much less often. In practical terms, such reliability could even allow one to adopt a tiered strategy to optimize costs: trying weaker (cheaper) models for tasks first and fall-back on more expensive models if verification fails.\nFurther analyses. The technical appendix also includes analyses of unverified cases (E) and runtime performance (F)."}, {"title": "Limitations and Future Directions", "content": "Since natural language is informal and ambiguous, any verification approach with NL specifications cannot guarantee full correctness. While SSV verification achieves near-perfect empirical precision (100% with GPT-4), we discuss the kinds of errors illustrated by the few failing cases observed with GPT-3.5 (specifically, one case in PrOntoQA and four in ProofWriter where incorrect answers passed verification).\n1. Concrete instantiations are insufficient. Since verification relies on concrete examples (test cases), these may not cover all aspects of a general constraint, particularly corner cases. This caused two failures with GPT-3.5. For instance, in one case, the conditions \"Gary is nice\" and \"Gary is kind\" were conflated into a single predicate \"is_kind(Gary)\" in the formalization. An instantiation asserting \u201cGary is nice but not kind\" could have detected this error.\n2. Concrete instantiation and program are both mutually consistent but wrong. This is the unlikely case where both the program and the test case have the same error and therefore pass verification. We found only one such case which was a rather confusingly trivial error: for some reason the constraint \u201cFiona is quiet\u201d was translated as its negation \"Not(is_quiet(Fiona))\u201d in both the program and the concrete instantiation independently generated by GPT-3.5.\n3. Missing or superfluous constraints. In such cases, the LLM may omit required constraints or introduce unintended ones. Since our approach relies on explicitly demarcated constraints parsed from the LLM-generated program, such errors can cause verification failures. Two GPT-3.5 failures resulted from superfluous constraints, including one where the condition to be checked in the question was incorrectly added as a constraint in the program.\nIn general, such errors are rare, more common in weaker LLMs, and expected to decrease as LLMs improve. Errors of types (1) and (2) could be mitigated with a more exhaustive examples inference strategy, as our implementation generates only one positive and one negative example per constraint. Class (3) errors arise from structural inconsistencies where program constraints do not match the original problem. While mature LLMs like GPT-4 handle this well, specialized modules may also be trained to enforce core structural properties with high accuracy.\""}, {"title": "Related work", "content": "Reasoning with LLMs. Improving the robustness of reasoning in large language models is a very active area of research, and many recent approaches have made significant advancements. One direction of work has been to fine-tune or train specialized models that show improved reasoning ability [Tafjord et al., 2022; Clark et al., 2020; Yang et al., 2022]. Another direction has been to develop sophisticated prompting strategies to elicit better reasoning from LLMs. Chain-of-thought prompting [Wei et al., 2022] has shown how the quality of reasoning can be improved by prompting the model to explicitly generate the steps of reasoning in natural language before arriving at the final answer. Other examples of prompting approaches include chain-of-thought with self-consistency [Wang et al., 2023], analogical reasoning [Yu et al., 2024], and various modular approaches to address complex problems by decomposition to simpler sub-problems [Zhou et al., 2023; Khot et al., 2023; Creswell et al., 2023]. While these approaches show relative improvements in accuracy, the reasoning is still based on informal natural language and is prone to errors made by the LLMs in the steps of reasoning. In contrast, we follow the approach of off-loading the reasoning task to a formal solver that can guarantee correctness of the reasoning steps, and our particular focus is on the key challenge of ensuring that the correct formalization of the problem is sent to the solver.\nTool-augmented reasoning. Integrating LLMs with specialized tools for performing various tasks is becoming increasingly common [Schick et al., 2023]. This approach has also been adopted to improve the reasoning quality by augmenting the LLM with logical solvers or automated reasoning tools [Pan et al., 2023; Ye et al., 2023; Nye et al., 2021]. The key challenge with these approaches is to ensure that the LLM correctly translates the reasoning problem from NL to the formal language of the solver. This is the main focus of our work, where we show how verification and refinement with respect to concrete instantiations generated by the LLM can improve the translation accuracy and also provide a near-perfect precision of verification. Tool-augmented approaches have also been explored in the related areas of planning [Kambhampati et al., 2024; Guan et al., 2024] and auto-formalization [Wu et al., 2022; Jiang et al., 2023; He-Yueya et al., 2023], where informal mathematical proofs are translated to formal specifications defined in theorem provers like Isabelle [Paulson, 1994] and Lean [de Moura et al., 2015]. While our work focuses on logical reasoning, the principle of consistency-based verificaion and refinement of formalizations using concrete instantiations is also potentially applicable to these other domains.\nSelf-verification approaches. Many related works have also explored the notion of self-verification by LLMs [Weng et al., 2023; Madaan et al., 2023; Xie et al., 2023; Ling et al., 2023; Miao et al., 2024]. The general idea is that using the LLM to inspect and verify its own reasoning can show improvements, though in some domains self-critiquing has also shown diminished performance [Valmeekam et al., 2023]. Our approach of verification is different: instead of asking the LLM to verify the abstract chain of reasoning, we only ask it to generate concrete examples of the general constraints in the problem. The task of verification is then totally on the logical solver to formally check that these examples are consistent with the abstract formalization. Thus apart from not relying purely on the LLM for verification, we also avoid the more complex task of verifying an abstract chain of reasoning which can itself be highly error-prone. We instead perform both abstract and concrete inference and check consistency between them. We have shown how this approach can provide a very high precision verification, as opposed to the above approaches which provide relative improvements in accuracy. Our approach of inferring concrete instantiations is also similar to automated test case generation and verification in code generation approaches [Chen et al., 2024; Sch\u00e4fer et al., 2024]. While our instantiations are similar to test cases, in general they can be arbitrary implications, and our focus is on logical expressions rather than code."}, {"title": "Conclusion", "content": "We have presented the Semantic Self-Verification approach, which infers strong problem formalizations based on concrete instantiations, using a consistency-based verification paradigm that leverages LLMs and logical solvers. Beyond achieving state-of-the-art accuracy, SSV introduces a novel verification feature that has near-perfect empirical precision. As the reasoning power of LLMs continues to advance, such near-certain verification can serve as a complementary dimension to general accuracy gains in order to ensure confidence on arbitrarily complex tasks."}]}