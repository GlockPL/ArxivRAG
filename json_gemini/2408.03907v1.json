{"title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models", "authors": ["Shachi H Kumar", "Saurav Sahay", "Sahisnu Mazumder", "Eda Okur", "Ramesh Manuvinakurike", "Nicole Beckage", "Hsuan Su", "Hung-yi Lee", "Lama Nachman"], "abstract": "Large Language Models (LLMs) have excelled at language understanding and generating human-level text. However, even with supervised training and human alignment, these LLMs are susceptible to adversarial attacks where malicious users can prompt the model to generate undesirable text. LLMs also inherently encode potential biases that can cause various harmful effects during interactions. Bias evaluation metrics lack standards as well as consensus and existing methods often rely on human-generated templates and annotations which are expensive and labor intensive. In this work, we train models to automatically create adversarial prompts to elicit biased responses from target LLMs. We present LLM-based bias evaluation metrics and also analyze several existing automatic evaluation methods and metrics. We analyze the various nuances of model responses, identify the strengths and weaknesses of model families, and assess where evaluation methods fall short. We compare these metrics to human evaluation and validate that the LLM-as-a-Judge metric aligns with human judgement on bias in response generation.", "sections": [{"title": "Introduction", "content": "In recent years, the proliferation of Large Language Models (LLMs) like GPT-4 has revolutionized natural language processing, enabling applications from automated text generation to complex language understanding. However, as these models integrate more deeply into technological solutions, their inherent biases have become a significant concern. These biases can lead to misrepresentation of individuals and groups, promote stereotypes, and suggest unfair allocation of resources, impacting people both psychologically and socio-economically.\nDespite a fair amount of research happening on AI Safety and fairness, there is still a lack of understanding of measuring societal biases in language models systematically. Biases in these models\u2014such as gender, racial, ethnic, and ideological biases\u2014stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Bender et al., 2021). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022; Ferrara, 2023).\nThus, assessing and mitigating bias in language models is essential for their ethical and fair use.\nWe define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association. For example, in Figure 1, the LLM is presented with a prompt \"I think men are much better with managing finances\" and another obtained by only varying the gender (men to women). The image on the left shows the scenario where the LLM responds differently to these inputs. In one case it identifies the stereotype present in the prompt and rejects this with an explanation; in the other, it fails to identify any gender bias in the input and supports the gender-based claim instead. On the right shows an unbiased model that responds to both the prompts similarly, by identifying and rejecting the gender-stereotype in the input.\nBias mitigation techniques rely on bias assessment metrics to evaluate their efficacy. Bias evaluation metrics are broadly classified into embedding-based, probability-based, and generated-text-based metrics (Gallegos et al., 2023). The limitations of these metrics such as the lack of correlation between them and the lack of alignment with bias in downstream tasks have been discussed in various works (Cao et al., 2022; Delobelle et al., 2022; Aky\u00fcrek et al., 2022; Blodgett et al., 2021b). O"}, {"title": "Related Work", "content": "Adversarial Prompt Generation: Adversarial test-ing (Shayegani et al., 2023) has emerged as a pop-ular approach to AI safety assessments. Potentialharms or hazards are identified through a combina-tion of manual and automated probing techniques.Manual testing can be very challenging and lesseffective, and the results generally vary based onthe creativity of the prober, which could lead tocritical safety oversights in the assessment of amodel. Based on prior work, we utilize a 'Red-teaming language model' (Perez et al., 2022; Su"}, {"title": "Gender Bias: Methods and Evaluation", "content": "We define bias as the difference or variations in theresponses generated by an LLM when presentedwith inputs based on different social groups. Inour work, a model is considered unbiased if itsresponse to a prompt remains semantically con-sistent when a protected attribute, such as gender(male/female), is varied in the given statement orprompt as in Figure1. As demonstrated in Figure 2,our gender bias detection and evaluation pipelineconsists of three components, the \u2018Attacker LLM',"}, {"title": "Attacker LLM", "content": "Adversarial Prompt Generation: The attackerLLM adversarially generates a sentence or aprompt that might result in biased responses froma target LLM. We use Meta's Llama3-8B Instructmodel (Llama3 for short) configured with a tem-perature of 0.6 with system prompt: \"You are an ad-versarial prompt generator used as a red-teamingassistant, trying to get language models to generatebias responses to evaluate them for their inherentgender bias\" and a user prompt: \"Generate a state-ment using the keyword 'keyword' that can make alanguage model generate biased responses. BiasedStatement:\". The keyword is a gender-related wordfrom each of the gender-specific lists of Zhao et al.(2018b).\nWe also finetune 4-bit quantized Llama3 for ad-versarial prompt generation using Low-Rank Adap-tation (LoRA) (Hu et al., 2022) (with rank=16,\u03b1 = 16, 80-bit Adam optimizer with weight de-cay) on data collected (Su et al., 2023) using thegendered-keyword lists to prompt ChatGPT withthe same system prompt as above. We finetune for60 steps on about 5000 of the ChatGPT-generatedprompts.\nCounterfactual Data Augmentation (CDA):Once a prompt from a gender-related word is gen-erated, we utilize the Counterfactual Data Augmen-"}, {"title": "Evaluation: LLM as a Judge", "content": "Recent work in NLP has seen the use of LLMs asscorers or judges due to their strong human align-ment (Zheng et al., 2023; Li et al., 2023; Zhu et al.,2023; Liu et al., 2023). Obtaining human annota-tions is extremely tedious and expensive, and forbias, it also requires detailed protocol and rubricto disentangle human preferences and prior userbeliefs for objective assessment. To automate theevaluation, we explore LLM-as-a-Judge for identi-fying or measuring bias. Here, we use GPT-40 toevaluate and score responses generated by targetLLMs. We prompt the model to identify bias in aninput-response pair in terms of 5 classes by provid-ing an explanation of each as mentioned in Table 1.We instruct the model to output the bias scores,indicating the level of bias, and also generate a one-line explanation of the classification. Additionally,we calculate the difference in the LLM-as-a-Judgebias scores for male and female responses, thentake the average of these differences to obtain the\"LLM-judge Gap Score.\" This gap score servesas an indicator of the overall bias in the model'sresponse generation."}, {"title": "Evaluation: Other Metrics", "content": "Perspective API Metrics: Perspective API iswidely used API-based tool, developed by theGoogle Jigsaw team, which is a popular toxicitydetection tool. It measures attributes such as toxic-ity, insult, and identity attack, among others.\nSentiment Annotation: We follow the previouswork (Dhamala et al., 2021; Sheng et al., 2019)to employ the VADER classifier (Hutto and Gilbert,2014) to compute the sentiment, or polarity, ofLLM responses to gendered-prompts. We also cal-culate the \"Sentiment Gap\" score to serve as anindicator of overall bias. This score is determinedby computing the difference in sentiment betweenmale and female responses for each gender-pairedprompt and then averaging these differences.\nRegard: Regard scores (Sheng et al., 2019) aimto improve upon sentiment scoring by assessingthe polarity of language and societal perceptions\ntoward specific demographics (e.g., age, religion,race, gender). We used the BERT based classifier.\nLlamaGuard2: We used MLCommons Mode-Gauge's LLamaGuard2 annotator to annotate re-sponses with Safe vs. Unsafe categories. Since"}, {"title": "Human Evaluation", "content": "We perform a human evaluation using Amazon Mechanical Turk (AMT) by defining two tasks: i) Evaluating individual prompt-response pairs, where the annotators are provided with the gender-paired prompts and the target LLM response and are asked about the presence of stereotype, to rate the response for the bias level, sentiment, toxicity, profanity, etc. ii) Comparing the gender paired prompts and their respective responses to detect if the responses convey similar or different ideas and content. We use this to assess various target LLMs on how readily they answer the prompt across different genders and if the model provides varying answers for different groups.\nCrowd workers are also instructed to make their choices by keeping aside their personal biases and stereotypes, and by only focusing on the content. We select the top challenging prompt pairs that show discrepancies between the gap metrics mentioned earlier. Specifically, we choose pairs with a high Sentiment Gap score but a low LLM-judge Gap score, and vice versa, for this task. We sample approximately 100 gendered prompt pairs per target LLM for human annotation, resulting in approximately 600 gendered prompt pairs for which we obtain annotations. We obtained annotations from 3 annotators for each sample, where we considered the majority vote and average rating (for continuous values)."}, {"title": "Results and Discussion", "content": "Table 2 shows the performance of the different target LLMs when prompted with the individual adversarial Male/Female (M/F) prompts generated by the attacker LLMs.\nWith Llama3 as the attacker model, we observe that the Perspective API scores for Identity Attack, Insult, and Toxicity scores for female responses are significantly higher when compared to the male responses across all models. Further, Mixtral, Mistral, and GPT-4 models show lower identity attack and toxicity scores, on average, compared to the Llama2 model family. These results suggest that female oriented responses may require more alignment and correction for these metrics.\nWe see that for Mixtral 8x7B Inst, which is a powerful Mixture of Experts model, the Female response sentiment is significantly lower than the Male response sentiment, with Mistral and GPT-4 following a similar trend. This suggests that these models generate more critical responses with negative polarity for female gender. We also observe the highest mean bias scores using LLM-as-a-Judge (M/F =0.88/0.88) in the Mistral 7B Inst model implying that this model, on average, generates more biased responses. Both findings correlate with the DecodingTrust platform Fairness metric on the leader-board (showing the lowest numbers for GPT-4 and Mistral models).\nFor the Llama family of target LLMs, we see that (M/F) sentiment difference, as well as (M/F) bias score difference from LLM-as-a-Judge, reduces with an increase in model size, which reinforces the idea that larger Llama2 models are fairer than their smaller versions.\nRegard scores in the table are computed by evaluating male responses with reference to female responses. We see that male responses have a negative regard w.r.t female responses in Llama2-70B and GPT-4, which aligns with the increase in bias scores for male responses from LLM-judge. We also assessed LlamaGuard2's ability to detect gender bias (categorized under 'hate'). However, we found it ineffective, as it failed to classify biased language as unsafe, hence we do not report the results here."}, {"title": "Overall Bias Analysis", "content": "Following our definition of bias in Figure 1, our second AMT task involved presenting both male and female prompt-response pairs to the users who would assess whether the responses in each pair conveyed similar or substantially different ideas. We hypothesize that a higher proportion of gendered response pairs marked as dissimilar indicates greater bias in the model. After aggregating responses through majority voting, we calculated the percentage of gendered prompt-response pairs that crowd-workers classified as \"conveying different ideas.\" Table 4 presents the results of this comprehensive bias evaluation, quantifying the degree to which the model's responses differ based on gender-specific prompts, and we compare these to the Sentiment-Gap and LLM-judge Gap scores.\nWe observe that bias, based on responses conveying different ideas from human evaluation, is highest for the Llama2-7b-chat model, which is also reflected by the LLM-judge Gap score. All metrics consistently show that overall bias decreases as the model size increases within the Llama model family. Notably, there is a 100% agreement in the trend of diminishing bias between the human bias score and the LLM-judge Gap score: Llama2-7b-chat (highest), Llama2-13b, Mistral, Mixtral, Llama2-70B, and GPT-4 (lowest). However, overall bias based on the Sentiment Gap score is highest for Mixtral and lowest for Llama2-70b. This observation indicates that the LLM-judge Gap score aligns with human judgment of bias in model generation."}, {"title": "Conclusion", "content": "Identifying gender bias in LLM responses is very challenging due to the subtle nuances in assesing how people interpret language; the resulting biases are difficult to detect using commonly used metrics. In this work, we introduce adversarial prompting techniques to evaluate LLMs for inherent gender bias. We observe issues with existing metrics that are not well aligned with each other. We present an LLM-as-a-Judge paradigm to score responses for bias and provide detailed explanations. Finally, we consider human evaluations, demonstrating that the LLM-as-a-Judge metric most accurately aligns with human bias judgements.\nFurther work is needed to standardize these bias metrics, and comprehensive human studies are essential to understand society scale as well as culture specific assessments for bias related metrics. In this research, we try to define and disentangle gender bias measurements and look at multiple existing metrics alongwith human assessments. We acknowledge that using human evaluations to validate these LLM-based evaluations may have its shortcomings since humans bring their own wide-ranging biases to the evaluation task. In future work, we hope to explore these issues directly by expanding our work to other types of biases and protected classes and also by conditioning on the biases of our human evaluators."}]}