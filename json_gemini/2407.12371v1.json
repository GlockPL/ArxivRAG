{"title": "HIMO: A New Benchmark for Full-Body Human Interacting with Multiple Objects", "authors": ["Xintao Lv", "Liang Xu", "Yichao Yan", "Xin Jin", "Congsheng Xu", "Shuwen Wu", "Yifan Liu", "Lincheng Li", "Mengxiao Bi", "Wenjun Zeng", "Xiaokang Yang"], "abstract": "Generating human-object interactions (HOIs) is critical with the tremendous advances of digital avatars. Existing datasets are typically limited to humans interacting with a single object while neglecting the ubiquitous manipulation of multiple objects. Thus, we propose HIMO, a large-scale MoCap dataset of full-body human interacting with multiple objects, containing 3.3K 4D HOI sequences and 4.08M 3D HOI frames. We also annotate HIMO with detailed textual descriptions and temporal segments, benchmarking two novel tasks of HOI synthesis conditioned on either the whole text prompt or the segmented text prompts as fine-grained timeline control. To address these novel tasks, we propose a dual-branch conditional diffusion model with a mutual interaction module for HOI synthesis. Besides, an auto-regressive generation pipeline is also designed to obtain smooth transitions between HOI segments. Experimental results demonstrate the generalization ability to unseen object geometries and temporal compositions. Our data, codes, and models will be publicly available for research purposes.", "sections": [{"title": "1 Introduction", "content": "Humans constantly interact with objects as daily routines. As a key component for human-centric vision tasks, the ability to synthesize human-object interactions (HOIs) is fundamental with numerous applications in video games, AR/VR, robotics, and embodied AI. However, most of the previous datasets and models [8, 17, 24, 26, 61] are limited to interacting with a single object, yet neglect the ubiquitous functionality combination of multiple objects. Intuitively, the multiple objects setting is more practical and allows for broader applications, such as manipulating multiple objects for robotics [46,72].\nThe scarcity of such datasets mainly results in the underdevelopment of the synthesis of human interacting with multiple objects, as listed in Tab. 1. GRAB [61] builds the dataset of 4D full-body grasping of daily objects (\u201c4D\u201d refers to 3D geometry and temporal streams, \"full-body\" emphasizes the body movements and dexterous finger motion), followed by several 4D HOI datasets with RGB modality [8, 24], interacting with articulated [17] or sittable [26] objects. For text-driven HOI generation, [15,48] manually label the textual descriptions for the BEHAVE [8] and CHAIRS [26] datasets, and Li et al. [33,34] collect a dataset of human interacting with daily objects with textual annotations. Despite the significant development, all these datasets focus on single-object interactions. Thus, a dataset of full-body humans interacting with multiple objects incorporated with detailed textual descriptions is highly desired.\nCapturing 4D HOIs is challenging due to the subtle finger motions, severe occlusions, and precise tracking of various objects. Compared with a single object, interacting with multiple objects is more complicated regarding the spatial movements between human-object and object-object, and the temporal schedule of several atomic HOI intervals. For example, the process of a person interacting with a teapot and teacup could be: \"Lift the teapot\" \u2192 \"Pour tea to the teacup\" \u2192 \"Drink tea\", as shown in Fig. 1. To facilitate the synthesis of Human Interacting with Multiple Objects, we build a large-scale dataset called HIMO.\nWe adopt the optical MoCap system to obtain precise body movements and track the motion of objects attached by reflective markers. For the dexterous finger motions, we employ wearable inertial gloves to avoid occlusions. In total, 3.3K 4D HOI sequences with 34 subjects performing the combinations of 53 daily objects are presented, resulting in 4.08M 3D HOI frames. Various subjects, object combinations, and interaction patterns also ensure the diversity of HIMO.\nTo facilitate the study of text-driven HOI synthesis [12, 15, 20, 64, 75], we annotate the HOI sequences with fine-grained textual descriptions. Different from existing datasets, we additionally segment the long interaction sequences temporally and align the corresponding texts semantically, which allows for fine-grained"}, {"title": "2 Related Work", "content": "Many human-object interactions together with the hand-object interaction datasets have been proposed to facilitate the development of modeling HOIs. Several works [10,21,22,25,41,42] focusing on hand-object interaction have been proposed. Though effective, modeling hand-object interactions falls significantly short of comprehending the nature of human-object interaction, since the interaction process involves more than just hand participation. Full-body interactions [8, 17, 24, 26, 44, 61, 71, 76, 78] may enhance our understanding of HOIs. GRAB [61], InterCap [24] and ARCTIC [17] broaden the range of 4D-HOIs to full-body interactions. However, all of these full-body datasets only involve one single object, while the combination of several different objects is common in our daily lives. The KIT Whole-body dataset [44] contains motion in which subjects manipulate several objects simultaneously. However, the human body is represented as a humanoid model instead of a more realistic SMPL-X model. We address the deficiency by introducing a full-body 4D-HOI dataset that involves full-body interaction with multiple objects. Comparisons with the existing HOI datasets are listed in Tab. 1."}, {"title": "2.2 Text-driven Motion Generation", "content": "Text-driven human motion generation aims at generating human motions based on textual descriptions. Benefiting from large-scale motion-text dataset like KIT-ML [51], BABEL [54] and HumanML3D [20], numerous works [12, 14, 34, 49, 64, 68, 75, 82] have emerged. TEMOS [49] proposes a transformer-based VAE to encode motion and an additional DistillBert [57] to encode texts. MDM [64] adopts the diffusion model [23] for motion generation. MLDM [12] denoises the low-dimensional latent code of motion instead of the raw motion sequences. T2M-GPT [75] utilizes VQ-VAE [45] to compress motion sequences into a learned codebook and then generate human motion in an auto-regressive fashion. GMD [28] further adds keyframe conditions to synthesize controllable human motion. In this work, we base our model on the widely adopted MDM [64] framework."}, {"title": "2.3 Human-Object Interaction Generation", "content": "Generating plausible and realistic human-object interaction sequences has attracted much attention in recent years [11, 13, 15, 19, 31, 34, 35, 48, 60\u201363, 66, 70, 73, 80]. GRAB [61] generates plausible hand poses grasping a given object. GOAL [60] and SAGA [66] extend it to full-body grasping motion generation. However, these methods convert the task into the generation of the final pose of a human interacting with the static object. Recent works like InterDiff [70] tackle the problem of synthesizing comprehensive full-body interactions with dynamic objects, which leverages a diffusion model [23] to predict future HOI with realistic contacts. IMoS [19] synthesizes vivid HOIs conditioned on an intent label, such as: \"use\", \"lift\" and \"pass\". Notice that none of the aforementioned methods can generate full-body HOI involving multiple objects."}, {"title": "2.4 Temporal Action Composition", "content": "Human motion composition [5, 7, 32, 36, 50, 55, 58,79] with text timeline control aims to generate arbitrary long human motion sequences with smooth and realistic transitions, which can be separated as two categories. First, several auto-regressive methods [5,32,36,55,79] are developed relying on the availability of motion transition annotations. These methods iteratively generate the subsequent motion based on the already generated motion. Second, a few diffusion-based methods [6,7,50,58,77] are proposed to modify the sampling process of diffusion models with overlaps of a predefined transition duration. In our setting of HIMO-SegGen, we also need to generate composite HOI sequences with smooth transitions. Thus, we adopt a simple yet effective auto-regressive pipeline to iteratively synthesize the HOI sequences."}, {"title": "3 The HIMO Dataset", "content": "We present HIMO, a large-scale 4D-HOI dataset of full-body human interacting with multiple objects, comprising accurate and diverse full-body human motion, object motions and mutual contacts. Next, we will introduce the data acquisition process in Sec. 3.1 and then describe the SMPL-X fitting process and the annotation of textual descriptions and the temporal segments in Sec. 3.2."}, {"title": "3.1 Data Acquisition", "content": "Hybrid Capture System. To obtain the accurate movement of the human body against occlusions, we follow [61] to adopt the optical MoCap scheme instead of multi-view RGB camera system [8, 24], which guarantees much lower error [61, 67]. We believe that high-quality moving sequences are more important than natural images for HOI motion data. An overview of our capturing setup can be seen in Fig. 2(a,c). We deploy Optitrack [3] MoCap system with 20 PrimeX-22 infrared cameras to capture the body motion, where each subject wearing MoCap suits with 41 reflective markers. Each camera can capture 2K frames at 120fps. To capture the dexterous finger movements, we select the inertial Noitom Perception Neuron Studio (PNS) gloves [2], which can precisely record the subtle finger poses in real-time. The inertial gloves can still work well under severe occlusions of human-object and object-object. We frequently re-calibrate the PNS gloves to ensure the capture quality. Spatially, the locating boards attached to the back of the hands provide the rotation information of the wrists, thus the human body movement can be integrated with the finger movements. Temporally, we utilize the Tentacle Sync device to provide timecodes for Optitrack and PNS gloves to synchronize them. Apart from the hybrid MoCap system for full-body motion capture, we also set up a Kinect camera to record the RGB videos from the front view with the resolution of 1080\u00d7768.\nCapturing Objects. We choose 53 common household objects in various scenes including the dining rooms, kitchens, living rooms and studies from ContactDB [9] and Sketchfab [4]. We further reshape them into appropriate sizes and 3D print them, so that the 3D object geometry is aligned with the real printed objects. To precisely track the object motion, we attach several (3-6) reflective markers on their surface with strong glue, as shown in Fig. 2(b), to track the rigid body composed of the attached markers. The coordinate system of the subject and the objects are naturally aligned. One critical rule of attaching the markers to objects is to mitigate the side effects of manipulating the objects. Empirical results show that 12.5 mm diameter spherical markers achieve more robust tracking performance than smaller markers. Since the Optitrack system tracks the 6-DoF poses of the centroid of the attached markers rather than the centroid of the object, thus we employ a post-calibration process to compensate for the bias between them. The object category definition is listed in the supplementary.\nRecording Process. Each subject is asked to manipulate the pre-defined combinations of 2 or 3 objects, such as \"pour tea from a teapot to a teacup\" and \"cut an apple on a knifeboard with a knife\". Initially, the objects are randomly"}, {"title": "3.2 Data Postprocessing", "content": "Fitting SMPL-X Parameters. The expressive SMPL-X [47] parametric model is widely adopted by recent works [18,39,43,50,74] for its generality and flexible body part editing ability. We also adopt the SMPL-X model to represent the human body with finger articulations. Formally, the SMPL-X parameters consist of global orient $g\\in R^3$, body pose $\\theta\\in R^{21\\times3}$, finger poses $\\theta_h \\in R^{30\\times3}$, root translation $t \\in R^3$ and shape parameter $\\beta\\in R^{10}$. An optimization algorithm is adopted to obtain the SMPL-X parameters for each HOI sequence based on our full-body MoCap data. We initialize the subjects' shape $\\beta$ based on their height and weight following [53]. The joint energy term $E_j$ optimizes body joints to our MoCap data as:\n$$E_j = \\sum_{n=0}^{N} \\sum_{j=0}^{J} ||P_j^n - \\hat{P}_j^n||_2^2,$$\nwhere $N$ is the frame number of the sequence, $J$ is the number of human joints, $P_j^n$ and $\\hat{P}_j^n$ represent our MoCap joint position and the joint position regressed from the SMPL-X model, respectively. The smoothing term $E_s$ smooths the motion between frames and mitigates pose jittering as:\n$$E_s = \\sum_{n=0}^{N-1} \\sum_{j=0}^{J} ||P_j^{n+1} - P_j^n||^2.$$\nFinally, a regularization term $E_r$ is adopted to regularize the SMPL-X pose parameters from deviating as:\n$$E_r = ||\\theta_b||_3 + ||\\theta_h||_2.$$\nIn total, the whole optimization objective can be summed as follows:\n$$E = E_j + \\lambda E_s + \\gamma E_r,$$\nwhere $a = 1, \\lambda = 0.1, \\gamma = 0.01$.\nTextual Annotations. Text-driven motion generation thrives greatly thanks to the emergence of text-motion datasets [20,51,65]. To empower the research on"}, {"title": "4 Text-driven HOI Synthesis", "content": "In this section, we benchmark two novel tasks of HIMO-Gen in Sec. 4.1 and HIMO-SegGen in Sec. 4.2. For HIMO-Gen, we propose a dual-branch diffusion-based framework to synthesize the motion of human and objects separately, with meticulously designed mutual interaction module and optimization objectives. For HIMO-SegGen, we apply an auto-regressive generation scheme."}, {"title": "4.1 The HIMO-Gen Framework", "content": "Data Representation. We denote the human motion as $H \\in R^{T\\times D_h}$ and the multiple object motions as $O = \\{O_i\\}_{i=0}^{N_o} \\in R^{T\\times D_o}$, where $T$ represents the frame number of the HOI sequence, $N_o$ is the number of the objects, $D_h$ and $D_o$ represent the dimension of the human and object motion, respectively. We adopt the SMPL-X [47] parametric model to represent the human movements. The pose state of human at frame i is denoted as $H^i$, which consists of the global joint positions $P^i \\in R^{52\\times 3}$, global joint rotation $Q^i \\in R^{52\\times 6}$ represented by the continuous 6D rotation format [81] and translation $t^i \\in R^3$. We also denote the object motion at frame i as $O_i$ (We omit the subscript of the i-th object/geometry for better expression), which comprises of the relative rotation $R^i \\in R^3$ with respect to the input object's frame and its global translation $T^i \\in R^3$. To encode the object geometries $G = \\{G_i\\}_{i=0}^{N_o}$, we follow prior works [33,61] to adopt the Basis Point Set (BPS) representation [52]. We sample 1,024 points from the surface of the object meshes, and then for each sampled point, we calculate the directional vector with its nearest neighbor from the basis points set, resulting in $G\\in R^{1024\\times 3}$ for each object.\nProblem Formulation. Given the textual description $L$, the initial states of $H^0$ and $O^0$ as the first frame human motion and object motions and the object geometries $G$. Our model aims to generate the spatio-temporally aligned human motion $H$ together with the object motions $O$ with plausible contacts.\nDual-branch Diffusion Models. Our proposed framework is demonstrated in Fig. A, which consists of two parallel diffusion-based branches for human and object motion generation and a mutual interaction module for feature fusion."}, {"title": "Mutual Interaction Module.", "content": "To model the interaction between human and objects, we fuse the features of the two branches via a mutual interaction module as depicted in Fig. A(c). The two branches share the same Transformer architecture with $lenc$ blocks without sharing weights. Each Transformer block is comprised of two attention layers and one feed-forward layer. The first self-attention layer embeds the aggregated human/object features $H^{(i)}$ and $O^{(i)}$ into embeddings $e_h^{(i)}$ or $e_o^{(i)}$. The second attention layer functions as a mutual attention layer, where the key-value pair of the human branch Transformer is provided by the object hidden embedding $e_o^{(i)}$, and vice versa, which can be formulated as:\n$$H^{(i+1)} = FF(softmax(\\frac{Q_hK_o^T}{\\sqrt{C}})V_o)); O^{(i+1)} = FF(softmax(\\frac{Q_oK_h^T}{\\sqrt{C}})V_h)),$$\n$$Q_h = e_h^{(i)}W^Q_h, K_o = H^{(i)}W^K_o, V_o = H^{(i)}W^V_o,$$\n$$Q_o = e_o^{(i)}W^Q_o, K_h = O^{(i)}W^K_h, W_o = O^{(i)}W^V_o,$$\nwhere the subscript $h$ and $o$ denote the human branch and the object branch, respectively, $W_h$ and $W_o$ denote the trainable parameters of the two branches.\nTo model the spatial relation between objects, we designed a novel object-pairwise loss. Our insight is that the relative distance between the interacted objects changes constantly with certain patterns, and explicitly adding constraints on the relative distance between the interacted objects is beneficial. For example, when cutting an apple with a knife, the knife gradually approaches and stays on the surface of the apple for a while, then moves away. We adopt the L2 loss as $L_{dis}$ to keep the consistency between the generated results and the ground truth."}, {"title": "4.2 The HIMO-SegGen Framework", "content": "Problem Formulation. The only difference with HIMO-Gen lies in the input text prompts, where HIMO-Gen takes one single text description Las a condition, yet HIMO-SegGen is conditioned on a series of several atomic text prompts as $\\mathcal{L} = \\{l_1,l_2,..., l_m\\}$. The key of HIMO-SegGen is to ensure smooth and realistic transitions between the generated HOI clips.\nGeneration Pipeline. Our generation pipeline is demonstrated in Fig. 5, which is simple yet effective. We segment the long HOI sequences into smaller motion-text pairs so that the natural transitions of consecutive HOIs are reserved. We first train the HIMO-Gen model to empower the ability to generate more fine-granularity HOIs. Different from the vanilla HIMO-Gen model which is only conditioned on the initial state of the first frame, we modify it to conditioned generation on the past few frames. During the inference time, we take the last few frames of the previous generation result as the condition for the next generation, to iteratively obtain the composite HOI results."}, {"title": "5 Experiments", "content": "In this section, we elaborate on the dataset and implementation details, the baseline methods and the evaluation metrics. Then we present extensive experimental results with ablation studies to show the effectiveness of our benchmark.\nDataset Details. We follow [20] to split our dataset into train, test, and validation sets with the ratio of 0.8, 0.15, and 0.05. The maximum motion length is set to 300 for HIMO-Gen and 100 for HIMO-SegGen. The maximum text length is set to 40 and 15, respectively. To mitigate the influence of the order of several objects, we randomly shuffle the objects when loading the data."}, {"title": "Generalization Experiments", "content": "Unseen object geometries. We experiment on the generalization ability to unseen object geometries of our HIMO-Gen model. We choose several object meshes that have the same category as our dataset but different geometries. We feed their geometries as input conditions to our model and synthesize corresponding human and objects motion. Visualization results are shown in Fig. B. We can observe that, to some extent, our model can generalize to unseen object meshes despite of some flaws of human-object contact, which may result from the deficiency of their geometric information in our model.\nNovel HOI compositions. We experiment on the ability of our HIMO-SegGen to generate novel HOI compositions. We choose several HOI combinations that never appear in our training set, for instance, \"Knocks on the beer with a hammer\"\u2192\"Grabs the beer to observe\"\u2192\"Puts them down on the table\". Then we feed the texts to our HIMO-SegGen consecutively to auto-regressively generate HOI clips."}, {"title": "F Societal impacts and responsibility to human subjects.", "content": "Our dataset can be leveraged to generate plausible human interactions with multiple objects, which may lead to the creation and spread of misinformation. For privacy concerns, all performers signed the agreement on the release of their motion data for research purposes. We focus on the pure motion rather than RGB, thus no RGB videos are released and their identity will not be leaked."}, {"title": "6 Conclusion", "content": "In this paper, we propose HIMO, a dataset of full-body human interacting with multiple household objects. We collect a large amount of 4D HOI sequences with precise and diverse human motions, object motions and mutual contacts. Based on that, we annotate the detailed textual descriptions for each HOI sequence to enable the text-driven HOI synthesis. We also equip it with elaborative temporal segments to decompose the long sequences into several atomic HOI steps, which facilitates the HOI synthesis driven by consecutive text prompts. Extensive experiments show plausible results, verifying that the HIMO dataset is promising for downstream HOI generation tasks.\nLimitations: Our work has the following limitations: 1) Large objects: Our HIMO dataset is built mainly based on small household objects, without human interacting with large objects such as chairs and suitcases. 2) RGB modality: Though we set up a Kinect camera to capture the monocular RGB sequences, the human and object appearances are unnatural since humans are wearing tight MoCap suits and objects are attached with reflective markers. 3) Facial expressions: We neglect the facial expressions since our dataset mainly focuses"}]}