{"title": "Can LLMs replace Neil deGrasse Tyson?\nEvaluating the Reliability of LLMs as Science Communicators", "authors": ["Prasoon Bajpai", "Niladri Chatterjee", "Subhabrata Dutta", "Tanmoy Chakraborty"], "abstract": "Large Language Models (LLMs) and AI assistants driven by these models are experiencing exponential growth in usage among both expert and amateur users. In this work, we focus on evaluating the reliability of current LLMs as science communicators. Unlike existing benchmarks, our approach emphasizes assessing these models on scientific question-answering tasks that require a nuanced understanding and awareness of answerability. We introduce a novel dataset, SCiPS-QA, comprising 742 Yes/No queries embedded in complex scientific concepts, along with a benchmarking suite that evaluates LLMS for correctness and consistency across various criteria. We benchmark three proprietary LLMs from the OpenAI GPT family and 13 open-access LLMs from the Meta Llama-2, Llama-3, and Mistral families. While most open-access models significantly underperform compared to GPT-4 Turbo, our experiments identify Llama-3-70B as a strong competitor, often surpassing GPT-4 Turbo in various evaluation aspects. We also find that even the GPT models exhibit a general incompetence in reliably verifying LLM responses. Moreover, we observe an alarming trend where human evaluators are deceived by incorrect responses from GPT-4 Turbo.", "sections": [{"title": "1 Introduction", "content": "The surge of Large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022; OpenAI, 2022) marks the beginning of an era of rapid development across a variety of natural language tasks. With the introduction of chatbots powered by instruction-tuned LLMs, users across diverse domains are becoming reliant on them in day-to-day activities. The increasing usage of LLM-based AI assistants in academia has triggered intense discussion recently. Multiple reports of inconsistent fragments of text appearing in scientific papers, apparently generated by AI assistants and overlooked due to lack of caution, have surfaced. Recent attempts have been made to outline the usage of AI assistants for literature surveys in research pipelines (Bhayana, 2024; Whitfield and Hofmann, 2023)\nHowever, there are innate risks associated with LLMs, attributed to overconfident generation and hallucination, that need to be addressed before their large-scale usage as surrogates of human expertise. Particularly in scientific communication in which nuance plays a vital role, LLMs missing out on small details can spread misconceptions (Dutta and Chakraborty, 2023). Another key challenge lies in the lack of self-awareness of current LLMs and overconfident generation leading to hallucination; given that realizing the lack of knowledge drives the pursuit of scientific exploration, an uncompromising quality of an AI assistant would be to reflect on the lack of knowledge. Existing STEM benchmarks, despite a variety in problem hardness, fail to incorporate these crucial characteristics.\nIn this work, we seek to close the gaps in evaluating LLMs towards faithful scientific question-answering. Specifically, we seek to address the following research questions:\n\u2022 RQ1: Can existing LLMs answer scientific reasoning questions successfully and faithfully that require understanding the nuances of scientific knowledge?\n\u2022 RQ2. Are LLMs effective at abstaining from assertively answering scientific open problems?\n\u2022 RQ3. Can LLMs successfully verify LLM-generated responses?\n\u2022 RQ4. Can human evaluators be misled by incorrect yet confident LLM responses to complex scientific questions?\nTo this end, we propose a novel dataset scientific QA dataset, SCiPS-QA (Specially Challenging Problems in Science - Question Answering), a collection of 742 complex boolean scientific problems that require deep knowledge retrieval and extensive reasoning to answer (Contribution #1).\u00b9 The problems are chosen from the most niche research areas across different subjects (see Figure 1 for sample questions from SCiPS-QA and answers generated by GPT-4 Turbo). SCiPS-QA contains closed (i.e., the answer exists within the scope of current scientific knowledge) as well as open problems. We benchmark a wide variety of proprietary and open-access LLMs from the OpenAI GPT series, Llama-2 and Llama-3 series, and Mistral series on SCIPS-QA using an exhaustive evaluation suit to judge their correctness, faithfulness, and hallucination, in terms of the final boolean answer as well as the reasoning explanation (Contribution #2). We find that while proprietary models like GPT-4 Turbo are generally better than open-access Llama-2, Mistral, or smaller Llama-3 variants, Llama-3-70B models (with or without instruction tuning) come as a strong competitor to GPT-4 Turbo (Findings #1). However, all the experimented LLMs are far from understanding the nuances of scien-"}, {"title": "2 Related Work", "content": "LLMs have demonstrated various types of reasoning capabilities, including logical, commonsense, mathematical, and temporal reasoning (Huang and Chang, 2023). In this section, we review relevant work that explores the limitations of LLMs in scientific reasoning.\nSeveral datasets provide comprehensive assessments of LLMs' abilities to solve mathematical problems. GSM8K (Cobbe et al., 2021) comprises high-school-level math word problems, while AQUA-RAT (Ling et al., 2017) includes a collection of algebraic word problems. Dolphin18K (Huang et al., 2016) features elementary-level problems designed to evaluate basic mathematical reasoning capabilities. The MATH dataset (Hendrycks et al., 2021) presents more challenging problems than those in the aforementioned datasets but focuses on simpler mathematical objects compared to the complex scientific concepts found in SCiPS-QA. Additionally, Ape210K (Zhao et al., 2020) offers a broad range of mathematical problems to further test LLMs' problem-solving skills. These datasets collectively highlight the strengths and limitations of LLMs in mathematical reasoning, providing a foundation for understanding their performance in more specialized scientific domains.\nScienceQA (Lu et al., 2022), SciQ (Lu et al., 2022) and MMLU (Hendrycks et al., 2020) are prominent datasets used to evaluate LLMs' scientific reasoning capabilities. MMLU-Pro (Wang et al., 2024) is an improved version of MMLU, offering more challenging problems and greater re-sistance to prompt variations. ScienceQA is a large-scale multimodal dataset with 21, 208 multiple-choice questions covering diverse science topics."}, {"title": "3 The SCiPS-QA Dataset", "content": "In this section, we describe the composition of SCiPS-QA and the methodology used to collect the Boolean queries that constitute SCiPS-QA.\nThe dataset comprises 742 complex Yes/No problems that require expert-level proficiency in scientific reasoning to answer correctly. We include both open and closed problems across subjects - Physics, Chemistry, Mathematics, Theoretical Computer Science, Astronomy, Economics, and Biology.  The difficulty of the problems in the dataset is deliberately kept very high to rigorously test the scientific reasoning and Boolean answering capabilities of state-of-the-art open-source and proprietary LLMs.\nWe randomly select 40 problems from each of four different subjects within SCiPS-QA to compare GPT-4 Turbo's performance in answering Boolean scientific queries against those from MMLU-Pro and SciQ. Additionally, we utilize GPT-3.5 Turbo to paraphrase 40 randomly chosen scientific problems per subject from MMLU-Pro and SciQ into a Yes/No format.  Figure 2 illustrates that GPT-4 Turbo performs the worst on SCiPS-QA, highlighting its higher level of difficulty regarding boolean question answering.\nClosed questions. These questions have definitive answers supported by scientific literature. We curate a list of complex topics for each subject in SCiPS-QA manually. For each topic, we utilize the wikipedia API to retrieve its summary. Subsequently, we provide this summary to GPT-4 Turbo, prompting it to generate Yes/No problems along with their corresponding answers. The resulting Boolean questions undergo manual assessment based on two criteria: (1) requiring scientific reasoning for accurate answers, and (2) correctness of the generated answers. The precise prompt used to generate closed questions can be found in Appendix D.1.\nOpen questions. These questions lack a definitive answer in the scientific literature. They are manually selected from wikipedia pages and research blogs. Further details on how open questions were collected can be found in Appendix B."}, {"title": "4 Experiments", "content": "This section presents the details of the experiments we performed to answer the research questions (RQs) we set to explore.\n4.1 Experimental Setup\nWe evaluate a total of 13 open-source models, including those from the Llama-2 family (Touvron et al., 2023), Llama-3 family, Mistral-7B-Instruct-v0.1 (Jiang et al., 2023), Mistral-7B-Instruct-v0.2, and Mistral-8x7B-Instruct-v0.1 (Jiang et al., 2024), on the SCiPS-QA dataset using custom-designed evaluation metrics. Additionally, we assess proprietary models such as GPT-4 Turbo (gpt-4-turbo-2024-04-09), GPT-3.5 Turbo (gpt-3.5-turbo-1106), and 'text-davinci-003'. For proprietary models, we follow the methodology outlined in (Li et al., 2024) to evaluate the reasoning passages generated by these models in response to boolean queries from SCiPS-QA. Evaluation criteria include attributes like factuality and convincingness (defined in Appendix A), assessed using GPT-3.5 Turbo and human experts as evaluators. We also evaluate the propensity for hallucination (see Appendix A) in these reasoning passages using SelfCheckGPT (Manakul et al., 2023), which employs a sampling-based approach. Further details on these evaluations are provided in subsequent sections. We collect responses through few-shot prompting. The details about exact prompts can be found in Appendix D.2. For each model, the responses are collected in two different settings. We call responses collected at temperature 0.0 the 'main responses' and those collected at temperature 1.0 the \u2018stochastic responses\u2019.\n4.2 Evaluation Metrics\nTowards a comprehensive evaluation of LLMs, we define the following metrics on the generated responses.\n(i) Main Response Accuracy (MACC). The accuracy of responses obtained at zero temperature.\n(ii) Major Stochastic Response Accuracy (MSACC). We collect the majority response from 10 different stochastic responses with temperature set to 1. We treat invalid responses as incorrect answers.\n(iii) Variation in Stochastic Responses (VSR). We report the variety in the 10 stochastic responses obtained at temperature 1. We map A \u2192 1, B \u2192 2, C\u2192 3 and rest of the invalid responses to 3 and calculate the standard deviation.\n(iv) Accuracy of Main responses for closed questions (CMACC) denotes the MACC score on the subset of SCiPS-QA containing closed questions.\n(v) Accuracy of Major Stochastic Responses for Closed Questions (CMSACC) reflects whether the majority of the LLMs' responses to the closed questions in a unit temperature decoding are correct or not.\n(vi) Accuracy of Main Responses for Open Questions (OMACC) is similar to CMACC but evaluated on the open questions instead. In addition to the overall correctness, this metric evaluates whether the model can identify if a question is scientifically unanswerable.\n(vii)Accuracy of Major Stochastic Responses for Open Questions (OMSACC) tests the answer abstinence of the LLM in a unit temperature generation regime.\n4.3 Hallucination Quantification\nWe employ SelfCheckGPT (Manakul et al., 2023), a sampling-based methodology that assigns hallucination scores within the range of [0, 1] (0: no hallucination and 1: full hallucination). This scoring is derived by measuring the deviations between the main response and multiple stochastic responses. We take the average of the hallucination scores of sentences in the main response to assign a hallucination score to the entire main response. We briefly describe each of the SelfCheckGPT variants in this section. More details about SelfCheckGPT and the variants we have implemented can be found in the Appendix C.\nSelfCheckGPT with BERTScore. For each reasoning sentence in the main response, we calculate the maximum semantic similarity across all sentences in the stochastic response passages. This score indicates the degree of semantic similarity between the main response sentence and various stochastic responses. To quantify hallucination, we derive the complement of this score and assign it as the hallucination score for the main response sentence. Our analysis employs two models, all-MiniLM-L6-v2 and all-mpnet-base-v2, sourced from sentence_transformer (Reimers and Gurevych, 2019), to generate sentence-level embeddings. This approach ensures mitigation of potential model bias in our results.\nSelfCheckGPT with NLI. Natural Language Inference (NLI) assesses whether a hypothesis logically follows from a premise, categorized as entailment, neutral, or contradiction. We compare each sentence of a main response reasoning passage as a hypothesis against each of the corresponding stochastic response reasoning passages as the premise. The logits associated with classes 'contradiction' and 'entailment' are considered and a score is assigned to the main response sentence, which is a proxy for the probability score of it being in 'contradiction' to the stochastic response reasoning passages. We use DeBERTa-v3-base (He et al., 2020) fine-tuned on MNLI (Williams et al., 2018) for collecting the logits associated with 'contradiction' and 'entailement' classes.\nSelfCheckGPT with Prompt. We employ an external LLM evaluator to determine if each sentence in a main response reasoning passage is supported by corresponding stochastic response reasoning passages. Specifically, we utilize GPT-3.5 Turbo as the external LLM; the exact prompt used can be found in Appendix D.3. The responses (Yes, No, NA) are mapped to hallucination scores (Yes \u2192 0, No \u2192 1, NA \u2192 0.5). The average of the GPT-3.5 Turbo response scores is calculated and assigned as the hallucination score for the corresponding sentence in the main response.\n4.4 NLG Evaluation of Reasoning Passages\nWe validate the main response reasoning passages generated by the models \u2013 GPT-4 Turbo, GPT-3.5 Turbo, and text-davinci-003 using GPT-3.5 Turbo as the verification model. Additionally, we verify responses from GPT-4 Turbo using GPT-4 Turbo itself as the verification model. Verification attributes are scored on a linear scale using prompt outputs in a zero-shot setting. All relevant prompt details can be found in Appendix D.4.\nConvince-factor. Responses that are highly convincing but rely on incorrect information are considered 'hallucinations' (Ji et al., 2022). We assign a convincingness score on a linear scale ranging from 1 to 5. This verification attribute is reported for main response reasoning passages using two different prompt settings: one where model answers are included in the prompt given to the evaluator models (denoted as convince-factor-with-answer), and another where model answers are absent (denoted as convince-factor-without-answer).\nFact-check. We assign scores on a linear scale (ranging from 1 to 5) to main response reasoning passages based on their factual accuracy. Our aim is to investigate whether evaluator LLMs can differentiate between incorrect reasoning passages and correct ones based on the factual correctness of responses.\nInformation Mismatch. We compare each main response reasoning passage with all ten different stochastic response reasoning passages Sk for the amount of information mismatch between them, which is scored on a linear scale ranging from 1 to 5. We assign the mean of such scores across stochastic responses to the main response reasoning passage.\n4.5 Human Evaluations\nWe randomly select 30 combinations of query and main response reasoning passages (from GPT-4 Turbo) for each subject \u2013 Physics, Chemistry, Mathematics, and Computer Science. For each subject, we employed two human evaluators. All human evaluators had at least a graduate degree in their respective subjects; they were male and aged be-tween 20-25.\nHuman evaluators were tasked with assigning a 'convince-factor' score to the main response reasoning passages, following the same evaluation setup used with LLMs as the evaluators. We divide human evaluators into two groups: one group sees both the model answer and reasoning, while the other group only views the reasoning itself. Both groups receive identical queries for evaluation."}, {"title": "5 Results", "content": "In this section, we look at the various quantitative results summarized in Table 2.\n5.1 SC\u0130PS-QA Benchmark\nWe observe that among both open-source and proprietary models, the Llama-2 family consistently performs the poorest across nearly all metrics. The GPT series of models show competitive performance, closely rivaling the higher-scale models within the Llama-3 family, which rank highest among the open-source models tested.\nMACC: Llama-3-70B achieves the highest score in the MACC metric at 0.693, closely followed by GPT-4 Turbo with a score of 0.646. Notably, among the Llama-2 and Llama-3 families, 'chat' models perform equivalently to their non-instruction fine-tuned counterparts, except for the lower scale members: Llama-2-7B and Llama-3-8B, where the instruction fine-tuned variants show score increases of 0.3 and 0.324, respectively. Mixtral-8x7B-Instruct-v0.1 significantly outperforms both Mistral-7B-Instruct-v0.1 and Mistral-7B-Instruct-v0.2. All three GPT models perform strongly, with GPT-4 Turbo achieving the highest score of 0.646 in the MACC metric.\nMSACC: GPT-4 Turbo outperforms all other models with a score of 0.651, closely followed by Llama-3-70B-instruct, which achieves a score of 0.623. In contrast to the MACC metric, where instruction fine-tuned models from the Llama-2 and Llama-3 families often performed equivalent to their non-instruction fine-tuned counterparts, here we observe that the instruction fine-tuned models outperform their counterparts.\nVSR: The Llama-2 family performs the worst in terms of VSR score indicating their limited capability to produce consistent results. In contrast, the GPT models exhibit high consistency, with GPT-4 Turbo reporting the lowest VSR score of 0.193 among all models. The Llama-3 family demonstrates better consistency compared to the Llama-2 family, while Mistral models also perform well but not as strongly as the top performers among open-source models. Among them, Llama-3-70B-instruct stands out with a VSR score of 0.295.\nCMACC, CMSACC: Llama-3-70B-instruct outperforms all models in CMACC and CMSACC metrics achieving a score of 0.780 and 0.784 respectively. GPT models also perform well in handling closed domain scientific queries with GPT-4 Turbo being the best among them, achieving a score of 0.75 and 0.754.\nOMACC, OMSACC: One of the major findings is that most of the open source and proprietary LLMs are really bad at accepting that they do not know the answers to open scientific queries in SCiPS-QA. This is evident from their low OMACC and OMSACC scores across the board. Llama-3-70B stands out as the top performer in terms of answer abstention for open scientific queries, achieving the highest OMACC (0.582) and OMSACC (0.487) scores. In contrast, Llama-2 models struggle significantly in handling open queries, while Mistral-7B models and Mixtral-8x-7B-Instruct-v0.1 perform reasonably well among open models. The GPT models demonstrate strong performance in responding to open scientific queries, with GPT-4 Turbo achieving the highest scores of 0.432 and 0.436 in OMACC and OMSACC metrics, respectively. Note that models also produced invalid responses to prompts. Small models \u2013 Llama-2-7B and Mistral-7B-Instruct-v0.1, produce a much larger fraction of invalid responses as compared to other open-source models. Proprietary models produce almost negligible invalid responses with GPT-4 Turbo reporting no invalid main response.\n5.2 Hallucination Quantification\nOur investigation using SelfCheckGPT fails to yield conclusive evidence of hallucination in the proprietary GPT models despite their high rate of mistakes. When employing the BERTScore variant, we observe normal distribution in the frequency dis-5.3 NLG Evaluation of Reasoning Passages\nWe assess the main response reasoning passages from all three proprietary models using GPT-3.5 Turbo as the verifier. Table 7 shows the convincingness (with and without answer), factuality and information-mismatch scores for all three models using GPT-3.5 Turbo as verifier. We use both GPT-4 Turbo and GPT-3.5 Turbo for verifying reasoning passages obtained from GPT-4 Turbo. Figure 3 shows the verification results for the GPT-4 Turbo's reasoning passages for the two verifier models.\n5.3.1 Convince-factor\nGPT-3.5 Turbo consistently assigns high scores to the main response reasoning passages from all three models (see Table 7). It rates both correct and incorrect reasoning responses highly across all models. Surprisingly, even when evaluating reasoning passages from GPT-4 Turbo, it itself struggles to distinguish between correct and incorrect responses. Interestingly, as depicted in Figure 3, GPT-4 Turbo assigns a higher fraction of reasoning passages (both correct and incorrect) a perfect score of 5 in convincingness (with and without answer) compared to GPT-3.5 Turbo. This suggests that GPT-4 Turbo performs worse than GPT-3.5 Turbo in terms of verifying its responses based on convincingness (with and without answer).\n5.3.2 Fact-check\nGPT-3.5 Turbo assigns high scores to the reasoning passages from all three models (see Table 7 in Appendix), often rating a majority of incorrect responses a perfect 5 in factuality verification. GPT-4 Turbo performs even worse in verifying its own reasoning passages (see Figure 3), assigning a higher fraction of incorrect reasoning passages a perfect 5 score compared to GPT-3.5 Turbo. This indicates that GPT-4 Turbo struggles more than GPT-3.5 Turbo in distinguishing between correct and incorrect reasoning passages, even when evaluating its own responses.\n5.3.3 Information Mismatch\nWe observe that GPT-3.5 Turbo assigns relatively high information-mismatch scores to main response reasoning passages from all three proprietary models. Table 7 shows that among the three models being tested, GPT-3.5 Turbo gives a lesser"}, {"title": "5.4 Human Evaluations", "content": "Human evaluators typically fare better than LLM evaluators. As we can see in Figure 4, correct responses are consistently given better scores than incorrect ones. However, a considerable fraction of incorrect responses can still deceive human judgment into getting scores greater than 3.\nNotably, human evaluators tend to judge incorrect responses better when the generated answer is attached. This can be possibly related to cases where the LLM infers incorrect answers even after providing correct reasoning context. Furthermore, correct responses are typically distributed towards the highest convince factor (i.e., 5); although, without the answer provided, some correct responses are given scores as low as 3. Interestingly, the scoring distribution provided by human evaluators is much closer to that provided by GPT-3.5 Turbo as verifier instead of GPT-4 Turbo."}, {"title": "6 Discussion and Conclusion", "content": "Our experiments on SCiPS-QA with a diverse array of LLMs using a comprehensive evaluation strategy reveal several key insights. Firstly, existing LLMs, whether open-access or proprietary, demonstrate a limited understanding of scientific methodologies required to serve as reliable assistants. While the parameter scaling law holds within each LLM family, models of similar size across different families are not directly comparable. For instance, Meta Llama-3 70B models emerge as formidable competitors to much larger GPT models, frequently outperforming GPT-4 Turbo in our evaluations. This reiterates earlier findings that parameter scaling alone does not reflect the capabilities of LLMs and current models, along with their training methodology, are underperforming their 'true' potential (Hoffmann et al., 2022).\nEchoing Huang et al. (2024)'s findings, we observe that powerful LLMs such as GPT-4 Turbo and GPT-3.5 Turbo struggle to reliably verify their responses. Hallucination detection techniques like SelfCheckGPT also prove ineffective in detecting incorrect reasoning posed by strong LLMs like GPT-4 Turbo in complex questions within SCiPS-QA. In fact, we notice a counterintuitive trend where GPT-3.5 Turbo assigns lower scores to incorrect responses compared to the stronger GPT-4 Turbo.\nHowever, the most concerning finding of this paper revolves around how human evaluators perceive LLM-generated scientific reasoning. When tasked with evaluating the convincingness of reasoning explanations generated by GPT-4 Turbo, human evaluators tend to assign higher ratings to a significant majority of incorrect answers. This aligns with the concern raised by Dutta and Chakraborty (2023) that current LLM-based AI assistants have the potential to propagate widespread scientific misunderstandings if left unchecked.\nImplications for future research. We hope that our proposed dataset, SCiPS-QA, along with the evaluation suit we design in this work, will serve as a valuable benchmark for future LLM research. Given the growing popularity of generalist as well as domain-specific AI assistants, we envision a positive future focus in building reliable scientific assistants. Finally, our findings with human evaluation calls upon further focus in trustworthy AI research."}, {"title": "7 Limitations", "content": "Boolean format of scientific questions has been adopted in SCiPS-QA. Having a long-text reasoning evaluation while maintaining the complexity of scientific objects should provide a stronger test for evaluating scientific communication. For this, SCIPS-QA needs to be augmented with golden reasoning passages provided by human experts. There is also a need to add more diverse topics to the SCIPS-QA, particularly in Physics, which is dominated by Quantum Mechanics (Appendix 5). There is also an issue of some queries in SCiPS-QA lying outside the knowledge cutoff of some models, making it difficult to accurately assess their reasoning capabilities. Human evaluations may be slightly limited because they do not include highly experienced evaluators in the respective subjects. The testing of reasoning passages from open-source models has also not been done as part of our analysis."}, {"title": "8 Ethical Considerations", "content": "The participants in human evaluation were not coerced into participating and were given clear and comprehensive information about the research before they provided informed consent. The identities of the human evaluators have been protected by ensuring their responses cannot be linked back to the specific individuals. The research results are communicated honestly and credibly and transparency has been maintained throughout the research process."}, {"title": "A Definitions", "content": "Hallucination: The generated content that is nonsensical or unfaithful to the provided source input (Filippova, 2020; Maynez et al., 2020), where the source input changes as the task. We take the world knowledge as the source input in our case.\nFactuality: Factuality refers to the property of quality of being actual or based on fact (Dong et al., 2020). In our work, we take \"facts\" as the world knowledge.\nConvincingness: Convincingness refers to the ability of a model to effectively influence the audience through language (Habernal and Gurevych, 2016)."}, {"title": "B Collection of Open Questions", "content": "We collect open-questions from List of unsolved problems article on wikipedia for all subjects. We also referred to the page List of open questions in theoretical computer science by Antoine Amarilli. We use GPT-3.5 Turbo to parse some of the entries on these web pages into a question format."}, {"title": "C SelfCheckGPT", "content": "C.1 Notation\nWe obtain two types of responses from proprietary models for quantifying hallucination. Let M, calling it the 'main response', denote the reasoning passage obtained at temperature 0.0. We sample N = 10 different stochastic responses:{S1, S2, ... SN}, each at temperature 1.0 using the same prompt structure, aiming to measure commonalities be-tween the stochastic responses and the main re-sponse. We use SelfGPTCheck to assign a hallucination score to ith sentence of the main response Mi: H(M)-> [0.0, 1.0], with 0.0 score given to such sentences that are completely faithful to source input and 1.0 if they are fully hallucinated. The following subsections describe the variants of SelfCheckGPT briefly that we have used in this paper.\nC.2 SelfCheckGPT with BERTScore\nLet M\u2081 and S denote the i-th sentence of the main response and the j-th sentence of the k-th stochastic response. Note all these responses are reasoning passages that are provided by the proprietary models tested. We assign a hallucination score to Mi depending on the BERTScore between Mi and Sh as follows:\n$H(M_{i}) = 1- \\frac{1}{N} \\sum_{k=1}^{N} max\\ B(M_{i}, S_{k})$ (1)\nwhere B(., .) is the dot score of sentence embeddings generated using model B. This way Mi shall be assigned a higher score if it is semantically less similar (according to BERTScore) to most of the sentences in different stochastic responses. However, if a sentence in the main response is semantically similar (or appears in) to sentences in different stochastic responses, then it will be assigned a lower hallucination score. We take the mean of the hallucination scores of each sentence of the main response to assign it a hallucination score.\nWe report results using two different models : B\u2208 {all-MiniLM-L6-v2, all-mpnet-base-v2} from sentence_transformer (Reimers and Gurevych, 2019) for generating sentence-level embeddings for eliminating any possible model bias.\nC.3 SelfCheckGPT with NLI\nThe input for NLI classifiers is typically the premise concatenated to the hypothesis, which for our methodology is the sampled passage Sk concatenated to the sentence to be assessed Mi. Only the logits associated with the 'entailment' and 'contradiction' classes are considered, We use DeBERTa-v3-base fine-tuned on MNLI for collecting the logits associated with 'contradict' class.\nSelfGPTCheck with NLI uses stochastic response Sk as the premise concatenated to the main response sentence M\u2081 to be assessed. The logits associated with token 'contradict' are used to assign a score.\nP(contradict|Mi, Sk) =$\\frac{exp(zc)}{exp(ze) + exp(zc)}$ (2)\nwhere Ze and ze are the logits of the 'entailment' and 'contradiction' classes, respectively. A higher probability denotes that the concerned main response sentence disagrees with the stochastic sample and hence, should be assigned a higher hallucination score, which is defined as,\nH(Mi) =$\\frac{1}{N}\\sum_{k=1}^{N} P(contradict|Mi, Sk)$ (3)\nWe take the average of the hallucination scores of sentences in the main response to assign a hallucination score to the entire main response M."}, {"title": "C.4 SelfCheckGPT with Prompt", "content": "We prompt GPT-3.5 Turbo to assess if the i-th sentence of the main response is supported by the k-th stochastic response, Sk. The exact prompt can be found in the appendix D.3.\nThe output from prompting when comparing the i-th sentence against sample Sk is converted to scorex through the mapping Yes: 0.0, No: 1.0, N/A: 0.5. The final inconsistency score is then calculated as:\nH(Mi) =$\\frac{1}{N}\\sum_{k=1}^{N}(x_{i}^{k})$ (4)\nNote, for all these variants, we report the results at only such data-points of SCiPS-QA where all 10 stochastic reponses are non-empty and valid. A stochastic response is considered invalid if it cannot be parsed into the boolean answer and the corresponding reasoning passage."}, {"title": "D Prompts", "content": "We shall now describe the exact prompts that we used.\nD.1 Collection of Closed Questions\nWe collect closed questions by prompting GPT-4 Turbo to create boolean problems from the passage given in the prompt. The passage is taken from the wikipedia pages of topics under different subjects."}, {"title": "D.2 Collecting Responses", "content": "We now describe the prompts that we used for collecting responses from open-source models and proprietary models.\nD.2.1 Open-source Models\nTable 3 shows the exact prompts that we used for collecting responses (A- Yes, B - No & C - I do not know) from open-source models.  We observe that low-scale models Llama-2-7B, Llama-3-8B and Mistral-7B-Instruct-v0.1 had a high percentage of invalid main responses. The instruction fine-tuned versions of models reported much lesser invalid responses at\nD.2.2 Proprietary Models\nThe prompt structure for proprietary models differs from that for open-source models with respect to the presence of 'Reason:' field in the exemplars. This is done to force these models to provide reasoning passages which are further quantified for hallucination and score for different attributes using human experts and GPT-3.5 Turbo as evaluators in a parallel setting.\nD.3 SelfCheckGPT with Prompt\nTable 4 shows the exact prompt that we used for this variant of SelfCheckGPT. The prompt is exactly same mentioned in the SelfCheckGPT paper (Manakul et al., 2023). The <CONTEXT> is replaced by each of the stochastic response passages and  is replaced by the main reasoning passages.\nD.4 NLG Evaluation of Reasoning Passages\nWe describe all the prompts that we used for this section. Note that we used GPT-4 Turbo, GPT-3.5 Turb and text-davinci-003 as the LLM modules for assigning scores to the main response reasoning passages.\nD.4.1 Convince-factor\nTable 4 shows the prompt that we used for two schemes: convince-factor-with-answer and convince-factor-without-answer. The two prompts differed only with respect to the presence of the model answer (to the boolean scientific query).\nD.4.2 Fact-check\nTable 4 shows the prompt that we used for assessing the factuality of main response reasoning passages (which replaced the  placeholder)\nD.4.3 Information-mismatch\nTable 4 shows the prompt that we used for assigning scores of this attribute. The  placeholder is replaced with the main response reasoning passage and the  placeholder is replaced with the stochastic response reasoning passages."}, {"title": "E Results", "content": "E.1 Invalid Responses\nTable 5 shows the percentage of invalid responses (to the 'answer' field of the prompt) to queries in SCIPS-QA. Llama-3 models and GPT models show fairly low numbers of invalid responses. Low scale models from Llama-2, Llama-3 and Mistral family report high percentage of invalid responses.\nE.2 Hallucination Quantification"}]}