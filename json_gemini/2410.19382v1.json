{"title": "Multi-Agent Reinforcement Learning with Selective State-Space Models", "authors": ["Jemma Daniel Ruan de Kock", "Louay Ben Nessir", "Sasha Abramowitz", "Omayma Mahjoub", "Wiem Khlifi", "Claude Formanek", "Arnu Pretorius"], "abstract": "The Transformer model has demonstrated success across a wide range of domains, including in Multi-Agent Reinforcement Learning (MARL) where the Multi-Agent Transformer (MAT) has emerged as a leading algorithm in the field. However, a significant drawback of Transformer models is their quadratic computational complexity relative to input size, making them computationally expensive when scaling to larger inputs. This limitation restricts MAT's scalability in environments with many agents. Recently, State-Space Models (SSMs) have gained attention due to their computational efficiency, but their application in MARL remains unexplored. In this work, we investigate the use of Mamba, a recent SSM, in MARL and assess whether it can match the performance of MAT while providing significant improvements in efficiency. We introduce a modified version of MAT that incorporates standard and bi-directional Mamba blocks, as well as a novel 'cross-attention' Mamba block. Extensive testing shows that our Multi-Agent Mamba (MAM) matches the performance of MAT across multiple standard multi-agent environments, while offering superior scalability to larger agent scenarios. This is significant for the MARL community, because it indicates that SSMs could replace Transformers without compromising performance, whilst also supporting more effective scaling to higher numbers of agents. Our project page is available at https://sites.google.com/view/multi-agent-mamba.", "sections": [{"title": "1 Introduction", "content": "Multi-Agent Reinforcement Learning (MARL) still faces significant challenges that must be overcome to unlock its full potential; one such challenge is the ability to scale to large numbers of agents while maintaining good performance.\nThe Multi-Agent Transformer (MAT) (Wen et al., 2022) boasts state-of-the-art performance in online MARL. MAT is an encoder-decoder framework that utilises the multi-agent advantage decomposition theorem (Kuba et al., 2022) to reframe the challenge of joint policy optimisation. It converts the problem into a sequential decision-making process, simplifying the search for optimal policies across multiple agents. However, Transformers scale quadratically in sequence length (Vaswani et al., 2023). This creates a computational bottleneck for MAT as the number of agents becomes large.\nRecently, State-Space Models (SSMs) (Gu et al., 2022; Gupta et al., 2022; Gu et al., 2021; Gupta et al., 2022; Smith et al., 2023; Fu et al., 2023) have offered a solution to this drawback in the Transformer architecture, with the ability to scale linearly in the sequence length. Of interest in this work is Mamba (Gu and Dao, 2024)-a selective SSM which boasts fast inference and linear scaling in the sequence length whilst matching the performance of attention architectures in the natural language processing domain. The innovations of the Mamba model are its input-dependent SSM parameters and its hardware-aware parallel algorithm in recurrent mode.\nIn this paper, we explore replacing attention in the MAT architecture with Mamba blocks. We make use of both existing vanilla and bi-directional Mamba blocks, as well as a novel 'cross-attention' Mamba block we designed to replace MAT's cross-attention. We evaluate our novel architecture,"}, {"title": "2 Background", "content": "In the following section we outline the necessary background for our work, including the MARL problem formulation, the multi-agent decomposition theorem, MAT and SSMs."}, {"title": "2.1 Problem Formulation: Cooperative Multi-Agent Reinforcement Learning", "content": "Like the Markov decision process formalisation of single-agent reinforcement learning, MARL can be framed as the Markov game (N, O, A, R, P, \\gamma) (Littman, 1994). Denoting N = {1, ..., n} as the set of agents, O = \\Pi_{i=1}^n O^i is the product of the agents' local observation spaces, and A = \\Pi_{i=1}^n A^i is the joint action space: a product of the agents' action spaces. R : O \\times A \\rightarrow [-R_{max}, R_{max}] is the joint reward junction. P:O\\times A\\times O \\rightarrow R denotes the transition probability function, and \\gamma\\in [0,1) is the discount factor.\nIn cooperative MARL a team of n agents seeks to maximise their joint reward over time. Based on their individual local observations o_t^i \\in O^i, each agent i = 1,...,n uses the i^{th} component of the team's joint policy \\pi to take an action a_t^i at timestep t \\in N. All agents take actions simultaneously, denoted jointly as a_t = (a_t^1,..., a_t^n), and the resulting reward R(o_t, a_t) is allocated to all agents, where o_t = (o_t^1,..., o_t^n) is the joint observation."}, {"title": "2.2 Multi-Agent Advantage Decomposition Theorem", "content": "In the multi-agent setting, Multi-Agent PPO (Yu et al., 2022) (MAPPO) has emerged as one of the leading algorithms across several benchmarks. MAPPO extends PPO to MARL by conditioning the"}, {"title": "2.3 Multi-Agent Transformer (MAT)", "content": "MAT is an encoder-decoder architecture that outputs agent actions from observations using the multi-agent advantage decomposition theorem, developed by Wen et al. (2022). It has theoretic monotonic improvement guarantees and a search space that only increases linearly in the number of agents. MAT's encoder maps agents' observation sequences to a latent representation used to estimate the value of the observations V(o) and then fed into the decoder. This is achieved using attention\n\\text{Attention}(Q, K, V) = AV, \\quad A = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}),\nwhere Q, K and V are the key, query and value matrices and A is called the attention matrix. It employs self-attention blocks in both the encoder and decoder to filter observations and actions, respectively, where Q, K and V are derived from the relevant single input sequence. An additional cross-attention block inside the decoder introduces information from agent observations during action-selection by making Q dependent on observations while K and V depend upon agent actions.\nNote that action-selection differs between training and evaluation. During evaluation, the decoder sequentially and auto-regressively transforms latent observation representations to actions, using masked (or causal) attention that prevents each agent viewing the 'future' actions of teammates. When masking, the attention matrix A becomes lower-triangular. During training, action-selection can be carried out in parallel, sampling previously collected actions from the replay buffer.\nAt time of writing, MAT represents the state-of-the-art in MARL, surpassing the performance of HAPPO and MAPPO on many tasks. MAT's use of attention allows it fine-grained control on the degree of importance each input token has on the output. However, attention has its drawbacks. MAT's attention mechanism has a time and memory cost that scales quadratically in the number of agents at inference. Additionally, MAT cannot include any information outside its context window. As a result, MAT will scale poorly as the number of agents becomes large."}, {"title": "2.4 State-Space Models (SSMs)", "content": "SSMs have recently gained popularity in deep learning applications. SSMs are inspired by the classical state-space representation from control engineering, which uses a set of first-order differential equations to map an input to an output to describe the behaviour of a physical system. The system's dynamics are determined by current observations and its existing state. For a continuous and time-invariant system with an input signal x(t) \\in R^D, an SSM will output y(t) \\in R^D via a hidden (or latent) state h(t) \\in R^N:\n\\begin{aligned}\n\\dot{h}(t) &= Ah(t) + Bx(t) \\\\\ny(t) &= Ch(t),\n\\end{aligned}\nwhere A \\in R^{N\\times N} is called the state matrix, B\\in R^{N\\times D} is called the input matrix and C\\in R^{D\\times N} is called the output matrix. The matrices A, B and C may all be made learnable during model training.\nOften, we wish to evaluate discrete inputs (x_0,...,x_t), transforming our continuous-time differential equation into a discrete-time difference equation. Using a timestep size \\Delta, we can discretise 5 using various discretisation rules, replacing 'continuous' parameters {A, B, A} with 'discrete' parameters {\\overline{A},\\overline{B}}. With the zero-order hold discretisation method, we obtain\n\\overline{A} = \\exp(\\Delta A)\n\\overline{B} = (\\Delta A)^{-1} (\\exp(\\Delta A) - I) \\cdot \\Delta B.\nOur SSM can now be evaluated for discrete inputs as a linear recurrence:\n\\begin{aligned}\nh_t &= \\overline{A} h_{t-1} + \\overline{B} x_t \\\\\ny_t &= C h_t.\n\\end{aligned}\nIn recurrent mode, an SSM is efficient when inferring (constant-time for a new sequence token) but slow during training (linear-time) since it is not parallelisable in this form.\nThe S4 (Gu et al., 2022) massages this representation into an equivalent convolutional form under a linear time invariance assumption to allow parallel training, and the authors initialise A using the HiPPO initialisation scheme for further performance boosts (Gu et al., 2020). Model design is a balance between efficiency and efficacy. While the S4 is efficient, its dynamics are constant through time. Therefore, it lacks the ability to adjust output in a content-aware manner, severely hampering effectiveness (Gu and Dao, 2024).\nMamba (Gu and Dao, 2024) attempts to plug the gap left by S4; it is a different flavour of SSM that discards the parallelisable convolutional form in favour of content-awareness, termed a selective SSM. Mamba allows some of its parameters-namely B, C and \\Delta-to depend on its inputs x_t, in particular\n\\begin{aligned}\nB_t &= \\text{Linear}_B(x_t) \\\\\nC_t &= \\text{Linear}_C(x_t) \\\\\n\\Delta_t &= \\text{softplus}(\\text{Linear}_\\Delta(x_t)),\n\\end{aligned}\nwhere softplus(\\cdot) = \\log(1 + \\exp(\\cdot)) and Linear(\\cdot) is a linear projection. Additionally, 7 can be written as an associative scan (Martin and Cundy, 2018), which allows us to use an efficient parallel algorithm (Blelloch, 1990) to offset the higher time-cost associated with training purely in recurrent mode. Mamba's expanded expressivity and computational efficiency allows it to deliver high-quality performance to match the Transformer, particularly for long sequences.\nA vanilla Mamba module is depicted in the leftmost schematic in Figure 4. The input sequence is duplicated and expanded to twice the input dimension.\nOne duplicate is processed through a causal convolution, then passed through the SiLU/Switch activation function (Ramachandran et al., 2017), and finally sent through the selective SSM. The other duplicate undergoes a SiLU activation and acts as a gate for the SSM output. The gated output is then projected back down to the original input dimension. The Mamba module in Figure 4 is wrapped with an additional normalisation layer and residual connection, forming what we will term a Mamba block in this paper. Many Mamba blocks can be chained together in this fashion, using an outermost normalisation layer for the final output; the authors use LayerNorm (Ba et al., 2016)."}, {"title": "3 Multi-Agent Mamba (MAM)", "content": "In this section, we introduce MAM as a scalable alternative to MAT, which replaces MAT's attention with Mamba blocks."}, {"title": "3.1 Encoder", "content": "A vanilla Mamba block is causal, processing a single input sequence from left to right (Gu and Dao, 2024). However, self-attention links tokens in either direction; in particular, MAT uses unmasked self-attention in its encoder (Wen et al., 2022). Therefore, we adopt a modified, bi-directional Mamba block that allows observation representations to encode information from every agent's local view, as is allowed in MAT (Wen et al., 2022). We use the architecture laid out by Schiff et al. (2024), shown in the rightmost schematic in Figure 4. While originally applied to DNA sequence modelling, its architecture is domain-agnostic. The bi-directional Mamba block applies the original Mamba module twice. We begin by applying the Mamba module to the original input sequence and then to a reversed copy along the length (or agent) dimension. To merge the results, we flip the reversed output along its length and add it to the forward output.\nTo avoid doubling the number of parameters in a bi-directional Mamba block compared to vanilla Mamba, we share parameters. The 'forward' and 'reverse' Mamba modules, which account for a large portion of the block's parameters (Gu and Dao, 2024), instead use the same projection weights."}, {"title": "3.2 Decoder", "content": "In the decoder, it is important to ensure the current agent only accesses the information of preceding agents. MAT achieves this using causal masking (Ba et al., 2016), but the Mamba module is causal out-of-the-box. This makes causal self-attention replacement straightforward: simply use Mamba as a drop-in replacement, feeding agent actions through the vanilla Mamba block to obtain latent action representations.\nHowever, replacement of MAT's cross-attention requires more care. Vanilla Mamba only processes a single input sequence, but cross-attention attends between two different sequences-latent observation and action representations in MAT's case. We adapt Mamba to process information from two inputs, creating a 'cross-attentional' Mamba block which we name CrossMamba. Following"}, {"title": "4 Experiments", "content": "In this section, we describe our experimental setup, followed by detailed and aggregated results across a wide range of tasks."}, {"title": "4.1 Experimental Setup", "content": "Environments We evaluate MAM on three JAX-based versions of well-established environments: Robotic Warehouse (RWARE) (Papoudakis et al., 2021) from Jumanji (Bonnet et al., 2024), Level-Based Foraging (LBF) (Christianos et al., 2021) and the StarCraft Multi-Agent Challenge (SMAX) (Rutherford et al., 2023). For each environment, we choose a wide range of tasks with varying difficulty and number of agents."}, {"title": "4.2 Results", "content": "Individual Task Results In Figure 1 we show the performance of MAM against MAT and MAPPO aggregated over all environments and task, including 95% confidence intervals. MAM matches the aggregated performance of MAT, currently state-of-the-art in the MARL research field, and displays faster learning capabilities. We find that MAM matches or outperforms MAT in 18 of the 21 tasks, indicating that attention can be replaced without sacrificing performance in MARL tasks.\nEnvironment Aggregated Results In order to get a more reliable comparison between the algorithms, and to avoid cherry-picking tasks, we additionally compute aggregated results per environment. In particular, we first normalise the scores on all tasks in an environment and then aggregate them using MARL-eval (Gorsane et al., 2022). The aggregated sample efficiency curves and performance profiles are given in Figure 7. From these plots, we can see that on all three environments MAM achieves very similar final performances to MAT. Moreover, MAM's sample efficiency closely matches MAT's on RWARE and LBF. Interestingly, on SMAX MAM's sample efficiency is significantly better than that of MAT. This is noteworthy because SMAX tasks feature an average of 9.7 agents, compared to 3 and 3.3 on LBF and RWARE, respectively. This suggests that, on tasks with more agents, MAM may have superior sample-efficiency. However, more experimental evidence on tasks with even larger numbers of agents is required to verify this claim.\nInference Speed Results Due to its selectivity, Mamba executes in only recurrent mode. This proves advantageous at inference-time, shown in Figure 2. Empirically, we observe that MAM scales better than MAT in number of agents.\nThis result aligns with existing scaling results (Vaswani et al., 2023; Gu and Dao, 2024). Faster inference time could allow real-world, many-agent systems to be better modelled by MAM than MAT-the latter of which will become prohibitively slow."}, {"title": "5 Model Ablations", "content": "The inclusion of Mamba in the MAT architecture introduces two Mamba-specific parameters: the size of the hidden state dimension, N, and the projection dimension of \\Delta. The parameter \\Delta controls the amount of information included from the current input, and generalises the RNN gating mechanism."}, {"title": "6 Discussion", "content": "Limitations In this paper, we have not explored the effectiveness of MAM in tasks involving very large numbers of agents. This constraint arises because such tasks are currently limited in the MARL literature. Naively scaling agent numbers in LBF and SMAX results in sharp rises in time and memory cost since the size of the observation space increases with the number of agents. RWARE offers control over observation space size through modification of agents' field of view, but this also substantially increases its difficulty, preventing informative benchmarking on a many-agent modification.\nAdditionally, MAM and MAT are implemented using JAX to leverage the accelerated MARL library Mava (de Kock et al., 2023). Although JAX's Accelerated Linear Algebra (XLA) compiler automatically optimises code at runtime for faster execution on various hardware-including CPUs, GPUs, and TPUs-we cannot replicate the hardware-aware parallelism achieved by the original Mamba implementation in PyTorch, which uses custom CUDA kernels. JAX's tool for writing custom kernels, Pallas, is relatively new and still under active development.\nFuture Work To address these limitations, we plan to create or modify environments to facilitate the evaluation of MAM in many-agent tasks. Furthermore, we intend to implement MAM in PyTorch to leverage the performance benefits of custom CUDA kernels"}, {"title": "7 Related Work", "content": "Two existing works are related to MAM: Decision Mamba (Ota, 2024) for offline single-agent RL, and CrossMamba for target sound extraction (Wu et al., 2024).\nDecision Mamba modifies the Decision Transformer (Chen et al., 2021) by replacing causal self-attention with a vanilla Mamba block. In contrast, MAM is tailored for online learning in a multi-agent setting.\nThe concurrent work by Wu et al. (2024) introduces the CrossMamba block for extracting target sounds from audio features. While similar to the cross-attention Mamba block we propose, CrossMamba was developed independently for an entirely different domain."}, {"title": "8 Conclusion", "content": "In this work, we introduced the Multi-Agent Mamba, a new sequence-based architecture for MARL that matches the performance of existing state-of-the-art methods with improved efficiency for many-agent scenarios. We demonstrate its performance against MAT, currently state-of-the-art, and MAPPO on a range of well-known MARL benchmarking environments. This suggests that Mamba can port well to MARL tasks, solving scaling issues in tasks with many agents. It is our hope that this investigation contributes to the further development of efficient and effective multi-agent decision-making algorithms."}, {"title": "A Implementation Details", "content": "We closely follow the original Mamba authors' (Gu and Dao, 2024) implementation, which includes some modifications to the SSM theoretical results laid out in this paper.\nThe first of these is the addition of a residual connection to the SSM equations in 5, which becomes\n\\begin{aligned}\n\\dot{h}(t) &= Ah(t) + Bx(t) \\\\\ny(t) &= Ch(t) + Dx(t),\n\\end{aligned}\nwhere all variables are defined the same as in 5 and D \\in R^{D\\times D} can be viewed as a residual connection that allows the current input x_t to further influence the SSM output.\nSecond is the use of a first-order approximation when discretising B, particularly \\overline{B} = \\Delta B.\n\\begin{aligned}\n\\overline{A}_{ij} &= S_C(\\vec{x}_i) \\left( \\prod_{k=j+1}^{i} \\exp \\left( \\text{softplus} (S_\\Delta(\\vec{x}_k) A) \\right) \\right) \\\\\nS_B(\\vec{x}_j) \\\\\n&= Q_i H_{i,j} K_j,\n\\end{aligned}\nImportantly for our purposes, this does not change cross-attention formulation for CrossMamba."}, {"title": "B Hyperparameter Settings", "content": "We utilise Mava's design architecture to distribute the entire reinforcement learning training loop across multiple devices with the JAX pmap transformation, in addition to vectorising training with the JAX vmap transformation."}, {"title": "B.1 Benchmarking Results", "content": "All algorithms on all tasks are run using 64 vectorised environments. We use the same default hyperparameters and hyperparameter search spaces for a given algorithm on all tasks. We constrain the maximum model size allowable during hyperparameter tuning similarly for both MAM and MAT. All algorithms are tuned using the TPE Bayesian optimisation algorithm from the Optuna library (Akiba et al., 2019) with a tuning budget of 40 trials."}, {"title": "B.2 Inference Speed Results", "content": "MAT and MAM are run for 30 evaluations of 32 episodes and averaged over three seeds, and we use two parallel environments. We plot the inverted mean steps per second during evaluation."}, {"title": "B.3 Ablation Results", "content": "We run ablation experiments using the same experimental setup as for the benchmarking results."}, {"title": "C Computational Resources", "content": "Model training and inference was run on various GPUs, namely NVIDIA Quadro RTX 4000 (8GB), Tesla V100 (32GB), and NVIDIA A100 (40GB and 80GB)."}]}