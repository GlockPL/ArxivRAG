{"title": "Pre-Trained Vision-Language Model Selection and Reuse for Downstream Tasks", "authors": ["Hao-Zhe Tan", "Zhi Zhou", "Lan-Zhe Guo", "Yu-Feng Li"], "abstract": "Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called Model Label Learning (MLL). The proposal contains three key modules: model labeling, which assigns labels to each VLM to describe their specialty and utility; model selection, which matches the requirements of the target task with model labels; and model reuse, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs.", "sections": [{"title": "1. Introduction", "content": "Vision-Language Models (VLMs), such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), etc, which are pre-trained on large-scale image-text datasets, have recently attracted significant attention due to their remarkable zero-shot prediction capabilities on visual tasks. However, though VLM shows impressive general ability, as highlighted in Radford et al. (2021), VLMs often fall short of supervised expert models in many downstream tasks. To address this limitation, numerous studies (Dosovitskiy et al., 2021; Yu et al., 2022; Fang et al., 2023) have sought to enhance the zero-shot performance of VLMs by studying model architectures, pre-training datasets, and training/fine-tuning methods. This effort has led to the development of many open-source pre-trained VLMs with diverse structures and parameters, contributing to VLM model hubs like open-clip (Ilharco et al., 2021), which currently hosts more than 100 pre-trained VLMs.\nAs more and more VLMs are open-sourced, the problem of how to select a VLM to reuse for specific downstream tasks naturally occurs. Although we can directly utilize the best-performing model on a universal dataset such as ImageNet, previous work (Fang et al., 2022) has shown that the performance of VLMs can vary greatly depending on dataset domain. For example, we evaluate the performance of various pre-trained VLMs in the open-clip library across several downstream tasks (1(a)) and within different classes of a specific task (1(b)). Figure 1(a) reveals that each VLM demonstrates distinct strengths in zero-shot visual tasks, with no single model outperforming all others across every task. Interestingly, models that perform worse on general tasks can sometimes surpass stronger models in specific downstream tasks. Furthermore, even in the same task, different VLMs exhibit varying levels of performance across specific classes, as illustrated in Figure 1(b).\nTherefore, it is important to design VLM selection methods, and it would be better if we could achieve more fine-grained selection, i.e., select different VLMs to handle different classes. The direct way to select a model is to evaluate all candidate models' performance on the target task. However, it is unrealistic due to time and computational resource limitations. Additionally, previous works on model selection (Tran et al., 2019; You et al., 2021) primarily focus on single-modal models, making them unsuitable for VLM selection since they only handle either image or text output and cannot incorporate data from the other modality. Zohar et al. (2023) is the first study to focus on VLM selection, proposing to evaluate VLM performance using textual information. However, their selection strategy heavily depends on the models' ground-truth performance on large-scale datasets, such as ImageNet. When models excel on large-scale datasets but under-perform on specific tasks, selection strategy effectiveness drops, as shown in our experiments."}, {"title": "2. Related Work", "content": "Vision-Language Model. In recent years, there have been significant advances in the field of Vision-Language Models (VLMs), including notable models such as CLIP (Radford et al., 2021), BLIP (Li et al., 2022), etc. These models leverage large-scale datasets containing image-text pairs, such as WIT (Srinivasan et al., 2021), to align visual and text features within a shared embedding space, which has led to impressive capabilities in feature extraction, particularly in the realm of zero-shot visual tasks. Tremendous works (Dosovitskiy et al., 2021; Yu et al., 2022; Fang et al., 2023) attempted to improve the zero-shot capabilities of VLMs by focusing on model architecture, pre-training datasets, and training/fine-tuning methods, which lead to the emergence of numerous open-source pre-trained VLMs. As a result, several VLM model hubs are constructed, such as open-clip (Ilharco et al., 2021) and HuggingFace (Wolf et al., 2020), which provide access to numerous VLMs. However, these model hubs lack effective model selection mechanisms; users cannot directly select suitable models for their tasks, they can only select models based on some quantitative indicators, such as download volume, popularity, etc.\nModel Selection. As pre-trained models become increasingly diverse, how to select appropriate pre-trained models to tackle specific tasks has become a significant challenge. Many researchers have started to focus on this aspect. For example, Negative Conditional Entropy (NCE) (Tran et al., 2019) proposes an information-theoretic quantity to learn the transferability and hardness between classification tasks; LEEP (Nguyen et al., 2020) utilizes source prediction probabilities instead of hard labels compared with NCE; LogME (You et al., 2021) estimates the correlation between source model features and the target outputs by maximum evidence; MetaGL (Park et al., 2023) solves the model selection problem on graph data by introducing a meta-learning method; EMMS (Meng et al., 2023) uses weighted linear regression to estimate the transferability of candidate models; Model Spider (Zhang et al., 2024) uses a re-ranking mechanism to enhance the task-model co-embedding. Although these methods achieve well-performing in different settings, most of them focus on single-modal which cannot be directly used for VLM selection. Moreover, the training data for VLM is inaccessible, which introduces more challenges. Model selection for VLM is still a relatively new topic. LOVM (Zohar et al., 2023) uses a text dataset to describe the prediction task to train a linear model to predict the performance of the VLM. However, this method can only exploit text information and becomes less effective when there is a domain shift between downstream tasks and training tasks.\nLearnware. Learnware (Zhou & Tan, 2022) is a novel paradigm that explores more effective model selection by constructing specifications to describe the capabilities of the model, closely aligning with our idea of model labeling. Compared with previous selection methods, learnware enables scalable and efficient model selection across diverse architectures and input types within a unified framework, improving as the system expands. Model specification is central to the learnware paradigm. Recent works (Tan et al., 2024) on learnware paradigm are built on Reduced Kernel Mean Embedding (RKME) (Wu et al., 2021), which maps training data distributions to points in Reproducing Kernel Hilbert Space (RKHS) and identifies models by comparing similarities in the RKHS. Furthermore, Guo et al. (2023) enhanced RKME for heterogeneous label spaces, while Tan et al. (2023) addressed challenges in heterogeneous feature spaces. However, learnware requires training data to construct specifications. Considering the scale of VLM pre-trained datasets, it is unrealistic to construct specifications for learnware to select models due to limited time and computational resources."}, {"title": "3. Preliminaries", "content": "3.1. Zero-Shot Vision Task of VLM\nPre-trained VLMs for zero-shot visual tasks are built using two encoders: image encoder and text encoder. The image encoder is used to transform an image into a vector embedding, which presents its feature. The text encoder tokenizes the text input and generates a embedding representation by the text token. Let $I : X \\rightarrow R^n$ denotes the image encoder and $T : Y\\rightarrow R^n$ denotes the text encoder, where $X \\in X$ is the image input, $Y \\in Y$ is the text input, and n is the dimension of the shared multi-modal embedding space of text embeddings and image embeddings.\nIn a particular downstream task T, there are $C_T$ classes $Y_T = \\{y\\}_{i=1}^{C_T}$. For a image $x \\in X$, we obtain the image embeddings $I(x)$ given by the image encoder I and the text embeddings $T(y)$ of class y produced by the text encoder T. Then, the prediction \u0177 of image x can be obtained as\n$\\hat{y} = arg max_{y \\in Y_T} \\frac{exp (sim (I(x), T(y)))}{\\sum_{y' \\in Y_T} exp (sim (I(x), T(y')))}$  (1)\nwhere sim (,) denotes cosine similarity.\n3.2. Problem Setup\nAssume the model hub has M pre-trained VLMs $\\{f_m = \\{I_m, T_m\\}\\}_{m=1}^{M}$, where $I_m$ denote image encoder of the VLM $f_m$ and $T_m$ denote text encoder of the VLM $f_m$. There are two stages in our setting: submission stage for model developers to upload models and identification stage for users to select models.\nIn submission stage, model developer submits a VLM $f_m$ to the model hub, and the model hub assigns a label $S_m$ to the model to describe its specialty and utility. It is particularly emphasized that uploaded models are anonymous, meaning we do not have access to their training data.\nIn identification stage, user attempts to select VLMs from the model hub for the zero-shot downstream task T, by uploading general information about the task, such as classes, domain type, and task type, to describe their requirements."}, {"title": "4. Our Approach", "content": "As illustrated in Figure 2, the MLL paradigm consists of three key modules: model labeling, model selection, and model reuse. In the model labeling process, MLL constructs a semantic graph G with commonly occurring visual concepts and representative samples as the evaluation datasets. When models are submitted to the model hub, they are pre-tested on the semantic graph and assigned labels Sm, which describe their capability on these semantic classes. In the model selection process, we generate caption descriptions for both the nodes in the semantic graph and the categories in the target task to compare their similarity. This enables us to evaluate the model's performance on the target classes by aligning the matched semantic nodes with the model labels. In the model reuse process, we apply an ensemble strategy that combines the selected models' predictions on a single class and chooses the highest confidence across all classes as the final prediction.\nThe model labeling process is completed immediately when the candidate VLM is added to the model hub, therefore, it is target task independent, which means the proposal is both data and computationally efficient in the model selection process. Moreover, the proposal is highly growable since the capability could grow with the number of candidate models in the model hub and the model labels are also scalable since more semantic nodes can be added continually. To facilitate related research, we further introduce a comprehensive benchmark for evaluating VLM selection methods. The benchmark includes 49 pre-trained VLMs and 17 target datasets as downstream tasks. The ground-truth model ranking for each target task is provided for evaluation. We construct a semantic graph that contains more than 9000 commonly used visual concepts to pre-test VLMs. The experiments demonstrate the effectiveness of our approach in both selecting and reusing VLMs, while also validating the scalability of the model hub.\nIn summary, our contributions are as follows:\n\u2022 Problem: We highlight that the performance of pre-trained VLM varies across different downstream tasks, even among classes within the same task. To address this, we first propose an important yet rarely studied problem called VLM selection and reuse, which will promote the deployment of VLM in more real tasks.\n\u2022 Method: We propose a novel paradigm called Model Label Learning, which includes the processes of model labeling, selection, and reuse. This paradigm is both time- and data-efficient, and highly scalable. It can give birth to new VLM model hubs, which can simplifying user selection and reuse of VLMs for their tasks.\n\u2022 Evaluation: We introduce a new benchmark for evaluating pre-trained VLM selection and reuse methods, advancing research in this field. Experimental results validate the effectiveness and scalability of our proposal for selecting and reusing VLMs.\nThe two main problems in our settings are:\n1) In submission stage, how can we design a label to fully characterize the capabilities of the submitted VLM?\n2) In identification stage, how can we select and reuse appropriate VLMs from the model hub to address users' downstream tasks based on their requirements and the model labels generated in the submission stage?"}, {"title": "4.1. Model Labeling", "content": "To thoroughly characterize the capabilities of the model, we initiate the process by constructing a Semantic Graph G as evaluation datasets utilizing the WordNet (Miller, 1995) synsets. Firstly, we represent each synset in WordNet as a corresponding node v within the semantic graph and establish links between nodes based on their relationships of hypernyms and hyponyms. Subsequently, to capture the real-world image distribution associated with each node, we randomly select images $X_v$ from sample datasets (detailed in Section 5.1) to serve as representations for each node v. Due to the limited information in synset name, we also need obtain the caption dataset $D_G = \\{d_v|v \\in V_G\\}$ for label generalization where $V_G$ denotes the set of nodes in Semantic Graph G, $d_v$ denotes the caption of node v. We use \"\\{synset name\\} which is \\{synset definition\\}\" as the caption for each node, where \"\\{synset name}\" and \"\\{synset definition}\" correspond to the synset name and definition of a synset. Utilizing the constructed semantic graph, we generate a label $S_m$ for each VLM $f_m$ in the model hub that accurately reflects its capabilities.\n$s_{m,x} = sim(I_m(x), T_m(D_G)), x \\in X_v$  (2)\n$s_m^v = \\{s_{m,x}| \\forall x \\in X_v\\}$  (3)\n$S_m = \\{s_m^v| v \\in V_G\\}$  (4)\nwhere $I_m(\\cdot)$, $T_m(\\cdot)$ denotes the image encoder and text encoder of model $f_m$.\nSpecifically, the constructed semantic graph allows for the seamless addition of new nodes and the incremental updating of model labels based on existing foundations. As the nodes in the semantic graph are expanded, its ability to reflect the performance capabilities of the models is enhanced. Once we have obtained labels for each model, we can utilize them for effective model selection."}, {"title": "4.2. Model Selection", "content": "In the model selection module, given a downstream task T with $C_T$ classes $Y_T = \\{y\\}_{i=1}^{C_T}$, in order to utilize the obtained model labels $S_m$, we need to match the downstream task classes $Y_T$ with the semantic graph nodes $V_G$. However, it can not match well using original class names. Inspired by previous work (Zohar et al., 2023), we construct expanded captions for both the downstream task classes and the semantic graph nodes. Large Language Models (OpenAI, 2023) have made significant advancements, facilitating the generation of text data. Assuming general information about the downstream tasks, such as task types and target domain, is accessible, we use GPT-4 with specific prompts to generate descriptions for each class as shown below, creating the caption dataset $D_T$ for downstream task T. The following is an example of a prompt used to generate a caption of the class cat.\nThen, we can use a language model to generate embeddings of graph captions $D_G$ and target task captions $D_T$. By comparing the cosine similarity between the embeddings, we can select the top k nodes for each class based on similarity and construct a transfer matrix $Z = (z_{vy}) \\in R^{\\|V_{Selected}\\| \\times C_T}$"}, {"title": "4.3. Model Reuse", "content": "To better utilize the selection and harness the capabilities of models in the model hub, we introduce a specific count k of models to reuse for each class y, we select up to k highest-score model to form the ensemble predictor $F_k = \\{f_m | f_m \\in top-k(\\{r_{m,y}\\}^M)\\}$. During testing, for the data x \u2208 X of the downstream task, ensemble predictor $F_k$ infers the confidence $p_k(x)$ of class y:\n$p_k(x) = \\sum_{f_m \\in F_k} w_{m,y} \\frac{exp (sim (I_m (x), T_m(y)))}{\\sum_{y' \\in Y_T} exp (sim (I_m(x), T_m (y')))}$  (8)\nwhere $w_{m,y}$ denotes the ensemble weight obtained from the output probability entropy H of each model within $F_k$. $w_{m,y}$ is defined as:\n$w_{m,y} = \\frac{H(\\{sim(I_m(x), T_m(y)| \\forall y \\in Y_T\\})}{\\sum_{f_{m'} \\in F_k} H(\\{sim(I_{m'}(x), T_{m'}(y)| \\forall y \\in Y_T\\})}$  (9)\nVishniakov et al. (2024) has shown VLMs predictions frequently exhibit overconfidence, which can degrade the performance of ensemble predictions, especially when the model makes incorrect predictions. To address this, we introduce $w_{m,y}$ based on the probability entropy H as a measure of uncertainty and assign lower weights to models with high confidence when they are overconfident, which reduces the impact of overconfidence, potentially erroneous predictions on the final ensemble output, thereby enhancing the robustness and accuracy of model's overall predictions.\nThen, the class with the highest confidence is selected as the prediction \u0177 for x:\n$\\hat{y}(x) = arg \\max_{y \\in Y_T} p_k(x)$  (10)\nFlow of model selection and reuse of MLL Paradigm are summarized in Algorithm 1."}, {"title": "4.4. Discussion", "content": "Our proposal achieves higher accuracy, efficiency, and scalability. In terms of accuracy, the proposal elucidates the functionalities of VLMs by labeling models with a semantic graph that covers the most common visual concepts and representative samples to describe different data distributions, enabling more accurate identification of suitable models for users' target tasks. For efficiency, the proposal generates model labels when the pre-trained model is uploaded to the model hub, thus, it is highly efficient in the model selection process, without the need to run the candidate models on the target dataset. Regarding scalability, the concepts in the semantic graph can be continually added, thus, the model labels are scalable flexibility. Moreover, as the number of VLMs in the model hub increases, our proposal identifies higher-quality models, leading to improved performance on zero-shot downstream visual tasks."}, {"title": "5. Experiments", "content": "5.1. MLL Benchmark\nTo evaluate the capabilities of the MLL paradigm in zero-shot visual tasks with VLMs, we need to obtain a set of sampling datasets for constructing semantic graph G, along with another set dedicated to downstream target tasks. For this study, we select 49 VLMs, 5 Sample Datasets, and 17 Target Datasets. Additionally, we collect general information about task types and domains associated with each dataset to provide a task description. For testing selected models on target tasks, we utilized same prompting strategy outlined in Radford et al. (2021), ensuring consistency in our evaluation methodology. Code of MLL Benchmark and our proposal is available at this anonymous link.\nModel Hub. We leverage the open-clip library (Ilharco et al., 2021), which encompasses a diverse set of pre-trained VLMs across multiple architectural frameworks, such as ViT(Dosovitskiy et al., 2021) and ConvNet(Liu et al., 2022). These models have been pre-trained on a variety of large-scale datasets, such as WIT (Srinivasan et al., 2021) and LAION-2B (Schuhmann et al., 2022). We select 49 models from this library to form our model hub for the purpose of our experiments. All models used in the experiments are directed downloaded from the library.\nDatasets. We utilized 5 datasets, ImageNet (Deng et al., 2009), ImageNet-V2 (Recht et al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b) and ImageNet-R (Hendrycks et al., 2021a), as Sample Datasets for semantic graph construction. Additionally, we used 17 commonly used datasets and their task general information as Target Datasets to evaluate VLM selection and reuse methods in zero-shot visual tasks (as shown in Table 5). These datasets demonstrate diversity in terms of domain, number of classes, and task types. They encompass various domains, including animals, food, text, landscapes, remote sensing, medical, and transportation. Additionally, they cover a range of tasks such as image classification, geo-localization, optical character recognition, facial expression recognition, and object distance estimation. To eliminate interference from additional modules during evaluation, all tasks can be assessed using the same VLM architecture.\nEvaluation Metrics. In our benchmark, methods are expected to select models from a hub of 49 pre-trained VLMs and reuse them across 17 target datasets as downstream tasks to achieve better performance. Notably, all models selected for use are without additional fine-tuning, as all downstream tasks are zero-shot. We use Acc. to evaluate methods' performance on both downstream target tasks and the average performance across all tasks."}, {"title": "5.2. Experiment Setup", "content": "Semantic Graph Construction. We construct a semantic graph G containing 9055 nodes using the WordNet synsets, which contains a wide range of items, such as animals, tools, clothing, vehicles, plants, and more. Each node is represented by up to 75 randomly selected images from the sample datasets, reflecting the distribution of the node's concepts. We use OpenAI text-embedding-3-large model to obtain caption embeddings of semantic graph nodes and downstream task class nodes, then match the similar node between them by cosine similarity between the embeddings.\nCompared Methods. Initially, we compare our proposal with ImageNet Baseline (INB), which simply select the model which has best performance on the ImageNet in the model hub to reuse. Additionally, we compare it with a VLM selection method called ModelGPT (Zohar et al., 2023). ModelGPT employs generated captions and synonyms for target task classes as substitutes for images of those classes, then evaluates the performance of VLMs by measuring their ability to correctly classify the captions and synonyms into their corresponding classes, which serves as the reuse metric in combination with INB. A linear model is then learned between the reuse metric and ground-truth performance on training downstream tasks. Finally, the zero-shot ability of VLMs on the target task is predicted using this linear model and the reuse metric.\nImplementation Details. We adopt the official code to implement ModelGPT. For a fair comparison, the experiment utilizes the ground-truth performance of VLMs on Sample Datasets for ModelGPT to train its linear model, and then evaluate it on the benchmark. For both INB and ModelGPT, the experiment selects the model with the highest predictive performance given by the method for reuse in the target task. Specifically, we employ the same prompting strategy outlined in Radford et al. (2021), which uses the prompt \"a photo of \\{class\\}\u201d, where \u201c\\{class}\" is replaced by the task class. All selected models are utilized without any further fine-tuning, given that all downstream tasks are conducted in a zero-shot manner. Additionally, the weight a for model selection in our setting is set to 0.7. All experiments are conducted on NVIDIA A800 GPUs."}, {"title": "5.3. Experiment Results", "content": "Zero-shot Performance. In our setup, the goal is to optimize the performance of VLMs on downstream zero-shot visual tasks. Therefore, in Table 1 and Table 2, we compare the performance of different model selection methods across 17 target datasets. We set two values for the count k of reused models, specifically 1 and 3, to test the effects of using a single model versus an ensemble of three models per class. In the ensemble setup, we first selected the top-k models from each method and then applied the same reuse strategies discussed in Section 4.4. The results show that our method achieves high performance on most downstream tasks. ModelGPT largely aligns with INB, indicating a strong correlation in their selection strategies. When INB fails to select a well-performing model, ModelGPT also struggles with selection. By comparing different counts k of reused models, MLL demonstrates that reusing the model with the best performance per class is often sufficient to outperform baseline methods in most downstream tasks, highlighting the practicality of the MLL paradigm. We also find that in datasets with a limited number of classes, such as PCam, employing a single model for each class tends to yield better results. Additionally, when the models available in the model hub are generally weak, as seen in several datasets, such as CLEVR-D, relying on ensemble methods may introduce more noise than benefit. In these cases, a single model per class often provides the ultimate balance between simplicity and effectiveness."}, {"title": "Scalability of Model Hub", "content": "We design a scenario where the model hub starts from scratch and gradually expands until it contains all available VLMs. Figure 3 provides a detailed illustration of the average performance of 17 downstream tasks throughout 30 randomly generated expansion schemes. The results show that as the model hub grows, our method can more efficiently reuse the well-performing VLM models for various tasks, reducing the limitations in model selection and boosting system performance across a range of visual tasks. This shows that our method is not only highly effective in the present but also holds the potential for continued improvement as the model hub grows.\nAblation Study. To analyze the influence of key hyperparameters in our proposal, we conducted an ablation study focusing on the effects of the parameter a (as defined in Equation 7) and the reuse model count k. The impact of a is presented in Table 3. The results demonstrate that our proposal consistently outperforms the compared methods across a range of a values, highlighting the robustness of the model to this parameter. Additionally, the effect of k is shown in Table 4, where the results show that using a smaller number of models strikes an optimal balance between performance and computational efficiency, with minimal performance loss compared to larger ensembles."}, {"title": "6. Conclusion", "content": "In this paper, we explore how to select and reuse pre-trained VLMs for a specific downstream task. To the best of our knowledge, this problem has been rarely studied. To address this, we propose a novel paradigm called Model Label Learning (MLL) that assigns each VLM a label to describe its utility on representative visual concepts. The MLL paradigm contains three key modules: model labeling, model selection, and model reuse. The proposal is highly efficient, scalable, and convenient for both model developers and users. Moreover, we introduced a benchmark for evaluating pre-trained VLM selection and reuse methods that contain 49 pre-trained VLMs and 17 target datasets. Experiments demonstrate the proposal can achieve state-of-the-art model selection performance for VLMs and the ability to deal with downstream tasks could grow with the scale of the model hub, showing the potential of building large model hubs with advanced model selection mechanisms.\nIn future work, we will endeavor to develop a novel model hub based on the MLL paradigm presented in this paper, allowing valid VLM developers from all over the world to submit their models. When users work on visual tasks, they will be able to select and reuse models from the hub. The limitation of this paper is that the current implementation focuses solely on VLMs and visual classification tasks. We will further attempt to extend our paradigm to more model types that have significant architectural differences compared with VLMs, and more complex tasks."}]}