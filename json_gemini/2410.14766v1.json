{"title": "EVALUATING QUANTIZED LARGE LANGUAGE MODELS FOR CODE GENERATION ON LOW-RESOURCE LANGUAGE BENCHMARKS", "authors": ["Enkhbold Nyamsuren"], "abstract": "Democratization of AI, which makes AI accessible and usable for everyone, is an important topic, within the broader topic of the digital divide. This issue is especially relevant to Large Language Models (LLM), which are becoming increasingly popular as AI co-pilots but suffer from a lack of accessibility due to high computational demand. In this study, we evaluate whether quantization is a viable approach toward enabling LLMs on generic consumer devices. The study assesses the performance of five quantized code LLMs in Lua code generation tasks. All code LLMs had approximately 7 billion parameters and were deployed on a generic CPU-only consumer laptop. To evaluate the impact of quantization, the models were tested at 2-, 4-, and 8-bit integer precisions and compared to non-quantized code LLMs with 1.3, 2, and 3 billion parameters. Along with tasks such as question answering, text summarization, and text generation, programming tasks are one of the popular applications of AI co-pilots. Furthermore, code generation is a high-precision task, which makes it a suitable benchmark to evaluate and compare quantized models for everyday use by individuals. Lua is chosen as a low-level resource language to avoid models' biases related to high-resource languages. The results suggest that the models quantized at the 4-bit integer precision offer the best trade-off between performance and model size. These models can be comfortably deployed on an average laptop without a dedicated GPU. The performance significantly drops at the 2-bit integer precision. The models at 8-bit integer precision require more inference time that does not effectively translate to better performance. The 4-bit models with 7 billion parameters also considerably outperform non-quantized models with lower parameter numbers despite having comparable model sizes with respect to storage and memory demand. While quantization indeed increases the accessibility of smaller LLMs with 7 billion parameters, these LLMs demonstrate overall low performance (less than 50%) on high-precision and low-resource tasks such as Lua code generation. While accessibility is improved, usability is still not at the practical level comparable to foundational LLMs such as GPT-40 or Llama 3.1 405B.", "sections": [{"title": "1 Introduction", "content": "Since the transformers were first proposed in 2017 in the seminal study by Vaswani et al. [1], there have been significant advancements in the development of Artificial Intelligence. Soon after, in less than a year, OpenAI published the first GPT (Generative Pre-trained Transformer) model that demonstrated an improvement of over 5% over existing best solutions [2]. Along with BERT (Bidirectional Encoder Representations from Transformers) [3], it became one of the first Large Language Models (LLM) as we know them today. Since then transformer-based large language models experienced rapid development.\nWhen ChatGPT, powered by OpenAI's GPT-3.5, became accessible to the public in 2022, it convincingly demonstrated its capability to assist humans in various tasks. Today, many attempts are trying to leverage the power of large (language) models in a variety of applications [4; 5]. Specific examples include using LLMs as AI-tutors in education [6], clinical decision support systems [7], and coding co-pilots [8].\nBecause LLMs are finding increasing adoption in everyday life, it is not far-fetched to assume that access to AI co-pilots can soon dictate how productive and, therefore, successful a person or an organization is. We have already experienced how increasing dependency on technology can lead to the digital divide [9], people not being able to enjoy the same opportunities because of lack of access to ICT. For example, the earliest form of the digital divide was and still is concerning the Internet in the early 2000s. Lythreatis, Singh, and El-Kassar [9] mention three levels of the digital divide regarding the Internet. The first level concerns the gap in access to the Internet. The second level concerns the digital inequality in skill and knowledge necessary for using the Internet. The final level is about the overall beneficial or adverse outcomes of using the Internet. The digital divide at all three levels still persists. Despite the increasing dependency on the Internet for essential everyday activities, people still have unequal access to the Internet [10]. Moreover, young people are often perceived as 'digital natives', yet they are also victims of the digital divide lacking both access and skills [11].\nThe three levels of the digital divide initially attributed to the Internet are also highly relevant to AI. If AI adoption follows the same trend then inequality in access to AI can lead to a wider digital divide [12; 13]. Unequal access to AI, lack of skills and knowledge to effectively and efficiently use Als, and misuse of AI can all contribute to the widening digital divide. This highlights the importance of the democratization of AI, that is how accessible AI is to ordinary individuals. Democratization is especially relevant to language models due to the issue of high computational demand that cannot be easily resolved by personal users.\nIn this study, we explore AI with respect to the first level of the digital divide. That is how accessible Large Language Models for code generation are to regular users. For this purpose, we evaluate the performance of smaller LLMs with less than 10 billion parameters in code generation tasks. We have specifically chosen the Lua programming language as a testbed. Lua is an example of a low-resource language [14], a language with a relatively small size of data for training LLMs. For this reason, we can avoid the bias toward high-resource languages and obtain a more representative performance evaluation of LLMs in the code generation task across different languages.\nOne of the main focuses of this study is to evaluate LLMs on a typical consumer device, e.g. a laptop. The mainstream online solutions, such as GPT-40 and Claude 3.5, are often pay-walled and/or have usage restrictions. Furthermore, these are black box models that have certain privacy and security risks. On the other hand, there are free and open-source alternatives such as Llama 3.1 [15]. However, even these open-source models can be too demanding to be deployed on consumer hardware. To make LLMs more accessible, a post-training quantization [16] can be applied to them. A quantized model is a compressed model with lessened demand for computing resources at the cost of reduced performance, aka quality of generated output. Depending on the number of parameters, quantized models can be deployed and run reasonably well on consumer devices.\nAs discussed earlier, quantization can result in a degraded performance. However, there is a distinct lack of studies exploring the effects of quantization on the performance of LLMs for code generation, or code LLMs for short. In this study, we evaluate how well LLMs quantized at different precision levels perform on Lua-based code generation benchmarks. More specifically, we infer how the quantization level affects the correctness of generated solutions, inference time, and types of errors produced.\nFinally, it may be possible that non-quantized models with a smaller number of parameters may perform better than quantized models with more parameters. To verify this assumption, we compare quantized models with 7 billion parameters with half-precision models with 3 billion and fewer parameters.\nOverall, this study aims to answer the following research questions:\n\u2022 RQ1: Which code LLMs with open source or permissible licenses can be feasibly run on consumer devices with the aid of quantization?\n\u2022 RQ2: How does quantization precision affect code LLMs concerning the quality of generated code, inference time, and types of error in the generated code?\n\u2022 RQ3: Which quantization precision provides a reasonable trade-off between performance degradation and decreased computational demand?\n\u2022 RQ4: How do quantized code LLMs perform compared to non-quantized code LLMs of similar model size?"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Code LLMs", "content": "HuggingFace offers a comprehensive repository\u00b9 of models that are free to use. The repository has become a go-to place for anyone interested in AI models including LLMs. As such, HuggingFace is an important resource toward the democratization of AI and LLMs particularly.\nMore relevant to this study is the Multilingual Code Models Evaluation leaderboard\u00b2 hosted on the HuggingFace ecosystem. The leaderboard maintains a comprehensive list of open-source and permissively licensed code LLMs ranked according to their performance in well-established coding benchmark datasets. Based on this leaderboard, this section reviews some of the better-performing models that are not derivatives of each other.\nThe well-performing code LLMs (relative to other code LLMs) include CodeLlama [17], CodeQwen 1.5 [18], DeepSeek Coder [19], CodeGemma [20], and StarCoder2 [21]. These code LLMs are trained on large amounts of coding data available on the Internet with popular websites, such as Stack Overflow and GitHub, being one of the primary sources.\nMany of these code LLMs are available with different parameters. For example, CodeLlama is offered with 7, 13, and 34 billion parameters. While the larger number of parameters increases the model's performance, it also increases computational demand. For example, CodeLlama 34B may require somewhere between 60-70GB of memory, while CodeLlama 7B may need about 13GB of memory.\nAll five models listed here are free to use and have either open-source or permissive licenses making them especially suitable for individual users. These are also multilingual code LLMs meaning that the models can generate code in more than one programming language. For example, StarCoder2 supports 17 programming languages while CodeQwen 1.5 claims to support 92 programming languages.\nThe programming tasks can be roughly divided into three categories [22]. The first is code generation given a natural language prompt. The second is code completion where a snippet of code is provided as a prompt and the model is expected to fill in the missing part. Finally, code understanding is the ability of a model to understand and explain a snippet of code. This study is only concerned with code generation, and the other two categories are ignored. The ranking on the Multilingual Code Models Evaluation leaderboard is also based on the code generation tasks."}, {"title": "2.2 Quantization", "content": "Quantization is a compression of a Large Language Model to reduce its size with the least impact on its performance [16]. There are several benefits of applying quantization to LLMs including reduced computational demands and greater accessibility of LLMs. There are two main methods of quantization, Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). While QAT methods apply quantization during the training process, PTQ methods are applied after the training phase. Because of the computational demand for QAT methods, PTQ methods are more prevalent [23]. This study is therefore concerned with PTQ only.\nPTQ methods can compress weights only or both weights and activations in LLMs. Training of LLMs usually happens at full precision, which uses a 32-bit floating point (FP32) to represent each weight and activation. After the training, LLMs can be deployed at half-precision (FP16) to improve their efficiency without a significant reduction in performance. In a quantized model, either weights or both weights and activations are represented with integers. For example, INT4/INT8 is a notation indicating that weights and activations were quantized to 4-bit and 8-bit integers respectively. Having integer bits also makes it more feasible to run a model on a CPU. However, quantizing activations can severely affect performance. Hence, it is common to quantize weights only and leave activations at full or half precision [16].\nZhu, Li, Liu, et al. [23] provides a comprehensive review of PTQ methods both weights-only and weight-activation combined. Among these methods, GPTQ [24] and AWQ [25] are of particular interest since they are the primary methods with which the models on HuggingFace were quantized\u00b3. Both of these methods apply weights-only quantization.\nSeveral studies evaluate quantized LLMs. Lee, Park, Kwon, et al. [26] evaluated quantized models on 13 benchmark datasets assessing among others common sense knowledge, algorithmic reasoning, language knowledge, instruction following, mathematical reasoning, and conversational skills. The quantized models included Vicuna, Gemma, Llama 2, and Llama 3.1 with parameters ranging from 2B to 405B. The models were quantized with GPTQ, AWQ, and SmoothQuant algorithms to 4- and 8-bit integers. Jin, Du, Huang, et al. [16] used 10 benchmark datasets on"}, {"title": "2.3 Evaluation benchmarks for code LLMs", "content": "HumanEval [28] and MBPP [29] are arguably the earliest benchmarks that were created for evaluating code LLMs. These are originally benchmarks for testing code generation capabilities specifically in Python language. HumanEval contains 164 hand-written programming tasks. In each task, a model is asked to generate a function that should pass several unit tests. On average, there are 7.7 unit tests per task. In the original study [28], HumanEval was used to evaluate GPT-3-based Codex models (OpenAI's model that powers Github Copilot) with 12 million to 12 billion parameters. Other applications of HumanEval include evaluating GitHub Copilot, Amazon CodeWhisperer, and ChatGPT [30].\nThe Mostly Basic Programming Problems (MBPP) benchmark consists of 974 code-generation tasks at the beginner level. Similar to HumanEval, each task in MBPP has a prompt asking to write a Python function. The generated function is also tested against the unit tests defined in the task. MBPP was originally tested on dense left-to-right decoder-only transformer language models with 244 million to 137 billion parameters [29]. Since then, it has become one of the main benchmarks for evaluating all major LLMs such as CodeLlama 7B-34B and StarCoder 15.5B [31]. Overall, HumanEval and MBPP are widely used benchmarks, and Jiang, Wang, Shen, et al. [32] provide a comprehensive review of both benchmarks and code LLMs.\nMultiPL-E [33] is a benchmark that combines HumanEval and MBPP. It is a multilingual benchmark that translated the original Python tasks into 18 other programming languages. These languages include Lua, Bash, C++, C#, D, Go, Java, JavaScript, Julia, Perl, PHP, R, Racket, Ruby, Rust, Scalia, Swift, and TypeScript. The benchmark was used to evaluate the InCoder 6.7B, CodeGen 16.1B, and Codex 12B models. A later study [14] applies MultiPL-E to evaluate fine-tuned versions of the StarCoderBase 1B, StarCoderBase 15B, and CodeLlama 34B models. Currently, MultiPL-E is arguably the most popular benchmark for testing multilingual code LLMs. Along with HumanEval, it is used to maintain the Multilingual Code Models Evaluation leaderboard.\nFinally, MCEVAL [22] is another multilingual benchmark that covers 40 languages including Lua. Unlike MultiPL-E, which translated Python-based HumanEval and MBPP to other programming languages, MCEVAL contains human-annotated tasks. MCEVAL contains three categories of programming tasks: code generation, code completion, and code understanding. The code generation tasks follow the same evaluation procedure as in MultiPL-E relying on function generation prompts accompanied with unit tests. All generation tasks are divided into easy, middle, and hard difficulty levels. The original study [22] evaluated 23 models with 7B to 72B parameters. These models include GPT-3.5-Turbo, GPT4-Turbo, GPT4-0, Qwen1.5-Chat, Llama3, Phi-3-medium, Yi, CodeQwen 1.5 Chat, DeepSeek Coder Instruct, DeepSeek Coder 1.5 Instruct, CodeLlama Instruct, OCTOCODER, CodeShell, MagiCoder, WizardCoder, Codegemma, Codestral v0.1, Nxcode, OpenCodeInterpreter, etc.\nSeveral studies used custom-developed benchmarks for specific domains. For example, Poldrack, Lu, and Begus [34] created a Python-based dataset with 32 data science coding problems. With this dataset, the authors tested ChatGPT and its underlying GPT-4 model. Liu, Le-Cong, Widyasari, et al. [35] compiled a dataset of 2033 programming tasks collected from LeetCode, an online platform for learning programming. There are versions of Python and Java for each task. The dataset was also used to evaluate ChatGPT.\nAs can be seen, past works focused on evaluating full- or half-precision models without quantization applied to them. Many of the models (models with 7B or more parameters) mentioned in this section are too computationally intensive to be feasibly deployed on generic consumer devices. Models of smaller sizes, such as StarCoderBase 1B, demonstrate only marginal performance compared to the other models. Therefore, it still remains a question whether quantized code LLMs with a larger number of parameters can be leveraged."}, {"title": "2.4 Evaluation metrics", "content": "The performance of a model in these benchmarks is usually evaluated with the pass@k metric [36; 28]. Because the next token prediction in LLMs is a stochastic process, an LLM can generate different outputs for the same prompt. For a code LLM, if at least one correct solution (a solution that passed all unit tests defined in the task) is found in the top k outputs then the LLM is considered to have passed the task. For example, with pass@10, a code LLM must produce a correct solution within its top 10 outputs. pass@1 is the strictest test where the code LLM is expected to produce a correct solution with its first attempt."}, {"title": "2.5 Low- and high-resource languages", "content": "Cassano, Gouwar, Nguyen, et al. [33] proposed a categorization of programming languages based on their frequency on GitHub and TIOBE rank. For example, C++, Java, JavaScript, and TypeScript are considered to be high-frequency languages. The other categories are medium, low, and niche. Lua, Racket, Julia, D, and Bash fall into the niche category, each having less than 1% representation on GitHub.\nThe low representation of these languages can result in a lack of training data for LLMs. For this reason, Cassano, Couwar, Lucchetti, et al. [14] categorized programming languages into high- and low-resource languages. The former include languages such as Java, Python, and JavaScript. The latter include Julia, R, Racket, and Lua.\nThis reflects the bias in existing code LLMs toward high-resource languages. Even if quantized models perform well on high-resource languages, this performance may not generalize to low-level languages. In fact, because of the very nature of optimization, quantization may negatively affect most code generation for low-resource languages. Therefore, it is more informative to evaluate quantized models against low-resource languages rather than high-resource languages. Among the niche low-resource languages, the non-quantized models demonstrated the best performance on Lua [33; 14; 22]. Therefore, Lua presents a good balance between being a low-resource language and being a benchmark."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Run-time environment", "content": "All models were run in a Python environment within a Windows 11 machine. Python 3.12.4 with Miniconda 24.4 was used. llama-cpp-python package was used to load and run the quantized models within the Python environment. llama-cpp-python provides a high-level Python interface to the llama.cpp library written in C/C++. llama.cpp was specifically designed for quantizing LLMs and working with quantized models in a GGUF format. Compared to the other solutions, such as the Transformers API from HuggingFace, llama-cpp-python is more efficient and has the least impact on performance when working with quantized models. The library also supports both GPU-based and CPU-only inferences.\nConcerning hardware, we used a consumer laptop Dell Latitude 5440 with Intel Core i5-1335U 1.30GHz with 12 CPU cores, 16GB DDR4 RAM, BG6 KIOXIA NVMe SSD, and no dedicated GPU. Therefore, all inference was done purely with the CPU. This device represents a generic work laptop that is typically used by different consumer segments such as businesses, academia, students, etc."}, {"title": "3.2 Choice of LLMs", "content": "This section addressed the research question RQ1. The code LLMs were chosen based on licensing, comparative performance, and computational demand that can meet the limitations of the hardware specified in the preceding section.\nThe following LLMs trained for code generation were selected for this study: DeepSeek Coder 6.7B Instruct [19], CodeQwen 1.5 7B Chat [18], CodeLlama 7B Instruct [17], StarCoder2 7b [21], and CodeGemma 7b [20]. As of August 14th, 2024, these models were ranked among the top in the Multilingual Code Models Evaluation leaderboard. This leaderboard ranks multilingual code-generation models based on their performance on HumanEval [28] and MultiPL-E [33] benchmarks.\nTo maximize the diversity of models, only original models were considered, and fine-tuned offshoots of these models were ignored. For example, Artigenz Coder DS 6.7B, while ranked high on the leaderboard, is a fine-tuned version of DeepSeek Coder 6.7B and was not included in this study."}, {"title": "3.3 Choice of benchmarks", "content": "Lua is a scripting language primarily for embedded use such as scripting in game engines and embedded devices [37]. Due to its niche application, Lua is considered a low-resource language [14] compared to other languages such as Python, Java, and JavaScript. Low-resource languages are defined by the relatively smaller size of training data available for code LLMs. As a result, code LLMs are not as good at generating code in low-resource languages as in high-resource languages [33; 14]. Therefore, quantized models are more likely to demonstrate performance degradation when generating code in low-resource languages such as Lua. Moreover, Lua employs unconventional programming paradigms, such as metatables, that are hard to directly generalize from other programming languages. Finally, using a low-resource language mitigates the effects of instruction fine-tuning that can be biased toward specific languages. For these reasons, Lua makes a good test case for quantized models and for answering the research question RQ2.\nBecause Lua is a low-resource language, most of the Lua-based benchmarks were derived from Python-based bench-marks listed in Table 2. MultiPL-E [33] is a multilingual benchmark that combines two Python-based benchmarks HumanEval [28] and MBPP [29] and translates them into 18 other programming languages including Lua. The Lua version of HumanEval and MBPP contain 161 and 397 programming tasks respectively. These programming tasks were used to test the quantized models.\nMCEVAL [22] is another multilingual benchmark that covers 40 languages including Lua. MCEVAL has human-generated tasks. This is in contrast to HumanEval and MBPP tasks, which were created for Python and may not fully reflect the uniqueness of the languages to which these tasks were translated. MCEVAL contains 50 code generation tasks for the Lua language. We used the 50 code generation tasks to test the quantized models.\nEach code generation task in the above benchmarks involves generating a standalone function given a prompt. An example of a prompt is shown in Listing 1. The prompt includes a brief description of what the function should do, example calls to the function, and the expected returns from these calls. These are presented as Lua comments. The"}, {"title": "3.4 Evaluation procedure", "content": "As stated in the research question RQ2, the code LLMs are evaluated against multiple metrics. Unit tests are used to facilitate the evaluation. Each code generation task includes several unit tests against which the model-generated function can be automatically tested. Listing 2 demonstrates the unit tests for the HumanEval_8_sum_product task from the HumanEval benchmark. Both HumanEval and MBPP benchmarks within MultiPL-E use the LuaUnit\u00ae third-party library for unit tests. On the other hand, MCEVAL uses the native assert function for unit testing. To streamline and standardize the automated evaluation procedure, we translated the native assertions in MCEVAL to LuaUnit-based assertions.\nTable 3 provides a summary of all metrics used to evaluate the performance of the quantized models. First, we measure pass@1, which is the model's capacity to generate a correct function in its first attempt. If the function generated by the model passed all unit tests then it is assumed that the model generated a correct solution. If at least one test failed then the model generated an incorrect solution.\nWith LuaUnit, it is also possible to automatically differentiate between failed unit tests, runtime errors, and syntax errors. Failed unit tests and runtime errors are delivered within the same stdout stream but with different error messages. Syntax errors are delivered within the stderr stream. Lastly, if a function call lasted more than 30 seconds, it was considered a timeout error. Differentiating between these four types of errors can give more insight into the challenges of code generation by LLMs and the effect of quantization."}, {"title": "3.5 Model parameters", "content": "The first parameter of relevance to all models is temperature. Temperature defines how predictable a model is. A lower temperature (t < 1) makes a model more deterministic in its next-token prediction. A higher temperature makes the model more unpredictable while inferring the next token in a sequence. For pass@1, a lower temperature is preferred [33]. The lower temperature also makes this study replicable. For these reasons, temperature is set at 0.1 for all models in this study.\nTop-k is the second parameter that makes a model more predictable. Top-k defines the greediness of the next token sampling by limiting to only the top-k tokens with the highest probability. In this study, top-k is set to 1 to enforce further the models' predictability and reproducibility of the study.\nThe last parameter is End-of-Sequence (EOS) tokens that tell the models when they should stop generating. By default, models can have different EOS tokens. For example, DeepSeek Coder uses \u2018<|EOT|>' as an EOS token, and CodeQwen uses '<lim_end|>'. However, MultiPL-E introduces its own EOS tokens that are '\\nlocal', '\\nfunction', '\\n--', and '\\n\\n'. We noticed that these additional EOS tokens make model outputs more succinct. Hence, we use these tokens in addition to the models' default EOS tokens."}, {"title": "3.6 Source code and data", "content": "The source code and data used in this study are available at https://github.com/E-Nyamsuren/ qLMM-Lua-Eval-Pipeline. The source code includes Python pipelines for downloading and preparing the datasets, generating Lua code with the models, evaluating generated Lua code, and an R script for statistical analysis."}, {"title": "4 Evaluation", "content": "A total of 10944 tasks in Lua programming language were solved by the five code LLMs at 2-bit, 4-bit, and 8-bit quantization levels. The total inference time was approximately 87 hours."}, {"title": "4.1 Pass@1 rates", "content": "Fig. 1 depicts pass@1 rates by (a) models and quantization and by (b) benchmarks and quantization. The exact values and corresponding means are listed in Table 4. For example, CodeLlama 7B Instruct with 2-bit integer quantization produced correct solutions for 31.6% of tasks across all benchmarks. In the HumanEval benchmark, the correct solutions constitute 34.9% of all solutions produced by the five models with 4-bit integer quantization. Here, a correct solution is a pass@1 solution that passed all unit tests. With the exception of 8-bit CodeQwen, the models demonstrate pass@1 rates below 50%. It is in line with the expectation for a low-resource language like Lua.\nFig. la suggests a negative effect of quantization on models' performance. The drop in performance is more evident in 2-bit models. A special case is CodeGemma, which failed to solve any task at the 2-bit quantization. The biggest performance gains are from the 2-bit model to the 4-bit models. However, compared to the 4-bit models, the 8-bit models perform only marginally better. The performance of DeepSeek Coder, StarCoder, and CodeLlama does not change much between 4-bit and 8-bit quantization. CodeQwen benefited the most from the 8-bit quantization by increasing its pass@1 rate by about 7%. According to Fig. 1b, in all three benchmarks, the 4-bit models perform better than the 2-bit models. This effect also holds after considering the skewed values for the 2-bit models due to CodeGemma. The performance gains level up with the 8-bit models.\nFig. 1 also reveals that CodeQwen is consistently among the top-performing models, while CodeLlama is a poorer-performing one. True to its name, MBPP is the easiest benchmark among the three. HumanEval is slightly more difficult than MCEVAL. However, Fig. 2 suggests that MCEVAL may contain a greater variety of tasks highlighting the differences between the models. The pass@1 rates for the 2-bit and 4-bit models are closer to each other in HumanEval and MBPP. But for MCEVAL, the pass@1 rates remain more spread out independently of quantization.\nTo statistically verify the descriptive analysis, a logistic regression was applied on pass@1. The predictors were the three benchmarks, five models, and three quantization levels. All were nominal predictors. The predictors were verified for multicollinearity with adjusted generalized VIF (GVIF). No GVIF value exceeded 2 suggesting no or low multicollinearity. Table 5 lists the result of the regression. The intercept represents the log odds of a correct solution by 2-bit CodeGemma in the HumanEval benchmark. According to the regression, the log-odds increase for 4-bit and 8-bit quantization but the increase from 4-bit to 8-bit (0.094) is not as high as from 2-bit to 4-bit (0.668). The log odds are also significantly higher for MBPP compared to HumanEval but do not change significantly for MCEVAL. Lastly, the log odds suggest that CodeQwen and DeepSeek Coder are the overall top-performing models. CodeLlama is shown to be better than CodeGemma but it is likely due to skewed data from the 2-bit models.\nThe logistic regression model should be interpreted with certain care. McFadden's Pseudo R2 for the regression model is 0.035 meaning that the model fails to explain a major part of variance. Also, Fig. 1 and 2 suggest at least two interaction effects: between quantization levels and models and between benchmarks and models. We also tested a logistic model that includes all combinations of two-way interactions between the predictors. This more complex model had only a marginal increase in McFadden's pseudo R\u00b2 (0.065) and a slightly lower AIC of 11275.06 compared to the AIC of 11602.03 of the first model. Furthermore, only one interaction effect was significant in the complex model and none of the main effects was significant. This result and the overall low pseudo R\u00b2 may be explained by a large variance in task difficulty that may be present in the benchmarks. This variance cannot be properly captured with the current predictors. Nevertheless, the model in Table 5 is sufficient for exploring the effects of quantization."}, {"title": "4.2 Errors", "content": "As discussed in Section 3, the failed solutions were categorized into failed unit tests, runtime errors, syntax errors, and timeouts. Fig. 3 depicts the proportions of these categories among all solutions generated by all models in the three benchmarks and at the three quantization levels.\nThe failed tests constitute the majority of errors in all three quantization levels followed by the syntax errors. It should be noted that the data for 2-bit quantization is skewed because 2-bit CodeGemma was not able to solve any task due to persistent hallucinations. Listing 3 shows example hallucinations from three tasks that demonstrate a complete breakdown of any ability to produce a coherent response.\nFor a more accurate view, Table 6 breaks down Fig. 3 by the models. There is a general trend of increasing syntax errors with lower-precision quantization (lower qbits). CodeLlama seems to be less susceptible to this, but it also demonstrated poorer overall performance among the five models. DeepSeek Coder shows a decreased rate of failed tests for its 2-bit model. It is likely explained by the induction of syntax errors rather than better performance. CodeQwen is among the better-performing models. However, it seems to be more prone to syntax errors when it fails to generate a correct solution. Overall, quantization seems to affect the models' ability to produce syntax error-free code resulting in lower performance for the lower-precision models."}, {"title": "4.3 Inference time", "content": "Fig. 4 depicts the inference times broken down by the models and benchmarks. The figure also shows the inference times separately for the correct (pass@1) and incorrect (fail@1) solutions.\nFor all models", "CodeQwen": "the rate of increase"}]}