{"title": "Revisiting SLO and Goodput Metrics in LLM Serving", "authors": ["Zhibin Wang", "Shipeng Li", "Yuhang Zhou", "Xue Li", "Rong Gu", "Nguyen Cam-Tu", "Chen Tian", "Sheng Zhong"], "abstract": "Large language models (LLMs) have achieved remarkable performance and are widely deployed in various applications, while the serving of LLM inference has raised concerns about user experience and serving throughput. Accordingly, service level objectives (SLOs) and goodput the number of requests that meet SLOs per second-are introduced to evaluate the performance of LLM serving. However, existing metrics fail to capture the nature of user experience. We observe two ridiculous phenomena in existing metrics: 1) delaying token delivery can smooth the tail time between tokens (tail TBT) of a request and 2) dropping the request that fails to meet the SLOs midway can improve goodput. In this paper, we revisit SLO and goodput metrics in LLM serving and propose a unified metric framework smooth goodput including SLOs and goodput to reflect the nature of user experience in LLM serving. The framework can adapt to specific goals of different tasks by setting parameters. We re-evaluate the performance of different LLM serving systems under multiple workloads based on this unified framework and provide possible directions for future optimization of existing strategies. We hope that this framework can provide a unified standard for evaluating LLM serving and foster researches in the field of LLM serving optimization to move in a cohesive direction.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved remarkable performance in many tasks and are widely deployed in various applications, such as chatbots (OpenAI, 2024; Zheng et al., 2024; Montagna et al., 2023) and virtual assistants (Vu et al., 2024; Dong et al., 2023). When service providers(SPs) offer LLM services to users, they face two major challenges. First, as model size and sequence length escalate, SPs must allocate significant computational resources, dramatically increasing the cost of service. Second, to ensure a satisfactory user experience, SPs must consider the constraints from the user. Therefore, many works are dedicated to improving the resource utilization efficiency of LLM serving systems (Yu et al., 2022; Kwon et al., 2023; Agrawal et al., 2024a), while others further consider ensuring meeting Service Level Objectives (SLOs) to improve users experience (Patel et al., 2023; Agrawal et al., 2024b; Zhong et al., 2024; Cheng et al., 2024; Patke et al., 2024). Different optimization objectives lead to different optimization dimensions in these works.\nSpecially, to evaluate user experience in LLM serving systems, many indicators have been used in previous work, such as time to first token (TTFT), and time between tokens (TBT). To further evaluate the performance of LLM serving systems ensuring the SLOs, metrics such as SLO attainment, goodput and capacity are proposed. However, we observe that these metrics fail to capture the nature of user experience. Moreover, the solutions based on these metrics may lead in the wrong direction.\nTBT is too tight for overall user experience while TPOT is too loose. TBT measures the time interval between each token within a request, while TPOT only reflects the average interval between them. Considering the following example, a serving system delivers 10 tokens to users quickly, then encounters a 1-second stall, and continues to deliver tokens. The tail TBT of this request is as long as 1 second, while users still have a good experience as they have enough tokens to read during the stall. Conversely, if users do not have enough information to process, such pauses can be a loss for user experience. For example, if the system delivers only 2 tokens before the 1-second stall, users will have to wait with nothing to read. In other words, the cost of iterations with high latency is shared with previous iterations.\nRecent work Sarathi-Serve (Agrawal et al.,"}, {"title": "2 Background and Related Works", "content": "In this section, we revisit the background of LLM serving, including the autoregressive inference, mainstream LLM serving systems, and metrics to evaluate their serving quality. Based on these metrics, many scheduling strategies have been proposed.\n2.1 Streaming LLM Serving\nPre-trained LLMs are widely deployed in production environments for different downstream tasks, including chatbot, code generation, text summarization, and so on. LLMs process autoregressive inference to generate output tokens based on input prompts."}, {"title": "2.2 LLM Serving Metrics", "content": "The metrics used to evaluate the performance of LLM serving can be divided into two main groups: SLOs that represent user experience and indicators that assess performance under SLO constraints.\nAs the protocol between the service provider and the user, SLOs have been widely used in LLM serving systems to support better user experience (Patel et al., 2023; Agrawal et al., 2024b; Zhong et al., 2024; Stojkovic et al., 2024; Cheng et al., 2024). Therefore, SLOs constrain the service to not only focus on throughput but also to provide satisfactory service quality to users. As shown in Figure 3, the mainstream SLOs in LLM serving systems are discussed as follows:\n\u2022 E2E (End-to-End) Latency and TPOT (Time-per-Output-Token): E2E latency and TPOT are base metrics in LLM serving. E2E latency reflects the time taken for a request (or a batch of requests) from commited by users to when it completed and delivered to users. It directly reflects the total time users consume to receive the output. Considering the output lengths of different requests are different, TPOT is proposed to reflect the average time taking to generate a token (sometimes excluding the first token). These two metrics can be converted to each other.\n\u2022 TTFT (Time-to-First-Token) and TBT (Time-between-Tokens): TTFT and TBT further delve into each token in LLM serving. TTFT represents the time taken for the generation of the first output token. While TBT represents the fine-grained time interval between two adjacent tokens of a request.\nBased on these SLOs, some high-level metrics have been proposed to measure the performance of the system in providing services:\n\u2022 SLO attainment: SLO attainment is used to describe the proportion of requests that meet the SLOs. Low SLO attainment indicates that the current system configuration is unable to handle the workload, and additional resources or configuration optimizations are needed. Capacity is defined as the maximum request rate under the constraint of certain SLO attainment.\n\u2022 Goodput: Goodput is defined as the number of completed requests that meet the SLOs per second. It considers the trade-off between the resource utilization and user experience. Low request rate leads to low resource utilization and reduces the goodput. Conversely, high request rate results in more requests exceeding the SLOs, which also reduces goodput.\nIn practical deployments, the sheer volume of requests poses significant challenges to both resource utilization and user experience. Consequently, numerous efforts are directed towards optimizing scheduling strategy to achieve superior service quality. Based on their evaluation metrics,"}, {"title": "3 Revisiting the SLOs", "content": "We revisit the design of SLOs of recent works on LLM serving and demonstrate that existing SLOs are irrational, and propose a new token-level SLO that is more aligned with user experience, focusing on the relationship between the information processing of the user and the delivery of information by the service.\n3.1 A Framework of SLOs\nTo align various SLOs, we introduce a unified framework of SLOs that can be customized to represent the various requirements proposed in different workloads. We notice that the token-level SLOs of existing works can be viewed as setting the deadline for the generation time of each token, whereas request-level SLOs only care about the generation time of the last token.\nDefinition. We define the deadline of the i-th output token of a request as $d_i$, while $t_i$ is the actual generation time of the i-th output token. Therefore, the SLO constraints can be formulated as:\n$\\forall i, t_i \\leq d_i.$\nCustomization of existing SLOs. The framework can be customized to represent the various requirements proposed in different works by adjusting the deadline of each token. The details customization of existing SLOs are following:\n\u2022 TTFT and TBT.\n$d_i = \\begin{cases} TTFT, & i = 1, \\\\\nt_{i-1}+TBT, & i>1. \\end{cases}$\nNotice that the deadline of the i-th token is determined by the generation time of the previous token, which, as we will show, is not aligned with user experience.\n\u2022 End-to-end latency.\n$d_i = E2E,$"}, {"title": "3.2 Optimization on Exsiting SLOs", "content": "Consider the situation shown in Figure 4, there are two requests A and B from users, where request A is in the decode phase and request B is just scheduled to the prefill phase. Due to the prefill-prioritizing principle for improving throughput in VLLM, the decode phase of the following tokens for request A will be stalled until the prefill phase of request B is finished, which results in a generation stall, i.e., a large TBT between $A_B$ and $A_D$. Therefore, Sarathi-Serve splits the prefill phase of B into multiple chunks ($B_1^P, B_2^P, B_5^P$) and fuses them with the decode phases of request A in the same batch. Specifically, one prefill chunk of request B will attach decoding one token of request A, like $A_B^P B_1^P$, $A_D^P B_2^P$ and $A_{EOS}^P B_5^P$. Assuming the prefill stage of B is split into $n_c$ chunks, the stall time of A is approximately reduced to about $\\frac{1}{n_c}$ of the original. By this way, the stall time is smoothed, resulting in a smaller TBT. However, we observe that the abslute latency from decode tokens of request B ($B_D^D, B_D^D....$) will not benefit from the optimization. Further, our concern arises that this slicing approach, by introducing frequent assessments of the KV cache, may inadvertently lead to an increase in overall latency rather than a decrease.\nTo summarize, the chunked-prefills smooths the TBT by slicing the prefill phase and fusing them with the decode phases of other requests. This provides an insight that instead of slicing, can we manually schedule the prefill and decode phases and achieve better performance?"}, {"title": "3.3 A Naive Imitation of Sarathi-Serve", "content": "We propose a naive imitation strategy, called decode prepone, which can achieve a comparable effect to chunked prefills on TBT by simply scheduling without slicing. As shown in Figure 4, specifically, the next n decode tokens for request A ($A_B^P$ and $A_D^P$) are preponed to be generated before the prefill of request B starts. Meanwhile, instead of directly outputting these tokens of request A, which can result in large TBT between n-th token ($A_D^P$) to n + 1-th token ($A_{EOS}^P$), we smoothly output these"}, {"title": "3.4 A New Token-level SLO Defination", "content": "Before delving into the details of the new token-level SLO, we first introduce a output delay trick that can be used to imporve the SLO attainment on TTFT and TBT to highlight the issue of exsiting metrics.\nOutput delay trick. Output delay is a simple tactic, similar to achieving smooth output as mentioned above, where tokens are released until the TBT deadline is reached, rather than immediately upon generation. Implementing output delay can be facilitated by adding an intermediate buffer layer between the inference engine and the client. This approach allows for looser constraints on the delivery of subsequent tokens.\nHowever, in practical scenarios, such a technique is generally considered inadvisable due to the inherent latency in delivering tokens to users, which is contrary to intuitive expectations. The effectiveness of this technique stems from the inherent unreasonableness of TBT as an SLO requirement. Specifically, premature delivery of current tokens inadvertently imposes stricter delivery time constraints on subsequent tokens. Thus, there is an urgent need to devise a novel SLO that not only protects the user experience but also refrains from penalizing the early delivery of tokens.\nIntuition. In fact, users do not frequently notice the lag of the last word during the generation process. We argue that generation stalls are not necessarily harmful to user experience, as long as the delivery of tokens is aligned with the user's reading speed. Given the limitations of TBT in setting the time interval between adjacent tokens, we shift the focus of the SLO to the actual user experience. For instance, we can set the constraint of each request according to the response delay that users can tolerate and the speed of processing output information, such as reading the output of the chatbot, understanding the summary of long text, listening, etc.\nDefinition: Porting the new token-level SLO to the framework, we have\n$d_i = V \\times i,$\nwhere V is the output information processing speed of the user, and i is the index of the output words. $d_i$ constraints the deadline of the i-th token, after which the user will perceive a pause in the output stream."}, {"title": "4 Revisiting the Goodput", "content": "Note that SLOs are only concerned with the user experience at request level. However, in the system view, the service provider is more concerned about the overall performance of the service. Specifically, the throughput of the service is a key metric, directly related to the capacity and efficiency of the service. Combining SLOs and throughput, the goodput is a metric that can reflect the throughput of the service that successfully meets the SLOs.\n4.1 Existing Strategy\nA common practice is the most urgent request-first strategy, based on the intuition that the request nearest to its deadline is the most important and should be processed first.\nIn addition to this greedy strategy, goodput-based scheduling is also a dominate strategy. Reviewing the definition of goodput as equation 5:\n$\\text{Goodput} = \\frac{\\sum_{r \\in R} 1(\\forall i, t_i \\leq d_i) \\cdot n_r}{T},$\nwhere R is the set of requests, $1(\\cdot)$ is the indicator function, T is the time interval of serving the requests in R, and $n_r$ is the number of tokens that the request r generates. We observe that if a request does not meet the SLOs, its goodput is assigned a value of 0. This approach, when optimizing for goodput, often leads to the abandonment of requests that cannot meet the SLOs. In LLM serving, however, this is an unacceptable outcome for users. While latency undoubtedly degrades the user experience, abandoning a request altogether poses an even greater threat.\n4.2 Smooth Goodput\nGiven the shortcomings of the existing goodput metric, a new metric must comprehensively consider the contribution of each request, even if they slightly exceed the SLO requirements. In such cases, users have to wait for the subsequent token to be generated, after they have finished reading all the previously delivered tokens.\nStreaming service and user experience. Unlike models with a single forward inference process, interactive LLM applications are typically deployed as streaming services due to the autoregressive nature of LLMs. Research (Egger et al., 2012) on web based streaming services has shown that the waiting time of users is a key factor affecting user experience.\nTherefore, we introduce the concept of user wait time, namely user idle latency, to measure the user experience. The user idle latency is cumulative duration during which a user is idle and waiting for new tokens to be generated due to the lower generation speed. Formally, the user idle latency l of a request r is defined as:\n$l_r = \\sum_{i=1}^{n} \\max(t_i - d_i),$\nwhere $t_i$ is the time when the i-th token is generated, $d_i$ is the deadline time of the i-th token delivered to the user, and n is the number of output tokens in the request r.\nDefinition: The smooth goodput is defined as the service benefit per unit of time. The benefit of a request is defined by two factors: the number of tokens that the request generates and the user idle latency of the request. Formally, we have:\n$\\text{benefit}(r) = n_r \u2013 \\alpha \\cdot f(l_r),$\nwhere $n_r$ is the number of tokens that the request r generates, f(\u00b7) is a function that maps the user idle latency to the percentage of the benefit that the request can generate, and \u03b1 is a weight. For intereacltive applications with tight latency requirements, \u03b1 should be set larger to ensure that the idle latency is strictly controlled. The smooth goodput is defined as:\n$\\text{smooth goodput} = \\frac{\\sum_{r \\in R} \\text{benefit}(r)}{T},$\nwhere T is the time interval of serving the requests committed by the users denoted by R.\nWe notice that Andes (Liu et al., 2024) also considers the benefit of the requests that miss the SLOs. However, they consider the average slowdown to the deadline in SLOs, while we consider the largest slowdown, i.e., the user idle latency. In practice, if the slowdown has happened, catching up in the future can not help to improve the user experience as the user has already waited for the token to be generated. Therefore, the slowdown can not be averaged and our smooth goodput is more reasonable in this case."}, {"title": "5 Evaluation", "content": "In this section, we re-evaluate different scheduling strategies under the unified metric framework we propose. Then we analyze the results and summarize the challenges of LLM servings. By comparing with the existing metrics, we demonstrate the advantages of smooth goodput.\n5.1 Experiment Setup\nLLM serving framework. We conduct experiments using vLLM v0.5.3.post1 (Kwon et al., 2023), the state-of-the-art LLM serving framework. It is fully open-source, with a community that rapidly updates new features and maintains the project, and is widely deployed in the industry. vLLM framework implements a throughput-optimal scheduling engine and optional chunked prefills to optimize tail TBT, which is consistent with the metrics we focus on and provides a fair and unified implementation. We use the default configuration of vLLM in our experiments.\nModel, environment and workload. We evaluate the performance of LLM serving on a single NVIDIA A100 GPU 40GB, using Qwen2-7B (Yang et al., 2024). For workload, we use Sharegpt_gpt4 as the simulation of the conversations with chatbots. Besides, we construct longer conversations by concatenating multiple Sharegpt_gpt4 item to the average length of 1600"}, {"title": "5.2 Analyze with Existing Metrics and Smooth Goodput at System Level", "content": "We first analyze the performance of different strategies using the existing metrics, showing the statistical regularity of vLLM with different request rate and analyzing the reasons for these regularities. Then, under the same scheduling strategy, we introduce smooth goodput for analysis, demonstrating the new features of that the existing metrics fail to capture.\nWe evaluate the performance of vLLM under different request rates, as shown in Figure 5a. We choose the Sharegpt_gpt4 dataset with relative short prompts and responses for the experiment, and the maximum request rate is 9 (far exceeding the capacity of the system). These metrics capture the performance of vLLM in different aspects. In the unsaturated stage, as the request rate increases, the computing resources are more fully utilized, and the throughput increases. With more requests in the running queue, more requests are accommodated in a batch, so the time of each batch also increases, leading to the improvement of TBT and TPOT. After reaching the system capacity, batches cannot accommodate more requests at the same time due to memory limitations and/or maximum batch size, so increasing the request rate cannot improve throughput, but will cause more requests in the waiting queue, significantly increasing TTFT. Since at the beginning of the request commited, no output information can be delivered to users, users have to wait, leading to long user idle time and seriously affect the user experience as a result.\nThen we analyze the performance of vLLM under the same experimental settings using smooth goodput. We set the information consumption speed to 10 tokens per second and \u03b1 = 5. As shown in Figure 5b, the performance of smooth goodput under different request rates is different from that of the existing metrics. In the unsaturated stage, smooth goodput increases with the request rate. At this time, the benefit from the throughput increase is greater, and increasing the request rate is beneficial to reduce the cost of the service provider. As the number of requests increases, the benefit slows down. And the user experience rapidly decreases due to long-time waiting, leading to a decrease in smooth goodput."}, {"title": "5.3 Analyze with SLOs at Request Level", "content": "We conduct experiments to demonstrate that our new SLOs can measure the benefit of each request. We verify this with prompts of 1600 tokens long in average. From the service logs of the two engines, we choose the same request under the same trace for comparison. As shown in Figure 6 and Figure 7, it describes the token generation process of the request with and without the chunked-prefills technology. The chunked prefills implemented in VLLM significantly reduces the number of generation stalls, providing a smoother token generation process. However, analysis of the data reveals that many token generation stalls caused by prefill pre-emption go unnoticed by users because some tokens have already been delivered to them. At this point, users are busy processing the information and may not even notice the generation stall, provided that a sufficient amount of tokens has already been delivered.\nVerify delay output trick. We simply verify the effectiveness of the output delay trick, and then prove our argument on SLOs again. As shown in Figure 8a, we implement the output delay trick by buffering the tokens and outputting them at a relatively slower rate. This trick is independent of the scheduling strategy of any framework and can be implemented on the both server and client side. Compared to no delay, the output delay trick can effectively reduce the tail TBT without affecting the throughput of the service as shown in Figure 8b. It delays the delivery of most tokens to the user but achieves better performance in existing metrics. This smooths the TBT to nearly a constant value (the information consumption rate of users), but does not reduce user idle time at all. This indicates that the total time users spend waiting has not been improved, and therefore users may still complain about the service. This is also the reason why we believe that the existing metrics cannot measure user experience well."}, {"title": "6 Conclusion", "content": "In this paper, we propose a metric framework to evaluate the performance of LLM serving. We demonstrate that the existing metrics fail to capture the nature of user experience and demonstrate the correlation between user experience and output delivery speed in streaming LLM serving. We propose smooth goodput to measure the service benefit per unit of time, which takes into account both the efficiency of services and user experience. Based on this unified framework, we re-evaluate the performance under multiple workloads, demonstrating its capability in analyzing service performance. We hope this framework can provide a unified standard for evaluating the performance of LLM serving and foster researches in the field of LLM serving optimization to move in a cohesive direction."}]}