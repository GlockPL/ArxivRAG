{"title": "Fast Calibrated Explanations: Efficient and Uncertainty-Aware Explanations for Machine Learning Models", "authors": ["Tuwe L\u00f6fstr\u00f6m", "Fatima Rabia Yapicioglu", "Alessandra Stramiglio", "Helena L\u00f6fstr\u00f6m", "Fabio Vitali"], "abstract": "This paper introduces Fast Calibrated Explanations, a method designed for generating rapid, uncertainty-aware explanations for machine learning models. By incorporating perturbation techniques from ConformaSight\u2014a global explanation framework-into the core elements of Calibrated Explanations (CE), we achieve significant speedups. These core elements include local feature importance with calibrated predictions, both of which retain uncertainty quantification. While the new method sacrifices a small degree of detail, it excels in computational efficiency, making it ideal for high-stakes, real-time applications. Fast Calibrated Explanations are applicable to probabilistic ex- planations in classification and thresholded regression tasks, where they provide the likelihood of a target being above or below a user-defined threshold. This approach maintains the versatility of CE for both classification and probabilistic regression, mak- ing it suitable for a range of predictive tasks where uncertainty quantification is crucial.", "sections": [{"title": "1. Introduction", "content": "Artificial Intelligence (AI) is becoming an integral part of modern society, influenc- ing everything from retail recommendations to medical diagnosis predictions [1, 2] and even defence strategies [3]. In many predictive tasks, AI systems are typically devel- oped as models trained via Machine Learning (ML) algorithms. While these models often achieve remarkable accuracy, such as outperforming medical professionals in cancer detection [4], they are neither flawless nor purely objective. Their performance is inherently tied to the data and algorithms used for training, making the outcomes sensitive to these factors.\nAs AI becomes more ubiquitous, the need for explainability in AI models has grown. In high-stakes applications, such as healthcare or autonomous driving, it is crucial that AI systems provide not only accurate predictions but also transparent rea- soning behind those predictions. Explainable AI (XAI) aims to make AI's decision- making processes more understandable to humans, which enhances trust and facilitates informed decision-making by users. Furthermore, explainable models allow develop- ers and stakeholders to identify potential weaknesses, limitations, or unintended conse- quences of the system. By providing insights into how the AI arrives at its conclusions, XAI helps bridge the gap between complex model operations and human interpretabil- ity, ultimately fostering more reliable and accountable AI systems across various do- mains [5].\nWhen making decisions based on Machine Learning (ML) models, it is essential to account for the inherent uncertainty in their predictions [6, 7]. While many mod- els provide point estimates, these alone fail to capture the degree of confidence in a given prediction, which can be crucial, particularly in safety-critical applications. Un- derstanding uncertainty helps quantify the reliability of the model's output and aids in making more informed decisions.\nUncertainty in ML models can generally be categorized into two types: aleatoric and epistemic uncertainty [8]. Aleatoric uncertainty, also known as statistical or irre- ducible uncertainty, arises from inherent noise in the data. It represents variability in outcomes due to factors that cannot be explained by the model, such as measurement"}, {"title": "", "content": "errors or inherent randomness in the data-generating process. This type of uncertainty cannot be reduced by gathering more data because it is intrinsic to the task.\nIn contrast, epistemic uncertainty reflects the model's lack of knowledge, typically caused by limited or insufficient data. It is also known as reducible uncertainty be- cause it can be decreased by acquiring more data or improving the model's capacity. Epistemic uncertainty is particularly important in situations where the model is making predictions on out-of-distribution or novel examples-scenarios where the model may be more prone to errors.\nIn light of these uncertainties, methods like Conformal Prediction (CP) [9] offer a principled framework for uncertainty quantification. CP is a distribution-free, model- agnostic approach that provides reliable confidence intervals for predictions. By not assuming any particular distribution for the data, CP can generate prediction intervals that account for both aleatoric and epistemic uncertainties, making it highly adaptable across different ML tasks and models.\nMoreover, for classification tasks or probabilistic estimates, Venn Predictors, an ex- tension of conformal prediction, are particularly useful. Venn Predictors provide a way to output multiple probability estimates. Unlike typical methods that output a single probability estimate (e.g., from softmax layers in neural networks), Venn Predictors yield a set of probabilities that, based on the current available data, ensures one of the probabilities will be correct. This added layer of uncertainty quantification offers more robust probabilistic estimates, which can be crucial in risk-sensitive applications like medical diagnostics or autonomous systems.\nThe strength of conformal methods, including Venn predictors, lies in their abil- ity to complement standard ML outputs by offering uncertainty estimates that are both mathematically rigorous and practically useful. This enhances the interpretability of AI systems and provides decision-makers with a clearer understanding of the model's confidence, making these methods preferable for applications where understanding un- certainty is as important as the prediction itself.\nA natural development has been to integrate conformal methods into explanation techniques, aiming to provide not only accurate predictions but also transparent and trustworthy explanations. Calibrated Explanations [10] exemplify this approach by"}, {"title": "", "content": "embedding uncertainty directly into the explanation process through calibration. By aligning the confidence of a model's prediction with the reliability of its explanation, this method are offering users a calibrated view of when the model's reasoning can be trusted. Calibrated Explanations provide local explanations with uncertainty quan- tification of both prediction and feature importances for individual instances and is applicable to both classification and regression.\nSimilarly, ConformaSight [11] builds on conformal prediction to provide a global explanation framework. It ensures robust explanation sets with guaranteed coverage, making it applicable across different models and robust even in the presence of noisy or perturbed data. This allows for consistent, high-confidence explanations, even in unpredictable environments.\nThis paper aims to combine the strengths of local explanations from Calibrated Explanations \u2013 which maintain applicability across classification and thresholded re- gression scenarios with the perturbation approach employed in ConformaSight. Specifically, we preserve the flexibility of explaining predictions for both classifica- tion and thresholded regression outputs (e.g., explaining the probability of exceeding a threshold). This is enhanced by incorporating ConformaSight's perturbation approach, enabling the generation of computationally efficient factual explanations providing fea- ture importance.\nThe resulting approach offers extremely fast explanations that can be highly advan- tageous in real-time decision-making processes, particularly where the outputs of these explanations are fed into subsequent steps or systems. Performance is essential for scenarios like machine teaching [12, 13], where explanation algorithms are required to function in real-time, ideally on resource-constrained platforms such as mobile devices. In scenarios requiring critical decisions within a short time frame, such as emergency response or automated monitoring, the ability to generate explanations quickly ensures that the system can continue operating seamlessly. Essentially, these fast explanations become an integral part of a continuous decision-making loop, where real-time in- sights flow directly into other processes for immediate action. Additionally, this could especially be crucial in time-series or sequential data tasks, where the significance of features may shift dynamically across different time points. In such cases, generating"}, {"title": "", "content": "a single static explanation for an entire series would be misleading, as the importance of each feature evolves over time. Hence, fast calibrated explanations can adapt to the changing context at each timestamp to ensure more accurate insights, enhancing the reliability of real-time predictive models.\nIn addition to delivering rapid explanations with feature importance, it also quanti- fies uncertainty for both the overall prediction and the individual contribution of each feature. This allows decision-makers to interpret not only the model's output but also the reliability of each contributing factor, providing deeper insights for risk-sensitive applications where understanding both prediction and explanation uncertainty is criti- cal.\nIn the next section, thorough descriptions of the building blocks for this paper are provided. In Section 3, our contribution is described. The experimental setup is de- scribed in Section 4 whereas the results provide both evaluation results and a demon- stration of its applicability. Section 6 wraps up the paper with a concluding discussion and pointers for future work."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Post-Hoc Explanation Methods", "content": "In machine learning (ML), there are two main approaches to generating explana- tions. One method involves using inherently interpretable and transparent models, also known as Interpretable AI, which describes the internals of a system in a way that is understandable to humans [14]. Alternatively, post-hoc techniques can be used to ex- plain complex models, making them suitable for explaining black-box models as well [15].\nPost-hoc explanations involve creating simpler models that clarify how a complex model's predictions are linked to input features. These explanations can be either local, focusing on a single instance, or global, providing insights into the overall model. They often incorporate visual aids like feature importance plots, pixel representations, or word clouds to emphasize the key features, pixels, or words influencing the model's predictions. The importance of making black-box models explainable is highlighted"}, {"title": "", "content": "by [16], which discusses various interpretability methods for machine learning models.\nMoradi et al. [17] propose the Confident Itemsets Explanation (CIE) method, which uses highly correlated feature values to explain black-box classifiers by discretizing the decision space into smaller subspaces. Another interesting approach is introduced by [18], which focuses on the quality of explanations in AI systems, particularly in the medical field, by introducing the concept of causability in addition to explainability.\nResearch on proposed post-hoc methods is diverse and frequently tailored to spe- cific tasks. For instance, [19] provides a visual explanation of black-box models by lo- calizing the region responsible for a prediction. This approach is tested on a classifica- tion task by localizing entire object classes within an image. Another popular approach is the use of counterfactual explanations, which, similar to perturbation methods, vary the input space to understand how it affects the output. This approach, employed in a statistical fashion, is used by [20] to produce human-friendly interpretations on classi- fication tasks.\nPost-hoc explanations for classification and regression have some distinguishing characteristics due to the nature of the insights they offer. In classification, explana- tions involve predicting the class an instance belongs to from predefined classes, with probability estimates reflecting the model's confidence for each class. Techniques like SHAP [21], LIME [22], and Anchor [23] explore factors contributing to class assign- ment, often using feature importance, such as words in textual data or pixels in images. However, both SHAP and LIME can also be used to generate explanations for regres- sion models. In regression, the focus is on predicting numerical values associated with instances without predefined classes. Explanations for regression models normally adapt techniques designed for classifiers by attributing features to predicted outputs.\nLocal explanations in classification often rely on probability estimates, which most machine learning models can generate to indicate the likelihood of each class. These probability estimates are commonly interpreted as a measure of prediction confidence. For example, in a binary classification scenario, a model predicting an instance belong- ing to the positive class with a probability estimate of 0.8 is considered more confident than one predicting the same instance with a probability estimate of 0.65. Probability estimates form the basis for local explanations in classification tasks, where the con-"}, {"title": "", "content": "fidence level indicates the likelihood of the positive class being the true class for that instance."}, {"title": "2.2. Calibration and Uncertainty Quantification", "content": "Basing decisions on accurate information is crucial in decision-making, placing an additional layer of requirements on predictive models to provide well-calibrated predictions and guarantees.\nConformal Prediction (CP) [9] is a distribution-free framework that offers pre- diction regions with guaranteed coverage, whose value and effectiveness have been demonstrated in numerous studies [24] [25]. Errors occur when the true target falls outside the predicted region. However, conformal predictors maintain automatic valid- ity under exchangeability, resulting in an error rate of e over time. Conformal regres- sion (CR) provides prediction intervals with user-decided guaranteed coverage, and conformal predictive systems (CPS) [26] provide a conformal predictive distribution (CPD). The CPDs can be queried for intervals with guaranteed coverage, similar to but more dynamic than CR. This is done by defining intervals based on percentiles in the distribution so that a symmetric interval with 90% coverage can be achieved using the percentiles [5,95]. The CPDs can also be queried for the probability of the actual instance value being below a user-given threshold, corresponding to the percentile of the threshold value in the distribution.\nFor classification, the focus is generally on the calibration of the probability esti- mates produced by the classifier, which can be defined as follows:\n$p(c | p^0) \u2248 p^0$,\nwhere $p^0$ represents the probability estimate for a particular class label c. This means that a well-calibrated model produces predicted probabilities that match observed accu- racy. Consequently, whenever a model assigns a probability estimate of 0.9 to a label, the accuracy for that label should be approximately 90%.\nIt is well-known that many predictive models produce poorly calibrated probability"}, {"title": "", "content": "estimates [27]. An external calibration method can be applied to calibrate a poorly"}, {"title": "", "content": "calibrated model using a separate portion of the labelled data, called the calibration set, to adjust the predicted probabilities.\nThe CP framework defines Venn [28] and Venn-Abers (VA) [29] predictors that produce multi-probabilistic predictions in the form of confidence-based probability in- tervals. Venn prediction involves a Venn taxonomy, categorising calibration data for probability estimation. The estimated probability for test instances falling into a cat- egory is the relative frequency of each class label among all calibration instances (in- cluding the test instance) in that category. Defining a proper Venn taxonomy can be challenging, which is the strength of VA."}, {"title": "2.2.1. Venn-Abers Calibration", "content": "VA Calibration offer automated taxonomy optimisation using isotonic regression, resulting in dynamic probability intervals for binary classification. Since the proba- bility interval includes the well-calibrated probability estimates for the true class label being both negative (lower bound) and positive (upper bound), and the instance must be one or the other, it follows that the interval must contain the true probability. The problem from a predictive perspective is that the true class label is not known. How- ever, the width and location of the interval can provide a lot of information. A smaller interval indicates higher certainty about the prediction, while a larger interval indicates more uncertainty. Since it is often impractical to have only an interval to indicate the probability estimate of the positive class, it is common to use a regularisation of the interval as an estimate for the positive class.\nTo define a VA predictor predicting a test object $x_{n+1}$, let $Z = \\{z_1, ..., z_n\\}$, where n = 1+q, be a training set. Each instance $z_i = (x_i, y_i)$ consists of two parts, an object $x_i$ and a target $y_i$. Normally, calibration requires a separate calibration set, motivating a split of the training set into a proper training set $Z_l$ with l instances and a calibration set $Z_q = \\{z_1,..., z_q\\}$. A scoring classifier is trained on $Z_l$ to compute s for $\\{x_1,..., x_q, x_{n+1}\\}$. The score s is defined as the probability estimate for the positive class from a classifier"}, {"title": "", "content": "h. Inductive VA prediction follows these steps:\n1. Use $\\{(s_1, y_1), ..., (s_q, y_q), (s_{n+1}, y_{n+1} = 0)\\}$ to derive the isotonic calibrator $g_0$ and use $\\{(s_1, y_1), ..., (s_q, y_q), (s_{n+1}, y_{n+1} = 1)\\}$ to derive the isotonic calibrator $g_1$.\n2. The probability interval for $y_{n+1}$ = 1 is defined as $[g_0(s_{n+1}), g_1(s_{n+1})]$ (hereafter referred to as $[p_{low}, p_{high}]$, representing the lower and upper bounds of the inter- val).\n3. The regularised probability estimate for $y_{n+1}$ = 1, minimising the log loss [29], can be defined as:\n$p = \\frac{p_{high}}{1 - p_{low} + p_{high}}$\nIn summary, VA produces a calibrated (regularised) probability estimate p together with a probability interval with a lower and upper bound $[p_{low}, p_{high}]$."}, {"title": "2.2.2. Conformal Predictive Systems", "content": "Conformal Predictive Systems produce CPDs for each test object $x_{n+1}$ when the target domain is numeric (i.e. regression). To define a CPS, assume the existence of an underlying regression model h trained using $Z_l$. Like all conformal predictors, CPS relies on nonconformity scores \u03b1, defining the strangeness of an instance. Unlike CR, where the nonconformity is usually defined as the absolute error $\u03b1_i = |y_i \u2013 h(x_i)|$, CPS defines nonconformity using the signed errors $\u03b1_i = y_i \u2013 h(x_i)$. The prediction for a test instance $x_{n+1}$ then becomes the following CPD:\n$CPD(y) = \\begin{cases} \\frac{i}{q+1}, \\text{ if } y \\in (C_{(i)}, C_{(i+1)}), \\text{ for } i \\in \\{0, ..., q\\}\\\\ \\frac{i-1+\\tau}{(q+1)}, \\text{ if } y = C_{(i)}, \\text{ for } i \\in \\{1, ..., q\\} \\end{cases}$ \nwhere $C_{(1)}, ..., C_{(q)}$ are obtained from the calibration scores $\u03b1_1,..., \u03b1_q$, sorted in in- creasing order:\n$C(i) = h(x_{n+1}) + \u03b1_i$\nwith $C(0) = -\u221e$ and $C(q+1) = \u221e$. In case of a tie, \u03c4 is sampled from the uniform distribution U(0, 1), and its role is to allow the p-values of target values to be uniformly distributed, $i''$ is the highest index such that $y \u2264 C(i'')$, while i' is the lowest index such that y \u2265 C(i')."}, {"title": "", "content": "The following cases provide some further intuition on how a CPD can be used:\n\u2022 Obtaining a two-sided symmetric prediction interval for a chosen significance level e can be done by $[C[(\\epsilon/2)(q+1)], C[(1-6/2)(q+1)]]$. Since the CPS has guaran- teed coverage, the expected error of the obtained interval will be e in the long run. Asymmetric prediction intervals are possible by selecting percentiles for the lower ($p^{low}$) and higher ($p^{high}$) bounds of the interval. The guaranteed coverage of the interval will be \u0454 = $p^{high}$ \u2013 $p^{low}$.\n\u2022 Still using the significance level e, a lower-bounded one-sided prediction interval can be obtained by $[C[\u2208(q+1)], \u221e]$, and an upper-bounded one-sided prediction in- terval can be obtained by $[\u2212\u221e, C[(1\u2212\u2208)(q+1)]]$. The coverage guarantees still apply.\n\u2022 Similarly, a point prediction corresponding to the median of the distribution can be obtained by $(C[0.5(q+1)] + C[0.5(q+1)])/2$. The median prediction can be seen as a calibration of the underlying model's prediction. Unless the model is biased, the median will tend to be very close to the prediction of the underlying model.\n\u2022 For a specific threshold t, the distribution can return the estimated probability p(C \u2264 t). Thus, it is possible to get the probability of the true target being below the threshold t.\nA CPS offers richer opportunities to define intervals and probabilities through query- ing the CPD compared to CR. A particular strength is the ability to calibrate the under- lying model. For example, if the underlying model is consistently overly optimistic, the median from the CPS will adjust for that and provide a calibrated prediction that is better adjusted to reality."}, {"title": "2.3. Calibrated Explanations", "content": "Calibrated Explanations is a recently released\u00b2 local explanation method support- ing both classification and regression, providing feature importance with uncertainty"}, {"title": "", "content": "quantification [10, 30]. Calibrated Explanations produce instance-based explanations, and a factual explanation is composed of a calibrated prediction from the underly- ing model accompanied by an uncertainty interval and a collection of factual feature rules, each composed of a feature weight with an uncertainty interval and a factual condition, covering that feature's instance value. Calibrated Explanations support both binary and multi-class classification. In binary classification, the explanation explains the calibrated probability estimate (and its level of uncertainty) for the positive class, whereas in multi-class classification, the most probable class (after calibration) is con- sidered the positive class and all other classes are treated as the negative class, i.e., not the predicted class. For regression, there are two alternative use cases:\n1. The regression explanation explains a calibrated estimate of the prediction from the regressor, with a confidence interval covering the true target with a user- assigned level of confidence.\n2. The thresholded explanation explains the calibrated probability estimate (and its level of uncertainty) for the calibrated estimate of the prediction being below a user-given threshold.\nThe algorithm's core is agnostic to whether it is a classification or regression prob- lem since it is defined based on a numeric estimate and a lower and an upper bound defining an uncertainty interval for the numeric estimate. For classification, the proba- bility estimate for the positive class is calibrated using a VA calibrator [29], producing a lower and an upper bound for the calibrated probability estimate (using a regularised mean of these bounds as the numeric estimate). For regression, a Conformal Predic- tive System (CPS) [31], producing a Conformal Predictive Distribution (CPD), is used as a calibrator of the underlying model. For the first use case, explaining the predic- tion value, the numeric estimate is the median from the CPD, and the lower and upper bounds are represented by user-selected percentiles in the CPD, defining the interval with guaranteed coverage. For the second use case, explaining the probability of be- ing below a user-given threshold, the percentile in the CPD representing the threshold position is used as a probability estimate (similar to classification) upon which a VA calibrator is applied. For details on how thresholded regression works, see the original"}, {"title": "", "content": "regression paper by L\u00f6fstr\u00f6m et al. [30].\nCalibrated Explanations assume the existence of a predictive model h, trained using the proper training set $Z_l$, outputting a numeric value when predicting an object h(xi). For classification, the model is a scoring classifier, producing probability estimates for the positive class. For regression, it is an ordinary regressor predicting the expected value. Algorithm 1 describes how Calibrated Explanations creates a factual explanation of x\u00b3.\nFor further details on the algorithm and how it is applied to classification and the two use cases for regression, see [10] and [30]."}, {"title": "2.4. Perturbation Based Explanations", "content": "Yapicioglu et al. [11] presents ConformaSight, a unique explanation approach based on conformal prediction methodology that provides insightful and resilient explana- tions regardless of the underlying data distribution. Its purpose is to give explanations for set-type predictions that conformal predictors create. ConformaSight highlights the influence of the calibration process on prediction outputs, in contrast to conventional explanation approaches that mainly concentrate on feature importance.\nIn classification tasks, it is essential to understand the factors that influence the for- mation of prediction sets within conformal prediction frameworks to enhance model in- terpretability and trust[32]. Analyzing metrics such as weighted coverage and weighted set size provides valuable insights into the model's uncertainty representation [33]. Weighted coverage measures the proportion of instances that are correctly classified and included within prediction sets, thereby indicating the model's reliability in identi- fying uncertain regions relative to class distribution [34]. On the other hand, weighted set size offers critical information about the granularity of uncertainty representation, reflecting the average number of instances within prediction sets while accounting for class imbalance. By examining these metrics, researchers can gain a deeper under- standing of the relationship between model predictions and input features."}, {"title": "3. Proposed Solution", "content": "This paper aims to incorporate the perturbation approach, constituting a core ele- ment in ConformaSight, into the explanation method Calibrated Explanations, making it possible to extract a new form of explanations, providing fast feature importance generation without rule conditions.\nAs described in 2.3, perturbations are done for each feature of the test instance at explanation time in Calibrated Explanations. The solution that we propose in this paper"}, {"title": "", "content": "performs all perturbations on the calibration set at initialisation of the Fast Calibrated Explanations, resulting in some additional overhead once when initialising Fast Cali- brated Explanations, while avoiding any perturbations at explanation time. Compared to the explanations in Calibrated Explanations, a main difference with the solution pro- posed here is that there is no rule conditions. Instead, each feature is assigned a feature weight determining the relative importance of that feature compared to other features. As such, the provided explanations are factual, conveying the feature importance per instance. The resulting solution provides very fast explanations with feature weights that can be analysed per instance.\nMore formally, the solution that we propose can be divided into two stages: 1) initialisation, and 2) explanation. The initialisation stage 1) is described in Algorithm 2 while the explanation stage 2) is described in Algorithm 3."}, {"title": "", "content": "Since the perturbation is performed at initialisation and is using a CPD when used for regression, all explanations from a single model that use the same uncertainty in- terval, i.e., the same percentiles, will result feature weights that have exactly the same size in relation to all other feature weights. The only thing that will differ from instance"}, {"title": "4. Experimental Setup", "content": "The evaluation is divided into two parts. The first part contains a comparative eval- uation between the Fast Calibrated Explanations, our proposed solution, and Calibrated Explanations on classification and regression problems. The evaluation is performed separately for classification and regression, focusing on complementary aspects. For code, see the calibrated_explanations/evaluation/FastCE folder in the repository.\nFor classification, an ablation study of the impact from possible permutation param- eters is performed. The evaluation covers both computational cost and how the mean variance of feature weights vary across different parameter settings. The ablation study includes various parameter values for the Fast Calibrated Explanations including both forms of noise type (uniform and gaussian), four different scaling factors (1, 3, 5, 10), and five different severity values (0, 0.25, 0.5, 0.75, 1). Computational time and mean variance per feature importance is reported for 25 binary classification data sets. Each data set was split into 50% training data, 25% calibration data and 25% test data and the underlying model was a RandomForestClassifier with 100 trees4.\nFor regression, focus is on computational speed, stability and robustness in compar- ison with SHAP and LIME applied on calibrated models (using the CPS as calibrator of the underlying model). All target values were min-max normalised to the range [0, 1]. Each data set was split using 200 calibration instances, 100 test instances, and the re- maining instances as training set. For each one of the 31 data sets, six different setups where evaluated, three for standard regression and two for thresholded regression (us-"}, {"title": "", "content": "to instance is the scale of the feature weights, making Fast Calibrated Explanations for standard regression clearly less useful. The same issue does not exist for probabilistic explanations (applicable to both classification and thresholded regression).\nThe similarities and differences between Calibrated Explanations and Fast Cali- brated Explanations are summarised in Table 1."}, {"title": "5. Experimental Results", "content": ""}, {"title": "5.1. Performance Evaluation for Classification and Regression", "content": ""}, {"title": "5.1.1. Comparison of Computation Time", "content": "The code used for the evaluation of Calibrated Explanations (CE) and Fast Cali- brated Explanations (FCE) can be found in the FastCE folder in the repository. As the difference in initialisation time is almost negligible, the results are not presented here but can be found in the folder above. The explanation time clearly differs between Cal- ibrated Explanations (CE) and Fast Calibrated Explanations (FCE). However, as the explanation time between different parameter settings for FCE does not differ much, only the default parameters (noise type=uniform, scale factor=5, and severity=0.5) are compared with CE in Table 2."}, {"title": "5.1.2. Stability and Robustness", "content": "Both stability and robustness have been evaluated for the regression data. The mean and median stability and robustness aggregated over all regression data sets are shown in Table 6. The motivation for using both aggregation methods is that some data sets deviated drastically from the general picture for some methods, having a huge impact on the mean but not on the median. For extremely low variation (less than 1e \u2013 30), the value was set to 0. Detailed result per data set can be found in the evaluation folder.\nThe robustness must be compared to the amount of variability in the predictions from the underlying model, which was 1.7e - 4 on average.\nLets start by considering the standard regression results from LIME, SHAP, CE and FCE, which can all be compared. Considering the results reported in [30], the results for LIME, SHAP and CE are as expected. It is clear that stability is worse for FCE than for CE and SHAP but comparable to LIME. Furthermore, the robustness of FCE is even better than all of the other three methods. Looking at the probabilistic results, it does not make sense to compare these results with the previously mentioned results, as their predictions are probabilities. Both PCE and PFCE are less stable and robust and the reason is related to the sensitivity of the probabilities derived from the CPD.\nThe reason for the sensitivity is that a relatively small change in prediction can easily result in a comparably much larger change in probability for exceeding the threshold, especially if the target is close to the threshold (which is set to 0.5, i.e., the mid-point in the interval of possible target values). Results are comparable between PCE and PFCE."}, {"title": "5.2. Demonstration", "content": "In the demonstration below, a few different data sets are included, to show exam- ples from different use cases. The examples will illustrate how the explanations may"}, {"title": "6. Concluding Discussion", "content": "This paper address a common drawback among explanation methods, namely the computational overhead of explaining an instance. The proposed solution combines fundamental building blocks from two recently proposed explanation methods: Cali- brated Explanations and ConformaSight. The proposed solution have used the pertur- bation strategy used in ConformaSight in combination with the explanation engine in Calibrated Explanations, allowing really fast local explanations with uncertainty quan- tification for both classification and thresholded regression. The uncertainty quantifi- cation extend both the calibrated predictions and the provided feature weights. Fast Calibrated Explanations is able to take advantage of Calibrated Explanations support for both binary and multi-class classification, as well as thresholded regression, pro- viding the probability of the true target being above a user-given threshold.\nPossible directions for future work include considering the issue of perturbations outside the natural scope of the data set as well as ways of speeding up Calibrated Ex- planations using insights from Fast Calibrated Explanations. Another important area for future work is to consider the decision-making aspect, focusing on situations where fast explanations are critical, exploring how our solution can help create trustworthy ex- planations in such use cases. Currently, Fast Calibrated Explanations does not convey much insights for standard regression, which should be addressed in future develop- ment."}]}