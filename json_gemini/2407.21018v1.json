{"title": "ThinK: Thinner Key Cache by Query-Driven Pruning", "authors": ["Yuhui Xu", "Zhanming Jie", "Hanze Dong", "Lei Wang", "Xudong Lu", "Aojun Zhou", "Amrita Saha", "Caiming Xiong", "Doyen Sahoo"], "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) (Hadi et al., 2023; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a,b; Scao et al., 2022; Reid et al., 2024) have emerged as a dominant paradigm in natural language processing, achieving state-of-the-art performance across various tasks. A key principle, the Scaling Law (Kaplan et al., 2020), suggests that LLMs exhibit emergent abilities as model size increases, enhancing their capacity to understand context and handle long sequences (Xiong et al., 2023). This capacity growth allows LLMs to generate coherent and contextually accurate responses and enables various downstream applications, such as document summarization (Zhang et al., 2019, 2024a), code generation (Chen et al., 2021b), and conversational AI (Bordes et al., 2016; OpenAI, 2022), .\nDespite their success in various applications, the generation of LLMs incurs significant expenses, which escalate with increasing model size and sequence length. Notably, both the training (Strubell et al., 2020; Hoffmann et al., 2022; Dong et al., 2024a) and inference (Ainslie et al., 2023) stages involve frequent generation by LLMs, further contributing to these costs. Consequently, efficient LLMs have gained popularity in recent years (Hu et al., 2021; Wan et al., 2023). To address these challenges, quantization (Frantar et al., 2022; Lin et al., 2024; Dettmers et al., 2024; Xu et al., 2023) and pruning methods (Frankle and Carbin, 2018; Blalock et al., 2020) are employed to reduce model size. Additionally, managing long sequences presents another cost due to the transformer attention mechanism. The quadratic complexity of the attention mechanism results in substantial computational burden when dealing with long sequences, scaling poorly with sequence length. Therefore, effective management of long sequences is essential for the practical deployment of LLMs. In this paper, we focus on the long-context scenario and aim to reduce the memory consumption associated with lengthy sequences.\nSpecifically, the number of KV cache parameters is the product of batch size B, sequence length S, number of layers L, number of heads N, channel size of each head D, i.e., $K, V \\in R^{B \\times S \\times L \\times N \\times D}$, which need to be stored in the GPU memory during inference. To reduce memory and computational costs during inference, efficiency can only be achieved by pruning the dimensions across S, L, N, D or performing quantization over the caches. It is well-acknowledged that token importance tends to be sparse. Consequently, KV eviction algorithms have been proposed to minimize the KV cache memory cost from dimension S (Xiao et al., 2023b; Li et al., 2024; Zhang et al., 2024c; Leviathan et al., 2023). Additionally, inter-layer redundancy has been explored (Liu et al., 2024a; Wu and Tu, 2024; Brandon et al., 2024) to address the layer dimension L. Despite these advances, existing methods have largely overlooked the channel dimension D. In this paper, we highlight that the magnitude across Key cache channel dimensions is significantly unbalanced. Additionally, we observe that the attention weights exhibit a low-rank structure. Based on these findings, we hypothesize that the channel dimension of the key cache exhibits redundancy. Consequently, we explore the redundancy of KV cache tensors specifically from dimension D, aiming to devise effective strategies that reduce costs without compromising performance.\nIn our paper, we propose a simple yet effective method Think, for KV cache pruning. To pinpoint the least significant channels, we formulate the task as an optimization problem, aiming to minimize the loss in attention weights attributable to pruning. To effectively address this problem, we establish a novel query-dependent criterion that assesses the importance of each channel. Using this criterion, we then select the most critical channels in a greedy fashion. We evaluate Think using the LLaMA3 (Meta, 2024) and Mistral (Jiang et al., 2023) models and validate its effectiveness across various long-sequence datasets. The results indicate that when paired with token eviction methods, Think not only achieves comparable or superior accuracy but also reduces KV cache memory costs by more than 20%.\nContributions. This work pioneers the investigation into the sparsity structure of the channels in the KV cache. Specifically, we discovered that the activated key cache is sparse given a specific query. This insight allows us to prune the key cache channels using a query-induced norm. Inspired by this phenomenon, we"}, {"title": "Observations", "content": "We identify several key observations that motivate our approach to prune the channel of KV-Cache. Specifically, we visualize the magnitude of KV-cache and perform singular value decomposition of the attention in LLaMA3-8B model. We show the visualization for certain heads and layers for illustration purposes.\nMagnitude of KV-Cache Channel Figure 2 visualizes the absolute values of key and value cache across the tokens in each channel\u00b9. Consistent with previous findings (Lin et al., 2024; Xiao et al., 2023a; Liu et al., 2024b), we observe that only certain channels have significant magnitudes in the key cache, whereas the value cache lacks obvious patterns. For instance, the absolute value around 50th channel of key cache has much larger magnitudes than other channels for all tokens in layer 14 (Figure 2 (a)). The same observation applies to 50th and 150th channels of the first head in layer 20 (Figure 2 (c)). Given such an observation, Liu et al. (2024b) proposed to perform quantization over the channels of the key cache based on such observations. In addition to channel quantization, the observation also suggests that it is feasible to prune away certain key cache channels that contribute less to the attention mechanism. Moreover, channel quantization and pruning techniques are orthogonal such that they can be utilized concurrently to enhance model efficiency."}, {"title": "Think", "content": "Notations. We use uppercase letters (e.g., X,Y) to denote scalar values and boldface uppercase letters (e.g., Q, K) to denote matrices and tensors. The notation $|| \\cdot ||_p$ denotes the $l_p$-norm for vectors. Unless otherwise specified, $|| \\cdot ||$ denotes the $l_2$-norm. The Frobenius norm is denoted by $|| \\cdot ||_F$. The floor function is denoted by $\\lfloor \\cdot \\rfloor$, and the ceiling function is denoted by $\\lceil \\cdot \\rceil$.\nPreliminary Study of KV Cache Optimization\nIn scenarios with extended contexts or batch processing, the main limitations in terms of memory and speed are due to the handling of the KV cache size. Considering a batch of requests to a Large Language Model (LLM) service that provides a long input prompt consisting of tokens $[x_{B_1},..., x_{B_S}]$, the total KV cache size can be computed as follows:\n$2 \\times B \\times S \\times L \\times N \\times D$,\nwhere L is the number of layers, N is the number of heads, D is the head dimension. The KV cach size grows linearly as the batch size B and sequence length S. For a model with multihead attention (MHA) (Vaswani et al., 2017), such as LLaMA2-7B (Touvron et al., 2023b), a context length of 2048 and a batch size of 13 require storing a 13 GB KV cache, which is equivalent to the size of the model parameters. The KV cache must be transferred from off-chip memory (HBM) (Jia et al., 2018) to on-chip memory (cache) for each token generated, leading to a memory bottleneck. This substantial memory demand highlights the challenges in managing large-scale models and the need for efficient memory utilization strategies. Current methods optimize the KV cache based on the sequence length S (Xiao et al., 2023b; Zhang et al., 2024c; Li et al., 2024) and precision (Hooper et al., 2024; Liu et al., 2024b). We will introduce a new method, Think, to optimize it from the perspective of the number of head dimensions D.\nMagnitude base Pruning: Based on the observations in Figure 2 which depicts the significant variation in the magnitudes across different channels, one straightforward criterion is to use the norm of the magnitude"}, {"title": "Query-Driven Pruning", "content": "to measure the importance of different channels in key cache.\n$M_{n,d} = ||K[n, :, d]||_p$\nGiven pruning ratio A, We only keep T = $\\lfloor (1 \u2013 \\lambda)D \\rfloor$ most important channels among the D channels of each head:\n$I = Top_T(M,T)$\nwhere $|| \\cdot ||_p$ is the $l_p$ norm of each channel. n \u2208 [1, N] and d\u2208 [1, D] are indicators of heads and channels in key cache. $I \\in (Z^+)^{N \\times T}$ stores the indicators of the top T values in tensor M per head.\nIn Table 1, we present the results of key cache pruning with various pruning ratios applied to the LLaMA3-8B model. We utilize the $l_1$ and $l_2$ norms as criteria for evaluation, and validate performance using the LongBench benchmark (Bai et al., 2023). Compared to the baseline methods, H2O (Zhang et al., 2024c) and SnapKV (Li et al., 2024), both with a KV length of 512, we further prune the channels of the key cache. A 30% pruning ratio can maintain accuracy; however, increasing it to 40% results in significant performance degradation, especially for $l_1$ norm based pruning. The results of magnitude-based pruning support our assumption that the key cache is redundant in the channel dimension. These results also indicate the need for a better pruning matrix to achieve higher pruning ratios effectively.\nQuery-Driven Pruning\nFor each head, the attention scores are computed using the queries and keys, and then applied to the values. The formula for the attention for head i is:\n$Attention(Q_i, K_i, V_i) = softmax(\\frac{Q_iK_i^T}{\\sqrt{D}})V_i$\nwhere $Q_i, K_i, V_i \\in R^{S \\times D}$. When pruning one channels of $K_i$, the corresponding channels in $Q_i$ will also be removed. We aim to find the optimal subset of channels to prune, denoted by the selection matrix $S \\in \\{0, 1\\}^{D \\times D}$, where S is a diagonal matrix with binary entries (1 for keeping a channel, 0 for pruning it). To better maintain the performance after pruning the channels, we minimize the Frobenius norm of the difference between the original and pruned attention weights:\n$\\min_S ||Q_iK_i^T - Q_iS(K_iS)^T||_F$\nGiven a pruning ratio \u5165, it can further expanded as:\n$\\min_S ||Q_iK_i^T - Q_i^TSK_i^T||_F$\nsubject to  trace(S) = $\\lfloor(1 \u2013 \\lambda)D\\rfloor$\nS = diag($s_1, s_2,..., s_D$), where $s_j \\in \\{0, 1\\}$\nFor simplicity, we use greedy algorithm to optimize S. To achieve the pruning goal, we define a criterion for evaluating the importance of each channel and greedily select the channels with largest scores:\n$Score[j] = ||Q_i[:, j]K_i[:, j]^T ||_F$\n$I_i = Top_T(Score_i, T)$\nHere's a detailed explanation of why it optimizes the selection matrix. The $score_i[j]$ measures the magnitude of the interaction between the query and key vectors for channel j in each head i. By selecting channels"}, {"title": "Implementations", "content": "with the highest interaction magnitudes, we aim to retain the most significant contributions to the attention mechanism. This criterion ensures that the selected channels preserve the primary information flow in the attention computation, thereby minimizing the loss of important information.\nObservation Window. Based on the observations in SnapKV (Li et al., 2024) that the last window of input sequence recognizes highly similar attention allocation pattern with the actual generation. To reduce the computation cost, we only use the last $S_{obs}$ window to calculate the score: $||Q_i[-S_{obs}:, j]K_i[:, j]^T ||_F$.\nImplementations\nFollowing SnapKV, we focus on the long context input scenario. We opt not to prune the most recent tokens and newly generated keys. Consequently, our key-value (KV) cache will store two distinct categories of keys: one subset consists of pruned keys with a reduced channel size, while the other retains keys at their original size. Additionally, we maintain a binary mask to indicate which channels have been pruned. Note that the memory overhead associated with this mask is negligible.\nFigure 4 illustrates one implementation of our method during the decoding stage. The pruned keys are first zero-filled using the mask to restore them to their original size, and then concatenated with the unpruned keys. This implementation can be effectively integrated with optimization techniques such as FlashAttention (Dao, 2023). The second implementation involves initially pruning the Query using the mask. The pruned Query is then multiplied by the pruned Key, while the un-pruned Query is applied to the unpruned Key. Subsequently, the two outputs are concatenated. In theory, this implementation has lower computational costs."}, {"title": "Experiment Results", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness of Think on performance and memory reduction. First, we introduce benchmark datasets, baseline methods, and implementation details in Section 4.1. Next, we report the performance of our KV pruning method with two different KV compression methods in long-context scenarios in Section 4.2. Finally, we present the results of ablation experiments in Section 4.4.\nSettings\nBenchmark Datasets. We perform evaluations of our method with state-of-the-art KV cache compression methods on two widely used benchmarks: LongBench and Needle-in-a-Haystack. LongBench (Bai et al., 2023) is designed to comprehensively evaluate LLM's long context understanding capabilities. It includes 17 datasets covering six different tasks: single-document QA, multi-document QA, summarization, few-shot learning, synthetic tasks, and code completion. The average input length of LongBench is 6,711 words, which necessitates reducing the KV cache to lower memory usage for inference. Needle-in-a-Haystack (Kamradt, 2023) is a recent popular test challenge that requires models to accurately identify a small piece of information (\"needle\") in a long document (\"haystack\"), where the needle is placed at a random position. This challenge can test if KV cache compression methods still retain the small piece of critical information."}, {"title": "Results on LongBench", "content": "Tables 2 and 3 present the results of KV compression methods and KV compression methods integrated with our channel pruning method of Key cache (Think) over two different base LLMs across various KV-sizes\nBaseline Approaches. The baseline methods include Heavy Hitter Oracle (H2O) and SnapKV, both of which are the state-of-the-art KV cache compression methods but use different strategies for selecting KV cache. H2O (Zhang et al., 2024c) is designed to reduce memory usage by dynamically balancing recent tokens and Heavy Hitter (H2) tokens, where H2 tokens are a small set of tokens that contribute most of the value when computing attention scores. SnapKV (Li et al., 2024) automatically compresses KV caches by selecting clustered important KV positions for each attention head. This is inspired by the fact that each attention head has a specific pattern, focusing on certain attention features during generation.\nImplementation Details. In this paper, we use LLaMA3-8B-Instruct (Meta, 2024) and Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) as the backbone LLMs, both accessible via HuggingFace (Wolf et al., 2020). Our Think aims to prune channels of the KV, which is agnostic to KV cache compression methods. If there is no other statement, we prune the key cache by default. All the experiments are conducted using one Nvidia A100. To fairly compare KV cache compression methods and KV cache compression integrated with Think, we use the same hyperparameters for both. For example, when comparing SnapKV and SnapKV integrated with Think, we set the maximum pooling kernel size to 7 and the observation window size to 32, using the same KV-size for both.\nResults on LongBench\nTables 2 and 3 present the results of KV compression methods and KV compression methods integrated with our channel pruning method of Key cache (Think) over two different base LLMs across various KV-sizes"}, {"title": "Results on Needle-in-a-Haystack", "content": "on LongBench. The following observations can be drawn: (1) Our method can further prune the channels of the key cache after compressing the KV cache with H2O and SnapKV. For the base model LLaMA3-8B, our method reduces memory usage and slightly improves performance for both H2O and SnapKV. For the base model Mistral-7B, our method reduces memory with only a slight drop in performance in some cases. (2) Comparing the results of SnapKV or H2O integrated with Think in Table 2 to SnapKV or H2O integrated with 11 or 12 norm shown in Table 1, our proposed query-based channel pruning shows better performance when the pruning ratio is 40% compared to no pruning. However, 11 and 12 norm methods cannot outperform the KV compression methods without pruning. (3) When the pruning ratio is small, performance tends to be better than when the pruning ratio is large. (4) When the KV-size is increased from 128 to 2048, the performance of our channel pruning method improves. Notably, with a KV cache size of 2048 and a pruning ratio of 40%, our method can even outperform the LLaMA3-8B with a full KV cache. The above observations indicate that our method is agnostic to existing KV cache compression methods and can further improve their performance and memory reduction. Additionally, query-based pruning is more effective than 11 and 12 norm for channel pruning in LLMs.\nResults on Needle-in-a-Haystack\nTable 4 presents the results of the Needle-in-a-Haystack test based on the SnapKV (Li et al., 2024) approach with KV cache size of input prompt from 128 to 2048. With a modest pruning ratio X = 40%, the accuracy of Think is consistently better or comparable than the original SnapKV across both LLaMA3 and Mistral with"}, {"title": "Ablation Studies", "content": "different KV cache sizes. Such comparisons demonstrate that the proposed query-driven channel pruning can effectively retain the informative channels and discard the noisy ones. With a pruning ratio of X \u2265 50%, we observe an accuracy drop with a small KV-cache size, particularly for 128 and 512 across both LLaMA3 and Mistral. However, Think is still able to obtain comparable performance with SnapKV when we have a larger KV cache size (i.e., 1024 and 2048). Intuitively, a larger pruning ratio with a smaller KV cache size could tend to lose more important information compared to a larger KV cache size. In addition, the performance on larger KV cache size also suggests Think is robust for long context retrieval tasks.\nFigure 6 (a)-(d) visualize the Needle-in-a-Haystack test accuracy against different token lengths and depth percent. The KV cache size is configured to 128 and 1024, with pruning ratio A equal to 40% and 50%, respectively. First, Think can maintain the original retrieval capability of SnapKV though the overall numerical accuracy is different (i.e., 77.8 vs 78.6 and 90.4 vs 90.6). Because ThinKachieves the same accuracy at most of the entries with different token limits and depths as Snap KV. In addition, Think also successfully retrieves some \"needles\" where SnapKV fails, leading to higher overall accuracy. The visualization results further demonstrate the robustness of Think in a fine-grained perspective and Think can enhance the original quantization approach.\nImpact of Different Recent Sizes. Preserve the most recent KV embeddings (Zhang et al., 2024c; Li et al., 2024) is important for maintaining the performance of LLMs after KV cache compression. Note that a tradeoff exists: increasing the recent-size allows more infomation to be propagated, while increasing the cache size to be stored. To study its impacts, we evaluate the performance produced by three recent-size, namely 0, 32 and 128, on LongBench, using Mistral-7B-Instruct-v0.2 as the basleline model. The results are summarized in Table 5. One can observe that a recent-size of 32 yields superior performance than 0 in terms of averaged score on LongBench which demonstrates the importance of keeping the most recent KVs. On"}, {"title": "Related Work", "content": "In scenarios involving long contexts, the most significant computational burden from the attention mechanism is the key-value (KV) cache. Reducing the KV cache is a high priority for optimizing deployment efficiency. System-level optimizations, such as FlashAttention (Dao, 2023) and PagedAttention (Kwon et al., 2023), have been developed to address this issue. Additionally, algorithm-level optimizations are being explored to further enhance efficiency.\nKV Cache Eviction. StreamingLLM (Xiao et al., 2023b) maintains a few initial tokens and some recent tokens based on the observation of attention sink, which may result in the loss of important information carried by the dropped tokens. H2O (Zhang et al., 2024c) retains only a small portion of the tokens by greedily dropping tokens based on their contributions to the cumulative attention. SnapKV (Li et al., 2024) selects clustered important KV positions for each attention head from an 'observation' window located at the end of the prompts. FastGen (Ge et al., 2023) adaptively evicts tokens from attention heads that emphasize local contexts. This approach focuses on discarding non-special tokens centered on special tokens, while the standard KV cache is used only for attention heads that broadly attend to all tokens. PyramidKV (Zhang et al., 2024b) and PyramidInfer (Yang et al., 2024) considers adjusting the KV cache size across different layers by allocating more cache in the lower layers and less in the higher ones.\nKV Cache Quantization. SmoothQuant (Xiao et al., 2023a) can quantize the KV cache to 8-bit with minimal performance degradation. Q-Hitter (Zhang et al., 2024c) uses accumulated attention scores and \u2018Quantization Friendliness' metrics to identify tokens that are essential for maintaining the generalization capabilities of LLMs and are suitable for KV cache quantization. Some studies have found that the key cache and value cache should be quantized differently (Liu et al., 2024b; Hooper et al., 2024): the key cache should be quantized per-channel, while the value cache should be quantized per-token.\nStructured Pruning of LLMs. Structured pruning (Ma et al., 2023; Ding et al., 2023) of LLMs traditionally focuses on removing unimportant layers, heads, and hidden dimensions, which often results in significant performance degradation. In contrast, our methodology preserves the original architecture of the LLM and specifically targets the channel dimension within each head's key cache. By dynamically identifying unimportant channels based on data dependant criterion, our approach greatly reduce the key cache-size with negligible performance loss."}, {"title": "End Note and Future Direction", "content": "Motivated by the observation that certain channels have significantly larger magnitudes compared to others, and the singular value analysis indicates that the key cache is inherently low-rank (Singhania et al., 2024), we propose Think to perform pruning over the key cache channel. The proposed pruning strategy is query-dependant and optimized based on the attention scores, ensuring that essential information is retained for each input query. In addition, Think can be seamlessly integrated with other popular token-level KV cache quantization techniques (Li et al., 2024; Zhang et al., 2024c), further enhancing inference efficiency. Extensive experiments on LongBench (Bai et al., 2023) and Needle-in-a-Haystack tests with two foundation models demonstrate the effectiveness and robustness of our query-dependent channel pruning method. Our approach achieves comparable or superior performance to baseline methods while reducing the key cache size by 40%. Our analysis indicates that Think can maintain superior performance over baselines with a smaller KV cache size under equivalent memory consumption conditions.\nFuture work will focus on strategies to increase the pruning ratio without compromising model performance, and exploring pruning techniques for value caches. We also plan to evaluate channel importance from a more sophisticated and compositional perspective, incorporating both token-level and channel-level information"}, {"title": "Value Cache Pruning", "content": "Similar to the approach used for the Key cache, the pruning of channels in the Value cache can be guided by two primary criteria: magnitude-based pruning and query-driven pruning. We find that query-driven pruning is still better than magnitude based pruning.\n$Score_{v,i} (Q_i, K_i, V_i) [j] = ||softmax(\\frac{Q_i[-S_{obs}:]K_i^T}{\\sqrt{D}})V_i[:, j]||_F$\n$I_i = Top_T(Score_{v,i}, T)$\nwhere $Q_i, K_i, V_i \\in R^{S \\times D}$. We define a criterion $Score_{v,i}$ to indicate the importance of each channel in the head i of value cache. Then, only top T channels are retained."}]}