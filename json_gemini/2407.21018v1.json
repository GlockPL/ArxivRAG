{"title": "ThinK: Thinner Key Cache by Query-Driven Pruning", "authors": ["Yuhui Xu", "Zhanming Jie", "Hanze Dong", "Lei Wang", "Xudong Lu", "Aojun Zhou", "Amrita Saha", "Caiming Xiong", "Doyen Sahoo"], "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Hadi et al., 2023; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023a,b;\nScao et al., 2022; Reid et al., 2024) have emerged as a dominant paradigm in natural language processing,\nachieving state-of-the-art performance across various tasks. A key principle, the Scaling Law (Kaplan et al.,\n2020), suggests that LLMs exhibit emergent abilities as model size increases, enhancing their capacity to\nunderstand context and handle long sequences (Xiong et al., 2023). This capacity growth allows LLMs\nto generate coherent and contextually accurate responses and enables various downstream applications,\nsuch as document summarization (Zhang et al., 2019, 2024a), code generation (Chen et al., 2021b), and\nconversational AI (Bordes et al., 2016; OpenAI, 2022), .\nDespite their success in various applications, the generation of LLMs incurs significant expenses, which es-\ncalate with increasing model size and sequence length. Notably, both the training (Strubell et al., 2020;\nHoffmann et al., 2022; Dong et al., 2024a) and inference (Ainslie et al., 2023) stages involve frequent gener-\nation by LLMs, further contributing to these costs. Consequently, efficient LLMs have gained popularity in\nrecent years (Hu et al., 2021; Wan et al., 2023). To address these challenges, quantization (Frantar et al.,\n2022; Lin et al., 2024; Dettmers et al., 2024; Xu et al., 2023) and pruning methods (Frankle and Carbin, 2018;\nBlalock et al., 2020) are employed to reduce model size. Additionally, managing long sequences presents\nanother cost due to the transformer attention mechanism. The quadratic complexity of the attention mech-\nanism results in substantial computational burden when dealing with long sequences, scaling poorly with\nsequence length. Therefore, effective management of long sequences is essential for the practical deployment\nof LLMs. In this paper, we focus on the long-context scenario and aim to reduce the memory consumption\nassociated with lengthy sequences.\nSpecifically, the number of KV cache parameters is the product of batch size B, sequence length S, number\nof layers L, number of heads N, channel size of each head D, i.e., $K, V \\in \\mathbb{R}^{B\\times S\\times L\\times N\\times D}$, which need to be\nstored in the GPU memory during inference. To reduce memory and computational costs during inference,\nefficiency can only be achieved by pruning the dimensions across S, L, N, D or performing quantization over\nthe caches. It is well-acknowledged that token importance tends to be sparse. Consequently, KV eviction\nalgorithms have been proposed to minimize the KV cache memory cost from dimension S (Xiao et al.,\n2023b; Li et al., 2024; Zhang et al., 2024c; Leviathan et al., 2023). Additionally, inter-layer redundancy has\nbeen explored (Liu et al., 2024a; Wu and Tu, 2024; Brandon et al., 2024) to address the layer dimension L.\nDespite these advances, existing methods have largely overlooked the channel dimension D. In this paper, we\nhighlight that the magnitude across Key cache channel dimensions is significantly unbalanced. Additionally,\nwe observe that the attention weights exhibit a low-rank structure. Based on these findings, we hypothesize\nthat the channel dimension of the key cache exhibits redundancy. Consequently, we explore the redundancy\nof KV cache tensors specifically from dimension D, aiming to devise effective strategies that reduce costs\nwithout compromising performance.\nIn our paper, we propose a simple yet effective method Think, for KV cache pruning. To pinpoint\nthe least significant channels, we formulate the task as an optimization problem, aiming to minimize the\nloss in attention weights attributable to pruning. To effectively address this problem, we establish a novel\nquery-dependent criterion that assesses the importance of each channel. Using this criterion, we then select\nthe most critical channels in a greedy fashion. We evaluate Think using the LLaMA3 (Meta, 2024) and\nMistral (Jiang et al., 2023) models and validate its effectiveness across various long-sequence datasets. The\nresults indicate that when paired with token eviction methods, Think not only achieves comparable or\nsuperior accuracy but also reduces KV cache memory costs by more than 20%.\nContributions. This work pioneers the investigation into the sparsity structure of the channels in the KV\ncache. Specifically, we discovered that the activated key cache is sparse given a specific query. This insight\nallows us to prune the key cache channels using a query-induced norm. Inspired by this phenomenon, we"}, {"title": "2 Observations", "content": "We identify several key observations that motivate our approach to prune the channel of KV-Cache. Specif-\nically, we visualize the magnitude of KV-cache and perform singular value decomposition of the attention in\nLLaMA3-8B model. We show the visualization for certain heads and layers for illustration purposes.\nMagnitude of KV-Cache Channel Figure 2 visualizes the absolute values of key and value cache across\nthe tokens in each channel\u00b9. Consistent with previous findings (Lin et al., 2024; Xiao et al., 2023a; Liu et al.,\n2024b), we observe that only certain channels have significant magnitudes in the key cache, whereas the value\ncache lacks obvious patterns. For instance, the absolute value around 50th channel of key cache has much\nlarger magnitudes than other channels for all tokens in layer 14 (Figure 2 (a)). The same observation applies\nto 50th and 150th channels of the first head in layer 20 (Figure 2 (c)). Given such an observation, Liu et al.\n(2024b) proposed to perform quantization over the channels of the key cache based on such observations.\nIn addition to channel quantization, the observation also suggests that it is feasible to prune away certain\nkey cache channels that contribute less to the attention mechanism. Moreover, channel quantization and\npruning techniques are orthogonal such that they can be utilized concurrently to enhance model efficiency."}, {"title": "3 Think", "content": "Notations. We use uppercase letters (e.g., X,Y) to denote scalar values and boldface uppercase letters\n(e.g., Q, K) to denote matrices and tensors. The notation $|| \\cdot ||_p$ denotes the $l_p$-norm for vectors. Unless\notherwise specified, $|| \\cdot ||$ denotes the $l_2$-norm. The Frobenius norm is denoted by $|| \\cdot ||_F$. The floor function\nis denoted by $[\\cdot]$, and the ceiling function is denoted by $[\\cdot]$."}, {"title": "3.1 Preliminary Study of KV Cache Optimization", "content": "In scenarios with extended contexts or batch processing, the main limitations in terms of memory and speed\nare due to the handling of the KV cache size. Considering a batch of requests to a Large Language Model\n(LLM) service that provides a long input prompt consisting of tokens $[x_{B1},..., x_{BS}]$, the total KV cache size\ncan be computed as follows:\n$2\\times B \\times S \\times L \\times N \\times D$,\nwhere L is the number of layers, N is the number of heads, D is the head dimension. The KV cach size grows\nlinearly as the batch size B and sequence length S. For a model with multihead attention (MHA) (Vaswani\net al., 2017), such as LLaMA2-7B (Touvron et al., 2023b), a context length of 2048 and a batch size of\n13 require storing a 13 GB KV cache, which is equivalent to the size of the model parameters. The KV\ncache must be transferred from off-chip memory (HBM) (Jia et al., 2018) to on-chip memory (cache) for\neach token generated, leading to a memory bottleneck. This substantial memory demand highlights the\nchallenges in managing large-scale models and the need for efficient memory utilization strategies. Current\nmethods optimize the KV cache based on the sequence length S (Xiao et al., 2023b; Zhang et al., 2024c; Li\net al., 2024) and precision (Hooper et al., 2024; Liu et al., 2024b). We will introduce a new method, Think,\nto optimize it from the perspective of the number of head dimensions D.\nMagnitude base Pruning: Based on the observations in Figure 2 which depicts the significant variation in\nthe magnitudes across different channels, one straightforward criterion is to use the norm of the magnitude"}, {"title": "3.2 Query-Driven Pruning", "content": "For each head, the attention scores are computed using the queries and keys, and then applied to the values.\nThe formula for the attention for head i is:\n$Attention(Q_i, K_i, V_i) = softmax(\\frac{Q_iK_i^T}{\\sqrt{D}})V_i$\nwhere $Q_i, K_i, V_i \\in \\mathbb{R}^{S\\times D}$. When pruning one channels of $K_i$, the corresponding channels in $Q_i$ will also\nbe removed. We aim to find the optimal subset of channels to prune, denoted by the selection matrix\n$S\\in \\{0,1\\}^{D\\times D}$, where S is a diagonal matrix with binary entries (1 for keeping a channel, 0 for pruning\nit). To better maintain the performance after pruning the channels, we minimize the Frobenius norm of the\ndifference between the original and pruned attention weights:\n$min_S ||Q_iK_i^T - Q_iS(K_iS)^T||_F$\nGiven a pruning ratio $\u03bb$, it can further expanded as:\n$min_S ||Q_iK_i^T - Q_iSK_i^T||_F$\nsubject to\n$trace(S) = [(1 - \u03bb)D]$\n$S = diag(s_1, s_2,...,s_D)$, where $s_j \\in \\{0,1\\}$\nFor simplicity, we use greedy algorithm to optimize S. To achieve the pruning goal, we define a criterion for\nevaluating the importance of each channel and greedily select the channels with largest scores:\n$Score[j] = ||Q_i[:, j]K_i[:, j]^T||_F$\n$I_i = Top_T(Score_i, T)$\nHere's a detailed explanation of why it optimizes the selection matrix. The $score_i[j]$ measures the magnitude\nof the interaction between the query and key vectors for channel j in each head i. By selecting channels"}, {"title": "3.3 Implementations", "content": "Following SnapKV, we focus on the long context input sce-\nnario. We opt not to prune the most recent tokens and newly\ngenerated keys. Consequently, our key-value (KV) cache will\nstore two distinct categories of keys: one subset consists of\npruned keys with a reduced channel size, while the other retains\nkeys at their original size. Additionally, we maintain a binary\nmask to indicate which channels have been pruned. Note that\nthe memory overhead associated with this mask is negligible.\nFigure 4 illustrates one implementation of our method during\nthe decoding stage. The pruned keys are first zero-filled using\nthe mask to restore them to their original size, and then con-\ncatenated with the unpruned keys. This implementation can\nbe effectively integrated with optimization techniques such as\nFlashAttention (Dao, 2023). The second implementation in-\nvolves initially pruning the Query using the mask. The pruned\nQuery is then multiplied by the pruned Key, while the un-\npruned Query is applied to the unpruned Key. Subsequently,\nthe two outputs are concatenated. In theory, this implementation has lower computational costs."}, {"title": "4 Experiment Results", "content": "In this section, we conduct extensive experiments to evaluate the effectiveness of Think on performance\nand memory reduction. First, we introduce benchmark datasets, baseline methods, and implementation\ndetails in Section 4.1. Next, we report the performance of our KV pruning method with two different KV\ncompression methods in long-context scenarios in Section 4.2. Finally, we present the results of ablation\nexperiments in Section 4.4."}, {"title": "4.1 Settings", "content": "Benchmark Datasets. We perform evaluations of our method with state-of-the-art KV cache compression\nmethods on two widely used benchmarks: LongBench and Needle-in-a-Haystack. LongBench (Bai et al.,\n2023) is designed to comprehensively evaluate LLM's long context understanding capabilities. It includes\n17 datasets covering six different tasks: single-document QA, multi-document QA, summarization, few-shot\nlearning, synthetic tasks, and code completion. The average input length of LongBench is 6,711 words, which\nnecessitates reducing the KV cache to lower memory usage for inference. Needle-in-a-Haystack (Kamradt,\n2023) is a recent popular test challenge that requires models to accurately identify a small piece of information\n(\"needle\") in a long document (\"haystack\"), where the needle is placed at a random position. This challenge\ncan test if KV cache compression methods still retain the small piece of critical information."}, {"title": "4.2 Results on LongBench", "content": "Tables 2 and 3 present the results of KV compression methods and KV compression methods integrated with\nour channel pruning method of Key cache (Think) over two different base LLMs across various KV-sizes\non LongBench. The following observations can be drawn: (1) Our method can further prune the channels of\nthe key cache after compressing the KV cache with H2O and SnapKV. For the base model LLaMA3-8B, our\nmethod reduces memory usage and slightly improves performance for both H2O and SnapKV. For the base\nmodel Mistral-7B, our method reduces memory with only a slight drop in performance in some cases. (2)\nComparing the results of SnapKV or H2O integrated with Think in Table 2 to SnapKV or H2O integrated\nwith $l_1$ or $l_2$ norm shown in Table 1, our proposed query-based channel pruning shows better performance\nwhen the pruning ratio is 40% compared to no pruning. However, $l_1$ and $l_2$ norm methods cannot outperform\nthe KV compression methods without pruning. (3) When the pruning ratio is small, performance tends to\nbe better than when the pruning ratio is large. (4) When the KV-size is increased from 128 to 2048, the\nperformance of our channel pruning method improves. Notably, with a KV cache size of 2048 and a pruning\nratio of 40%, our method can even outperform the LLaMA3-8B with a full KV cache. The above observations\nindicate that our method is agnostic to existing KV cache compression methods and can further improve\ntheir performance and memory reduction. Additionally, query-based pruning is more effective than $l_1$ and\n$l_2$ norm for channel pruning in LLMs."}, {"title": "4.3 Results on Needle-in-a-Haystack", "content": "Table 4 presents the results of the Needle-in-a-Haystack test based on the SnapKV (Li et al., 2024) approach\nwith KV cache size of input prompt from 128 to 2048. With a modest pruning ratio $\u03bb$ = 40%, the accuracy of\nThink is consistently better or comparable than the original SnapKV across both LLaMA3 and Mistral with"}, {"title": "4.4 Ablation Studies", "content": "Impact of Different Recent Sizes. Preserve the most recent KV embeddings (Zhang et al., 2024c; Li\net al., 2024) is important for maintaining the performance of LLMs after KV cache compression. Note that\na tradeoff exists: increasing the recent-size allows more infomation to be propagated, while increasing the\ncache size to be stored. To study its impacts, we evaluate the performance produced by three recent-size,\nnamely 0, 32 and 128, on LongBench, using Mistral-7B-Instruct-v0.2 as the basleline model. The results are\nsummarized in Table 5. One can observe that a recent-size of 32 yields superior performance than 0 in terms\nof averaged score on LongBench which demonstrates the importance of keeping the most recent KVs. On"}, {"title": "5 Related Work", "content": "In scenarios involving long contexts, the most significant computational burden from the attention mechanism\nis the key-value (KV) cache. Reducing the KV cache is a high priority for optimizing deployment efficiency.\nSystem-level optimizations, such as FlashAttention (Dao, 2023) and PagedAttention (Kwon et al., 2023),\nhave been developed to address this issue. Additionally, algorithm-level optimizations are being explored to\nfurther enhance efficiency.\nKV Cache Eviction. StreamingLLM (Xiao et al., 2023b) maintains a few initial tokens and some recent\ntokens based on the observation of attention sink, which may result in the loss of important information\ncarried by the dropped tokens. H2O (Zhang et al., 2024c) retains only a small portion of the tokens by\ngreedily dropping tokens based on their contributions to the cumulative attention. SnapKV (Li et al., 2024)\nselects clustered important KV positions for each attention head from an 'observation' window located at the\nend of the prompts. FastGen (Ge et al., 2023) adaptively evicts tokens from attention heads that emphasize\nlocal contexts. This approach focuses on discarding non-special tokens centered on special tokens, while the\nstandard KV cache is used only for attention heads that broadly attend to all tokens. PyramidKV (Zhang\net al., 2024b) and PyramidInfer (Yang et al., 2024) considers adjusting the KV cache size across different\nlayers by allocating more cache in the lower layers and less in the higher ones.\nKV Cache Quantization. SmoothQuant (Xiao et al., 2023a) can quantize the KV cache to 8-bit with\nminimal performance degradation. Q-Hitter (Zhang et al., 2024c) uses accumulated attention scores and\n\u2018Quantization Friendliness' metrics to identify tokens that are essential for maintaining the generalization\ncapabilities of LLMs and are suitable for KV cache quantization. Some studies have found that the key\ncache and value cache should be quantized differently (Liu et al., 2024b; Hooper et al., 2024): the key cache\nshould be quantized per-channel, while the value cache should be quantized per-token.\nStructured Pruning of LLMs. Structured pruning (Ma et al., 2023; Ding et al., 2023) of LLMs tra-\nditionally focuses on removing unimportant layers, heads, and hidden dimensions, which often results in\nsignificant performance degradation. In contrast, our methodology preserves the original architecture of the\nLLM and specifically targets the channel dimension within each head's key cache. By dynamically identify-\ning unimportant channels based on data dependant criterion, our approach greatly reduce the key cache-size\nwith negligible performance loss."}, {"title": "6 End Note and Future Direction", "content": "Motivated by the observation that certain channels have significantly larger magnitudes compared to others,\nand the singular value analysis indicates that the key cache is inherently low-rank (Singhania et al., 2024),\nwe propose Think to perform pruning over the key cache channel. The proposed pruning strategy is query-\ndependant and optimized based on the attention scores, ensuring that essential information is retained for\neach input query. In addition, Think can be seamlessly integrated with other popular token-level KV\ncache quantization techniques (Li et al., 2024; Zhang et al., 2024c), further enhancing inference efficiency.\nExtensive experiments on LongBench (Bai et al., 2023) and Needle-in-a-Haystack tests with two foundation\nmodels demonstrate the effectiveness and robustness of our query-dependent channel pruning method. Our\napproach achieves comparable or superior performance to baseline methods while reducing the key cache\nsize by 40%. Our analysis indicates that Think can maintain superior performance over baselines with a\nsmaller KV cache size under equivalent memory consumption conditions.\nFuture work will focus on strategies to increase the pruning ratio without compromising model performance,\nand exploring pruning techniques for value caches. We also plan to evaluate channel importance from a more\nsophisticated and compositional perspective, incorporating both token-level and channel-level information"}, {"title": "A Additional Experimental Details", "content": ""}, {"title": "A.1 Value Cache Pruning", "content": "Similar to the approach used for the Key cache, the pruning of channels in the Value cache can be guided\nby two primary criteria: magnitude-based pruning and query-driven pruning. We find that query-driven\npruning is still better than magnitude based pruning.\n$Score_{v,i} (Q_i, K_i, V_i) [j] = ||softmax(\\frac{Q_i[-S_{obs}:]K_i^T}{\\sqrt{D}})V_i[:, j]||_F$\n$I_i = Top_T(Score_{v,i}, T)$\nwhere $Q_i, K_i, V_i \\in \\mathbb{R}^{S\\times D}$. We define a criterion $Score_{v,i}$ to indicate the importance of each channel in the\nhead i of value cache. Then, only top T channels are retained."}]}