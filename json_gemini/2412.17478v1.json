{"title": "Signal Transformation for Effective Multi-Channel Signal Processing", "authors": ["Sunil Kumar Kopparapua"], "abstract": "Electroencephalography (EEG) is an non-invasive method to record the electrical activity of the brain. The EEG signals are low bandwidth and recorded from multiple electrodes simultaneously in a time synchronized manner. Typical EEG signal processing involves extracting features from all the individual channels separately and then fusing these features for downstream applications. In this paper, we propose a signal transformation, using basic signal processing, to combine the individual channels of a low-bandwidth signal, like the EEG into a single-channel high-bandwidth signal, like audio. Further this signal transformation is bi-directional, namely the high-bandwidth single-channel can be transformed to generate the individual low-bandwidth signals without any loss of information. Such a transformation when applied to EEG signals overcomes the need to process multiple signals and allows for a single-channel processing. The advantage of this signal transformation is that it allows the use of pre-trained single-channel pre-trained models, for multi-channel signal processing and analysis. We further show the utility of the signal transformation on publicly available EEG dataset.", "sections": [{"title": "1. Introduction", "content": "Electroencephalography (EEG) is a method to record the spatial electrical activity of the brain non-intrusively. Typically the instrument used to record EEG has several electrodes mounted on a cap which when placed on the head can sense the electrical activity of the brain at the location or in the spatial vicinity of the electrode. Each electrode captures the brain activity underneath the electrode at a low sampling rate, typically between 250 and 1000 Hz. As a consequence if one uses an EEG instrument with n electrodes, there are n-channels of signals that are simultaneously recorded from each of electrode and are time synchronized. So any analysis on the EEG signal implies processing all the n electrode output signals together to exploit the property that these signal are time synchronized. This, EEG signal, can be considered as an example of a multi-channel signal.\nThere are several disadvantages and challenges when processing multi-channel signals, like EEG. For example, to mention a few, (a) multi-channel data involves multiple signals recorded simultaneously, which makes it more difficult to identify relevant patterns and interpret results compared to single-channel signals, (b) processing and analyzing multi-channel data requires more computational resources and time longer processing times, (c) different channels may be correlated due to shared sources of brain activity or artifacts, throwing up challenges during analysis, use of techniques to disentangle the signals, (d) interpretation of results from multi-channel EEG can be difficult, relating the observed activity in different channels to specific cognitive processes or brain regions, (e) lack of standardization in terms of the number of electrodes used to record EEG data or the sampling rate at which the signal is digitized leads to variability in results across studies; making it difficult to compare findings or even build pre-trained foundational models as is common in single-channel signals like audio and speech, (f) a risk of overfitting models to the data, especially if the number of data samples is not sufficiently large relative to the number of channels, leading to poor generalization while analyzing unseen data, and (g) visualizing multi-channel EEG data requires methods to represent both the spatial and temporal aspects in a way that is interpretable. In this paper, we propose a simple yet effective signal transformation that overcomes some of these disadvantages of processing and analyzing a multi-channel signal. The proposed transformation allows for representation of a multi-channel signal as a single-channel signal where the transformed signal retains the properties of the original multi-channel signal, making the transformation reversible. The single-channel signal not only allows for better visualization of the signal, but also allows for use of pre-trained foundational models that are available in plenty for single-channel signals, like speech and audio.\nThe multi-channel EEG signals are low-frequency (low-bandwidth),ranging from 0.1 Hz to about 100 Hz, signals. For the purposes of brain activity analysis, EEG signals are typically divided into different frequency bands, namely, \u03b4, \u03b8, \u03b1, \u03c3, \u03b2, and y. These well researched frequency bands have been found to exhibit certain characteristics as shown in Table 1, for example an alert and active brain effects the \u1e9e band. These frequency bands are essential in EEG signal processing to understand different brain states and activities based on the patterns observed in these frequency ranges. Analyzing and processing multi-channel EEG data often requires specialized knowledge and training in signal processing, and neuroscience.\nIt is well known, from basics of signal processing literature that signals need to be sampled at twice the maximum frequency present in the signal (also called the Nyquist rate [8]) to preserve the signal characteristics. Since the frequency bands of interest (0.1 to 100 Hz) are low, most EEG instruments sample the brain activities at rates, typically in the range of 250 to 1000 Hz. Observe, in contrast, music, speech and audio signals are sampled at 44.1 kHz for CD quality and typical sampling rates used in speech and audio signal processing is either 8 or 16 kHz. Formally, the Nyquist rate is a fundamental concept in signal processing, which refers to the minimum rate at which a signal must be sampled to accurately represent the signal without losing any information in terms of fidelity. Specifically, the Nyquist rate is twice the highest frequency (bandwidth) of the signal [8]. Our motivation to explore a method for transforming a multi-channel low-bandwidth signal into a single-channel high-bandwidth signal comes from two key observations:\n1. Lack of Pre-trained Models for Low-bandwidth multi-channel EEG: There are currently no large pre-trained models specifically designed for low-bandwidth EEG signals. This is largely due to the lack of standardization in multi-channel EEG data, which varies both in terms of the number of electrodes used and the sampling rates.\n2. Availability of Pre-trained Models for High-bandwidth single-channel Signal: In contrast, there are several well-established pre-trained foundational models, such as VGGish, that are available for high-bandwidth single-channel audio signals. These models can be leveraged for various applications, making them highly useful.\nBy developing a mechanism to transform a low-bandwidth multi-channel EEG signal into a single-channel high-bandwidth signal, we aim to bridge this gap and potentially enable the use of existing pre-trained models for EEG analysis. Focusing on the question, \"Can we use existing pre-trained single-channel (audio or speech) models to analyze or process multi-channel (EEG) signals?\", in this paper, we propose a transformation, based on simple signal processing, that allows representing a multi-channel low-bandwidth EEG signal as a single-channel high-bandwidth signal. The proposed transformation is bi-directional (reversible) meaning that the single-channel high-bandwidth signal can be converted back into the original multi-channel low-bandwidth EEG signals without any loss of information. This is the main contribution of the paper. The rest of the paper is organized as follows. We describe the proposed transformation to convert a multi-channel low-bandwidth signal into a single-channel high-bandwidth signal in Section 2, we show in Section 3 the utility of such a transformation by conducting a set of experiments on a publicly available EEG dataset for odour and subject classification. We conclude in Section 4 and provide future directions."}, {"title": "2. Transforming a multi-channel signal to a single-channel signal", "content": "Consider a p-channel EEG signal (multi-channel, p electrodes), of duration T (in seconds) sampled at a sampling frequency of f, Hz. Let {e;[n = 1,\u2026, N]}=1 denote the p-channel EEG signal where e\u2081[n] denotes the nth sample in the ith channel and N = T \u00d7 fs. The maximum frequency component in ei[n] is (fs/2) which is also the bandwidth occupied by each of the individual p-channel's. Note that the cumulative bandwidth of the p-channel EEG signal, if one were to stack the p-channel's one on top of other is (p\u00d7 fs/2), though each individual channel has a bandwidth of (f/2). The essential idea of the proposed signal transformation is to transform the p-channel EEG signals into a single-channel signal while retaining the individual characteristics of all the p channels of the p-channel EEG signal.\nConsider s[n'] to be the transformed signal, sampled at a sampling frequency of Fs, namely, the bandwidth of s[n'] is (Fs/2). We now describe a process to transform {e;[n: 1,\u2026, N]}=1, the p-channel EEG signal, each channel having a bandwidth of (f/2) into a single-channel s[n'] having a bandwidth of (Fs/2), namely,\n$\\{e_i\\}_{i=1}^{P} \\implies s.$ (1)\nNote that the transformation is bi-directional, namely, we are able to reconstruct {e;}=1 from s when there is a priori knowledge of how s was obtained from {e;}=1. The complete implementation of constructing s from {e;} is shown in Algorithm 1. We now describe, in detail, the transformation process based on simple signal processing. There are essentially four steps in the transformation process.\nStep #1 For each of the p channels, we compute the well known Fast Fourier Transform (FFT) as shown in Line 9, Algorithm 1, namely,\n$\\xrightarrow{FFT} e_i[n]  E_i[k]$ where, $E_i[k] = \\sum_{n=0}^{N-1} e_i[n] \\exp^{-j\\frac{2\\pi}{N}kn},$ (2)\nfor k = 0, \u2026, (N \u2212 1) and has the same number of samples as e\u2081. Note that the maximum frequency component in E; is (f/2) and the kth sample, E\u2081[k] corresponds to the amplitude of the frequency component $(k\\times\\frac{f_s/2}{(N-1)})$ Hz in the signal e\u2081.\nStep #2 Now we stretch E\u2081 so that the stretched Estretch has a maximum frequency of (Fs/2p). This is essentially an index stretching operation, where we take E\u00a1[k] where k runs from 1,\u2026, N and map those indices to a new set of indices, namely, k' which runs from 1,\u2026,\u03b1N, where \u03b1 = (Fs/2p). As captured in Line 11, Algorithm 1, the frequency domain signal E\u2081[k] is re-indexed to construct Estretch [k'], namely,\n$\\xrightarrow{k'=ak} E_i[k]  E_i^{stretch}[k']$ (3)\nwhere \u03b1 = (Fs/2p) and k = 0,\u2026,(N \u2212 1). Note that the N samples in E\u2081 would result in \u03b1N samples in Estretch, namely, k' = 0,\u2026, \u03b1(N \u2212 1). Observe that the value of Estretch [k'] is the same as the value of the kth sample, E\u2081[k]. However, Estretch[k'] corresponds to the the frequency component $(\u03b1\\times k\\times(\\frac{f_s/2}{(N-1)}))$ Hz while E\u2081[k] corresponds to the frequency component $(k\\times(\\frac{f_s/2}{(N-1)}))$ Hz. While the maximum frequency component in E\u2081[k] is (f/2), the maximum frequency component in Estretch is (Fs/2p) This makes sure that when all the p-channels are stacked together, the combined bandwidth of the p-channel's is (F/2). The process of stretching or reindexing from the implementation perspective is best understood with the help of Algorithm 1.\nStep #3 We now concatenate or stack the p-channels of $\\{ E_i^{stretch}[k']\\}_{i=1}^{p}$ on top of one another as shown in Figure 2 to form a single-channel signal of bandwidth (F5/2). Namely,\n$\\xrightarrow{stack} \\{E_i^{stretch}[k']\\}_{i=1}^{p} \\rightarrow S[k'].$ (4)\nNote that we could stack the p-channels in p! different ways. However, in our implementation, without loss of generality, we choose to stack the p-channels such that the (i + 1)th channel was stacked on top of the ith channel as shown in Figure 2. Line 12 to 21 in Algorithm 1 captures the implementation of the stacking $\\{E_i^{stretch}[k']\\}_{i=1}^{p}$, to construct S[k'].\nStep #4 As the last step in the signal transformation process we compute the inverse FFT (IFFT) of S[k'] to construct the time domain signal s[n'] (Line 23, Algorithm 1), namely,\n$\\xrightarrow{IFFT} S[k']  s[n']$ where, $s[n'] = \\frac{1}{N}\\sum_{k'=1}^{N}S[k'] \\exp^{\\frac{j2 \\pi}{N}k'n}$ (5)\nNote that the bandwidth of the time domain single-channel signal s[n'] is (F/2).\nlinspace(0, fs, N') (Line 6) is a well known NumPy function that generates N' evenly spaced values between 0 and f\u2081. The whole process of stacking or reindexing (4) is implemented as shown from Line 12 to 21 which allows to transfer the amplitude associated with the stretched ith (Line 11) channel frequency of the EEG signal to the closest frequency available in the desired signal, S obtained using (Line 6) described earlier. It should be noted that by reversing the sequence of operations seen in (6), we can get back {e;[n]}_1 from s[n']. Namely,\n$\\xrightarrow{FFT} s[n']  S[k'] \\xrightarrow{unstack} \\{E_i^{stretch}[k']\\}_{i=1}^{P} \\xrightarrow{k=(1/a)k'} \\{E_i[k]\\}_{i=1}^{P} \\xrightarrow{IFFT} \\{e_i[n]\\}_{i=1}^{P}$ (7)\nTypical values of f is around 250 Hz (EEG signal) while typical F\u00b8 is 8 or 16 kHz (audio). We hypothesize that s[n'] constructed as described in this section represents all the information in the p channel's, namely, {e;[n]}_1 in a single-channel signal, namely, s[n']. The process of this multi-channel to single-channel transformation allows one to (a) process a single-channel signal s[n'] instead of processing all the p-channels {e;[n]}=1, (b) make use of any of the known audio processing tools and algorithms, especially when F, is chosen to be 8 or 16 kHz, and more importantly (c) allows the use of existing pre-trained models trained on single-channel signal.\nFortunately, unlike multi-channel signals, there are quite a few pre-trained models for single-channel signals. For example, VGGish [4] is a popular pre-trained model that has been used for audio classification tasks, YAMNet [11] is another pre-trained deep learning model for sound event detection, DeepSpeech [3] and Wav2Vec [12] are popular pre-trained models used for speech recognition tasks. While almost all of the pre-trained models have not been trained on EEG like data, we reckon that they can be used to extract embeddings or features from the transformed signal s which can be treated as features corresponding to all the p channels of EEG, namely, {e;}=1"}, {"title": "3. Experimental Analysis", "content": "We use the widely used, publicly available odour-EEG dataset (EEGDOT) [7] for training (a) odour-classification [10] system and (b) subject identification [9] system. The EEGDOT dataset consists of a 32 channel EEG signal recorded using the Cerebus system from 11 healthy individuals (8 males and 3 females), right-handed, aged 24.9 \u00b1 3.0 years in response to 13 odour stimuli (rose, caramel, rotten smell, canned peaches, excrement, mint, tea, coffee, rosemary, jasmine, lemon, vanilla, and lavender). Two of the 32 channels are reference channels, making it, p = 30 usable channels for analysis. The electrodes for collecting EEG are arranged according to the international 10-20 system, and sampled at f\u2081 = 1 kHz. Each sample collected was for a duration of 10 seconds (N = 10000 samples), called a trial. In total, EEGDOT dataset has 11 (Subjects) \u00d7 13 (odours) \u00d7 35 (trials) resulting in a total of 5005 30-channel EEG samples. We partition the EEGDOT dataset into non-intersecting 80: 20 train and test sets. The training set consists of 4004 samples while the remaining 1001 samples were used for test. We retained the split in all our experiments for consistency in experiments.\nWe converted the 30-channel (p = 30) EEG signal into a single-channel high bandwidth audio signal of F\u2081 = 16 kHz as described in the previous section following Steps #1, #2, #3, and #4 implemented as mentioned in Algorithm 1. In the first set of experiments, using the single-channel high bandwidth (16 kHz) transformed signal, we extracted the spectrogram (for example, Figure 1 (middle) is the spectrogram of the e\u2081) using 64 msec (1024 samples) as the window and 48 msec (768 samples) as the overlap. The resulting size of the spectrogram was 513 \u00d7 621, which is a representation of all the 30 EEG channels simultaneously. So each EEG trial was represented by a spectrogram matrix of size 513 \u00d7 621. We used a Convolutional Neural Network (CNN) architecture [5] as shown in Table 2 for the purpose of odour (13 class, 157945997 trainable parameters) and subject (11 class, 157945739 trainable parameters) classification. The CNN was implemented in keras and no hyper parameter tuning was done. As is common, 20% of the train data was used for the purposes of validation. So we had 3203, 801, 1001 samples for training, validation and test respectively with no overlap.\nThe CNN architecture (Table 2) with a batch size of 32 was run for 150 epochs for odour classification. The training and validation losses (Figure 3a) and accuracy is shown in Figure 3b. The test accuracy for 13 class odour classification was 51.85%. The CNN architecture (Table 2) with a batch size of 32 was run for 25 epochs for subject (11 class) classification. The training and validation losses are shown in Figure 3c while the accuracy is shown in Figure 3d for subject classification. The test accuracy for 11 class subject classification was 99.70%. As seen in Table 3, we achieve the best performance for both odour and subject classification when a single-channel s is used instead of the multi-channel {e;}30. Also seen in Table 3 are the performance of SVM (27.63%) and RF (29.85%) classifiers on hand crafted features for odour classification as reported in [10] and subject classification [9]. The total number of features used were 720 which included energy, entropy, discrete wavelet transform for each of the bands.\nAs mentioned earlier, one of the benefits of transforming a multi-channel EEG signal into a single-channel high bandwidth signal is that it allows the use of existing pre-trained models trained on massive amounts of single-channel data. The use of pre-trained models allows to extract embeddings from these models which"}, {"title": "4. Conclusions", "content": "In this paper, we proposed a simple yet effective signal processing method to transform a multi-channel low bandwidth EEG signal into a single-channel high bandwidth signal. The signal transformation is such that it retains all the properties of the original multi-channel signal in the single-channel signal; further the transformation is bi-directional, meaning it allows for reconstructing the multi-channel signal for the single-channel signal. This is the main contribution of this paper. We motivated the need for such a transformation, primarily because the signal transformation allows for a comprehensive, time synchronized view of the multi-channel signal in a single-channel waveform. This is useful, especially in an end-to-end deep learning setup, because there is no extra burden on the end-to-end deep learning setup to learn the association between different channels in the multi-channel signal. Single-channel representation derived from the multi-channel signal allows for all the channels to be simultaneously available for processing. Another motivation is that this transformation enables exploit the use of existing pre-trained single-channel acoustic models especially because of the absence of any such pre-trained models for multi-channel signals like EEG. Experimental results show that the single-channel high bandwidth signal is able to retain the properties of the EEG signal very effectively; indicated by the superior performance for both odour and subject classification compared to hand crafted features extracted from multi-channel EEG signal."}, {"title": "4.1. Discussion", "content": "While we have experimented with varying number of convolution layers, different convolution sizes, and varying number of nodes in the dense layers and presented the best performing architectures, we have not experimented with hype-parameter tuning. There is plenty of scope for further experimentation using different deep learning architectures and hype-parameter tuning. However, as emphasized earlier, the focus of this paper was on demonstrating that a simple signal processing transformation allowed for a better representation of a multi-channel signal.\nThe use of pre-trained models trained on acoustic data for acoustic event detection of classification task in the context of EEG data does throw up several questions including the relevance of these pre-trained models for EEG data processing. However, listening to the transformed signal (s) derived from (e\u2081) made us believe that the transformed signal might have some relevance to acoustic events. The experimental results seem to demonstrate that the pre-trained models are able to extract embeddings that seem to capture the odour and the subject characteristics in the transformed signal. As future work, the single-channel data is neither speech nor audio but converting EEG multi-channel data into a single-channel data, as mentioned in this paper, might allow building models which best work for EEG data.\nWhile both the pre-trained models have been trained for downstream acoustic event detection or classification, it is not clear why one works better than the other. While the focus in this paper was to use the pre-trained models as a black box to extract embeddings or features and not to looking at the architecture or training of the pre-trained models, as future work, it might be a good idea to understand and correlate the performance of odour and subject classification to the architecture and training data associated with the pre-trained model."}]}