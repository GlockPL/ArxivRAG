{"title": "A UNIFIED FRAMEWORK FOR NEURAL COMPUTATION AND LEARNING OVER TIME", "authors": ["Stefano Melacci", "Alessandro Betti", "Michele Casoni", "Tommaso Guidi", "Matteo Tiezzi", "Marco Gori"], "abstract": "This paper proposes Hamiltonian Learning, a novel unified framework for learning with neural networks \u201cover time\", i.e., from a possibly infinite stream of data, in an online manner, without having access to future information. Existing works focus on the simplified setting in which the stream has a known finite length or is segmented into smaller sequences, leveraging well-established learning strategies from statistical machine learning. In this paper, the problem of learning over time is rethought from scratch, leveraging tools from optimal control theory, which yield a unifying view of the temporal dynamics of neural computations and learning. Hamiltonian Learning is based on differential equations that: (i) can be integrated without the need of external software solvers; (ii) generalize the well-established notion of gradient-based learning in feed-forward and recurrent networks; (iii) open to novel perspectives. The proposed framework is showcased by experimentally proving how it can recover gradient-based learning, comparing it to out-of-the box optimizers, and describing how it is flexible enough to switch from fully-local to partially/non-local computational schemes, possibly distributed over multiple devices, and BackPropagation without storing activations. Hamiltonian Learning is easy to implement and can help researches approach in a principled and innovative manner the problem of learning over time.", "sections": [{"title": "1 Introduction", "content": "Motivations. A longstanding challenge in machine learning with neural networks is the one of designing models and learning strategies that are naturally conceived to learn \u201cover time\u201d, progressively adapting to the information from a stream of data [1, 2]. This implies dealing with possibly infinite streams, online learning, no access to future information, thus going beyond classic statistical methods exploiting offline-collected datasets. In this paper, tools from optimal control theory [3] are exploited to rethink learning over time from scratch, proposing a unifying framework named Hamiltonian Learning (HL). Differential equations drive the learning dynamics, which are integrated going \"forward\" in time, i.e., without back-propagating to the past. Although related tools are widely used (in a different way) in reinforcement learning [4], and there exist works exploiting control theory in the context of neural networks [5, 6, 7], to the best of our knowledge they have not addressed the general problem of learning over time from a continuous, possibly infinite, stream of data. A few recent exceptions do exist, but they are focused on specific cases with significant limitations [8, 9]. HL is rooted on a state-space formulation of the neural model, that yields computations which are fully local in space and time. The popularity of state-space models has become prominent in recent literature"}, {"title": "2 Preliminaries", "content": "Notation. At time $t \\geq 0$ data $u_t \\in \\mathbb{R}^d$ is provided to a neural network (for inference and learning), with learnable parameters collected into vector $\\theta$, with $t \\in [0, N)$, $N > 0$, that could be possibly $\\infty$. Our framework is devised in a continuous-time setting, which, as usual, is implemented by evaluating functions at discrete time instants, that might be not evenly spaced. We avoid introducing further notation to formalize the transition from continuous-time differential equations to the outcome of the discrete-time integration steps, which will be clear from the context. Vectors are indicated in bold (they are column vectors\u2013lowercase), where $\\cdot^\\top$ is the transpose of $\\cdot$. The notation $[o, u]$ is the concatenation of the comma-separated vectors in brackets. Given an $m$-real-valued function, e.g., $r(u)$ returning vector $o$, the Jacobian matrix with respect to its argument of length $d$ is a matrix in $\\mathbb{R}^{m \\times d}$. As a consequence, gradients of scalar functions ($m = 1$) are row vectors. The only exception to this rule is in the case of derivatives with respect to time, indicated with $\\dot{o}$, that are still column vectors, as $\\dot{o}$. We will frequently follow the convention of writing $o$ in place of $r(u, h)$, keeping track of the dependencies with respect to the arguments of $r$. Dependencies do not propagate through time. The $\\odot$ operator is the Hadamard product.\nFeed-Forward and Recurrent Networks. A feed-forward neural model (including convolutional nets) is a (possibly deep) architecture that, given input $u$, computes the values of the output neurons as $h = f(u, \\theta)$. Recurrent models are designed to process finite sequences of samples, such as $u_0, u_1, .., u_{n-1}$, where we can evaluate the output values of the recurrent neurons at step $\\kappa + 1$ as,\n\\[ h_{\\kappa+1} = f(u_\\kappa, h_\\kappa, \\theta), \\quad \\text{for } \\kappa = 0, 1, ..., n - 1. \\] \nIn this case, computations depend on the initial $h_0$ which is commonly set to zeros or to random values [14]. Notice that $\\theta$ is not a function of $\\kappa$, since the same weights are used to process the whole sequence. Moreover, Eq. 1 is a generalization of feed-forward networks, which are obtained by keeping $\\kappa$ fixed and setting the second argument of $f$ to 0 (vector of zeros). Learning consists in determining the optimal value of $\\theta$, in the sense of minimizing a given loss function. For example, in the case of classic online gradient, weights are updated after having processed each example (feed-forward nets), or each sequence (recurrent nets). Formally, at a generic step $v$,\n\\[ \\theta_{v+1} = \\theta_v - \\gamma \\partial_{\\theta} \\] \nwhere $\\gamma > 0$ is the learning rate and $-\\gamma \\partial_{\\theta}$ is the variation.\nState and Stream. In this paper we consider an extended definition of state with respect to the one used in recurrent networks, $h$. In particular, the state here is $[h, \\theta]$, which represents a snapshot of the model at a certain time instant, including all the information that is needed to compute Eq. 1 when an input is given. Differently from dataset-oriented approaches, here we consider a unique source of information, the stream $S$, that, at time $t$, yields an input-target pair, $S_t = (u_t, \\hat{y}_t)$,\u00b9. We will avoid explicitly mentioning batched data, even if what we present in this paper is fine also in the case in which $S$ returns mini-batches. In practice, $S$ yields data only at specific time instants $t_1, t_2, .., t_{o \\infty}$, which may be evenly or unevenly spaced. To keep the notation simple, we will assume a consistent spacing of $\\tau$ between all consecutive samples, although our proposed method does not require evenly spaced samples (see also Appendix A)."}, {"title": "3 Hamiltonian Learning", "content": "Control theory is focused on finding valid configurations (controls) to optimally drive the temporal dynamics of a system. In the case of neural networks, we aim at finding the \u201coptimal\u201d way to drive the predictions of the model over time, by controlling the changes in $\\theta$. This section presents HL by introducing the main involved components one after the other, and finally describing the differential equations at the core of HL. In order to keep HL accessible to a larger audience, we skip those formal aspects that are not explicitly needed to discuss HL from an operational perspective (see Appendix B for a formal description of control theory).\nState-Space Formulation. Following the formalism of continuous-time state-space models [12], we split the computations of the network as the outcome of two sub-networks: the neuron state network, implementing function $f_h$, is responsible of computing how $h$ changes over time; the output network, implementing function $f_y$, is a recurrence-free portion of the network that receives data from $h$ and, if needed, from the input units, propagating information to the output ones, as sketched in Fig. 1. Formally,\n\\[ \\dot{h_t} = f_h (u_t, h_t, \\theta) \\\\ \\dot{y_t} = f_y (u_t, h_t, \\theta) \\\\ \\dot{\\theta_t} = \\beta \\theta_t \\] \nwhere $\\theta^h$ and $\\theta^y$ are weights of the two networks, both of them function of time, with $\\theta_t = [\\theta_t^h, \\theta_t^y]$. Eq. 5 formalizes the dynamics of the weights, that will change over time. The term $\\beta$ is a customizable vector of positive numbers of the same size of $\\theta$, that can be used to tune the relative importance of each weight (it can be written also as function of time), while $\\theta$ is the unscaled weight velocity. Computing $h$ (resp. $\\theta$) requires us to integrate Eq. 3 (resp. Eq. 5). When using the explicit Euler's method with step $\\tau$, replacing the derivative with a forward finite difference, we get\n\\[ h_{t+\\tau} = h_t + \\tau \\dot{h_t}, \\qquad \\theta_{t+\\tau} = \\theta_t + \\tau \\dot{\\theta_t}, \\]"}, {"title": "4 Recovering Gradient-based Learning", "content": "HL offers a wide perspective to model learning over time with strong locality, that will be showcased in Section 5. However, in order to help trace connections with existing mainstream technologies, in this section we neglect the important locality properties of HL to clarify the relations to the usual gradient-based minimization by BackPropagation (BP) or BackPropagation Through Time (BPTT) [17]. BPTT explicitly requires to store all the intermediate states, while BP assumes instantaneous propagation of the signal through the network. First, we show how to recover gradient-based learning in feed-forward networks (BP), presenting two different ways of modeling them in HL, either by means of the output network or of the state one. Then, we do the same for recurrent networks, where, in addition to the state/output network based implementations, we also show how HL can explicitly generalize BPTT by tweaking the way data is streamed.\nFeed-Forward Networks (Output Nets). Implementing a feed-forward net by means of Eq. 4 sounds natural due to the static nature of the output map $f_y$. The notion of neuron state is lost, thus $h_t = 0, \\forall t$, and, consequently $H'$ of Eq. 9 boils down to the usual loss function. The costate-related HE of Eq. E*4 (lower part), due to our choice of $s = -1$, is the gradient of the loss with respect to the weights (let us temporarily discard the dissipation term). When integrating the output-net-related HEs, Eq. E*4, Eq. E+2, respectively, setting $H' = L$ and $s = -1$, we get\n\\[ \\omega_{t+\\tau} = \\omega_t + \\tau \\eta \\frac{\\partial L(y_t, \\hat{y}_t)}{\\partial \\theta}^\\top, \\quad \\dot{\\theta_t} = -\\tau \\beta \\odot \\omega_t. \\]\n\\[ \\tilde{\\theta}_{t+\\tau} = \\tilde{\\theta}_t - \\tau \\beta \\odot \\omega_t = (1 - \\tau \\eta) \\tilde{\\theta}_t + \\tau \\phi_t \\frac{\\partial L(y_t, \\hat{y}_t)}{\\partial \\theta}^\\top. \\]\nEq. 11 is a momentum-based computation of the gradient of $L$ with respect to $\\theta$ [18], where $1 - \\tau \\eta$ is the momentum factor and $\\tau \\phi_t$ is a time-dependent dampening coefficient. Eq. 12 is the classic weight update step, as the one we introduced in Eq. 2, with (per-parameter) learning rate $\\tau \\beta$, being $\\beta \\odot$. However, in classic gradient-based learning, a specific order of operations is strictly followed: first gradient computation and, only afterwards, weights are updated with the just computed gradients. Hence, in HL we have to replace $\\omega$ of Eq. 12 with $\\omega_{t+\\tau}$, and then use the definition of $\\omega$, given by Eq. 11. Under this constraint, we get a perfect match with classic gradient-based learning. Notice that when $\\eta = 1/\\tau$, the momentum term disappears. In such a setting, if $\\phi$ is an exponential function, it acts as an exponential scheduler of the learning rate, while if $\\phi$ is constant then no scheduling is applied [19]. Of course, we can also get rid of the momentum term by zeroing the weight-costate, $\\dot{\\omega} = 0$ right before processing the example at time t. In Appendix D we report the exact rules to map learning parameters in momentum based gradient-descent and HL parameters.\nFeed-Forward Networks (State Nets). Implementing a feed-forward network using the state network requires to clear the state $h_t$ and costate $z_t$ for each $t$, setting them to 0, to avoid propagating neuron-level information from previous examples. In this case, $f_y$ is the identity function, and we can exploit the residual-like formulation of Eq. 10 to compensate the effects of the integration procedure (that depends on $\\tau$). In such a setting, we get (details in Appendix E)\n\\[ h_{t+\\tau} = f_h (u_t, h_t, \\theta) = \\tau^{-1} (\\dot{\\theta}_{t+\\tau} - h_t), \\]\n\\[ z_{t+\\tau} = \\frac{\\partial L(h_{t+\\tau}, \\hat{y}_t)}{\\partial h_{t+\\tau}}^\\top, \\]\nwhere we used $h_{t+\\tau}$ as argument of $L$ instead of $h_t$. This choice is the direct counterpart of what we did when enforcing sequential ordering in gradient computation and weight update in the previously discussed implementation. In fact, we want the evaluation of the loss $L$ to happen after the state neurons have been updated ($h_{t+\\tau}$), which is coherent with what happens when performing a prediction first, and only afterward evaluating the loss. Notice that, for the same reason, we also differentiate with respect to $h_{t+\\tau}$. Eq. 13 and Eq. 14 collect the two HEs of Eq. E*1 and Eq. E+3, respectively. In turn, the HE of Eq. E*4 becomes,\n\\[ \\omega_{t+\\tau} = -s \\tau \\phi \\frac{\\partial^2 L(h_{t+\\tau}, \\hat{y}_t)}{\\partial \\theta \\partial h_{t+\\tau}} \\frac{\\partial h_{t+\\tau}}{\\partial \\theta} - \\eta \\omega_t = ( \\phi \\tau \\frac{\\partial L(h_{t+\\tau}, \\hat{y}_t)}{\\partial h_{t+\\tau}} \\frac{\\partial h_{t+\\tau}}{\\partial \\theta}^\\top ) - \\eta \\omega_t, \\]"}, {"title": "5 Leveraging Hamiltonian Learning", "content": "HL is designed to drive learning over time in stateful models in a temporally local manner. While it is general enough to recover the popular gradient-based learning in feed-forward and recurrent nets, as we discussed in Section 4, the importance of dealing with stateful networks goes beyond it. We distinguish among two use-cases where HL can be leveraged to design efficient models that learn over time.\nFully Local Learning. The stateful nature of learning allows us to instantiate networks that are not only temporally local, but also spatially local, once we design $f_h$ to return the variations of the output values of all the its neurons. Given the graph that describes the parent/child relations among neurons, being it Directed Acyclic/Cyclic Graph, all the neurons can compute their outputs in parallel. In fact, they have the use of the full state $h_t$ (and/or the current $u_t$)-computed during the previous time step-to feed their inputs, with important connections to biologically plausible computational models and related studies [21, 22, 23]. This choice introduces delays in the computations, but it yields fully local learning, with known benefits in terms of parallelization [24] and pipeline parallelism in neural networks for multi-device computation [25] (Appendix A)."}, {"title": "6 Related Work", "content": "Optimal Control. Optimal control theory [16] provides a clear framework to handle optimization problems on a temporal horizon (Pontryagin Maximum Principle [30, 31], Dynamic Programming [32]), usually involving boundary-value problems which require iterative forward/backward schemes over the whole considered time interval or working with receding horizon control [33]. Jin et al. [34] considered the possibility of exploiting HEs for learning system dynamics and controlling policies forward-in-time, while [8] evaluated HEs in continual online learning, artificially forcing the costate dynamic to converge to zero. To our best knowledge, the relations between HEs, control theory, and gradient-based approaches were not studied from a foundational perspective in the context of learning from a stream of data with neural networks. The work of LeCun [35] mentions them, indeed, in relation to classical mechanics.\nOnline Learning. There exist several works focussed on learning in an online manner from streamed data, such as in the case of physics-inspired models [36, 37] or approaches to continual online learning [38, 1]. The framework of this paper is devised by revisiting the learning problem from scratch, instead of trying to directly adapt classic statistical approaches typical of offline learning.\nNeural ODE. The methods exploited in this paper are inherited from optimal control, as it is done also in the case of Neural ODE and related works [5, 6, 27, 28, 39]. However, here we focus on the problem of learning over time in an online manner, with a possibly infinite horizon, which is different from what is commonly done in the literature of Neural ODE/CDE.\nReal-Time Recurrent Learning. The classic approach to learning online with recurrent models is RTRL [40], which requires to store, exploit, and progressively update the temporal Jacobian matrix, with high space/time complexities. Several approximations were proposed to reduce the complexity of RTRL (UORO and others, see [41]). In HL, no temporal Jacobian matrices are stored. Hence, it is not a generalization of RTRL and related work (such as Online LRU [42])."}, {"title": "7 Conclusions", "content": "We presented HL, a unified framework for neural computation and learning over time, exploiting tools from control theory. Differential equations drive learning, leveraging stateful networks that are fully local in time and space, recovering classic BackPropagation (and BPTT) in the case of non-local models. HL represents a novel perspective, that might inspire researchers to further investigate learning over time in a principled manner."}, {"title": "A Further Details", "content": "Stream. The perspective of this paper is the one in which learning consists of the online processing of a single stream of data, that could be possibly infinite. In order to convert the classic notion of dataset to the one of stream $S$, we can consider that $S$, at time $t$, yields an input-target-tag triple, $S_t = (u_t, \\hat{y}_t, d_t)$, being $d_t$ a binary tag whose role will be clear in the following. A given dataset of samples can be streamed one sample after the other, possibly randomizing the order in case of stochastic learning, and $d_t = 1, \\forall t$. If samples are sequences, then they are streamed one after the other. However, in this case each single sequence must be further streamed token-by-token, thus $S$ provides a triple with $d_t = 1$ for the last token of the current sequence (otherwise $d_t = 0$), to preserve the information about the boundary between consecutively streamed sequences.\nScheduling. We assume the most basic schedule of computations: the agent which implements our neural networks monitors the stream and, when a batch of data is provided, it starts processing it, entering a \u201cbusy\u201d state, and leaving it when it is done. Data from $S$ is discarded when the agent is busy. At time $t_j$, the agent is aware of the time $\\Delta(t_j)$ that passed from the previously processed sample.\nDistributed Neural Computations. The aforementioned notion of spatial locality can be structured in several different ways. In a multi-layer feed-forward network, we can create groups of consecutive layers, and assume $f_h$ returns the variation of the output values of the last layer of each group, thus the state $h$ is not composed by all the neuron outputs of the net. Computations within each group happens instantaneously, and delays are only among groups. This weakly spatially local architecture corresponds to the one exploited to build pipeline parallelism in neural networks [25], when each group is processed on a different devices/GPUs. Interestingly, the notion of costate in HL is"}, {"title": "B Optimal Control Theory", "content": "A control system consists of a pair $(f, A)$, where $A \\subset \\mathbb{R}^m$ is called the control set and it is the set of admissible values of the control\u2074 and $f : \\mathbb{R}^n \\times A \\times [t_0, +\\infty) \\to \\mathbb{R}^n$ is a continuous function that is called the dynamics of the system. Using the notation $\\x(t)$ instead of subscript t as in we did in the main paper, the state equation associated to the system then is\n\\[\\begin{aligned} \\dot{x(t)} &= f(x(t), a(t), t), \\quad \\text{a.e. in } [t_0, +\\infty) \\\\ x(t_0) &= x \\end{aligned} \\]\nwhere $t_0 \\in \\mathbb{R}$, $x \\in \\mathbb{R}^n$. The function $a$, misurable, is what is usually called the control strategy or control function and we denote with $x(\\cdot ; t_0, x, a)$ the unique solution of Eq. (21).\nThen an optimal control problem consists in choosing the control policy $a$ in such a way that a functional, usually called cost is minimized. In other words the objective of such class of problems is to steer the dynamics of the system so that it performs \u201cwell\u201d according to a certain criteria. In this paper we are interested in what is known as control problem of Bolza type, in which, given a control system $(f, A)$, a terminal cost $g \\in C(\\mathbb{R}^n)$ a time $N > 0$ and a function $l(\\cdot, \\cdot, s) \\in C(\\mathbb{R}^n \\times A \\times [t, N]; \\mathbb{R})$ usually called lagrangian, for all $(t, x) \\in [0, N] \\times \\mathbb{R}^n$ the functional to be minimized is the total cost:\n\\[ C_{t,x}(a) := \\int_{t}^{N} l(x(s; t, x, a), a(s), s) ds + g(x(N;t, x, a)). \\]\nHence, more concisely the problem that optimal control (of Bolza type) is concerned with is\n\\[ \\inf \\{ C_{t,x}(a) \\}\nover all possible control trajectories a: [t, N] \u2192 A.\nHere we briefly summarize some of the classical results that comes from the method of dynamic programming that are most relevant in this work and we refer to the excellent books [32, 51] for additional details, and in general for a more precise and consistent definition of the theory. The main idea behind dynamic programming is to embed the minimization problem (23) into a larger class of such problems through the definition of the value function:\n$V(t, x) := \\inf\\{C_{t,x}(a): a: [t,T] \\to A \\text{ is measurable}\\}$."}, {"title": "C Robust Hamiltonian and Forward Hamiltonian Equations", "content": "Considering the cost of Eq. 7, the Hamiltonian function is the outcome of adding to the instantaneous cost the dot product between state and costate, evaluated at the minimum with respect to the control,\n\\[\\begin{aligned} H(h, \\theta, z, \\omega, \\dot{\\theta}) &= \\eta e^{nt} C'(h, \\theta, \\dot{\\theta}, t) + z^\\top f_h(u, h, \\theta^h) + \\omega^\\top (\\beta \\dot{\\theta}) \\\\ &= \\eta e^{nt} \\Bigg[ L \\big(f_y (u, h, \\theta^v), y\\big) \\phi_t + \\frac{1}{2} ||\\dot{\\theta}||^2 \\Bigg] + z^\\top f_h (u, h, \\theta^h) + \\omega^\\top (\\beta \\dot{\\theta}) \\end{aligned} \\]\nwhere we dropped the time index to keep the notation simple and where, from Eq. 5, $\\dot{\\theta} = \\beta \\dot{\\theta}$. The definition of $H$ remarks that, for each quadruple of arguments in Eq. 26, we are in a stationary point w.r.t. to the control $\\dot{\\theta}$. From now on, we completely avoid reporting the arguments of the involved functions, considering them to be the ones of Eq. 26. We get\n\\[ \\frac{\\partial H}{\\partial \\dot{\\theta}} = \\eta e^{nt} \\dot{\\theta} + \\omega \\beta. \\]\nSetting Eq. 26 to zero (since it is known to be a stationary point), we get,\n\\[ \\dot{\\theta} = - e^{-nt} \\beta \\omega. \\]\nOnce we use Eq. 27 as value of the first argument of Eq. 26, the Hamiltonian becomes,\n\\[\\begin{aligned} H &= e^{nt} L \\phi_t + \\frac{1}{2} e^{-2nt} ||\\beta \\odot \\omega||^2 + z^\\top f_h - e^{-nt} \\omega^\\top (\\beta \\beta \\omega) \\\\ &= e^{nt} L \\phi_t + \\frac{1}{2} e^{-2nt} ||\\beta \\odot \\omega||^2 + z^\\top f_h - e^{-nt} \\omega^\\top (\\beta \\beta \\omega) \\end{aligned} \\]"}, {"title": "D Out-of-the Box Tools vs. Hamiltonian Learning", "content": "Converting Parameter Values. In our experimental comparison, we exploited out-of-the box tools for gradient-based optimization in neural networks. The form of the update step in the case of gradient with momentum we considered\n\\[\\begin{aligned} b_{t++} &= \\mu b_t + (1 - \\mu) g_t \\\\ \\dot{\\theta}_{t++} &= \\dot{\\theta}_t + \\gamma (-b_{t++}) \\end{aligned} \\]\nwhere g is the gradient of the loss with respect to $\\theta$ (with $b_0 = g_0$), while $\\mu, \\rho, \\gamma$ are the momentum term, the dampening factor, and the learning rate, respectively, coherently with the Pytorch implementation of the SGD optimizer, that is what we used in our comparisons (time indices are adjusted following the notation of this paper)."}, {"title": "E Feed-forward Networks and State Net", "content": "When implementing a feed-forward network using the state network, we rely on the instantaneous propagation of Eq. 10. We get\n\\[\\begin{aligned} h_{t+\\tau} &= h_t + \\tau \\cdot \\tau^{-1} \\big(-h_t + f_h (u_t, h_t, \\dot{\\theta}) \\big) \\\\ &= f_h (u_t, h_t, \\dot{\\theta}) \\end{aligned} \\]\nIn order to avoid dependencies from the past, we clear the state, $h_t = 0$ and, since $\\dot{h_t} = \\tau^{-1} \\big(-h_t + f_h(u_t, h_t, \\dot{\\theta})\\big)$, we get $h_t = \\tau^{-1} f_h (u_t, h_t, \\dot{\\theta})$. Comparing this last equation with Eq. 31, we obtain\n\\[ h_{t++} = f_h(u_t, h_t, \\dot{\\theta}) = h_t. \\]\nwhich is the same as Eq. 13. Finally, Eq. 14 is trivially obtained from Eq. E*3, setting $z_t = 0$, to avoid propagating past information."}, {"title": "F Learning in Recurrent Networks", "content": "Learning in Recurrent Neural Networks (RNNs) is usually instantiated with the goal of minimizing a loss function defined on a window of sequential samples whose extremes are $a$ and $b$, with $b > a$, such as $\\sum_{k=a}^b L(h_k, \\hat{y}_k)$, where $h_k$ is the hidden state at step $k$. Weights are constant while processing the data of the window. Let us consider the case of a dataset of sequences, sampled in a stochastic manner, and let us assume that at a certain stage of the optimization the values of the weights of the RNN are indicated with $\\theta$, while $a = 1$ and $b$ is the sequence length. Once gradients $\\sum_{k=a}^b \\frac{\\partial L(h_k, \\hat{y}_k)}{\\partial \\theta}$ are computed, weights are updated following the direction of the negative gradient.\nEven if we avoided explicitly showing the dependence of L on \\theta, to keep the notation simple, the loss function at step k, i.e., $L(h_k, \\hat{y}_k)$, depends on \\theta both due to its direct involvement into the computation of the current hidden state $h_k$ and also through all the previous states, up to $h_a$. When applying the chain rule, following the largely known BackPropagation Through Time (BPTT) [17], we get,\n\\[\\begin{aligned} &\\frac{\\partial \\sum_{k=a}^b L(h_k, \\hat{y}_k)}{\\partial \\theta} = \\sum_{k=a}^b \\frac{\\partial L(h_k, \\hat{y}_k)}{\\partial h_k} \\frac{\\partial h_k}{\\partial \\theta} \\\\ &= \\sum_{k=a}^b \\sum_{q=a}^k \\frac{\\partial L(h_k, \\hat{y}_k)}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_q} \\frac{\\partial h_q}{\\partial \\theta} \\\\ &= \\sum_{q=a}^b \\Bigg( \\sum_{k=q}^b \\frac{\\partial L(h_k, \\hat{y}_k)}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_q} \\Bigg) \\frac{\\partial h_q}{\\partial \\theta}, \\end{aligned} \\]\nwhere in the last derivation we swapped the order of the two sums in $k$ and $q$, adjusting the extremes of the summations. This can be easily done while noticing that the two summations in $k$ and $q$ are represent the sum of the elements on a triangular matrix (lower triangular, if $k$ is the row index), thus we can sum on rows or columns first, interchangeably. We grouped some terms in brackets since the $\\frac{\\partial h_q}{\\partial \\theta}$ does not depend on index $k$. Such terms can be iteratively evaluated going backward in time, exploiting the following relation,\n\\[\\frac{\\partial h_b}{\\partial h_q} = \\prod_{k=q+1}^b \\frac{\\partial h_k}{\\partial h_{k-1}}, \\]\nwith $q < b$, that can be used to compute the gradient w.r.t. \\theta in an iterative manner, going over the input sequence in a backward manner. In fact, from Eq. 34 we get\n\\[ \\frac{\\partial h_b}{\\partial h_q} = \\frac{\\partial h_b}{\\partial h_{b-1}} \\frac{\\partial h_{b-1}}{\\partial h_q}, \\]\nwhich, starting from $q = b - 1$, can be efficiently evaluated for $q = b - 1, b \u2013 1, ..., a$. We introduce additional variables to progressively accumulate gradients, i.e., h and \\dot{\\theta}, with $\\dot{\\theta}_b = 0$ and $\\dot{\\theta}_b = 0$. For $q = b, b \u2013 1, ..., a$ (here we have to consider also the gradient of the loss, that is why q starts from b), we have\n\\[ \\dot{h_{q-1}} = \\frac{\\partial L(h_q, \\hat{y}_k)}{\\partial h_q} + \\dot{h_q}\n\\[ \\dot{\\theta_{q-1}} = \\dot{\\theta_q} + \\frac{\\partial \\dot{h_{q-1}}}{\\partial \\theta}."}]}