{"title": "Symbolic Regression with a Learned Concept Library", "authors": ["Arya Grayeli", "Atharva Sehgal", "Omar Costilla-Reyes", "Miles Cranmer", "Swarat Chaudhuri"], "abstract": "We present a novel method for symbolic regression (SR), the task of searching\nfor compact programmatic hypotheses that best explain a dataset. The problem is\ncommonly solved using genetic algorithms; we show that we can enhance such\nmethods by inducing a library of abstract textual concepts. Our algorithm, called\nLASR, uses zero-shot queries to a large language model (LLM) to discover and\nevolve concepts occurring in known high-performing hypotheses. We discover\nnew hypotheses using a mix of standard evolutionary steps and LLM-guided steps\n(obtained through zero-shot LLM queries) conditioned on discovered concepts.\nOnce discovered, hypotheses are used in a new round of concept abstraction and\nevolution. We validate LASR on the Feynman equations, a popular SR benchmark,\nas well as a set of synthetic tasks. On these benchmarks, LASR substantially\noutperforms a variety of state-of-the-art SR approaches based on deep learning and\nevolutionary algorithms. Moreover, we show that LASR can be used to discover a\nnovel and powerful scaling law for LLMs.", "sections": [{"title": "1 Introduction", "content": "Symbolic regression (SR) [32] is the task of finding succinct programmatic hypotheses \u2014 written in\na flexible, domain-specific programming language \u2013 that best explain a dataset. Initially proposed in\nthe 1970s, SR has recently emerged as a prominent approach to automated scientific discovery, with\napplications in domains from astrophysics [29, 11] to chemistry [2, 21] to medicine [49].\nComputational complexity is a fundamental challenge in SR, as the space of hypotheses that an SR\nalgorithm must search is discrete and exponential. Previous work has approached this challenge\nusing methods like genetic programming [39, 9], neural-guided search [10, 41], deep reinforcement\nlearning [36] and hybrid algorithms [27]. However, new tools to enhance the scalability of SR remain\na critical need for applications in SR and scientific discovery.\nIn this paper, we show that abstraction and knowledge-directed discovery can be powerful principles\nin building such scaling tools in SR. State-of-the-art genetic algorithms for SR [9] evolve pools of\ncandidate hypotheses using random mutation and crossover operations. By contrast, a human scientist\ndoes not just randomly mutate their explanations of data. Instead, they synthesize background\nknowledge and empirical observations into abstract concepts, then use these concepts to derive new\nexplanations. We show that zero-shot queries to large language models (LLMs) can be used to\nimplement such a discovery process on top of a standard SR algorithm.\nConcretely, we present a new method for symbolic regression, called LASR, that discovers a library\nof abstract, reusable and interpretable textual concepts and uses it to accelerate SR. LASR alternates\nbetween three phases: (i) concept-directed hypothesis evolution, where standard genetic operations"}, {"title": "2 Problem Formulation", "content": "Symbolic Regression. We formulate symbolic regression (SR) as a program synthesis [6] problem.\nThe inputs to this problem include a language L of programmatic hypotheses and a dataset D :=\n{(x\u2081, y\u2081)}\u1d62\u208c\u2081\u207f of input-output examples. The syntax of L is described by a context-free grammar [23].\nThe grammar allows each hypothesis \u03c0 to be represented using a set of mathematical operators (e.g.,\naddition, multiplication, trigonometric functions) that facilitate the composition of simpler hypotheses\ninto more complex ones. We abstractly define the fitness of a hypothesis \u03c0 as the likelihood pc(D | \u03c0)\nthat it generates D.\nIn order to prevent finding non-useful solutions, we impose a prior probability distribution pc(\u03c0)\nover hypotheses \u03c0 that penalizes syntactically complex hypotheses. We now pose SR as the task\nof finding a hypothesis \u03c0* that maximizes the fitness while minimizing syntactic complexity. The\nproblem can be expressed as a maximum a posteriori (MAP) estimation problem [14]:\n\u03c0* = arg max pc(\u03c0|D) = arg maxpc(D|\u03c0)\u00b7 pc(\u03c0)\n                              \u03c0               \u03c0\nRecent work leverages large language models (LLMs) for program synthesis [8, 18, 31]. Large\nlanguage models (LLMs) approach program synthesis as a token prediction problem, directly approx-\nimating the likelihood of programs by training on internet-scale datasets. That is,\nPc(\u03c0|D) \u2248 PLLM((\u03c0) | \u3008L\u3009, desc(D)),\nwhere (\u03c0) and (L) are, respectively, textual representations of \u03c0 and a specification of the syntax of\nL, and the task description desc(D) is a few-shot serialization of a subset of the examples in D.\nSymbolic Regression with Latent Concept Libraries. Classical symbolic regression typically\nassumes no prior knowledge or intuition about the problem. In contrast, human scientific discovery\noften leverages empirical patterns [50] and intuitions derived from previously observed data. For\nexample, recognizing a 'power law relationship between variables' has led to the formulation of\nfundamental empirical laws across various fields, such as the Arrhenius equation in Chemistry, the\nRydberg formula in Physics, Zipf's law in Linguistics, and Moore's law in Computer Science.\nWe model such empirical patterns as natural-language concepts drawn from a latent concept library\nC. We frame the relationship between the concept library and programs as a Hierarchical Bayesian\nmodel consisting of: (i) a prior p(C) representing the natural distribution over concept libraries; (ii)\na model pc(\u03c0 | C) that quantifies the likelihood of various hypotheses for a given concept library\nC; and (iii) the previously mentioned fitness function pc(D | \u03c0) for programs \u03c0. We assume that\nthe distributions p(C) and pc(\u03c0 | C) can be approximated using LLMs. That is, we can prompt an\nLLM to generate interesting concepts, and we can prompt an LLM with a set of concepts to generate\ntoken-sequence representations of hypotheses that adhere to the concepts. Now we state the problem\nof symbolic regression with latent concept learning as one of simultaneously inducing an optimal\nconcept library and an optimal programmatic hypothesis:\narg max p(\u03c0, C|D) = arg max p(D|\u03c0) \u00b7p(\u03c0|C)\u00b7 p(C)\n   \u03c0,C               \u03c0,C"}, {"title": "3 Method", "content": "LASR performs a two-stage evolution over natural-language concepts and programmatic hypotheses.\nThe two stages follow an alternating maximization strategy shown in Figure 1: (1) Hypothesis\nevolution: We fix the set of concepts and focus on maximizing the hypotheses' fitness to the dataset,\nand (2) Concept abstraction and evolution: We leverage the best hypotheses found to induce a new\nlibrary of concepts.\nIn the rest of this section, we first describe PySR, the SR algorithm [9] that LASR extends. Next,\nwe show how to modify this algorithm into one guided by natural-language concepts. Finally, we\nshow how these concepts can be naturally extracted and evolved into new concepts. The full LASR\nalgorithm is presented in Algorithm 1 and visualized in Figure 2. LASR is built in Julia with an\nadditional Python interface\u00b2 and uses an open-source, optimized framework for LLM inference [24]."}, {"title": "4 Experiments", "content": "We demonstrate the effectiveness of LASR on multiple tasks integral to scientific discovery. First,\nwe evaluate LASR's performance on the Feynman Equation dataset, a widely adopted scientific\ndiscovery benchmark, under a variety of ablations and additional priors. Second, we measure the\neffect of data leakage by evaluating LASR's performance on a procedurally generated synthetic\ndataset of challenging equations. Finally, we conduct a case study using LASR to discover LLM\nscaling laws with data from the BIG-Bench evaluation suite [15].\nLASR's main focus is to serve as a practical toolkit for scientists. Therefore, our evaluation primarily\ntargets slightly noisy environments, using exact solution rate to gauge performance rather than\nstatistical similarity measures like correlation R\u00b2, which are less relevant to scientific discovery\napplications."}, {"title": "4.1 Comparison against baselines in the Feynman Equation Dataset", "content": "Dataset: The Feynman Equations dataset is a well established benchmark for Symbolic Regression\nalgorithms [47]. This dataset consists of 100 physics equations extracted from the Feynman lectures\non Physics. We compare against performance reported on SRBench [25]: a continuously updated\npublic benchmark of SR methods on many datasets. Specifically, we compare against GPlearn,\nAFP, AFP-FE, DSR, uDSR, PySR, and AI Feynman [40, 45, 47, 27, 36]. Within this subset PySR\nrepresents an ablation of our model without the LLM genetic operations and the concept evolution\n(Section 3). We evaluate on a slightly noisy version of this dataset in order to simulate experimental\nerrors common in scientific discovery domains. Details about this dataset are presented in Section\nA.4.1.\nSetup: We instantiate LASR using gpt-3.5-turbo-0125 [4] as the backbone LLM and calling\nit with p = 0.01 for 40 iterations, and compare our results with PySR which uses the same default\nhyperparameters. For the other baselines, we use the numbers reported in SRBench with one exception\nbeing uDSR [27], for which we couldn't find any benchmarking numbers. For this method, we derive\nthe exact solve rate from [35].\nResults: We showcase results in Table 1. We draw three observations from this experiment. First,\nLASR achieves a higher exact solve rate than all other baselines. Second, both PySR and LASR\noutperform the other baselines by a wide margin, indicating that scalable and efficient synthesis is\nimperative to practical scientific discovery algorithms. Finally, and most notably, a subset of the\nequations LASR finds could not be derived with any of the previous methods."}, {"title": "4.2 Cascading Experiments", "content": "LASR's performance is inherently bottlenecked by the reasoning capabilities of the backbone LLMs\nand the frequency of their invocation in each iteration. To evaluate the effect of the backbone LLM\non LASR's performance, we instantiate a model cascade over two of LASR's hyperparameters: the\nbackbone model (11ama3-8b [16], gpt-3.5-turbo-0125) and the probability p with which we\ncall that model in the evolution step (p = [1%, 5%, 10%]).\nSetup: Our cascade operates as a tournament. We start LASR with the configuration that provides\nthe least language guidance (11ama3-8b at p = 1%) and progressively increase the value of p and\nthen the backbone model. Each subsequent model is only evaluated on the problems that the previous\nmodel could not solve. We compare this against PySR's performance on the Feynman equation\ndataset. To ensure a fair comparison, we cascade PySR using the same procedure but find it does not\nsolve any additional equations. For this experiment, we tag each equation with a qualitative rating\ncomparing the equation to the ground truth form (Exact Solve, Almost Solve, Close, and Not Close).\nAn in-depth discussion on this metric is presented in Section A.6."}, {"title": "4.3 Ablation Experiments", "content": "We conduct ablations on the use of Concept Evolution, Concept Crossover, variable names, and user\nhints. Figure 3 shows how these ablations affect performance over 40 iterations. We designate an\nequation as \"solved\u201d if, after N iterations, the MSE of our predicted equation is less than 10\u207b\u00b9\u00b9. This\nmetric differs from 'Exact Solved' as defined in the prior experiments: an equation can be 'exactly\nsolved' yet have an MSE higher than 10\u207b\u00b9\u00b9 due to the noise floor in the target variables, and an\nequation can have low loss but not be an exact match. We observe from the results that: (1) Removing\nvariable names results in a substantial performance drop, as we lose semantic meaning provided\nby variables (for instance, observing \u03b8 could suggest employing trigonometric functions on \u03b8). (2)\nLearning a concept library enables faster convergence to solutions. Without the concept library, task\nconvergence is slower, and widens under higher concept guidance conditions (p > 0.1%)."}, {"title": "4.4 Qualitative Analysis and User Hints", "content": "The concept library provides an interpretable window into our evolutionary search process. To\nshowcase the concepts learned by LASR, we take a sample equation from the Feynman dataset, the\nelectric field of a dipole Ef = 3\ud835\udc5d\ud835\udc4e cos \ud835\udf03 sin \ud835\udf03/4\ud835\udf0b\ud835\udf00\ud835\udc5f\u00b3, and comment on the libraries learned at various intervals.\nWe see rudimentary concepts emerge in the second iteration:\n\"The presence of basic trigonometric functions like sin in the good expressions contributes to their\nquality, indicating a connection to physical concepts such as waveforms or periodic phenomena.\u201d\nAnd, in subsequent iterations, the concepts become even more refined:\n\"The good mathematical expressions exhibit a balance between mathematical operations such as\nmultiplication, division, and trigonometric functions, which are known to have physical interpretations\nand relevance in various scientific phenomena.\u201d\nThis iterative refinement helps LASR consistently maintain high-quality concepts, allowing it to\nconverge to an exact match within 40 iterations. By contrast, PySR and the concept library ablations\nfail to converge on an exact match solution, returning equations that \u2014 while low-loss \u2014 involve\nmany extra terms absent from the ground truth. This reinforces our hypothesis that injecting semantic\nmeaning into the search process not only improves search efficiency, but also regularizes against\ncomplex equations \u2014 as the LLM-generated concepts help filter out irrelevant terms. A deeper\nqualitative analysis is in Appendix A.7.\nExtending LASR with Hints: A benefit of LASR is that its search can be initialized with a set\nof user-specified, natural-language \u201chints.\u201d To evaluate this capability, we generate hints for each\nequation based on variations of the chapter title of the Feynman lecture that the equation belongs to.\nWe intentionally keep the hints vague to see if knowledge about just the general field is sufficient in\nimproving LASR's performance. We showcase results in Figure 3. We observe a noticeable boost in\nperformance from injecting these hints, even for our weakest performing model, indicating that even\nminimal user input can substantially enhance LASR's effectiveness in discovering equations."}, {"title": "4.5 Data Leakage Validation", "content": "An important consideration in using LLMs for existing SR problems is the possibility that the LLM\nwas exposed to the hold-out problems in the validation set, presenting an unfair advantage to LLMs\ntrained on massive datasets. Intuitively, LASR generates its own concepts which are conditioned on\nsuboptimal programs, which are unlikely to be within the LLM's memorized responses. To validate\nthis, we generate a dataset of 41 synthetic equations that are engineered to deviate from common\nphysical and mathematical structures and have arbitrary variables. For example, one such equation is\ny = (0.782\ud835\udc65\u00b3+0.536)/(\ud835\udc65\u00b2\ud835\udc52\u02e3\u2081(log \ud835\udc65\u2082\u2212\ud835\udc65\u2082\ud835\udc52cos \ud835\udc65\u2081)). We find that PySR struggles to solve equations with these characteristics\n(given 400 iterations). Hence, solving such equations hinges on the language guidance components."}, {"title": "4.6 Using LASR to discover LLM Scaling Laws", "content": "So far, we have demonstrated that LASR can discover equations that are practical but already known\n(Feynman Dataset) and equations that are novel but aren't practical (Synthetic Dataset). To investigate\nLASR's utility in finding novel and practical empirical trends, we investigate whether LASR can\ndiscover novel LLM scaling laws on the BigBench dataset [15]. More details on this experiment are\npresented in Section A.5.\nTraditionally, to identify an LLM scaling law, practitioners must first manually posit a \u201cskeleton\nequation\" with a fixed set of known variables and unknown free parameters, and then optimize\nthe unknown parameters based on a dataset of model hyperparameters and resulting dataset fitness\n[22, 1, 5]. Instead of starting with a predefined equation, we use LASR to discover the skeleton\nequation that best fits various subsets of the BigBench dataset.\nSetup. BigBench contains 204 tasks with scored responses from 55 language models trained with\ndifferent hyperparameters. We evaluate on the subset of tasks where the preferred metric is 'Multiple\nchoice grade' (53,812 samples). Our goal is to find the equation that best predicts the test score given\nthe model hyperparameters and the dataset hyperparameters. We run LASR with 3840 populations\nof 200 candidates each for 7 hours (overnight). The runtime of LASR is comparable to other SR\nalgorithms for this experiment as the slowest operation isn't generating candidate equations but rather\noptimizing and evaluating candidate equations.\nResults. LASR discovers the following scaling law on the subset of BigBench:\nscore = \ud835\udc34/((#shots/trainsteps)^\ud835\udc35) + \ud835\udc38"}, {"title": "5 Related Work", "content": "Symbolic Regression. The field SR started in the 1970s [17, 28] and has recently become a\nprominent approach to AI-for-science [32, 33, 38]. Two algorithmic themes here are:\nNon-parametric Algorithms: Most work on SR focuses on improving search efficiency using heuris-\ntics or parallelization. Specifically, PySR [9] builds a multi-population evolutionary algorithm that\nincorporates various preexisting heuristics [37], and introduces novel ones such as simulated anneal-\ning, an evolve-simplify-optimize loop, and an adaptive parsimony metric. PySR has been successfully\napplied to study problems in domains such as cosmology [11], international economics [48], and\nclimate modeling [19]. LASR extends PySR to enable the discovery of latent concepts.\nParametric Algorithms: Recent work in SR and program synthesis has often used neural networks\nto accelerate search [41, 38, 36, 27, 33, 12]. The interplay between the neural and the symbolic\ncomponents in these works can be abstracted into two categories: (1) leveraging LLMs to induce\nprogram scaffolds [33, 38], and (2) learning a neural policy to accelerate search [36, 27, 41, 12]. We\nhighlight two methods from the first category: Funsearch [38] and LLM-SR [43]. Funsearch [38] uses\na pretrained LLM to implement a mutation operator on a database of executable programs under a fixed"}, {"title": "6 Conclusion", "content": "We have presented LASR, a framework that uses zero-shot queries to an LLM to induce abstract,\nreusable concepts that can be used to accelerate SR. We have shown that LASR outperforms state-\nof-the-art approaches on the standard Feynman equation task. We have also used the algorithm to\ndiscover a novel scaling law for LLMs.\nA key benefit of LASR is that its capabilities are ultimately bottlenecked by those of the underlying\nLLM. LLMs are rapidly gaining capability and getting cheaper, and future versions of LASR should\nbe able to tap into this progress.\nMany directions of research remain open. First, our strategy of accelerating evolutionary search with\nLLM-based concept induction may be applicable beyond the SR setting. Future research should\nexplore such applications. Second, while our approach here was entirely based on in-context learning,\nit is worth exploring if finetuning improves the performance of the LLM. Finally, we evaluated the\nlearned concept library exclusively on the downstream SR task. However, the library may also be\nvaluable in other tasks such as clustering or explanation synthesis. Exploring these other tasks is an\nattractive topic for future work.\nLimitations. The current instantiation of LASR has several limitations. First, it cannot guarantee\nthat the concepts it learns are correct or insightful. Even a concept that leads to strong performance\nin downstream SR tasks may do so because of quirks of the model and data, and end up misleading\nscientists using the method in a discovery process. Also, we do not currently have a way to ensure\nthat the learned concepts are mutually consistent. Finally, our evaluation here was constrained by\nour compute budgets for LLMs and search. Whether the trends we see generalize to higher-compute\nregimes remains to be seen."}, {"title": "A Appendix", "content": "A.1 Broader Societal Impacts\nWe have presented LASR: a symbolic regression framework that leverages concept guidance to\naccelerate symbolic regression. We hope that LASR helps accelerate the search for empirical laws in\nthe broader scientific community. In this section, we discuss the broader societal impacts and ethical\nconsiderations of our work.\nPotential for Misuse: As with other ML techniques, symbolic regression can be leveraged by\nbad actors to inflict societal harm. Our experiments show that LASR accelerates the search for\nempirical laws from raw observations. In our setting, we are restricted to observations about physical\nphenomena. However, a malicious actor could misuse LASR to find patterns in datasets that violate\npersonal rights.\nPrivacy Concerns: As mentioned before, LASR enables finding patterns in raw observations. We\nhope that LASR is leveraged by scientists to explain physical phenomena. However, it is possible to\nuse such models to learn behavioral profiles without the active knowledge or explicit consent of the\nsubjects.\nBias and Fairness: LASR generates two artifacts: a hypothesis that maximizes a fitness function\n(represented as an equation) and a library of concepts that helped discover that hypothesis. LASR\nensures fairness and lack of bias in the generated equation as long as the fitness function is free of\nbiases as well. However, we leverage foundation models to induce our library of concepts which\ncould be trained on biased data which may reflect in our concept library. Furthermore, we cannot\ndirectly evaluate the efficacy of the concept library and its factual correctness. This doesn't affect\nequation generation \u2013 since equations are quantitatively evaluated. However, a human analyzing the\nconcepts LASR learns might misinterpret trends that the model picks up on.\nA.2 LLM Prompts\nNote that in the prompts in Figure 4, Figure 5, and Figure 6, we refer to our hypothesis as expressions\nand the concepts as hypotheses and suggestions. This prompting style was found to work best for the\nLLM.\nA.3 Implementation Details\nA.3.1 Compute Usage\nWe run all experiments on a server node with 8xA100 GPUs with 80 GB of VRAM each. However,\nour experiments can be reproduced with a GPU with 16 GB of VRAM. We were even able to run\nLASR on a laptop utilizing a quantized model hosted locally 3. Moreover, certain models are hosted\non external servers (such as gpt-3-turbo-0125) which allows running LASR on machines without\nGPUs. For this project, we chose to run llama3-8b using vLLM [24]. However, our framework is\ncompatible with any LLM inference framework that allows hosting an OpenAI compliant RESTful\nserver. For reference, each iteration makes around 60, 000 calls. Each call to the LLM is just under\n1000 tokens. This gives an upper bound on total compute of 60,000,000 tokens per iteration if\np = 100%. Hence, running our model at p = 1% for 40 iterations would result in just under 25M\ntokens for each equation.\nA.3.2 Concept Sampling\nIn order to determine which concepts from the concept library we sample for the LLM Hypothesis\nEvolution, we randomly choose the top-K most recent concepts in the library. This ensures that we\nuse the latest concepts, which are generally reflective of more informed hypotheses, and thus better\nto use. In practice, we set K = 20. Additionally, for Concept Evolution, we exclude the top-K\nmost recent concepts from being used, and rather use older concepts. This is motivated by the desire\nto not have the concept library converge on a few ideas, rather we want diversity of thought. Our\nconcepts are intended to be longer lasting than the hypotheses that generated them, similar to how\nobservational data comes and goes, but the conclusions from them are more persistent.\n\"\n\""}, {"title": "A.3.3 Hyperparameters", "content": "Figure 7 showcases the hyperparameters used for all our experiments. Wherever possible, we use\nthe default PySR parameters. Additionally, LASR introduces three new hyperparameters: (1) % of\nLLM calls, (2) List of user hints, and (3) a dictionary of parameters pertaining to backend LLM\ncommunication. Following other methods in SRBench, we utilize only a subset of the necessary\noperators for solving the Feynman equations, excluding special operators like arcsin and arctan.\nThese operators are seldom required, and removing them speeds up the search process. We generally\nset the number of iterations to 40. However, certain experiments may demand more or less iterations.\nA.4 Dataset Details\nA.4.1 Feynman Equations\nDataset: The Feynman Equation dataset is a widely adopted benchmark for scientific discovery\n[47]. The dataset consists of 100 physics equations extracted from the Feynman lectures on Physics.\nEach equation is in the form y = f(x\u2081,x\u2082,...). The number of input variables ranges from two\nto ten, and the dataset provides 100,000 samples for each equation. We compare against publically\navailable methods benchmarked on SRBench [25]. SRBench is a continuously updated benchmark\nwhich catalogs the performance of various methods on the Feynman dataset as well as other symbolic\nregression problems. Specifically, we compare against GPlearn, AFP, AFP-FE, DSR, uDSR, PySR,\nand the original AI Feynman algorithm [40, 45, 47, 27, 36]. Within this subset, notably, PySR\nrepresents an ablation of our model without the LLM genetic operations and the concept evolution\n(Section 3). We evaluate on a slightly noisy version of this dataset in order to simulate experimental\nerrors common in scientific discovery domains. Specifically, we compare numbers against those\nreported in and reproduced by SRBench with a target noise of 0.001.\nMethodology: For the Feynman dataset, we took the equations and the bounds at which each variable\nwas sampled at and generated our dataset. Then, we added additional noise of 0.001 to our target\nvariable, following the noise formula detailed in the Appendix A.4 of [25], as well as additional\nrandom noise variables with arbitrary names to force the model for proper feature selection. We then\nevaluate exact matches by looking at if the predicted equation symbolically simplifies into the ground\ntruth equation. For the ablation graphs, we used the PySR hyperparameter \"early_stop_condition\" to\ncheck if there is a \"solution\" after N iterations.\nA.4.2 Synthetic Dataset\nFor the synthetic dataset, we ran a script that generates uncommon mathematical hypotheses that\nsatisfy our constraints at random. Then, we ran PySR for 400 iterations and found all the equations\nthat PySR performed poorly in, i.e. MSE loss greater than 1, while having a complexity less than 20."}, {"title": "A.5 Using LASR to find an LLM Scaling Law", "content": "So far, we have used LASR to discover equations that are already well-established in prior work. In\nthis section, we investigate LASR's utility in making novel empirical discoveries. Specifically, we\ninvestigate whether LASR can discover novel scaling laws for LLMs.\nMotivation: Traditionally, to identify an LLM scaling law, practitioners must first manually posit\na \"skeleton equation\" with a fixed set of known variables and unknown free parameters, and then\noptimize the unknown parameters based on a dataset of model hyperparameters and resulting dataset\nfitness [22, 1, 5]. Instead of starting with a predefined equation, we use LASR to discover the skeleton\nequation that best fits various subsets of the BigBench dataset. Removing the need to manually\nposit a skeleton equation simplifies the methodology for finding scaling laws in many ways. First, it\nremoves human preconceptions about the expected relationships between hyperparameters. Second,\nit increases the number of variables and the type of variables human practitioners can jointly reason\nabout. Finally, it enables positing equations of much higher complexity and variable interdependence\nthan otherwise possible.\nDataset: BigBench is a collaborative benchmark intended to probe large language models and ex-\ntrapolate their future capabilities [15]. It contains 204 tasks drawing upon problems from linguistics,\ncognitive science, math, physics, biology, software development, etc. For each task, Bigbench mea-\nsures the performance on many model families (OpenAI's GPT models, Google's dense transformer\narchitectures, and Switch-style sparse transformers). Each BigBench task is evaluated on a preferred\nmetric (chosen by dataset creators). In our experiments, we consider a subset of tasks where the\npreferred metric is 'multiple choice grade.' This subset contains the highest diversity of tasks and\naround 53,812 total data points.\nMethodology: We run LASR with 3840 populations (parallelized across 16 cores) with each\npopulation evolving 200 candidates evaluated with the Zygote autodifferentiation backend. We ran\nLASR overnight (for 7 hours). BigBench necessitates optimizing a matrix of parameters rather than\nsingular scalar constants. This shifts the compute bottleneck from generating a pool of candidates (in\nwhich LASR is slower than PySR) to evaluating a pool of candidates (which is equally slow for both\nalgorithms).\nResults: LASR discovers the following scaling law on the subset of BigBench:\nscore = ((\ud835\udc34)/(#shots^{\ud835\udc35})) + 0.363219((#shots)/(trainsteps))^{-0.015540} + \ud835\udc38"}, {"title": "A.6 Metrics for Cascading experiment", "content": "For the cascading experiment, we aim to evaluate the progression of different configuration towards\nsolving equations. The quantitative metric used in the SRBench comparision experiment, Exact Solve,\ndoes not allow for such fine-grained analysis. Therefore, we categorize the synthesized equations\ninto four buckets: Exact Solve, Almost Solve, Close, and Not Close. Exact Solve is quantitatively\nevaluated using a symbolic match. An equation is tagged as 'Almost Solve' if the dataset loss is\nsmall, but the generated equation has an extra term or lacks one term. A Close equation captures the\ngeneral structure of the solution (such as a square root nested in an exponential) but not more than\nthat, and Not Close includes all equations that are far from the solution."}, {"title": "A.7 Further Qualitative Analysis", "content": "LASR generates two artifacts: the best fit program, and the library of natural language concept that\nhelped find that program. These artifacts provide a unique window into the inner workings of LASR.\nThis section goes over a qualitative study of how LASR and PySR go about discovering Coulomb's\nlaw \ud835\udc39 = \ud835\udc5e\u2081\ud835\udc5e\u2082/4\ud835\udf0b\ud835\udf00\ud835\udc5f\u00b2 from data. Both methods are able to find an answer to this equation. However, their\napproach to finding the best fit equation as well as the form of the equation they discover differs\nsignificantly.\nSetup: Coulomb's law is equation #10 in the Feynman equation dataset. It describes how the force\nbetween two point charges changes with respect to the distance between the charges, the magnitudes\nof the charges, and the permittivity of free space constant. The corresponding data for this equation\nhas a target noise of 0.001 to simulate experimental errors.\nBy analyzing the form of the equation and relationships between variables in Coulomb's law, we can\nuncover several interesting properties: First, observe that this is an inverse square law (The force\n\ud835\udc39 varies inversely with the square of the distance \ud835\udc5f between the charged particles). Second, notice\nthat the \ud835\udc39 is directly proportional to the magnitude of the charges \ud835\udc5e\u2081 and \ud835\udc5e\u2082. Third, observe that\nthe resultant force is symmetric with respect to the magnitude of the charged particles (i.e.: The\nmagnitude of the \ud835\udc39 doesn't change if the magnitude of the charged particles is swapped).\nPySR Solution: PySR finds the following solution to this equation:\n\ud835\udc39 = (((((((((23.382) - (((((((((((((((((23.382)\n\u2212(((((47.72)))*0.087))/r)\u22120.191)\nThis equation has a complexity of 26 and achieves a loss of 2.191505 \u00d7 10\u207b\u00b9\u00b2 on the dataset.\nObtaining a simplification of this solution is rather painstaking.\nLASR'S Solution: LASR finds the following solution to this equation. We also present three steps of\nsimplification:"}, {"title": "A.8 Subset of equations discovered by LaSR", "content": "Equation 2 (1.6.20)\nEquation Number & Reference Ground Truth Equation\np(x) = -x\u00b2/2\u03c3\u00b2\nDiscovered Equation\nf = (0.46314177)/((0.6059228/\u221ax)*0.86212635)\nEquation 7 (I.11.19)\na . b = axbx+ayby + azbz\nA = (x1y1) + ((x2y2) + (x3y3))\nEquation 51 (I.50.26)\nXout(t) = K (cos wt + e cos\u00b2 wt)\nX = (((cos(wt) + 1.3385427)-0.23739222)((x1(\u03b1 \u2212 2.991099 \u00d7 10^(-8)) - 1.37413245 \u00d7 10^(-8)) + x1) cos(wt) + 1.575935)\nEquation 21 (I.18.4)\nRm1r1+m2r2/m1+m2\nx = ((m1r1)+(m2r2))/m1+m2\nEquation 63 (II.11.20)\nP-NPE/3kT\nPol = ((3.0001059 +Pd2)/NPPa)/3kT\nEquation 96 (III.15.14)\nMeff = 2462\nMeff = (Pa x cos .0.39283985)/1-0.00012676959 + 0.23802117\nEquation 57 (II.6.15b)\nE1 = 4\u03c0/3cos \u03b8sino/Ef = ((0 - 0.8882483)+5.0833223 \u00d7 10\u22125/"}]}