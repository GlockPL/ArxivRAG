{"title": "Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension", "authors": ["Wenbo Gong", "Meyer Scetbon", "Chao Ma", "Edward Meeds"], "abstract": "Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. This paper makes a step towards the systematic design of such optimizers through the lens of structured Fisher information matrix (FIM) approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to FIM approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: Row and Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation (Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, Alice achieves better than 2\u00d7 faster convergence over Adam, while RACS delivers strong performance on the 1B model with SGD-like memory.", "sections": [{"title": "1 Introduction", "content": "Adaptive optimizers are critical in training large language models (LLMs). Yet, as models and datasets continue to grow, one important issue associated with scaling is the memory overhead of many optimizers, especially in a distributed training setup [Dubey et al., 2024, Korthikanti et al., 2023]. This has several implications for training, including increased GPU requirements or reduced per-device batch size, which lowers overall training throughput. Adam, for instance, triples memory requirements due to the storage of two internal exponential moving average (EMA) states, while other optimizers [Gupta et al., 2018, Vyas et al., 2024] with faster convergence (in terms of training steps) can further inflate the total memory. Meanwhile, some memory efficient optimizers, like stochastic gradient descent (SGD), fail to train the LLMs effectively. Thus, designing efficient optimizers has become increasingly important."}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Basic notations and setup", "content": "Throughout the paper we consider 2D matrix parameters W (i.e. layer weights) of size m \u00d7 n and m < n; the operation Vec(\u00b7) vectorizes the input matrix by stacking its columns; Mat(\u00b7) is the inverse of Vec(.) and reshapes the vector back into a matrix. We use L as the loss function where \u03b8 = Vec(W). G = \u2207wL is the matrix gradient and \u011d is the vectorized gradient, i.e. \u011d = Vec(G)."}, {"title": "2.2 Fisher information and natural gradient descent", "content": "Fisher information is a fundamental concept in statistics that measures the amount of information a random variable carries about a parameter of interest. In this paper, we ignore dependence between layers and treat each layer independently. Under the context of LLMs, with the vectorized mini-batched gradient of one layer \u011f, we define the empirical FIM for that layer as F = E[\u011f\u011fT]. Natural gradient descent (NGD) leverages the inverse of FIM to smooth the local geometry to improve convergence and stabilize training [Martens, 2020]. In practice, the square-root inverse of FIM may be preferred due to its properties and improved performance on certain optimization problems [Bergstra and Bengio, 2012, Choi, 2019, Lin et al., 2024, Loshchilov and Hutter, 2016, Yang and Laaksonen, 2008]. The corresponding update of W is:\nW \u2190 W \u2212 Mat(F\u2212\u00bd\u2207\u03b8L).\n(1)"}, {"title": "3 Structural Approximation to FIM", "content": "The structured FIM approximation framework consists of three steps: first, choose a structure to approximate FIM F; second, find a solution for the approximation by minimizing the following objective:\nmin || F \u2013 F||,\nF\u2208H\n(2)"}, {"title": "3.1 Adam: purely diagonal structure", "content": "There have been many work arguing that Adam's second moment aims to approximate the diagonal of FIM [Hwang, 2024, Kingma, 2014, Sun and Spall, 2021]. Although it is easy to prove that this is, in fact, optimal approximation under Eq. (2), we include the following proposition for completeness.\nProposition 1 (diagonal approximation). Assuming H = {Diagv (v); vi > 0}, then Eq. (2) has analytic solution\nF* = Diagv (E[g\u00b2])\n(3)\nIt is trivial to show the resulting square-root NGD recovers Adam's second moment when using the EMA to estimate E. Together with the first moment, one can recover Adam."}, {"title": "3.2 Shampoo: Kronecker product structure", "content": "Although previous work [Morwani et al., 2024] has demonstrated the connection of Shampoo [Gupta et al., 2018] to the Kronecker product approximation to FIM through power iteration algorithm, we will make its connection to Eq. (2) more explicit and provide an alternative view: Shampoo's pre-conditioner can be derived by minimizing an upper bound of Eq. (2) with this structural assumption:\nH = {RLM; Rn \u2208 Rnxn, Lm \u2208 Rmx\u00d7m}\nwhere Rn and Lm are symmetric positive definite (SPD) matrices.\nTheorem 3.1 (Shampoo pre-conditioner). Assume the above structural assumption, then we have an upper bound of Eq. (2)\n||F \u2212 F|| \u2264 3(mn||A\u00b2 \u2013 C2||F||B2 \u2013 C2 || F\n+ \u221amn||C||(||A2 \u2013 C2 || F + ||B2 \u2013 C\u00b2 ||F))\n(4)\nwhere A = R\u00bd \u2297 Im, B = In \u2297 L\u00bd, C = E[\u011d\u011dT]\u00bd. Minimizing the upper-bound admits\n1  [GTG], L = 1E[GGT]"}, {"title": "3.3 Normalization and Whitening: Simple block diagonal structures", "content": "For an input G, the whitening and normalization operator are defined as\nWhitening(G) = (GGT)\u00af\u00bdG\nNorm(G) = GS-\u00bd\nwhere (\u00b7)\u00af\u00bd denotes square root inverse, and Diag(S) contains the squared column 12 norm of G. Next we provide an new interpretation of these operators and show that they are the square-root NGD updates under the following structural assumptions:\nH ={In \u2297 M} (Whitening)\nH ={S \u2297 Im; Sii > 0, \u2200i} (Normalization)\n(5)\n(6)"}, {"title": "3.4 Eigen-Adam: Generalization to Adam with eigenspace rotation", "content": "All structures considered above are simple and do not strictly generalize Adam's purely diagonal structure. In this and the following sections, we consider two structures that strictly generalize Adam, normalization, and whitening. Here, we first consider a block diagonal matrix with a shared eigen-space.\nPrecisely, we propose the following structure that generalizes Adam*:\nH = {Diage(M1,..., Mn); M\u2081 = UfD{U}}\n(9)\nwhere Uf defines a shared full-rank eigen-space, and Di is a positive eigenvalue matrix. Adam is a special case by constraining Uf = I. Additionally, the structures in Sec. 3.3 are also special cases. Whitening is obtained by a shared D (i.e. D\u2081 = D); and normalization is by Uf = I, D\u2081 = SiI. However, this structure does not directly lead to an analytic solution for Eq. (2). Instead, we propose to approximate the solution by solving 1-iteration alternating optimization:\nTheorem 3.2 (1-iteration refinement). For the structure in Eq. (9), we consider the following 1- iteration alternating optimization of objective (2): (i) constrain D\u2081 = D to be equal, and solve \nU = arg minu Diag (U\u2081 DU, DUDUT - F (ii) fix the U and find {D;} =\narg {Di}min || Diage (UD\u2081U,...,UDU\u00b9) - F. Then, (i) and (ii) admits the following analytic solution:\nU = EVD(E[GGT]).\n(10)\nwhere EVD is the eigenvalue decomposition; and:\nD* = Diagm (E[(UTG)\u00a92])\n(11)\nBased on this result, we can derive the corresponding square-root NGD with given U (refer to App. C.3):\nMat(F\u00af\u00bd\u011d) = Uf\nUG\n(12)"}, {"title": "3.5 SOAP: Combination of Kronecker product with block diagonal structure", "content": "All previous structures, apart from the one behind Shampoo, are under the class of block diagonal structures. However, such block-diagonal structure does not takes into account the off-diagonal part of FIM. Structure under Kronecker product, like the one behind Shampoo, can go beyond this. Therefore, we can consider combining the structure of Eigen-Adam with Shampoo, to obtain a more general structural assumption. We show this exactly recovers SOAP [Vyas et al., 2024].\nSpecifically, we consider the following structural assumption:\nH = {(UR\u2297 UL)\u010e(UR\u2297 UL)T}\n(14)\nwhere UR \u2208 Rn\u00d7n, U\u2081 \u2208 Rm\u00d7m are orthonormal matrix, and \u010e \u2208 Rmn\u00d7mn is a diagonal matrix with positive values. We can easily show that structure behind Eigen-Adam is a special case by constraining UR = In; and Shampoo is also a special case by constraining D to be decomposed by Kronecker product (refer to App. E.1).\nSimilar to Eigen-Adam, it is hard to directly minimizing Eq. (2) with this assumption. We can approximate the solution by a similar 1-iteration alternating optimization procedure as Eigen-Adam.\nTheorem 3.3 (SOAP as 1-iteration alternating optimization of Eq. (2)). Assuming the above struc- tural assumptions. Consider the following 1-iteration aternating optimization of Eq. (2): (i) assuming D can be decomposed as Kronecker product of two diagonal matrix, then solve for UR, U\u2081 = arg minur,UL,\u010e ||(UR \u00ae UL)\u010e(UR \u00ae UL)T \u2013 F||; (ii) fix Up, U, solve for \u010e* = arg min ||(UR\u00aeU\u2081)\u010e(UR\u00aeU\u2081)T \u2013 F || without Kronecker product assumption of \u010e. Then, (i) admits analytic solution when minimizing the upper bound of Eq. (2) (i.e. Eq. (4)):\nUR = EVD(E[GGT]), U\u2081 = EVD(E[GGT]).\nStep (2) admits an analytic solution for Eq. (2):\nD* = Diagm (E[(UT GUR*)2])"}, {"title": "4 RACS: memory-efficient optimizer from a carefully selected structure", "content": "The structured FIM approximation reveals two important insights: there exists a correspondence between structural assumption and optimizers, and structural generality often comes at the cost of practical efficiency. For example, while the structures of Eigen-Adam and SOAP offer more accurate FIM approximations than a simple structure like gradient normalization, they require expensive computation and memory consumption (Table 1), making them impractical for training LLMs. Building on this, our first design recommendation is to select structures that balance generality and practical efficiency.\nTo demonstrate this, we select a structure that generalizes gradient normalization, which scales both the rows and columns simultaneously:\nH = {S \u2297 Q}\n(15)\nwhere S \u2208 Rn\u00d7n, Q \u2208 Rm\u00d7m are positive diagonal matrices. The idea of diagonal approximation has also been leveraged in previous work under different setups [Li, 2018, Shazeer and Stern, 2018, Zhao et al., 2024b]. The optimal solution of Eq. (2) can be solved by a fixed-point iterative procedure:\nProposition 3 (Two-sided scaling). Assuming the structure of Eq. (15), and E[G2] contains only positive values, solving Eq. (2) admits an iterative fixed point procedure:\nS =\nDiag (E[GTQG])\nq =\nDiag (E[GSGT])\n(16)\nwhere s = Diag(S), q = Diag(Q). Additionally, the fixed point solution s, q converges to the right and left principal singular vector of E[G\u00a92] up to a scaling factor with unique S* Q*.\nIn practice, we find initializing q = 1 and use 1-sample estimate of E[\u00b7] gives good performance. Interestingly, Morwani et al. [2024] also connects Shampoo to 1-step power iteration. Here, the Eq. (16) can also be viewed as a power iteration algorithm. The main difference is that Morwani et al. [2024] assumes full SPD matrix S and Q, but our structural constraint is positive diagonal matrix. Consequently, our procedure is computationally efficient and allows for multiple iterations.\nThe corresponding square-root NGD update scales both rows and columns through S and Q (i.e. Mat(F) = Q\u00af\u00bdGS\u00af\u00bd). We name this optimizer, Row and Column Scaled SGD (RACS) (Algorithm 1). Although analytic solutions of s, q exist, we perform 5 steps of Eq. (16) as an efficient approximation. To further stabilize training, we also incorporate the norm-growth limiter used in Chen et al. [2024a]. RACS is highly memory efficient since it only needs the storage of two diagonal matrices S and Q and a scalar for the limiter, consuming m + n + 1 memory. In App. E.5 we discuss connections to Adapprox, Apollo and Adafactor [Shazeer and Stern, 2018, Zhao et al., 2024b, Zhu et al., 2024]."}, {"title": "5 Alice: memory-efficient optimizer from low-rank extension framework", "content": "In this section, we propose a novel low-rank framework consisting of three steps, low-rank tracking, subspace switching; and compensation. Tracking aims to reduce the memory cost of EMA states, whereas switching and compensation are designed to correct the potential issues caused by tracking and the limitations of low-rank projections. We demonstrate this procedure by converting Eigen- Adam to its low-rank version, Alice. While the procedure could be applied to SOAP in a similar manner, we leave this for future work.\nReduce computational cost To improve the computational efficiency, we make two modifications to Eigen-Adam. First, we propose to use 1-step subspace iteration algorithm as an efficient scheme to find leading eigenvectors (Algorithm 10 in App. B). Second, we only update projection U every K steps, effectively partitioning the training into time blocks with size K, and amortizing the cost. Consequently, U is fixed within a time block."}, {"title": "5.1 Tracking: low-rank projections to reduce memory", "content": "By carefully examining the Eigen-Adam's procedure (Eq. (12)), we notice all internal states are connected through the projection Uf,t. To reduce the memory cost, we can obtain a low-rank Ut by keeping only the top r eigenvectors, and denote the remaining m \u2212 r basis as Uc,t (i.e. Uf,t =\n[Ut, Uc,t]). For tracking state Qt, we can also apply low-rank approximation and only track the projected states. We call it low-rank tracking:\n\u03c3t = UGt; Qt+1 = \u03b23Qt + (1 \u2212 \u03b23)\u03c3t\u03c3T\n(17)\nwhere ot is the projected gradient, and Qt is the low-rank tracking state. One can easily reconstruct back Qt \u2248 U+Q+UT when needed. This reduces the memory from m\u00b2 to r2.\nHowever, this low-rank projection comes with two major consequences: (1) the reconstructed state Qt is no longer accurate; (2) the resulting parameter update \u2206 in Eq. (12) ignores the information within Uc,t due to low-rank Ut. Next, we propose two additional steps, switching and compensation, rooted in theoretical insights to address these two problems, respectively."}, {"title": "5.2 Switching: mixing leading basis with the complements", "content": "We omit the subscript t in U and Uc in the following since they are fixed within a time block. Since the projected gradient ot only maintains the information in the space spanned by U, the low-rank tracking state Qt necessarily discards information in Uc. Therefore, even if those directions should become the leading basis at the time we update the U, Qt will ignore their contributions, causing the stability of the previous leading eigenvectors and preventing the exploration of other spaces. We prove that this is possible by showing that the true tracking state Qt can be decomposed into low-rank tracking reconstruction and a residual term quantifying the importance of Uc:\nProposition 4 (Subspace switching). Assuming the setup mentioned above and all assumptions of Eigen-Adam are satisfied. We further assume the low-rank U \u2208 Rm\u00d7r is obtained at the beginning of i + 1 time block by EVD(Qk,r) where Qik is the true tracking state. Further, we assume the stability of the eigen-basis such that gradient G+ during i + 1 time block shares the same eigen-basis as Qk. Then, the true tracking state at the end of i + 1 block, Q(i+1)k, can be decomposed into:\nQ(i+1)k =\n\u03a3 G\u2081G = \u011e\u011e{ +Uc\u03a3\u2081UT\n(18)\nwhere \u011et = Uot is the low rank reconstructed gradients, \u03a3t \u2208 R(m-r)\u00d7(m-r) is a diagonal matrix with positive values, and Uc is the remaining eigen-basis such that [U, Uc] will form the complete eigen-basis of Qk."}, {"title": "5.3 Compensation: convert low-rank update to be full-rank", "content": "Another problem with low-rank projection U is the information loss in the resulting parameter update A at each step. The goal of compensation step is to compensate for this information loss with minimal memory overhead. Firstly, we need to know which information has been discarded. We show that the update A with full-rank Uf in Eq. (12) can be decomposed into the low-rank update with U and complement update controlled by Uc (proof in App. E.6):\nA = U\nU\u2191G\n\u221aE[(UTG)\u00a92]\n+ Uc\nUCG\n\u221aE[(UCG)\u00a92]\nMat(F\u00af\u00bd\u011d),\n(19)\nwhere F = DiagB(UcDc,1U, \u2026\u2026\u2026,UcDcnUT) is the approximated FIM corresponding to the complement basis Uc, Dc,i = Diagv(E[(UTgi)\u00b2]) and \u00af\u00bd is the square-root pseudo-inverse.\nWe notice that the discarded information, Mat(F\u1ef9), has the same format as the square-root NGD with FIM F. From the proposed FIM view point, the design of compensation term becomes the selection of a structure to approximate F, and the application of the corresponding square-root NGD. Considering the trade-off between structural generality and practical efficiency, one structural choice is the diagonal structure of normalization operator, which simply scales the columns of gradient matrix and is highly memory efficient. In addition, we only want to focus on the discarded information UUG for compensation, rather than the entire gradient G. We propose the following compensation at each step t:\nC\u2081 = Mat((SU\u00bfUT)\u011dt) = U\u00bfUG\u1ea0S\n(20)\nwhere S is a positive diagonal matrix, U\u00bfUG = (G \u2013 UUTG) is the gradient information within the remaining basis. We show that such design choice admits an optimal solution of S to the FIM approximation problem.\nTheorem 5.1 (Optimal compensation). Assume that the conditions of Eigen-Adam are satisfied. With the proposed form of compensation (Eq. (20)), minimizing FIM reconstruction loss\n||(SU\u00bfUT) \u2013 F||}\nadmits analytic solution:\nDiag(St) =\n\u221am-r\nE[1\u2122 G\u00a92 \u2013 1 (UTG\u2081)\u00a92]\n(21)\nwhere 1m \u2208 RM, 1 \u2208 R are the column vectors with element 1."}, {"title": "5.4 Alice optimizer", "content": "By combining Eigen-Adam with low-rank U, tracking, switching and compensation, we obtain Alice, a novel low-rank optimizer. One can also design a simple variant, Alice without tracking (Alice-0), by disabling the tracking for better memory efficiency.\nConnections to GaLore Interestingly, GaLore, in fact, is an approximation to Alice without tracking, switching and compensation. Based on the connection of Alice to Eigen-Adam, we reveal that GaLore is a simple low-rank extension of Eigen-Adam, a more general optimizer than Adam. This also reflects the advantage of the FIM view point, which provides a deeper understanding and an explanation on its better performance than Adam under certain scenarios [Zhao et al., 2024a]."}, {"title": "6 Related Work", "content": "Optimizer based on structural approximation Due to the desirable properties and convergence of second-order optimization, various work has been proposed to efficiently approximate Hessian-like matrix, e.g. FIM. KFAC [Martens and Grosse, 2015] was one of the first work that goes beyond the simple diagonal approximations, and approximate the layer-wise FIM. Subsequent works extends KFAC beyond MLP layers [Grosse and Martens, 2016, Martens et al., 2018]. Further refinements to KFAC are also proposed, including refinement of eigenvalues [George et al., 2018], fixing the trace [Gao et al., 2021], and refinement by Kronecker product singular value decomposition [Koroko et al., 2022]. Our proposed view point is different from KFAC, where KFAC decompose the FIM using the back-proped gradients and layer input. In addition, KFAC needs to be re-derived for different types of layers. On the other hand, our proposed view point is closer to another line of work, aiming to approximate the full AdaGrad [Duchi et al., 2011]. In particular, Shampoo [Anil et al., 2020, Gupta et al., 2018] is proposed as a Kronecker product approximation to AdaGrad. Later, [Morwani et al., 2024] explicitly proved that it is a 1-step power iteration to optimal Kronecker product approximation. In here, we propose an alternative view of Shampoo as minimizing a upper bound of the approximation error. SOAP [Vyas et al., 2024] is a recently proposed adaptive optimizer that further improves Shampoo based on the insights from George et al. [2018]. In this work, we make explicit connection of those approaches to FIM approximation, and establish the equivalence of structural assumption to optimizers. We also additionally provide connections of gradient operators to FIM approximation, and design new optimizers from this view point. We provide discussions of our approach to many existing optimizers, including Apollo [Zhu et al., 2024], GaLore [Zhao et al., 2024a], Muon [Jordan et al., 2024], SWAN [Ma et al., 2024], Adapprox [Zhao et al., 2024b], Lars [You et al., 2017], Lamb [You et al., 2019], Fira [Chen et al., 2024a] and AdaDiag [Anonymous, 2024], in App. E.5. In addition to the above, preconditioning SGD (PSGD) [Li, 2017, Pooladzandi and Li, 2024] aims to directly approximate the inverse Hessian or FIM through different structural assumptions. Li [2018] also discussed the diagonal Kronecker product structure as in RACS, but they apply this structural assumption under the framework of PSGD to directly approximate the inverse of FIM through gradient descent.\nMemory-efficient optimizer Practical efficiency is a crucial factor when training large models. In particular, there are many works that focus on optimizing memory efficiency, as less memory consumption allows larger batch size, effectively improving throughput. There are two main lines of research: (1) use low-rank approximation to reduce memory of optimizer internal states; (2) remove the internal states. GaLore [Zhao et al., 2024a], a well-know low-rank optimizer, proposed to use singular value decomposition (SVD) for a low-rank projection, followed by applying Adam within it. It can be seen as a special case of Alice without tracking, switching and compensation. Fira [Chen et al., 2024a], an extension to GaLore, adds compensation term to turn low-rank update to be full-rank, substantially improves the performance. Flora [Si et al., 2024] used randomly sampled Gaussian matrix as the subspace to save compute and memory. However, it is mainly focused on the fine-tuning tasks. ReLora [Lialin et al., 2023], an extension to LoRA [Hu et al., 2021], periodically merges the LoRA weights to enable full-rank learning. On the other hand, many optimizers require fewer internal states compared to Adam. Lion [Chen et al., 2024b] and Signum [Bernstein et al., 2018] only require the storage of the first moment, offering a balance between memory efficiency and performance. Apollo [Zhu et al., 2024], a recently proposed approach, maintains a low-rank GaLore states (e.g. Apollo-mini uses rank 1) for estimating the scaling matrix for the raw gradient. Although it still requires GaLore states, using rank 1 allows it to achieve SGD-like memory. At the same time, Ma et al. [2024] developed SWAN, which manages to completely removes the internal states through two gradient operators: normalization and whitening, and obtains stronger performance than Adam. In this paper, we also show that normalization and whitening operators are special cases of FIM approximation."}, {"title": "7 Experiments", "content": "We include all setup details along with additional experiment results in App. F."}, {"title": "7.1 Pretraining LLaMA with C4 dataset", "content": "Setup We evaluate the proposed RACS, Alice and its variant Alice-0 on pre-training LLaMA [Touvron et al., 2023] with the C4 dataset [Raffel et al., 2020]. We train the following model sizes: 60M, 130M, 350M and 1.3B using a similar setup as Zhao et al. [2024a], Zhu et al. [2024]. For baselines, we consider GaLore, Fira, Apollo-mini, Apollo-svd and Adam. An important consideration in our experiments is that all previous low-rank methods rely on full-rank Adam to train the last layer, which is arguably one of the most important layers [Zhao et al., 2024c]. To thoroughly assess their effectiveness, we report performance for both cases when evaluating low-rank methods\u2014training the last layer with and without Adam\u2014but prioritize the latter as the main evaluation criterion. For full-rank methods (i.e. RACS, Apollo-mini, Apolli-svd and Adam), we assume the last layer is trained by Adam."}, {"title": "5 Alice optimizer", "content": "By combining Eigen-Adam with low-rank U, tracking, switching and compensation, we obtain Alice, a novel low-rank optimizer. One can also design a simple variant, Alice without tracking (Alice-0), by disabling the tracking for better memory efficiency.\nConnections to GaLore Interestingly, GaLore, in fact, is an approximation to Alice without tracking, switching and compensation. Based on the connection of Alice to Eigen-Adam, we reveal that GaLore is a simple low-rank extension of Eigen-Adam, a more general optimizer than Adam. This also reflects the advantage of the FIM view point, which provides a deeper understanding and an explanation on its better performance than Adam under certain scenarios [Zhao et al., 2024a]."}]}