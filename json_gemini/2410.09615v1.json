{"title": "SLIM : ONE-SHOT QUANTIZED SPARSE PLUS LOW-RANK APPROXIMATION OF LLMS", "authors": ["Mohammad Mozaffari", "Maryam Mehri Dehnavi"], "abstract": "Large Language Models (LLMs) have revolutionized natural language under-\nstanding and generation tasks but suffer from high memory consumption and slow\ninference times due to their large parameter sizes. Traditional model compression\ntechniques, such as quantization and pruning, mitigate these issues but often re-\nquire retraining to maintain accuracy, which is computationally expensive. This\npaper introduces SLIM, a novel approach for compressing LLMs using a one-shot\nQuantized Sparse Plus Low-rank Approximation. SLIM eliminates the need for\ncostly retraining by combining a symmetric quantization method (SLIM-Quant)\nwith a saliency-based low-rank approximation. Our method reduces quantization\nerror while leveraging sparse representations compatible with accelerated hard-\nware architectures. Additionally, we propose a parameter-efficient fine-tuning\nrecipe that significantly reduces overhead compared to conventional quantization-\naware training. SLIM achieves up to a 5.4% improvement in model accuracy for\nsparsity patterns like 2:4, and the fine-tuning step further enhances accuracy by up\nto 5.8%, demonstrating state-of-the-art performance. This work provides a path-\nway for efficiently deploying large models in memory-constrained environments\nwithout compromising accuracy.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) (Brown et al., 2020; Radford et al., 2019) are transformative for\nnatural language understanding and generation (Suzgun et al., 2022; Zhou et al., 2023); however,\ntheir extensive parameter count leads to large memory footprints and longer inference times, making\nthem expensive to execute. Model compression methods, such as sparsity and quantization, have\nshown promising results in reducing the inference costs of LLMs. However, these methods often\nrequire costly retraining on large amounts of data to restore the original model accuracy (Sanh et al.,\n2020; Park et al., 2018), while facing numerical and optimization stability challenges when dealing\nwith quantized weights in quantization-aware-training (Gholami et al., 2022).\nTo address these issues, one-shot pruning methods have emerged, eliminating the need for the re-\ntraining. They achieve high accuracy using only a small set of calibration data. Optimal Brain\nDamage (OBD) (LeCun et al., 1989) pioneered the use of second-order information of the loss\nfunction for model compression (Singh & Alistarh, 2020; Mozaffari et al., 2023), though at a high\ncomputational cost. Subsequent methods like Optimal Brain Surgeon (OBS) (Hassibi et al., 1993)\nand modern approaches such as SparseGPT (Frantar & Alistarh, 2023) and WANDA (Sun et al.,\n2023) build on these ideas, and introduce computationally feasible alternatives for LLMs. While\nthese methods perform well with unstructured sparsity, they struggle with semi-structured spar-\nsity patterns like the NVIDIA 2:4 sparsity pattern (Mishra et al., 2021), which are necessary for\nhardware-accelerated inference.\nPost-training quantization (Rokh et al., 2023) methods are an effective compression strategy for\nreducing both memory usage and computation costs. SmoothQuant (Xiao et al., 2023) preserves\naccuracy by leaving the more salient weight channels unquantized, but this results in inefficient de-"}, {"title": "PRELIMINARIES", "content": "Optimal Brain Surgeon. The primary objective in model compression is to minimize the output\ndiscrepancy between the compressed models and the original models. Optimal Brain Surgeon (Has-\nsibi et al., 1993) simplifies this approach by minimizing the output difference at each network layer\nover a calibration dataset. Consider a single feed-forward layer with input $X \\in \\mathbb{R}^{b \\times d_{in}}$, weight\n$W \\in \\mathbb{R}^{d_{in} \\times d_{out}}$, and output $Y \\in \\mathbb{R}^{b \\times d_{out}}$, where b, $d_{in}$, and $d_{out}$ represent the batch size, input\nhidden dimension, and output hidden dimension, respectively. Denoting the compressed matrices\nwith a superscript C, OBS aims to minimize Equation 1. This formulation allows OBS to focus\non maintaining layer-wise fidelity during compression, potentially leading to better overall model\nperformance."}, {"title": "QUANTIZED SPARSE PLUS LOW-RANK APPROXIMATION OF LLMS", "content": "To effectively apply quantization, sparsity, and low-rank adapters to LLMs, SLIM introduces a novel\nquantization scheme called SLIM-Quant. This method reduces quantization error and is followed\nby pruning the quantized model using the importance metric proposed in Section 2. Subsequently,\nSLIM adds low-rank adapters to minimize the saliency of the compression error introduced by\nsparsity and quantization. Figure 1 illustrates this process. In the following subsections steps in this\napproach are explained."}, {"title": "SLIM QUANTIZATION METHOD", "content": "SLIM focuses on symmetric weight quantization due to its low dequantization and memory over-\nhead and ease of implementation. Denoting the quantized matrices by Q superscript, Equation 2\nshows the symmetric quantization formula for q-bit quantization, where a is the quantization pa-\nrameter and clip(.) operator clips the input to values between [-1,1].\n$W^Q = round(clip(\\frac{W}{\\alpha}))2^{q-1}$ (2)\nThe objective of quantization is to reduce the weight reconstruction error shown in Equation 3,\nwhere the * superscript shows the optimal value. But the objective function in Equation 3 is not\nconvex, and to our best knowledge, does not have a closed form solution.\n$\\alpha^* = arg min ||W^Q - W||^2 = arg min ||round(clip(\\frac{W}{\\alpha}))2^{q-1} - W||^2$ (3)\n$\\alpha$                                                                                                                                                                                                                                                                                                         $\\alpha$\nTo solve the mean squared error (MSE) problem in Equation 3, we propose a probabilistic refor-\nmulation as shown in Equation 4, where Q(.) and $Q^{-1}(.)$ are the quantization and dequantization\nfunctions respectively and f(.) is the probability distribution function (PDF) of the weight elements.\n$\\alpha^* = arg min min ||W^Q - W||^2 = arg min \\int_{-\\infty}^{\\infty} f(x)|Q^{-1}(Q(x)) - x|^2dx$ (4)\n$\\alpha$                                                     $\\alpha$\nBy incorporating the quantization formula from Equation 2 into Equation 4, we can simplify the\nintegration into the sum of two terms based on the absolute value of the data: the quantization error\nfor absolute values less than a (Equation 5) and the clipping error for absolute values larger than a\n(Equation 6). Here, fabs(.) represents the probability density function (PDF) of the absolute value\nof the weights. Equation 7 presents the simplified version of Equation 4.\n$E_{quant}(\\alpha) = \\int_{0}^{\\alpha} fabs(x)|\\alpha \\times round(\\frac{x}{\\alpha}) \\times 2^{1-q} - x|^2dx$ (5)\n$E_{clip}(\\alpha) = \\int_{\\alpha}^{\\infty} fabs(x)|x - \\frac{x}{\\alpha}|^2dx$ (6)\n$\\alpha^* = arg min E_{Q}(\\alpha) = arg min E_{quant}(\\alpha) + E_{clip}(\\alpha)$ (7)\n$\\alpha$                 $\\alpha$\nEquation 7 can theoretically be solved by differentiating the objective function with respect to a,\nassuming the probability density function (PDF) of the distribution is known. However, in prac-\ntice, neural network weight distributions don't follow standard distributions. We tested models like\nGaussian, Laplace, Pareto, q-Gaussian, and Weibull, but none fit the observed weight distributions,\nhighlighting their uniqueness.\nTo overcome the lack of a closed-form weight PDF, we use numerical integration over the weight\nhistogram to solve Equation 7. We implement a multi-grid approach to optimize efficiency, starting\nwith 10 uniform samples in the range (0, max(W)) and iteratively refining around the minimum\nerror to find the optimal a. The full method is detailed in Algorithm 1."}, {"title": "SLIM LOW-RANK ADAPTERS", "content": "The effects of quantization and pruning of a weight matrix can be modeled as additive noise, such\nthat $W^C = W + E_Q + E_S$, where $E_Q$ and $E_S$ are the quantization and sparsity errors respectively.\nWe aim to add low-rank adapters to the weights that cancel the compression errors, i.e. $W \\approx\nW^C + LR$, where $L \\in \\mathbb{R}^{d_{in} \\times r}$ and $R \\in \\mathbb{R}^{r \\times d_{out}}$ are the low-rank adapters and r is the adapter\nrank."}, {"title": "POST-COMPRESSION FINE-TUNING", "content": "Fine-tuning models after applying one-shot compression presents significant challenges, primarily\ndue to the limitations imposed by the integer representation of parameters in quantized weights.\nQuantized weights have limited precision and restricted value ranges, making gradient-based up-\ndates difficult and potentially leading to loss of information during fine-tuning. Moreover, the high\nparameter count of large language models renders traditional fine-tuning extremely computationally\nexpensive and time-consuming, necessitating more parameter-efficient methods.\nSLIM addresses these issues by introducing fine-tunable parameters in the form of low-rank\nadapters. In its optional fine-tuning phase, SLIM freezes the sparse and quantized weights, al-\nlowing only the tuning of these low-rank adapters. This parameter-efficient fine-tuning approach\nenables rapid improvement in the compressed model's accuracy using a short fine-tuning phase over\njust thousands of tokens. By focusing the fine-tuning process on a small subset of parameters (the\nadapters), SLIM significantly reduces the computational requirements while still allowing the model\nto adapt to new data or tasks. This approach strikes a balance between maintaining the benefits of\ncompression and enabling post-compression adaptation."}, {"title": "TILED LOW-RANK ADAPTER QUANTIZATION", "content": "Pruning and quantizing the weights reduces the computation and memory footprint of the mod-\nels significantly (~8\u00d7 reduction in memory size), but adding low-rank adapters in full precision\nwill result in an extra overhead. To reduce the adapter overheads, we compress the adapters using\nquantization. The distribution of the elements in the factors have long tails, making even advanced\nmethods that don't use group quantization such as SLIM-Quant impractical. On the other hand,\navailable group quantization methods use 1-dimensional tiles for quantization, which does not match\nthe layout used for tensor cores, hence making them not hardware-friendly.\nTo address these issues, we propose a tiled quantization scheme for the low-rank adapters, in which\n256 elements of the adapter are quantized using the same quantization parameter, creating 16 \u00d7 16\ntiles. The choice of 16 \u00d7 16 blocks is made based on the input size of tensor cores in NVIDIA A100\nand H100 GPUs, making it the tiling strategy more hardware friendly and allowing the warps in\nthe GPU to dequantize the data in for different tensor cores in parallel using only one quantization\nparameter per tensor core. The quantization of each tile is done using the AbsMax algorithm."}, {"title": "EXPERIMENTAL RESULTS", "content": "Models, Datasets, and Evaluation. We evaluate SLIM on the OPT (Zhang et al., 2022) and\nLLaMA-2 (Touvron et al., 2023) model families, both of which serve as standard baselines in model\ncompression studies (Frantar et al., 2022; Frantar & Alistarh, 2023; Sun et al., 2023). Model ac-\ncuracy is assessed on a range of zero-shot downstream tasks, including MMLU (Hendrycks et al.,"}, {"title": "Fine-tuning Costs", "content": "Fine-tuning Costs. Fine-tuning compressed models can help recover lost accuracy. However, fine-tuning quantized weights presents challenges due to the discrete nature of the weights. The most commonly used approach for addressing these challenges, and one that has shown promising results, is the straight-through estimator (STE) (Bengio et al., 2013). In this method, during the backward pass and optimization step, the weights are treated as continuous, allowing for effective fine-tuning despite the quantization.\nIn addition to the challenges posed by discrete values during fine-tuning, the high parameter count of the models leads to time-consuming computations and substantial memory costs. In our experiments, we measured the time required to fine-tune the models under various conditions. For models with low-rank adapters, the quantized weights are frozen, allowing only the low-rank adapters to be fine-tuned. This approach results in a more parameter-efficient fine-tuning strategy, reducing both memory and computational costs. When no low-rank adapter is employed, the straight-through estimator (STE) is used for fine-tuning the quantized weights. Table 4 summarizes the fine-tuning results for 300,000 tokens from the C4 dataset, with a batch size of 64 and a sequence length of 1024 on a single H100 GPU. The fine-tuning costs for models without low-rank adapters range from 12 hours for 125M parameter models to over 36 days for 13B parameter models. Due to these high costs, we faced challenges completing the fine-tuning with our limited resources. In contrast, utilizing low-rank adapters and freezing the sparse quantized weights enables a much more parameter-efficient fine-tuning method, making it feasible for us to report the accuracy results for these cases in Table 1."}, {"title": "CONCLUSION, LIMITATIONS, AND FUTURE WORK", "content": "In this paper, we introduced SLiM, a one-shot quantized sparse plus low-rank approximation method for large language models, designed to balance memory efficiency and accuracy. By leveraging symmetric quantization, sparsity, and saliency-based low-rank adapters, SLiM achieves significant reductions in both memory and computational costs while maintaining competitive performance. Our method demonstrates improved accuracy, particularly for models with structured sparsity patterns like 2:4 sparsity, compared to state-of-the-art approaches.\nSLiM relies on current available libraries all of which lack support for advanced quantization schemes like 2:4 mixed 8-bit and 4-bit group quantization. Additionally, low-rank adapters, while effective, introduce overheads. Future work will focus on developing efficient kernels for 2:4 group quantization and compressing low-rank adapters to further optimize memory and speed."}, {"title": "LANGUAGE MODELING EXPERIMENTS", "content": "We have tested all the benchmarks discussed in Section 4 on a language modeling task on the Wiki-Text2 (Merity et al., 2016) dataset. Table 5 summarizes the results for different pruning and quantization approaches when using 4-bit weight and 8-bit group input quantization. Similar to Section 4, SLIM outperforms all the previous methods, including SparseGPT with Group OPTQ. Using saliency based methods for low-rank adapters is also improving the perplexity of the models in comparison to Wanda-SVD. Additionally, a short fine-tuning step can improve the perplexity of the models significantly."}, {"title": "FINE-TUNING HYPERPARAMETERS", "content": "For fine-tuning the models, we utilized the Hugging Face Trainer (Wolf, 2019). The ADAMW (Loshchilov, 2017) optimizer was employed during the fine-tuning process, accompanied by linear learning rate scheduling. The optimization and learning rate scheduling parameters were set to their default values in the Hugging Face Trainer. To prevent numerical overflow and divergence, we used BFloat-16 data types (Wang & Kanwar) available on NVIDIA H100 GPUs during fine-tuning. The training was conducted with a local batch size of 1 and a gradient accumulation factor of 64 to reduce memory overhead. Weight updates for the sparse and/or quantized weights, as well as the corresponding biases, were disabled. Due to our limited resources, we did not tune any of the hyperparameters aimed at improving fine-tuning speed or accuracy; tuning these parameters is planned for future work."}, {"title": "COMPRESSION COSTS", "content": "An important factor in model compression is the computational cost of the chosen method. In terms of memory usage, all approaches can be adapted to store only a single layer of the model in the GPU's global memory at a time, allowing them to be compressed on a single GPU. However, the computational costs vary depending on the complexity of the method. Techniques like Wanda, which rely solely on matrix multiplication, are significantly faster than more complex methods like SparseGPT, which computes the inverse Hessian matrix for each layer. Adding low-rank adapters in Wanda-SVD and SLIM requires performing singular value decomposition (SVD) on different matrices, resulting in a computational complexity similar to that of SparseGPT. Table 6 summarizes the time required to compress various models using the methods discussed in this paper. Generally, methods incorporating low-rank adapters (SLIM and Wanda-SVD) have higher complexity. However, SparseGPT's compression time is comparable to methods with low-rank adapters, despite only"}, {"title": "RANK ANALYSIS", "content": "The key hyperparameter in low-rank approximation is the rank of the adapters. While increasing the rank reduces approximation error, it also leads to higher computational and memory overhead. Therefore, it is crucial to analyze the trade-off between the accuracy improvements and the overhead introduced by the chosen approximation rank.\nAssuming the rank of the low-rank adapter is $rd$, where $r < 1$ is a fixed factor and $d$ is the dimension of the weights in a square feed-forward layer, the low-rank adapters are represented as $L,R^T \\in R^{d \\times rd}$, resulting in a memory overhead of $O(2rd^2)$ for storing them. To compute $XLR$, where $X \\in R^{b \\times d}$ is the input with a batch size of b, the computational complexity is $O(2brd^2)$. Given that the original memory and computational complexity of the layer are $O(d^2)$ and $O(bd^2)$, respectively, the overhead introduced by the low-rank adapters becomes negligible when $r < 1$."}, {"title": "EFFECTS OF CALIBRATION SAMPLE COUNT", "content": "Similar to SparseGPT and Wanda, SLIM leverages a set of calibration data from the C4 dataset to assess weight saliency for pruning and low-rank approximations. Figure E-b illustrates the perplexity of LLaMA-2-7B using varying numbers of calibration samples. As shown, SLIM demonstrates low sensitivity to the number of calibration samples, making it effective even in scenarios with limited data."}]}