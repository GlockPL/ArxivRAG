{"title": "A DUAL-FUSION COGNITIVE DIAGNOSIS FRAMEWORK FOR OPEN STUDENT LEARNING ENVIRONMENTS", "authors": ["Yuanhao Liu", "Shuo Liu", "Yimeng Liu", "Jing-Wen Yang", "Hong Qian"], "abstract": "Cognitive diagnosis model (CDM) is a fundamental and upstream component in intelligent education. It aims to infer students' mastery levels based on historical response logs. However, existing CDMs usually follow the ID-based embedding paradigm, which could often diminish the effectiveness of CDMs in open student learning environments. This is mainly because they can hardly directly infer new students' mastery levels or utilize new exercises or knowledge without retraining. Textual semantic information, due to its unified feature space and easy accessibility, can help alleviate this issue. Unfortunately, directly incorporating semantic information may not benefit CDMs, since it does not capture response-relevant features and thus discards the individual characteristics of each student. To this end, this paper proposes a dual-fusion cognitive diagnosis framework (DFCD) to address the challenge of aligning two different modalities, i.e., textual semantic features and response-relevant features. Specifically, in DFCD, we first propose the exercise-refiner and concept-refiner to make the exercises and knowledge concepts more coherent and reasonable via large language models. Then, DFCD encodes the refined features using text embedding models to obtain the semantic information. For response-related features, we propose a novel response matrix to fully incorporate the information within the response logs. Finally, DFCD designs a dual-fusion module to merge the two modal features. The ultimate representations possess the capability of inference in open student learning environments and can be also plugged in existing CDMs. Extensive experiments across real-world datasets show that DFCD achieves superior performance by integrating different modalities and strong adaptability in open student learning environments.", "sections": [{"title": "1 INTRODUCTION", "content": "Nowadays, intelligent education is gaining increasing attention in the field of computer science Liu (2021); Chen et al. (2023); Liu et al. (2023); Zhou et al. (2024). Cognitive diagnosis (CD), which is a fundamental upstream task in intelligent education Anderson et al. (2014), acts as a pivotal role in current student learning environments Liu (2021). It has a significant and primary impact on subsequent components such as computer adaptive testing Zhuang et al. (2022), course recommendations Huang et al. (2019); Xu & Zhou (2020), and learning path recommendations Liu et al. (2019). As illustrated in the left part of Figure 1, its goal is to deduce students' mastery level on each concept and other attributes, such as the difficulty levels of exercises through historical response logs and a Q-matrix.\nClassical educational measurement cognitive diagnosis models (CDMs), such as item response theory (IRT) and the deterministic input, noisy and gate model (DINA) De La Torre (2009), either rely on hand-crafted interaction functions or stringent assumptions (e.g., students must master all concepts associated with an exercise to answer it correctly) or complex parameter estimation methods. These make them unsuitable for large-scale student learning environments. Consequently, neural-based CDMs have recently emerged rapidly. Most existing neural-based CDMs Wang et al. (2020a);"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 COGNITIVE DIAGNOSIS MODELS", "content": "ID-based Cognitive Diagnosis Models. Most existing CDMs adhere to the ID-based embedding paradigm, which involves vectorizing students, exercises, and concepts through embeddings and distinguish them by their IDs. They can be categorized by the dimension of mastery levels into two types: latent factor models (e.g., using a fixed length vector to represent students' latent mastery levels), such as multidimensional item response theory (MIRT)Sympson (1978), and models based on patterns of concept mastery (i.e., the dimension of mastery level is the number of concepts), such as DINA De La Torre (2009). These two methods either rely on hand-crafted interaction functions or impose stringent assumptions and complex parameter estimation methods, which may not be effective in today's large-scale student learning environments. NCDM Wang et al. (2020a) employs multi-layer perceptrons (MLP) as interaction function and represents mastery patterns as continuous variables within the range of [0, 1]. Various approaches have been employed to capture fruitful information in the response logs, such as MLP-based Ma et al. (2022); Wang et al. (2023), graph attention network based Gao et al. (2021), Bayesian network based Li et al. (2022). However, this paradigm can fail in open student learning environments. Due to the limitations of IDs, for instance, ID-embedding methods require model retraining for new students, which is unacceptable in real online platforms where timely diagnostic results are expected.\nCognitive Diagnosis Models for Open Student Learning Environments. As online education platforms become increasingly popular, designing CDMs for open student learning environments is crucial. ICD Tong et al. (2022) makes the first attempt to target streaming log data with the goal of updating students' mastery levels in real-time without the need for retraining. However, it may require substantial time when there are numerous records in a short period. DCD Chen et al. (2023), IDCD Li et al. (2024) and ICDM Liu et al. (2024a) rely on simple interaction matrices or hand-crafted graph structures as the feature space, which either demonstrate unpromising performance in open student learning environments or solely focus on a single scenario (e.g., new students). And it is worth noting that unlike the cold-start issues addressed by TechCD Gao et al. (2023) and ZeroCD Gao et al. (2024), open student learning environment focus on inferring the attribue for new students, new exercises and new concepts with unseen response logs during the training phase, which is commonly seen in current online education or testing platforms."}, {"title": "2.2 TEXT-BASED REPRESENTATION LEARNING IN INTELLIGENT EDUCATION SYSTEMS", "content": "Text-based representation learning in intelligent education systems has recently gained significant popularity. NCDM+ Wang et al. (2020a) utilizes exercise text via TextCNN Kim (2014) to complete the Q-Matrix in CD. EKT Liu et al. (2021) enhances student performance prediction in knowledge tracing by utilizing exercise text descriptions. However, neither of them fuse the exercise text or concept name into representations in CD. The most related work is ECD Zhou et al. (2021), which fuses student context-aware features (e.g., parental education level, monthly study expenses) into representations of students in cognitive diagnosis. However, such features are often difficult to obtain in real-world scenarios due to the need to protect the privacy of students and teachers. TechCD Gao et al. (2023) and ZeroCD Gao et al. (2024) use BERT Devlin (2018) for simply extracting exercise text feature which is different from our focus."}, {"title": "3 PRELIMINARIES", "content": "Let us consider open student learning environments which contain three sets: $S = {s_1, ..., s_i}$, $E = {e_1,..., e_j}$, and $C = {c_1, . . . , c_k}$. The relationship between exercises and concepts is represented by"}, {"title": "4 METHODOLOGY: THE PROPOSED DFCD", "content": "In this section, we present the textual feature constructor and response feature constructor. Following that, we delve into the proposed dual-fusion framework. We conclude the section by discussing the model's training. Notably, the strength of DFCD lies in addressing CD in open learning environments. Hence, all its underlying notions are derived from this scenario. Nevertheless, we assert that DFCD is versatile enough to be applied in standard scenarios like previous works Wang et al. (2020a). The framework of DFCD is shown in Figure 2."}, {"title": "4.1 TEXTUAL FEATURE CONSTRUCTOR", "content": "The exercise text can, to some extent, reflect the difficulty level of specific concepts for the students. However, it is evident that exercise text alone cannot directly reflect the expert annotated concepts being tested. For instance, as shown in Figure 2(a), it may related to many concepts (e,g, trigonometric functions, calculate ability), but the annotated concept is \"Square Roots\". The name of the concept also has this issue; the same concept, such as \"time\" is completely different in physics and mathematics. To bridge the gap between real text and its inherent concepts, inspired by the recent successes of large language models (LLMs) in reasoning, we utilize LLMs as exercise refiner and concept refiner. Specifically inspired by recent advancements Xi et al. (2023); Ren et al. (2024), we design the system prompt $a_e$, $a_c$ to function as part of the input for LLMs. This prompt aims to explicitly outline the LLM's role in creating precise summarizations for exercises or concepts by clearly defining the input-output content and the desired output format. By combining this system prompt with the"}, {"title": "4.2 RESPONSE FEATURE CONSTRUCTOR", "content": "As shown in Figure 1, we contend that directly replacing the ID-embedding with text embedding fails primarily because the textual descriptions do not accurately reflect the actual context of student responses. For instance, a question might have a simple textual description, which could result in an embedding that reflects a lower difficulty level. However, certain details may be prone to errors, significantly reducing the students' accuracy and revealing a higher actual difficulty level. Therefore, fusing response feature into the representations is also very crucial. The previous work Chen et al. (2023); Li et al. (2024), following the paradigm of recommendation systems Liang et al. (2018), utilizes the historical interaction matrix $I^O$ as features for students or exercises. This approach may lead to an imbalance in the size of the student and exercise feature space, causing it to fail in certain open student learning environments, and fails to incorporate characteristics of the concepts, which have shown success in recent works Ma et al. (2022); Wang et al. (2023). To this end, we propose the response matrix $R^O \\in R^{(\\vert S^O\\vert+\\vert E^O\\vert+\\vert C^O\\vert) \\times (\\vert S^O\\vert+\\vert E^O\\vert+\\vert C^O\\vert)}$ which incorporate both $I^O$ and $Q^O$ and balance the size of feature space well. It can be elegantly expressed in matrix form\n$R^O = \\begin{pmatrix}\n  0 & I^O & 0 \\\\\n  I^{OT} & 0 & Q^O \\\\\n  0 & Q^{OT} & 0\n\\end{pmatrix}\n\nZ^{(2)}_{S_i} = R^O_{S_i:(\\vert S^O \\vert + \\vert E^O \\vert )}, Z^{(2)}_{e_j} = R^O_{e_j+(\\vert S^O\\vert + \\vert E^O \\vert )}, Z^{(2)}_{C_k} = R^O_{C_k+(\\vert S^O\\vert + \\vert E^O\\vert )}.$\n\nAs shown in equation 3, students' features consist of their responses to exercises, exercises' features consist of student responses and their related concepts, and concepts' features consist of the exercises that assess them. We can easily derive the response features from $R^O$ as shown in the right part of equation 3, namely, $Z^{(2)}_{S_i}, Z^{(2)}_{e_j}, Z^{(2)}_{C_k}$ and $Z^{(2)}_{S_i} \\in R^{1\\times(\\vert S^O\\vert+\\vert E^O\\vert+\\vert C^O\\vert)}."}, {"title": "4.3 DUAL FUSION FRAMEWORK", "content": "Projectors. After obtaining the textual features and response features, the key challenge is how to fuse these two modalities, which have different dimensions, in a personalized manner. Firstly, we introduce T-Projector and R-Projector to align features from two modalities in the same dimension, facilitating subsequent processing. Concretely, in each projector, we utilize three different MLP for students, exercises, and concepts. Here, we take student $s_i$ as an example. It can be expressed as\n$\\tilde{Z}^{(1)}_{S_i} = MLP^{(1)}_{S_i}(Z^{(1)}_{S_i}), \\tilde{Z}^{(2)}_{S_i} = MLP^{(2)}_{S_i}(Z^{(2)}_{S_i}),$\nwhere $\\tilde{Z}^{(1)}_{S_i}, \\tilde{Z}^{(2)}_{S_i} \\in R^{1\\times d}$ denotes the aligned student features in the dual modalities. $MLP^{(1)}$ and $MLP^{(2)}$ are trainable neural networks to change the dimension into d."}, {"title": "4.4 TRAINING FOR DFCD", "content": "Integrating Existing CDMs. To integrate DFCD with most existing CDMs, we need to modify the dimensions to align with the specific type of CDM being used. Since our goal is to infer the students' mastery levels in a fixed dimension, we assume that the total number of concepts is already known (i.e., $\\vert C^O\\vert+ \\vert C^U\\vert$). For CDMs where the embedding size is a latent dimension (e.g., KaNCD), we directly employ $H_{S_i}$, $H_{e_j}$ and $H_{c_k}$ as the input embedding for the integrated CDMs. Otherwise (e.g., NCDM), following Liu et al. (2024a), we introduce transformation layers. Here, we take student $s_i$ as an example, which can be formulated as\n$H_{S_i} = H_{S_i} W^{(s)}_t + b^{(s)}_t,$\nwhere $H_{S_i}$ will be employed as input embedding for incorporated CDMs and $W^{(s)} \\in R^{d\\times(\\vert C^O\\vert+\\vert C^U\\vert)}, b^{(s)} \\in R^{1\\times(\\vert C^O\\vert+\\vert C^U\\vert)}$ are trainable parameters. This significantly reduces the time complexity of graph convolution by encoder which will be further analyzed in the Appendix C.4. Therefore, we train the DFCD with integrated CDMs in an end-to-end manner.\nSimpleCD. Existing neural-based CDMs Gao et al. (2021); Wang et al. (2023); Liu et al. (2024a) except NCDM often have numerous parameters, which may not be effective in open learning environments because they tend to overfit the historical response logs Li et al. (2024). Therefore, we propose a CDM called \u201cSimpleCD\" which is parameter-free except for the interaction function. It can be expressed as\n$\\hat{y}_{ij} = F((\\sigma(H_{S_i} H_{e_j}) - \\sigma(H_{e_j} H_{c_k})) \\copyright Q_{e_j})),$\nwhere $\\hat{y}_{ij} \\in [0, 1]$ represents the prediction score of i-th student practice j-th exercise, $F(\\cdot)$ denotes the Positive MLP which is commonly utilized in CD and $\\sigma$ typically employs the Sigmoid. $\\sigma(H_{S_i}, H_{e_j} \\in R^{1\\times(\\vert C^O\\vert+\\vert C^U\\vert)})$ denotes the mastery level of student $s_i$, namely $Mas_{s_i}$. \u201c$\\copyright$\" represents the element-wise product. $Q_{e_j} \\in R^{1\\times(\\vert C^O\\vert+\\vert C^U\\vert)}$ signifies the concepts associated with the j-th exercise. More details about Postive MLP and SimpleCD can be found in Appendix C. We empirically find that it works well in open student learning environments.\nOptimization. Given input features of students, exercises and concepts, existing CDMs can predict the score of students on certain exercises, which can be formulated as\n$y_{ij} = MCD(H_{S_i}, H_{e_j}, H_{c_k}),$\nwhere $MCD()$ denotes the CDMs, and H represents the input features that contains the representation of the student, exercises and concepts. In the CD task, the main loss function involves computing the"}, {"title": "5 EXPERIMENT", "content": "In this section, we first delineate three real-world datasets and evaluation metrics. Then through comprehensive experiments, we aim to manifest the preeminence of DFCD in both open student learning environment and standard scenario. Due to space constraints, we place the experiments in the standard scenario in Appendix D.5. To ensure reproducibility and robustness, all experiments are conducted ten times. Our code is available at https://github.com/BW297/DFCD."}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "Datasets. Our experiments are conducted on three real-world datasets, i.e., NeurIPS2020 Wang et al. (2020b), XES3G5M Liu et al. (2024b) and MOOCRadar Yu et al. (2023). These three datasets represents diverse educational contexts and subject, which are collected from a wide variety of courses includes the educational contexts and subjects from chinese, history, economics ,math, physics and so on. For more detailed statistics on these three datasets, please refer to Table 1. The details about datasets source and data preprocessing are depicted in the Appendix D.1. Notably, \"Sparsity\" refers to the sparsity of the dataset, which is calculated as $\\frac{S}{E*N}$. \u201cAverage Correct Rate\u201d represents the average score of students on exercises, and \"Q Density\u201d indicates the average number of concepts per exercise.\nEvaluation Metrics. To assess the efficacy of DFCD, we utilize both score prediction and interpretability metrics following the previous works Wang et al. (2020a); Chen et al. (2023). This approach offers a holistic evaluation from both the predictive accuracy and interpretability standpoints.\nScore Prediction Metrics: Evaluating the efficacy of CDMs poses difficulties owing to the absence of the true mastery level. A prevalent workaround is to appraise these models based on their capability to predict students' scores on exercises in the test data. The classic classification metrics such as area under the curve (AUC), Accuracy (ACC) are used in our paper.\nInterpretability Metric: Diagnostic results are highly interpretable hold significant importance in CD. In this regard, we employ the degree of agreement (DOA), which is consistent with the approach used in Wang et al. (2020a); Li et al. (2022). The detailed description about DOA can be found in Appendix D.2. We compute the top 10 concepts with the highest number of response logs in our experiment and refer to it as DOA@10.\nImplementation Details. For parameter initialization, we employ the Xavier Glorot & Bengio (2010), and for optimization purposes, Adam Kingma & Ba (2015) is adopted. The batch size is set as 1024 for all datasets. The learning rate is fixed as le-4. We adjust the dimension d within the range {32, 64, 128, 256}, the type of graph encoder within the range {MLP, GCN, GAT, GT}. We utilize four attention heads for attention-based encoders, with all other parameters set to the PyG Fey & Lenssen (2019) defaults. We employ grid search to find the best hyperparameters using the validation"}, {"title": "5.2 PERFORMANCE COMPARISON IN OPEN STUDENT LEARNING ENVIRONMENT", "content": "Compared Methods. We compare DFCD against other methods and utilize the hyperparameter settings described in their respective original publications. More details can be found in Appendix D.\n\u2022 KaNCD-Mean Wang et al. (2023): As the original KaNCD is designed solely for the standard scenario. We assigns the embedding of unseen students or exercises to the average of the seen ones Liu et al. (2024a).\n\u2022 KaNCD-Nearest Wang et al. (2023): For each unseen students, exercises or concepts in TU, we assign their embedding based on the most similar one in TO, who is selected based on the similarity of response logs. Here, we use cosine similarity as the similarity measure function Liu et al. (2024a).\n\u2022 IDCD Li et al. (2024): It propose an identifiable cognitive diagnosis framework based on a novel response-proficiency response paradigm and its diagnostic module leverages inductive learning representations which can be used in the open student learning environment.\n\u2022 ICDM Liu et al. (2024a): It utilizes a student-centered graph and inductive mastery levels as the aggregated outcomes of students' neighbors in student-centered graph which enables to infer the unseen students by finding the most suitable representations for different node types.\nDetails. To evaluate the effectiveness of our proposed DFCD in open student learning environments, we conduct experiments following Liu et al. (2024a) on datasets with unseen students, unseen exercises, and unseen concepts. For the unseen student scenario, we randomly select students who do not appear in the training data. For the unseen exercise scenario, we randomly select exercises not present in the training data. For the unseen concept scenario, we randomly select exercises with concepts that are not in the training data. The test size pt is set to 0.2, following the previous researches Wang et al. (2020a); Li et al. (2022). In order to prevent data leakage, we retain the test data intact and partition the training data by students, exercises, or concepts at a ratio of 0.2, with"}, {"title": "6 CONCLUSION AND DISCUSSION", "content": "Conclusion. This paper proposes a dual fusion cognitive diagnosis framework (DFCD), where most existing CDMs can be integrated. For the first time, we identify that directly utilizing exercise text features may not benefit CDMs and can even degrade their performance. Therefore, we leverage LLMs as refiners to enhance the textual content. Via DFCD, we fuse the textual features with response-relevant features and integrating existing CDMs to achieve remarkable performance in open student learning environments on three real-world datasets. Our work enables the CDM to better grasp the semantic meaning of exercise through leveraging LLMs' inference capabilities and"}, {"title": "7 STATEMENT", "content": "The research presented does not involve human subjects or raise concerns related to privacy, security, or legal compliance. The datasets used in this study are publicly available, and their use complies with all applicable licenses and terms of use. We have taken several steps to ensure the reproducibility of the results presented in this paper. Detailed descriptions of datasets and implementation are provided in Sections 5.1 of the main paper. We also provide our data and code in the repository at https://github.com/BW297/DFCD."}]}