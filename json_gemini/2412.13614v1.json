{"title": "Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity Linking", "authors": ["Zhengfei Xu", "Sijia Zhao", "Yanchao Hao", "Xiaolong Liu", "Lili Li", "Yuyang Yin", "Bo Li", "Xi Chen", "Xin Xin"], "abstract": "Visual Entity Linking (VEL) is a crucial task for achieving fine-grained visual understanding, matching objects within images (visual mentions) to entities in a knowledge base. Previous VEL tasks rely on textual inputs, but writing queries for complex scenes can be challenging. Visual inputs like clicks or bounding boxes offer a more convenient alternative. Therefore, we propose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses pixel masks from visual inputs to refer to objects, supplementing reference methods for VEL. To facilitate research on this task, we have constructed the MaskOVEN-Wiki dataset through an entirely automatic reverse region-entity annotation framework. This dataset contains over 5 million annotations aligning pixel-level regions with entity-level labels, which will advance visual understanding towards fine-grained. Moreover, as pixel masks correspond to semantic regions in an image, we enhance previous patch-interacted attention with region-interacted attention by a visual semantic tokenization approach. Manual evaluation results indicate that the reverse annotation framework achieved a 94.8% annotation success rate. Experimental results show that models trained on this dataset improved accuracy by 18 points compared to zero-shot models. Additionally, the semantic tokenization method achieved a 5-point accuracy improvement over the trained baseline.", "sections": [{"title": "Introduction", "content": "Visual Entity Linking (VEL) is an open-domain visual entity recognition task that expands the label space to web-scale knowledge bases. As a key task for achieving fine-grained visual understanding, VEL contributes to various tasks such as multimodal knowledge graphs completion (Wu et al. 2023), visual question answering (VQA) (Qiu et al. 2024), image caption (Zhang et al. 2024c), image retrieval (Sain et al. 2023; Saito et al. 2023) and so on.\nCurrent VEL tasks (Hu et al. 2023; Caron et al. 2024a; Xiao et al. 2024) relying on textual queries struggle with some complex scenes. For example, in fig. 1, a simple query like what is on the plate? cannot accurately refer to Broccoli, requiring more complex queries, such as what is the small tree-like vegetable next to the fries on the plate? Creating such queries demands extensive background knowledge and precise comprehension of visual relationships. This adds an additional burden on users, and we cannot assume that downstream models are equipped with such capabilities.\nIn such complex scenes, visual prompts such as clicks, boxes, and pixel masks can be supplementary methods for more efficient and accurate reference. Therefore, this work introduces Pixel-Level Visual Entity Linking (PL-VEL), which uses pixel masks to refer to visual mentions and link them to knowledge-base entities, as shown in fig. 1. With promptable segmentation models like SAM (Kirillov et al. 2023) and SEEM (Zou et al. 2023), users or downstream models can easily create pixel masks through simple actions such as clicking, and drawing boxes. It makes PL-VEL more practical than traditional VEL tasks in real-world applications, such as VQA(Qiu et al. 2024) and visual reasoning (Chen and Wu 2024). To support the research on this task, a large-scale open-domain PL-VEL dataset that aligns pixel-level mask regions in images with entities in a knowledge base is required."}, {"title": "Related Work", "content": "Visual Entity Linking. Previous studies, such as Tag2Text (Huang et al. 2024) and RAM (Zhang et al. 2024b), generated common category tags for images but failed to recognize entity-level tags. To address this, OVEN-Wiki (Hu et al. 2023) was proposed as an open-domain visual entity linking benchmark, which links regions of interest to 6M Wikipedia entities based on text queries. This benchmark also validated the effectiveness of the generative entity recognition framework (GER). Building on this, GER-ALD (Caron et al. 2024b) demonstrated that unAmbiguous Language-based Discriminative (ALD) entity codes offer a performance advantage within the GER framework. AUTOVER (Xiao et al. 2024) achieved an accuracy 11.9 points higher than GER-ALD on the OVEN-Wiki test set through retrieval-augmented constrained decoding.\nIn contrast to text-based references, Wikiperson (Sun et al. 2022), a VEL dataset using bounding box references, was introduced. However, Wikiperson is limited to \"person\" entities and is limited in scale. To address this, we propose an open-domain PL-VEL task, for advancing fine-grained visual understanding."}, {"title": "Region-specific Visual Understanding", "content": "It focuses on semantic information in local image regions, including region-specific conversation (Rasheed et al. 2024), region captioning (Yuan et al. 2023), and referring expressions comprehension (Guo et al. 2024). Our PL-VEL is also a region-specific recognition task. Recent works on region-specific visual understanding focus on MLLMs. Although MLLMs like BLIP (Li et al. 2022), LLaVA (Liu et al. 2023a), and MiniGPT-4 (Zhu et al. 2023) extend LLMs' capabilities to vision. However, they struggle to comprehend effectively specific visual regions. Kosmos-2 (Peng et al. 2023) and Shikra (Chen et al. 2023) input bounding boxes as location-aware reference tokens into LLMs, while GPT4RoI (Zhang et al. 2024a) and GlaMM (Rasheed et al. 2024) use specialized visual modules for bounding box regions.\nThese models, however, cannot describe pixel-level features accurately. Osprey (Yuan et al. 2023) achieves pixel-level understanding with a mask-aware visual extractor. Expanding on this, we introduce cross-attention interactions of pixel-level features and train the model on MaskOVEN-Wiki to enhance pixel-level visual understanding and provide a baseline for PL-VEL."}, {"title": "Pixel-Level Visual Entity Linking Task", "content": "Task Definition\nOriginal Task (PL-VEL) The PL-VEL task takes an image $I$ and a pixel mask $m$ as input. The pixel mask $m$ represents a visual object in $I$, referred to as a visual mention $V_m$. The goal of PL-VEL is to link this visual mention $V_m$ to its corresponding entity $e$ in the knowledge base $K$.\nReverse Annotation (Dataset Construction) The dataset construction task is the reverse process of the PL-VEL task. Given an entity $e$, an image $I$ containing $e$, and a text query $q$ for $e$, it takes them as input, and its goal is to segment the pixel mask $m$ of the visual object of the entity $e$ in $I$.\nThe PL-VEL task assumes that mask references for visual mentions are provided. Various visual and textual prompts can be processed into pixel masks using preprocessing models such as SAM (Kirillov et al. 2023) and SEEM (Zou et al. 2023). This integration enhances the PL-VEL system's adaptability and supports interactive and fine-grained visual entity comprehension."}, {"title": "The MaskOVEN-Wiki Dataset Construction", "content": "To define and address the PL-VEL task, we have developed the MaskOVEN-Wiki dataset, a benchmark with approximately 5 million annotations, covering various categories of entities. Each annotation includes an image, a visual mention represented by a pixel mask, a text query, and the corresponding entity label from Wikipedia.\nFor the source of data, we use an open-domain entity recognition dataset, OVEN-Wiki (Hu et al. 2023), where each sample includes an image, a text query for visual mention and its corresponding entity. This dataset uses a 6 million-entity set derived from Wikipedia. The dataset aggregates 14 existing datasets and is divided into two subsets based on the original tasks of the source datasets: entity split (ES) for image recognition/retrieval and query split (QS) for visual question answering. Additionally, OVEN-Wiki provides a high-quality evaluation dataset, the human set, which is manually annotated. Based on this data, we developed and employed an automated method to annotate pixel-mask visual references for visual mentions in those three subsets. Additionally, we enriched it by annotating visual mentions for entities with images on Wikipedia pages. This additional content serves as a supplement to the knowledge base, referred to as wiki split (WS)."}, {"title": "Model Architecture", "content": "Figure 6 illustrates our model overview. We employ visual instruction tuning to train the MLLM in autoregressively decoding the pre-constructed target entity ALD code. Following the generative entity recognition framework of GER-ALD (Caron et al. 2024b), we construct the ALD code for entity $e \\in K$ as\n$ALDe = SL (TT (e), UTT (ei))$\n$e\u017c EK$                                  (1)\nWhere $TT$ is the text tokenizer of LLM, and $SL$ denotes a function taking the first $L$ tokens in ascending order of term frequency. $L$ denotes the ALD code length. LLM autoregressively generates $ALDe$ with embedding matrix $Y$, instruction $Xins$, image $I$'s features $X1$ and mask query embedding $Xm$ as follows\n$ALD = LLM(Xins, X1, Xm, YALD<<)$                   (2)\nOur backbone is based on Osprey(Yuan et al. 2023), a pixel-level MLLM designed for general visual understanding. Following Osprey's settings, we employ the ConvNeXt CLIP (Liu et al. 2022) as the vision encoder, Vicuna (Chiang et al. 2023) as the foundational LLM, and a vision-language projector using a multilayer perceptron (MLP). Additionally, we reuse its mask-aware visual extractor for constructing regional-level features.\nOur method utilizes visual semantic tokenization to extract the fine-grained semantic features from images. It achieves this by reusing feature maps from the vision encoder and parameters from the mask-aware visual extractor, enabling minimal computational and parameter overhead."}, {"title": "Visual Semantic Tokenization for Region-Interacted Attention", "content": "Current MLLMs (Liu et al. 2023b; Yuan et al. 2023) use vision encoders like ViT (Dosovitskiy et al. 2020) or ResNet (He et al. 2016). These encoders tokenize images based on spatial location rather than semantic content, so that the visual tokens contain incomplete and non-independent semantics, and require additional cross-modal projectors. While Osprey (Yuan et al. 2023) and GLaMM (Rasheed et al. 2023) use region encoders to represent user-specified regions, they do not enhance overall image understanding. PL-VEL focuses on pixel-level visual understanding, motivating us to tokenize images based on semantic content. This approach aligns the semantic granularity of image tokens with the instruction or entity text tokens by controlling each visual token to represent an object, enabling feature interaction within a unified semantic space.\nTo achieve this, a SAM-like model, FastSAM (Zhao et al. 2023), executes \u201csegment-everything\u201d on the image $I$ as a visual semantic tokenizer $T^1$. Subsequently, the mask-aware visual extractor M takes the binary mask of the region $r$ and the image $I$ as input, encoding these into two embeddings,\n$Xreg = {xsem, pos = M(I,r) | r \u2208 T^1 (I)}$                  (3)\nCompared to position-based tokenization, semantic tokenization loses the natural token order. Similar to human visual habits, which typically begin with an overview of larger image areas before concentrating on finer details, we arrange the $Xreg$ in descending order based on their area $a_r$. This method emulates the human visual attention habit ensuring that larger areas receive broader attention within the autoregressive framework. Then we concatenate region features with the patch features $Xpat$ to form the image feature $X_1$.\n$XI = [Xpat; (Xr1, x2,..., Xrxes)]$\n$xr\u2208Xegari >ari+1$                   (4)"}, {"title": "Training", "content": "We have implemented a two-stage training strategy for our model. The vision encoder ConvNeXt CLIP (Liu et al. 2022) and the semantic tokenizer FastSAM (Zhao et al. 2023) remain frozen, while the mask-aware visual extractor M and the visual-language projector are fully fine-tuned. The base LLM is fine-tuned with the LoRA (Hu et al. 2022) approach. Both stages employ autoregressive language modeling loss to predict the next token (Liu et al. 2023a). In the first stage, we pre-train on the wiki split to embed entities from knowledge base K into the model parameters. In the second stage, we fine-tune the model on the entity and query splits to enhance its capability of fine-grained visual entity linking."}, {"title": "Experiments", "content": "Experimental Setting\nMetrics. We evaluate model performance on the validation and test sets of MaskOVEN-Wiki using accuracy as the primary metric. Accuracy is computed for the entity and query splits, as well as the human set (test only). To address the challenges zero-shot models face in generating ALD codes and valid entity names, we use BM25 to search the 6 million Wikipedia entity names and take the top-1 result as the prediction.\nData Processing. The pre-train stage used about 2 million wiki split samples. Due to computational resource constraints and the large size of the dataset (approximately 4.5 million samples), we limited the number of annotated samples per entity to fewer than 50 during the fine-tuning stage. As a result, we used about 7% of the total samples (approximately 0.3 million) in the fine-tuning stage. In addition, all input images were uniformly preprocessed to 512 \u00d7 512. The length of the ALD code is limited to 4 tokens.\nMain Results\nIn table 3, we compare the results of VEL models based on different types of prompts in the validation and test sets of OVEN-Wiki (Hu et al. 2023) (Text) and MaskOVEN-Wiki (Mask). Where the \"None\" prompt denotes that no prompt was utilized to reference the visual mention. Text-based results are from Hu et al. (2023) and Xiao et al. (2024).\nEffectiveness of MaskOVEN-Wiki. In the box and mask prompts, Z denotes whether the result has been fine-tuned using our dataset. Osprey-7B (Yuan et al. 2023) achieves 1.3% in the zero-shot setting and 20.0% after fine-tuning, demonstrating the usefulness of our dataset. By introducing visual semantic tokenization, Osprey-7B-Seg improves the overall performance by 3.4% on the validation set and 5.2% on the test set.\nAdvances of Pixel Mask Reference. Results in table 3 verify the advantages compared with text and box. Compared with text-based results (6.4%-25.5%), our mask representation methods achieve similar performance (0.8%-25.2%), despite text prompts offering more detailed descriptions. Compared with box results (around 1.6%), mask prompts achieve better results. Additionally, we analyzed the limitations of mask methods when dealing with query split, where some questions include additional intents (e.g. \"made of\", \"produced by\") from original VQA datasets. These situations fall outside the scope of VEL.\nAnalysis and Ablation Study\nDirect versus Reverse Process. Comparing the experimental results in tables 1 and 3, we observe a performance gap between the direct PL-VEL methods and reverse annotation approaches. GPT-4V achieves an accuracy of 25.5% in the direct setting. The reverse annotation process, which is an open-vocabulary segmentation task, achieves an accuracy of 94.8%. These findings show the usefulness of our proposed reverse annotation approach for the PL-VEL task."}, {"title": "Conclusion", "content": "In this paper, we introduce the Pixel-Level Visual Entity Linking (PL-VEL) task, which links visual mentions indicated by pixel masks to entities in a knowledge base. This task is a supplement to the text-based VEL, enhancing VEL's practicality for tasks like VQA, visual reasoning, and detailed image captioning. We developed the MaskOVEN-Wiki dataset, a multimodal dataset aligning pixel-level regions with entity-level labels, achieving 94.8% annotation accuracy. Models trained on this dataset achieved over an 18-point improvement in accuracy compared to zero-shot models, with our visual semantic tokenization method contributing an additional 5-point increase. Despite these gains, the final model's linking accuracy was about 25%, indicating both the effectiveness of reverse annotation and the potential of the MaskOVEN-Wiki dataset for enabling fine-grained visual understanding in MLLMs."}, {"title": "Experiment Details", "content": "Annotation Setup\nWe utilized a cluster of 30 nodes for the annotation of large-scale data. Each node was configured with 7 CPU cores, 30 GB of memory, and an NVIDIA Tesla P40-24G GPU. For the MaskOVEN-Wiki dataset, annotating the Entity split and Query split took approximately 120 hours, while annotating the Wiki split took about 35 hours. The specifications for the annotation models are as follows. SAM (Kirillov et al. 2023) used the ViT Huge (ViT-H) version, GroundDINO (Liu et al. 2023c) used the Swin-T version, and SEEM (Zou et al. 2023) used the Focal-L version. The bounding box threshold was set to 0.3, the text query threshold to 0.25, and the annotation batch size was 3.\nExperimental Setup\nWe conducted the PL-VEL experiments on a machine with 2 NVIDIA A100-40G GPUs. The pre-training parameters were as follows: batch size of 8, gradient accumulation over 2 steps, and 30,000 training steps, which took approximately 157 hours. The learning rate was initially tested with different settings [le-7, 1e-5, 1e-4, 1e-3] during the first 2,000 steps and was ultimately set at le-4.\nThe fine-tuning parameters were as follows: batch size of 8, gradient accumulation over 4 steps, and 10,000 training steps that took approximately 48 hours. The learning rate was tested with settings [1e-7, 1e-5, 1e-4] over the first 2,000 steps and was finalized at 1e-4. Due to the large dataset and limited time, we limited the maximum number of samples per entity to 50 during fine-tuning.\nThe entire experiment was implemented using Py-Torch, with model parameters optimized by the AdamW (Loshchilov and Hutter 2019) algorithm and data parallel training facilitated by DeepSpeed ZeRO-0 (Rasley et al. 2020). The maximum sequence length for the LLM was set to 2048, and the image resolution was scaled to 512 \u00d7 512.\nData Filtering\nThe evaluation results in table 1 demonstrate the effectiveness of our heuristic filtering rules based on model ensembles. This section provides further qualitative analysis and discusses the technical details. We identified 4 primary issues. Figure 9 lists some cases corresponding to the above question.\n\u2022 incomplete depiction of entities in images.\n\u2022 references to non-visual entities.\n\u2022 foreground-background confusion in dense object scenes.\n\u2022 error propagation in the segmentation pipeline.\nThe first case involves an error of incomplete depiction of entities in images. The image shows a partial view of the entity Rolls-Royce Museum. This issue primarily occurs with entities of the 'location' and 'building' types. To address this, we set the pixel mask for such cases to cover the entire image."}, {"title": "Additional Dataset Statistics", "content": "Table 6 presents the statistical information of the sample set used for the manual evaluation of annotation quality. The data samples are sourced from the Entity Split, Query Split, and Wiki Split. The sampling process involves two steps. First, we randomly select one sample from the annotated samples for each entity. Second, we sample based on the number of entities corresponding to each split from different dataset splits. For the Wiki Split, we randomly sample 200 instances.\nThe second and third cases fall under references to non-visual entities. The main issue involves either non-visual entities, such as Industrial Revolution, or those not visible in the image, such as Engine. To address this, we filter these errors based on the entity type and specific interrogative words in the text query. Specifically, we exclude entities of types such as time, location, method, event, game, and technology, as well as queries containing interrogative words like \"when,\u201d \u201chow,\" and \"why.\u201d As a result, we exclude 124,896 annotations in Entity Split, 7,920 in Query Split, and 176 in Human Set.\nThe fourth and fifth examples both involve foreground-background confusion but for different reasons. the fourth example is a typical dense object scene, while the fifth is due to error propagation. We apply different correction methods for these two types of errors.\nFor dense objects, we first perform morphological transformations, including erosion and dilation, on the segmented masks. We then calculate their connected regions, and if the number exceeds the threshold, we classify it as a dense object scene. In such cases, we combine predictions from different models and use the confidence scores of these predictions to distinguish between foreground and background.\nFor error propagation, the fifth example shows that foreground-background confusion arises because Grounding DINO (Liu et al. 2023c)predicts a bounding box that encompasses multiple objects. Consequently, this causes an error in the subsequent SAM (Kirillov et al. 2023) step, which lacks a text prompt. To correct those cases, we use the annotation results from the end-to-end SEEM model (Zou et al. 2023).\"\n    }"}]}