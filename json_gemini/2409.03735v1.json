{"title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models", "authors": ["Yan Shvartzshnaider", "Vasisht Duddu", "John Lacalamita"], "abstract": "Large language models (LLMs), while memorizing parts of their training data scraped from the Internet, may also inadvertently encode societal preferences and norms. As these models are integrated into sociotechnical systems, it is crucial that the norms they encode align with societal expectations. These norms could vary across models, hyperparameters, optimization techniques, and datasets. This is especially challenging due to prompt sensitivity-small variations in prompts yield different responses, rendering existing assessment methodologies unreliable. There is a need for a comprehensive framework covering various models, optimization, and datasets, along with a reliable methodology to assess encoded norms.\nWe present LLM-CI, the first open-sourced framework to assess privacy norms encoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette methodology to assess the encoded norms across different contexts and LLMs. We propose the multi-prompt assessment methodology to address prompt sensitivity by assessing the norms from only the prompts that yield consistent responses across multiple variants. Using LLM-CI and our proposed methodology, we comprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior work, examining the impact of model properties (e.g., hyperparameters, capacity) and optimization strategies (e.g., alignment, quantization).", "sections": [{"title": "1 Introduction", "content": "Recent advancements in generative models, including large language models (LLMs), have led to significant performance improvements and their adoption in various sociotechnical systems, such as education [23] and healthcare [10]. LLMS, which generate responses to input prompts, require vast amounts of data for training scraped from the Internet [27]. However, the training of LLMs have several side effects. LLMs memorize parts of the training dataset, which may include personal or sensitive information [8, 9, 19, 40]. Moreover, during training, LLMs could inadvertently encode societal preferences and norms that directly bias their responses.\nA misalignment between norms which are socially acceptable and those which are encoded by an LLM, could cause it to reveal information inappropriately in its responses, thereby violating privacy [31, 38, 49]. Several prior works have quantified these privacy violations from LLMs by identifying personally identifiable information [31], and extracting potentially sensitive training data [8, 9, 40, 58]. However, the orthogonal problem of assessing encoded norms in LLMs has not been explored before. Understanding the norms encoded in LLMs can help ensure they adhere to socially acceptable norms, prevent inappropriate information leakage, and mitigate social and ethical harms [56].\nTo address the novel problem of assessing encoded norms in the context of LLMs, we use the theory of contextual integrity (CI) [41]. CI defines privacy as the appropriate flow of information according to contextual norms. Prior work have used CI to evaluate societal expectations in various sociotechnical systems [2, 3, 17, 32, 34, 35, 36], including the alignment of LLMs with human annotations [5, 21, 37]. However, the assessment of encoded norms is not trivial. Firstly, we conjecture that norms vary across different model types, capacities, hyperparameters, and optimization strategies (e.g, alignment [44, 46] and quantization [26]). Secondly, LLMs are affected by prompt sensitivity, where minor changes in phrasing can alter responses [7, 7, 12, 16, 28, 29, 48]. This issue has not been addressed in prior work [21, 37], making them unsuitable for our evaluation.\nTo tackle the above challenges, we developed LLM-CI, a modular open-source framework for running various LLMs with different optimizations, hyperparameters, and datasets using CI-based vignettes as prompts. We also introduce the multi-prompt assessment framework, which addresses prompt sensitivity by evaluating norms based only on prompts that produce consistent responses across variants. This approach enables comprehensive and reliable assessment of encoded norms in LLMs. We claim the following main contributions: we present"}, {"title": "2 Background and Related Work", "content": "We present a brief primer on LLMs (Section 2.1) and CI (Section 2.2), followed by describing related work at the intersection of CI and LLMs (Section 2.3) and evaluation of socio-technical properties in LLMs (Section 2.4)."}, {"title": "2.1 Large Language Models", "content": "Current state-of-the-art language models use transformers with billions of model parameters [6, 53]. These language text generation models are trained to predict the next tokens in a sentence given previous tokens. The model learns the distribution \\(Pr(x_1,x_2,...,x_n) = \\prod_{i=1}^{n}Pr(X_i | X_1,...,x_{i-1})\\) where \\(X_1,X_2,..., X_n\\) is a sequence of tokens taken from a given vocabulary. A neural network, \\(f_{\\theta}\\), with parameters \\(\\theta\\), is used to estimate this probability distribution by outputting the likelihood of token \\(x_i\\) given by \\(f_{\\theta}(x_i | X_1,..., X_{i-1})\\).\nDuring training, a language model learns to maximize the probability of the data in a training set containing text documents (e.g., news articles or webpages). Formally, the training involves minimizing the loss function \\(L(\\theta) = - log \\prod_{i=1}^{n} f_{\\theta} (x_i | X_1,..., X_{i-1})\\) over each training example in the training dataset. Once trained, a language model can generate new text conditioned on some prompt as prefix with tokens \\(x_1,...,x_i)\\) by iteratively sampling \\(x_{i+1} \\sim f_{\\theta}(x_{i+1} | X_1,..., X_i)\\) and then feeding \\(x_{i+1}\\) back into the model to sample \\(x_{i+2} \\sim f_{\\theta}(X_{i+2} | x_1,...,x_{i+1})\\).\nTraining LLMs is resource- and time-intensive, so pre-trained public models are fine-tuned for specific objectives before deployment. Popular fine-tuning optimization techniques include, alignment for matching with human annotations, and quantization reduce model capacity for efficiency.\nAlignment. Training data scrapped from the Internet can include inappropriate content such as hate speech, stereotypes) [20, 39, 52, 54] that LLMs might memorize and reproduce during inference. To address this drawback, LLMs are aligned through fine-tuning on human-annotated datasets that correct inappropriate responses. As a result, the model replaces inappropriate responses with a standard responses like"}, {"title": "2.2 Contextual Integrity", "content": "Contrary to predominant accounts of privacy that focus on aspects such as protecting sensitive information types [43], enforcing access control [45] or mandating procedural policies and purposes [13], the theory of CI defines privacy as an appropriate flow of information as governed by established societal norms [41]. According to CI, privacy is prima facie violated only when an information flow breaches an established contextual informational norm (aka CI norms or privacy norms), which reflect the values, purposes and function of a given context. A CI-based assessment of privacy implication of a system or a service involves two main phases: a) identifying the norm breaching flow using CI and b) examining the breach using the CI heuristic to determine how the novel flow contributes the values and purposes of the context.\nIdentifying the norm breaching flow. The CI framework requires identifying five essential parameters to capture the information flow and the establishes norms in a given context including: (i) roles or capacities of senders, subjects, and recipients in the context they operate (like professors in an educational context and doctors in the health context); (ii) the type of information they share; (iii) transmitted principle to state the conditions, for purposes or constraints under which the information flow is conducted. A canonical example below describes a typical interaction between a patient and a doctor."}, {"title": "2.3 Contextual Integrity and LLMs", "content": "A number of recent studies have applied CI to evaluate LLMs. Mireshghallah et al. [37] use CI and theory of mind to evaluate the alignment of LLMs with human annotated responses. They present, ConfAIde, a benchmark to use CI for LLMs with 98 prompts from Martin and Nissenbaum [33]. Their study shows that LLM responses have low correlation with human annotations, with GPT-4 demonstrating better alignment compared to other models. In a follow up work, Huang et al. [21] have used ConfAIde to investigate the alignment of 16 mainstream LLMs with human annotations. They find that \"most LLMs possess a certain level of privacy awareness\" as the probability of LLMs refusing to answer private information increases significantly when they are instructed to follow privacy policies or maintain confidentiality. Similar to results of Mireshghallah et al. [37], they show that Pearson's correlation between human and LLM agreement varies widely and ChatGPT has the highest correlation among other models.\nShao et al. [49] evaluate the norms of LLMs when used as agents with a focus on privacy norms in LLM-mediated communication (i.e., LLMs being used to send emails). They assess how well LLM responses align with crowd-sourced ground truth and measure privacy leakage from out-of-context information sharing.\nFan et al. [14] align LLMs with specific legal statutes to evaluate privacy violations and understand complex contexts for identifying real-world privacy risks. They generate synthetic cases and fine-tune their model to improve LLMs' ability to recognize privacy risks in actual court cases. However, their approach relies on limited number of expert-annotated norms and social contexts. To address these gaps, Li et al. [25] develop a comprehensive checklist that includes social"}, {"title": "2.4 Evaluating Sociotechnical Properties", "content": "Several benchmarks evaluate various LLMs sociotechnical properties such as toxicity, fairness, bias, sycophancy, privacy, robustness, and ethics [20, 37, 39, 52, 54].\nOn the other hand, LLMs have been shown to be sensitive to small variations in prompts which can drastically alter responses [7, 12, 16, 28, 29, 48]. Previous studies comparing LLM decision-making to human behavior often overlook this sensitivity. Loya et al. [28] demonstrate that simple prompt adjustments can make LLMs exhibit more human-like behavior, questioning the reliability of current evaluation methods [1]. There are limited studies consider prompt sensitivity: Lu et al. [30] propose generating synthetic prompts for better results. However, this is not suitable for assessing the encoded norms in LLMs as we require to query using CI-based vignettes and not synthetic prompts. Hence, a methodology for accounting for prompt sensitivity is largely an open problem.\nThere are a number of prior works that focus solely on assessing the leakage of sensitive data, including personally identifiable information to enhance LLMs privacy [8, 9, 31]. We can view them as assessment of a single CI parameter (data type), whereas a comprehensive CI approach requires all five parameters to make a privacy violation determination. Hence, these are orthogonal to our work."}, {"title": "3 Problem Statement", "content": "We aim to reliably extract and evaluate the contextual information norms embedded in LLMs. In this section, we present the research questions, challenges, and limitations of using closely related work.\nResearch Questions. We pose the following questions:\nRQ1 How can we develop a comprehensive framework to assess the encoded norms in LLMs at scale?\nRQ2 What methodology can reliably assess encoded norms?\nRQ3 How do different factors influence the encoded norms?"}, {"title": "4 LLM-CI Framework", "content": "We present LLM-CI, the the open-sourced CI norm assessment framework for LLMs to address C1 and answer RQ1.\nDesign. Figure 1 shows the modular design of LLM-CI, which comprises the Vignette, Inference, Clean-up, and Analysis & Plotting modules.\n1. Vignette module includes vignettes datasets. Similar to prior work [2, 3, 50, 59], we use a script to create all possible vignettes from the combinations of the five CI parameters and a vignette template. The resulting vignettes are saved in a csv file. LLM-CI includes datasets covering the following contexts: i) IoT devices [2], ii) COPPA regulations for IoT devices [3], iii) Internet privacy [36], iv) location data [35], v) public records [34], vi) privacy as a social contract [32].\nFor a chosen dataset, the module converts each vignette into a prompt before passing it to the LLM. Depending on the LLM, the module uses a corresponding prompt template (e.g., [INST] and [/INST] for Llama, |user| and assistant | for tulu). The prompt template also appends additional text to asking for the acceptability of the described information flow described in the vignette.\n2. Inference module requires users to provide a model description, after which it loads the pre-trained weights (e.g., from Huggingface or OpenAI), executes the model, and offers an API for sending prompts and receiving responses. The model runs on an inference engine for efficient execution with minimal overhead. For Huggingface models, we use vLLM [24], while the OpenAI models run on our custom implementation.\nAll the model descriptions include their capacity (e.g., 7B for seven billion parameters) along with the fine-tuning"}, {"title": "5 Methodology", "content": "We present the experimental setup and hyperparameter choices in LLM-CI for evaluation (Section 5.1), followed by our multi-prompt CI assessment methodology (Section 5.2)."}, {"title": "5.1 LLM-CI: Setup and Configuration", "content": "Vignette Module. We focus on two datasets: IoT [2], and COPPA [3]. IoT contains 6912 vignettes to gauge appropriateness of information flows in context of IoT devices [2]. COPPA contains 1800 vignettes to gauge the perceptions of information flows in context of IoT devices prescribed in Children's Online Privacy Protection Rule (COPPA) [3]. Both datasets have vignettes of the form:"}, {"title": "5.2 Multi-Prompt CI Assessment Methodology", "content": "We first empirically illustrate prompt sensitivity and then describe our proposed novel assessment methodology.\nIllustrating Prompt Sensitivity. We use ChatGPT to rephrase the original prompt template to generate ten different syntactic variants. For example, one prompt says: Please indicate the extent to which this scenario is acceptable: {scenario} and its variation is Please rate how acceptable this scenario is: {scenario}. For the full list of prompt variants, refer to Table 5 in the Appendix.\nProposed Assessment Methodology To address the prompt sensitivity challenge (C2), we propose a methodology that only evaluates norms from prompts with consistent responses across all the prompt variants (addressing RQ2). We quantify consistency using either simple majority (\u226550%) or super majority (\u226567%) of responses for each prompt variant. A stricter majority threshold and greater diversity in prompt variants both increase confidence in assessing encoded norms."}, {"title": "6 Evaluation", "content": "We now discuss how to reliably valuate CI norms and examine the factors influencing these norms to address RQ3, by way of addressing the following two questions:"}, {"title": "6.1 Assessing Encoded CI Norms in LLMs", "content": "We chose gpt-40-mini and llama-3.1-8B-Instruct, which produced the most consistent responses in our evaluation, to illustrate a subset of encoded norms (RQ3.1) on IoT. We omit the evaluation on COPPA due to space limitation. Figure 4 shows a sample output of LLM-CI's plotting module-a heatmap of the extracted norms for a fitness tracker as a sender. For a full set of extracted norms, refer to Figure 9 and 10 in the Appendix. The empty (gray) squares in the heatmap represent information flows where LLM-CI could not deduce the corresponding encoded norm due to a lack of sufficient number of prompts with valid responses: ten or more prompts for llama-3.1-8B-Instruct or three prompts for gpt-40-mini.\nOverall, compared to gpt-40-mini, llama-3.1-8B-Instruct is more conservative in its responses, with the majority of flows deemed \u201csomewhat unacceptable.\u201d A notable exception is the \"if the owner has given consent\u201d transmission principle. Under this transmission principle, the llama-3.1-8B-Instruct model viewed most information flows as \u201csomewhat acceptable,\" except when the fitness tracker shares information with \"government intelligence agencies.\u201d Furthermore, two specific information types \"audio [and video] of the owner\"-stand out. The model deemed the information flow \"strongly unacceptable\u201d when the information \u201cis stored indefinitely.\u201d This norm stance seems to align with the original survey result in [2] that found that \"fitness tracker sending recorded audio is considerably less acceptable than the same device sending exercise data.\" This is also reflected in gpt-40-mini, which sees sharing audio and video information types for a large number of transmission principles as unacceptable. While overall producing a more positive responses, gpt-40-mini views sharing information for advertising, indefinite storage consistently as \u201csomewhat unacceptable\" or \"strongly unacceptable.\""}, {"title": "6.2 Evaluating Influencing Factors", "content": "We now evaluate the influence of the following factors on the encoded CI norms RQ3.2: i) model type, ii) model capacity (7B models vs. 13B models), iii) alignment (base models vs. DPO), iv) quantization (base models vs. AWQ). To gauge a factor's influence, we use a multi-dimensional heatmap to show responses for each across four models in both datasets. We compare models with specific factor values, such as the 7B and 13B models to assess the impact of model capacity on norms. To ensure reliable norm extraction, we use a multi-prompt assessment that considers the majority norm across all prompts for each model.\nModel Type. We conjecture that CI vignettes produce different responses for different model type due to differences in their training datasets and prompt sensitivity. We used LLM-CI to extract encoded norms in ten LLMs (see Table 1 for a complete list) on both IoT and COPPA.\nVarious biases. For example, the model tulu-2-7B-AWQ produced largely \u201cstrongly unacceptable\u201d responses compared to llama-3.1-8B-Instruct where the responses were split between \u201csomewhat unacceptable\u201d and \u201csomewhat acceptable.\u201d\nOverall, we noted a significant variability in agreement on norms across various LLMs. For IoT, tulu-* models provided the same response for only 241 information flows and only five information flows in COPPA. The tulu-* and llama-3.1-8B-Instruct models agreed on ten information flows in IoT and none in COPPA. The tulu-* and gpt-40-mini-8B models agreed on the 207 information flows and only three in COPPA. The llama-3.1-8B-Instruct and gpt-40-mini-8B seemed to be the most aligned, agreeing on a total of 2519 information flows in IoT and 1107 in COPPA. As we discuss in Section 7, without knowing the exact datasets used to train the LLM models, we can only speculate about the differences for these apparent biases.\nModel Capacities. To evaluate the influence of model capacity, we compare the 7B models with the 13B models: tulu-2-7B (\u2714) with tulu-2-13B (\u25c1); and tulu-2-dpo-7B (\u25b3) with tulu-2-dpo-13B (\u25b7). Heatmap in Figure 6 shows the embedded norms for all four models related to information flows with the senders \"a fitness tracker\" (IoT) and \"smart watch\" (COPPA). The squares with same color for all four triangles represents consistent responses across all four models. Conversely, the different triangle colors reflect the inconsistencies in the models' responses.\nWe first focus on the responses with the same (or similar) color shades for all triangles to understand the norms that are consistent across different model capacities. For example, in IoT, perhaps reflective of the training dataset, all four models ranked the majority of flows involving \"a fitness tracker\" sharing information with \u201cthe government intelligence agencies\" (second column in each section of the corresponding data type) as \"somewhat unacceptable\u201d and \u201cstrongly unacceptable.\" For COPPA, all four models viewed information flows involving the transmission principle of \u201c[serving] contextual ads\u201d as \u201csomewhat unacceptable\u201d or \u201cstrongly unacceptable.\" This observation aligns with the prior work in [3] that found: \"Information flows with the transmission principle \"if the information is used to serve contextual ads\" have negative average acceptability scores across almost all senders, recipients, and attributes.\u201d Nevertheless, in contrast to the reported result in [3], the models also viewed information flows even if information \"is deleted\" as \"somewhat unacceptable\" or \"strongly unacceptable.\"\nFor several flows, however, in both IoT and COPPA, depend-"}, {"title": "7 Discussion and Conclusions", "content": "Our work builds on prior efforts aimed at the challenging task of evaluating the sociotechnical properties of LLM models. This task requires a deep understanding of both societal factors and the inner workings of the models. We discuss the limitations of our work and suggest future directions.\nEncoded norms provenance. While LLM-CI identifies encoded norms in LLMs, it does not trace their origin. The training datasets significantly impact a model's \"view of the world,\u201d and without dataset transparency, LLMs remain black boxes, making it difficult to understand their responses. Therefore, examining the dataset source is crucial to ensure LLMs are trained on valid and socially acceptable norms. For instance, 82% of the training data for falcon-180B includes a massive scrape of the Web, 6% books, 5% conversations (e.g., from Reddit, StackOverflow, HackerNews), 5% code, and 2% from technical sources like arXiv, PubMed, and USPTO2. Making the contents available could help shed light on the biases in the models' responses.\nCI privacy (ground truth) norms alignment. Having reliably identified encoded norms in LLMs, we can use LLM-CI to detect deviations from socially acceptable ground truths, which may be based on regulations, laws, or survey studies [2, 3, 50, 59]. Our CI-inspired approach can evaluate norms in LLMs across various settings and can be extended to compare with crowd-sourced ground truth on information flow acceptability, similar to Shao et al. [49], to measure privacy leakage or correlation with human annotations as in Mireshghallah et al. [37] and Huang et al. [21]. To ensure that the LLM norms align with the ground truths, we would need to fine-tune the models using alignment objectives (see Section 2.1). We consider this an area for future work.\nSummary. This paper introduces LLM-CI, the first open-source framework based on CI for evaluating contextual norms in LLMs. We propose a multi-prompt assessment methodology to extract encoded norms while addressing prompt sensitivity. Using LLM-CI, we evaluate norms in 10 LLMs, considering factors like model capacity, alignment, and quantization, and discuss their impact. Our work aims to provide a reliable evaluation guideline for future research."}]}