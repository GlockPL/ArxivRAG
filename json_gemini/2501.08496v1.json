{"title": "Quantifying the Importance of Data Alignment in Downstream Model Performance", "authors": ["Krrish Chawla", "Aryan Sahai", "Mario DePavia", "Sudharsan Sundar", "Brando Miranda"], "abstract": "Contrary to the conventional emphasis on dataset size, we explore the role of data alignment - an often overlooked aspect of data quality - in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled interventional experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training", "sections": [{"title": "1 Introduction", "content": "Research within the domain of Large Language Models (LLMs) has historically placed an emphasis on the size of datasets used for pre-training, claiming it is one of the primary determinants of LLM performance Chowdhery et al. (2022); Nostalgebraist (2022); OpenAI (2023); Google (2023b). Empirical evidence demonstrates this trend, as models trained on large datasets exhibit superior performance. Notably, GPT-4, with its conjectured 1 petabyte dataset, markedly surpasses GPT-3\u2014which is trained on a comparatively modest 45 terabytes in terms of response quality and contextual accuracy OpenAI (2023). However, emerging research indicates that other dimensions, such as dataset diversity, play a crucial role in the efficacy of LLMs, with high-performing models often arising from datasets with high diversity coefficients Lee et al. (2023).\nCurrent discourse predominantly highlights the scale of a dataset as a pivotal factor in its capacity to effectively pre-train or fine-tune a model, with emphasis frequently placed on quantitative metrics-specifically, the sheer size of the dataset. Lee et al. (2023) This investigation, however, seeks to shift this paradigm to consider qualitative assessments, notably the alignment of datasets with the specific evaluation tasks. Building upon methodologies established in previous studies for quantifying dataset alignment, our research aims to examine the role of data quality in the pre-training and fine-tuning process, verifying the hypothesis that increased data alignment could significantly improve LLM performance. This paradigm challenges the emphasis on dataset size, suggesting an alternative approach to dataset importance and optimization in the context of LLM training \u2013 i.e., select the most aligned data to your target task. We explore this via Autoformalization.\nAutoformalization is defined as the transformation of concepts in natural language to formalized, structured language like mathematical proofs or code. The creation of a proficient Autoformalization tool would not only drastically reduce the substantial costs associated with manual formalization efforts but could also serve as a bridge linking the automated theorem verification and computational algebra with the extensive body of mathematical knowledge predominantly recorded in natural language. Moreover, the capacity for Autoformalization underscores a machine's adeptness at navigating the subtleties of human language and the precision required by formal linguistic systems Wu et al. (2022).\nWe employ a comprehensive evaluation by comparing the performance fine-tuned LLMs quantitatively aligned data sets against those calibrated primarily for scale. We engage a broad spectrum of Autoformalization tasks across different domains and complexities, ensuring the thoroughness and robustness of our results."}, {"title": "2 Methods", "content": "Our experiment is designed to explore the hypothesis that there exists a negative correlation between the alignment score of a dataset with a benchmark and the perplexity score (see"}, {"title": "2.1 Conceptual Framework", "content": "The alignment score is a critical metric in our analysis, offering insights into the degree of congruence between a dataset and the chosen benchmark for evaluating downstream performance, such as Autoformalization. We posit that an LLM trained on a dataset that mirrors the characteristics of the benchmark will demonstrate superior performance. This performance is quantitatively measured using the perplexity score. For example, in the fine-tuning setting we measure model perplexity on the debug1AF benchmark, where lower scores denote higher model accuracy and effectiveness."}, {"title": "2.2 Dataset Alignment Quantification", "content": "To quantify dataset alignment, we employ the Task2Vec Alignment Coefficient, which facilitates a rigorous comparative assessment of dataset similarity Lee et al. (2023).\nThe alignment coefficient between two datasets, D\u2081 and D2, is calculated as:\n$align(D_1, D_2) = 1 \u2212 E_{B_1~D_1,B_2~D_2}[d(f(B_1), f(B_2))]$ (1)\nwhere E denotes the expectation over batches B\u2081 and B2 sampled from datasets D\u2081 and D2, respectively, and $d(f(B_1), f(B_2))$ represents the distance between the embeddings of these batches, derived through the Task2Vec framework.\nFor the purposes of our experimental framework, we consider the alignment of the entire dataset rather than focusing solely on specific subsets. We assume that the alignment properties of a dataset subset are reflective of the dataset as a whole. Consequently, our alignment evaluations are predicated on the comprehensive dataset, offering a holistic view of dataset congruence and its impact on model performance."}, {"title": "3 Experiments & Results", "content": null}, {"title": "3.1 Effects of Data Alignment Between Pre-Training and Evaluation Data", "content": null}, {"title": "3.1.1 EXPERIMENTAL SETUP AND MOTIVATION", "content": "To evaluate the effect of data alignment between pre-training data and downstream task, we pre-train 51M parameter GPT-2 models (Radford et al., 2019) for 1.31B tokens on one of three datasets: PubMed Abstracts, a dataset of medicine-related abstracts; USPTO Backgrounds, a dataset of patent application background sections; and a dataset produced by concatenating USPTO and PubMed Abs.\nBy controlling for all training hyperparameters aside from the pretraining dataset, we minimize the effect of confounding variables on relationship between data alignment and downstream performance. We proceed to evaluate these pre-trained models on a variety of evaluation datasets, which vary in terms of their similarity to the three pre-training datasets, both empirically in terms of the alignment coefficient and qualitatively based on"}, {"title": "3.1.2 PRE-TRAINING EXPERIMENT RESULTS", "content": null}, {"title": "3.2 Effects of Data Alignment Between Fine-Tuning and Evaluation Data", "content": null}, {"title": "3.2.1 EXPERIMENTAL SETUP AND MOTIVATION", "content": "In order to test whether an LLM will be better able to perform AF when fine-tuned on a dataset that is closely aligned to the AF benchmark, we must finetune LLMs on datasets of differing alignment to the benchmark. This allows us to observe a relationship between alignment and perplexity loss.\nWe strategically chose the following datasets to run our experiment on in order to ensure a range of alignment values in our results:\n1. AF Dataset (AF): A dataset consisting of informal statements and their formal counterparts in Isabelle designed for training LLMs to perform Autoformalization. We use its test set as a benchmark of LLM performance on statement Autoformalization. Thus, we also believe it will result in the lowest perplexity among the proof datasets when used to train an LLM for AF Miranda (2021).\n2. Destructed AF Dataset (AF-split): This dataset is composed of the AF Dataset's formal and informal statements but the two are split into different lines so that the LLM trains on data that does not explicitly indicate a relationship between the two; we expect this to still obtain a relatively low perplexity score given its high alignment.\n3. The Stack Smol Python Docstrings dataset (Docstring): a dataset consisting of concise function headers written in informal language and their implementations in python; we use it to assess how well coding datasets can finetune for Autoformalization Bird (2023a).\n4. The Stack Dedup Python Docstrings 1.0 percent unified dataset (Docstring 2): A dataset consisting of function headers written in informal language and their implementations in python; given its nature we anticipate it scoring among the lowest of perplexity scores against the Docstring benchmark Bird (2023b).\n5. C4-EN-10K Dataset (C4): A ten-thousand-entry subset of a database composed of text pulled from Common Crawl (an internet archive) meant for pre-training for general English language modeling. Given it's entries are all informal statements not related to mathematics, we predict a high perplexity score in performing AF Raffel et al. (2019).\n6. wikitext-2-raw-v1 Dataset (Wikitext): A subset of the Wikitext dataset; Wikitext is a dataset composed of text taken from Wikipedia pages that met the score guidelines to qualify as either a 'good' or 'featured' article; given its nature and lack of relevance to AF, we expect a high perplexity score Merity et al. (2016).\n7. minif2f-lean4 Dataset (LeanDojo4): A subset of the miniF2F dataset which is comprised of math exercise statements and their formal counterparts in lean; given that it is in a different formal language, we expect a mid-range perplexity score Zheng et al. (2021)."}, {"title": "3.3 Analysis of results", "content": "Introduction of Data Alignment in LLM Training: A novel approach that integrates data alignment as a key factor in the training of Large Language Models, leading to improved model performance.\n1. Empirical Evidence of Alignment Impact: Through systematic experimentation, the paper provides empirical evidence that higher alignment between training data and the target domain leads to a decrease in perplexity scores, indicative of enhanced model accuracy.\n2. Analysis Across Multiple Datasets: The study conducts a comprehensive analysis across a variety of datasets, establishing the consistency of the negative correlation between data alignment and perplexity across both proof and code datasets."}, {"title": "3.3.1 PROOF DATASET RESULTS", "content": "For each dataset we calculated the alignment scores using Task2Vec Alignment Coefficient as depicted in Table 1. Our final perplexity scores for each of our models trained can be found in Table 2. This can be difficult to visualize, so we plotted our results as shown in Figure 2.\nOur results are significant in validating our initial thesis that a highly aligned data is capable of producing an LLM that performs better than one that was trained on a dataset with lower alignment.\nWe found that an untuned, standard gpt-2 LLM received a perplexity score of 78.7413. However, after finetuning it on AF it received the best perplexity score in our results: 41.8261. This further bolsters our claim as AF-AF also had the greatest alignment (approximately 0.945) and the best performance as well.\nThe proofnet dataset did not perform as well as AF fine tuned, with a perplexity score of 67.8906 and alignment of 0.67. However, this is expected based on our thesis as we see that a drop in alignment contributes to an increase in perplexity score for the model.\nThe C4 dataset has a much lower score in alignment (approximately 0.32) compared to AF (approximately 0.95). Judging by this metric alone, we would expect to see a higher perplexity than gpt-2 finetuned by the debug1AF dataset based on our thesis. When fine-"}, {"title": "3.4 Impact of Data Alignment versus Dataset Size on LLM Performance", "content": "This experiment was designed to explicitly compare the impact of data alignment with the downstream task against the size of the dataset used for fine-tuning. We hypothesized that a smaller, highly aligned dataset would lead to better LLM performance on the downstream task of Autoformalization, as measured by perplexity loss, compared to a larger but less aligned dataset.\nTwo datasets were used for fine-tuning a pre-trained GPT-2 model:\n1. A small dataset, extracted directly from the debuglAF benchmark, comprising approximately 1.4k tokens. This dataset was expected to have high alignment (close to 1) with the Autoformalization task, given its direct sampling from the task's benchmark.\n2. A larger, mixed dataset designed to have a lower alignment score of 0.54 with the debug1AF benchmark. The dataset size was significantly larger than the first (approximately 4100 tokens), intended to test the effect of dataset size versus alignment."}, {"title": "Broader Impact Statement", "content": "Validating our hypothesis, indicating a consistent improvement in Autoformalization results through data alignment, would have significant implications for the NLP field, particularly given the recent surge in attention towards LLMs. By demonstrating the critical role of data quality and alignment in optimizing LLM performance Lee et al. (2023), our work challenges the prevailing emphasis on dataset size as the principal metric for pre-training efficacy. Furthermore, our findings advocate for a more strategic allocation of computational resources, potentially leading to substantial time and cost savings in the development of LLMs across both academic and industry spheres."}, {"title": "Appendix A. Detailed Pre-Training Alignment Experiment Results", "content": "In Table 3, we detail the specific alignment coefficient values between (pre-)training and evaluation data with 95% confidence intervals. Once again, we observe that increased alignment coefficients between train and evaluation data show a strong trend of leading to lower evaluation loss."}, {"title": "Appendix B. Perplexity Calculation", "content": "Perplexity serves as a measure of a model's prediction accuracy, with lower values indicating better performance. It is calculated using the following formula:\n$PPL(X) = exp \\{- \\frac{1}{T}  \\sum_{i} log p_{\\theta}(x_i | x_{<i})\\}$  (2)\nwhere PPL(X) denotes the perplexity of sequence X, T is the total number of tokens in X, xi is the ith token, x<i represents all tokens preceding xi, and log po(xi|x<i) is the log-likelihood of token xi given its preceding context as predicted by the model parameters \u03b8."}, {"title": "Appendix C. Data Tables for Controlled Alignment Experiments", "content": null}, {"title": "Appendix E. Experiment to Verify That Each Subset Will Have a Similar Perplexity Loss to That of the Entire Dataset", "content": "We have so looked at the perplexity loss of one subset of the dataset on which we have trained on rather than the perplexity score of the entire dataset. However, we conducted an experiment to show that these two values are comparable. We have kept the token sizes around 4000 tokens as such:"}, {"title": "E.1 Discussion of C4 Subset Experiment Results", "content": "As seen in Table 5, each subset of C4 has comparable perplexity scores. This is further high-lighted in the graph where we can see that the subsets are all closely clustered together; this does not affect our line of regression significantly and our claim still holds. This experiment serves as a proof-of-concept that a subset of a dataset can be used to approximate the subset of the entire dataset."}, {"title": "Appendix F. Experiment on splitting Formal and Informal Statements in the Training Process:", "content": "So far we have pre-processed our data as depicted in Figure 3, where each input contains a formal and informal statement (proof dataset) or code and docstring (code dataset). However, we conducted an experiment to observe if inputting formal and informal statements as separate inputs and training on that would produce better results. Figure 5 depicts what this would look like.\nWe compared the results of AF and AF-Split as follows. We first standardized the number of tokens to 4000 as seen in 6\nThen, we calculated the alignment as shown in Table 1."}, {"title": "F.1 Discussion of AF-Split Experiment Outcomes", "content": "The investigation revealed a discernible reduction in alignment for the AF-Split dataset by approximately 21.7 percent, which constitutes a moderate deviation. Furthermore, there was a notable increase in perplexity loss for AF-Split, approximately 38.2 percent under-scoring a significant impact. These findings suggest that models are more adept at Auto-formalization tasks when trained on datasets that present related information cohesively, rather than on datasets where related content is disjointed. Specifically, models excel in Autoformalization when they can discern the intrinsic connection between an informal and a formal statement, as exemplified in the format \u201cInformal Statement Formal State-ment ,\u201d implying an inherent correlation. Conversely, when such relational cues are absent, as in the case of AF-Split where informal and formal statements are segregated, model performance in Autoformalization tasks diminishes."}, {"title": "F.2 Related Work (Cont.)", "content": "The article Google (2023a) \u201cPaLM 2 Technical Report\" by Google discusses the development and performance of PaLM 2. The study showcases PaLM 2's versatility but also emphasizes the role of architectural enhancements and diverse model objectives in achieving superior results. The inclusion of a diverse data mixture, even incorporating a small amount of translation pairs, results in performance comparable to dedicated translation services, a state-ment which supports our belief that data quality can be a critical factor in determining how well a dataset can train an LLM. This sentiment is also expressed in the article \"Model Performance Scaling with Multiple Data Sources\u201d by Tatsunori Hashimoto.Hashimoto (2021) It discusses the challenges of training ML models using data from various sources that vary"}]}