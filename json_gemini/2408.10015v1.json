{"title": "Deterministic Policy Gradient Primal-Dual Methods\nfor Continuous-Space Constrained MDPS", "authors": ["Sergio Rozada", "Dongsheng Ding", "Antonio G. Marques", "Alejandro Ribeiro"], "abstract": "We study the problem of computing deterministic optimal policies for constrained Markov\ndecision processes (MDPs) with continuous state and action spaces, which are widely encountered in\nconstrained dynamical systems. Designing deterministic policy gradient methods in continuous state\nand action spaces is particularly challenging due to the lack of enumerable state-action pairs and the\nadoption of deterministic policies, hindering the application of existing policy gradient methods for\nconstrained MDPs. To this end, we develop a deterministic policy gradient primal-dual method to\nfind an optimal deterministic policy with non-asymptotic convergence. Specifically, we leverage\nregularization of the Lagrangian of the constrained MDP to propose a deterministic policy gradient\nprimal-dual (D-PGPD) algorithm that updates the deterministic policy via a quadratic-regularized\ngradient ascent step and the dual variable via a quadratic-regularized gradient descent step. We\nprove that the primal-dual iterates of D-PGPD converge at a sub-linear rate to an optimal regularized\nprimal-dual pair. We instantiate D-PGPD with function approximation and prove that the primal-dual\niterates of D-PGPD converge at a sub-linear rate to an optimal regularized primal-dual pair, up to a\nfunction approximation error. Furthermore, we demonstrate the effectiveness of our method in two\ncontinuous control problems: robot navigation and fluid control. To the best of our knowledge, this\nappears to be the first work that proposes a deterministic policy search method for continuous-space\nconstrained MDPs.", "sections": [{"title": "1 Introduction", "content": "Constrained Markov decision processes (MDPs) are a standard framework for incorporating system\nspecifications into dynamical systems [Altman, 2021, Brunke et al., 2022]. In recent years, constrained"}, {"title": "2 Preliminaries", "content": "We consider a discounted constrained MDP, denoted by the tuple (S, A, p, r, u, b, \u03b3, \u03c1). Here, S C Rds\nand A C Rda are continuous state-action spaces with dimensions ds and da, and bounded actions\n||a|| \u2264 Amax for all a \u2208 A; p(\u00b7 | s, a) is a probability measure over S parametrized by the state-action\npairs (s, a) \u2208 S \u00d7 A; r, u: S \u00d7 A \u2192 [0, 1] are reward/utility functions; b is a constraint threshold;\n\u03b3\u2208 [0, 1) is a discount factor; and p is a probability measure that specifies an initial state. We consider\nthe set of all deterministic policies II in which a policy \u03c0: S \u2192 A maps states to actions. The transition\np, the initial state distribution p, and the policy \u03c0 define a probability distribution over trajectories\n{St, At, rt, Ut}\u221et=0, where so ~ p, at = \u03c0(st), rt = r(st, at), ut = u(st, at) and st+1 ~ p(\u00b7 | st, at) for all\nt \u2265 0. Given a policy \u03c0, we define the value function V\u2122: S \u2192 R as the expected sum of discounted\nrewards\n\\(V^{\\pi}(s) :=\\mathbb{E}_{\\pi} \\left[\\sum_{t=0}^{\\infty} \\gamma^{t} r\\left(S_{t}, a_{t}\\right) \\mid S_{0}=s\\right] .\\)"}, {"title": "2.1 Zero Duality Gap", "content": "With the convexity of the deterministic value image VD in hand, we next establish zero duality gap for\nProblem (1). We begin with a standard feasibility assumption.\nWe dualize the constraint by introducing \u5165 \u2208 R+ and the Lagrangian L(\u03c0, \u03bb) := V,(\u03c0) + AV,(\u03c0)\nfor Problem (1). For a fixed \u5165, let \u03a0(\u03bb) be the set of Lagrangian maximizers. The Lagrangian\nL(\u03c0, \u03bb) is equivalent to the value function V\u2081(\u03c0) associated with the combined reward/utility function\nrx(s, a) = r(s, a) + Ag(s, a). The dual function D(X) := max\u03c0\u03b5\u03c0 V\u5165(\u03c0) is an upper bound of Problem\n(1), and the dual problem searches for the tightest primal upper bound\n\\(\\min _{\\lambda \\in \\mathbb{R}_{+}} D(\\lambda) .\\)\nWe denote by V* the optimal value of the dual function, where X* is a minimizer of the dual Problem (2).\nDespite being non-convex in the policy, if we replace the deterministic policy space in Problem (1) with\nthe stochastic policy space, then it is known that Problem (1) has zero duality gap [Paternain et al., 2019].\nThe proof capitalizes on the convexity of the occupancy measure representation of (1) for stochastic\npolicies. However, this occupancy-measure-based argument does not carry to deterministic policies,\nsince the occupancy measure representation of Problem (1) is non-convex when only deterministic\npolicies are used [Dolgov, 2005]. Instead, we leverage the convexity of the deterministic value image\nVD to prove that strong duality holds for Problem (1); see Appendices A and C.2 for more details and\nthe proof."}, {"title": "2.2 Constrained Regulation Problem", "content": "We illustrate Problem (1) using the following example\n\\begin{aligned}\n&\\max _{\\pi \\in \\Pi} \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^{t} \\operatorname{sG}_{1} s_{t}+a_{t}^{\\top} R_{1} a_{t}\\right]  &\\\\\n&\\text { s. t. } \\mathbb{E}\\left[\\sum_{t=0}^{T} \\gamma^{t} \\operatorname{sG}_{2} s_{t}+a_{t}^{\\top} R_{2} a_{t}\\right] \\geq b &\\\\\n& -b_{s} \\leq C_{s} s_{t} \\leq b_{s}, -b_{a} \\leq C_{a} a_{t} \\leq b_{a} &\\\\\n& s_{t+1}=B_{0} s_{t}+B_{1} a_{t}+w_{t}, s_{0} \\sim \\rho &\n\\end{aligned}\nwhere B0 \u2208 Rds xds and B\u2081 \u2208 Rdsxda denote the system dynamics, w\u2081 represents zero-mean Gaussian\nnoise, p is the initial state distribution, and G1, G2 \u2208 Rdsxds and R1, R2 \u2208 Rdaxda are negative\nsemi-definite reward matrices. The constraint value is b, with Cs \u2208 Rdsxds, Ca \u2208 Rdaxda, bs \u2208 Rds,\nand ba \u2208 Rda specifying the state-action space constraints. A typical case is when Cs and Ca are\nidentity matrices, and bs, ba limit state and action ranges. Equations (3a), (3c), and (3d) describe the\nconstrained regulation problem under Gaussian disturbances [Bemporad et al., 2002, Stathopoulos\net al., 2016], where the optimal policy \u03c0 is deterministic [Scokaert and Rawlings, 1998], allowing us\nto consider only deterministic policies II. We add an extra constraint defined in (3b). The Markovian\ntransition dynamics in (3d) are linear, and the Gaussian noise wt is non-atomic, rendering the transition\nprobabilities non-atomic. If p is non-atomic, the underlying MDP of (3) is also non-atomic. The reward"}, {"title": "3 Method and Theory", "content": "While our problem has duality gap zero, finding the optimal \\* poses a significant challenge, due to\nthe presence of multiple saddle points in the Lagrangian. To address it, we resort to regularization\ntechniques. More specifically, we introduce two regularizers. First, the term h(x) := x\u00b2 promotes\nconvexity in the Lagrange multiplier \u5165. Second, the term ha(a) := -||a||2 promotes concavity in the\nreward function r by penalizing large actions selected by the policy \u03c0. The associated value function is\ndefined as H\u2122 (s) := \u0395\u03c0 [\u2211t=0 tha(at) | s], and leads to the regularizer \u0397(\u03c0) := \u0395\u03c1[H", "lambda)": "V_{\\lambda}(\\pi)+\\tau H(\\pi)+\\frac{\\tau}{2} h(\\lambda) .\\)\nwhere \u03c4 \u2265 0 is the regularization parameter and L\u2081(\u03c0, \u03bb) is the regularized Lagrangian. For a fixed \u5165,\nthe objective of Problem (4) is equivalent to an unconstrained regularized MDP plus a regularization of\nthe dual variable. Consider the composite regularized reward function ra,+(s, a) := r(s, a) + Ag(s, a)\u2212\nha(a). The value function associated with the reward function ra, can be expressed as V\u03bb,\u03c4(\u03c0) =\nVx(\u03c0) + H(\u03c0). Then, we can reformulate the regularized Lagrangian as L\u2081(\u03c0, \u03bb) := Vx,7(\u03c0) + 512.\nThe global saddle points of the regularized Lagrangian \u220f \u00d7 A are guaranteed to exist; see Lemma 13\nin Appendix C. Moreover, a global saddle point (\u03c0, *) satisfies\n\\(V_{\\lambda^{*}}(\\pi)+H(\\pi) \\leq V_{\\lambda^{*}}\\left(\\pi^{*}\\right) \\leq V_{\\lambda^{*}}(\\pi)+\\frac{\\tau}{2} h\\^{2}\\)\nfor all (\u03c0, \u03bb) \u0395 \u03a0 \u00d7 \u039b. Hence, (\u03c0, \u03bb\u207a) is also a global saddle point of the original Lagrangian L(\u03c0, \u03bb)\nup to two T-terms."}, {"title": "3.1 Deterministic Policy Search Method", "content": "We propose a deterministic policy gradient primal-dual (D-PGPD) method for finding a global saddle\npoint (\u03c0, *) of L\u2081(\u03c0, \u03bb). In the primal update, as is customary in RL, we maximize the advan-\ntage function rather than the value function directly. Specifically, we use the regularized advantage\nA,(s,a) := Q,(s, a) - V(s) -(||a||2 \u2013 ||\u03c0(s)||2) associated with the regularized reward \u03b3\u03bb,\u03c4\u00b7\nThe primal update (6a) performs a proximal-point-type ascent step that solves a quadratic-regularized\nmaximization sub-problem, while the dual update (6b) performs a gradient descent step that solves a"}, {"title": "3.2 Non-Asymptotic Convergence", "content": "Finding deterministic optimal policies is a computationally challenging problem [Feinberg, 2000,\nDolgov, 2005]. To render the problem tractable, we assume concavity and Lipschitz continuity of the\nregularized action value functions."}, {"title": "4 Function Approximation", "content": "To instantiate D-PGPD (6) with function approximation we begin by expanding the objective in (6a)\nand dropping the terms that do not depend on the action a,\n\\( \\pi_{t+1}(s)=\\underset{a \\in A}{\\arg \\max } \\tilde{A}_{\\tau}^{\\pi_{t}}(s, a)-\\frac{\\tau}{2\\eta}\\left(\\frac{\\tau}{2 \\eta}+\\tau\\right)\\|a\\|^{2} .\\)\nThe usual function approximation approach [Agarwal et al., 2021, Ding et al., 2022] is to introduce a\nparametric estimator of the policy \u03c0, and a compatible parametric estimator of the action value function\nQ7. Instead, we approximate the augmented action-value function J\"(s, a) := Q^,,(s, a) + (s) \u03a4\u03b1\nusing a linear estimator Jo(s,a) = $(s, a). At time t, we estimate J\u2122t (s, a) by computing the\nparameters \u03b8\u0165 via a mean-squared-error minimization\n\\(\\theta_{t}:=\\underset{\\theta}{\\arg \\min } \\mathbb{E}_{(s, a) \\sim \\nu}[\\|\\phi(s, a)^{\\top} \\theta-J^{\\pi_{t}}(s, a)\\|^{2}],\\)\nwhere v is a pre-selected state-action distribution. Problem (8) can be easily addressed using, e.g.,\nstochastic approximation. A subsequent policy \u03c0t+1 results from a primal update based on Jor. This\nleads to an approximated D-PGPD algorithm (AD-PGPD) that updates \u03c0\u0165 and A\u0165 via\n\\begin{aligned}\n&\\pi_{t+1}(s)=\\underset{a}{\\arg \\max } J_{\\theta_{t}}(s, a)-\\left(\\frac{\\tau}{\\eta}+2 \\tau\\right)\\|a\\|^{2} \\\\\n& \\lambda_{t+1}=\\underset{\\lambda \\in \\Lambda}{\\arg \\min } \\eta\\left(V_{g}(\\pi_{t})+\\tau \\lambda_{t}\\right)+\\frac{1}{2\\eta}\\|\\lambda-\\lambda_{t}\\|^{2} .\n\\end{aligned}\nWe remark that solving the sub-problem (9a) requires inverting the gradient of (9a) with respect to\na, which can be challenging when the model of the MDP is unknown or when value-functions cannot be\ncomputed in closed form. Addressing this problem is the subject of Section 5, but first, we analyze the\nconvergence of the primal-dual iterates of (9)."}, {"title": "4.1 Non-Asymptotic Convergence", "content": "To ease the computational tractability of AD-PGPD, we assume concavity of the approximated aug-\nmented action-value function and bounded approximation error."}, {"title": "5 Model-Free Algorithm", "content": "When the model of the MDP is unknown or when value-functions cannot be computed in closed form,\nwe can leverage sample-based approaches to compute the primal and dual iterates of AD-PGPD defined\nin (9a) and (9b). To that end, we assume access to a simulator of the MDP from where we can sample\ntrajectories given a policy \u03c0. The sample-based algorithm requires modifying the policy evaluation step"}, {"title": "6 Computational Experiments", "content": "We demonstrate the effectiveness of D-PGPD in two classical continuous constrained control problems:\nrobot navigation and fluid control, as depicted in Figure 1. For problem details and more experimental\nresults, please see Appendix F.\nNavigation Problem. An agent moves in a horizontal plane following some linearized dynamics\nthat are characterized by the double integrator model with zero-mean Gaussian noise [Shimizu et al.,\n2020, Ma et al., 2022]. We aim to drive the agent to the origin, but constraining its velocity. When\nthe dynamics are known and the reward function linearly weights quadratic penalties on position and\naction, this problem is an instance of the constrained linear regulation problem [Scokaert and Rawlings,"}, {"title": "7 Concluding Remarks", "content": "We have presented a deterministic policy gradient primal-dual method for continuous state-action con-\nstrained MDPs with non-asymptotic convergence guarantees. We have leveraged function approximation\nto make the implementation practical and developed a sample-based algorithm. Furthermore, we have\nshown the effectiveness of the proposed method in a classical navigation problem and a non-linear fluid\ncontrol problem under constraints. For future directions, our work stimulates many interesting problems\non constrained MDPs with continuous state-action spaces: (i) minimal assumption on value functions;\n(ii) online exploration; (iii) optimal sample complexity; (iv) general function approximation."}, {"title": "A Convexity of Value Images", "content": "Strong duality holds for Problem (1), as established in Theorem 1, despite the non-convexity of the\nvalue functions V\u2084(\u03c0) and V\u338f(\u03c0) with respect to the policy \u03c0. This section aims to shed some light into\nthe reasons behind this phenomenon. First, recall that the policy class I considered in this paper is\nrestricted to deterministic policies. Importantly, this set is not necessarily convex. For any a \u2208 [0,1]\nand two deterministic policies \u03c0, \u03c0' \u0395 \u03a0, their convex combination\n\\(\\pi_{\\alpha}=\\alpha \\pi+(1-\\alpha) \\pi^{\\prime},\\)\ndoes not generally yield to a deterministic policy, i.e., \u03c0\u03b1 \u2209 \u03a0. Furthermore, consider the vector value\nfunction\n\\(V(\\pi):=\\begin{bmatrix}V_{r}(\\pi) \\\\ V_{g}(\\pi)\\end{bmatrix}\\)\nassociated with a given policy \u03c0. The value function V(\u03c0) is non-convex in \u03c0. However, let us now\nfocus on the set of all attainable vector value functions corresponding to deterministic policies, which\ndefines the deterministic value image:\n\\(V_{\\mathcal{D}}:=\\{V(\\pi) \\mid \\pi \\in \\Pi\\} .\\)\nUnder the non-atomicity assumption (see Lemma 3), the set VD is convex. This convexity implies that\nthere exists a policy \u03c0\u03b1 \u0395 \u03a0 such that\n\\(V\\left(\\pi_{\\alpha}\\right)=\\alpha V(\\pi)+(1-\\alpha) V\\left(\\pi^{\\prime}\\right),\\)\neven though \u03c0\u03b1 is not a convex combination of \u03c0and \u03c0', and the vector value function V remains\nnon-convex.\nLastly, we consider the value image Vr for all policies. Under the non-atomicity assumption, these\ntwo sets, VD and Vr, are equivalent (see Lemma 4). Consequently, the optimal policy is contained\nwithin VD. Therefore, restricting the search space to deterministic policies is justified in the context of\nnon-atomic MDPs. The convexity of the deterministic value image and its equivalence with the value\nimage for all policies are illustrated in Figure 4."}, {"title": "B Supporting Lemmas", "content": "Lemma 2 (Discounted and uniformly absorbing MDP equivalence). A non-atomic discounted MDP\ncan be equivalently represented as a non-atomic uniformly absorbing MDP.\nProof. See Feinberg and Piunovskiy [2019, Lemma 3.12]."}, {"title": "C Proofs", "content": null}, {"title": "C.1 Proof of Existence of Global Saddle Points", "content": "Lemma 13 (Existence of global saddle points). There exists a primal-dual pair (\u03c0, \u03bb\u207a) \u2208 \u03a0\u00d7A\nsuch that L\u2081(\u03c0, \u03bb) \u2265 L(,) \u2265 L\u2084(\u03c0, \u03bb). Furthermore, the following property holds for all\n(\u03c0, \u03bb) \u0395 \u03a0 \u00d7 \u039b,\n\\(V_{\\lambda^{*}}(\\pi)+\\frac{\\tau}{2} h(\\lambda) \\leq V_{\\lambda^{*}}\\left(\\pi^{*}\\right) \\leq V_{\\lambda^{*}}(\\pi)+\\frac{\\tau}{2} h_{\\lambda^{*}}^{2}\\)\nProof. We know that the Lagrangian L(\u03c0, \u03bb) has global saddle points due to strong duality; see Theorem\n1. We want to show that the regularized Lagrangian L\u2081(\u03c0, \u03bb) has global saddle points too. With that\ngoal in mind, we consider the following regularized problem\n\\begin{array}{cl}\n\\max _{\\pi \\in \\Pi} & V_{r}(\\pi)+\\tau H(\\pi) \\\\\n\\text { s. t. } & V_{g}(\\pi) \\geq 0\n\\end{array}"}, {"title": "C.2 Proof of Theorem 1", "content": "Proof. To establish zero duality gap we consider the perturbed function of Problem (1). For any \u03b4 \u2208 R,\nthe perturbation function P(d) associated with (1) is defined as follows\n\\(P(\\delta):=\\underset{\\pi \\in \\Pi}{\\max } V_{r}(\\pi)\\)\n\\(\\text { s. t. } V_{g}(\\pi) \\geq \\delta .\\)\nThe proof relies on Lemma 8, which states that if Slater's condition holds for (1) and its perturbation\nfunction P(d) is concave on d, then (1) has zero duality gap, even if the problem in (1) is non-convex.\nTherefore, we have to show that P(d) is convex. More precisely, we need to establish that for a\nperturbation \u03b4\u03b1 := \u03b1\u03b4 + (1 \u2212 \u03b1)\u03b4', where a \u2208 [0, 1], it holds that\n\\(P\\left(\\delta_{\\alpha}\\right):=P\\left(\\alpha \\delta+(1-\\alpha) \\delta^{\\prime}\\right) \\geq \\alpha P(\\delta)+(1-\\alpha) P\\left(\\delta^{\\prime}\\right)\\)\nLet \u03c0 and \u03c0' be the policies that achieve the maximum value of P(8) for perturbations \u03b4 and \u03b4'\nrespectively. This implies\n\\(V_{g}(\\pi) \\geq \\delta \\text { and } V_{g}\\left(\\pi^{\\prime}\\right) \\geq \\delta^{\\prime} .\\)\nNow, for a given policy \u03c0, we consider the vector value function V(\u03c0) := [V\u2084(\u03c0), V,(\u03c0)]\u03a4. We define\nthe deterministic value image as the span of value functions associated with the class of deterministic\npolicies II as:\n\\(V_{\\mathcal{D}}:=\\{V(\\pi) \\mid \\pi \\in \\Pi\\} .\\)\nThe deterministic value image VD is convex for non-atomic MDPs (see Feinberg and Piunovskiy\n[2019, Corollary 3.10]. This implies that there exists \u03c0\u03b1 \u2208 \u03a0 such that V(\u03c0\u03b1) := \u03b1V(\u03c0)+(1-a)V(\u03c0').\nMore precisely:\n\\begin{aligned}\nV_{r}\\left(\\pi_{\\alpha}\\right) &=\\alpha V_{r}(\\pi)+(1-\\alpha) V_{r}\\left(\\pi^{\\prime}\\right) \\\\\nV_{g}\\left(\\pi_{\\alpha}\\right) &=\\alpha V_{g}(\\pi)+(1-\\alpha) V_{g}\\left(\\pi^{\\prime}\\right) .\n\\end{aligned}"}, {"title": "C.3 Proof of Unique Saddle Point", "content": "Proof. The regularized Lagrangian L\u2081(\u03c0, \u03bb) is a regular unconstrained MDP for a fixed \u03bb. For all \u5165,\nif the reward function r\u5165,\u30f6 is convex on s and strictly concave on a, and the dynamics are linear, the\nresultant unconstrained MDP is guaranteed to have a unique maximizer \u03c0\u2021(\u03bb) [Cruz-Su\u00e1rez et al., 2004,\nMontes-de Oca et al., 2013]. Now, consider the regularized dual function\n\\(D_{\\tau}(\\lambda)=\\lambda^{2}+\\max _{\\pi \\in \\Pi} V_{\\lambda, \\tau}(\\pi) .\\)\nWe know by Danskin's Theorem (see Lemma 10) that (i) the function (32) is convex w.r.t \u5165, and\n(ii) the minimizer \u5165 is unique if the Lagrangian maximizer \u03c0\u2021(\u03bb) is unique for all \u00c0 \u2208 A. Therefore,\nwe can conclude uniqueness of the primal-dual optimal pair of L\u2081(\u03c0, \u03bb) under concavity of the reward\nfunction r\u5165,7 and linearity of the dynamics."}, {"title": "C.4 Proof of Theorem 2", "content": "Proof. We set up the stage by introducing the constants that will be relevant throughout the proof. First,\nwe have Cp := Lr + >maxLg + TLh + T\u221adaAmax, where Lr, Lg, and Lh are the Lipschitz constants of\naction value functions introduced in Assumption 4, and Amax is the maximum action of the bounded\naction space A. Then, we have CD := >(1+) \u2265 V\u2084(\u03c0\u03b5) + \u03c4\u03bb\u2081, where & comes from the feasibility\nAssumption 2.\nWe also recall that the operator PII projects a policy onto the set of regularized optimal policies I\nwith state visitation distribution do. For a given policy \u03c0\u03c4, we denote \u03c0:= P\u03c0 (\u03c0\u03b9). Similarly, the\noperator PA projects a Lagrangian multiplier onto the set of regularized Lagrangian multipliers\nA, and the projection is denoted as At:= PA (At).\nWe proceed by decomposing the primal-dual gap by adding and substracting L\u2081(\u03c0\u03c4, \u03bb\u03b5),\n\\(L_{\\tau}\\left(\\pi_{\\tau}^{}, \\lambda^{}\\right)-L_{\\tau}\\left(\\pi_{t}, \\lambda_{t}\\right)=\\underline{L_{\\tau}\\left(\\pi_{\\tau}^{}, \\lambda^{}\\right)-L_{\\tau}\\left(\\pi_{t}, \\lambda^{}\\right)}+\\underline{L_{\\tau}\\left(\\pi_{t}, \\lambda^{}\\right)-L_{\\tau}\\left(\\pi_{t}, \\lambda_{t}\\right)} .\\)"}, {"title": "C.5 Proof of Corollary 1", "content": "Proof. Due to Theorem (2), taking \u0442 = O(\u20ac) + \u03c4\u03bf and \u03b7 = O(\u20ac) leads to \u03a6t+1 = O(\u20ac) for any\nt = (log), where \u03a9 encapsulates some problem-dependent constants. For some t = (log) the"}, {"title": "C.6 Proof of Theorem 3", "content": "Proof. We recall the constants that will be relevant throughout the proof. First, we have Cp :=\nLr + AmaxLg + TLh + T\u221adaAmax, where Lr, Lg, and Lh are the Lipschitz constants of action value\nfunctions introduced in Assumption 4, and Amax is the maximum action of the bounded action space A.\nThen, we have CD := \u2081=(1+) \u2265 V\u2084(\u03c0\u2081) +\u03c4\u03bb\u2081, where & comes from the feasibility Assumption 2. We\nalso introduce again the operator P\u220f\u2260 that projects a policy onto the set of regularized optimal policies\nII with state visitation distribution do. For a given policy \u03c0\u03c4, we denote \u03c0\u2081t := P\u2161 (\u03c0\u03c4). Similarly, the\noperator PA projects a Lagrangian multiplier onto the set of regularized optimal Lagrangian multipliers\nA, and the projection is denoted as At := P(At)."}, {"title": "C.7 Proof of Corollary 3", "content": "Proof. The proof is an extension of the proof of Theorem 3. However, we need to take into account the\nrandomness of estimating 0\u0165 using samples. As the updates in (12) are performed using projected SGD,\nwe can leverage known error bounds Lacoste-Julien et al. [2012] to bound the expected estimation error\n\\(\\delta\\left(\\theta_{t}, \\hat{\\theta} t, \\nu\\right):=\\mathbb{E} \\nu\\left[\\delta_{\\hat{\\theta}_{t}}(s, a)-\\delta_{\\theta_{t}}(s, a)\\right] .\\)\nWith that goal in mind, we need to check:\n(i) The domain ||0|| \u2264 0max is convex and bounded.\n(ii) The gradient gr\u03bb,\u03c4(n) is unbiased since Qt is an unbiased estimate of Qt.\n(iii) The minimizer of (11) is unique since Assumption 8 guarantees that \u03a3\u03bd \u2265 \u03ba\u03bf\u0399 for some \u043a\u043e > 0.\n(iv) The squared norm of the gradient gr\" (n) is bounded."}, {"title": "D Control Regulation Problem", "content": null}, {"title": "D.1 Lipschitz Continuity of Action-value Function", "content": "We aim to show that the action-value function Q for reward function r is Lipschitz-continuous. For\nany state s \u2208 S, and two different actions a1, a2 \u2208 A, we can expand"}, {"title": "E Algorithms", "content": null}, {"title": "F Additional experiments", "content": "This section outlines the experiments conducted to evaluate the performance of the D-PGPD method.\nThe experiments were executed on a computing cluster powered by an AMD Ryzen Threadripper\n3970X processor, featuring a 64-core architecture with 128 threads, and supported by 220 GiB of RAM.\nThe hyperparameters used in the experiments are detailed within this section. For precise implementation\nspecifics, including seeding strategies and initialization procedures, please refer to the accompanying\ncode repository."}, {"title": "F.1 Constrained Quadratic Regulation", "content": "We have tested our algorithms in a navigation control problem. An agent moves on a horizontal plane,\nwhere the linearized dynamics follow the double integrator model with zero-mean Gaussian noise. The\ngeneral goal is to drive the agent to the origin. The state s has four dimensions, the 2-dimensional\nposition px and py and the 2-dimensional velocity Vx and vy. The control action is the 2-dimensional\nacceleration ux and Uy. The linearized dynamics of the double integrator model used in the experiment\nare characterized by\n\\(B_{0}=\\begin{bmatrix}1 & 0 & T_{s} & 0 \\\\0 & 1 & 0 & T_{s} \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1\\end{bmatrix} \\text { and } B_{1}=\\begin{bmatrix}\\frac{T_{s}^{2}}{2} & 0 \\\\ 0 & \\frac{T_{s}^{2}}{2} \\\\T_{s} & 0 \\\\ 0 & T_{s}\\end{bmatrix},\\)"}, {"title": "F.2 Non-linear Constrained Regulation", "content": "We have assessed the performance of D-PGPD in a non-linear control problem, specifically the control of\nthe velocity of an incompressible Newtonian fluid described by the one-dimensional Burgers' equation\nBaker et al. [2000]. The velocity profile s of the fluid varies in a one-dimensional space x bounded in\nthe interval [0, 1] and time t, described by the equation\n\\(\\frac{\\partial s(x, t)}{\\partial t}=\\varepsilon \\frac{\\partial^{2} s(x, t)}{\\partial x^{2}}-\\frac{1}{2} \\frac{\\partial(s(x, t))^{2}}{\\partial x}+a(t)+\\theta_{\\max }(t),\\)"}]}