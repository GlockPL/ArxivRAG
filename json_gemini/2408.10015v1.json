{"title": "Deterministic Policy Gradient Primal-Dual Methods for Continuous-Space Constrained MDPS", "authors": ["Sergio Rozada", "Dongsheng Ding", "Antonio G. Marques", "Alejandro Ribeiro"], "abstract": "We study the problem of computing deterministic optimal policies for constrained Markov decision processes (MDPs) with continuous state and action spaces, which are widely encountered in constrained dynamical systems. Designing deterministic policy gradient methods in continuous state and action spaces is particularly challenging due to the lack of enumerable state-action pairs and the adoption of deterministic policies, hindering the application of existing policy gradient methods for constrained MDPs. To this end, we develop a deterministic policy gradient primal-dual method to find an optimal deterministic policy with non-asymptotic convergence. Specifically, we leverage regularization of the Lagrangian of the constrained MDP to propose a deterministic policy gradient primal-dual (D-PGPD) algorithm that updates the deterministic policy via a quadratic-regularized gradient ascent step and the dual variable via a quadratic-regularized gradient descent step. We prove that the primal-dual iterates of D-PGPD converge at a sub-linear rate to an optimal regularized primal-dual pair. We instantiate D-PGPD with function approximation and prove that the primal-dual iterates of D-PGPD converge at a sub-linear rate to an optimal regularized primal-dual pair, up to a function approximation error. Furthermore, we demonstrate the effectiveness of our method in two continuous control problems: robot navigation and fluid control. To the best of our knowledge, this appears to be the first work that proposes a deterministic policy search method for continuous-space constrained MDPs.", "sections": [{"title": "Introduction", "content": "Constrained Markov decision processes (MDPs) are a standard framework for incorporating system specifications into dynamical systems [Altman, 2021, Brunke et al., 2022]. In recent years, constrained MDPs have attracted significant attention in constrained Reinforcement Learning (RL), whose goal is to derive optimal control policies through interaction with unknown dynamical systems [Achiam et al., 2017, Tessler et al., 2018]. Policy gradient-based constrained learning methods have become the workhorse driving recent successes across various disciplines, e.g., navigation [Paternain et al., 2022], video compression [Mandhane et al., 2022], and finance [Chow et al., 2018].\nThis paper is motivated by two observations. First, continuous state-action spaces are pervasive in dynamical systems, yet most algorithms in constrained RL are designed for discrete state and action spaces [Achiam et al., 2017, Tsiamis et al., 2021, Altman, 2021]. Second, the literature on constrained RL largely focuses on stochastic policies. However, randomly taking actions by following a stochastic policy is often prohibitive in practice, especially in safety-critical domains [Li et al., 2022, Gao et al., 2023]. Deterministic policies alleviate such concerns, but (i) they might lead to sub-optimal solutions [Ross, 1989, Altman, 2021]; and (ii) computing them can be NP-complete [Feinberg, 2000, Dolgov, 2005]. Nevertheless, there is a rich body of constrained control literature that studies problems where optimal policies are proved to be deterministic [Posa et al., 2016, Tsiamis et al., 2020, Zhao and You, 2021, Ma et al., 2022]. Viewing this gap between constrained RL and control, we study the problem of finding optimal deterministic policies for constrained MDPs with continuous state-action spaces.\nA key consideration of this paper is the fact that deterministic policies are sub-optimal in finite state-action spaces, but sufficient for constrained MDPs with continuous state-action spaces [Feinberg and Piunovskiy, 2002, 2019]. This enables our formulation of a constrained RL problem with deterministic policies. To develop a tractable deterministic policy search method, we introduce a regularized Lagrangian approach that leverages proximal optimization methods. Moreover, we use function approximation to ensure scalability in continuous state-action spaces. Our contribution is four-fold.\n(i) We introduce a deterministic policy constrained RL problem for a constrained MDP with continuous state-action spaces and prove that the problem exhibits zero duality gap, despite being constrained to deterministic policies.\n(ii) We propose a regularized deterministic policy gradient primal-dual (D-PGPD) algorithm that updates the primal policy via a proximal-point-type step and the dual variable via a gradient descent step, and we prove that the primal-dual iterates of D-PGPD converge to a set of regularized optimal primal-dual pairs at a sub-linear rate.\n(iii) We propose an approximation for D-PGPD by including function approximation. We prove that the primal-dual iterates of the approximated D-PGPD converge at a sub-linear rate, up to a function approximation error.\n(iv) We demonstrate that D-PGPD addresses a classical constrained navigation problem with a range of cost functions and constraints. Moreover, we show that D-PGPD can solve non-linear fluid control problems under constraints."}, {"title": "Preliminaries", "content": "We consider a discounted constrained MDP, denoted by the tuple (S, A, p, r, u, b, \\gamma, \\rho). Here, S \\subset \\mathbb{R}^{d_s} and A \\subset \\mathbb{R}^{d_a} are continuous state-action spaces with dimensions ds and da, and bounded actions ||a|| \\leq A_{\\text{max}} for all a \\in A; p(\\cdot | s, a) is a probability measure over S parametrized by the state-action pairs (s, a) \\in S \\times A; r, u: S \\times A \\rightarrow [0, 1] are reward/utility functions; b is a constraint threshold; \\gamma\\in [0, 1) is a discount factor; and p is a probability measure that specifies an initial state. We consider the set of all deterministic policies II in which a policy \\pi: S \\rightarrow A maps states to actions. The transition p, the initial state distribution p, and the policy \\pi define a probability distribution over trajectories {S_t, A_t, r_t, U_t}_{t=0}^\\infty, where s_0 \\sim \\rho, a_t = \\pi(s_t), r_t = r(s_t, a_t), u_t = u(s_t, a_t) and s_{t+1} \\sim p(\\cdot | s_t, a_t) for all t \\geq 0. Given a policy \\pi, we define the value function V^\\pi: S \\rightarrow \\mathbb{R} as the expected sum of discounted rewards\nV^{\\pi}(s) := \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t, a_t) | s_0 = s \\right]."}, {"title": "Zero Duality Gap", "content": "With the convexity of the deterministic value image VD in hand, we next establish zero duality gap for Problem (1). We begin with a standard feasibility assumption.\nAssumption 2 (Feasibility). There exists a deterministic policy \\tilde{\\pi} \\in \\Pi and \\xi > 0 such that V_u(\\tilde{\\pi}) \\geq \\xi.\nWe dualize the constraint by introducing \\lambda \\in \\mathbb{R}_+ and the Lagrangian L(\\pi, \\lambda) := V_r(\\pi) + \\lambda V_g(\\pi) for Problem (1). For a fixed \\lambda, let \\Pi(\\lambda) be the set of Lagrangian maximizers. The Lagrangian L(\\pi, \\lambda) is equivalent to the value function V_{\\lambda}(\\pi) associated with the combined reward/utility function r_{\\lambda}(s, a) = r(s, a) + \\lambda g(s, a). The dual function D(\\lambda) := \\max_{\\pi \\in \\Pi} V_{\\lambda}(\\pi) is an upper bound of Problem (1), and the dual problem searches for the tightest primal upper bound\n\\min_{\\lambda \\in \\mathbb{R}_+} D(\\lambda)."}, {"content": "We denote by $V_r^*$ the optimal value of the dual function, where $\\lambda^*$ is a minimizer of the dual Problem (2).\nDespite being non-convex in the policy, if we replace the deterministic policy space in Problem (1) with\nthe stochastic policy space, then it is known that Problem (1) has zero duality gap [Paternain et al., 2019].\nThe proof capitalizes on the convexity of the occupancy measure representation of (1) for stochastic\npolicies. However, this occupancy-measure-based argument does not carry to deterministic policies,\nsince the occupancy measure representation of Problem (1) is non-convex when only deterministic\npolicies are used [Dolgov, 2005]. Instead, we leverage the convexity of the deterministic value image\nVD to prove that strong duality holds for Problem (1); see Appendices A and C.2 for more details and\nthe proof."}, {"title": "Constrained Regulation Problem", "content": "We illustrate Problem (1) using the following example\n\\begin{aligned}\n\\max_{\\pi \\in \\Pi} & \\quad \\mathbb{E} \\left[ \\sum_{t=0}^T \\gamma^t (s_t^\\top G_1 s_t + a_t^\\top R_1 a_t) \\right]  \\\\ \ns. t. & \\quad  \\mathbb{E} \\left[ \\sum_{t=0}^T \\gamma^t (s_t^\\top G_2 s_t + a_t^\\top R_2 a_t) \\right] \\geq b \\\\ \n& \\quad -b_s \\leq C_s s_t \\leq b_s, \\quad -b_a \\leq C_a a_t \\leq b_a \\\\ \n& \\quad s_{t+1} = B_0 s_t + B_1 a_t + w_t, \\quad s_0 \\sim \\rho\n\\end{aligned}"}, {"title": "Method and Theory", "content": "While our problem has duality gap zero, finding the optimal \\lambda^* poses a significant challenge, due to the presence of multiple saddle points in the Lagrangian. To address it, we resort to regularization techniques. More specifically, we introduce two regularizers. First, the term h(\\lambda) := \\frac{\\tau}{2} \\lambda^2 promotes convexity in the Lagrange multiplier \\lambda. Second, the term h_\\tau(a) := -\\frac{\\tau}{2} ||a||^2 promotes concavity in the reward function r by penalizing large actions selected by the policy \\pi. The associated value function is defined as H^{\\pi} (s) := \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\tau h_\\tau(a_t) | s \\right], and leads to the regularizer H(\\pi) := \\mathbb{E}_\\rho [H^{\\pi} (s)]. Now, we consider the problem\n\\min_{\\lambda \\in \\Lambda} \\max_{\\pi \\in \\Pi} L_\\tau(\\pi, \\lambda) := V_\\lambda(\\pi) + H(\\pi) + \\frac{\\tau}{2} \\lambda^2,"}, {"title": "Deterministic Policy Search Method", "content": "We propose a deterministic policy gradient primal-dual (D-PGPD) method for finding a global saddle point (\\pi^*, \\lambda^*) of L_\\tau(\\pi, \\lambda). In the primal update, as is customary in RL, we maximize the advantage function rather than the value function directly. Specifically, we use the regularized advantage A_\\tau^{\\lambda}(s,a) := Q_\\tau^{\\lambda}(s, a) - V_\\tau^{\\lambda}(s) -\\frac{\\tau}{2}(\\|a\\|^2 - \\|\\pi(s)\\|^2) associated with the regularized reward r_{\\lambda,\\tau}.\nThe primal update (6a) performs a proximal-point-type ascent step that solves a quadratic-regularized maximization sub-problem, while the dual update (6b) performs a gradient descent step that solves a quadratic-regularized minimization sub-problem\n\\begin{aligned}\n\\pi_{t+1}(s) &= \\underset{a \\in A}{\\text{argmax}} A_{\\tau}^{\\lambda_t}(s, a) - \\frac{1}{2 \\eta} \\| a - \\pi_t(s) \\|^2  \\\\ \n\\lambda_{t+1} &= \\underset{\\lambda \\in \\Lambda}{\\text{argmin}} \\lambda \\left( V_g(\\pi_t) + \\tau \\lambda_t \\right) + \\frac{1}{2 \\eta} \\| \\lambda - \\lambda_t \\|^2,\n\\end{aligned}"}, {"title": "Non-Asymptotic Convergence", "content": "Finding deterministic optimal policies is a computationally challenging problem [Feinberg, 2000, Dolgov, 2005]. To render the problem tractable, we assume concavity and Lipschitz continuity of the regularized action value functions.\nAssumption 3 (Concavity). The regularized state-action value function $Q_\\tau^{\\lambda}(s, a) - \\tau_0 \\|\\pi_0(s) - a\\|^2$ is concave in action a for any policy $\\pi_0$ and some $\\tau_0 \\in [0, \\tau)$.\nAssumption 4 (Lipschitz continuity). The action-value functions $Q_\\tau^{\\lambda}(s, a), Q_g^{\\lambda}(s, a),$ and $H^\\tau(s, a)$ are Lipschitz in action a with Lipschitz constants $L_r, L_g,$ and $L_h$, i.e.,\n\\begin{aligned}\n& \\|Q_r^{\\lambda}(s, a) - Q_r^{\\lambda}(s, a')\\| \\leq L_r \\|a - a'\\|  \\\\ \n& \\|Q_g^{\\lambda}(s, a) - Q_g^{\\lambda}(s, a')\\| \\leq L_g \\|a - a'\\| \\\\ \n& \\|H^\\tau(s, a) - H^\\tau(s, a')\\| \\leq L_h \\|a - a'\\| \\quad \\forall a, a' \\in A.\n\\end{aligned}\nAssumption 3 states that there exists a \\tau_0-strongly concave regularizer that renders $Q_\\tau^{\\lambda}$ concave in the action a. When \\tau_0 = 0, $Q_\\tau^{\\lambda}$ is concave in the action a. An example of this is Problem (3), where the original reward and utility functions are concave and the transition dynamics are linear, thus leading to the associated regularized value function being concave. Assumption 4 implies Lipschitz continuity of the reward function and the probability transition kernel, which holds for several dynamics that can be expressed as a deterministic function of the actual state-action pair and some stochastic perturbation; see Appendix D.1 for a detailed explanation over the example introduced in Section 2.2.\nTo show convergence of D-PGPD, we introduce first two projection operators. The operator $P_\\Pi$ projects a policy into the set of optimal policies with state visitation distribution $d_\\pi$, and the operator $P_\\Lambda$ projects a Lagrangian multiplier onto the set of optimal Lagrangian multipliers $\\Lambda$. Then, we characterize the convergence of the primal-dual iterates of D-PGPD using the potential function\n\\Phi_t := \\mathbb{E}_{d_\\rho} \\left[ \\frac{1}{2} \\| P_\\Pi(\\pi_t(s)) - \\pi_\\tau^*(s) \\|^2 \\right] + \\frac{1}{2(1 + \\eta (\\tau - \\tau_0))} \\| P_\\Lambda(\\lambda_t) - \\lambda_\\tau^* \\|^2."}, {"title": "Function Approximation", "content": "To instantiate D-PGPD (6) with function approximation we begin by expanding the objective in (6a) and dropping the terms that do not depend on the action a,\n\\underset{a \\in A}{\\text{argmax}}  A_\\tau^{\\lambda_t}(s, a) - (\\frac{\\tau}{2} + \\frac{1}{2 \\eta} ) \\|a\\|^2.\nThe usual function approximation approach [Agarwal et al., 2021, Ding et al., 2022] is to introduce a parametric estimator of the policy \\pi, and a compatible parametric estimator of the action value function $Q_r^{\\lambda,\\tau}$. Instead, we approximate the augmented action-value function $J^\\tau(s, a) := Q_\\tau^{\\lambda,\\tau}(s, a) + \\frac{\\tau}{2} \\|\\pi(s)\\|^2$ using a linear estimator J_\\theta(s,a) = \\phi(s, a)^\\top \\theta. At time t, we estimate $J^{\\pi_t, \\lambda_t}(s, a)$ by computing the parameters \\theta_t via a mean-squared-error minimization\n\\theta_t := \\underset{\\theta}{\\text{argmin}} \\mathbb{E}_{(s,a) \\sim \\nu} [ \\|\\phi(s, a)^\\top \\theta - J^{\\pi_t, \\lambda_t}(s,a)\\|^2],"}, {"content": "where \\nu is a pre-selected state-action distribution. Problem (8) can be easily addressed using, e.g., stochastic approximation. A subsequent policy \\pi_{t+1} results from a primal update based on $J_{\\theta_t}$. This leads to an approximated D-PGPD algorithm (AD-PGPD) that updates $\\pi_t$ and $\\lambda_t$ via\n\\begin{aligned}\n\\pi_{t+1}(s) &= \\underset{a}{\\text{argmax}} J_{\\theta_t} (s, a) - (\\frac{\\tau}{2} + \\frac{1}{2 \\eta} ) \\|a\\|^2  \\\\ \n\\lambda_{t+1} &= \\underset{\\lambda \\in \\Lambda}{\\text{argmin}} \\lambda \\left( V_g(\\pi_t) + \\tau \\lambda_t \\right) + \\frac{1}{2 \\eta} \\| \\lambda - \\lambda_t \\|^2.\n\\end{aligned}\nWe remark that solving the sub-problem (9a) requires inverting the gradient of (9a) with respect to a, which can be challenging when the model of the MDP is unknown or when value-functions cannot be computed in closed form. Addressing this problem is the subject of Section 5, but first, we analyze the convergence of the primal-dual iterates of (9)."}, {"title": "Non-Asymptotic Convergence", "content": "To ease the computational tractability of AD-PGPD, we assume concavity of the approximated augmented action-value function and bounded approximation error.\nAssumption 5 (Concavity of approximation). The function $J_{\\theta_t}(s, a) - \\tau_0 \\|\\pi_0(s) - a\\|^2$ is concave with respect to the action a for some arbitrary policy $\\pi_0$ and some $\\tau_0 \\in [0, \\tau)$.\nAssumption 6 (Approximation error). The approximation error $\\delta_{\\theta_t}(s, a)$ is bounded such that $\\mathbb{E}_{s \\sim d_\\pi, a \\sim u} [\\|\\delta_{\\theta_t}(s, a)\\|] \\leq \\frac{\\epsilon_{\\text{approx}}}{2 (2 A_{\\text{max}})^{d_a}}$, where u is the uniform distribution and $\\epsilon_{\\text{approx}}$ is a positive error constant."}, {"title": "Model-Free Algorithm", "content": "When the model of the MDP is unknown or when value-functions cannot be computed in closed form, we can leverage sample-based approaches to compute the primal and dual iterates of AD-PGPD defined in (9a) and (9b). To that end, we assume access to a simulator of the MDP from where we can sample trajectories given a policy \\pi. The sample-based algorithm requires modifying the policy evaluation step in (8), and the dual update in (9b). For the former, in time-step t for a given policy \\pi_t, we have the following linear function approximation problem\n\\underset{\\theta, \\|\\theta\\| \\leq \\theta_{\\text{max}}}{\\text{min}} \\mathbb{E}_{s,a \\sim \\nu} \\left[ \\|\\phi(s_n, a_n)^\\top \\theta - \\hat{J}_{\\tau, t} (s_n, a_n) \\|^2 \\right],"}, {"content": "where the parameters @ are bounded, i.e., ||0|| \u2264 0max, and \u00a2 is the basis function. The approximated augmented value-function \u00ce\u2122t := QX (Sn, An) + \u03c0(sn) Tan is estimated from samples, which comes down to approximating Qt (Sn, an). The dual update (9b) also requires the approximated value-function Vg(\u03c0t) to be estimated. We detail how to estimate V\u2084(\u03c0\u2081) and Q7(Sn, an) via rollouts in Algorithms 1 and 2, which can be found in Appendix E. We use random horizon rollouts [Paternain et al., 2020, Zhang et al., 2020] to guarantee that the stochastic estimates of Q and V\u2084(\u03c0\u03c4) are unbiased. From [Paternain et al., 2020, Proposition 2], we have Q(s, a) = E[Q(s, a) | s, a] and V\u2084(\u03c0\u03b9) = E[Vt (s)], where the expectations E are taken over the randomness of drawing trajectories following \u03c0\u03c4. We solve Problem (11) at time t using projected stochastic gradient descent (SGD),\n\\begin{aligned}\n& \\theta_t^{(n+1)} = P_{\\|\\theta\\| \\leq \\theta_{\\text{max}}} \\left( \\theta_t^{(n)} - \\alpha_n g_t^\\tau(n) \\right) \\\\ \n& g_t^\\tau(n) = 2 \\left( \\phi(s_n, a_n)^\\top \\theta_t^{(n)} - \\hat{J}_{\\tau, t} (s_n, a_n) \\right) \\phi(s_n, a_n)\n\\end{aligned}\nwhere n \u2265 0 is the iteration index, an is the step-size, g\u0142 7(n) is the stochastic gradient of (11), and P|| ||\u22640max is an operator that projects onto the domain ||0|| \u2264 0max, which is convex and bounded. Each projected SGD update (12) forms the estimate \u03b8t. We run N projected SGD iterations and form the weighted average t := \u039d(N+1) \u03a3\u03b7=0 (\u03b7 \u03a3\u039d-1 (n + 1)0t, which is the estimation of the parameters \u03b8\u2081. Combining (9), the SGD rule in (12), and averaging techniques lead to a sample-based algorithm presented in Algorithm 3."}, {"title": "Computational Experiments", "content": "We demonstrate the effectiveness of D-PGPD in two classical continuous constrained control problems: robot navigation and fluid control, as depicted in Figure 1. For problem details and more experimental results, please see Appendix F.\nNavigation Problem. An agent moves in a horizontal plane following some linearized dynamics that are characterized by the double integrator model with zero-mean Gaussian noise [Shimizu et al., 2020, Ma et al., 2022]. We aim to drive the agent to the origin, but constraining its velocity. When the dynamics are known and the reward function linearly weights quadratic penalties on position and action, this problem is an instance of the constrained linear regulation problem [Scokaert and Rawlings, 1998]."}, {"title": "Control Regulation Problem", "content": "We aim to show that the action-value function Q for reward function r is Lipschitz-continuous. For any state s \u2208 S, and two different actions a1, a2 \u2208 A, we can expand\n||Q_r^\\pi(s, a_1) - Q_r^\\pi(s, a_1)|| = ||r(s, a_1) + \\gamma \\mathbb{E}_{s' \\sim p(\\cdot | s, a_0)} [V^\\pi (s')] - r(s, a_2) - \\gamma \\mathbb{E}_{s' \\sim p(\\cdot | s, a_2)} [V^\\pi (s')]||\n= \\bigg| r(s, a_1) - r(s, a_2) + \\gamma \\int_S (p(s'|s, a_1) - p(s'|s, a_2)) V^\\pi (s') ds' \\bigg|.\nWe know that V\u2122 (s') is bounded in the interval [0, 1/(1 \u2013 \u03b3)]. Therefore, Lipschitz continuity of Q is guaranteed if the terms ||r(s, a\u2081) \u2013 r(s, a\u2082)|| and ||p(s'|s, a\u2081) \u2013 p(s'|s, a\u2082)|| are Lipschitz continuous.\nA function is Lipschitz continuous if it is continuously differentiable over a compact domain. The Lipschitz constant is equal to the maximum magnitude of the derivative. Consider now the example in Section 2.2. The reward function r is a Lipschitz-continuous quadratic function with Lipschitz constant equal to the maximum eigenvalue of the matrix R. Now, recalling the linear Gaussian structure of the transition dynamics, we have\np(s'|s, a_1) \u2013 p(s'|s, a_2) \u2248 \\exp{\\-\\|s' \u2013 \\mu(s, a_1)\\|^2} \u2013 \\exp{\\-\\|s' \u2013 \\mu(s, a_2)\\|^2},"}, {"content": "where the mean of the distribution is given by \u03bc(s,a) := Bos + B\u2081a. It is evident that the difference of the Gaussian kernels in (62) is continuously differentiable over A, and therefore, Lipschitz continuous. This implies that the action-value function Q associated with the example defined in Section 2.2 is Lipschitz continuous. A similar reasoning applies to the action-value functions Q and H\u2122."}]}