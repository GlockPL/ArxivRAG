{"title": "Circuit Complexity Bounds for RoPE-based Transformer Architecture", "authors": ["Bo Chen", "Xiaoyu Li", "Yingyu Liang", "Jiangxuan Long", "Zhenmei Shi", "Zhao Song"], "abstract": "Characterizing the express power of the Transformer architecture is critical to understanding its capacity limits and scaling law. Recent works provide the circuit complexity bounds to Transformer-like architecture. On the other hand, Rotary Position Embedding (RoPE) has emerged as a crucial technique in modern large language models, offering superior performance in capturing positional information compared to traditional position embeddings, which shows great potential in application prospects, particularly for the long context scenario. Empirical evidence also suggests that RoPE-based Transformer architectures demonstrate greater generalization capabilities compared to conventional Transformer models. In this work, we establish a tighter circuit complexity bound for Transformers with ROPE attention. Our key contribution is that we show that unless TCO = NC\u00b9, a RoPE-based Transformer with poly(n)-precision, O(1) layers, hidden dimension d \u2264 O(n) cannot solve the arithmetic problem or the Boolean formula value problem. This result significantly demonstrates the fundamental limitation of the expressivity of the RoPE-based Transformer architecture, although it achieves giant empirical success. Our theoretical framework not only establishes tighter complexity bounds but also may instruct further work on the RoPE-based Transformer.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs), such as GPT-4 [AAA+23], Claude [Ant24], Llama [LT24], and more recently, OpenAI's o1 [Ope24], have exhibited remarkable potential to revolutionize numerous facets of daily life, including conversational AI [LCT+24], AI agents [XCG+23, CYL+24], search capabilities [Ope24], and AI assistants [KHC+24, FJL+24], among others. One of the most significant emergent capabilities of LLMs is their proficiency in handling long-context information, which is essential for effectively processing complex documents such as academic papers, official reports, and legal texts. LLMs also have demonstrated exceptional capabilities in tackling long-context tasks, such as zero-shot summarization [CAM24, ZJV+24] and sustaining coherent, extended conversations [XGW+22, MLT+24]. The o1 model from OpenAI [Ope24] represents a major advancement in this field. By leveraging Chain-of-Thought (CoT) reasoning [WWS+22, KGR+22] and incorporating Retrieval Augmented Generation (RAG) [LPP+20, GXG+23], it showcases a level of expertise comparable to PhD-level problem solving, with both techniques heavily relying on extensive contextual understanding.\nLarge Language Models (LLMs) are primarily built upon the Transformer architecture [VSP+17], which uses the self-attention mechanism as its core component. Given this foundational structure, an important question arises: what computational primitives can the components of the Transformer implement, and what problems can the entire system solve collectively? These questions are crucial for interpreting Transformers in a principled manner, understanding the potential limitations of their reasoning capabilities, and fostering trust in deployed Transformer-based systems.\nTo address the aforementioned questions and to investigate the expressiveness of transformers, prior research has made significant strides. Recent studies, such as [MS23b], have established two key results concerning both non-uniform and L-uniform settings: first, any depth-d transformer with clogn-precision can be simulated by a threshold circuit family with constant depth; second, such a transformer can also be simulated by a L-uniform threshold circuit family of constant depth. Further advancing these findings, [MS23a] demonstrate that DLOGTIME-uniform TC\u00ba circuits are capable of simulating softmax-attention transformers. Building on this foundation, [Chi24] refine these results by increasing the accuracy of approximation. They enhance the precision for softmax-attention transformers from O(log n) to O(poly(n)), confirming that these transformers fall within the DLOGTIME-uniform TC\u00ba class. Additionally, they show that a softmax-attention transformer with an absolute error bound of 2-0(poly(n)) is also contained within DLOGTIME-uniform TC\u00ba.\nOn the other hand, first introduced_by [SAL+24], Rotation Position Embedding (RoPE) enhances Transformers by encoding both absolute and relative positional information through a rotation matrix, enabling greater sequence length flexibility, improved attention mechanism efficiency, and better performance on long-text tasks, exemplified by RoPE-based language models that can summarize an entire book in a single pass. Due to these advantageous properties, RoPE has been widely adopted in numerous empirical studies [CND+23, BBH+22, BSA+23]. However, despite its considerable success, the underlying mechanisms of RoPE remain largely unknown, posing an intriguing mystery in the field. A natural question arises:\nDoes ROPE enhance the expressiveness of the Transformer-based Large Language Model?\nThis work aims to address this question from the perspective of circuit complexity, taking a step forward in theoretically understanding the underlying mechanisms of RoPE. We present a rigorous theoretical investigation of RoPE-based Transformers, establishing fundamental limits on their computational power.\nOur core approach involved a systematic examination of the circuit complexity for each component of the RoPE-based architecture, from the basic trigonometric functions to the complete"}, {"title": "2 Related work", "content": "This section briefly reviews the related research work on the complexity and neural networks, limitations of Transformers. These topics have a close connection to our work.\nComplexity and Neural Networks. Circuit complexity, a branch of computational complexity theory, studies circuit families as models of computation. Several circuit complexity classes are significant in machine learning. Specifically, AC\u00ba represents problems highly parallelizable with standard logic gates, while TC\u00ba extends this to include threshold gates, and NC\u00b9 denotes the language recognizable by O(log n)-depth circuits with bounded gate arity [MSS22]. It is known that AC\u00ba \u2282 TC\u00ba \u2282 NC\u00b9, but whether TC\u00ba \u2260 NC\u00b9 remains an open question. Assuming this inequality, [LAG+22] shows that Transformer depth must depend on input sequence length when simulating non-solvable semiautomata. [LLZM24] explore relationships among constant-depth Transformers, Transformers with Chain-of-Thought (CoT), and circuit complexity. They demonstrate: T[poly(n),1,1] \u2286 CoT[log n, poly(n), 1,1] \u2286 AC\u00ba and T[poly(n), logn, 0] \u2286 CoT[logn, poly(n), log n, 0] \u2286 TC\u00ba where T[d(n), s(n), e(n)] denotes a constant-depth Transformers with embedding size d(n), precision s(n) bits, and exponent bits e(n) for input length n and CoT[T(n), d(n), s(n), e(n)] denotes a T(n)-step CoT of a constant-depth Transformer T[d(n), s(n), e(n)]. Their results provide theoretical insights into the emergent CoT ability of Transformers, showing that intermediate reasoning steps enable tackling more complex problems. The Strong Exponential Time Hypothesis (SETH), introduced by [IP01], strengthens the P \u2260 NP conjecture by asserting that current best SAT algorithms are roughly optimal: for every \u03f5 > 0, there exists k \u2265 3 such that k-SAT cannot be solved in O(2(1-\u03f5)n) time, even randomly. SETH is widely used to prove fine-grained lower bounds for various algorithmic problems [Wil18] and has been applied to derive lower bounds for Transformer training/inference [AS23a, AS24b, LSS+24c] and tensor attention [AS23b, LSSZ24b]. Specifically, [AS23a] demonstrates that unless the SETH fails, no algorithm exists that can compute the forward pass of an attention network in truly-subquadratic time. On the other hand, [AS24b] establishes that the same condition applies to the backward computation of attention networks, i.e., unless the SETH fails, no truly-subquadratic time algorithm can be devised for the backward computation of attention networks. In essence, complexity theory provides a powerful framework for investigating the computational capabilities of neural networks, by rigorously analyzing the computational problems they can efficiently solve.\nLimitations of Transformers. Transformers have shown exceptional capabilities in natural language processing tasks, yet their effectiveness in mathematical computations remains limited [Cha22]. Consequently, research efforts have increasingly focused on defining the computational boundaries of Transformers. These studies investigate two types of Transformers: (1) the average-head attention Transformer, where the largest entry in the probability vector is set to 1 and all other entries are set to 0; (2) the softmax-attention Transformer, where the probability vector is produced using a softmax function, formally defined as Softmax(X) = diag(exp(X) \u00b7 1n)\u207b\u00b9 \u00b7 exp(X). For the average-head attention Transformer, Merrill, Sabharwal, and Smith [MSS22] demonstrate that it can recognize languages beyond the circuit complexity class AC\u00ba but can be simulated by constant-depth threshold circuits, placing it within the non-uniform TC\u00ba class. Additionally, [LAG+22] prove that softmax-attention Transformers can be simulated by a non-uniform TC\u00ba circuit. Extending this analysis, [MS23b] introduce a generalized similarity function s : {0,1}p \u00d7 {0,1} \u2192 {0,1}P, applicable to any similarity function within this mapping, and show that softmax-attention Transformers belong to L-uniform TC\u00ba. Through the conversion of Transformer operations into sentences in FOM (first-order logic extended to include MAJORITY quantifiers [Imm98]), [MS23a] demonstrate that DLOGTIME-uniform TC\u00ba can simulate softmax-attention Transformers. [Chi24] further refine these findings by enhancing approximation accuracy. Specifically, they eliminate error in average-head attention Transformers and improve the precision for softmax-attention Transformers from O(logn) to O(poly(n)), proving that these Transformers belong to the DLOGTIME-uniform TC\u00ba class. Additionally, they show that a softmax-attention Transformer with an absolute error of at most 2-0(poly(n)) is also within DLOGTIME-uniform TC\u00ba. Regarding more practical tasks such as mathematical and decision-making problems, [FZG+23] show that, unless TC\u00b0 = NC\u00b9, no log-precision Transformer can solve arithmetic and equation-solving problems, nor can any log-precision autoregressive Transformer generate correct answers for the Context-Free Grammar (CFG) Membership Testing problem [Sip96]. These theoretical constraints help explain some of the practical limitations observed when applying Transformers to mathematical tasks. Much of the relevant work in recent years has been related to this such as looper transformer [AS24a, LSS+24a, LSS+24b, CLS+24], acceleration [HYW+23, CLL+24, LLS+24e, LSSY24, LLSS24, LLS+24d, SMN+24, LLS+24c, LLS+24b, HWL+24b, HCL+24, HLSL24, WHL+24, XHH+24, HCW+24, SZZ24, WHHL24, HWL24a], graph attention [VCC+17, WJS+19, BAY21, CHL+24] and other related works[DSWY22, SY23, SSX23, GMS23, XSL24, LLS+24a, LSSZ24a, HSK+24]"}, {"title": "3 Preliminary", "content": "In this section, we present some preliminary concepts and definitions of our paper. In Section 3.1, we introduce some basic notations used in our paper. In Section 3.2, we introduce the basics of"}, {"title": "3.1 Notations", "content": "For any positive integer n, we use [n] to denote set {1,2,\u2026\u2026,n}. We use N := {0, 1, 2, . . . } to denote the set of natural numbers. We use Pr[], E[], and Var[] to denote the probability, expectation, and variance, respectively. For two vectors x \u2208 R\" and y \u2208 R\", we use \u27e8x, y\u27e9 to denote the inner product between x, y. We use 1n to denote a length-n vector where all the entries are ones. We use Xi,j to denote the i-th row, j-th column of X \u2208 Rm\u00d7n. We use ||A||\u221e to denote the l\u221e norm of a matrix A \u2208 Rn\u00d7d, i.e. ||A||\u221e := maxi\u2208[n],j\u2208[d] |Ai,j|. For Xi \u2208 {0,1}*, xi is a binary number of arbitrary length, more generally speaking, xi is a binary string of length p, where each bit is either 0 or 1."}, {"title": "3.2 Circuit Complexity", "content": "The Boolean circuit, using AND, OR, and NOT gates, is a fundamental computational model in computer science, which is formally defined as follows.\nDefinition 3.1 (Boolean circuit, Definition 6.1 on page 102 of [AB09]). A Boolean circuit with n variables is a function Cn : {0,1}n \u2192 {0,1} defined on a directed acyclic graph. The nodes in this graph represent logic gates such as AND, OR, and NOT. Input nodes, which have an in-degree of 0, are assigned one of the n Boolean variables. The circuit evaluates each non-input gate's value by computing the inputs it receives from other gates.\nIt is natural to examine the languages that can be recognized by specific families of Boolean circuits since it offers insights into the computational capabilities and efficiency of a certain family of computational models.\nDefinition 3.2 (Languages recognized by a circuit family, Definition 6.2 on page 103 of [AB09]). We say that a language L \u2286 {0,1}* is recognized by a family C of Boolean circuits if for all x \u2208 {0,1}*, there exists a Boolean circuit C|x| \u2208 C over |x| variables such that C|x|(x) = 1 if and only if x \u2208 L.\nWe now define classes of languages by imposing constraints on the types of logic gates that can be utilized within the circuit families necessary for their recognition. The weakest one we are going to introduce is the NC class.\nDefinition 3.3 (NC\u00b9, Definition 6.21 on page 109 of [AB09]). The class NC\u00b9 consists of languages that can be recognized by Boolean circuits with O(poly(n)) size, O((log n)O(1)) depth, and bounded fan-in AND, OR gates, and NOT gates.\nWhen Boolean circuits permit AND and OR gates with unbounded fan-in, they gain the capacity to recognize a large class of languages. We define AC' class as follows.\nDefinition 3.4 (AC\u00b9, Definition 6.22 on page 109 of [AB09]). The class AC\u00b9 consists of languages that can be recognized by Boolean circuits with O(poly(n)) size, O((logn)O(1)) depth, and unbounded fan-in AND, OR gates, and NOT gates.\nIn fact, AND, OR gates, and NOT gates can all be implemented by MAJORITY gates, where the MAJORITY gate outputs 0 when half or more arguments are 0 and outputs 1 otherwise. Thus, if we allow Boolean circuits to be equipped with MAJORITY gates, we get a larger class TC'."}, {"title": "3.3 Float Point Numbers", "content": "In this section, we introduce some important definitions. To establish a foundation for our computational framework, we first introduce the essential definitions of floating-point numbers and their operations, which are crucial for implementing Transformer calculations efficiently."}, {"title": "3.4 Transformer Blocks", "content": "With our mathematical foundation established, In this section, we can now describe the key components of Transformer architecture, beginning with the softmax operation that is fundamental to attention mechanisms.\nDefinition 3.20 (Softmax). Let z \u2208 Fn. We define Softmax: Fn \u2192 Fn satisfying\nSoftmax(z) := exp(z)/\u27e8exp(z), 1n\u27e9.\nA key innovation in modern Transformers is the RoPE, which begins with a basic rotation matrix:\nDefinition 3.21 (Rotation matrix block). Let n be the input sequence length, d is given the embedding dimension, \u03b8 \u2208 Fp, we define the rotation matrix as\nR(\u03b8) :=   cos \u03b8   \u2212 sin \u03b8 \nsin \u03b8    cos \u03b8  \nThis basic rotation matrix is then extended to handle relative positions in the sequence."}, {"title": "4 Complexity of RoPE-based Transformers", "content": "In this section, we establish several fundamental results regarding the circuit complexity of basic operations required in Transformer computations. In Section 4.1, we begin by analyzing trigonometric functions, which are essential for rotary position embeddings. In Section 4.2, we then proceed to study matrix operations. In Section 4.3, we examine the RoPE-based attention matrix. In Section 4.4, we analyze the single RoPE-Attention layer. In Section 4.5, we compute some common components other than the self-attention layer. In Section 4.6, we show more details about the complete RoPE-based Transformer mechanism.\nFinally, in Section 4.7, we show our main results that the circuit complexity bound of RoPE-based Transformer. These results will serve as building blocks for our main theorem on Transformer expressiveness."}, {"title": "4.1 Approximating Trigonometric Functions", "content": "In this section, we first demonstrate that basic trigonometric functions, which are fundamental to RoPE embeddings, can be efficiently computed by threshold circuits. The following lemma is one of the key tools in this work.\nProof. For sin(x) where x \u2208 Fp, we can define: k :=  2x/\u03c0 and\nr:=  x \u2212 k\u03c0/2 if x \u2212 k\u03c0/2 < \u03c0/4,\n (k + 1)\u03c0/2 \u2212 x else.\nUsing truncated Taylor series of sin(r), we have:\nsin(r) = \u2211i=0N\u22121 (\u22121)i r2i+1/(2i + 1)! + RNsin(r)"}, {"title": "4.2 Computing Matrix Products", "content": "In this section, we show that basic matrix multiplication can be computed efficiently in TC\u00ba."}, {"title": "4.3 Computing RoPE-based Attention Matrix", "content": "In this section, we extend this to the computation of the attention matrix with positional embeddings, i.e., RoPE-based attention matrix computation."}, {"title": "4.4 Computing Single RoPE-based Attention Layer", "content": "Finally, we analyze the complete attention layer, the approach allows us to carefully track the circuit depth requirements at each stage."}, {"title": "4.5 Computing Common Buliding Blocks other than Self-attention layer", "content": "In Definition 3.25, we define Multi-layer RoPE-based Transformer with self-attention layer and other components, for example layer-norm and MLP. In this section, we show how to compute these components.\nFirst, we give the circuit complexity for the MLP layer."}, {"title": "4.6 Computing Multi-layer RoPE-based Transformer", "content": "In this section, we show how to compute the multi-layer RoPE-Transformer."}, {"title": "4.7 Main Result: Circuit Complexity Bound of RoPE-based Transformers", "content": "In this section, we are ready to represent our main result. We show the circuit complexity bound of RoPE-based Transformer."}, {"title": "5 Hardness", "content": "In this section, we state two important problems and the corresponding hardness results. In Section 5.1, we introduce the arithmetic formula evaluation problem. In Section 5.2, we introduce the Boolean formula value problem. In Section 5.3, we state our two hardness results."}, {"title": "5.1 Arithmetic Formula Evaluation Problem", "content": "In this section, we first provide a foundational definition as established in [BCGR92]."}, {"title": "5.2 Boolean Formula Value Problem", "content": "In this section, we now shift our focus to the domain of Boolean formulas and their evaluation."}, {"title": "5.3 Hardness Results", "content": "In this section, we state our two hardness results."}, {"title": "6 Conclusion", "content": "In this work, we provide a rigorous theoretical analysis of RoPE-based Transformers, establishing fundamental bounds on their computational capabilities. Our main idea was to systematically analyze the circuit complexity of each component in the RoPE-based architecture, from basic trigonometric functions to the complete attention mechanism, ultimately proving that these models can be simulated by uniform TC\u00ba circuits. More importantly, we demonstrate that unless TC\u00b0 = NC1, RoPE-based Transformers with poly(n)-precision, O(1) layers, and hidden dimension d \u2264 O(n) cannot solve either arithmetic formula evaluation or Boolean formula value problems. This result is particularly significant as it reveals fundamental limitations in the expressivity of RoPE-based architectures, despite their empirical success in modern language models.\nOne limitation is that our analysis focuses primarily on the forward computation aspects and assumes constant-depth nonlinear activation functions, leaving open questions about training dynamics and the impact of more complex activation functions. It would be interesting to extend our"}]}