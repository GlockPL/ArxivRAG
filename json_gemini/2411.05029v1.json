{"title": "Ultrasound-Based AI for COVID-19 Detection: A Comprehensive Review of Public and Private Lung Ultrasound Datasets and Studies", "authors": ["Abrar Morshed", "Abdulla Al Shihab", "Md Abrar Jahin", "Md Jaber Al Nahian", "Md Murad Hossain Sarker", "Md Sharjis Ibne Wadud", "Mohammad Istiaq Uddin", "Muntequa Imtiaz Siraji", "Nafisa Anjum", "Sumiya Rajjab Shristy", "Tanvin Rahman", "Mahmuda Khatun", "Md Rubel Dewan", "Mosaddeq Hossain", "Razia Sultana", "Ripel Chakma", "Sonet Barua Emon", "Towhidul Islam", "Mohammad Arafat Hussain"], "abstract": "The COVID-19 pandemic has affected millions of people globally, with respiratory organs being strongly affected in individuals with comorbidities. Medical imaging-based diagnosis and prognosis have become increasingly popular in clinical settings for detecting COVID-19 lung infections. Among various medical imaging modalities, ultrasound stands out as a low-cost, mobile, and radiation-safe imaging technology. In this comprehensive review, we focus on AI-driven studies utilizing lung ultrasound (LUS) for COVID-19 detection and analysis. We provide a detailed overview of both publicly available and private LUS datasets and categorize the AI studies according to the dataset they used. Additionally, we systematically analyzed and tabulated the studies across various dimensions, including data preprocessing methods, AI models, cross-validation techniques, and evaluation metrics. In total, we reviewed 60 articles, 41 of which utilized public datasets, while the remaining employed private data. Our findings suggest that ultrasound-based AI studies for COVID-19 detection have great potential for clinical use, especially for children and pregnant women. Our review also provides a useful summary for future researchers and clinicians who may be interested in the field.", "sections": [{"title": "1 Introduction", "content": "The World Health Organization (WHO) declared Coronavirus Disease 2019 (COVID-19) a global pandemic in March 2020, and despite preventive measures, the virus has led to over 704 million cases and 7 million deaths worldwide (Worldometer, 2024). COVID-19, like other respiratory infections, primarily affects the lungs, especially in individuals with comorbidities such as heart disease and diabetes (Huang et al, 2020; Stokes et al, 2020). With the continued rise in cases and the emergence of new variants, medical imaging modalities such as computed tomography (CT), X-ray, and lung ultrasound (LUS) have become increasingly essential for diagnosing and monitoring COVID-19 lung infections (Wang et al, 2022; Dong et al, 2020; Qian et al, 2020).\nMedical imaging is undeniably the most important tool for the diagnosis and management of treatments in clinical settings (Willemink et al, 2020). Despite ultrasound being known to be a noisy imaging modality compared"}, {"title": "2 Lung Ultrasound COVID-19 Datasets", "content": "Supervised learning using deep neural networks, a category of AI, has been extensively used for medical imaging applications in recent years (Wynants et al, 2020). Adequate training of deep models for medical data requires prohibitive amounts of annotated data at the image/pixel/voxel level. Using such deep models on LUS data for COVID-19 detection and analysis is also not an exception. Furthermore, it is also critical to have public access to such datasets, as many research groups lack the clinical setup for data collection. In addition, reproducing a claimed performance by an AI method and possible future improvement greatly relies on access to the exact dataset. However, only a few publicly accessible LUS datasets are available. In this section, we discuss such datasets and their attributes."}, {"title": "2.1 Publicly Accessible LUS COVID-19 Datasets", "content": "In Table 1, we list publicly accessible LUS COVID-19 datasets and their associated class labels. We briefly discuss each dataset below:\nPoint-of-Care Ultrasound (POCUS): Born et al (2020a, 2021) published and have been maintaining the POCUS dataset since 2020. This dataset initially contains a total of 261 lung ultrasound recordings by combining 202 videos and 59 still images collected from 216 patients. In this dataset, data from 92, 90, 73, and 6 are associated with COVID-19, healthy control, bacterial pneumonia, and viral pneumonia, respectively. These data were collected using either convex or linear probes. Each film in their dataset also comes with visual pattern-based expert annotation (e.g., B-Lines or consolidations).\nItalian COVID-19 Lung Ultrasound Database (ICLUS-DB): Soldati et al (2020) published an internationally standardized acquisition protocol and four-level scoring schemes for lung ultrasound (LUS) in March 2020, shortly known as ICLUS-DB. This dataset contains 277 ultrasound videos (consisting of 58,924 frames) of 17 confirmed COVID-19, four suspected COVID-19, and 14 healthy subjects. These data were collected at various clinical centers in Italy using ultrasound scanners using either linear or convex probes. To evaluate the progress of pathology, this data consortium defined a four-level scoring system ranging from 0 to 3. The presence of continuous pleural-line and horizontal A-lines indicates a healthy lung with a score of 0. Score 1 is tagged for initial abnormality when alterations in the pleural line appear. Score 2 is more severe than one and is associated with small consolidations in the lung. Score 3 is the most severe grade, which is associated with the presence of a"}, {"title": "2.2 Non-Accessible LUS COVID-19 Private Datasets", "content": "In contrast to the publicly accessible datasets described in section 2.1, some studies used private datasets, and some of these datasets are mentioned as available on request. However, these data sets have variations in terms of patient origin, hospital location, and data collection protocols. We list these"}, {"title": "2.3 Data Pre-processing and Augmentation", "content": "Various image processing techniques are typically used before feeding the data to AI models. Image processing techniques include, but are not limited to,"}, {"title": "2.3.1 Curve-to-linear/Linear-to-curve Conversion", "content": "Acquired ultrasound videos and images using convex transducers are typically fan-shaped (i.e., narrower close to the probe surface while wider at depth). In contrast, ultrasound videos and images that use linear transducers are usually rectangular. Thus, harmonizing images acquired by convex and linear transducers requires the conversion of fan-shaped images to rectangular images and vice versa. Therefore, various automatic built-in conversion techniques in the scanner, as well as external user-defined interpolation techniques (Bottenus and \u00dcst\u00fcner, 2015), are typically used for this conversion task, and ultrasound-based COVID-19 AI studies are not an exception, e.g., (Carrer et al, 2020; Li et al, 2024; Zeng et al, 2024)."}, {"title": "2.3.2 Image Resizing", "content": "Image resizing is the most common image pre-processing technique used for AI model training. Typically, ultrasound images come with various resolutions in terms of pixel count. On the other hand, AI models, especially deep learning models, typically require all input images to be of equal dimension. In addition, the larger input image dimension and the number of channels cause a higher computational overhead in the AI model optimization process. Therefore, AI studies often resize input images to a widely used common dimension across datasets. Most of the reviewed articles in this paper, for example, (Ebadi et al, 2021; Rojas-Azabache et al, 2021; Nabalamba, 2022; Quentin Muller et al, 2020; Karar et al, 2021a; Karnes et al, 2021; Perera et al, 2021; Song et al, 2023; Madhu et al, 2024), etc., also used the common image dimension of 224\u00d7224 pixels as well-known computer vision deep learning models are typically designed to intake images of 224\u00d7224 pixels. However, other image dimensions are also found for ultrasound COVID-19 studies. For example, Karar et al (2021b) resized all ultrasound images to 28\u00d728 pixels to avoid a higher computational overhead. In addition, Nehary et al (2023), Mateu et al (2022), Lucassen et al (2023), Durrani et al (2022), Vinod et al (2024), Muhammad and Hossain (2021), and Gare et al (2021) resized their ultrasound images to 128\u00d7128, 254\u00d7254, 384\u00d7256, 806\u00d7550, 512\u00d7512, and 624\u00d7464 pixels, respectively."}, {"title": "2.3.3 Intensity Normalization", "content": "Intensity normalization is another common image pre-processing technique used in AI studies. This process ensures a common intensity range across images and datasets. In most cases, all image data are converted to a common intensity range of [0, 1], or [0, 255] (Perera et al, 2021), followed by mean subtraction and division by standard deviation (Muhammad and Hossain, 2021;"}, {"title": "2.3.4 Image Augmentation", "content": "Image augmentation is a widely used technique in AI studies, which is used to increase the amount of training data and the variation and diversity in the appearance of an image. one of the most prevalent steps that have been executed in most of the studies. Various conventional (as in Hussain et al (2017)) and learning-based data augmentation (Momeny et al, 2021) techniques are present in the literature. Conventional image augmentation techniques such as image cropping, random rotation, horizontal and vertical flipping, histogram equalization, random image shifting, zooming in and out, and/or a combination of these operations, etc., are more prevalent in AI studies, and articles in this review (e.g., Born et al (2020a); Gare et al (2021); Muhammad and Hossain (2021); Roy et al (2020); Arntfield et al (2020); Adedigba and Adeshina (2021); La Salvia et al (2021); Nabalamba (2022); Rojas-Azabache et al (2021); Khan et al (2023); Howell et al (2024); Faita et al (2024); Song et al (2023); Zhao et al (2024)) mostly adopted this type of augmentation."}, {"title": "2.3.5 Other Image Processing Techniques", "content": "Apart from the common image pre-processing techniques discussed above, other processes are often used in ultrasound AI studies. Ultrasound images are known to be a noisy modality (Pal et al, 2021). Therefore, ultrasound-based studies often use noise reduction filters for pre-processing of images (Dastider et al, 2021), such as circular averaging filter (de Araujo et al, 2016), median filter (Hussain et al, 2012), non-linear diffusion filter (Hussain et al, 2015), contrast-limited adaptive histogram equalization (CLAHE) (Sadik et al, 2021), etc.\nEbadi et al (2022) performed several pre-processing operations to make resulting ultrasound images in COVIDx-US\u00b3 dataset easily usable to AI models. They cropped video frames into rectangular windows to remove the background or visible text from the image periphery. Any video frame with a moving pointer on it was also ignored when frames were extracted to use as images.\nOther image preprocessing techniques used by reviewed articles in this study include image blurring (Khan et al, 2023; Zhao et al, 2024), elastic warping (Khan et al, 2023), and variable time-gain compensation (Howell et al, 2024)."}, {"title": "3 AI in LUS COVID-19 Studies", "content": "The accuracy of identifying COVID-19 infection and assessing its severity is based primarily on the expertise of clinicians, which is often difficult and time-consuming. To overcome this limitation, AI approaches have been widely used in recent years. AI approaches used in COVID-19 ultrasound studies can be categorized into conventional machine learning (CML) and deep learning (DL) approaches. CML approaches (e.g., support vector machine (SVM), linear regression, etc.) typically require hand-engineering of features, which are often difficult to define optimally (Hussain et al, 2017). Overcoming this limitation, DL using convolutional neural networks (CNN) has exploded in popularity throughout the last decade. Various CNN architectures have been widely used on natural image and medical image-based classification and segmentation tasks. However, medical imaging data are often very difficult to collect, which results in a small training data cohort. To overcome this limitation, DL on medical imaging often leverages the transfer learning strategy, where the deep model is pre-trained on a much larger natural image dataset and then fine-tuned on the target smaller medical data. This transfer learning strategy is also used in many articles (e.g., Diaz-Escobar et al (2021); Al-Jumaili et al (2021); Barros et al (2021); Nabalamba (2022); Rojas-Azabache et al (2021)) we reviewed in this study. In addition, many studies in this review (e.g., Diaz-Escobar et al (2021); Born et al (2020a)) used cross-validation techniques to avoid overfitting."}, {"title": "3.1 AI Models", "content": "In Table 3, we list all the articles we reviewed in this study and the corresponding AI methods used by those articles. We also mark in the table whether a study used CML, DL, or both. We see in the table that only three studies used CML approaches (see rows 12, 40, and 44 of Table 3), and five studies combined CML and DL (see rows 2, 14, 53, 55, and 56 of Table 3). Except for these studies, all other studies we reviewed used DL approaches. This tendency to prefer DL approaches over CML approaches is motivated by the fact that DL models are capable of learning optimal feature representation by themselves without requiring manual intervention and the availability of more complex and powerful computation facilities. In Fig. 4, we organized all the reviewed articles in terms of the type of AI model and configuration. We also describe different types of AI models used by state-of-the-art LUS COVID-19 studies in the following sections."}, {"title": "3.1.1 Convolutional Neural Networks (CNN)", "content": "SqueezeNet, MobileNetV2, VGG-16/19, NasNetMobile, DenseNet-121/201, ResNet-18/50/101/152V2, ResNet(2+1)D-18, ResNet3D-18, Inception V3, GoogleNet, EfficientNetB0/B2/B7, XCovNet: These CNNs are DL models specifically designed for image processing tasks. They typically consist of convolutional layers that extract features from input images and pooling layers"}, {"title": "3.1.2 Recurrent Neural Networks (RNN)", "content": "RNNs are a type of neural network that can process sequential data by capturing temporal dependencies. They are commonly used for tasks involving sequential inputs or outputs, such as natural language processing and time series analysis. RNNs have recurrent connections that allow information to flow from one time step to the next. This enables the network to maintain a memory of previous inputs and utilize that information to make predictions or analyze the current input. At each time step, an RNN produces an output based on the current input and the hidden state from the previous time step. The hidden state serves as the network's memory, storing information about previous inputs. It is updated and passed along to the next time step, allowing the network to learn and capture long-term dependencies in the sequence. RNNs can be \"unfolded\" in time, creating a series of interconnected layers that correspond to each time step. This unfolding helps visualize the flow of information through the network and enables the application of backpropagation through time, a training algorithm that adjusts the network's weights based on the sequence of inputs and desired outputs. One of the articles we reviewed in this study used RNN (Azimi et al, 2022)."}, {"title": "3.1.3 COVID-Net", "content": "COVID-Net US, COVID-Net US-X, COVID-Net USPro, COVID-Net L2C-ULTRA: These architectures are specifically developed for the detection and diagnosis of COVID-19 from medical imaging, particularly chest X-ray images. COVID-Net US is a CNN architecture designed for the classification of chest X-ray images to detect COVID-19 cases. It has been trained on a large dataset"}, {"title": "3.1.4 Long Short-Term Memory (LSTM)", "content": "LSTM is a type of RNN architecture that addresses the vanishing gradient problem of traditional RNNs and is capable of capturing long-term dependencies in sequential data. LSTMs are widely used in various tasks involving sequential data, such as natural language processing, speech recognition, and time series analysis. The key feature of LSTM networks is their memory cell, which allows them to retain information over long sequences and selectively forget or update that information. LSTMs achieve this through a set of gates, including an input gate, a forget gate, and an output gate. These gates regulate the flow of information and enable the LSTM to remember or forget specific information based on the context. One of the articles we reviewed in this study used LSTM (Dastider et al, 2021)."}, {"title": "3.1.5 Hidden Markov Model (HMM)", "content": "HMM and Viterbi Algorithm are both fundamental concepts in the field of probabilistic modeling and sequential data analysis. One of the articles we reviewed in this study used both HMM and Viterbi Algorithm (Carrer et al, 2020).\nHidden Markov Model: An HMM is a statistical model that represents a system with unobservable (hidden) states and observable outputs. It is a generative model that assumes the underlying system can be modeled as a Markov process, where the current state depends only on the previous state. However, the actual state is not directly observable; instead, it emits observable symbols or outputs. HMMs have been widely used in various applications such as speech recognition, natural language processing, bioinformatics, and pattern recognition.\nViterbi Algorithm: The Viterbi Algorithm, on the other hand, is an efficient dynamic programming algorithm used to find the most likely sequence of hidden states in a Hidden Markov Model. Given a sequence of observations, the Viterbi Algorithm computes the optimal sequence of hidden states that maximizes the probability of the observations. It takes into account both the transition probabilities between states and the emission probabilities of observations from the states. The algorithm iteratively computes the most likely path by considering the accumulated probabilities at each time step, resulting in the most probable sequence of hidden states."}, {"title": "3.1.6 Generative Adversarial Networks (GAN)", "content": "GAN is a class of machine learning models that consists of two neural networks, namely the generator and the discriminator, which are trained together in a competitive setting. The generator network takes random noise as input and generates synthetic samples, such as images, based on that noise. The objective of the generator is to generate samples that resemble real data as closely as possible. On the other hand, the discriminator network takes both real samples from the dataset and synthetic samples from the generator as input and aims to classify them correctly as real or fake. The discriminator's objective is to distinguish between real and generated samples accurately. During training, the generator and discriminator are trained in alternating steps. The generator tries to fool the discriminator by generating realistic samples, while the discriminator aims to improve its ability to distinguish real from fake samples. This back-and-forth training process creates a competitive dynamic"}, {"title": "3.1.7 Transformer", "content": "Transformer is a DL architecture designed for sequence modeling tasks like natural language processing. It relies on self-attention mechanisms to capture relationships between all elements in a sequence, allowing it to process data in parallel and learn long-range dependencies more efficiently than recurrent models.\nVision Transformer (ViT): ViT applies the Transformer architecture to image data by dividing an image into patches, treating each patch as a token similar to words in a sentence. It processes these patches through self-attention mechanisms, allowing the model to capture global image features and achieve strong performance in image classification tasks. Some studies (Perera et al, 2021; Nehary et al, 2023; Lucassen et al, 2023; Rahhal et al, 2022) we reviewed in this work used ViT for LUS COVID-19 data analysis and classification.\nKnowledge Fusion with Latent Representation (KFLR) Transformer: KFLR Transformer is a specialized Transformer model that integrates multiple sources of information by learning a shared latent representation. This fusion of knowledge from different domains or modalities improves the ability of a model to handle complex tasks, as it combines diverse insights while maintaining high-quality latent feature representations. One study in our review used KFLR Transformer to predict the severity of COVID-19 from LUS (Vinod et al, 2024)."}, {"title": "3.1.8 Spatial Transformer Network (STN)", "content": "STN is a type of neural network module that can be integrated into deep learning architectures to enable the spatial transformation of input data. The purpose of the STN network is to learn spatial transformations, such as rotations, translations, scaling, and cropping, that can be applied to input images or feature maps. The key idea behind the STN network is to introduce a spatial transformer module that can learn to automatically align and transform"}, {"title": "3.1.9 U-Net", "content": "U-Net is a convolutional neural network architecture that was specifically designed for biomedical image segmentation but has since been applied to various other domains. It consists of an encoder-decoder structure with skip connections. The encoder part gradually reduces the spatial dimensions while capturing hierarchical features, and the decoder part upsamples the feature maps and recovers the spatial resolution. The skip connections help preserve fine-grained details by concatenating feature maps from the encoder to the corresponding decoder layers. U-Net has been widely used for tasks such as medical image segmentation, cell segmentation, and more. Several articles we reviewed in this study used U-Net(Mento et al, 2021a; Roshankhah et al, 2021; Roy et al, 2020; Xue et al, 2021; Gare et al, 2021).\nU-Net++: U-Net++ is an extension of the U-Net architecture that aims to further enhance the segmentation performance. It introduces a nested and densely connected skip pathway structure. In U-Net++, each encoder block is connected to all corresponding decoder blocks through skip connections, creating a more extensive and interconnected network. This architecture allows for better information flow and feature reuse across different scales, leading to improved segmentation accuracy and boundary delineation. One of the articles we reviewed in this study used Reg-UNet++ (Roy et al, 2020)."}, {"title": "3.1.10 Few-shot Learning", "content": "Few-shot learning is a machine learning paradigm that addresses the problem of learning from limited labeled data. In traditional machine learning approaches, a large amount of labeled data is typically required to train a model effectively. However, in real-world scenarios, collecting and annotating large datasets can be time-consuming, expensive, or impractical. Few-shot learning aims to overcome this limitation by enabling models to learn new concepts or tasks with only a few labeled examples. It focuses on the ability of a model to generalize and adapt to new classes or tasks based on a small amount of labeled data, often referred to as the \"support set.\" The key idea in few-shot learning is to leverage prior knowledge or information learned from related tasks or classes to facilitate learning on new tasks or classes with limited examples. This is achieved through various techniques such as meta-learning, where the model learns to quickly adapt to new tasks based on its previous experience, or by using generative models to synthesize additional training examples. One of the articles we reviewed in this study used Few-shot learning (Karnes et al, 2021)."}, {"title": "3.1.11 Transfer Learning", "content": "Transfer learning is a machine learning technique that involves leveraging knowledge learned from one task or domain to improve performance on another related task or domain. In transfer learning, a pre-trained model that has been trained on a large dataset and a related task is used as a starting point for a new task. The idea is that the pre-trained model has learned general features and representations that can be useful for the new task, even if the specific classes or labels are different. By using transfer learning, the model can benefit from the knowledge and representations learned from the large pre-training dataset, which can save training time and improve performance, especially when the target dataset is limited or the target task is challenging.\nReverse Transfer Learning: Reverse transfer learning, on the other hand, is a less commonly used term and refers to the process of transferring knowledge or models from a target domain or task back to the source domain or task. It involves utilizing the information or insights gained during the target task and applying them to improve the performance of the original source model or"}, {"title": "3.1.12 Support Vector Machine (SVM)", "content": "SVM is a popular CML algorithm used for classification and regression tasks. SVM is known for its ability to handle both linear and non-linear data by finding an optimal hyperplane that separates different classes or predicts the continuous target variable. In the case of classification, SVM aims to find the best decision boundary that maximally separates different classes in the input feature space. This decision boundary is determined by a subset of training samples called support vectors. SVM works by mapping the input data into a higher-dimensional feature space using a kernel function, which allows the algorithm to find a hyperplane that effectively separates the classes. The choice of the kernel function, such as linear, polynomial, or radial basis function (RBF), affects the SVM's ability to handle complex patterns and non-linear relationships. In the case of regression, SVM aims to find a hyperplane that best fits the data while minimizing the error between the predicted and actual target values. The SVM regression algorithm aims to find a balance between fitting the data closely and controlling the complexity of the model to avoid overfitting. Several articles we reviewed in this study used SVM (Al-Jumaili et al, 2021; Carrer et al, 2020; Chen et al, 2021; Wang et al, 2021)."}, {"title": "3.1.13 Decision Tree", "content": "A decision tree is another supervised CML algorithm that is commonly used for classification and regression tasks. It is a flowchart-like structure where internal nodes represent feature tests, branches represent the outcomes of those tests, and leaf nodes represent the predicted class or value. The decision tree algorithm recursively splits the data based on different features to create a tree-like model that can make predictions. At each internal node, a decision is made based on the values of a particular feature, and the data is split into subsets accordingly. This splitting process continues until a stopping criterion is met, such as reaching a maximum tree depth or a minimum number of samples at a node. During training, the decision tree algorithm determines the optimal splits by evaluating different feature and split point combinations based on certain criteria, such as Gini impurity or information gain. The goal is to create splits that result in homogeneous subsets with respect to the target variable. Once the decision tree is trained, it can be used to make predictions by traversing down the tree based on the feature values of an unseen sample. The path followed through the tree leads to a leaf node, which provides the predicted class for classification tasks or the predicted value for regression tasks. Decision trees are popular due to their interpretability and simplicity."}, {"title": "3.1.14 K-means", "content": "K-means is an unsupervised clustering algorithm that partitions data into a specified number of clusters (k). It assigns each data point to the nearest cluster by minimizing the distance to the cluster's centroid, which is iteratively updated until the assignments stabilize. It is commonly used for unsupervised learning tasks like grouping similar data points or segmenting datasets. One of the studies (Torti et al, 2024) we reviewed used K-means on the generated features by ResNet-50."}, {"title": "3.1.15 Random Forest", "content": "Random Forest is a CML ensemble method used for both classification and regression tasks. It builds multiple decision trees (described in section 3.1.13) during training and combines their predictions to improve accuracy and reduce overfitting. Each tree is trained on a random subset of the data, making Random Forest robust and capable of handling complex data patterns. One of the studies (Torti et al, 2024) we reviewed used Random Forest on the generated data by GAN."}, {"title": "3.1.16 Other Architectures", "content": "Saab transform-based successive subspace learning model: It refers to a specific approach for feature extraction and dimensionality reduction in image processing and computer vision tasks. It is based on a series of transformations called the Successive Subspace Learning (SSL) framework, with the Saab transform being one of the key components. The Saab transform is a non-linear transformation that aims to capture discriminative and compact representations of image features. It operates on local image patches and applies a series of operations, including patch-wise mean removal, PCA (Principal Component Analysis), and non-linear transformation using sigmoid functions. These operations are performed successively to obtain a hierarchical representation of the input image.\nNon-local channel attention ResNet: It refers to a variant or modification of the ResNet architecture that incorporates non-local channel attention mechanisms. It aims to enhance the representation power of ResNet models by introducing non-local operations that capture long-range dependencies across channels. In the context of the \u201cNon-local channel attention ResNet,\u201d the term \"non-local\" refers to the inclusion of non-local operations within"}, {"title": "3.2 Loss Functions", "content": "A classification model can be defined as $\\hat{y} = f_\\theta(x)$, where the AI model $f_\\theta$ is parameterized by a set of parameters $\\theta$ and an input image $x$ is assigned to the most probable class $\\hat{y}$. Given a training set of ultrasound images $x_i$ and their ground truth class $Y_i{(x_i, Y_i); i = 1,..., N}$, training a classification model consists of finding the model parameters $\\theta$ that minimize loss $L$, such as:\n$\\theta^* = arg \\min_\\theta \\sum_{i=1}^{N} L(\\hat{y}_i | Y_i)$"}, {"title": "3.2.1 Cross-entropy Loss", "content": "Training an AI model on a binary decision-making task (e.g., COVID-19 vs. CAP, or COVID-19 vs. healthy, etc.) usually utilizes binary cross-entropy or simply cross-entropy loss defined as:\n$L_{CE}(X,Y; \\theta) = - \\frac{1}{N} \\sum_{i=1}^{N} Y_i \\times log(\\hat{y}_i) + (1 - y_i) \\times log(1 - \\hat{y}_i).$"}, {"title": "3.2.2 Categorical Cross-entropy", "content": "Categorical cross-entropy works on multiclass (more than two classes; e.g., COVID-19 vs. CAP vs. Healthy) classification problems. This loss is typically used in an AI model when the model must select one or more categories among numerous possible categories/classes. This loss can be defined as:\n$L_{CCE}(X,Y; \\theta) = \\frac{1}{N} \\sum_{i=1}^{N} Y_i \\times log(\\hat{y}_i).$"}, {"title": "3.2.3 L1 Loss", "content": "L1 loss, also known as mean absolute loss, is typically used when an AI model is tasked to predict a continuous value (e.g., the distance between two landmarks, optimal location for lung scanning using ultrasound, etc.). It is defined as:\n$L_1(X,Y; \\theta) = \\sum_{i=1}^{N} | Y_{true} - Y_{predict} |,$"}, {"title": "3.2.4 Focal Loss", "content": "The focal loss is a dynamically scaled cross-entropy loss and is used when there is a class in the training data. Focal loss incorporates a modulating term in the conventional cross-entropy loss so that it can emphasize learning from difficult data samples that lead to misclassification more often. This loss is defined as:\n$L_{FL}(X,Y; \\theta) = - \\frac{1}{N} \\sum (1 - Y_i)^{\\gamma} \\times log(y_i),$"}, {"title": "3.2.5 Soft Ordinal (SORD) Loss", "content": "When output classes are independent of each other, their relative order in the loss calculation during deep model training does not matter. This scenario allows using one-hot encoding, i.e., setting all wrong classes to be infinitely far from the true class. However, there exists a soft order among classes in an ordinal regression scenario, where certain categories are more correct than others with respect to the true label (Diaz and Marathe, 2019) (i.e., a true class is no longer infinitely far from false classes, resulting in a continuity among classes). For these continuously related classes, Roy et al (2020) introduced a modified cross-entropy, called soft ordinal (SORD) loss, defined as:\n$L_{SORD}(X,Y; \\theta) = \\sum_{i=1}^{|N|} (\\frac{e^{-\\delta(\\eta,i)}}{\\sum_{j \\in N} e^{-\\delta(j,i)}}) \\times log(\\frac{e^{f_\\theta(x_i)}}{\\sum e^{f_\\theta(x_j)}})$"}, {"title": "3.2.6 Dice Loss", "content": "Dice Loss is a widely used loss function for image segmentation tasks", "as": "n$L_{Dice} = 1 - \\frac{2 \\cdot | A"}]}