{"title": "Interactive Counterfactual Exploration of Algorithmic Harms in Recommender Systems", "authors": ["Yongsu Ahn", "Quinn K Wolter", "Jonilyn Dick", "Janet Dick", "Yu-Ru Lin"], "abstract": "Recommender systems have become integral to digital experiences, shaping user interactions and preferences across various platforms. Despite their widespread use, these systems often suffer from algorithmic biases that can lead to unfair and unsatisfactory user experiences. This study introduces an interactive tool designed to help users comprehend and explore the impacts of algorithmic harms in recommender systems. By leveraging visualizations, counterfactual explanations, and interactive modules, the tool allows users to investigate how biases such as miscalibration, stereotypes, and filter bubbles affect their recommendations. Informed by in-depth user interviews, this tool benefits both general users and researchers by increasing transparency and offering personalized impact assessments, ultimately fostering a better understanding of algorithmic biases and contributing to more equitable recommendation outcomes. This work provides valuable insights for future research and practical applications in mitigating bias and enhancing fairness in machine learning algorithms.", "sections": [{"title": "INTRODUCTION", "content": "The integration of recommender systems into various aspects of digital life has revolutionized user experiences, from personalized content delivery to targeted advertisements. However, these systems, often shrouded in opacity, pose significant challenges related to fairness and bias. As recommender systems increasingly influence users' choices, it becomes crucial to address how algorithmic biases impact decision-making and user satisfaction.\nDespite the algorithmic harms' potential impact towards thousands of users who have different characteristics and perceptions, the solutions are not often human-centered, still largely relying on automatic methods to quantify and mitigate algorithmic harms at one-size-fit-all manner. Our preliminary results indicate that users generally lack the understanding of algorithmic harms, including their mechanisms and the extent to which they impacts recommendations. In addition, perceived impacts vary significantly, highlighting the needs for a nuanced solution to effectively mitigate the algorithmic harms.\nBased on the findings revealed from pilot study, we propose an interactive dashboard as a human-centered solution to support users in explore algorithmic harms in recommender systems. By introducing several modules and visualization, we pursue a user-centric exploration of three algorithmic harms, including miscalibration, stereotype, and filter bubble, perceived as exhibiting high distrust and desire to transparency. The current challenges compiled from pilot study are translated into system requirements, textual and visual explanations of algorithmic harms' inner workings and impact, social contextualization, and counterfactual retrieval.\nThis study aims to bridge the gap by presenting an interactive tool designed to enhance users' understanding of algorithmic harms in recommender systems. Our research not only contributes to the ongoing discourse on algorithmic fairness but also offers actionable insights into"}, {"title": "RELATED WORK", "content": ""}, {"title": "Visual analytics for fairness and algorithmic harms", "content": "Recent studies in visual analytics have advanced tools and visualizations to enable domain users and practitioners to foster fair data-driven decision-making practices. Compared to data-driven measures and methods, human-centered solutions that incorporate visual representations of biases in classification and ranking results have proven effective in helping users make sense of how biases arise [2, 5, 23, 24] or specific types of biases, such as intersectional biases in subgroups [6, 12] and biases in image classification [11]. Recently, such systems were developed to be oriented towards involving end-users\u00e2\u0102\u017a perceptions of fairness in the loop [14]and operationalize the real-world decision-making process in specific teams and organizations with the support of decision diagram [12]. Other studies focus on specialized analyses like graph mining [16,26] and causal analysis, employing causal diagram visualizations to promote fairness [8, 27].\nDespite these advancements, algorithmic harms and fairness issues in recommender systems, affecting thousands of users, remain largely unaddressed beyond conceptual frameworks for fair recommender systems, which include explanation, audit, and mitigation engines [9]. Our study represents a pioneering effort to inform users about various types of algorithmic harms and empower them to comprehend and simulate the impacts within recommender system usage."}, {"title": "Interactive counterfactual fairness", "content": "Recommender systems are known to perpetuate various types of algorithmic harms and effects. While a number of automatic methods to quantify and mitigate such biases have been proposed, studies [4, 15, 21, 22] indicate that their impacts can vary significantly among different users. For instance, in career recommendations [21], users tend not to perceive gender stereotypes as harmful as long as recommendations are effective for them. [15] has shown that users develop their own mental models very differently based on experiences and background, resulting in different perception of algorithmic harms.\nIn response to the opaque nature of recommender systems, studies have attempted to design various types of explanations such as why/why-not [25] or personal explanations [10] to provide users with rationales behind recommendations. Counterfactual explanations have recently gained attention for their intuitiveness and low cognitive load to understand its perceived efficacy and mitigate filter bubbles [7, 19].\nOur study, informed by in-depth interviews with a diverse group of users, introduces an interactive tool designed to help users make sense of algorithmic harms. We present a suite of visualizations, counterfactual explanations, and interactive modules to allow users to make sense of the impact of algorithmic harms over their use of recommender systems."}, {"title": "FROM USER PERCEPTION INTERVIEWS TO SYSTEM REQUIREMENTS", "content": ""}, {"title": "Pilot Interviews", "content": "Our study was motivated by pilot interviews aimed at understanding how current users in existing recommender platforms perceive different types of algorithmic harms. This preliminary study consisted of 1-hour interview sessions with 8 participants who daily consume recommender platforms and content including video (Youtube), social media (Instagram, Facebook), and online news (Google, Bing, and algorithm-based news platforms such as Ground news). Participants were recruited in a diverse pool of nationalities (US, Portugal, Mexico, South Korea, and Taiwan), gender (Male: 2, Female: 2, LGBTQ+: 3), age (18-24: 2, 25-34: 3, 35-44: 2, 45-54: 1), and education levels based on the highest degree completed (High school: 2, Undergraduate: 2, Master's: 3, Ph.D: 1). Each session began with a semi-structured interview allowing them to share their daily use and unsatisfactory experiences of recommendation platforms, followed by card sorting activities that express the degree to which they perceive different types of algorithmic harms regarding trust (i.e., how much do you think it makes you distrust the system if it exists?) or transparent (i.e., how important is it to know if the system informs you of it?). In the activity, participants were instructed to place multiple cards describing well-known algorithmic harms including filter bubble, stereotype, popularity bias, miscalibration, and echo chamber within a two-dimensional space, with each axis representing degrees of transparency and trustworthiness, ranging from low to high.\nAfter the coding analysis of pilot studies, we identified three major challenges in user perception of algorithmic harms in recommender systems:\nC1. Lack of understanding in algorithmic harms: Participants had noticeably little understanding of how algorithmic harms work or potentially worsen their experiences. Especially, all pilot interviewees were initially unaware of stereotyping effects, though they considered these serious once informed.\nC2. Users' focus on categorical preferences. Despite recommendation studies emphasizing item-level performance, users' interests lie at topic and category levels. Participants mentioned broad categories like health, music, or politics. Three users were particularly interested in how recommendations are distributed and miscalibrated across their interest categories.\nC3. Different levels of perceived impacts over algorithmic harms. Card sorting results showed that perceived impacts of algorithmic effects vary among users. While all harms were considered harmful by at least one participant, miscalibration, stereotype, and filter bubble triggered higher distrust and desire for transparency.\nC4. Recommendations as social space and needs of transparency: Five participants viewed recommendations from a social perspective, expressing interest in investigating their groups (e.g., women, users with similar preferences) or identifying themselves within the broader user context. This stems from the perceived nature of recommender systems as spaces where numerous users interaction and collective preferences highly influence the inner workings of algorithms."}, {"title": "System Requirements", "content": "Based on the findings from the pilot interviews, we derived the following system requirements to address the identified challenges:\nR1. Transparency of algorithmic harms: To address C1, the system should provide clear and understandable explanations of different algorithmic harms, such as miscalibration, stereotype, and filter bubble effects. These explanations should be accessible to users with varying levels of technical knowledge.\nR2. Category-Level analysis: To address C2, the system should allow users to analyze and understand recommendations and algorithmic effects at a categorical level. Users should be able to view and compare recommendation behavior across different demographic categories and genre distributions.\nR3. Evaluation of personalized impact of algorithmic harms: In light of C3, the system should enable users to assess the perceived impact of algorithmic harms on their personal experience using recommendation algorithms. This feature should support individual-level analysis, allowing users to see how specific harms"}, {"title": "INTERACTIVE DASHBOARD FOR EXPLORING ALGORITHMIC HARMS IN RECOMMENDER SYSTEMS.", "content": ""}, {"title": "Method", "content": ""}, {"title": "Dataset and Algorithm", "content": "In the study, we use the MovieLens 1M dataset that contains demographic attributes such as gender and age. We binarize 1,000,209 user ratings in the dataset to convert it to implicit feedback by dropping all ratings less than 4. ranging from 1 to 5 over movie items across 18 movie genres. We removed all users with less than 20 interactions to ensure the visualization of each user with a meaningful number of interactions. After the preprocessing steps, the data includes 562,800 interactions from 5,180 users and 3,526 items. The dataset includes 18 movie genres to allow us to engage category-level analysis.\nSimilar to previous literature [1,20], the implicit feedback data was split into 80% and 20% for training and testing using user-fixed setting where the interactions within each user were divided into training and test set based on chronological order. We trained Bayesian Personalized Ranking (BPR), one of recommendation algorithms widely studied in recommendation studies."}, {"title": "Measures for algorithmic harms", "content": "In this section, we introduce measures for three algorithmic harms, miscalibration, stereotype, and filter bubble, proposed in existing literature.\nMiscalibration. Introduced by [20], miscalibration quantifies the discrepancy between a user's actual preference p(cu) and predicted preference q(cu) over item categories \u0109 (e.g., movie genres) for a user \u0438.\nThe individual-level miscalibration, $MC_u$, for a user u, uses Kullback-Lieber divergence DKL to calculate the discrepancy between two probability distributions to measure how q deviates from p [20], as follows:\n$MCu(p,q) = DKL(p||q) = \\sum_c p(cu) log\\frac{p(cu)}{q(cu)}$\n(1)\nMiscalibration ranges from [0, inf), with smaller values indicating greater similarity between q and p. Zero miscalibration represents perfect calibration. System-level miscalibration is denoted as MC(P,Q), where P and Q are sets of actual and predicted preferences for all users.\nStereotype. Following [3], stereotype quantifies the degree of systematic over generalization of a predicted preference q for a user u based on certain characteristics of individuals, which can be computed as follows:\n$STu(p,q) = DJs(p||P) \u2013 Djs(q||Q)$.\n(2)\nwhere $DJs (p||P) = \\frac{DKL(P||P)+DKL(P||P)}{2}$ is a symmetric measure of quantifying the distance between two probabilistic distributions.\nFilter bubble. The term, \"filter bubble\", as defined by Pariser [17], describes the systematic reinforcement of narrowing exposure to diverse content. It involves entropy as a measure of content diversity where Entropy(u) = \u2013 $ \\sum_c p(cu) log p(cu)$. In this study, we quantify the difference in entropy of category-wise preferences between actual (p) and predicted (q) preferences:\n$FilterBubbleu(p,q) = DVu(q) \u2013 DVu(p)$\n(3)\nwhere $DVu = Entropy(u) = \u2212 \\sum_{c \\in \\hat{c}}p(c|u)logp(c|u)$ measures the diversity of a user u. We also introduce the term of \"inflated diversity\" [3] to describe a counterpart effect of filter bubble in which a user receives an over-diverse prediction q."}, {"title": "Counterfactual retrieval", "content": "Our study leverages counterfactual retrieval to support the simulation of \u00e2\u0102IJwhat if I were another user?\u00e2\u0102\u0130. Unlike counterfactual generation, our module is operationalized to retrieve actual users rather than generate them for users to be more immersive of what-if situation. To facilitate the counterfactual search (e.g., what if I were a man?), we borrow matching techniques, which retrieve counterpart users or groups in observational data (i.e., control group) who have as the most similar conditions and characteristics (e.g., categorical preferences) for all variables but a treatment variable (e.g., gender). Our study supports two types of counterfactual queries:\n\u2022 Demographic counterfactual: e.g., if I were a man, what recommendations would I get?\n\u2022 Preference counterfactual: e.g., if I preferred more Sci-fi genre movies, what items would I get?"}, {"title": "Interactive System", "content": ""}, {"title": "Counterfactual Retrieval View", "content": "This component provides an interface for counterfactual user retrieval. Users can select demographic or preference-based counterfactual queries to see how different attributes would change their recommendations. For example, they can explore what recommendations they would receive if they were of a different gender or had different genre preferences. By allowing queries based on both genre preferences and demographics, users can identify effects on both item-level and category-level preferences. This view enhances transparency and offers a personalized impact assessment, helping users understand the algorithmic harms they may be facing."}, {"title": "User Space", "content": "The User Space visualizes users to help them understand the system as a social space in which algorithmic harms may occur. In this two-dimensional visualization users are represented as a glyph or circle whose high-dimensional representations of their category-wise preferences are projected into 2D space using UMAP [13]."}, {"title": "USAGE SCENARIO", "content": ""}, {"title": "Data practitioners' understanding of the penetration of algorithmic harms", "content": "Consider Jane, a data practitioner working at a movie streaming company. She wanted to investigate how systematic biases affect users with different demographics and preferences, with a particular focus on understanding the disparities in the recommendations they receive.\nUpon loading the interface, Jane first encountered the User Space, where users are positioned based on how much their preferences deviate from the mean. When she selected \"stereotype\" in the Algorithmic Harm Indicator, she immediately noticed that, while most of users exhibited a higher degree of stereotyping, atypical users positioned at"}, {"title": "End-users' sense-making of algorithmic negative impact on their recommendations", "content": "Consider Sarah, a frequent user of entertainment media who often felt that the recommendations she received did not align with her interests. She wished to understand the reasons behind this misalignment.\nUpon loading the interface, Sarah first encountered her glyph in the User Space, where the algorithmic effects on her preferences were visualized. She immediately noticed that her glyph appeared closer to the typical preference. Selecting \"stereotype\" in the Algorithmic Harm Indicator and hovering over her icon (Fig. 1C), she observed that her original preferences had deviated significantly from the typical ones. However, due to stereotyping in the algorithm, her preferences were forced toward the typical (Fig. 1B-ii).\nBy examining the prototypical user in her cluster, she discovered that this user exhibited higher levels of stereotyping and miscalibration, indicated by the dark red stereotyping bar on the glyph pointing towards the typical preference (Fig. 4-iv). In the user details (Fig. 1A), she noticed that her preference for Romance was inflated (as indicated by the blue bar) and her preference for Horror was deflated (as indicated by the red bar). Curious about the impact of her demographics, Sarah used the Counterfactual Simulation to explore the query: \"What if I were a man?\" The system retrieved recommendations that a male user with similar overall preferences would receive. In the User Space, she found that, unlike her, the counterfactual user was visualized with an inverse-stereotyping effect (as indicated by a solid circle (Fig. 4-iii). Through these interactions, Sarah was able to understand that the recommender algorithm has a disparate impact from algorithmic effects that force some types of content to varying extents for different users."}, {"title": "CONCLUSION", "content": "In this study, we present a human-centric approach to addressing the transparency of algorithmic harms in recommender systems, with an interactive tool designed to help users make sense of them. A suite of visualizations, counterfactual explanations, and interactive modules, which were developed based on in-depth user perception interviews, enables users to explore the impacts of algorithmic harms on their recommendations. As a future work, we will focus on expanding the scope of our system to include more diverse datasets and enhanced data exploration capabilities as well as evaluate our tool with various types of users to understand the real-world impact of our tool."}]}