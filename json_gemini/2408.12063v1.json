{"title": "A Deconfounding Approach to Climate Model Bias Correction", "authors": ["Wentao Gao", "Jiuyong Li", "Debo Cheng", "Lin Liu", "Jixue Liu", "Thuc Duy Le", "Xiaojing Du", "Xiongren Chen", "Yanchang Zhao", "Yun Chen"], "abstract": "Global Climate Models (GCMs) are crucial for predicting future climate changes by simulating the Earth systems. However, GCM outputs exhibit systematic biases due to model uncertainties, parameterization simplifications, and inadequate representation of complex climate phenomena. Traditional bias correction methods, which rely on historical observation data and statistical techniques, often neglect unobserved confounders, leading to biased results. This paper proposes a novel bias correction approach to utilize both GCM and observational data to learn a factor model that captures multi-cause latent confounders. Inspired by recent advances in causality based time series deconfounding, our method first constructs a factor model to learn latent confounders from historical data and then applies them to enhance the bias correction process using advanced time series forecasting models. The experimental results demonstrate significant improvements in the accuracy of precipitation outputs. By addressing unobserved confounders, our approach offers a robust and theoretically grounded solution for climate model bias correction.", "sections": [{"title": "Introduction", "content": "Global Climate Models (GCMs), such as those developed by the Coupled Model Intercomparison Project phase 6 (CMIP6) (Eyring et al. 2016), are vital tools for predicting future climate changes. These models simulate the physical and chemical processes of the Earth systems including the atmosphere, oceans, land, and ice to provide detailed climate forecasts. Despite significant advancements in GCM, their output still exhibits systematic biases. These biases primarily come from uncertainties within the models, simplifications in parameterization processes, and inadequate representations of complex climate phenomena (Mouatadid et al. 2023). For instance, processes such as cloud formation, precipitation, radiation, and convection are often simplified into parameterization formulas in GCMs, which may not accurately reflect real-world conditions, thus leading to biases. Consequently, (Lafferty and Sriver 2023) highlighted that bias corrections were particularly crucial for near-term precipitation projections, especially in cases where observational data are inconsistent with GCMs output.\nTo enhance the reliability and accuracy of GCM outputs, numerous bias correction techniques, ranging from simple linear scaling to advanced quantile mapping, have been proposed to modify GCM outputs and align them more closely with actual observations (Casanueva et al. 2020; Chen et al. 2013; Dowdy 2020; Enayati et al. 2020; Feigenwinter et al. 2018; Heo et al. 2019; Lafon et al. 2013; Maraun 2016; Mehrotra, Johnson, and Sharma 2018; Miao et al. 2016; Nahar, Johnson, and Sharma 2018; Piani, Haerter, and Coppola 2010; Smitha et al. 2018; Teutschbein and Seibert 2012a; Wu et al. 2022b). These methods typically rely on historical observation data and statistical approaches to adjust climate model outputs and correct systematic errors (Maraun and Widmann 2018).\nHowever, these traditional methods have been found to inflate simulated extremes, raising concerns about their use in climate change applications where extremes are significant, such as drought and flooding (Huang, Hall, and Berg 2014; Past\u00e9n-Zapata et al. 2020). Another major limitation of most bias correct methods is their assumption that all relevant factors are known and observable, which is unrealistic in practical applications. Climate systems are complex and include many factors that may not be fully observed, such as microclimate effects, regional climate characteristics, and anthropogenic influences. These potential unobserved (confounding) factors dynamically impact time series forecast. However, since they cannot be fully observed, they are frequently overlooked by current bias correction methods. This overlook limits the potential of most existing methods in bias correction.\nTo deal with the challenge of extreme conditions, the study by (Nivron et al. 2024) incorporates advanced time series forecasting models into the bias correction of extreme weather events, such as heatwaves, by considering the bias correction as a time-indexed regression model with stochastic output. This provides a new perspective: adapting time series forecasting models to bias correction can improve model performance. However, although advanced models have shown significant potential in time series forecasting (Zhou et al. 2021; Wu et al. 2022a, 2023a; Wang et al. 2024), particularly in climate forecasting (Bi et al. 2023; Wu et al. 2023b), they typically overlook the existence of unobserved confounding factors, which leads to biased results.\nIn the field of causal inference with time series, recent studies have started addressing the challenge of unobserved confounders in predicting the potential outcomes of treatments in time series data. Instead of assuming unconfoundedness (Pearl 2000), these studies operate under a weaker assumption that only multi-cause confounders exist (Bica, Alaa, and van der Schaar 2020; Wang and Blei 2019; Li et al. 2024; Cheng et al. 2023; Xu et al. 2023). A variable is said to be a confounder if it is a common cause of both the treatment and the outcome (Pearl 2009). For example, in the Directed Acyclic Graph (DAG) shown in Figure 1(a), the confounder X affects both the treatment A and outcome Y. In our bias correction problem, precipitation output is always treated as the outcome, and other related climate variables are treated as treatment variables. The most common method for predicting potential outcomes is to use all treatment variables. However, neglecting unobserved multi-cause confounders Z, such as large-scale atmospheric circulation patterns or oceanic processes, can lead to simultaneous effects on various climate variables, including precipitation. These multi-cause confounders make it possible to apply current research outcomes to bias correction in GCM.\nInspired by the work of (Bica, Alaa, and van der Schaar 2020), in this paper, we propose a deconfounding bias correction method. It is important to note that estimating hidden confounders in climate bias correction is much more complex. This increased complexity arises not only from the intricate nature of hidden confounders in climate science but also because these confounders need to be inferred from GCM and observation data.\nOur contribution can be summarized as the following:\n1.  We use causality to identify and understand unobserved confounders, allowing us to obtain unbiased outcome in the presence of multi-cause confounders. By identifying latent confounders through constructing a factor model over time, our approach can capture these unobserved factors, resulting in unbiased outcomes.\n2.  We develope a two-phase algorithm: Deconfounding and Correction. The Deconfounding Bias Correction (BC) factor model captures the confounders from both Global Climate Model (GCM) output data and observational data, and the correction model uses these confounders as additional information for the bias correction task. Inspired by the TSD (Time Series Deconfounder) (Bica, Alaa, and van der Schaar 2020), our method extend to GCM and observation data."}, {"title": "Problem Formulation", "content": "As shown in Table 1, let the random variable V represent the set of climate variables, such as humidity, temperature, pressure, precipitation and others. Among these variables, Y denote the outcome of interest, for example, precipitation. In our problem setting, because we will use both historical and current data of these variables, to avoid confusion, $X_t$ represent historical version of $V \\setminus Y$. $A_t$ respresnt current version of $V \\setminus Y$.\nThese variables are collected from two sources: Global climate models (GCMs) and observations. Moreover, we use $X^G_t$ and $A^G_t$ to represent historical and current variables of the GCM data respectively, while $X^O_t$ and $A^O_t$ for historical and current variables. The outcome from GCMs and observations are denoted as $Y^G_t$ and $Y^O_t$ respectively.\nThe data for a location, also known as the location trajectory, consist of realizations of the previously described random variables {$\\{x^G_t, a^G_t, y^G_t, x^O_t, a^O_t, y^O_t\\}$}. Let $y(\\bar{a}_t)$ represent the potential outcome (precipitation), which could be either factual or counterfactual, for each possible treatment course $\\bar{a}_t$, where $\\bar{a}_t = (a_1, ..., a_t)$. Consequently, we have $y^G(\\bar{a}_t)$ and $y^O(\\bar{a}_t)$. The concept of potential outcomes allows us to consider what the outcome y would be under different treatment scenarios, which is essential for causal inference (Rubin 1974).\nTo correct future k steps precipitation bias $\\Delta Y = Y^O - Y^G$ which is the difference between observational precipitation $y^O$ and GCM output precipitation $y^G$, a common way is to use all data to obtain a regression model (Nivron et al. 2024) which can be represented by Figure 1(a). However, the existence of hidden confounders (e.g. unobserved atmospheric and oceanic circulation) will result in biased result.\nTo address the issue of hidden confounders in time series data, (Bica, Alaa, and van der Schaar 2020) have developed a method, assuming the presence of multi-cause hidden confounders Z, as shown in the Figure 1(b).\nHowever, no attempt has been made to adapt their work to bias correction, including bias correction for climate models. In this paper, we extend the work for the bias correction problem as shown in the Figure 1(c) which make the most of information we have. We assume that hidden variables Z affect both GCM and observations. Learning Z, we would estimate the potential outcome in observations for each location conditional on the location history of covariates $X = (X_1, ..., X_t) \\in \\chi^O$, treatments $A = (A_1, ..., A_t) \\in \\mathcal{A}$ and confounders $Z_t = (Z_1, ..., Z_t) \\in \\mathcal{Z}_t$:\n$\\mathbb{E} [Y^O (a_t) | A_t, X, Z_t]$"}, {"title": "Proposed method", "content": "The existence of confounders can result in the obtained association among X, A, Y not accurately representing the true relationships, potentially leading to biased results. To solve this, we (1) input the climate model data into the Deconfounding Bias Correction (BC) factor model to obtain the multi-cause hidden confounder which is essential for bias correction (we call this step 'Deconfounding'). Then (2) we use a hidden multicause confounder as a bias source, combine it with observational data, building a precipitation correction model to help the climate model to have a better estimate of future output (we call this step 'Correction')."}, {"title": "Deconfounding", "content": "To address the challenge of deconfounding time series data with time varing latent confounder, (Bica, Alaa, and van der Schaar 2020) proposed the Time Series Deconfounder (TSD), which extends the deconfounder methodology introduced by (Wang and Blei 2019) to the time series domain. The fundamental principle of the TSD is that It utilizes factor rmodel to infer substitutes for hidden confounders as treatments.\nThe goal of this part is to generalize the factor model proposed by (Bica, Alaa, and van der Schaar 2020) to a Deconfounding BC factor model for bias correction. This extension aims to extend the Time Series Deconfounder to a complex time series setting of two sources."}, {"title": "Deconfounding BC factor model", "content": "To extend time series deconfounder (Bica, Alaa, and van der Schaar 2020) to bias correction, We propose an enhanced multi-source factor model specially for this task, termed the Deconfounding BC factor model.\nFor single source data, the unobserved confounder affects X, A, Y from one source, allowing us to infer the sequence of unobserved confounders $z_t = g(h_{t-1})$, where $h_{t-1} = \\{a_{t-1}, X_{t-1}, Z_{t-1}\\}$ is the realization of $H_{t-1}$. Specifically, factorization can be expressed as follows:\n$p(\\bar{a}_{t_1}, ..., \\bar{a}_{t_k} | Z_t, x_t) = \\prod_{j=1}^k p(\\bar{a}_{t_j} | Z_t, x_t)$.\nTo extend this model for multi-source data, consider both GCM and Observations. The unobserved confounder will affect X, A, Y in both sources, allowing us to infer the sequence of unobserved confounders $z_t = g(h_{t-1})$ that can be used to render both source treatments conditionally independent, where $h_{t-1} = \\{h^G_{t-1}, h^O_{t-1}\\}$ is the realization of $H_{t-1}$. Specifically, the factorization for the multi-source scenario can be expressed as follows:\n$p(\\bar{a}^G_{t_1}, ..., \\bar{a}^G_{t_k} | Z_t, x^G_t) = \\prod_{j=1}^k p(\\bar{a}^G_{t_j} | Z_t, x^G_t)$,\n$p(\\bar{a}^O_{t_1}, ..., \\bar{a}^O_{t_k} | Z_t, x^O_t) = \\prod_{j=1}^k p(\\bar{a}^O_{t_j} | Z_t, x^O_t)$.\nIt allows us to infer the sequence of latent variables Z that can be used to render the treatments conditionally independent with the observed location covariates $x_t$. As in one location, the same confounders Z wil affect both GCM and observations. In this case, the Z we learned should render treatments in both GCM and observational data.\nSince the structure of the factor model depends on causality, which relies on the assumptions listed below.\n*assumption 1*. Consistency. If $A_{>t} = \\bar{a}_{>t}$, then the potential outcomes for following the treatment $\\bar{a}_{>t}$ is the same as the factual outcome $Y(\\bar{a}_{>t}) = Y$.\n*assumption 2*. Positivity (Overlap): if $\\mathbb{P}(A_{t-1} = \\bar{a}_{t-1} | X_t = x_t) \\neq 0$ then $\\mathbb{P}(A_t = a_t | A_{t-1} = \\bar{a}_{t-1}, X_t = x_t) > 0$ for all $a_t$.\n*assumption 3*. Sequential Single Strong Ignorability\n$Y(a_t) \\bot A_{tj} | X_t, H_{t-1}$, for all $t \\in \\{0, ..., T\\}$, and for all $j\\in \\{1, ...$\nNext, we provide a theoretical analysis for the soundness of the learned $Z_t$ by introducing the concept of sequential Kallenberg construction (Bica, Alaa, and van der Schaar 2020), as follows.\n*Definition 1*. Sequential Kallenberg construction At timestep t, we say that the distribution of assigned causes $(A_{t_1}^G, ..., A_{t_k}^G)$, $(A_{t_1}^O, ..., A_{t_k}^O)$ admits a sequential Kallenberg construction from the random variables $Z_t = g(H^G_{t-1}, H^O_{t-1})$ and $X^G, X^O$ if there exist measurable functions $f_G: \\mathcal{Z}_t \\times \\mathcal{X}^G \\times \\mathcal{U}^G \\rightarrow A^G$ and $f_O: \\mathcal{Z}_t \\times \\mathcal{X}^O \\times"}, {"title": "Correction", "content": "After obtaining hidden multi-cause confounders Z from both GCM and observational data in one area. In our problem formulation, we aim to correct the bias between $Y^G$ and $Y^O$ while considering unobserved confounders. A key question is how to utilize this information for bias correction. Advanced time series forecasting models such as iTransformer (Liu et al. 2024) have demonstrated significant potential in capturing temporal information, and integrating these models into our bias correction approach promises substantial improvements.\nWe propose to use probability model for bias correction. As Figure 4 shown, we build a prediction model based on the observational data and the multi-cause confounder Z to predict future precipitation.\nThe probability model employed in is to obtain predicted $\\Delta Y$. The probability model can theoretically be any time series prediction model capable of generating samples from $P_O(\\Delta Y_{t+k} | A^G, A^O, Z)$, possessing an explicit likelihood form, and utilizing the constructed training data for this purpose. For instance, a linear regression model may serve as the forecasting model. However, due to its inherent assumption of independence among the elements of $\\Delta Y_{t+k}$, it is likely to result in a poor fit.\nTo perform well in such a forecasting task. We choose the state of art forecasting model iTransformer (Liu et al. 2024). The reason why we choose this model is: first, its performance in long-term forecasting is great. second, it is a simple model that is an inverted transformer designed for time series. More details about iTransformer can be found in the Appendix.\nAfter we get the $\\Delta Y_{t+k}$, We can obtain our corrected GCM output with:\n$\\hat{Y}^G_{t+1}, \\hat{Y}^G_{t+2}, ..., \\hat{Y}^G_{t+k} = \\{Y^G_{t+1} + \\Delta Y_{t+1}, Y^G_{t+2} + \\Delta Y_{t+2},...,Y^G_{t+k} + \\Delta Y_{t+k}\\}$"}, {"title": "Experiments and results", "content": "The objectives of the experiments are as follows: 1) Use synthetic data sets to evaluate the correctness of the latent confounder captured by our deconfounding method. 2) Build a correction model to correct precipitation predictions made by the climate model. Implementation code will be given later."}, {"title": "Experiments on Synthetic Data", "content": "To assess the effectiveness of our deconfounding method proposed in this paper, we conducted experiments using synthetic data in which we use predictive analysis to assess the influence of hidden confounders. Validation with real-world data is challenging because the true impact of hidden confounding cannot be precisely determined (Wang and Blei 2019).\n*Simulated dataset* To maintain the generality of the simulation process, we propose generating a dataset using a two-source autoregressive model based on the causal summary graph shown in Figure 1(c). At each timestep t, we simulate time-varying covariates $X_t$ from two sources ($X_{source1}$ and $X_{source2}$), along with a multi-cause hidden confounder $Z_t$. A detailed description of the data generation process is provided in the Appendix.\nWe create datasets that include 500 locations, 3650 timesteps, and $k = 3$ covariates and treatments. To introduce time dependencies, we assign $p = 5$. Each dataset is divided into an 80/10/10 split for training, validation, and testing purposes, respectively.\nOur method's effectiveness relies on two key points. First, the inferred latent variable Z must, along with X, accurately predict the treatment A. This ensures that A is conditionally independent given Z and X. Second, and most importantly, our method aims to learn the latent confounder Z. If the inferred Z closely matches or has a similar distribution to the simulation-generated Z, it demonstrates that our method successfully captures the true latent confounders.\nThe theory posits that using inferred latent variables as substitutes for hidden confounders can yield unbiased outcomes, leveraging the factor model's ability to accurately capture the distribution of the assigned causes. If the multi-layer output aligns well with the generated treatment, we can assert that our method effectively captures the treatment and ensures these treatments are conditionally independent when conditioned on covariates and confounders, as previously discussed.\nTo validate this, we performed a predictive check. The mean squared error (MSE) for the assigned treatment is 0.06157, indicating the effectiveness of our method in capturing treatments. Additionally, we compared the inferred Z with the simulation-generated Z. The resulting MSE is 0.00187. This further demonstrates the effectiveness of our method in learning latent confounders."}, {"title": "Case study: South Australia", "content": "In a bias correction (BC) process, data from climate models are compared to actual observations (or their proxies, like re-analysis products) to adjust for biases. Since global climate models (GCMs) usually have a lower resolution compared to observations or reanalysis reference data, BC often involves downscaling the resolution of GCMs. We adopt this methodology as well. For the climate model data, we selected the 15 initial condition runs from the Institut Pierre-Simon Laplace (IPSL) climate model as part of the sixth Coupled Model Intercomparison Project (CMIP6) historical experiment. The data, which is available on a monthly basis, includes climate variables such as tmax (maximum temperature 2 meters above the surface) and prate (precipitation rate), with our bias correction efforts centered on prate.\nThe IPSL\u00b9 model is run at a 250km nominal resolution and is not re-gridded. We selected the closest geographical point to South Australia for the case study, covering the period from 1948 to 2014.\nFor observational reference data, NCEP-NCAR Reanalysis 12, provided by the National Oceanic and Atmospheric Administration (NOAA), is utilized. The dataset encompasses the same variables as the IPSL model (e.g., tmax & prate). NCEP-NCAR reanalysis data is available at a monthly frequency, covering the period from 1948 to 2014, with a global resolution of 2.5 degrees in both the latitudinal and longitudinal directions, and has not been re-gridded. The nearest geographical point to South Australia was selected.\nThis research concentrates on bias correction of the precipitation rate (prate) to improve the accuracy of precipitation predictions.\nData Preprocessing We extracted and converted climate data from 1948 to 2014 for the South Australia region as shown in Figure 5. The data, originally in NetCDF format, were transformed into CSV files for further processing. Subsequently, we split the data into a training period (1948-1992) and a testing period (1993-2014). The IPSL dataset contains 30 initial conditions; to generalize our model, we selected 15 initial conditions for the experiment. In this paper, we present results from three of these initial conditions(r5i1p1f1, r6i1p1f1 and r7i1p1f1) along with their average results. Complete results can be found in the Appendix.\n*Experiment settings* To evaluate our proposed method, we compare our methods with several bias correction baseline methods, including linear scaling (Teutschbein and Seibert 2012b), variance scaling (Teutschbein and Seibert 2012b), quantile mapping (Cannon, Sobie, and Murdock 2015), quantile delta mapping (Tong et al. 2021) and Temporal BC (Nivron et al. 2024) methods. Implementation details of baselines are introduced in Appendix. The metric we are using to compare the performance is the Mean Squared Error (MSE) and the Mean Absolute Error (MAE)."}, {"title": "Results", "content": "Table 2 illustrates the three-step-ahead predictions using our deconfounding bias correction method on the dataset with different GCM initial settings. After applying the deconfounding step and augmenting the dataset with substitutes for the hidden confounders, we evaluated the performance of various bias correction methods using MSE and MAE. The results in Table 2 support the effectiveness of the Deconfounding BC method, which consistently achieves the lowest MSE and MAE across all experimental conditions. This indicates its superior performance in reducing prediction errors and aligning closely with the observed data.\nThe QQ plots (Figure 6) reveal that among the evaluated bias correction methods, Deconfounding BC shows the closest alignment with the red dashed line (y=x) across the entire range of observed precipitation values. This indicates that Deconfounding BC is the most effective in accurately capturing the observed precipitation levels, outperforming other methods in reducing biases and providing reliable predictions.\nFigure 7 presents box plots of monthly precipitation values for South Australia using different bias correction methods, with 'obsp' representing the observed (real) values. Among the methods compared, Deconfounding BC stands out for its effectiveness in bis correction.. The box plot for Deconfounding BC shows a median and distribution range that closely match the observed values, indicating that this method captures the variability and distribution of real precipitation data more accurately than other methods."}, {"title": "Ablation Study", "content": "To demonstrate that the latent confounder learned by our Deconfounding BC factor model contains essential information, we compared our correction model's results with and without the latent confounder, as shown in Table 3. The performance improved significantly with the inclusion of the hidden confounder."}, {"title": "Conclusion", "content": "In this paper, we proposed the deconfounding bias correction method, with the presence of multi-cause confounders. By integrating climate bias correction techniques with advancements in causality based time series deconfoudning method, our approach offers a novel perspective for future studies. This highlights the importance of not always assuming that all variables are observed."}]}