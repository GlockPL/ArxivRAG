{"title": "ASTRA: A Scene-aware TRAnsformer-based model for trajectory prediction", "authors": ["Izzeddin Teeti", "Aniket Thomas", "Munish Mongat", "Sachin Kumar", "Uddeshya Singh", "Andrew Bradley", "Biplab Banerjeet", "Fabio Cuzzolin"], "abstract": "We present ASTRA (A Scene-aware TRAnsformer-based model for trajectory prediction), a lightweight pedestrian trajectory forecasting model that integrates the scene context, spatial dynamics, social inter-agent interactions and temporal progressions for precise forecasting. We utilised a U-Net-based feature extractor, via its latent vector representation, to capture scene representations and a graph-aware transformer encoder for capturing social interactions. These components are integrated to learn an agent-scene aware embedding, enabling the model to learn spatial dynamics and forecast the future trajectory of pedestrians. The model is designed to produce both deterministic and stochastic outcomes, with the stochastic predictions being generated by incorporating a Conditional Variational Auto-Encoder (CVAE). ASTRA also proposes a simple yet effective weighted penalty loss function, which helps to yield predictions that outperform a wide array of state-of-the-art deterministic and generative models. ASTRA demonstrates an average improvement of 27%/10% in deterministic/stochastic settings on the ETH-UCY dataset, and 26% improvement on the PIE dataset, respectively, along with seven times fewer parameters than the existing state-of-the-art model (see Figure 1). Additionally, the model's versatility allows it to generalize across different perspectives, such as Bird's Eye View (BEV) and Ego-Vehicle View (EVV).", "sections": [{"title": "1 Introduction", "content": "The pursuit of forecasting human trajectories is central, acting as a cornerstone for devising secure and interactive autonomous systems across various sectors. This endeavour is crucial in a wide array of applications, encompassing autonomous vehicles, drones, surveillance, human-robot interaction, and social robotics. Furthermore, it is crucial for predictive models to strike a balance between accuracy, dependability, and computational efficiency, given the imperative for these models to function on in-vehicle processing units with limited capabilities. The challenge of trajectory prediction involves estimating the future locations of agents within a scene, given its past trajectory. This estimation task can be tackled either through Bird's Eye View (BEV) (Figure 2a) or Ego-Vehicle View (EVV) (Figure 2c) perspectives. This demands a comprehensive understanding of the scene, in addition to spatial, temporal, and social aspects that govern human movement and interaction. To solve the prediction problem, various building blocks, including RNNs, 3D-CNNs, and transformers, have been employed to address the temporal dimension, with transformers demonstrating superior efficacy [1, 2]. However, temporal modelling alone is unaware of the social behaviour of the agents within the scene, i.e. how agents interact with one another. In addressing the social dimension, methods such as Social Pooling [3] and Graph Neural Networks [4] (GNNs) have been explored, with GNN emerging as the most effective [5]. Some researchers have integrated both transformers and GNN, either sequentially or in parallel, to refine the prediction paradigm [6, 7, 8, 9]. However, these approaches entail heightened computational burdens due to the resource-intensive nature of both GNNs and transformers. Furthermore, transformers, by their inherent design, may pose potential challenges in preserving information, as they do not inherently accommodate the graph structure in their input. On the other hand, scene dimension, or scene embedding, delves into the interaction between an agent and its surroundings. [10] and [11] utilised semantic segmentation maps which enhanced the model's grasp of environmental context. Another aspect across all surveyed papers, however, is their tendency to focus exclusively on either BEV or EVV, rarely considering both like [12]. This narrow focus becomes particularly problematic in, e.g., unstructured environments where a BEV might not be available, limiting the applicability of these methods.\nIn light of these challenges, this paper introduces a lightweight model, coined ASTRA (A Scene-aware TRAnsformer-based model for trajectory prediction). By integrating a U-Net-based key-point extractor [13], ASTRA captures essential scene features without relying on explicit segmentation map annotations and alleviates the data requirements and preprocessing efforts highlighted earlier. This method also synergises the strengths of GNNs in representing the social dimension of the problem and of transformers in encoding its temporal dimension. Crucially, our approach processes spatial, temporal, and social dimensions concurrently, by embedding the graph structure into the token's sequence prior to the attention mechanism, rendering the transformer graph-aware. The model does so while keeping the complexity of the model minimal. To refine the model's ability to accurately learn trajectories, we implemented a modified version of the trajectory"}, {"title": "2 Related Work", "content": "The trajectory prediction problem is usually approached in two ways: stochastic (multi-modal) predictions [18, 14, 19, 12] and deterministic (uni-modal) predictions [20, 21, 22]. The model produces only one prediction (most probable) per input motion in a deterministic setting. In contrast, a stochastic setting involves the model generating multiple predictions for each input motion. Stochastic approaches utilize generative techniques like Conditional Variational Auto-Encoders (CVAEs) [14, 12], Generative Adversarial Networks (GANs) [23], Normalizing Flows [24], or Denoising Diffusion Probabilistic Models [18] to introduce randomness into the prediction process, thereby generating diverse future trajectories with varying qualities for each pedestrian, aiming to elucidate the distribution of potential future coordinates of pedestrian trajectories. ASTRA also offers both deterministic and stochastic predictions like some of the previous works [5, 12, 25].\nThe social dimension focuses on capturing agent-agent interactions, emphasizing how individuals or objects influence each other's movements within a shared space. Notably, some methodologies incorporate social pooling, concurrently with attention mechanisms [3]. Algorithms in this domain predominantly leverage various forms of Graph Neural Networks (GNNs) to encapsulate the social dynamics among agents. Some methods employ a fully connected undirected graph, encompassing all scene agents [5, 26, 27]. This approach, albeit comprehensive, escalates exponentially with the number of agents (nodes). Conversely, other methods opt for sparsely connected graphs, establishing connections solely among agents within a proximal range, thereby reducing the linkage count substantially [28, 29, 25, 30]. In a similar vein, [14, 25] proposes sparse, directional graphs, predicated on the premise that different agent types possess varying perceptual ranges. Regarding the optimal depth of GNN layers, [31] advocate for deeper graphs to enhance performance. This stands in contrast to the findings of [30] and [32], who posit that two layers are optimal. Nevertheless, this depth increases computational demands, particularly when agent nodes are numerous, posing challenges for autonomous vehicle applications reliant on edge devices for processing.\nThe scene dimension, extracted from the video frames, includes the low-level representation of the physical environment, obstacles, and any elements that could affect the agent's path, ensuring a comprehensive understanding of both social and environmental factors in predicting movement trajectories. [10] and [11] capture scene dimension with the help of semantic segmentation to delineate visual attributes of varied classes, subsequently elucidating their interrelations via attention. However, obtaining a panoptic segmentation mask, might not be always feasible. Also, this approach introduces a considerable dependency on"}, {"title": "2.1 Trajectory Prediction", "content": null}, {"title": "2.2 Social and Scene Dimension", "content": null}, {"title": "2.3 Temporal Dimension", "content": "Understanding the trajectory history of an agent significantly augments the predictive accuracy regarding its potential future path. Predominantly, ego-camera-based models are tailored to shorter temporal horizons and employ 3D Convolutional Neural Networks (3D-CNNs) [28, 33]. Some research, instead, adopts Hidden Markov Models (HMMs) for temporal analysis [34]. For scenarios necessitating extended time horizon considerations, more intricate structures are proposed, including Transformers [14, 5, 35, 36] and various forms of Recurrent Neural Networks (RNNs) [29], including Long Short-Term Memory networks (LSTMs) [2, 37, 28] and Gated Recurrent Units (GRUs) [26]. Both Transformer and RNN-based models have exhibited superior performance, often achieving state-of-the-art results in this domain. However, some of these models tend to address the temporal dimension in isolation from the social context. This segregated approach potentially results in information loss and contributes to an increased computational load, necessitated by the addition of separate components to handle the social dimension. Consequently, there emerges a pressing demand for integrated models capable of concurrently processing both temporal and social dimensions. A promising direction in this regard is the development of graph-aware transformers, which encapsulate the essence of both temporal dynamics and social interactions within a unified framework."}, {"title": "2.4 Graph-aware Transformers", "content": "Graph-aware transformers aim to compound the benefits of graphs (with their associated social embeddings) and of transformers, with their acclaimed attention mechanism and temporal embeddings. Notably, these advancements have predominantly catered to graph-centric datasets like ACTOR [38] and CHAMELEON SQUIRREL [39]. Direct application of graph-aware transformers remains untouched in pedestrian trajectory forecasting, with prevalent methodologies leaning towards transformers processing embeddings emanating from graphs [6, 7, 8]. There has been a discernible preference for using GNN and transformer blocks, rather than fully-integrated graph-aware transformers.\nA comprehensive evaluation of numerous contemporary graph-transformer models across three graph-centric datasets is conducted in [40]. The analysis reveals a consistent pattern: models employing Random Walk for structural encoding exhibit superior performance across all tested datasets. Building on this empirical evidence, our approach utilizes Random Walk to encode the pedestrian graph, which is then seamlessly integrated into the transformer architecture. This integration is designed to yield a graph-aware transformer, thereby enhancing the model's capability to effectively capture and interpret complex pedestrian dynamics within various environments. To the best of the authors' knowledge, this is the first work towards utilising a graph-aware transformer to solve the trajectory prediction problem, opposing many methods which use graphs along with transformers."}, {"title": "3 Methodology", "content": "Problem Formulation. The objective of pedestrian trajectory prediction is to forecast the future position of a pedestrian based on the observed historical sequence of the pedestrian's positions. For this, the historical sequence of the pedestrians is provided as a sequence of coordinates X = {X | t \u2208 (1,2,..., Tobs) ; a \u2208 (1,2,...,A)} related to A target agents extracted over the previous Tobs time instants. Here X is a pair of 2D coordinates {x,y} for BEV datasets (see Figure 2a), and a set of bounding box coordinates, {x1,t, Yi,t, x2,t, Y2,t}, for EVV datasets (see Figure 2c). In addition to the sequence coordinates X, Tobs input frames/images are also available, denoted as I = {It | t \u2208 (1, 2, . . ., Tobs)}. The goal of ASTRA is to output deterministic or multi-modal trajectories of the pedestrian. In the deterministic setting, the problem consists of predicting the output prediction coordinates of the A agents in the subsequent Tpred future frames, formally \u0176 = {\u0176\u00ba | t \u2208 (1, 2, . . ., Tpred); a \u2208 (1, 2, ..., A)} where \u0176a denotes the coordinates of the agent a at a future time t, and the corresponding ground truth being Y = {Y\u00ba | t \u2208 (1, 2, ...,Tpred); a \u2208 (1, 2, . . ., A)}. To output multimodal trajectories we need to learn a generative model, denoted by pe (Y|X, I) which is parameterized by 0 and given X and I, outputs K predicted trajectories denoted by y = {\u0176(1), \u0177 (2), ...,\u0176(K)}."}, {"title": "3.1 Components", "content": "The encoder part of our model consists of two main components: A scene-aware component and an agent-aware component. While the former is dedicated to encoding the scene, and encapsulating the contextual details, the latter focuses on encoding the spatial, temporal, and social dimensions of the agents, as shown in Figure 3. The output from these two components is aggregated before being decoded to generate single or multiple predicted future trajectories for deterministic/multi-modal predictions, respectively. In order to learn essential information about the scene's spatial layout and the positional dynamics of agents within it, the U-Net [41] is pre-trained using the method detailed in [13] which utilizes a specialized loss function, Weighted Hausdorff Distance to learn a latent representation of the scene context (Figure 4). More sophisticated schemes to generate the scene representation, like transformer-based architectures, and fusing social representations via gated cross-attention can also be considered but we leave exploring possibly more effective and sophisticated architecture designs as future work. The Grad-CAM visualizations (Figures 2b and 2d) highlight this capability, showing that the pre-trained model pays attention to regions with a high likelihood of pedestrian activity. The U-Net-based keypoint extractor is frozen after the pretraining when used in ASTRA model architecture."}, {"title": "3.1.1 Temporal Encodings", "content": "To guide the network to model the temporal dimension, we add temporal encodings to the agents and scene to capture the temporal dependencies within the sequence of pedestrian trajectories from past frames, adopting a time encoder akin to the positional encoding found in the original Transformer architecture [42].\n\nTemporal (t, i) =\n\n{\nsin (\\frac{t}{10000^{2i/d}})\ncos (\\frac{t}{10000^{2i/d}})\nif i is even\nif i is odd\n\n(1)\nwhere t is the time step, i is the dimension, and d is the dimensionality of the model."}, {"title": "3.1.2 Scene-aware embeddings.", "content": "A latent representation of pedestrian characteristics is obtained using a pre-trained U-Net encoder (Figure 3); this latent vector can include some crucial characteristics like spatial groupings and interactions with the environment. This step is crucial as the U-Net extractor possesses the proficiency to discern both labelled and unlabelled pedestrians, depicted in green and red, respectively in Figure 5a."}, {"title": "3.1.3 Scene-aware transformer encoder.", "content": "The latent representation of all frames (scene) is treated as input tokens to the scene-aware single-layer transformer encoder (Tscene-aware), which in turn generates scene-aware embeddings (Scene) for each frame. The single-layer transformer encoder architectural choice significantly contributes to the lightweight nature of our model. The resulting scene embedding is:\n\nScene = \u0393Scene (YEncoder (I))\n\nScene = TScene-aware ([\u03a8Scene; \u0424Temporal])\n\n(2)\n(3)\nwhere past input frame images projected using a Multi-Layer Perceptron (MLP) layer (\u0393Scene), and YEncoder(.) denotes the encoder part of the U-Net.\nThe temporal encoding Temporal, crucial for capturing the temporal dynamics within the observed frames, adopts the design of the traditional positional encoding [42] and follows the work of [14]."}, {"title": "3.1.4 Agent-aware embeddings.", "content": "The second component is dedicated to encoding the different dimensions of each agent for all agents in the scene.\nThe spatial coordinates X of each agent are linearly projected to a latent space using an MLP layer (\u0393Spatial) to get spatial encoding (Spatial)\n\nSpatial =\u300cSpatial (X)\n\n(4)\nSpatial embeddings are not enough to capture the full information about the agents. If two agents in two frames have the same location in the image, their spatial embedding will be the same. Hence, temporal encoding (equation in Supplementary material) is also included to distinguish them.\nHaving the spatial and temporal dimensions of the agents is still not enough to understand their interaction in the scene. To capture the social dimension in this multi-agent environment, we generate a fully connected undirected graph between agents, in which the nodes are the agents' locations, and the edges between agents are the reciprocal of the distance. Consequently, the closer the agents are to each other, the stronger the link between them. Formally, we represent the social dimension using a graph G = (V, E), where V is the set of agents and E is the collection of edges, with weights\n\nCij =\n\\frac{1}{d(vi, vj)}\n\n(5)\nwhere d(vi, vj) is the distance between agents vi and vj.\nSubsequently, Random Walk Positional Encodings (RWPEs) [43] is used to capture the structural relationships between nodes in the graph, such as their proximity to each other and the number of paths between them. These RWPEs are further projected to latent space using a separate MLP (\u0393Social) to get social encodings (social), mathematically:\n\nSocial = \u0393Social (RWPE(G))\n\n(6)"}, {"title": "3.1.5 Agent-aware transformer encoder.", "content": "After calculating the spatial, temporal and social representations for each agent, our model concatenates them. This concatenated vector is then fed into an agent-aware single-layer transformer encoder (TAgent-aware) that generates agent-aware embedding (Agents).\n\n\u03a6Agents TAgent-aware ([Spatial; Temporal; Social])\n\n(7)"}, {"title": "3.1.6 Decoder.", "content": "To generate multiple stochastic trajectories, we learn a generative model, po (Y|X, I) for which we adopted CVAE(Conditional Variational Auto Encoder). We train CVAE to learn the inherent distribution of future target trajectories conditioned on observed past trajectories, by utilizing a latent variable Z. CVAE consists of three components - prior network (pe(Z|X, I)), recognition network (q\u2084(Z|Y, X, I)) and generation network(g, (Y|Z)), parameterized by \u03b8, \u03c6 and v respectively. Here Y is the output of the generation network and is the latent representation of the future trajectories. To generate future trajectories, we pass Y to the MLP decoder (\u0393Decoder).\nFor deterministic predictions, CVAE is skipped and the outputs of both the scene transformer (Scene) and the agents' transformer (\u03a6Agents) are concatenated and directly passed through an MLP decoder (FDecoder) to produce future trajectories (Y) of the agents in the future frames as shown in Figure 3, namely:\n\n\u300cDecoder ([Scene; Agents])\n\n(8)"}, {"title": "3.1.7 Weighted trajectory loss function.", "content": "We introduce a weighted-penalty strategy that can be applied to common loss functions used in trajectory prediction such as MSE and Smooth L1 Loss. The application of this strategy is through a dynamic penalty function w(t), designed to escalate or de-escalate the significance of prediction errors as one moves further into the future. The definition of the weighted loss function is given by:\n\nLweighted (Y, Y) = \u03a3 \u2211w(t). L(Y, Y\u2081),\n\nt=1\n\n(9)\nwhere \u0176 and Y are the predicted and actual trajectories respectively, Tpred denotes the number of prediction timesteps, w(t) represents the dynamic weighting function at time t, and L(\u0176t, Yt) is the predefined loss function (e.g., MSE or SmoothL1 Loss) applied to the predicted and true positions at each time step t.\nIn time series data, as we move further into the future relative to the last observed data, the drift in predictions tends to increase, leading to higher errors. Motivated by this intuition, we initially penalized the predictions using linear and quadratic loss functions. These approaches showed improvements in overall prediction accuracy. However, upon closer analysis of the results, we observed that in some trajectories, there was a noticeable offset in the earlier parts of the predictions. To address this issue, we experimented with a parabolic weighting function for the penalty. Empirically, this approach outperformed the linear and quadratic strategies, yielding the most balanced and accurate predictions across the trajectories.\nThe weight function w(t) is designed to be versatile, accommodating a spectrum of mathematical formulations that align with the specific needs of the predictive model. It is generically defined as:\n\nw(t) = f(t, Tpred, \u03b1, \u03b2),\n\n(10)\nwhere a and \u1e9e are parameters that establish the bounds of the weighting function, and f is an adaptable function that governs the progression of weights at each timestep t.\nIn particular, the function w(t) may be selected from various mathematical forms, such as linear, parabolic, or quadratic, which are discussed in greater detail within the supplementary materials. The choice of function enables the model to adjust the penalty progression in alignment with the anticipated prediction challenge at each timestep."}, {"title": "4 Experiments", "content": "For a comprehensive evaluation, we benchmarked our model on three trajectory prediction datasets; namely, ETH [15], UCY [16], and PIE dataset [17]. ETH and UCY offer a bird's-eye view of pedestrian dynamics in urban settings, including five datasets with 1,536 pedestrians across four scenes. For evaluation, we used their standard protocol; leave-one-out strategy, observing eight time steps (3.2s) and predicting the following 12 steps (4.8s).\nIn contrast, the PIE dataset provides an Ego-vehicle perspective, containing over 6 hours of ego-centric driving footage, along with bounding box annotations for traffic objects, action labels for pedestrians, and ego-vehicle sensor information [17]. A total of 1,842 pedestrian samples are considered with the following split: Training(50%), Validation (40%) and Testing(10%) [17]. Model performance is evaluated based on a shorter observational window of 0.5 seconds and a prediction window of 1 second, providing insights into the model's capability in rapidly evolving traffic scenarios[2]."}, {"title": "4.1 Datasets and Evaluation Protocol", "content": null}, {"title": "4.2 Evaluation Metrics", "content": "We used the standard evaluation metrics of ADE and FDE for ETH-UCY deterministic settings and minADE and minFDE for their stochastic settings. ADE, FDE, CADE, CFDE, ARB and FRB for PIE dataset, the supplementary material explains these metrics."}, {"title": "4.3 Setting up the experiments", "content": "We trained the model on the ETH-UCY and PIE datasets using the AdamW optimizer with a weight decay of 5 \u00d7 10-4 for 200 epochs. The initial learning rate was set to 1 \u00d7 10-3, and a cosine annealing scheduler was employed. Training was conducted on a NVIDIA DGX A100 machine, equipped with 8 GPUs, each having 80 GB of memory."}, {"title": "4.4 FLOPS", "content": "For Agent Former, the model has approximately 3.084 GFLOPs, whereas ASTRA is comparatively lighter with 1.7 MFLOPs (including U-Net and CVAE), highlighting ASTRA's computational efficiency. With pretrained U-net the model has 839 KFLOPs and without including the CVAE, model has 16 KFLOPs."}, {"title": "4.5 Discussing the Results", "content": null}, {"title": "4.5.1 Quantitative Results.", "content": "For ETH-UCY, we compared our model results against several baselines. These comparisons are presented in Table 1 and Table 2, which contains results primarily sourced from the EqMotion (CVPR 2023) [5] for deterministic predictions and LeapFrog (CVPR 2023) [18] for stochastic predictions respectively. It is important to note that to provide a thorough comparative framework, we independently computed and included additional models [46, 12] to their respective tables as they were not originally part of the EqMotion or LeapFrog analysis. Our model significantly advances the state-of-the-art on ETH-UCY, outperforming"}, {"title": "4.5.2 Qualitative results.", "content": "We can clearly see the results of our prediction from Figure 6 for deterministic predictions and Figure 8 for stochastic prediction. Figure 7 exemplifies the proximity of our model's results to the ground truth, it also shows how using the weighted penalty strategy has yielded better results than the unpenalised one, highlighting the improved effectiveness of our strategy."}, {"title": "4.5.3 Ablation studies.", "content": "In the ablation study presented in Table 3, we evaluated the contribution of each component in our trajectory prediction model to ascertain their individual and collective impact on the performance metrics on the ETH-UCY (UNIV) dataset. Initially, the model incorporated only spatial information, which served as a baseline for subsequent enhancements. The sequential integration of temporal and social components yielded successive improvements, demonstrating their respective significance in capturing the dynamics of agent movement. The addition of the data augmentation technique (detailed in the supplementary material) further refined the model's performance, illustrating the value of varied training samples in enhancing generalization capabilities. Moreover, the incorporation of U-Net features contributed to a substantial leap"}, {"title": "5 Conclusion", "content": "We presented ASTRA, a model in the domain of pedestrian trajectory prediction, that outperforms the existing state-of-the-art models. This advancement renders ASTRA particularly suitable for deployment on devices with limited processing capabilities, thereby broadening the applicability of high-accuracy trajectory prediction technologies. ASTRA's adeptness in handling both BEV and EVV modalities further solidifies its applicability in diverse operational contexts. With the ability to produce deterministic and stochastic results, it enhances the predictive robustness and situational awareness of autonomous systems. Moving forward, we aim to extend the capabilities of the ASTRA model beyond pedestrian trajectory prediction to encompass a broader range of non-human agents. This expansion will involve adapting the model to understand and predict the movements of various entities within shared environments using more sophisticated architectural design choices to encode the scene and its fusion with social dimension. By broadening our focus, we hope to contribute to the development of truly comprehensive and adaptive systems capable of navigating the complexities of real-world interactions among a wide array of agents."}, {"title": "A Conditional Variational Auto-Encoder Preliminaries", "content": "ASTRA employs a Conditional Variational Autoencoder (CVAE) [56] framework to address the inherent stochasticity of the prediction task, enabling the generation of K distinct trajectories for each agent under consideration. Throughout the training phase, it aims to approximate the latent distribution Z by deducing its mean and variance by utilising Multilayer Perceptrons (MLPs). Subsequently, by employing the divergence loss function (second term in Equation 14), the model pushes the learned distribution pe (zp|x), parameterized by 0, to be as close as possible to the ground truth distribution qo(zq|x, y), parameterized by \u0424. We use the reparameterization trick to present zp and zq through the mean and variance pairs of (\u03bc\u03b1\u03c1\u03b9\u03c3zp) and (\u00b5zq,\u03c3z\u0105), respectively. After training, K samples are drawn from the Z distribution and decoded to form the final trajectories."}, {"title": "B Loss Function Formulation", "content": "The loss for multi-modal trajectories is given in equations equation 13 and equation 14.\nLweighted (Uk, Y) is calculated similar to Equation 15.\n\nLweighted (Y, Y) = min Lweighted (Yk, Y)\nk=1,...,K\n\nLfinal = Lweighted(\u0176, Y) + DKL(N(\u03bczq,ozq) || N(\u03bczp, ozp))\n\nFor deterministic predictions, the final loss is the same as the weighted loss:\n\nTpred\nLfinal = Lweighted (Y,Y) = \u2211 w(t). L(\u0176t, Yt),\n\nt=1\n\n(13)\n\n(14)\n\n(15)\nwhere w(t) represent the weighted penalty function (section C) and L(\u0176t, Yt) is the predefined loss function: MSE or Smooth L1 loss (discussed below).\nMean square error (MSE)\n\nMSE =\n\\frac{1}{N}\n\u2211(Yi - Yi)2\n\ni=1\n\n(16)\nwhere yi and \u0177i represent, the actual and predicted coordinates, respectively. MSE penalises larger trajectory prediction errors more heavily, ensuring model accuracy in critical scenarios.\nSmooth L1 loss (SL1)\n\nSL1(Yi, Yi) =\n\n{\n0.5 x (Yi-Yi)2 if Yi - Yi\n||Yi - Yi - 0.5\n< 1\notherwise.\n\n(17)\nUnlike MSE, SL1 effectively balances the treatment of small and large errors. This loss is also less sensitive to outliers, due to its combination of L1 and L2 loss properties."}, {"title": "C Weighted Penalty Functions", "content": "In trajectory prediction, particularly for dynamic entities like pedestrians, vehicles, or other agents, the accuracy of predictions is paramount. The inherent challenge lies in managing the variability and uncertainty that escalates with longer prediction horizons. To address this, we implement different penalization strategies that adjust the model's emphasis to enhance reliability over extended prediction horizons. Our ablation focuses on three distinct penalty strategies: Linear, Quadratic, and Parabolic. Table 6 presents a quantitative analysis comparing the three penalty strategies as applied to the ETH-UCY (UNIV) dataset using SL1 loss. It can be observed that the Parabolic penalty gives better results compared to other penalization strategies. Figure 9 compares the three weighted penalty strategies for a prediction window of 12 frames. Subsequent sections provide a detailed explanation for each of the penalty strategies."}, {"title": "C.1 Linear Weighted Penalty", "content": "The Linear Weighted Penalty employs a weight function, w(t), that linearly increases from a start weight (a) to an end weight (\u03b2), over the prediction period. This approach aims to progressively increase the penalty for prediction inaccuracies, particularly toward the latter part of the prediction horizon.\nThe weight function w(t) is defined as:\n\nw(t) = a +\n\\frac{t}{Tpred}\n(\u03b2-\u03b1),\n\n(18)\nwhere a and \u1e9e are the weights assigned to the initial and final predicted time steps, respectively."}, {"title": "C.2 Quadratic Weighted Penalty", "content": "The quadratic weighted penalty strategy intensifies the penalty in a quadratic manner as the difference between the prediction time and the past frames increases. This approach is more aggressive than the linear strategy, applying an exponentially increasing weight to errors in later prediction frames. The weight function w(t) in this case is defined as:\n\nw(t) = (a + (\\frac{t}{Tpred}) (\u03b2-\u03b1))2\n\n(19)"}, {"title": "C.3 Parabolic Weighted Penalty", "content": "The Parabolic Weighted Penalty assigns the maximum weight, a, to both the initial and final predicted time steps, highlighting their significance. Meanwhile, the minimum weight, \u03b2 (\u03b2 <a), is allocated to the midpoint of the prediction interval. This distribution forms a parabolic trajectory (shown in Figure 9) of weights across the prediction period, as defined by:\n\nw(t) = (\u03b1 \u2013 \u03b2)\u00b7\n(\\frac{t}{Tpred}\n\u2013\n\\frac{1}{2}\n)2\n+ \u03b2,\n\n(20)"}, {"title": "C.4 Augmentation", "content": "To enhance the robustness and generalization of our trajectory prediction model, we implement a data augmentation strategy. This strategy randomly applies rotation and translation transformations to the trajectory sequences [57]. By applying random rotations and translations, our model is trained to be orientation-agnostic and adept at handling positional shifts in agents. These augmentations effectively increase the diversity of the training data, enabling the model to learn more generalized representations of agent movements. This, in turn, enhances the model's predictive accuracy and robustness, particularly in complex and unpredictable scenarios where pedestrian trajectories can vary significantly due to factors like group dynamics, obstacles, and varying crowd densities."}, {"title": "D Evaluation Metrics", "content": "ETH-UCY. To evaluate our model on ETH-UCY, we used commonly employed evaluation metrics [5, 58, 7, 59]: ADE/FDE and minADEK/minFDEK. Average Displacement Error (ADE) computes the average Euclidean distance between the predicted trajectory and the true trajectory across all prediction time steps for each agent. minADEK refers to the minimum ADE out of K randomly generated trajectories and ground truth future trajectories. We also used the Final Displacement Error (FDE), which focuses on the prediction accuracy at the final time step. It computes the Euclidean distance between the predicted and actual positions of each agent at the last prediction time step. minFDEK refers to the minimum FDE out of K randomly generated trajectories and ground truth future trajectories. For multimodal trajectory prediction, minADEK and minFDEK metrics are used for evaluation.\n\nADE =\n\\frac{1}{Tpred}\n\u03a3\nTpred t=1\n\u03a3||Yi - Yi||2,\n\nminADEK = min (\n\\frac{1}{Tpred}\n\u03a3||Ypred,k-\nYtruth,k||2\n),\n\n(21)\n\n(22)\n\nFDE = ||Yipred,k-\nYfpread,k||2\n\nminFDEK = min (||Ypred,k-\nYpred,k||2)\n\n(23)\n\n(24)\nPIE. For the PIE Dataset, the ADE and FDE metrics are calculated based on the centroid of the bounding box [17, 2], denoted as Centre average displacement error for the bounding box (CADE) and Centre final displacement error for the bounding box (CFDE). In addition, we reported the average and final Root Mean Square Error (RMSE) of bounding box coordinates, denoted as ARB and FRB, respectively [10]."}, {"title": "E Grad-CAM visualizations", "content": "Grad-CAM images were obtained by generating heatmaps overlaid onto the original image to aid in validating the relevance of highlighted regions. To obtain the Grad-CAM visualization, a single-channel output segmentation map was obtained from the pre-trained U-Net network, representing the probability of each pixel location being a keypoint [13]. Probabilities were aggregated across all pixels, by comparing them with true keypoints and gradients of activation for the initial layer were extracted, similar to the approach taken by [60]. Utilizing these gradients, a weighted average of the activation maps of the initial layer was computed to reconstruct the heatmap, similar to the method described in [61], for the Grad-CAM visualization. Overlaying this heatmap onto the original image highlights the regions that contribute significantly to the keypoint predictions made by the model.\nTo ease reading the paper, Table 7 and 8 list the abbreviations and the mathematical symbols mentioned in the paper, respectively."}]}