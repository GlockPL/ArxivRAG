{"title": "Functional Acceleration for Policy Mirror Descent", "authors": ["Veronica Chelu", "Doina Precup"], "abstract": "We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-\nbased PMD update. By taking the functional route, our approach is independent\nof the policy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a special\ncase. We theoretically analyze several properties of this approach and complement\nwith a numerical ablation study, which serves to illustrate the policy optimization\ndynamics on the value polytope, relative to different algorithmic design choices\nin this space. We further characterize numerically several features of the problem\nsetting relevant for functional acceleration, and lastly, we investigate the impact of\napproximation on their learning mechanics.", "sections": [{"title": "1 Introduction", "content": "The RL framework (Sutton and Barto, 2018) refers to the problem of solving sequential decision\nmaking tasks under uncertainty, together with a class of solution methods tailored for it. The RL\nproblem has found applications in games (Tesauro, 1994; Mnih et al., 2013; Silver et al., 2014; Mnih\net al., 2016; Silver et al., 2017; Hessel et al., 2017; Bellemare et al., 2017; Schrittwieser et al., 2019;\nZahavy et al., 2023), robotic manipulation (Schulman et al., 2015, 2017; Haarnoja et al., 2018),\nmedicine (Jumper et al., 2021; Schaefer et al., 2004; Nie et al., 2020) and is formally described by\nmeans of discounted Markov Decision Processes (MDPs) (Puterman, 1994). On the solution side,\nincreased interest has been devoted to the study of policy-gradient (PG) methods based on optimizing\na parameterised policy with respect to an objective (Williams, 1992; Konda and Borkar, 1999; Sutton\net al., 1999; Agarwal et al., 2019; Bhandari and Russo, 2019; Kakade, 2001; Bhandari and Russo,\n2021; Mei et al., 2020b,a).\nPolicy Mirror Descent (PMD) (Agarwal et al., 2019; Bhandari and Russo, 2021; Xiao, 2022; Johnson\net al., 2023; Vaswani et al., 2021) is a general family of algorithms, specified by the choice of mirror\nmap covering a wide range of novel and fundamental methods in RL. PMD is a proximal algorithm\n(Parikh et al., 2014) and an instance of Mirror Descent (MD) (Beck and Teboulle, 2003) on the\npolicy simplex (Bhandari and Russo, 2021). MD is an iterative optimization algorithm that extends\ngradient descent (GD) by adapting to the geometry of the problem using different distance functions,\nparticularly Bregman divergences (Amari, 1998; Bubeck, 2015; Banerjee et al., 2005). PMD applies\na proximal regularization, with a Bregman divergence, to the improvement step of Policy Iteration\n(PI), and converges to it as regularization decreases. In the discounted setting, with an adaptive\nstep-size, PMD converges linearly at the optimal rate, determined by the discount factor, independent\nof the dimension of the state space or problem instance (Johnson et al., 2023), recovering classical\napproaches, like PI and VI, as special cases. PMD has been extended to linear approximation by\nYuan et al. (2023) and to general function approximation by Alfano et al. (2024)."}, {"title": "Motivation", "content": "The running time of PMD algorithms scales with the number of iterations. With a\nparametrized policy class, each iteration of an approximate PMD method generally requires multiple\n\"inner-loop\" updates to the policy parameter (Tomar et al., 2020; Vaswani et al., 2021). Actor-critic\n(AC) methods (Sutton et al., 1999; Konda and Borkar, 1999) additionally require the computation or\nupdating of an inexact critic at each iteration. It is therefore desirable to design algorithms which\nconverge in a smaller number of iterations, resulting in significant empirical speedups, as has been\npreviously argued by Johnson et al. (2023); Xiao (2022); Goyal and Grand-Clement (2021); Russo\n(2022).\nIn this work, we leverage duality and acceleration to build a novel surrogate objective for momentum-\nbased PMD, leading to faster learning in terms of less iterations necessary to converge. The novelty\nof our approach is the application of acceleration mechanics to the direct or functional policy\nrepresentation \u03c0\u2014hence named functional acceleration, as opposed to classic acceleration applied\nto the policy parameter \u03b8 (e.g., Mnih et al. (2016); Hessel et al. (2017); Schulman et al. (2017) use\nAdam (Kingma and Ba, 2015) or RMSProp (Hinton et al., 2012)). Specifically, we use momentum\nin the dual policy space to accelerate on \"long ravines\" or decelerate at \u201csharp curvatures\" at the\nfunctional level of the policy optimization objective. Intuitively, adding momentum to the functional\nPG (the gradient of the policy performance objective with respect to the direct policy representation\n\u03c0) means applying, to the current directional policy derivative, a weighted version of the previous\npolicy ascent direction, encouraging the method to adaptively accelerate according to the geometry\nof the optimization problem."}, {"title": "Contributions", "content": "We illustrate and analyze theoretically the impact of applying functional acceleration on the\noptimization dynamics of PMD, leading to a practical momentum-based PMD algorithm.\nWe characterize the properties of the problem setting, and those intrinsic to the algorithm,\nfor which applying functional acceleration is conducive to faster learning.\nWe study the influence of an inexact critic on the acceleration mechanism proposed."}, {"title": "Outline", "content": "This document is organized as follows. After placing our work in existing literature in\nSec. 2, and setting up the context in which it operates in Sec. 3, we introduce our main ideas in Sec. 4.\nWe complement with numerical studies in Sec. 5, ending with a short closing in Sec.6."}, {"title": "2 Related Work", "content": "Accelerated optimization methods have been at the heart of convex optimization research, e.g.,\nNesterov's accelerated gradients (NAG) (Nesterov, 1983; Wang and Abernethy, 2018; Wang et al.,\n2021), extra-gradient (EG) methods (Korpelevich, 1976), mirror-prox (Nemirovski, 2004; Juditsky\net al., 2011), optimistic MD (Rakhlin and Sridharan, 2013; Joulani et al., 2020b), AO-FTRL (Rakhlin\nand Sridharan, 2014; Mohri and Yang, 2015), Forward-Backward-Forward (FBF) method (Tseng,\n1991).\nAs far as we know, our idea of applying acceleration to the direct (functional) policy representation\n\u03c0\u03b8-independent of the policy parametrization \u2014is novel. This is important because it means\nuniversality of the approach to any kind of parameterization and functional form a practitioner\nrequires. Within the context of RL, acceleration has only been applied to value learning (Vieillard\net al., 2019; Farahmand and Ghavamzadeh, 2021; Goyal and Grand-Clement, 2021), or in the context\nof PG methods, classic acceleration is applied to the policy parameter \u03b8\u2014all recent deep RL works\n(e.g. Mnih et al. (2016); Hessel et al. (2017); Schulman et al. (2017)) use some form of adaptive\ngradient method, like Adam (Kingma and Ba, 2015) or RMSProp (Hinton et al., 2012). The idea of\nacceleration generally relies on convexity of the objective relative to the representation of interest.\nThe transformation from parameters @ to functional representation of the policy as probabilities \u03c0\u00ba,\ncan be highly complex, non-linear, and problem-dependent. Proximal algorithms operate on this\nfunctional representation, and rely on relative-convexity and relative-smoothness (Lu et al., 2017) of\nthe objective with respect to \u03c0 when constructing surrogate models (Bhandari and Russo, 2019, 2021;\nAgarwal et al., 2019; Vaswani et al., 2021). These properties suggests the functional acceleration\nmechanism is feasible and promising in our setting, since it is able to successfully accelerate convex\noptimization (Joulani et al., 2020b)."}, {"title": "Approximate PMD", "content": "PMD has been extended to function approximation by Tomar et al. (2020), and\nlater analyzed by Vaswani et al. (2021), who treat the surrogate objective as a nonlinear optimization\nproblem, that of approximately minimizing at each iteration, a composite proximal objective, denoted\nl(\u03c0\u03b8) with respect to the policy parameter \u03b8. In contrast, Alfano et al. (2024) applies the PMD update\nin the dual form, as (generalized) Projected Gradient Descent (PGD), i.e. a gradient update in the\ndual space followed by a projection (Bubeck, 2015), rather than in proximal form (Beck and Teboulle,\n2003), as used by Tomar et al. (2020). Vaswani et al. (2023) further relies on a dual policy norm,\ninduced by the chosen mirror map of an approximate PMD update, to measure the critic's evaluation\nerror in a decision-aware actor-critic algorithm, whereas Alfano et al. (2024) relies on the L2-norm\non the dual representation of the policy to measure its function approximation error. In a similar vein\nwe leverage critic differences in dual space to gain momentum and accelerate the policy optimization."}, {"title": "Limitations & Future Work", "content": "Our focus is on developing a foundation that motivates further study.\nA translation to practical large-scale implementations and deep RL remains for further investigation,\ni.e. with non-standard proximal methods, e.g., TRPO (Schulman et al., 2015), PPO (Schulman et al.,\n2017), MDPO (Tomar et al., 2020), MPO (Abdolmaleki et al., 2018). Additional guarantees of\naccelerated convergence for general policy parametrizations using the dual policy norm, as well as\ntheoretical analysis for the stochastic setting, are also deferred for future work."}, {"title": "3 Background & Preliminaries", "content": "RL We consider a standard RL setting described by means of a Markov decision process (MDP)\n(S, A, r, P, \u03b3, \u03c1), with state space S, action space A, discount factor \u03b3 \u2208 [0, 1), initial state distribution\n\u03c1\u03b5\u0394(S) (\u0394(\u03a7)\u2014the probability simplex over a set X), rewards are sampled from a reward function\nR~r(S, A), r : S \u00d7 A \u2192 [0, Rmax], and next states from a transition probability distribution\nS' ~ P(.\\S, A) \u2208 \u0394(8). The RL problem (Sutton and Barto, 2018) consists in finding a policy\n\u03c0: 8 \u2192 \u0394\u0391 \u0395 \u03a0 = = \u0394\u0399 \u2206, maximizing the performance objective defined as the discounted\nexpected cumulative reward V = Es~V \u2208 R, where V\" \u2208 R|S| and Q\" \u2208 R|S|\u00d7|A| are the\nvalue and action-value functions of a policy \u03c0, such that V = E[=0 Y\u00b2 Ri+1|So = s], Q\",a =\n\u0395\u03c0 [\u03a3\u03c4\u03bf Rio = s, Ao = a] and V\u2122 = Ea\u223c\u03c0 [Q(s, a)]. There exists an optimal deterministic\npolicy \u03c0* that simultaneously maximises V\u2122 and Q\" (Bellman, 1957). Let d\u2122 be the discounted\nvisitation distribution d = (1\u2212y) \u03a3\u03c4\u03bf \u03b3\u00b2 Pr(Si = s|So ~ p, Aj ~ \u03c0sj,\u2200j \u2264 i).\nWe use the shorthand notation (,)\u2014the dot product, \u2207 f(x) = \u25bcxf(x)-gradients and partial\nderivatives, \u2207 f(x, y) = \u221axf(x,y), \u03c0\u207a = \u03c0\u00bat, Qt = Q\u2122t, Vt = V\u00abt, dt = d*, \u03c0\u03c2 = \u03c0(\u00b7|s),\nQs = Q(s), V = V(s), rs,a = r(s, a), \u03c0\u03b1\\s = n(a|s), Qs,a = Q(s, a).\nPG Algorithms update the parameters \u03b8 \u2208 \u0472 of a parametric policy \u03c0\u00ba using surrogate objectives that\nare local approximations of the original performance. In the tabular setting, the direct parameterisation\nassociates a parameter to each state-action pair, allowing the shorthand notation \u03c0 = \u03c0\u03b8. The gradient\nof the performance V\u2122 with respect to the direct representation \u03c0 (Sutton et al., 1999; Agarwal et al.,\n2019; Bhandari and Russo, 2019)\u2014which we call the \u201cfunctional\u201d gradient, to distinguish it from\nthe gradient VoV relative to a policy parameter 0, is \u2207 V = 1/(1-1)d,Q, \u2208 RIAT. Then,\nwe define VV \u2208 R|S|\u00d7|A| as the concatenation of \u2207\u201e\u201eV,\u2200s \u2208 S (yielding a PGT for directional\nderivatives in Lemma 2 in Appendix B.1.1).\nMirror Descent (MD) is a general gradient descent algorithm, applicable to constrained spaces\nC, which relies on Fenchel conjugate duality to map the iterates of an optimization problem x* =\nargmin\u2208xnc f(X), back and forth between a primal X and a dual space X*. The algorithm\nuses a convex function of the Legendre-type, called a mirror map h, to map the MD iterates x\nto the dual space where the gradient update is performed \u2207h(y) = \u2207h(x) \u2013 n\u2207f(x), with \u03b7 a\nstep size. A new iterate satisfying the primal constraints C is obtained using a Bregman projection\nx' = proje(y) = argminz\u2208c Dh(x, \u2207h*(\u2207h(y))) of the updated dual iterate \u2207h(y) mapped back\nin the primal space using the conjugate function of h, h*(x*) = sup\u2208x(x,x*) - h(x). This\nprojection relies on a Bregman divergence Dh(x,y) = h(x) \u2013 h(y) \u2013 (\u2207h(y), x - y) (Amari,\n1998; Bubeck, 2015; Banerjee et al., 2005). Furthermore, cf. Amari (2016), the divergences derived"}, {"title": "4 Functional Acceleration for PMD", "content": "In this work, we primarily focus on a momentum-based PMD update. To build some intuition around\nthe proposed update, consider first an idealized update, called PMD(+lookahead), anticipating one\niteration ahead on the optimization path using the lookhead return, denoted Qt,\n\\(Q_{s,a} = E_{s'\\sim P(.\\s,a)} [r'_{s,a} + E_{a'\\sim\\tilde{\\pi}^t} [Q_{s',a'}]]\\), with \\(\\tilde{\\pi}^t = greedy(Q^t)\\), \u2200s \u2208 S, a \u2208 A                                                                 (1)\nand defined as the expected return of acting greedily with t for one iteration, and fol-\nlowing \u03c0\u00b9 thereafter. With \u1fc6t an adaptive step-size, it leads to the PGD update \u03c0t+1\n\\(argmin_{\\pi\\in\\triangle(A)} D_h(\\pi_s, \\nabla h^*(\\nabla h(\\pi) - \\tilde{\\eta}Q))\\) (cf. Alfano et al. (2024)), and to the proximal\nupdate (cf. Johnson et al. (2023))\n\\(\\pi^{t+1} = argmin_{\\pi\\in\\triangle(A)} - (Q^t, \\pi_s) + 1/\\tilde{\\eta}^t D_h(\\pi_s,\\pi_s^t)\\)\n(2)\nProp. 1 indicates PMD (+lookahead) (Eq. 1 & Eq. 2) accelerates convergence by changing the\ncontraction rate, via the discount factor 2, instead of the traditional \u03b3, corresponding to the one-step\nlookahead horizon chosen here, H = 1 (generalizing to y\u0397+1 for multi-step). Proofs in Appendix B.2.\nProposition 1. (Functional acceleration with exact PMD(+lookahead)) The policy iterates \u03c0t+1\nof PMD(+lookahead) satisfy \\(||V^* \u2013 V\u2020||\u221e \u2264 (y\u00b2)^*(||V^* \u2013 V\u00b0||\u221e + \u2211i<t \u20aci/(x\u00b2)\u00b2)\\), with step-size\nadaptation, \u1fc6t \u2265 1/\u20ac+Dh (greedy(Q), \u03c0\u2021), \u2200et arbitrarily small.\nFor inexact critics Qt, it is known that the hard greedification operator = greedy(Q) can yield\nunstable updates. Taking inspiration from the \"mirror prox\" method of Nemirovski (2004), aka the\n\"extragradient\" (extrapolated gradient) method, we further relax the lookahead by replacing the hard\ngreedification in Eq. 1 with another PMD update, using an adaptive step-size nt,\n\\(\\tilde{\\pi}^t = argmin_{\\pi\\in\\triangle(A)} - (Q^t, \\pi_s) + 1/\\eta^t D_h(\\pi_s,\\pi_s^{t})\\)\n(3)"}, {"title": "4.1 Approximate Functional Acceleration for Parametric Policies", "content": "We are interested in designing algorithms feasible for large-scale optimization, so we further consider\nparametrized versions of the functional acceleration algorithms introduced, which we illustrate\nnumerically in Sec. 5.\nQ-function Approximation For the exact setting, we compute model-based versions of all updates,\nQ = Qt. For the inexact setting, we consider approximation errors between  and Qt (Sec.5.3)."}, {"title": "5 Numerical Studies", "content": "In this section, we investigate numerically the aforementioned algorithms, focusing on the fol-\nlowing questions: (i) When is acceleration possible?\u2014Sec. 5.1 investigates for which settings is\nfunctional acceleration opportune, and attempts to characterize properties of the problem which\nmake it advantageous. (ii) What are the policy optimization dynamics of each functional acceler-\nation method?-Sec. 5.2 illustrates the policy optimization dynamics of the methods introduced\non the space of policy value. (iii) Should we expect acceleration to be effective with an inexact\ncritic? Sec. 5.3 investigates the implications of using value approximation."}, {"title": "5.1 When is Acceleration Possible?", "content": "Experimental Setting We consider randomly constructed finite MDPs-Random MDP problems\n(Archibald et al., 1995), abstract, yet representative of the kind of MDP encountered in practice,\nwhich serve as a test-bench for RL algorithms (Goyal and Grand-Clement, 2021; Scherrer and Geist,\n2014; Vieillard et al., 2019). A Random MDP generator M = (|S|, |A|, b, y) is parameterized by\n4 parameters: number of states |8|, number of actions |A|, branching factor b specifying for each\nstate-action pair the maximum number of possible next states, chosen randomly. We vary b, \u03b3, and\n|A| to show how the characteristics of the problem, and the features of the algorithms, impact learning\nspeed with or without functional acceleration. Additional details in Appendix G.2.\nMetrics We measure the following quantities. (i) The optimality gap or cumulative regret after T\niterations, Regret\u2081 = \u2211t<TV* - V. The relative difference in optimality gap between the PMD\nbaseline and PMD(+mom) (henceforth used as shorthand for Lazy PMD(+momentum)) shows whether\nfunctional acceleration speeds up convergence. To quantify the complexity of the optimization\nproblem and ill-conditioning of the optimization landscape (significant difference in scaling along\ndifferent directions) for a Random MDP instance, we use the dual representation form of Wang et al.\n(2008) for policies, aka the successor representation (Dayan, 1993) or state-visitation frequency.\nSpecifically, we define the matrix \u03a8 = (I \u2212 \u03b3P\")\u22121, with P*Vs = Ea~\u03c0,s'~P(\u00b7\\s,a) [Vs']. Policy\niteration is known to be equivalent to the Newton-Kantorovich iteration procedure applied to the\nfunctional equation of dynamic programming (Puterman and Brumelle, 1979), V\u03c0*+1 =\n\u03bd\u03c0\u03b5\u03a8\u2207f(V), where \u2207 f(V) = (I \u2013 T)(V)\u2014with T the Bellman operator can be treated as the\""}, {"title": "5.2 Policy Dynamics in Value Space", "content": "We study the map \u3160 \u2192 V\u2122 from stationary policies to their respective value functions. This functional\nmapping from policies to values has been characterized theoretically as a possibly self-intersecting,\nnon-convex polytope (Dadashi et al., 2019). Specifically, we illustrate the expected dynamics of\nthe functional acceleration algorithms introduced in Sec. 4 (summarized in Alg. 2 in Appendix C),\nover the joint simplex describing all policies. The space of value functions V is the set of all value\nfunctions that are attained by some policy and corresponds to the image of II under the functional\nmapping \u03c0\u2192 V\": V = {V\"|\u03c0\u03b5\u03a0}."}, {"title": "5.3 Functional Acceleration with an Inexact Critic", "content": "For the same experimental setting as Sec. 5.2, Fig. 3 illustrates the impact of an inexact critic on\nthe relative advantage of functional acceleration, in two settings: (Left) controlled\u2014the critic's\nerror is sampled from a random normal distribution with mean 0 and standard deviation 7, such\nthat Q = Q + N(0, \u03c4), \u2200s. (Right) natural\u2014the critic is an empirical estimate of the return\nobtained by Monte-Carlo sampling, and its error arises naturally from using m truncated trajectories\nup to horizon 1/1-7, i.e. Q = 1/m i<m G/N, where G is the ith empirical return sampled with\n\u03c0 and N is the empirical visitation frequency of s.\nWe observe a larger relative difference in suboptimality on average between PMD (+mom) and PMD for\nhigher values of k, highlighting the difference between functional acceleration (cf. Sec.4) and classic\nacceleration (applied to the parameter vector 0), corresponding to k = 1, reinforcing evidence from\nSec. 5.1. Further, we confirm PI performs increasingly poor when paired with an inexact critic with\ngrowing error. Then, we observe a range in which functional acceleration is particularly advantageous,\nwhich extends from having negligible benefit, for small k, to more impactful differences in optimality\ngap for larger k. Beyond a certain sweet spot, when it is maximally advantageous, the critic's error\nbecomes too large, leading to oscillations and considerable variance. Additional illustrations of this\nphenomenon in Appendix H.3."}, {"title": "6 Closing", "content": "Inspired by functional acceleration from convex optimization theory, we proposed a momentum-based\nPMD update applicable to general policy parametrization and large-scale optimization. We analyzed\nseveral design choices in ablation studies designed to characterize qualitatively the properties of the\nresulting algorithms, and illustrated numerically how the characteristics of the problem influence\nthe added benefit of using acceleration. Finally we looked at how inexact critics impact the method.\nFurther analysis with these methods using stochastic simulation and function approximation would\nbe very useful."}, {"title": "A Notation", "content": ""}, {"title": "B Proofs and derivations", "content": ""}, {"title": "B.1 Proofs and Derivations for Sec.3: Background & Preliminaries", "content": ""}, {"title": "B.1.1 Functional Policy Gradient", "content": "The Performance Difference Lemma (PDL) is a property that relates the difference in values of\npolicies to the policies themselves.\nLemma 1. (Performance Difference Lemma from Kakade and Langford (2002)) For any policies\n\u03c0\u207a+1 and \u03c0\u207a, and an initial distribution p\n\\(V_{\\pi^{t+1}} - V_{\\pi^{t}} = \\frac{1}{1-\\gamma} \\sum_s \\sum_a (\\pi^{t+1}(a|s)-\\pi^{t}(a|s), Q^t_{s,a}) = \\frac{1}{1-\\gamma}E_{s\\sim d^{\\pi^{t+1}}}[Q_{\\pi^{t+1}} - \\pi^t]\\)\nProof. According to the definition of the value function\n\\(V_{\\pi^{t+1}} - V_{\\pi^{t}} = (Q_{\\pi^{t+1}},\\pi^{t+1}) - (Q^t, \\pi^t)\\)\n\\(= (Q^t, \\pi^{t+1} - \\pi^t) + (Q_{\\pi^{t+1}} - Q^t, \\pi^{t+1})\\)\n\\(= (Q^t, \\pi^{t+1} - \\pi^t) + \\gamma \\sum_s \\sum_a P(s'|s, a) [V_{\\pi^{t+1}}-V^t]\\)\n\\(= \\frac{1}{1-\\gamma}\\sum_s d^{\\pi^{t+1}}(s) [Q^t,\\pi^{t+1} - \\pi^t]\\)"}, {"title": "B.1.2 Helpful Lemmas for Policy Mirror Descent", "content": "Key to the analysis of Xiao (2022) and Johnson et al. (2023) is the Three-Point Descent Lemma, that\nrelates the improvement of the proximal gradient update compared to an arbitrary point. It originally\ncomes from Chen and Teboulle (1993) (Lemma 3.2).\nLemma 4. (Three-Point Descent Lemma, Lemma 6 in Xiao (2022)). Suppose that X CRn is a\nclosed convex set, \u03c8 : X \u2192 R is a proper, closed convex function, Dh(\u00b7,\u00b7) is the Bregman divergence\ngenerated by a function h of Legendre type and rint dom h\u2229 X \u2260 \u00d8. For any x\u207a \u2208 rint dom h, let\nxt+1 = argminz\u2208x \u03c8(x) + Dh(x, xt)\nThen xt+1 \u2208 rint dom h\u2229 X and \u2200x \u2208 X,\n\\(\u03c8(x^{t+1}) + D_h(x^{t+1},x^{t}) < \u03c8(x) + D_h(x,x^{t}) \u2013 D_h(x, x^{t+1})\\)\nThe PMD update is an instance of the proximal minimisation with X = \u2206(A),xt = \u03c0 and\n\u03c8(x) = \u2212nt (Q, x). Plugging these in, the Three-Point Descent Lemma relates the decrease in the\nproximal objective of +1 to any other policy, i.e. \u2200\u03c0\u03c2 \u2208 \u0394(A),\n\u2212\u03b7 (Q, \u03c0+1 \u2013 \u03c0s) \u2264 Dh(\u03c0\u03c2,\u03c0) \u2013 Dh(\u03c0+1, \u03c0) \u2013 Dh(\u03c0\u03c2, \u03c0+1)\nThis equation is key to the analysis of convergence of exact PMD, leading to Lemma 6 regarding the\nmonotonic improvement in Q-functions of PMD iterates.\nLemma 5. (Descent Property of PMD for Q-functions, Lemma 7 in Xiao (2022) Consider the\npolicies produced by the iterative updates of exact PMD. For any t > 0\n\\((Q^t, \\pi^{t+1} - \\pi^t) \\geq 0, \\forall s \\in S,\\)"}, {"title": "B.1.3 Detailed Derivation of the Suboptimality Decomposition and Convergence of PMD", "content": "Suboptimality decomposition Fix a state s. For any \u03c0\u03c2, \u00d1s, let D\u2212v (\u03c0s, \u00d1s) be analogous to a\nstandard Bregman divergence with mirror map - V, capturing the curvature of \u2013 V at \u03c0\u03b5\n\\(D_{-v}(\\pi_s, \\tilde{\\pi}_s) = -V_s \u2013 (-\\tilde{V}_s) - (-Q_s^t, \\pi_s - \\tilde{\\pi}_s)\\)\n\\(= -V_s^ + \\tilde{V}_s + (Q_s^t, \\pi_s - \\tilde{\\pi}_s)\\)\n\\(= -(Q^t - Q^t, \\pi_s)\\)\n(using Holder's inequality)\n\\(\\geq - ||Q - Q^t|| ||\\pi_s||_1\\)\n\\(\\geq -||V^ \u2013 V^t||_\\infty\\)\nFor the general case, using the approximation Q \u2248 Q, the per-iteration suboptimality is\n\\(V^ \u2013 V^{t+1} = -(Q^, \\pi_s^{t+1} \u2013 \\pi^*) - (Q^ \u2013 Q^, \\pi^ - \\pi^{t+1}) \u2013 D_{-v}(\\pi_s^t,\\pi_s^) \u2013 (Q-Q^t, \\pi_s^ \u2013 \\pi_s^t)\\)\nThe first term, -(Q^, \\pi_s^{t+1} \u2013 \\pi^*), is the forward regret (cf. Joulani et al. (2020a)), defined as the\nregret of a \"cheating\" algorithm that uses the wt+1 at time t, and depends only on the choices of the\nalgorithm and the feedback it receives. This quantity can be can be upper-bounded using an idealized\nlookahead policy, \u5143+1\u2014greedy with respect to Q (cf. Johnson et al. (2023)).\nIf \u03c0+1 is the result of a PMD update, then Johnson et al. (2023) show that using the Three-Point\nDescent Lemma (Lemma 6, Xiao (2022), included in Appendix B.1, Lemma 4), denoting the step\nsizes nt > 0, the forward regret is further upper-bounded by\n\\(-Q^, \\pi_s^{t+1} \u2013 \\pi_s^{t+1})\\leq \\epsilon_t\\)\n<<@(s^{t+1}) - +1)\\leq\\frac{1}{\\eta^t}D_h(\\pi_s^{t+1}, \\pi_s^*)<\\epsilon^{t}\\)\n<<@,\u5143-\u5143< \\frac{1}{n^4}D(\u5143,\u5143-10D(+10D(+1,) ) < "}, {"title": "B.2 Proofs for Sec. 4: Functional Acceleration for PMD", "content": "Definition 7. (Functional gradient of the Bregman divergence ) Fix a state s. For any policies\n\u03c0\u03b9"}, {"title": "Functional Acceleration for Policy Mirror Descent", "authors": ["Veronica Chelu", "Doina Precup"], "abstract": "We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-\nbased PMD update. By taking the functional route, our approach is independent\nof the policy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a special\ncase. We theoretically analyze several properties of this approach and complement\nwith a numerical ablation study, which serves to illustrate the policy optimization\ndynamics on the value polytope, relative to different algorithmic design choices\nin this space. We further characterize numerically several features of the problem\nsetting relevant for functional acceleration, and lastly, we investigate the impact of\napproximation on their learning mechanics.", "sections": [{"title": "1 Introduction", "content": "The RL framework (Sutton and Barto, 2018) refers to the problem of solving sequential decision\nmaking tasks under uncertainty, together with a class of solution methods tailored for it. The RL\nproblem has found applications in games (Tesauro, 1994; Mnih et al., 2013; Silver et al., 2014; Mnih\net al., 2016; Silver et al., 2017; Hessel et al., 2017; Bellemare et al., 2017; Schrittwieser et al., 2019;\nZahavy et al., 2023), robotic manipulation (Schulman et al., 2015, 2017; Haarnoja et al., 2018),\nmedicine (Jumper et al., 2021; Schaefer et al., 2004; Nie et al., 2020) and is formally described by\nmeans of discounted Markov Decision Processes (MDPs) (Puterman, 1994). On the solution side,\nincreased interest has been devoted to the study of policy-gradient (PG) methods based on optimizing\na parameterised policy with respect to an objective (Williams, 1992; Konda and Borkar, 1999; Sutton\net al., 1999; Agarwal et al., 2019; Bhandari and Russo, 2019; Kakade, 2001; Bhandari and Russo,\n2021; Mei et al., 2020b,a).\nPolicy Mirror Descent (PMD) (Agarwal et al., 2019; Bhandari and Russo, 2021; Xiao, 2022; Johnson\net al., 2023; Vaswani et al., 2021) is a general family of algorithms, specified by the choice of mirror\nmap covering a wide range of novel and fundamental methods in RL. PMD is a proximal algorithm\n(Parikh et al., 2014) and an instance of Mirror Descent (MD) (Beck and Teboulle, 2003) on the\npolicy simplex (Bhandari and Russo, 2021). MD is an iterative optimization algorithm that extends\ngradient descent (GD) by adapting to the geometry of the problem using different distance functions,\nparticularly Bregman divergences (Amari, 1998; Bubeck, 2015; Banerjee et al., 2005). PMD applies\na proximal regularization, with a Bregman divergence, to the improvement step of Policy Iteration\n(PI), and converges to it as regularization decreases. In the discounted setting, with an adaptive\nstep-size, PMD converges linearly at the optimal rate, determined by the discount factor, independent\nof the dimension of the state space or problem instance (Johnson et al., 2023), recovering classical\napproaches, like PI and VI, as special cases. PMD has been extended to linear approximation by\nYuan et al. (2023) and to general function approximation by Alfano et al. (2024)."}, {"title": "Motivation", "content": "The running time of PMD algorithms scales with the number of iterations. With a\nparametrized policy class, each iteration of an approximate PMD method generally requires multiple\n\"inner-loop\" updates to the policy parameter (Tomar et al., 2020; Vaswani et al., 2021). Actor-critic\n(AC) methods (Sutton et al., 1999; Konda and Borkar, 1999) additionally require the computation or\nupdating of an inexact critic at each iteration. It is therefore desirable to design algorithms which\nconverge in a smaller number of iterations, resulting in significant empirical speedups, as has been\npreviously argued by Johnson et al. (2023); Xiao (2022); Goyal and Grand-Clement (2021); Russo\n(2022).\nIn this work, we leverage duality and acceleration to build a novel surrogate objective for momentum-\nbased PMD, leading to faster learning in terms of less iterations necessary to converge. The novelty\nof our approach is the application of acceleration mechanics to the direct or functional policy\nrepresentation \u03c0\u2014hence named functional acceleration, as opposed to classic acceleration applied\nto the policy parameter \u03b8 (e.g., Mnih et al. (2016); Hessel et al. (2017); Schulman et al. (2017) use\nAdam (Kingma and Ba, 2015) or RMSProp (Hinton et al., 2012)). Specifically, we use momentum\nin the dual policy space to accelerate on \"long ravines\" or decelerate at \u201csharp curvatures\" at the\nfunctional level of the policy optimization objective. Intuitively, adding momentum to the functional\nPG (the gradient of the policy performance objective with respect to the direct policy representation\n\u03c0) means applying, to the current directional policy derivative, a weighted version of the previous\npolicy ascent direction, encouraging the method to adaptively accelerate according to the geometry\nof the optimization problem."}, {"title": "Contributions", "content": "We illustrate and analyze theoretically the impact of applying functional acceleration on the\noptimization dynamics of PMD, leading to a practical momentum-based PMD algorithm.\nWe characterize the properties of the problem setting, and those intrinsic to the algorithm,\nfor which applying functional acceleration is conducive to faster learning.\nWe study the influence of an inexact critic on the acceleration mechanism proposed."}, {"title": "Outline", "content": "This document is organized as follows. After placing our work in existing literature in\nSec. 2, and setting up the context in which it operates in Sec. 3, we introduce our main ideas in Sec. 4.\nWe complement with numerical studies in Sec. 5, ending with a short closing in Sec.6."}, {"title": "2 Related Work", "content": "Accelerated optimization methods have been at the heart of convex optimization research, e.g.,\nNesterov's accelerated gradients (NAG) (Nesterov, 1983; Wang and Abernethy, 2018; Wang et al.,\n2021), extra-gradient (EG) methods (Korpelevich, 1976), mirror-prox (Nemirovski, 2004; Juditsky\net al., 2011), optimistic MD (Rakhlin and Sridharan, 2013; Joulani et al., 2020b), AO-FTRL (Rakhlin\nand Sridharan, 2014; Mohri and Yang, 2015), Forward-Backward-Forward (FBF) method (Tseng,\n1991).\nAs far as we know, our idea of applying acceleration to the direct (functional) policy representation\n\u03c0\u03b8-independent of the policy parametrization \u2014is novel. This is important because it means\nuniversality of the approach to any kind of parameterization and functional form a practitioner\nrequires. Within the context of RL, acceleration has only been applied to value learning (Vieillard\net al., 2019; Farahmand and Ghavamzadeh, 2021; Goyal and Grand-Clement, 2021), or in the context\nof PG methods, classic acceleration is applied to the policy parameter \u03b8\u2014all recent deep RL works\n(e.g. Mnih et al. (2016); Hessel et al. (2017); Schulman et al. (2017)) use some form of adaptive\ngradient method, like Adam (Kingma and Ba, 2015) or RMSProp (Hinton et al., 2012). The idea of\nacceleration generally relies on convexity of the objective relative to the representation of interest.\nThe transformation from parameters @ to functional representation of the policy as probabilities \u03c0\u00ba,\ncan be highly complex, non-linear, and problem-dependent. Proximal algorithms operate on this\nfunctional representation, and rely on relative-convexity and relative-smoothness (Lu et al., 2017) of\nthe objective with respect to \u03c0 when constructing surrogate models (Bhandari and Russo, 2019, 2021;\nAgarwal et al., 2019; Vaswani et al., 2021). These properties suggests the functional acceleration\nmechanism is feasible and promising in our setting, since it is able to successfully accelerate convex\noptimization (Joulani et al., 2020b)."}, {"title": "Approximate PMD", "content": "PMD has been extended to function approximation by Tomar et al. (2020), and\nlater analyzed by Vaswani et al. (2021), who treat the surrogate objective as a nonlinear optimization\nproblem, that of approximately minimizing at each iteration, a composite proximal objective, denoted\nl(\u03c0\u03b8) with respect to the policy parameter \u03b8. In contrast, Alfano et al. (2024) applies the PMD update\nin the dual form, as (generalized) Projected Gradient Descent (PGD), i.e. a gradient update in the\ndual space followed by a projection (Bubeck, 2015), rather than in proximal form (Beck and Teboulle,\n2003), as used by Tomar et al. (2020). Vaswani et al. (2023) further relies on a dual policy norm,\ninduced by the chosen mirror map of an approximate PMD update, to measure the critic's evaluation\nerror in a decision-aware actor-critic algorithm, whereas Alfano et al. (2024) relies on the L2-norm\non the dual representation of the policy to measure its function approximation error. In a similar vein\nwe leverage critic differences in dual space to gain momentum and accelerate the policy optimization."}, {"title": "Limitations & Future Work", "content": "Our focus is on developing a foundation that motivates further study.\nA translation to practical large-scale implementations and deep RL remains for further investigation,\ni.e. with non-standard proximal methods, e.g., TRPO (Schulman et al., 2015), PPO (Schulman et al.,\n2017), MDPO (Tomar et al., 2020), MPO (Abdolmaleki et al., 2018). Additional guarantees of\naccelerated convergence for general policy parametrizations using the dual policy norm, as well as\ntheoretical analysis for the stochastic setting, are also deferred for future work."}, {"title": "3 Background & Preliminaries", "content": "RL We consider a standard RL setting described by means of a Markov decision process (MDP)\n(S, A, r, P, \u03b3, \u03c1), with state space S, action space A, discount factor \u03b3 \u2208 [0, 1), initial state distribution\n\u03c1\u03b5\u0394(S) (\u0394(\u03a7)\u2014the probability simplex over a set X), rewards are sampled from a reward function\nR~r(S, A), r : S \u00d7 A \u2192 [0, Rmax], and next states from a transition probability distribution\nS' ~ P(.\\S, A) \u2208 \u0394(8). The RL problem (Sutton and Barto, 2018) consists in finding a policy\n\u03c0: 8 \u2192 \u0394\u0391 \u0395 \u03a0 = = \u0394\u0399 \u2206, maximizing the performance objective defined as the discounted\nexpected cumulative reward V = Es~V \u2208 R, where V\" \u2208 R|S| and Q\" \u2208 R|S|\u00d7|A| are the\nvalue and action-value functions of a policy \u03c0, such that V = E[=0 Y\u00b2 Ri+1|So = s], Q\",a =\n\u0395\u03c0 [\u03a3\u03c4\u03bf Rio = s, Ao = a] and V\u2122 = Ea\u223c\u03c0 [Q(s, a)]. There exists an optimal deterministic\npolicy \u03c0* that simultaneously maximises V\u2122 and Q\" (Bellman, 1957). Let d\u2122 be the discounted\nvisitation distribution d = (1\u2212y) \u03a3\u03c4\u03bf \u03b3\u00b2 Pr(Si = s|So ~ p, Aj ~ \u03c0sj,\u2200j \u2264 i).\nWe use the shorthand notation (,)\u2014the dot product, \u2207 f(x) = \u25bcxf(x)-gradients and partial\nderivatives, \u2207 f(x, y) = \u221axf(x,y), \u03c0\u207a = \u03c0\u00bat, Qt = Q\u2122t, Vt = V\u00abt, dt = d*, \u03c0\u03c2 = \u03c0(\u00b7|s),\nQs = Q(s), V = V(s), rs,a = r(s, a), \u03c0\u03b1\\s = n(a|s), Qs,a = Q(s, a).\nPG Algorithms update the parameters \u03b8 \u2208 \u0472 of a parametric policy \u03c0\u00ba using surrogate objectives that\nare local approximations of the original performance. In the tabular setting, the direct parameterisation\nassociates a parameter to each state-action pair, allowing the shorthand notation \u03c0 = \u03c0\u03b8. The gradient\nof the performance V\u2122 with respect to the direct representation \u03c0 (Sutton et al., 1999; Agarwal et al.,\n2019; Bhandari and Russo, 2019)\u2014which we call the \u201cfunctional\u201d gradient, to distinguish it from\nthe gradient VoV relative to a policy parameter 0, is \u2207 V = 1/(1-1)d,Q, \u2208 RIAT. Then,\nwe define VV \u2208 R|S|\u00d7|A| as the concatenation of \u2207\u201e\u201eV,\u2200s \u2208 S (yielding a PGT for directional\nderivatives in Lemma 2 in Appendix B.1.1).\nMirror Descent (MD) is a general gradient descent algorithm, applicable to constrained spaces\nC, which relies on Fenchel conjugate duality to map the iterates of an optimization problem x* =\nargmin\u2208xnc f(X), back and forth between a primal X and a dual space X*. The algorithm\nuses a convex function of the Legendre-type, called a mirror map h, to map the MD iterates x\nto the dual space where the gradient update is performed \u2207h(y) = \u2207h(x) \u2013 n\u2207f(x), with \u03b7 a\nstep size. A new iterate satisfying the primal constraints C is obtained using a Bregman projection\nx' = proje(y) = argminz\u2208c Dh(x, \u2207h*(\u2207h(y))) of the updated dual iterate \u2207h(y) mapped back\nin the primal space using the conjugate function of h, h*(x*) = sup\u2208x(x,x*) - h(x). This\nprojection relies on a Bregman divergence Dh(x,y) = h(x) \u2013 h(y) \u2013 (\u2207h(y), x - y) (Amari,\n1998; Bubeck, 2015; Banerjee et al., 2005). Furthermore, cf. Amari (2016), the divergences derived"}, {"title": "4 Functional Acceleration for PMD", "content": "In this work, we primarily focus on a momentum-based PMD update. To build some intuition around\nthe proposed update, consider first an idealized update, called PMD(+lookahead), anticipating one\niteration ahead on the optimization path using the lookhead return, denoted Qt,\n\\(Q_{s,a} = E_{s'\\sim P(.\\s,a)} [r'_{s,a} + E_{a'\\sim\\tilde{\\pi}^t} [Q_{s',a'}]]\\), with \\(\\tilde{\\pi}^t = greedy(Q^t)\\), \u2200s \u2208 S, a \u2208 A                                                                 (1)\nand defined as the expected return of acting greedily with t for one iteration, and fol-\nlowing \u03c0\u00b9 thereafter. With \u1fc6t an adaptive step-size, it leads to the PGD update \u03c0t+1\n\\(argmin_{\\pi\\in\\triangle(A)} D_h(\\pi_s, \\nabla h^*(\\nabla h(\\pi) - \\tilde{\\eta}Q))\\) (cf. Alfano et al. (2024)), and to the proximal\nupdate (cf. Johnson et al. (2023))\n\\(\\pi^{t+1} = argmin_{\\pi\\in\\triangle(A)} - (Q^t, \\pi_s) + 1/\\tilde{\\eta}^t D_h(\\pi_s,\\pi_s^t)\\)\n(2)\nProp. 1 indicates PMD (+lookahead) (Eq. 1 & Eq. 2) accelerates convergence by changing the\ncontraction rate, via the discount factor 2, instead of the traditional \u03b3, corresponding to the one-step\nlookahead horizon chosen here, H = 1 (generalizing to y\u0397+1 for multi-step). Proofs in Appendix B.2.\nProposition 1. (Functional acceleration with exact PMD(+lookahead)) The policy iterates \u03c0t+1\nof PMD(+lookahead) satisfy \\(||V^* \u2013 V\u2020||\u221e \u2264 (y\u00b2)^*(||V^* \u2013 V\u00b0||\u221e + \u2211i<t \u20aci/(x\u00b2)\u00b2)\\), with step-size\nadaptation, \u1fc6t \u2265 1/\u20ac+Dh (greedy(Q), \u03c0\u2021), \u2200et arbitrarily small.\nFor inexact critics Qt, it is known that the hard greedification operator = greedy(Q) can yield\nunstable updates. Taking inspiration from the \"mirror prox\" method of Nemirovski (2004), aka the\n\"extragradient\" (extrapolated gradient) method, we further relax the lookahead by replacing the hard\ngreedification in Eq. 1 with another PMD update, using an adaptive step-size nt,\n\\(\\tilde{\\pi}^t = argmin_{\\pi\\in\\triangle(A)} - (Q^t, \\pi_s) + 1/\\eta^t D_h(\\pi_s,\\pi_s^{t})\\)\n(3)"}, {"title": "4.1 Approximate Functional Acceleration for Parametric Policies", "content": "We are interested in designing algorithms feasible for large-scale optimization, so we further consider\nparametrized versions of the functional acceleration algorithms introduced, which we illustrate\nnumerically in Sec. 5.\nQ-function Approximation For the exact setting, we compute model-based versions of all updates,\nQ = Qt. For the inexact setting, we consider approximation errors between  and Qt (Sec.5.3)."}, {"title": "5 Numerical Studies", "content": "In this section, we investigate numerically the aforementioned algorithms, focusing on the fol-\nlowing questions: (i) When is acceleration possible?\u2014Sec. 5.1 investigates for which settings is\nfunctional acceleration opportune, and attempts to characterize properties of the problem which\nmake it advantageous. (ii) What are the policy optimization dynamics of each functional acceler-\nation method?-Sec. 5.2 illustrates the policy optimization dynamics of the methods introduced\non the space of policy value. (iii) Should we expect acceleration to be effective with an inexact\ncritic? Sec. 5.3 investigates the implications of using value approximation."}, {"title": "5.1 When is Acceleration Possible?", "content": "Experimental Setting We consider randomly constructed finite MDPs-Random MDP problems\n(Archibald et al., 1995), abstract, yet representative of the kind of MDP encountered in practice,\nwhich serve as a test-bench for RL algorithms (Goyal and Grand-Clement, 2021; Scherrer and Geist,\n2014; Vieillard et al., 2019). A Random MDP generator M = (|S|, |A|, b, y) is parameterized by\n4 parameters: number of states |8|, number of actions |A|, branching factor b specifying for each\nstate-action pair the maximum number of possible next states, chosen randomly. We vary b, \u03b3, and\n|A| to show how the characteristics of the problem, and the features of the algorithms, impact learning\nspeed with or without functional acceleration. Additional details in Appendix G.2.\nMetrics We measure the following quantities. (i) The optimality gap or cumulative regret after T\niterations, Regret\u2081 = \u2211t<TV* - V. The relative difference in optimality gap between the PMD\nbaseline and PMD(+mom) (henceforth used as shorthand for Lazy PMD(+momentum)) shows whether\nfunctional acceleration speeds up convergence. To quantify the complexity of the optimization\nproblem and ill-conditioning of the optimization landscape (significant difference in scaling along\ndifferent directions) for a Random MDP instance, we use the dual representation form of Wang et al.\n(2008) for policies, aka the successor representation (Dayan, 1993) or state-visitation frequency.\nSpecifically, we define the matrix \u03a8 = (I \u2212 \u03b3P\")\u22121, with P*Vs = Ea~\u03c0,s'~P(\u00b7\\s,a) [Vs']. Policy\niteration is known to be equivalent to the Newton-Kantorovich iteration procedure applied to the\nfunctional equation of dynamic programming (Puterman and Brumelle, 1979), V\u03c0*+1 =\n\u03bd\u03c0\u03b5\u03a8\u2207f(V), where \u2207 f(V) = (I \u2013 T)(V)\u2014with T the Bellman operator can be treated as the\""}, {"title": "5.2 Policy Dynamics in Value Space", "content": "We study the map \u3160 \u2192 V\u2122 from stationary policies to their respective value functions. This functional\nmapping from policies to values has been characterized theoretically as a possibly self-intersecting,\nnon-convex polytope (Dadashi et al., 2019). Specifically, we illustrate the expected dynamics of\nthe functional acceleration algorithms introduced in Sec. 4 (summarized in Alg. 2 in Appendix C),\nover the joint simplex describing all policies. The space of value functions V is the set of all value\nfunctions that are attained by some policy and corresponds to the image of II under the functional\nmapping \u03c0\u2192 V", "V": "\u03c0\u03b5\u03a0}."}, {"title": "5.3 Functional Acceleration with an Inexact Critic", "content": "For the same experimental setting as Sec. 5.2, Fig. 3 illustrates the impact of an inexact critic on\nthe relative advantage of functional acceleration, in two settings: (Left) controlled\u2014the critic's\nerror is sampled from a random normal distribution with mean 0 and standard deviation 7, such\nthat Q = Q + N(0, \u03c4), \u2200s. (Right) natural\u2014the critic is an empirical estimate of the return\nobtained by Monte-Carlo sampling, and its error arises naturally from using m truncated trajectories\nup to horizon 1/1-7, i.e. Q = 1/m i<m G/N, where G is the ith empirical return sampled with\n\u03c0 and N is the empirical visitation frequency of s.\nWe observe a larger relative difference in suboptimality on average between PMD (+mom) and PMD for\nhigher values of k, highlighting the difference between functional acceleration (cf. Sec.4) and classic\nacceleration (applied to the parameter vector 0), corresponding to k = 1, reinforcing evidence from\nSec. 5.1. Further, we confirm PI performs increasingly poor when paired with an inexact critic with\ngrowing error. Then, we observe a range in which functional acceleration is particularly advantageous,\nwhich extends from having negligible benefit, for small k, to more impactful differences in optimality\ngap for larger k. Beyond a certain sweet spot, when it is maximally advantageous, the critic's error\nbecomes too large, leading to oscillations and considerable variance. Additional illustrations of this\nphenomenon in Appendix H.3."}, {"title": "6 Closing", "content": "Inspired by functional acceleration from convex optimization theory, we proposed a momentum-based\nPMD update applicable to general policy parametrization and large-scale optimization. We analyzed\nseveral design choices in ablation studies designed to characterize qualitatively the properties of the\nresulting algorithms, and illustrated numerically how the characteristics of the problem influence\nthe added benefit of using acceleration. Finally we looked at how inexact critics impact the method.\nFurther analysis with these methods using stochastic simulation and function approximation would\nbe very useful."}, {"title": "A Notation", "content": ""}, {"title": "B Proofs and derivations", "content": ""}, {"title": "B.1 Proofs and Derivations for Sec.3: Background & Preliminaries", "content": ""}, {"title": "B.1.1 Functional Policy Gradient", "content": "The Performance Difference Lemma (PDL) is a property that relates the difference in values of\npolicies to the policies themselves.\nLemma 1. (Performance Difference Lemma from Kakade and Langford (2002)) For any policies\n\u03c0\u207a+1 and \u03c0\u207a, and an initial distribution p\n\\(V_{\\pi^{t+1}} - V_{\\pi^{t}} = \\frac{1}{1-\\gamma} \\sum_s \\sum_a (\\pi^{t+1}(a|s)-\\pi^{t}(a|s), Q^t_{s,a}) = \\frac{1}{1-\\gamma}E_{s\\sim d^{\\pi^{t+1}}}[Q_{\\pi^{t+1}} - \\pi^t]\\)\nProof. According to the definition of the value function\n\\(V_{\\pi^{t+1}} - V_{\\pi^{t}} = (Q_{\\pi^{t+1}},\\pi^{t+1}) - (Q^t, \\pi^t)\\)\n\\(= (Q^t, \\pi^{t+1} - \\pi^t) + (Q_{\\pi^{t+1}} - Q^t, \\pi^{t+1})\\)\n\\(= (Q^t, \\pi^{t+1} - \\pi^t) + \\gamma \\sum_s \\sum_a P(s'|s, a) [V_{\\pi^{t+1}}-V^t]\\)\n\\(= \\frac{1}{1-\\gamma}\\sum_s d^{\\pi^{t+1}}(s) [Q^t,\\pi^{t+1} - \\pi^t]\\)"}, {"title": "B.1.2 Helpful Lemmas for Policy Mirror Descent", "content": "Key to the analysis of Xiao (2022) and Johnson et al. (2023) is the Three-Point Descent Lemma, that\nrelates the improvement of the proximal gradient update compared to an arbitrary point. It originally\ncomes from Chen and Teboulle (1993) (Lemma 3.2).\nLemma 4. (Three-Point Descent Lemma, Lemma 6 in Xiao (2022)). Suppose that X CRn is a\nclosed convex set, \u03c8 : X \u2192 R is a proper, closed convex function, Dh(\u00b7,\u00b7) is the Bregman divergence\ngenerated by a function h of Legendre type and rint dom h\u2229 X \u2260 \u00d8. For any x\u207a \u2208 rint dom h, let\nxt+1 = argminz\u2208x \u03c8(x) + Dh(x, xt)\nThen xt+1 \u2208 rint dom h\u2229 X and \u2200x \u2208 X,\n\\(\u03c8(x^{t+1}) + D_h(x^{t+1},x^{t}) < \u03c8(x) + D_h(x,x^{t}) \u2013 D_h(x, x^{t+1})\\)\nThe PMD update is an instance of the proximal minimisation with X = \u2206(A),xt = \u03c0 and\n\u03c8(x) = \u2212nt (Q, x). Plugging these in, the Three-Point Descent Lemma relates the decrease in the\nproximal objective of +1 to any other policy, i.e. \u2200\u03c0\u03c2 \u2208 \u0394(A),\n\u2212\u03b7 (Q, \u03c0+1 \u2013 \u03c0s) \u2264 Dh(\u03c0\u03c2,\u03c0) \u2013 Dh(\u03c0+1, \u03c0) \u2013 Dh(\u03c0\u03c2, \u03c0+1)\nThis equation is key to the analysis of convergence of exact PMD, leading to Lemma 6 regarding the\nmonotonic improvement in Q-functions of PMD iterates.\nLemma 5. (Descent Property of PMD for Q-functions, Lemma 7 in Xiao (2022) Consider the\npolicies produced by the iterative updates of exact PMD. For any t > 0\n\\((Q^t, \\pi^{t+1} - \\pi^t) \\geq 0, \\forall s \\in S,\\)"}, {"title": "B.1.3 Detailed Derivation of the Suboptimality Decomposition and Convergence of PMD", "content": "Suboptimality decomposition Fix a state s. For any \u03c0\u03c2, \u00d1s, let D\u2212v (\u03c0s, \u00d1s) be analogous to a\nstandard Bregman divergence with mirror map - V, capturing the curvature of \u2013 V at \u03c0\u03b5\n\\(D_{-v}(\\pi_s, \\tilde{\\pi}_s) = -V_s \u2013 (-\\tilde{V}_s) - (-Q_s^t, \\pi_s - \\tilde{\\pi}_s)\\)\n\\(= -V_s^ + \\tilde{V}_s + (Q_s^t, \\pi_s - \\tilde{\\pi}_s)\\)\n\\(= -(Q^t - Q^t, \\pi_s)\\)\n(using Holder's inequality)\n\\(\\geq - ||Q - Q^t|| ||\\pi_s||_1\\)\n\\(\\geq -||V^ \u2013 V^t||_\\infty\\)\nFor the general case, using the approximation Q \u2248 Q, the per-iteration suboptimality is\n\\(V^ \u2013 V^{t+1} = -(Q^, \\pi_s^{t+1} \u2013 \\pi^*) - (Q^ \u2013 Q^, \\pi^ - \\pi^{t+1}) \u2013 D_{-v}(\\pi_s^t,\\pi_s^) \u2013 (Q-Q^t, \\pi_s^ \u2013 \\pi_s^t)\\)\nThe first term, -(Q^, \\pi_s^{t+1} \u2013 \\pi^*), is the forward regret (cf. Joulani et al. (2020a)), defined as the\nregret of a \"cheating\" algorithm that uses the wt+1 at time t, and depends only on the choices of the\nalgorithm and the feedback it receives. This quantity can be can be upper-bounded using an idealized\nlookahead policy, \u5143+1\u2014greedy with respect to Q (cf. Johnson et al. (2023)).\nIf \u03c0+1 is the result of a PMD update, then Johnson et al. (2023) show that using the Three-Point\nDescent Lemma (Lemma 6, Xiao (2022), included in Appendix B.1, Lemma 4), denoting the step\nsizes nt > 0, the forward regret is further upper-bounded by\n\\(-Q^, \\pi_s^{t+1} \u2013 \\pi_s^{t+1})\\leq \\epsilon_t\\)\n<<@(s^{t+1}) - +1)\\leq\\frac{1}{\\eta^t}D_h(\\pi_s^{t+1}, \\pi_s^*)<\\epsilon^{t}\\)\n<<@,\u5143-\u5143< \\frac{1}{n^4}D(\u5143,\u5143-10D(+10D(+1,) ) < "}, {"title": "B.2 Proofs for Sec. 4: Functional Acceleration for PMD", "content": "Definition 7. (Functional gradient of the Bregman divergence ) Fix a state s. For any policies\n\u03c0\u03b9, \u03c0\u03bf, we denote the gradient of the Bregman divergence with respect to the first argument\n\\(\u25bdD_h(\\pi, \\pi^{0}) = \\nabla h(\\pi_s) \u2013 \\nabla h(\\pi^{0})\\)\nThe following lemma can be also interpreted as a definition for the difference of differences of\nBregman divergences.\nLemma 8. (Four-Point Identity Lemma of Bregman divergences) For any four policies\n\u03c0\u00b3, \u03c02, \u03c01, \u03c0\u00ba, we have\\((VD_h(\\pi^, \\pi^{0}), \\pi^{0} \u2013 \\pi^{2}) = D_h(\\pi^, \\pi^{0}) \u2013 D_h(\\pi^, \\pi^{0}) \u2013 [D_h(\\pi^2, \\pi^{0}) \u2013 D_h(\\pi, \\pi^{2})", "tilde{\\pi})": "ngiven step-sizes \u03b7\u207a, \u03c0\u2021, Q. For some Q, nt and \u2200ns \u2208 \u0394(A), let\n\\(l(\\pi_s, \\nabla h(\\pi) \u2013 \\eta Q) = -\\eta^ \\pi_s) + D_h(\\pi_s,\\pi_s^{t})\\)\nWith this notation, given step-sizes \u1fc6t, we write the surrogate objectives for the next policy iterates of\nt"}]}]}