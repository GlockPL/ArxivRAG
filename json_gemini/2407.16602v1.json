{"title": "Functional Acceleration for Policy Mirror Descent", "authors": ["Veronica Chelu", "Doina Precup"], "abstract": "We apply functional acceleration to the Policy Mirror Descent (PMD) general\nfamily of algorithms, which cover a wide range of novel and fundamental methods\nin Reinforcement Learning (RL). Leveraging duality, we propose a momentum-\nbased PMD update. By taking the functional route, our approach is independent\nof the policy parametrization and applicable to large-scale optimization, covering\nprevious applications of momentum at the level of policy parameters as a special\ncase. We theoretically analyze several properties of this approach and complement\nwith a numerical ablation study, which serves to illustrate the policy optimization\ndynamics on the value polytope, relative to different algorithmic design choices\nin this space. We further characterize numerically several features of the problem\nsetting relevant for functional acceleration, and lastly, we investigate the impact of\napproximation on their learning mechanics.", "sections": [{"title": "1 Introduction", "content": "The RL framework (Sutton and Barto, 2018) refers to the problem of solving sequential decision\nmaking tasks under uncertainty, together with a class of solution methods tailored for it. The RL\nproblem has found applications in games (Tesauro, 1994; Mnih et al., 2013; Silver et al., 2014; Mnih\net al., 2016; Silver et al., 2017; Hessel et al., 2017; Bellemare et al., 2017; Schrittwieser et al., 2019;\nZahavy et al., 2023), robotic manipulation (Schulman et al., 2015, 2017; Haarnoja et al., 2018),\nmedicine (Jumper et al., 2021; Schaefer et al., 2004; Nie et al., 2020) and is formally described by\nmeans of discounted Markov Decision Processes (MDPs) (Puterman, 1994). On the solution side,\nincreased interest has been devoted to the study of policy-gradient (PG) methods based on optimizing\na parameterised policy with respect to an objective (Williams, 1992; Konda and Borkar, 1999; Sutton\net al., 1999; Agarwal et al., 2019; Bhandari and Russo, 2019; Kakade, 2001; Bhandari and Russo,\n2021; Mei et al., 2020b,a).\nPolicy Mirror Descent (PMD) (Agarwal et al., 2019; Bhandari and Russo, 2021; Xiao, 2022; Johnson\net al., 2023; Vaswani et al., 2021) is a general family of algorithms, specified by the choice of mirror\nmap covering a wide range of novel and fundamental methods in RL. PMD is a proximal algorithm\n(Parikh et al., 2014) and an instance of Mirror Descent (MD) (Beck and Teboulle, 2003) on the\npolicy simplex (Bhandari and Russo, 2021). MD is an iterative optimization algorithm that extends\ngradient descent (GD) by adapting to the geometry of the problem using different distance functions,\nparticularly Bregman divergences (Amari, 1998; Bubeck, 2015; Banerjee et al., 2005). PMD applies\na proximal regularization, with a Bregman divergence, to the improvement step of Policy Iteration\n(PI), and converges to it as regularization decreases. In the discounted setting, with an adaptive\nstep-size, PMD converges linearly at the optimal rate, determined by the discount factor, independent\nof the dimension of the state space or problem instance (Johnson et al., 2023), recovering classical\napproaches, like PI and VI, as special cases. PMD has been extended to linear approximation by\nYuan et al. (2023) and to general function approximation by Alfano et al. (2024)."}, {"title": "Motivation", "content": "The running time of PMD algorithms scales with the number of iterations. With a\nparametrized policy class, each iteration of an approximate PMD method generally requires multiple\n\"inner-loop\" updates to the policy parameter (Tomar et al., 2020; Vaswani et al., 2021). Actor-critic\n(AC) methods (Sutton et al., 1999; Konda and Borkar, 1999) additionally require the computation or\nupdating of an inexact critic at each iteration. It is therefore desirable to design algorithms which\nconverge in a smaller number of iterations, resulting in significant empirical speedups, as has been\npreviously argued by Johnson et al. (2023); Xiao (2022); Goyal and Grand-Clement (2021); Russo\n(2022).\nIn this work, we leverage duality and acceleration to build a novel surrogate objective for momentum-\nbased PMD, leading to faster learning in terms of less iterations necessary to converge. The novelty\nof our approach is the application of acceleration mechanics to the direct or functional policy\nrepresentation \u03c0\u2014hence named functional acceleration, as opposed to classic acceleration applied\nto the policy parameter \u03b8 (e.g., Mnih et al. (2016); Hessel et al. (2017); Schulman et al. (2017) use\nAdam (Kingma and Ba, 2015) or RMSProp (Hinton et al., 2012)). Specifically, we use momentum\nin the dual policy space to accelerate on \"long ravines\" or decelerate at \u201csharp curvatures\" at the\nfunctional level of the policy optimization objective. Intuitively, adding momentum to the functional\nPG (the gradient of the policy performance objective with respect to the direct policy representation\n\u03c0) means applying, to the current directional policy derivative, a weighted version of the previous\npolicy ascent direction, encouraging the method to adaptively accelerate according to the geometry\nof the optimization problem."}, {"title": "Contributions", "content": "We illustrate and analyze theoretically the impact of applying functional acceleration on the\noptimization dynamics of PMD, leading to a practical momentum-based PMD algorithm.\nWe characterize the properties of the problem setting, and those intrinsic to the algorithm,\nfor which applying functional acceleration is conducive to faster learning.\nWe study the influence of an inexact critic on the acceleration mechanism proposed."}, {"title": "Outline", "content": "This document is organized as follows. After placing our work in existing literature in\nSec. 2, and setting up the context in which it operates in Sec. 3, we introduce our main ideas in Sec. 4.\nWe complement with numerical studies in Sec. 5, ending with a short closing in Sec.6."}, {"title": "2 Related Work", "content": "Accelerated optimization methods have been at the heart of convex optimization research, e.g.,\nNesterov's accelerated gradients (NAG) (Nesterov, 1983; Wang and Abernethy, 2018; Wang et al.,\n2021), extra-gradient (EG) methods (Korpelevich, 1976), mirror-prox (Nemirovski, 2004; Juditsky\net al., 2011), optimistic MD (Rakhlin and Sridharan, 2013; Joulani et al., 2020b), AO-FTRL (Rakhlin\nand Sridharan, 2014; Mohri and Yang, 2015), Forward-Backward-Forward (FBF) method (Tseng,\n1991).\nAs far as we know, our idea of applying acceleration to the direct (functional) policy representation\n\u03c0\u03b8-independent of the policy parametrization\u2014is novel. This is important because it means\nuniversality of the approach to any kind of parameterization and functional form a practitioner\nrequires. Within the context of RL, acceleration has only been applied to value learning (Vieillard\net al., 2019; Farahmand and Ghavamzadeh, 2021; Goyal and Grand-Clement, 2021), or in the context\nof PG methods, classic acceleration is applied to the policy parameter \u03b8\u2014all recent deep RL works\n(e.g. Mnih et al. (2016); Hessel et al. (2017); Schulman et al. (2017)) use some form of adaptive\ngradient method, like Adam (Kingma and Ba, 2015) or RMSProp (Hinton et al., 2012). The idea of\nacceleration generally relies on convexity of the objective relative to the representation of interest.\nThe transformation from parameters @ to functional representation of the policy as probabilities \u03c0\u00ba,\ncan be highly complex, non-linear, and problem-dependent. Proximal algorithms operate on this\nfunctional representation, and rely on relative-convexity and relative-smoothness (Lu et al., 2017) of\nthe objective with respect to \u03c0 when constructing surrogate models (Bhandari and Russo, 2019, 2021;\nAgarwal et al., 2019; Vaswani et al., 2021). These properties suggests the functional acceleration\nmechanism is feasible and promising in our setting, since it is able to successfully accelerate convex\noptimization (Joulani et al., 2020b)."}, {"title": "Approximate PMD", "content": "PMD has been extended to function approximation by Tomar et al. (2020), and\nlater analyzed by Vaswani et al. (2021), who treat the surrogate objective as a nonlinear optimization\nproblem, that of approximately minimizing at each iteration, a composite proximal objective, denoted\nl(\u03c0\u03b8) with respect to the policy parameter \u03b8. In contrast, Alfano et al. (2024) applies the PMD update\nin the dual form, as (generalized) Projected Gradient Descent (PGD), i.e. a gradient update in the\ndual space followed by a projection (Bubeck, 2015), rather than in proximal form (Beck and Teboulle,\n2003), as used by Tomar et al. (2020). Vaswani et al. (2023) further relies on a dual policy norm,\ninduced by the chosen mirror map of an approximate PMD update, to measure the critic's evaluation\nerror in a decision-aware actor-critic algorithm, whereas Alfano et al. (2024) relies on the L2-norm\non the dual representation of the policy to measure its function approximation error. In a similar vein\nwe leverage critic differences in dual space to gain momentum and accelerate the policy optimization."}, {"title": "Limitations & Future Work", "content": "Our focus is on developing a foundation that motivates further study.\nA translation to practical large-scale implementations and deep RL remains for further investigation,\ni.e. with non-standard proximal methods, e.g., TRPO (Schulman et al., 2015), PPO (Schulman et al.,\n2017), MDPO (Tomar et al., 2020), MPO (Abdolmaleki et al., 2018). Additional guarantees of\naccelerated convergence for general policy parametrizations using the dual policy norm, as well as\ntheoretical analysis for the stochastic setting, are also deferred for future work."}, {"title": "3 Background & Preliminaries", "content": "We consider a standard RL setting described by means of a Markov decision process (MDP)\n(S, A, r, P, \u03b3, \u03c1), with state space S, action space A, discount factor \u03b3 \u2208 [0, 1), initial state distribution\n\u03c1 \u2208 \u0394(S) (\u0394(X)\u2014the probability simplex over a set X), rewards are sampled from a reward function\nR ~ r(S, A), r : S \u00d7 A \u2192 [0, Rmax], and next states from a transition probability distribution\nS' ~ P(.|S, A) \u2208 \u0394(S). The RL problem (Sutton and Barto, 2018) consists in finding a policy\n\u03c0 : S \u2192 \u0394A \u2208 \u03a0 = \u0394|S| A, maximizing the performance objective defined as the discounted\nexpected cumulative reward $V^{\\pi} = E_{S_0 \\sim \\rho} [\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1}] \\in \\mathbb{R}$, where $V^{\\pi} \\in \\mathbb{R}^{|S|}$ and $Q^{\\pi} \\in \\mathbb{R}^{|S|\\times |A|}$ are the\nvalue and action-value functions of a policy \u03c0, such that $V^{\\pi}(s) = E[\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s]$, $Q^{\\pi}(s, a) =\nE_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} | S_0 = s, A_0 = a]$ and $V^{\\pi}(s) = E_{a \\sim \\pi} [Q(s, a)]$. There exists an optimal deterministic\npolicy \u03c0* that simultaneously maximises V\u03c0 and Q\u03c0 (Bellman, 1957). Let d\u03c0 be the discounted\nvisitation distribution $d^{\\pi}(s) = (1 - \\gamma) \\sum_{t=0}^{\\infty} \\gamma^t Pr(S_t = s | S_0 \\sim \\rho, A_j \\sim \\pi(S_j), \\forall j \\leq i)$.\nWe use the shorthand notation $(\\cdot,\\cdot)$\u2014the dot product, $\\nabla f(x) = \\nabla_x f(x)$-gradients and partial\nderivatives, $\\nabla f(x, y) = \\nabla_x f(x,y)$, $\\pi^{t+1} = \\pi^{\\theta_t}$, $Q_t = Q^{\\pi_t}$, $V_t = V^{\\pi_t}$, $d_t = d^{\\pi_t}$, $\\pi_s = \\pi(\\cdot|s)$,\n$Q_s = Q(s)$, $V_s = V(s)$, $r_{s,a} = r(s, a)$, $\\pi_{a|s} = \\pi(a|s)$, $Q_{s,a} = Q(s, a)$.\nPG Algorithms update the parameters \u03b8 \u2208 \u0398 of a parametric policy \u03c0\u03b8 using surrogate objectives that\nare local approximations of the original performance. In the tabular setting, the direct parameterisation\nassociates a parameter to each state-action pair, allowing the shorthand notation \u03c0 = \u03c0\u03b8. The gradient\nof the performance V\u03c0 with respect to the direct representation \u03c0 (Sutton et al., 1999; Agarwal et al.,\n2019; Bhandari and Russo, 2019)\u2014which we call the \u201cfunctional\u201d gradient, to distinguish it from\nthe gradient $V_{\\theta} V$ relative to a policy parameter \u03b8, is $\\nabla_{\\pi} V = \\frac{1}{(1-\\gamma)}d^{\\pi}Q^{\\pi} \\in \\mathbb{R}^{|S||A|}$. Then,\nwe define $\\nabla_{\\pi}V \\in \\mathbb{R}^{|S|\\times|A|}$ as the concatenation of $\\nabla_{\\pi_s} V, \\forall s \\in S$ (yielding a PGT for directional\nderivatives in Lemma 2 in Appendix B.1.1).\nMirror Descent (MD) is a general gradient descent algorithm, applicable to constrained spaces\nC, which relies on Fenchel conjugate duality to map the iterates of an optimization problem $x^* =$\nargminx\u2208X\u2229C f(x), back and forth between a primal X and a dual space X*. The algorithm\nuses a convex function of the Legendre-type , called a mirror map h, to map the MD iterates x\nto the dual space where the gradient update is performed $\\nabla h(y) = \\nabla h(x) \u2212 \\eta \\nabla f(x)$, with \u03b7 a\nstep size. A new iterate satisfying the primal constraints C is obtained using a Bregman projection\n$x' = proj_C(y) = argmin_{z\\in C} D_h(x, \\nabla h^*(\\nabla h(y)))$ of the updated dual iterate \u2207h(y) mapped back\nin the primal space using the conjugate function of h, $h^*(x^*) = sup_{x \\in X} \\langle x, x^* \\rangle - h(x)$. This\nprojection relies on a Bregman divergence $D_h(x,y) = h(x) \u2212 h(y) \u2212 \\langle \\nabla h(y), x \u2212 y \\rangle$ (Amari,\n1998; Bubeck, 2015; Banerjee et al., 2005). Furthermore, cf. Amari (2016), the divergences derived"}, {"title": "4 Functional Acceleration for PMD", "content": "In this work, we primarily focus on a momentum-based PMD update. To build some intuition around\nthe proposed update, consider first an idealized update, called PMD(+lookahead), anticipating one\niteration ahead on the optimization path using the lookhead return, denoted $\\tilde{Q}^t$,\n$\\tilde{Q}^t_{s,a} = E_{s' \\sim P(\\cdot |s,a)} [r'_{s,a} + \\gamma E_{a' \\sim \\pi_t^{greedy}} [\\tilde{Q}^t_{s',a'}]]$, with $\\pi_t^{greedy}(s) = argmax_{a} Q_t(s, a), \\forall s \\in S, a \\in A$ (1)\nand defined as the expected return of acting greedily with $\\pi_t$ for one iteration, and fol-\nlowing \u03c0t thereafter. With $\\tilde{\\eta}^t$ an adaptive step-size, it leads to the PGD update $\\pi^{t+1}=$\nargmin\u03c0s\u2208\u0394(A) Dh(\u03c0s, \u2207h*(\u2207h(\u03c0) \u2212 $\\tilde{\\eta}^t \\tilde{Q}^t$)) (cf. Alfano et al. (2024)), and to the proximal\nupdate (cf. Johnson et al. (2023))\n$\\pi^{t+1}_s = argmin_{\\pi \\in \\Delta(A)} - \\langle \\tilde{Q}^t, \\pi_s \\rangle + \\frac{1}{\\tilde{\\eta}^t} D_h(\\pi_s,\\pi^t_s)$ (2)\nProp. 1 indicates PMD (+lookahead) (Eq. 1 & Eq. 2) accelerates convergence by changing the\ncontraction rate, via the discount factor \u03b32, instead of the traditional \u03b3, corresponding to the one-step\nlookahead horizon chosen here, H = 1 (generalizing to \u03b3H+1 for multi-step). Proofs in Appendix B.2.\nProposition 1. (Functional acceleration with exact PMD(+lookahead)) The policy iterates \u03c0t+1\nof PMD(+lookahead) satisfy $||V^* - V^t||_{\\infty} \\leq (\\gamma^2)^t (||V^* - V^0||_{\\infty} + \\sum_{i<t} \\epsilon_i/(\\tilde{\\eta}^i)^2)$, with step-size\nadaptation, $\\tilde{\\eta}^t \\geq 1/\\epsilon_t D_h(greedy(\\tilde{Q}^t), \\pi^t_s), \\forall \\epsilon_t$ arbitrarily small.\nFor inexact critics Qt, it is known that the hard greedification operator $\\pi = greedy(\\tilde{Q})$ can yield\nunstable updates. Taking inspiration from the \"mirror prox\" method of Nemirovski (2004), aka the\n\"extragradient\" (extrapolated gradient) method, we further relax the lookahead by replacing the hard\ngreedification in Eq. 1 with another PMD update, using an adaptive step-size \u03b7t,\n$\\tilde{\\pi}_{s}^{t+1} = argmin_{\\pi \\in \\Delta(A)} - \\langle Q^t, \\pi_s \\rangle + \\frac{1}{\\eta^t} D_h(\\pi_s,\\pi_s^t)$ (3)"}, {"title": "4.1 Approximate Functional Acceleration for Parametric Policies", "content": "We are interested in designing algorithms feasible for large-scale optimization, so we further consider\nparametrized versions of the functional acceleration algorithms introduced, which we illustrate\nnumerically in Sec. 5.\nQ-function Approximation For the exact setting, we compute model-based versions of all updates,\n$\\tilde{Q} = \\tilde{Q}^t$. For the inexact setting, we consider approximation errors between $\\tilde{Q}$ and $\\tilde{Q}^t$ (Sec.5.3)."}, {"title": "5 Numerical Studies", "content": "In this section, we investigate numerically the aforementioned algorithms, focusing on the fol-\nlowing questions: (i) When is acceleration possible?\u2014Sec. 5.1 investigates for which settings is\nfunctional acceleration opportune, and attempts to characterize properties of the problem which\nmake it advantageous. (ii) What are the policy optimization dynamics of each functional acceler-\nation method?-Sec. 5.2 illustrates the policy optimization dynamics of the methods introduced\non the space of policy value. (iii) Should we expect acceleration to be effective with an inexact\ncritic? Sec. 5.3 investigates the implications of using value approximation."}, {"title": "5.1 When is Acceleration Possible?", "content": "Experimental Setting We consider randomly constructed finite MDPs-Random MDP problems\n(Archibald et al., 1995), abstract, yet representative of the kind of MDP encountered in practice,\nwhich serve as a test-bench for RL algorithms (Goyal and Grand-Clement, 2021; Scherrer and Geist,\n2014; Vieillard et al., 2019). A Random MDP generator M = (|S|, |A|, b, y) is parameterized by\n4 parameters: number of states |S|, number of actions |A|, branching factor b specifying for each\nstate-action pair the maximum number of possible next states, chosen randomly. We vary b, \u03b3, and\n|A| to show how the characteristics of the problem, and the features of the algorithms, impact learning\nspeed with or without functional acceleration. Additional details in Appendix G.2.\nMetrics We measure the following quantities. (i) The optimality gap or cumulative regret after T\niterations, $Regret_T = \\sum_{t<T} V^* - V^t$. The relative difference in optimality gap between the PMD\nbaseline and PMD(+mom) (henceforth used as shorthand for Lazy PMD(+momentum)) shows whether\nfunctional acceleration speeds up convergence. To quantify the complexity of the optimization\nproblem and ill-conditioning of the optimization landscape (significant difference in scaling along\ndifferent directions) for a Random MDP instance, we use the dual representation form of Wang et al.\n(2008) for policies, aka the successor representation (Dayan, 1993) or state-visitation frequency.\nSpecifically, we define the matrix $\\Psi = (I - \\gamma P^{\\pi})^{-1}$, with $P^{\\pi} V_s = E_{a\\sim \\pi,s'\\sim P(\\cdot |s,a)} [V_{s'}]$. Policy\niteration is known to be equivalent to the Newton-Kantorovich iteration procedure applied to the\nfunctional equation of dynamic programming (Puterman and Brumelle, 1979), $\\Psi\\nabla f(V^{\\pi})$, where $\\nabla f(V) = (I \u2212 T)(V)$\u2014with T the Bellman operator can be treated as the\ngradient operator of an unknown function f : $\\mathbb{R}^{|S|} \\rightarrow \\mathbb{R}$ (Grand-Cl\u00e9ment, 2021) (see Appendix F).\nFrom this perspective, the matrix \u03a8 can be interpreted as a gradient preconditioner, its inverse is\nthe Hessian \u22072 f(V), the Jacobian of a gradient operator Vf. We use the condition number of this\nmatrix, defined as \u03ba(\u03a8) = |\u03bbmax|/|\u03bbmin|, for \u03bbmax, \u03bbmin the max and min eigenvalues in the spectrum\nspec(\u03a8). We measure (ii) the condition number \u03ba0 = \u03ba(\u03a8\u03c00) of a randomly initialized (diffusion)\npolicy \u03c00 (Fig. 1(a-b)) and (iii) the average condition number \u03bat<T = 1/T\u03a3t<T \u03ba(\u03a8\u03c0t), for policies\non the optimization path of an algorithm (Fig. 1(c)). Lastly, we also measure (iv) the mean entropy\nof a randomly initialized policy $H_0 \\propto \\sum_{s \\in S} \\pi_s^0 log \\pi_s^0$ (Fig. 1(d)), inversely correlated with \u03ba0.\nSimilar observations can be made using the condition number of \u03a8\u22121 or the spectral radius \u03c1(\u03a8)."}, {"title": "5.2 Policy Dynamics in Value Space", "content": "We study the map \u03c0 \u2192 V\u03c0 from stationary policies to their respective value functions. This functional\nmapping from policies to values has been characterized theoretically as a possibly self-intersecting,\nnon-convex polytope (Dadashi et al., 2019). Specifically, we illustrate the expected dynamics of\nthe functional acceleration algorithms introduced in Sec. 4 (summarized in Alg. 2 in Appendix C),\nover the joint simplex describing all policies. The space of value functions V is the set of all value\nfunctions that are attained by some policy and corresponds to the image of \u03a0 under the functional\nmapping \u03c0 \u2192 V\u03c0: V = {V\u03c0|\u03c0\u2208\u03a0}."}, {"title": "5.3 Functional Acceleration with an Inexact Critic", "content": "For the same experimental setting as Sec. 5.2, Fig. 3 illustrates the impact of an inexact critic on\nthe relative advantage of functional acceleration, in two settings: (Left) controlled\u2014the critic's\nerror is sampled from a random normal distribution with mean 0 and standard deviation \u03c4, such\nthat $\\tilde{Q}_{s,a} = Q^t_{s,a} + \\mathcal{N}(0, \\tau), \\forall s$. (Right) natural\u2014the critic is an empirical estimate of the return\nobtained by Monte-Carlo sampling, and its error arises naturally from using m truncated trajectories\nup to horizon 1/1\u2212\u03b3, i.e. $\\tilde{Q}_{s,a} = \\frac{1}{m} \\sum_{i<m} G_s^i/N^i$, where $G_s^i$ is the ith empirical return sampled with\n\u03c0 and $N_s^i$ is the empirical visitation frequency of s.\nWe observe a larger relative difference in suboptimality on average between PMD (+mom) and PMD for\nhigher values of k, highlighting the difference between functional acceleration (cf. Sec.4) and classic\nacceleration (applied to the parameter vector \u03b8), corresponding to k = 1, reinforcing evidence from\nSec. 5.1. Further, we confirm PI performs increasingly poor when paired with an inexact critic with\ngrowing error. Then, we observe a range in which functional acceleration is particularly advantageous,\nwhich extends from having negligible benefit, for small k, to more impactful differences in optimality\ngap for larger k. Beyond a certain sweet spot, when it is maximally advantageous, the critic's error\nbecomes too large, leading to oscillations and considerable variance. Additional illustrations of this\nphenomenon in Appendix H.3."}, {"title": "6 Closing", "content": "Inspired by functional acceleration from convex optimization theory, we proposed a momentum-based\nPMD update applicable to general policy parametrization and large-scale optimization. We analyzed\nseveral design choices in ablation studies designed to characterize qualitatively the properties of the\nresulting algorithms, and illustrated numerically how the characteristics of the problem influence\nthe added benefit of using acceleration. Finally we looked at how inexact critics impact the method.\nFurther analysis with these methods using stochastic simulation and function approximation would\nbe very useful."}, {"title": "A Notation", "content": "iteration number\nmax number of iterations\nnumber of GD updates for the \"inner-loop\" proximal optimization procedure\nstep sizes for the proximal update (regularization strength of the divergence)\nstep size for the \"inner-loop\u201d parameter-level optimization procedure\nthe mirror map\nBregman divergence associated with the mirror map h"}, {"title": "B Proofs and derivations", "content": "Proofs and Derivations for Sec.3: Background & Preliminaries\nFunctional Policy Gradient\nThe Performance Difference Lemma (PDL) is a property that relates the difference in values of\npolicies to the policies themselves.\nLemma 1. (Performance Difference Lemma from Kakade and Langford (2002)) For any policies\n\u03c0t+1 and \u03c0t, and an initial distribution \u03c1"}, {"title": "C Details on PMD updates for Sec. 4: Functional Acceleration for PMD", "content": "Details on Algorithmic Implementation for Sec. 4.1: Approximate\nFunctional Acceleration for Parametric Policies\nWe use the following shorthand notation for the updates PMD,PMD(+ext),PMD(+cor),PMD(+lzc),\nPMD(+mom).\nPolicy approximation We parametrize the policy iterates using a Bregman policy class {\u03c0\u03b8 : \u03c0 =\nproj(A) (\u2207h*(f)), s \u2208 8} with a tabular parametrization 9. For the updates requiring two policies,\nwe keep them parametrized separately with w and \u03c0\u03c1.\nWe formulate the policy optimization problem using the extension proposed by Tomar et al. (2020).\nEach iteration, in an \u201cinner-loop\" optimization procedure, we update @ and w using k and n,\nrespectively, updates with standard GD on the composite PMD surrogate model, denoted l : \u2192 R\n(with \u2299 = |R|S|\u00d7|A| cf. the tabular parametrization) associated with the policy represented by those\nparameters \u03c0\u03bf, or \u00f1w, respectively. We execute the parameter optimization in expectation over the\nstate-action space. Concretely, for PMD we use the surrogate\n$l(\\theta) = E_{s \\sim d^t} [-\\langle Q^t, \\pi^{\\theta} \\rangle + \\frac{1}{\\eta^t}D_h(\\pi_s^\\theta, \\pi^t_s)] (43)$\nand update in an \"inner-loop\" optimization procedure\n$\\theta^{(init)} = \\theta (0) = \\theta (for i \\in [0..k]) \\theta^{(i+1)} = \\theta^{(i)} - \\beta \\nabla_{\\theta^{(i)}} l(\\theta^{(i)}) (final) \\theta_{t+1} = \\theta^{(k)}$\nwith \u03b2 a small learning rate. The optimization procedure for w is analogous. The rest of the\nalgorithms use the surrogate objectives as described in Sec. 4."}, {"title": "E Approximate Policy Mirror Descent as Projected Gradient Descent (PGD)", "content": "In this section we provide an alternative perspective on PMD\u2014cf. Lemma 3, stating that the MD\nupdate can be rewritten in the following ways"}, {"title": "F Newton's method", "content": "The Newton-Kantorovich theorem generalizes Newton's method for solving nonlinear equations to\ninfinite-dimensional Banach spaces. It provides conditions under which Newton's method converges\nand gives an estimate of the convergence rate. Newton-Kantorovich theorem deals with the conver-\ngence of Newton's method for a nonlinear operator F : X \u2192 Y, where X and Y are Banach spaces.\nThe method iteratively solves F(x) = 0 using\n$x^{t+1} = x^t - (\\nabla F)^{-1}F(x^t)$\nwhere VF is a generalization of the Jacobian of F, provided F is differentiable. Intuitively, at\neach iteration, the method performs a linearization of F(x) = 0 close to x, using a first order\nTaylor expansion: F(x + \u2206x) \u2248 F(x) +\u2207F(x)\u2206x, where F(x) + \u2207F(x)\u2206x = 0 \u21d4 \u2206x =\n\u2212(VF)\u22121F(x). For x close to x\u2217, x \u2212 (VF)\u22121F(x) is a good approximation of x\u2217. The iterative\nsequence {xt}t>0 converges to x\u2217, assuming the Jacobian matrix exists, is invertible, and Lipschitz\ncontinuous.\nQuasi-Newton methods Any method that replaces the exact computation of the Jacobian matrices\nin the Newton's method (or their inverses) with an approximation, is a quasi-Newton method. A\nquasi-Newton method constructs a sequence of iterates {xt}t>0 and a sequence of matrices {Jt}t\u22650\nsuch that Jt is an approximation of the Jacobian VF(x) for any t > 0 and\n$x^{t+1} = x^t - (J^t)^{-1}F(x^t)$\nIn Anderson's acceleration (Anderson, 1965), information about the last iterates is used to update the\napproximation of Jt.\nPolicy iteration as Newton's method In the context of Markov Decision Processes (MDPs), policy\niteration may be interpreted as Newton's method with the following notations and analogies. First,\nusing the Bellman optimality operator $T V = maxa[rsa+\u2211s, Ps, saVs, ], the aim is to find V such\nthat V = TV, which is akin to finding the roots V, such that F(V) = V \u2212 TV = (I \u2212 T)(V) = 0.\nWe interpret F = Vf as the gradient of an unknown function f : Rn \u2192 Rn, despite the Bellman\noperator being non-differentiable in general due to the max. Where the greedy policy \u03c0t attains the\nmax in TV, we obtain Jt = I \u2212 \u03b3P\u03c0t, which is invertible for \u03b3\u2208 (0, 1). Expanding the Bellman\noperator, we have\n$V^{\\pi^{t+1}} = \\Psi^t \\bar r^{\\pi^{t+1}}$\n$J^t V^{\\pi^{t+1}} = \\bar r^{\\pi^{t+1}}$"}, {"title": "G Experimental details for Sec. 5: Numerical Studies", "content": "Details of two-state Markov Decision Processes\nIn this section we give the specifics of the two-state MDPs presented in this work. We make use of\nthe notation\n$P(s_k | s_i, a_j"}]}