{"title": "DAQ: Density-Aware Post-Training Weight-Only Quantization For LLMs", "authors": ["Yingsong Luo", "Ling Chen"], "abstract": "Large language models (LLMs) excel in various tasks\nbut face deployment challenges due to hardware con-\nstraints. We propose density-aware post-training weight-only\nquantization (DAQ), which has two stages: 1) density-centric\nalignment, which identifies the center of high-density weights\nand centers the dynamic range on this point to align high-\ndensity weight regions with floating-point high-precision re-\ngions; 2) learnable dynamic range adjustment, which adjusts\nthe dynamic range by optimizing quantization parameters\n(i.e., scale and zero-point) based on the impact of weights\non the model output. Experiments on LLaMA and LLaMA-\n2 show that DAQ consistently outperforms the best baseline\nmethod, reducing perplexity loss by an average of 22.8% on\nLLaMA and 19.6% on LLaMA-2. Our code is available at\nhttps://anonymous.4open.science/r/DAQ-E747.", "sections": [{"title": "Introduction", "content": "In recent years, large language models (LLMs) based on\ntransformers (Vaswani et al. 2017) have demonstrated re-\nmarkable performance in various natural language process-\ning benchmarks (OpenAI 2023; Touvron et al. 2023a,b;\nZhang et al. 2022). These models exhibit deep semantic\nunderstanding and reasoning capabilities by learning from\nmassive amounts of text. They often have billions of param-\neters, e.g., LLaMA-2 (Touvron et al. 2023b) has up to 70\nbillion parameters. The immense size of these models leads\nto extremely high memory capacity requirements. In addi-\ntion, recent studies (Kim et al. 2024; Frantar et al. 2023; Lin\net al. 2024) identify memory bandwidth as a primary bottle-\nneck for LLMs small-batch inference.\nAlthough numerous model compression methods (e.g.,\nquantization-aware training (Liu et al. 2023b), pruning\n(Frantar and Alistarh 2023), and knowledge distillation (Gu\net al. 2023)) can alleviate the memory demands, these meth-\nods require retraining for model compression. For LLMs\nwith hundreds of billions of parameters, computational re-\nsources and the data requirements of retraining can be pro-\nhibitively expensive. In contrast, post-training quantization\n(PTQ) eliminates the need for model retraining, making it a\npromising solution in resource-constrained environments.\nMainstream PTQ methods (Frantar et al. 2023; Lin et al.\n2024) quantize weights or activations into uniform inte-"}, {"title": "Related Work", "content": "In this section, we review two main categories of rele-\nvant research in PTQ. Specifically, we first discuss weight-\nactivation quantization, followed by an overview of weight-\nonly quantization. This review situates our work within the\nexisting methods and highlights the significance of our pro-\nposed DAQ."}, {"title": "Weight-Activation Quantization", "content": "Weight-activation quantization methods attempt to quantize\nboth weights and activations, aiming to utilize specialized\nINT8 general matrix multiplication kernels, which can re-\nduce computational requirements by up to 50% compared\nto FP16 kernels (Wu et al. 2020). These works focus on ad-\ndressing the challenge posed by the highly-dynamic inter-\nmediate activations of LLMs. The ranges of different chan-\nnels often exceed a thousandfold, resulting in a significant\nperformance drop. To address this challenge, researchers\npropose various methods. For example, SmoothQuant (Xiao\net al. 2023) and Outlier Suppression (Wei et al. 2022) em-\nploy channel-wise quantization to handle the large dynamic\nrange of activations caused by a coarse quantization gran-\nularity; OmniQuant (Shao et al. 2024) and PaC (Tu et al.\n2023) propose learnable boundaries of dynamic range to\nmitigate the large dynamic range of activations caused by\noutliers. However, these methods typically can only quantize\nweights to 8 bits, and lower bit width leads to a significant\nmodel performance degradation, which is insufficient to ef-\nfectively address the memory bottleneck caused by weight\naccess in small-batch inference (Lin et al. 2024)."}, {"title": "Weight-Only Quantization", "content": "To further alleviate the memory bottleneck of small-batch\ninference, numerous researchers attempt to quantize the\nweights only. Weight-only quantization speeds up inference\nby reducing the size of weight access. In this scenario, the\nprimary challenge is to address the degradation in quanti-\nzation precision caused by outliers when using 4 bits or\nlower bits for weight representation (Dettmers and Zettle-\nmoyer 2023). For example, AdaQuant (Hubara et al. 2021),\nLAPQ (Nahshan et al. 2021), and ACIQ (Banner, Nahshan,\nand Soudry 2019) focus on truncating outliers by optimiz-\ning quantization parameters; SpQR (Dettmers et al. 2024),\nOWQ (Lee et al. 2024), and AWQ (Lin et al. 2024) empha-\nsize that salient weights have more impact on the activations.\nTo protect salient weights, SpQR and OWQ isolate salient\nweights and store them with FP16, while AWQ proposes a\nhardware-friendly method by utilizing a per-channel scaling\nmethod.\nDue to the limited expressiveness of integer quantization,\nsome researchers explore how to utilize FP quantization to\nimprove the performance of previous methods. For exam-\nple, MoFQ (Zhang et al. 2023b) determines the optimal data\ntype from various candidates (e.g., INT4 and FP4) for each\nlayer based on tensor error; AFPQ (Zhang et al. 2023a) em-\nploys separate scales for positive and negative weights, ad-\ndressing the asymmetric distributions commonly found in\nweights. However, these methods usually determine the dy-\nnamic range based on the minimum and maximum values or\nnarrow said dynamic range to truncate outliers, which may\nnot fully leverage the non-uniform properties of FP repre-\nsentation, particularly in cases where expanding or shifting\nthe dynamic range could be beneficial.\nIn this work, we propose a dynamic range optimization\nmethod for FP quantization, which considers adjusting the\ndynamic range to align high-density weight regions with FP\nhigh-precision regions and then adjusts the dynamic range\nbased on the impact of weights on the model output."}, {"title": "Preliminaries", "content": "Quantization. Quantization aims to represent model\nweights or activations with lower bit-width representations.\nThe basic idea is to map continuous real numbers to a finite\nset, and this mapping is typically implemented as a function.\nFor given original weights W, the quantized weights Wq can\nbe calculated as follows:\n$$W_{q} = RTN(\\frac{W+z}{s})$$\nwhere RTN represents the round-to-nearest function, s is the\nscale, and z is the zero-point. The quantization parameters s\nand z can be calculated as follows:\n$$s = \\frac{\\beta - \\alpha}{X_{max} - X_{min}}$$\n$$z = \\frac{\\alpha X_{min}}{s}$$\nwhere $[\\alpha, \\beta]$ is the dynamic range, and $[X_{min}, X_{max}]$ is the\nquantization range, which represents the range of the quan-\ntized values.\nDequantization. Dequantization aims to restore the quan-\ntized weights to their original bit-width during the inference"}, {"title": "Density-Centric Alignment", "content": "As shown in Figure 2, the middle part of the weight dis-\ntribution often exhibits a relatively high-density character-\nistic, while the weights on both sides of most groups are\nsparse and asymmetric. Thus, directly using the maximum\nand minimum values to determine the dynamic range will\ncause high-density weight regions to be mapped to FP low-\nprecision regions.\nTo address this issue, we propose DCA, which aims to\nalign high-density weight regions with FP high-precision re-\ngions. To find the center of high-density weights, we intro-\nduce the concept of the p-th quantile. Given a weight group\nw, the p-th quantile is the value below which p% of the\nweights fall. the center of high-density weights $p_c$ can be\ncalculated as follows:\n$$p_{c} = \\frac{Quantile(w, m) + Quantile(w, 100 - m)}{2}$$\nwhere Quantile represents the function that calculates the\nquantile of weights, and m represents the specific clipping\nrate, which is usually a small value.\nAfter determining the center of high-density weights $p_c$,\nwe introduce a variable k to represent the maximum distance\nbetween $p_c$ and the extreme values of weights. Let $w_{max}$ and\n$w_{min}$ denote the maximum and minimum values of w, re-\nspectively. The variable k can be calculated as follows:\n$$k = max(w_{max} - p_c, p_c - w_{min})$$\nSubsequently, we establish the dynamic range $[\\alpha, \\beta]$ by\nsetting it to $[p_c - k, p_c + k]$, which centers the dynamic range"}, {"title": "Learnable Dynamic Range Adjustment", "content": "Although DCA can align high-density weight regions with\nthe FP high-precision regions by adjusting the dynamic\nrange, DCA still has two limitations: 1) The degree of out-\nlierness varies among different weight groups. As shown\nin Figure 1a, for weight groups with a low degree of out-\nlierness, expanding the dynamic range can lead to higher\nprecision; 2) Absence of considering the impact of weights\non the model output may result in a suboptimal dynamic\nrange. Therefore, it is necessary to further adjust the dy-\nnamic range.\nThus, we propose LDRA, a quantization parameter op-\ntimization method based on the finite difference method.\nSpecifically, this method aims to adjust the dynamic range\nby optimizing the quantization parameters based on the im-\npact of weights on the model output."}, {"title": "Optimization Objective in Weight-Only Quantization.", "content": "The optimization objective in weight-only quantization is to\nfind the optimal zero-point $z^*$ and scale $s^*$ that minimize the\nquantization loss $\\mathcal{L}$, which can be formulated as follows:\n$$z^*, s^* = argmin_{\\substack{z,s}} \\mathcal{L}$$\nThe quantization loss $\\mathcal{L}$ is defined as the difference be-\ntween the intermediate output of the quantized layer and the\noutput of the original layer, which can be calculated as fol-\nlows:\n$$\\mathcal{L} = ||WX - W_qX||_F^2$$\nwhere X represents the input data of each layer obtained\nfrom the calibration dataset, and $||.||_F$ represents the Frobe-\nnius norm operator."}, {"title": "Methodology", "content": "In this section, we present our proposed DAQ, which has two\nstages: 1) DCA identifies the center of high-density weights\nand centers the dynamic range on this point to align high-\ndensity weight regions with FP high-precision regions; 2)\nLDRA further adjusts the dynamic range by optimizing the\nquantization parameters based on the impact of weights on\nthe model output."}, {"title": "Optimizing quantization parameters", "content": "Optimizing quantization parameters to reduce quantiza-\ntion error has been used in previous works (Hubara et al.\n2021; Nahshan et al. 2021; Banner, Nahshan, and Soudry\n2019). However, these methods are primarily designed for\ninteger quantization, where expanding or shifting the dy-\nnamic range typically leads to decreased quantization pre-\ncision. Consequently, these methods commonly employ sat-\nuration quantization, which involves decreasing the scale\nto narrow the dynamic range and filter outliers. In con-\ntrast, our method is tailored for FP quantization and based\non the fact that the dynamic range is affected by quanti-\nzation parameters, as shown in Figure 3. We aim to adjust\nthe dynamic range by optimizing quantization parameters to\nachieve higher precision, without restricting both expansion\nand shift of the dynamic range, which can leverage the non-\nuniform properties of FP representation.\nWe reformulate the loss function in Equation (5) to ex-\nplicitly include the quantization parameters:\n$$\\mathcal{L} = ||s(RTN(\\frac{W+z}{s}) - \\frac{z}{s})X - WX||_F^2$$\nThis formulation shows that by optimizing the quantization\nparameters s and z, we can adjust the dynamic range, which\nin turn affects the quantization loss. Our goal is to find the\noptimal values of s and z that minimize this loss, effectively\ndetermining the best dynamic range for quantization.\nTo overcome the non-differentiability of the RTN func-\ntion used in the quantization, we employ the finite difference\nmethod to estimate the gradient, which can be calculated as\nfollows:\n$$\\nabla \\mathcal{L}(x, \\epsilon) = \\frac{\\mathcal{L}(x + \\epsilon) - \\mathcal{L}(x - \\epsilon)}{2\\epsilon} \\approx \\frac{\\Delta \\mathcal{L}(x, \\epsilon)}{2\\epsilon}$$\nwhere x represents the value to be optimized, namely, the\nquantization parameters, and $\\epsilon$ represents the step size of\nthe finite difference method.\nDue to the non-smoothness of the objective function\ncaused by RTN function, there may be drastic changes in\nsome numerical values. Consequently, choosing an appro-\npriate learning rate $\\eta$ becomes particularly complex, which\nin turn leads to difficulties in the convergence of gradient\ndescent.\nIn recent years, sign-based gradient descent (SignGD)\nmethods (Bernstein et al. 2018) show good robustness, as\nthey only focus on the direction of the gradient rather than\nthe numerical value. To a certain extent, they can resist the\nimpact of drastic changes in the numerical value of the non-\nsmooth objective function in certain regions. The parameters\nof the k-th iteration $x_k$ can be optimized as follows:\n$$x_k = x_{k-1} - \\eta sign(\\nabla \\mathcal{L})$$\nwhere $\\eta$ is the learning rate, a small positive number repre-\nsenting the step size of each update. The sign function can\nbe calculated as follows:\n$$sign(x) =\\begin{cases}\n-1 & \\text{if } x < 0\\\\\n0 & \\text{if } x = 0\\\\\n1 & \\text{if } x > 0\n\\end{cases}$$\nThis function extracts only the sign of the gradient, effec-\ntively normalizing the update step and making it indepen-\ndent of the gradient value.\nBy using the direction of the gradient rather than its nu-\nmerical value as the basis for the update step, SignGD effec-\ntively avoids the problem of unstable update step size caused\nby drastic fluctuations in the gradient values. In addition,\nSignGD simplifies the gradient calculation and reduces the\ncomputational complexity, thereby improving the efficiency\nof the overall optimization process.\nThe learning rate $\\eta$ in SignGD should be chosen to bal-\nance the speed and stability of the optimization process. To\nfurther improve the efficiency and stability of the optimiza-\ntion, we introduce a learning rate decay mechanism within\nSignGD. This mechanism gradually reduces the learning\nrate as the number of iterations increases, allowing for rapid\nprogress in the early stages of optimization and more refined\nupdates when approaching the optimal solution. The learn-\ning rate $\\eta_t$ of the t-th iteration can be calculated as follows:\n$$\\eta_t = \\frac{\\eta_0}{1 + d \\cdot t}$$\nwhere $\\eta_0$ is the initial learning rate, d is the decay coefficient,\nand t is the number of iterations. The learning rate decay\nmethod can not only accelerate the convergence speed in the\nearly stage but also avoid oscillation and non-convergence\nin the later stage of optimization."}, {"title": "Experiments", "content": "In this section, we present comprehensive experiments to\nevaluate the effectiveness of DAQ. We first introduce the ex-\nperimental settings, including models, datasets, and imple-\nmentation details. Next, we compare DAQ with state-of-the-\nart methods, conduct ablation studies, examine performance\nunder limited calibration data, and demonstrate the integra-\ntion of DAQ with existing quantization methods."}, {"title": "Settings", "content": "Quantization. Our study primarily focuses on weight-\nonly quantization, as it can demonstrate the capability to\nlargely maintain the performance integrity of LLMs (Fran-\ntar et al. 2023). The integer (INT) and NormalFloat (NF)\n(Dettmers et al. 2023) are employed in the experiments.\nWe utilize group-wise quantization, a method that allows\nfor the independent optimization of quantization parameters\nfor each weight group. This method is extensively adopted\nin the field of LLM quantization (Lin et al. 2024; Frantar\net al. 2023; Zhang et al. 2023a). The size of the weight\ngroup is set to 256 in the experiments except otherwise spec-\nified. We sample the calibration dataset from the Pile (Gao\net al. 2020). This dataset is employed to mitigate the risk\nof overfitting to any particular downstream domain, thereby\nenhancing the generalizability of quantization methods.\nHyperparameters. LDRA is initialized with a learning\nrate of 1e-3, coupled with a decay rate of 0.05, a configu-\nration chosen to enhance training stability. The step size for\nthe finite difference method is established at 1e-4, striking a"}, {"title": "Ablation Study", "content": "To validate the effectiveness of DCA and LDRA in DAQ, we\ncompare DAQ with the following variants on LLaMA-2-7B\nusing NF4:"}, {"title": "Limited Calibration Dataset Experiment", "content": "To assess the effectiveness of DAQ under the limited calibra-\ntion dataset, we compare it to AWQ, the best PTQ method in\nour previous experiments, across varying calibration dataset\nsizes.\nAs shown in Figure 4, DAQ consistently outperforms\nAWQ across all calibration dataset sizes. The performance\ngap is most significant with extremely limited data (1\u00d7512\ntokens). Notably, DAQ using just 2x512 tokens achieves\ncomparable performance to AWQ using 16x512 tokens,\ndemonstrating its superior efficiency in utilizing limited cal-\nibration data. This superior performance is attributed to\nLDRA, which successfully determines an optimal dynamic\nrange even with limited calibration data."}, {"title": "Integration with Existing Methods", "content": "To demonstrate the versatility and potential synergies of\nDAQ, we conducted experiments combining our method\nwith AWQ, a state-of-the-art weight-only quantization\nmethod. DAQ employs the same inference process as vanilla\nRTN, focusing solely on optimizing quantization parame-\nters. This design choice enables DAQ to seamlessly integrate\nwith and enhance the performance of advanced quantiza-\ntion methods. We compared the performance of AWQ alone,\nDAQ alone, and the combined AWQ+DAQ using 4-bit NF\nquantization across various models on LLaMA-2."}, {"title": "Computational Complexity", "content": "The effectiveness of DAQ comes with certain computa-\ntional considerations during the quantization process. The\nprimary computational overhead stems from LDRA, which\ninvolves iterative optimization of quantization parameters.\nFor each weight group, LDRA performs T iterations of gra-\ndient descent, each requiring two forward passes through\nthe layer to compute the finite difference. Given N weight\ngroups in a single layer, this results in a time complexity\nof O(NTL), where L represents the time for a single layer\nforward pass. In practice, we observed that convergence is\ntypically achieved with a relatively small number of itera-\ntions (T\u2264 1000), keeping the overall computational cost\nmanageable even for large-scale models. Moreover, the de-\nsign of DAQ allows for parallel quantization across multiple\nlayers, which can significantly reduce the total quantization\ntime.\nIt is important to note that the additional computation\nrequired by DAQ is confined to the offline quantization\nprocess. During model inference, DAQ preserves the run-\ntime performance of the quantized model, introducing no\nadditional computational or storage overhead compared to\nvanilla RTN. This characteristic makes DAQ particularly\nsuitable for resource-constrained environments, where im-\nproved quantization performance is desired without compro-\nmising inference efficiency."}, {"title": "Conclusions", "content": "In this paper, we propose DAQ, a density-aware post-\ntraining weight-only quantization method. To leverage the\nnon-uniform properties of floating-point representation,\nDAQ takes both the density and impact of weights into con-\nsideration. Specifically, DCA is introduced to align high-\ndensity weight regions with FP high-precision regions.\nThen, LDRA is employed to further adjust the dynamic\nrange by optimizing the quantization parameters based on\nthe impact of weights on the model output. Comprehensive\nexperiments are conducted on LLaMA and LLaMA-2, and\nthe results demonstrate the superiority of DAQ over state-\nof-the-art methods across various model sizes, quantization\ngranularities, and calibration dataset sizes."}]}