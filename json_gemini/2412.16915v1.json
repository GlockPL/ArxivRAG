{"title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation", "authors": ["Tianyun Zhong", "Chao Liang", "Jianwen Jiang", "Gaojie Lin", "Jiaqi Yang", "Zhou Zhao"], "abstract": "Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage http://fadavatar.github.io.", "sections": [{"title": "1. Introduction", "content": "Talking avatar synthesis aims to animate a given portrait image using driven video or audio. Due to the ease of obtaining audio inputs and their low usage barrier, there has been a surge of recent work in this area. Based on the technical route, talking avatar synthesis methods can be categorized into GAN-based, NeRF-based, and the more recent diffusion model-based approaches. Diffusion model-based methods, in particular, have gained attention due to their strong generative capabilities. These models exhibit excellent robustness in input images and produce vivid and highly expressive videos that naturally align with the audio. However, the current diffusion model-based talking head generation methods suffer from slow inference speeds, which hinder their practical application. Methods based on Stable Diffusion (SD), all require multiple denoising steps. To ensure the correlation between the audio input and the image input in the final video, multi-CFG (Classifier-Free Guidance) inference is often employed, further increasing the runtime. Although most methods achieve 30-40 denoising steps with the help of DDIM, they still face speed challenges due to CFG calculations\nRecently, several works have explored diffusion distillation in text-to-image and text-to-video tasks to accelerate inference. Nonetheless, there has been no direct exploration of diffusion distillation for talking avatar synthesis tasks. Our observations indicate that simply applying text-to-image and text-to-video diffusion distillation methods results in a noticeable decline in performance. Unlike text to image or video tasks, talking avatar synthesis involves two key conditions: audio and a reference image. Since audio has a relatively weaker influence on video generation, the final output is jointly influenced by both conditions, necessitating careful parameter tuning in multi-CFG settings. The sensitivity to condition guidance often leads to severe artifacts when applying diffusion distillation methods with few-step inference.\nIn this paper, we propose FADA (FAst Diffusion Avatar"}, {"title": "2. Related Works", "content": "2.1. Audio-Driven Talking Avatar\nThe task of audio-driven talking avatars has gained increasing attention recently, propelled by advancements in video-generation technologies. GAN-based methods typically employ a two-stage pipeline involving an audio-to-motion module and a motion-to-video module. Sadtalker introduces a 3DMM-based motion representation along with a conditional VAE for generating high-fidelity talking heads. LivePortrait expands the training dataset incorporating images and videos, using implicit key-points for high-quality generation. NeRF-based methods explore animating talking heads in 3D neural spaces. Real3D-Portrait combines a pre-trained image-to-triplane module with a motion adapter for zero-shot and one-reference 3D talking head generation. MimicTalk captures the talking style of individuals through in-context-learning audio-to-motion modules and rapid fine-tuning of triplanes for mimic talking head generation.\nDiffusion-based talking avatar synthesis has swiftly become a crucial and efficient approach. EMO pioneers a double UNet framework in audio-driven avatar creation, yielding impressive and lifelike results. Concurrently, EchoMimic trains on audio and facial landmarks for versatile control inputs. Hallo incorporates a hierarchical audio-driven visual synthesis module to refine audio-visual alignments, while Hallo2 explores image augmentation strategies for 4K avatar video synthesis. Loopy introduces a motion-frame squeeze technique and audio-to-latent training for expressive results. Despite their progress in expressive audio-driven generation, diffusion-based methods still face challenges with slow inference speeds.\n2.2. Diffusion Model Acceleration\nDiffusion acceleration involves lowering costs for denoising runs and reducing the number of runs. Various denoising methods aim to cut costs, such as utilizing Model quantization to decrease the model parameters directly. ViDiT-Q introduced a new diffusion transformer quantization framework for video and image with mixed-precision quantization. DeepCache takes cues from video compression to propagate reusable result regions through the model's internal structure, while Faster Diffusion emphasizes re-using computed encoder features.\nReducing the number of denoising runs is a direct way to accelerate diffusion models. SD-Turbo leverages the ideas from GAN and introduces adversarial diffusion distillation to enhance the generation fidelity. SnapFusion proposes progressive distillation to achieve fewer steps by learning the previous student model progressively."}, {"title": "3. Methodology", "content": "In this section, we introduce our proposed method, FADA, a diffusion distillation framework for avatar synthesis, as illustrated in Figure 1. Section 3.1 provides an overview of the diffusion distillation framework, based on recent PeR-Flow. Section 3.2 delves into the specifics of the adaptive mixed-supervised distillation process. In Section 3.3, we present the multiple-CFG distillation technique with a learnable token implementation.\n3.1. Overall Framework and Preliminary\nBuilding upon cutting-edge diffusion-based methods for talking avatar synthesis, we introduce a dual-Unet architecture for the teacher model, leveraging pre-trained weights from Stable Diffusion 1.5. As depicted in Figure 1, the reference image and motion frames (i.e., images from the last clip) undergo processing by a fixed VAE and reference net. The output from each reference layer is then input to the corresponding layer in the denoising net for spatial attention. To facilitate audio-driven controls and ensure temporal consistency in talking avatar synthesis, audio attention and temporal attention layers are integrated into the denoising net. Initially following the Loopy design, we simplify the model by removing the TSM and Audio2Latents modules, streamlining subsequent model distillation. The student model mirrors the teacher's structure, with the addition of a CFG control layer, detailed in Section 3.3.\nWe choose the recent PeRFlow as the optimization target for distillation. Initially, we train the teacher model on carefully curated high-quality datasets that prioritize motion amplitude and audio-visual synchronization. The teacher model is optimized using the e-prediction DDPM loss, where it predicts noise $\\epsilon_{\\theta_{tea}}(\\cdot)$ from noisy latent $z_t$ at timestep $t$. The training objective can be succinctly described as:\n$\\mathcal{L}_{tea} = \\mathbb{E}_{z_t, c, t, \\epsilon} [||\\epsilon - \\epsilon_{\\theta_{tea}} (z_t, t, c; \\Theta_{tea})||^2]$\nWhere $c$ includes the audio feature, reference image and motion frames. Note that it is not necessary for the teacher model to be trained with flow-matching loss, and both e-prediction and v-prediction are valid for PeRFlow distillation. During the student distillation training, firstly same-interval $K$ time windows $\\{[t_k, t_{k-1})\\}_{k=1}^{K}$ are created where $t_k = k/K, k \\in \\{0, 1, ..., K\\}$. A time window $[t_k, t_{k-1})$ is randomly sampled out, and the standpoint $z_{t_k}$ can be derived from the marginal distribution of the ground-truth targets, where $z_{t_k} = \\sqrt{1 - \\sigma^2(t_k)}z_0 + \\sigma(t_k)\\epsilon$ and $\\sigma(t)$ denotes the noise schedule. Then the teacher-predicted ending point $z_{t_{k-1}}$ of this time window can be predicted and solved by ODE solver $\\Phi(\\cdot)$:\n$z_{t_{k-1}} = \\Phi(z_{t_k}, t_k, t_{k-1}; \\Theta_{tea})$\nWhere $\\Phi(\\cdot)$ denotes the DDIM solver which performs several inference steps iteratively to reach the endpoint of this time window. Note that the parameters of the teacher model are frozen and no gradient computations are needed here. After that, the input noisy latent of student $\\tilde{z_t}$ will be derived from a linear interpolation between starting point $t_k$ and ending point $t_{k-1}$:\n$\\tilde{z_t} = \\frac{t_{k-1} - (t - t_k)}{t_k - t_{k-1}} z_{t_k} + \\frac{t - t_k}{t_k - t_{k-1}} z_{t_{k-1}}$\nThrough the linear interpolation above, the ODE flow will be rectified to piecewise straight lines so only a few steps are enough to denoising in a time window, which reduces the number of timesteps at inference stage. Meanwhile, we can figure out the target noise $\\hat{\\epsilon}(t)$ via parameterization:\n$\\hat{\\epsilon}(t) = \\frac{z_{t_{k-1}} - A_k z_{t_k}}{N_k}$\nWhere $A_k = \\sqrt{\\alpha_{t_{k-1}}} / \\sqrt{\\alpha_{t_k}}$ and $N_k = \\sqrt{1 - A_k^2} = \\sqrt{1 - \\frac{\\alpha_{t_{k-1}}}{\\alpha_{t_k}}}, respectively. Finally, the training objective of the distillation process can be obtained as:\n$\\mathcal{L}_{distill} = \\mathbb{E}_{k, t \\in [t_k, t_{k-1}), z_t, c, \\epsilon} [|| \\hat{\\epsilon}(t) - \\epsilon_{\\theta_{stu}} (z_t, t, c; \\Theta_{stu}) ||^2]$\nWhere $\\epsilon_{\\theta_{stu}}(\\cdot)$ refers to the predicted noise from the student model and $\\mathcal{L}_{distill}$ represents the distillation loss. Note that $\\mathcal{L}_{distill}$ is only a trivial part of our framework, the mixed-supervised training and multi-CFG distillation will be proposed in Section 3.2 and 3.3.\n3.2. Adaptive Mixed-Supervised Distillation\nIn our experiments, we found that distillation significantly degrades the model's performance, especially overall video synthesis quality, as shown in Table 3. Increasing data can improve performance, but in audio-driven portrait generation, the weak control of audio over motion requires the"}, {"title": "3.3. Multi-CFG Distillation with Learnable Token", "content": "In audio-driven talking avatar synthesis tasks, the Multi-CFG technique is utilized to achieve robust control of multiple conditions (reference image and audio) effectively,\nSpecifically, the single CFG calculates the direction from unconditional predicted values to a certain conditional predicted value and then moves a certain distance from the unconditional predicted values to obtain conditional predicted values. Furthermore, the Multi-CFG recursively computes the next CFG based on the first CFG. In this paper, multi-CFG inference result noise $\\epsilon_{cfg}$ can be derived as follows:\n$\\hat{\\epsilon}_{cfg} = c_{fga} \\times (\\hat{\\epsilon}_a - \\hat{\\epsilon}_r) + c_{fgr} \\times (\\hat{\\epsilon}_r - \\hat{\\epsilon}_b) + \\hat{\\epsilon}_b$\nWhere $c_{fga}$ and $c_{fgr}$ indicate the CFG guidance scale of audio and reference condition. $\\hat{\\epsilon}_a$ refers to the noise prediction with both audio and reference conditions, while $\\hat{\\epsilon}_r$ removes the audio condition and $\\hat{\\epsilon}_b$ lacks audio and reference condition. Therefore, during inference, the model needs to run three times for each denoising step, which is time-consuming.\nIt naturally comes to mind to inject the CFG guidance scale as a condition into the network, and then let the student model learn the CFG reasoning characteristics of the teacher model during the distillation process, which echoes a similar concept to the w-condition. Specifically, the teacher model will conduct complete CFG reasoning during training to obtain predictions $\\hat{\\epsilon}_{cfg}$ controlled by CFG. Subsequently, these predictions are further integrated into Formulas 2, 3 and 4 to derive input noisy latent $\\hat{z_t}$ and target noise $\\hat{\\epsilon}_{cfg}(t)$ controlled by CFG, eventually undergoing a similar distillation procedure as Formula 5.\nHowever, directly adding the CFG value as a condition to the network does not yield satisfactory results. A careful design is needed for the injection method of the CFG scale condition. By observing the calculation of Multi-CFG reasoning in Formula 9, it is evident that this is akin to performing linear transformations on three independent vectors $\\hat{\\epsilon}_b$, $\\hat{\\epsilon}_a$, and $\\hat{\\epsilon}_r$ in the noise space using the given CFG strengths $c_{fga}$ and $c_{fgr}$. Meanwhile, $\\hat{\\epsilon}_r$ serves as a crucial link between the $\\hat{\\epsilon}_b$ and $\\hat{\\epsilon}_a$. Simply embedding the CFG values overlooks the aforementioned relationships. Therefore, we introduce learnable tokens $Y_b, Y_r$, and $Y_r$ into the model to learn the features required for CFG strength control and help the student model mimic the multi-CFG process. To achieve this, we design the CFG embedding $\\mathcal{Emb}_{cfg}$ of the CFG guidance scale $c_{fga}$ and $c_{fgr}$, as follows:\n$\\mathcal{Emb}_{cfg} = c_{fga} \\times (Y_a - Y_r) + c_{fgr} \\times (Y_r - Y_b) + \\gamma_b$\nAfter obtaining the CFG embedding $\\mathcal{Emb}_{cfg}$, we similarly introduce a CFG layer after each audio layer in the denoising network, injecting the information from $\\mathcal{Emb}_{cfg}$ into the denoising process through cross attention. Importantly, the CFG layer does not introduce excessive performance overhead, especially when compared to the original cost of running three CFG runs. Therefore, our proposed multi-CFG distillation with learnable tokens can achieve nearly three times faster operation speed."}, {"title": "4. Experiments", "content": "In this section, we will introduce the experimental results. In Section 4.1, we will present the details of the implementation of FADA and the meta-information of our training and testing datasets. In Section 4.2, we will compare our proposed methods with other state-of-the-art diffusion-based talking avatar methods via quantitative and qualitative comparison. In Section 4.3, we will analyze our proposed techniques including the basic distillation method, adaptive mixed-supervised distillation and multi-CFG distillation, and prove their effectiveness.\n4.1. Experimental Settings\nDatasets Regarding the training dataset, we primarily used speaker videos obtained from the internet, sliced and cropped to durations ranging from two to ten seconds with facial frames, and resized to 512\u00d7512 resolution. We designed multi-dimensional filters for this dataset, including filters for data source, image quality, audio-visual synchronization, background stability and degree of movement,"}, {"title": "5. Conclusion", "content": "In summary, we proposed FADA, a fast diffusion avatar synthesis framework with mixed-supervised multi-CFG distillation. By designing a mixed-supervised loss, we leverage data of varying quality to enhance the robustness of generated results. Furthermore, a learnable token-based multi-CFG condition design is introduced to maintain the correlation between the audio and the generated video in the distilled models. The quantitative and qualitative experiments across multiple datasets show that our balanced setting achieves fast inference and high-fidelity generation ability. Ablation studies prove our proposed methods are effective in FADA."}]}