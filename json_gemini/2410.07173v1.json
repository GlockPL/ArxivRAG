{"title": "DO BETTER LANGUAGE MODELS HAVE CRISPER VISION?", "authors": ["Jona Ruthardt", "Gertjan J. Burghouts", "Serge Belongie", "Yuki M. Asano"], "abstract": "How well do text-only Large Language Models (LLMs) grasp the visual world? As LLMs are increasingly used in computer vision, addressing this question becomes both fundamental and pertinent. However, existing studies have primarily focused on limited scenarios, such as their ability to generate visual content or cluster multimodal data. To this end, we propose the Visual Text Representation Benchmark (ViTeRB) to isolate key properties that make language models well-aligned with the visual world. With this, we identify large-scale decoder-based LLMs as ideal candidates for representing text in vision-centric contexts, counter to the current practice of utilizing text encoders. Building on these findings, we propose ShareLock, an ultra-lightweight CLIP-like model. By leveraging pre-computable frozen features from strong vision and language models, ShareLock achieves an impressive 51% accuracy on ImageNet despite utilizing just 563k image-caption pairs. Moreover, training requires only 1 GPU hour (or 10 hours including the precomputation of features) \u2013 orders of magnitude less than prior methods. Code will be released.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are solely pretrained on unimodal textual data, yet they are increasingly incorporated into systems that perceive and interact with the natural world (Ahn et al., 2022; Driess et al., 2023; Wayve, 2023). The lack of direct sensory experience raises fundamental questions to which extent such models can develop a meaningful and accurate understanding of visual reality. Do these models merely regurgitate visually relevant factual knowledge from their training corpus, or do they form internal representations that correspond to real-world phenomena? Despite the successful integration of LLMs into large-scale Vision-Language Models (VLMs), it is difficult to judge the visual capabilities already inherent to LLMs this way. This is not only because of widely varying training recipes and proprietary data sources but particularly due to fine-tuning with paired image-text data, which dilutes any visual knowledge already contained in text-only models.\nIn contrast, Sharma et al. (2024) and Huh et al. (2024) more immediately assess the visual nature of LLMs and highlight a non-trivial degree of visual understanding and cross-modal alignment. These works do this by compiling proxy tasks or measures such as generating code to represent visual concepts (Sharma et al., 2024) or correlating visual with language-based representations (Huh et al., 2024). However, the reliance on highly constrained and synthetic tasks with limited practical significance fails to gauge the aptitude of LLMs when deployed in more realistic settings.\nTo this end, we propose the Visual Text Representation Benchmark (ViTeRB), a novel benchmark that directly measures performance on the downstream task of zero-shot open-vocabulary image classification, as popularised by CLIP (Radford et al., 2021). This enables us to quantify the visual understanding of language models and their ability to encode text for vision-centric tasks. To prevent concept leakage during the training stage a significant factor underlying the robust \"zero-shot\u201d performance of many VLMs (Fang et al., 2022; Udandarao et al., 2024; Parashar et al., 2024) we revert to the traditional notion of zero-shot learning (ZSL) where seen and unseen concepts can be strictly delineated and are disjoint (cf. (Lampert et al., 2009)). With the advent of VLMs like CLIP (Radford et al., 2021), these formerly strict assumptions have been watered down in favor of scaling to large volumes of web data that likely contain most but the rarest entities and objects. Consequently, by enforcing a clear training and evaluation protocol, we can accurately assess the true generalization capabilities facilitated by the language embeddings."}, {"title": "2 RELATED WORK", "content": "Visual understanding of large language models. Many previous works (Liu et al., 2023; Wang et al., 2023; Li et al., 2023) enable LLMs to interact with visual information by mapping image features into the token embedding space of the language model, an approach that requires extensive alignment on multi-modal corpora. However, LLMs can also infer and reason about visual content without explicit multi-modal training (Bowman, 2023). By transcribing images into text form using separate VLMs, LLMs can be naturally interfaced via language (Hakimov & Schlangen, 2023). Sharma et al. (2024) tasked LLMs to draw common objects and scenes using simple shapes, indicating present spatial understanding and illustrating that LLMs can conceptualize real-world settings. Various works highlight the plausibility and utility of LLM-generated descriptions of objects in the context of image classification and demonstrate that LLMs possess encyclopedic knowledge about visual characteristics (Pratt et al., 2022; Menon & Vondrick, 2023; Yang et al., 2022; Saha et al., 2024). These capabilities suggest that the extensive pretraining on large volumes of diverse textual data aids the visual understanding of LLMs. Prompted by correlations between semantic representations in the language and vision space, Huh et al. (2024) argue that the embedding spaces of neural networks converge towards a shared representation of reality irrespective of the concrete optimization objectives, model architectures, and data utilized during training. Similarly, we investigate the degree of visual alignment inherent to exclusively language-based representations but assess this in the practically more relevant context of zero-shot image classification and design a rigorous benchmark to measure the true generalization capabilities facilitated by language embeddings.\nData-efficient CLIP-like models. Prevailing VLMs heavily rely on large-scale corpora. While the original CLIP (Radford et al., 2021) was trained on 400M image-text pairs, ALIGN (Jia et al.) forwent extensive data cleansing, utilizing a total of 1.8B samples and showing that the noisiness of web-scraped data can be offset through scale. However, more recent work suggests improving the data quality rather than quantity as the more promising alley towards better performance, and a litany of filtration methods has been proposed as a result (Schuhmann et al., 2021; Mahmoud et al., 2024; Joshi et al., 2024; Yu et al., 2023). Such investigations aim at identifying data subsets that effectively facilitate generalization while keeping the training recipes fixed (Gadre et al., 2023). Additionally, advances have been made in the model architecture and training regime to improve data and computational efficiency. It has been shown that even smaller language encoders with notably fewer layers can perform similarly to more expressive language models (Cui et al., 2022). Zhai et al. (2022) leverage representations of pretrained image encoders and only tune the text encoder. LilT (Khan & Fu, 2023) takes this further by exploiting pretrained encoders for both vision and language modalities. However, only a small subset of parameters and additional alignment modules are unlocked and optimized instead of full fine-tuning. Keeping both pretrained encoders entirely frozen, ASIF (Norelli et al., 2023) aligns their representations in a training-free manner with only a few million image-text pairs. Compared to these works, our approach focuses on maximizing the utility of existing unimodal models by aligning them with minimal compute and limited paired data."}, {"title": "3 ShareLock: SHARED VISION-LANGUAGE-LOCKED TUNING", "content": "Inspired by the efficiency and effectiveness of late fusion architectures in CLIP-like models, Share- Lock comprises two separate encoders for vision and language inputs. The outputs of either encoder $() are subsequently mapped into a shared d-dimensional latent space through a projection p(). The latent representation for a given input image $x_i$ or caption $t_i$ is therefore computed by $z_{img} = p_{img}(\\phi_{img}(x_i)) \\in R^d$ and $z_{txt} = p_{txt}(\\phi_{txt}(t_i)) \\in R^d$, respectively. Due to the normalization following the projection, the cosine similarity between two embeddings $z_i$ and $z_j$ is given by their dot product (i.e., $sim(z_i, z_j) = (z_i, z_j)$). During training, the contrastive loss encourages the model to maximize the similarity between embeddings of correct image-caption pairings while decreasing the similarity of non-corresponding pairs. For an image-caption pair i in a batch with N items, it is given by\n$\\mathcal{L}(i) = - log \\frac{exp \\left(sim(z_{m}, z_i) /\\tau \\right)}{\\sum_{j=1}^N exp(sim(z_{m}, z_j) / \\tau)}$       (1)\nfor both alternated modalities pairings (m, n) \u2208 {(txt, img), (img, txt)} and with \u03c4 being a fixed temperature parameter. Given a set of classes C and their corresponding textual class representations"}, {"title": "4 VISUAL TEXT REPRESENTATION BENCHMARK", "content": "The objective of our proposed visual alignment benchmark ViTeRB is to assess how language models facilitate generalization to novel concepts. It retains the model architecture and optimization objectives of ShareLock but places restrictions on the data akin to traditional ZSL approaches (cf. Lampert et al. (2009)). To rigorously attest to the true generalization performance without being affected by concept leakage through supervision with arbitrary image-caption pairs, we split conventional image classification datasets into sets of seen classes S as well as unseen classes U, ensuring that $S \\cap U = \\emptyset$. To provide coverage across natural and human artifacts (e.g., aircrafts and animals), coarse and fine-grained categories (e.g., zebra vs. dolphin and fish crow vs. American crow), and different scales (40 \u2264 |S| \u2264 1000), the reported scores are averaged per-class accuracies over U across four datasets. Namely, AWA2 (Xian et al., 2017), CUB (Wah et al., 2011), FGVCAircraft (Maji et al., 2013), and ImageNet+are selected for their complementary characteristics. ImageNet defines the ImageNet-1k classes as seen concepts and treats the 500 most populated classes (i.e., highest number of training samples) of ImageNet-21k as unseen ones. For AWA2 and CUB, we utilize the splits proposed by Xian et al. (2017) while randomly assigning aircraft types into 50 seen and 20 unseen classes.\nAs the classification performance on unseen classes is primarily contingent on the validity and semantic continuity of the class representation, the proposed setup can assess the visual alignment of language embeddings. In the absence of image-specific captions, text-based class representations f(yi) are used as supervision signals during training and for zero-shot transfer during inference. Besides the template-based targets proposed by Radford et al. (2021) that solely substitute the respective class names, we generate more comprehensive auxiliary information about classes (e.g., visual descriptions) using the instruction-tuned version of the Llama-3 8B model and acquire human-curated information from Wikipedia (details provided in A.2)."}, {"title": "5 LANGUAGE MODELS FOR VISUAL ZERO-SHOT GENERALIZATION", "content": "Utilizing the Visual Text Representation Benchmark(ViTeRB), we investigate the impact of specific design choices to identify critical factors that promote generalization and inform subsequent decisions when building a locked-image-locked-text model.\nLLMs are comprehensive repositories for real-world knowledge. While simple templates have proven effective classification targets on large-scale CLIP-like models (Radford et al., 2021; LAION AI, 2022), training with conventional image classification datasets opens up the possibility of using alternative semantic class representations as well. Especially the class-wise supervision combined with limited diversity and number of concepts can impede vision-language alignment. Therefore,"}, {"title": "6 IMAGE-CAPTION PRETRAINING EXPERIMENTS", "content": "The previous section has provided us with prerequisite insights to propose ShareLock and motivated the choice to leverage the strong visually aligned representations of LLMs in the context of a CLIP-like model. Forgoing the strict zero-shot setup and moving toward larger-scale image-caption datasets, we intend to explore how well these observations translate in the context of a general- purpose VLM and whether only optimizing a lightweight network on top of frozen features is sufficient to compete with full pretraining or fine-tuning. This analysis will illustrate the current upper limits of utilizing unimodal foundation models as building blocks while applying minimal additional compute and multi-modal data to achieve high-performing VLMs."}, {"title": "6.1 EXPERIMENTAL SETUP", "content": "Pretrained Vision and Language Models. Given its strong performance, broad pretraining regime, and popularity, the ViT-B/14 variant of the DINOv2 model family (Oquab et al., 2023) is used as the default vision backbone unless noted otherwise. Language features are extracted from a Llama-3 8B LLM through last token pooling, as shown in Figure 3. For LiT baselines, we initialize the language encoder with pretrained BERT weights (Devlin et al., 2019), in accordance with the original implementation (Zhai et al., 2022). When comparing LiT, ASIF, and ShareLock models in the following, the exact same pre-computed input features (barring the language component of LiT).\nProjection Networks. As in Zhai et al. (2022), no transformations are applied to the vision features. The MLP projection network after the language model comprises four layers. Between consecutive layers, inputs are normalized via Batch Normalization (Ioffe & Szegedy, 2015) and fed into a ReLU non-linearity. Dropout (Srivastava et al., 2014) with p = 0.2 is applied during training. We have also explored more sophisticated projection networks, but found the MLP to overall provide best performances, see details and ablations in Appendix A.3.\nDatasets. Our investigation focuses on minimizing the amount of paired data required and explores how unimodal embeddings can drive robust multimodal performance with minimal supervision and alignment. As a result, our evaluation is limited to comparably small paired datasets. COCO Captions. Containing human annotations for around 80k images, COCO Captions (Chen et al., 2015) is a small but high-quality multimodal dataset. As multiple captions per image are available, a random caption is sampled during each iteration. CC3M. The Conceptual Captions dataset (Sharma et al., 2018) was built by scraping image-alt-text pairs from websites, applying filters to remove noisy or mismatched data. Due to expired links, our version of CC3M contains around 2.8M image-text pairs. We also use a smaller subset filtered for more balanced concept coverage for the LLaVA VLM (Liu et al., 2023)."}, {"title": "6.2 COMPARISON TO STATE-OF-THE-ART", "content": "Comparison to prior works on IN-1k. Taking the ImageNet-1k zero-shot classification performance as the principal benchmark for model performance, ShareLock clearly outperforms other models trained with similar amounts of data, as demonstrated in Table 3. Compared to the small-scale CC3M CLIP model (Fan et al., 2023), ShareLock performs notably better, achieving an accuracy 52.1% vs. 16.0%. Adding LLM-based features further proves effective when considering the 44.1% accuracy of LiT (Zhai et al., 2022), which utilizes the same vision backbone as ShareLock. Our optimization-based alignment also consistently outperforms the training-free ASIF (Norelli et al., 2023) method, which relies on a large reference dataset with diverse concepts covered for performance. As the dataset size increases, fine-tuning encoders becomes more feasible. Yet, ShareLock still maintains performance gains of 3% - 18% to LiT and CLIP even for CC12M.\nRobustness. To evaluate the robustness of the VLMs under distribution shifts, the ImageNet-1k classification objective is repeated with visual out-of-distribution inputs. As seen in Table 3, columns 'IN-v2', 'IN-R', etc., ShareLock still compares favorably to previous approaches. On average, it surpasses other models trained with datasets comparable in scale and approaches vanilla CLIP models trained with orders of magnitude more data (8.5M vs. 400M for the original CLIP).\nFine-grained classification. As shown in Table 4, the strong unimodal features of ShareLock similarly contribute positively to fine-grained problems. Here, ShareLock outperforms CLIP, LIT, and DataComp models by large margins on 8/12 evaluations. We also find that on these datasets, the models benefit much more noticeably from increased data scale, e.g. from 10.6% on Flowers to 48.8% when increasing the dataset 100\u00d7. Intuitively, exposure to a more diverse and nuanced set of concepts makes methods more capable of performing fine-grained classification. Nonetheless, the"}, {"title": "6.3 QUALITATIVE RESULTS", "content": "In addition to the extensive suite of quantitative evaluations, we present several qualitative results in Figure 5 to illustrate the effectiveness of our method. Across diverse textual prompts, our approach demonstrates strong alignment between textual and visual representations. Compared to versions of CLIP and LiT also trained on CC3M, we find that ShareLock generally performs better for fine-grained (i.e., \"a photo of a BMW.\") and more abstract (i.e., \"[...] heavy seas.\") prompts."}, {"title": "7 CONCLUSION", "content": "We introduce ViTeRB, a benchmark for evaluating the visual capabilities and alignment of language models. With it, we show that LLM quality, measured by MMLU, correlates with visual understanding, and decoder-based LLMs excel in extracting visually informed representations. Building on these insights, we propose ShareLock, a simple CLIP-like VLM that leverages the large-scale pretraining and internal knowledge of frozen LLMs. Our method achieves strong performances and requires fewer image-caption pairs than models like CLIP or LiT for similar performances. Combined with its extremely fast training time, this work highlights the potential of frozen decoder-only LLMs for vision-language tasks."}, {"title": "A APPENDIX", "content": "A.1 REPRODUCIBILITY STATEMENT\nWe acknowledge and emphasize the importance of reproducibility in our work and take active measures to facilitate reproducibility efforts. Besides providing comprehensive documentation of our methods throughout the main paper, with additional details in the supplementary materials, we will publish source code for the proposed ShareLock model.\nA.2 TEXTUAL CLASS REPRESENTATIONS FOR VISUAL TEXT REPRESENTATION\nBENCHMARK\nMore details about the characteristics and acquisition of these class representations are provided in Section A.2 of the appendix. Besides the template-based targets proposed by Radford et al. (2021) that solely substitute the respective class names, we generate more comprehensive auxiliary information about classes (e.g., visual descriptions) using the instruction-tuned version of the Llama- 3-8B model and acquire human-curated information from Wikipedia (details provided in A.2).\nClass representations are essential for facilitating the knowledge transfer between classes in the traditional definition of zero-shot learning. Compared to attributes or other forms of class semantics, language-based class representations are more conveniently accessible at various scales and may come in diverse manifestations. The advent of LLMs adds further possibilities for generating and obtaining such auxiliary information. The following paragraphs specify the respective properties and acquisition process. Here, all LLM-based class representations are generated using the instruction- tuned version of LLama-3 8B.\nClass Names. A set of 80 human-engineered prompt templates in the style of \"a photo of a <class name>\" are adopted from Radford et al. (2021).\nDescription. This type of class representation is generated by tasking an LLM to generate short, one-sentence descriptions of how a given class looks like. Multiple descriptions are generated for each class by slightly varying the LLM prompt and utilizing different seeds as a form of augmentation.\nWikipedia Page. Being a comprehensive and mostly factually correct source of information, Wikipedia constitutes an interesting source of auxiliary information in the context of zero-shot classification. To obtain class-article correspondences, class names are automatically matched with page names, after which additional manual quality checks are performed. Nonetheless, an ideal match does not always exist due to high class specificity or generality, in which case superordinate articles are considered or template-based fallbacks are employed.\nLLM-based Wikipedia Style Articles. Despite being specifically prompted for articles mimick- ing Wikipedia, the Llama-3-generated texts tend to show significant differences in style compared to their real counterparts.\nAs the lengthy nature of Wikipedia(-style) articles might dilute the information content captured by the language embeddings, the texts are split into individual sentences, which are used as targets during training. For all types of class representations, predictions are made by aggregating class scores through averaging over all individual class-specific texts.\nA.3 PROJECTION NETWORK ARCHITECTURE\nThe multi-layer perceptron (MLP) projection networks of ShareLock as introduced in Section 3 are conceivably simple. As these are the only unfrozen and tunable parts of the model architecture and thus responsible for aligning vision and language inputs, they are of particular significance to aptly process and transform the inputs. Following Zhai et al. (2022), no transformation to the vision inputs is applied for any of the architectures. With a hidden size of 4096 and four layers, the MLP processing the language features comprises approximately 53M parameters."}, {"title": "A.4 SUPPLEMENTARY QUANTITATIVE RESULTS", "content": "The following tables include additional results and analyses that were omitted in the main body of the paper due to space constraints. These supplementary results offer extended insights from additional model variants and further buttress previously drawn conclusions."}]}