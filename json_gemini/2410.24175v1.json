{"title": "Constraint Back-translation Improves Complex Instruction Following of Large Language Models", "authors": ["Yunjia Qi", "Hao Peng", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "abstract": "Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have achieved remarkable performance in numerous natural language processing tasks (Zhao et al., 2023; OpenAI, 2024; Yang et al., 2024; Dubey et al., 2024; Team et al., 2024). However, they still fall short in following instructions with complex constraints (Zhou et al., 2023; Jiang et al., 2024; Qin et al., 2024), such as length constraints shown in Figure 1, which limits their effectiveness and usability.\nTo enhance the instruction-following ability of LLMs, the standard practice is to post-train the targeted LLM on a large set of instruction-response data pairs. For the complex instruction-following with multiple constraints, existing efforts (Sun et al., 2024; He et al., 2024) synthesize complex datasets by adding multiple constraints to existing instructions and generating responses with advanced LLMs like GPT-4 (OpenAI, 2024). While this data generation pipeline is straightforward and widely adopted, even the most capable LLMs cannot follow complex instructions well (Jiang et al., 2024; Qin et al., 2024), which limits the quality of generated data and necessites laborious filtering. The status quo urges the development of automatic data generation methods relying less on existing LLMs' complex instruction-following abilities.\nOur key observation is that existing datasets inherently include implicit complex constraints so that we can reuse the widely-available high-quality instruction-following datasets (Xu et al., 2023;\nTaori et al., 2023; Mukherjee et al., 2023; K\u00f6pf et al., 2024) to synthesize complex instruction-response pairs. As shown in Figure 1, although the original concise instruction does not explicitly specify constraints like writing style or length, the response already satisfies some constraints in multiple dimensions. Therefore, we can efficiently create high-quality complex instruction-response pairs from existing datasets by generating constraints from responses and adding them to instructions. We dub this data generation method as constraint back-translation. It only requires discovering the constraints already met by responses rather than following the complex instructions with multiple constraints, which significantly reduces requirements for model capability. As a result, it is both cost-effective and capable of producing high-quality data with limited noise. We also find that constraint back-translation can serve as a useful auxiliary training objective in post-training, dubbed as the reverse training technique. Specifically, we use instructions and responses as inputs to train the model to output constraints in post-training. The intuition is that reverse training may enhance the model's understanding of constraints and improve its efficacy (Golovneva et al., 2024).\nWe adopt Llama3-70B-Instruct (Dubey et al., 2024) to back-translate constraints from a collection of existing data, generating a large-scale complex instruction-following dataset, named CRAB. Specifically, we sample a total of 13, 500 instances from existing high-quality instruction-following datasets (Peng et al., 2023; Es, 2023; Xu et al., 2023; K\u00f6pf et al., 2024) as the seed data, and manually define a scope of common constraints. We then use the original instruction, response, and constraint scope as inputs to Llama3-70B-Instruct to generate the corresponding implicitly satisfied constraints. Following previous works (Sun et al., 2024; He et al., 2024), we train the LLMs using the mixture of CRAB and ShareGPT dataset (Chiang et al., 2023), and we jointly adopt standard supervised fine-tuning and reverse-training on CRAB. In the experiments, we select the capable open-source LLMs Llama3 8B (Dubey et al., 2024) and Mistral 7B (Jiang et al., 2023) as backbone models and evaluate the complex instruction-following abilities of our models against various baselines on IFEval (Zhou et al., 2023) and FollowBench (Jiang et al., 2024). The results demonstrate that training on CRAB significantly enhances LLM performance in complex instruction following. We also conduct"}, {"title": "2 Method", "content": "This section introduces the construction process of CRAB (\u00a7 2.1) and the training method (\u00a7 2.2).\n2.1 Constructing CRAB\nWe begin by introducing the notions. Given an instruction $x$, which typically defines a specific task, such as \"Write a blog on French cuisine\u201d, a set of constraints $c$, which specify conditions for the response, such as length restrictions, and a response $y$ that satisfies both the constraints $c$ and the instruction $x$, our goal is to construct a high-quality dataset of $(x, c, y)$ triples. We first collect a set of high-quality $(x, y)$ pairs from existing datasets and then apply constraint back-translation to generate the constraints $c$ for each $(x, y)$ pair. The data construction process is illustrated in Figure 2, which consists of three steps: data collection, constraint back-translation, and constraint combination. In the data collection process, we collect a comprehensive set of high-quality $(x, y)$ pairs from existing datasets. We then back-translate the corresponding $c$ for each $(x, y)$ using Llama3-70B-Instruct and Python scripts automatically. Finally, we perform"}, {"title": "3 Experiments", "content": "In this section, we introduce the experimental setup (\u00a7 3.1), experimental results (\u00a7 3.2), and further analyses on our model (\u00a7\u00a7 3.3 to 3.5).\n3.1 Experimental Setup\nBackbone Models We adopt two widely-used open-source base models, Mistral 7B (Jiang et al., 2023) and Llama 3 8B (Dubey et al., 2024), as our backbone models for developing Llama3CRAB and MistralCRAB. Specifically, we employ Mistral-7B-v0.3 and Meta-Llama-3-8B, downloaded from Hugging Face (Wolf et al., 2019).\nDuring the SFT stage, we adopt a $5 \\times 10^{-6}$ learning rate, 256 batch size, and train the Mistral for 4 epochs and Llama 3 for 3 epochs. During the DPO optimization stage, we adopt $5 \\times 10^{-7}$ learning rate, 64 batch size, and 1 training epoch.\nBaselines Our baselines include popular open-source and proprietary LLMs, divided into three main categories for comparison: (1) Proprietary LLMs, including GPT-3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2024). (2) General instruction-tuning LLMs, including Vicuna-V1.5 13B (Chiang et al., 2023), trained on the 125k ShareGPT dataset, WizardLM-V1.2 13B (Xu et al., 2023), trained on the 196k Evol-Instruct dataset, and Zephyr beta 7B (Tunstall et al., 2023), trained with the Ultra-Feedback (Cui et al., 2023) dataset using the DPO objective (Rafailov et al., 2023), which achieves leading performance on chat benchmarks based on Mistral 7B. (3) Models specifically optimized for complex instruction-following tasks. We use the Conifer series (Sun et al., 2024), which first generates constraints based on seed instructions and then generates corresponding responses based on instructions and constraints using GPT-4, and achieves the state-of-the-art performance in complex instruction-following at the 7B model scale.\nEvaluation Datasets We use two widely-used and challenging complex instruction-following datasets IFEval (Zhou et al., 2023) and FollowBench (Jiang et al., 2024) for evaluation. IFEval consists of 541 instructions that can be automatically validated using Python scripts. Each instruction contains 1 to 3 constraints, primarily focusing on strict lexical and formatting constraints. FollowBench is a fine-grained, multi-constraint instruction-following benchmark and it categorizes the difficulty into five levels (L1 to L5) based on the number of constraints of an instruction, where L1 represents the simplest level with only one constraint, while L5 is the most difficult, with a combination of five constraints. It also includes five constraint categories, including content, situation, style, format, and example, along with a mixed constraint category that combines various categories of constraints. FollowBench contains a total of 820 instructions across more than 50 different NLP tasks, and it is automatically evaluated using either Python scripts or GPT-4. Please refer to the original paper for more details (Jiang et al., 2024)."}, {"title": "3.2 Experimental Results", "content": "The experimental results are presented in Table 1. Our observations are as follows: (1) After training on the CRAB dataset, our models significantly outperform the corresponding base models and the open-source models trained through SFT on general instruction-following datasets. Our DPO version of models achieves the best performance among the compared models. It demonstrates the effectiveness of our data and training approach. (2) Our models surpass Conifer Sun et al. (2024), which is specifically trained for complex instruction-following, on IFEval. It suggests that our model performs better in following lexical and format constraints. However, our models slightly lag behind Conifer on FollowBench. We provide an in-depth discussion on the performance across"}, {"title": "3.3 Analysis on General Instruction Following", "content": "The complex instruction-following ability not only involves following complex constraints but also encompasses the basic ability to follow instructions themselves, e.g., \u201cWrite a blog on French cuisine\", named as general instruction following. In this section, we further evaluate our model's general instruction-following capability. Given that IFEval and FollowBench primarily focus on evaluating the ability to follow constraints, we adopt another widely-used dataset, AlpacaEval (Li et al., 2023b), which serves as an easy-to-use and high-quality automatic evaluator for instruction-following ability. Specifically, we use AlpacaEval 2.0, which contains 805 instructions, and use gpt-4-1106-preview as the evaluator to get the final weighted win rate. The evaluation results are presented in Table 2, where the \u201cLC WinRate\""}, {"title": "3.4 Ablation Study", "content": "We conduct an ablation study to analyze the key factors influencing model performance. Specifically, we investigate three key factors in developing our model: reverse training, forward training, i.e., standard supervised fine-tuning, and in-context demonstrations. We exclude each factor and keep all other conditions identical, to the model separately. When excluding reverse and forward training, we set the loss ratio \u03b1 in \u00a7 2.2 to 1 and 0, respectively. The backbone model is Mistral. The results are presented in Table 3, where L1-L2 in Follow Bench represent simpler constraints and L3-L5 denote more complex constraints. We can observe that removing any of these factors leads to a decline in model performance, which demonstrates the effectiveness of these factors in developing our model. For more complex constraints following, adding in-context demonstrations during training is effective, as excluding in-context demonstrations leads to a significant performance drop in L3-L5. The reason may be that in-context demonstrations enhance the model's ability to understand multiple in-context instructions and complex constraints.\nWe further compare with a competitive baseline model, InstBackTSFT, which is trained on the data generated by instruction back-translation (Li et al., 2024). The key difference between instruction and constraint back-translation is that the former uses advanced LLMs to generate both instructions and constraints from responses, while the latter focuses"}, {"title": "3.5 Analysis on Constraint Category", "content": "We further investigate our model's performance across different constraint categories to analyze its strengths and potential limitations. Specifically, we analyze the results on FollowBench, which includes five categories of constraints, including example, content, situation, style, and format. Please refer to the original paper (Jiang et al., 2024) for the detailed definitions for each constraint category. FollowBench also includes a mixed category which is designed for simulating real-world scenarios (Jiang et al., 2024), where various types of constraints are combined to form the final constraint. We compare our model MistralCrab with the Conifer model, which is trained on the data generated using the standard pipeline: generating the constraints first and then generating the response based on the instruction and constraints. The results on different constraint categories of FollowBench are shown in Figure 5. We can observe that our model significantly outperforms Conifer on the mixed constraint, which represents real-world scenarios, suggesting that our model is more effective in handling complex instruction-following scenarios. However, in the style constraint category, e.g., \u201cWrite in the style of Shakespeare\", our model per-"}, {"title": "4 Related Work", "content": "4.1 Instruction Following\nInstruction following involves following user intentions to generate helpful responses, which is fundamental to modern LLMs (Zhang et al., 2023). Ouyang et al. (2022) first propose the practice of aligning LLMs to follow human instructions, using SFT and RLHF to train models, which is the key factor in the success of ChatGPT (OpenAI, 2022). Subsequently, numerous studies focus on enhancing the instruction-following capabilities of LLMs, particularly for open-source models, which can be summarized in two main aspects: (1) data-driven approaches, which design an automated pipeline or use human annotation to produce high-quality training data (Xu et al., 2023; Taori et al., 2023; Chiang et al., 2023; Peng et al., 2023; Kim et al., 2023; Cui et al., 2023; Mukherjee et al., 2023; Ivison et al., 2023; K\u00f6pf et al., 2024; Qi et al., 2024; Liu et al., 2024; Li et al., 2024; Bai et al., 2024a). (2) new training methods, including novel objectives (Rafailov et al., 2023; Gallego, 2024; Zhou et al., 2024; Hejna and Sadigh, 2024; Meng et al., 2024) or training pipelines (Tunstall et al., 2023; Li et al., 2024; Yuan et al., 2024; Chen et al., 2024).\nA more challenging instruction following scenario is complex constrained instruction following, where the responses should further satisfy specific constraints, such as length. Previous studies have shown that LLMs struggle to follow these instructions (Jiang et al., 2024; Qin et al., 2024). Recent efforts focus on enhancing this ability by constructing high-quality training data (Sun et al., 2024; He et al., 2024). This process typically involves"}, {"title": "4.2 Back-translation", "content": "Back-translation is first proposed in the field of machine translation (Sennrich, 2015; Hoang et al., 2018), which mainly is used for data augmentation. It first trains a model to back-translate the target language into the source language, then uses this model to generate parallel training data from a large amount of monolingual target language data, which sufficiently saves human translation efforts. Considering its simplicity and efficacy, back-translation has also been widely applied to various tasks, such as style transfer (Prabhumoye et al., 2018; Toshevska and Gievska, 2021) and paraphrase generation (Wieting et al., 2017; Mallinson et al., 2017).\nRecently, several studies have explored applying back-translation to the field of large language models to efficiently generate high-quality data automatically (Li et al., 2023a; Pham et al., 2024; K\u00f6ksal et al., 2023). Li et al. (2023a) proposed reversing the training objective to automatically generate corresponding instructions for existing unsupervised corpora, while Pham et al. (2024) and K\u00f6ksal et al. (2023) leveraged the powerful general capabilities of LLMs to generate instructions from the corpus directly. These works usually focus on instruction back-translation, which generate instructions from existing text corpora. In this work, we propose constraint back-translation, which generates high-quality constraints based on instructions and responses. It can effectively generate constraints without altering the original responses, and reduces data noise and the cost of data generation."}, {"title": "5 Conclusion", "content": "In this paper, we aim to enhance large language models' capability for complex constrained instruction following. We propose a constraint back-translation data generation method, which can reduce data noise and generation costs, resulting in a high-quality complex instruction-following dataset CRAB. We also propose a reverse training method and develop Llama3CRAB and MistralCrab based on CRAB. Extensive experiments demonstrate the"}, {"title": "Limitations", "content": "As discussed in \u00a7 3.5, for certain types of constraints, such as style constraint, the constraints generated through constraint back-translation may lack sufficient diversity if the original response data itself is not diverse enough. We leave further improvements to constraint back-translation as future work. Another limitation of our study is that we do not use a larger base model due to computational constraints. We believe that using a larger base model could develop a more advanced LLM in following complex constraints, but this does not affect our overall experimental conclusions."}, {"title": "Ethical Considerations", "content": "We discuss potential ethical concerns related to this work: (1) Intellectual property. Our research leverages several widely used SFT datasets, and we strictly comply with their licensing terms. We will share CRAB under the CC BY-SA 4.0 license2. (2) Intended use and Potential risk control. The goal of this paper is to introduce CRAB, designed to enhance the performance of LLMs on complex instruction tasks. CRAB is built using widely available public datasets. We trust that the original publishers have anonymized and sanitized these datasets appropriately. Additionally, we randomly sampled 100 instances and found no sensitive information. (3) AI assistance. We used GPT-4 to paraphrase some sentences and check grammar."}, {"title": "Appendices", "content": "A Data Collection\nIn this section, we provide a detailed explanation of our data construction process, divided into two parts: the details of constraint construction (appendix A.1) and the data distribution of CRAB (appendix A.2).\nA.1 Details of Constraints Construction\nTable 4 presents all 19 types of constraints we defined. It is important to note that for the \"Define circumstances\", clarifying the subject or object, or defining the circumstances under which the instruction applies, we observed that generating this constraint independently often results in this additional constraint being too similar to the original instruction. Therefore, we integrate it directly with the original interaction to develop a refined instruction. If selected during the combination process, instead of being added to the instruction like other constraints, it replaces the original instruction. Since not all constraints are applicable to every output text (e.g., Special Output Format refers to the special format of the output, such as Python, tables, JSON, HTML, or LaTeX), we apply weighted sampling during the combination process to ensure a more balanced sampling of all constraints.\nAmong the constraints calculated using Python scripts, two categories are particularly unique: (1) Number-related categories: such as Length and Words Per Sentence, where we used NLTK (Loper and Bird, 2002) for calculation. (2) Keyword: We applied the lightweight, unsupervised keyword extraction method Yake (Campos et al., 2020) to extract the top 3 most significant keywords from the output text.\nA.2 Dataset Distrubution\nFigure 6 shows the distribution of 13, 500 instances in the CRAB. The left chart categorizes data by the number of constraints after combination, while the right chart categorizes data by the source dataset. To enhance data diversity during the combination stage, we randomly introduced 25% of data with a constraint count outside the 6\u20138 range, with the maximum number of constraints being 14.\nB Model Training\nFor model training, we utilize the repository 'The Alignment Handbook' (Tunstall et al., 2023) to"}, {"title": "C Details on the impact of constraints on output quality", "content": "To explore the impact of constraints on output quality, we sampled 100 instruction pairs from Follow-Bench and IFEval, with each pair consisting of a instruction without constraints and its corresponding multi-constraint version (with over 3 constraints). Since IFEval does not provide instruction without constraints, we randomly selected 50 instances and manually removed the constraints. For FollowBench, we selected level 0 instructions along"}]}