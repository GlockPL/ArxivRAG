{"title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "authors": ["Yifan Wang", "Sukrut Rao", "Ji-Ung Lee", "Mayank Jobanputra", "Vera Demberg"], "abstract": "Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural models. Meanwhile, B-cos networks have been introduced to improve model explainability through architectural and computational adaptations, but their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos networks empowered for NLP tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous B-cos methods. Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we provide practical guidelines for effectively building B-cos LMs based on our findings.", "sections": [{"title": "1 Introduction", "content": "Pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) have significantly advanced performance across a plethora of NLP tasks (Wang et al., 2018; Gao et al., 2023). However, their complex architectures and black-box nature make understanding their behavior a persistent challenge (Bommasani et al., 2021). To address this, research has increasingly focused on explaining model predictions, particularly in relation to the input. These input-based explanations, often referred to as local explanations or rationales, aim to reveal how specific inputs influence a model's predictions (Arras et al., 2019; Atanasova et al., 2020; Lyu et al., 2024).\nMost explanation methods for neural models are post-hoc, meaning that they attempt to explain a model's behavior only after it has been trained and deployed (Sundararajan et al., 2017; Ribeiro et al., 2016). While these methods are widely used and easy to apply, they have been shown to produce unfaithful and less interpretable explanations (Smilkov et al., 2017; Kindermans et al., 2019; Slack et al., 2020; Pruthi et al., 2020).\u00b9 Prior research has attributed these shortcomings to the lack of explainability in contemporary neural models (Kindermans et al., 2018; Alvarez Melis and Jaakkola, 2018; Rudin, 2019).\nTo overcome these limitations, we introduce B-cos LM, a dynamic linear model that learns task-relevant patterns through increased input-weight alignment pressure. Building upon B-cos networks from computer vision (B\u00f6hle et al., 2022; Arya et al., 2024), we improve explainability of B-cos LMs through mathematically grounded architecture-"}, {"title": "2 Related Work", "content": "Post-hoc Explanation Methods Various methods have been proposed to provide post-hoc explanations for neural model predictions (Atanasova et al., 2020). These methods can be broadly cate-\nFrom Post-hoc Explanations to Explainable Models The limitations of post-hoc explanation methods may be attributed to the inherent lack of explainability in contemporary neural models, which are typically optimized solely for task performance (Kindermans et al., 2018; Rudin, 2019; Atanasova et al., 2022). For instance, studies have shown that existing models struggle to provide faithful explanations (Alvarez Melis and Jaakkola, 2018) or tend to learn noisy patterns, resulting in less interpretable explanations (Ismail et al., 2021).\nIn response, various efforts have been made to enhance model explainability. Some work has introduced constraints that improve specific explanation properties, such as faithfulness (Tutek and \u0160najder, 2022; Moradi et al., 2020, 2021), consistency (Atanasova et al., 2022), locality (Alvarez Melis and Jaakkola, 2018), and plausibility (Ismail et al., 2021). However, as these constraints are typically imposed as regularizers, their effectiveness in improving explanation quality is not guaranteed (Pruthi et al., 2020). Others have proposed self-explanatory model architectures such as rationale-based models that utilize an \u201cexplain-then-predict\u201d pipeline, where one module selects rationales for another to make predictions based on them (Lei et al., 2016). Although seemingly transparent, both components rely on neural networks, making the rationale extraction and utilization processes opaque (Zheng et al., 2022; Jacovi and Goldberg, 2021). Besides, such models may face optimization challenges that limit their practicality in real-world tasks (Lyu et al., 2024).\nTo tackle these shortcomings, B\u00f6hle et al. (2022) proposed B-cos networks. Unlike methods that impose external constraints, B-cos networks improve explainability through mathematically grounded architectural and computational adaptations. Moreover, these adaptations are designed as drop-in replacements for conventional model components, making B-cos networks easy to train with minimal performance loss. Most recently, Arya et al. (2024) explored B-cosification techniques to convert existing models into B-cos models, which reduces the training costs of adopting B-cos architectures.\nDespite their successful application in vision tasks, B-cos networks have yet to be explored in NLP, where input modalities and training paradigms differ significantly. In this work, we adapt B-cos models for the language domain, integrating them efficiently into NLP pipelines."}, {"title": "3 Methodology", "content": "In this section, we outline the architecture and training process of B-cos LMs and how their design ensures faithful and human interpretable explanations. We first introduce B-cos networks (\u00a7 3.1) and then describe how we transform PLMs to task-specific B-cos LMs (\u00a7 3.2). Finally, we demonstrate how to generate explanations from B-cos LMs (\u00a7 3.3).\n3.1 B-cos Networks\nComplex neural networks can be interpreted as generalized linear models (Nair and Hinton, 2010; Alvarez Melis and Jaakkola, 2018; Srinivas and Fleuret, 2019). For each input x, the network applies a linear transformation: f(x) = W(x)x + b(x), where both the weight W(x) and bias b(x) depend on x. Given that many activation functions are (approximately) piecewise linear, the overall network can be viewed as (approximately) piecewise affine (Alvarez Melis and Jaakkola, 2018). Earlier work refers to such models as dynamic linear models (B\u00f6hle et al., 2021; B\u00f6hle et al., 2022), highlighting the fact that the weight and bias terms dynamically change according to x.\nUnder this dynamic linear perspective, the linear mapping W(x) can be seen as attributing model predictions to individual input features. However, two challenges hinder the direct use of this interpretation. First, W(x) alone provides an incomplete and unfaithful explanation since f(x) \u2260 W(x)x due to the presence of the bias term b(x), and incorporating b(x) into explanations is highly non-trivial (Wang et al., 2019). Second, W(x) is often difficult for humans to interpret, as it does not necessarily align only with task-relevant input patterns (Smilkov et al., 2017) and therefore yields noisy and irrelevant explanations.\nto address these issues, B\u00f6hle et al. (2022) introduced B-cos networks by replacing the conventional linear transformation:\n\\(f(x; w,b) = w^x + b = ||w||||x||cos(x, w) + b\\)   (1)\nwith a B-cos transformation:\n\\(B-cos(x; w) = wTx x cos(x, w)|B-1\\)\n= \\(||\u0175||||x|||cos(x, w)|B \u00d7 sgn(cos(x, \u0175))\\)   (2)\nwhere \u0175 is a scaled version of w with unit norm and sgn denotes the sign function.\nB-cos(x; w) can be seen as a linear transformation of x with the dynamic linear weight w(x) = |cos(x, \u0175)|B-1 \u00d7 \u0175. The absence of b(x) ensures the completeness of summary w(x). We demonstrate that this completeness extends to an entire network composed of bias-free dynamic linear modules in 3.3. Moreover, with additional alignment pressure (B>1), the weight w is forced to align closely with task-relevant patterns to achieve a high cosine similarity and strong activation within the B-cos module. As a result, only the most relevant features are highlighted in explanations, making them more interpretable to humans.\nWhile early B-cos models were trained from scratch, Arya et al. (2024) recently introduced B-cosification, an efficient method to obtain B-cos models. This approach first modifies conventional models with task capacities to adopt the B-cos architecture, followed by fine-tuning on downstream datasets for B-cos conversion. B-cosified models generate explanations as faithful and interpretable as B-cos models trained from scratch but at a much lower training cost. However, directly applying B-cosification to NLP models is non-trivial and inefficient due to the significant differences in model architectures and training pipelines.\n3.2 B-cosification in NLP\nIn this section, we present our B-cosification approach for NLP. We summarize the differences be-"}, {"title": "3.2.1 B-cos Adaptations", "content": "Given a conventional model, we first modify its architecture and computation to integrate the B-cos framework.\nArchitectural Adaptations For completeness and faithfulness of explanations, we follow Arya et al. (2024) and remove all bias terms in models, including those in the affine transformations of layer normalization and attention blocks. Additionally, a prediction head is typically added on top of the transformer before fine-tuning for downstream tasks in the NLP pipeline. This head often includes activation functions that are not (approximately) piecewise linear, such as sigmoid and tanh. To accommodate the unique architecture of NLP models, we remove all activation functions in the prediction heads, as they reduce the locality of explanations and introduce numerical instability during their generation. We expect the added non-linearity from B>1 to compensates for this removal.\nIntroducing B-cos Computation To promote input-weight alignment and improve human interpretability of explanations, we follow Arya et al. (2024) and replace all linear transformations with B-cos transformations in \u00a7 3.1. For a more efficient B-cosification, B-cos layers are initialized with the corresponding weights W of the original model."}, {"title": "3.2.2 Fine-tuning", "content": "The B-cos adaptations above modify the architecture and computation of models, requiring fine-tuning to restore their capabilities and adapt to alignment pressure. Following the NLP-typical \"pre-train then fine-tune\u201d paradigm, we directly transform PLMs to B-cos LMs, rather than adapting task-specific models as done in previous work (Arya et al., 2024). This fundamental difference in the training pipeline adds complexity to B-cosification in NLP, as the objective involves both B-cos conversion and task fine-tuning. While there are multiple ways to conjoin these two steps (cf. \u00a7 7), we find that the most efficient way is to combine them by first applying B-cos adaptations to a PLM and then fine-tuning it on a downstream task. Following B\u00f6hle et al. (2022), we use the binary cross-entropy (BCE) loss instead of conventional cross-entropy loss, as it explicitly maximizes the absolute target logits and strengthens alignment pressure. We provide an extensive comparison of B-cosification setups in \u00a7 7."}, {"title": "3.3 Computing B-cos Explanations", "content": "Once trained, the B-cos LM can generate explanations that faithfully summarize its decision-making process during inference. As all components are dynamic linear with no bias terms (cf. Appendix D), the entire model computation can be expressed as a sequence of dynamic linear transformations:\n\\(W_L(A_L) W_{L-1}(A_{L-1})...\u0174_1(A_1 = X)X\\) (3)\nwhich can be completely summarized as a single dynamic linear function \\(\\prod_{l=1}^L W_l(A_l)\\). Considering the textual inputs specific to NLP, we attribute the model's predictions to the embedding representations. Specifically, to quantify the contribution of a token i to a model prediction, we compute the dot product W(x\u1d62)x\u1d62 between its embedding x\u1d62 and the corresponding dynamic linear weight W(x) for the predicted class logit. For the remainder of the paper, we will refer to such explanations as B-cos explanations."}, {"title": "4 Experiments", "content": "We evaluate the task performance of B-cos LMs and faithfulness of B-cos explanations against conventional models and baseline explanation methods across various tasks, PLMs, and metrics.\nDatasets and Models Our experiments include three sequence classification datasets: AG News (topic classification, Zhang et al., 2015), IMDB (sentiment analysis, Maas et al., 2011), and HateXplain (hate speech detection, Mathew et al., 2021). We use BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and DistilBERT (Sanh et al., 2019) as the basis for conventional fine-tuning and for obtaining B-cos LMs (B=1.5) with the same training hyperparameters (cf. Appendix E for details on fine-tuning, B-cosification, and data splits).\nFaithfulness Metrics For a more comprehensive evaluation, we employ two different methods to assess faithfulness. First, we report two perturbation-based metrics (DeYoung et al., 2020):\n\u2022 Comprehensiveness (Comp) measures the average drop in predicted class probability after"}, {"title": "5 Human Evaluation", "content": "We conduct a human study to evaluate the human interpretability and agreement of B-cos explanations, comparing them against three strong post-hoc explanation methods on the conventional BERT model. For the study, we randomly select 50 instances from AG News and HateXplain where the B-cos and conventional model predict the same label. We then ask five annotators to rate the respective explanations in terms of human interpretability"}, {"title": "6 Qualitative Analysis", "content": "provides an example of B-cos and other (post-hoc) explanations. It can be seen the B-cos explanation highlights important tokens well with little focus on irrelevant ones. In contrast, ShapSampl attributes the highest importance to the [SEP] token and provides only little useful information. Meanwhile, DecompX extracts a significant amount of irrelevant information. Overall, we find that the B-cos explanation provides clearer and more relevant attributions compared to the post-hoc explanations."}, {"title": "7 Comparison of B-cosification Setups", "content": "Transforming PLMs into task-specific B-cos LMs involves two key objectives: task fine-tuning and B-cos conversion. While our main experiments combine these two phases, they can also be performed separately. To assess their effects, we compare two alternative training setups:\n\u2022 Task then B-cos: PLMs are first fine-tuned on a downstream task. B-cos adaptations are then applied, followed by further fine-tuning on the same task for B-cos conversion. This setup is equivalent to Arya et al. (2024) who apply B-cosification to models with task capabilities.\n\u2022 B-cos then task: B-cos adaptations are applied to PLMs first, followed by pre-training on unsupervised texts to enhance B-cosification (cf. Appendix E). The pre-trained B-cos models are then fine-tuned on the downstream task.\nWe evaluate these setups against the B-cosification approach used in our main experiments (B-cos LM) and compare task performance, faithfulness, and training efficiency. Additionally, we report results for conventional fine-tuning (Conv."}, {"title": "8 Effects of B-cosification and B Values", "content": "For a deeper understanding of how B-cosification and parameter B affect model performance and behavior, we compare conventional and B-cos BERT trained on HateXplain across different B values. We also provide an empirical analysis of the impact of B on input-weight alignment in Appendix K.\nModel Performance shows the effects of varying B on the task performance and explanation faithfulness. Classification accuracy initially improves slightly as B increases from 1 to 1.25, benefiting from the extra non-linearity introduced by B>1. However, beyond this point, accuracy declines as higher alignment pressure reduces model flexibility. A similar trend is observed for Comp, peaking around B=1.5 before decreasing. This differs from previous findings in vision models (B\u00f6hle et al., 2022), which we attribute to the high sparsity of explanations at larger B values. As alignment"}, {"title": "9 Explanation Efficiency", "content": "Beyond improved faithfulness and human interpretability, B-cos explanations are also efficient to extract. Comparing their computational costs with strong post-hoc methods shows that B-cos explanations are the most efficient in both time and memory usage (Table 4)."}, {"title": "10 Conclusion", "content": "In this work, we introduce B-cos LM, a dynamic linear model that learns task-relevant patterns through increased input-weight alignment pressure. B-cos LMs generate more faithful and human interpretable explanations while maintaining strong task performance and fast convergence. Based on our in-depth analysis of B-cosification, we provide three recommendations for effectively transforming PLMs into B-cos LMs: (1) combine B-cos conversion and task fine-tuning for efficient B-cosification. If resources allow, additional B-cos pre-training can further improve task performance and explanation faithfulness; (2) carefully select the parameter B, as excessively large values can reduce model capacity and lead to overly sparse explanations; and (3) be mindful of biases in training data, especially at high B values, as B-cosification may amplify existing biases."}, {"title": "11 Limitations", "content": "This study has certain limitations that should be acknowledged.\nFirstly, the automatic evaluation metrics we use may not fully capture the faithfulness of different explanation methods (Feng et al., 2018; Lapuschkin et al., 2019). However, since there is no universal consensus on the most reliable evaluation metrics, this remains an open challenge in explainability research.\nSecondly, our study does not include a direct comparison with other methods designed to enhance model explainability, which may limit the scope of our findings. This omission is due to two reasons: (1) existing explainable models often provide only marginal improvements over post-hoc explanation methods (Brinner and Zarrie\u00df, 2024), and (2) incorporating them into our study would require substantial computational resources, as many baseline explanation methods are computationally expensive.\nFinally, although B-cos LMs can be applied to different model architectures and tasks, our experiments focus only on encoder-only models for sequence classification tasks. Extending our approach to other architectures and tasks remains an avenue for future work."}, {"title": "12 Ethical Considerations", "content": "As discussed in \u00a7 8, B-cos LMs can overfit to biases present in the training data. Although their more faithful and human interpretable explanations make biased predictions easier to detect, this does not eliminate the risk of unintended bias amplification. We encourage users to carefully assess potential biases in their specific use cases before deploying B-cos LMs and to incorporate bias mitigation strategies where necessary.\nAll models and datasets used in this work comply with their respective licenses. Their usage aligns with their intended purpose as specified by their creators.\nThe human study complies with all ethical research guidelines set by our institutes. All participants of the human evaluation study were master's or doctoral students with backgrounds in computer science or computational linguistics and were proficient in English. They were volunteers and were compensated with the standard hourly salary set by the university (at least 5% above minimum wage). Before participation, all participants were informed about the content and purpose of the study, the collected data and its usage. They were instructed on how they could access, modify, or delete their data post-study and provided their informed consent."}, {"title": "A Terminology", "content": "To ensure clarity, we define key terms used in this work as follows:\n\u2022 Faithfulness. The extent to which an explanation accurately reflects the model's actual reasoning process (Jacovi and Goldberg, 2020). A faithful explanation should directly correspond to the internal mechanisms that led to the model's prediction.\n\u2022 Human Interpretability. The ease with which a person can understand the model's reasoning from the explanation (Lage et al., 2019). A highly interpretable explanation should be clear, concise, and focused on relevant information while avoiding unnecessary or distracting information. However, an explanation that is easy for humans to interpret may not necessarily reflect the model's actual reasoning process or align with human reasoning patterns.\n\u2022 Human Agreement. The degree to which a model's explanation aligns with the reasoning a human would use for the same prediction. A high-agreement explanation should follow intuitive, logical reasoning patterns similar to human decision-making.\n\u2022 Explainability. The extent to which a model's computations can be faithfully explained and its learned patterns are understandable to humans. A highly explainable model should yield explanations that are both faithful to its actual reasoning process and interpretable to humans."}, {"title": "B Notation", "content": "In this paper, we use lowercase letters for scalars (e.g., b), bold lowercase letters for vectors (e.g., w, x), and bold uppercase letters (W) for matrices. Additionally, we use bold uppercase letters X and A to denote a sequence of model inputs or hidden state activations. In \u00a7 3, we use x to denote the input when a function is applied to each element of the input sequence separately. In contrast, we use X or A when the function involves interactions between elements, such as in the attention mechanism."}, {"title": "C Ablation Study", "content": "To gain deeper insights into B-cosification, we conduct an ablation study to evaluate the effects of key design choices on model performance. \nConsistent with \u00a7 8, B=1 results in worse task performance and lower explanation faithfulness. Using binary cross-entropy (BCE) loss instead of conventional cross-entropy loss has minimal impact on classification accuracy, but leads to better faithfulness results in perturbation-based evaluations. Additionally, architectural adaptations, including removing bias terms and eliminating activation functions in prediction heads, play a crucial role in improving both model performance and explainability in B-cos LMs. Besides, we encountered numerical instability when generating explanations without these architectural adaptations, as the dynamic linear weight for tanh (tanh(x)) becomes unstable when x is close to 0.\nBeyond ablating components in model design and training, we also examine different explanation methods across models. First, replacing dynamic linear weights W(x) with gradients for computing input contributions (equivalent to InputXGradient, Kindermans et al., 2016) results in less faithful explanations. Moreover, directly extracting B-cos-like explanations, W(x)x, from a conventional"}, {"title": "D Dynamic Linear Representation of Model Components", "content": "Here we describe how each model component functions as a dynamic linear module in B-cos LMs.\nB-cos Layers B-cos layers are designed as dynamic linear modules with a dynamic linear weight matrix W(x) = |cos(x, W)|B-1 W. Here, scales the rows of the matrix W to its right by the scalar entries of the vector to its left.\nNon-linear activation functions In transformer models, non-linearity is typically introduced using (approximately) piecewise linear activation functions, such as ReLU (Nair and Hinton, 2010) and GELU (Hendrycks and Gimpel, 2016). These functions can be easily interpreted as linear transformations with input-dependent weights. For example, GELU(x) = x \u00d7 (0.5 + 0.5 \u00d7 erf(x/\u221a2)) can be interpreted as a linear transformation where the second term acts as a dynamic linear weight.\nAttention block B\u00f6hle et al. (2024) showed that attention computations can be seamlessly integrated into B-cos networks as a dynamic linear module:\n\\(Att(X; Q,K, V) = softmax(XTQTKX)VX\\)\n= \\(A(X)VX = W(X)X\\) (4)\nFor multi-head self-attention (MSA), the output can be viewed as the concatenation of the outputs from H attention heads, followed by a linear projection with matrix U:\n\\(MSA(X) = U[W_1(X)X, ..., W_H(X)X]\\) (5)\nSince this operation maintains a dynamic linear structure, the multi-head attention block remains a dynamic linear module."}, {"title": "E Implementation Details", "content": "Fine-tuning Setups For all PLMs used in the experiments, we use the uncased base version from huggingface (Wolf et al., 2020). For both conventional models and B-cos LMs, we train them for 5 epochs with 10% linear warm-up steps on the\ndownstream task datasets. The learning rates are set to 2e-5 for IMDB and HateXplain, and 3e-5 for AG News. All models use a batch size of 16 and a maximum sequence length of 512. For validation, we randomly sample half of the test set from IMDB and AG News.\nPost-hoc Explanation Baselines For IxG and ShapSampl, we use the Captum (Kokhlikyan et al., 2020) implementations. We implement the Attention method ourselves, and LIME is sourced from the lit library. For DecompX and SIG, we use their official implementations with default configurations. The number of samples is set to 25 for ShapSampl and 3,000 for LIME, with [MASK] as the baseline token. For all explanation methods at the embedding level, model predictions are attributed to the combined sum of word, position, and token type embeddings (if applicable). In the main experiments, we compute token attribution scores by summing over all embedding dimensions, as this approach demonstrates better faithfulness results than using the L2 norm.\nSeqPG Examples When constructing examples for SeqPG, we set the sequence length to 50 for AG News, 256 for IMDB, and 25 for HateXplain, aligning with their median lengths. Only examples longer than these thresholds are selected, and they are truncated to construct synthetic examples. Additionally, we only use examples that are correctly predicted with a minimum confidence of 75% after truncation. For a fair comparison, we evaluate B-cos LMs on the same sets of examples constructed based on the predictions of the corresponding conventional models.\nEvaluation Setups For task performance evaluation, we use the complete test set for each task. For faithfulness evaluation, we conduct perturbation-based evaluations on 2000 test examples and SeqPG on 500 test examples for AG News and IMDB. For HateXplain, we use the full test set for perturbation-based evaluation (1,924 examples) and construct 269, 310, and 308 SeqPG examples from it using BERT, DistilBERT, and RoBERTa, respectively."}, {"title": "F SeqPG Example", "content": "presents a SeqPG example from AG News using B-cos BERT. For better visualization, each segment is truncated to 20 tokens instead of 50 used in the experiments. Unlike the hybrid document evaluation proposed by Poerner et al. (2018), our approach explicitly controls segment length and position to ensure a fair comparison. Additionally, we measure the proportion of correctly assigned positive attributions rather than relying solely on the highest attribution value."}, {"title": "G Task Performance of Other B-cos LMs", "content": "illustrate the task performance of conventional and B-cos DistilBERT and ROBERTa across datasets. Consistent with findings from BERT models (cf. Figure 2), B-cos LMs exhibit strong performance comparable to conventionally fine-tuned models."}, {"title": "H Faithfulness Evaluation of Other B-cos LMs", "content": "present the faithfulness evaluation results for DistilBERT and ROBERTa. The findings are consistent with our main experiments (cf. Table 2), confirming that B-cos LMs produce more"}, {"title": "I Human Evaluation Details", "content": "In the human study, we select only examples shorter than 25 tokens for HateXplain and 40 tokens for AG News to improve visualization. Additionally, we replace [CLS] and [SEP] with ## to make the examples more understandable for lay users. Below, we provide the instructions along with a detailed description of the criteria and scoring used in our human evaluation."}, {"title": "K Impact of B on Input-weight Alignment", "content": "To analyze how B-cosification and alignment pressure influence the behavior of B-cos LMs, we compute the alignment (cosine similarity) between each input and its corresponding weight in B-cos modules across all layers. This analysis is performed on 100 examples from the HateXplain dataset. In Figure 13, we plot different percentiles of input-weight alignment for conventional and B-cos BERT models with varying B values. For better visualization, we display only the 10th to 90th percentiles.\nOverall, larger B values generally lead to stronger input-weight alignment compared to smaller B and conventional models, as evidenced by the curves for B=1.5 and B=2.5 lying above those for the conventional model and B=1. However, the alignment pattern becomes more complex when comparing B=1.5 and B=2.5. Specifically, at B=2.5, the most aligned input-weight pairs exhibit higher alignment than in other models, but some pairs show very low alignment. This result may arise because certain weights are highly optimized for specific input patterns, leading to poor alignment with others, particularly in later layers where input features become more anisotropic (Ethayarajh, 2019; Li et al., 2020). As a result, some outputs from the B-cos layers are highly negative. When these outputs are fed into GELU activation functions, their dynamic weights approach zero, making the explanations more sparse."}, {"title": "L Effects of B on Other Metrics", "content": "presents the complete results on how B values affect task performance, explanation faithfulness and explanation entropy, as shown in Figure 5. Similar to Comp, SeqPG scores also decline with higher alignment pressure. This could also be attributed to the high sparsity of explanations. As B increases, fewer tokens receive attribution scores that are not close to zero, and in some SeqPG examples, B-cos LMs may attribute predictions to a single segment. This can lead to numerical instability when computing the positive attribution ratio."}, {"title": "M B-cos Explanations with Different B Values", "content": "illustrates that with increased alignment pressure, B-cos LMs learn fewer but more task-relevant features. Consequently, they produce sparser explanations, with fewer tokens receiving significant attribution. This finding aligns with the statistics presented in \u00a7 8."}, {"title": "N Example of Model Bias", "content": "In the example shown in , models become increasingly confident in the incorrect prediction"}]}