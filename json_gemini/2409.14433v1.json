{"title": "OStr-DARTS: Differentiable Neural Architecture Search based on Operation Strength", "authors": ["Le Yang", "Ziwei Zheng", "Yizeng Han", "Shiji Song", "Gao Huang", "Fan Li"], "abstract": "Differentiable architecture search (DARTS) has emerged as a promising technique for effective neural architecture search, and it mainly contains two steps to find the high-performance architecture: First, the DARTS supernet that consists of mixed operations will be optimized via gradient descent. Second, the final architecture will be built by the selected operations that contribute the most to the supernet. Although DARTS improves the efficiency of NAS, it suffers from the well-known degeneration issue which can lead to deteriorating architectures. Existing works mainly attribute the degeneration issue to the failure of its supernet optimization, while little attention has been paid to the selection method. In this paper, we cease to apply the widely-used magnitude-based selection method and propose a novel criterion based on operation strength that estimates the importance of an operation by its effect on the final loss. We show that the degeneration issue can be effectively addressed by using the proposed criterion without any modification of supernet optimization, indicating that the magnitude-based selection method can be a critical reason for the instability of DARTS. The experiments on NAS-Bench-201 and DARTS search spaces show the effectiveness of our method.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, neural architecture search (NAS) [1] has shown its potential in automatically discovering network architectures with high performance. Early works on NAS are mostly realized by reinforcement learning (RL) [1]\u2013[3] and evolutionary algorithms [4]-[6], which commonly need massive computation overheads, consuming hundreds of GPU days for searching. To improve the search efficiency, researchers propose one-shot methods [7]\u2013[9] that adopt weight sharing strategy for supernet optimization and then derive the final architecture from the optimized supernet. Based on the idea of one-shot NAS, Liu et al. [10] propose differentiable architecture search (DARTS) that allows architecture parameters to be optimized via a gradient-based algorithm based on continuous relaxation of the architecture representation, which makes architecture search more efficient. The searching procedure of DARTS contains two major steps (shown in Fig. 1): 1) The supernet optimization step that jointly learns the model weights and architecture parameters via a gradient-based algorithm; 2) The architecture selection step that finds the operations with the largest architecture parameters to build final architecture, a.k.a. magnitude-based selection method.\nAlthough DARTS enjoys high computational efficiency, it frequently suffers from searching instability [11]. The performance of the final architecture obtained by the magnitude-based selection method can only be guaranteed when an implicit assumption holds [12]: the value of an architecture parameter can correctly reflect the contribution of the corresponding operation to the supernet. However, this assumption cannot be held in many cases. On the one hand, recent researches [12], [13] show that the architecture parameters corresponding to the operations with fewer parameters, especially the skip connections, tend to be large after long-epoch weight-sharing optimization. On the other hand, as the searching procedure in DARTS ignores the coupling relationship of the parameters, the architecture parameter of a certain operation cannot be used individually to indicate the importance of this operation [14]. Therefore, the performance of the final architecture obtained by DARTS can collapse when applied with the magnitude-based selection method.\nMost existing research proposes to improve the supernet optimization procedure (Step-1 in Fig. 1) to address the issue by ensuring the applicability of the magnitude-based selec- tion method in DARTS. Early works mainly stop the search procedure before the performance collapses based on some handcrafted criteria [10], [11], [15]. Different regularization terms for architecture parameters [16] and the norm of Hessian matrix w.r.t a [11], [17] are also developed to stabilize the searching procedure, preventing the domination of skip connections. Moreover, researchers design methods to promote fair optimization for operations during searching [13], [18]. Other improvements for DARTS [14], [19] propose to decouple the interaction between different operations to improve the correlation between the learned architecture parameters and the selected architectures. While all these works still apply the magnitude-based method for generating the final architecture, little attention has been paid to improving the selection process (Step-2 in Fig. 1).\nDesigning a criterion that can accurately reflect the con- tribution of an operation to the supernet is a challenging problem. The original selection problem leads to an NP-"}, {"title": "II. RELATED WORK", "content": "Recent years have witnessed the rapid development of deep learning, most of which are manually designed [20]\u2013[24]. To improve the efficiency for designing network architectures, researchers start to design NAS algorithms [2], [5] to automatically design neural architectures, mixed-precision-quantized networks [25] and Dendritic neural model [26]. Moreover, NAS methods have been applied to improve the efficiency of modern CNNs and Vision Transformers [27], [28]. Existing NAS approaches can be divided into three categories: RL-based approaches [1], [2], [29], evolutionary-algorithm-based methods [4], [5], [30] and gradient-based approaches [10]. Techniques, like knowledge distillation [31] and language models [32], have also been applied to the NAS field. Also, other works [33], [34] try to reduce the computational demands of NAS methods to solve the obstacles in real deployments. Recent studies have also achieved NAS using multi-agent [35] or large language model [36] for searching.\nDifferentiable architecture search (DARTS) [10] fastens and simplifies the searching procedure by enabling NAS to use gradient descent for search. The recent work in [37] provides a comprehensive survey on DARTS. Recent works further improve DARTS by reducing the memory costs during the searching phase [38], [39], and have extended the DARTS for zero-shot learning [40] and remote sensing applications [41]. Although DARTS learns differentiable architecture weights and reduces the search costs, its stability has been challenged because of yielding deteriorating architectures during the search procedure. The final architecture can be dominated by parameter-free operations when training epochs become large. To address this issue, researchers [10], [11], [15] apply different early stopping strategies to stop the searching before the loss landscape becomes precipitous and prevent the performance collapse of the search process. Explicit [16] and implicit [17], [42] regularization terms are also introduced to the supernet optimization procedure to improve the stability of DARTS. Moreover, a series of works [13], [18], [43] propose to balance the imbalance training procedure for different operations caused by the Matthew Effect, which means that operations with fewer parameters would be trained maturely earlier. The degradation issue is also addressed by decoupling the relationship among different operators [14], [19], [44], especially skip connection [45]. However, most of the existing DARTS methods only consider improving the optimization procedure for DARTS supernet, the architecture selection process after optimization has received little attention.\nPrevious research has shown that: after optimization of the supernet, architecture parameters a, cannot accurately reflect the importance of each operation [11], [12]. Wang et. al., [12] have shown that several failure modes of DARTS can be greatly alleviated with a perturbation-based search method correctly picking the operations with the largest contribution to the supernet. However, the perturbation-based selection method is conducted edge by edge on the optimized supernet,"}, {"title": "III. METHOD", "content": "This section first briefly reviews the formation of DARTS. Then we rethink the discrete NAS selection problem and introduce how to approximately solve this problem motivated by the techniques from network pruning, based on which we further introduce the formulation of operation strength and the proposed OStr-DARTS.\nThe overview of our method is shown in Fig. 2. Our work focuses on improving the architecture selection procedure to guarantee the final architecture with high performance can be explored from the optimized supernet. Given a search space, we first obtain the optimized supernet via the gradient descent algorithm. The optimization procedure is not limited to the classical one in DARTS [10], and can be replaced with any improved methods, such as the memory-efficient PC-DARTS [38]. With the optimized supernet, we conduct the importance estimation on each edge. The importance of an operation is estimated by the operation strength, which is calculated by the loss change induced by selecting a specific operation. Then the operation with the largest operation strength will be selected on this edge. This procedure is repeated for all edges in the cell to obtain the final architecture."}, {"title": "A. Formulation of DARTS", "content": "The aim of DARTS [10] is to find the best cell that can stack to form the network with high performance. Each cell is considered as a directed acyclic graph of N nodes, which are latent representations in the network. Moreover, there are E edges in a cell, and an edge e is the connection between different nodes, and consists of a set of candidate operations, 0 (|0| = P). To effectively find the optimal operations for each edge, DARTS introduces a mixed operation between every two connected nodes, which can be represented as:\n$\\overline{o^{e}}(x_{i})=\\sum_{o\\in O} \\beta_{o}o(x_{i}), \\beta = \\frac{exp(\\alpha_{o})}{\\sum_{o' \\in O} exp(\\alpha_{o})}$ (1)\nwhere o and $(x_{i})$ mean the operation and the features generated by the mixed operation on edge e, and xi is the input features of edge e. Moreover, $\\alpha \\in \\mathbb{R}^{P\\times E}$ represents the learnable architecture parameters, representing the weights of the corresponding operations. Then the network parameters w and the architecture parameters a can be learned by:\n$\\min_{\\alpha} L_{val} (w^{*}(\\alpha), \\alpha)$\ns.t. $w^{*}(a) = arg \\min_{W} L_{train} (w, a)$, (2)\nwhere network parameters w and architecture parameters a are optimized on the training and validation sets, respectively. At the end of the search phase, the architecture selection step will replace each mixed operation with the most likely operation, $o^{e} = arg \\max_{o'\\in O} \\alpha_{o'}^{e}$, to generate the final target architecture. However, a larger value of a does not necessarily mean that the corresponding operation has more contribution to the whole supernet. Therefore, it is necessary to design a better criterion to accurately estimate the importance of different operations."}, {"title": "B. Rethinking selection problem of DARTS", "content": "Actually, the final selection problem of DARTS can be denoted as\u00b9:\n$\\min_{C} L_{val} (cO(w^{*}); D_{val})$\n$\\text{s.t.} \\quad w^{*}(a) = \\arg \\min_{W} L_{train} (w; a, D_{train}),$ (3)\n$c \\in \\{0,1\\}^{P\\times E}, || c_{e} ||_{0} = 1, e = 1, ..., E,$\nwhere c represents operation indicators, D represents the datasets. Moreover, O is the collection of generated features for all operations in the cell. We use c O to denote the obtained features masked by the learned indicator matrix c. The constraint, || ce ||0= 1, forces that each edge will retain the best operation to build the final cell. Due to the complexity of solving the discrete optimization problem (3), DARTS relaxes c to the continuous \u03b2\u2208 [0,1]P\u00d7E for easier optimization.\nWhile, in network pruning fields, a similar discrete opti- mization problem is solved by exploring the weight connection sensitivity of the loss function as a proxy [48]. The pruning problem can be represented as:\n$\\min_{C,W} L(cw; D)$\ns.t. $c\\in \\{0,1\\}^{m}, || c ||_{0} \\leq k$. (4)\nHere, we consider the one-cell situation for simplicity."}, {"title": "C. Operation Strength", "content": "The operation strength of an operation o is defined by the absolute change of final loss when the optimized supernet selects this operation as the final operation to replace the mixed one, which can be represented byR1:\n$s_{o}^{e} = \\Delta L_{O}^{e} = |L(cO^{e}; D) - L(c\\overline{O^{e}}; D)|$ (8)\nNote that the gradient of architecture parameters a of the supernet can be written asR1\n$\\frac{\\partial L}{\\partial \\alpha_{o}^{e}} = \\frac{\\partial L}{\\partial \\overline{o^{e}}} \\frac{\\partial \\overline{o^{e}}}{\\partial \\beta_{o}^{e}} \\frac{\\partial \\beta_{o}^{e}}{\\partial \\alpha_{o}^{e}}$\n$=\\frac{\\partial L}{\\partial \\overline{o^{e}}}(\\beta_{o}^{e} (1 - \\beta_{o}^{e}) - \\sum_{o'\\neq o} \\beta_{o'}^{e} \\beta_{o}^{e}) \\frac{\\partial \\overline{o^{e}}}{\\partial \\beta_{o}^{e}}$ (9)\n$= \\frac{\\partial L}{\\partial \\overline{o^{e}}}(\\overline{o^{e}} - o_{0}^{e}),$\nthen we have $s_{o}^{e} = |\\frac{\\partial L}{\\partial \\alpha_{o}^{e}}|$. For L-cells case, we still haveR1:\n$s_{o}^{e} = |\\frac{\\partial L}{\\partial \\alpha_{o}^{e}}| = | \\sum_{l=1}^{L} \\frac{\\partial L}{\\partial \\alpha_{ol}^{e}} |$ (10)\n$= |\\sum_{l=1}^{L} \\frac{\\partial L}{\\partial \\overline{o^{e}}}(\\overline{o_{ol}^{e}} - o_{ol}^{e})|$\nwhere $\\overline{o^{el}}$ is the generated feature of the mixed operation on edge e at l-th cell, and ood is the feature from operation o at l-th cell. Therefore, the proposed operation strength can be obtained directly from the architecture parameter gradients without any computational cost, which is readily available during the optimization phase.\nDuring the searching phase, the gradients of architecture parameters will be computed using a minibatch of data, and the final so will be calculated for B minibatches, and averaged over B. However, after sufficient training of supernet, the gradient $\\frac{\\partial L}{\\partial \\alpha_{o}^{e}}$ should be 0 at the optimal ideally, which means that: $\\frac{\\partial L}{\\partial \\alpha_{o}^{e}} \\rightarrow 0$, and therefore, the operation strength will tend to be zero, namely, $\\mathbb{E}_{D}(s_{o}^{e}) \\rightarrow 0$. Empirically, the gradient w.r.t a can be represented as $\\sum L_{n} \\approx 0$, where Ln denotes the loss of n-th sample. Nevertheless, as a result of stochastic gradient evaluations, the practical operation strength would never be zero. The operation strength takes the absolute value of the gradient and is averaged over B different minibatches. This means that so is actually computed by:\n$\\mathbb{s_{o}^{e}} = \\frac{1}{B} \\sum_{b=1}^{B} |\\frac{\\partial L_{n}}{\\partial \\alpha_{o}^{e}}|_{(\\alpha)}^{\\alpha_{b}},$ (11)\nwhere $(.)_{(\\alpha)}^{\\alpha_{b}}$ means the results for b-th minibatch. As we can see, in the extreme case that the batch size equals 1, the operation strength will tend to ED(||), which satisfies ED(||) \u03c3 (\u03c3 is the standard deviation of $\\frac{\\partial L_{n}}{\\partial \\alpha_{o}^{e}}$) under the assumption that samples are drawn with i.i.d. Therefore, the calculated operation strength is proportional to the standard deviation of $\\frac{\\partial L_{n}}{\\partial \\alpha_{o}^{e}}$ value which is empirically more informative as it is stated in [57].\nWith the calculated operation strength for each operation, we can build the final architecture with the operation corre- sponding to the largest operation strength on each edge."}, {"title": "Algorithm 1 OStr-DARTS", "content": "Input: A0; C; T; Er;\n1: t = 1;cnt = 0;\n2: while cnt <Cor t<T do\n3: Update network weights w and parameters a\n4: if Er or t == T then\n5: Calculate s for each operation o in each edge e\n6: Select the architecture At based on each s\n7: if At-1 == At then\n8: cnt = cnt +1\n9: else\n10: cnt = 0\n11: t=t+1\nOutput: Final selected architecture A* = At"}, {"title": "D. OStr-DARTS", "content": "We now introduce the proposed differentiable architecture search based on operation strength, OStr-DARTS. The opti- mization procedure can be either the same as the classical bi-level method in [10], or be other improved ones, such as the memory-efficient procedure in [38]. During each searching step, we can directly obtain the so for different operations on each edge without extra costs since the gradient information has already been calculated during the optimization procedure. Moreover, as the resulted architectures of OStr-DARTS converge fast in some search spaces, we can stop the searching procedure early if the selected architecture does not change for a given number of epochs C. We use Er to define if we want an early stop criterion during the optimization and use T and Ao to denote the total number of searching epochs and the initial random architecture. The fast convergence speed of the final further demonstrates the stability of our method. The complete algorithm is given in Algorithm 1."}, {"title": "IV. THEORETICAL ANALYSIS AND DISCUSSIONS OF OSTR-DARTS", "content": "We show why OStr-DARTS can avoid the domination of skip connections. Following [12], we provide a case study about the skip connection in DARTS and show that the optimal solution of \u1e9eskip in the sense of minimizing the variance of feature map estimation.\nProposition 1: Without loss of generality, consider one cell from a simplified search space consisting of two operations: skip connection and conv. Let m* denote the optimal feature map, which is shared across all edges according to the unrolled estimation view [59]. Let oconv(xe) be the output"}, {"title": "A. Avoid the domination of skip connections", "content": "of the convolution operation, and let xe be the input. Assume m*, Oconv(xe) and xe are normalized to the same scale. The current estimation of m* can then be written as:\n$\\overline{o^{e}} = \\beta_{skip}x_{e} + \\beta_{conv}o_{conv}(x_{e}).$\nThen optimal \u03b2skip and \u03b2conv minimizing var(\u014de \u2013 m*) are:\n$\\beta_{conv} \\propto var(x_{e} - m^{*}), \\beta_{skip} \\propto var(o_{conv}(x_{e}) - m^{*}).$\nFrom the conclusion from [12], we can see that the magnitudes of \u03b2skip and \u03b2conv will converge to the values which one of the residual features is closer to m* in variance. xe comes from the mixed output of the previous edge. Since the goal of every edge is to estimate m* (unrolled estimation), xe is also directly estimating m*. However, Oconv (Xe) is the output of a single convolution operation, so it can deviate from m* even at convergence. Therefore, in a well-optimized supernet, xe will naturally be closer to m* than Oconv(xe), causing \u03b2skip to be greater, which leads to the domination of skip connections.\nHowever, xe being closer to m* can lead to a lower value of $\\frac{\\partial L}{\\partial \\alpha_{o}^{e}}-x_{e}$, resulting in the decrease of the operation strength of skip connections. These two terms balance and avoid the domination of skip connections in OStr-DARTS.\nEmpirical results are also provided in Fig. 5 (a), showing the illustration of a cell architecture and the optimal cell\u00b3 in NAS-Bench-201 space. The proposed operation strength consists of \u03b2 and the norm of |$\\frac{\\partial L}{\\partial \\alpha_{o}^{e}}$ \u2013 0|, which can be referred to as the residual features (RF). The results in Fig. 6 (a) and (c) show that the \u03b2 of skip connection is larger than others, resulting in the domination of skip connections using the magnitude-based selection methods. However, since that operation strength further uses the RF to estimate the importance of an operation, the skip connection with little effect on final loss will be ignored during selection, guaranteeing criterion can select the essential operations from the supernet.\nWe further investigate what a higher value of the RF norm represents here. Consider the RF as $\\frac{\\partial L}{\\partial \\alpha_{o}^{e}}$ \u2013 0, it can be rewritten as:\n$|\\frac{\\partial L}{\\partial \\alpha_{o}^{e}} - \\overline{o}| = |\\frac{\\partial L}{\\partial \\alpha_{o}^{e}} - \\sum_{o'} \\beta_{o'}^{e} o_{o'}^{e}|$ (12)\n$= |\\sum_{o'\\neq o} \\beta_{o'}^{e} (\\frac{\\partial L}{\\partial \\alpha_{o}^{e}} - o_{o'}^{e})| \\leq \\sum_{o'\\neq o} \\beta_{o'}^{e} |(\\frac{\\partial L}{\\partial \\alpha_{o}^{e}} - o_{o'}^{e})|$ (13)\nFrom the inequality, we see that a higher value of RF means that the features of obtained by the target operation are distinctive compared to the features o, generated by the rest operations on average with the scaled factor B. This indicates that the impact of the generated features is also an important factor in selecting the operation contributing to the supernet most, which meets the recent findings in large language model (LLM) fields [60].\nTherefore, operation strength will evaluate the importance of an operation from two aspects: 1) Whether the magnitude of the architecture parameter for the corresponding operation is large. 2) Whether the features generated by this operation"}, {"title": "B. Operation strength: architecture parameter perspective", "content": "In this subsection, we provide the analysis about why using |$\\frac{\\partial L}{\\partial \\alpha_{o}^{e}}$| as a selection criterion in Eq. (10) provides a better final architecture. Again, considering the discrete NAS problem in (3), we may be able to determine the importance of a given operation by measuring its effect on the loss function when the operation is selected, which is shown to be an effective way to solve the discrete problem in network pruning. To measure the effect of a target operation on the loss, one can try to measure the difference in loss with the $\\alpha$ and \u03b1\u221e:\n$\\Delta L_{o} = |L(\\alpha, w; D) \u2013 L(\\alpha \u2013 (\\alpha \u2013 \\alpha_{o\\rightarrow\\infty}), w; D)|$, (14)\nwhere ao\u2192\u221e is the architecture parameter that element o equals to +\u221e and others remain the same as a, resulting in that Softmax(ao\u2192\u221e) \u2192 \u03b2. It is easy to see that \u2206Lo attempts to measure the influence when using operation o on the loss function. Following [48], \u2206Lo can be approximated by the directional derivative of L, which can be written as:\n$\\Delta L_{o} \\approx | lim_{\\delta\\rightarrow 0} \\frac{L(\\alpha, w; D) \u2013 L(\\alpha \u2013 \\delta(\\alpha \u2013 \\alpha_{o\\rightarrow\\infty}), w; D)}{\\delta} |$, (15)\nwhere d is infinitesimal of higher order. Moreover, note that the direction of a \u2212 ao\u2192\u221e is the same as the \\^{e} (the all-zero"}, {"title": "C. The novelty and superiority of operation strength", "content": "In this section, we provide a discussion about why the proposed operation strength criterion works well compared to magnitude-based selection. First, due to the complexity of the DARTS problem, using the magnitude-based method to transform the relaxed continuous optimal solution to the discrete one may not be a good way to solve the original selection problem in (3). As shown in previous researches [16], [45], the issue-causing discrete step can be the culprit of the instability of DARTS. While the proposed operation strength ties to directly estimate the optimal solution of the problem (3), avoiding the issue-causing discrete step, and therefore can be more stable when selecting the final architecture. Second, the analysis at the beginning of Section III and Section IV-B both indicate that the operation strength can be used as a good approximation for the optimal solution for the problem (3). Last but not least, the proposed Ostr-DARTS can effectively select the most important skip connections and eliminate the redundant ones. This also guarantees the performance of the final selected architecture using operation strength.\nAlthough both our method and previous work in [48] use the Taylor expansion-based to achieve the importance estimation, they are designed for different purposes. The importance indicator in [48] is specialized for the pruning problem, while we design a new Taylor expansion-based importance indicator for architecture selection in DARTS. This design does not follow the design routine in existing works that all proposed to estimate the importance by removing the target weights/kernels/filters/operations. From Fig. 3, we see that pruning and selection procedures are different. Therefore, directly using the existing importance indicators [48] is not reasonable and should be rebuilt and redesigned for the architecture selection in the NAS problem, which motivated this work. Moreover, the experimental results in Section V further confirm that directly using the importance indicator in [48] can lead to sub-optimal performance, demonstrating the effectiveness and novelty of our method."}, {"title": "D. OStr-DARTS*", "content": "Actually, the \u2206Lo can be calculated using the first-order Taylor expansion with two different formulations expanded at $\\overline{o}$ or \u014d, respectivelyR1:\n$\\Delta L_{o} = |L(o_{o}^{e}; D) \u2013 L(\\overline{o^{e}}; D) | \\approx |\\frac{\\partial L}{\\partial \\overline{o^{e}}} (\\overline{o^{e}} - o_{o}^{e})|$ (17)\n$= \\beta_{o}^{e} |\\frac{\\partial L}{\\partial \\overline{o^{e}}} (\\overline{o^{e}} - o_{o}^{e})|,$\nor,\n$\\Delta L = |L(o_{o}^{e}; D) \u2013 L(\\overline{o^{e}}; D)| \\approx \\frac{\\partial L}{\\partial \\alpha_{o}^{e}} (\\overline{o^{e}} - o_{o}^{e}).$ (18)\nWe refer to the new selection criterion in Eq. (18) as operation strength*, s*, and based on this we can develop the OStr- DARTS*. According to Eq.(17), the new indicator can be cal- culated by s*\u00b0 = s\u00b0/\u03b2%. Our experiments further confirm that OStr-DARTS* also boosts the performance compared to the Naive one, indicating the limitation of the direct application of pruning techniques in architecture selection.\nThe difference between OStr-DARTS and OStr-DARTS* is the expansion points using Taylor expansion. After optimiza- tion, the \u014d will tend to be near the optimal point where the gradient can be flattened, resulting in a less accurate estimation of the loss change (illustrated in Fig. 7). While, we find that both OStr-DARTS and OStr-DARTS* can achieve promising performance in both NAS-Bench-201 and DARTS, which again demonstrates the main reason for the poor generalization in DARTS can be the failure of the magnitude-based method."}, {"title": "V. EXPERIMENTS", "content": "We first evaluate our methods on NAS-Bench-201 [65]. The test performances for all candidate architectures on CIFAR- 10/100 [66], ImageNet-16-120 [67]) were reported. NAS- Bench-201 provides a standard cell-based search space, where each cell contains 6 edges with 5 candidate operations (none, average pooling, 3x3 and 1x1 convolution, skip connection). There are 56 = 15625 candidates in total. The architecture contains three stages connected by a basic residual block [20] with a stride of 2 between them, and each cell was stacked five times to build the stage."}, {"title": "D. Transferring to Object Detection", "content": "The obtained neural architectures are further used as the backbone networks in object detection tasks. We integrate the searched architecture with RetinaNet [77], a popular single- stage detection framework. The detection models are trained on the train set of MS-COCO [78] and tested on the validation set. We follow the standard training setting in [77] and only modify the backbone of RetinaNet."}, {"title": "VI. LIMITATIONS AND FUTURE WORKS", "content": "While experimental results show the effectiveness of Ostr- DARTS, our method still has some limitations.\nFirst, the optimality of the proposed method can only be guaranteed with the assumption that the change of loss can be used to estimate the solution of the discrete NAS selection problem in (3). Although such a solution method is effective in pruning fields [79], how good it is should be further inves- tigated in DARTS. However, the experiments in Fig. 12 and Fig. 13 show a high correlation between the accuracy of the final model and the selected operations, indicating that using operation strength can be a possible way for approximating the solution of the NAS selection problem.\nSecond, the optimality of the proposed method can only be guaranteed with the assumption that the operation strength can be used to estimate the loss changes when a certain operation is selected. As DARTS is notorious for its precipitous validation loss landscape, whether the proposed operation strength can be used to accurately estimate the loss change still needs to be further studied.\nThird, our method relies on approximating operation strength from gradient information. Considering the precipi- tous validation loss landscape, the gradient information might sometimes with high variance. We believe that using the second-order Taylor term might lead to a better loss change estimator for the architecture selection problem. However, we only apply the first-order Taylor expansion for loss change estimation due to the high computational costs of calculating the Hessian Matrix.\nAs we do not apply any improvements to the supernet optimization of DARTS, we indicate that the poor general- ization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than the op- timization of its supernet entirely. Our research found that besides architecture parameters, the generated features might also be a key factor in identifying the important connections or operations in a neural network. Recent work [60] has also proposed a similar pruning strategy that considers both weights and activation values during pruning Large Languages Models (LLM) and achieved promising performance. Future work mainly includes building the sophisticated importance estimation criteria to serve the NAS community as well as the pruning and large model fields, or even NAS for LLM. We believe that all of them will contribute a lot to building high-performance neural architectures with less complexity."}, {"title": "VII. CONCLUSION", "content": "In this paper, we propose a novel architecture selection cri- terion for DARTS-based methods, based on which we further develop an effective NAS approach named OStr-DARTS. The new architecture selection method measures the contribution of an operation to the supernet by introducing the effect of this operation on the final loss. We find that the frequent degeneration issue in DARTS can be effectively addressed by simply substituting the original magnitude-based selection method with ours. Moreover, the proposed selection criterion can be combined with various orthogonal improvements for supernet optimization if necessary. Experimental results show that our methods can consistently explore outperformed ar- chitectures from supernets compared to other related baseline models on several search spaces and datasets. We hope that our work can bring a new perspective to the NAS community to design high-performance DARTS-based methods by devel- oping sophisticated architecture selection methods from the pre-trained or scratch DARTS supernet."}]}