{"title": "CLAMP-VIT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs", "authors": ["Akshat Ramachandran", "Souvik Kundu", "Tushar Krishna"], "abstract": "We present CLAMP-ViT, a data-free post-training quantization method for vision transformers (ViTs). We identify the limitations of recent techniques, notably their inability to leverage meaningful inter-patch relationships, leading to the generation of simplistic and semantically vague data, impacting quantization accuracy. CLAMP-ViT employs a two-stage approach, cyclically adapting between data generation and model quantization. Specifically, we incorporate a patch-level contrastive learning scheme to generate richer, semantically meaningful data. Furthermore, we leverage contrastive learning in layer-wise evolutionary search for fixed- and mixed-precision quantization to identify optimal quantization parameters while mitigating the effects of a non-smooth loss landscape. Extensive evaluations across various vision tasks demonstrate the superiority of CLAMP-ViT, with performance improvements of up to 3% in top-1 accuracy for classification, 0.6 mAP for object detection, and 1.5 mIoU for segmentation at similar or better compression ratio over existing alternatives. Code is available at https://github.com/georgia-tech-synergy-lab/CLAMP-ViT.git", "sections": [{"title": "Introduction", "content": "Vision transformers [12] (ViTs) have recently gained a lot of traction due to their state-of-the-art (SoTA) performance across various computer vision (CV) tasks [34,37,48, 50]. Concurrently, the growing need to deploy these parameter-heavy models at the resource-limited edge [14], has inspired research on various model compression techniques. Model quantization [14, 30, 36, 45] has emerged as a popular technique to achieve memory and compute efficient deployment. Quantization reduces memory footprint and improves computation speed of a model by mapping full-precision (FP) weights to reduced precision formats (e.g., < 8-bit INT) [13, 17, 19, 46]. In particular, quantization-aware training (QAT) allows a model to train by taking quantization approximation in to account, enabling ease of quantization. Post-training quantization (PTQ), in contrast, acts as a plug-and-play quantization applied on a pre-trained model. PTQ has become popular as it can leverage the pre-trained model and does not add training overhead [21, 23, 26]. We thus consider the PTQ setting in this work. However, PTQ requires access to a calibration set that is often drawn from the training data [18,45]. This may be infeasible in situations involving privacy and security concerns [20,49], making these techniques ill-suited to vield optimal performance.\nRecent works [25, 27] propose\ndata-free PTQ (DFQ), generating\nsynthetic calibration data \\(T_{syn}\\) from\nGaussian noise [7, 27], embedding in-\nformation from the original dataset\n\\(T_{orig}\\), where \\(T_{syn} << T_{orig}\\).\nExisting DFQ methods for CNNs\n[2,51] exploit the batch-normalization\n(BN) layer statistics [33,44] to gener-\nate synthetic samples mimicking the\noriginal data distribution. For ViTs,\nabsence of the BN layer makes these techniques obsolete. Recent efforts to extend DFQ to ViTs include PSAQ-ViT v1 [27] and PSAQ-ViT v2 [25]. They rely on information embedded in the attention score output of the multi-head self attention (MHSA) layer. PSAQ-ViT v1 and v2 [25,27] introduce a relative value metric namely, patch similarity, to optimize Gaussian noise towards synthetic data by maximizing the entropy of patch similarity in a global manner. However, the metric considered in PSAQ-ViT v1 and v2 assumes all patches\u00b9to be equally important, without considering spatial sensitivity [47]. This may fail to capture semantically meaningful inter-patch relations, potentially affecting robustness of the synthetic data. Moreover, the synthetic data generation stage in these methods does not consider the informativeness of the generated samples towards the quantization process, nor do they establish countermeasures to ameliorate the non-smooth loss landscape during quantization, resulting in sub-optimal parameter search and poor generalization to test set [5].\nOur contributions. The discussion above hints at the potential limitation of [25,27] in capturing semantically meaningful and robust inter-patch relationships to generate synthetic data that is well-suited to quantization. Towards solving these limitations, we present contrastive data-free learning for adaptive post-training quantization of ViTs (CLAMP-ViT), a general DFQ method applicable to a wide range of vision tasks. To the best of our knowledge, CLAMP-"}, {"title": "Related Works", "content": "Data-Driven PTQ for ViTs. PTQ offers an efficient alternative to QAT [23,26] by directly quantizing pre-trained models without the need for compute-heavy retraining. In specific, PTQ4ViT [45] employs twin uniform quantization and Hessian-guided scale optimization. FQ-ViT [30] uses a power-of-two factor quantization to handle inter-channel variation in Layer Norm and log-INT-Softmax. RepQ-ViT [28] separates quantization and inference, optimizing accuracy and efficiency by employing scale-reparameterized quantizers. These methods apply fixed-precision quantization, assuming all layers support similar approximations, potentially leading to sub-optimal accuracy [36]. On the other hand, techniques like, VT-PTQ [32] adopt mixed precision for specific modules based on sensitivity metrics, while PMQ [41] and LRP-QViT [36] allocate bit-widths by assessing both layer sensitivity and contribution to the output,\nweights/activations quantized to same precision for all layers.\nweights/activations quantized to different precision for different layers."}, {"title": "CLAMP-ViT Framework", "content": "We present an overview of CLAMP-ViT. In this section, we first go through notations, computational process of ViTs and quantization strategy in the preliminaries, followed by a detailed description of the proposed contrastive loss and the two stages of CLAMP-ViT. Finally, the overall DFQ pipeline is summarized and presented (refer to Supplementary for the detailed Algorithm).\nPreliminaries\nNotations. Let \\(X \\in R^{H \\times W \\times C}\\) be the input image to an L-layer ViT, where (H, W, C) are the height, width, and channels, respectively (we ignore the batch dimension for simplicity). The input is partitioned into N non-overlapping patches that are then linearly transformed to d-dimensional patch embeddings, \\((\\mathbb{R}^{N \\times d})\\) that is passed through an encoder consisting of a series of transformer layers each composed of an MHSA and an MLP module. Each MHSA module is composed"}, {"title": "Contrastive Objective", "content": "Contrastive learning based on the infoNCE loss [39] helps learn an anchor sample from both the similar (positive) and dissimilar (negative) samples, typically using a softmax function to normalize the similarities into probabilities. However, infoNCE loss suffers from a disproportionate impact of a single positive and many negative samples [14]. This can affect learning the synthetic data as well as the quantized model parameters. Inspired by [39], we present a modified"}, {"title": "Synthetic Data Generation", "content": "Our goal in Stage 1 is to generate semantically rich and meaningful images that can reliably exploit inter-patch relations while ensuring informativeness to the quantization process. For each \"anchor patch\" of an MHSA output in the quantized model, we first locate positive and negative patches same layer id in the FP model, from a neighborhood of the anchor's spatial position. We then leverage a contrastive learning objective (see Sec. 3.2) that operates to maximize the similarity between the anchor and positive patches while minimizing similarity with the negative patches. Selection of the anchor patch from the quantized model for guiding data generations helps the model to recognize the semantic context within the generated data, during the subsequent quantization stage (informativeness).\nPatch Neighborhood. For every kth anchor patch \\(P_{ijk}\\) corresponding to the jth head in ith MHSA layer of the quantized model, we identify a neighborhood of size \\(N_{ijk}\\) with the same spatial location of \\(P_{ijk}\\) as the center, but located in the MHSA layer output of the FP model. Within \\(N_{ijk}\\), to identify the most semantically correlated patches (\\(P_N \\in N_{ijk}\\)) we use cosine similarity as follows,\n\\rho(i, j, k) = \\frac{P_{ijk} P_N}{||P_{ijk}||_2||P_N||_2}\n(5)\nThe cosine similarity \\(\\rho\\) is estimated \\(\\forall P_N \\in N_{ijk}\\). We then select the positive patches to be the top-n patches that have the highest \\(\\rho(i, j, k)\\) with the anchor patch and the rest of the patches in the neighborhood correspond to negative patches. We empirically set n = 4 in our experiments for a neighborhood of size 3\u00d73. We then compute \\(\\mathcal{L}_s^1\\) for all anchor patches for each attention head output over all layers to get the contrastive loss (\\(\\mathcal{L}_C^1\\)). We then compute the sample"}, {"title": "Stage 2: Quantization", "content": "We now present our layer-wise evolutionary search with a local contrastive loss-based fitness function to rank suitable quantization parameters from a large search space. We detail the fitness function and the search steps as follows.\nFitness Function. To evaluate the quantization parameters explored by the evolutionary search algorithm, we introduce a fitness function \\(\\mathcal{L}_F\\) that combines the contrastive loss \\(\\mathcal{L}_C^2\\) (Eq. (4)) and the MAE loss (\\(\\mathcal{L}_O\\)) computed with respect to the targets at the output (explained in Sec. 3.5) i.e, \\(\\mathcal{L}_F = \\mathcal{L}_C^2 + \\mathcal{L}_O\\). Unlike Stage 1, we use the intermediate activations after each transformer layer to compute the contrastive loss \\(\\mathcal{L}_C^2\\). For a ViT, let the set of intermediate representations of FP and quantized model be denoted as, \\(O^{fp} = \\{O_0^{fp}, O_1^{fp}, ....O_{L-1}^{fp}\\} \\) and \\(O^q = \\{O_0^q, O_1^q, ....O_{L-1}^q\\}\\), respectively. However, directly using high dimensional \\(O^{fp}\\) and \\(O^q\\) can result in high compute overhead. Therefore for each layer i, at the intermediate output, we perform mean pooling along the patch dimension to obtain a low-dimensional representation (\\(O_i \\in \\mathbb{R}^N\\)), reducing it by a factor of h \u00d7 d. We then apply the contrastive loss (Eq. (4)) sampled within the batch dimension on the low-dimensional \\(O_i^{fp}\\) and \\(O_i^q\\). For the layer output activation generated from each constituent of a batch of synthetic data, the anchor (AP) corresponds to the intermediate layer output of the quantized model, positive (\\(\\mathcal{X}^{p+}\\)) and negative (\\(\\mathcal{X}^{p-}\\)) samples correspond to set of intermediate layer outputs of FP model that have the same and different targets respectively, relative to the anchor within the batch.\nStep 1: Candidate Initialization. A candidate quantization solution is encoded as a set \\(\\mathcal{A}\\) of L tuples, such that for layer i, tuple \\(\\mathcal{A}[i]\\) represents the two quantization parameters (b, \\(\\gamma\\)). b can take any integer value between 2 and 8, while \\(\\gamma\\) is constrained to a uniform ball of radius 10-3 centered around, \\(\\gamma[i]_{init}\\).\nThe candidate scale factors are sampled as \\(\\gamma[i] = \\gamma[i]_{init} + f(-10^{-3},+10^{+3})\\), where f is a random sampling function. We begin the evolutionary search by creating a population via randomly sampling K candidate \\(\\mathcal{A}\\)s, each consisting of layer-wise quantization parameters. We then evaluate the fitness function \\(\\mathcal{L}_F\\) for each candidate \\(\\mathcal{A}\\). We create K tuples each with (\\(\\mathcal{A}, \\mathcal{L}_F\\)) to form the initial population. \\(\\mathcal{L}_F\\) of each initial candidate with corresponding set \\(\\mathcal{A}\\) is pre-computed and stored to avoid recomputation.\nStep 2: Re-generation (Crossover and Mutation). Each candidate in the population is ranked based on corresponding \\(\\mathcal{L}_F\\)s (lower the better) of which the top two serve as the parents for the next candidate generation (child). When evolving candidates, perturbing too many layer parameters based on parents can lead to search instability. To mitigate this, at each evolution step we employ a layer-wise regeneration approach, evolving a single transformer layer at a time based on chosen parents, keeping all other layer parameters to that of the top-1"}, {"title": "Diversity Promoting Selection", "content": "To avoid overfitting during search we follow [11] and introduce diversity into the population. In specific, we create 'P = 5' random parents and use each of them to crossover with the child generated in Step 2 and create 'P' diverse children following Eq. (6), Eq. (7).\nStep 4: Evaluation and Population Update. We evaluate all generated children in the steps above and acquire \\(\\mathcal{L}_F\\)s. The child generated in Step 2 and the corresponding fitness function value is added to the population. We then rank the diversity promoting children from Step 3 and select the best child to be added to the population for the next iteration. In our layer-wise evolutionary search strategy, we employ P passes over all layers of a ViT, and each layer is iterated over C cycles in each pass. So, the population is updated P\u00d7C\u00d7L times, i.e., the Step 2, 3, and 4 are iteratively executed P\u00d7C \u00d7 L times.\nActivation Quantization. We note that the sensitivity to quantization for activations is closely correlated to the sensitivity of weight parameters. Therefore for layer i, we determine the output activation quantization parameters as follows, \\(b_{act}[i] = min(8, b[i] \\times 2)\\) and \\(\\gamma_{act}[i] = \\gamma_{act}[i - 1] + \\gamma[i]\\)."}, {"title": "Overall Pipeline", "content": "In, we illustrate the complete CLAMP-ViT framework. To ensure adaptive data-generation and informativeness for quantization, we use a cyclic strategy between the two stages, updating generated data based on the quantized model's needs for optimal parameter search. The framework requires an input batch of B random Gaussian images \\(X_B\\), and corresponding task-specific targets \\(T_{GB}\\) (TGB for each task is detailed in Sec. 4). The targets direct the synthetic image generation towards task-specific goals as well as penalize inaccurate predictions of quantized model through the output loss \\(\\mathcal{L}_O\\),\n\\mathcal{L}^O = \\frac{1}{N_c}(\\||Q(X_B) - T_{GB} \\||_1 + ||FP(X_B) - T_{GB} \\||_1)\n(8)\nwhere \\(n_c\\) is the number of output classes (classification) or prediction map size (segmentation/detection). The quantized model is initialized to the best candidate from K tuples. The framework assumes a range of bit-widths and a single bit-width for mixed- and fixed-precision search, respectively. In Stage 1, \\(X_B\\) is fed to the frozen quantized (Q) and full-precision model (FP) to minimize the sample generation loss \\(\\mathcal{L}_{SG} = \\mathcal{L}_C^1 + \\mathcal{L}_O\\), updating \\(X_B\\) via backpropagation for G iterations. In Stage 2, we use the generated data to quantize Q for P \u00d7 C\u00d7L iterations by minimizing \\(\\mathcal{L}_F\\). We cyclically update the generated data every C/2 iterations. In every subsequent execution of Stage 1, we do not restart from Gaussian noise but use \\(X_B\\) from the previous Stage 1 execution and update it for G/2 iterations. In this manner, the two stages are executed alternately."}, {"title": "Experimental Results", "content": "Experimental Setup\nModels and Datasets. We evaluate CLAMP-ViT on various ViT model families (pre-trained FP models sourced from timm [40]) for image classification, object detection and semantic segmentation detailed as follows.\nImage Classification. We use ImageNet-1K [10] having 50K testset, with DeiT-B/T/S [38], Swin-T/S [31], and ViT-B/S [12] to evaluate accuracy.\nObject detection. We use the COCO 2017 dataset [29] having approximately 20K test data. Following [25,28,30], we use the Cascade Mask R-CNN [3] framework from MMdetection library [6] with DeiT-S and Swin-S as the backbone.\nSemantic Segmentation. We use the ADE20K dataset [52] with 3K test data encompassing 150 categories with DeiT-S and Swin-S as the backbone. We adopt the UperNet framework [42] in the MMsegmentation library [9] similar to [25].\nBaselines. CLAMP-ViT is evaluated against SOTA PTQ (real data) and DFQ (synthetic data) methods for quantizing models from FP in various vision tasks. For image classification, it's compared with fixed-precision methods like PSAQ-"}, {"title": "Quantization Results for Image Classification", "content": "As highlighted in Sec. 3.5, CLAMP-ViT requires a batch B of task-specific targets \\(T_{GB}\\). For image classification on the ImageNet-1K, we create \\(T_{GB} \\in \\mathbb{R}^{B \\times 1000}\\), where the class-wise probabilities are randomly determined and assigned. We discuss and compare the performance of CLAMP-ViT for two settings, fixed-precision and mixed-precision. In specific, as shown in Tab. 1 CLAMP-ViT consistently provides similar or better accuracy at W8/A8, while for lower precision W4/A8 CLAMP-ViT shows significant performance boost over all the existing alternatives. We yield ~ 2.2% and ~ 1% average accuracy improvement compared to DFQ methods [25,27] and data-driven methods, respectively. The superior performance of CLAMP-ViT can be attributed to the cyclically adaptive data-generation process, which ensures the generated data matches the requirements and representational capabilities of the quantized model and effective traversal of the search space through evolutionary search and contrastive learning. Whereas, PSAQ-ViT v2 [25], generates increasingly difficult samples which is less beneficial for aggressive 4-bit quantization. Surprisingly, PSAQ-ViT v1 [27] achieves poor accuracy of 25.34% (W4/A8) on ViT-B despite achieving reasonable accuracy on other ViTs. This result potentially supports our initial intuition that PSAQ-ViT [25,27] does not consider the informativeness of the generated data to the quantization process.\nEvident from Tab. 2, CLAMP-ViT consistently outperforms all baselines across models for the mixed-precision setting, maintaining accuracy close to"}, {"title": "Quantization Results for Object Detection", "content": "The target for object detection is \\(T_{GB} \\in \\mathbb{R}^{B \\times b \\times 5}\\) where bb is the number of bounding boxes in the image that is randomly selected from the integer set [1, 3]. \\(T_{GB} [B, :, 0]\\) corresponds to the bounding box category and \\(T_{GB} [B,:, 1:4]\\) is the bounding box coordinates x, y, w, h [16]. Tab. 5 presents the fixed- and mixed-precision performance of CLAMP-ViT with respect to the baselines. Across different settings and models, CLAMP-ViT consistently outperforms DFQ method PSAQ-ViT v2 [25] by 0.6 box AP and 0.4 mask AP on average while closely matching performance to the SoTA data-driven method, RepQ-ViT [28]. Similar to Sec. 4.2, we observe improved performance with mixed-precision quantization, achieving near FP baseline performance. The average W/A for mixed-precision quantization for object detection is found to be higher than that for image classification due to the higher complexity of the task demanding larger bit-widths to maintain accuracy."}, {"title": "Quantization Results for Semantic Segmentation", "content": "The target \\(T_{GB}\\) for this task is a pixel-wise classification map of the same size as \\(X_p\\) i.e, \\(T_{GB} \\in \\mathbb{R}^{B \\times 150 \\times H \\times W}\\). In Tab. 6, we show the quantization performance comparison, where CLAMP-ViT achieves average weight bit-width close"}, {"title": "Analysis of Generated Samples", "content": "4 visualizes generated samples from PSAQ-ViT v1 [27], v2 [25], and CLAMP-ViT (after 1st round of stage 1 execution). PSAQ-ViT v1 (Fig. 4(a)) creates images with clear class-specific foregrounds but with overly simplistic and uniform backgrounds, resulting in a lack of realism potentially affecting model accuracy. PSAQ-ViT v2 (Fig. 4(b)) introduces more complex details but fails to convey meaningful semantic information, generating images with intricate but semantically vague structures due to its unguided, difficulty-increasing data-generation strategy. In contrast, CLAMP-ViT (Fig. 4(c)) excels by synthesizing data that mirrors real-world imagery, showcasing a sophisticated understanding of semantic relationships between patches. It ensures objects are detailed and in contextually fitting backgrounds, boosting realism and informativeness. For example, CLAMP-ViT places boats on water and zebras in grasslands showing its capability for creating semantically relevant and visually consistent synthetic data. We believe our patch semantics exploration with a contrastive objective, makes image generation informative that mimic real-world scenes."}, {"title": "Ablations and Discussions", "content": "Evolutionary Search Parameters. In Fig. 5(a), we detail an experiment to determine the ideal number of passes P and cycles C for the evolutionary search process by studying the variation in Top-1 accuracy of DeiT-S with different passes and cycles keeping the other fixed at their optimal value . It is evident that a cycle count of C=6 is optimal, as accuracy tends to decline with more cycles. Conversely, passes show a modest yet consistent improvement beyond 10, but due to the substantial rise in computational complexity, P=10 is deemed the most suitable choice.\nEffect of Batch Size B. We also show the accuracy comparison with different batch sizes ranging from 8 to 64. It is apparent that there is minimal increase in accuracy beyond 32 for CLAMP-ViT, justfiying the choice of batch\nChoice of Objective Function. The study in Tab. 7 examines how different loss function components of \\(\\mathcal{L}_{SG}\\) i.e., \\(\\mathcal{L}_C^1\\) and \\(\\mathcal{L}_O\\) affect synthetic data generation effectiveness and its impact on top-1 accuracy of W4/A8 quantization of DeiT-S.We chose fixed-precision quantization\nto avoid any bias from mixed-precision quantization, which might typically favor higher bitwidths to lessen accuracy loss due to low-quality images. Results show that a linear mix of \\(\\mathcal{L}_C^1\\) and \\(\\mathcal{L}_O\\) (\\(\\mathcal{L}_{SG}\\)) achieves highest accuracy, while using \\(\\mathcal{L}_O\\) alone leads to the lowest, indicating its limited utility in leveraging ViTs for synthetic data. corresponds to the patch similarity metric employed in [25]. While combining  does offer a moderate accuracy boost, it falls short of the performance with \\(\\mathcal{L}_{SG}\\), due to inherent limitations of the patch similarity metric highlighted in Sec. 1. Furthermore, using all three loss functions simultaneously closely matches the performance of \\(\\mathcal{L}_{SG}\\) further demonstrating that our proposed \\(\\mathcal{L}_C^1\\) has the major contribution towards final accuracy.\nFor mixed-precision quantization of DeiT-S, the fitness function's accuracy (\\(\\mathcal{L}_F = \\mathcal{L}_C^2 + \\mathcal{L}_O\\)) is compared against global contrastive loss [14], MSE, and KL-divergence. The accuracy analysis shows MSE and KL-divergence tend to overfit to synthetic data, evidenced by plateauing accuracy. Meanwhile, global contrastive loss initially matches but then accuracy gap widens from CLAMP-ViT's performance which is due to premature convergence, implying"}, {"title": "Effect of Adaptivity", "content": "We investigate the effectiveness of the cyclic evolution every C/2 iterations to ensure the data generation adapts to the requirements of the quantized model. It can be observed that with adaptivity, we are not only able to achieve lower average W/A but also have improved accuracy.\nEffect of Informativeness. To show CLAMP-ViT's effectiveness in generating informative data, we compare misprediction rates in PSAQ-ViT v1, v2, and CLAMP-ViT during quantization. PSAQ-ViT has higher misprediction rates (34% for v1, 41% for v2) than CLAMP-ViT's 22%, indicating PSAQ-ViT's data is less informative, leading to erratic predictions, sub-optimal performance.\nLimitation Discussion: Runtime Comparison. In, we show the runtime and top-1 accuracy of different techniques on an NVIDIA Titan GPU for DeiT-S. CLAMP-ViT achieves significantly higher accuracy compared to the other methods (fixed-precision) with only a minimal increase in runtime (5%\u2191) compared to the best DFQ method. Note, Evol-Q takes similar time to calibrate a pre-quantized model on real data."}, {"title": "Conclusions", "content": "This paper presents CLAMP-ViT, a novel mixed-precision DFQ technique using cyclic adaptation and contrastive learning. It employs patch-level contrastive learning that leverages properties of the MHSA modules for data generation. A local contrastive objective and layer-wise evolutionary search identify optimal quantization parameters while ensuring a smooth loss landscape. Experiments across CV tasks show superior performance of CLAMP-ViT, achieving up to 3% top-1 accuracy for classification, 0.6 mAP for detection, and 1.5 mIoU for segmentation. Future work aims to focus on extending its application to a wider range of architectures, like VLMs. While this study focuses on a useful impact and beneficial application of synthetic data generation for optimized and carbon efficient models for deployment, it is important to also be cognizant of the potential adverse effects of synthetic data such as deepfakes or racial biases."}]}