{"title": "CLAMP-ViT: Contrastive Data-Free Learning for Adaptive Post-Training Quantization of ViTs", "authors": ["Akshat Ramachandran", "Souvik Kundu", "Tushar Krishna"], "abstract": "We present CLAMP-ViT, a data-free post-training quantization method for vision transformers (ViTs). We identify the limitations of recent techniques, notably their inability to leverage meaningful inter-patch relationships, leading to the generation of simplistic and semantically vague data, impacting quantization accuracy. CLAMP-ViT employs a two-stage approach, cyclically adapting between data generation and model quantization. Specifically, we incorporate a patch-level contrastive learning scheme to generate richer, semantically meaningful data. Furthermore, we leverage contrastive learning in layer-wise evolutionary search for fixed- and mixed-precision quantization to identify optimal quantization parameters while mitigating the effects of a non-smooth loss landscape. Extensive evaluations across various vision tasks demonstrate the superiority of CLAMP-ViT, with performance improvements of up to 3% in top-1 accuracy for classification, 0.6 mAP for object detection, and 1.5 mIoU for segmentation at similar or better compression ratio over existing alternatives. Code is available at https://github.com/georgia-tech-synergy-lab/CLAMP-ViT.git", "sections": [{"title": "Introduction", "content": "Vision transformers [12] (ViTs) have recently gained a lot of traction due to their state-of-the-art (SoTA) performance across various computer vision (CV) tasks [34,37,48, 50]. Concurrently, the growing need to deploy these parameter-heavy models at the resource-limited edge [14], has inspired research on various model compression techniques. Model quantization [14, 30, 36, 45] has emerged as a popular technique to achieve memory and compute efficient deployment. Quantization reduces memory footprint and improves computation speed of a model by mapping full-precision (FP) weights to reduced precision formats (e.g., < 8-bit INT) [13, 17, 19, 46]. In particular, quantization-aware training (QAT) allows a model to train by taking quantization approximation in to account, enabling ease of quantization. Post-training quantization (PTQ), in contrast, acts as a plug-and-play quantization applied on a pre-trained model. PTQ has become popular as it can leverage the pre-trained model and does not add training overhead [21, 23, 26]. We thus consider the PTQ setting in this work. However, PTQ requires access to a calibration set that is often drawn from the training data [18,45]. This may be infeasible in situations involving privacy and security concerns [20,49], making these techniques ill-suited to vield optimal performance.\nRecent works [25, 27] propose data-free PTQ (DFQ), generating synthetic calibration data \\(T_{syn}\\) from Gaussian noise [7, 27], embedding information from the original dataset \\(T_{orig}\\), where \\(T_{syn} << T_{orig}\\).\nExisting DFQ methods for CNNs [2,51] exploit the batch-normalization (BN) layer statistics [33,44] to generate synthetic samples mimicking the original data distribution. For ViTs, absence of the BN layer makes these techniques obsolete. Recent efforts to extend DFQ to ViTs include PSAQ-ViT v1 [27] and PSAQ-ViT v2 [25]. They rely on information embedded in the attention score output of the multi-head self attention (MHSA) layer. PSAQ-ViT v1 and v2 [25,27] introduce a relative value metric namely, patch similarity, to optimize Gaussian noise towards synthetic data by maximizing the entropy of patch similarity in a global manner. However, the metric considered in PSAQ-ViT v1 and v2 assumes all patches\u00b9to be equally important, without considering spatial sensitivity [47]. This may fail to capture semantically meaningful inter-patch relations, potentially affecting robustness of the synthetic data. As we can see in Fig. 1(a) even insignificant perturbations in the generated images (augmenting pixels) or weights (to simulate quantization process) may cause significant jaggedness in the loss landscape of PSAQ-ViT v2. This also implies that the predictions may have large discrepancy even for small input/weight perturbation. Moreover, the synthetic data generation stage in these methods does not consider the informativeness of the generated samples towards the quantization process, nor do they establish countermeasures to ameliorate the non-smooth loss landscape during quantization, resulting in sub-optimal parameter search and poor generalization to test set [5].\nOur contributions. The discussion above hints at the potential limitation of [25,27] in capturing semantically meaningful and robust inter-patch relationships to generate synthetic data that is well-suited to quantization. Towards solving these limitations, we present contrastive data-free learning for adaptive post-training quantization of ViTs (CLAMP-ViT), a general DFQ method applicable to a wide range of vision tasks. To the best of our knowledge, CLAMP-"}, {"title": "Related Works", "content": "Data-Driven PTQ for ViTs. PTQ offers an efficient alternative to QAT [23,26] by directly quantizing pre-trained models without the need for compute-heavy retraining. In specific, PTQ4ViT [45] employs twin uniform quantization and Hessian-guided scale optimization. FQ-ViT [30] uses a power-of-two factor quantization to handle inter-channel variation in Layer Norm and log-INT-Softmax. RepQ-ViT [28] separates quantization and inference, optimizing accuracy and efficiency by employing scale-reparameterized quantizers. These methods apply fixed-precision quantization, assuming all layers support similar approximations, potentially leading to sub-optimal accuracy [36]. On the other hand, techniques like, VT-PTQ [32] adopt mixed precision for specific modules based on sensitivity metrics, while PMQ [41] and LRP-QViT [36] allocate bit-widths by assessing both layer sensitivity and contribution to the output,"}, {"title": "CLAMP-ViT Framework", "content": "We present an overview of CLAMP-ViT in Fig. 2. In this section, we first go through notations, computational process of ViTs and quantization strategy in the preliminaries, followed by a detailed description of the proposed contrastive loss and the two stages of CLAMP-ViT. Finally, the overall DFQ pipeline is summarized and presented (refer to Supplementary for the detailed Algorithm).\nPreliminaries\nNotations. Let \\(X \\in R^{H \\times W \\times C}\\) be the input image to an L-layer ViT, where (H, W, C) are the height, width, and channels, respectively (we ignore the batch dimension for simplicity). The input is partitioned into N non-overlapping patches that are then linearly transformed to d-dimensional patch embeddings, \\((R^{N \\times d})\\) that is passed through an encoder consisting of a series of transformer layers each composed of an MHSA and an MLP module. Each MHSA module is composed"}, {"title": "Contrastive Objective", "content": "Contrastive learning based on the infoNCE loss [39] helps learn an anchor sample from both the similar (positive) and dissimilar (negative) samples, typically using a softmax function to normalize the similarities into probabilities. However, infoNCE loss suffers from a disproportionate impact of a single positive and many negative samples [14]. This can affect learning the synthetic data as well as the quantized model parameters. Inspired by [39], we present a modified"}, {"title": "Stage 1: Synthetic Data Generation", "content": "Our goal in Stage 1 is to generate semantically rich and meaningful images that can reliably exploit inter-patch relations while ensuring informativeness to the quantization process. For each \"anchor patch\" of an MHSA output in the quantized model, we first locate positive and negative patches from same layer id in the FP model, from a neighborhood of the anchor's spatial position. We then leverage a contrastive learning objective (see Sec. 3.2) that operates to maximize the similarity between the anchor and positive patches while minimizing similarity with the negative patches. Selection of the anchor patch from the quantized model for guiding data generations helps the model to recognize the semantic context within the generated data, during the subsequent quantization stage (informativeness).\nPatch Neighborhood. For every kth anchor patch \\(P_{ijk}\\) corresponding to the jth head in ith MHSA layer of the quantized model, we identify a neighborhood of size \\(N_{ijk}\\) with the same spatial location of \\(P_{ijk}\\) as the center, but located in the MHSA layer output of the FP model. Within \\(N_{ijk}\\), to identify the most semantically correlated patches (\\(P_{N} \\in N_{ijk}\\)) we use cosine similarity as follows,\n\\(\\rho (i, j, k)=\\frac{P_{i j k} \\cdot P_{N}}{\\left|\\left|P_{i j k}\\right|\\right|_{2}\\left|\\left|P_{N}\\right|\\right|_{2}}\nThe cosine similarity \\(\\rho\\) is estimated \\(\\forall P_{N} \\in N_{i j k}\\). We then select the positive patches to be the top-n patches that have the highest \\(\\rho(i, j, k)\\) with the anchor patch and the rest of the patches in the neighborhood correspond to negative patches. We empirically set n = 4 in our experiments for a neighborhood of size 3\u00d73. We then compute \\(L_{i j}^{c 1}\\) for all anchor patches for each attention head output over all layers to get the contrastive loss (\\(L^{c 1}\\)). We then compute the sample"}, {"title": "Stage 2: Quantization", "content": "We now present our layer-wise evolutionary search with a local contrastive loss-based fitness function to rank suitable quantization parameters from a large search space. We detail the fitness function and the search steps as follows.\nFitness Function. To evaluate the quantization parameters explored by the evolutionary search algorithm, we introduce a fitness function \\(L^{F}\\) that combines the contrastive loss \\(L^{c 2}\\) (Eq. (4)) and the MAE loss (\\(L^{O}\\)) computed with respect to the targets at the output (explained in Sec. 3.5) i.e, \\(L^{F}=L^{c 2}+L^{O}\\). Unlike Stage 1, we use the intermediate activations after each transformer layer to compute the contrastive loss \\(L^{c 2}\\). For a ViT, let the set of intermediate representations of FP and quantized model be denoted as, \\(O^{f p}=\\left\\{O_{0}^{f p}, O_{1}^{f p}, \\ldots, O_{L-1}^{f p}\\right\\}\\) and \\(O^{q}=\\left\\{O_{0}^{q}, O_{1}^{q}, \\ldots, O_{L-1}^{q}\\right\\}\\), respectively. However, directly using high dimensional \\(O^{f p}\\) and \\(O^{q}\\) can result in high compute overhead. Therefore for each layer i, at the intermediate output, we perform mean pooling along the patch dimension to obtain a low-dimensional representation (\\(O_{i} \\in R^{N}\\)), reducing it by a factor of h \u00d7 d. We then apply the contrastive loss (Eq. (4)) sampled within the batch dimension on the low-dimensional \\(O_{i}^{f p}\\) and \\(O_{i}^{q}\\). For the layer output activation generated from each constituent of a batch of synthetic data, the anchor (AP) corresponds to the intermediate layer output of the quantized model, positive (\\(X^{P+}\\)) and negative (\\(X^{P-}\\)) samples correspond to set of intermediate layer outputs of FP model that have the same and different targets respectively, relative to the anchor within the batch.\nStep 1: Candidate Initialization. A candidate quantization solution is encoded as a set A of L tuples, such that for layer i, tuple A[i] represents the two quantization parameters (b, \u03b3). b can take any integer value between 2 and 8, while y is constrained to a uniform ball of radius 10-3 centered around, \\(\\gamma[i]_{init}\\). The candidate scale factors are sampled as \\(\\gamma[i]=\\gamma[i]_{init}+f\\left(-10^{-3},+10^{+3}\\right)\\), where f is a random sampling function. We begin the evolutionary search by creating a population via randomly sampling K candidate As, each consisting of layer-wise quantization parameters. We then evaluate the fitness function \\(L^{F}\\) for each candidate A. We create K tuples each with (A, \\(L^{F}\\)) to form the initial population. \\(L^{F}\\) of each initial candidate with corresponding set A is pre-computed and stored to avoid recomputation.\nStep 2: Re-generation (Crossover and Mutation). Each candidate in the population is ranked based on corresponding \\(L^{F}s\\) (lower the better) of which the top two serve as the parents for the next candidate generation (child). When evolving candidates, perturbing too many layer parameters based on parents can lead to search instability. To mitigate this, at each evolution step we employ a layer-wise regeneration approach, evolving a single transformer layer at a time based on chosen parents, keeping all other layer parameters to that of the top-1"}, {"title": "Experimental Results", "content": "Models and Datasets. We evaluate CLAMP-ViT on various ViT model families (pre-trained FP models sourced from timm [40]) for image classification, object detection and semantic segmentation detailed as follows.\nImage Classification. We use ImageNet-1K [10] having 50K testset, with DeiT-B/T/S [38], Swin-T/S [31], and ViT-B/S [12] to evaluate accuracy.\nObject detection. We use the COCO 2017 dataset [29] having approximately 20K test data. Following [25,28,30], we use the Cascade Mask R-CNN [3] framework from MMdetection library [6] with DeiT-S and Swin-S as the backbone.\nSemantic Segmentation. We use the ADE20K dataset [52] with 3K test data encompassing 150 categories with DeiT-S and Swin-S as the backbone. We adopt the UperNet framework [42] in the MMsegmentation library [9] similar to [25].\nBaselines. CLAMP-ViT is evaluated against SOTA PTQ (real data) and DFQ (synthetic data) methods for quantizing models from FP in various vision tasks. For image classification, it's compared with fixed-precision methods like PSAQ-"}, {"title": "Quantization Results for Image Classification", "content": "As highlighted in Sec. 3.5, CLAMP-ViT requires a batch B of task-specific targets \\(T_{G B}\\). For image classification on the ImageNet-1K, we create \\(T_{G B} \\in R^{B \\times 1000}\\), where the class-wise probabilities are randomly determined and assigned. We discuss and compare the performance of CLAMP-ViT for two settings, fixed-precision (Tab. 1) and mixed-precision (Tab. 2). In specific, as shown in Tab. 1 CLAMP-ViT consistently provides similar or better accuracy at W8/A8, while for lower precision W4/A8 CLAMP-ViT shows significant performance boost over all the existing alternatives. We yield ~ 2.2% and ~ 1% average accuracy improvement compared to DFQ methods [25,27] and data-driven methods, respectively. The superior performance of CLAMP-ViT can be attributed to the cyclically adaptive data-generation process, which ensures the generated data matches the requirements and representational capabilities of the quantized model and effective traversal of the search space through evolutionary search and contrastive learning. Whereas, PSAQ-ViT v2 [25], generates increasingly difficult samples which is less beneficial for aggressive 4-bit quantization. Surprisingly, PSAQ-ViT v1 [27] achieves poor accuracy of 25.34% (W4/A8) on ViT-B despite achieving reasonable accuracy on other ViTs. This result potentially supports our initial intuition that PSAQ-VIT [25,27] does not consider the informativeness of the generated data to the quantization process.\nEvident from Tab. 2, CLAMP-ViT consistently outperforms all baselines across models for the mixed-precision setting, maintaining accuracy close to"}, {"title": "Quantization Results for Object Detection", "content": "The target for object detection is \\(T_{G B} \\in R^{B \\times b b \\times 5}\\) where bb is the number of bounding boxes in the image that is randomly selected from the integer set [1, 3]. \\(T_{G B}[B, :, 0]\\) corresponds to the bounding box category and \\(T_{G B}[B,:, 1:4]\\) is the bounding box coordinates x, y, w, h [16]. Tab. 5 presents the fixed- and mixed-precision performance of CLAMP-ViT with respect to the baselines. Across different settings and models, CLAMP-ViT consistently outperforms DFQ method PSAQ-ViT v2 [25] by 0.6 box AP and 0.4 mask AP on average while closely matching performance to the SoTA data-driven method, RepQ-ViT [28]. Similar to Sec. 4.2, we observe improved performance with mixed-precision quantization, achieving near FP baseline performance. The average W/A for mixed-precision quantization for object detection is found to be higher than that for image classification due to the higher complexity of the task demanding larger bit-widths to maintain accuracy."}, {"title": "Quantization Results for Semantic Segmentation", "content": "The target \\(T_{G B}\\) for this task is a pixel-wise classification map of the same size as \\(X_{p}\\) i.e, \\(T_{G B} \\in R^{B \\times 150 \\times H \\times W}\\). In Tab. 6, we show the quantization performance comparison, where CLAMP-ViT achieves average weight bit-width close"}, {"title": "Analysis of Generated Samples", "content": "Fig. 4 visualizes generated samples from PSAQ-ViT v1 [27], v2 [25], and CLAMP-ViT (after 1st round of stage 1 execution). PSAQ-ViT v1 (Fig. 4(a)) creates images with clear class-specific foregrounds but with overly simplistic and uniform backgrounds, resulting in a lack of realism potentially affecting model accuracy. PSAQ-ViT v2 (Fig. 4(b)) introduces more complex details but fails to convey meaningful semantic information, generating images with intricate but semantically vague structures due to its unguided, difficulty-increasing data-generation strategy. In contrast, CLAMP-ViT (Fig. 4(c)) excels by synthesizing data that mirrors real-world imagery, showcasing a sophisticated understanding of semantic relationships between patches. It ensures objects are detailed and in contextually fitting backgrounds, boosting realism and informativeness. For example, CLAMP-ViT places boats on water and zebras in grasslands (Fig. 4(c), row 2), showing its capability for creating semantically relevant and visually consistent synthetic data. We believe our patch semantics exploration with a contrastive objective, makes image generation informative that mimic real-world scenes."}, {"title": "Ablations and Discussions", "content": "Evolutionary Search Parameters. In Fig. 5(a), we detail an experiment to determine the ideal number of passes P and cycles C for the evolutionary search process by studying the variation in Top-1 accuracy of DeiT-S with different passes and cycles keeping the other fixed at their optimal value (Tab. 3). It is evident that a cycle count of C=6 is optimal, as accuracy tends to decline with more cycles. Conversely, passes show a modest yet consistent improvement beyond 10, but due to the substantial rise in computational complexity, P=10 is deemed the most suitable choice.\nEffect of Batch Size B. We also show the accuracy comparison with different batch sizes ranging from 8 to 64 in Fig. 5(c). It is apparent that there is minimal increase in accuracy beyond 32 for CLAMP-ViT, justfiying the choice of batch"}, {"title": "Conclusions", "content": "This paper presents CLAMP-ViT, a novel mixed-precision DFQ technique using cyclic adaptation and contrastive learning. It employs patch-level contrastive learning that leverages properties of the MHSA modules for data generation. A local contrastive objective and layer-wise evolutionary search identify optimal quantization parameters while ensuring a smooth loss landscape. Experiments across CV tasks show superior performance of CLAMP-ViT, achieving up to 3% top-1 accuracy for classification, 0.6 mAP for detection, and 1.5 mIoU for segmentation. Future work aims to focus on extending its application to a wider range of architectures, like VLMs. While this study focuses on a useful impact and beneficial application of synthetic data generation for optimized and carbon efficient models for deployment, it is important to also be cognizant of the potential adverse effects of synthetic data such as deepfakes or racial biases."}]}