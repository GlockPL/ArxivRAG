{"title": "PainterNet: Adaptive Image Inpainting with Actual-Token Attention and Diverse Mask Control", "authors": ["Ruichen Wang", "Junliang Zhang", "Qingsong Xie", "Chen Chen", "Haonan Lu"], "abstract": "Recently, diffusion models have exhibited superior performance in the area of image inpainting. Inpainting methods based on diffusion models can usually generate realistic, high-quality image content for masked areas. However, due to the limitations of diffusion models, existing methods typically encounter problems in terms of semantic consistency between images and text, and the editing habits of users. To address these issues, we present PainterNet, a plugin that can be flexibly embedded into various diffusion models. To generate image content in the masked areas that highly aligns with the user input prompt, we proposed local prompt input, Attention Control Points (ACP), and Actual-Token Attention Loss (ATAL) to enhance the model's focus on local areas. Additionally, we redesigned the MASK generation algorithm in training and testing dataset to simulate the user's habit of applying MASK, and introduced a customized new training dataset, PainterData, and a benchmark dataset, PainterBench. Our extensive experimental analysis exhibits that PainterNet surpasses existing state-of-the-art models in key metrics including image quality and global/local text consistency.", "sections": [{"title": "1. Introduction", "content": "The rapid progress of diffusion models [13, 14] has significantly advanced image generation techniques [4, 34, 37], which are utilized in various applications, including prompt-based conditional editing [3, 12], controllable generation [28, 52], and personalized image synthesis [9, 24, 36]. Among these, image inpainting [49] is a key application that uses guidance information to restore missing regions in images, allowing users to create content in specified areas based on textual prompts, making it a highly sought-after feature in recent years.\nConventional diffusion-based inpainting methods can be divided into two main categories: modifying the sampling strategy [1, 2, 6, 25, 51] and utilizing dedicated inpainting models [35, 43, 45, 50]. The former involves sampling from a pretrained diffusion model over masked regions while maintaining the integrity of unmasked areas, which is a training-free approach. However, this method often leads to discontinuities in image generation due to inadequate attention to mask boundaries and contextual information. The latter approach enhances the diffusion model by extending input channels and fine-tuning to effectively address corrupted images and masks (as shown in Fig. 1(a)). While dedicated inpainting models yield improved results, they require fine-tuning of the diffusion backbone and handle both conditioning and generation within a single UNet branch. This not only necessitates extensive data but also limits the model's portability.\nRecently, control-based image colorization techniques, such as ControlNet-Inpainting [52] and BrushNet [16], have emerged as promising alternatives. These methods incorporate additional control branches to achieve flexible, plug-and-play solutions, and can perform well without the need for heavy training of dedicated inpainting models. Despite the significant improvements and flexibility offered by existing control-based methods, they still face several challenges. ControlNet-Inpainting [52] has flaws in terms of image pixel control, making it difficult for inpainted images to remain completely consistent with the original input. BrushNet [16] is trained on segmentation mask data(as shown in the first row of Fig. 2(b)), which introduces additional information(mask shape) during the training process while more flexible and personalized mask inputs are expected considering user habit. In addition, all the above methods usually rely on global textual prompts that do not provide localized detail descriptions, which may lead to in-"}, {"title": "2. Related Work", "content": "In recent years, diffusion-based image generation techniques [1, 14, 20] have gradually gained significant attention as a research field. This surge of interest is primarily due to the superior image generation quality of diffusion models compared to traditional generative models (such as GANS [10, 17, 18]). Therefore, with the introduction of diffusion-based models, the quality of text-to-image generation has significantly improved. DALL-E 2 [34] utilizes CLIP [32] to achieve text-to-image mapping through a diffusion mechanism and trains a CLIP decoder. Imagen [37], on the other hand, leverages large pretrained language models like T5 [33] to achieve exceptional alignment between images and text using textual data. Stable Diffusion [35] employs efficient encoding in latent space to generate images with rich details and diverse styles. Notably, stable diffusion is one of the most popular open-source text-to-image generation methods, and several different versions have been developed, ranging from stable diffusion v1.5 (SD 1.5) to stable diffusion v2.0 (SD 2.0), and then to stable diffusion XL (SD XL) [28]. Each version has demonstrated significant improvements in image fidelity and generation speed. Additionally, in downstream applications like generating anime styles [11], Van Gogh styles [7], and specific roles [40], diffusion models demonstrate remarkable style adaptability and high-quality generation capabilities.\nOur PainterNet is also built on the foundation of stable diffusion models, implementing hierarchical dense control to fully leverage their capability for generating high-fidelity images. It can also be easily adapted to various downstream tasks, providing plug-and-play flexibility."}, {"title": "2.2. Image Inpainting", "content": "Given a masked scene image, the objective of image in-painting is to recover the occluded regions in a natural and plausible manner [31, 49]. In the early stages of development, most deep learning methods were based on paradigms such as autoencoders [29, 54], autoregressive transformers [42], and GAN-based paradigms [38, 48, 53, 55]. These methods typically relied on auxiliary handcrafted features, resulting in suboptimal performance. Recently, techniques"}, {"title": "3. Method", "content": "Our proposed PainterNet, shown in Fig.3, is designed to precisely capture detailed information in masked regions during image generation. To this end, we adopt a dual-branch strategy to embed mask information, leveraging hierarchical dense control through layer control points and Attention Control Points (ACP). Additionally, we introduce a Actual-Token Attention Loss (ATAL) that directs the model's focus to the masked regions, ensuring alignment between text prompts and generated image content. In addition, to meet the high-fidelity demands of realistic mask shapes and local semantics, we propose a novel data construction pipeline. This pipeline combines local textual prompts with diverse masks (e.g., bounding box, irregular, segmentation-based), effectively training our model for improved performance."}, {"title": "3.1. Preliminaries", "content": "In this paper, we employ Stable Diffusion(SD) [35] as the foundational model for image restoration. The model takes a text prompt P as input and generates the corresponding image xo. Stable Diffusion comprises three main components: an autoencoder (E(\u00b7), D(\u00b7)), a CLIP text encoder T(), and a U-Net $\\epsilon_{\\theta}(\u00b7)$. Typically, the model is trained under the following diffusion loss constraint:\n$\\mathcal{L}_{diff} = \\mathbb{E}_{z_0,\\epsilon \\sim \\mathcal{N}(0,1),t,c} [||\\epsilon - \\epsilon_{\\theta} (z_t, t, c)||_2^2]$, (1)"}, {"title": "3.2. PainterNet", "content": "To help the model better capture masked image information and achieve high-quality image inpainting, we designed PatinerNet based on a diffusion model. PatinerNet introduces an additional branch copied from the pretrained Stable Diffusion U-Net, as shown in Fig.3. Not like BrtushNet [16] that excludes its cross-attention layers, we keep all the U-Net structure but only change the input dims from 4(for $z_t$) to 9(4 for $z_t$, 4 for masked image latent $z_m$ and 1 for downsampled mask m). The $z_m$ is extracted using the same VAE model from Stable Diffusion. Moreover, instead of using global textual prompts, we utilize local textual prompts for cross-attention layers both in SD U-Net and PatinerNet-Branch. So we have $c_l = \\tau(P_l)$, where $P_l$ is the local textual prompt.Unlike most existing control-based methods that only use layer control points, we further proposed Attention Control Points(ACP) to apply directly influence on cross-attention layers as the global textual prompts has been changed to local ones.\nSimilar to ControlNet [52], we employ zero convolution layers to connect the frozen model with the trainable PatinerNet, avoiding noise interference during the early stages of training. The features of PatinerNet are gradually inserted into the frozen diffusion model, enabling pixel-level fine-grained control:\n$\\begin{aligned}\\epsilon'_{0,attn}(z_t, t, c_l)_i &= \\epsilon_{0,attn}(z_t, t, c_l)_i + \\omega \\cdot Z_{attn} (attn_i)\\\\epsilon'_{0,lay}(z_t, t, c_l)_i &= \\epsilon_{0,lay}(z_t, t, c_l)_i + \\omega \\cdot Z_{lay} (lay_i)\\end{aligned}$ (2)\nwhere $\\epsilon_{0,attn}^{PN}, \\epsilon_{0,lay}^{PN}$ denote the cross-attention output and layer output in layer i(i \u2208 [1, N]) of PainterNet $PN$, where N is the number of layers. And $\\epsilon_{0,attn}, \\epsilon_{0,lay}$ denote the cross-attention input and layer input in layer i of SD U-Net $\\epsilon_{\\theta}$. [.] refers to the concatenation operation, and $Z_{attn}, Z_{lay}$ are zero-convolution operations. $\\omega$ is the preservation scale used to adjust the influence of PainterNet on the pre-trained diffusion model."}, {"title": "3.3. Actual-Token Attention Loss", "content": "During the training of diffusion models, the diffusion loss $\\mathcal{L}_{diff}$ ensures the model's ability to generate content, but"}, {"title": "4. Experiments", "content": "Currently, commonly used datasets in the field of image synthesis include CelebA [23], CelebA-HQ [15], ImageNet [8], MSCOCO [21], and Open Images [19]. However, these datasets are not suitable for training and evaluating image synthesis methods based on diffusion models due to issues such as small focused regions or low quality. The recently proposed BrushBench [16] benchmark is specifically designed for image synthesis methods based on diffusion models. However, the captions in BrushBench are global text prompts, which overlook local detail descriptions. Moreover, most of the masks in BrushBench are segmentation-based, which can be challenging for users"}, {"title": "4.1. Experimental Setup", "content": "Benchmark."}]}