{"title": "On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems", "authors": ["Siyu Wang", "Xiaocong Chen", "Lina Yao"], "abstract": "In Reinforcement Learning-based Recommender Systems (RLRS), the complexity and dynamism of user interactions often result in high-dimensional and noisy state spaces, making it challenging to discern which aspects of the state are truly influential in driving the decision-making process. This issue is exacerbated by the evolving nature of user preferences and behaviors, requiring the recommender system to adaptively focus on the most relevant information for decision-making while preserving generaliability. To tackle this problem, we introduce an innovative causal approach for decomposing the state and extracting Causal-InDispensable State Representations (CIDS) in RLRS. Our method concentrates on identifying the Directly Action-Influenced State Variables (DAIS) and Action-Influence Ancestors (AIA), which are essential for making effective recommendations. By leveraging conditional mutual information, we develop a framework that not only discerns the causal relationships within the generative process but also isolates critical state variables from the typically dense and high-dimensional state representations. We provide theoretical evidence for the identifiability of these variables. Then, by making use of the identified causal relationship, we construct causal-indispensable state representations, enabling the training of policies over a more advantageous subset of the agent's state space. We demonstrate the efficacy of our approach through extensive experiments, showcasing our method outperforms state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender Systems (RS) are crucial in navigating the vast digital environment, tailoring suggestions to align with individual user preferences. The integration of Reinforcement Learning (RL) within RS, known as RLRS, has transformed the recommendation experience into a dynamic, sequential decision-making process. Unlike traditional systems that passively suggest content based on static data, RLRS actively engage with users, continuously adapting recommendations in response to real-time feedback. This approach seeks to maximize long-term user engagement by treating each interaction as an opportunity to learn and enhance the personalization of content, thereby maintaining alignment with the evolving preferences and behaviors of the users.\nIn RLRS, the efficiency hinges on three essential components: State Representation, Policy Optimization, and Reward Formulation [1, 8]. While much of the current research in RLRS is centered on policy optimization [5, 7, 33, 39] and reward formulation [4, 9, 16], the role of state representation should not be understated. The state in RLRS is a composite of varied attributes: user characteristics (like age, gender, and recent activities), item properties (including price, category, and popularity), and contextual elements (such as time and location). Effectively distilling this rich tapestry of information poses a significant challenge. Neglecting key features might result in suboptimal recommendations, while incorporating too much detail could clutter the system with irrelevant data, diminishing its predictive accuracy. Recent advances in representation learning algorithms in RL aim to extract abstract features from high-dimensional data, which has proven beneficial in enhancing the efficiency of existing RL algorithms [12, 15, 17]. For instance, Zhang et al. [36] developed a method to learn representations by ignoring task-irrelevant information through the bisimulation metric, while Wang et al. [34] proposed a state abstraction technique in model-based RL by learning a dynamic model that minimizes dependencies between state variables and actions. Despite these advancements in RL, the exploration of state representation in RLRS is still limited. RLRS often involves complex, high-dimensional data and intricate causal relationships within the recommender system. Rather than merely condensing the state aggregation, we aim to discern and highlight the specific state dimensions that are causally critical for decision-making in RLRS, providing a more targeted and effective approach to recommendation processes.\nIn RLRS, the agent's actions involve recommending items, while the rewards typically correspond to user feedback like clicks, purchases, or exits. However, rewards alone do not clearly indicate which aspects of the state influence user behavior, making it challenging to discern critical from irrelevant state dimensions for effective decision-making. To tackle this, we introduce Causal-InDispensable State Representations (CIDS) in RLRS. CIDS leverages causal relationships between actions and state variables, as well as among different state dimensions, to identify key elements crucial for policy learning.\nCIDS focuses on two types of causal relationships. The first involves state dimensions directly influenced by actions, termed Directly Action-Influenced State Variables (DAIS). For instance,"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Reinforcement Learning based Recommender Systems", "content": "Reinforcement Learning (RL) in Recommender Systems (RS) is focused on optimizing decision-making through ongoing user interaction, framed within a Markov Decision Process (MDP) modelled by the tuple $(S, A, R, P, \\gamma)$. Here, $S$ denotes the state space, containing user data, historical interactions, item characteristics, and contextual elements; $A$ represents the action space, including all candidate items; $R: S \\times A \\rightarrow \\mathbb{R}$ is the reward function, related to the user feedback; $P$ covers the transition probabilities of transitioning from one state to another; and $\\gamma$ is the discount factor, prioritizing immediate versus future rewards.\nIn the MDP framework, the agent (RS) interacts with its environment over discrete time steps denoted by $t = 0, 1, 2, ..., n$. At each time step $t$, the agent examines the current state $s_t$, which encompasses user preferences, past interactions, and item information, all contained within the state space $S$. The agent then selects an action $a_t$ from the action space $A(s_t)$, often comprising a set of item recommendations. This action prompts a transition to a subsequent state $s_{t+1}$, and the agent receives a corresponding reward $r_t$, reflecting the user's response and the effectiveness of the recommended items. The RS strives to establish a policy $\\pi: S \\rightarrow A$ that optimizes the cumulative discounted return, thus assessing the long-term effectiveness of its recommendations."}, {"title": "2.2 Causal Graphical Model", "content": "Let's formally define the causal graphical model followed by [25]. Consider a set of finitely many random variables denoted as $X = (X_1, ..., X_d)$ with an index set $V := \\{1, ..., d\\}$. These random variables have a joint distribution $P_X$ and a density function $p(x)$.\nA causal graphical model is represented by a Directed Acyclic Graph (DAG) $G = (V, \\mathcal{E})$, where $V$ represents the nodes or vertices of the graph, and $\\mathcal{E}$ represents the edges between the nodes. The edges $\\mathcal{E} \\subseteq V^2$ satisfy the property that for any node $v \\in V$, $(v, v) \\notin \\mathcal{E}$, meaning there are no self-loops in the graph. In a causal graph, a random variable $X^i$ is considered a direct cause of $X^j$ if and only if $(i, j) \\in \\mathcal{E}$ and $(j, i) \\notin \\mathcal{E}$. Hence, it is assumed that the causal graph is acyclic, meaning that there are no directed cycles in the graph. This ensures that there are no causal loops where the causal influence could propagate indefinitely.\nIn a DAG, two disjoint subsets of vertices, denoted as $A$ and $B$, are considered d-separated (see Definition A.1 in appendix) by a third disjoint subset $S$ if all paths connecting nodes in $A$ and $B$ are blocked by $S$. This relationship is denoted as $A \\perp_G B | S$."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Causal-indispensable State Representation", "content": "To characterize a set of causal-indispensable state representations for RLRS, we consider decomposing $s_t$ into $d$ different dimensions denoted as $s_t = (s_t^1, ..., s_t^d)$. Given the causal graphical model for one-step MDP $G = (V, \\mathcal{E})$, where $V = \\{a_t, s_t^1, ..., s_t^d, s_{t+1}^1, ..., s_{t+1}^d\\}$ represents the nodes of the graph, and $\\mathcal{E}$ represents the edges describing the causal relationships between actions and different dimensions of states. It is commonplace that the action variable may not influence every dimension of the state variable, and there are structural relationships among different dimensions of $s_t$. For example, consider the causal"}, {"title": "Definition 3.1 (Causal Decomposition of State)", "content": "Given the causal graphical model, such as the model in Figure 1, that describes the causal relationship among the actions and dimensions of states within the MDP environment, the state can be decomposed into two subsets: Causal-InDispensable State Representations (CIDS) and causal-dispensable state representations, where the CIDS is defined as including the subsets of state dimensions satisfying one of the following causal relationships:\n(i) Directly Action-Influenced State Variables (DAIS): A state variable $s_t^i \\in S$ belongs to DAIS$_t$ if there exists a direct edge from $a_{t-1} \\in A$ to $s_t^i$ in the causal graph.\n(ii) Action-Influence Ancestors (AIA): A state variable $s_t^i \\in S$ belongs to AIA$_t$ if it is an ancestor of any state variable in DAIS in the causal graph. That is, there exists a directed path in the causal graph from $s_t^i$ to some $s_{t+1}^j \\in$ DAIS$_{t+1}$, and there is no direct edge from any action variable $a_{t-1} \\in A$ to $s_t^i$.\nCorrespondingly, a state variable $s_t^m \\in S$ belongs to causal-dispensable state representations (CDS) if it is neither a part of DAIS nor AIA. That is, $s_t^m$ has no direct causal relationship with action variable $a_t \\in A$ or any state in DAIS.\nIn RLRS, the state often includes a variety of user and item features. RL agents learn to choose appropriate actions according to the current state vector $s_t$ to improve user satisfaction and engagement, in which some dimensions may be redundant for policy learning. In an RLRS, the policy dictates how recommendations are made based on the current state. Utilizing CIDS would mean that the policy learning is based on the most causally indispensable and sufficient state representations when the structural relationship is given. Figure 1 shows different types of state dimensions in the example causal structural relationship. The state variables in purple nodes belong to DAIS, as these are state variables that have a direct causal relationship with action variables (e.g. the historical interaction or the item popularity will be impacted after the action). In other words, they are the descendants of action variables in the causal graph. These state dimensions change directly in response to the actions taken by the agent. The state variables in blue nodes belong to AIA, as these are state variables that are ancestors of the DAIS in the causal graph. They do not directly interact with action variables but have an impact on the DAIS (e.g. user age or gender can influence preferences and, consequently, historical interaction). They form a preceding layer in the causal structure, influencing the states that are directly affected by the agent's actions.\nThen we demonstrate how the proposed CIDS can be determined by using the conditional dependence and independence relationship among the variables under the following assumptions [23]:\nA 1. Markov condition\u00b9 and faithfulness for the underlying DAG.\nA 2. There is an arrow from $s_{t-1}^i$ to $s_t^i$.\nA 3. There are no long-range arrows, i.e. arrows from $s_{t-m}^i$ to $s_t^j$ for any $m > 1$, and no backward arrows in time.\nA 4. There are no arrows between state variables at the same timestep, i.e., no arrow from $s_t^i$ to $s_t^j$ for all $i$ and $j$."}, {"title": "THEOREM 3.1.", "content": "Under the assumptions A1-A4, $s_{t+1}^{i} \\in$ DAISt+1 if and only if $a_t \\notperp s_{t+1}^{i} | DAIS_t$.\nPROOF. (Proof by Contradiction)\nForward Direction ($\\Rightarrow$):\nSuppose $s_{t+1}^{i} \\in$ DAISt+1. By definition, there exists a direct edge from $a_t$ to $s_{t+1}^{i}$ in the causal graph. To prove by contradiction, assume that $a_t \\perp s_{t+1}^{i} | DAIS_t$.\nHowever, $a_t \\perp s_{t+1}^{i} | DAIS_t$ would violate the assumption A1, under which the direct edge from $a_t$ to $s_{t+1}^{i}$ implies that $a_t$ and $s_{t+1}^{i}$ are not conditionally independent given any set that doesn't include one of them.\nTherefore, we show that if $s_{t+1}^{i} \\in$ DAISt+1, then $a_t \\notperp s_{t+1}^{i} | DAIS_t$.\nBackward Direction ($\\Leftarrow$):\nSuppose $a_t \\notperp s_{t+1}^{i} | DAIS_t$, implying a direct or indirect influence from $a_t$ to $s_{t+1}^{i}$. For contradiction, assume $s_{t+1}^{i} \\notin$ DAISt+1, meaning there is no direct edge from $a_t$ to $s_{t+1}^{i}$.\nConsidering assumptions A2-A4, the only permissible connection from $a_t$ to $s_{t+1}^{i}$ under these constraints is a direct edge, since no long-range arrows, backward arrows, or arrows between state variables at the same timestep are permitted. Consequently, if $a_t \\notperp s_{t+1}^{i} | DAIS_t$, it implies the existence of a direct edge from $a_t$ to $s_{t+1}^{i}$, necessitating that $s_{t+1}^{i}$ is indeed a part of DAIS$_{t+1}$. This stands in contradiction to our initial assumption that $s_{t+1}^{i} \\notin$ DAISt+1.\nTherefore, it is established that if $a_t \\notperp s_{t+1}^{i} | DAIS_t$, then it must follow that $s_{t+1}^{i} \\in$ DAISt+1.\nBy contradiction in both directions, the theorem is proven. Under assumptions A1-A4, $s_{t+1}^{i} \\in$ DAISt+1 if and only if $a_t \\notperp s_{t+1}^{i} | DAIS_t$."}, {"title": "THEOREM 3.2.", "content": "Under the assumptions A1-A4, $s_{t-1}^{i} \\in$ AIA$_{t-1}$ if and only if $a_{t-1} \\notperp s_{t-1}^{i} | DAIS_t$\nPROOF. (Prove by Contradiction)\nForward Direction ($\\Rightarrow$):\nSuppose $s_{t-1}^{i} \\in$ AIA$_{t-1}$, meaning there's a directed edge from $s_{t-1}^{i}$ to some $s_t^j \\in$ DAIS$_t$. Given $s_t^j \\in$ DAIS$_t$, there is a directed edge from $a_{t-1}$ to $s_t^j$. Hence, there is a path between $a_{t-1}$ and $s_{t-1}^{i} \\in$ AIA$_{t-1}$: $a_{t-1}\\rightarrow s_t^j \\leftarrow s_{t-1}^{i}$.\nAssume for contradiction that $a_{t-1} \\perp s_{t-1}^{i} | DAIS_t$, which suggests that DAIS blocks every path between $a_{t-1}$ and $s_{t-1}^{i}$. However, this contradicts the path $a_{t-1} \\rightarrow s_t^j \\leftarrow s_{t-1}^{i}$, which is not be blocked by DAIS (see Definition A.1).\nTherefore, we conclude that if $s_{t-1}^{i} \\in$ AIA$_{t-1}$, then $a_{t-1} \\notperp s_{t-1}^{i} | DAIS_t$."}, {"title": "3.2 Causal-indispensable State Representation Learning", "content": "When learning the CIDS in practice, a significant challenge arises from the unknown nature of the causal graphical model. Therefore, instead of learning CIDS directly from the causal graphical model, we rely on the collected transition data $D = \\{(s_t, a_t, s_{t+1}, r_t)\\}$. To tackle this, we initially define the causal structure of a DAG $G$, which describes the causal relationships within the MDP environment. This structure explicitly encodes the causal relationships between actions and various state dimensions, alongside the relationships among dimensions themselves:\n$s_{t+1}^j = f(\\mathcal{M} s_{t}, \\mathcal{M} a_{t}s_t), \\text{ for } j = 1, ..., d.$ (1)\nHere, $s_{t+1}^j$ is a dimension of the state at time $t + 1$, and $s_{t+1} = (s_{t+1}^1, ..., s_{t+1}^d)$. The binary mask $\\mathcal{M}$ captures the causal structure, and $\\odot$ denotes the element-wise multiplication. The matrix $\\mathcal{M}_{s\\rightarrow s} \\in \\{0, 1\\}^{d\\times d}$ indicates causal relationships between dimensions of the state $s_t$ and the next state $s_{t+1}$. Specifically, $\\mathcal{M}_{ij}^{(j)} = 1$ implies an edge from $s_t^i$ to $s_{t+1}^j$. Similarly, the matrix $\\mathcal{M}_{a\\rightarrow s} \\in \\{0, 1\\}^{1\\times d}$ represents causal relationships between the action $a_t$ and the dimensions of the next state $s_{t+1}$. The notation $\\mathcal{M}_{a\\rightarrow s}^{(j)}$ denotes the j-th column of this matrix. We operate under the assumption that the causal structure linking $s_t$, $s_{t+1}$, and $a_t$ is time-invariant without any unobserved confounders.\nBased on Definition 3.1, DAIS comprises a set of state variables that have direct edges from the action $a_{t-1}$. We can represent DAIS using the causal structure mask $\\mathcal{M}$ as: DAISt = $\\mathcal{M}_{a\\rightarrow s}s_t$. Similarly, AIA consists of state variables at time $t$ that have a causal influence on any state variable in DAISt+1 at the next time step. Accordingly, AIA can be formulated as AIAt = $\\mathcal{M}_{s\\rightarrow s} s_t$.\nAs discussed in Section 3.1, DAISt and $a_{t-1}$ are not conditionally independent given DAISt-1. In contrast, other state dimensions, except for DAISt, are conditionally independent given DAISt-1. Similarly, for the Action-Influence Ancestors (AIA), AIAt-1 and $a_{t-1}$ are not conditionally independent given DAIS. However, other state dimensions, excluding AIAt, exhibit conditional independence given DAIS. We proceed to formalize the learning process CIDS using the principles of Conditional Mutual Information (CMI). We aim to maximize the CMI to learn the DAIS and AIA in each case."}, {"title": "THEOREM 3.3.", "content": "(Identifiability of Causal Structure) Given the observable state $s_t$ and action $a_t$, which form the equation in Equation (1), the causal structure masks $\\mathcal{M}_{s\\rightarrow s}$ and $\\mathcal{M}_{a\\rightarrow s}$ are identifiable under the global Markov condition and faithfulness assumption.\nThe proof of Theorem 3.3 is detailed in Appendix B. This theorem lays the theoretical groundwork necessary for identifying the causal structure masks from observed data."}, {"title": "3.3 Objective Function", "content": "The predictive models are designed with distinct objectives for learning the DAIS and AIA components, parameterized by $\\Theta_D$ and $\\Theta_A$ respectively. The objective function for each component is tailored to minimize loss while incorporating a regularization term to promote sparsity in the learned structural matrices.\nThe objective function for the DAIS predictive model is given by:\n$\\mathcal{L}_{DAIS-\\text{model}} = -\\mathcal{L}_{DAIS} + \\lambda_1 ||\\mathcal{M}_{a\\rightarrow s} ||_1, $ (8)\nwhere $\\mathcal{L}_{DAIS}$ is related to the learning of DAIS and $\\lambda_1$ is a hyperparameter that controls the sparsity of the action-to-state causal structure matrix.\nFor the AIA predictive model, the objective function is:\n$\\mathcal{L}_{AIA-\\text{model}} = -\\mathcal{L}_{AIA} + \\lambda_2||\\mathcal{M}_{s\\rightarrow s} ||_1, $ (9)\nwhere $\\mathcal{L}_{AIA}$ is associated with the learning of AIA and $\\lambda_2$ is a hyperparameter for the sparsity of the state-to-state causal structure matrix.\nThe detailed formulations for $\\mathcal{L}_{DAIS}$ and $\\mathcal{L}_{AIA}$ are provided in Section 3.2, while the comprehensive expressions for both objective functions are available in Appendix C."}, {"title": "3.4 Recommendation Policy Learning", "content": "Upon establishing the DAIS and AIA established, we can construct the causal-indispensable state representation by combining these elements: CIDS = (DAIS, AIA). To operationalize this, we define a causal structure matrix for CIDS such that $\\mathcal{M}_{CIDS} = \\{\\mathcal{M}_{s\\rightarrow s}\\} \\cup \\{\\mathcal{M}_{a\\rightarrow s}\\}$. The CIDS at time t can then be succinctly expressed as CIDSt = $\\mathcal{M}_{CIDS}s_t$, which serves as the input for learning the recommendation policy. In this way, the recommendation policy chooses the action only with the causal-indispensable state dimensions. The detailed steps of this comprehensive learning approach are outlined in Algorithm 1, encompassing three critical phases: (i) data collection using a sub-optimal policy, (ii) learning of DAIS and AIA from the collected data, and (iii) learning of recommendation policy with CIDS."}, {"title": "4 EXPERIMENTS", "content": "In this section, we begin by performing experiments on an online simulator and recommendation datasets to highlight the remarkable performance of our methods. We then conduct an ablation study to demonstrate the effectiveness of the causal-indispensable state representation."}, {"title": "4.1 Experimental Setup", "content": "We introduce the experimental settings with regard to environments and state-of-the-art RL methods. The implementation details can be found in Appendix D."}, {"title": "4.1.1 Recommendation Environments.", "content": "Online Evaluation. For online evaluation, we employ Virtual-Taobao [26], a simulation platform that replicates an online retail environment. This platform leverages data from Taobao, one of China's largest online retail sites, using hundreds of millions of genuine data points. VirtualTaobao generates virtual customers and interactions, enabling our agent to be tested in a simulated \"live\" environment.\nOffline Evaluation. For offline evaluation, we use the following benchmark datasets:\n\\bullet MovieLens (100k\u00b2 and 1M\u00b3): These datasets, derived from the MovieLens website, feature user ratings of movies. The ratings are on a 5-star scale, with each user providing at least 20 ratings. Movies and users are characterized by 23 and 5 features, respectively.\n\\bullet Douban-Book4: The Douban-Book dataset is a collection of user interactions and book information derived from the Douban website, a popular Chinese social networking service. This dataset primarily focuses on user ratings of books."}, {"title": "4.1.2 Baseline.", "content": "In our experiments, we employ the following algorithms as the baseline:\n\\bullet Deep Deterministic Policy Gradient (DDPG) [18]: An off-policy method suitable for environments with continuous action spaces, employing a target policy network for action computation.\n\\bullet Soft Actor-Critic (SAC) [13]: An off-policy maximum entropy Deep RL approach, optimizing a stochastic policy with clipped double-Q method and entropy regularization.\n\\bullet Twin Delayed DDPG (TD3) [11]: An enhancement over DDPG, incorporating dual Q-functions, less frequent policy updates, and noise addition to target actions.\n\\bullet MACS [30] A method that introduces a counterfactual synthesis policy into the RL-based recommender systems, generating the counterfactual user interaction based on the casual view of MDP for data augmentation.\n\\bullet TPGR [3]: A model for large-scale interactive recommendations, combining RL and a binary tree structure.\n\\bullet PGPR [35]: An explainable recommendation model that integrates knowledge awareness with RL techniques.\n\\bullet DRR-att [20]: A model for interactive recommender systems that introduces an attention network into the state representation module of a deep reinforcement learning recommendation framework. The DRR-att is implemented on the DDPG.\nNote that the methods TPGR and PGPR use the knowledge graphs which are not available in our experiment, hence we removed the related part to conduct the experiments."}, {"title": "4.1.3 Evaluation Measures.", "content": "The effectiveness of our model is assessed using different measures in online and offline environments:\n\\bullet Online Evaluation: VirtualTaobao uses click-through rate (CTR) as the indicator of effectiveness. The formula for CTR is as follows:\n$\\text{CTR} = \\frac{\\text{episode\\_return}}{\\text{episode\\_length} \\times \\text{maximum\\_reward}},$\nwhere maximum\\_reward represents the highest potential reward obtainable in a single step within an episode.\n\\bullet Offline Evaluation: For dataset evaluation, we utilize three widely recognized numerical criteria: Precision, Recall, and Accuracy."}, {"title": "4.2 Overall Results", "content": "Online Simulator. The performance comparison, as seen in Figure 2 exhibits that algorithms enhanced with CIDS, namely DDPG-CIDS, SAC-CIDS, and TD3-CIDS, surpass the baseline methods significantly across the learning episodes. DDPG-CIDS, for instance, indicates a considerable uplift in CTR over the standard DDPG algorithm. This improvement highlights the impact of integrating a causal understanding into the policy learning mechanism. SAC-CIDS similarly outperforms the conventional SAC model, which underscores the robustness of CIDS in environments with high-dimensional action spaces. The TD3-CIDS also shows substantial gains, especially in the later stages of the learning curve, suggesting the effectiveness of CIDS in enhancing temporal-difference learning algorithms. Figure 3 details the 1-step CTR performance of all baselines and our DDPG-CIDS(since the method DRR-att is also implemented on the DDPG framework) in the VirtualTaobao simulation. The DDPG-CIDS again stands out, affirming its superior policy learning capacity by consistently achieving higher average CTRs compared to the other methods.\nOffline Dataset. Our experimental evaluation showcases the efficacy of the DDPG-CIDS framework across several datasets, including MovieLens-100k, MovieLens-1M, Douban-Book, and BookCrossing. We benchmarked against a range of state-of-the-art algorithms and highlighted the best and second-best results with bold and asterisks, respectively, as detailed in Table 1. Particularly on MovieLens-100k, DDPG-CIDS surpassed all baselines, reinforcing the benefits of integrating CIDS into the recommendation mechanism. Its top scores in Recall, Precision, and Accuracy highlight the method's effectiveness. On the larger MovieLens-1M, DDPG-CIDS maintained top-tier results, suggesting scalability. DDPG-CIDS's superior performance on the Douban-Book and BookCrossing datasets confirmed its adaptability to various content types and user interactions.\nOverall, these results clearly manifest the advantages of incorporating CIDS into the recommendation policy learning framework. The boosted performance in Precision and Recall signifies that CIDS facilitates the identification of more relevant information, while the improved Accuracy reflects the overall reliability of the policy."}, {"title": "4.3 Ablation Study", "content": "In this section, we first delve into the ablation study designed to dissect the contributions of the different components of our framework, specifically focusing on CIDS and its constituents: DAIS and AIA. Then we investigate the impact of the sparsity of the learned causal structure on recommendation policy learning."}, {"title": "4.3.1 Impact of CIDS, DAIS, and AIA on Recommendation Policy Learning.", "content": "Our analysis is aimed at understanding the individual and combined effects of DAIS and AIA components on the policy learning process. We considered three variants: DDPG enhanced with only DAIS (DDPG-DAIS), only AIA (DDPG-AIA), and both DAIS and AIA (DDPG-CIDS). As depicted in Figure 4, DDPG-CIDS consistently outperforms the other variants, suggesting that the combination of DAIS and AIA is critical for capturing the complete causal structure necessary for robust policy learning. While the DDPG-DAIS variant demonstrates some improvements over the baseline DDPG, the DDPG-AIA variant does not perform as well, even falling behind the baseline performance. This observation can be attributed to the inherent characteristics of the state representations. DAIS focuses on the aspects of the state that are directly affected by the action, often containing the most important information needed for making decisions. In contrast, AIA represents state dimensions that do not directly interact with action variables but influence DAIS indirectly. Training with only the AIA representation reveals that a state representation lacking direct action-relevant information can detrimentally affect the learning of the recommendation policy. Hence, it is the combined effect of DAIS and AIA within the DDPG-CIDS framework that leads to superior recommendation performance, underscoring the importance of integrating comprehensive causal knowledge into the recommendation process."}, {"title": "4.3.2 Results with Different RL Frameworks.", "content": "Further, we extended our ablation study to evaluate the impact of integrating CIDS into different RL frameworks: DDPG, SAC, and TD3. As illustrated in Figure 2 and Figure 4, the inclusion of CIDS enhances the performance of all backbone architectures. Notably, SAC-CIDS and TD3-CIDS demonstrate a marked improvement in CTR, highlighting our method's flexibility with different reinforcement learning algorithms. The consistent improvement boost across diverse frameworks validates the adaptability and effectiveness of the CIDS framework for RL-based recommender systems."}, {"title": "4.3.3 Impact of the Sparsity of Learned Causal Structure on Recommendation Policy Learning.", "content": "The sparsity of the structural matrices in our model is regulated by the hyperparameters $\\lambda_1$ and $\\lambda_2$, as described in Section 3.3. A greater value for these parameters induces a sparser causal structure. Specifically, we examine the role of $\\lambda_1$, which influences the causal structure between actions and state dimensions, to assess its impact on the performance of the recommendation policy. This focus stems from observations in Section 4.3.1, which highlighted that the action-state causal structure significantly affects the recommendation policy, more so than the inter-state causal relationships.\nTable 2 summarizes the mean and standard deviation of the CTR across a spectrum of $\\lambda_1$ values, tracked over successive timesteps within the VirtualTaobao simulation. It becomes apparent from the data that the sparsity level, governed by $\\lambda_1$, is a critical determinant in the policy's learning performance. Optimal sparsity can enhance policy learning, but an overly sparse structure (such as when $\\lambda_1$ is set to 9e-4) can be detrimental, possibly due to insufficient information for the policy to leverage. Conversely, a lower $\\lambda_1$ value may facilitate a more robust early learning phase by providing a richer informational context. However, too low a $\\lambda_1$ value may introduce noise through non-essential dimensions, thus impeding the policy's optimization process."}, {"title": "5 RELATED WORK", "content": ""}, {"title": "DRL-based recommender system.", "content": "DRL-based recommender systems model the interaction recommendation process as Markov Decision Processes (MDPs), utilizing deep learning to estimate the value function and tackle high-dimensional MDPs [8, 22]. Recognizing the significance of negative feedback in understanding user preferences, Zhao et al. [40] introduced DEERS, which processes positive and negative signals separately at the input layer to avoid negative feedback overwhelming positive signals due to their sparsity. Chen et al. [5] incorporated knowledge graphs into DRL for interactive recommendation, employing a local knowledge network to enhance efficiency. Hong et al. [14] proposed NRRS, a model-based approach integrating nonintrusive sensing and reinforcement learning for personalized dynamic music recommendation. NRRS trains a user reward model that derives rewards from three user feedback sources: scores, opinions, and wireless signals. Moving beyond predefining a reward function, Chen et al. [9] introduced InvRec, utilizing inverse reinforcement learning to infer a reward function from user behaviors and directly learning the recommendation policy from these behaviors. InvRec employs inverse DRL as a generator to augment state-action pairs, offering a novel approach to the task. Addressing offline RL methods, Wang et al. [29] proposed CDT4Rec, which designed a new causal mechanism to estimate the reward function. Finally, Chen et al. [6] proposed a causal augmentation method for RLRS, providing a new perspective to address the exploration problem in RLRS."}, {"title": "Causal Recommendation.", "content": "In recent years, the recommendation domain has witnessed significant advancements through the integration of causal inference techniques. These techniques, especially in de-biasing training data, have been transformative for the field. Bonner and Vasile [2] developed a domain adaptation algorithm, capitalizing on biased logged feedback to predict randomized treatment effects and address the challenge of random exposure. Further expanding this field, Liu et al. [19] presented KDCRec, a knowledge distillation framework aimed at addressing bias in recommender systems by extracting insights from uniformly distributed data. Zhang et al. [38] tackled the pervasive issue of popularity bias, devising a novel causal inference paradigm to adjust recommendation scores through targeted causal interventions. Additionally, Ding et al. [10] proposed CI-LightGCN, a Causal Incremental Graph Convolution method for updating graph convolutional networks in recommender systems, efficiently handling model updates with new data while maintaining recommendation accuracy. The application of counterfactual inference in recommender systems has also gained traction, being utilized for path-specific effects removal [31] and out-of-distribution (OOD) generalization [32]. Moreover, an increasing number of studies are adopting counterfactual reasoning for objectives such as providing explanations, enhancing model interpretability, and learning robust representations [21, 28, 37]."}, {"title": "6 CONCLUSION", "content": "In this study, we address the challenge of high-dimensional and noisy state spaces in RLRS by introducing Causal-Indispensable"}, {"title": "A DEFINITIONS IN CAUSALITY", "content": "Here we briefly mention some fundamental definitions [24, 25, 27], which we use in our paper to present and prove our methodology.\nDefinition A.1 (d-Separation [25]). In Directed Acyclic Graph (DAG) G, a path between two nodes, denoted as in and im, is considered blocked by a set S. This occurs when neither in nor im are included in S, and there exists a node ik that satisfies one of the following two possibilities:\n(i) ik $\\in$ S and ik-1 $\\rightarrow$ ik $\\rightarrow$ ik+1, ik-1$\\leftarrow$ik $\\rightarrow$ ik+1, or ik-1 $\\leftarrow$ ik $\\rightarrow$ ik+1;\n(ii) ik and its descendants are not part of the blocking set S, and ik-1$\\\\perp$ik$\\\\perp$ ik+1.\nDefinition A.2 (Structural Causal Models [24]). A Structural Causal Model (SCM) $\\mathcal{M} := (\\mathcal{S}, \\mathcal{P}_U)$ is associated with a DAG G, which consists of a collection $\\mathcal{S}$ of n structural assignments :\n$X_i := f_i(\\text{PA}_i, U_i), i = 1, ..., n, $ (10)\nwhere V = {$X_1, ..., X_n$} is a set of endogenous variables, and $\\text{PA}_i \\subseteq$ {$X_1, ..., X_n$} \\ {$X_i$} represent parents of $X_i$, which are also called direct causes of $X_i$. And U = {$U_1, ..., U_n$} are noise variables, determined by unobserved factors. We assume that noise variables are jointly independent. Correspondingly, $\\mathcal{P}_U$ is the joint distribution over the noise variables. Each structural equation $f_i$ is a causal mechanism that determines the value of $X_i$ based on the values of $\\text{PA}_i$ and the noise term $U_i$.\nDefinition A.3 (Markov property [25]). Given a DAG G, and a joint distribution $\\mathbb{P}_X$, the distribution is said to satisfy\n(i) global Markov property in relation to the DAG G if the condition A $\\perp_G$ B|S implies that A $\\perp$ B|S holds true for all distinct sets of vertices A, B, S.\n(ii) Markov factorization property in relation to the DAG G if the expression $p(x) = p(x_1, x_2,...,x_d) = \\prod_{j=1}^d p(x_j|pa_j)$ holds true.\nDefinition A.4 (Causal Faithfulness [24, 25, 27]). A distribution $\\mathbb{P}$ is faithful to a DAG G if no conditional independence relations other than the ones entailed by the Markov property are present."}, {"title": "B PROOF OF THEOREM 3.3", "content": "PROOF. Firstly, the Markov condition ensures that for any pair of non-adjacent variables in the causal graph G = (V, &), denoted Vi and Vj, there is conditional independence between them given a set of other variables. This condition allows us to infer the absence of direct causal links between certain variables in the MDP.\nSecondly, the faithfulness assumption posits that all observed conditional independencies are indicative of true separations in the causal graph. This implies that if two variables are found to be conditionally independent given a set of other variables, there is indeed no direct causal path between them in the graph.\nTogether, these conditions permit the identification of the binary structural masks Ms\\rightarrow s and Ma\\rightarrow s defined over the set V, which are identifiable through the examination of conditional independence relationships."}, {"title": "C OBJECTIVE FUNCTION", "content": "The objective function for the DAIS predictive model is given by:\n$\\mathcal{L}_{DAIS-\\text{model}} = -\\mathcal{L}_{DAIS} + \\lambda_1 ||\\mathcal{M}_{a\\rightarrow s} ||_1$\\n$=-\\mathbb{E}_{s_t, a_t, s_{t+1}\\sim D}\\left[ \\sum_{j=1}^d \\log P(s_{t+1}^{(j)} \\in \\text{DAIS}_{t+1} | s_t, \\mathcal{M}_{a\\rightarrow s}^{(j)};\\Theta_{D_1})\\right] \\\\ -\\mathbb{E}_{s_t, a_t, s_{t+1}\\sim D}\\left[ \\sum_{j=1}^d \\log P(s_{t+1}^{(j)} \\in \\text{DAIS}_{t+1} | a_t, s_t, \\mathcal{M}_{a\\rightarrow s};\\Theta_{D_2})\\right] + \\lambda_1 || \\mathcal{M}_{a\\rightarrow s} ||_1, $ (11)\nwhere $\\mathcal{L}_{DAIS}$ is the loss related to the learning of DAIS and $\\lambda_1$ is a hyperparameter that controls the sparsity of the action-to-state causal structure matrix.\nFor the AIA predictive model, the objective function is:\n$\\mathcal{L}_{AIA-\\text{model}} = -\\mathcal{L}_{AIA} + \\lambda_2||\\mathcal{M}_{s\\rightarrow s} ||_1$\\n$=-\\mathbb{E}_{s_{t-1}, a_{t-1}, s_t\\sim D}\\left[ \\sum_{j=1}^d \\log P(s_{t-1}^{(j)} \\in \\text{AIA}_{t-1} | s_t, \\mathcal{M}_{s\\rightarrow s}^{(j)};\\Theta_{A_1})\\right] \\\\+\\mathbb{E}_{s_{t-1}, a_{t-1}, s_t\\sim D}\\left[ \\sum_{j=1}^d \\log P(s_{t-1}^{(j)} \\in \\text{AIA}_{t-1} | a_{t-1}, s_t, \\mathcal{M}_{s\\rightarrow s};\\Theta_{A_2})\\right] + \\lambda_2|| \\mathcal{M}_{s\\rightarrow s} ||_1,$ (12)\nwhere $\\mathcal{L}_{AIA}$ is the loss associated with the learning of AIA and $\\lambda_2$ is a hyperparameter for the sparsity of the state-to-state causal structure matrix."}, {"title": "D IMPLEMENTATION DETAILS", "content": "Initially, we train a sub-optimal recommendation policy utilizing the DDPG algorithm with default parameters as delineated in [18]. This policy undergoes training across 1,000,000 episodes, with the best-performing iteration saved as our expert policy. Utilizing this expert policy, we generate a dataset of expert trajectories within the environment. It should be noted that the expert's exposure to the environment is limited, with only a finite number of trajectories being sampled. These expert trajectories comprise the observed dataset employed for training the DAIS and AIA components.\nFor the DAIS and AIA predictive models, we opt for Multilayer Perceptron (MLP) networks. Both models feature a tri-layered fully connected architecture with each layer outputting 128 units. Following each hidden layer is a ReLU activation function. The hyperparameters $\\lambda_i$ are set as follows: $\\lambda_1 = 5 \\times 10^{-4}$ and $\\lambda_2 = 10^{-4}$.\nIn training the recommendation policy, we fix the actor network's learning rate at $10^{-4}$ and the critic network's at $10^{-3}$. The discount factor, denoted by $\\gamma$, is established at 0.95, paired with a soft target update rate, $\\tau$, of 0.001. The network's hidden layer size is determined to be 128, and the replay buffer capacity is $10^6$ entries.\nWe adhere to parameter configurations as specified in stable baselines36 or as originally reported in the respective baseline papers for all baseline methods."}]}