{"title": "On Causally Disentangled State Representation Learning for Reinforcement Learning based Recommender Systems", "authors": ["Siyu Wang", "Xiaocong Chen", "Lina Yao"], "abstract": "In Reinforcement Learning-based Recommender Systems (RLRS), the complexity and dynamism of user interactions often result in high-dimensional and noisy state spaces, making it challenging to discern which aspects of the state are truly influential in driving the decision-making process. This issue is exacerbated by the evolving nature of user preferences and behaviors, requiring the recommender system to adaptively focus on the most relevant information for decision-making while preserving generaliability. To tackle this problem, we introduce an innovative causal approach for decomposing the state and extracting Causal-InDispensable State Representations (CIDS) in RLRS. Our method concentrates on identifying the Directly Action-Influenced State Variables (DAIS) and Action-Influence Ancestors (AIA), which are essential for making effective recommendations. By leveraging conditional mutual information, we develop a framework that not only discerns the causal relationships within the generative process but also isolates critical state variables from the typically dense and high-dimensional state representations. We provide theoretical evidence for the identifiability of these variables. Then, by making use of the identified causal relationship, we construct causal-indispensable state representations, enabling the training of policies over a more advantageous subset of the agent's state space. We demonstrate the efficacy of our approach through extensive experiments, showcasing our method outperforms state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommender Systems (RS) are crucial in navigating the vast digital environment, tailoring suggestions to align with individual user preferences. The integration of Reinforcement Learning (RL) within RS, known as RLRS, has transformed the recommendation experience into a dynamic, sequential decision-making process. Unlike traditional systems that passively suggest content based on static data, RLRS actively engage with users, continuously adapting recommendations in response to real-time feedback. This approach seeks to maximize long-term user engagement by treating each interaction as an opportunity to learn and enhance the personalization of content, thereby maintaining alignment with the evolving preferences and behaviors of the users.\nIn RLRS, the efficiency hinges on three essential components: State Representation, Policy Optimization, and Reward Formulation [1, 8]. While much of the current research in RLRS is centered on policy optimization [5, 7, 33, 39] and reward formulation [4, 9, 16], the role of state representation should not be understated. The state in RLRS is a composite of varied attributes: user characteristics (like age, gender, and recent activities), item properties (including price, category, and popularity), and contextual elements (such as time and location). Effectively distilling this rich tapestry of information poses a significant challenge. Neglecting key features might result in suboptimal recommendations, while incorporating too much detail could clutter the system with irrelevant data, diminishing its predictive accuracy. Recent advances in representation learning algorithms in RL aim to extract abstract features from high-dimensional data, which has proven beneficial in enhancing the efficiency of existing RL algorithms [12, 15, 17]. For instance, Zhang et al. [36] developed a method to learn representations by ignoring task-irrelevant information through the bisimulation metric, while Wang et al. [34] proposed a state abstraction technique in model-based RL by learning a dynamic model that minimizes dependencies between state variables and actions. Despite these advancements in RL, the exploration of state representation in RLRS is still limited. RLRS often involves complex, high-dimensional data and intricate causal relationships within the recommender system. Rather than merely condensing the state aggregation, we aim to discern and highlight the specific state dimensions that are causally critical for decision-making in RLRS, providing a more targeted and effective approach to recommendation processes.\nIn RLRS, the agent's actions involve recommending items, while the rewards typically correspond to user feedback like clicks, purchases, or exits. However, rewards alone do not clearly indicate which aspects of the state influence user behavior, making it challenging to discern critical from irrelevant state dimensions for effective decision-making. To tackle this, we introduce Causal-Indispensable State Representations (CIDS) in RLRS. CIDS leverages causal relationships between actions and state variables, as well as among different state dimensions, to identify key elements crucial for policy learning.\nCIDS focuses on two types of causal relationships. The first involves state dimensions directly influenced by actions, termed Directly Action-Influenced State Variables (DAIS). For instance,"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Reinforcement Learning based\nRecommender Systems", "content": "Reinforcement Learning (RL) in Recommender Systems (RS) is focused on optimizing decision-making through ongoing user interaction, framed within a Markov Decision Process (MDP) modelled by the tuple (\ud835\udc46, \ud835\udc34, \ud835\udc45, \ud835\udc43, \ud835\udefe). Here, \ud835\udc46 denotes the state space, containing user data, historical interactions, item characteristics, and contextual elements; \ud835\udc34 represents the action space, including all candidate items; \ud835\udc45 : \ud835\udc46 \u00d7 \ud835\udc34 \u2192 R is the reward function, related to the user feedback; \ud835\udc43 covers the transition probabilities of transitioning from one state to another; and \ud835\udefe is the discount factor, prioritizing immediate versus future rewards.\nIn the MDP framework, the agent (RS) interacts with its environment over discrete time steps denoted by t = 0, 1, 2, ..., n. At each time step t, the agent examines the current state st, which encompasses user preferences, past interactions, and item information, all contained within the state space S. The agent then selects an action at from the action space \ud835\udc34(\ud835\udc60t), often comprising a set of item recommendations. This action prompts a transition to a subsequent state st+1, and the agent receives a corresponding reward rt, reflecting the user's response and the effectiveness of the recommended items. The RS strives to establish a policy \ud835\udf0b: \ud835\udc46 \u2192 \ud835\udc34 that optimizes the cumulative discounted return, thus assessing the long-term effectiveness of its recommendations."}, {"title": "2.2 Causal Graphical Model", "content": "Let's formally define the causal graphical model followed by [25]. Consider a set of finitely many random variables denoted as \ud835\udc4b = (\ud835\udc4b1, ..., \ud835\udc4b\ud835\udc51) with an index set \ud835\udc49 := {1, ..., \ud835\udc51}. These random variables have a joint distribution \ud835\udc43\ud835\udc65 and a density function \ud835\udc5d(\ud835\udc65).\nA causal graphical model is represented by a Directed Acyclic Graph (DAG) \ud835\udc3a = (\ud835\udc49, \u2130), where \ud835\udc49 represents the nodes or vertices of the graph, and \u2130 represents the edges between the nodes. The edges \u2130 \u2286 \ud835\udc492 satisfy the property that for any node \ud835\udc63 \u2208 \ud835\udc49, (\ud835\udc63, \ud835\udc63) \u2209 \u2130, meaning there are no self-loops in the graph. In a causal graph, a random variable \ud835\udc4b\ud835\udc56 is considered a direct cause of \ud835\udc4b\ud835\udc57 if and only if (\ud835\udc56, \ud835\udc57) \u2208 \u2130 and (\ud835\udc57, \ud835\udc56) \u2209 \u2130. Hence, it is assumed that the causal graph is acyclic, meaning that there are no directed cycles in the graph. This ensures that there are no causal loops where the causal influence could propagate indefinitely.\nIn a DAG, two disjoint subsets of vertices, denoted as A and B, are considered d-separated (see Definition A.1 in appendix) by a third disjoint subset S if all paths connecting nodes in A and B are blocked by S. This relationship is denoted as \ud835\udc34 \u22a5\ud835\udc3a \ud835\udc35|\ud835\udc46."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Causal-indispensable State Representation", "content": "To characterize a set of causal-indispensable state representations for RLRS, we consider decomposing st into d different dimensions denoted as st = (\ud835\udc601, ..., \ud835\udc60\ud835\udc51). Given the causal graphical model for one-step MDP \ud835\udc3a = (\ud835\udc49, \u2130), where \ud835\udc49 = {\ud835\udc4e\ud835\udc61, \ud835\udc60\ud835\udc61\ud835\udc61+1, \ud835\udc60\ud835\udc61\ud835\udc61+1} represents the nodes of the graph, and \u2130 represents the edges describing the causal relationships between actions and different dimensions of states. It is commonplace that the action variable may not influence every dimension of the state variable, and there are structural relationships among different dimensions of st. For example, consider the causal"}, {"title": "Definition 3.1 (Causal Decomposition of State).", "content": "Given the causal graphical model, such as the model in Figure 1, that describes the causal relationship among the actions and dimensions of states within the MDP environment, the state can be decomposed into two subsets: Causal-InDispensable State Representations (CIDS) and causal-dispensable state representations, where the CIDS is defined as including the subsets of state dimensions satisfying one of the following causal relationships:\n(i) Directly Action-Influenced State Variables (DAIS): A state variable \ud835\udc60\ud835\udc56 \u2208 \ud835\udc46 belongs to DAISt if there exists a direct edge from \ud835\udc4e\ud835\udc61\u22121 \u2208 A to \ud835\udc60\ud835\udc56 in the causal graph.\n(ii) Action-Influence Ancestors (AIA): A state variable \ud835\udc60\ud835\udc57 \u2208 S belongs to AIAt if it is an ancestor of any state variable in DAISt in the causal graph. That is, there exists a directed path in the causal graph from \ud835\udc60\ud835\udc57 to some \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2208 DAISt+1, and there is no direct edge from any action variable \ud835\udc4e\ud835\udc61\u22121 \u2208 A to \ud835\udc60\ud835\udc57.\nCorrespondingly, a state variable \ud835\udc60\ud835\udc5a\u2208 \ud835\udc46 belongs to causal-dispensable state representations (CDS) if it is neither a part of DAIS nor AIA. That is, \ud835\udc60\ud835\udc5a has no direct causal relationship with action variable \ud835\udc4e\ud835\udc61 \u2208 A or any state in DAIS.\nIn RLRS, the state often includes a variety of user and item features. RL agents learn to choose appropriate actions according to the current state vector st to improve user satisfaction and engagement, in which some dimensions may be redundant for policy learning. In an RLRS, the policy dictates how recommendations are made based on the current state. Utilizing CIDS would mean that the policy learning is based on the most causally indispensable and sufficient state representations when the structural relationship is given. Figure 1 shows different types of state dimensions in the example causal structural relationship. The state variables in purple nodes belong to DAIS, as these are state variables that have a direct causal relationship with action variables (e.g. the historical interaction or the item popularity will be impacted after the action). In other words, they are the descendants of action variables in the causal graph. These state dimensions change directly in response to the actions taken by the agent. The state variables in blue nodes belong to AIA, as these are state variables that are ancestors of the DAIS in the causal graph. They do not directly interact with action variables but have an impact on the DAIS (e.g. user age or gender can influence preferences and, consequently, historical interaction). They form a preceding layer in the causal structure, influencing the states that are directly affected by the agent's actions.\nThen we demonstrate how the proposed CIDS can be determined by using the conditional dependence and independence relationship among the variables under the following assumptions [23]:\nA 1. Markov condition\u00b9 and faithfulness for the underlying DAG.\nA 2. There is an arrow from \ud835\udc60\ud835\udc61\u22121\ud835\udc56 to \ud835\udc60\ud835\udc56\ud835\udc61+1.\nA 3. There are no long-range arrows, i.e. arrows from \ud835\udc60\ud835\udc5a\ud835\udc61 to \ud835\udc60\ud835\udc57\ud835\udc61+1 for any \ud835\udc5a > 1, and no backward arrows in time.\nA 4. There are no arrows between state variables at the same timestep, i.e., no arrow from \ud835\udc60\ud835\udc56\ud835\udc61 to \ud835\udc60\ud835\udc57\ud835\udc61 for all \ud835\udc56 and \ud835\udc57."}, {"title": "THEOREM 3.1.", "content": "Under the assumptions A1-A4, \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2208 DAISt+1 if and only if \ud835\udc4e\ud835\udc61 \u21f4 \ud835\udc60\ud835\udc56\ud835\udc61+1|DAISt."}, {"title": "PROOF.", "content": "(Proof by Contradiction)\nForward Direction (\u21d2):\nSuppose \ud835\udc60\ud835\udc56\ud835\udc61+1 \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2208 DAISt+1. By definition, there exists a direct edge from \ud835\udc4e\ud835\udc61 to \ud835\udc60\ud835\udc56\ud835\udc61+1 in the causal graph. To prove by contradiction, assume that \ud835\udc4e\ud835\udc61 \u2aeb \ud835\udc60\ud835\udc56\ud835\udc61+1|DAISt.\nHowever, \ud835\udc4e\ud835\udc61 \u2aeb \ud835\udc60\ud835\udc56\ud835\udc61+1|DAIS\ud835\udc61 would violate the assumption A1, under which the direct edge from \ud835\udc4e\ud835\udc61 to \ud835\udc60\ud835\udc56\ud835\udc61+1 implies that \ud835\udc4e\ud835\udc61 and \ud835\udc60\ud835\udc56\ud835\udc61+1 are not conditionally independent given any set that doesn't include one of them.\nTherefore, we show that if \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2208 DAISt+1, then \ud835\udc4e\ud835\udc61 \u21f4\ud835\udc60\ud835\udc56\ud835\udc61+1|DAISt.\nBackward Direction (\u21d0):\nSuppose \ud835\udc4e\ud835\udc61 \u21f4 \ud835\udc60\ud835\udc56\ud835\udc61+1|DAISt, implying a direct or indirect influence from \ud835\udc4e\ud835\udc61 to \ud835\udc60\ud835\udc56\ud835\udc61+1. For contradiction, assume \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2209 DAISt+1, meaning there is no direct edge from \ud835\udc4e\ud835\udc61 to \ud835\udc60\ud835\udc56\ud835\udc61+1.\nConsidering assumptions A2-A4, the only permissible connection from \ud835\udc4e\ud835\udc61 to \ud835\udc60\ud835\udc56\ud835\udc61+1 under these constraints is a direct edge, since no long-range arrows, backward arrows, or arrows between state variables at the same timestep are permitted. Consequently, if \ud835\udc4e\ud835\udc61 \u21f4\ud835\udc60\ud835\udc56\ud835\udc61+1|DAISt, it implies the existence of a direct edge from \ud835\udc4e\ud835\udc61 to \ud835\udc60\ud835\udc56\ud835\udc61+1, necessitating that \ud835\udc60\ud835\udc56\ud835\udc61+1 is indeed a part of DAISt+1. This stands in contradiction to our initial assumption that \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2209 DAISt+1.\nTherefore, it is established that if \ud835\udc4e\ud835\udc61 \u21f4 \ud835\udc60\ud835\udc56\ud835\udc61+1|DAISt, then it must follow that \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2208 DAISt+1.\nBy contradiction in both directions, the theorem is proven. Under assumptions A1-A4, \ud835\udc60\ud835\udc56\ud835\udc61+1 \u2208 DAISt+1 if and only if \ud835\udc4e\ud835\udc61 \u21f4 \ud835\udc60\ud835\udc56\ud835\udc61+1|DAISt."}, {"title": "THEOREM 3.2.", "content": "Under the assumptions A1-A4, \ud835\udc60\ud835\udc61\u22121\ud835\udc56 \u2208 AIAt\u22121 if and only if \ud835\udc4e\ud835\udc61\u22121 \u21f4 \ud835\udc60\ud835\udc61\u22121\ud835\udc56|DAISt"}, {"title": "PROOF.", "content": "(Prove by Contradiction)\nForward Direction (\u21d2):\nSuppose \ud835\udc60\ud835\udc61\u22121\ud835\udc56 \u2208 AIAt\u22121, meaning there's a directed edge from \ud835\udc60\ud835\udc61\u22121\ud835\udc56 to some \ud835\udc60\ud835\udc57\ud835\udc61 \u2208 DAIS\ud835\udc61. Given \ud835\udc60\ud835\udc57\ud835\udc61 \u2208 DAIS\ud835\udc61, there is a directed edge from \ud835\udc4e\ud835\udc61\u22121 to \ud835\udc60\ud835\udc57\ud835\udc61. Hence, there is a path between \ud835\udc4e\ud835\udc61\u22121 and \ud835\udc60\ud835\udc61\u22121\ud835\udc56 \u2208 AIAt\u22121: \ud835\udc4e\ud835\udc61\u22121\u2192 \ud835\udc60\ud835\udc57 \u2190\ud835\udc60\ud835\udc61\u22121\ud835\udc56.\nAssume for contradiction that \ud835\udc4e\ud835\udc61\u22121 \u2aeb \ud835\udc60\ud835\udc61\u22121\ud835\udc56|DAISt, which suggests that DAIS\ud835\udc61 blocks every path between \ud835\udc4e\ud835\udc61\u22121 and \ud835\udc60\ud835\udc61\u22121\ud835\udc56. However, this contradicts the path \ud835\udc4e\ud835\udc61\u22121 \u2192 \ud835\udc60\ud835\udc57 \u2190\ud835\udc60\ud835\udc61\u22121\ud835\udc56, which is not be blocked by DAIS (see Definition A.1).\nTherefore, we conclude that if \ud835\udc60\ud835\udc61\u22121\ud835\udc56 \u2208 AIAt\u22121, then \ud835\udc4e\ud835\udc61\u22121 \u21f4 \ud835\udc60\ud835\udc61\u22121\ud835\udc56|DAISt."}, {"title": "3.2 Causal-indispensable State Representation\nLearning", "content": "When learning the CIDS in practice, a significant challenge arises from the unknown nature of the causal graphical model. Therefore, instead of learning CIDS directly from the causal graphical model, we rely on the collected transition data \ud835\udc37 = {(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc46\ud835\udc61+1, \ud835\udc60\ud835\udc61+1, \ud835\udc5f\ud835\udc61)}. To tackle this, we initially define the causal structure of a DAG \ud835\udc3a, which describes the causal relationships within the MDP environment. This structure explicitly encodes the causal relationships between actions and various state dimensions, alongside the relationships among dimensions themselves:\n$$s^{t+1}_j = f(M_{S \rightarrow S}^{(i,j)} s_t, M_{a \rightarrow s}^{(i,j)} a_t), \\text{for } j = 1, ..., d.$$       (1)\nHere, \ud835\udc60\ud835\udc57\ud835\udc61+1 is a dimension of the state at time \ud835\udc61 + 1, and \ud835\udc60\ud835\udc61+1 = (\ud835\udc461\ud835\udc61+1, ..., \ud835\udc46\ud835\udc51\ud835\udc61+1). The binary mask \ud835\udc40 captures the causal structure, and \u2299 denotes the element-wise multiplication. The matrix \ud835\udc40\ud835\udc46\u2192\ud835\udc46 \u2208 {0, 1}\ud835\udc51\u00d7\ud835\udc51 indicates causal relationships between dimensions of the state st and the next state st+1. Specifically, \ud835\udc40\ud835\udc56\ud835\udc57 = 1 implies an edge from \ud835\udc60\ud835\udc56\ud835\udc61 to \ud835\udc60\ud835\udc57\ud835\udc61+1. Similarly, the matrix \ud835\udc40\ud835\udc4e\u2192\ud835\udc60 \u2208 {0,1}1\u00d7\ud835\udc51 represents causal relationships between the action at and the di- mensions of the next state st+1. The notation \ud835\udc40\ud835\udc4e\u2192\ud835\udc60(\ud835\udc57) denotes the \ud835\udc57-th column of this matrix. We operate under the assumption that the causal structure linking st, St+1, and at is time-invariant without any unobserved confounders.\nBased on Definition 3.1, DAIS comprises a set of state variables that have direct edges from the action at-1. We can represent DAIS using the causal structure mask \ud835\udc40 as: DAISt = \ud835\udc40\ud835\udc4e\u2192\ud835\udc60 \u2299st. Similarly, AIA consists of state variables at time t that have a causal influence on any state variable in DAISt+1 at the next time step. Accordingly, AIA can be formulated as AIAt = \ud835\udc40\ud835\udc60\ud835\udc60 \u2299 st.\nAs discussed in Section 3.1, DAISt and at-1 are not conditionally independent given DAISt-1. In contrast, other state dimensions, except for DAISt, are conditionally independent given DAISt-1. Similarly, for the Action-Influence Ancestors (AIA), AIAt-1 and at-1 are not conditionally independent given DAISt. However, other state dimensions, excluding AIAt, exhibit conditional independence given DAISt. We proceed to formalize the learning process CIDS using the principles of Conditional Mutual Information (CMI). We aim to maximize the CMI to learn the DAIS and AIA in each case."}, {"title": "The learning process for DAIS can be formulated by maximizing\nthe following objective:", "content": "\ud835\udc3f\ud835\udc37\ud835\udc34\ud835\udc3c\ud835\udc46 = I(DAISt+1; at | DAISt),           (2)\nwhere I() denotes the Mutual Information. This equation captures the mutual information between DAISt+1 and at conditioned on DAISt.\nSubsequently, AIA can be learned by maximizing the following:\n\ud835\udc3f\ud835\udc34\ud835\udc3c\ud835\udc34 = I(AIAt-1; at-1|DAISt),           (3)\nwhich similarly utilizes mutual information measures to identify the most relevant state dimensions constituting AIAt-1.\nThe mutual information I(DAISt+1; at|DAISt) can be expressed in terms of conditional entropy, which is defined as the difference between the conditional entropy of one variable given the conditioning variable and the conditional entropy of the same variable given both the conditioning variable and the second variable. Thus, it can be defined using conditional entropy as follows:\nI(DAISt+1; at DAISt) = H(DAISt+1|DAISt) \u2013 H(DAISt+1 at, DAISt). (4)\nBy leveraging the probabilistic predictive model of DAIS param- eterized by \u0398\ud835\udc37. Then the conditional entropy of DAISt+1 given DAISt is calculated as:\nH(DAISt+1|DAISt)\n= -Est,at,St+1~D [log P(DAISt+1|DAISt; \u0398\ud835\udc37\u2081)]\n= -Est,at,St+1~D \n\u2211(j) log P(\ud835\udc60\ud835\udc57\ud835\udc61+1 \u2208 DAISt+1|St, \ud835\udc40\ud835\udc4e\ud835\udc60\ud835\udc60; \u0398\ud835\udc37\u2081). (5)\n\ud835\udc56=1\nThe conditional entropy of DAISt+1 given both at and DAISt is calculated similarly, but conditioning on both the previous action and DAISt:\nH(DAISt+1 at, DAISt)\n= -Est,at,St+1~D [log P(DAISt+1|at, DAISt; \u0398\ud835\udc372)]\n= -Est,at,St+1~D \n\u2211(j) log P(\ud835\udc60\ud835\udc57\ud835\udc61+1 \u2208 DAISt+1|at, St, \ud835\udc40\ud835\udc4e\ud835\udc60\ud835\udc60; \u0398\ud835\udc372). (6)\n\ud835\udc56=1\nFor the AIA and the corresponding mutual information term, the expression can be reformulated in a similar way:\nI(AIAt-1; at-1 DAISt)\n=H(AIAt-1 DAISt) \u2013 H(AIAt-1|at-1, DAISt)\n= - Est-1,at-1,St~D \n\u2211(j) log P(\ud835\udc60\ud835\udc57\ud835\udc61\u22121 \u2208 AIA\ud835\udc61\u22121|st, \ud835\udc40\ud835\udc60\ud835\udc60; \u0398\ud835\udc341)\n\ud835\udc56=1\n+ Est-1,at-1,St~D \n\u2211(j) log P(\ud835\udc60\ud835\udc57\ud835\udc61\u22121 \u2208 ALA\ud835\udc61\u22121|at-1, St, \ud835\udc40\ud835\udc4e\ud835\udc60\ud835\udc60; \u0398\ud835\udc342) .        (7)\n\ud835\udc56=1\nwhere \u0398\ud835\udc57 are the parameters of the probabilistic predictive model of AIA."}, {"title": "THEOREM 3.3.", "content": "(Identifiability of Causal Structure) Given the ob- servable state st and action at, which form the equation in Equa- tion (1), the causal structure masks \ud835\udc40\ud835\udc60\ud835\udc60 and \ud835\udc40\ud835\udc4e\u2192\ud835\udc60 are identifiable under the global Markov condition and faithfulness assumption."}, {"title": "3.3 Objective Function", "content": "The predictive models are designed with distinct objectives for learning the DAIS and AIA components, parameterized by \u0398\ud835\udc5d and \u0398\ud835\udc34 respectively. The objective function for each component is tailored to minimize loss while incorporating a regularization term to promote sparsity in the learned structural matrices.\nThe objective function for the DAIS predictive model is given by:\n\ud835\udc3f\ud835\udc37\ud835\udc34\ud835\udc3c\ud835\udc46-\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 = -\ud835\udc3f\ud835\udc37\ud835\udc34\ud835\udc3c\ud835\udc46 + \ud835\udf061||\ud835\udc40\ud835\udc4e\u2192\ud835\udc60 ||1,        (8)\nwhere LDAIS is related to the learning of DAIS and \ud835\udf061 is a hyper- parameter that controls the sparsity of the action-to-state causal structure matrix.\nFor the AIA predictive model, the objective function is:\n\ud835\udc3f\ud835\udc34\ud835\udc3c\ud835\udc34-\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59 = -\ud835\udc3f\ud835\udc34\ud835\udc3c\ud835\udc34 + \ud835\udf062||\ud835\udc40\ud835\udc60\ud835\udc60 ||1,       (9)\nwhere LAIA is associated with the learning of AIA and \ud835\udf062 is a hyperparameter for the sparsity of the state-to-state causal structure matrix.\nThe detailed formulations for LDAIS and LAIA are provided in Section 3.2, while the comprehensive expressions for both objective functions are available in Appendix C."}, {"title": "3.4 Recommendation Policy Learning", "content": "Upon establishing the DAIS and AIA established, we can construct the causal-indispensable state representation by combining these elements: CIDS = (DAIS, AIA). To operationalize this, we define a causal structure matrix for CIDS such that MCIDS = {\ud835\udc40\ud835\udc60\ud835\udc60} \u222a {\ud835\udc40\ud835\udc4e\u2192\ud835\udc60}. The CIDS at time t can then be succinctly expressed as CIDSt = MCIDS \u2299st, which serves as the input for learning the recommendation policy. In this way, the recommendation policy chooses the action only with the causal-indispensable state dimensions. The detailed steps of this comprehensive learning approach are outlined in Algorithm 1, encompassing three critical phases: (i) data collection using a sub-optimal policy, (ii) learning of DAIS and AIA from the collected data, and (iii) learning of recommendation policy with CIDS."}, {"title": "4 EXPERIMENTS", "content": "In this section, we begin by performing experiments on an online simulator and recommendation datasets to highlight the remarkable performance of our methods. We then conduct an ablation study to demonstrate the effectiveness of the causal-indispensable state representation."}, {"title": "4.1 Experimental Setup", "content": "We introduce the experimental settings with regard to environments and state-of-the-art RL methods. The implementation details can be found in Appendix D."}, {"title": "Algorithm 1 Training Procedure for Predictive Models and Policy\nLearning", "content": "1: Input: Dataset D, empty reply buffer B, initial neural networks parameters \u0398\ud835\udc5d and \u0398\ud835\udc34, initial recommendation policy parame- ter \u0398\ud835\udc5f\ud835\udc52\ud835\udc50, hyperparameters \ud835\udf061, \ud835\udf062\n2: for episode = 1, ..., E do\n3: for t = 1, ..., T do\n4: Sample a random minibatch of K trajectories from D\n5: // Optimize DAIS Predictive Model\n6: Update \u0398\ud835\udc5d by minimizing \ud835\udc3f\ud835\udc37\ud835\udc34\ud835\udc3c\ud835\udc46-\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59( eq. (8)) with minibatch\n7: // Optimize AIA Predictive Model\n8: Update \u0398\ud835\udc34 by minimizing \ud835\udc3f\ud835\udc34\ud835\udc3c\ud835\udc34-\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc59(eq. (9)) with mini- batch\n9: end for\n10: end for\n11: // Training of the recommendation policy \ud835\udf0b\n12: for episode = 1, ..., E do\n13: Receive initial observation state \ud835\udc601\n14: for t = 1, ..., T do\n15: Calculate \ud835\udc40\ud835\udc36\ud835\udc3c\ud835\udc37\ud835\udc46 based on \ud835\udc40\ud835\udc60\u2192\ud835\udc60 and \ud835\udc40\ud835\udc4e\u2192\ud835\udc60\n16: Observe state \ud835\udc60\ud835\udc61 and select action a ~ \ud835\udf0b(\ud835\udc40\ud835\udc36\ud835\udc3c\ud835\udc37\ud835\udc46 \u2299 \ud835\udc60\ud835\udc61)\n17: Execute at in the environment\n18: Observe next state \ud835\udc60\ud835\udc61+1 and receive reward \ud835\udc5f\ud835\udc61\n19: Store transition (\ud835\udc40\ud835\udc36\ud835\udc3c\ud835\udc37\ud835\udc46 \u2299 \ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc60\ud835\udc61+1, \ud835\udc5f\ud835\udc61) in B\n20: Sample a random minibatch of K transition from D\n21: Update recommendation policy parameter \u0398\ud835\udc5f\ud835\udc52\ud835\udc50\n22: end for\n23: end for"}, {"title": "4.1.1 Recommendation Environments.", "content": "Online Evaluation. For online evaluation, we employ Virtual- Taobao [26], a simulation platform that replicates an online retail environment. This platform leverages data from Taobao, one of China's largest online retail sites, using hundreds of millions of genuine data points. VirtualTaobao generates virtual customers and interactions, enabling our agent to be tested in a simulated \"live\" environment.\nOffline Evaluation. For offline evaluation, we use the following benchmark datasets:\n\u2022 MovieLens (100k\u00b2 and 1M\u00b3): These datasets, derived from the MovieLens website, feature user ratings of movies. The ratings are on a 5-star scale, with each user providing at least 20 ratings. Movies and users are characterized by 23 and 5 features, respectively.\n\u2022 Douban-Book4: The Douban-Book dataset is a collection of user interactions and book information derived from the Douban website, a popular Chinese social networking service. This dataset primarily focuses on user ratings of books.\n\u2022 Book-Crossing5: The Book-Crossing dataset originates from an online book club known for its book exchange and tracking services. This dataset encompasses user ratings, book information, and user-book interactions."}, {"title": "4.1.2 Baseline.", "content": "In our experiments, we employ the following algo- rithms as the baseline:\n\u2022 Deep Deterministic Policy Gradient (DDPG) [18]: An off-policy method suitable for environments with continuous action spaces, employing a target policy network for action computation.\n\u2022 Soft Actor-Critic (SAC) [13]: An off-policy maximum entropy Deep RL approach, optimizing a stochastic policy with clipped double-Q method and entropy regularization.\n\u2022 Twin Delayed DDPG (TD3) [11]: An enhancement over DDPG, incorporating dual Q-functions, less frequent policy updates, and noise addition to target actions.\n\u2022 MACS [30] A method that introduces a counterfactual synthesis policy into the RL-based recommender systems, generating the counterfactual user interaction based on the casual view of MDP for data augmentation.\n\u2022 TPGR [3]: A model for large-scale interactive recommendations, combining RL and a binary tree structure.\n\u2022 PGPR [35]: An explainable recommendation model that integrates knowledge awareness with RL techniques.\n\u2022 DRR-att [20]: A model for interactive recommender systems that introduces an attention network into the state representation module of a deep reinforcement learning recommendation framework. The DRR-att is implemented on the DDPG.\nNote that the methods TPGR and PGPR use the knowledge graphs which are not available in our experiment, hence we removed the related part to conduct the experiments."}, {"title": "4.1.3 Evaluation Measures.", "content": "The effectiveness of our model is assessed using different measures in online and offline environments:\n\u2022 Online Evaluation: VirtualTaobao uses click-through rate (CTR) as the indicator of effectiveness. The formula for CTR is as follows:\n$$CTR = \\frac{episode\\_return}{episode\\_length \\times maximum\\_reward}$$,\nwhere maximum_reward represents the highest potential reward obtainable in a single step within an episode.\n\u2022 Offline Evaluation: For dataset evaluation, we utilize three widely recognized numerical criteria: Precision, Recall, and Accuracy."}, {"title": "4.2 Overall Results", "content": "Online Simulator. The performance comparison, as seen in Figure 2 exhibits that algorithms enhanced with CIDS, namely DDPG- CIDS, SAC-CIDS, and TD3-CIDS, surpass the baseline methods significantly across the learning episodes. DDPG-CIDS, for instance, indicates a considerable uplift in CTR over the standard DDPG algorithm. This improvement highlights the impact of integrating a causal understanding into the policy learning mechanism. SAC- CIDS similarly outperforms the conventional SAC model, which underscores the robustness of CIDS in environments with high- dimensional action spaces. The TD3-CIDS also shows substantial gains, especially in the later stages of the learning curve, suggesting the effectiveness of CIDS in enhancing temporal-difference learning algorithms. Figure 3 details the 1-step CTR performance of all baselines and our DDPG-CIDS(since the method DRR-att is also implemented on the DDPG framework) in the VirtualTaobao simulation. The DDPG-CIDS again stands out, affirming its superior policy learning capacity by consistently achieving higher average CTRs compared to the other methods.\nOffline Dataset. Our experimental evaluation showcases the efficacy of the DDPG-CIDS framework across several datasets, including MovieLens-100k, MovieLens-1M, Douban-Book, and BookCross- ing. We benchmarked against a range of state-of-the-art algorithms and highlighted the best and second-best results with bold and aster- isks, respectively, as detailed in Table 1. Particularly on MovieLens- 100k, DDPG-CIDS surpassed all baselines, reinforcing the benefits of integrating CIDS into the recommendation mechanism. Its top scores in Recall, Precision, and Accuracy highlight the method's ef- fectiveness. On the larger MovieLens-1M, DDPG-CIDS maintained top-tier results, suggesting scalability. DDPG-CIDS's superior per- formance on the Douban-Book and BookCrossing datasets confirmed its adaptability to various content types and user interac- tions.\nOverall, these results clearly manifest the advantages of incorporating CIDS into the recommendation policy learning framework. The boosted performance in Precision and Recall signifies that CIDS facilitates the identification of more relevant information, while the improved Accuracy reflects the overall reliability of the policy."}, {"title": "4.3 Ablation Study", "content": "In this section, we first delve into the ablation study designed to dissect the contributions of the different components of our framework, specifically focusing on CIDS and its constituents: DAIS and AIA. Then we investigate the impact of the sparsity of the learned causal structure on recommendation policy learning."}, {"title": "4.3.1 Impact of CIDS, DAIS, and AIA on Recommendation Policy\nLearning.", "content": "Our analysis is aimed at understanding the individual and combined effects of DAIS and AIA components on the policy learning process. We considered three variants: DDPG enhanced with only DAIS (DDPG-DAIS), only AIA (DDPG-AIA), and both DAIS and AIA (DDPG-CIDS). As depicted in Figure 4, DDPG-CIDS consistently outperforms the other variants, suggesting that the combination of DAIS and AIA is critical for capturing the complete causal structure necessary for robust policy learning. While the DDPG-DAIS variant demonstrates some improvements over the baseline DDPG, the DDPG-AIA variant does not perform as well, even falling behind the baseline performance. This observation can be attributed to the inherent characteristics of the state representations. DAIS focuses on the aspects of the state that are directly affected by the action, often containing the most important information needed for making decisions. In contrast, AIA represents state dimensions that do not directly interact with action variables but influence DAIS indirectly. Training with only the AIA representation reveals that a state representation lacking direct action-relevant information can detrimentally affect the learning of the recommendation policy. Hence, it is the combined effect of DAIS and AIA within the DDPG-CIDS framework that leads to superior recommendation performance, underscoring the importance of integrating comprehensive causal knowledge into the recommendation process."}, {"title": "4.3.2 Results with Different RL Frameworks.", "content": "Further, we extended our ablation study to evaluate the impact of integrating CIDS into different RL frameworks: DDPG, SAC, and TD3. As illustrated in Figure 2 and Figure 4, the inclusion of CIDS enhances the performance of all backbone architectures. Notably, SAC-CIDS and TD3-CIDS demonstrate a marked improvement in CTR, highlighting our method's flexibility with different reinforcement learn- ing algorithms. The consistent improvement boost across diverse frameworks validates the adaptability and effectiveness of the CIDS framework for RL-based recommender systems."}, {"title": "4.3.3 Impact of the Sparsity of Learned Causal Structure on Recom- mendation Policy Learning.", "content": "The sparsity of the structural matrices in our model is regulated by the hyperparameters \ud835\udf061 and \ud835\udf062, as described in Section 3.3. A greater value for these parameters induces a sparser causal structure. Specifically, we examine the role of \ud835\udf061, which influences the causal structure between actions and state dimensions, to assess its impact on the performance of the recommendation policy. This focus stems from observations in Section 4.3.1, which highlighted that the action-state causal structure significantly affects the recommendation policy, more so than the inter-state causal relationships.\nTable 2 summarizes the mean and standard deviation of the CTR across a spectrum of \ud835\udf06\u2081 values, tracked over successive timesteps within the VirtualTaobao simulation. It becomes apparent from the data that the sparsity level, governed by \ud835\udf061, is a critical determinant in the policy's learning performance. Optimal sparsity can enhance policy learning, but an overly sparse structure (such as when \ud835\udf06\u2081 is set to 9e-4) can be detrimental, possibly due to insufficient information for the policy to leverage. Conversely, a lower \ud835\udf06\u2081 value may facilitate a more robust early learning phase by providing a richer informational context. However, too low a \ud835\udf061 value may introduce noise through non-essential dimensions, thus impeding the policy's"}]}