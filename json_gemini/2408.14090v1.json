{"title": "Exploring GPU-to-GPU Communication: Insights into Supercomputer Interconnects", "authors": ["Daniele De Sensi", "Lorenzo Pichetti", "Flavio Vella", "Tiziano De Matteis", "Zebin Ren", "Luigi Fusco", "Matteo Turisini", "Daniele Cesarini", "Kurt Lust", "Animesh Trivedi", "Duncan Roweth", "Filippo Spiga", "Salvatore Di Girolamo", "Torsten Hoefler"], "abstract": "Multi-GPU nodes are increasingly common in the rapidly evolving landscape of exascale supercomputers. On these systems, GPUs on the same node are connected through dedicated networks, with bandwidths up to a few terabits per second. However, gauging performance expectations and maximizing system efficiency is challenging due to different technologies, design options, and software layers. This paper comprehensively characterizes three supercomputers Alps, Leonardo, and LUMI each with a unique architecture and design. We focus on performance evaluation of intra-node and inter-node interconnects on up to 4,096 GPUs, using a mix of intra-node and inter-node benchmarks. By analyzing its limitations and opportunities, we aim to offer practical guidance to researchers, system architects, and software developers dealing with multi-GPU supercomputing. Our results show that there is untapped bandwidth, and there are still many opportunities for optimization, ranging from network to software optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Supercomputers are a key infrastructure enabling advance- ments in several science domains and transformative societal changes. New workloads' computing requirements, ranging from machine learning (ML) to scientific computing and extending to big-data analytics, are driving supercomputer architecture evolution. Due to their massive parallelism, energy efficiency, and memory bandwidth, GPUs became the core of such evolution, characterized by the development of multi- GPU nodes and high-performance intra-node interconnection networks. Nowadays, exascale [1] and pre-exascale systems in the Top500 [2] are equipped with up to 8 GPUs per node, connected with fast dedicated networks with bandwidth up to 3.6 Tb/s per direction [3]. At the same time, due to a steady increase in computing and memory requirements, the number of nodes increased to tens of thousands of nodes [4], leading to systems with up to 75,000 GPUs [5], [6].\nMoving data efficiently across such a high number of GPUs is challenging for multiple reasons. First, there is a significant interconnect, topology, and hardware diversity, thus making the mapping of communications to the underlying system non-trivial. Secondly, on the software side, programmers can rely on different software solutions, ranging from manually copying data between GPU memory on a single node to using transparent and higher-level GPU-Aware solutions such as MPI [7] or NCCL/RCCL [8], [9] (referred as *CCL in the rest of the paper) across nodes. However, the most effective approach for managing a large number of GPUs and the maturity level of the software is still unclear. Lastly, large- scale network-related effects such as congestion and network noise [10], [11], [12] can severely impact the scalability when increasing the number of GPUs, thus hampering the computational power and high bandwidth of these systems.\nTo investigate the aforementioned challenges, we com- prehensively characterize three supercomputers with different architectures: Alps (NVIDIA H100 GPUs and HPE Cray Slingshot interconnect [12]), Leonardo (NVIDIA A100 GPUs and NVIDIA InfiniBand HDR interconnect), and LUMI (AMD InstinctTM MI250X GPUs and HPE Cray Slingshot intercon- nect). We systematically benchmark the performance of intra- node GPU networks (Sec. III and Sec. IV), and inter-node networks (Sec. V and Sec. VI) up to 4,096 GPUs on the three supercomputers. Our study includes a detailed analysis of data movement performed through explicit device-to-device copies, *CCL, and GPU-Aware MPI. Lastly, we evaluate the impact of network noise on GPU-GPU data movements (Sec. VI), showing that it can severely impact workload scalability.\nThis paper provides the first at-scale study characterizing multi-GPU interconnect performance across hardware tech- nologies and diverse communication APIs and software stacks. We spotlight several sources of inefficiencies, ranging from routing to communication libraries tuning. We show that the best way to move data between GPUs depends on several factors like transfer size, communication pattern, and number of GPUs, and might change across systems. We present eight key observations, offering valuable insights to system architects, researchers, practitioners, and software developers to optimize data movements in large-scale multi-GPU systems and exploit current and upcoming systems to their fullest."}, {"title": "II. SYSTEMS DESCRIPTION", "content": "In the following, we describe the main characteristics of the three analyzed systems, and we summarize them in Table I. Since all the analyzed interconnects are full-duplex, when we refer to intra-node and inter-node network bandwidth, we will always report the unidirectional bandwidth and express it in bits per second for consistency.\nA. Alps\nAlps is a 270 PFlop/s supercomputer ranked 6th in the Top500 (June 2024). It is deployed by CSCS [13] and currently under provisioning. Thus, some presented results might be subject to further tuning in the upcoming months before opening to production. For this paper, we used the early access Santis partition [14], where we had access to 512 nodes.\nNode architecture Each node is composed of four GH200 Grace Hopper Superchip [15] connected in an all-to-all topol- ogy using NVLink 4.0. Six 200 Gb/s links connect each GH200 pair, for a total of 1.2 Tb/s between any GPU pair (see Fig. 1a). Every GH200 has 96 GB HBM3 and 120 GB LPDDRX5 memory. Every node acts as a single NUMA system (composed of 8 NUMA domains), with 288 CPU cores and 4 GPUs.\nInter-node connectivity Each node has one HPE Cray Cassini-1 200 Gb/s Network Interface Card (NIC) for each GH200. Nodes are connected in a Dragonfly topology [16] through an HPE Cray Slingshot-11 network [12], [17]. Each switch has 16 ports to endpoints, 31 ports to other switches within the same Dragonfly group, and 17 ports to switches in other Dragonfly groups.\nB. Leonardo\nLeonardo [18] is a 240 PFlop/s supercomputer, ranked 7th on the Top500 [2] (June 2024). It is owned by the EuroHPC Joint Undertaking and hosted by CINECA. We consider Leonardo's Booster GPU partition, consisting of 3,456 computing nodes.\nNode architecture Each node is equipped with a single socket 32-core Intel Xeon\u00ae 8358 CPU and four NVIDIA A100 TensorCore GPUs [19] (13,824 GPUs in total). Each node has 512 GB CPU memory, organized in eight DDR4 slots, and 64 GB HBM2e memory per GPU. Within a node, GPUs are connected through NVIDIA NVLink 3.0, with each GPU connected to each of the other three GPUs through four 200 Gb/s links (see Fig. 1b). The intra-node communication is completed by a 256 Gb/s 16-lane PCIe\u00ae Gen4.0 bus per GPU, used to communicate with the host CPU and with the NIC.\nInter-node connectivity Nodes are interconnected through an InfiniBand HDR network, and each node is equipped with two 200 Gb/s dual port NVIDIA Connect-X6 NICs. Each node thus has four 100 Gb/s network ports, all connected to the same switch at the time of writing. For this paper, we consider them as being four separate NICs. Nodes are connected through a Dragonfly+ [20], with each group containing 180 nodes and structured as a two-level fat tree. Each group has 18 spine and 18 leaf switches. Switches have 40 200Gb/s ports (each of which can be configured as 2 100Gb/s ports). Leaf switches connect 40 100Gb/s ports to 10 nodes (4 GPUs per node) and 18 200 Gb/s ports to spine switches (with 2 200Gb/s ports unused). Spine switches connect 18 200Gb/s ports to leaf switches and 22 200Gb/s ports to other spine switches in different Dragonfly+ groups.\nC. LUMI\nLUMI is a 380 PFlop/s supercomputer ranked 5th in the Top500 (June 2024). It is owned by the EuroHPC Joint Undertaking and hosted by CSC [21]. This paper considers the LUMI-G GPU partition consisting of 2,978 nodes."}, {"title": "III. INTRA-NODE POINT-TO-POINT PERFORMANCE", "content": "We start our analysis by assessing the performance of the intra-node GPU-GPU interconnection for point-to-point communications. We first describe the benchmarking method- ology (Sec. III-A) and the performance tuning performed on each system (Sec. III-B). We then analyze the point-to- point performance (Sec. III-C), focusing on LUMI intra-node architecture (Sec. III-D).\nA. Benchmarking Methodology\nIn all the experiments we have a separate MPI process managing each GPU in the system. We set the affinity of the processes so that each MPI rank manages the GPU closest to the core it is mapped to. We run each experiment between 100 times and 1,000 times (depending on the transfer size). For experiments involving collective communication, we report the maximum time (or minimum goodput) across all the participating ranks [23]. We do not include communicators' creation time. Unless specified otherwise, we always refer to unidirectional bandwidth in Gb/s. We analyze the performance of point-to-point and collective communication using different mechanisms and techniques to transfer data between GPUs. Namely, we consider:\n\u2022 Trivial Staging: We copy buffers to and from the GPU memory to the host memory. Then, we transfer data between processes using MPI. This is a trivial implementation to use as a baseline. We pinned the memory but implemented no pipelining or parallel copies between memories. Data thus moves in a store-and-forward fashion. For point-to-point transfers, we can estimate the peak goodput by summing the time required to transfer the data from device memory to host memory and the time to copy the data between two host memory buffers.\n\u2022 Device-Device Copy: Buffers are copied directly from the device to the device memory. We share memory handles across the processes managing the different GPUs, allowing them to transfer data directly between GPU memories. For the alltoall collective, each GPU copies data to all the other GPUs asynchronously to overlap the copies.\n\u2022 *CCL: We transfer data between GPUs using NCCL [8], [24] (on Leonardo and Alps) or RCCL [9], [25] (on LUMI).\n\u2022 GPU-Aware MPI: We transfer data using GPU-Aware MPI [26].\nWe developed our benchmark from scratch due to some lim- itations of existing benchmarks. OSU [27] lacks benchmarks for explicit device-device copy, and nccl-/rccl-tests [28], [29] only support *CCL. Also, both do not report individual per- iteration timings, which are needed to assess network noise and performance variability (Sec. VI). Creating our benchmark ensured consistency across all communication mechanisms an- alyzed. We used MPI_Wtime for timing individual iterations, with resolutions of 25ns on LUMI and Leonardo, and 30ns on Alps (measured experimentally). The timing excludes one- time operations like buffer allocation and handles exchange. The benchmark synchronizes with the GPU before stopping the timer to ensure full data receipt, except for MPI, where this is implicit. For *CCL, the timing includes the group start/end. This is consistent with nccl-/rccl-tests. We publicly released the code as part of the paper artifact.\nB. Performance Tuning\nWe tuned the performance of all analyzed systems, as the initial default configuration did not fully leverage their potential. This involved searching for and setting environment variables and analyzing their impact on performance.\n*CCL On Alps and LUMI we forced *CCL to ig- nore the CPU affinity set by Slurm (by setting the NCCL_IGNORE_CPU_AFFINITY to 1), obtaining up to 1.6x performance improvement on alltoall and up to 6x on allreduce starting from two nodes. We further improved performance by 2x on alltoall and 3x on allreduce, by increasing the distance at which the GPU can use Direct RDMA when com- municating with the NIC (NCCL_NET_GDR_LEVEL=3). On LUMI, we set NCCL_NCHANNELS_PER_PEER=32 for intra- node point-to-point tests, which led to a 3.5x performance improvement.\nC. Point-to-point Latency and Goodput\nIn Fig. 3, we report the goodput between two GPUs on the same node, measured through a ping-pong microbench- mark for different transfer sizes. We report the unidirectional goodput (in Gb/s), defined as the number of bytes in the buffer divided by half the runtime. The inner plot reports the runtime (in microseconds) for small messages. Each data point represents the mean across the experiments, and the width of the shaded area around the line is the interquartile range (for some plots it is too small to be visible). We report with dashed horizontal lines both the GPU-GPU unidirectional nominal goodput and the trivial staging expected goodput. For example, on Leonardo, any GPU pair is connected with four 200 Gb/s links (see Fig. 1b), for a total of 800 Gb/s nominal goodput per direction between any pair of GPUs.\nOn LUMI, the peak GPU-GPU goodput depends on the specific GPU selected. For this experiment, we selected GPUs 0 and 1, connected through four 400 Gb/s Infinity Fabric links. In Sec. III-D, we analyze the performance for different GPUs combinations. Moreover, disabling SDMA enables GPUs to use more than one Infinity Fabric link at a time [5]. On Alps, we did not run the experiments involving explicit device to device copies since GPU peer access is not enabled on the nodes at the time being.\nGoodput First, we observe that the goodput of trivial staging is up to one order of magnitude lower than the other implementations due to the low bandwidth when moving data between host and device memory. *CCL, MPI GPU-Aware, and device-device copies provide a comparable goodput. On Leonardo, we observe a goodput for GPU-Aware on medium- sized messages that is up to 2x higher than that of NCCL.\nLatency When analyzing the runtime for small messages, we observe similar performance for *CCL and MPI on Alps, but a large performance gap on Leonardo and LUMI. On Leonardo, this is due to the use of GDRCopy [30]. On LUMI, Cray MPICH transfers small buffers between GPUs on the same node by copying them through host memory (rather than doing device-device copy), using an optimized memcpy, where the CPU issues load/store operations directly to GPU HBM. In contrast, on NVIDIA GPUs, CPU load/store operations to GPU memory are not permitted, resulting in higher latency on Alps."}, {"title": "D. Impact of GPU Location on LUMI", "content": "On LUMI, each node has 8 GPUs, connected to each other with a different number of Infinity Fabric links, ranging from one to four (see Sec. II-C). We report in Fig. 4 the unidirectional goodput between GPU 0 and the other seven GPUs on a node when transferring a 1 GiB buffer. We denote with a dashed horizontal line the nominal goodput for each GPU pair, computed by considering the single path with the highest bandwidth between the two GPUs.\nAs expected, we do not observe any difference for the trivial staging, since data is not moved directly between GPUs but across the host memory. Both GPU-Aware MPI and the device- device copy achieve around 70% of the nominal goodput on any GPU pair. On the other hand, in some cases (e.g., when GPU 0 and 5 communicate) RCCL achieves less than half the goodput of GPU-Aware MPI and device-device copy.\nBy analyzing the data more in detail, we can observe that, although GPU-Aware MPI and device-device copy achieve the same goodput both towards GPU 6 and 7 (towards which GPU 0 has the same nominal goodput), RCCL achieves a much higher goodput towards GPU 6 compared to GPU 7.\nBy analyzing RCCL debug information, we identified it as- sumes a lower available bandwidth towards GPU 7 compared to 6, and thus does not fully exploit the available bandwidth. We believe this might happen because the available bandwidth is estimated considering the number of hops rather than the number of paths connecting the two GPUs, which might be helpful for collective communications, where multiple GPUs concurrently communicate, sharing the available links. How- ever, this setup underutilizes the GPU-GPU interconnect for sparser communication patterns."}, {"title": "IV. INTRA-NODE COLLECTIVES PERFORMANCE", "content": "We now focus on intra-node collective performance, by an- alyzing expected and measured goodput for alltoall (Sec. IV-A and Sec. IV-B) and allreduce (Sec. IV-C and Sec. IV-D).\nA. Alltoall Expected Goodput\nFor collectives, we define the goodput as the buffer size divided by the runtime. To compute the expected goodput, we determined the edge forwarding index [31] of the graph corresponding to the intra-node GPUs connectivity. The edge forwarding index is defined as the maximum number of paths crossing any edge and gives an estimate of the maximum load across any network link and, thus, of the worst-case peak bandwidth (e.g., for an alltoall). On Alps and Leonardo, GPUs are fully connected, and each link is crossed by only one path (i.e., the maximum edge forwarding index is one), and we expect a peak goodput equal to the GPU injection bandwidth.\nOn LUMI, assuming data is routed between GPUs using shortest paths, the most loaded link is the one between GCD 1 and 5 (and that between GCD 7 and 3), which is used in four separate paths. Thus, because each IF link has a 400 Gb/s bandwidth, we can expect a 100 Gb/s peak goodput between any pair of GCDs during an alltoall. Because any GCD can send data on six different IF links simultaneously, we expect a peak alltoall goodput of 600 Gb/s. It is worth noting that each GCD on the MI250X GPUs has the same injection bandwidth as one A100 GPU. However, the per-GPU alltoall goodput on LUMI is lower because the GCDs on the same node are not fully connected and, thus, the graph describing the intra-node connectivity has a higher edge forwarding index.\nB. Alltoall Measured Goodput\nWe report in Fig. 5 the measured goodput for an alltoall between all GPUs on a node, and the expected goodput (denoted with a dashed horizontal line). While MPI and RCCL natively provide an alltoall implementation, NCCL does not, and we implemented the alltoall with a trivial algorithm where each GPU sends the data to all the other nodes at the same time (as suggested in the documentation [32]). We used the same algorithm to implement the alltoall using device-device copies. Moreover, we observed no performance difference when com- paring RCCL native and trivial alltoall implementations.\nOn Alps and LUMI, *CCL provides the best performance for large transfers since *CCL collectives are specifically op- timized for the target systems. For example, communications are mapped according to the specific topology, and the number of in-flight data chunks during pipelined operations is tuned by considering the bandwidth available between GPU pairs. Such fine-grained optimizations are not performed by MPI, which thus does not entirely exploit the bandwidth of the intra- node GPU interconnect. On Leonardo, *CCL provides slightly lower performance than MPI. For small transfers, on Alps and Leonardo the performance of *CCL is comparable with that of MPI. On the other hand, on LUMI, for small transfers GPU- Aware MPI is up to 3x faster than *CCL, consistent with what we observed for point-to-point transfers."}, {"title": "C. Allreduce Expected Goodput", "content": "On Alps and Leonardo, since each GPU is directly con- nected to the other GPUs on the same node and can receive from all the other GPUs at the same time, the optimal allreduce algorithm would consist of a pipelined ternary tree reduce (with one of the GPUs as the root and the other three as leaves) followed by a ternary tree broadcast. We thus estimate the peak goodput as the sum of the bandwidth across all the outgoing links from a GPU. On LUMI, since the GPUs are not fully connected, the optimal algorithm for large messages would be the Rabenseifner algorithm, with a ring reduce-scatter followed by a ring allgather [33]. The specific connectivity between the GPUs allows for four edge-disjoint bidirectional rings [22], each using 400 Gb/s Infinity Fabric links. Because the Rabenseifner algorithm sends twice the number of bytes in the buffer, we can thus expect 800 Gb/s peak goodput.\nD. Allreduce Measured Goodput\nIn Fig. 6, we show the allreduce performance for different message sizes. On Alps and Leonardo, *CCL outperforms MPI at any transfer size. On LUMI, on the other hand, GPU-Aware MPI is characterized by the lowest runtime for small transfers whereas, although *CCL performs best on large transfers, its performance is far from the expected peak. This is consistent with what we observed for point-to-point transfers.\nGPU-Aware MPI exhibits low performance on all the ana- lyzed systems, and we observe a higher performance gap be- tween *CCL and GPU-Aware MPI on the allreduce compared to the alltoall. Indeed, the allreduce involves data aggregation, which *CCL performs on the GPUs. Although MPICH also performs data aggregation on the GPUs, we believe that *CCL coordinates GPU execution better. On Leonardo, we note an even larger gap, since Open MPI runs the allreduce on the host [34], similarly to what we do in the baseline implementation. It is worth remarking that on Leonardo Open MPI does not support UCC [35]. The implementation relying on device-device copies performs a reduction on GPU 0, followed by a broadcast. We do not implement any form of pipelining, and we mostly use it for reference and to show that implementing efficient multi-GPU collectives is non-trivial.\nLast, we observe a higher gap between measured and ex- pected performance on collectives compared to point-to-point communications, showing there is still space for collective algorithms optimization. Measured goodput on LUMI gets closer to the expected one. Indeed, LUMI has a lower expected goodput, which is thus easier to saturate."}, {"title": "V. INTER-NODE PERFORMANCE", "content": "We now analyze the interconnect performance when running on multiple nodes. We first analyze point-to-point performance (Sec. V-A), focusing on the impact of network distance on performance (Sec. V-B). Then, we analyze the performance of collective communications up to 4,096 GPUs for alltoall (Sec. V-C) and allreduce (Sec. V-D). We apply the bench- marking methodology described in Sec. III-A, and we run a number of MPI processes on each node equal to the number of available GPUs. For completeness, to assess the overhead of managing GPUs (especially for small transfers), we also analyze the performance when transferring buffers located in the host memory. In this case, we run one MPI process per NIC. On Alps and LUMI, we set the affinity of each process so that it uses the closest GPU and NIC.\nA. Unidirectional Latency and Goodput\nWe run a ping-pong test to measure the point-to-point goodput between two nodes, with each process exchanging data with the corresponding process on the other node. We report the results of our analysis in Fig. 7. We report on the x-axis the number of bytes transmitted on each NIC, and, on the y-axis, the total goodput of the node (i.e., the sum of the goodput on each NIC). MPI provides the highest goodput and lowest latency on all the analyzed systems, regardless of whether the buffer is in host or GPU memory. This is mostly due to overheads introduced by *CCL when launching and managing GPU kernels.\nB. Impact of Network Distance on Performance\nWe now analyze the impact of the GPU network location on performance. Namely, on all systems, two GPUs on different nodes can be connected to the same switch, to two switches on the same Dragonfly/Dragonfly+ group, or to two switches in two different groups. We measure latency and goodput with a ping-pong test sending one byte and 1 GiB, respectively. We use MPI since we showed in Fig. 7 that, for point-to- point transfers, it provides the highest bandwidth and lowest latency on all the analyzed systems. We report the results of our analysis in Fig. 8a for buffers allocated on GPU memory. For completeness and to separate the impact of the network from that of the GPU management, we report in Fig. 8b the same analysis, but for buffers allocated on host memory.\nEach box's top and bottom borders represent the third and first quartiles, respectively. The middle line represents the median, the \u00d7 marker the mean, and the top and bottom whiskers are the 5th and 95th percentile. Last, the notch around the median represents the 95% confidence interval of the median. For the sake of readability, we do not report individual outliers but we annotate on the plot the minimum and maximum observed values.\n1) GPU memory buffers: We first focus on the performance when transferring buffers allocated on GPU memory (Fig. 8a). We observe that when the two GPUs are connected to the same switch, all systems exhibit a similar latency, between 3.7us and 5.7us. However, the location of the GPUs impacts both latency and goodput on all three systems. On Alps and LUMI, when the two GPUs are on different Dragonfly groups, the average latency increases by 28% compared to the case where the two GPUs are under the same switch (from 4.33us to 5.56us on Alps), whereas on Leonardo it increases by 2x (from 2.03us to 4.23us). We observe a similar effect on the goodput. All three systems reach 95% of theoretical peak bandwidth when the two GPUs are connected to the same switch. However, when the two GPUs are on different Dragonfly/Dragonfly+ groups, the average goodput decreases by 1% on Alps and LUMI, and by 17% on Leonardo (from 395 Gb/s to 328 Gb/s).\nThis is mostly caused by a large performance variability experienced on Leonardo when the two GPUs are not under the same switch. This is due network noise [36], [37], [38], i.e., interference by other jobs sharing the same inter-node interconnection network. We analyze this in detail in Sec. VI. We observe that the 95th latency percentile on Leonardo increases to more than 8us when the two nodes are in different groups, with a maximum latency of 132us. Similarly, we observed a minimum 216 Gb/s goodput."}, {"title": "VI. NETWORK CONGESTION AND NOISE", "content": "Sec. V-B shows that Leonardo is affected by network noise, which severely impacts performance when GPUs are not under the same network switch. In this section, we detail such impact and how it affects the scalability of collective operations. We have not performed a similar analysis on Alps and LUMI since, as shown in Sec. V-B, and also in previous works [12], [5], Slingshot is largely unaffected by network noise.\nA. Performance Isolation through Service Level Selection\nThe variability observed in Sec. V-B largely comes from variable queueing delays experienced by packets when cross- ing the network (i.e., network noise). Indeed, we observed that variability only when the GPUs are not under the same switch. We thus try to reduce network performance variability by exploiting service levels.\nIn InfiniBand, service levels can be used to mark the class of service of an application and are mapped to switch virtual lanes. Each virtual lane is characterized by (logical) separate buffering and flow control. This means that applications share queues on network switches with other applications mapped to the same service level. Assuming a round-robin arbitration between the different virtual lanes, traffic forwarded on low- utilized service levels experiences lower queueing delays.\nSelecting a low-utilized service level can reduce the impact of network noise. On Leonardo, all the traffic is mapped by default on service level 0. To validate our hypothesis, we repeat the same experiment we ran in Fig. 8, by selecting a service level different from the default one. When switching to a service level different from the default one, we observed a significant reduction in performance variability, with a mea- sured difference lower than 1% between the minimum and maximum goodput for nodes in different groups (we do not explicitly show the results due to space constraints). It is worth remarking that, on Leonardo, adaptive routing is enabled on all service levels, and thus, the network noise reduction cannot be attributed to using or not using adaptive routing.\nAlthough switching to a different service level mitigated the impact of network noise, it is important to note that this is only possible because, on Leonardo, all the traffic is mapped to the same service level by default. We would observe a similar performance variability if other applications run on the non-default service level. We demonstrate this by running an allreduce on 128 GPUs and, concurrently and on the same service level, another microbenchmark (running either an alltoall or an incast) on other 128 GPUs (benchmarks are allocated on nodes randomly). We repeat the same experiment both on the default service level and on a non-default service level. We show the result of our analysis in Fig. 12. We observe that, when the allreduce is run together with the incast, the goodput drops regardless of which service level we run the application on. We also ran the same test, but by allocating the nodes so to minimize the sharing of network switches between the two applications. In that case, we observed no performance impact of the incast on the allreduce (results are not shown in the figure due to space constraints).\nThis demonstrates that network transfers on Leonardo are severely affected by network noise. Using a different service level only partially addresses the issue, and is strongly depen- dent on the number of jobs running on that service level. It is worth remarking that, to our knowledge, Leonardo is the only system deploying a Dragonfly+ topology at such a scale, and the routing algorithm might require further tuning to minimize the impact of network noise on performance.\nB. Noise Impact at Scale\nStudies on the impact of network noise on workload scal- ability have traditionally been carried out either through sim- ulations [40], [37], or by injecting synthetic traffic [10], [12]. However, because on Leonardo all the traffic is mapped to the same service level, by analyzing the performance difference between the default and non-default service levels we have the unique opportunity to estimate the impact of real production network noise on multi-GPU workload scalability. Indeed, when the application runs on the default service level, it experiences the real production network noise, whereas when it runs on the non-default service level, it performs similarly to running on an empty system.\nFor this reason, we report in Fig. 13 the goodput of a 2 MiB alltoall and a 1 GiB allreduce when running on the default and non-default service levels. We observe no differences when running on few GPUs, since only a few communications will occur between GPUs not connected to the same switch. When the number of GPUs increases, so does the number of inter-switch communications and the impact of congestion (i.e., the performance gap between the two service levels). Although, regardless of congestion, the performance decreases when increasing the number of GPUs (as described in Sec. V), we observe that, on 1,024 GPUs, network noise causes an additional 20% performance drop on alltoall, and a 50% drop on the allreduce. We want to remark that running on a non- default service level is only a temporary solution, possible because, at the time being, all the traffic runs by default on the same service level. Addressing this problem would thus require improvements in the adaptive routing algorithm. Moreover, on Leonardo Slurm is aware of which switch each node is connected to and to which Dragonfly+ group belongs, and can thus already optimize the job placement."}, {"title": "VII. STATE OF THE ART", "content": "A. Intra-node Interconnect\nDifferent works analyzed the intra-node GPU-GPU inter- connect. Pearson [41] characterizes the interconnect bandwidth heterogeneity within multi-GPU nodes using AMD MI250x GPUs, whereas Siefert et al. [42] provide a detailed analysis of the intra-node GPU-GPU performance across various sys- tems. However, neither paper analyzes inter-node performance. Moreover, while the former focuses on device-device copies without using any higher-level communication library, the latter only focuses on MPI. As we shown in Sec. IV, however, there are several tradeoffs to consider to determine the best communication library to use when moving data across GPUs on the same node, depending on the technology, transfer size, and communication pattern.\nSimilarly, Atchley et al. [5] characterize the Frontier su- percomputer, covering network, storage, and intra-node per- formance. The analysis performed on the GPU interconnect, differently from this work, does not analyze the performance that can be obtained through different software solutions (rang- ing from device-device explicit copies to MPI). Moreover, although the paper shows full-scale results on an alltoall collective through MPI using GPCNet [10], it does not specify whether the buffers are allocated on GPU memory (and thus GPU-Aware MPI is used) or on host memory. Because GPCNet allocates buffers on host memory [43], we assume the tests did not use GPU-Aware MPI. Thus, on the GPU side, it only focuses on intra-node point-to-point transfers.\nB. Inter-node Interconnect\nOther works extend the analysis to multiple nodes. Li et al. [44] evaluate modern NVIDIA GPU interconnect tech- nologies across different systems, including the Summit su- percomputer [45]. Khorassani et al. [17] compare different MPI implementations with RCCL for on multiple nodes of the Spock system, an early access cluster deployed with Slingshot and AMD MI100 GPUs. Both studies consider point-to-point and collective communications, but the analysis is limited to a few nodes (8 and 16, respectively). As we showed in Sec. V and Sec. VI, however, effects related to scalability and network noise are only visible at larger scales.\nSeveral works analyze network noise and interference be- tween jobs at scale [5], [37], [12], [38], [40], [36], [11], [46], often providing solutions to mitigate it. Most of these works, however, either simulate or synthetically generate noise. Dif- ferently from these works, in this paper we show the impact of noise induced by real production workloads.\nC. Other\nSeveral benchmarks have been proposed to assess the per- formance of intra- and inter-node interconnection networks, including OSU [27], NCCL and RCCL test [28], [29], Tar- tan [47], and others [48]. The focus of this paper is, however, on the analysis of performance and scalability of multi- GPU supercomputers, rather than on the benchmarking itself. Last, some works analyze multi-GPU performance on several workloads including deep learning [49], [50], [51], linear algebra [52], computational physics [53], biology [54], data management [55], and others. Although this helps understand application scalability, it does not allow for evaluating the impact of the interconnect on the overall performance, nor to characterize and identify potential network bottlenecks."}]}