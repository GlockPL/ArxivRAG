{"title": "Exploring GPU-to-GPU Communication:\nInsights into Supercomputer Interconnects", "authors": ["Daniele De Sensi", "Lorenzo Pichetti", "Flavio Vella", "Tiziano De Matteis", "Zebin Ren", "Luigi Fusco", "Matteo Turisini", "Daniele Cesarini", "Kurt Lust", "Animesh Trivedi", "Duncan Roweth", "Filippo Spiga", "Salvatore Di Girolamo", "Torsten Hoefler"], "abstract": "Multi-GPU nodes are increasingly common in the\nrapidly evolving landscape of exascale supercomputers. On these\nsystems, GPUs on the same node are connected through dedicated\nnetworks, with bandwidths up to a few terabits per second.\nHowever, gauging performance expectations and maximizing\nsystem efficiency is challenging due to different technologies,\ndesign options, and software layers. This paper comprehensively\ncharacterizes three supercomputers Alps, Leonardo, and\nLUMI each with a unique architecture and design. We\nfocus on performance evaluation of intra-node and inter-node\ninterconnects on up to 4,096 GPUs, using a mix of intra-node\nand inter-node benchmarks. By analyzing its limitations and\nopportunities, we aim to offer practical guidance to researchers,\nsystem architects, and software developers dealing with multi-\nGPU supercomputing. Our results show that there is untapped\nbandwidth, and there are still many opportunities for optimiza-\ntion, ranging from network to software optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Supercomputers are a key infrastructure enabling advance-\nments in several science domains and transformative societal\nchanges. New workloads' computing requirements, ranging\nfrom machine learning (ML) to scientific computing and\nextending to big-data analytics, are driving supercomputer\narchitecture evolution. Due to their massive parallelism, energy\nefficiency, and memory bandwidth, GPUs became the core of\nsuch evolution, characterized by the development of multi-\nGPU nodes and high-performance intra-node interconnection\nnetworks. Nowadays, exascale [1] and pre-exascale systems\nin the Top500 [2] are equipped with up to 8 GPUs per node,\nconnected with fast dedicated networks with bandwidth up to\n3.6 Tb/s per direction [3]. At the same time, due to a steady\nincrease in computing and memory requirements, the number\nof nodes increased to tens of thousands of nodes [4], leading\nto systems with up to 75,000 GPUs [5], [6].\nMoving data efficiently across such a high number of GPUs\nis challenging for multiple reasons. First, there is a significant\ninterconnect, topology, and hardware diversity, thus making\nthe mapping of communications to the underlying system\nnon-trivial. Secondly, on the software side, programmers can\nrely on different software solutions, ranging from manually\ncopying data between GPU memory on a single node to using\ntransparent and higher-level GPU-Aware solutions such as\nMPI [7] or NCCL/RCCL [8], [9] (referred as *CCL in the\nrest of the paper) across nodes. However, the most effective\napproach for managing a large number of GPUs and the\nmaturity level of the software is still unclear. Lastly, large-\nscale network-related effects such as congestion and network\nnoise [10], [11], [12] can severely impact the scalability\nwhen increasing the number of GPUs, thus hampering the\ncomputational power and high bandwidth of these systems.\nTo investigate the aforementioned challenges, we com-\nprehensively characterize three supercomputers with different\narchitectures: Alps (NVIDIA H100 GPUs and HPE Cray\nSlingshot interconnect [12]), Leonardo (NVIDIA A100 GPUs\nand NVIDIA InfiniBand HDR interconnect), and LUMI (AMD\nInstinctTM MI250X GPUs and HPE Cray Slingshot intercon-\nnect). We systematically benchmark the performance of intra-\nnode GPU networks (Sec. III and Sec. IV), and inter-node\nnetworks (Sec. V and Sec. VI) up to 4,096 GPUs on the three\nsupercomputers. Our study includes a detailed analysis of data\nmovement performed through explicit device-to-device copies,\n*CCL, and GPU-Aware MPI. Lastly, we evaluate the impact\nof network noise on GPU-GPU data movements (Sec. VI),\nshowing that it can severely impact workload scalability.\nThis paper provides the first at-scale study characterizing\nmulti-GPU interconnect performance across hardware tech-\nnologies and diverse communication APIs and software stacks.\nWe spotlight several sources of inefficiencies, ranging from\nrouting to communication libraries tuning. We show that the\nbest way to move data between GPUs depends on several\nfactors like transfer size, communication pattern, and number\nof GPUs, and might change across systems. We present\neight key observations, offering valuable insights to system\narchitects, researchers, practitioners, and software developers\nto optimize data movements in large-scale multi-GPU systems\nand exploit current and upcoming systems to their fullest."}, {"title": "II. SYSTEMS DESCRIPTION", "content": "In the following, we describe the main characteristics of the\nthree analyzed systems, and we summarize them in Table I.\nSince all the analyzed interconnects are full-duplex, when we\nrefer to intra-node and inter-node network bandwidth, we will\nalways report the unidirectional bandwidth and express it in\nbits per second for consistency.\nA. Alps\nAlps is a 270 PFlop/s supercomputer ranked 6th in the\nTop500 (June 2024). It is deployed by CSCS [13] and currently\nunder provisioning. Thus, some presented results might be\nsubject to further tuning in the upcoming months before\nopening to production. For this paper, we used the early access\nSantis partition [14], where we had access to 512 nodes.\nNode architecture Each node is composed of four GH200\nGrace Hopper Superchip [15] connected in an all-to-all topol-\nogy using NVLink 4.0. Six 200 Gb/s links connect each\nGH200 pair, for a total of 1.2 Tb/s between any GPU pair\n(see Fig. 1a). Every GH200 has 96 GB HBM3 and 120\nGB LPDDRX5 memory. Every node acts as a single NUMA\nsystem (composed of 8 NUMA domains), with 288 CPU cores\nand 4 GPUs.\nInter-node connectivity Each node has one HPE Cray\nCassini-1 200 Gb/s Network Interface Card (NIC) for each\nGH200. Nodes are connected in a Dragonfly topology [16]\nthrough an HPE Cray Slingshot-11 network [12], [17]. Each\nswitch has 16 ports to endpoints, 31 ports to other switches\nwithin the same Dragonfly group, and 17 ports to switches in\nother Dragonfly groups.\nB. Leonardo\nLeonardo [18] is a 240 PFlop/s supercomputer, ranked\n7th on the Top500 [2] (June 2024). It is owned by the\nEuroHPC Joint Undertaking and hosted by CINECA. We\nconsider Leonardo's Booster GPU partition, consisting of\n3,456 computing nodes.\nNode architecture Each node is equipped with a single\nsocket 32-core Intel Xeon\u00ae 8358 CPU and four NVIDIA\nA100 TensorCore GPUs [19] (13,824 GPUs in total). Each\nnode has 512 GB CPU memory, organized in eight DDR4\nslots, and 64 GB HBM2e memory per GPU. Within a node,\nGPUs are connected through NVIDIA NVLink 3.0, with each\nGPU connected to each of the other three GPUs through four\n200 Gb/s links (see Fig. 1b). The intra-node communication\nis completed by a 256 Gb/s 16-lane PCIe\u00ae Gen4.0 bus per\nGPU, used to communicate with the host CPU and with the\nNIC.\nInter-node connectivity Nodes are interconnected through\nan InfiniBand HDR network, and each node is equipped with\ntwo 200 Gb/s dual port NVIDIA Connect-X6 NICs. Each node\nthus has four 100 Gb/s network ports, all connected to the\nsame switch at the time of writing. For this paper, we consider\nthem as being four separate NICs. Nodes are connected\nthrough a Dragonfly+ [20], with each group containing 180\nnodes and structured as a two-level fat tree. Each group has\n18 spine and 18 leaf switches. Switches have 40 200Gb/s ports\n(each of which can be configured as 2 100Gb/s ports). Leaf\nswitches connect 40 100Gb/s ports to 10 nodes (4 GPUs per\nnode) and 18 200 Gb/s ports to spine switches (with 2 200Gb/s\nports unused). Spine switches connect 18 200Gb/s ports to\nleaf switches and 22 200Gb/s ports to other spine switches in\ndifferent Dragonfly+ groups.\nC. LUMI\nLUMI is a 380 PFlop/s supercomputer ranked 5th in the\nTop500 (June 2024). It is owned by the EuroHPC Joint\nUndertaking and hosted by CSC [21]. This paper considers\nthe LUMI-G GPU partition consisting of 2,978 nodes."}, {"title": "III. INTRA-NODE POINT-TO-POINT PERFORMANCE", "content": "We start our analysis by assessing the performance of\nthe intra-node GPU-GPU interconnection for point-to-point\ncommunications. We first describe the benchmarking method-\nology (Sec. III-A) and the performance tuning performed\non each system (Sec. III-B). We then analyze the point-to-\npoint performance (Sec. III-C), focusing on LUMI intra-node\narchitecture (Sec. III-D).\nA. Benchmarking Methodology\nIn all the experiments we have a separate MPI process\nmanaging each GPU in the system. We set the affinity of the\nprocesses so that each MPI rank manages the GPU closest to\nthe core it is mapped to. We run each experiment between 100\ntimes and 1,000 times (depending on the transfer size). For\nexperiments involving collective communication, we report\nthe maximum time (or minimum goodput) across all the\nparticipating ranks [23]. We do not include communicators'\ncreation time. Unless specified otherwise, we always refer to\nunidirectional bandwidth in Gb/s. We analyze the performance\nof point-to-point and collective communication using different\nmechanisms and techniques to transfer data between GPUs.\nNamely, we consider:\n\u2022 Trivial Staging: We copy buffers to and from the GPU\nmemory to the host memory. Then, we transfer data between\nprocesses using MPI. This is a trivial implementation to use\nas a baseline. We pinned the memory but implemented no\npipelining or parallel copies between memories. Data thus\nmoves in a store-and-forward fashion. For point-to-point\ntransfers, we can estimate the peak goodput by summing\nthe time required to transfer the data from device memory\nto host memory and the time to copy the data between two\nhost memory buffers.\n\u2022 Device-Device Copy: Buffers are copied directly from the\ndevice to the device memory. We share memory handles\nacross the processes managing the different GPUs, allowing\nthem to transfer data directly between GPU memories. For\nthe alltoall collective, each GPU copies data to all the other\nGPUs asynchronously to overlap the copies.\n\u2022 *CCL: We transfer data between GPUs using NCCL [8],\n[24] (on Leonardo and Alps) or RCCL [9], [25] (on LUMI).\n\u2022 GPU-Aware MPI: We transfer data using GPU-Aware\nMPI [26].\nWe developed our benchmark from scratch due to some lim-\nitations of existing benchmarks. OSU [27] lacks benchmarks\nfor explicit device-device copy, and nccl-/rccl-tests [28], [29]\nonly support *CCL. Also, both do not report individual per-\niteration timings, which are needed to assess network noise\nand performance variability (Sec. VI). Creating our benchmark\nensured consistency across all communication mechanisms an-\nalyzed. We used MPI_Wtime for timing individual iterations,\nwith resolutions of 25ns on LUMI and Leonardo, and 30ns\non Alps (measured experimentally). The timing excludes one-time operations like buffer allocation and handles exchange.\nThe benchmark synchronizes with the GPU before stopping\nthe timer to ensure full data receipt, except for MPI, where this\nis implicit. For *CCL, the timing includes the group start/end.\nThis is consistent with nccl-/rccl-tests. We publicly released\nthe code as part of the paper artifact.\nB. Performance Tuning\nWe tuned the performance of all analyzed systems, as\nthe initial default configuration did not fully leverage their\npotential. This involved searching for and setting environment\nvariables and analyzing their impact on performance.\n*CCL On Alps and LUMI we forced *CCL to ig-\nnore the CPU affinity set by Slurm (by setting the\nNCCL_IGNORE_CPU_AFFINITY to 1), obtaining up to 1.6x\nperformance improvement on alltoall and up to 6x on allreduce\nstarting from two nodes. We further improved performance\nby 2x on alltoall and 3x on allreduce, by increasing the\ndistance at which the GPU can use Direct RDMA when com-\nmunicating with the NIC (NCCL_NET_GDR_LEVEL=3). On\nLUMI, we set NCCL_NCHANNELS_PER_PEER=32 for intra-\nnode point-to-point tests, which led to a 3.5x performance\nimprovement."}, {"title": "C. Point-to-point Latency and Goodput", "content": "In Fig. 3, we report the goodput between two GPUs on\nthe same node, measured through a ping-pong microbench-\nmark for different transfer sizes. We report the unidirectional\ngoodput (in Gb/s), defined as the number of bytes in the\nbuffer divided by half the runtime. The inner plot reports the\nruntimes (in microseconds) for small messages. Each data point\nrepresents the mean across the experiments, and the width of\nthe shaded area around the line is the interquartile range (for\nsome plots it is too small to be visible). We report with dashed\nhorizontal lines both the GPU-GPU unidirectional nominal\ngoodput and the trivial staging expected goodput. For example,\non Leonardo, any GPU pair is connected with four 200 Gb/s\nlinks (see Fig. 1b), for a total of 800 Gb/s nominal goodput\nper direction between any pair of GPUs.\nOn LUMI, the peak GPU-GPU goodput depends on the\nspecific GPU selected. For this experiment, we selected GPUs\n0 and 1, connected through four 400 Gb/s Infinity Fabric links.\nIn Sec. III-D, we analyze the performance for different GPUs\ncombinations. Moreover, disabling SDMA enables GPUs to\nuse more than one Infinity Fabric link at a time [5]. On Alps,\nwe did not run the experiments involving explicit device to\ndevice copies since GPU peer access is not enabled on the\nnodes at the time being.\nGoodput First, we observe that the goodput of trivial\nstaging is up to one order of magnitude lower than the other\nimplementations due to the low bandwidth when moving data\nbetween host and device memory. *CCL, MPI GPU-Aware,\nand device-device copies provide a comparable goodput. On\nLeonardo, we observe a goodput for GPU-Aware on medium-\nsized messages that is up to 2x higher than that of NCCL.\nLatency When analyzing the runtime for small messages,\nwe observe similar performance for *CCL and MPI on Alps,\nbut a large performance gap on Leonardo and LUMI. On\nLeonardo, this is due to the use of GDRCopy [30]. On\nLUMI, Cray MPICH transfers small buffers between GPUs\non the same node by copying them through host memory\n(rather than doing device-device copy), using an optimized\nmemcpy, where the CPU issues load/store operations directly\nto GPU HBM. In contrast, on NVIDIA GPUs, CPU load/store\noperations to GPU memory are not permitted, resulting in\nhigher latency on Alps."}, {"title": "Observation 2: GPU-Aware MPI provides the highest\ngoodput for intra-node point-to-point transfers on all the\nanalyzed systems. For small transfers, the optimal solution\nchanges across the systems, depending on architectural\nfeatures and specific optimization implemented by MPI.", "content": ""}, {"title": "D. Impact of GPU Location on LUMI", "content": "On LUMI, each node has 8 GPUs, connected to each\nother with a different number of Infinity Fabric links, ranging\nfrom one to four (see Sec. II-C). We report in Fig. 4 the\nunidirectional goodput between GPU 0 and the other seven\nGPUs on a node when transferring a 1 GiB buffer. We denote\nwith a dashed horizontal line the nominal goodput for each\nGPU pair, computed by considering the single path with the\nhighest bandwidth between the two GPUs.\nAs expected, we do not observe any difference for the trivial\nstaging, since data is not moved directly between GPUs but\nacross the host memory. Both GPU-Aware MPI and the device-\ndevice copy achieve around 70% of the nominal goodput\non any GPU pair. On the other hand, in some cases (e.g.,\nwhen GPU 0 and 5 communicate) RCCL achieves less than\nhalf the goodput of GPU-Aware MPI and device-device copy.\nBy analyzing the data more in detail, we can observe that,\nalthough GPU-Aware MPI and device-device copy achieve the\nsame goodput both towards GPU 6 and 7 (towards which GPU\n0 has the same nominal goodput), RCCL achieves a much\nhigher goodput towards GPU 6 compared to GPU 7.\nBy analyzing RCCL debug information\u00b9, we identified it as-\nsumes a lower available bandwidth towards GPU 7 compared\nto 6, and thus does not fully exploit the available bandwidth.\nWe believe this might happen because the available bandwidth\nis estimated considering the number of hops rather than the\nnumber of paths connecting the two GPUs, which might be\nhelpful for collective communications, where multiple GPUs\nconcurrently communicate, sharing the available links. How-\never, this setup underutilizes the GPU-GPU interconnect for\nsparser communication patterns."}, {"title": "Observation 3: On LUMI, RCCL point-to-point communi-\ncation primitives do not correctly determine the bandwidth\navailable between GPUs on the same node, thus underuti-\nlizing the available bandwidth.", "content": "1 This\ncan\nbe done by setting the environment variables\nNCCL_DEBUG_SUBSYS=INIT,GRAPH and NCCL_DEBUG-INFO."}, {"title": "IV. INTRA-NODE COLLECTIVES PERFORMANCE", "content": "We now focus on intra-node collective performance, by an-\nalyzing expected and measured goodput for alltoall (Sec. IV-A\nand Sec. IV-B) and allreduce (Sec. IV-C and Sec. IV-D).\nA. Alltoall Expected Goodput\nFor collectives, we define the goodput as the buffer size\ndivided by the runtime. To compute the expected goodput,\nwe determined the edge forwarding index [31] of the graph\ncorresponding to the intra-node GPUs connectivity. The edge\nforwarding index is defined as the maximum number of paths\ncrossing any edge and gives an estimate of the maximum load\nacross any network link and, thus, of the worst-case peak\nbandwidth (e.g., for an alltoall). On Alps and Leonardo, GPUs\nare fully connected, and each link is crossed by only one path\n(i.e., the maximum edge forwarding index is one), and we\nexpect a peak goodput equal to the GPU injection bandwidth.\nOn LUMI, assuming data is routed between GPUs using\nshortest paths, the most loaded link is the one between GCD\n1 and 5 (and that between GCD 7 and 3), which is used in\nfour separate paths. Thus, because each IF link has a 400 Gb/s\nbandwidth, we can expect a 100 Gb/s peak goodput between\nany pair of GCDs during an alltoall. Because any GCD can\nsend data on six different IF links simultaneously, we expect a\npeak alltoall goodput of 600 Gb/s. It is worth noting that each\nGCD on the MI250X GPUs has the same injection bandwidth\nas one A100 GPU. However, the per-GPU alltoall goodput on\nLUMI is lower because the GCDs on the same node are not\nfully connected and, thus, the graph describing the intra-node\nconnectivity has a higher edge forwarding index.\nB. Alltoall Measured Goodput\nWe report in Fig. 5 the measured goodput for an alltoall\nbetween all GPUs on a node, and the expected goodput\n(denoted with a dashed horizontal line). While MPI and RCCL\nnatively provide an alltoall implementation, NCCL does not,\nand we implemented the alltoall with a trivial algorithm where\neach GPU sends the data to all the other nodes at the same time\n(as suggested in the documentation [32]). We used the same\nalgorithm to implement the alltoall using device-device copies.\nMoreover, we observed no performance difference when com-\nparing RCCL native and trivial alltoall implementations.\nOn Alps and LUMI, *CCL provides the best performance\nfor large transfers since *CCL collectives are specifically op-\ntimized for the target systems. For example, communications\nare mapped according to the specific topology, and the number\nof in-flight data chunks during pipelined operations is tuned\nby considering the bandwidth available between GPU pairs.\nSuch fine-grained optimizations are not performed by MPI,\nwhich thus does not entirely exploit the bandwidth of the intra-\nnode GPU interconnect. On Leonardo, *CCL provides slightly\nlower performance than MPI. For small transfers, on Alps and\nLeonardo the performance of *CCL is comparable with that of\nMPI. On the other hand, on LUMI, for small transfers GPU-\nAware MPI is up to 3x faster than *CCL, consistent with what\nwe observed for point-to-point transfers."}, {"title": "C. Allreduce Expected Goodput", "content": "On Alps and Leonardo, since each GPU is directly con-\nnected to the other GPUs on the same node and can receive\nfrom all the other GPUs at the same time, the optimal allreduce\nalgorithm would consist of a pipelined ternary tree reduce\n(with one of the GPUs as the root and the other three as leaves)\nfollowed by a ternary tree broadcast. We thus estimate the peak\ngoodput as the sum of the bandwidth across all the outgoing\nlinks from a GPU. On LUMI, since the GPUs are not fully\nconnected, the optimal algorithm for large messages would\nbe the Rabenseifner algorithm, with a ring reduce-scatter\nfollowed by a ring allgather [33]. The specific connectivity\nbetween the GPUs allows for four edge-disjoint bidirectional\nrings [22], each using 400 Gb/s Infinity Fabric links. Because\nthe Rabenseifner algorithm sends twice the number of bytes\nin the buffer, we can thus expect 800 Gb/s peak goodput."}, {"title": "D. Allreduce Measured Goodput", "content": "In Fig. 6, we show the allreduce performance for different\nmessage sizes. On Alps and Leonardo, *CCL outperforms MPI\nat any transfer size. On LUMI, on the other hand, GPU-Aware\nMPI is characterized by the lowest runtime for small transfers\nwhereas, although *CCL performs best on large transfers, its\nperformance is far from the expected peak. This is consistent\nwith what we observed for point-to-point transfers.\nGPU-Aware MPI exhibits low performance on all the ana-\nlyzed systems, and we observe a higher performance gap be-\ntween *CCL and GPU-Aware MPI on the allreduce compared\nto the alltoall. Indeed, the allreduce involves data aggregation,\nwhich *CCL performs on the GPUs. Although MPICH also\nperforms data aggregation on the GPUs, we believe that\n*CCL coordinates GPU execution better. On Leonardo, we\nnote an even larger gap, since Open MPI runs the allreduce\non the host [34], similarly to what we do in the baseline\nimplementation. It is worth remarking that on Leonardo Open\nMPI does not support UCC [35]. The implementation relying\non device-device copies performs a reduction on GPU 0,\nfollowed by a broadcast. We do not implement any form of\npipeling, and we mostly use it for reference and to show that\nimplementing efficient multi-GPU collectives is non-trivial.\nLast, we observe a higher gap between measured and ex-\npected performance on collectives compared to point-to-point\ncommunications, showing there is still space for collective\nalgorithms optimization. Measured goodput on LUMI gets\ncloser to the expected one. Indeed, LUMI has a lower expected\ngoodput, which is thus easier to saturate."}, {"title": "Observation 4: For single node collectives, *CCL outper-\nforms GPU-Aware MPI in most cases, except for small\ncollectives on LUMI. Indeed, unlike MPI, *CCL collectives\nare optimized for the specific GPU models. Nevertheless,\nthere is still room for collective algorithms optimization.", "content": ""}, {"title": "V. INTER-NODE PERFORMANCE", "content": "We now analyze the interconnect performance when running\non multiple nodes. We first analyze point-to-point performance\n(Sec. V-A), focusing on the impact of network distance on\nperformance (Sec. V-B). Then, we analyze the performance\nof collective communications up to 4,096 GPUs for alltoall\n(Sec. V-C) and allreduce (Sec. V-D). We apply the bench-\nmarking methodology described in Sec. III-A, and we run a\nnumber of MPI processes on each node equal to the number\nof available GPUs. For completeness, to assess the overhead\nof managing GPUs (especially for small transfers), we also\nanalyze the performance when transferring buffers located in\nthe host memory. In this case, we run one MPI process per\nNIC. On Alps and LUMI, we set the affinity of each process\nso that it uses the closest GPU and NIC.\nA. Unidirectional Latency and Goodput\nWe run a ping-pong test to measure the point-to-point\ngoodput between two nodes, with each process exchanging\ndata with the corresponding process on the other node. We\nreport the results of our analysis in Fig. 7. We report on the\nx-axis the number of bytes transmitted on each NIC, and, on\nthe y-axis, the total goodput of the node (i.e., the sum of\nthe goodput on each NIC). MPI provides the highest goodput\nand lowest latency on all the analyzed systems, regardless of\nwhether the buffer is in host or GPU memory. This is mostly\ndue to overheads introduced by *CCL when launching and\nmanaging GPU kernels."}, {"title": "Observation 5: On inter-node point-to-point communi-\ncations, MPI outperforms *CCL by up to one order of\nmagnitude on small transfers, and by up to 3x on larger\ntransfers.", "content": ""}, {"title": "B. Impact of Network Distance on Performance", "content": "We now analyze the impact of the GPU network location on\nperformance. Namely, on all systems, two GPUs on different\nnodes can be connected to the same switch, to two switches on\nthe same Dragonfly/Dragonfly+ group, or to two switches in\ntwo different groups. We measure latency and goodput with\na ping-pong test sending one byte and 1 GiB, respectively.\nWe use MPI since we showed in Fig. 7 that, for point-to-\npoint transfers, it provides the highest bandwidth and lowest\nlatency on all the analyzed systems. We report the results of\nour analysis in Fig. 8a for buffers allocated on GPU memory.\nFor completeness and to separate the impact of the network\nfrom that of the GPU management, we report in Fig. 8b the\nsame analysis, but for buffers allocated on host memory.\nEach box's top and bottom borders represent the third and\nfirst quartiles, respectively. The middle line represents the\nmedian, the \u00d7 marker the mean, and the top and bottom\nwhiskers are the 5th and 95th percentile. Last, the notch\naround the median represents the 95% confidence interval\nof the median. For the sake of readability, we do not report\nindividual outliers but we annotate on the plot the minimum\nand maximum observed values.\n1) GPU memory buffers: We first focus on the performance\nwhen transferring buffers allocated on GPU memory (Fig. 8a).\nWe observe that when the two GPUs are connected to the same\nswitch, all systems exhibit a similar latency, between 3.7us and\n5.7us. However, the location of the GPUs impacts both latency\nand goodput on all three systems. On Alps and LUMI, when\nthe two GPUs are on different Dragonfly groups, the average\nlatency increases by 28% compared to the case where the two\nGPUs are under the same switch (from 4.33us to 5.56us on\nAlps), whereas on Leonardo it increases by 2x (from 2.03us to\n4.23us). We observe a similar effect on the goodput. All three\nsystems reach 95% of theoretical peak bandwidth when the\ntwo GPUs are connected to the same switch. However, when\nthe two GPUs are on different Dragonfly/Dragonfly+ groups,\nthe average goodput decreases by 1% on Alps and LUMI, and\nby 17% on Leonardo (from 395 Gb/s to 328 Gb/s).\nThis is mostly caused by a large performance variability\nexperienced on Leonardo when the two GPUs are not under\nthe same switch. This is due network noise [36], [37], [38],\ni.e., interference by other jobs sharing the same inter-node\ninterconnection network. We analyze this in detail in Sec. VI.\nWe observe that the 95th latency percentile on Leonardo\nincreases to more than 8us when the two nodes are in different\ngroups, with a maximum latency of 132us. Similarly, we\nobserved a minimum 216 Gb/s goodput.\n2) Host memory buffers: To provide a complete picture and\nisolate network performance from overheads related to GPU\nmanagement, we report in Fig. 8b the same experiment, but\ntransferring buffers allocated on host memory. We observe that\nlatency on Leonardo is more than 3x smaller than on Alps and\nLUMI (1.02us vs. 3.66us for nodes connected to the same\nnetwork switch). We attribute part of this difference to Sling-\nshot relying on an Ethernet-based protocol, thus characterized\nby a slightly higher overhead compared to InfiniBand [12]\n(e.g., due to larger headers [12], [39]). Although using the\nsame network technology, Alps latency is slightly higher\nthan LUMI's. However, Alps is not yet in production and\noptimizations across the entire stack are still ongoing."}, {"title": "Observation 6: On Alps and LUMI, GPU's network loca-\ntion has a marginal impact on average performance (below\n30% for latency and 1% for goodput). On the other hand,\non Leonardo, the average latency increases by up to 2x\nwhen the GPUs are in different groups rather than under the\nsame switch. Similarly, the average goodput decreases by\n17%. This is mainly due to network performance variability\ncaused by network noise.", "content": ""}, {"title": "C. Alltoall", "content": "We first analyze in Fig. 9 the goodput of a 2 MiB alltoall\nwhen increasing the number of allocated GPUs. We consider\nan asymptotically expected goodput, i.e., the expected goodput\nfor a sufficiently large number of GPUs. This can be computed\nas the inter-node bandwidth available to each GPU (100\nGb/s on LUMI and Leonardo, and 200 Gb/s on Alps). This\nunderestimates the actual goodput for a small number of GPUs\nsince a larger fraction of communications happens on the intra-\nnode rather than the inter-node network. The actual expected\ngoodput can be easily computed by dividing the asymptotically\nexpected goodput by the ratio of communications occurring\non the inter-node network. However, we only report the\nasymptotic one to improve the readability of the plot."}, {"title": "VI. NETWORK CONGESTION AND NOISE", "content": "Sec. V-B shows that Leonardo is affected by network noise,\nwhich severely impacts performance when GPUs are not under\nthe same network switch. In this section, we detail such impact\nand how it affects the scalability of collective operations. We\nhave not performed a similar analysis on Alps and LUMI\nsince, as shown in Sec. V-B, and also in previous works [12", "n[5": "Slingshot is largely unaffected by network noise.\nA. Performance Isolation through Service Level Selection\nThe variability observed in Sec. V-B largely comes from\nvariable queueing delays experienced by packets when cross-\ning the network (i.e., network noise). Indeed, we observed\nthat variability only when the GPUs are not under the same\nswitch. We thus try to reduce network performance variability\nby exploiting service levels.\nIn InfiniBand, service levels can be used to mark the class\nof service of an application and are mapped to switch virtual\nlanes. Each virtual lane is characterized by (logical) separate\nbuffering and flow control. This means that applications share\nqueues on network switches with other applications mapped\nto the same service level. Assuming a round-robin arbitration\nbetween the different virtual lanes, traffic forwarded on low-\nutilized service levels experiences lower queueing delays.\nSelecting a low-utilized service level can"}]}