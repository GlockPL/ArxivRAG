{"title": "Few is More: Task-Efficient Skill-Discovery for Multi-Task Offline Multi-Agent Reinforcement Learning", "authors": ["Xun Wang", "Zhuoran Li", "Hai Zhong", "Longbo Huang"], "abstract": "As a data-driven approach, offline MARL learns superior policies solely from offline datasets, ideal for domains rich in historical data but with high interaction costs and risks. However, most existing methods are task-specific, requiring retraining for new tasks, leading to redundancy and inefficiency. To address this issue, in this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). Unlike existing offline skill-discovery methods, SD-CQL discovers skills by reconstructing the next observation. It then evaluates fixed and variable actions separately and employs behavior-regularized conservative Q-learning to execute the optimal action for each skill. This approach eliminates the need for local-global alignment and enables strong multi-task generalization from limited small-scale source tasks. Substantial experiments on StarCraftII demonstrates the superior generalization performance and task-efficiency of SD-CQL. It achieves the best performance on 10 out of 14 task sets, with up to 65% improvement on individual task sets, and is within 4% of the best baseline on the remaining four.", "sections": [{"title": "1 Introduction", "content": "Multi-agent reinforcement learning (MARL), as a cornerstone of artificial intelligence, provides advanced methodologies to tackle complex challenges requiring coordinated, task-driven decision-making among multiple agents through interaction [32, 6]. Integrated with deep neural networks, MARL has demonstrated exceptional success across a diverse range of critical applications, e.g., video games [21], autonomous systems [30] and finance [14]. With growing acknowledgment of data's critical role in machine learning, offline MARL has drawn increased attention from researchers\nTherefore, we need to confront the following two key challenges: (i) Addressing distributional shifts in offline training, which involves how to identify appropriate skills and learn higher-performing policies using limited task data. (ii) Maintaining policy performance amidst changes, which entails that policies are sufficiently general to remain robust across varying task scales and environments. Although we can overcome the first challenge via conservative policy optimization, increased conservatism on the source task usually leads to poorer generalization to unseen tasks, highlighting"}, {"title": "2 Related Work", "content": "MARL. Multi-Agent Reinforcement Learning (MARL) has seen substantial progress in recent years, with numerous approaches developed under different paradigms. The centralized training with decentralized execution (CTDE) paradigm [24, 22, 2] has been particularly influential, with methods such as HASAC[17], MAPPO [42], QMIX [27], VDN [34], and MADDPG [20]. These approaches use centralized training for better coordination and decentralized execution for real-time decision-making. On the other hand, fully decentralized training and execution schemes have also been explored [35, 1, 3]. However, the performance of such methods is often constrained by the absence of information sharing. In this paper, we mainly focus on CTDE paradigm with QMIX backbone.\nOffline MARL. Due to the absence of online interaction with the environment, offline training faces a fundamental challenge-distributional shift. To address this issue, several techniques for single-agent RL have been proposed, many of which leverage conservatism to regularize either the policy [5, 11] or the Q-value function [13, 12, 28]. These methods mitigate the risks of overestimating the value of unseen state-action pairs. However, specific challenges caused by multiple agents, such as the exponential explosion of the complexity, hinder these techniques from directly extending to multi-agent scenarios. Therefore, several tailored approaches [9, 41, 25, 16, 31, 19] have been"}, {"title": "3 Background", "content": "3.1 Multi-task MARL and Multi-Task Offline MARL\nA cooperative MARL task, indexed by m, can be formulated as a decentralized partially ob-servable Markov decision process (Dec-POMDP) [23]. A Dec-POMDP is represented by a tuple (Nm, Sm, Om, Am, Pm, Rm, y), where Nm is the set of agents, Sm is the set of states, Om is the joint observation space, Am is the joint action space, Pm is the state transition probability (defining the probability of transitioning to the next state given the current state and joint action), Rm is the immediate reward shared by all agents, and y is the discount factor.\nThe multi-task MARL problem is a collection of such MARL tasks, which can be represented by the tuple (N, S, O, A, P, R, \u03b3, \u03a4). Here, T is the set of tasks, y is the discount factor shared by all tasks, and the remaining elements are the union of corresponding elements across all tasks. For example, N = UmET Nm represents the union of the sets of agents across all tasks.\nFor the task m, each agent maintains its observation-action history \u03c4\u00b2 \u2208 Tm, and the corresponding joint history is denoted by\u03c4\u03b5Tm. T = Umer Tm represents the collection of observation-action histories across all tasks, and the goal is to find a joint policy \u03c0: \u03a4\u2192 A that maximizes the expected discounted return average over all tasks:\n$J = \u0395_{\\pi,\u03a4} \\left[\\sum_{t=0}^{T} \u03b3^{t} \\cdot r_m(s_t^m, a_t^m) \\right]$\nwhere T is the time horizon and rm(sm, am) is the reward for taking joint action am at state si in task m."}, {"title": "3.2 CTDE Framework and QMIX", "content": "A common framework for cooperative MARL is Centralized Training for Decentralized Execution (CTDE). In this framework, agents can leverage centralized information during training but must rely solely on their local observations during execution. In this work, we adopt QMIX [27] as our backbone algorithm, which is one of the most popular discrete-action CTDE algorithms. In principle, our method can be applied to any value-based algorithm.\nIn QMIX, each agent maintains an individual Q-function Qi(r\u00b2, a\u00b2), which is conditioned on its own observation-action history \u03c4\u00b2 and action a\u00b2. Then, it calculates a joint Q-function Qtot(r, a) from individual Q-functions through a mixing network fs, such that:\n$Q_{tot} (T, a) = f_s(Q_1(r^1, a^1), ..., Q_n(\u03c4\u03b7, \u03b1\")).$\nThe mixing network is designed to satisfy the monotonicity constraint, ensuring that the partial derivative of Qtot with respect to each Qi is non-negative. To train the Q-function, QMIX minimizes the temporal-difference error on Qtot.\""}, {"title": "4 Skill-Discovery Conservative Q-Learning", "content": "To develop scalable policies that can generalize to varying unseen tasks through data from only a few source tasks, we propose Skill-Discovery Conservative Q-Learning (SD-CQL), a task-efficient algorithm for multi-task offline MARL. As illustrated in Figure 3, SD-CQL discovers cross-task generalizable skills in the latent space through observation reconstruction, and then trains scalable policies via multi-task CQL and BC regularization. This approach mitigates distributional shifts and error accumulation in offline MARL, as well as the generalization challenges inherent in multi-task learning. Below, we introduce each component in detail. The full algorithm is shown in Algorithm 1."}, {"title": "4.1 Observation Reconstruction", "content": "To improve policy generalization across tasks, a natural idea is to make agents learn task-shared decision features, i.e., \"skills\", from offline data. Hence, we choose to achieve this by reconstructing the next observations without global state or task-specific rewards. However, abandoning these global or task-specific information requires the skills to enhance the ability of capturing the local features. Therefore, we leverage non-linear activation masks to address this issue.\nThis design is motivated by two main reasons. First, assuming unseen tasks share similar structures"}, {"title": "4.1.1 Observation Encoder", "content": "Similar to the previous work [8, 43, 36], to accommodate tasks with a variable number of agents, we utilize a transformer architecture [38] to handle observations of different sizes. Specifically, we first decompose the i-th agent's observation at t-th time step of into o related to itself, and {k}1{k}KKa related to other entities, where Ka is the number of entities that can interact\nik Ki Ot with agents i, and Ki is the total entities observed by agent i.\nWe further encode these heterogeneous entity information into embeddings with same dimen-sions:\n$e_{i,k} = Emb(ok), k = 0, 1, 2 . . ., K_i$\nwhere Emb(.) represents the heterogeneous embedding network. Finally, we use a single-layer"}, {"title": "4.1.2 Observation Decoder", "content": "To enable agents to select appropriate skills corresponding to their specific situations, we introduce an observation decoder. By reconstructing the next observation, agents can discover skill vectors in a latent space. Concretely, for the embeddings {Ei,k}Ko from the observation encoder, the decoder first projects the own information E\u00bf,0 into a latent skill vector z\u2208 Z through a linear layer. Then, to make the skill vector extract more information, we reconstruct the embedding associated with the agent itself directly with z:\n$\u00ca_{\u00bf,0} = W_o ReLU(z) + b_o$\nWhile for embeddings of other entities, we first use a ReLU function to mask parts of the information, and then concatenate them with z for reconstruction:\n$Eik = W_k [ReLU(E_{i,k}), z] + b_k, k = 1, 2, ..., K_i$\nwhere [,] represents the concatenation operation.\nFinally, similar to the encoder, the reconstructed embeddings are passed through a single-layer transformer, restored into entity vectors {\u00f4i,k}, and reassembled into the next observation +1.\nThen, the final reconstruction loss is:\n$L_{Rec} = \\frac{1}{N} \\sum_{i=1}^{N} MSE(\u00f4_{i,t+1}, o_{i,t+1})$\nwhere N is the number of agents to control in the task, and MSE(\u00b7,\u00b7) represents the Mean Square Error."}, {"title": "4.2 Skill-conditioned Policy Optimization", "content": "After identifying and selecting the appropriate skill, existing multi-task offline MARL algorithms primarily execute the corresponding actions through behavior cloning (BC). However, since a single skill often encompasses multiple specific actions, selecting the optimal skill does not necessarily ensure that all associated actions are optimal.\nTherefore, we leverage conservative Q-learning to optimize the skill-conditioned policy \u03c0(\u03b1|s, z), enabling the execution of optimal actions associated with the selected skill. To mitigate the"}, {"title": "4.2.1 Skill-conditioned Q-value", "content": "To handle the variation in the number of actions across different tasks, we utilize two separate Q-networks. One network, called Qown, is responsible for a fixed set of actions related to the agent itself, while the other, called Quar, handles a variable set of actions associated with other entities in its observation. Both networks receive the corresponding entity embeddings Ei,k and skill vectors z as inputs and output the Q-values for their respective actions. The parameters of Q-value networks are shared by all the agents. Therefore, the individual Q-values for the i-th agent with skill zi are:\n$Qi(aOi, Zi) =\\begin{cases}Q^{own}(a | E_{i,0}, z_i) & \\text{if } a \\in A^{own} \\\\Q^{var} (a | E_{i,k}, z_i) & \\text{if } a \\in A_{i}^{k}\\end{cases}$\nwhere Aown is the set of actions that only related to the i-th agent itself, and A is the set of actions\nthat related to the i-th agent and the k-th other entity.\nTo avoid interference between observation reconstruction and policy optimization, we truncate the gradient propagation of the embeddings and skill vectors before feeding them into the network.\nFinally, we employ a mixing network to aggregate the individual Q-values into a global Q-value Qtot according to (1). Then, we can optimize Qtot through the temporal difference loss outlined below:\n$\\mathcal{L}_{TD} = [Q_{ott, a}(T, az) - (r + y max Q_{tot}Q_{tot}(T', a' | 2'))]^2$\nwhere \u03c4', a' and z' denote the next observation-action history, joint action and joint skill, respectively, and Qtot represents the target joint action-value function."}, {"title": "4.2.2 Conservative Q-learning", "content": "In the offline setting, agents are unable to ascertain the true environment distribution. Conse-quently, even on tasks with available training data, agents may still make detrimental decisions by overestimating out-of-distribution (OOD) state-action pairs. To address this issue, we employ Conservative Q-learning (CQL) [13] regularization term. This regularization term enhances the evaluation of in-dataset state-action pairs while suppressing the evaluation of OOD ones, thereby achieving conservative offline value estimation:\n$\\mathcal{L}_{CQL} = E_{\u03c4~D, a~\u03bc} [Q(\u03c4, \u03b1 | z)] \u2013 E_{(\u03c4,a)~D} [Q(t, a | z)]$"}, {"title": "4.2.3 Behavior Cloning Regularization", "content": "In addition to distributional shifts, the accumulation of estimation errors caused by multiple agents is another challenging issue. In multi-agent environments, relying solely on Q-learning remains inadequate to mitigate the rapidly escalating Q-value estimation errors [26]. Therefore, drawing inspiration from previous studies [5, 25], we additionally incorporate a BC regularization to improve the stability of SD-CQL's performance across tasks of different scales.\n$\\mathcal{L}_{BC} = \\frac{1}{N} \\sum_{i=1}^{N} CE (Softmax(Qi), \u03b2_i)$\nwhere CE (,) is the cross entropy loss, N is the number of agents, Qi is the individual Q-values of all the action available for the i-th agent, and Bi is the corresponding one-hot action in the offline datasets.\nThen, the total loss function for of SD-CQL is:\n$L = (1 \u2212 \u03b7) \u00b7 LQ + \u03b7 \u00b7 LBC + LRec$\nwhere \u03b7 \u2208 [0, 1] is the hyperparameter to control the strength of the BC regularization."}, {"title": "5 Experiments", "content": "To evaluate the performance of SD-CQL in multi-task offline MARL scenarios, we establish multiple transfer training task sets based on the StarCraft Multi-Agent Challenge (SMAC) [29]. Through these experimental task sets, we aim to address the following questions: (i) Compared to existing algorithms, does SD-CQL demonstrate better performance in multi-task offline MARL (Section 5.2), (ii) Do skill vectors indeed characterize the decision-making features of agents in different contexts (Section 5.3), and (iii) Are components within SD-CQL critical to its performance (Appendix \u0392)."}, {"title": "5.1 Setup", "content": "Datasets. Following the definition by D4RL [4], our experiments utilize datasets of four quality: Expert, Medium, Medium-Replay, and Medium-Expert. For fair comparisons, we use the datasets provided by ODIS [43]. Details are provided in Appendix A.1.\nTask Sets. To comprehensively simulate different transfer scenarios, we establish five representative offline training zero-shot transfer task sets: Marine-Easy, Marine-Hard, Stalker-Zealot, Marine-Single, and Marine-Single-Inv. They can be categorized into two types: (i) Training on multiple tasks and test on multiple tasks (Multi-to-Multi) and (ii) Training on one task and test on multiple tasks (One-to-Multi). The first three task sets, which fall under the Multi-to-Multi category, are consistent with those in ODIS [43], while the last two, classified as One-to-Multi, are additionally designed by us. More details are included in Appendix A.2.\nBaselines. We primarily select four (offline) multi-task MARL algorithms as baselines: BC-t, BC-r, UpDeT [8], and ODIS [43]. Among them, BC-t is a behavior cloning approach based on a multi-task transformer, BC-r incorporates return-to-go information into the input of BC-t, UpDeT"}, {"title": "5.2 Evaluation", "content": "5.2.1 Multi-to-Multi\nThe Multi-to-Multi task sets include Marine-Easy, Marine-Hard, and Stalker-Zealot, each comprising three source tasks of varying scales and several unseen tasks. Agents are trained offline using datasets from the source tasks and are tested across all tasks, with zero-shot testing conducted on the unseen tasks. For each task set, we set up four datasets of different qualities. We report the average performance of the algorithms across all tasks for each task set and dataset in Table 1. Detailed results can be found in Appendix C.\nThe results show that, SD-CQL achieves the best performance for 8 out of 12 task sets, with the other four being very close to the best baseline algorithms (within 4%). In particular, in Stalker-Zealot-Medium-Expert and Marine-Easy-Medium-Replay task sets, SD-CQL significantly outperforms other algorithms, with performance improvements of 65% and 47%, respectively, compared to the best-performing baseline. This demonstrates that SD-CQL is capable of maintaining superior multi-task offline generalization performance across various dataset qualities.\n5.2.2 One-to-Multi\nThe One-to-Multi task sets include Marine-Single and Marine-Single-Inv. Specifically, Marine-Single requires agents to train offline using only the dataset from the 3m task and to test on tasks with scales up to 12m to assess the agents' generalization ability to larger-scale tasks. (3m refers to each side having 3 Marine units, and similarly for the others). Conversely, Marine-Single-Inv requires agents to train offline only from the 10m task and to test on tasks with a minimum scale of 3m to evaluate whether the agents can truly learn more general decision-making skills. It is unlikely to expect a well-generalized policy from a small set of tasks with poor datasets. Hence, we use the expert dataset to simulate mastering simpler tasks before extending to more complex ones. We report the full results in Table 2.\nIt can be seen that, since the Marine-Single task set is trained using only 3m-expert data, all algorithms perform better on tasks with smaller agent scales. However, as the scale of the test tasks increases, the performance of baseline algorithms based on BC sharply degrades. In contrast, the performance degradation of SD-CQL is significantly less, maintaining an average win rate of 30% even when the task scale is expanded to 12m, four times the original size. Moreover, SD-CQL exhibits the best performance across all test tasks, with average performance far surpassing other baseline algorithms. This indicates that SD-CQL indeed learns general decision-making skills through skill-discovery with higher task-efficiency, enabling it to better handle larger-scale unseen tasks.\nFor the Marine-Single-Inv task set, SD-CQL also demonstrates superior generalization performance on inverse generalization tasks. In contrast, the BC algorithms completely fail on smaller-scale tasks despite performing adequately on source tasks, as they do not learn generalizable skills. Although UpDeT performs relatively well on the 3m task, it exhibits high variance, and its overall"}, {"title": "5.3 Skill-Discovery Visualization", "content": "To provide a more intuitive demonstration of the multi-task generalization capability of SD-CQL, we visualize part of decision scenarios of the source task 3m and the target task 12m from the Marine-Single task set in Figure 1. Specifically, we record the battle replays (1a and 1b) and project the corresponding skill vectors z at each decision point onto a two-dimensional plane using t-SNE [37] (1c, where each marker represents the skill vector z chosen by an agent at the moment recorded in la and 1b). More details are available in Appendix A.5.\nIt can be observed that SD-CQL successfully learns two skills, retreat and attack, in the 3m task and applies them effectively in the unseen 12m task. When the healthy level is low, the agents choose to retreat, while when health is sufficient, they opt to attack, which is consistent with the visualization in 1c. This demonstrates that SD-CQL indeed discovers effective skills. Notably, in 12m, agents adopting the attack skill exhibit two specific actions: firing and advancing (highlighted by the green dashed box), further supporting that SD-CQL extracts cross-task generalizable decision structures instead of mimicking specific actions."}, {"title": "6 Conclusion", "content": "In this paper, we propose a task-efficient multi-task offline MARL algorithm, Skill-Discovery Conservative Q-Learning (SD-CQL). It discovers skills offline through observation reconstruction and trains scalable policies via behavior-regularized conservative Q-learning. We conduct substantial experiments on StarCraftII to present the superior generalization performance and task-efficiency of SD-CQL: It achieves the best performance on 10 out of 14 task sets, with up to 65% improvement on individual task sets, and is within 4% of the best baseline on the remaining four. We hope this work inspires future researchers to engage in the study of scalable multi-task offline MARL, further improving the performance and efficiency of multi-agent decision-making."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}]}