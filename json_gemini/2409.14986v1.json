{"title": "Evaluating Theory of (an uncertain) Mind:\nPredicting the Uncertain Beliefs of Others in Conversation Forecasting", "authors": ["Anthony Sicilia", "Malihe Alikhani"], "abstract": "Typically, when evaluating Theory of Mind, we consider the\nbeliefs of others to be binary: held or not held. But what\nif someone is unsure about their own beliefs? How can we\nquantify this uncertainty? We propose a new suite of tasks,\nchallenging language models (LMs) to model the uncertainty\nof others in dialogue. We design these tasks around conver-\nsation forecasting, wherein an agent forecasts an unobserved\noutcome to a conversation. Uniquely, we view interlocutors\nthemselves as forecasters, asking an LM to predict the uncer-\ntainty of the interlocutors (a probability). We experiment with\nre-scaling methods, variance reduction strategies, and demo-\ngraphic context, for this regression task, conducting exper-\niments on three dialogue corpora (social, negotiation, task-\noriented) with eight LMs. While LMs can explain up to 7%\nvariance in the uncertainty of others, we highlight the dif-\nficulty of the tasks and room for future work, especially in\npractical applications, like anticipating \"false uncertainty.\"", "sections": [{"title": "Introduction", "content": "Theory-of-mind and, specifically, false belief prediction is\nan integral part of planning and decision-making in conver-\nsations (Ho, Saxe, and Cushman 2022). For instance, an in-\nterlocutor recognizing another's false beliefs can offer clar-\nification or explanation. While beliefs are often treated as\nexisting in a binary state (held or not held), there are sit-\nuations where an interlocutor's belief is better represented\nmore flexibly (e.g., held, not held, or unsure), capturing the\nuncertainty or more general intensity of the belief. Strangers\nin a first encounter may be unsure of how they feel about\neach other and competing stakeholders in a negotiation may\nbe similarly perplexed about each other's objectives. In task-\noriented settings, people may even be unsure of their goals\n(Sicilia et al. 2023), impeding success when this is not con-\nsidered (e.g., see Figure 1). How we respond in conversation\ndepends not only on our anticipation of another's belief, but\nalso how strongly they hold that belief. Yet, it's unclear how\nconversational AI, like language models, represent the un-\ncertainty of others in dialogue.\nThis paper tackles this issue, proposing new tasks to eval-\nuate ToM capacity in language models, specifically pertain-\ning to uncertainty. To do this, we use conversation forecast-\ning as a tool. Whereas existing forecasting tasks (Sokolova,"}, {"title": "Conversation Forecasting", "content": "We focus on the setup of Sicilia et al. (2024) where an agent\nobserves a conversation and is asked to express their uncer-\ntainty about a potential outcome for this conversation; e.g.,\n\"How much does Speaker A like Speaker B?\" or \"Will the\nnegotiation result in a deal?\" As implied, the conversation\nis just a partial window into the true (or, eventual) ground-\ntruth. Hidden information, such as future events or mental\nstates, creates an inherent randomness about reality, which\nmay not be fully determined by the available evidence. In\nthis context, we assume a (human) agent forms a mental\nmodel capturing their uncertainty about the outcome a\n\"forecast\" about whether the outcome will occur."}, {"title": "Comparing Forecasts with Ground-Truth", "content": "Given a (potentially partial) conversation and any accompa-\nying evidence about the situation (e.g., interlocutor con-\ntext), the forecasting agent expresses their uncertainty $P$\nabout the outcome of interest. For now, we assume $P$ is a\nprobability estimate, but later allow other expressions of un-\ncertainty (\u00a7 3.1). The forecast $P$ is evaluated by Brier score:\n$BS = (P \u2013 O)^2$\nwhere $O$ is a binary indicator of the outcome (e.g., 1 if a\ndeal occurs, 0 else). Forecasters with accurate uncertainty\nestimates (agreeing exactly with the distribution of $O$) will\nhave a lower Brier score than other, less accurate forecasters.\nBrier score also ranks sub-optimal forecasts with considera-\ntion of both calibration and variance (Br\u00f6cker 2009)."}, {"title": "The Missing Building Blocks for ToM", "content": "We observe that Brier score, alone, does not capture the full\nstory about an agent's uncertainty $P$. Indeed, the Brier score\nmeasures two individual aspects of uncertainty:\n$E[BS] = Var[O] + E[(P-p)^2]$\naleatoric uncertainty epistemic uncertainty\nwhere $p$ is the probability $O = 1$. While the outcome vari-\nance captures the inherent randomness of the forecasting\ntask, the latter quantifies the forecaster's excess errors that\nshould not be attributed to this randomness. These model-\nspecific aspects of error make up the epistemic uncertainty\n(Lahlou et al. 2022; H\u00fcllermeier and Waegeman 2021).\nIntegrating ToM in Forecasting Uniquely, we consider\nthe epistemic uncertainty of human interlocutors (treated as\nforecasters) in a conversation. This dual interpretation cap-\ntures the individual aspects of an interlocutor's uncertainty\nby comparing their forecast to ground-truth. Precisely, it\nmeasures the fluctuations in uncertainty caused by the in-\nterlocutor themselves \u2013 their knowledge, perceptions, and\nbiases - rather than those (fluctuations) which may be at-\ntributed to changes in ground-truth. Based on the epistemic\nuncertainty, we define an interlocutor's false uncertainty as:\n$FUn = P - p$.\nFalse uncertainty similarly captures subjective fluctuations,\nbut preserves the direction of this subjectivity, distinguish-\ning between positive (over-confident) or negative (under-\nconfident) forms of uncertainty. Quantifying false uncer-\ntainty will be the primary motivation for our task design.\nWhile works have focused on improving the quality of a\nforecast (the Brier score), ours is first to propose quantifi-\ncation of other interlocutors' uncertainty."}, {"title": "New Uncertainty Quantification Tasks", "content": "As we are (uniquely) interested in humans as conversation\nforecasters, probability annotations are not necessarily the\nmost effective way to elicit uncertainty or intensity of belief.\nIndeed, most of the corpora we study (\u00a7 3.3) annotates be-\nlief intensity on a Likert scale; e.g., \u201con a scale from 1 to\n10, how much do you think Speaker B likes you.\" We focus\non probability estimates because these can be compared to\nground-truth world states; i.e., whether B actually \"likes,\u201d\nto enforce non-merging (\u00a7 2.2). Without \"the world\" or \"re-\nality\" as reference, we have no way to define subjective, or\nfalse, uncertainty. Thus, we map human expressions to prob-\nability estimates to enable comparison.\nCalibration Strategy: \"More Than Chance\" Mapping\nverbal or quasi-continuous expressions of belief uncertainty\nto probabilities is a calibration problem; e.g., it has been\napproached for language models using scaling (Tian et al.\n2023). In this work, we enable calibration by making a\nslight semantic change to the outcome of interest. Instead\nof studying \"whether Speaker B likes Speaker A\" we study\n\"whether Speaker B likes Speaker A more than would oc-\ncur by chance.\" This alteration ties belief intensity annota-\ntions to a ground-truth outcome that is observable in data.\nPrecisely, following the colloquial meaning of \"more than\nchance\" in the statistics literature, the ground-truth proba-\nbility is defined by a p-value for the magnitude of the be-\nlief, computed from the data. In turn, appending \"more than\nchance\" defines both ground-truth outcome probabilities and\nan appropriate calibration function for human expressions of\nintensity. We provide details in \u00a7 A.1."}, {"title": "Uncertainty Quantification (UQ) Tasks", "content": "1st-Order ToM Uncertainty (1TUQ) To quantify false\nuncertainty, one first needs to quantify an interlocutor's base\nuncertainty about their belief (i.e., the forecast $P$). Aptly,\nour first task evaluates a language model's ability to quan-\ntify the base uncertainty of others. For instance, suppose an\ninterlocutor A expresses their uncertainty about \"whether A\nis happy\" and this is calibrated to a probability forecast $P$.1\nThe language model's task is to make a prediction $P$ about\nthe uncertainty of A's belief. We evaluate this prediction us-\ning regression metrics; e.g., the correlation between $P$ and\n$P$, the absolute error, and the explained variance.\n2nd-Order ToM Uncertainty (2TUQ) Besides their own\nbeliefs, interlocutors also hold uncertainty about the beliefs\nof others. For instance, an interlocutor A can express their\nuncertainty about \"whether interlocutor C likes A\". Then,\nTUQ tasks the language model with quantifying the uncer-\ntainty of A about C's belief. Similar to the first-order task,\nwe evaluate a language model's prediction by comparing it\nto A's true uncertainty.\nFalse Uncertainty (FUnQ) Finally, we ask language\nmodels to directly quantify an interlocutor's false uncer-\ntainty. In essence, this requires them to quantify both the in-\nterlocutor's uncertainty about a belief as well as the ground-\ntruth probability that the belief is true (the outcome proba-\nbility $p$). For instance, $P$ may be a forecast about \u201cwhether\ninterlocutor C likes A\u201d and $p$ may be the ground-truth prob-\nability that \"C likes A.\u201d The language model is tasked with\nquantifying $FUn = P p$, and we evaluate this estimation\nusing regression metrics."}, {"title": "Corpora and Basic Prompts", "content": "CaSiNo is a corpus of negotiations about camp-resource\nallocation (Chawla et al. 2021). Interlocutors barter over\navailable resources, such as fire-wood and water, based on\n(assigned) resource preferences. Performance-based mone-\ntary incentives stimulate competitive behaviors. Interlocu-\ntors indicate their satisfaction with the final deal on a 5-point\nscale. For an interlocutor A, we ask language models to pre-\ndict \"how certain A is that they are more satisfied than would\noccur by chance.\" Precise details are in \u00a7 A.2. This formu-\nlation allows us to evaluate language models for 1st Order\nToM uncertainty quantification (1TUQ). The average num-\nber of tokens in a conversation is 320.\nCANDOR is a corpus of spoken conversations between\nstrangers, conducted over video communication platform\n(Reece et al. 2023). Conversations are social in nature with\nminimum time constraints and an assigned goal of \"getting\nto know each other.\" Exit interviews (conducted privately)\nask interlocutors to quantify how much they like each other\non a 7-point scale, as well as how much they think their con-\nversation partner likes them. For two interlocutors A and B,\nwe ask language models to predict \"how certain is B that\nthey like A more than would occur by chance.\" As with\nCaSiNo, this lets us evaluate language models at the first-\norder task (1TUQ). Because of the available data, we also"}, {"title": "Methods", "content": "4.1 Forecasting the Uncertainty of Beliefs\nDirect Forecasting (DF, Sicilia et al. 2024) is a good \"out-of-\nthe-box\" method for uncertainty-aware conversation fore-\ncasting with language models. Adapted to our belief antici-\npation problem, we prompt the language model to express its\npredicted uncertainty for the interlocutor on a 10-point scale.\nWe parse the prediction directly from the model's sampled\ncompletion and divide by 10 to get an estimate P for the in-\nterlocutor's true forecast P. In general, we use the Chain of\nThought (CoT) strategy proposed by Kojima et al. (2022),\nasking the model to approach the prediction \"step-by-step.\"\nPost-Hoc Scaling Post-hoc scaling (calibration) tends to\nimprove direct forecasts (Tian et al. 2023; Sicilia et al.\n2024), requiring only a small amount of data. Notably, our\nToM tasks work with continuous uncertainty annotations in\nplace of traditional, discrete outcome annotations. We pro-\npose new scaling methods to accommodate our data.\nPlatt Scaling (PS) One option is to assume the relation-\nship between the true uncertainty P and the predicted un-\ncertainty P is linear in the logits; e.g., this is common in\nsoft classification (Platt et al. 1999). In our new setting,\n$logit(P) \u2248 a logit(P) + \u03b2$.\nThe new (re-scaled) forecast is:\n$P_{ps} = expit (a logit(P) + \u03b2)$\nwhere \u03b1, \u03b2 are the OLS estimates of Eq. (4).\nLinear Scaling (LS) We also suggest linear scaling,\nwhich instead learns a direct linear map:\n$P_{LS} = clip(a \u00b7 P + \u03b2,0,1)$.\nFine-Tuning a Regression Head Fine-tuning a classi-\nfication head on a language model's latent features can\nhelp forecasting performance in soft classification (Kada-\nvath et al. 2022). Again, since our annotations are contin-\nuous, we slightly modify this, replacing the classification"}, {"title": "Experiments", "content": "5.1 Setup\nData & Seeds We use the 3 datasets/prompting schemes\ndiscussed in \u00a7 3.3. More details on prompts are in \u00a7 A.2. We\nuse 5 different random seeds to create 5 distinct train/test\nsplits. For training, n = 100 unless otherwise noted.\nModels Direct forecasting (DF) is conducted with\nLlama3 8B and 70B (AI@Meta 2024), Mixtral 8x7B\nand x22B (v0.1 Jiang et al. 2024), Gemma 7B (Team\net al. 2024), GPT 3.5 (turbo-0125, OpenAI) and GPT-40\n(2024-05-13, OpenAI). All models are instruction-tuned\n(chat) versions. We use default sampling parameters, given\non the API or model repository. For fine-tuning (FT), we\nuse latent representations from a pre-trained masked lan-\nguage model, specifically fine-tuned for long-context em-\nbedding (M2-BERT, Fu et al. 2024), which regularly beats\nmuch larger models at embedding tasks (Fu, Arora, and R\u00e9\n2023). We use Together AI and Open AI APIs for inference.\nMetrics We report standard regression metrics including\nthe Pearson (linear) correlation R, the Spearman (rank) cor-"}, {"title": "Results & Analysis", "content": "We structure our results using a research question (RQ) /\nanswer (A) format, with detailed discussion following each.\nRQ1: Can language model's predict the uncertainty\nof other interlocutors in a conversation?\nA: No. Inference \u201cout-of-the-box\" is poor, but some\npost-hoc scaling methods enable better prediction.\nComparison of Scaling Methods Table 1 reports regres-\nsion metrics for first-order ToM UQ (1TUQ) split accord-\ning to different methods of inference. While direct forecasts\nare ineffective \"out-of-the-box,\" linear scaling (DF LS) with\n100 data points can improve scores to a positive explained\nvariance, on average. These results suggest a consistent (if\nslight) linear relationship between the language model's in-\nferences and the interlocutors' true uncertainty. Explained\nvariance sometimes exceeds 7%, or with more data, 12%.\nContrary to conventional wisdom (using soft classifiers to\nforecast outcomes), a logit-linear relationship between the\nmodel's inferences and it's target seems unlikely, due to the\npoor performance of DF PS.\nRQ2: Does variance reduction via bagging improve\nlanguage model capability at our regression task?\nA: Yes. Random forests trained on language model\nembeddings show promise. The proposed Bag of\nThoughts (BoT) strategy also improves inference.\nVariance Reduction Strategies Use of bagging in fine-\ntuning (i.e., via random forests) did improve performance\nas anticipated, compared to other tuning strategies. We re-\ncall, bagging is a known variance reduction strategy, which\ncan ultimately reduce errors by this mechanism. Another\nvariance reduction strategy we propose is Bag of Thoughts\n(BoT). Table 2 reports ablation study of BoT for first-order\nTUQ, limited to CANDOR and CaSiNo. Ablation is also re-\nported for second-order TUQ, limited to CANDOR, in Ta-\nble 4. Findings show BoT has positive impact on small mod-\nels on average, with particular models/setups seeing sub-\nstantial gain (2% bump for Gemma 7B on 1TUQ, more for\nGPT 3.5 on 2TUQ). Performance is amplified more so in\nTable 3 (includes MultiWOZ). Averaged across all 1TUQ\ndata, BoT allows small models to surpass some large models\n(particularly, Llama3 70B). We did not try BoT for large\nmodels, as their lower throughput (tokens/second) made re-\npeated sampling time consuming. Comparison between BoT"}, {"title": "Use of Demographic Context", "content": "RQ3: Can interlocutor demographic information be\nused by an LM to improve ToM UQ?\nA: Yes, depending on model size. Larger models show\nmore consistent ability to use demographic context.\nIn Table 2 we ablate the\nrole of including demographic data in the prompt (DEM),\nlimited to first-order TUQ on CANDOR and CaSiNo. With\nor without BoT, adding demographics tends to hurt perfor-\nmance of small models (0.7% and 0.4% drop in average\nR2, respectively). Meanwhile, the scaled inferences of larger\nmodels are all improved by including demographics. In sim-\nilar ablation for second-order TUQ (Table 4), we did find\nless conclusive evidence of a distinction between smaller\nand larger models use of demographic context. Without BoT,\ndemographics seem to help both small/large models, but\nwith BoT smaller models show mixed responses.\nDemographics and Bias Our initial hypothesis was the\nbias reduction was the principle mechanism by which de-\nmographic context could reduce error. This is consistent (for\nlarge models, 1TUQ) with observed reduction in bias af-\nter including demographics (-0.1%). The limited effect size\ndoes suggest potential for other factors. For instance, similar"}, {"title": "Data Comparison", "content": "RQ4: What factors of conversation context and ToM\ntask impact LM inferences about uncertainty?\nA: Longer conversations are more difficult for LMs.\nHumans may also hide goals in some contexts (e.g.,\nnegotiation), compounding difficulty. LMs also show\ndifficulty when shifting perspectives.\nTable 3 reports explained variance for\n1TUQ for DF LS, split by model and dataset. We observe\nCANDOR to be the most difficult dataset for 1TUQ, fol-\nlowed by CaSiNo, then MultiWOZ. One hypothesis for the\ndifficulty of CANDOR is the length of it's conversations,\nwhich average more than 11K tokens (GPT-2 tokenizer).\nThis may also be compounded because dialogue is between\nstrangers. Small, but important, nuances can become domi-\nnated other perhaps, superficially polite interactions. The\nreality that humans can hide their true mental states may\nalso explain increased difficulty in CaSiNo, a negotiation\ncorpus. Rather than \"acting\" to be polite, interlocutors in\nthe CaSiNo corpus hide motives and intentions, as a strat-\nergy, to receive a better deal. In contrast, in the collaborative\nand task-oriented MultiWOZ corpus, interlocutors have in-\ncentive to reveal many aspects of their mental state; e.g., to\nindicate satisfactory constraints for their booking task.\nTask Comparison"}, {"title": "Final Thoughts", "content": "Our methods are effective for (at least\nsome models) on both first-order and second-order TUQ.\nWe refrain from commenting on the differences in difficulty\nbetween these, since it varies widely depending on model.\nThe task that stood our was False Uncertainty Quantifica-\ntion (FUnQ), which has results split by method in Table 5.\nThe key observation is that, unlike previous tasks, no model\nachieves positive explained variance via direct forecasting,\neven after scaling. This is evidenced by the negative max-\nimum R2 value. The best fine-tuning strategies offer some\nimprovement, but performance is still relatively low.\nWhy FUnQ is Hard We hypothesize this difficulty may,\nin part, come because model errors compound across multi-\nple inference steps; e.g., the inference for interlocutor's un-\ncertainty P and the inference for the ground-truth probabil-\nity p. Indeed, one data point in favor of this hypothesis is the\npositive explained variance of the joint fine-tuning proce-\ndure (FT RF-J), which conducts non-linear inference over\nthe embedding of both prompts (i.e., to infer P and \u00ee\u00ea\u00ee) and\nthen produces a single estimate for the difference P p."}, {"title": "Humans", "content": "RQ5: How do humans perform at ToM UQ tasks?\nA: Slightly better than LMs (scaling is still needed).\nPerformance Because of available annotations\n(\u00a7 3.3), we infer human performance at first-order To\u041c\nUQ on CANDOR. Interestingly, linear scaling also im-\nproves the performance for human forecasts, which may be\nsuggestive of individual baselines for how people express\ntheir uncertainty (or, intensity) about beliefs. Human perfor-\nmance is not drastically higher than models (R2 = 2.7%,\nMAE=17.7), which is again suggestive of the difficulty of\nthis corpus (recall, our data comparison).\nRQ6: Can uncertainty estimates improve LM infer-\nence at routine ToM (i.e., belief classification)?\nA: Yes. In \u00a7 A.5, we conduct a case-study with Llama\n3 and CaSiNo. Using uncertainty estimates to infer\nbeliefs improves accuracy and F1 score."}, {"title": "Conclusions", "content": "This paper details tasks and methods to explore how lan-\nguage models represent the uncertainty of other interlocu-\ntors in conversations. We make connections between this\ntask and Theory of Mind, suggesting a continuous analog of\nfalse beliefs (we call, false uncertainty) and asking models\nto quantify it. High-level findings are summarized below:\nlanguage models can explain variance in others' uncer-\ntainty on some corpora (up to 7%), but humans may hide\nmental states in social dialogue and negotiations;\nhumans themselves can have trouble with this task (in\nsocial dialogue) and appear to have different baselines\nfor the intensity of their beliefs;\nvariance and bias reduction are important mechanisms\nfor tailoring language models to regression tasks;\nperspective shift in uncertainty modeling is difficult, as\nlanguage models experience compounding errors that\nprevent accurate prediction of false uncertainty.\nWe also highlight some areas of future work.\nDialogue Policy and Generation A motivation of our\nwork is machine dialogue, as recognizing false uncertainty\ncan inform dialogue acts, like explanation, as well as how to\nexecute such an act. Prior work uses ToM to directly inform\ndialogue policies and generation (Zhou et al. 2023), and our\nTOM UQ methods may be fruitful in such contexts.\nCommunication Theories Accurate and automated quan-\ntification of uncertainty in interlocutors has the potential\nto scale up studies of uncertainty in communication, and\ncommunication theories founded on uncertainty (Berger and\nCalabrese 1974; Sunnafrank 1986) to large corpora. For in-\nstance, in data driven studies of these theories, participant\nannotation of uncertainty is needed (Yoo 2009), but may be\nrelaxed with suitable automated methods. The role of uncer-\ntainty in alternate communicative theories, such as ground-\ning (Clark and Brennan 1991) is also of interest.\nDemographics and Uncertainty"}, {"title": "A Appendix", "content": "A.1 Extreme Value Uncertainty\nGiven an annotation m about an interlocutor A's magnitude\nof belief, we consider the forecasting problem with outcome\ny = \"whether A's magnitude of belief is more extreme than\nwould be observed by chance.\" Then, in the context of the\nfull dataset, the magnitude annotation m implicitly defines\nthe ground-truth probability of our outcome:\n$p = P{m > M}$\nwhere M is sampled from all dialogue annotations, and thus,\np can be computed from data. A separate agent (e.g., an-\nother interlocutor B) can annotate their own perception m'\nof A's belief, which is an expression of uncertainty/intensity\nof their belief about A's belief. This can then be calibrated to\na probability estimate about the outcome y, using the same\nformula:\n$P = P{m' > M}$.\nThe important qualities of this outcome formulation are that:\n(a) it implicitly defines both uncertainty annotations and\n(ground-truth) calibration functions, which are not available\nin typical forecasting problems; and (b) it is general, since\nit implicitly models certainty about any belief for which we\nwe have magnitude annotations.\n\u201c\nThe semantics of the outcome are, in fact, not much differ-\nent than a more typical decision \"whether A believes ...\u201d. In-\nstead asking a question about relativity of belief, to induce\nthe (implicit) certainty annotations from those (magnitude\nannotations) that already exist. As a caveat, this outcome\nformat does not work for calibrating uncertainty/intensity of\nbeliefs about many types of \u201cfuture events\u201d; e.g., whether\na deal will occur. In these contexts, the human expression\nmay need to be calibrated with data in order to map human\nexpressions of belief intensity to the same scale as ground-\ntruth outcome probabilities for comparison."}, {"title": "Case Study: Does Considering Uncertainty Improve ToM Predictions Outright", "content": "A.5 Throughout the paper, we have argued for the importance\nof estimating the uncertainty in others' beliefs, pointing to\nsubstantial existing literature as well as a few motivating ex-\namples. Here, we show how reasoning about others' uncer-\ntainty can even help language models to improve their accu-\nracy at a traditional ToM task (i.e., an existing belief predic-\ntion task). Specifically, we use a belief prediction task built"}]}