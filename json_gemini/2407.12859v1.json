{"title": "Automated Question Generation on Tabular Data for Conversational Data Exploration", "authors": ["Ritwik Chaudhuri", "Rajmohan C", "Kirushikesh DB", "Arvind Agarwal"], "abstract": "Exploratory data analysis (EDA) is an essential step for analyzing a dataset to derive insights. Several EDA techniques have been explored in the literature. Many of them leverage visualizations through various plots. But it is not easy to interpret them for a non-technical user, and producing appropriate visualizations is also tough when there are a large number of columns. Few other works provide a view of some interesting slices of data but it is still difficult for the user to draw relevant insights from them. Of late, conversational data exploration is gaining a lot of traction among non-technical users. It helps the user to explore the dataset without having deep technical knowledge about the data. Towards this, we propose a system that recommends interesting questions in natural language based on relevant slices of a dataset in a conversational setting. Specifically, given a dataset, we pick a select set of interesting columns and identify interesting slices of such columns and column combinations based on few interestingness measures. We use our own fine-tuned variation of a pre-trained language model(T5) to generate natural language questions in a specific manner. We then slot-fill values in the generated questions and rank them for recommendations. We show the utility of our proposed system in a coversational setting with a collection of real datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Exploratory data analysis (EDA) is a critical step for understanding a structured dataset. EDA usually comprises of a set of visualizations, providing a compact view of a subset of data and measures like cen-tral tendencies (mean, median, standard deviation etc.) of various columns or a combination of columns in the dataset. Non-technical users may find it challenging to grasp insights from visualizations like histograms, pie charts etc. as these plots might not immediately highlight the key insights. Also, visualizations on more than three columns/attributes are difficult to represent using plots and it is even more difficult to consume it. Hence, visualizations are not enough to summarize the insights from a dataset thoroughly often. Even for technical users, understanding which columns to select next for the EDA process can be difficult, leading to time-consuming analysis due to the vast number of possibilities in the dataset. There are few tools for EDA in some automated fashion like Microsoft Power BI[14], Pandas Profiling[24] or Tableau[21]. They provide an overview of data through plots but have limitations as they don't perform analysis on different slices of the data where relevant in-sights might reside for a user. Also, user intent is hardly considered in the automated analysis of the data. For example, consider the dataset in Table 1, such tools may provide a histogram of the salary but that will not be sufficient to capture the insight: \"The average salary of employees residing in New York and older than 35 years is $235000\". This insight is significant because the average salary of employees more than the age of 35 is significantly more than that of the employees who are younger than 35 years. Consider another sample question - what is the average salary of employees in New York? The rationale behind considering such a question is aver-age salary in New York is significantly more than that of Columbus. Such insights are possible when analysis is done on relevant and interesting slices of data.\nRecently, data exploration via a conversational interface is get-ting some traction. But there is limited work on automating the data exploration tasks in a conversational setting. In this paper, we propose a system and related methods for data exploration"}, {"title": "2 LITERATURE REVIEW", "content": "Data exploration and analysis is a challenging and time-consuming task hence there have been a lot of work in automating and im-proving the processes. There are many tools like Tableau [21] and Microsoft Power BI[14] that aid in exploratory data analysis by producing various visualization plots of the data. There have been various recommender systems proposed for EDA that are aimed at assisting users in choosing the next best exploratory step to perform or suggesting dataset slices that are likely to be of interest. To that end, various interestingness measures have been proposed in the literature.\nData-driven EDA systems define a notion of interestingness and use it to rank EDA operations and the utility of their results. Some early works like [10, 22] have looked at data cubes, roll-ups, drill-downs etc. in traditional databases and OLAP settings. Some works[26, 28] have looked at it from a visualization perspec-tive. Specifically, [26] talks about visualizations that demonstrate a larger deviation from the original dataset as interesting ones. Some systems[1, 6, 25] utilize logs from past exploration sessions or of the current session itself to generate recommendations for exploratory operations to be performed next. [16] proposed a hybrid approach combining data-driven and log-based approaches.\nThere have also been some machine learning-based approaches for modelling user interests in order to recommend EDA operations. [16] considers a set of interestingness measures, and formulates a multi-class classification problem for selecting interestingness measure dynamically that best captures user interests in every step of an ongoing EDA session. [9] proposes an active learning approach to get feedback on whether the presented tuples are inter-esting and use it to model user's interest incrementally. [13] utilizes user-annotated data visualizations for training and builds a ranking model that is able to assess the quality of data visualizations and decide, given two data visualizations, which one is more interesting. [2, 15] employs deep reinforcement learning techniques to auto-generate exploratory sessions given a dataset as input which can then be presented to users in some form like a python notebook.\nConversation data exploration over datasets is a fairly recent phenomenon. There have been some early works from the research community in the related area of natural language interfaces to interact with databases. For example, [12] takes natural language sentences from the user and generates a query in a technical lan-guage like SQL, presenting its usability and limitations. More re-cently, [4] presents a no-code platform for binding conversational flows to relational data sources visually. [20] presents an interactive paradigm for the rapid prototyping of chatbots for data exploration. [5] presents an approach for generating chatbots to query Open Data sources published as web APIs.\n[3] presents a data-driven design paradigm for building conver-sational interfaces for data exploration. It exploits the properties of data models and proposes schema annotations to enable the gener-ation of conversation paths for the exploration of database tables. [7] presents a conversational approach for querying multidimen-sional data, that captures users' intentions and links them to the database schema metadata using a bot development framework. It relies on database schema vocabulary for detecting user intentions and establishing parameters for query execution.\nQuestion generation on tables aims to generate questions from given tabular and associated textual data. Initial approaches have been largely based on syntax rules or templates. Later, supervised neural models have been tried out for the same. These supervised methods generally require large amounts of human-written ques-tions for training. While many works focused on fact-based single-hop questions [27], there have also been some works which have looked at more complex multi-hop questions [18] as well. After pre-trained language models became more popular, they have been used widely for question generation tasks[23]. [19] presents an answer-aware question generation from tables along with text using a transformer-based model. But it is based on the assumption that the answer is known and then fact-based questions are generated."}, {"title": "3 INTERESTING NL QUESTION GENERATION FOR EDA", "content": ""}, {"title": "3.1 Problem Statement", "content": "Given a dataset D with m column headers denoted by \\(C_1, C_2,..., C_m\\), the goal is to generate relevant questions spanning over a subset of columns \\(\\{C_{j_1}, C_{j_2}, ..., C_{j_r} \\}, 1 \u2264 r < m\\) where without loss of generality we assume \\(j_1 < j_2 < ... < j_r\\) and \\(j_i \u2208 \\{C_1, C_2, ..., C_m\\}\\). The questions are generated based on relevant slices from the sub-dataset formed by \\([C_{j_1}: C_{j_2}: : C_{j_r}]\\). After generation and ranking of questions, the system improves relevance of the ques-tions in subsequent iterations based on the user's past interactions."}, {"title": "3.2 The Pipeline Architecture", "content": "Our proposed system comprises of mainly three steps: (1) Perform-ing EDA on user uploaded dataset (2) Generating questions based on the significant results obtained from EDA (3) Feedback provided by the user to the system to fine-tune the questions in the next iterations. This entire pipeline is shown in Figure 1. A user uploads"}, {"title": "4 DETAILED METHODOLOGY", "content": "The entire methodology for generating and ranking of questions given a dataset can be divided into four broad steps (1) Determining interesting columns (2) Determining relevant slices of sub-dataset (3) Question formation and slot-filling (4) Ranking of generated questions. There are certain set of operators that are considered for columns of the dataset. For example, consider the question, \"What fraction of employees has a job title as Software Developer ?\" involving the column job titles. In the question, the operator is fraction which is essentially applied to the proportion of instances for employees with the job title software developer as compared to all the job titles. In this case, a single operator is applied to a single column. Another example question can be, \"What is the average salary of the employees with a salary above $6000 ?\". Here essentially two operators average and above are applied on the numeric column salary. Hence two operators are applied to a single column. In another example, \"What is the average salary of employees belonging to top 4 job titles ?\", involving the columns salary and job titles, the operator average is applied on the numerical column salary and the other operator top K (here K = 4) is applied on the categorical"}, {"title": "4.1 Determine interesting columns", "content": "Consider the dataset D with m columns namely \\(C_1, C_2, ..., C_m\\). On each column \\(C_i\\), \\(1 \u2264 i \u2264 m\\) an interestingness score function \\(f: C_i \u2192 \\mathbb{R}\\) is applied which generates an interestingness score for the column \\(C_i\\). The function f can be any of the popular functions such as Entropy, Unalikeability, Peculiarity, Fisher's score, Chi-squared, and Correlation available in the literature. Then, top K, \\(1 \u2264 K\u2264 m\\) columns, \\(\\{C_1, C_2, ..., C_K\\}\\), are chosen based on the interestingness score. The entire pipeline for determining interesting columns in an input dataset is given in Figure 2. Every subset of the K columns is considered as a sub-dataset and within that sub-dataset the relevant slices are searched for."}, {"title": "4.2 Determine relevant (important) slices within a sub-dataset", "content": "Consider a subset of r columns \\(\\{C_{j_1}, C_{j_2}, ..., C_{j_r} \\}\\), where without loss of generality assume \\(j_1 < j_2 < ... < j_r\\) and \\(j_i \u2208 \\{1, 2, ..., K\\}\\), \\(\\forall i, 1 \u2264 i \u2264 r\\) and the sub-datasets created by the r columns are denoted by \\(D^{\\{C_{j_1}C_{j_2}...C_{j_r}\\}} = [C_{j_1}: C_{j_2}:...: C_{j_r}]\\). Among r columns, one column is fixed on which a centrality measure, such as average, median, mode, standard deviation etc. denoted by f is applied. For example, in the question, \"What is the average salary above age 45 among females?\", the fixed column is salary on which the measure f average is computed. Now, the sub-dataset is sliced based on the values within the remaining r - 1 columns, specifically for this example columns age and gender. Hence for each slice the average salary is compared against the salary of the rest of the sub-dataset without the competing slice and checked if it is significantly different from the salary belonging to rest of the sub-dataset. If the difference is significant then the specific slice is taken as a relevant/important slice within the sub-dataset. The entire pipeline for finding the relevant slices of data is given in Figure 3. As shown in Figure 5 the column Salary is the fixed column and the rest of the columns age, gender are the remaining 2 columns. Based on the combination of values within age and gender the sub-dataset is sliced as shown in Figure 5 bordered with a red box. The data bordered with the green box is the remaining sub-dataset. The choice of the measure f varies based on the column on which it is applied being a numerical column, categorical column or a date-type column.\nIf the fixed column is numerical then the function f can be any measure of central tendency for which various statistical tests such as two-sample tests can be done to check if the sample of the numer-ical column within the slice is significantly different from the rest. If the slice is significantly different from the rest, then it is considered to be one of the relevant slices of the data. Based on the structure of the slice, on r - 1 columns the operators \\(\\{o_1, o_2, ..., o_{r-1}\\}\\) where \\(o_i \u2208 \\{before, after, in, more than, less than,\\}\\) among, within} operators are applied on each of the r 1 columns depending on whether the column is numerical or categorical or a date-type. For the fixed column based on the function f the operator \\(o_r\\) can be average, median, quartiles, quantiles, standard deviation, trend, coefficient of variation etc..\nIf the fixed column is categorical then the function f can be a ma-jority or minority which essentially determines the class of the fixed column within a slice. The slice is created based on the combination of values in the rest of the r - 1 columns. The function f can also be fraction of elements within a class. Based on the slice, as stated earlier the operators \\(\\{o_1, o_2, ..., o_{r\u22121}\\}\\) applied on each of the r - 1 columns can be before, after, within, more than, less than, among. The opera-tor \\(o_r\\) on the fixed column can be a majority, minority, fraction etc. Once the relevant slice \\(D^{\\{C_{j_1}C_{j_2}...C_{j_r}\\}}\\) is de-termined on the sub-dataset formed by columns \\(\\{C_{j_1}, C_{j_2}, \u2026\u2026\u2026, C_{j_r} \\}\\) and operators \\(\\{o_1, o_2, ..., o_r\\}\\), Step 4.3 is executed next."}, {"title": "4.3 Question formation and slot filling", "content": "In this step, we generate a question based on the slice of the data \\(D^{\\{C_{j_1}C_{j_2}...C_{j_r}\\}} \\), columns \\(\\{C_{j_1}, C_{j_2}, ..., C_{j_r} \\}\\) and operators \\(\\{o_1, o_2, ..., o_r\\}\\). This step however is divided into two stages (a) Question formation based on the column headers within the sub-dataset. (b) Slot-filling of generated questions with numbers."}, {"title": "4.3.1 Question formation based on the column headers within the sub-dataset.", "content": "We use pre-trained text-to-text models such as T5-base for the questions generation. T5 \u2013 base is a popular pretrained language model. It can be fine-tuned for a specific task. In fact, for aggregated question generation since there is no training dataset available we hand-curated a training corpus comprised of multiple datasets with (i) a title of the dataset if there is any (ii) a short sentence mentioning the names of the columns contained within the dataset (iii) a dictionary which maps the name of the column header to operator applied on it. The corresponding output was a question generated based on the names of column headers within the sub-dataset. The I/O structure is shown in Figure 4. The system learns how to use numerical operators such as average, median, mode, standard deviation, maximum, minimum etc. on the numer-ical columns and categorical operators such as most, least etc. on categorical columns. Also, operators such as within, from-to etc. are applied on date-type columns. It is not mandatory to use T5 as the only model since our methodology can easily be extended to other pre-trained text-to-text models. Let the tuned text-to-text model through few-shot learning be denoted by M. The input pa-rameters for the model M are as shown in Figure 4. For example, if the following inputs (i) Title: A dataset with age, gender, location and salary of employees (ii) Description: The dataset contains the age of employees, gender of employees, location of employees, the salary of employees (iii) Dictionary: age: above, female: among, location:"}, {"title": "4.3.2 Slot-filling of generated questions with numbers.", "content": "The slot-filling method described here is one of the many techniques that can be used for slot-filling.\nFrom stage 1 described in Subsection 4.3.1, let the generated question be denoted by q with multiple blanks which are to be slot-filled. Based on the relevant slice of data denoted by \\(D^{\\{C_{j_1}C_{j_2}...C_{j_r}\\}} \\) we obtain range, maximum and minimum as potential slot-filler values of each numerical column. For each categorical column the categories within the slice \\(D^{\\{C_{j_1}C_{j_2}...C_{j_r}\\}}\\) are chosen as potential"}, {"title": "4.4 Ranking of generated questions", "content": "Let us assume from Step 3, given in Subsection 4.3, the set of all questions generated are Q = \\(\\{q_1, q_2,..., q_n\\}\\). For the question \\(q_i\\), \\(1 \u2264 i \u2264 n\\) the corresponding interesting score is denoted by \\(s_i\\). In the initial iteration, the questions are ranked in the decreasing order of \\(s_i\\). However, at the beginning of iteration 2 we assume the user is equally likely to choose questions with any of the columns \\(C_i\\), \\(1 \u2264 i \u2264 K\\). Hence we keep a counter denoted by \\(T_i\\) initialized at 1 to denote the number of times a column appeared within the ques-tions selected by the user. Whenever the user chooses a question with the column \\(C_i\\) then \\(T_i\\) is increased by 1. The probability for choosing column \\(C_i\\) is \\(p_i = \\frac{T_i}{\\sum_{j=1}^{K} T_j}\\). Starting from Iteration 2, for each question with columns \\(C_{j_1}, C_{j_2}, ..., C_{j_r}\\), the probability of the columns appearing in those questions are computed as \\(\\Pi_{i=1}^{r} p_{j_i}\\)."}, {"title": "5 SYSTEM IMPLEMENTATION", "content": ""}, {"title": "5.1 Dataset", "content": "We picked a set of Kaggle [11] datasets primarily belonging to Fi-nance, Retail and Education domains then created a bunch of in-teresting aggregate questions for each dataset manually. With that we prepared a new dataset called agg-questions dataset that has the basic metadata of the input dataset columns along with our manually curated aggregate questions.\nThe metadata considered includes\n\u2022 table name\n\u2022 table description\n\u2022 column name(s)\n\u2022 column type(s)\n\u2022 operators\nFigure 7 shows operators used in the training data for the three categories of columns that we consider namely numerical, categori-cal and date columns. In addition, Trend, and Seasonality operators are also considered. Please note that the terms dataset and table are used interchangeably in this section to convey the same thing."}, {"title": "5.2 Model", "content": "We have used the T5 pre-trained language model(t5-base) from Huggingface model repository and fine-tuned it on this custom dataset(agg-questions) as explained in Section 4. We have experi-mented with this fine-tuned model as a part of the whole system and the results obtained are discussed in the next section."}, {"title": "5.3 Output", "content": "Figure 8(top) shows a snapshot of a table called Customer_details that is given as input to the system. The top 3 questions generated by the system are shown just below it. These questions are framed on some important attributes of the table and subsequent slot-filling ensures important slices of the data are captured by these questions. Figure 8(bottom) shows a snapshot of Employee_offboarding ta-ble and at the bottom of the table, some sample questions generated by the system are shown. The column selections and slot-fillings are done based on the methodology explained in section 4.\nIn Table 2, some of the generated questions by the system on a set of Kaggle datasets from diverse domains are shown. Next we demonstrate a user interaction with the system for exploratory data analysis."}, {"title": "5.4 Exploratory analysis using the system", "content": "The user interaction with the system begins when the user uploads a dataset for analysis. Figure 9a is the introduction screen of the Question Generation Toolkit which the user finds when the appli-cation is launched. The user can see the catalog on the left pane with all the datasets that are within the catalog. As shown in Figure 9a the user may upload a new dataset or the user can continue with one of the previously saved sessions. If the user chooses the option to upload a dataset then as shown in Figure 9b user needs to choose between two more options, (i) upload data from a local machine or (ii) upload data from Catalog. In case the user chooses the option"}, {"title": "6 DISCUSSION", "content": "Although the system generates meaningful and valid questions in most cases, sometimes the question generated can be invalid. To eliminate such questions from the ranking process and subsequent recommendations, we have used GPT-2 [8] language model right after the generation process to verify the language correctness of questions. The questions generated by the system without any user feedback largely rely on the interestingness measure used and the ranking of these questions can be inaccurate but as the iterative exploration proceeds, the search space reduces leading to more relevant questions for the user.\nWe have performed an user evaluation study to understand the system's performance comprehensively. The questions generated were evaluated by three experts on three different criterias namely Fluency, Faithfulness, and Interestingness. We define them as fol-lows.\nFluency: The question should be coherent without any gram-matical errors.\nFaithfulness: The question should be answerable using the information from the input table only. This ensures that the question is grounded on the input table.\nInterestingness: The question should be about an interesting insight on the dataset. It captures how interesting is the generated question(inherently with answer).\nEach question was scored from 1 to 5 with 1 being the worst rating and 5 being the best for each criteria, and the final score is an average across all the expert evaluators as shown in Table 3. Though this evaluation study is very limited, it shows the utility of the system to some extent. It motivates us to think about a promising direction of research on how to evaluate generated questions in a better way. Also We can enhance the system by considering more complex questions with a larger number of columns and operators. The choice of T5 language model is not a strict requirement as any other similar language model can be used too."}, {"title": "7 CONCLUSION", "content": "Conversational data exploration is gaining a lot of traction in the exploratory data analysis space of late. But there is limited work on automating the data exploration tasks. In this paper, we have taken a step towards building an end-to-end system for recommending interesting and relevant aggregate questions during conversational data exploration sessions. Firstly, we have created a custom dataset of interesting aggregate questions manually using a set of Kaggle datasets. We then used it for fine-tuning a pre-trained language model(T5) for question generation. We considered a select set of interestingness measures from the literature to identify interesting columns and data slices. This information is then used to slot-fill the questions generated by the fine-tuned language model. When the user starts exploration, the system recommendations are largely driven by the dataset but as the exploration proceeds, the user feedback helps to rank the recommendations better. We have ex-perimented with this system on a variety of Kaggle datasets from different domains. The results show that the system generates in-teresting questions which will be very useful during exploratory data analysis. Thus our system uses fine-tuned language models along with interestingness measures and slot-filling to recommend interesting aggregate questions during exploratory analysis of struc-tured datasets. In future, we plan to evaluate the system on more enterprise datasets. We also plan to experiment with other inter-estingness measures and slot-filling approaches to improve the system."}]}