{"title": "a-DPO: ADAPTIVE REWARD MARGIN IS WHAT DIRECT PREFERENCE OPTIMIZATION NEEDS", "authors": ["Junkang Wu", "Xue Wang", "Zhengyi Yang", "Jiancan Wu", "Jinyang Gao", "Bolin Ding", "Xiang Wang", "Rong Jin", "Xiangnan He"], "abstract": "Aligning large language models (LLMs) with human values and intentions is crucial for their utility, honesty, and safety. Reinforcement learning from human feedback (RLHF) is a popular approach to achieve this alignment, but it faces challenges in computational efficiency and training stability. Recent methods like Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO) have proposed offline alternatives to RLHF, simplifying the process by reparameterizing the reward function. However, DPO depends on a potentially suboptimal reference model, and SimPO's assumption of a fixed target reward margin may lead to suboptimal decisions in diverse data settings. In this work, we propose \\(\\alpha\\)-DPO, an adaptive preference optimization algorithm designed to address these limitations by introducing a dynamic reward margin. Specifically, \\(\\alpha\\)-DPO employs an adaptive preference distribution, balancing the policy model and the reference model to achieve personalized reward margins. We provide theoretical guarantees for \\(\\alpha\\)-DPO, demonstrating its effectiveness as a surrogate optimization objective and its ability to balance alignment and diversity through KL divergence control. Empirical evaluations on AlpacaEval 2 and Arena-Hard show that \\(\\alpha\\)-DPO consistently outperforms DPO and SimPO across various model settings, establishing it as a robust approach for fine-tuning LLMs. Our method achieves significant improvements in win rates, highlighting its potential as a powerful tool for LLM alignment. The code is available at https://github.com/junkangwu/alpha-DPO.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning from human feedback is essential for aligning large language models (LLMs) with human values and intentions (Leike et al., 2018), ensuring they are helpful, honest, and harmless (Askell et al., 2021). Reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Stiennon et al., 2020) is a widely used method for fine-tuning LLMs to achieve this goal. However, RLHF faces challenges, particularly in computational efficiency and training stability due to its multi-stage process. Recently, alternative offline algorithms like DPO (Rafailov et al., 2023) and SimPO (Meng et al., 2024) have been explored. Specifically, DPO reparameterizes the reward function in RLHF to directly learn a policy model (\\(\\pi_{\\theta}\\)) from preference data, removing the need for an explicit reward model. Building on DPO, SimPO further simplifies the process by eliminating the need for a reference model (\\(\\pi_{ref}\\)), and introduces a target reward margin \\(\\gamma\\) to enlarge the distance between the response pair, thereby achieving strong performance. This naturally raises the question:\n\nDo we really need a reference model in the alignment process?\n\nMotivated by this question, we examine SimPO: it can be viewed as a variant of DPO where the original reference model \\(\\pi_{ref}\\) is effectively replaced by an implicit reference model \\(\\pi_{\\theta}\\). In SimPO, the target reward margin \\(\\gamma\\) actually reflects a constant difference between the log likelihoods of a selected response and a rejected one, i.e., (\\(\\log \\pi_{ref}(y_w|x) - \\log \\pi_{ref}(y_l|x)\\)). As the constant difference"}, {"title": "2 PRELIMINARIES", "content": "Offline Alignment. In the offline alignment problem, we have access to a dataset \\(D = \\{(x, y_w, y_l)\\}\\) comprising prompts x and labeled response pairs (yw, y\u0131) obtained from a reference policy \\(\\pi_{ref}\\). Here, yw is the preferred (winning) response and yr is the less preferred (losing) response. Although the underlying latent reward function r*(x, y) that governs these preferences is not directly observ-"}, {"title": "3 METHOD", "content": "In this section, we establish a unified framework that connects DPO and SimPO (Section 3.1), highlighting the critical role of the reference model in preference optimization. We then introduce \\(\\alpha\\)-DPO (Section 3.2), a novel preference optimization algorithm that synergizes the strengths of both DPO and SimPO."}, {"title": "3.1 A COMMON FRAMEWORK FOR DPO AND SIMPO", "content": "A key insight in our work is that SimPO implicitly adopts a uniform distribution over the vocabulary as its reference model, whereas DPO employs the SFT model as the reference. By examining the role of the reference model in both methods, we derive the following result:\n\nTheorem 3.1. Let U(y|x) denote a uniform distribution over the vocabulary for a given input x, replacing \\(\\pi_{ref}(y|x)\\) in the DPO loss function. Then, the DPO loss function simplifies to:\n\n\\(L(\\pi_{\\theta};U) = -E_{(x,y_w,y_l)\\sim D} [log \\sigma (\\beta (log \\pi_{\\theta}(y_w|x) \u2013 log \\pi_{\\theta}(y_l|x)) - \\gamma)]\\),"}, {"title": "3.2 PROPOSED METHOD: a-DPO", "content": "Our analysis highlights the significant impact of the reference model in preference optimization. To overcome the limitations identified in both DPO and SimPO, we propose the following principles:\n\nPrinciple 1: The reference model should contribute to differentiating between preferred and less preferred responses.\n\nPrinciple 2: The reference model should adapt to discrepancies between response pairs to capture instance-specific nuances.\n\nPrinciple 1 addresses the shortcoming in DPO, where the reference model may inadequately distinguish between yw and y\u0131, introducing uncertainty without a guaranteed margin. Principle 2 rectifies the oversimplification in SimPO, where the absence of a reference model fails to account for variability across different instances.\n\nDeriving the a-DPO Objective. Starting from the standard Reinforcement Learning (RL) objective for preference optimization, we redefine the reference model \\(\\pi_{ref}\\) as an implicit reference model \\(\\hat{\\pi}_{ref}\\), formulated as:\n\n\\(\\hat{\\pi}_{ref}(y|x) = U(y|x) (\\frac{\\pi_{\\theta}(y|x)}{\\pi_{ref}(y|x)})^\\alpha\\),\n\nwhere \\(\\alpha\\) is a hyperparameter controlling the influence of the policy model on the reference model, and U(yx) is a uniform distribution serving as a constant baseline. When \\(\\alpha\\) = 0, \\(\\hat{\\pi}_{ref}\\) reduces to the uniform distribution as in SimPO; when \\(\\alpha\\) = 1, it incorporates the ratio between the policy and reference models as in DPO.\n\nSubstituting \\(\\hat{\\pi}_{ref}\\) into the original DPO loss function, we obtain the \\(\\alpha\\)-DPO objective:\n\n\\(L_{\\alpha-DPO}(\\pi_{\\theta}, \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} [log \\sigma (\\beta (log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{\\theta}(y_l|x)} - \\beta log \\frac{\\hat{\\pi}_{ref}(y_w|x)}{\\hat{\\pi}_{ref}(y_l|x)})]\\)"}, {"title": "4 THEORETICAL ANALYSIS OF a-DPO", "content": "In this section, we provide a theoretical analysis of a-DPO by connecting its objective to the online SimPO loss. We also explore how a-DPO manages the trade-off between alignment and diversity through KL divergence control."}, {"title": "4.1 RELATION OF a-DPO TO ONLINE SIMPO", "content": "In this section, we relate the a-DPO objective to the online SimPO loss and demonstrate how our preference optimization method leads to a generalization bound for the policy model. We consider the following definitions:\n\nDefinition 4.1 (Online SimPO Loss). Let \\(\\pi_{\\theta}\\) represent the current policy and \\(\\pi_{\\theta}^{old}\\) represent the policy from a previous iteration. The online SimPO loss is defined as:"}, {"title": "4.2 BALANCING ALIGNMENT AND DIVERSITY VIA KL DIVERGENCE CONTROL", "content": "Balancing alignment performance with response diversity is crucial in recent alignment methods (Zeng et al., 2024; Wang et al., 2024a; Ji et al., 2024a). A popular approach is the Token-Level Direct Preference Optimization (TDPO) method (Zeng et al., 2024), which introduces fine-grained control of the KL divergence at the token level. Given a prompt x and preceding tokens \\(y_{<t}\\), the policy \\(\\pi_{\\theta}\\) generates the next token z by sampling from \\(\\pi_{\\theta}(z|x, y_{<t}\\)).\n\nBy mapping the reward model to a token-level format, the TDPO loss is defined as:"}, {"title": "5 EXPERIMENTS", "content": "In this section, we present the main results of our experiments, highlighting the superior performance of a-DPO over existing methods on various benchmarks and ablation studies to analyze the impact of different components of a-DPO."}, {"title": "5.1 EXPERIMENTS SETUP", "content": "Models and training settings. We optimize preferences using three model families: Llama3-8B (AI@Meta, 2024), Mistral2-7B (Jiang et al., 2023), and Gemma2-9B (Rivi\u00e8re et al., 2024), all in the Instruct setup. Following Meng et al. (2024), we utilize pre-trained instruction-tuned models (meta-llama/Meta-Llama-3-8B-Instruct, mistralai/Mistral-7B-Instruct-v0.2, google/gemma-2-9b-it) as SFT models. For a fair comparison, we use the same training data as SimPO: princeton-nlp/llama3-ultrafeedback-armorm, princeton-nlp/mistral-instruct-ultrafeedback, and princeton-nlp/gemma2-ultrafeedback-armorm for Llama3-8B, Mistral2-7B, and Gemma2-9B, respectively. Additionally, the v0.2 Llama3-Instruct setup uses RLHFlow/ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024b) as the reward model for ranking generated data, significantly enhancing performance. These configurations represent state-of-the-art methods, positioning our models among the top performers on various leaderboards.\n\nEvaluation benchmarks. We evaluate our models using two widely recognized open-ended instruction-following benchmarks: AlpacaEval 2 (Li et al., 2023) and Arena-Hard (Li et al., 2024). These benchmarks assess the models' conversational abilities across a diverse range of queries and are extensively used by the research community. For AlpacaEval 2, we report the length-controlled"}, {"title": "5.2 MAIN RESULTS", "content": "\\(\\alpha\\)-DPO consistently outperforms existing preference optimization methods. As shown in Table 1, while all preference optimization algorithms improve over the SFT baseline, \\(\\alpha\\)-DPO achieves superior performance compared to existing methods specifically on the AlpacaEval 2 LC metric. These significant improvements highlight the robustness and effectiveness of \\(\\alpha\\)-DPO. Specifically, \\(\\alpha\\)-DPO outperforms the best baseline by an average of 3 percentage points in AlpacaEval 2 LC win rate. Furthermore, on benchmarks such as Arena-Hard, \\(\\alpha\\)-DPO achieves state-of-the-art or second-best results, demonstrating its competitiveness across different evaluation settings."}, {"title": "5.3 KL DIVERGENCE CONTROL IN a-DPO", "content": "Outstanding Performance and Lower KL. As noted in Rafailov et al. (2023); Zeng et al. (2024), it is crucial to consider both performance and KL divergence when comparing algorithms. A slightly higher win rate accompanied by a significantly higher KL divergence is often not desirable. In line with the design principles of TDPO, we implemented SimPO and a-DPO. Figure 3a 3b presents the KL divergence curves. The results indicate that as \\(\\alpha\\) increases, the KL divergence of \\(\\alpha\\)-DPO remains stable or even decreases slightly when compared to SimPO. This demonstrates that \\(\\alpha\\)-DPO not only achieves superior performance but also maintains a lower KL divergence, indicating a better balance between alignment and control of KL divergence during the training process.\n\nMitigating Over-Optimization. Over-optimization, as described by Gao et al. (2023) and Rafailov et al. (2024), refers to a phenomenon where model performance exhibits a hump-shaped pattern across different targets: beyond an optimal point, further increasing the KL budget results in diminishing returns. To investigate this, we evaluate SimPO and \\(\\alpha\\)-DPO at four intermediate checkpoints, corresponding to different KL budgets. As illustrated in Figure 3c, it is intriguing that while the performance of our approach does decrease with increasing KL budget, the decline is relatively modest. This indicates that our method effectively mitigates the issue of over-optimization."}, {"title": "5.4 THE IMPACT OF a IN a-DPO", "content": "Effect of a on Performance. We investigated how the parameter \\(\\alpha\\) in \\(\\alpha\\)-DPO impacts the win rate on AlpacaEval 2 and Arena-Hard. The results, as shown in Figure 4 (a), indicate that the style-control win rate on Arena-Hard initially increases and then decreases with increasing \\(\\alpha\\). In contrast, the length-control win rate on AlpacaEval 2 exhibits a consistently increasing trend. This suggests that the optimal value of \\(\\alpha\\) varies depending on the evaluation benchmarks. Further experiments refer to Appendix B.3.\n\nImpact of a on the reward distribution. We visualize the distribution of the learned reward margin r(x,yw) \u2013 r(x, y\u0131) and the log likelihood of the chosen response \\(log \\pi_{\\theta}(y_w|x)\\) under different \\(\\alpha\\) values in Figure 4 (b,c). Decreasing \\(\\alpha\\) results in a flatter reward margin, while the log likelihood distribution remains relatively unchanged. Conversely, in SimPO (cf. Figure 6), increasing \\(\\gamma\\) yields a flatter reward margin distribution but at the cost of also flattening the log likelihood distribution, which undesirably lowers the log likelihood of positive samples. This indicates that \\(\\alpha\\)-DPO can better balance the relationship between the reward margin and log likelihood."}, {"title": "6 RELATED WORK", "content": "Reinforcement learning from human feedback. RLHF is a technique that aligns large language models with human preferences and values (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022; Azar et al., 2023). Traditional RLHF can be divided into three stages: supervised fine-tuning (Zhou et al., 2023; Taori et al., 2023; Geng et al., 2023; Conover et al., 2023; K\u00f6pf et al., 2023; Ding et al., 2023), reward modeling (Gao et al., 2023; Luo et al., 2023; Chen et al., 2024b; Lightman et al., 2023; Havrilla et al., 2024; Lambert et al., 2024), and policy optimization (Schulman et al., 2017; Anthony et al., 2017). In the third stage, Proximal Policy Optimization (PPO) is a widely used algorithm. Additionally, Xiong et al. (2023) proposed efficient algorithms for the reverse-KL regularized contextual bandit framework in RLHF. Ye et al. (2024) introduced provably efficient algorithms for KL-regularized Nash-Learning from Human Feedback (NLHF). Furthermore, Ji et al. (2024b) developed an active-query-based PPO algorithm with specific regret bounds and query complexity.\n\nOffline direct preference optimization. Several alternative preference optimization objectives have been proposed in addition to DPO (Rafailov et al., 2023). IPO (Azar et al., 2023) addresses the overfitting issues associated with DPO. ORPO (Hong et al., 2024) and SimPO (Meng et al., 2024) aim to eliminate the dependence on a reference model. R-DPO (Park et al., 2024) focuses on mitigating exploitation based on sequence length. KTO (Ethayarajh et al., 2024) deals with preference optimization when data are not pairwise. CPO (Xu et al., 2024) and \\(\\beta\\)-DPO(Wu et al., 2024) emphasize the quality of preference data. Another line of research explores comparisons among more than two instances (Dong et al., 2023; Liu et al., 2024; Song et al., 2024; Yuan et al., 2023).\n\nOnline direct preference optimization. Offline direct preference optimization methods are simple but rely on preference data collected offline. RLHF methods interact online with the language model being aligned but require policy gradients. In contrast, online direct preference optimization methods combine the advantages of both approaches. Yuan et al. (2024) proposed a \"self-rewarding\" approach in which the policy being aligned provides online feedback to itself. Alternatively, OAIF (Guo et al., 2024) is a novel online preference optimization method that can leverage feedback from any LLM, including those stronger than the LLM being aligned. Swamy et al. (2024) also concurrently investigate the importance of online preference but still rely on reward models (RMS). SELMA (Zhang et al., 2024) improves exploration efficiency by selectively favoring responses with high potential rewards rather than indiscriminately sampling unseen responses."}, {"title": "7 DISCUSSION", "content": "Conclusion. We proposed \\(\\alpha\\)-DPO, an adaptive preference optimization method that improves LLM alignment by introducing a dynamic reward margin based on instance-specific differences. \\(\\alpha\\)-DPO addresses limitations in previous methods like DPO and SimPO by balancing alignment and diversity through KL divergence control. Our theoretical guarantees and empirical results show that \\(\\alpha\\)-DPO consistently outperforms baselines on benchmarks like AlpacaEval 2 and Arena-Hard, with significant improvements in win rates, establishing it as a robust solution for LLM fine-tuning.\n\nLimitations and Future Work. While \\(\\alpha\\)-DPO enhances performance, it introduces an additional hyperparameter, \\(\\alpha\\), requiring manual tuning. Future work could focus on developing an adaptive approach to automatically adjust this parameter. Additionally, although we show \\(\\alpha\\)-DPO's theoretical equivalence to online methods, it remains an offline approach. Extending it to online learning would allow real-time adaptation, broadening its application in interactive environments. Lastly, we observed that different benchmarks, such as AlpacaEval 2 and Arena-Hard, require distinct parameter settings for optimal performance. Investigating a more generalized approach that adapts effectively across multiple benchmarks would further improve the model's versatility."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 PROOF OF THEOREM 3.1", "content": "Theorem 3.1. Let U(y|x) denote a uniform distribution over the vocabulary for a given input x, replacing \\(\\pi_{ref}(y|x)\\) in the DPO loss function. Then, the DPO loss function simplifies to:\n\n\\(L(\\pi_{\\theta};U) = -E_{(x,y_w,y_l)\\sim D} [log \\sigma (\\beta (log \\pi_{\\theta}(y_w|x) \u2013 log \\pi_{\\theta}(y_l|x)) - \\gamma)]\\),\n\nwhere \\(\\gamma\\) = \\(\\beta\\) (log U(yw|x) \u2013 log U(y\u0131|x)) is a constant. Under a length-normalized reward formulation, this loss function becomes:"}, {"title": "A.2 PROOF OF LEMMA 4.2", "content": "Lemma 4.2 (a-DPO provides a lower bound on the online SimPO loss). For any policy model \\(\\pi_{\\theta}\\) and reference model \\(\\pi_{ref}\\), there exists a sufficiently small \\(\\alpha\\) > 0 such that the following inequality holds:"}, {"title": "A.3 PROOF OF LEMMA 4.3", "content": "Lemma 4.3 (Equivalence of Margin Terms). Let \\(\\delta(x, y_w,y_l)\\) denote the difference in sequential KL divergences between the reference policy \\(\\pi_{ref}\\) and the policy \\(\\pi_{\\theta}\\) along the sequences \\(y_w\\) and \\(y_l\\), respectively, defined as:"}, {"title": "B EXPERIMENTS", "content": ""}, {"title": "B.1 IMPLEMENTATION DETAILS", "content": "We observed that the performance of various methods is highly sensitive to model parameters and learning rates. To ensure a fair comparison, we conducted a hyperparameter search as specified in the respective papers. The specific search ranges are detailed in Table 3. Furthermore, due to recent updates to both Llama3-8b and Instruct-7b models, we had to re-implement SimPO as the original results were no longer directly applicable.\n\nTraining hyperparameters. For other parameters, we used a consistent batch size of 128 across all methods. The learning rate was searched within the range of [3e-7, 5e-7, 8e-7, 1e-6], and all models were trained for a single epoch with a cosine learning rate schedule and a 10% warmup phase. Adam was used as the optimizer (Kingma & Ba, 2014). Additionally, the maximum sequence length was set to 2048."}, {"title": "B.2 a-DPO WITHOUT LENGTH-NORMALIZED", "content": "In this paper, we consider length-normalized training as a stability technique and not as a primary contribution of this work. Existing research (Meng et al., 2024) has demonstrated that length normalization can indeed enhance model performance, particularly with respect to the length control win rate. However, to validate the general applicability of \\(\\alpha\\)-DPO\u2014specifically, its stability and performance without length normalization\u2014we conducted experiments across several models: meta-llama/Meta-Llama-3-8B-Instruct, mistralai/Mistral-7B-Instruct-v0.2, and google/gemma-2-9b-it.\n\nWe evaluated DPO, SimPO without length normalization, and \\(\\alpha\\)-DPO without length normalization. The experimental results, as shown in Table 5, demonstrate that \\(\\alpha\\)-DPO consistently achieves performance improvements even without the use of length normalization. This indicates the robustness and general effectiveness of \\(\\alpha\\)-DPO."}, {"title": "B.3 a-DPO WITH DIFFERENCT a", "content": "To analyze the impact of \\(\\alpha\\) on the model, we adjust its value for four different models. The results are illustrated in Figure 5. When \\(\\alpha\\) is set to 0, the model degenerates to SimPO. As \\(\\alpha\\) increases, performance improves across all models, although the optimal value of \\(\\alpha\\) varies among them. This highlights the significance of \\(\\alpha\\)."}, {"title": "B.4 COMPARISON WITH TDPO", "content": "To investigate the relationship between TDPO and \\(\\alpha\\)-DPO, we conducted the experiments, with the results outlined below.\n\nIn its original form, TDPO did not perform well on LLAMA2-8B. By applying Lemma 4.3, we modified the expression M(x, yw, Y\u0131) in a-DPO to use TDPO's d(x, yw, y\u0131), converting our sentence-level estimations to a token-level calculation. This adjustment resulted in a noticeable performance improvement, which we attribute to the length-normalization, \\(\\gamma\\) and z-score normalization of \\(\\delta(x, y_w, y_l)\\). Nevertheless, the modified TDPO still underperformed compared to \\(\\alpha\\)-DPO. This indicates that, when the \\(\\pi_{ref}\\) is suboptimal, token-level calculations are prone to significant errors."}]}