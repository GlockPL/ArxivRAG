{"title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration [Preliminary Version]", "authors": ["Yukun Cao", "Zengyi Gao", "Zhiyang Li", "Xike Xie", "S Kevin Zhou"], "abstract": "GraphRAG addresses significant challenges in Retrieval-Augmented Generation (RAG) by leveraging graphs with embedded knowledge to enhance the reasoning capabilities of Large Language Models (LLMs). Despite its promising potential, the GraphRAG community currently lacks a unified framework for fine-grained decomposition of the graph-based knowledge retrieval process. Furthermore, there is no systematic categorization or evaluation of existing solutions within the retrieval process. In this paper, we present LEGO-GraphRAG, a modular framework that decomposes the retrieval process of GraphRAG into three interconnected modules: subgraph-extraction, path-filtering, and path-refinement. We systematically summarize and classify the algorithms and neural network (NN) models relevant to each module, providing a clearer understanding of the design space for GraphRAG instances. Additionally, we identify key design factors, such as Graph Coupling and Computational Cost, that influence the effectiveness of GraphRAG implementations. Through extensive empirical studies, we construct high-quality GraphRAG instances using a representative selection of solutions and analyze their impact on retrieval and reasoning performance. Our findings offer critical insights into optimizing GraphRAG instance design, ultimately contributing to the advancement of more accurate and contextually relevant LLM applications.", "sections": [{"title": "1 INTRODUCTION", "content": "In recent years, the development of large language models (LLMs) has garnered significant attention and driven advancements across multiple fields. Through extensive pre-training on vast textual datasets and their massive network parameters, LLMs have demonstrated impressive capabilities in semantic understanding and contextual reasoning. However, LLMs exhibit certain limitations when addressing domain-specific queries or handling complex contexts that require external knowledge. For instance, LLMs are prone to generating \u201challucinations\u201d, wherein the information produced appears credible but is factually incorrect and misaligned with real-world knowledge. To address these limitations, retrieval-augmented generation (RAG) has emerged as an effective solution. This approach enhances the reasoning performance of LLMs by incorporating external knowledge during the reasoning process, thereby improving both the factual accuracy and contextual relevance of generated answers. Early RAG frameworks primarily relied on document retrieval, where relevant documents are retrieved from knowledge repositories based on the query context and integrated into the input provided to LLMs, enriching their understanding of real-world information. However, document-based retrieval methods often introduce noise and excessive context [13]. As a result, the latest trend in the RAG community focuses on developing graph-based RAG frameworks: i.e., GraphRAG.\nThe GraphRAG process replaces documents with graphs 1 in the retrieval process of RAG, enabling the extraction of specific \"reasoning paths\" from the graph based on the query context to"}, {"title": "2 GRAPHRAG PRELIMINARIES", "content": "2.1 Problem Formalization\nThe GraphRAG aims to integrate structured knowledge to enhance the reasoning capabilities of LLMs, enabling more accurate and contextually appropriate content generation. In GraphRAG, knowledge is typically represented in the form of Text-Attributed Graphs (TAGs), where both nodes and edges are rich in textual information, with knowledge graphs serving as a typical example. For clarity and without loss of generality, the graphs referred to in this paper will follow the structure outlined in Definition 2.1.\nDefinition 2.1 (Graph in GraphRAG). In GraphRAG, a graph is defined as a directed labeled graph G = (V, E), where V represents the set of nodes (entities), and E denotes the set of directed edges that signify relationships between those entities. Each node v \u2208 V and each edge e \u2208 E carries semantic information. Specifically, nodes represent entities with associated semantic attributes (e.g., names or descriptions), while edges represent relationships with additional semantic context (e.g., the type of relation). Formally, the graph is a set of triples:\n$G = \\{(s, r, t) | s, t \\in V, r \\in E\\},$  (1)\nwhere each triple (s, r, t) indicates that the source entity s is connected to the target entity t by the relation r, encapsulating structured, domain-specific knowledge.\nHere is a specific example of a graph:"}, {"title": "2.2 Related Works of GraphRAG", "content": "Retrieval-Augmented Generation (RAG). Retrieval-Augmented Generation (RAG) addresses the limitations of LLM-generated content by incorporating knowledge from external databases, thereby improving factual accuracy and contextual relevance in the outputs. Early RAG frameworks relies on basic retrieval mechanisms, such as chunking documents and applying text embeddings with cosine similarity for top-k retrieval [25]. These naive approaches, however, often retrieved noisy or irrelevant information, leading to suboptimal reasoning and degraded performance [13].\nTo address these shortcomings, advanced RAG frameworks incorporate more refined retrieval mechanisms to improve document filtering and ranking. Focusing on improving factual accuracy and contextual relevance, these methods utilize pre-retrieval [13, 34, 53] and post-retrieval [8, 9, 52] mechanisms to enhance the quality of retrieved information.\nRecent advancements have shifted toward modular RAG approaches [14], such as LlamaIndex [28] and LangChain [4], which decompose the RAG process into distinct modules (e.g. rewrite, retrieve, rerank, fusion, memory) for more flexible and efficient retrieval and generation. These frameworks not only improve reasoning capabilities and factual accuracy, but also underscore the potential of modular design for language understanding and generation tasks. For instance, by separately fine-tuning retrieval and generation stages, approaches such as those by Yu et al. [51] allow for better control over the RAG process. Despite these advancements, RAG still encounters challenges with noise and irrelevant information retrieval [10].\nCurrent GraphRAG Implementations (or Instances). In GraphRAG, graphs replace document sets in RAG retrieval, allowing the extraction of precise \"reasoning paths\u201d based on the query context, which improves LLM reasoning by drawing on structured knowledge. Graphs provide organized representations of knowledge, ensuring that retrieved paths are both relevant and accurate. In addition, graph-based searching and ranking algorithms help"}, {"title": "3 LEGO-GRAPHRAG", "content": "In this section, we present the LEGO-GraphRAG framework, which decomposes the retrieval phase of the GraphRAG process into three distinct modules, with certain modules being optional depending on the workflow. For each module, we categorize and summarize the available methods (i.e., the design solutions of the framework) and examine them based on key factors that influence the performance of each module (i.e., the design factors of the framework). This systematic classification provides a comprehensive overview of existing works and helps build unexplored GraphRAG instances.\n3.1 Framework Overview\nThe LEGO-GraphRAG framework retains the two primary phases of the GraphRAG process: retrieval and generation, following the problem definition in Section 2. The core retrieval phase is structured through three distinct modules: subgraph-extraction, path-filtering, and path-refinement modules.\nThe design of these three modules is driven by two key considerations: (1) Modularization of conventional RAG: A recent survey study [14] suggests that conventional RAG's retrieval phase naturally divides into three parts: pre-retrieval, retrieval, and post-retrieval. Following this structure, the process of retrieving \"inference paths\" in GraphRAG can be logically segmented into three functional and sequential modules: subgraph-extraction, path-filtering, and path-refinement. (2) Facilitating analysis of recent GraphRAG research: A review of technical advancements in existing GraphRAG instances show that most contributions can be systematically categorized as enhancements to one or more of these three modules, as summarized in Table 1. Formal definitions of the three modules are provided in the sequel.\nDefinition 3.1 (Subgraph-Extraction (SE)). The primary goal of this module is to enhance both the effectiveness and efficiency"}, {"title": "3.2 Design Solutions of LEGO-GraphRAG", "content": "To comprehensively explore the design solutions with the proposed framework, it is essential to systematically classify and summarize existing methods relevant to each module, which typically involve algorithms or neural network (NN) models. The workflows of the Subgraph-Extraction, Path-Filtering, and Path-Refinement modules exhibit similarities, as each can be viewed as a form of \"retrieval\u201d process aimed at identifying a smaller subset of relevant information from a larger collection in response to a specific query. Consequently, solutions applicable to three modules demonstrate"}, {"title": "3.3 Design Factors of LEGO-GraphRAG", "content": "For the design solutions of the LEGO-GraphRAG, we can distill two critical design factors to provide a succinct analysis of the various methods encompassed within it: Graph coupling G and Computational Cost C.\nGraph coupling: This factor refers to the extent to which a method integrates with specific graph structures and sematics during the retrieval process. As illustrated in Table 3, methods such as small-scale specialized models and fine-tuned LLMs exhibit a high degree of graph coupling, meaning they are pre-training or fine-tuned with specific graphs. In contrast, non-NN models and small-scale general neural models are generally non-coupled, as they do not rely on the specific graphs or leverage it only in a generic manner."}, {"title": "4 INSTANCES OF LEGO-GRAPHRAG", "content": "To investigate the impact of various solution types within the three modules established by LEGO-GraphRAG on the overall performance of GraphRAG, in this paper, we construct 21 distinct GraphRAG instances by selecting representative methods from each category within the modules. Through empirical experiments conducted on these instances, our objective is to distill key findings beneficial for the future research of GraphRAG. To ensure the"}, {"title": "4.1 Representative Methods", "content": "Subgraph-Extraction Module. We select 8 methods for this module, covering all solution types: Personalized PageRank (PPR), Random Walk with Restart (RWR), Best Matching 25 (BM25)&PPR, Sentence-Transformers (ST)&PPR, Rerank model (Reranker)&PPR, Fine-tuned ST&PPR, Vanilla LLMs&PPR, and Fine-tuned LLMs&PPR. Notably, as a foundational structure-based method, PPR underpins the implementation of other methods that focus on evaluating the relevance between entities, relationships and queries within structured search process.\nPPR: In the subgraph-extraction module, the PPR algorithm operates on a graph G = (V, E) by initiating with query-relevant nodes (entities) v \u2208 eq. This algorithm leverages structural features based on node degrees to identify nodes most closely related to these initial query nodes. Specifically, each node v \u03b5 \u03bd is assigned an iteratively updated score pu, calculated as follows:\n$p_v^{(t+1)} = \\lambda E_v + (1 - \\lambda) \\sum_{u \\in N(v)} \\frac{p_u^{(t)}}{deg(u)},$ (9)\nwhere \u03bb denotes the teleportation probability, which influences the likelihood of returning to nodes within eq, and Ev represents the preference vector highlighting nodes relevant to the query entities. Here, N(v) refers to the neighborhood of node v, and deg(u) denotes the degree of node u. This iterative process proceeds until convergence, resulting in a stable set of PPR scores. Some nodes (denoted as set V') with the higher scores are subsequently chosen as the top-ranked nodes associated with the query. Using this selection, we construct a subgraph gq = (vi, r, vj) | vi, vj \u2208 V', (vi, r, vj) \u2208 G, which serves as a foundational structure for identifying reasoning paths aligned with the contextual objectives of the query.\nRWR: Like PPR, RWR ranks node v \u2208 V by iteratively updating the relevance score of the query-relevant nodes (entities) v (9) \u2208 eq. The key difference is that PPR assigns distinct initial probabilities to enable personalized ranking, whereas RWR restarts at a fixed starting node after each random walk, emphasizing node centrality.\nBM25, ST, Reranker, Fine-tuned ST, Vanilla LLMs, Fine-tuned LLMs with PPR: Since structure-based methods like the PPR algorithm only consider structural information when extracting subgraphs, there is potential to integrate methods that can evaluate semantic similarity to further refine the extracted subgraphs. Formally, for a subgraph gq extracted by the PPR algorithm based on query q and the associated set of entities and relations eq = {(9), e (9) }, a scoring function S(v, e; eq) is introduced to measure the semantic similarity between nodes and edges in gq and the entities and relations within eq. By applying threshold scores \u03c4\u03bf and \u03c4\u03b5, only nodes and edges in gq that demonstrate a high semantic relevance to the query are retained, resulting in a refined subgraph 9q: 9'q = {(v, e) \u2208 gq | S(v, eq) \u2265 \u03c4\u028a and S(e, eq) \u2265 \u03c4\u03b5}. The choice of scoring functions for evaluating semantic similarity is diverse, encompassing a range from statistical semantic methods to NN models of varying scales, with some models incorporating fine-tuning while others remain in their vanilla (pre-trained) forms.\nPath-Filtering Module. The module incorporates 8 methods, with foundational structure-based methods such as shortest path-filtering and complete path-filtering. Similar to subgraph-extraction module, semantic relevance assessment methods including BM25, ST, Reranker, Fine-tuned ST, Vanilla LLMs, and Fine-tuned LLMs"}, {"title": "4.2 Grouping of GraphRAG Instances", "content": "As shown in Table 4, from the perspective of empirical research, the constructed GraphRAG instances can be categorized into 4 groups:\nBasic Group. In Table 4, Instance 0 is designated as the basic instance and serves as a common reference for all other instances. This is because Instance 0 adopts the simplest yet relatively effective methods across the three modules, which also function as fundamental components in the other instances.\nSubgraph-Extraction Group. Instances 1-7 in Table 4 can be regarded as iterations over various types of methods within the subgraph-extraction module, with the method of path-filtering and path-refinement modules held constant. Therefore, we group these instances to facilitate the study of how different methods within the subgraph-extraction module impact performance of GraphRAG.\nPath-Filtering Group (Instances 8-14) and Path-Refinement Group (Instances 15-10) are grouped following a same rationale as described above."}, {"title": "5 EXPERIMENT", "content": "5.1 Experimental Settings\nDatasets. This study employs four well-established KBQA datasets: WebQSP [49], CWQ [44], GrailQA [15], and WebQuestions [2], all of which are extensively utilized within the KBQA and GraphRAG research communities [17, 21, 23, 30, 31, 37-43, 47, 50]. The statistics of the datasets used in this paper are shown in Table 5. Detailed information of datasets can be found in the Appendix A.1.\nMetrics. To evaluate the performance of instances of LEGO-GraphRAG, we use Precision, Recall, F1-score (F1), and Hit Ratio (HR) as evaluation metrics. Additionally, we record retrieval, generation and fine-tuning times for each module.6\nFor a given set of predictions P and corresponding ground-truth answers A over a set of queries Q, Precision and Recall are defined as: Precision = $\\frac{\\sum_{q \\in Q} |P_q \\cap A_q|}{\\sum_{q \\in Q} |P_q|}$, Recall = $\\frac{\\sum_{q \\in Q} |P_q \\cap A_q|}{\\sum_{q \\in Q} |A_q|}$, where Pq denotes the predicted answer set for query q, and Aq represents the ground-truth answer set for q. F1 is defined as the harmonic mean of Precision and Recall: F1 = $\\frac{2 \\times Precision \\cdot Recall}{Precision + Recall}$.\nHR measures the proportion of queries for which at least one predicted answer matches the ground truth, formally defined as: HR = $\\sum_{q \\in Q} I(|P_q \\cap A_q| > 0)$, where I(\u00b7) is the indicator function, taking a value of 1 if its argument is true (indicating a hit) and 0 otherwise."}, {"title": "5.2 Evaluating on Retrieval Phase", "content": "Different Solutions on Three Modules. By grouping and evaluating the intermediate results of 21 GraphRAG instances across three modules, Figures 1, 2, 3, and 4 illustrate the local performance of different solution types within each module across four datasets. Additionally, Table 6 presents a detailed execution time for each of the 21 GraphRAG instances within each module. The fine-tuning time for the LLM and Sentence Transformer is shown in Table 8. Based on the experimental results, we draw the following findings.\nFor subgraph-extraction module:\n*   Regarding the two foundational structure-based methods, although PPR exhibits a lower F1 score compared to RWR, its higher recall value demonstrates PPR's superiority in capturing relevant nodes. This indicates that PPR is more adept at comprehensively identifying results pertinent to the query, despite its lower precision relative to RWR. Given that the subgraph-extraction module serves as the foundation for subsequent modules, the increased recall is crucial for ensuring that key information is not overlooked, thereby facilitating the retrieval of the correct reasoning paths. Consequently, it is more appropriate to select PPR as the fundamental search algorithm for the subgraph-extraction module. However,\n*   Statistic-based methods are not suitable for enhancing the PPR algorithm, despite their rapid execution times. While these methods do improve the F1 score compared to the PPR algorithm, they also lead to a decline in recall. This indicates that statistic-based method soversimplify semantic considerations and apply overly stringent filtering of relevant information, potentially resulting in the loss of critical reasoning information.\n*   Among the various methods that enhance the PPR algorithm through semantic scoring and ranking, methods based on (fine-tuning) LLMs have achieved the best results. These methods not only maintain the high recall of the PPR algorithm but also improve the F1-Score, indicating that they effectively balance both the comprehensiveness and accuracy of search results. However, it is important to note that the execution time for methods utilizing LLMs is the longest among all examined techniques. For instance, in four datasets, the execution times for the methods based on LLMs were recorded as 5.93 seconds, 5.48 seconds, 5.59 seconds, and 5.57 seconds, approximately four times longer than the execution times of methods based on small-scale (specialized) models.\n*   In summary, two effective design strategies can be adopted for the subgraph-extraction module. First, one may opt to implement only the most fundamental methods, such as PPR, which maintains a high recall rate while delegating more detailed retrieval steps to subsequent modules. Alternatively, this module could also leverage small-scale models, such as Reranker, to optimize the PPR algorithm. This approach can enhance the relevance of extracted subgraphs to queries without incurring significantly high execution times."}, {"title": "5.3 Evaluating on Generation", "content": "GraphRAG Instances. The performance of 21 GraphRAG instances across all datasets during the generation phase is illustrated in Figure 7, 8 9, and 10, with reasoning conducted using three LLMs, respectively.\nNumber of Reasoning Paths. Figure 11 shows the performance (measured by HR and F1 scores) of three models Llama2-7b, Llama3-8b, and Qwen2-7b as the number of reasoning paths increases. Across all models, there is a clear positive correlation between the number of reasoning paths and performance scores."}, {"title": "6 CONCLUSION", "content": "In this paper, we present LEGO-GraphRAG, a structured framework for the modular analysis and design of GraphRAG instances. By breaking down the retrieval process of GraphRAG into distinct modules and identifying critical design factors, LEGO-GraphRAGprovides a clearer path for optimizing GraphRAG implementations. Our experiments demonstrate the practical advantages of this modular approach, highlighting how specific choices in algorithm and NN model selection can enhance both retrieval accuracy and reasoning effectiveness. These insights contribute to advancing both research and practical applications of GraphRAG, paving the way for more reliable, domain-specific applications of LLMs."}, {"title": "A APPENDIX", "content": "A.1 Datasets\nWe referenced the training and testing splits of these four datasets from TOG and performed a filtering process based on that. The final distribution of the dataset is shown in Table 5. The training portion is used to fine-tune the LLM and the Sentence Transformer, while the testing portion is used to evaluate the performance of different modules and the reasoning result of the LLM.\nA.2 Experimental Settings\nAll experiments are running on Ubuntu 20.04.6 LTS (Intel(R) Xeon(R) Platinum 8358 CPU@2.60GHz Processor, 4 A100-80G, 400GB memory). The detailed experimental settings are as follows:\nHyperparameters The detailed hyperparameter settings for different phase modules are shown in Table 7. Here, max_ent represents the maximum number of entities, set to 2000; restart_prob represents the restart probability, set to 0.8; windows represents the window size, set to 24; top_k represents the number of top-k entities, set to 64; and beam_width represents the beam width, set to 128. In Inference phase, the temperature parameter is set to 0.01, and the maximum token length for generation is fixed at 256, and stop_tokens is [\"<|eot_id]>\"]. We use zero-shot reasoning prompt across all datasets except the prompt-base reasoning experiment, and the prompt templates are presented in Appendix A.2.\nPrompts"}]}