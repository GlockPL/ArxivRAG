{"title": "Bottom-up Anytime Discovery of Generalised Multimodal Graph Patterns for Knowledge Graphs", "authors": ["Wilcke WX", "Mourits RJ", "Rijpma, A", "Zijdeman RL"], "abstract": "Vast amounts of heterogeneous knowledge are becoming publicly available in the form of knowledge graphs, often linking multiple sources of data that have never been together before, and thereby enabling scholars to answer many new research questions. It is often not known beforehand, however, which questions the data might have the answers to, potentially leaving many interesting and novel insights to remain undiscovered. To support scholars during this scientific workflow, we introduce an anytime algorithm for the bottom-up discovery of generalized multimodal graph patterns in knowledge graphs. Each pattern is a conjunction of binary statements with (data-) type variables, constants, and/or value patterns. Upon discovery, the patterns are converted to SPARQL queries and presented in an interactive facet browser together with metadata and provenance information, enabling scholars to explore, analyse, and share queries. We evaluate our method from a user perspective, with the help of domain experts in the humanities.", "sections": [{"title": "1 Introduction", "content": "In only a short span of time, knowledge graphs have transitioned from an academic curiosity to an attractive data model for storing and publishing scientific data [13]. Amongst the multitude of adopters of this data model are many of the world's galleries, libraries, archival institutions, and museums [5, 12, 39], as well as various scientific communities including linguistics, archaeology, humanities, and history [7, 22, 24]. The combined efforts of these institutes and communities have resulted in a considerable number of publicly-available knowledge graphs which, together, surmount to vast amounts of interconnected heterogeneous knowledge. Much of this knowledge used to be stored in analogue or digital silos, and has never been brought together before. Now linked to one another, this federative network of knowledge offers great opportunities for scholars, who can now potentially ask and answer many new research questions.\nDrafting research questions is an essential step in the scientific research workflow. Such questions can either be derived from the scholarly literature or from patterns in data. However, without study, it is often not known beforehand which questions these data might have the answers to. Even if accompanied by rich metadata, these alone are often not enough to guide scholars in this process, limiting them to insights from the literature or sparks of their own imagination. This may result in many potentially interesting and novel insights to remain unstudied, due to possible biases or blind spots in the literature and the scholars' thinking. This work aims to support scholars during this early stage of the scientific workflow, by highlighting potentially interesting patterns in their data that may form the building blocks for new research questions, and which can be used as evidence for already existing lines of research.\nPattern detection on graph-shaped data can take on various forms. On the most fundamental level, graph patterns are recurrent and statistically significant subgraphs in which some or all of the vertices have been replaced by unbound variables [19]. Generalized graph patterns take this a step further, by having special variables that cover an entire set of vertices, such as all members of a certain class [14]. Scholars can use such patterns to explore structural regularities in the graph; other regularities, such as those between the various numerical, temporal, and textual attributes values are generally not considered, however, despite their prevalence in many knowledge graphs. This is particularly evident in the soft sciences where measurements, dating, and note taking are commonplace [37]. Since these multimodal data often contain insightful and unique information about the subject they belong to, it becomes all the more important to treat them as first-class citizen. By doing so, we can integrate non-structural regularities into generalized graph patterns and obtain more expressive patterns that offers scholars a more fine-grained view of their data.\nFigure 1 illustrates the merit of combining structural and non-structural regularities. The left side of the figure depicts a civil record about an unemployed woman, named Jane, who died at an age of 23.6 years old, whereas, on the right, a graph pattern is shown that covers this record. The depicted pattern likewise covers other records about unemployed women, provided that they died at an age that falls within the learned distribution.\nThis distribution is an example of a non-structural regularity; without it, the pattern would have been limited to unemployed women whose age of death is on record, irrespective of the value. For other attributes, in this example the person's name, the variation between values might be too great to constitute a regularity, hence it being excluded from the pattern."}, {"title": "2 Related Work", "content": "Pattern detection methods generally fall in one of two categories: the symbolic approaches, which employ some form of (logical) rule mining, and the non-symbolic approaches, which often involve (graph) neural networks in an unsupervised learning setting. The method described in this paper falls into the first category.\nRule Mining for Graphs A considerable body of literature is available on rule mining for relation data. Many of these approach the problem from either a frequentist or inductive school of thought. Methods that belong to the latter (e.g. [27, 28, 34]) generally apply inductive logic programming, which involves learning logical rules that explain all true arcs and no false ones [26]. This technique is less suited for knowledge graphs, however, due to challenges with scalability and the need for a closed world [29]. In contrast, frequentist approaches emphasize (relative) coverage, and typically involves the mining of association rules or frequent substructures. This work falls under the umbrella of frequentist approaches, by introducing a method to discover statistically relevant substructures in knowledge graphs.\nFrequent subgraph mining is a mature field of research which involves finding all subgraphs that occur more than some predefined number of times [15]. These subgraphs, and the graphs from which they are mined, are assumed to be labelled simple graphs. For knowledge graphs, in which the vertices and arcs have types, it is therefore more interesting to look for generalized subgraphs, for example by abstracting away to the level of the classes [2, 21, 31], or by generalizing over controlled vocabularies and taxonomies [6, 32]. To discover the subgraphs, these methods commonly employ a bottom-up approach, similar to our method, that begins with the most basic rules and incrementally adds new arcss until some condition is reached or the search space has been exhausted.\nClosely related to subgraph discovery is graph-based association rule mining, which aims at finding rules that imply frequent co-occurrences between subgraphs. Such rules can be found by adapting the Apriori algorithm for graph data, in which case so-called item sets of correlated arcs are sought, which are then clustered hierarchically to induce an ordering on frequency [3, 35, 42]. Other methods are specifically tailored to graphs and use a bottom-up approach comparable to many subgraph mining methods [9, 23, 41]. In some cases, background knowledge is levered to infer implicit knowledge [25, 30].\nMany methods employ optimization and smart pruning strategies to reduce the search space to a more manageable size, for example by cutting unviable branches at an early stage or by avoiding duplicate subgraphs found via different paths [8, 20, 41]. Similar strategies are being used by our method.\nQuery Generation To the best of our knowledge, generating queries directly from the data has received little attention. Instead, most literature explores query log mining [43], top-down query construction [11], or the use of language models [40] for this purpose. A notable exception is Shen et al. [38], who cluster biomedical data on semantic closeness of the relationships and convert these clusters into SPARQL queries. Some studies have also looked into the conversion of SPARQL queries into other formats, including logical formulae [33, 36]. Our"}, {"title": "3 Perquisites", "content": "Central to our approach are knowledge graphs and SPARQL queries. This next section will briefly introduce these concepts.\n3.1 Knowledge Graphs\nA knowledge graph G = (R,P,A) is a labelled multidigraph with R and P denoting the set of resources (vertices) and predicates (arc types), respectively, and with A\u2286 P\u00d7E\u00d7E \u222a P\u00d7E\u00d7L representing the set of all assertions (arcs) that make up the graph. The set of resources R = E\u222a L can be further divided into the set of entities, E, which represent unique things, tangible or otherwise, and the set of literals, L, which represent attribute values such as text and numbers, and which belong to exactly one entity. Literals can optionally be annotated with their datatype (or language tag, from which the datatype can be inferred) which itself is an entity.\nAn example of a knowledge graph is depicted in Figure 1-left, showing a small graph from the civil registry domain. This particular graph contains seven entities, two of which are classes, and two literals: a number and a string. These elements are linked to each other by exactly eight assertions, two of which represent the same predicate: has_type.\nThere are various data models available to model knowledge graphs with. In this work, we consider the Resource Description Framework (RDF), which is a popular choice for this purpose. However, our approach can be adapted to other data models with minor changes.\n3.2 SPARQL Queries\nSPARQL is a query language for RDF-encoded knowledge graphs that supports searching for graph patterns. A typical SPARQL query consists of three parts: 1) a prologue, in which the namespaces are defined, 2) a SELECT clause, which specifies the return variables, and 3) a WHERE clause, which contains the graph pattern we are to match against. SPARQL also provides many other capabilities, but these are out of the scope of this paper.\nGraph patterns in a SPARQL query are similar to their logical counterpart except that the clauses are written in infix notation\u2014R \u00d7 P \u00d7 R\u2014and that the conjunctions between them are implicit. Additionally, variables are prepended by a question mark (?), and the FILTER keyword can be used to constrain the result set. An example is listed in Listing 1-right, showing the SPARQL query corresponding to the graph pattern in Figure 1-right."}, {"title": "4 Defining Generalized Multimodal Graph Patterns", "content": "Generalized multimodal graph patterns are recurrent and statistically significant subgraphs in which some or all of the resources have been replaced by special variables. This allows for graph patterns that abstract away from the level of the individual resources by modelling structural regularities between and non-structural regularities within sets of resources. From now on, we will refer to such patterns as graph patterns or simply as patterns unless the meaning is not evident from the context.\nFormally, a graph pattern $ = C_i \u2227 C_j \u2227 ... C_k is a conjunction of k clauses, with k \u2265 1, where each clause c = p(a, b) is a binary predicate that represents the relationship p\u2208 P between the elements a and b. Here, a and b can be constants that represent actual resources in the graph, in which case p(a, b) corresponds to an assertion in A, or they can be variables, representing a set of entities or literals. In either case, we will refer to a and b as the head and tail of a relationship, respectively.\nIn this work we consider three different kinds of variables, namely object-type, data-type, and value-range variables. The set of resources that each variable covers is called its domain. For each of the three variable types, we define their domain as follows.\nObject-type: Let T_\u025b be the set of object types in G, and T(e,t) a binary predicate that holds if entity e \u2208 E is of type t. The domain of an object-type variable of type t \u2208 T_e can now be defined as the set of entities E_t \u2286 E such that \u2200e \u2208 E_t: T(e,t).\nData-type: Let T_c be the set of datatypes in G, and T(l,t) a binary predicate that holds if literal l \u2208 L is of datatype t. The domain of a data-type variable of type t \u2208 T_c can now be defined as the set of literals L_t \u2286 L such that \u2200l\u2208 L_t: T(l, t).\nValue-range: Let predicate p\u2208 P represent a relationship with value space S_L such that \u2200l \u2208 L,\u2203e \u2208 E : p(e,l) \u2192 l\u2208 S. The domain of a value-range variable can now be defined as the set of attribute values S_F \u2286 S that fall within a distribution F defined on S.\nBoth object-type and data-type variables allow for a generalization over structure. Examples of the former are the object types Person and Occupation, which cover all people and jobs, whereas the datatypes String and Float encompass all text and real-valued attribute values. Value-range variables offer a further generalization over attribute values, for example by fitting one or more Gaussian distributions on a collection of years, or by defining a uniform distribution over a set of characters (encoded as regular expression, e.g. \"^[:alnum:]{3,6}$\").\nThe clauses in a pattern are subject to several rules to safeguard their logical and semantic validity. Firstly, the head of a clause must be an object-type variable, for else the pattern is bound to a specific resource, making generalization impossible. Secondly, for all-but-one object-type variables in the head of a clause there must exist a clause which has the same variable in the tail"}, {"title": "5 Discovering Graph Patterns", "content": "Our algorithm employs a two-phase approach for discovering graph patterns. During the first phase, the algorithm generates all possible single-clause patterns that satisfy the minimal requested support. These so-called base patterns form the building blocks for more complex graph patterns, which are generated during the second phase by extending previously discovered graph patterns with appropriate base patterns. Since all complex graph patterns are a combination of base patterns, and since generating and evaluating new patterns involve simple set operations, minimal further resource-intensive computation is necessary after completing the first phase. By also providing each pattern with a description of its domain (e.g. via a set of integer-encoded resources) we no longer require to keep the original graph in memory while retaining the minimal information necessary to derive the domain and support for new patterns.\nNew graph patterns are generated breadth first, by first computing all possible patterns of minimal size and by then iteratively combining these to form ever"}, {"title": "5.1 Constructing Base Patterns", "content": "Base patterns are generated by generalizing over all assertions of which the entity in the head position is of the same type, as shown in Procedure 1. This type-centric approach is chosen because the members of a class are likely to possess similar characteristics and, by extension, are also likely to share similar regularities. By replacing the specific head entities in these assertions by their corresponding object-type variables, we obtain clauses of the form p(v_{ot}, r) which represent a relationship p between an entity of type t and a resource r. After computing the domain and support, each clause that enjoys a sufficiently high score is made into a pattern, \u03c6 = p(v_{ot}, r), and added to polytree \u03a9 as root.\nFor brevity, the pseudocode in Procedure 1 omits the computation of clauses with a variable in the tail position. Similar steps can be used, however, to generate the remaining three cases: for object-type and data-type variables, we simply need to keep count of the various types of entities and literals, respectively, and, when this count meets the minimal requested support, create a new base pattern \u03c6 = p(v_{ot}, v_{ot}) or = p(v_{ot}, v_{dt}), with v_{dt} a data-type variable of type t', which"}, {"title": "5.2 Combining Graph Patterns", "content": "Going from the base patterns to more complex patterns involves the generation of candidate extensions C, which, if deemed favourable, are appended to their parents' set of clauses to form new graph patterns \u03c8'. These new patterns are then added to \u03a9 as children to their parents, provided that they meet the minimal requested support. Procedure 2 and 3 outline this process.\nEach of the candidate extensions is a pair of clauses (c_i, c_j), where c_i = p_k(\u00b7, v_{ot}) is one of the parent's outer clauses a candidate endpoint and c_j = p_i(v_{ot}, \u00b7) is a suitable base pattern. Both clauses are ensured to hold the same object-type variable, thus providing a semantically valid connection. Depending on the element in the tail position of clause c_j, the extension, if added, will be terminal or non-terminal. If a pattern has no further non-terminal clauses it will be omitted from future iterations of the algorithm.\nThere are often multiple extensions possible per pattern within the same iteration. To exhaustively explore the space of multiple extensions, our algorithm evaluates each of these extensions separately, as well as their k-combination (without repetition) with k ranging from two to the number of candidate extensions |C|. Note that, since not all combinations are accepted, the actual maximum value for k will generally be less than |C| in practice.\nFor each new pattern we compute the domain and associated support score. Since patterns carry a description of their own domain, we can easily compute the domain of a newly derived pattern by taking the intersection of its parent's domain with that of the recently-added clause, and by then propagating this change through the other clauses in the pattern. Figure 2 illustrates this principle, by showing how adding a new clause reduces the domain of the pattern as a whole."}, {"title": "5.3 Search Optimization", "content": "Smart pruning techniques and other optimizations are used to reduce the search space by avoiding duplicate, invalid, and/or poorly supported patterns and clauses. We provide a brief description of the most important techniques next.\nSince every added clause makes a pattern more specific, it must follow that the corresponding domain should be a proper subset of that of its parent. Hence, patterns that have the same domain as their parent are pruned and disallowed from becoming a parent themselves. The sole exception are clauses with an object-type variable as tail, which are kept for one iteration more in case they might farther a pattern that does reduce the domain.\nPatterns that were not extended during the current iteration are omitted from future iterations. The intuition behind this is that future iterations necessarily involve more specific patterns; if the patterns did not meet the minimal support during the current iteration, then it follows that this will also be the case for future iterations. The same holds for base patterns.\nCandidate extensions that do not meet the minimal required support are omitted from future iterations. Since the domain of an extension will stay unchanged during the entirety of a run, it follows that adding them can never result in a pattern with a sufficiently high support score. Base patterns for which this is the case are already filtered during their creation.\nDuplicate patterns (which might occur via different routes) are pruned early on by creating a cheap proxy-the logical formula as string and checking this against a hash table before creating the actual object and computing its domain.\nPatterns that only have terminal clauses or no appropriate object-type variables are disallowed from becoming a parent, whereas patterns which exceed the maximum allowed length, width, or depth are pruned early on for obvious reasons."}, {"title": "5.4 Pattern Browser", "content": "A simple facet browser (Fig. 3) was created to assist scholars with the exploration and analysis of the discovered patterns. Build upon open web standards, the pattern browser facilitates the filtering of patterns over various dimensions, including support, depth, length, and width, as well as provide full-text search capabilities. The filtered selection can be saved in a separate file, which itself can be opened in the pattern browser for further analysis. Alternatively, the saved selection can be shared with others or published on the web, facilitation reuse and reproducibility.\nMetadata is stored together with the patterns and can be viewed directly from the pattern browser. Amongst others, these data include provenance information and hyperparameter settings from the process that created the patterns. Additional data is appended to the provenance information upon saving a filtered selection, allowing users to trace back all performed actions. Both patterns and metadata are stored using RDF."}, {"title": "6 Evaluation", "content": "We evaluate our algorithm and the patterns it produces from a user-centric perspective, by conducting a user study amongst a select group of domain experts from the humanities. The primary goal of this user study is to ascertain the perceived interestingness of the discovered patterns, as well their interpretability. For this purpose, graph patterns were discovered within a domain-specific"}, {"title": "6.1 Dataset", "content": "The knowledge graph used in our experiments contains the civil records from Dutch citizen who were alive between 1811 and 1974. Each record includes information about a person's pedigree, marital status, occupation, and location, as well as various important life events including birth, death, and becoming a parent. Due to the sensitivity of these data we are prohibited from sharing this dataset, unfortunately.\nIn its entirety, the dataset contains the records from over 5.5 million people. For experimental purposes, a subset was created by randomly sampling 100 thousand individuals together with their context, resulting in a graph with just over one million assertions between roughly 635 thousand resources. We believe that these numbers are sufficiently large enough for the same patterns to emerge as those present in the original dataset.\nTwo example patterns that were found during these experiments are listed in Listing 2, together with their description in natural language. The left-hand pattern covers the set of all 29 year old men who are married and who work in the agricultural sector, which accounts for 1,238 individuals in the dataset. The graph pattern on the right accounts for 13,632 people, and encompasses all women with two first names between two and 14 characters each, and with a family name between three and 16 characters."}, {"title": "6.2 User Study", "content": "The user study took the form of an online questionnaire, lowering the barrier for participation and allowing for a cross-border audience. To ensure a good fit between this audience and the topic at hand it was decided to make the questionnaire open to invitation only. While this might have resulted in a lower number of participants, we are confident that their responses are more valuable.\nThe questionnaire was split into four sections. In the first section, participants were asked about their familiarity with the core concepts surrounding this research. The answers to these questions allowed us to weight the participants' responses on later questions. The second and third sections involved questions about the graph patterns and the pattern browser, respectively, whereas the last section asked several overarching questions about the perceived usefulness of our method and the patterns it yields. In all cases (save for open questions) the responses were recorded using a five-point Likert scale ranging from Strongly Disagree (negative) to Strongly Agree (positive).\nTo assess the graph patterns on interestingness [10], participants were presented with several hand-picked patterns in SPARQL format, and asked to rate each one on novelty, validity, and utility, as well as on interpretability. A similar setup was used for the pattern browser, but instead using screenshots and rated on helpfulness (in analysing the patterns), intuitiveness (of the interface), pleasantness (of the colour scheme), and understandability (of the displayed information).\nTo determine the agreement and reliability amongst participants, we employ Kendall's coefficient of concordance W for its suitability to evaluate ordinal data with multiple ratings over multiple items [17]. For similar reasons, we use Kendall rank correlation coefficient to measure dependencies between responses [16]. We furthermore employ factor analyses to obtain a better understanding of the interactions between criteria."}, {"title": "6.3 Results & Discussion", "content": "A total of 13 out of the 42 experts on social and economic history to whom we reached out took part in the user study, corresponding to a fair response"}, {"title": "7 Conclusion & Future Work", "content": "This work introduced an anytime, bottom-up, and easily parallelizable algorithm to efficiently discover generalised multimodal graph patterns in knowledge graphs. To facilitate further filtering and analysis, the discovered patterns are converted"}]}