{"title": "LEARNING TO GENERATE AND EVALUATE FACT-CHECKING\nEXPLANATIONS WITH TRANSFORMERS", "authors": ["Darius Feher", "Abdullah Khered", "Hao Zhang", "Riza Batista-Navarro", "Viktor Schlegel"], "abstract": "In an era increasingly dominated by digital platforms, the spread of misinformation poses a sig-\nnificant challenge, highlighting the need for solutions capable of assessing information veracity.\nOur research contributes to the field of Explainable Artificial Antelligence (XAI) by developing\ntransformer-based fact-checking models that contextualise and justify their decisions by generating\nhuman-accessible explanations. Importantly, we also develop models for automatic evaluation of ex-\nplanations for fact-checking verdicts across different dimensions such as (self)-contradiction,\nhallucination, convincingness and overall quality. By introducing human-centred eval-\nuation methods and developing specialised datasets, we emphasise the need for aligning Artificial\nIntelligence (AI)-generated explanations with human judgements. This approach not only advances\ntheoretical knowledge in XAI but also holds practical implications by enhancing the transparency,\nreliability and users' trust in AI-driven fact-checking systems. Furthermore, the development of our\nmetric learning models is a first step towards potentially increasing efficiency and reducing reliance\non extensive manual assessment. Based on experimental results, our best performing generative\nmodel ROUGE-1 score of 47.77, demonstrating superior performance in generating fact-checking\nexplanations, particularly when provided with high-quality evidence. Additionally, the best perform-\ning metric learning model showed a moderately strong correlation with human judgements on ob-\njective dimensions such as (self)-contradiction and hallucination, achieving a Matthews\nCorrelation Coefficient (MCC) of around 0.7.", "sections": [{"title": "Introduction", "content": "Assessing the veracity of claims is a vital capability in the modern world, but it is a task that the public is often\nill-equipped to do. This is evidenced, for example, by people's vulnerability to online fake news, especially with\nrespect to topics related to public health policies (Rocha et al., 2021; Vidgen et al., 2021), human contribution to\nclimate change (Taddicken and Wolff, 2023) and political elections (Grossman and Helpman, 2023). Due to targeted\ndisinformation campaigns, many users are inadvertently spreading misinformation, without critically reflecting about\nits sources, as the information is often presented without further context. Since experts cannot provide contextualising\nexplanations about the validity of a claim instantaneously, there is an opportunity for the natural language processing\n(NLP) community to investigate automated fact verification approaches that are capable of generating explanations.\nState-of-the-art research on fact verification has mostly focussed on the capability to identify misleading\nclaims (Thorne et al., 2018). However, for end-users, it is important to provide explanations of why exactly a claim\nwas identified as wrong. These explanations serve both as context for the claim and as an insight into the reasoning\nprocess that led to the veracity decision. Existing fact verification approaches rely on deep learning-based models op-\ntimised on large static datasets to automatically classify whether a claim is true or false, based on retrieved supporting"}, {"title": "Related work", "content": "In this section, we provide an overview of the relevant literature. First, we present a summary of methods employed\nfor fact checking, followed by an outline of existing datasets for this task. Furthermore, existing methodologies used\nfor the automated evaluation of Natural Language Generation (NLG) methods are detailed."}, {"title": "Explainable fact-checking approaches", "content": "Existing explainable approaches to fact checking can be categorised into four distinct groups. First, there are methods\nthat provide an explanation by extracting sentences from the evidence used to check the veracity of a claim. This\nprocess can also be seen as extractive summarisation (Alhindi et al., 2018; Lakhotia et al., 2020; Atanasova et al.,\n2020; Fan et al., 2020). While these endeavours are scientifically justified and useful, their real-world application is\npotentially limited, as they typically do not provide a human-accessible way to intuitively understand and reconstruct\nthe reasoning behind the system's prediction. The ability to explain its logic is however a key property for developing\ntrust in an autonomous system (Glass et al., 2008). Next, some approaches focussed on generating explanations by\nusing question answering (QA) as a proxy task (Chen et al., 2022; Dai et al., 2022; Yang et al., 2022; Pan et al., 2023).\nWhile these approaches improve explainability, they face challenges such as longer response times due to reliance\non APIs and large language models (LLMs) (Pan et al., 2023), lack of representation due to the domain specificity\nof datasets, difficulty in aligning with human judgements (Chen et al., 2022), and potential error propagation from\ninaccurately generated questions (Dai et al., 2022). Additionally, the relevance of these questions can be limited, given\nthat claims are usually short, hence, lacking context. Furthermore, another way of generating explanations is by using\ninformation available in knowledge graphs (Gad-Elrab et al., 2019; Nikopensius et al., 2023). In this method, the\nexplanation consists of paths used by the agent to fact-check the claim. However, while useful, they can be complex\nfor users to interpret. Finally, another approach that leverages advancements in NLG is the generation of explanations\nthat are easy to understand by humans, framing the task as abstractive summarisation, as was proposed by Kotonya\net al., (2020b) and Yao et al., (2023). However, the former is using a healthcare-specific dataset for training, and both\nuse traditional (proxy-based) NLG evaluation metrics such as ROUGE or BERTSCORE (Zhang et al., 2019)."}, {"title": "Fact-checking datasets", "content": "Most of the existing fact-checking datasets include the claim being checked, the evidence article and the veracity\nlabel, yet lack a justification or explanation of the truthfulness of the claim (Thorne et al., 2018; Hanselowski, Stab,\nSchulz, Li and Gurevych, 2019). Conversely, some datasets contain explanations written by journalists, but they\nare multimodal and thus include both text and images (Yao et al., 2023), or are catering to particular domains such as\nhealthcare (Kotonya et al., 2020b) or politics (Alhindi et al., 2018). A recently released dataset called FactEx (Althabiti\net al., 2023), for instance, includes journalist explanations from politifact.com, a platform for fact-checking claims\nby politicians.\nFurthermore, existing fact verification datasets may exhibit quality issues because they were gathered via crowdsourc-\ning (Br\u00fchlmann et al., 2020; Schlegel et al., 2020). Crowdsourced datasets have been shown to exhibit dataset artefacts\nsuch as arbitrary expressions that cue the ground truth label (Thorne et al., 2019). As a result, models optimised on\nthese datasets learn to exploit these cues rather than reliably performing the task."}, {"title": "NLG automated evaluation metrics", "content": "In the field of NLG, automated metrics play a crucial role in evaluating the quality of generated text. These metrics\ncan be broadly classified into two categories: task-agnostic and human-aligned. Examples of the former include\nperplexity (Jelinek et al., 1977), BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie,\n2005), or SELF-BLEU (Zhu et al., 2018) scores, which typically measure aspects like n-gram overlap or grammatical\ncorrectness. While they offer quick, objective and reproducible assessments, they often fail to capture the nuances of\nhuman language (e.g., coherence, relevance). Conversely, human-aligned metrics focus on how well the generated text\naligns with human judgement or expectation. G-Eval (Liu et al., 2023) utilises GPT-4 together with a chain-of-thought\nand form-filling framework to evaluate generated text across various dimensions including coherence, engagingness\nor fluency. However, there is a potential bias in G-Eval towards texts generated by LLMs, and its effectiveness\ndepends on the availability and accessibility of these models, which incur usage costs. In contrast, other systems like\nUniEval (Zhong et al., 2022) approach the text evaluation problem as a boolean question answering task. Moreover,\nanother automated metric, RoMe (Rony et al., 2022), makes use of different pre-trained transformers such as ALBERT\nto evaluate texts based on informativeness, naturalness and quality. However, to the best of our knowledge, there are\nno metrics specifically optimised for qualitatively evaluating fact-checking explanations."}, {"title": "Methodology", "content": "In this section, we will discuss the approach and methods used to address the research questions outlined in Section 1.\nWe first describe the data collection process, which is followed by a presentation of the methods used for the generative\nexplanation model. We then explain how the dataset used to train the metric learning model was annotated, and outline\nthe training methodologies employed."}, {"title": "Explanation Generation", "content": "In this section, we focus on the explanation generation component of our architecture. This includes detailing the data\npreparation process and the methods used for training the various explanation generation models."}, {"title": "Data preparation and analysis", "content": "Motivated by the limitations highlighted in Section 2, we created our own explanation generation dataset, with\nthe initial step involving the collection of fact checks of various claims carried out by journalists, using Google's\nFactCheck API. Specifically, we targeted the following three fact verification outlets: bbc.co.uk, factcheck.org\nand fullfact.org. Entries consist of (a) the claim to be checked, (b) the verdict of its veracity as a free-form string,"}, {"title": "Methods", "content": "Given a claim and some evidence, our approach is to jointly generate the veracity of the claim and provide a justifica-\ntion for it. Specifically, we employed a sequence-to-sequence model which takes as input a sequence $S$, obtained by\nconcatenating the given claim $C$ with the evidence $E$ and separating them using a new line (i.e., \u201c\\n\u201d). Thus, $S$ takes the\nfollowing form: \u201csummarize: C \\n E.\u201d In selecting a model for our sequence-to-sequence task, we decided to initially\nchoose t5, a unified Text-to-Text Transfer Transformer (Raffel et al., 2020), which achieved strong results on tasks\nsuch as summarisation or classification. Therefore, the input sequence $S$ is fed to t5, and, given its auto-regressive"}, {"title": "Metric learning model", "content": "In this section, we outline our approach for the metric learning model. Similar to the explanation generation model,\nwe start by detailing the data annotation process, followed by the methods used for training the models."}, {"title": "Data collection and annotation", "content": "In order to prepare our dataset for the metric learning model, we took the explanations generated by different optimised\nmodels including t5-base, LED-base and t5-large. The motivation behind our approach was to create a diverse"}, {"title": "Methods", "content": "We approached the prediction of overall quality as a regression task, while the prediction tasks for the other\nfour quality dimensions were treated as binary classification tasks, training a separate model for each dimension.\nDue to the presence of noise in the collected judgements (see Section 4.3), we experimented with different selection\nstrategies and trained a combination of models to predict the collected crowd judgements. The selection strategies were\nbased on agreement between an annotator and all their annotation peers averaged across their overall annotations,\nor per annotation question. Here, a score of 0.5 is the expected agreement of a random answering strategy (for\nbinary questions). Thus, we experimented with higher thresholds such as 0.69 (i.e., on average, more than 2 out\nof 3 annotations are agreed upon) and 0.75 (i.e., 3 out of 4 annotations agreed upon on average). The results of\nthese experiments are detailed in Section 4.3. Additionally, to obtain training and evaluation data, we averaged the\njudgements of those annotators deemed eligible based on the selection strategy and split the dataset into 2100 training\nand 521 evaluation examples.\nWe experimented with transformer-based DeBERTa-base and DeBERTa-xxlarge models to investigate the extent\nto which model scale influences the ability to mimic human judgements. The choice of DeBERTa over BERT or\nROBERTa was influenced by its superior performance on the majority of natural language understanding (NLU) tasks\nincluding natural language inference, e.g., on the MNLI dataset by Williams et al., (2018), and binary classification,\ne.g., on the SST-2 dataset by Socher et al., (2013). The improved performance is attributed to the use of disentangled\nattention mechanism (He et al., 2020). Additionally, when employing DeBERTa for regression, the output layer has\nonly one neuron, without applying an activation function, and the cross-entropy loss is replaced by the mean squared\nerror (MSE). To obtain a more comprehensive understanding of the model's performance, we will evaluate it with both\nmean absolute error (MAE) and MSE metrics."}, {"title": "Evaluation and Results", "content": "In this section, we present the experimental setup that was employed in this research as well as the results of evaluating\nthe different approaches described in the previous section."}, {"title": "Experimental setup", "content": "Following the convention used in previous NLG work, especially in summarisation and machine translation, we report\nthe performance obtained by each of the generative approaches in terms of Recall-Oriented Understudy for Gisting\nEvaluation (ROUGE) metric. Specifically, we use ROUGE-1, which measures the overlap of unigrams between the"}, {"title": "Explanation generation", "content": "As described in Section 3.1.2, our preliminary experiments made use of t5-base and LED-base transformer models.\nThus, we firstly fine-tuned t5-base on the full explanation dataset and we found that the generative model is perform-\ning well on the task of contextualising the verdict, when given good evidence in the form of the full article. Table 1\nbelow shows five randomly selected explanations generated by the model optimised on the full explanation, and the\ncorresponding ground truth explanations. Generally, the model demonstrates a capability to perform claim verification\nand provides a sensible explanation, although omitting some details at times."}, {"title": "Metric learning model", "content": "Considering the results obtained in the previous section (see Table 5) and the methodology for our metric learning,\noutlined in Section 3.2.2, we opted to use the explanations generated by different models, optimised on the full expla-\nnation dataset, as input for annotation. We performed this task by following the procedure outlined in Section 3.2.1.\nThe overall agreement statistics for the crowdsourced annotations of the generated explanations are reported in Table 6.\nWe note that the perfect agreement scores were low, which hints at the subjectiveness of the task (e.g., evaluating\nconvincingness) as well as the presence of noise in the annotations. Investigating the average agreement per annotator,\nwe find that some annotators performed only marginally better than the random selection strategy (see Figure 5).\nAdditionally, the agreement scores tend to be even lower if averaging per question.\nQualitatively investigating the annotated data, we found examples of noisy annotations (e.g., annotators indicating that\nthe explanation does contradict itself while being convincing at the same time). To reduce this noise, we regarded only\nannotations of those annotators whose average agreement was higher than 0.75. In some cases, this led us to situations\nwithout agreement on the binary questions (where one annotator was excluded, and the remaining two did not reach\na consensus). In alleviating this issue, we took inspiration from the recent finding that large language models (e.g.,\nGPT3 and ChatGPT) can perform annotation tasks at a comparable performance with lay annotators (Kalyan, 2023),\nand used OpenAI's ChatGPT-3.5-turbo API to perform a tie break on the objective questions aimed at NLG quality\n(i.e., contradiction and hallucination). Based on Figure 5, it is evident that, on average, ChatGPT's agreement surpasses\nthat of two-thirds of the crowd-workers. It is important to note that, for subjective questions (i.e., convincingness and\noverall quality rating), we refrained from relying on ChatGPT annotations for metric optimisation."}, {"title": "Discussion", "content": "The findings of this study demonstrate significant advancements in the area of fact-checking explanation generation\nand evaluation. Firstly, our results indicate that it is feasible to train models to generate claim explanations effectively,\nwhich receive positive evaluations from human annotators regarding their quality. Secondly, we have shown that it is\npossible to develop models capable of directly predicting the quality of these generated explanations. Notably, LLMs,\nsuch as ChatGPT, exhibit a considerable degree of success in this task in a zero-shot setting, as evidenced by the\nhigh overall agreement of ChatGPT with human crowd-workers presented in Figure 5 and discussed in Section 4.3.\nThis suggests that such models could potentially be employed to assess the quality of generated outputs, reducing the\nreliance on annotations. These findings align with recent research on using LLMs as judges for generated outputs,\nsuch as the more general PanelGPT (Sun et al., 2023) and Li et al., (2024)'s work focussed on argument mining.\nThey have also explored the capabilities of large language models in evaluating generated texts. While these results\nare promising, further research is needed to fully understand the implications and potential applications of these\nadvancements specifically for evaluating the quality of explanations from a human perspective.\nThe contribution from our study can also be viewed from the angle of learning human preferences in natural lan-\nguage processing tasks. Unlike approaches that focus on developing automated metrics and maximising their corre-\nlations with human judgements (Sellam et al., 2020), our work investigates the ability to directly learn from human\njudgements. This approach aligns with recent advancements in preference learning, such as the Direct Preference\nOptimization (DPO) method (Rafailov et al., 2024), which learns latent human preferences from pairwise rankings\nof two model outputs. Our method learns the preferences directly-albeit based on pre-defined quality criteria in-\nstead of pairwise comparison\u2014which is similar to the use of a learned reward model for optimising LLMs to follow\ninstructions (Ouyang et al., 2022). These approaches represent a shift towards more human-centered evaluation and\noptimisation in generative model training. While we have not directly incorporated the human quality judgements\ninto the training of generative models due to the low number of annotated samples, an intriguing direction for future\nresearch would be to scale up these annotation efforts and directly integrate them into the training process."}, {"title": "Conclusions and Future Work", "content": "In this paper, we present our work on generating human-accessible explanations as well as a human-centered ap-\nproach for automatically evaluating the generated explanations. To facilitate the development and evaluation of our\napproaches, two novel datasets were developed: one for generating explanations within the context of fact-checking,\nand the other for the automatic evaluation of these explanations, using human annotations.\nBased on our results, we revisit and answer the research questions presented in Section 1:"}, {"title": "RQ1: How effectively can transformer-based models generate human-accessible explanations?", "content": "As shown by the qualitative analysis in Table 1 and the quantitative results in Table 5, the transformer-based\nmodels are effective in generating an explanation within the context of fact checking, when presented with\ngood evidence. However, some details are omitted at times. Conversely, when noisy evidence is supplied\ninstead, the models' performance decreased significantly (Table 2), showing signs of input copying and self-\ncontradictions. Furthermore, our empirical results show that there is a correlation between increasing dataset\nsize and model performance, as evidenced in Table 2."}, {"title": "RQ2: To what extent can the evaluation of fact-checking explanations be automated to align with human judgements\nacross various qualitative dimensions?", "content": "Based on the results presented in Figure 6, it can be seen that automating the evaluation of fact-checking\nexplanations to align with human judgements across different dimensions is feasible, but challenging.\nTransformer-based models, particularly DeBERTa-xxlarge, when fine-tuned on our annotated dataset, show\nmoderately strong correlations with human ratings primarily for objective dimensions such as article\ncontradiction, self-contradiction and hallucination. However, automating the assessment of"}, {"title": "RQ2: To what extent can the evaluation of fact-checking explanations be automated to align with human judgements\nacross various qualitative dimensions?", "content": "more subjective dimensions like convincingness and overall quality of explanations remains a chal-\nlenge, with only marginal performance improvements over statistical baselines. This indicates that while au-\ntomation can be effective in certain dimensions, achieving perfect agreement with human judgements across\nall qualitative dimensions is a difficult task, highlighting the need for ongoing research in the area of aligning\nmodel outputs with human standards.\nThe implications of our research are two fold. First, from a theoretical perspective, our research advances the field\nof explainable artificial intelligence (XAI) by developing models capable of generating human-understandable expla-\nnations for fact-checking verdicts. This addresses a crucial gap in most fact-checking approaches which primarily\nfocus on providing only a verdict, without an explanation. By integrating human-centered evaluation methods, our\nwork emphasises the importance of the sought-after alignment with human assessments (Schlegel, Mendez-Guzman\nand Batista-Navarro, 2022) and how AI systems can be made more interpretable and accountable. Second, from a\npractical perspective, our work enhances the reliability and transparency of AI-driven fact-checking systems by en-\nabling them to provide understandable explanations for their decisions. This contributes to promoting users' trust and\nencourages critical engagement with the rationale behind the verdicts, addressing misinformation more effectively.\nSuch advancements have the potential to improve the landscape of digital information verification. Additionally, the\nmetric learning models we developed have the potential to improve the efficiency and effectiveness of AI systems in\ndelivering reliable and trustworthy explanations for their decisions. By automating this evaluation, the model aids in\nsignificantly reducing the time and resources needed for manual assessment.\nOne limitation of our study is the reliance on text-only evidence for generating explanations, which overlooks the\nincreasingly multi-modal nature of information online. In today's digital era, misinformation often spreads through\nvarious media, which usually include images, videos and audio, making it crucial for fact-checking systems to consider\nthese modalities. Such systems could provide more comprehensive and nuanced explanations, hence, improving their\neffectiveness. Future work should explore integrating multi-modal inputs to better reflect real-world information,\nsimilar to the work presented by Yao et al. (2023). Additionally, while our metric learning models demonstrate\npotential, their agreement with human judgement varies across different quality dimensions. This underscores the\nneed for further refinement of these models, particularly in capturing the nuances of human judgements."}, {"title": "Statistical Analysis for Metric Learning Models", "content": ""}, {"title": "Hyper-parameters used for model training", "content": ""}]}