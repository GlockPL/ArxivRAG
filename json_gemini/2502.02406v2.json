{"title": "LV-XATTN: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models", "authors": ["Tzu-Tao Chang", "Shivaram Venkataraman"], "abstract": "Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique enabling support for longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 5.58\u00d7 end-to-end speedup compared to existing approaches.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have shown exceptional performance in language processing tasks that involves long context, such as long document understanding (Sun et al., 2024; Bertsch et al., 2023) and repository-level code completion (Shrivastava et al., 2023; Zhang et al., 2023). Their strong reasoning capabilities have motivated efforts to expand beyond language inputs, giving rise to multimodal large language models (MLLMs). These models can process and reason about other modalities, such as visual inputs, enabling applications like video understanding (Qian et al., 2024; Islam et al., 2024; He et al., 2024) and image processing (Guo et al., 2024; Li et al., 2023b).\n\nA common approach to integrating visual inputs into LLMs is through cross-attention (Alayrac et al., 2022; Lauren\u00e7on et al., 2024; Li et al., 2023a; Ye et al., 2024; Grattafiori et al., 2024), where queries derived from the text input interact with keys and values derived from the visual inputs. This enables effective fusion of multimodal information. In MLLMs, cross-attention layers are inserted between language model blocks, enabling the LLM to process intermediate representations that are integrated with visual information.\n\nHowever, the memory requirement of cross-attention layers is a limiting factor for applications involving large visual inputs, such as long video understanding. For example, in mPLUG-Owl3 (Ye et al., 2024), cross-attention applied to a text of sequence length 2048 and a 23-minute video sampled at 1 frame per second (fps) requires over 220GB of memory. This exceeds the memory capacity of existing accelerators, necessitating the distributed computation of the attention operation across multiple workers.\n\nExisting distributed attention approaches can be categorized as two classes: head-parallelism and sequence-parallelism. Head-parallelism methods such as Deepspeed-Ulysses (Jacobs et al., 2024) and Megatron-LM (Korthikanti et al., 2023a) partition the computation along the head dimension of multi-head attention. Consequently, maximum degree of parallelism is capped by the number of heads used in multi-head attention. This translates to an upper bound in terms of memory capacity, preventing them from processing longer visual inputs which have memory demands beyond this (Table 6 in Section 4). In addition, the number of workers must be divisible by the total number of heads to ensure a balanced load across workers. Otherwise, resource underutilization may occur due to stragglers. On the other hand, sequence parallel methods such as Ring Attention (Liu et al., 2024a) partition the computation along the input sequence dimension, overcoming the limitation of head-parallelism methods. However, when applied to cross-attention with large visual inputs, these approaches suffer from large communication overheads even after overlapping computation and communication. Figure 2 shows that cross-attention operations distributed with Ring Attention (Liu et al., 2024a) can account for up to 87% of the iteration time, despite comprising only 2% of the total parameters.\n\nIn this work, we present LV-XAttn, a distributed, exact cross-attention mechanism that employs sequence-parallelism with minimal communication overhead. Our main observation is that while keys and values derived from visual inputs are large, the queries derived from text input are typically small in MLLMs. For example, in the video understanding benchmark Video-MME (Fu et al., 2024), an input processed with mPLUG-Owl3 models results in an average sequence length of 1,739,394 for keys and values and 5,514 for queries, when frames are sampled at 1 fps. Based on this, LV-XAttn organizes each worker to locally store a partition of the large key and value blocks, while small query blocks are transmitted between workers to compute the attention output in a blockwise fashion. This significantly reduces the communication volume compared to Ring Attention. For instance, with the Video-MME benchmark, LV-XAttn reduces communication volume to just 0.48% of that required by Ring Attention. Furthermore, the reduced communication can be effectively overlapped by computation, allowing distributed cross-attention to be performed without incurring any communication overhead.\n\nTo further enable the processing of longer visual inputs, we employ an activation recomputation technique that is specific to MLLMs. In standard attention implementations, activations including queries, keys, and values need to be saved for backward pass (Korthikanti et al., 2023b). Storing the large key and value tensors for every cross-attention layer introduces additional memory pressure. We observe that since cross-attentions in MLLMs share input visual tokens, we can maintain a single copy of the visual tokens accessible to all cross-attention layers and recompute activations during the backward pass. This allows us to process up to 1.6\u00d7 longer visual inputs with just less than 8% overhead.\n\nWe perform comprehensive evaluation of LV-XAttn on mPLUG-Owl3 models and OpenFlamingo (Awadalla et al., 2023) models across multiple cluster configurations, including setups with A100 and A30 GPUs. LV-XAttn speeds up the cross-attention operation by up to 45.85\u00d7 and overall model iteration time by up to 5.58\u00d7 compared to Ring Attention. By minimizing communication volume and further overlapping communication with computation, we demonstrate that LV-XAttn incurs less than 0.42% overhead compared to a no-communication baseline."}, {"title": "2. Background", "content": "Cross-attention Cross-attention (Vaswani et al., 2017) is a variant of self-attention to model interactions between different sequences. The input to cross-attention consists of two sequences $x \\in \\mathbb{R}^{S_Q \\times d_{embed}}$ and $y \\in \\mathbb{R}^{S_{KV} \\times d_{embed}}$, where $S_Q$ and $S_{KV}$ denote the sequence lengths of $x$ and $y$, respectively, and $d_{embed}$ is the embedding dimension. The input sequence $x$ is multiplied with the projection matrices $W_Q \\in \\mathbb{R}^{d_{embed} \\times d}$ to obtain the queries $Q \\in \\mathbb{R}^{S_Q \\times d}$, while the input sequence $y$ is multiplied with the projection matrices $W_K, W_V \\in \\mathbb{R}^{d_{embed} \\times d}$ to obtain the keys and values $K, V \\in \\mathbb{R}^{S_{KV} \\times d}$, where $d$ is the hidden dimension. The attention output $O \\in \\mathbb{R}^{S_Q \\times d}$ is then computed as:\n\n$O = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V$\n\nMultimodal Large Language Models As LLMs continue to evolve, researchers are investigating how to incorporate vision and other modalities into these models. One common way is to embed cross-attention layers into the language model. This design has been adopted by a number of models including Flamingo (Alayrac et al., 2022), Otter (Li et al., 2023a), mPLUG-Owl3 (Ye et al., 2024), IDEFICS (Lauren\u00e7on et al., 2024), and LLama 3-V (Grattafiori et al., 2024). Broadly, these models follow the architecture illustrated in Figure 1. They include a visual encoder, a projection layer, and an LLM. Cross-attention layers are interleaved between the layers of the LLM. Language inputs are fed directly into the LLM and the resulting intermediate representations are passed to the cross-attention layers as $x$. Visual inputs are processed by the visual encoder and the projection layer to produce visual tokens, which are then passed to the cross-attention layers as $y$, enabling the incorporation of visual information."}, {"title": "3. LV-XAttn: Distributed Cross-Attention with Minimal Communication Overhead", "content": "In this section, we introduce LV-XAttn, our method for efficiently distributing the cross-attention operation with minimal communication overhead. We also present an activation recomputation technique specific to MLLMs to reduce memory pressure, enabling the processing of longer visual contexts."}, {"title": "3.1. Method", "content": "The primary observation that motivates our work is that in applications involving large visual inputs, the size of the query block is typically much smaller than that of the key-value blocks. For instance, in the widely-used video understanding benchmark, Video-MME (Fu et al., 2024), the average text prompt for long videos consists of $S_Q$ = 3128 words, including the question, options, answer, and subtitles. On the other hand, videos have an average duration of 2,386 seconds; each frame is encoded by the visual encoder and projection layer in an MLLM into multiple visual tokens. For example, mPLUGOwl-3 generates 729 visual tokens per frame. With a sampling rate of 1 fps, each video results in a sequence length of $S_{KV}$ = 1739394. As a result, distributed attention mechanisms that involves movement of key-value blocks incur substantial communication overhead.\n\nTo address this, we propose LV-XAttn, which keeps the large key-value blocks locally on each worker, while smaller query blocks, attention blocks, and necessary softmax statistics are exchanged among workers in a ring-style fashion. This is illustrated in Figure 3. During each round, each worker i computes attention using its local key-value blocks $K_i$ and $V_i$ and query blocks $Q_j$ received from peers. This computation generates partial attention blocks $\\Delta O$ and partial softmax statistics $\\Delta m$ and $\\Delta l$. The worker then updates the received attention block $O_j$ and softmax statistics $m_j$ and $l_j$ by rescaling them using $\\Delta O$, $\\Delta m$, and $\\Delta l$. The worker then sends $Q_j$, $O_j$, $m_j$, and $l_j$ to the next worker in the ring topology and receives $Q_{j-1}$, $O_{j-1}$, $m_{j-1}$, and $l_{j-1}$ from the previous worker. After n rounds, the computed attention block $O_i$ and softmax statistics $m_i$ and $l_i$ are returned to worker i.\n\nOverlapping Computation and Communication To further reduce communication overhead, we can overlap the attention computation with data transmission between workers. While performing attention computation with $Q_j$, $K_i$ and $V_i$, worker i also does the following in parallel\n\n\u2022 Receive $O_j$, $m_j$ and $l_j$ from worker i \u2013 1, which are needed for rescaling in this round.\n\n\u2022 Receive $Q_{j-1}$ from worker i \u2013 1, which is needed for attention computation in the next round."}, {"title": "Runtime Analysis", "content": "Let $f(\\text{query size}, \\text{key-value size})$ represent the time required to perform the forward-pass attention computation, and let $\\text{comm}(\\text{tensor size})$ denote the time to transmit a tensor. In LV-XAttn, $Q_i$, $O_i \\in \\mathbb{R}^{\\frac{S_Q}{n} \\times d}$ and $m_i, l_i \\in \\mathbb{R}^{\\frac{S_Q}{n}}$ are transmitted during each round. This results in a per-round runtime of:\n\n$\\max \\left(f(\\frac{S_Q d}{n}, \\frac{S_{KV}d}{n}), comm(2 \\cdot \\frac{S_Q d}{n} + 2 \\cdot \\frac{S_Q}{n})\\right) \\quad(1)$\n\nIn contrast, Ring Attention transmits key-value blocks $K_i, V_i \\in \\mathbb{R}^{\\frac{S_{KV}}{n} \\times d}$ in each round, leading to a per-round runtime of:\n\n$\\max \\left(f(\\frac{S_Q d}{n}, \\frac{S_{KV}d}{n}), comm(2 \\cdot \\frac{S_{KV}d}{n})\\right) \\quad(2)$"}, {"title": "3.2. Activation Recomputation for MLLM", "content": "In standard attention implementation, during forward pass computation, input tensors $Q_i$, $K_i$, $V_i$ and output tensors $O_i$ and $L_i$ are saved for backward pass, where $L_i = m_i + \\log l_i$ (Dao, 2024). However, storing large key-value blocks $K_i$ and $V_i$ increases memory usage, thereby limiting the maximum number of visual inputs that can be processed. For instance, in the case of mPLUG-Owl3-7b with a 4096-frame video, storing $K_i$ and $V_i$ takes 79.73GB per cross-attention layer.\n\nTo address this, we observe that while language features $x$ differ across cross-attention layers as they pass through various LM blocks, the visual features $y$ remain unchanged throughout all cross-attention layers, as they are only fed into the cross-attention layers. Thus, instead of storing key-value blocks for each cross-attention layer, we propose to keep a single copy of visual features $y$ that can be accessed by all cross-attention layers. During the backward pass, $y$ is projected to recompute key-value blocks $K_i$ and $V_i$. With $Q_i$ also being recomputed, we only need to save $x$, $O_i$, and $L_i$ during each cross-attention forward pass.\n\nAs demonstrated in the ablation study in Section 4.4, this approach incurs an overhead of less than 8% while enabling the system to handle 1.6\u00d7 more visual inputs."}, {"title": "4. Evaluation", "content": "4.1. Experimental Setup\n\nModel Setup We evaluate our methods and baselines on 5 models shown in Table 2. Following their default configuration, each frame is encoded into 729 visual tokens for the mPLUG-Owl3 models and 64 visual tokens for the OpenFlamingo models. This implies that given the same amount of memory capacity, we can fit more frames to OpenFlamingo models than mPLUG-Owl3 models.\n\nFor all models, a special token <image> must be included in the text prompt for each frame. Consequently, the length of the text prompt must be at least equal to the number of frames in the visual input.\n\nWe use a batch size of 1 and fully sharded tensor parallelism for all models to enable a larger context length.\n\nCluster Setup We evaluate our method and baselines on the following configurations: (1) A 16-GPU cluster, each node equipped with 4 A100 80GB GPUs, with the GPUs within a node interconnected via NVLink and a cross-node bandwidth of 25 GB/s, representing a typical setting for cross-node training of up to millions of tokens. (2) An 8-GPU cluster, each node equipped with 1 A30 24GB GPU, with a cross-node bandwidth of 1.25 GB/s, representing a more resource-constrained setup with slower interconnect bandwidth. (3) A 12-GPU cluster, each node equipped with 3 A100 40GB GPUs, with the GPUs interconnected via 64 GB/s PCIe and a cross-node bandwidth of 25 GB/s, used for smaller-scale case studies and ablation studies.\n\nBaselines For our method, we use LV-XAttn for the cross-attention layers and Ring Attention for the LM blocks. Our primary baseline is the setup where Ring Attention is used for both the cross-attention layers and LM blocks. We apply our activation recomputation technique to both of these settings for enabling longer context length. We also compare against Deepspeed-Ulysses (Jacobs et al., 2024), which employs sequence parallelism for non-attention layers and head parallelism for attention layers. All methods uses Flash Attention."}, {"title": "4.2. Comparison with Ring Attention", "content": "Table 3 shows the per iteration time of 5 models using LV-XAttn and Ring Attention on 16 A100 80GB GPUs. For the mPLUG-Owl3 models, LV-XAttn speeds up the cross-attention operation by 7.04 \u2013 15.32\u00d7. Since the cross-attention operation accounts for the majority of the total iteration time when using Ring Attention, this reduction results in a significant total iteration speedup of 3.3 \u2013 5.58x. For the OpenFlamingo models, which process a larger number of frames and thus have longer text lengths (due to the inclusion of a special token <image> per frame) and larger $S_Q$, the speedup is less pronounced, LV-XAttn achieves 1.47 -2.16x speedup on the cross-attention operation and 1.16 \u2013 1.92\u00d7 speedup on the total iteration time. Additionally, OpenFlamingo-3b, with denser cross-attention layers, spends a larger portion of its time in cross-attention compared to OpenFlamingo-9b when using Ring Attention. Consequently, the speedup in cross-attention translates to a more substantial end-to-end speedup for OpenFlamingo-3b.\n\nTable 4 shows the same experiment on 8 A30 24GB GPUs. We have smaller text lengths and fewer frames due to the smaller memory capacity. In this setup, the speedup for cross-attention operation is greater than that on 16 A100 GPUs: 20.6 \u2013 45.85x for the mPLUG-Owl3 models and 3.977.2x for the OpenFlamingo models. This is due to smaller query block sizes (shorter computations favors computation-bound LV-XAttn) and slower interconnect bandwidth (longer communication hurts communication-bound Ring Attention), as shown in Table 1. However, the larger cross-attention speedups do not translate into a larger total speedup, as the portion of time spent on cross-attention layers decreases due to slower self-attention layers in LM blocks (caused by the slower interconnect). Despite this, the total speedup remains 1.37 \u2013 3.45\u00d7 for the mPLUG-Owl3 models and 1.04 \u2013 2.22\u00d7 for the OpenFlamingo models."}, {"title": "4.3. Comparison with DeepSpeed-Ulysses", "content": "For Deepspeed-Ulysses, each attention operation involves two all-to-all communications: one before the computation to gather input query, key and value blocks, and another afterward to distribute attention output along the sequence dimension. The first all-to-all is expensive as it involves communicating the large key-value blocks. To see this, we compare Deepspeed-Ulysses with LV-XAttn on mPLUG-Owl3-2b using the cluster with A100 80GB GPUs. As shown in Table 5, LV-XAttn achieves 1.21 \u2013 1.55\u00d7 speedup compared to Deepspeed-Ulysses.\n\nIn addition, without activation recomputation, the larger memory footprint of Deepspeed-Ulysses limits its ability to process large visual inputs. When using the OpenFlamingo-3b model on the cluster with A30 24GB GPUs, Table 5 shows that LV-XAttn is able to process up to 4\u00d7 longer text and visual inputs compared to Deepspeed-Ulysses.\n\nNotably, the head parallelism in Deepspeed-Ulysses restricts both its scalability and flexibility: the maximum degree of parallelism is limited by the number of heads, and the number of heads has to be divisible by the number of workers."}, {"title": "4.4. Ablation Study", "content": "Overlapping Communication and Computation Figure 5 shows the time spent on cross-attention in OpenFlamingo-3b using Ring Attention and LV-XAttn, with and without overlapping communication and computation, on 6 A100 40GB GPUs. While overlapping reduces the runtime for Ring Attention, its effect is limited as the large communication overhead of key-value blocks cannot be fully hidden by computation. In contrast, LV-XAttn reduces communication time by transmitting significantly smaller query, output, and softmax statistics blocks. The overlapping further hides the communication time, enabling distributed attention with no communication overhead.\n\nActivation Recomputation Figure 6a and 6b show the iteration time for running mPLUG-Owl-7b and OpenFlamingo-3b on a single node with 3 A100 40GB GPUs, with and without employing activation recomputation for cross-attention layers. By omitting the saving of large key-value blocks, the reduced memory consumption enables the processing of a larger number of frames, increasing by 1.6\u00d7 and 1.5\u00d7 for mPLUG-Owl-7b and OpenFlamingo-3b, respectively, with a negligible overhead of less than 8%."}, {"title": "5. Related Work", "content": "Multimodal Large Langauge Models There are two main classes of MLLM designs. The first design concatenates tokenized visual inputs with text tokens and feeds them into the LLM. Models such as LLaVA (Liu et al., 2023), InternVL (Chen et al., 2024), NVILA (Liu et al., 2024b), and MiniGPT-4 (Zhu et al., 2024) follow this paradigm. Although this approach naturally extends text-only LLMs, the large number of tokens significantly slows down both training and inference (Ye et al., 2024; Grattafiori et al., 2024). The second design relies on cross-attention mechanisms, where cross-attention layers are inserted between LLM layers to incorporate visual features into its intermediate representations. Models such as Flamingo (Alayrac et al., 2022), IDEFICS (Lauren\u00e7on et al., 2024), Otter (Li et al., 2023a), mPLUG-Owl3 (Ye et al., 2024) and Llama 3-V (Grattafiori et al., 2024) adopt this strategy. While this method avoids processing a large number of visual tokens through the LLM backbone, cross-attention layers remain computationally expensive with current sequence-parallel approaches (Grattafiori et al., 2024). In this work, we introduce LV-XAttn to address this bottleneck.\n\nMemory-efficient Attention The attention operation has a memory complexity that scales quadratically with sequence length, limiting its scalability for longer contexts. Approximate methods (Kitaev et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020; Choromanski et al., 2021; Ding et al., 2023) and compression techniques (Chevalier et al., 2023; Munkhdalai et al., 2024) reduce memory requirements by sacrificing some model quality. For exact attention, FlashAttention (Dao et al., 2022; Dao, 2024) proposes block-wise computation, which reduces memory complexity to linear while providing runtime speedups by minimizing I/O be-"}, {"title": "6. Conclusion", "content": "We introduced LV-XAttn, a distributed, exact cross-attention mechanism for MLLMs with minimal communication overhead. By storing large key-value blocks locally on each worker and transmitting only smaller query blocks, LV-XAttn significantly reduces communication volume, which can be fully hidden by computation. Additionally, the activation recomputation technique reduces memory usage, enabling the processing of longer visual inputs with minimal overhead. Our evaluation demonstrates that LV-XAttn speeds up MLLM iteration by up to 5.58x and enables the processing of visual inputs up to 1.6x longer."}, {"title": "A. Comparison of LV-XAttn and Ring Attention for General Use Case", "content": "We have shown that for applications with large $S_{KV}$ and small $S_Q$ such as long video understanding, LV-XAttn achieves significant speedup over Ring Attention. Here, we provide a more in-depth analysis that generalizes to a broader range of cases.\n\nFigure 7 plots the theoretical speedup of LV-XAttn over Ring Attention for general $S_Q$ and $S_{KV}$. When $S_{KV}$ is large enough (above the horizontal bright red line), the transmission of $O_i, Q_i, m_i$ and $l_i$ in LV-XAttn is hidden by computation, making LV-XAttn compute-bound. On the other hand, when $S_Q$ is not too large (to the left of the vertical dark red line), the transmission of $K_i$ and $V_i$ are too large to be hidden by computation, making Ring Attention compute-bound. Their intersection (the top-left quadrant) represents the typical MLLM use case with large visual inputs and small text prompt.\n\nFor smaller visual inputs, the reduced $S_{KV}$ causes LV-XAttn to also become communication-bound (the bottom-left quadrant). When both LV-XAttn and Ring Attention are communication-bound, their relative speed depends on communication volume: LV-XAttn sends $2 \\cdot \\frac{S_Q d}{n} + 2 \\cdot \\frac{S_Q}{n}$, while Ring Attention sends $2 \\cdot \\frac{S_{KV}d}{n}$. Roughly, when $S_{KV} > S_Q$, LV-XAttn still remains faster than Ring Attention. For MLLMs, each image is encoded into a large number of visual tokens \u2013 e.g., 729 for mPLUG-Owl3 and 64 for OpenFlamingo \u2013 so this condition is typically satisfied, making LV-XAttn faster for MLLMS in general.\n\nThis also suggests that for self-attention, where $S_Q = S_{KV}$, Ring Attention is preferable. This is why in our experiments, we apply Ring Attention to LM blocks for all baselines. However, when the context length is very large (top-right quadrant), both LV-XAttn and Ring Attention become compute-bound, resulting in identical iteration times, making the choice between them effectively irrelevant."}]}