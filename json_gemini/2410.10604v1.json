{"title": "BrainMVP: Multi-modal Vision Pre-training for Brain Image Analysis using Multi-parametric MRI", "authors": ["Shaohao Rui", "Lingzhi Chen", "Zhenyu Tang", "Lilong Wang", "Mianxin Liu", "Shaoting Zhang", "Xiaosong Wang"], "abstract": "Accurate diagnosis of brain abnormalities is greatly enhanced by the inclusion of complementary multi-parametric MRI imaging data. There is significant potential to develop a universal pre-training model that can be quickly adapted for image modalities and various clinical scenarios. However, current models often rely on uni-modal image data, neglecting the cross-modal correlations among different image modalities or struggling to scale up pre-training in the presence of missing modality data. In this paper, we propose BrainMVP, a multi-modal vision pre-training framework for brain image analysis using multi-parametric MRI scans. First, we collect 16,022 brain MRI scans (over 2.4 million images), encompassing eight MRI modalities sourced from a diverse range of centers and devices. Then, a novel pre-training paradigm is proposed for the multi-modal MRI data, addressing the issue of missing modalities and achieving multi-modal information fusion. Cross-modal reconstruction is explored to learn distinctive brain image embeddings and efficient modality fusion capabilities. A modality-wise data distillation module is proposed to extract the essence representation of each MR image modality for both the pre-training and downstream application purposes. Furthermore, we introduce a modality-aware contrastive learning module to enhance the cross-modality association within a study. Extensive experiments on downstream tasks demonstrate superior performance compared to state-of-the-art pre-training methods in the medical domain, with Dice Score improvement of 0.28%-14.47% across six segmentation benchmarks and a consistent accuracy improvement of 0.65%-18.07% in four individual classification tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "MULTI-PARAMETRIC MRI (mpMRI) images combine various imaging modalities to comprehensively depict the structural and pathological features of the brain [42]. This approach substantially enhances diagnostic accuracy and thoroughness [40]. For instance, in the subregional segmentation of brain tumors, different MRI modalities reveal distinct lesion characteristics. Areas with high signal intensity in T1Gd images compared to T1 images and healthy white matter identify the enhancing tumor (ET), whereas the tumor core, including the ET and necrotic portion, appears with low signal intensity in T1Gd images relative to T1 images. The whole tumor encompasses the tumor core and the surrounding edematous or invaded tissue, characterized by the abnormally high signal intensity in T2-FLAIR images [4].\nCurrent methods for lesion delineation and disease classification using mpMRI heavily rely on supervised models trained on specific datasets, which limits their applicability across different datasets and tasks. Developing a multi-parametric brain MRI foundation model capable of addressing cross-modal representation and improved performance is crucial for advancing medical applications.\nHowever, challenges like the high cost of medical image annotation, the limited accessibility of large amounts of multi-modal data [45], and the lack of dedicated multi-modal pre-training paradigm with medical image domain expertise [3] complicate the construction of such a generalized mpMRI foundation model.\nObtaining a comprehensive set of modalities in mpMRI scans can be challenging due to the complexity associated with acquisition protocol and limitations in equipment capabilities. This often leads to mismatched modality data, e.g., missing modalities cross datasets, especially when the scale of the data amount increases dramatically. Furthermore, current approaches to dealing with missing modalities primarily focus on particular downstream tasks, e.g., BraTS, and have not undergone extensive investigation in large-scale cross-modal pre-training and downstream tasks [57, 14, 48, 25, 39].\nExisting research predominantly focuses on monotone image modality and utilizes cross-modal prompt learning by aligning text-based information with medical images [11, 47, 54]. However, these approaches do not directly address the challenge of effectively fusing multi-modal image information for mpMRI images. While some studies have begun exploring this issue in cross-modal mpMRI scenarios, their methodologies are often restricted to a small number of modalities and small datasets in either pre-training and downstream tasks [52, 45, 31, 41], thus leading to limited model generalizability.\nMaximizing the capability of pre-training models in downstream tasks holds paramount importance ([52, 44]) and is closely related to the designed pre-training tasks. Frequently, pre-trained proxy tasks lack direct correlations to downstream applications, e.g., masked image modeling, resulting in sub-optimal performance when blindly applied. By strategically"}, {"title": "", "content": "incorporating these links, we can guide and enhance the efficacy of the pre-training process, ensuring that learned representations align more closely with the requirements of multi-modal fusion into the unified diagnosis.\nIn this paper, we introduce BrainMVP, a novel multi-modal vision pre-training framework for the multi-parametric MRI images of the brain that demonstrates distinctive and generalizable cross-modal image representation. Initially, we gather a dataset of 16,022 publicly available brain mpMRI scans from various multi-center, multi-device sources. The dataset covers different types of brain imaging modalities, including diseased and healthy brains.\nTo address the issue of insufficient scalability (due to mismatched or missing modalities), we propose using single-modal MRI image inputs instead of fixed modality numbers in the pre-training stage. This allows for the inclusion of arbitrary numbers of modalities in the pre-training, significantly expanding the magnitude of available pre-training data. Importantly, we propose cross-modal image reconstruction via mask modeling. A key aspect of this design is the observation that different MRI modalities for the same subject often exhibit significant similarity in anatomy. By employing cross-modal reconstruction, we encourage the model to learn the disentanglement across modalities while mining the modality-invariant representations.\nToward a more generalizable pre-training model for downstream tasks, we extract condensed representations of different modality structures using modality-wise data distillation. Our approach is inspired by the technique of data distillation, which involves learning a small synthetic dataset. The performance achieved by the model training on this synthetic dataset can rival that achieved on the original large-scale datasets [49, 58, 55]. The learned synthesized dataset indeed encapsulates dense representations of the original dataset. In a similar idea, we optimize a set of learnable modality templates tailored for each individual modality. Intuitively, the distilled modality templates retain rich structural and statistical information about a specific modality while avoiding privacy leakage concerns associated with individual patients. Moreover, the learned distilled modality templates can serve as a linkage of data between pre-training and downstream tasks, i.e., as a form of information to carry and adapt between the data domains for downstream applications.\nIn summary, our contributions are three fold:\n\u2022\n\u2022\nTo the best of our knowledge, BrainMVP is the first multi-modal vision pre-training paradigm that aligns the features across modalities, targeting distinctive modality-aware representations. We collect a dataset of 16,022 mpMRI scans (over 2.4 million images) to facilitate the pre-training, covering a wide range of MRI brain image sequences in both diseased and healthy populations.\nWe design two novel proxy task settings for the multi-modal vision pre-training, i.e., cross-modal reconstruction and cross-modality contrastive learning. To improve the generalization for downstream tasks, we also introduce modality-wise data distillation to extract the template of each modality, benefiting both the pre-training and downstream tasks.\n\u2022\nWe demonstrate the superior performance gain and the enhanced generalizability by utilizing our BrainMVP pre-trained models on ten public segmentation and classification benchmarks, compared to state-of-the-art methods."}, {"title": "II. RELATED WORK", "content": "A. Multi-modal Pre-training for Natural Image Analysis\nIn the pursuit of acquiring knowledge, humans typically engage with data from multiple modalities. These diverse data sources, obtained from various perspectives, complement one another, enabling a more comprehensive understanding and facilitating the completion of more advanced semantic tasks. Recently, research in visual-language pre-training has seen significant advancements, primarily aiming to enhance the performance of various downstream related to vision through the alignment of different modal data. CLIP [36] pioneered large-scale image-text feature alignment by employing contrastive learning to maximize the mutual information between matched image-text pairs while minimizing it for mismatched pairs. Subsequent improvements, as noted in works like [35, 15, 30], have demonstrated robust generalization and zero-shot reasoning capabilities achieved through cross-modal knowledge alignment.\nAnother type of multi-modal pre-training focuses on the fusion of different modal information to enhance cross-modal data understanding and address the limitations of feature richness in uni-modal data. For instance, [2] employs a gated cross-attention mechanism to integrate features extracted from a frozen vision encoder and language model. To reduce the cost of end-to-end visual-language pre-training on large-scale datasets, BLIP-2 [28] proposes leveraging off-the-shelf frozen pre-trained image and language models. It introduces a lightweight, learnable Q-Former module to bridge the gap between modalities, facilitating image-to-text transformation through a two-stage learning process. ALBEF [27] suggests aligning visual and text features before inputting them into a multi-modal Transformer network. Additionally, to enable efficient learning from noisy web data, ALBEF [27] introduces a momentum distillation method to aid model training.\nWhile the aforementioned works focus on cross-domain interactions, our research centers on multi-modal integration within a single domain, specifically developing pre-training methods for the fusion of different modalities in MRI images.\nB. Self-Supervised Learning for Medical Image Analysis\nMedical image data annotation is notably expensive, making self-supervised learning (SSL) a promising avenue for the development of efficient annotation techniques. Given the typically limited datasets available for specific medical tasks, pre-training on large-scale unlabeled data to extract highly generalizable representations is emerging as a new paradigm. Existing SSL methods related to medical images can be roughly divided into cross-domain and in-domain fashion.\n1) Cross-domain SSL: Typical multi-modal medical image self-supervised pre-training is achieved through the joint in-volvement of images and text. MGCA [46] leverages the"}, {"title": "", "content": "semantic correspondence between medical images and radiology reports across three distinct levels: pathology region level, instance level, and disease level to facilitate generalized medical visual representation learning. Additionally, a multi-modal approach [42] introduces a multi-modal puzzle task designed to enhance rich representation learning from various image modalities. By obfuscating image modalities at the data level and employing the Sinkhorn operator to frame the puzzle solution as a permutation matrix inference, this method efficiently addresses multi-modal jigsaw puzzles of varying complexity. Furthermore, [10] propose a self-supervised learning paradigm for medical images and texts, named the multi-modal masked self-coder. This method acquires cross-modal domain knowledge by reconstructing missing pixels and tokens in randomly masked images and texts.\n2) In-domain SSL: Most current SSL methods specific to medical images are based on contrastive learning and masked image modeling (MIM) to extract useful information within images. For instance, [20] introduces a geometric visual similarity pre-training framework that leverages the high topological similarity of medical images. This approach incorporates a priori information about topological invariance into the similarity metric between images and employs a proposed z-matching head to learn the similarity of semantic features at different scales. PCRLv2 [56] addresses the issue of local information loss in medical images within the contrastive learning SSL paradigm by suggesting pixel recovery and feature alignment at various scales for diverse enhancement samples. Additionally, PCRLv2 [56] recommends implementing SSL without using skip connections to avoid shortcut solutions in pixel restoration. SwinMM [50] trains several proxy tasks involving masked multi-view observation, such as image reconstruction, rotation, contrastive learning, and a novel task that exploits the consistency of multiple views to extract hidden multi-view information in 3D medical data. During the fine-tuning stage, SwinMM [50] utilizes cross-attention blocks to aggregate multi-view information. Leveraging the high structural similarity of medical images, TransVW [16] conceptualizes subregions of an image as transferable visual words and learns generic visual representations by predicting and reconstructing their region categories. Specifically, TransVW [16] identifies similar samples of the current image through nearest-neighbor classification on encoded image features and uses the four regions of these similar samples for region categorization to ensure semantic consistency. TransVW [16] also applies perturbations to transform the visual word parts, and reconstructs the transformed image to learn structured semantic representations.\nOur work integrates contrastive learning and MIM based SSL methods, and we have deviated from previous MIM based approaches by discarding masked portions or filling them with noise or zero values. Instead, we use informative images from other modalities for filling, which improves the learning efficiency and highlights the correlations between modalities.\nC. Data distillation\nData distillation is first inspired by knowledge distillation, proposed to distill the dataset in order to construct a core"}, {"title": "", "content": "subset of the data, the model is trained on the core subset to achieve a performance comparable to that of the complete data [49]. In this way, model training and data storage costs can be significantly reduced [55]. Several studies have already applied data distillation in the field of continual learning to achieve data replay to attenuate the significant oblivion of distributional bias on the knowledge capability of models. Inspired by this, we propose to use data distillation to preserve the structural statistical information of different models from pre-trained data, and to construct upstream-downstream linkage through modality-wise data distillation.\nOverall, we propose cross-modal reconstruction and modality-aware contrastive learning as the two main proxy tasks, as well as enhancing the network's learning for discriminative information through contrastive learning. Different from previous methods, we propose a novel masking strategy to learn efficient modal fusion capability through cross-modal reconstruction and build upstream and downstream correlations through data distillation to better adapt the pre-trained models to the downstream tasks."}, {"title": "III. MULTI-MODAL VISION PRE-TRAINING", "content": "As shown in Fig. 1, BrainMVP consists of three proxy tasks: cross-modal reconstruction, modality-wise data distillation, and modality-aware contrastive learning. The proposed cross-modal reconstruction (via two ways of masking and mix) module aims to achieve the disentanglement across modalities while mining the modality-invariant representations. Modality-wise data distillation is designed to learn compressed structural information for each modality from pre-trained unlabeled data while allowing the model to extract modality-wise information and learn modality-agnostic features. Furthermore, the distilled modality templates are applied to downstream tasks to establish the association of data between the pre-training and downstream domains, which helps to improve the generalization performance of the foundation model. Finally, modality-aware contrastive learning is integrated to ensure the consistency of semantic features between different masked versions of the same sample, as well as to extract discriminative information between modalities.\nA. Cross-Modal Reconstruction\nProblem Setting. Given an unlabeled dataset $D = \\{X_{im} \\in \\mathbb{R}^{D \\times H \\times W} | m \\in \\{1,...,M_i\\}, i \\in \\{1,...,N\\}\\}$. $M_i$ denotes the number of modalities in i-th sample and N represents total number of samples. Masked image modeling (MIM) first masks (with constant values, zeros, or noise, etc, denoted as $\\Phi(\\cdot)$) a large portion of $X_{im}$ to obtain a masked input $\\Phi(X_{im})$, and then reconstructs the original image from it to learn efficient representations. Specifically, let the encoder and decoder of the model be $f_{enc}(\\cdot)$ and $f_{dec}(\\cdot)$ respectively, where the model $f(\\cdot)$ is the composition of the encoder and decoder functions, i.e., $f(\\cdot) = f_{dec} \\circ f_{enc}$. MIM minimizes the following reconstruction loss:\n$L_{rec} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{M_i} \\sum_{m=1}^{M_i} ||f_{dec}(f_{enc}(\\Phi(X_{im}))) - X_{im}||_2$ (1)"}, {"title": "", "content": "Pixel-level cross-modal masking. Given a uni-modal input volume $X_{im}$ sampled from a mpMRI case (with $M_i$ modalities), cross-modal masking aims to mask a large region of $X_{im}$ with another modality image $X_{in}$ (also sampled from $X_i$, $n\\neq m$). Specifically, we first randomly mask a region of size $r \\times r \\times r$ in $X_{im}$, where $r$ denotes the size of each dimension of input 3D volumes. Then, we fill the masked region with a patch cropped with the same location and size on another modality of the sampled case. Finally, we repeat the above masking-filling operation until the proportion of masked pixels over the total input volume $(X_{im})$ pixels arrives $p^*$. Referring to [31], we empirically set $r = 8$ and $p^* = 0.8$ to learn useful representations. Details can be seen in Algorithm 1.\nCross-modal reconstruction. Let our proposed cross-modal masking strategy be $\\Phi_{modal}(\\cdot)$. Given that the masking operation masks a large portion of the image, the resulting masked input volume $\\Phi_{modal}(X_{im})$ will contain information predominantly from $X_{in}$. The extracted representation $f_{enc}(\\Phi_{modal}(X_{im}))$ will thus encode a significant amount of semantic information from $X_{in}$. Since we do not introduce skip connections between the encoder and decoder, we only reconstruct $X_{im}$ from the latent representation $f_{enc}(\\Phi_{modal}(X_{im}))$, which is a challenging task for natural"}, {"title": "", "content": "images. However, due to the high structural similarity between different modalities in mpMRI images, with strong contrasts only in certain regions, the cross-modal reconstruction can encourage the model to learn cross-modal representations and explore the correlations between different modalities. Furthermore, $\\Phi_{modal}(X_{im})$ still contains $(1-p^*)$ proportion of information about $X_{im}$, and reconstructing this part will retain some original modality information, which can help reduce the difficulty of pure cross-modal reconstruction and extract the semantic information of the $X_{im}$ image itself. Formally, the cross-modal reconstruction loss can be expressed as:\n$L_{CMR} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{M_i} \\sum_{m=1}^{M_i} ||f_{dec}(f_{enc}(\\Phi_{modal}(X_{im}))) - X_{im}||_2$ (2)\nB. Modality-wise Data Distillation\nThe primary objective of the foundation model is to extract highly generalizable latent representations. However, the proxy tasks currently used in pre-training models are often unrelated to the downstream application tasks. We attempt to introduce certain bridging components during the model pre-training stage that can guide the pre-training process to acquire the necessary specific representations. Simultaneously, we hope that these bridging components can facilitate the feature expression of the pre-trained model when applied to downstream tasks. As shown in Fig. 1, the proposed modality-wise data distillation is in conjunction with the cross-modal reconstruction process. Specifically, in the cross-modal reconstruction part, we use data either from another modality image $X_{in}$ to fill in the masked region in $X_{im}$ or from the corresponding learnable modality template.\nSpecifically, the learnable modality templates $T = \\{T_m\\}_{m=1}^S$ sized $S \\times H \\times W \\times D$ are initialized with zero, where $S$ represents the number of modalities in the pre-training datasets. Similar to cross-modal reconstruction, the image needed for filling $X_{im}$ is $T_m$ ($m$ represents the corresponding modality) instead of another modality in the modality-wise data distillation process. The remaining steps are consistent with the cross-modal reconstruction process. An example of learned modality templates is shown in the Results section"}, {"title": "", "content": "(Fig. 4), which shows a compact representation of the structural information for each modality in the pre-training datasets. It is worth noting that the random masked regions are used in the two processes for the same input, which further accelerates the learning. Let us denote the masking strategy for modality-wise data distillation as $\\Phi_{distill}$, and the corresponding loss can be expressed as:\n$L_{MD} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{M_i} \\sum_{m=1}^{M_i} ||f_{dec}(f_{enc}(\\Phi_{distill}(X_{im}))) - X_{im}||_2$ (3)\nCross-modal reconstruction and modality-wise data distillation are performed simultaneously. The model needs to learn not only the structural information of a specific modality to form the distilled modality templates but also the transformation relationship between modalities. The representations learned by our pre-trained model are considered modality-agnostic and contain fused representations of different modalities.\nC. Modality-aware Contrastive Learning\nDuring the pre-training, we also need to consider the data distribution bias across different datasets due to the variances in MRI imaging equipment, acquisition protocols, etc. It could largely enhance the generalization of pre-trained models in the downstream tasks when unseen data are applied.\nInspired by the use of contrastive learning for aligning the paired features in multi-modal pre-training schemes, e.g., InfoNCE loss in CLIP, we first compose the positive and negative pairs. A sample masked and mixed with another modality together with another sample masked and mixed with the corresponding modality template forms a positive pair. Similarly, a sample masked and mixed with another modality together with another sample (from different datasets) masked and mixed with the corresponding modality template forms a negative pair. In such a way, the model tends to learn both dataset- and modality-independent features.\nWe denote the sets obtained by encoding N data samples using the aforementioned two masking strategies as $\\{\\Phi_{modal}(X_{im})\\}_{i=1}^N$ and $\\{\\Phi_{distill}(X_{im})\\}_{i=1}^N$, respectively. As shown in Fig. 1, We use contrastive loss [36] to bring the distance between features of positive pairs closer while repelling the distance between features of negative pairs. This can be formalized as:\n$L_{CL} = \\frac{1}{2N} \\sum_{i=1}^{N} \\left[ \\log \\left( \\frac{e^{f_i \\cdot g_i / \\tau}}{\\sum_{j=0}^{N} e^{f_i \\cdot g_j / \\tau}} \\right) + \\log \\left( \\frac{e^{g_i \\cdot f_i / \\tau}}{\\sum_{j=0}^{N} e^{g_i \\cdot f_j / \\tau}} \\right) \\right]$ (4)\nwhere $f_i=f_{enc}(\\Phi_{modal}(X_{im}))$ represents current modality image masked and mixed with another modality image in the same sample, $g_i=f_{enc}(\\Phi_{distill}(X_{im}))$ represents current modality image masked and mixed with corresponding distilled modality template. $g_{j,j\\neq i}$ represents modality images from other samples (datasets) masked and mixed with corresponding modality templates. N is the number of samples and $\\tau$ is the distillation temperature."}, {"title": "", "content": "Overall loss function. In summary, the total loss function is a combination of $L_{CMR}, L_{MD}$, and $L_{CL}$:\n$L = L_{CMR} + \\lambda_{MD} \\cdot L_{MD} + \\lambda_{CL} \\cdot L_{CL}$ (5)\nwhere $\\lambda_{MD}$ as well as $\\lambda_{CL}$ are used to balance the corresponding loss term contributions, which are both set to 1.0 in the experiments for equal treatment.\nD. Distilled Modality Template for Downstream Tasks\nIn this section, we will elaborate on how the distilled modality templates obtained from pre-training can be applied in downstream tasks, as a form of data augmentation. As shown in Fig. 2, in the downstream fine-tuning stage, the distilled modality templates are frozen. Let $D_{ds} = \\{(X_i, Y_i)\\}_{i=1}^M$ denote the downstream dataset, where M represents the number of annotated samples. $X_i$ is the multi-modal MRI input volume, and $Y_i$ represents the corresponding label, which can be a segmentation map for segmentation tasks or a one-hot vector for classification tasks. Specifically, we randomly select $n$ modalities in $X_i$ and replace them with the corresponding modalities from $\\{T_m\\}_{m=1}^S$, obtaining two augmented copies $X_i'$ and $X_i''$. The encoded features of these two copies are $f_{enc}(X_i')$ and $f_{enc}(X_i'')$, respectively. Since the two embeddings are representations of the same sample with different numbers of replaced modalities, we use the L2 norm to maintain semantic consistency in the feature space.\n$L_{cons} = \\frac{1}{N} \\sum_{i=1}^{N} ||f_{enc}(X_i') - f_{enc}(X_i'')||_2$ (6)\nSubsequently, the features of the two copies are decoded to the output space to calculate supervision loss with the ground-truth annotations. The overall loss is:\n$L_{total} = \\frac{1}{N} \\sum_{i=1}^{N} L_{sl}(f(X_i'), Y_i) + L_{sl}(f(X_i''), Y_i) + \\lambda_{cons} * L_{cons}$ (7)\nwhere $\\lambda_{cons}$ is the weight of the consistency loss $L_{cons}$ term and $L_{sl}$ is the supervision loss used in segmentation or classification tasks, e.g., Dice Loss in segmentation or CrossEntropyLoss in classification.\nFor the uni-modal input scenario, instead of replacing the selected modalities with distilled modality templates, we perform a partially masking strategy like Algorithm 1 where $X_{in}$ is replaced with the corresponding distilled modality template. Then we randomly mask the uni-modal input volume twice to obtain two augmented copies of $X_{im}$, and the remaining procedures are the same as the aforementioned multi-modal scenario."}, {"title": "IV. EXPERIMENTS", "content": "A. Datasets\nPre-training Datasets We collect five publicly available mpMRI datasets for pre-training, spanning 8 modalities with a total of 3,755 cases and 16,022 scans. The task types and number of modalities for each dataset are summarized in Table I."}, {"title": "B. Experiments setting", "content": "Data Pre-processing. During the pre-training stage, data pre-processing is performed sequentially in Python based on \u039c\u039f\u039d\u0391\u0399 1.3.0\u00b9 library. The orientation of the mpMRI images is first unified to the RAS axcodes and co-registered to the same anatomical template. Subsequently, each MRI scan is resampled to an isotropic voxel spacing of 1.0mm \u00d7 1.0mm \u00d7 1.0mm using bilinear interpolation, and skull-stripping is performed as well. We linearly clip the pixel values between the 1st and 99th percentiles and re-scale them to [0, 1]. The images are then cropped into 96 \u00d7 96 \u00d7 96 voxel patches centered on either foreground or background areas, to ensure that the modality-wise data distillation is learned sufficiently. We do not apply any other data augmentation techniques.\nImplemention details. We use UniFormer [29] for pre-training and downstream tasks, benefiting from its natural multi-modal fusion capabilities. In addition, we have conducted experiments related to the UNET3D [37] architecture. We conduct all experiments using the PyTorch framework on"}, {"title": "", "content": "8 NVIDIA GeForce RTX 4090 GPUs. During the training process, we utilize the AdamW [32] optimizer with a momentum of 0.9 and the weight decay is 1e-5. We train the model for 1,500 epochs with a batch size of 3 and introduce the modality-aware contrastive learning module at epoch 900. The initial learning rate is set to 3e-4 and we employ a cosine learning rate decay strategy. Detailed hyperparameters for downstream experiments can be found in the Appendix I.\nComparison methods. We compare our BrainMVP against three different types of approaches, i.e., training from scratch, general domain SSL methods, and medical domain SSL methods. There are three mainstream medical image segmentation networks for training from scratch: UNETR [18], UNET3D [37] [37], and Swin-UNETR [17]. UniFormer [29] is a novel 3D medical image segmentation network initially developed in the field of video object detection and extensive experiments have been conducted to verify its effectiveness. The subsequent SSL methods are pre-trained on the above architectures, allowing for a fair comparison of the impact of different network architectures on the final performance. The baseline SSL methods include MAE3D [19, 9], MIM-based SimMIM [53], and contrastive learning related MoCoV3 [8] for general domain, and MG [59], TransVW [16], GVSL [20], Swin-UNETR [43], and VoCo [51] for medical domain. Specifically, two MIM-based methods in medical domain, namely, DAE [45] and M\u00b3AE [31], are also taken as comparisons.\nEvaluation metrics. For segmentation tasks, we use the Dice Score and Hausdorff distance at 95th percentile (HD95) as evaluation metrics. For classification tasks, we report accuracy (ACC), area under the curve (AUC), and F1 score for comprehensive assessment with higher metric values indicating better classification performance.\nLabel efficiency experiments. To validate if our BrainMVP, pre-trained on large-scale mpMRI image datasets, can significantly reduce annotation workload in clinical practices, particularly for handling label-deficient segmentation tasks (which incur higher annotation costs), we conduct label efficiency experiments on five segmentation and one classification datasets. Specifically, we randomly split the training labeled samples into five partitions and gradually increase the training set size by one partition at a time until reaching the full dataset size. The resulted experiments are configured with 20%, 40%, 60%, 80%, and 100% of the total training data."}, {"title": "V. RESULTS", "content": "A. Experimental Results on Ten Downstream Tasks\n1) Segmentation Results: Superior performance on tumor segmentation datasets. To evaluate the improvement of downstream segmentation performance after pre-training, we select two brain gliomas subregion segmentation datasets, BraTS-PED [24] and UPENN-GBM [6]. BraTS-PED, dedicated to pediatric glioma, comprises only 99 annotated cases, making it a challenging testbed for assessing the generalization capability of pre-trained foundation models. Consequently, initial comparative experiments are conducted on this dataset. As depicted in Table II, SSL methods tailored for medical image domains consistently outperform general SSL methods. It is worth noting that models pre-trained on natural image demonstrate poorer generalization on medical image domains. Specifically, the best average Dice Score achieved by general SSL methods based on MIM is 67.65%, which is 9.15% lower than BrainMVP's best result of 76.80%. Also, MoCoV3 [8] performs less effectively, achieving 9.16% lower in Dice Score compared to BrainMVP. This disparity arises because typical pre-training methods developed primarily for 2D image tasks often require full images or large patches as input, which is usually impractical for 3D medical images. Our BrainMVP also outperforms medical SSL methods based on mask modeling, such as M\u00b3AE [31] (76.80% vs. 74.14%) and DAE [45] (76.80% vs. 72.07%). We further validate the effectiveness of BrainMVP on UPENN-GBM [6], as shown in Table II."}, {"title": "", "content": "BrainMVP achieves an average Dice Score of 90.01% and outperforms state-of-the-art methods.\nPerformance improvement on brain structure segmentation dataset. We utilize the MRBrainS13 [33] dataset for the segmentation of normal brain structures to assess the efficacy of BrainMVP in scenarios with limited normal brain structure samples during pre-training. As detailed in Table II, our BrainMVP achieves an average Dice Score of 80.27%. In contrast, MG [59], employing multiple proxy tasks, attains 75.51%, and VoCo [51], leveraging contrastive learning, achieves 76.37%. Based on the UniFormer [29] architecture, BrainMVP surpasses all previous methods and demonstrates a notable 4.49% average Dice Score improvement over training from scratch. This underscores its robust capability to effectively improve downstream tasks performance, even under constraints with limited normal brain structure data samples in pre-training.\nStrong generalization performance on Unseen datasets. Given that our pre-training datasets primarily include normal brain structures and those afflicted with glioma, we aim to verify the generalization capabilities of BrainMVP on other types of samples. To assess this, we evaluate our BrainMVP on three datasets: BraTS-MET [34], ISLES22 [21], and VSseg [38]. For the BraTS-MET [34] dataset focusing on brain metastasis subregion segmentation, as seen in Table II, our BrainMVP achieves an average Dice Score of 73.67%. Further, BrainMVP notably outperforms existing state-of-the-art methods in medical applications, including MG [59] (63.19%), and Swin-UNETR* [43] (63.23%). In the context of the ISLES22 [21] ischemic stroke segmentation task, which involves abnormalities distinct from tumors targeted in pre-training, BrainMVP achieves substantial improvement compared to MG [59] (86.60% vs. 83.53%) and GVSL [20] (86.60% vs. 80.05%). For the VSseg [38] dataset focusing on vestibular schwannoma segmentation task, in previous methods, M\u00b3AE [31] achieves the best performance with 79.31% Dice Score, while our BrainMVP outperforms all previous methods with 83.64% Dice Score, proving the effectiveness of BrainMVP."}, {"title": "2) Classification Results", "content": "We select four distinct classification tasks to assess the generalizability of BrainMVP across diverse domains. Initially, experiments are conducted on the BraTS2018 [5", "31": "achieves the best ACC compared with other methods with 0.7895, while our BrainMVP achieves an outstanding 0.8596, surpassing the state-of-the-art methods by a large margin. For example, M\u00b3AE [31", "51": "achieves an accuracy of 0.7368, and GVSL [20", "23": "dataset to assess its ability to differentiate between healthy and diseased states, a task not represented in the pre-training datasets. Notably, the pre-training datasets comprise only a small fraction of normal brain data (12.5%). Experimental results reveal that BrainMVP displays robust generalization capabilities, achieving the highest accuracy of 0.6765 with the UNET3D [37", "20": 0.6661}]}