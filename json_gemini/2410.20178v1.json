{"title": "LLMs Can Evolve Continually on Modality for X-Modal Reasoning", "authors": ["Jiazuo Yu", "Haomiao Xiong", "Lu Zhang", "Haiwen Diao", "Yunzhi Zhuge", "Lanqing Hong", "Dong Wang", "Huchuan Lu", "You He", "Long Chen"], "abstract": "Multimodal Large Language Models (MLLMs) have gained significant attention due to their impressive capabilities in multimodal understanding. However, existing methods rely heavily on extensive modal-specific pretraining and joint-modal tuning, leading to significant computational burdens when expanding to new modalities. In this paper, we propose Path Weave, a flexible and scalable framework with modal-path switching and expansion abilities that enables MLLMs to continually evolve on modalities for X-modal reasoning. We leverage the concept of Continual Learning and develop an incremental training strategy atop pre-trained MLLMs, enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining. In detail, a novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration. Additionally, an MoE-based gating module is applied between two types of adapters to further enhance the multimodal interaction. To investigate the proposed method, we establish a challenging benchmark called Continual Learning of Modality (MCL), which consists of high-quality QA data from five distinct modalities: image, video, audio, depth and point cloud. Extensive experiments demonstrate the effectiveness of the proposed AnA framework on learning plasticity and memory stability during continual learning. Furthermore, PathWeave performs comparably to state-of-the-art MLLMS while concurrently reducing parameter training burdens by 98.73%.", "sections": [{"title": "1 Introduction", "content": "With recent advances in artificial intelligence, Large Language Models (LLMs) have demonstrated impressive capacities in language understanding and reasoning. The success of LLMs [69, 68, 56, 23] has spurred researchers to develop Multimodal LLMs (MLLMs) by integrating additional input for multimodal tasks, such as image-text understanding [16, 17, 49], audio recognition [66, 65] and 3D question answering [67, 51]. Aided by large-scale image-text paired data from the Internet [46, 89, 17, 27, 16], vision LLMs have become a thriving area in the research community. The typical framework comprises a visual encoder, a frozen or trainable LLM, and a projection module for vision-language alignment. Through stepwisely pretraining on large-scale image-text pairs and instruction tuning on specific datasets, vision LLMs exhibit promising generalization abilities on downstream applications such as detection [70], grounding [72, 71], and captioning [17, 27]. Subsequently, the LLM-based framework and training pipeline of vision LLMs serve as the basis and drive the extension to other modalities, including video [87, 88], audio [65, 66], and point cloud [51, 67]. However, these modal-specific LLMs that inject single-modal data into language models struggle to tackle the challenge of perceiving different modalities like us humans.\nTo address this issue, recent approaches [2, 3, 19, 1] extend the architecture and training strategies of modal-specific MLLMs, and try to integrate multiple modalities into a unified system. Some early attempts [19, 1] utilize specific projection modules to align image, video, and audio encoders into a frozen LLM. However, a complex training process is usually required to enhance cross-modal alignment, involving separate pretraining on uni-modal data and joint fine-tuning on multimodal data. Subsequent attempts try to enhance the scalability of MLLMs by unifying the architecture and simplifying the training process. For instance, X-InstructBLIP [2] proposes a unified projection architecture for all modalities and constructs high-quality instruction tuning data to simplify modal-specific customization and pretraining. OneLLM [3] leverages a unified encoder and projection module and introduces an incremental pretraining strategy to achieve parameter unification for a wide range of modalities. While effective, most approaches still rely on joint-modal optimization that is high-resource demanding (see Figure 1 (a)). When expanded to new modalities, the models have to re-access all the historical data and repeat the complete training process, limiting the continual extension of MLLMs.\nIn this paper, we propose PathWeave, a flexible and scalable framework with modal-path switching and expansion capabilities that enables MLLMs to continually evolve on modality for X-modal reasoning. PathWeave leverages the concept of Continual Learning (CL) and forms an incremental training pipeline on uni-modal data, eliminating the necessity for joint-modal pretraining or finetuning. To this end, we employ a pre-trained vision LLM [2] as the interface and propose a novel Adapter-in-Adapter (AnA) framework, allowing efficient extension and alignment for other modalities. We set two types of adapters in AnA, uni-modal and cross-modal, and seamlessly incorporate them to boost modality alignment and collaboration during incremental learning. Specifically, the uni-modal adapters are progressively added to the interface and optimized on the corresponding modality data, which will be frozen once trained. Meanwhile, we insert in-adapters into the previous uni-modal adapters to form cross-modal adapters, allowing the effective integration between historical knowledge and ongoing modality. Additionally, an MoE-based gating module is implemented between uni-modal and cross-modal adapters to further enhance multimodal collaboration. As shown in Figure 1 (b), our PathWeave can be flexibly implemented on the pretrained MLLMs and efficiently expand to more modalities in an incremental manner.\nTo evaluate the proposed PathWeave, we establish a challenging benchmark, namely Continual Learning of multi-Modality (MCL). It consists of data from five distinct modalities: image, video, depth, audio, and point cloud. In our setting, the modalities data are incrementally fed to the MLLMs. Thus, we leverage the commonly-used metrics from [2, 3] to investigate the precision on newly learned modalities. Furthermore, we introduce a metric to measure the forgetting rate in MCL to demonstrate the effectiveness of the proposed AnA strategy on historical modality memorization. Finally, we conduct extensive experiments to compare with state-of-the-art continual learning approaches, demonstrating that PathWeave is effective at incorporating multimodal data in an incremental manner. Moreover, our method achieves comparable performance with state-of-the-art MLLMs without requiring joint-modal pretraining or fine-tuning.\nIn summary, our contributions are summarized as follows:\n\u2022 We present an efficient and scalable framework, PathWeave, which enables MLLM to progressively expand on multiple modalities, without the need for joint-modal pretraining."}, {"title": "2 Related Work", "content": "Multimodal Large Language Models. In recent years, researchers have been exploring the potential of LLMs in multimodal perceptions, such as visual question answering [16, 27] and captioning [17, 61]. This leads to the rapid development of Multimodal LLMs [17, 16, 3, 19]. For example, LLaVA [16] utilizes a simple linear layer to project visual information into language space, enduing LLMs the ability to perceive natural scenes. Subsequently, several methods attempt to expand the supported modalities of LLMs by modifying architecture designs or training strategies. For instance, X-LLM [19] and Chatbridge [1] use modal-specific modules to extract features for multiple modalities and exploit modal-specific projection layers for multimodal alignment on a frozen LLM. However, a complex training process is usually required to enhance cross-modal alignment, which involves separate pretraining on uni-modal data and joint instruction tuning on multimodal data. Later, X-InstructBLIP [2] proposes a unified projection architecture (Q-former) for all modalities and collects large-scale, high-quality instruction tuning data to eliminate the need for uni-modal pretraining. OneLLM [3] explores parameter unification by introducing a unified encoder and projection module for a wide range of modalities. Although an incremental pretraining strategy is proposed to alleviate the high resource demand of cross-modal alignment, OneLLM still relies on cross-modal finetuning on large-scale instruction datasets. In contrast to these methods, we incorporate the continual learning concept into MLLMs and propose an incremental training strategy to allow MLLMs' modal expansion by finetuning on uni-modal data, without requiring joint-modal pretraining or finetuning. Among these approaches, X-InstructBLIP [2] is highly related to our method, as it separately tunes Q-former to align multimodal into a uniform system. However, our method designs an adapter-based expansible framework that significantly reduces the parameter training burdens by at least 98.73%.\nContinual Learning in Foundational Models. Continual Learning (CL) has been applied to large foundational models [4, 26, 52, 18], allowing them to continually acquire new knowledge. To address the forgetting issue in CL, significant efforts [73] have been made, including data replay, regularization constraints, and dynamic frameworks. Data replay-based methods [74, 75, 76, 77, 6] retain the historical data in a memory bank and mix them with new data to execute the general training process. However, the redundant historical data would incur increasing resource demand during lifelong learning. Regularization-based methods add explicit regularization terms on weights [78, 79, 80] or data [81, 82, 83, 5] to achieve a balance between historical and new tasks, which are usually used as an auxiliary trick in data-replay or dynamic methods. In contrast, dynamic methods [18, 26, 84, 85, 86] exhibit impressive expansible abilities by incrementally adding new parameters into a shared interface. Recently, the dynamic frameworks have been combined with efficient tuning techniques to achieve efficient, cost-friendly continual learning on visual-textual domain [18, 52, 26]. This inspires us to eliminate joint-modal pertaining from MLLMs by developing an efficient, scalable framework where new modalities are incrementally involved by accessing uni-modal data. To this end, we propose an adapter-in-adapter framework, which incorporates uni-modal and cross-modal adapters for efficient modality alignment and collaboration.\nTransfer learning. In the realm of Natural Language Progressing (NLP), fine-tuning large-scale models (e.g., 175B GPT-3 [23]) imposes significant burdens in both parameter complexity and time consumption. As a result, transfer learning methods [9, 58, 59, 60] have gained significant attention to facilitate the efficient adaption of LLMs on downstream applications. The techniques usually activate a small set of parameters on the frozen models while achieving comparable performance with fully-finetuned approaches. Among these methods, LoRA [9] reduces the trainable parameters through low-rank matrix decomposition, leading to the generalization of the pre-trained model on diverse downstream tasks. The success of LoRA further promotes the development of parameter-efficient transfer learning of MLLMs [16, 24, 25] and uni-modal continual learning approaches [52, 26, 62]. However, these methods cannot be directly applied to fix the proposed MCL task due to the significant variations in modality spaces. In this paper, we propose a modality continual learning method that"}, {"title": "3 Path Weave", "content": "3.1 Preliminaries\nContinual learning can empower large-scale foundation modals to constantly acquire new knowledge without accessing the entire historical data. We introduce this concept into MLLMs to form an incremental training strategy on uni-modal data called Continual Learning on Modality (MCL), eliminating the necessity of modal-specific pertaining and joint-modal datasets. Given a set of M modalities {Mm}M_1, we enforce LLMs to sequentially access and learn on each modality for question answering. Here, each modality Mm contains Nm datasets, which can be represented as Mm = {D}. More specifically, D = {im,sm, om} denotes the i-th data of the m-th modality Mm, in which i, s and o are text instruction, modality samples, and answering, respectively.\n3.2 Framework Overview\nThis work presents PathWeave, an efficient and extensible framework that empowers MLLMs to constantly evolve on modalities, without requiring modal-specific pretraining. Considering the complicity of training MLLMs from scratch, we start from a pretrained vision LLM and align other modalities in an incremental manner. The overall framework of PathWeave is illustrated in Figure 2. Specifically, we build the PathWeave on X-InstructBLIP [2], providing a unified Q-Former architecture for various modalities. Given the samples from m-th modality, a modal-specific encoder Em pretrained on the corresponding modality is first exploited for feature extraction. Then, the Q-Former Q takes the input of modality feature, learnable query qm, and instruction embedding Im for"}, {"title": "3.3 Adapter-in-Adapter", "content": "X-InstructBLIP [2] utilizes Q-Former as a unified framework to extend MLLMs' capabilities on more diverse modality reasoning, eliminating the need for modal-specific pretraining. However, instruction tuning on uni-modal data is implemented on separated Q-Formers, which leads to significant computational costs and parameter burdens when integrating more modalities. Recently, some attempts [26, 52] have demonstrated that adapters with few parameters can enhance the adaption of foundation modal on downstream tasks. Inspired by this, we leverage an effective transfer learning technique, LoRA [9], to serve as the basic unit of our AnA framework, enabling the efficient adaption of subsequent modalities during incremental learning.\nUni-modal Adapters. Given the current modality Mm, we implement uni-modal adapters Am in the pretrained Q-Former for new modal alignment. The adapters Am are inserted into different linear layers I of pretrained model in parallel. The output of layer l with adapters Am can be expressed as:\ny = Q\u0131(x) + Am (xm), (1)\nwhere x and y are the input and output embedding of l-th layer when aligning m-th modality. Am is the adapter of m-th modality in l layer, and Am (x) = Fm (Fm (x)), where Fu and Fa are the up and down projection of adapter. The uni-modal adapters are effective at acquiring modal-specific knowledge. Besides, the parallel architecture of adapters endows our system with the capabilities to flexibly switch and expand to diverse modalities.\nCross-modal Adapters. The uni-modal adapters are effective at preserving the uni-modal knowledge and alleviating the forgetting issue in long-term learning. Based on it, we introduce a modal-special in-adapter module (Fm) to form a cross-modal adapter (\u00c2m), which can help the ongoing modality learn previous knowledge and encourage inter-modality collaboration. Specifically, the in-adapters are inserted into the previously learned uni-modal adapters to effectively acquire the learned knowledge without reactivating their parameters. Then, the output of l-th layer ym after adding In-Adapter Fm can be reformulated as:\nym = Q\u0131(x) + \u2211 A\u00b2 (xm) + Am (x), (2)\ni=1\nwhere A(x) = F(Fm (F(x))), i \u2208 [1, m \u2013 1] represents the cross-modal adapters for current modality Mm. Fm is the in-adapter that is inserted into i-th frozen uni-adapters A\u00b2, which is a single linear layer with the dimension of adapters' low rank. The uni-modal and cross-modal adapters collaborate to facilitate the new modality alignment and cross-modal integration during incremental learning. Furthermore, the proposed in-adapter serves as a plug-and-play module that will not affect the performance of previously learned adapters, thereby effectively alleviating the modality forgetting.\nMoE-based Gating. Cross-modal adapters rely on in-adapters to effectively leverage historical knowledge to boost the alignment of ongoing modality. However, the output of cross-modal and uni-modal adapters are treated equally in the original Q-Former. Considering the significant gap between distinct modalities, this simple integration strategy might pose performance degradation affected by the interfering information from other modalities. To address this issue, we propose an MoE-based gating module between cross-modal and uni-modal adapters for adaptive multimodal integration. Our MoE-based gating Gm automatically assigns weights of paths Pm of different"}, {"title": "4 Continual Learning on Modality", "content": "MCL Benchmark. We establish a challenging benchmark, Continual Learning on Modality (MCL), which consists of multimodal high-quality QA data to evaluate the effectiveness of our method on continual uni-modal finetuning. These datasets are collected from five distinct modalities: image, video, depth, audio and point cloud. Based on this benchmark, our PathWeave is trained and tested along the multimodal sequence without requiring modal-specific pretraining or joint-modal finetuning. More details of the dataset list and size for each modality are illustrated in Table A6 of the Appendix.\nMCL Metrics. We formulate the metrics from two aspects to evaluate the proposed MCL strategy on multimodal reasoning. On the one hand, we use the general metrics from MLLMs [2, 3] to investigate the model's overall performance on learned new modalities. On the other hand, we modify the conventional metrics of continual learning to verify the performance of our method on \u201ccatastrophic forgetting\u201d. Specifically, for each modality and dataset, suppose Smi represents the evaluation score on n-th datasets of modality M\u00b2 after training on modality Mm. We redefine the forgetting rate [18] to measure the degree of forgetting Fm on all old modalities after each modality stage m:\nFm = \u2211 FNi, (6)\nwhere FN is the average forgetting across N\u00bf datasets of modality i after modality m training, and N\u2081 is the number of datasets in modality i. And the FN are defined:\nFNi = max Smi , (7)\nIn addition, we define the forgetting \u00can for the n-th dataset in modality i during the training of all modalities:\nFn =\u2211 max Smi, (8)\nTo measure the overall performance on learned modalities, we further report the average scores of across Nm datasets of modality m after training on m modality, it can be expressed as:\nTm = \u2211 Smm, (9)\nAnd the performance on learned modalities \u00cen for the n-th dataset in modality i can be expressed as \u00cen = Si"}, {"title": "6 Conclusion and Discussion", "content": "We propose a flexible and scalable framework for multi-modal language reasoning that enables MLLMS to continually expand on multiple modalities without joint-modal datasets. We introduce an incremental Adapter-in-Adapter (AnA) strategy, incorporating two types of adapters to enhance modality plasticity and collaboration during expanding on other modalities. Moreover, we design an MoE-based gating module to further enhance multi-modal integration by modulating the output space of different modalities. Extensive experimental results in our proposed benchmark demonstrate the superiority of our method over previous arts in terms of modality alignment and memorization.\nA limitation of this paper is that we only explored the extension of five modalities and do not cover all modal information in real-world scenarios. Furthermore, the implicit interaction between the modalities in our method cannot accomplish cross-modal joint language reasoning tasks in an incremental manner."}]}