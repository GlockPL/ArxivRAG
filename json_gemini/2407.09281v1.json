{"title": "Predicting and Understanding Human Action Decisions: Insights from Large Language Models and Cognitive Instance-Based Learning", "authors": ["Thuy Ngoc Nguyen", "Kasturi Jamale", "Cleotilde Gonzalez"], "abstract": "Large Language Models (LLMs) have demonstrated their capabilities across various tasks, from language translation to complex reasoning. Understanding and predicting human behavior and biases are crucial for artificial intelligence (AI)-assisted systems to provide useful assistance, yet it remains an open question whether these models can achieve this. This paper addresses this gap by leveraging the reasoning and generative capabilities of the LLMs to predict human behavior in two sequential decision-making tasks. These tasks involve balancing between exploitative and exploratory actions and handling delayed feedback-both essential for simulating real-life decision processes. We compare the performance of LLMs with a cognitive instance-based learning (IBL) model, which imitates human experiential decision-making. Our findings indicate that LLMs excel at rapidly incorporating feedback to enhance prediction accuracy. In contrast, the cognitive IBL model better accounts for human exploratory behaviors and effectively captures loss aversion bias\u2014the tendency to choose a sub-optimal goal with fewer step-cost penalties rather than exploring to find the optimal choice, even with limited experience. The results highlight the benefits of integrating LLMs with cognitive architectures, suggesting that this synergy could enhance the modeling and understanding of complex human decision-making patterns.", "sections": [{"title": "Introduction", "content": "Understanding and predicting human behavior in decision-making settings is crucial for developing AI systems that can effectively collaborate with and assist people to help them make informed decisions and avoid cognitive biases and limitations (Hoffman, Bhattacharjee, and Nikolaidis 2023; Bansal et al. 2019; Zhang et al. 2021; Rastogi et al. 2023). One common approach to predicting individual behavior is using machine learning techniques to model their decision-making processes based on past behaviors. These techniques include imitation learning (e.g., behavior cloning (Torabi, Warnell, and Stone 2018)) and machine theory of mind (e.g., inverse reinforcement learning (RL) (Abbeel and Ng 2004), Bayesian Theory of Mind (Baker et al. 2017), or neural networks (Rabinowitz et al. 2018)). However, these methods often require extensive training datasets and struggle to model human decision-making accurately with limited samples.\nThe recent rise of large language models (LLMs) such as ChatGPT (Ouyang et al. 2022), PaLM (Chowdhery et al. 2023), and LLAMA (Touvron et al. 2023) has demonstrated their remarkable capabilities in semantic understanding and intent reasoning (Brown et al. 2020; Zhang et al. 2023) by encoding a wide range of human behaviors from their training data. These advancements offer new opportunities for employing LLMs as work assistants, particularly in creating LLM-powered decision support systems (Allen, He, and Gadiraju 2023; Chiang et al. 2024; Wang et al. 2024).\nA growing body of research has shown that these LLMs can perform at human levels, or even above, in many experiments (Binz and Schulz 2023b; Dasgupta et al. 2022; Shiffrin and Mitchell 2023) and tasks designed to test different aspects of reasoning (Mahowald et al. 2024). However, empirical findings on their ability to reason about the mental states of others, known as theory of mind, are mixed. While some studies show promising results (Strachan et al. 2024), others highlight limitations in accurately reasoning about the mental states of others in different theory of mind tasks (Ullman 2023). Furthermore, from an empirical standpoint, little is currently understood about whether these models can predict and capture human-like behavioral characteristics, especially human cognitive biases (Mitchell and Krakauer 2023). For instance, an experiment in sequential decision-making that required a trade-off between exploitation and exploration showed that GPT-3 outperformed human subjects by heavily relying on exploitative strategies (Binz and Schulz 2023b). In contrast, people tended to apply a combination of elaborate exploration strategies (Wilson et al. 2014).\nPrior research has shown that humans rely on various cognitive mechanisms when making decisions (Gonzalez, Lerch, and Lebiere 2003a; Erev et al. 2010; Lebiere et al. 2013). These cognitive models have been instrumental in understanding the strengths and limitations of human performance and machine learning algorithms (Thomson, Lebiere, and Bennati 2014; Mitsopoulos et al. 2022). With the rise of LLMs, how these cognitive models compare to LLMs in predicting human decision-making strategies is unclear. Addressing this gap is crucial for gaining deeper insights into LLMs' potential, providing a cognitive grounding between human users and these models, and guiding the development of LLM systems that can effectively interact with people.\nIn this work, we investigate the capabilities of LLMs, specifically open-source models, in predicting human action strategies in two sequential decision-making tasks, and compare their performance with a cognitive instance-based learning (IBL) model (Gonzalez, Lerch, and Lebiere 2003a). Grounded in the theory of decisions from experience, IBL models simulate human decision-making by incorporating mechanisms and limitations from the ACT-R cognitive architecture (Anderson and Lebiere 2014). These models have proven effective in emulating human decisions in various tasks, including gambling choices (Gonzalez and Dutt 2011; Hertwig 2015), complex dynamic resource allocation (Somers, Oltramari, and Lebiere 2020), cybersecurity (Gonzalez et al. 2020), and predicting the actions of other RL agents (Nguyen and Gonzalez 2022)\nOur goal is to understand whether LLMs and the cognitive IBL model can predict human action strategies and capture human biases, such as loss aversion, characterized by the tendency to choose sub-optimal goals with fewer step-cost penalties rather than exploring optimal choices. We focus on multi-step, goal-directed decision-making tasks in interactive environments that require balancing exploitative and exploratory actions and handling delayed feedback-essential components of real-life decision processes.\nTo achieve this, we analyze the discrepancies between the strategy predictions of the models and real human strategies, which enabled us to uncover the ability of these models to capture the nuances of human behavior in balancing risk and reward during decision-making. We used schema-based and demonstration-based prompts to provide task instructions and users' action trajectory history from previous trials, allowing pre-trained LLMs to use this in-context information to predict the next action plans in subsequent trials. We employed two open-source LLMs for our experiments: Mistral 7B (Jiang et al. 2023) and Llama-3 70B (the largest of Meta Al's Llama03 models with 8B and 70B parameters) (Touvron et al. 2023; Meta 2024). We chose these state-of-the-art open-source LLMs over a closed-source commercial service like GPT-4 (Achiam et al. 2023) as they provide transparency and public access, thus promoting reproducibility and responsible LLM use by giving researchers full access to the network architecture and its pre-trained weights.\nOur results from comparing the predicted behaviors of the models and humans demonstrate that the lightweight Mistral-7B model outperforms both Llama-3 70B and the cognitive IBL model in predicting human strategies. The LLMs also demonstrated an ability to quickly incorporate feedback and improve prediction accuracy as more demonstrated data was provided. As expected, predicting human behavior is more challenging in complex decision environments with high cost-reward tension. Importantly, we observed that the cognitive IBL model more accurately accounted for human exploratory behavior with few samples and aligned closely with human exploratory strategies under limited information, which reflects the tendency towards risk-averse or \u201csatisficing\u201d behavior (Simon 1956) to choose the closest sub-optimal option instead of seeking the optimal one. These findings suggest that integrating LLMs with cognitive architectures could enhance the modeling and understanding of complex human decision-making patterns."}, {"title": "Related Work", "content": "LLMs for Agent Behavior Modeling. Generative agents use LLMs to drive their behavior, taking advantage of the extensive data on human behavior encoded in these models (Brown et al. 2020). Research often relies on templates with few-shot prompts (Gao, Fisch, and Chen 2020) or chain-of-thought prompts (Wei et al. 2022) to effectively generate behavior based on the agent's environment. These templates have proven effective in the control and decision-making tasks. Recent work has shown that LLMs can produce human-like interactions in multi-player games involving natural language communication (Park et al. 2023).\nAdditionally, LLMs have been used to enhance agent modeling with reinforcement learning (RL) agents. Research has shown that integrating feedback into RL models through LLMs provides a learning experience similar to RL with human feedback, without requiring human judgments (Wu et al. 2023b,a; McDonald et al. 2023). LLMs have also improved offline RL, reducing the need for computationally intensive online learning (Shi et al. 2023).\nWe argue that LLMs offer an opportunity to leverage generative models for understanding and predicting human behavior. Unlike much existing work that models optimal AI agents, we focus on capturing human behavior.\nLLMs in Theory of Mind Reasoning. A growing body of research has explored LLMs' Theory of Mind (ToM) capabilities by testing them with various ToM tasks (Kosinski 2023; Strachan et al. 2024). Results show that leading LLMs can solve 90% of false-belief tasks, sometimes performing at or above human levels, indicating ToM-like abilities. However, Ullman (2023); Shapira et al. (2023) found that LLMs' performance deteriorates with slight modifications to task structure, highlighting mixed results in this area.\nFrom a modeling perspective, ToM has been used to improve Al agent performance in different contexts. Recent studies have applied ToM with LLMs to enhance collaboration in multi-agent reinforcement learning (Li et al. 2023; Sun, Huang, and Pompili 2024). We distinguish our work by evaluating the ToM abilities of LLMs to understand human action strategies and biases across various decision-making complexities rather than focusing on learning to infer the intentions of other RL agents.\nCognitive Modeling and Human Behavior. Cognitive architectures like ACT-R have demonstrated successful reasoning with limited training instances through experience (Anderson and Lebiere 2014; Gonzalez, Lerch, and Lebiere 2003a). They achieve human-level performance and capture cognitive biases in various decision-making tasks (Lebiere et al. 2013; Erev et al. 2010; Thomson et al. 2015). Specifically, Lebiere et al. (2013) showed that the cognitive instance-based learning (IBL) model could predict whether an analyst would be risky or risk-averse based on feedback from previous trials. Prior research has also indicated that the IBL models can align with human judgment in predicting RL agents' goals (Nguyen and Gonzalez 2022).\nOur work investigates how cognitive models, compared to LLMs, can predict and account for human action strategies based on past observations in decision-making processes."}, {"title": "Preliminaries", "content": "Task Scenario. We studied goal-seeking task environments (Rabinowitz et al. 2018; Nguyen, McDonald, and Gonzalez 2023) that were set in 10 \u00d7 10 gridworld mazes containing obstacles and four terminal targets. Each target had a different value, with only one target having the highest value. The reward function over the four terminal objects was drawn randomly from a Dirichlet distribution with a concentration parameter of 0.01. During each episode, the player navigated through the grid by making a series of decisions using the common action space (up, down, left, or right) to locate the target with the highest value. The player could consume the targets by moving on top of them. Episodes ended when a target was collected or when the time horizon was reached ($T_{max}$ = 31). The player received points for reaching the target but was also penalized for each movement (-0.01) and for walking into an obstacle (-0.05).\nTask Formulation. The task is modeled as a partially observable Markov Decision Process (POMDP), represented by the tuple (S, A, O, T, R, \u03a9, \u03b3). Here, S denotes the state space, with each square in the grid called a state s \u2208 S; A is the action space; O is the observation space; $T:S\u00d7A\u2192S$ is the transition function; $R: S\u00d7A \u2192 R$ is the reward function; $\u03a9 : S \u2192 O$ is the observation function; and \u03b3\u2208 [0, 1) is the discount factor controlling the player's emphasis on future rewards compared to immediate rewards.\nAt every step t\u2208 0, ..., $T_{max}$, a player is required to take an action a \u2208 A after observing $o_t$ \u2208 O. The player receives a reward $r_t$ \u2208 R after taking the action, as the environment transitions to a new state. Each player follows their policy $\u03c0_i$ (i.e., strategy) to decide how to act. y executing its policy $\u03c0_i$ in the gridworld M following episode j, the player $P_i$ generates a trajectory denoted by $T_{ij} = (s_t, a_t)_{t=0}^{T_{max}}$."}, {"title": "Prediction Models", "content": "We describe a general approach that adapts LLMs for predicting human decisions in sequential goal-directed decision-making tasks. We compare our LLM-based prediction models with the cognitive IBL model. The overall framework of our approach is illustrated in Fig. 1a.\nLLM-based Prediction Models\nWe use LLMs to predict human behaviors in the described task, using both instruction-following and demonstration-based paradigms. Building on prior successes in using LLMs for control settings (Wu et al. 2023a,b; Park et al. 2023), we prompt the LLM to generate a trajectory that human players would take to succeed in the task. Unlike previous research (Wu et al. 2023a; McDonald et al. 2023), which focused on LLMs deciding the provision or value of a reward for optimal performance, we ask the LLM to predict the sequence of actions a human player would take. Specifically, we frame our query to predict the trajectory in the next episode, aiming to match the human strategy.\nFor each user, we construct a prompt consisting of two main parts: task instructions and sequential interaction histories. The task instructions ((Task_Instruction)) are detailed in the Appendix. The interaction histories include the starting position, the trajectory taken in previous episodes, and information about any consumed targets and their associated values. The prompt design is as follows:"}, {"title": "POMDP Formulation.", "content": "The LLM-based prediction model can be formalized as follows: For each player $P_i$, we aggregate their historical trajectories $T = \\bigcup_j T_{ij} = (s_t, a_t)_{t=0}^{T_{max}}$, generated after executing a sequence of actions at each time step t across j episodes.\nThe context C is defined by the combination of task instructions and historical interactions T, paired with the consumed targets $G = \\{g_1,...,g_j\\}$ corresponding to the trajectory of each episode j, and the values $V = \\{v_1,..., v_j\\}$ associated with obtaining these targets. The LLM model M uses this context C as input to predict the trajectory for the next episode $T_{i(j+1)}$. Essentially, given the context C spanning from the first to the j-th episode, the LLM-based model predicts the trajectory for episode j + 1. The details of the algorithm are provided in Algorithm 1.\nWe note that the output from LLMs may still contain natural language text. We address this by employing text processing methods to parse and ground the generated results in the specified environment. Additionally, we have occasionally observed instances where LLMs produce coordinates that are invalid within the given environment scope. In such cases, we reprocess these illegal outputs to ensure compliance with environmental constraints."}, {"title": "Instance-Based Learning (IBL) for Prediction", "content": "The IBL model used for comparing and predicting human behavior is based on Instance-Based Learning Theory (IBLT) (Gonzalez, Lerch, and Lebiere 2003b) for dynamic decision-making, which is connected to the ACT-R cognitive architecture through the activation function, which is used to predict the estimated utility of performing an action in a state based on the utility outcomes of similar past experiences held in declarative memory (Thomson et al. 2015).\nIn IBLT, declarative memory consists of instances k = (o, a, x) represented by the observation that describes the state of the environment o, the action performed by the agent a, and the utility outcome of that action x. This instance structure can be related to the POMDP environment formulation by taking the state s to be the agent observation o, and the utility outcome x to be the observed reward r.\nAgent actions are determined by maximizing the value $V_{k,t}$ of an available action a in an instance k performed at time-step t, calculated using the \u201cblending\u201d function (Gonzalez, Lerch, and Lebiere 2003b):\n$V_{k,t} = \\sum_{i=1}^{n_{k,t}} p_{i,k,t} x_{i,k,t}$  (1)\nwhere $n_{k,t}$ are the previously generated instances held in procedural memory, $x_{i,k,t}$ are the outcomes of those instances, and $p_{i,k,t}$ is the probability of retrieving an instance in memory, calculated by Equation 2.\n$p_{i,k,t} = \\frac{exp(A_{i,k,t}/\\tau)}{\\sum_j exp(A_{j,k,t}/\\tau)}$   (2)\nFurther, $A_{i,k,t}$ is given by Equation 3.\n$A_{i,k,t} = ln(\\sum_{t' \\in T_{i,k,t}} (t - t')^{-d} + \\sigma) + o ln(\\frac{1 - S_{i,k,t}}{S_{i,k,t}})$  (3)\nwhere d and \u03c3 are decay and noise parameters, and $T_{i,k,t} \\subset \\{0, ..., t - 1\\}$ is the set of previous timesteps where instance k was stored in memory. The $S_{i,k,t}$ term is used to capture noise in the individual differences in memory recall. Because of the relationship between noise \u03c3 and temperature \u03c4in IBLT, the temperature parameter \u03c4 is typically set to \u03c3\u221a2. In our experiments, we use all default parameters of d = 0.25 and \u03c3 = 0.5. We also set the default utility to 1.0 to encourage exploration through an optimistic prior.\nA key aspect of applying IBLT to decision-making is determining the utility of actions. Prior research on temporal credit assignment in IBL models has shown that models assigning equal credit to all decisions closely match human performance (Nguyen, McDonald, and Gonzalez 2023, 2024), which we consequently have chosen to adopt this approach. Formally, if a target is reached at step T, the target's value RT is assigned to each instance in the trajectory $T = (s_t, a_t)_{t=0}^{T}$, i.e., $x_t = R_T$ for all (st, at). The step-level costs are assigned to each instance if no target is reached.\nThe IBL prediction model, functioning as an observer, learns by observing past decisions made by human agents. This past experience is incorporated into the model's memory through pre-populated instances, a mechanism that demonstrates how the IBL model can dynamically represent the development of ToM by observing actions of other learning agents in a gridworld task (Nguyen and Gonzalez 2022). Specifically, for each player Pi, the trajectory Tij produced by the player, following its policy $\u03c0_i$ in the gridworld M after episode j, is stored in the model memory."}, {"title": "Methods", "content": "Our research aims to determine if LLMs and the cognitive IBL model can accurately predict human strategic planning in uncertain decision-making environments, formulated as POMDPs, given past interaction histories. These environments require balancing potential high rewards against the risks or losses associated with high-value objects.\nWe utilized data from two human-subject experiments using interactive browser-based gridworld applications, which incorporate two levels of decision complexity. Moreover, we explored how the models predict human strategies under different levels of environment presentation: full grid information in Experiment 1 and restricted grid information in Experiment 2. Our primary research questions are:\n\u2022 RQ1: How accurately can LLMs and the cognitive IBL model predict human action in uncertain decision-making environments based on past interaction histories?\n\u2022 RQ2: To what extent do LLMs and the cognitive IBL model capture human decision biases, such as loss aversion, across different levels of decision complexity?\n\u2022 RQ3: How do different levels of environment presentation (full grid information vs. restricted grid information) affect the accuracy of LLMs and the cognitive IBL model in predicting human decision strategies?\nExperimental Design and Procedure\nThe two experiments used the same gridworlds, but the information provided to participants varied. Human subjects were presented with gridworlds randomly chosen from a set of 100 grids, with the selection based on the decision complexity level assigned to each participant's condition.\nParticipants were recruited from Amazon Mechanical Turk and provided informed consent before completing each session. After receiving instructions, participants completed 40 episodes in the same gridworld environment, with each session lasting 15-30 minutes. They received a base payment of $1.50 and could earn up to $3.00 in bonuses based on their accumulated scores. The study, approved by our institution's IRB, used a between-subjects design and was preregistered with the Open Science Framework\u00b9.\nDecision Complexity. The experiments manipulate the level of decision complexity defined by the trade-offs between the highest value target and the nearest distractor relative to the agent's initial spawn location in the gridworld (Nguyen and Gonzalez 2020). This complexity is quantified by $\u2206_d$ = d \u2013 d', where d is the distance to the highest value target, and d' is the distance to the nearest distractor. Higher $\u2206_d$ values represent greater complexity, posing a strategic dilemma to agents: pursue a distant high-reward target or opt for a closer, less valuable one. Fig. 2 illustrates simple and complex decision scenarios."}, {"title": "Experiment 1: Full Grid Information", "content": "Participants viewed the full grid interface (Fig.1a), with their current position indicated by a black dot. After each move, the new location content (empty cell, obstacle, or target) was revealed.\nParticipants. A total of 206 participants: 102 in the Simple condition (age: 36.5 \u00b1 10.3; 34 female) and 104 in the Complex condition (age: 37.9 \u00b1 10.8; 37 female).\nExperiment 2: Restricted Grid Information Participants received limited information, viewing only one cell at a time (Fig. 1a). They were informed of their current (x, y) position, the steps taken, and the immediate cost or reward of the previous step. All other information about the shape and size of the grid were concealed.\nParticipants. A total of 194 participants: 99 in the Simple condition (age: 37.7 \u00b1 11.8; 40 female) and 95 in the Complex condition (age: 38.2 \u00b1 11.3; 30 female).\nObjective Measures\n\u2022 Trajectory Divergence: We used Kullback-Leibler (KL) divergence to measure the difference between the trajectory distribution of human subjects and that predicted by the model. Each trajectory is converted into a probability distribution over the grid cells, normalized to sum to 1. The KL divergence from Q (predicted trajectory) to P (human trajectory) is defined as:\n$D_{KL}(P||Q) = \\sum_i P(i) log \\frac{P(i)}{Q(i)}$  (4)\nwhere i indexes each possible state in the trajectory grid. Low KL divergence indicates that the model closely matches human behavior.\n\u2022 Prediction Accuracy: This measures the percentage of episodes where the predicted target, derived from the last coordinate of the predicted trajectory, matches the target consumed by human players.\n\u2022 Exploration Entropy Difference: This metric measures the difference in entropy of the distribution over how often each target is explored by humans (human goal entropy) and by the model (predicted goal entropy) in the first 10 episodes. The entropy difference is determined by subtracting the human goal entropy from the predicted goal entropy. Lower entropy difference suggests that the model's exploration behavior closely aligns with human behavior, indicating similar patterns in exploring targets."}, {"title": "Analysis", "content": "We compared two LLM-based prediction models, Mistral 7B (Jiang et al. 2023) and Llama-3 70B (Meta 2024), and the cognitive IBL model across Experiments 1 and 2. Each experiment included two levels of decision complexity: simple and complex, resulting in a 2 \u00d7 2 study design.\nTrajectory Divergence. Table 1 shows that in both experiments, the Mistral model with 7B consistently achieved the lowest KL divergence in simple and complex conditions, indicating better alignment with human trajectories compared to Llama-3 with 70B and the cognitive IBL model.\nComparing the simple and complex decision settings, we find that predicting human strategies in complex decision environments is more challenging, as indicated by the increase in KL divergence in complex conditions.\nInterestingly, when comparing Experiments 1 and 2, we observe that both Llama-3 and Mistral show better alignment with human trajectories under restricted information (indicated by a decrease in KL divergence in Experiment 2), while this is not the case for the IBL model. This improved alignment for the LLMs can be attributed to the advantages of pre-training on vast amounts of data and the help of instructions. When human participants are given restricted grid information (as in Experiment 2), they might adopt more predictable strategies that are easier for these LLM models to capture. By contrast, the IBL model shows poorer alignment with human trajectories under restricted information as it struggles to distinguish the differences that human players encountered in the two different conditions."}, {"title": "Discussions", "content": "In this paper, we investigate whether large language models (LLMs) can predict human action strategies and capture human biases, such as loss aversion, in decision-making scenarios involving cost-reward tension. We used two state-of-the-art open-source LLMs (Llama-3 70B and Mistral-7B) and compared them to a well-known cognitive instance-based learning (IBL) model. We tested these models in two experimental studies where human participants engaged in multi-step decision-making tasks in interactive environments with varying levels of information presentation.\nOur results show that Mistral-7B outperforms both Llama-3 70B and the cognitive IBL model in predicting human strategies. We also found that predicting human behavior becomes more challenging in complex decision environments with high tension between costs and rewards. Moreover, we learned that the cognitive IBL model effectively captures initial human exploratory behavior with minimal demonstration samples. However, as more samples are provided, LLMs quickly improve their prediction accuracy. Importantly, our findings suggest that the IBL model closely aligns with human exploration strategies under limited information conditions, effectively capturing the human tendency towards risk-averse, \u201csatisficing\" behavior, where people often select an option that is good enough rather than optimal.\nImplications for Trust and Synergy in AI-assisted Systems with LLMs and Cognitive Models\nOne of our key findings is that a lightweight LLM, Mistral-7B, performs better than the widely recognized Llama-3 70B and the cognitive IBL model in accurately predicting and capturing human action strategies across various decision-making settings. This result highlights the potential of leveraging these open-source, lightweight LLMs to develop more reliable and trustworthy AI systems in decision-making contexts. While much of the extant research on LLM-powered human interaction focuses on investigating black-box LLMs, such as ChatGPT, which only provides an accessible API, our study emphasizes the potential capabilities of open-source pre-trained LLMs. These models enable fine-tuning and provide full access to network architecture and pre-trained weights, facilitating cognitive-plausible understanding and integration (Binz and Schulz 2023a; Malloy and Gonzalez 2024). Furthermore, prior research indicates that humans initially under-trust AI and tend to overtrust it with more experience, underscoring the need for explicit calibration to the AI's competencies (Rechkemmer and Yin 2022; Bu\u00e7inca, Malaya, and Gajos 2021). By leveraging these open-source models, we can create trustworthy LLM-powered systems that are more accurate, explainable, and aligned with human expectations, thereby enhancing their acceptance and effectiveness in real-world applications.\nIn the context of AI-assisted human decision-making, our findings highlight the utility of the cognitive IBL model in capturing initial human exploratory behavior and the tendency towards loss aversion in high cost-reward tension scenarios without requiring large amounts of training data. Conversely, LLMs can quickly learn and predict human strategies as more data becomes available, indicating a synergy between these models. For instance, LLMs can support cognitive models by synthesizing large amounts of information and serving as knowledge repositories to construct representations of the environment (Wu et al. 2023a; Binz and Schulz 2023a). Cognitive models, which can predict human performance and preferences with few-shot learning, can help LLMs adapt early on and further personalize their responses to human users (Malloy and Gonzalez 2024; Thomson and Bastian 2023). This synergy would benefit for effective human-AI teaming, where AI evolves alongside human learning and adaptation to support human decision-making."}, {"title": "Limitations and Future Work", "content": "The experiments we pursued here were simple and aimed to shed light on the capabilities of open-source LLMs and cognitive architectures in predicting human behavior. There is considerable room for further investigation. First, our study focused on purely open-source LLMs, so caution should be exercised when extrapolating our findings to closed-source commercial services like ChatGPT, which may exhibit different performance levels. Second, we examined the vanilla versions of these models without fine-tuning their parameters to human data, particularly in the cognitive IBL models, where an equal credit assignment mechanism leads to more conservative exploratory strategies. Future research could enhance these LLM-based and cognitive prediction models to better match human behavior and uncover new behavioral structures. Finally, while our study highlights the strengths of different models, it does not fully explore the integration of LLMs and cognitive models in a cohesive framework, which we consider as future work. Additionally, it would be useful to investigate the potential of these models in interactive systems to predict human decisions in real-time."}]}