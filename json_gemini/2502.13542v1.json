{"title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference", "authors": ["Qingfa Xiao", "Jiachuan Wang", "Haoyang Li", "Cheng Deng", "Jiaqi Tang", "Shuangyin Li", "Yongqi Zhang", "Jun Wang", "Lei Chen"], "abstract": "Recent advances in large language models (LLMs) have showcased exceptional perfor-mance in long-context tasks, while facing sig-nificant inference efficiency challenges with limited GPU memory. Existing solutions first proposed the sliding-window approach to accu-mulate a set of historical key-value (KV) pairs for reuse, then further improvements selectively retain its subsets at each step. However, due to the sparse attention distribution across a long context, it is hard to identify and recall rele-vant KV pairs, as the attention is distracted by massive candidate pairs. Additionally, we found it promising to select representative to-kens as probe-Query in each sliding window to effectively represent the entire context, which is an approach overlooked by existing meth-ods. Thus, we propose ActQKV, a training-free, Activation-aware approach that dynami-cally determines probe-Query and leverages it to retrieve the relevant KV pairs for inference. Specifically, ActQKV monitors a token-level indicator, Activation Bias, within each context window, enabling the proper construction of probe-Query for retrieval at pre-filling stage. To accurately recall the relevant KV pairs and minimize the irrelevant ones, we design a dy-namic KV cut-off mechanism guided by infor-mation density across layers at the decoding stage. Experiments on the Long-Bench and \u221e Benchmarks demonstrate its state-of-the-art performance with competitive inference quality and resource efficiency.", "sections": [{"title": "1 Introduction", "content": "With the emergence of large language models (LLMs) capable of handling extended context lengths (Wang et al., 2024b; Achiam et al., 2023; Dubey et al., 2024), researchers are leveraging their advanced information understanding and filtering abilities to tackle various downstream tasks, in-cluding web-based search chatbot (Semnani et al., 2023) and document-level question answering (QA) (Lewis et al., 2020). Inevitably, the con-text length has increased significantly, even sur-passing the models' context limitations. However, the computational complexity of attention mecha-nism (Vaswani, 2017) grows quadratically $O(N^2)$ with the context length N during inference. Specifically, each token from context will be embedded"}, {"title": "2 Related Works", "content": "KV cache retrieval (Adnan et al., 2024; Zhang et al., 2023; Xiao et al., 2025) has become a critical optimization strategy aimed at reducing memory usage, minimizing inference latency and improving overall throughput in long-context LLMs inference.\nRecent studies employ a sliding window mecha-nism to address challenges in long-text inference, where tokens outside the window are stored in the cache and only used when needed for the current window. To accelerate the retrieval of essential KV, several approaches have proposed index-based methods that organize and access the KV cache at the block or cluster level, enabling efficient query-ing and extraction. InfLLM (Xiao et al., 2024a) maintains the full KV cache in blocks and uses a hierarchical storage strategy to facilitate long-sequence processing. This framework employs CPU-GPU memory orchestration, keeping essen-tial KV and computational units in GPU memory while offloading less frequently accessed units to CPU memory. Q-LLM (Li et al., 2024b) enhances long-sequence processing by prioritizing memory related to task descriptions. This approach mimics human reading behavior: first reading the question, then searching for the answer in the context.\nIn contrast to methods which use uniform KV block sizes, TokenSelect(Hao et al., 2025) is based on the observation of sparsity in non-continuous at-tention patterns. It uses the Query-Key dot product to assess the importance of each KV cache stored at the token level. For each query, they dynamically calculates the importance of past KV caches per head at the token level and selects the most impor-tant tokens through a soft voting mechanism across heads. EM-LLM (Fountas et al., 2025) dynamically segments incoming tokens into episodic events, em-ploying a hybrid retrieval mechanism that com-bines semantic similarity matching with temporal context to efficiently access relevant KV cache seg-ments. Additionally, some researchers focus on KV cache budget allocation across layers (Cai et al., 2024; Yang et al., 2024) and heads (Feng et al., 2024; Fu et al., 2025) due to the hierarchical archi-tecture of LLMs.\nMost methods overlook the importance of probes for retrieval, especially given the fact that LLMs are not optimized for retrieval tasks. Therefore, this realization inspires our further exploration of probe-Query construction in this paper."}, {"title": "3 Background", "content": "In this section, we first introduce the two stages of inference for long-context LLMs using sliding win-dow attention with KV cache (in Sec. 3.1), and then define the problem of KV Retrieval (in Sec. 3.2).\n3.1 Sliding Window Attention with KV Cache\nGiven an input sequence X, the generation of the output sequence Y during LLMs inference can be divided into two stages: pre-filling the input X and decoding the output Y.\nTo handle long sequences input of tasks, exit-ing works (Xiao et al., 2024b,a; Li et al., 2024b) use sliding window attention to process the text iteratively. In this mechanism, the lengthy input sequence X is partitioned into T windows, denoted as $W = {w^1,...,w^T}$, $W \\in R^{T\\times m}$ and m indicates the window size (see Fig. 2(a)). To reduce computational costs, the model processes each win-dow sequentially and stores the historical key-value pairs in a cache (i.e., Kcache and Vcache) for future reuse (see Fig. 2(b)).\nDuring t-th pre-filling step (t < T), the model utilizes the KV cache $K_{cache}^{t-1}$ and $V_{cache}^{t-1}$ from the historical sequence W[: t -1] to compute the atten-tion output $O^t \\in R^{m\\times d}$ for the current m window tokens $w^t \\in R^{m}$ as follows:\n$O^t = Attention (Q^t, [K^t, K_{cache}^{t-1}], [V^t, V_{cache}^{t-1}])$, (1)\nwhere the triplet $Q^t = {q_i}^m_{i=1}, K^t = {k_i}^m_{i=1}, V^t = {v_i}^m_{i=1} \\in R^{m\\times d}$ represents the generated attention vectors, each corresponds to m tokens with d hidden dimensions. To further save GPU memory, current methods select partial KV cache $K^*$ and $V^*$ for inference, denoted as:\n$O^t = Attention (Q^t, [K^t, K^*], [V^t, V^*])$, (2)\nwhere $K^* \\subset K_{cache}^{t-1}$ and $V^* \\subset V_{cache}^{t-1}$\nDuring t-th decoding step (t > T), the model generates the output sequence Y token-by-token. Unlike pre-filling, the model uses only one single query vector $q_t \\in R^{1\\times d}$ along with corresponding key and value vectors $k_t, v_t \\in R^{1\\times d}$ to predict one next token $y_t \\in Y$ in each step. Its corresponding attention output $o_t \\in R^{1\\times d}$ can be computed as:\n$o_t = Attention (q_t, [k_t, K^*], [v_t, V^*])$. (3)\nAfter the t-th step, the newly generated key-value pairs will be stored in the cache (see Fig. 2(e)), updating it as demonstrated below:\n$K_{Cache}^t, V_{cache}^t = K_{cache}^{t-1} \\cup K^t, V_{cache}^{t-1} \\cup V^t$, (4)"}, {"title": "3.2 Problem Setting", "content": "During long-context inference in LLMs, the his-torical key-value pairs are essential for maintain-ing long-range dependencies and overcoming win-dow size limitations. Given a cache comprising $K_{cache}^{t-1}$ and $V_{cache}^{t-1}$, the objective of KV retrieval is to identify the top-k relevant subset $K^*$ and $V^*$ using the probe-Query $Q_{probe}$ for the t-th inference step (Xiao et al., 2024a; Fountas et al., 2025; Hao et al., 2025), as described below:\n$K^*, V^* = K_{cache}[I^*], V_{cache}[I^*]$,\n$I^* = \\underset{I \\subset [m], |I|=k} {arg \\, max} \\sum_{i \\in I} \\frac{Q_{probe} \\cdot K_{cache}[i]^T}{||Q_{probe} || \\times ||K_{cache}[i]||}$, (5)\nwhere $[m] = {1, 2, . . ., m}$,\nwhere $Q_{probe} \\in R^{1\\times d}$ denotes the overall represen-tation of window context $w^t$ and k is the number of selected KV. These two factors significantly impact the factual relevance of the retrieved KV index $I^*$ for each transformer layer inference."}, {"title": "4 Methods", "content": "In this section, we first present the overall frame-work of our ActQKV, as illustrated in Fig. 2. We then demonstrate our two-stage approach: the Activation-aware Probe-Query Construction for KV matching (in Sec. 4.1)and the Dynamic KV Cut-off Mechanism for KV recall (in Sec. 4.2).\n4.1 Activation-Aware Probe-Query\nTo identify the relevant KV pairs, we leverage the query vectors of each window to construct the attention-aware probe-Query for retrieval. The primary distinction between our activation-aware probe-Query and other representation methods lies in the emphasis on identifying anchor tokens that effectively represent the entire context of the win-dow for KV matching. The main challenge is to accurately distinguish and activate these tokens.\nFormally, given a subset of context $w^t = {x_1,...,x_m}$ extracted from a long sequence W, we obtain the hidden states ${z_i}^m_{i=1} = {f(x_i)}^m_{i=1}$ at each transformer layer, where m denotes the window size and f denotes the function mapping tokens to corresponding states. Intuitively, hidden states that deviate significantly from their statisti-cal mean (i.e., $z_i^t$) can be considered that they are from anchor tokens compared to others. Specifi-cally, token $x_i$ is deemed more essential than $x_j$ for the quality of generation, as indicated by pre-"}, {"title": "4.2 Dynamic KV Cut-off Mechanism", "content": "During the decoding stage, the quality of the pre-dicted answer greatly depends on the top-k relevant pairs $K^*$ and $V^*$. However, due to the sparse and irregular attention pattern across each layer, the selection of k KV pairs is highly sensitive to the probe-Query $Q_{probe} = q$. Therefore, we propose a KV cut-off mechanism to dynamically determine k based on information density assessment for L transformer layers. Compared to the preset thresh-old, this mechanism dynamically removes redun-dant KV pairs and improves the recall of relevant ones within a limited KV budget.\nIn the t-th decoding step, we first calculate the similarity scores $S^l = {s_1,..., s_n}$ between the"}, {"title": "5 Experiments", "content": "In this section, we first present the experimental setup of this paper (in Sec. 5.1). Then we demon-strate the logical reasoning and factual retrieval ability of our ActQKV in long-context inference through two widely-used benchmark (in Sec. 5.2). Finally, we conduct the ablation study (in Sec. 5.3) and reveal the influence of our method (in Sec. 5.4).\n5.1 Experimental Setup\nDatasets and Implementation Details. We uti-lize 21 tasks from two widely used long docu-ment benchmarks: Long-Bench (Bai et al., 2023) and \u221e-Bench (Zhang et al., 2024) for evalua-tion. Specifically, Long-Bench has a 95% se-quence length of 32K, while \u221e-Bench averages about 122K in sequence length. We utilize LLaMA3-8B-inst (AI@Meta, 2024) and Qwen2.5-7B-Instruct (Team, 2024) as our base models with maximum input lengths of 8K and 32K, respec-tively. In each inference step, we reuse only 2K KV pairs and store the remaining pairs in the Cache"}, {"title": "5.2 Main Experiment Results", "content": "We first utilize Long-Bench to evaluate the long-context reasoning capabilities of ActQKV, and then test the fact retrieval ability using \u221e-Bench. We report the results based on Llama-3-8B-Instruct, and the others can be found in Appendix C.\nLong-Bench. We present the results in Tab. 1. (1) ActQKV achieves an average score of 49.40, surpassing the full context setting (31K tokens) by 4.67 points while utilizing only 2K tokens. This highlights the efficiency of its key-value retrieval method in handling long-context inference with a significantly smaller KV budget. (2) Compared to the static KV selection methods Infinite and Stream, ActQKV excels in capturing critical information required for reasoning tasks. (3) In comparison to SOTA KV retrieval methods such as TSLLM and EMLLM, our activation-aware retrieval approach"}, {"title": "5.3 Ablation Studies", "content": "In this subsection, we present ablation studies shown in Tab. 3 to evaluate two key components of our method: the Activation-aware Probe-Query $Q_{probe}^t$ (APQ, see Sec. 4.1) and the Dynamic Cut-off Mechanism (DCM, see Sec. 4.2).\nWhen using APQ for key-value (KV) pair match-ing, our method attains a comparable score of 48.8, especially getting the best result 98.0 in retrieval tasks. These results demonstrate that the APQ com-ponent effectively captures the semantic context"}, {"title": "5.4 Analysis of Retrieved KV Pairs", "content": "In this subsection, we compare the retrieved KV pairs from our ActQKV and InfLLM methods to evaluate the specific impact of our proposed ap-proach. To facilitate this comparison, we present the distribution of cosine similarity scores and aver-age perplexity in Fig. 3 and analyze the following:\nCosine Similarity. The box of cosine similarity clearly shows that ActQKV consistently achieves higher similarity scores across most layers com-pared to InfLLM. This outcome can be attributed to the activation-aware query (probe-Query) we intro-duced, which more effectively captures the underly-ing semantic information of the window context for each inference step. Furthermore, the enlargement of the box plots indicates that the distribution of similarities becomes more dispersed. This suggests that our probe-Query covers a broader semantic space, thereby resulting in a more robust KV re-trieval process. The greater spread in the similarity values also reflects the model's ability to account"}, {"title": "6 Conclusion", "content": "In this paper, we present ActQKV, a training-free method to KV retrieval efficiency for long-context LLMs inference. The primary challenge in KV retrieval stems from the inherent vagueness of ex-isting probe-Query, which inadequately filter ir-relevant KV pairs. To address this limitation, we develop an activation-aware probe-Query construc-tion strategy and a layer-wise KV cut-off mecha-nism to effectively match and recall the relevant KV pairs. We hope this work can inspire the broader research for LLMs representation methods, lead-ing to improved long-context information filtering capabilities akin to specialized embedding models."}, {"title": "Limitations", "content": "Our method achieves promising performance to enhance the relevant KV pairs retrieval for long-context LLMs inference. And we believe that the interpretability of the retrieved KV pairs requires further exploration in future works. Unlike non-autoregressive architectures in embedding models, the auto-regressive architecture of LLMs results in the semantics of current tokens being influenced by historical KV pairs. When processing a long context all at once, this interaction makes it diffi-cult to separate the semantics from various events because the retrieved key-value pairs mostly show historical information. This introduces challenges in interpreting the retrieval results."}, {"title": "Ethics Statement", "content": "Throughout the development and execution of this work, we strictly adhered to ethical guidelines es-tablished by the broader academic and open-source community. All datasets and models utilized are publicly available. There are no conflicts of interest among the authors involved in this research. Our approach aligns with ethical AI practices, prioritiz-ing trust, accountability, and responsible research."}, {"title": "A The Complexity of LLMs Inference", "content": "In this section, we focus on the attention computa-tion and analyze the complexity of exiting methods shown in Tab. 4 as follows:\nStandard Attention Mechanism. Under the standard attention mechanism, during the pre-filling stage, each token in the input sequence un-dergoes attention calculations with all other tokens, resulting in a time complexity of $O(N^2)$. In the decoding stage, as the context grows, the com-plexity of generating each new token increases ac-cordingly. When generating the t-th token, the length of the context to be processed is N + t, so the total time complexity of the decoding stage is $\u039f(\\sum_{t=1}^{M}(N + t)^2)$, which is approximately $O(N^2M + M^3)$. Since the decoding length M is usually much smaller than the input sequence length N, the overall complexity can be simplified to $O(N^2 + MN^2)$.\nSliding Window Mechanism with KV Cache.\nThe sliding window mechanism divides the input sequence into several windows of fixed size, each with a size of m. During the pre-filling stage, the processing complexity of the tokens within each window is $O(m^2)$, and the interaction complexity between the KV caches of the windows is approx-imately O(N), so the overall time complexity is $O(\\frac{N}{m} \\times m^2) = O(mN)$, which is equivalent to $O(N^2)$ when the window size m is constant and linearly dependent on N. In the decoding stage, the decoding of each new token only needs to interact with the m tokens in the current window and some tokens in the adjacent windows, resulting in a to-tal time complexity of O(MN). Overall, the time complexity can be simplified to O(N2 + MN).\nKV Retrieval for Long-Context Inference."}]}