{"title": "Time and Tokens: Benchmarking End-to-End Speech Dysfluency Detection", "authors": ["Xuanru Zhou", "Jiachen Lian", "Cheol Jun Cho", "Jingwen Liu", "Zongli Ye", "Jinming Zhang", "Brittany Morin", "David Baquirin", "Jet Vonk", "Zoe Ezzes", "Zachary Miller", "Maria Luisa Gorno-Tempini", "Gopala Anumanchipalli"], "abstract": "Speech dysfluency modeling is a task to detect dysfluencies in speech, such as repetition, block, insertion, replacement, and deletion. Most recent advancements [1-3] treat this problem as a time-based object detection problem. In this work, we revisit this problem from a new perspective: tokenizing dysfluencies and modeling the detection problem as a token-based automatic speech recognition (ASR) problem. We propose rule-based speech and text dysfluency simulators and develop VCTK-token, and then develop a Whisper-like seq2seq architecture to build a new benchmark with decent performance. We also systematically compare our proposed token-based methods with time-based methods, and propose a unified benchmark to facilitate future research endeavors. We open-source these resources for the broader scientific community. The project page is available at https://rorizzz.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent developments in automatic speech recognition (ASR) [1]-[3] have achieved human parity in rich-resource languages, however, only at the word level. Human speech is a hierarchical form of words, phonemes, prosody, dysfluencies, etc., and research in understanding those non-word cues, which are essential parts of human conversations, is still limited. This work focuses on a specific non-word cue: dysfluency\u00b9. Dysfluencies are defined as a set of non-fluencies such as sound repetition, insertion, deletion, replacement, or hesitation [4]. Understanding dysfluencies in speech can help understand the intent of speech in dialogue systems and also has a huge impact on speech therapy and speech disorder screening [4].\nEarly work on dysfluency modeling primarily focused on stutter detection, which was expressed as a speech classification problem [5]\u2013[15]. However, there are several important limitations. First, stutter is just one aspect of dysfluencies. Second, there can be multiple dysfluencies in one utterance. Third, dysfluencies are text-dependent [4]. Fourth, accurately locating timing information of dysfluencies is essential in clinical applications [4]. Fifth, the datasets used by these methods are of small scale, and the annotations are still subject to the aforementioned limitations. To overcome these limita- tions, UDM [4] proposed modeling the dysfluency problem as a time-based object detection problem: identifying what the\ndysfluency is interchangable with disfluency\ndysfluencies are and where they occur. H-UDM [16] boosted UDM via a dynamic word boundary updating algorithm. YOLO-Stutter [17] directly adapts YOLO [18] to simulated speech-text alignments [19] and detects the dysfluencies in an end-to-end manner.\nWhile time-based methods exhibit huge potential, we ap- proach this problem from a fresh perspective: token-based modeling. We list our intuitions as follows. First, token se- quences are more straightforward to understand than time- based predictions, as visualized in Fig. 1. Second, all types of dysfluencies are tokenizable, and both type and time information can still be derived from token sequences. To design this token-based pipeline, we follow Yolo-Stutter [17] for the simulation pipeline and use the same dysfluency rules (Text Simulator) to generate dysfluent text at both word and phoneme levels. Then, a speech simulator is employed to generate dysfluent speech samples, with the ground truth an- notations being simply the dysfluent text (word or phoneme). We name this new dataset VCTK-Token. In the subsequent phase, we implement the Whisper paradigm [20], which processes dysfluent speech as input and generates predictions for dysfluent text tokens (comprising both word or phoneme sequences and dysfluency markers), as illustrated in Fig. 2. Both time-aware and token-aware metrics are developed to form a new benchmark with decent performance.\nLastly, we aim to unify both time-based and token-based methods. To do so, we first propose the integration of longest common subsequence (LCS) prior to enhance the time-based method [17], which we call YOLO-Stutter-LCS. Then, we cre- ate a comprehensive collection of simulated datasets (VCTK- Stutter [17], VCTK-TTS [17], and VCTK-Token). We also develop a joint time-based and token-based metric for unified evaluation. Results on both simulated data and disordered speech suggest that token-based methods outperform time- based methods in most metrics. To facilitate research in end-to-end dysfluency modeling and provide a platform for benchmark purposes, we open-source our work at https:// rorizzz.github.io/."}, {"title": "II. TIME-BASED DYSFLUENCY DETECTION", "content": "Current de facto methods for dysfluency modeling are time- based methods [4], [16], [17]: detecting the type of dysfluency in speech and returning its time range. We will review the state-of-the-art end-to-end method YOLO-Stutter [17] and also propose YOLO-Stutter-LCS, which includes the longest common subsequence (LCS) algorithm as alignment constraint to boost dysfluency detection."}, {"title": "A. YOLO-Stutter", "content": "YOLO-Stutter [17] takes speech-text alignment [19] as input, followed by a spatial encoder and a temporal encoder to predict dysfluency regions through a 1D adaptation of the YOLO objective [18]. For each timestep, YOLO-Stutter predicts boundary scores, dysfluency class scores, and dys- fluency confidence scores. To handle the data scarcity issue, it simulates dysfluent speech in the acoustic domain (VCTK- Stutter) and text domain (VCTK-TTS) respectively. We take YOLO-Stutter as our time-based baseline and further develop an enhanced version, YOLO-Stutter-LCS, by incorporating additional LCS dysfluent alignment."}, {"title": "B. YOLO-Stutter-LCS", "content": "Dysfluencies usually have aligned targets. For example, when the ground truth text is \"Please call stella\" and the speech is \"Please c-c-call s-stalla\", to detect dysfluency, we need to examine the pronunciation of \"call\" and \"stella\" respectively. We found that by applying the longest common subsequence (LCS) [21] algorithm on two sequences, the dysfluencies automatically align to their corresponding words (e.g., c-c-call \u2192 call, s-stalla \u2192 stella). This is due to its local sequence alignment property: the cost function is only decided by a subsequence (the first alignment), in contrast to global sequence aligners such as DTW. Based on this intuition, we pre-segment the data used in YOLO-Stutter via the LCS algorithm so that speech-text alignment can capture more dysfluency-related information. We name this method YOLO-Stutter-LCS."}, {"title": "III. TOKEN-BASED DYSFLUENCY DETECTION", "content": null}, {"title": "A. Dysfluency Simulation", "content": "1) Text Simulator: We inject the tokenizing dysfluencies into text space at both the word and phoneme levels, obtaining annotated texts and their corresponding IPA sequences for generating dysfluent speech. The rules for injecting word or phoneme-level dysfluencies and deriving IPA sequences are consistent with [17]. Notably, in this work, we also introduce 6 dysfluency tokens, as illustrated in Fig. 2(b), to denote the presence, type, and position of dysfluency within the text.\n2) Speech Simulator: Similar to the method pipeline de- scribed in [17], we employ the VITS [19] as our speech synthesizer. We feed IPA sequences generated by the text simulator into the VITS model to produce dysfluent speech. These (dysfluent speech, word / phoneme-level annotated text) pairs serve as training data for our token-based detector."}, {"title": "B. Speech Dysfluency Detector", "content": "We treat the task of token-based dysfluency detection as an automatic speech recognition (ASR) problem. We adopt Whisper architecture [20] which accurately predicts output tokens, including reference speech transcription and the dysflu- ency tokens at both word and phoneme levels. To perform the detection, we require several essential components including Feature Extractor and Dysfluent Text Tokenizer to process dysfluent speech and annotated text respectively. Finally, a Whisper Detector is applied for dysfluency prediction. Details are discussed below, and the overall pipeline is illustrated in Fig. 2(a).\n1) Feature Extractor: We employ the default Whisper [20] feature extractor, which outputs a log-mel spectrogram using an n_fft of 400 and a hop_length of 160 on 16 kHz waveform.\n2) Dysfluent Text Tokenizer: We employ word-level tok- enizer and phoneme-level tokenizer on dysfluent text from Text Simulator respectively, as shown in Fig. 2(b).\n\u2022 Word Tokenizer: We utilize the default Whisper tok- enizer [20] which takes annotated text as input and outputs tokens that correspond to the indices of the predicted text within its predefined vocabulary dictionary. The vocabulary comprises 50,258 vocabulary tokens along with additional special tokens to designate language, tasks, etc. Addition- ally, we incorporate 4 dysfluency tokens: [REP], [DEL], [INS], and [PAU] into the vocabulary to enable its address- ing of dysflunecy information.\nPhoneme Tokenizer: Since the phoneme-level annotated text is in CMU phoneme [22] format, we employ the CMU phoneme tokenizer. Its vocabulary consists of 39 stress- free CMU phonemes, added by four dysfluency tokens: [REP], [DEL], [SUB], and [PRO]. Additionally, we retain the special tokens from the default Whisper tokenizer.\n3) Whisper Detector: We employ the Whisper-small [20] seq2seq architecture. It maps a sequence of audio spectrogram features to a sequence of dysfluency-injected text tokens. The encoder processes speech features to generate a series of hidden representations. The decoder autoregressively generates text tokens based on both hidden states from the encoder and previously generated tokens. We also modify the input dimension of the embedding layer and the output dimension of the last projection layer in the decoder to align with the updated or custom dictionary size (word or phoneme)."}, {"title": "IV. EXPERIMENTS", "content": null}, {"title": "A. Datasets", "content": "1) VCTK-Token is a VITS-based [17] simulated dataset proposed in this work, extended from the VCTK [23]. It comprises pairs of dysfluent speech and annotated text. 2) VCTK-Stutter [17] is a rule-based simulated dataset extended from VCTK, where dysfluency is directly injected into the extended acoustic space. 3) VCTK-TTS [17] is a VITS- based simulated dataset extended from VCTK. Dysfluencies are injected into the text space, and a text-to-speech model is used to generate the dysfluent speech. 4) Aphasia Speech [17] We apply real dysfluent speech data comprises 38 English speakers diagnosed with Primary Progressive Aphasia (PPA). VCTK-Stutter and VCTK-TTS are simulated datasets for time-based dysfluency detection. In order to evaluate the rationality and naturalness, we collected the Mean Opinion Score (MOS, 1-5) ratings from 10 people for three simulated datasets. Statistics for these datasets and MOS are presented in Table.I. Notably, since both VCTK-TTS and VCTK-Token speech samples are generated via VITS-based method [17]and extended by VCTK [23], they consistently exhibit similar content and dysfluency characteristics. This similarity is sub- stantiated by the comparable MOS ratings."}, {"title": "B. Metrics", "content": "1) Token Error Rate (TER) measures the accuracy of tran- scribed text by comparing it to a reference text. It calculates the percentage of errors in terms of substitutions, deletions, and insertions. 2) Dysfluency Exist Accuracy (EAcc.) measures the accuracy of detecting whether dysfluencies are present in speech utterances. It is defined by the proportion of correctly identified instances in all evaluated utterances. 3) Dysfluency Class Accuracy (CAcc.) measures the accuracy of correctly detecting types of dysfluencies in utterances. It is defined by the proportion of accurately predicted dysfluent instances in all evaluated utterances. 4) Bound Loss (BL) [17] is the mean squared loss between the predicted and the actual bounds of the dysfluent regions on a time scale using a 20ms sampling frequency. 5) Token Distance(TD) measures the token-level displacement between predicted and actual dysfluency positions in text."}, {"title": "C. Training Details", "content": "The Whisper detector was trained using the CTC-Loss criterion and AdamW optimizer, incorporating gradient norm clipping and a linear learning rate decay to zero after a first 500 warmup steps, with a inital learning rate is le-5. The data was split into a training and testing ratio of 90/10 and processed in batches of 8 per device. For word-level dys- fluency detection, we finetuned the pretrained Whisper-small checkpoint for 6,000 steps. For phoneme-level dysfluency detection, we trained the Whisper detector from scratch for 200,000 steps. All models were trained on 8 RTX A6000 GPUs, taking approximately 18 hours for word-level and 67 hours for phoneme-level training."}, {"title": "D. Time-based Detection and YOLO-Stutter-LCS", "content": "We trained the Time-based dysfluency detector [17] on pre- segment VCTK-TTS datasets using LCS algorithm. To as- sess performance improvements, we evaluated YOLO-Stutter and YOLO-Stutter-LCS on two simulated datasets used in [17], with results shown in Table. III. It indicates a clear improvement in the BL metric, which is due to pre-segmenting dysfluency into shorter durations, thus easing the challenge of localizaion. Additionally, Eacc. and Cacc. remain roughly at the same level as the method not using LCS."}, {"title": "E. Token-based Detection", "content": "To assess the performance of the trained Whisper Detector, we evaluated it on the VCTK-Token dataset, as detailed in Ta- ble. IV. From the Table, the low TERs suggests accurate over- all transcription. Both EAcc. and CAcc. indicate that model's adeptness at detecting and classifying types of dysfluency, with notably better performance in identifying repetition, pause, and prolongation with relative better performance. Comparison between word and phoneme levels shows that detection at the word level is intuitively easier. For TD, the results demonstrate that the model can effectively locate the position of dysfluency. Moreover, localization at the phoneme level proves to be more challenging, as indicated by higher TDs in the table. This increased difficulty is expected, considering that phonemes are smaller units of speech and thus pose greater challenges for precise localization."}, {"title": "F. Unified Benchmark for Time and Token-based Methods", "content": "We conducted a comparative analysis between time-based and token-based methods by evaluating them using both simulated datasets and Aphasia speech data. Since VCTK- TTS and VCTK-Token show similar content and dysfluency characteristics (Table. I), we used them as evaluation datasets for each respective method. The results are detailed in Ta- ble. II. For both simulated datasets and Aphasia speech, the token-based method excels in identifying the existence of dysfluency far more effectively than the time-based method. In terms of classification, for simulated datasets, the token- based method performs comparably to the time-based method, and it surpasses the time-based method in the types of deletion/missing and replace/substitution. For Aphasia speech, the token-based method shows significant improvements in addressing missing/deletion and replace/substitution, and it achieves comparable performance in managing repetition."}, {"title": "V. CONCLUSION", "content": "In this work, we explored the token-based method for dysfluency detection as an alternative to time-based [4], [16], [17] methods. We propose a token-based benchmark with decent performance on both simulated data and disordered speech. We unify time-based and token-based methods with a unified benchmark and show the superiority of the proposed token-based methods. However, a more robust evaluation metric is needed to account for dysfluencies occurring before or after text tokens, which current metrics fail to capture. Privacy concerns still necessitate robust de-identification meth- ods. [24]. Future work will also focus on further scaling efforts at both simulation and model levels. Essentially, we aim to cover many more types of dysfluencies such as filler words, prosody distortion, etc. We also believe this method has the potential to become a framework for a generalized speech rich transcription pipeline. Additionally, we would like to explore fine-grained speech simulation techniques with articulatory features [25]-[28]."}]}