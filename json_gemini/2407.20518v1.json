{"title": "High-Resolution Spatial Transcriptomics from Histology Images using HisToSGE", "authors": ["Zhiceng Shi", "Shuailin Xue", "Fangfang Zhu", "Wenwen Min"], "abstract": "Spatial transcriptomics (ST) is a groundbreaking genomic technology that enables spatial localization analysis of gene expression within tissue sections. However, it is significantly limited by high costs and sparse spatial resolution. An alternative, more cost-effective strategy is to use deep learning methods to predict high-density gene expression profiles from histological images. However, existing methods struggle to capture rich image features effectively or rely on low-dimensional positional coordinates, making it difficult to accurately predict high-resolution gene expression profiles. To address these limitations, we developed HisToSGE, a method that employs a Pathology Image Large Model (PILM) to extract rich image features from histological images and utilizes a feature learning module to robustly generate high-resolution gene expression profiles. We evaluated HisToSGE on four ST datasets, comparing its performance with five state-of-the-art baseline methods. The results demonstrate that HisToSGE excels in generating high-resolution gene expression profiles and performing downstream tasks such as spatial domain identification. All code and public datasets used in this paper are available at https://github.com/wenwenmin/HisToSGE and https://zenodo.org/records/12792163.", "sections": [{"title": "I. INTRODUCTION", "content": "Spatial transcriptomics (ST) is an advanced genomic technology that combines histology and transcriptomics, enabling spatial localization analysis of gene expression on tissue sections [1, 2]. This technology enables researchers to simultaneously observe the spatial heterogeneity of gene expression at the cellular and tissue levels, thereby revealing cell types, functional states, and microenvironment interactions in complex biological processes [3-5]. This is of great significance for understanding disease mechanisms, tissue development, and regeneration.\nDespite the significant advantages of ST technology in biological research, its expensive equipment, reagents, and data analysis costs limit its widespread application [6]. Furthermore, the sparse distribution of sequencing spots on the sample surface and low spatial resolution result in incomplete or missing gene expression information in certain areas, affecting the comprehensiveness and interpretability of the data [7-9]. In contrast, whole-slide images [10] stained with hematoxylin and eosin (H&E) are more convenient and cost-effective, widely used in clinical practice. Predicting ST gene expression profiles using H&E images has become a common and cost-effective research method [11]. Therefore, employing cost-effective methods to address the sparse distribution of sequencing spots and improve spatial resolution in gene expression prediction is crucial for advancing ST technology.\nRecent approaches have enhanced gene expression prediction by incorporating histological images. For instance, STNet [12] employs the DenseNet-121 network to train on image patches corresponding to spots and their gene expression profiles, subsequently predicting gene expression for new spots using these image patches. DeepSpaCE [13] operates similarly to STNet, but it employs the VGG16 network for gene expression prediction. However, these approaches rely exclusively on histological image information. HistoGene [6] integrates both histological images and spatial information. It trains a Vision Transformer (ViT) [14] on image patches, spatial locations, and gene expression profiles from measured points. It then predicts gene expression for unmeasured points by averaging the gene expression profiles predicted from surrounding image patches. THItoGene [15] employs dynamic convolution, multi-effect capsule networks, ViT and graph attention networks. This multi-dimensional attention mechanism adaptively captures the intricate relationships among spatial locations, histological images, and gene expression. Although these methods have achieved good performance, the limited number of histological images used for training may result in insufficient extraction of image features. STAGE [16] improves gene expression prediction by combining spatial information and gene expression data. It utilizes a spatially supervised autoencoder generator to produce the two-dimensional or three-dimensional coordinates of unmeasured points, which are then used to generate the corresponding gene expression data. However, low-dimensional positional coordinates may not be sufficient to represent high-dimensional gene expression profiles.\nTo address these issues, we developed HisToSGE, integrates histological image information, spatial information, and gene expression data to robustly generate high-resolution gene expression profiles in ST. HisToSGE comprises two main modules: the feature extraction module and the feature learning module (Fig. 1). The feature extraction module, utilizing the UNI [17] model trained on one hundred million histological images, generates multimodal feature maps that include RGB, positional, and histological features. The feature learning module employs a multi-head attention mechanism to integrate spot coordinates and learn features from these"}, {"title": "II. PROPOSED METHODS", "content": "We propose HisToSGE to enhance the resolution of spatial gene expression using histological images. During the training phase, we divide the histological images into patches based on the original resolution coordinates of the spots. HisToSGE starts with the Feature Extraction Module to obtain multimodal image feature maps. These feature maps are then processed by the Feature Learning Module. Finally, the Gene Projection Heads project the processed image features into the gene expression dimension. In the testing phase, we downsample the spot coordinates N-fold to achieve high-resolution spot positions, which are then divided into image patches. Using the trained HisToSGE model, we generate high-resolution gene expression data from these patches (Fig. 1)."}, {"title": "B. HisSGE model", "content": "1) Feature Extraction Block of HisSGE: The Feature Extraction Block utilizes the UNI model [17] trained on a large set of histological images to generate multimodal feature maps that include RGB, positional, and histological features.\nWe segment 50 \u00d7 50 pixel image patches from histological images based on the positions of spots as input. Each patch can be represented as $Patch_i \\in R^{50 \\times 50 \\times 3}$, where i \u2208 [1, spot_num].\n$Z_i = UNI(Patch_i), Z_i \\in R^{1 \\times R^{1024}}$\n(1)\nwhere $Z_i$ is the histological feature map.\nSince the spot coordinates correspond directly to the pixel coordinates in the image, we use the corresponding image pixel coordinates as the location feature map $L_i \\in R^{1\\times2}$.\nAdditionally, we obtain the RGB feature map $T_i \\in R^{1\\times3}$ by resizing the entire histological image to the desired size 50 \u00d7 50 using average pooling. From this point, we have obtained histological feature map $Z_i$, location feature map $L_i$, and RGB feature map $T_i$. By stacking these feature maps along the channel dimension, we obtain the multimodal feature map:\n$M_i = concat (Z_i, L_i, T_i) \\in R^{1 \\times R^{1024+2+3}}$\n(2)\nwhere $M_i$ represents the multimodal feature map.\n2) The Feature Learning of HisSGE: The Feature Learning Block uses a multi-head attention mechanism to integrate spot coordinates and learn features from the multimodal feature maps. This block includes layers for multi-head attention to enhance feature representation.\nMulti-head attention is an extension of the attention mechanism, enhancing the model's ability to capture complex patterns and global information in input sequences by simultaneously learning multiple independent sets of attention weights as follows:\n$MHSA(Q, K, V) = [head_1, ..., head_n]W^O$\n(3)\nwhere $W^O$ represents the weight matrix used for aggregating the attention heads, while n denotes the number of heads. Additionally, Q, K, and V correspond to Query, Key, and Value, respectively. The attention mechanism is defined as follows:\n$head_i = Attention(QW^Q_i, KW^K_i,VW^V_i)$\n(4)\n$Attention(Q, K, V) = softmax(\\frac{QKT}{\\sqrt{dk}})V$\n(5)\nwhere $W^Q_i$, $W^K_i$ and $W^V_i$ are weight matrices. The term $(\\frac{QKT}{\\sqrt{dk}})$ is called Attention Map, whose shape is N \u00d7 N. The term V is the value of the self-attention mechanism, where V = Q = K.\nFor the multimodal feature map $M_i$, we further refine the feature representations using the multi-head attention mechanism and integrate spot location information with learnable positional encodings:\n$H_i = MHSA(M_i + PE_i), H_i \\in R^{1 \\times R^{1024+2+3}}$\n(6)\nwhere $PE_i$ represents the positional encoding of spot.\n3) The Gene Projection Heads of HisSGE: The Gene Projection Heads take the learned features from the Feature Learning Block and project them into the gene expression dimension.\n$X_{pred} = MLP(H_i), X_{pred} \\in R^{1 \\times R^{gene\\_dim}}$\n(7)\nwhere $X_{pred}$ represents the predicted gene expression."}, {"title": "4) The Loss function of HisSGE:", "content": "The loss function of HisSGE is designed to minimize the difference between the predicted and actual gene expression data. It ensures the model accurately generates high-resolution gene expression profiles from the input histological images. We apply the mean square errors loss as follows:\n$Loss = \\sum ||X_{observed} - X_{pred} ||^2$\n(8)\nwhere $X_{observed}$ and $X_{pred}$ are the observed and predicted gene expression, respectively."}, {"title": "C. Evaluation metrics", "content": "We use Pearson Correlation Coefficient (PCC), Mean Squared Error (MSE), and Mean Absolute Error (MAE) to evaluate the proposed method against baselines.\n$PCC = \\frac{Cov (X_{observed}, X_{pred})}{\\sqrt{Var(X_{observed}) \\times Var(X_{pred})}}$\n(9)\nwhere Cov() is the covariance, and Var() is the variance. $X_{observed}$ and $X_{pred}$ are the observed and predicted gene expression, respectively.\n$MSE = \\frac{1}{N} \\sum_{i=1}^{N}|X_{observed} - X_{pred}|^2$\n(10)\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N}|X_{observed} - X_{pred}|$\n(11)\nIn the assessment of spatial clustering performance, we employ the Adjusted Rand Index (ARI) to measure the correlation"}, {"title": "III. EXPERIMENTAL RESULTS", "content": "Four publicly available ST datasets were used in this study (Table I).\nImage data preprocessing. For H&E images, we partitioned a W \u00d7 H pixel region around each sequencing spot based on its positional coordinates. Both W and H are set to 50.\nGene expression data preprocessing. The raw gene expression counts were normalized and log-transformed. The HisToSGE model uses the top 1,000 highly variable genes from the normalized data as input.\nConstruction of unmeasured spots. To maximize coverage of tissue sections, we translate the measurement points in $N^1$ specific directions by $N^1$ specific distances according to the sampling multiple N, thereby constructing unmeasured points. Given that the translation direction and distance for each measurement point are identical, we utilize polar coordinates to describe the relationship between the unmeasured points and the measurement points. Let R be the Euclidean distance between adjacent measured spots. We denote r and @ as the polar radius and polar angle of the polar coordinate system. Specifically, for an 8-fold downsampling, we translated the measured spots to (r,\u03b8) = ($\\frac{R}{2\\sqrt{2}}$, $\\frac{\\pi}{4}$), ($\\frac{R}{\\sqrt{2}}$, 0), ($\\frac{R}{2\\sqrt{2}}$, $\\frac{3\\pi}{4}$), ($\\frac{R}{\\sqrt{2}}$, $\\frac{\\pi}{2}$), ($\\frac{R}{2\\sqrt{2}}$, $\\frac{5\\pi}{4}$), ($\\frac{R}{\\sqrt{2}}$, \\pi), ($\\frac{R}{2\\sqrt{2}}$, $\\frac{7\\pi}{4}$), ($\\frac{R}{\\sqrt{2}}$, $\\frac{3\\pi}{2}$).\nIn this study, we selected five representative state-of-the-art"}, {"title": "B. Baseline methods", "content": "In this study, we selected five representative state-of-the-art methods:\n\u2022 DLPFC dataset [18] consists of 12 sections of the dor- solateral prefrontal cortex (DLPFC) sampled from three individuals. The number of spots for each section ranges from 3498 to 4789. The original authors have manually annotated the areas of the DLPFC layers and white matter. The datasets are available in the spatialLIBD package.\n\u2022 MouseBrain dataset [19] includes a coronal brain sec- tion sample from an adult mouse, with 2903 sampled spots. The datasets are available in 10x Genomics Web- site.\n\u2022 Human Breast Cancer1 (BC1) dataset [20] includes a fresh frozen invasive ductal carcinoma breast tissue section sample, with 3813 sampled spots. The datasets are available in 10x Genomics Website.\n\u2022 Human Breast Cancer2 (BC2) dataset [20] includes a formalin-fixed invasive breast carcinoma tissue section sample, with 2518 sampled spots. The datasets are avail- able in 10x Genomics Website.\nC. Implementation Details\nFor all baselines, we used the default parameters specified in the original papers. Our experiments were executed on a single NVIDIA RTX 4090 GPU using PyTorch (version 2.2.0) and Python 3.10. The training protocol was established for 1000 epochs, entailinga batch size of 512 and a learning rate set at 0.001.\nD. HisToSGE enables more accurate generation of high-resolution gene expression.\nTo quantitatively evaluate the performance of HisToSGE and other methods in generating high-density gene expression"}, {"title": "E. HisToSGE enhances gene expression patterns", "content": "Here, we downsampled the low-resolution histological images by a factor of N and used these images as the training set to generate high-resolution gene expression data. We presented the generated high-resolution gene expression data with downsampling factors ranging from 2x to 8x. We compared the raw and generated gene expression profiles of several marker genes, including MOBP, SCGB2A2, HBB, and PCP4 (Fig. 4), which are highly expressed in myelin. Compared to the raw data, the generated gene expression profiles displayed clearer expression patterns, demonstrating the effective enhancement capability of HisToSGE."}, {"title": "IV. CONCLUSIONS", "content": "The generation of high-resolution ST is a pivotal challenge in both experimental and computational biology. In this study, we developed a robust method, HisToSGE, to integrate histological images, gene expression data, and spatial location information. This method aims to learn rich image features and continuous spatial expression patterns, and predict spatial gene expression profiles for unmeasured spots at any resolution. HisToSGE consists of two main modules: the feature extraction module and the feature learning module. The feature extraction module generates rich image features and combines multimodal image features. The feature learning module employs a multi-head attention mechanism to integrate spot coordinates and learn features from multimodal feature maps, thereby enhancing feature representation. We compared our method with other approaches on multiple real ST datasets. The results demonstrate that, in generating high-density gene expression profiles, our method improves the average PCC by 9% to 32% compared to state-of-the-art methods. Additionally, our method not only enhances gene expression patterns but also effectively preserves the original spatial structure."}]}