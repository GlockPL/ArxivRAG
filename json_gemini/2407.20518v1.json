{"title": "High-Resolution Spatial Transcriptomics from Histology Images using HisToSGE", "authors": ["Zhiceng Shi", "Shuailin Xue", "Fangfang Zhu", "Wenwen Min"], "abstract": "Spatial transcriptomics (ST) is a groundbreaking genomic technology that enables spatial localization analysis of gene expression within tissue sections. However, it is significantly limited by high costs and sparse spatial resolution. An alternative, more cost-effective strategy is to use deep learning methods to predict high-density gene expression profiles from histological images. However, existing methods struggle to capture rich image features effectively or rely on low-dimensional positional coordinates, making it difficult to accurately predict high-resolution gene expression profiles. To address these limitations, we developed HisToSGE, a method that employs a Pathology Image Large Model (PILM) to extract rich image features from histological images and utilizes a feature learning module to robustly generate high-resolution gene expression profiles. We evaluated HisToSGE on four ST datasets, comparing its performance with five state-of-the-art baseline methods. The results demonstrate that HisToSGE excels in generating high-resolution gene expression profiles and performing downstream tasks such as spatial domain identification. All code and public datasets used in this paper are available at https://github.com/wenwenmin/HisToSGE and https://zenodo.org/records/12792163.", "sections": [{"title": "I. INTRODUCTION", "content": "Spatial transcriptomics (ST) is an advanced genomic technology that combines histology and transcriptomics, enabling spatial localization analysis of gene expression on tissue sections [1, 2]. This technology enables researchers to simultaneously observe the spatial heterogeneity of gene expression at the cellular and tissue levels, thereby revealing cell types, functional states, and microenvironment interactions in complex biological processes [3-5]. This is of great significance for understanding disease mechanisms, tissue development, and regeneration.\nDespite the significant advantages of ST technology in biological research, its expensive equipment, reagents, and data analysis costs limit its widespread application [6]. Furthermore, the sparse distribution of sequencing spots on the sample surface and low spatial resolution result in incomplete or missing gene expression information in certain areas, affecting the comprehensiveness and interpretability of the data [7-9]. In contrast, whole-slide images [10] stained with hematoxylin and eosin (H&E) are more convenient and cost-effective, widely used in clinical practice. Predicting ST gene expression profiles using H&E images has become a common and cost-effective research method [11]. Therefore, employing cost-effective methods to address the sparse distribution of sequencing spots and improve spatial resolution in gene expression prediction is crucial for advancing ST technology.\nRecent approaches have enhanced gene expression prediction by incorporating histological images. For instance, STNet [12] employs the DenseNet-121 network to train on image patches corresponding to spots and their gene expression profiles, subsequently predicting gene expression for new spots using these image patches. DeepSpaCE [13] operates similarly to STNet, but it employs the VGG16 network for gene expression prediction. However, these approaches rely exclusively on histological image information. HistoGene [6] integrates both histological images and spatial information. It trains a Vision Transformer (ViT) [14] on image patches, spatial locations, and gene expression profiles from measured points. It then predicts gene expression for unmeasured points by averaging the gene expression profiles predicted from surrounding image patches. THItoGene [15] employs dynamic convolution, multi-effect capsule networks, ViT and graph attention networks. This multi-dimensional attention mechanism adaptively captures the intricate relationships among spatial locations, histological images, and gene expression. Although these methods have achieved good performance, the limited number of histological images used for training may result in insufficient extraction of image features. STAGE [16] improves gene expression prediction by combining spatial information and gene expression data. It utilizes a spatially supervised autoencoder generator to produce the two-dimensional or three-dimensional coordinates of unmeasured points, which are then used to generate the corresponding gene expression data. However, low-dimensional positional coordinates may not be sufficient to represent high-dimensional gene expression profiles.\nTo address these issues, we developed HisToSGE, integrates histological image information, spatial information, and gene expression data to robustly generate high-resolution gene expression profiles in ST. HisToSGE comprises two main modules: the feature extraction module and the feature learning module (Fig. 1). The feature extraction module, utilizing the UNI [17] model trained on one hundred million histological images, generates multimodal feature maps that include RGB, positional, and histological features. The feature learning module employs a multi-head attention mechanism to integrate spot coordinates and learn features from these multimodal maps, thereby enhancing feature representation. We evaluated HisToSGE using ST four datasets and compared its performance with five existing methods. Our results demonstrate that HisToSGE can accurately generate high-resolution gene expression profiles, enhance gene expression patterns, and preserve the original gene expression spatial structure.\nThe main contributions of our proposed method are:\n\u2022 We developed HisToSGE, a deep learning-based model that predicts high-resolution spatial gene expression profiles from histological images. HisToSGE employs a large histological image model to extract rich image features and uses a feature learning module to integrate the spatial positions of spots, enhancing feature representation.\n\u2022 Our method was compared with other approaches on multiple real ST datasets. The results demonstrate that our method improves the average Pearson Correlation Coefficient (PCC) by 9% to 32% in generating high-density gene expression profiles compared to state-of-the-art methods. Additionally, Our approach not only enhances the original gene expression patterns but also effectively preserves the original spatial structure.\n\u2022 Compared to other image-to-gene expression methods, HisToSGE can generate spatial gene expression profiles at any desired resolution."}, {"title": "II. PROPOSED METHODS", "content": "We propose HisToSGE to enhance the resolution of spatial gene expression using histological images. During the training phase, we divide the histological images into patches based on the original resolution coordinates of the spots. HisToSGE starts with the Feature Extraction Module to obtain multimodal image feature maps. These feature maps are then processed by the Feature Learning Module. Finally, the Gene Projection Heads project the processed image features into the gene expression dimension. In the testing phase, we downsample the spot coordinates N-fold to achieve high-resolution spot positions, which are then divided into image patches. Using the trained HisToSGE model, we generate high-resolution gene expression data from these patches (Fig. 1)."}, {"title": "B. HisToSGE model", "content": "1) Feature Extraction Block of HisSGE: The Feature Extraction Block utilizes the UNI model [17] trained on a large set of histological images to generate multimodal feature maps that include RGB, positional, and histological features.\nWe segment 50 \u00d7 50 pixel image patches from histological images based on the positions of spots as input. Each patch can be represented as $\\text{Patch}_{i} \\in \\mathbb{R}^{50 \\times 50 \\times 3}$, where $i \\in [1, \\text{spot\\_num}]$.\n$Z_{i} = \\text{UNI}(\\text{Patch}), Z_{i} \\in \\mathbb{R}^{1 \\times 1024}$ (1)\nwhere $Z_i$ is the histological feature map.\nSince the spot coordinates correspond directly to the pixel coordinates in the image, we use the corresponding image pixel coordinates as the location feature map $L_i \\in \\mathbb{R}^{1 \\times 2}$.\nAdditionally, we obtain the RGB feature map $T_i \\in \\mathbb{R}^{1 \\times 3}$ by resizing the entire histological image to the desired size 50 \u00d7 50 using average pooling. From this point, we have obtained histological feature map $Z_i$, location feature map $L_i$, and RGB feature map $T_i$. By stacking these feature maps along the channel dimension, we obtain the multimodal feature map:\n$M_{i} = \\text{concat} (Z_{i}, L_{i}, T_{i}) \\in \\mathbb{R}^{1 \\times 1024+2+3}$ (2)\nwhere $M_i$ represents the multimodal feature map.\n2) The Feature Learning of HisSGE: The Feature Learning Block uses a multi-head attention mechanism to integrate spot coordinates and learn features from the multimodal feature maps. This block includes layers for multi-head attention to enhance feature representation.\nMulti-head attention is an extension of the attention mechanism, enhancing the model's ability to capture complex patterns and global information in input sequences by simultaneously learning multiple independent sets of attention weights as follows:\n$\\text{MHSA}(Q, K, V) = [\\text{head}_{1}, ..., \\text{head}_{n}]W^{O}$ (3)\nwhere $W^{O}$ represents the weight matrix used for aggregating the attention heads, while $n$ denotes the number of heads. Additionally, $Q, K,$ and $V$ correspond to Query, Key, and Value, respectively. The attention mechanism is defined as follows:\n$\\text{head}_{i} = \\text{Attention}(QW_{i}^{Q}, KW_{i}^{K}, VW_{i}^{V})$ (4)\n$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$ (5)\nwhere $W_{i}^{Q}, W_{i}^{K}$ and $W_{i}^{V}$ are weight matrices. The term $(\\frac{QK^{T}}{\\sqrt{d_{k}}})$ is called Attention Map, whose shape is $N \\times N$. The term $V$ is the value of the self-attention mechanism, where $V = Q = K$.\nFor the multimodal feature map $M_i$, we further refine the feature representations using the multi-head attention mechanism and integrate spot location information with learnable positional encodings:\n$H_{i} = \\text{MHSA}(M_{i} + \\text{PE}_{i}), H_{i} \\in \\mathbb{R}^{1 \\times 1024+2+3}$ (6)\nwhere $PE_i$ represents the positional encoding of spot.\n3) The Gene Projection Heads of HisSGE: The Gene Projection Heads take the learned features from the Feature Learning Block and project them into the gene expression dimension.\n$\\text{X}_{pred} = \\text{MLP}(H_{i}), \\text{X}_{pred} \\in \\mathbb{R}^{1 \\times gene\\_dim}$ (7)\nwhere $X_{pred}$ represents the predicted gene expression."}, {"title": "4) The Loss function of HisSGE:", "content": "The loss function of HisSGE is designed to minimize the difference between the predicted and actual gene expression data. It ensures the model accurately generates high-resolution gene expression profiles from the input histological images. We apply the mean square errors loss as follows:\n$\\text{Loss} = \\sum ||X_{\\text{observed}} \u2013 X_{\\text{pred}}||^{2}$ (8)\nwhere $X_{\\text{observed}}$ and $X_{\\text{pred}}$ are the observed and predicted gene expression, respectively."}, {"title": "C. Evaluation metrics", "content": "We use Pearson Correlation Coefficient (PCC), Mean Squared Error (MSE), and Mean Absolute Error (MAE) to evaluate the proposed method against baselines.\n$\\text{PCC} = \\frac{\\text{Cov} (X_{\\text{observed}}, X_{\\text{pred}})}{\\sqrt{\\text{Var}(X_{\\text{observed}}) \\times \\text{Var}(X_{\\text{pred}})}}$ (9)\nwhere Cov() is the covariance, and Var() is the variance. $X_{\\text{observed}}$ and $X_{\\text{pred}}$ are the observed and predicted gene expression, respectively.\n$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N}|X_{\\text{observed}} \u2013 X_{\\text{pred}}|^{2}$ (10)\n$\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N}|X_{\\text{observed}} \u2013 X_{\\text{pred}}|$ (11)\nIn the assessment of spatial clustering performance, we employ the Adjusted Rand Index (ARI) to measure the correlation between the clustering outcomes and the actual pathological annotation regions. The ARI can be mathematically expressed as follows:\n$\\text{ARI} = \\frac{\\sum_{i} \\binom{n_{i}}{2} \u2013 \\frac{[\\sum_{i} \\binom{a_{i}}{2}][\\sum_{j} \\binom{b_{j}}{2}]}{\\binom{n}{2}}}{\\frac{[\\sum_{i} \\binom{a_{i}}{2} + \\sum_{j} \\binom{b_{j}}{2}]}{2} \u2013 \\frac{[\\sum_{i} \\binom{a_{i}}{2}][\\sum_{j} \\binom{b_{j}}{2}]}{\\binom{n}{2}}}$ (12)\nwhere $a_i$ and $b_j$ are the number of samples appearing in the $i$-th predicted cluster and the $j$-th true cluster, respectively. $n_{ij}$ means the number of overlaps between the $i$-th predicted cluster and the $j$-th true cluster."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "Four publicly available ST datasets were used in this study (Table I).\n\u2022 DLPFC dataset [18] consists of 12 sections of the dorsolateral prefrontal cortex (DLPFC) sampled from three individuals. The number of spots for each section ranges from 3498 to 4789. The original authors have manually annotated the areas of the DLPFC layers and white matter. The datasets are available in the spatialLIBD package.\n\u2022 MouseBrain dataset [19] includes a coronal brain section sample from an adult mouse, with 2903 sampled spots. The datasets are available in 10x Genomics Website.\n\u2022 Human Breast Cancer1 (BC1) dataset [20] includes a fresh frozen invasive ductal carcinoma breast tissue section sample, with 3813 sampled spots. The datasets are available in 10x Genomics Website.\n\u2022 Human Breast Cancer2 (BC2) dataset [20] includes a formalin-fixed invasive breast carcinoma tissue section sample, with 2518 sampled spots. The datasets are available in 10x Genomics Website.\nImage data preprocessing. For H&E images, we partitioned a $W \\times H$ pixel region around each sequencing spot based on its positional coordinates. Both $W$ and $H$ are set to 50.\nGene expression data preprocessing. The raw gene expression counts were normalized and log-transformed. The HisToSGE model uses the top 1,000 highly variable genes from the normalized data as input.\nConstruction of unmeasured spots. To maximize coverage of tissue sections, we translate the measurement points in $N$ 1 specific directions by $N$ 1 specific distances according to the sampling multiple N, thereby constructing unmeasured points. Given that the translation direction and distance for each measurement point are identical, we utilize polar coordinates to describe the relationship between the unmeasured points and the measurement points. Let $R$ be the Euclidean distance between adjacent measured spots. We denote $r$ and $\\theta$ as the polar radius and polar angle of the polar coordinate system. Specifically, for an 8-fold downsampling, we translated the measured spots to $(r,\\theta) = (\\frac{R}{2\\sqrt{2}}, 0), (\\frac{R}{\\sqrt{2}}, \\frac{\\pi}{4}), (\\frac{R}{\\sqrt{2}}, \\frac{\\pi}{2}), (\\frac{R}{\\sqrt{2}}, \\frac{3\\pi}{4}), (\\frac{R}{\\sqrt{2}}, \\pi), (\\frac{R}{\\sqrt{2}}, \\frac{5\\pi}{4}), (\\frac{R}{\\sqrt{2}}, \\frac{3\\pi}{2}), (\\frac{R}{\\sqrt{2}}, \\frac{7\\pi}{4})$."}, {"title": "B. Baseline methods", "content": "In this study, we selected five representative state-of-the-art methods:\n\u2022 STnet [12] utilizes DenseNet-121 as the image encoder to extract H&E image features, which were then embedded into the feature space and projected onto the dimension of gene expression through fully connected layers.\n\u2022 DeepSpaCE [13] operates similarly to STNet, but it employs the VGG16 network for gene expression prediction.\n\u2022 HisToGene [6] adopts a vision Transformer as the image encoder, leveraging self-attention mechanism to extract global features, which were subsequently projected onto the dimension of gene expression through fully connected layers.\n\u2022 THItoGene [15] uses H&E images as input and employed dynamic convolutional and capsule networks to capture signals of potential molecular features within histological samples.\n\u2022 STAGE [16] utilizes a spatially supervised autoencoder generator to produce the two-dimensional or three-dimensional coordinates of unmeasured points, which are then used to generate the corresponding high resolution gene expression data."}, {"title": "C. Implementation Details", "content": "For all baselines, we used the default parameters specified in the original papers. Our experiments were executed on a single NVIDIA RTX 4090 GPU using PyTorch (version 2.2.0) and Python 3.10. The training protocol was established for 1000 epochs, entailinga batch size of 512 and a learning rate set at 0.001."}, {"title": "D. HisToSGE enables more accurate generation of high-resolution gene expression.", "content": "To quantitatively evaluate the performance of HisToSGE and other methods in generating high-density gene expression profiles across all datasets, we randomly removed 50% of the spots, using the remaining spots as the training set and the removed spots as the test set. The generation performance was assessed by comparing the PCC, MSE, and MAE between the original data and the recovered data from different methods.\nAs shown in Table II, HisToSGE achieved the highest PCC and the lowest MSE and MAE across all datasets. In evaluating tissue slices from 151676 of DLPFC, Mouse Brain, BC1, and BC2, the HisToSGE method demonstrates superior performance compared to the second-best method, STAGE. HisToSGE achieves PCC that are 27%, 9%, 32%, and 15% higher than those of STAGE for the respective tissue slices. In terms of error reduction, HisToSGE demonstrates MSE that are 14%, 11%, 11%, and 4% lower. Similarly, the method also achieves MAE that are 28%, 14%, 13%, and 9% lower compared to STAGE.\nTo visually analyze the performance of different methods in generating high-density gene expression profiles, we conducted spatial visualization of the partially generated data and compared it with the downsampled data and the true expression. We presented the gene expression generation results across four datasets using various methods (Fig. 2 and Fig. 3). Compared to other methods, HisToSGE's generation results are closer to the true gene expression patterns, demonstrating superior performance. As shown in Fig. 2A, for the marker genes PCP4, FABP4, and MBP on the 151676 slice from the DLPFC dataset, the PCC values between HisToSGE's generated gene expression and the true gene expression are 0.71, 0.72, and 0.85, respectively, which are 21%, 17%, and 7% higher than those achieved by the second-best method, STAGE. Additionally, as shown in Fig. 2B and Fig. 3, HisToSGE also demonstrates excellent performance in gene expression generation on the Mouse Brain, BC1, and BC2 datasets. In contrast, DeepSpaCE and STNet showed relatively poor performance, failing to effectively retain the original gene expression patterns."}, {"title": "E. HisToSGE enhances gene expression patterns", "content": "Here, we downsampled the low-resolution histological images by a factor of N and used these images as the training set to generate high-resolution gene expression data. We presented the generated high-resolution gene expression data with downsampling factors ranging from 2x to 8x. We compared the raw and generated gene expression profiles of several marker genes, including MOBP, SCGB2A2, HBB, and PCP4 (Fig. 4), which are highly expressed in myelin. Compared to the raw data, the generated gene expression profiles displayed clearer expression patterns, demonstrating the effective enhancement capability of HisToSGE."}, {"title": "F. HisToSGE can better preserve spatial structures", "content": "To further explore the effectiveness of HisToSGE in spatial domain recognition tasks, we conducted clustering analysis on both the original and generated data using K-means, STAGATE [21], and STMask [22] clustering methods. Manual annotations served as the ground truth, and we evaluated the results using the ARI as the evaluation metric.\nIn spatial domain recognition, HisToSGE showed improvements over STAGE and THItoGene in K-means (ARI scores for the recovered data of HisToSGE, STAGE, and THItoGene were 0.37, 0.31, and 0.28, respectively), STAGATE (ARI scores were 0.52, 0.50, and 0.44, respectively), and STMask (ARI scores were 0.54, 0.51, and 0.42, respectively) (Fig. 5 A, B and C).\nAdditionally, we visualized the clustering results of slices 151673 and 151674 from the DLPFC datasets using K-means, STAGATE, and STMask methods. The clustering results in Fig. 5D demonstrate that HisToSGE consistently outperforms other methods across different slices and clustering techniques. In slice 151673, HisToSGE achieves higher ARI scores than STAGE and THItoGene across K-means, STAGATE, and STMask clustering methods, with particularly notable performance in the STMask clustering. This trend is also observed in slice 151674, where HisToSGE consistently achieves higher ARI scores than the other methods in all clustering approaches. These results indicate that HisToSGE is more accurate and robust in spatial domain recognition, effectively capturing the spatial structure of the data."}, {"title": "G. Ablation studies", "content": "To evaluate the learning capabilities of various feature extraction backbone networks for image features, we conducted ablation experiments (Table III). We tested three different backbone networks, and the results indicated that the Transformer network achieved the highest performance. Furthermore, the FeedForward backbone network surpassed the performance of the second-ranked method, STAGE, suggesting that rich histological image features can effectively represent gene expression profiles.\nWe also developed the STAGE_Plus model, which utilizes rich image features extracted by the UNI module to replace gene expression features as the input for STAGE. In this model, STAGE uses these image features to generate two-dimensional coordinates of spots through an encoder, which are subsequently employed to generate gene expression profiles. The findings suggest that two-dimensional coordinates alone are insufficient to represent the rich image and gene expression features. In contrast, the feature learning module of HisToSGE can effectively learn these rich image features, enabling high-resolution gene expression mapping."}, {"title": "IV. CONCLUSIONS", "content": "The generation of high-resolution ST is a pivotal challenge in both experimental and computational biology. In this study, we developed a robust method, HisToSGE, to integrate histological images, gene expression data, and spatial location information. This method aims to learn rich image features and continuous spatial expression patterns, and predict spatial gene expression profiles for unmeasured spots at any resolution. HisToSGE consists of two main modules: the feature extraction module and the feature learning module. The feature extraction module generates rich image features and combines multimodal image features. The feature learning module employs a multi-head attention mechanism to integrate spot coordinates and learn features from multimodal feature maps, thereby enhancing feature representation. We compared our method with other approaches on multiple real ST datasets. The results demonstrate that, in generating high-density gene expression profiles, our method improves the average PCC by 9% to 32% compared to state-of-the-art methods. Additionally, our method not only enhances gene expression patterns but also effectively preserves the original spatial structure."}, {"title": "ACKNOWLEDGMENT", "content": "The work was supported in part by the National Natural Science Foundation of China (62262069), in part by the Program of Yunnan Key Laboratory of Intelligent Systems and Computing (202205AG070003), and the Yunnan Talent Development Program - Youth Talent Project."}]}