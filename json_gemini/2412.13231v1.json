{"title": "C2F-TP: A Coarse-to-Fine Denoising Framework for Uncertainty-Aware Trajectory Prediction", "authors": ["Zichen Wang", "Hao Miao", "Senzhang Wang", "Renzhi Wang", "Jianxin Wang", "Jian Zhang"], "abstract": "Accurately predicting the trajectory of vehicles is critically important for ensuring safety and reliability in autonomous driving. Although considerable research efforts have been made recently, the inherent trajectory uncertainty caused by various factors including the dynamic driving intends and the diverse driving scenarios still poses significant challenges to accurate trajectory prediction. To address this issue, we propose C2F-TP, a coarse-to-fine denoising framework for uncertainty-aware vehicle trajectory prediction. C2F-TP features an innovative two-stage coarse-to-fine prediction process. Specifically, in the spatial-temporal interaction stage, we propose a spatial-temporal interaction module to capture the inter-vehicle interactions and learn a multimodal trajectory distribution, from which a certain number of noisy trajectories are sampled. Next, in the trajectory refinement stage, we design a conditional denoising model to reduce the uncertainty of the sampled trajectories through a step-wise denoising operation. Extensive experiments are conducted on two real datasets NGSIM and highD that are widely adopted in trajectory prediction. The result demonstrates the effectiveness of our proposal.", "sections": [{"title": "Introduction", "content": "Vehicle trajectory prediction aims to predict the future trajectory of the target agent based on the historical trajectories of itself and its surrounding neighbours. Accurately predicting the future trajectory of vehicles is crucial for many autonomous driving applications, including optimal driving path planning, making accurate driving decisions in dynamic environments, and enhancing driving safety (Liu et al. 2021; Lun et al. 2024).\nTraditionally, statistical models are employed to predict future trajectories based on the historical trajectories of individual agents (Hearst et al. 1998; Williams and Rasmussen 2006). Nonetheless, they do not consider the interactions between the target agent and the surrounding agents, which deteriorates the prediction performance. To address this issue, various deep learning based models (Alahi et al. 2016; Deo"}, {"title": "Related Work", "content": "Trajectory Prediction. Trajectory prediction attracts increasing interest due to the increasing availability of trajectory data (Krajewski et al. 2018; Deo and Trivedi 2018). Recently, significant advances have been achieved in developing deep learning based methods (Liu et al. 2021) for trajectory prediction. First, RNNs (Morton, Wheeler, and Kochenderfer 2016; Wang, Cao, and Philip 2020) have been applied to predict the future movement paths of agents with historical trajectory data. However, in complex traffic environments, surrounding vehicles may significantly affect the prediction result due to the perception of interactions between agents. To address this issue, many recent studies (Alahi et al. 2016; Mohamed et al. 2020; Chen et al. 2022; Wang et al. 2023; Miao et al. 2024a) try to model the spatial interactions across agents by means of various emerging pooling techniques. Despite their advances, existing methods suffer from effective data uncertainty capturing.\nDenoising diffusion probabilistic models. Denoising diffusion probabilistic models have achieved advanced performance on various applications (Sohl-Dickstein et al. 2015; Ho, Jain, and Abbeel 2020), such as image generation (Dhariwal and Nichol 2021) and audio synthesis (Kong et al. 2020). The diffusion model is first proposed by DPM (Sohl-Dickstein et al. 2015), which seeks to imitate the diffusion process in non-equilibrium statistical physics and reconstruct the data distribution using the denoising model. Then, a line of studies are proposed to improve the efficiency of the diffusion model by developing fast sampling techniques, such as DDPM (Ho, Jain, and Abbeel 2020) and DDIM (Song, Meng, and Ermon 2021). Recently, the diffusion model has been applied to trajectory data, which obtain good performance in trajectory-related applications, such as trajectory synthesis (Zhu et al. 2024) and trajectory prediction (Mao et al. 2023). However, these methods fall short in effectively modeling the interactions across agents."}, {"title": "Problem Definition and Preliminaries", "content": "Problem Definition. We consider trajectory prediction as a sequence generation problem which generates the future trajectory of an agent based on the historical trajectories of itself and surrounding agents. For a target agent, given its historical trajectories $X_{tar}^{t-T_h:t} = \\{X_{t-T_h}^{tar},..., X_{t-2}^{tar}, X_{t-1}^{tar}, X_t^{tar}\\}$ and trajectories of surrounding N agents $X_N = \\{X_1,X_2, ..., X_i, ..., X_N\\}$ over $T_h$ time steps, we aim to predict its future trajectory $Y_{tar} = \\{Y_{t+1}^{tar}, Y_{t+2}^{tar},..., Y_{t+T_f}^{tar}\\}$ over $T_f$ future time steps, where $X_t$ is the two-dimensional coordinate (x, y) of the agent v at the i-th time step.\nPreliminaries. In quantum mechanics, an entity (e.g., electron, photon) is usually represented by a wave function (e.g., de Broglie wave) containing both amplitude and phase (Arndt et al. 1999). Inspired by Wave-MLP (Tang et al. 2022), an agent is represented as a wave $\\check{z}_j$ with both amplitude and phase information, i.e.,\n$\\check{z}_j = |z_j| e^{i\\theta_j}, j = 1,2,\\dots,n, \\qquad(1)$\nwhere i is the imaginary unit, $\\odot$ is element-wise multiplication. The amplitude $|z_j|$ represents the content of each agent. $\\Theta$ indicates the phase, which is the current location of an agent within a wave period. With both amplitude and phase, each agent $\\check{z}_i$ is represented in the complex-value domain. We view agent interaction as a superposition of waves, supposing $Z_r = \\check{z}_i + \\check{z}_j$ is the aggregated results of agent"}, {"title": "Methodology", "content": "Figure 3 shows the framework of C2F-TP, which is a coarse-to-fine denoising framework for uncertainty-aware vehicle trajectory prediction. C2F-TP contains a spatial-temporal interaction module and a refinement module. The former captures the dynamics and temporal correlation of inter-vehicle interactions and generates a multimodal future trajectory distribution for the target vehicle. Then we sample k noisy trajectories from the multimodal trajectory distribution. Next, the refinement module denoises the k trajectories based on a conditional denoising network and generates the final refined future trajectories.\nSpatial-Temporal Interaction Module\nIn order to accurately capture the interactions between the target and surrounding agents, we propose the Spatial-Temporal Interaction Module to generate an initial future trajectory distribution. This module contains motion encoding, interaction pooling and re-weighted multimodal trajectory predictor. Next, we will describe the module in detail.\nMotion Encoding. Following the previous approach (Deo and Trivedi 2018; Wang et al. 2023), we assume that the neighbors of the target agent (i.e., surrounding agents) are within \u00b190 feet in the longitude direction and within four adjacent lanes centered on the target agent, as shown on the"}, {"title": "Interaction Pooling", "content": "Interactions between agents are highly dynamic and time-dependent. Simultaneously capturing the dynamic and temporal correlations of interactions between agents helps to more accurately model the future trajectories of target agents. Motivated by WSiP (Wang et al. 2023), Interaction Pooling considers the agents in the road scene as waves, and uses the superposition of waves to dynamically model the interactions between agents. A multi-head self-attention is also adopted to capture the temporal correlations between social representations.\nInspired by Wave-MLP (Tang et al. 2022), for any agent $a_i$, given the hidden state of the current timestamp $h_i$ of the agent, we use a plain fully connected layer (Plain-FC) to obtain the amplitude embedding $z_i$. Therefore, amplitude could represent the dynamics of the agent. Phase is used to modulate the aggregation of information from surrounding agents. We also use Plain-FC as follows to learn the phase embedding $\\theta_i$ so that it can be dynamically adapted to the motion state of the agent,\n$z_i = Plain-FC (h_i, W^z), \\qquad(4)$\n$\\theta_i = Plain-FC (h_i, W^{\\theta}), \\qquad(4)$\nwhere $W^z$ and $W^{\\theta}$ are learnable weights. We design a Surrounding-FC module for aggregating different agent interaction information based on the token mixing module of Wave-MLP (Tang et al. 2022) as follows\n$\\bar{o}_j = Surrounding-FC (\\check{Z}, M_{pos}, W^t);\\qquad j = 1,2,\\dots, n,\\qquad (5)$\nwhere $\\check{Z} = [\\check{z}_1, \\check{z}_2,\\dots, \\check{z}_n]$ denotes n agent waves, $M_{pos}$ denotes the position mask, and $W^t$ is a learnable weight. The amplitude and phase information of different agents is aggregated by Surrounding-FC and the resulting $\\bar{o}_j$ is a complex-valued representation of the aggregated features. Then we obtain the real-valued output $o_j$ by summing the real and imaginary parts of $\\bar{o}_j$ with the weights (Jacobs and Steck 2006) as follows\n$o_j = \\sum_k W^r cos \\theta_k + W^i z_k \\odot sin\\theta_k, \\qquad(6)$\n$j = 1,2,\\dots, \\eta,\\qquad (6)$\nwhere $W^r$ and $W^i$ are learnable weights, $\\odot$ is the element-wise multiplication and $j$ indicates the j-th output representation. In the above equation, the phase $\\theta_k$ is dynamically adjusted according to the history states of various agents. Different agents with both the phase and amplitude information interact with each other through Surrounding-FC and obtain the social encoding.\nThe above social interaction feature is calculated for each timestamp t separately without considering the temporal"}, {"title": "Re-weighted Multimodal Trajectory Predictor", "content": "To predict the multimodal trajectory of a vehicle, we propose the Re-weighted Multimodal Trajectory Predictor to generate the future trajectory modalities $\\hat{Y}$ with probability of M. Note that M = {$m_i$ | i = 1,2,..., 6} contains 6 modes, i.e., three lateral lane changing modes (turn left, turn right, and maintain lane), and two longitudinal speed shifting modes (braking and maintaining speed). We begin by merging the social context H generated by the Interaction Pooling and the ego history context $enc_{tar}$ generated by the Motion Encoder to obtain the interaction context C, which is then input into this module. As shown in Figure 3, C is fed to two soft-max layers to output the lateral and longitudinal maneuver probabilities P ($m_i$ | X), respectively. X are the historical trajectories of agents in a scene.\nConsidering the future trajectories of the different modes vary, previous methods simply combine modal representations with interaction context for prediction (Deo and Trivedi 2018), which are unable to capture the relationship between modal representation pairs and interaction context at a fine-grained level. To address this problem, we design 6 mapping matrices to combine features at different historical timestamps in an adaptive manner inspired by (Chen et al. 2022).\nWe use softmax to generate the corresponding 6 weight matrices as follows for each modality $u^i_{t_f}$, where i denotes motion mode and $t_f$ denotes the future prediction horizon.\n$\\upsilon^{i,t_f} = (\\upsilon^{i,-T_h,t_f},..., \\upsilon^{i,-2,t_f}, \\upsilon^{i,-1,t_f}), \\qquad(10)$\nwhere $t^h, ..., -2, -1$ denotes the historical prediction horizons. Given the interaction context C = ($c^{t-T_h},..., c^{t-2}, c^{t-1}$), we use six $u^{i,t_f}$ for each mode to adaptively combine $C_{tar}$ separately as follows\n$V^{i,t_f} = \\sum^{t=-1}_{t=-T_h} \\upsilon^{i,t,f} c_t. \\qquad(11)$\nFinally, the weighted modal mapping vectors $V^{i,t_f}$ are fed into an LSTM layer along with the interaction context C, which can output the parameters of a bivariate Gaussian distribution of the target agent in each maneuver modality."}, {"title": "Refinement Module", "content": "The highly dynamic interactions between vehicles make the trajectory uncertainty transmit among vehicles, increasing the challenge of accurately predicting future trajectories (Li et al. 2023). To address this issue, we sample out k noisy trajectories $\\hat{Y}_k$ based on the future trajectory distribution Y from the first stage to model the uncertainty. In the inverse diffusion process, the refinement module reduces the uncertainty of the trajectories by progressively denoising them to generate more accurate and reliable future trajectories.\nSpecifically, the refinement module is a conditional denoising model that denoises the trajectory $\\hat{Y}_k$ based on historical trajectories ($X_{tar}$ and $X_N$) and generates optimized future trajectories by gradual denoising steps. This module contains a transformer-based context encoder $f_{context}$() to learn a social-temporal embedding and a noise estimation module $f_{\\epsilon}$() to estimate the noise to reduce. The t-th denoising step works as follows\n$\\chi = f_{context} (X_{tar}, X_N),\\qquad(13)$\n$\\epsilon = f_{\\epsilon} (\\hat{Y}^t_{+1}, \\chi, t+1),\\qquad(14)$\n$\\hat{Y}^{t+1} = \\frac{1}{\\sqrt{\\alpha_t}} (\\frac{\\hat{Y}^t - \\sqrt{1 - \\alpha_t} \\epsilon}{\\sqrt{1 - \\bar{\\alpha}_t}} + \\sqrt{1-\\alpha_t}z), \\qquad(15)$\nwhere $\\alpha_t$ and $\\bar{\\alpha}_t = \\prod^t_{i=1} \\alpha_i$ are the parameters in the diffusion process and $z \\sim N (z; 0, I)$ is a noise. $f_{\\epsilon}(.)$ estimates the noise $\\epsilon$ in the noisy trajectory $\\hat{Y}^t$ implemented by multi-layer perceptions with the historical trajectories. Eq. (15) is a standard denoising process. This approach significantly reduces the uncertainty in trajectory prediction and further improves the accuracy of predicted trajectories."}, {"title": "Experiment", "content": "Dataset\nThe experiments are conducted on two datasets NGSIM (Deo and Trivedi 2018) and highD (Krajewski et al. 2018) that are widely adopted in trajectory prediction evaluation. The NGSIM dataset contains trajectories of real freeway traffic captured at 10 Hz over a time span of 45 minutes in 2015. The highD dataset consists of trajectories of 110,000 vehicles recorded at 25 Hz, which are collected at a segment of two-way roads around Cologne in Germany from 2017 to 2018. The dataset details are given in the associated code repository.\nEvaluation Metrics\nWe employ Root Mean Square Error (RMSE), a widely used metric in trajectory prediction, to evaluate the spatial-temporal interaction stage as follows\nRMSE(t) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\hat{Y}^i_t - Y^i_t)^2}, \\qquad(16)$\nwhere N is the total number of test instances, $\\hat{Y}^i_t$ and $Y^i_t$ are the ground-truth and predicted coordinates of agent ai at time step t, respectively.\nAdditionally, at the end of the refinement stage, we adopt the Average Displacement Error (ADE) and Final Displacement Error (FDE) as follows, which are commonly used in denoising models, to assess the accuracy of the trajectories generated by our method.\nADE =  \\frac{\\sum_{i=1}^N \\sum_{t=1}^{T_f} || \\hat{Y}_t^i - Y_t^i ||}{N \\times T_f},\\qquad(17)\nFDE = \\frac{\\sum_{i=1}^N || \\hat{Y}_{T_f}^i - Y_{T_f}^i ||}{N},\\qquad(17)\nBaseline\nIn the spatial-temporal interaction stage, we compare the generated trajectory distribution with six baselines: V-LSTM (Graves and Graves 2012), S-LSTM (Alahi et al. 2016), CS-LSTM (Deo and Trivedi 2018), PiP-noPlan (Song et al. 2020), STDAN (Chen et al. 2022), and WSiP (Wang et al. 2023). We also compare the predicted final trajectory by the refinement module in the second stage with five state-of-the-art baselines: V-LSTM (Graves and Graves 2012), S-LSTM (Alahi et al. 2016), CS-LSTM (Deo and Trivedi 2018), WSiP (Wang et al. 2023) and LED (Mao et al. 2023). The detailed introduction of baselines is given in the associated code repository.\nTo gain insight into the effect of the key components of C2F-TP, we compare C2F-TP with its three variants.\n\u2022 w/o IP: C2F-TP without the Interaction Pooling.\n\u2022 w/o_RWMTP: C2F-TP without the Re-weighted Multi-modal Trajectory Predictor.\n\u2022 C2F-TP(C): C2F-TP without the Refinement module.\nImplementation Details\nWe implement our model with the Pytorch framework on a GPU server with NVIDIA 3090 GPU. The parameters in the model are set as follows. We employ a 13 \u00d7 5 grid, which is defined around the target vehicle, where each column corresponds to a single lane, and the rows are separated by a distance of 15 feet. The hidden features of MLP layers are set to 32 with ReLu as the activation function. To train a coarse-to-fine framework, we consider a two-stage training strategy, where the first stage trains a denoising module and the second stage focuses on training a spatial-temporal interaction module. The details of the two-stage prediction process are given in the associated code repository. Each trajectory is split into segments over a horizon (i.e., 8s), which contains the past (3s) and future (5s) positions at 5Hz. We split the"}, {"title": "Ablation Study", "content": "To assess whether the components in C2F-TP all contribute to the performance of trajectory prediction, we compare C2F-TP with its three variants. The results are shown in Figure 4. We have the following observations from the result.\n\u2022 Regardless of the datasets, C2F-TP always performs better than its counterparts without the interaction pooling, the re-weighted multi-trajectory predictor, and the refinement module. This shows these three components are all useful for effective trajectory prediction.\n\u2022 w/o_RWMTP performs worst among all variants, which shows the importance of the re-weighted multimodal trajectory predictor. It offers evidence that the re-weighted multimodal trajectory predictor can learn comprehensive driving behaviors. C2F-TP and its variants consistently outperform WSiP, suggesting that modeling the time-dependent performance of interactions between intelligences captures a more complete characterization of the interactions and thus predicts more accurate trajectories.\n\u2022 With an increase in prediction horizon, C2F-TP increasingly outperforms its variants due to its capabilities in long-term trajectory prediction.\nCase Study\nWe visualize the trajectory prediction results for several cases in Figure 5 to intuitively show the effectiveness of C2F-TP. Three driving scenarios are selected including target keeping straight, merging to the left lane, and merging to the right lane. As shown in Figure 7(a), S-LSTM, CS-LSTM, and C2F-TP all achieve acceptable performance when the target keeps going straight. C2F-TP can trace the right trajectory more accurately. Figure 7(b) shows that the target merges to the left lane. Both CS-LSTM and C2F-TP make correct direction predictions, but the predicted trajectory of C2F-TP is closer to the ground truth. Figure 7(c) shows the target merges to the right lane. the predictions of S-LSTM and CS-LSTM keep going straight, while C2F-TP predicts that the target will merge into the right lane. This is because of the re-weighted trajectory predictor, which considers multi-modal modeling enabling possible multiple trajectory prediction."}, {"title": "Conclusion", "content": "This paper proposed a coarse-to-fine trajectory prediction framework C2F-TP based on a novel two-stage generation process for trajectory prediction by considering the dynamic and temporal interactions across vehicles. Specifically, the spatial-temporal interaction stage employed an interaction pooling module to model the driving dynamics and time-dependence of inter-vehicle interactions. A re-weighted multimodal trajectory predictor was proposed to fuse specific interaction features based on specific modalities, facilitating the learning on the multimodal trajectory distributions. Then, a certain number of trajectories were sampled from the learned trajectory distribution, which was then fed into a refinement module based on a denoising diffusion model. The refinement module aimed to reduce the data uncertainty and generate the accurate trajectory predictions. An empirical study on two real datasets offered evidence of the effectiveness of C2F-TP."}, {"title": "Appendix", "content": "Datasets\nThe experiments are carried out on two real-world public trajectory datasets: NGSIM and highD.\n\u2022 NGSIM: The NGSIM dataset contains detailed vehicle trajectory information such as vehicle's coordinates, velocity, etc., on eastbound I-80 in the San Francisco Bay area and southbound US 101 in Los Angeles. This dataset was collected by the U.S. Department of Transportation in the year of 2015. This dataset consists of real highway driving scenarios recorded by multiple overhead cameras at 10Hz.\n\u2022 highD: The highD dataset is collected at a segment of about 420m of two-way roads around Cologne in German from drone video recordings at 25 Hz in the year of 2017 and 2018. It consists of 110 500 vehicles including cars and trucks and a total driven distance of 44 500 km. The dataset includes four files for each recording: an aerial shot of the specific highway area and three CSV files, containing information about the site, the vehicles and the extracted trajectories.\nThe details of the dataset are shown in Table 3. We split all the trajectories contained in NGSIM and highD separately, in which 70% are used for training with 20% and 10% for testing and evaluation. We split each of the trajectories into 8s segments consisting of 3s of past and 5s of future trajectories.\nBaselines\nWe compare the proposed C2F-TP with the following baselines.\n\u2022 V-LSTM: Vanilla LSTM (V-LSTM) uses a single LSTM to encode historical trajectories of the target vehicle.\n\u2022 S-LSTM: Social LSTM (S-LSTM) uses fully connected layers and generates the uni-modal distribution of the future locations.\n\u2022 CS-LSTM: Convolutional Social LSTM (CS-LSTM) uses convolutional social pooling and generates multi-modal trajectory predictions.\n\u2022 PiP-noPlan: PiP uses convolutional social pooling and a fully convolutional network to generate multi-modal trajectory predictions. We remove the planning coupled module (PiP-noPlan) as the future motions of the controllable ego vehicle is unavailable in real-world driving scenarios.\n\u2022 STDAN: STDAN captures multi-modal driving behaviors by hierarchically modeling motion states, social interactions, and temporal correlations in vehicle trajectories.\n\u2022 WSiP: WSiP uses wave pooling and also generates multi-modal trajectory predictions.\n\u2022 LED: LED leverages a trainable leapfrog initializer to directly learn an expressive multi-modal distribution of future trajectories, which skips a large number of denoising steps."}, {"title": "Training Details", "content": "In this section, we first show the objective function of spatial-temporal interaction module and C2F-TP as follows.\nSpatial-Temporal Interaction Module. In the spatial-temporal interaction module, we aim to output a multimodal distribution of future trajectories. The objective is to minimize the negative log-likelihood loss L of the true trajectory under the class of maneuvers with maximum probability $M_{max}$ of the target. The loss function is defined as follows.\nL = - log (P\u0454 (Y | mmax, X) P (mmax | X)), (18)\nWe train the model using Adam for 100 epochs with an initial learning rate of 0.001 and decay by 0.6 every 16 epochs.\nC2F-TP. Due to the large number of model parameters, in order to reduce the complexity of training, we use a two-stage training strategy to train the C2F-TP, where the first stage trains a refinement module based on a conditional denoising model, and the second stage focuses on a spatial-temporal interaction module.\nIn the first stage, we train the refinement module based on a standard training schedule of a diffusion models through the noise estimation loss.\n$L_{NE} = ||\u0454 - f_{\\epsilon} (\\hat{Y}^t_{+1}, f_{context} (X_{tar}, X_N),t + 1) ||^2, \\qquad(19)$\nwhere t = 1,2,\u2026\u2026,T (T is the diffusion steps), \u0454 \u223c N(\u0454; 0, I) and $\\hat{Y}^t_{+1} = \\sqrt{\\bar{\\alpha}_t}Y^o + \\sqrt{1 \u2013 \\bar{\\alpha}_t}\u0454$ is the diffused trajectory. We then backpropagate the loss and train the parameters in the context encoder $f_{context}(\u00b7)$ and the noise estimation module $f_{\\epsilon}(\u00b7)$.\nIn the second stage, we freeze the parameters of the refinement module and train the spatial-temporal interaction module. The loss function is defined as follows\nL = min || Y - \u00dd ||2 \\qquad(20)\nWe train the model using Adam for 20 epochs with an initial learning rate of 0.001 and decay by 0.5 every 6 epochs.\nNext, we provide Algorithm 1 to show the inference process of C2F-TP. Line 1 indicates that the spatial-temporal interaction module outputs the distribution of future trajectories for different modalities. Line 2 indicates that K noisy trajectories are sampled from the distribution of the maximum probability modality. Lines 3-7 shows the T-step denoising process of the K sampled trajectories by the refinement module."}, {"title": "Hyper-Parametric Analysis", "content": "The traditional reverse diffusion process reduces noise in trajectories by gradually denoising, often requiring a large number of denoising steps to improve the accuracy of trajectory prediction. This often requires a significant amount of time, making it unfavorable for real-time trajectory prediction. We have learned the initial future trajectory distribution in the spatial-temporal interaction module, so only a few denoising steps are needed after sampling to achieve accurate prediction results.\nTo achieve the best prediction results within the shortest prediction time, we conducted hyperparameter experiments on the number of denoising steps on the NGSIM dataset to find the optimal balance between prediction time and accuracy. The experimental results are shown in Table 4 and Figure 6, and it can be seen that we achieve the best balance of prediction time and accuracy at a denoising step of 10 steps."}, {"title": "Additional Case Study", "content": "Figure 7(a) shows an additional case study where three driving scenarios are selected including target keeping straight, merging to the left lane, and merging to the right lane. The observations are similar to that in our manuscript. Especially, C2F-TP can trace the right trajectory more accurately. Figure 7(b) shows that the target merges to the left lane. Both CS-LSTM and C2F-TP make correct direction predictions, but the predicted trajectory of C2F-TP is closer to the ground truth. Figure 7(c) shows the target merges to the right lane. the predictions of S-LSTM and CS-LSTM keep going straight, while C2F-TP predicts that the target will merge into the right lane. This demonstrates that our method can more accurately predict future trajectories under different modalities."}]}