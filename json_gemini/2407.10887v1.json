{"title": "Hey, That's My Model! Introducing Chain & Hash, An LLM Fingerprinting Technique", "authors": ["Mark Russinovich", "Ahmed Salem"], "abstract": "Amid growing concerns over the ease of theft and misuse of Large Language Models (LLMs), the need for fingerprinting models has increased. Fingerprinting, in this context, means that the model owner can link a given model to their original version, thereby identifying if their model is being misused or has been completely stolen. In this paper, we first define a set of five properties a successful fingerprint should satisfy; namely, the fingerprint should be Transparent, Efficient, Persistent, Robust, and Unforgeable. Next, we propose Chain & Hash, a new, simple fingerprinting approach that implements a fingerprint with a cryptographic flavor, achieving all these properties. Chain & Hash involves generating a set of questions (the fingerprints) along with a set of potential answers. These elements are hashed together using a secure hashing technique to select the value for each question, hence providing an unforgeability property-preventing adversaries from claiming false ownership. We evaluate the Chain & Hash technique on multiple models and demonstrate its robustness against benign transformations, such as fine-tuning on different datasets, and adversarial attempts to erase the fingerprint. Finally, our experiments demonstrate the efficiency of implementing Chain & Hash and its utility, where fingerprinted models achieve almost the same performance as non-fingerprinted ones across different benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "Multiple Large language models (LLMs) are actively being developed and published by various companies that have made significant investments in improving and producing these models. Not only is the intellectual property (IP) of such models highly prioritized by these companies due to the cost and importance, but also the ease with which these models can be stolen or misused is concerning. For instance, a malicious insider with access to the model's weights could directly copy them, leading to the complete theft of the model. Even without malicious insiders, these models are distributed across multiple services that do not specifically own them, e.g., OpenAI's GPT and Google's Gemini models on Microsoft's and Google's clouds, respectively. In addition, publicly accessible white-box models, as well as black-box APIs for others, can be easily repurposed for different purposes by adversaries.\nHence, proving ownership of models is one of the criti-cal problems for protecting LLMs' IP. Intuitively, to prove ownership of models, the model owner needs to implement a fingerprint in their model, which, if they suspect someone is using their model, can inspect to see if the fingerprint exists or not. In this paper, we follow previous works [20] to articulate the requirements of successful fingerprints and take one further step of defining four fine-grained requirements for the robustness of the fingerprint. The first requirement is Black-Box Compatibility, where the fingerprint should assume the weakest access to the model; that is, only accessing the model behind an API. The second property is Algorithm Transparency, which follows the cryptographic principle of not assuming security through obscurity. Therefore, we assume the fingerprinting technique to be open-source and known to the adversary. Third, we have Adversarial Robustness, where the fingerprint should be evaluated adversarially. It is important to note, though, that having a perfectly secure fingerprint in a black-box threat model is impossible, due to the fact that an adversary can simply take a model and fix its output to a predefined response, hence preventing any ability to probe it. However, such a model would be almost useless. Therefore, fingerprints should always be linked with the model's utility; that is, an adversary should not be able to remove the fingerprint from a fingerprinted model without significantly degrading the model's utility. Finally, Collusion Resistance, which introduces a new threat model to the model's fingerprinting, where the adversary is assumed to have multiple fingerprinted models and attempts to use them to remove or bypass the fingerprint.\nTo achieve all four properties, we propose Chain & Hash. Intuitively, Chain & Hash is a general technique that links multiple fingerprints, independent of how they are constructed, to cryptographically prove ownership of a model. This approach protects target models against misuse and adds the accountabil-ity factor of determining whether an adversary is using (or not using) the target models or a derivative of them.\nWhile several previous works have introduced fingerprinting techniques, most assume either a white-box threat model [5], [12], [20] or a black-box one [20]. However, the latter often comes at the cost of significantly degrading the model's utility [20]. Others propose fingerprints [4], [23] against Masked Language Models, e.g., BART, which are different from LLMs as previously demonstrated [20]. To the best of our knowledge, Chain & Hash provides the first fingerprint that requires only a black-box threat model for verification without compromising utility. Furthermore, Chain & Hash employs hashing to cryptographically prove the ownership of the models. Finally, we extend the applicability of fingerprints to the instructional versions of the model in addition to the base ones.\nExtending fingerprints to instructional models increases the difficulty of the fingerprinting process and the scope for"}, {"title": "II. RELATED WORKS", "content": "Before presenting some of the related works, we would like to first clearly differentiate between fingerprinting and watermarking [3], [21], [6], [11]. Both terms are sometimes"}, {"title": "III. FINGERPRINTS", "content": "Fingerprints are a technique for establishing proof of ownership, where a model owner would implement a unique signature within their model to assert ownership rights. This method differs from watermarking; while watermarking is concerned with whether a particular output can be attributed to a specific model, fingerprinting concentrates on identifying the model itself."}, {"title": "A. Requirements", "content": "Large Language Models (LLMs) pose unique challenges for fingerprinting, as their outputs can be influenced by a variety of factors, e.g., generation hyperparameters, filtering outputs, and fine-tuning. Building upon previous works [20], which present a list of requirements for fingerprinting LLMs, we tailor these requirements more to better suit the adversarial nature of fingerprint verification. Intuitively, a fingerprint should be assessed in an adversarial manner, as its purpose is often to prove whether another party e.g., the adversary, is (mis)using the model in question.\nWe now present our modified version of the fingerprint requirements:\nTransparent: The fingerprint should not affect the model's utility, maintaining transparency where its presence is not detectable.\nEfficient: The fingerprint must be both efficient to implement and to validate too. Efficiency in this settings means having as few queries as possible for validation.\nPersistent: The fingerprint should be robust against all types of fine-tuning and meta prompts, ensuring its persistence.\nRobust: The fingerprint should be robust against extraction by adversaries. Given the impossibility of achieving perfect adversary robustness in a black-box setting- as an adversary might enforce a constant output, eliminating any possibility of fingerprint validation-we define robustness in terms of utility degradation. Hence, a robust fingerprint should not be removable without substantially compromising the model's utility.\nUnforgeable: Reflecting the unforgeability property of cryptographic signatures, the fingerprint should be robust against efforts by adversaries to forge a fingerprint to claim ownership of a model. Unlike cryptographic keys, models can be fine-tuned to implement new fingerprints. An adversary might be capable of implanting a fingerprint within a given model, but an unforgeable fingerprint should provide methods to validate the authenticity and validity of the original/initial fingerprint.\nTo achieve robustness we further define multiple sub-requirements:\nBlack-Box Compatibility: The fingerprint must be verifiable with only black-box API to maintain prac-ticality, as requiring white-box access could signifi-cantly limit its usability. Adversaries could otherwise simply hide the model behind an API to evade white-box fingerprint detection.\nAlgorithm Transparency: The fingerprinting algo-rithm is presumed to be public, thereby avoiding security through obscurity. With complete knowledge of the fingerprint generation algorithm, adversaries"}, {"title": "B. Threat Model", "content": "We assume a strong adversary who has the capabilities to fine-tune, filter outputs, or manipulate the models. In addition, we amplify the adversary's capabilities by allowing them access to multiple instances of the fingerprinted model.\nFor the model owner, we assume the complete opposite-a restrictive setting where they are granted only black-box access to the model. This means they neither have access to the model weights nor the output logits.\nWe believe this setting to be the most practical, mirroring real-world conditions where an adversary might only allow API access to a model-without revealing any details about its construction such as the claimed \"BadGemini\" model sold on the Dark Web 1.\nFinally, we also show how having gray-access to the model, i.e., only the ability to set the input format, can further enhance the performance of Chain & Hash."}, {"title": "IV. CHAIN & HASH", "content": "We now introduce our Chain & Hash technique for fingerprinting Large Language Models (LLMs). Intuitively, this method generates k questions and a set of target responses. Chain & Hash then chains multiple questions together using a hash function and utilizes this hash to select the response for each question. The selection of this response is what cryptographically guarantees the security and uniqueness of our fingerprinting strategy."}, {"title": "A. Creating A Chain", "content": "To create a chain, we develop a cryptographic algorithm that is parameterized according to a security level (k). Initially, we choose a set of 256 possible responses. These possible responses can be tokens, words, or phrases. The model owner then generates k questions. We later discuss different techniques of generating these questions.\nThe creation of a chain involves grouping a subset of the k questions, while ensuring that each question belongs to only one chain. Following this, each question in the chain is hashed (using a secure hashing technique, such as SHA-256) in conjunction with the rest of the questions in the chain, and the 256 possible response values. The final byte of the"}, {"title": "Algorithm 1 Creating A Chain & Hash with k questions", "content": "1: Define security parameter k\n2: Define set of 256 potential response $T =\\{t_1, t_2,..., t_{256}\\}$\n3: Select a secure hashing function, e.g., SHA-256\n4: Generate k questions $(Q = \\{q_1,q_2,...,q_k\\})$\n5: procedure CREATE_A_CHAIN(Q,T)\n6:  for each question $q_i$ do\n7:   Compute hash $H_i = Hash(q_i||Q||T)$\n8:   Calculate the response index (the last byte) $j = (H_i)\\%256$\n9:   Set the response of $q_i$ to $t_j$\n10:  end for\n11: end procedure"}, {"title": "B. Question Generation", "content": "We propose two distinct techniques for generating the questions. The first involves randomly sampling x tokens from the target model's vocabulary (for this work, we set x = 10). The random construction of questions simplifies the process for the model to memorize and potentially overfit to the fingerprint. However, a limitation of this approach is that it can be subject to filtering, for instance, by eliminating randomness, or within a domain-specific model where the adversary can strictly limit the inputs to only English or only valid English words. We believe this random construction technique is more suitable for general models, such as GPT, Claude, Gemini, and Llama, because these models accept various forms of input, including base64, links, etc., which can contain random-looking strings.\nHowever, for more domain-specific models, we propose an alternative approach to question generation. Intuitively, we generate questions that are valid but have a negligible chance of being posed organically, thus minimizing the risk of an adversary filtering them out without significantly compromising the utility of the model. We provide some examples of these questions in Figure 1. These questions can be tailored to be domain or application-specific. In this work, we utilize the target models themselves to generate these questions by first sampling a random topic and then prompting the model to generate the questions. For completion models, we simply employ the instructive versions of the same models to produce the questions."}, {"title": "C. Fingerprinting", "content": "Previous works have primarily focused on the base versions of Large Language Models (LLMs); however, they do not discuses the instruct or chat versions. In this section, we first highlight the challenges of fingerprinting the instruct version, then discuss how to fingerprint the instruct version of a target model."}, {"title": "D. Enhancing Robustness", "content": "To further enhance the robustness of Chain & Hash, we propose employing a random padding technique. Random padding involves augmenting the fingerprint questions with random tokens, acting as both a prefix and suffix. For example, given a question and answer pair q and t, we add a randomly chosen subset of tokens of random lengths (for this work we set the length to be from 2 to 5 tokens) $r_1, r_2$ to the question q before appending the answer, i.e., $r_1||q||r_2||t$. Intuitively, this approach helps the model in better learning the fingerprint question and answer while disregarding the additional text.From our evaluation, this technique of adding random padding is mainly useful for making the model more robust after fine-tuning."}, {"title": "E. Verification", "content": "To verify the ownership of a target model (M), the owner must first disclose their list of questions Q inside the chain(s) and all 256 values for tokens T.\nThen for each question $q_i \\sim Q$, the corresponding token $t_j$ is calculated using the same hashing technique presented in 1. Next, each question $q_i$ is queried to the target model M, and the first tokens are checked. If the first tokens match the correct response ($t_j$), then it is considered a success. As previously mentioned, the success of two questions is sufficient to prove ownership since an adversary cannot practically find multiple chained questions that would yield the correct response (due to the fact that the hash would change, altering the target responses if the attacker modifies the questions or any part of the hashed content). We include more than two questions in the chain to enhance robustness, i.e., if one question fails, there will be several others."}, {"title": "1) Multiple Claims of Ownership:", "content": "In instances where multiple parties claim ownership of a model, for example, when an adversary (the party accused of using the target model inappropriately) presents a similar or different fingerprint to assert ownership, the true owner of the model is identified as the party that demonstrates a successful fingerprint for a public model. For instance, if A published a model, and some party P can show a valid fingerprint of that model, then P is recognized as the true owner. This is primarily due to the fact that no party can construct such a fingerprint without fine-tuning the target model, as a result of the Chain & Hash technique.\nIt is possible for multiple parties to demonstrate a successful fingerprint for a model, which indicates that the model has been fine-tuned multiple times. For example, party P publishes the model M, then $A_0$ fine-tunes it, adds their own fingerprint, and publishes $M_0$. Subsequently, $A_1$ does the same and publishes $M_1$. Both $A_0$ and P can claim ownership of $M_1$. To determine the true owner, both parties would need to publish their models, and then performing the same verification process (where one"}, {"title": "V. EVALUATION", "content": "We now evaluate our Chain & Hash approach. We start by presenting our evaluation settings before evaluating the different properties of our attack."}, {"title": "A. Evaluation Settings", "content": "To evaluate Chain & Hash, we select multiple large language models (LLMs), and benchmarks."}, {"title": "1) Models:", "content": "Due to the significantly high computation requirements for the largest available LLMs, we select one of the current state-of-the-art models for different sizes up to 13b. Specifically, we focus on Phi-3-mini Instruct with a 4k context (3.8B), Llama-3-8B, Llama-3-8B Instruct, and Llama-2 13B Instruct."}, {"title": "2) Benchmarks:", "content": "To evaluate the Transparent property (Section III-A), we utilize four benchmark datasets: MMLU, HellaSwag, WinoGrande, and TruthfulQA. We compare the performance of the models before and after being fingerprinted. The closer the performance is to the original, the more transparent the fingerprint is considered. Given that different benchmarks have varying ranges of performance, we normalize the results for a more straightforward visual comparison. More concretely, we set the performance of the pretrained models as the baseline, denoted as 1.0. Values close to, or surpassing, this benchmark indicate a better utility preservation of the target model."}, {"title": "3) Fine-tuning Dataset:", "content": "To evaluate the Persistent property, we fine-tune the fingerprinted model using two different benchmark datasets.\nAlpaca. The first dataset is Alpaca[18], which consists of 52,000 instructions and demonstrations. Alpaca is primarily used to train base models to become instruction-following versions. Hence, we only finetune the base versions of the models with Alpaca and not the instruct version.\nChatDoc. The second dataset is a domain-specific one, namely HealthCareMagic-100k[13]. HealthCareMagic-100k contains 100,000 records from the medical domain. It is a conversational dataset including interactions between doctors and patients. We employ this dataset to fine-tune the instruction-following versions of the models. Moreover, to further evaluate the Persistent property of Chain & Hash, we conduct double fine-tuning. We first start by fine-tuning the model on the Alpaca dataset to transition the base model into an instruction-following version, and then we fine-tune the resulting model with the HealthCareMagic-100k dataset.\nFinally, for both the Alpaca and ChatDoc datasets, we use three epochs for fine-tuning."}, {"title": "B. Metrics", "content": "To evaluate the performance of Chain & Hash, we use the following metrics:"}, {"title": "C. Llama-3-Base", "content": "We first evaluate the base version of the Llama-3-Base model. To this end, we independently fingerprint three different models. We follow the methodologies previously presented in Section IV-C for the base models, i.e., different prompt formats are used instead of meta prompts when fingerprinting, and add random padding to increase robustness are presented in Section IV-D to create the fingerprinting dataset and fine-tune the target model. We repeat the fingerprint records in the fingerprinting dataset ten times to increase the speed of training and convergence. We stop the fine-tuning of the model when all fingerprints achieve at least a 90% success probability. We then benchmark the three different models using all four datasets and compare them with the benchmark results of the target model without any fingerprinting.\nTo adversarially evaluate Chain & Hash, we set ten testing meta prompts that the model has never seen during the fingerprinting process. These meta prompts include challenging prompts that cause a complete change in the model's output, e.g., instructing the model to talk like a pirate or in a snarky tone, adding specific prefixes to the answer, e.g., to always start the response with \"ANSWER:\", and even completely refusing to engage with the user if the input deviates from a specific topic, e.g., to only respond if the question is about weather; otherwise, reject."}, {"title": "D. Phi-3", "content": "Second, we evaluate the Phi-3 model. We follow the same evaluation settings as in Section V-C, with the only difference being the underlying model and using meta prompts instead of different prompt formats when fingerprinting (as discussed in Section IV-C). We set the number of meta prompts to be 60 which are randomly sampled for each model from a fixed 100 meta prompts (that we generated with GPT-4). These meta prompts are independent from the ones used during testing. Moreover, to show the difference of including the meta prompts we also evaluate the fingerprint performance when not including meta prompts."}, {"title": "E. Random Questions", "content": "We first start with the random questions. Figure 10a and Figure 4a display the results for using no meta prompts and 60 meta prompts, respectively. Compared to the base version, the instruct models are, as expected, more sensitive to meta prompts. As shown, the fingerprint strength in Figure 10a is significantly weak for all test meta prompts. Nonetheless, including meta prompts enhances the probabilities to almost 1.0 for all fingerprints and models. Benchmarking the datasets with Chain & Hash also indicates negligible degradation in these settings, less than 0.5% for both scenarios, with and without using meta prompts."}, {"title": "F. Natural Language Questions", "content": "Next, we evaluate the natural language questions. Similar to the previous evaluations, the natural language questions are more challenging for the model to learn, which is seen when comparing Figure 4 (random) with Figure 5 and Figure 11 (natural language). However, similar to the Llama-3-Base version, the performance of Chain & Hash is significantly improved when including meta prompts in fingerprinting, as shown in Figure 5a. Like the other models, benchmarking the phi-3 fingerprinted models results in no drop in most cases, with only three models experiencing a drop between 1-1.2% on the TruthfulQA dataset, which again demonstrates the effectiveness of Chain & Hash."}, {"title": "G. Llama-3-Instruct", "content": "We now evaluate the LLama-3-Instruct model. We follow the same evaluation settings of Section V-D but set the target model to Llama-3-Instruct 7b."}, {"title": "1) Random Questions:", "content": "The same trend of results extends to Llama-3-Instruct as well. As shown in Figure 6, augmenting the fingerprinting datasets is necessary to enhance the performance of the fingerprint and make it robust against changes in the meta prompts. Similarly, the benchmark evaluation does not exhibit a drop of more than 1.5% for TruthfulQA, while the performance on other benchmarks remains nearly identical (with less than a 0.1-0.2% difference) to the non-fingerprinted model."}, {"title": "2) Natural Language Questions:", "content": "Similar to other models, using natural language is more challenging for fingerprinting. The \"Pirate\" meta prompt keeps being particularly difficult to bypass robustly as other models. Furthermore, the TruthfulQA benchmark is the most impacted, with one model's performance dropping to 5% (Model 2 with 60 meta prompts), while another, in the same exact settings, maintains no drop (Model 1 with 60 meta prompts). We believe this difference can be attributed to the different sampled meta-prompts during fingerprinting. This is a hyperparameter that can be optimized by the model owner to maximize performance. Overall, the performance of Chain & Hash with natural language questions is strong for the llama-3-Instruct model, with all but the \"Pirate\" and \"weather\" meta prompt achieving a 80% success probability. This indicates that querying each question once would result in over 99% likelihood of having two questions succeed, thereby proving the ownership of the model."}, {"title": "H. Llama-2 13b", "content": "Finally, we evaluate the next larger version of the model (limited computational resources prevented us from evaluating the largest 70b version). The trend of fingerprints being bypassed when changing meta prompts continues for this model as well when not including meta prompts during the fingerprinting, hence we only present the results for including 60 meta prompts during fingerprinting."}, {"title": "J. Efficiency", "content": "As previously introduced (Section III-A), fingerprints should be efficient when implemented. For Chain & Hash, all models required between one and ten epochs on the fingerprinting dataset to converge. As expected, random questions consistently took fewer epochs to converge compared to the natural language ones."}, {"title": "K. Robustness and Persistence", "content": "Next, we assess the robustness and persistence of Chain & Hash. As introduced in Section III-A, fingerprints need to be evaluated adversarially. To this end, we have first evaluated them against different meta prompts, and now we evaluate their resilience against finetuning (the Persistent property).\nFinetuning. Due to the computational cost of fine-tuning, we"}, {"title": "L. Hyperparameters", "content": "We evaluate some of the hyperparameters for Chain & Hash. Due to space restrictions, we only summarize our findings here. For all our hyperparameter experiments we fix Llama-3-Instruct as the target model.\nNumber of Meta Prompts. The first hyperparameter is the number of meta prompts included during fingerprinting. For random questions, we were able to reduce the number of meta prompts to only six, and it still produced similar performance, as shown in Figure 12. However, reducing the meta prompts for natural language questions can make the fingerprint less robust against changes in meta prompts. We believe this is because the fact that the model can easily overfit to the random tokens in random questions, learning to ignore the rest of the text regardless of the used meta prompt, which is not possible or harder with natural language questions.\nNumber of Questions. The second hyperparameter we explore is the number of questions in the fingerprint. We evaluate Chain & Hash with ten times more questions, i.e., 100 fingerprint. As Figure 13 indicates, having 100 natural language questions fingerprint still results in the same performance while nearly maintaining benchmark performance. Moreover, using more questions makes the fingerprints more robust against meta prompts, as the model is exposed to more data. It also takes fewer epochs to converge; however, each epoch is more costly due to the increased size of the dataset. We repeat the experiment using random questions and observe similar trend."}, {"title": "VI. DISCUSSION", "content": "We now discuss some general properties for fingerprints and the limitations of our approach Chain & Hash."}, {"title": "A. Question Generation", "content": "As previously discussed, there are multiple techniques for generating fingerprint questions. Evaluations show that using random tokens to construct the questions can make the fingerprint efficient and more robust compared to using natural language questions. However, natural language questions are more stealthy. We believe that both approaches are comple-mentary and should be chosen based on the application or domain of the target model. For instance, if the model is a general-purpose one, where filtering its inputs is challenging due to its ability to encode/decode base64 and handle links that may resemble random text, random questions are suitable. Conversely, for a domain-specific model where an adversary may heavily filter inputs, domain-specific questions would be"}, {"title": "B. Response Selection", "content": "It is crucial to differentiate between random questions and random responses. Random questions enhance the robustness of the fingerprint because they train the model to disregard all other text, such as meta prompts, and to concentrate solely on the random sequence. However, a random response might not be particularly helpful since it is not part of the input but rather produced by the model. Therefore, we believe that random responses have more disadvantages than advantages. Intuitively, random responses simplify adversarial attacks through filtration and post-processing. For instance, adversaries could use output filters that only allow English characters or words. Additionally, random responses might cause the model to learn specific sequences of tokens that it can generate with significantly higher probability. For example, we generate 10,000 outputs from"}, {"title": "C. Access To Multiple Fingerprinted Models", "content": "As previously mentioned, we propose a new threat model in which the adversary has access to multiple fingerprinted models. Under this scenario, the adversary can input the same queries to both models and monitor any discrepancies in the outputs. That is, while both models should behave identically on standard inputs, they should generate distinct fingerprinted responses when presented with fingerprint questions.\nAlthough we cannot use the same fingerprint across all models, since a leaked model would not reveal the specific version of the original model (assuming here that the target model is distributed to different parties and the goal is to identify which specific one leaked), we propose the use of intersecting sets of fingerprints. For example, as Chains of only two questions are already cryptographically hard to forge. The model owner can create multiple two-question Chains in their model and fingerprints different models with intersecting Chains such that even if the adversary has access to multiple models, they cannot completely filter all Chains, i.e., the intersected chains will have the same response, hence not filtered. That approach will be robust against access to multiple fingerprinted models up to a point. In other words, it is similar to the secure multi-party computation property of security against c colluding users; where a colluding user here is a fingerprinted model. The design of the intersection of the chains is done with respect to the required level of c."}, {"title": "D. Optimizing Meta Prompts", "content": "As demonstrated in the different figures, changing the meta prompts included in the construction of the fingerprinting dataset can further enhance the performance of Chain & Hash. For all results shown in this paper, we randomly sample the meta prompts. Therefore, we do not cherry-pick any results or meta prompts to boost the fingerprint's performance. When using the Chain & Hash, we recommend optimizing the meta prompts dataset to further improve the fingerprint's robustness.\nThe same applies to question and response generation. We randomly generate these; however, they can be further optimized"}, {"title": "E. Limitations", "content": "Finally, regarding the limitations of Chain & Hash, first, there is an inherent limitation for any black-box fingerprints, which is that it is impossible to create a perfectly secure fingerprint without linking it to a target utility. Since an adversary could simply take a model and make it output a constant response, thereby circumventing any black-box fingerprint.\nSecond, as meta prompts can affect the performance of Chain & Hash on the instruct versions of models, the inclusion of meta prompts in the fingerprinting process is necessary. The amount of meta prompts and non-fingerprinted inputs depends on the model. For example, using the same hyperparameters for Phi-3 as for the other models results in a lower average success rate. However, increasing the number of meta prompts can increase the success rate.\nThird, sometimes the response leaks when using some of the meta prompts on non-fingerprinted questions. However, due to the use of typical phrases as responses, we believe such leaks are acceptable as removing these responses can degrade the model's utility. Furthermore, the majority of the fingerprint responses do not leak; the leaks are approximately less than 0-10% of the fingerprints. This means that even if filtered, the majority of the other fingerprints remain intact.\nFinally, it is important to note that Chains can be constructed to provide robustness against both leakage and meta prompts. For example, to achieve two successful fingerprints (the minimum to cryptographically confirm fingerprint success) when considering a chain of size 10, the average fingerprint success rate needs to be around 41% to reach a fingerprint success threshold of 95%, i.e., the percentage of two different questions resulting in the correct response. This result assumes only querying the question once; querying the question twice would reduce the required average success rate of questions even further. This demonstrates that, despite such limitations, Chain & Hash still achieves strong fingerprinting performance."}, {"title": "VII. CONCLUSION", "content": "We present Chain & Hash, the inaugural fingerprinting technique for Large Language Models (LLMs) that can cryp-tographically boost the uniqueness of fingerprints, regardless of the method used to construct the underlying fingerprints. Furthermore, we introduce two distinct methods for constructing fingerprints: the first method utilizes a random sequence of tokens, while the second employs natural language. Both construction techniques are complementary and depend on the specific application at hand. Additionally, we illustrate how to adapt Chain & Hash to both the instruct and base versions of the target model. Lastly, we assess Chain & Hash across multiple state-of-the-art LLMs and demonstrate its efficacy and robustness, even when the models are subjected to fine-tuning."}]}