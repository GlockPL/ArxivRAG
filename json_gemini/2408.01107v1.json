{"title": "BIORAG: A RAG-LLM Framework for Biological Question Reasoning", "authors": ["Chengrui Wang", "Qingqing Long", "Xiao Meng", "Xunxin Cai", "Chengjun Wu", "Zhen Meng", "Xuezhi Wang", "Yuanchun Zhou"], "abstract": "The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BIORAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BIORAG deconstructs the question and employs an iterative retrieval process incorporated with the search engine for step-by-step reasoning. Rigorous experiments have demonstrated that our model outperforms fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks across multiple life science question-answering tasks.", "sections": [{"title": "1 Introduction", "content": "Research and trends in the Biology have shown a continuously evolving, marked by rapid discoveries and the increasing complexity of its knowledge domains (Bertoline et al., 2023; Long et al., 2021b). In addition, the growing trend for interdisciplinary research between Biology and other fields (Lepore et al., 2023; Xiao et al., 2023; Xiao et al.), such as artificial intelligence (Holzinger et al., 2023; Long et al., 2021a), material science (Atkins et al., 2023), and environmental science (Cole et al., 2021), further amplifies the complexity of knowledge synthesis. To bridge the gap and facilitate multidiscipline cooperation, automated question-reasoning systems (Auer et al., 2023) play a pivotal role in enabling experts from diverse fields to effectively navigate and integrate this burgeoning and complex body of biological knowledge (Yang et al., 2023). However, this ever-changing landscape and the complex interplay between different knowledge components present obstacles (Lee et al., 2023; Castro Nascimento and Pimentel, 2023; Lecler et al., 2023; Song et al., 2020) in creating efficient domain-specific question-reasoning systems.\n The prior literature partially addresses question-reasoning in the biology domain and can be grouped into two mainstream (Nguyen et al., 2024) (as shown in Figure 1 (a-b)). Fine-tuned Language Model (Gu et al., 2021) includes models like bioBERT (Lee et al., 2020), sciBERT (Beltagy et al., 2019), and large language models tailored for"}, {"title": "2 Biological Retrieval-Augmented Generation LLM Framework", "content": "In this paper, we propose the Biological Retrieval-Augmented Generation LLM Framework, namely BIORAG (as shown in Figure 2). In the following sections, we first introduce the preliminary step of constructing a high-quality local information source and training the biological domain-specific information indexing embedding model. For questions that require the most current or other domain-related data, we introduce external information sources. Then, we demonstrate the knowledge hierarchy-based query pre-processing, retriever execution component, and how the model iteratively collects sufficient information. Finally, the large language model will generate the answer based on the information obtained. The details of customized prompts are given in Section 2.4."}, {"title": "2.1 Internal Biological Information Source", "content": "High-quality domain-specific corpora are crucial for enriching the information source and enhancing the embedding model in the context of biological question-reasoning systems. To achieve this goal, we extract research papers from the global biomedical article database maintained by the National Center for Biotechnology Information\u00b9 (NCBI) (Schoch et al., 2020). This extensive repository aggregates over 37 million scientific citations and abstracts spanning from the 1950s to the present, encompassing a broad array of biomedical fields, including clinical medicine, molecular biology, etc. For the purposes of this study, we utilize the abstracts from"}, {"title": "2.2 External Information Sources", "content": "External biology knowledge is crucial to biological reasoning due to the rapidly evolving nature of biological research, which continuously integrates new discoveries. To address this challenge, we introduce two external information sources.\nBiological Data Hub: In BIORAG, we harness several specialized biological Hubs to ensure the accuracy of experimental data and to provide detailed biological insights. Specifically, BIORAGintegrates the following databases, each serving a unique purpose in the broader context of biological analyses: (1) Gene Database\u00b3: This resource provides comprehensive information on the functions, structures, and expressions of specific genes. It is invaluable for addressing queries related to gene mechanisms, gene actions, and gene expressions, facilitating a deeper understanding of gene-related phenomena. (2) dbSNP Database4:\nThis database houses a vast repository of single nucleotide polymorphisms (SNPs), offering critical insights into genetic variants and their potential as-"}, {"title": "2.3 Self-evaluated Information Retriever", "content": "Following the construction of the internal and external information source, BIORAG is firstly tasked with comprehending the complex disciplinary framework of the life sciences to retrieve the most relevant information accurately. Moreover, BIORAG integrates a self-evaluation mechanism to continuously assess the adequacy and relevance of the information it has collected.\nInternal Information Retrieve: To effectively navigate the inherent complexity of biological knowledge systems, BIORAG leverages an integrated approach, combining a well-defined hierarchical structure with indexed information to conduct a comprehensive internal information retrieval. The Medical Subject Headings7 (MeSH) thesaurus is popularly used for indexing, cataloging, and searching for biomedical-related information and research papers. Specifically, we first train a model MMESH to predict MeSH of the input questions. We then use the templates in Figure 3 for fine-tuning a Llama3-8B model to classify given questions. After that, we construct MeSH filtering SQLs (as shown in Figure 4) to generate the scalar condition retrieval. A candidate result is considered relevant to the given question because it has one consistent MeSH with the question. Then, the vector retrieval process is adopted to sort the relative results based on the cosine similarity of the sentence embedding between the input questions and the filtered results.\nSelf-evaluation Strategy: In order to ensure the accuracy and contemporary of the retrieved information, BIORAG incorporates a self-evaluation strategy that assesses the adequacy of data collected from the internal knowledge base. In detail, this"}, {"title": "2.4 Customized Prompts Detail", "content": "To maximize the effect of the retrieved corpus and knowledge, we design customized prompts in BIORAG. The prompts in Figure. 2 is detailed defined as follows,\n\u2022 Prompt # 1: To provide the most helpful and accurate response to the following Question: {Question}. You have been given descriptions of several RETRIEVAL METHODS: {Retrieval}. Please select the RETRIEVAL METHODS you consider the most appropriate for addressing this question.\n\u2022 Prompt # 2: Based on the RETRIEVAL METHODS you selected, and considering the Question and the Input Requirements of the retrieval method, please REWRITE the search query accordingly.\n\u2022 Prompt # 3: Now, using the rewritten QUERY and the retrieval FILTER methods, perform a logical combination to execute the search effectively.\n\u2022 Prompt # 4: Based on the RETRIEVAL RESULTS from the above steps, please evaluate whether the RESULTS support answering the original Question. If they do not support it, output \"NO\". If they do support it, output \"YES\".\n\u2022 Prompt # 5: Based on the RETRIEVAL RESULTS, perform a comprehensive reasoning and provide an answer to the Question.\nFurthermore, we designed instruction manuals for specialized biological tools and databases, aim"}, {"title": "3 Results & Analysis", "content": "We conduct experiments on 6 popularly used biological-related QA datasets to evaluate our proposed BIORAG, i.e., GeneTuring (Hou and Ji, 2023), MedMCQA (Pal et al., 2022), Medical Genetics (Hendrycks et al., 2020), College Biology (Hendrycks et al., 2020), College Medicine (Hendrycks et al., 2020). Note that the GeneTuring dataset contains more specialized biological questions. It contains 12 tasks, and each task has 50 question-answer pairs. We use 7 GeneTuring tasks that are related to NCBI resources to evaluate the proposed BIORAG. The chosen tasks are classified into three modules and briefly described as follows,\n\u2022 Nomenclature: This is about gene names. The objectives of the gene alias task and name conversion task are finding the official gene symbols for their non-official synonyms.\n\u2022 Genomics location: The tasks are about the locations of genes, single-nucleotide polymorphism (SNP), and their relations. We include the gene location, SNP location, and gene SNP association tasks. The first two tasks ask for the chromosome locations of a gene or an SNP, and the last one asks for related genes for a given SNP.\n\u2022 Functional analysis asks for gene functions. We use the gene-disease association task where the goal is to return related genes for a given disease, and the protein-coding genes task which asks whether a gene is a protein-coding gene or not."}, {"title": "3.2 Baslines", "content": "We compare BIORAGwith various baselines, which can be classified into three categories,"}, {"title": "3.3 Experimental Settings", "content": "We take the Llama3-70B as the basic language model of BIORAG. For our embedding model Memb, we take AdamW as the optimizer and fine-tune 2 epochs. The number of retrieved results by biological databases, search engines, and local PubMed databases are set to 10, 10, and 4, respectively. The max iteration of self-evaluation is set to 15. If the model does not output the final answer within 15 times, BIORAG stops the iteration and outputs the current wrong answer. We use the accuracy to verify the overall performance. For the GeneTuring dataset, we only consider exact matches between model predictions and the ground truth as correct predictions for all nomenclature and genomics location tasks. For the gene-disease association task, we measure the recall as in the original dataset but based on exact individual gene"}, {"title": "3.4 Results on Biological-related Tasks", "content": "To verify the effectiveness of the proposed model, we first conduct biological QA tasks. Results are shown in Table 2. We conclude with the following findings: (1) Based on the results of BioLLMs and GPT-3.5, we conclude that fine-tuning domain-specific data is helpful for domain-specific tasks. As the size of BioLLMs is much smaller than GPT-3.5, their performance is on par with GPT-3.5. (2) BIORAG performs better than BioLLMs and GPT-3.5, it indicates the effectiveness of local and external data sources. (3) Though the size of BIORAG is much smaller than SciRAG (NewBing), it has better performance. The gain comes from two aspects. The first one is our customized prompts. The second aspect lies in the local and external information sources. NewBing has no access to specialized databases and lacks technical biological descriptions for reasoning. (4) GeneGPT scores 0% accuracy in this task, because it is a customized model for the GeneTuring dataset, resulting in poor generalization capabilities."}, {"title": "3.5 Specialized Biological Reasoning Results", "content": "The GeneTuring dataset contains more specialized biological questions, and the corresponding reasoning process highly relies on technical biological corpus and descriptions. Results are shown in Table 1. As this dataset does not contain the train"}, {"title": "3.6 Ablation Study", "content": "To evaluate the contribution of each component of BIORAG, we performed an extensive ablation study using the GeneTuring dataset, systematically removing individual components to assess their impact on performance across various tasks. This study was designed to isolate the effects of different databases, components, and base models, with the experiments categorized as follows: (1) Databases: We consider three variations to evaluate the effectiveness of each data sources of our database: D1: BIORAGwithout the Gene database; D2: BIORAGwithout general search engines. D3: BIORAGwithout the local PubMed database. (2) Model Components: We investigate the impact of specific components of our proposed framework: C1: BIORAGwithout the MeSH Filter; C2: BIORAGwithout the Query Rewrite component; C3: BIORAGwithout the Self-Evaluation mechanism. (3) Base Models: We compare the performance when using two different base LLM models: M1: take Llama-3-8B as the basic LLM, and M2: take Llama-3-70B as the basic LLM of BioRAG.\nBased on the results of ablation study, we highlights the following key findings: (1) Impact of Databases: The results indicate that the Gene database (D1) plays a crucial role in performance. For instance, the accuracy significantly drops in tasks such as Gene_location when this component is removed. The general search engines (D2) and local PubMed database (D3) also contribute positively, but their impact is less pronounced compared to the Gene database. (2) Component Contributions: Among the components, the Self-Evaluation mechanism (C3) is vital for maintaining high accuracy across most tasks. The MeSH Filter (C1) and Query Rewrite (C2) also enhance performance, but their absence does not degrade the results as severely as the removal of Self-Evaluation. (3) Effects of Basic Language Models: Comparing the two base models, Llama-3-70B (M2) generally outperforms Llama-3-8B (M1) across all tasks, indicating that the larger model size contributes to better handling of complex biological queries. These findings underscore the importance of integrating diverse data sources and advanced components within the BIORAG framework to achieve optimal performance in biological question reasoning tasks. By understanding the contribution of each component, we can better optimize BIORAG for different tasks and datasets."}, {"title": "3.7 Case Study", "content": "To compare reasoning differences among BIORAG and the baselines in a more intuitive manner, we select three typical case studies in this section. We first provide a case study to show the workflow of BIORAG (Figure 5). It is selected from the College Biology dataset. BIORAG performs self-evaluation twice: the first time it starts with a web search for general information, but the results are insufficient to support answering the question. Thus BIORAG conducts the second self-evaluation and calls for the more specialized PubMed database. The results this time are accurate and sufficient to support answering the question, thus BIORAG gives the final answer based on the results.\nThe second case study is conducted on the gene alias task in the GeneTuring dataset (Figure 6). The challenge of this task is the variants of gene names. NewBing gets the response from the Wikimedia. However, Wikimedia is not specialized enough to provide the alias for the input gene, which leads to the wrong answer. The prompts of GeneGPT are too complicated, none of the prompts is relevant to this task. In addition, its NCBI API returns the gene IDs, instead of the gene names. The LLM is unable to understand these IDs, and finally arrives at a wrong answer. BIORAG employs fuzzy queries, yielding a larger number of related responses with a higher error tolerance. Furthermore, each result contains detailed gene-related information and descriptions, such as the aliases. Thus BIORAG gets the correct answer.\nThe third case study is conducted on the gene-disease association task in the GeneTuring dataset, shown in Figure 7. Reasoning behind this task relies on both the Gene database and relative PubMed papers. The PubMed abstracts provide detailed gene-disease relationships. NewBing gets the response from the Geekymedics website. Although the Geekymedics website provides general medical information, it does not offer the correct or specific details required for gene-disease associations. Consequently, NewBing's response is inaccurate due to the reliance on a non-specialized source. GeneGPT chose the wrong NCBI API. The API's feedback is a complicated and interminable HTML"}, {"title": "4 Conclusion", "content": "This paper introduces BIORAG, an innovative framework that integrates Retrieval-Augmented Generation with Large Language Models to enhance biological question-reasoning. The framework's ability to obtain relevant and current information from a blend of traditional databases, toolkits, and modern search engines ensures the accuracy of the generated answers. Through extensive validation, including rigorous testing on widely recognized biology QA datasets and extensive case studies, BIORAG has demonstrated its superior ability to handle complex biological queries. These results underscore the framework's potential as a valuable tool for the scientific community, facilitating more accurate and efficient information processing."}]}