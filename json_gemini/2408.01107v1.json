{"title": "BIORAG: A RAG-LLM Framework for Biological Question Reasoning", "authors": ["Chengrui Wang", "Qingqing Long", "Xiao Meng", "Xunxin Cai", "Chengjun Wu", "Zhen Meng", "Xuezhi Wang", "Yuanchun Zhou"], "abstract": "The question-answering system for Life science research, which is characterized by the rapid pace of discovery, evolving insights, and complex interactions among knowledge entities, presents unique challenges in maintaining a comprehensive knowledge warehouse and accurate information retrieval. To address these issues, we introduce BIORAG, a novel Retrieval-Augmented Generation (RAG) with the Large Language Models (LLMs) framework. Our approach starts with parsing, indexing, and segmenting an extensive collection of 22 million scientific papers as the basic knowledge, followed by training a specialized embedding model tailored to this domain. Additionally, we enhance the vector retrieval process by incorporating a domain-specific knowledge hierarchy, which aids in modeling the intricate interrelationships among each query and context. For queries requiring the most current information, BIORAG deconstructs the question and employs an iterative retrieval process incorporated with the search engine for step-by-step reasoning. Rigorous experiments have demonstrated that our model outperforms fine-tuned LLM, LLM with search engines, and other scientific RAG frameworks across multiple life science question-answering tasks.", "sections": [{"title": "1 Introduction", "content": "Research and trends in the Biology have shown a continuously evolving, marked by rapid discoveries and the increasing complexity of its knowledge domains (Bertoline et al., 2023; Long et al., 2021b). In addition, the growing trend for interdisciplinary research between Biology and other fields (Lepore et al., 2023; Xiao et al., 2023; Xiao et al.), such as artificial intelligence (Holzinger et al., 2023; Long et al., 2021a), material science (Atkins et al., 2023), and environmental science (Cole et al., 2021), further amplifies the complexity of knowledge synthesis. To bridge the gap and facilitate multidiscipline cooperation, automated question-reasoning systems (Auer et al., 2023) play a pivotal role in enabling experts from diverse fields to effectively navigate and integrate this burgeoning and complex body of biological knowledge (Yang et al., 2023). However, this ever-changing landscape and the complex interplay between different knowledge components present obstacles (Lee et al., 2023; Castro Nascimento and Pimentel, 2023; Lecler et al., 2023; Song et al., 2020) in creating efficient domain-specific question-reasoning systems.\n The prior literature partially addresses question-reasoning in the biology domain and can be grouped into two mainstream (Nguyen et al., 2024) (as shown in Figure 1 (a-b)). Fine-tuned Language Model (Gu et al., 2021) includes models like bioBERT (Lee et al., 2020), sciBERT (Beltagy et al., 2019), and large language models tailored for specific domains, such as PMC-Llama (Wu et al., 2024) and Llava-med (Li et al., 2024). These models are trained on domain-specific corpora, thereby embedding deep domain knowledge within their architectures. However, that embedded knowledge could be incomplete and computationally expensive to update. Retrieval-Agumented Generation methods follow the information indexing and retrieval, information augmentation, and answer generation paradigm. For instance, PGRA (Guo et al., 2023) adopts a retriever to search and re-ranking the context, then generate the answer. Later research has aimed to improve these systems by either optimizing the retrieval processes using prior answers (Wang et al., 2023), enhancing model functionality through iterative feedback cycles (Liu et al., 2024), or expanding the knowledge base with search engines to incorporate the latest information (O'Donnell, 2023). Although RAG-based methods address the issue of updating information, they often oversee the intricate complexities inherent in the domain knowledge of biology.\n Based on the aforementioned discussion, we summarize three challenges in building efficient biology question-reasoning systems: (C1) The scarcity of high-quality domain-specific corpora. While biological research publications are abundant, there remains a significant void in the availability of extensive, high-quality datasets to build robust information indexing models. (C2) The inherent complexity of biological knowledge systems. This complexity is compounded by the interdisciplinary nature of modern biological research. Consequently, automated question-reasoning systems must be able to understand and process multifaceted and often ambiguous biological query. (C3) The continual updating of knowledge. Biology is a dynamic field where discoveries are frequently made, and existing theories are regularly revised or replaced. This fluidity necessitates that question-reasoning systems adeptly select the knowledge source from databases or contemporary search engines to reflect the correct scientific understanding.\n Our Perspective and Contributions: To solve the above challenges, we proposed BIORAG, a novel Retrieval-Augmented Generation framework integrated with Large Language Models for biological question-reasoning. To obtain a robust domain-specific information indexing embedding model, we start by parsing, indexing, and segmenting extensive research articles from the biology domain and constructing high-quality training corpora. BIORAG then addresses the complexity of biological knowledge systems by combining a pre-built research hierarchy with an embedding model for accurate context retrieval. To cope with emerging biology knowledge, BIORAG can adaptively select knowledge sources from search engines, existing domain-specific tools, or indexed research articles. Once the framework determines that it has gathered sufficient information, it will generate the answer based on the reasoned material.\n We illustrate the question-reasoning power of BIORAG on 6 popularly used biology QA datasets and compare it against 6 baseline methods. Extensive case studies show the great potential to apply this framework to general science question-reasoning scenarios."}, {"title": "2 Biological Retrieval-Augmented Generation LLM Framework", "content": "In this paper, we propose the Biological Retrieval-Augmented Generation LLM Framework, namely BIORAG (as shown in Figure 2). In the following sections, we first introduce the preliminary step of constructing a high-quality local information source and training the biological domain-specific information indexing embedding model. For questions that require the most current or other domain-related data, we introduce external information sources. Then, we demonstrate the knowledge hierarchy-based query pre-processing, retriever execution component, and how the model iteratively collects sufficient information. Finally, the large language model will generate the answer based on the information obtained. The details of customized prompts are given in Section 2.4."}, {"title": "2.1 Internal Biological Information Source", "content": "High-quality domain-specific corpora are crucial for enriching the information source and enhancing the embedding model in the context of biological question-reasoning systems. To achieve this goal, we extract research papers from the global biomedical article database maintained by the National Center for Biotechnology Information\u00b9 (NCBI) (Schoch et al., 2020). This extensive repository aggregates over 37 million scientific citations and abstracts spanning from the 1950s to the present, encompassing a broad array of biomedical fields, including clinical medicine, molecular biology, etc. For the purposes of this study, we utilize the abstracts from"}, {"title": "2.2 External Information Sources", "content": "External biology knowledge is crucial to biological reasoning due to the rapidly evolving nature of biological research, which continuously integrates new discoveries. To address this challenge, we introduce two external information sources.\n Biological Data Hub: In BIORAG, we harness several specialized biological Hubs to ensure the accuracy of experimental data and to provide detailed biological insights. Specifically, BIORAGintegrates the following databases, each serving a unique purpose in the broader context of biological analyses: (1) Gene Database\u00b3: This resource provides comprehensive information on the functions, structures, and expressions of specific genes. It is invaluable for addressing queries related to gene mechanisms, gene actions, and gene expressions, facilitating a deeper understanding of gene-related phenomena. (2) dbSNP Database4: This database houses a vast repository of single nucleotide polymorphisms (SNPs), offering critical insights into genetic variants and their potential associations with various diseases. It is instrumental for studies exploring the genetic basis of disease and trait inheritance. (3) Genome Database: Providing complete genome sequences, this database is essential for studying the structure, function, and evolution of genomes across different organisms. It supports comprehensive genomic analyses and comparative studies, enhancing our understanding of genomic architecture and its functional implications. (4) Protein Database: This resource offers detailed information about the sequences, structures, and functions of proteins. It is crucial for exploring protein-related biological processes, understanding molecular functions, and investigating the complex interactions within the proteome.\n Search Engine: To ensure access to the most current discussions and developments, BIORAG incorporates a variety of search engines, including Google, Bing, arXiv, Wikimedia, and Crossref. Each platform contributes uniquely to the aggregation of information: (1) Google and Bing: These search engines scour the web for a diverse range of content, including news articles, blogs, and forums, providing insights into public discussions and concerns related to scientific topics. This breadth of information is crucial for understanding the societal impact and general discourse surrounding scientific issues. (2) arXiv: As a repository for preprint papers, arXiv offers access to the latest research reports and scholarly articles across multiple scientific disciplines before they undergo peer review. This source is invaluable for staying abreast of the newest scientific theories and experiments. (3) Wikimedia: Known for its user-friendly content, Wikimedia offers easily digestible explanations of complex scientific concepts and principles. This resource helps simplify advanced topics for broader public understanding and educational purposes. (4) Crossref: This service acts as a comprehensive aggregator of academic citation data, providing links to peer-reviewed scholarly publications and their citation networks. Crossref is essential for accessing high-quality research outputs and understanding their impact on the academic community."}, {"title": "2.3 Self-evaluated Information Retriever", "content": "Following the construction of the internal and external information source, BIORAG is firstly tasked with comprehending the complex disciplinary framework of the life sciences to retrieve the most relevant information accurately. Moreover, BIORAG integrates a self-evaluation mechanism to continuously assess the adequacy and relevance of the information it has collected.\n Internal Information Retrieve: To effectively navigate the inherent complexity of biological knowledge systems, BIORAG leverages an integrated approach, combining a well-defined hierarchical structure with indexed information to conduct a comprehensive internal information retrieval. The Medical Subject Headings7 (MeSH) thesaurus is popularly used for indexing, cataloging, and searching for biomedical-related information and research papers. Specifically, we first train a model MMESH to predict MeSH of the input questions. We then use the templates in Figure 3 for fine-tuning a Llama3-8B model to classify given questions. After that, we construct MeSH filtering SQLs (as shown in Figure 4) to generate the scalar condition retrieval. A candidate result is considered relevant to the given question because it has one consistent MeSH with the question. Then, the vector retrieval process is adopted to sort the relative results based on the cosine similarity of the sentence embedding between the input questions and the filtered results.\n Self-evaluation Strategy: In order to ensure the accuracy and contemporary of the retrieved information, BIORAG incorporates a self-evaluation strategy that assesses the adequacy of data collected from the internal knowledge base. In detail, this critical evaluation is driven by the backend large language model which aims to determine whether the information retrieved internally is sufficient to address the posed question substantively. If the internal content is insufficient, the model will loop back to pertinent external knowledge sources. Additionally, when the initial assessment indicates that the scientific questions require broader searches or retrieval of entity-specific data, the model tends to deploy external tools. This methodology supports the framework's goal of providing precise, up-to-date, comprehensive answers, facilitating more informed decision-making, and advancing research and applications in the life sciences."}, {"title": "2.4 Customized Prompts Detail", "content": "To maximize the effect of the retrieved corpus and knowledge, we design customized prompts in BIORAG. The prompts in Figure. 2 is detailed defined as follows,\n\u2022 Prompt # 1: To provide the most helpful and accurate response to the following Question: {Question}. You have been given descriptions of several RETRIEVAL METHODS: {Retrieval}. Please select the RETRIEVAL METHODS you consider the most appropriate for addressing this question.\n\u2022 Prompt # 2: Based on the RETRIEVAL METHODS you selected, and considering the Question and the Input Requirements of the retrieval method, please REWRITE the search query accordingly.\n\u2022 Prompt # 3: Now, using the rewritten QUERY and the retrieval FILTER methods, perform a logical combination to execute the search effectively.\n\u2022 Prompt # 4: Based on the RETRIEVAL RESULTS from the above steps, please evaluate whether the RESULTS support answering the original Question. If they do not support it, output \"NO\". If they do support it, output \"YES\".\n\u2022 Prompt # 5: Based on the RETRIEVAL RESULTS, perform a comprehensive reasoning and provide an answer to the Question.\n Furthermore, we designed instruction manuals for specialized biological tools and databases, aiming at exploiting their potentialities. These instructions are shown as follows,\n\u2022 Manual #Gene: The Gene database search engine is a valuable tool for retrieving comprehensive information about genes, including gene structure, function, and related genetic events. It is particularly useful for answering detailed questions regarding gene-related research and findings. To utilize this search engine effectively, the input must be a specific gene name.\n\u2022 Manual #dbSNP: The dbSNP database search engine is an essential tool for retrieving detailed information about single nucleotide polymorphisms (SNPs) and other genetic variations. It is particularly useful for answering questions related to genetic diversity, allele frequency, and related genetic studies. To utilize this search engine effectively, the input must be a specific SNP identifier or genetic variant name.\n\u2022 Manual #Genome: The Genome database search engine is an indispensable tool for accessing comprehensive information about entire genomes, including their sequences, annotations, and functional elements. It is particularly useful for answering complex questions about genomic structures, variations, and comparative genomics. To use this search engine effectively, the input must be a specific genome name or identifier.\n\u2022 Manual #Protein: The Protein database search engine is a crucial resource for obtaining detailed information about proteins, including their sequences, structures, functions, and interactions. It is particularly useful for answering questions related to protein biology, biochemical properties, and molecular function. To use this search engine effectively, the input must be a specific protein name or identifier.\n\u2022 Manual #Web Search: The Web Search Engine is a powerful tool designed to help you find information about current events quickly and efficiently. It is especially useful for obtaining the latest news, updates, and developments on a wide range of topics. To use this search engine effectively, simply enter a relevant search query.\n\u2022 Manual #PubMed: The PubMed local vector database search engine is an advanced tool designed for retrieving biomedical literature and research articles using vector-based search techniques. It is particularly useful for answering detailed questions about medical research, clinical studies, and scientific discoveries. To utilize this search engine effectively, the input should be a specific query or topic of interest."}, {"title": "3 Results & Analysis", "content": "We conduct experiments on 6 popularly used biological-related QA datasets to evaluate our proposed BIORAG, i.e., GeneTuring (Hou and Ji, 2023), MedMCQA (Pal et al., 2022), Medical Genetics (Hendrycks et al., 2020), College Biology (Hendrycks et al., 2020), College Medicine (Hendrycks et al., 2020). Note that the GeneTuring dataset contains more specialized biological questions. It contains 12 tasks, and each task has 50 question-answer pairs. We use 7 GeneTuring tasks that are related to NCBI resources to evaluate the proposed BIORAG. The chosen tasks are classified into three modules and briefly described as follows,\n\u2022 Nomenclature: This is about gene names. The objectives of the gene alias task and name conversion task are finding the official gene symbols for their non-official synonyms.\n\u2022 Genomics location: The tasks are about the locations of genes, single-nucleotide polymorphism (SNP), and their relations. We include the gene location, SNP location, and gene SNP association tasks. The first two tasks ask for the chromosome locations of a gene or an SNP, and the last one asks for related genes for a given SNP.\n\u2022 Functional analysis asks for gene functions. We use the gene-disease association task where the goal is to return related genes for a given disease, and the protein-coding genes task which asks whether a gene is a protein-coding gene or not."}, {"title": "3.2 Baslines", "content": "We compare BIORAGwith various baselines, which can be classified into three categories,"}, {"title": "3.3 Experimental Settings", "content": "We take the Llama3-70B as the basic language model of BIORAG. For our embedding model Memb, we take AdamW as the optimizer and fine-tune 2 epochs. The number of retrieved results by biological databases, search engines, and local PubMed databases are set to 10, 10, and 4, respectively. The max iteration of self-evaluation is set to 15. If the model does not output the final answer within 15 times, BIORAG stops the iteration and outputs the current wrong answer. We use the accuracy to verify the overall performance. For the GeneTuring dataset, we only consider exact matches between model predictions and the ground truth as correct predictions for all nomenclature and genomics location tasks. For the gene-disease association task, we measure the recall as in the original dataset but based on exact individual gene matches. For the protein-coding genes task, we consider exact matches as correct after applying a simple vocabulary mapping that converts model-predicted \"yes\" / \"no\" to \"TRUE\" / \"NA\" and Latin species names to their informal names, respectively. The final answer of other datasets is \"yes\" / \"no\"."}, {"title": "3.4 Results on Biological-related Tasks", "content": "To verify the effectiveness of the proposed model, we first conduct biological QA tasks. Results are shown in Table 2. We conclude with the following findings: (1) Based on the results of BioLLMs and GPT-3.5, we conclude that fine-tuning domain-specific data is helpful for domain-specific tasks. As the size of BioLLMs is much smaller than GPT-3.5, their performance is on par with GPT-3.5. (2) BIORAG performs better than BioLLMs and GPT-3.5, it indicates the effectiveness of local and external data sources. (3) Though the size of BIORAG is much smaller than SciRAG (NewBing), it has better performance. The gain comes from two aspects. The first one is our customized prompts. The second aspect lies in the local and external information sources. NewBing has no access to specialized databases and lacks technical biological descriptions for reasoning. (4) GeneGPT scores 0% accuracy in this task, because it is a customized model for the GeneTuring dataset, resulting in poor generalization capabilities."}, {"title": "3.5 Specialized Biological Reasoning Results", "content": "The GeneTuring dataset contains more specialized biological questions, and the corresponding reasoning process highly relies on technical biological corpus and descriptions."}, {"title": "3.6 Ablation Study", "content": "To evaluate the contribution of each component of BIORAG, we performed an extensive ablation study using the GeneTuring dataset, systematically removing individual components to assess their impact on performance across various tasks. This study was designed to isolate the effects of different databases, components, and base models, with the experiments categorized as follows: (1) Databases: We consider three variations to evaluate the effectiveness of each data sources of our database: D1: BIORAGwithout the Gene database; D2: BIORAGwithout general search engines. D3: BIORAGwithout the local PubMed database. (2) Model Components: We investigate the impact of specific components of our proposed framework: C1: BIORAGwithout the MeSH Filter; C2: BIORAGwithout the Query Rewrite component; C3: BIORAGwithout the Self-Evaluation mechanism. (3) Base Models: We compare the performance when using two different base LLM models: M1: take Llama-3-8B as the basic LLM, and M2: take Llama-3-70B as the basic LLM of BioRAG.\n Based on the results of ablation study, we highlights the following key findings: (1) Impact of Databases: The results indicate that the Gene database (D1) plays a crucial role in performance. For instance, the accuracy significantly drops in tasks such as Gene_location when this component is removed. The general search engines (D2) and local PubMed database (D3) also contribute positively, but their impact is less pronounced compared to the Gene database. (2) Component Contributions: Among the components, the Self-Evaluation mechanism (C3) is vital for maintaining high accuracy across most tasks. The MeSH Filter (C1) and Query Rewrite (C2) also enhance performance, but their absence does not degrade the results as severely as the removal of Self-Evaluation. (3) Effects of Basic Language Models: Comparing the two base models, Llama-3-70B (M2) generally outperforms Llama-3-8B (M1) across all tasks, indicating that the larger model size contributes to better handling of complex biological queries. These findings underscore the importance of integrating diverse data sources and advanced components within the BIORAG framework to achieve optimal performance in biological question reasoning tasks. By understanding the contribution of each component, we can better optimize BIORAG for different tasks and datasets."}, {"title": "3.7 Case Study", "content": "To compare reasoning differences among BIORAG and the baselines in a more intuitive manner, we select three typical case studies in this section. We first provide a case study to show the workflow of BIORAG (Figure 5). It is selected from the College Biology dataset. BIORAG performs self-evaluation twice: the first time it starts with a web search for general information, but the results are insufficient to support answering the question. Thus BIORAG conducts the second self-evaluation and calls for the more specialized PubMed database. The results this time are accurate and sufficient to support answering the question, thus BIORAG gives the final answer based on the results.\n The second case study is conducted on the gene alias task in the GeneTuring dataset (Figure 6). The challenge of this task is the variants of gene names. NewBing gets the response from the Wikimedia. However, Wikimedia is not specialized enough to provide the alias for the input gene, which leads to the wrong answer. The prompts of GeneGPT are too complicated, none of the prompts is relevant to this task. In addition, its NCBI API returns the gene IDs, instead of the gene names. The LLM is unable to understand these IDs, and finally arrives at a wrong answer. BIORAG employs fuzzy queries, yielding a larger number of related responses with a higher error tolerance. Furthermore, each result contains detailed gene-related information and descriptions, such as the aliases. Thus BIORAG gets the correct answer.\n The third case study is conducted on the gene-disease association task in the GeneTuring dataset, shown in Figure 7. Reasoning behind this task relies on both the Gene database and relative PubMed papers. The PubMed abstracts provide detailed gene-disease relationships. NewBing gets the response from the Geekymedics website. Although the Geekymedics website provides general medical information, it does not offer the correct or specific details required for gene-disease associations. Consequently, NewBing's response is inaccurate due to the reliance on a non-specialized source. GeneGPT chose the wrong NCBI API. The API's feedback is a complicated and interminable HTML page, with massive irrelevant information or descriptions. Based on the ambiguous backgrounds, GeneGPT outputs the wrong answer. In the reasoning process of BIORAG, BioRAG uses multiple tools, i.e., Gene database, local PubMed database, and Web search, to gather and conduct mutual confirmation on the information of genes associated with B-cell immunodeficiency. The process involves preprocessing queries, executing searches, and conducting self-evaluations at each step to ensure comprehensive and accurate results. The reasoning process is thorough, incorporating various data sources to confirm the association of specific genes with B-cell immunodeficiency."}, {"title": "4 Conclusion", "content": "This paper introduces BIORAG, an innovative framework that integrates Retrieval-Augmented Generation with Large Language Models to enhance biological question-reasoning. The framework's ability to obtain relevant and current information from a blend of traditional databases, toolkits, and modern search engines ensures the accuracy of the generated answers. Through extensive validation, including rigorous testing on widely recognized biology QA datasets and extensive case studies, BIORAG has demonstrated its superior ability to handle complex biological queries. These results underscore the framework's potential as a valuable tool for the scientific community, facilitating more accurate and efficient information processing."}]}