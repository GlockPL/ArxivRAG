{"title": "Enhancing Automated Loop Invariant Generation for Complex Programs with Large Language Models", "authors": ["Ruibang Liu", "Guoqiang Li", "Minyu Chen", "Ling-I Wu", "Jingyu Ke"], "abstract": "Automated program verification has always been an important component of building trustworthy software. While the analysis of real-world programs remains a theoretical challenge, the automation of loop invariant analysis has effectively resolved the problem. However, real-world programs that often mix complex data structures and control flows pose challenges to traditional loop invariant generation tools. To enhance the applicability of invariant generation techniques, we proposed ACInv, an Automated Complex program loop Invariant generation tool, which combines static analysis with Large Language Models (LLMs) to generate the proper loop invariants. We utilize static analysis to extract the necessary information for each loop and embed it into prompts for the LLM to generate invariants for each loop. Subsequently, we employ an LLM-based evaluator to assess the generated invariants, refining them by either strengthening, weakening, or rejecting them based on their correctness, ultimately obtaining enhanced invariants. We conducted experiments on ACInv, which showed that ACInv outperformed previous tools on data sets with data structures, and maintained similar performance to the state-of-the-art tool AutoSpec on numerical programs without data structures. For the total data set, ACInv can solve 21% more examples than AutoSpec and can generate reference data structure templates.", "sections": [{"title": "1 Introduction", "content": "Program verification has progressively emerged as a fundamental component in the development of reliable software within the industry, thereby necessitating advanced tools for automated program verification. Nonetheless, the verification of program properties in the presence of loops is inherently undecidable. Within the framework of Hoare logic (Hoare, 1969), loop invariants, which are usually assertions that are always true during the entry and execution of the loop, serve as the abstractions of loop properties. However, in the practical implementation of program verification tools such as CPAchecker (Beyer and Keremoglu, 2011), CBMC (Kroening and Tautschnig, 2014), SMACK (Rakamari\u0107 and Emmi, 2014) and Frama-C (Kirchner et al., 2015), the derivation of loop invariants often necessitates manual intervention by domain experts, which poses a substantial impediment to the full automation of program verification.\nLoop invariants are usually defined inductively, which means that they must meet the following three conditions: (1) Initialization: It is true before the first iteration of the loop; (2) Maintenance: If the invariant holds true at the beginning of an iteration, it must also hold true at the end of that iteration. Abstract interpretation-based methods (Rodr\u00edguez-Carbonell and Kapur, 2004; Rodr\u00edguez-Carbonell and Kapur, 2007; de Oliveira et al., 2016) was initially adopted by researchers to automated loop invariants generation. These approaches typically employ static analysis techniques, such as value analysis and interval analysis, to trace the values manipulated by the program and subsequently synthesize these values to construct loop invariants. However, these methods are limited to analyzing programs with simple numerical types and fail to adequately address intricate relationships between variables. Template-based methods (Sankaranarayanan et al., 2004; Yao et al., 2023a; Liu and Li, 2024; Col\u00f3n et al., 2003; Liu et al., 2022) typically involve predefining invariants using SMT formulas, proposing parameterized invariant templates via uninterpreted functions or inequalities, and employing heuristic strategies to determine the parameters. While this approach can reveal more complex relationships between variables, it remains largely confined to linear numerical inequalities and imposes significant computational costs due to the introduction of SMT constraint solving. The widespread adoption of machine learning has led many researchers to propose learning-based methods (Ryan et al., 2020; Garg et al., 2014; Si et al., 2018; Yu et al., 2023), which infer suitable invariant templates through specific deductions and learn the corresponding parameters using various learning techniques to generate invariants. However, their effectiveness remains limited by the predefined templates and the quality of the training data. Although there is a substantial body of prior work on the automatic generation of loop invariants, these methods often face challenges when applied to the verification of real-world programs, primarily due to their generality and associated performance overhead.\nIn recent years, Large Language Models (LLMs), such as ChatGPT, have emerged as powerful tools for automating various generative tasks. Several studies (Li et al., 2024; Su et al., 2024) have demonstrated their strong performance in program-related reasoning tasks. Notably, recent works (Chakraborty et al., 2023; Pei et al., 2023; Wen et al., 2024; Liu et al., 2024; Kamath et al., 2023) have highlighted the potential of LLMs in generating loop invariants. LLMs excel at recognizing complex structures, such as loops, and are capable of automating invariant generation across extensive codebases, significantly reducing the need for manual effort. On the other hand, as noted in previous work (Liu et al., 2023; Wei et al., 2022; Yao et al., 2023b; Yang et al., 2024), LLMs face certain challenges in invariant generation. First, LLMs may produce incorrect invariants, and these errors can accumulate when the models are invoked repeatedly. Second, without sufficient information or carefully crafted prompts, even for correct invariants, LLMs tend to generate weaker invariant constraints, which may prevent the verification of the program's target properties.\nTo address these challenges, we propose ACInv, an Automated Complex program loop Invariant generation tool leveraging LLMs in conjunction with static analysis. Specifically, we argue that one of the complexities of real-world programs arises from the variety of data types and user-defined data structures, as well as the complexity of loop constructs. Consequently, ACInv must first leverage static analysis techniques to extract information on both the data structures and control flow of the program, and the information obtained can enhance the LLM's understanding of the program. Furthermore, it is crucial to provide the LLM with opportunities to assess and revise the generated invariants, which not only corrects erroneous invariants but also attempts to strengthen the valid ones. Therefore, ACInv, after leveraging static analysis to extract essential loop information, generates predicates that abstract the properties of data structures used in the program and subsequently generate the corresponding invariants. Once the invariants are generated, ACInv employs an LLM-based evaluator to assess their correctness. Based on this evaluation, the LLM further optimizes the invariants. For correct invariants, ACInv attempts to strengthen them, refining their scope. For incorrect invariants, ACInv weakens them to encompass the correct invariant space. If even after weakening, the invariant cannot cover the correct space, the invariant is rejected.\nWe contend that ACInv offers several distinct advantages over existing tools. First, it is capable of analyzing a wide range of complex, real-world code, with its sophisticated template processing for data structures and effective handling of nested loops enabling it to tackle intricately structured programs. Second, ACInv delivers a fully automated, end-to-end solution for loop invariant generation, significantly reducing the need for manual intervention throughout the process. Lastly, while maintaining a competitive level of accuracy, ACInv minimizes time overhead and optimizes the quality of the generated invariants, thereby enhancing their precision and robustness in practical applications.\nTo evaluate ACInv, we conducted a series of experiments. We conducted experiments on ACInv on datasets with and without data structures. The experiments showed that ACInv performed better than all current tools on datasets with data structures. On datasets without data structures, ACInv also performed close to the performance of the state-of-art tool AutoSpec (Wen et al., 2024). Overall, ACInv can solve 21.08% more examples than AutoSpec.\nIn summary, our contribution is:\n\u2022 We constructed a dataset that is more representative of real-world programs for loop invariants\n\u2022 We generated some predicate templates for some commonly used data structures and other user-defined data structures.\n\u2022 We developed a more realistic, real-world program-oriented automatic loop invariant generation framework and implemented it as a tool ACInv."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Loop Invariant", "content": "To provide a formal characterization of the invariant, we first abstract a loop of programs as follows:\n$\\text{Pre}(X) \\text{while}(B)\\{S\\} \\text{Post}(X)$\nwhere $X$ is the set of variables occurring in the program, $\\text{Pre}(X)$ and $\\text{Post}(X)$ denote the pre-conditions of the loop and post-conditions of the loop, $B$ represents the loop condition, and $S$ is the loop body.\nTraditional symbolic methods for invariant generation typically involve direct code analysis, mainly including abstraction interpretation and constraint solving. Abstract interpretation (Cousot and Cousot, 1977) approximates a program's behavior by mapping its concrete semantics to a simplified abstract domain, often using fixed-point calculations to abstract program states in the presence of loops, such as Enric et al.(Rodr\u00edguez-Carbonell and Kapur, 2004) and Steven et al. (de Oliveira et al., 2016). Constraint-based methods, on the other hand, rely on pre-defined templates and employ SMT solvers to filter out appropriate invariant expressions. Farkas' lemma (Sankaranarayanan et al., 2004; Yao et al., 2023a; Liu and Li, 2024; Col\u00f3n et al., 2003; Liu et al., 2022) is frequently used to extract linear relationships from loops, while other constraint templates, such as those based on Craig's interpolant (McMillan, 2003; Dillig et al., 2013a,b), are also employed.\nLearning-based methods often build upon template-based approaches by learning the parameters within these templates through various machine-learning techniques and ultimately invoking an SMT solver to filter suitable invariants. C2I (Sharma and Aiken, 2014) employs a randomized search to discover candidate invariants, followed by a checker to validate them. ICE (Garg et al., 2014) did learning for synthesizing invariants, leveraging examples, counterexamples, and implications. Then they combined them with decision trees (Garg et al., 2016). CODE2INV (Si et al., 2018, 2020) utilizes reinforcement learning and graph neural networks to learn program structures. LIPuS (Yu et al., 2023) also adopts reinforcement learning and uses pruning to reduce the search space. CLN2INV (Ryan et al., 2020) introduces Continuous Logic Networks (CLN) to automatically learn loop invariants directly from program execution traces, while Gated-CLN (G-CLN) (Yao et al., 2020) extends this approach to allow the model to robustly learn general invariants across a large number of terms."}, {"title": "2.2 Large Language Models (LLMs)", "content": "In recent years, LLMs have emerged as powerful tools for understanding and generating human-like language. The development of large-scale pre-trained models like GPT (Generative Pre-trained Transformer) (Brown et al., 2020) has led to substantial improvements across a variety of NLP tasks including code-related task with prompting skills like Chain-of-Thought (CoT) prompting (Wei et al., 2022) and few-shot learning (Brown et al., 2020).\nSeveral works have explored the intersection of LLMs and loop invariant generation with promising outcomes. Autospec (Wen et al., 2024) employs hierarchical specification generation to produce ACSL-style annotations, which include loop invariants, and has achieved good performance in the processing of numerical programs. LIG-SE (Liu et al., 2024) fine-tuned LLMs and combining it with a checker gives the large model the ability to handle separation logic. It can also handle data structures and abstract predicate properties to a certain extent. The iRank tool (Chakraborty et al., 2023) introduces a re-ranking approach that evaluates loop invariants produced by LLMs. Unlike other methods, iRank does not focus on generating loop invariants but instead addresses the challenge of verifying the correctness of LLM-generated invariants. To achieve this, the authors employed two distinct LLM-based embedding models to learn the criteria for determining invariant correctness. Kexin et al.(Pei et al., 2023) aimed to fine-tune a pre-trained LLM for abstract reasoning over program executions, allowing for the generation of loop invariants. Their approach relies on a dynamic analyzer to trace program executions, which are then used to derive invariants. However, their evaluation is limited to a comparison against invariants generated by the dynamic analyzer, which poses inherent constraints. Loopy(Kamath et al., 2023), on the other hand, continuously queries LLMs to generate invariants and employs a verification tool to ensure correctness. While they enhance the generated invariants by cyclically applying the Houdini algorithm and making repairs, the repeated invocation of the verifier introduces significant performance overhead."}, {"title": "3 Our ACInv", "content": ""}, {"title": "3.1 Overview", "content": "In this section, we will first provide an overview of the entire workflow, followed by a detailed explanation of each component. In Figure 1, we present an overview diagram illustrating our proposed approach. To address the aforementioned challenge, we introduce ACInv, an LLM-based tool for automatic loop invariant generation. ACInv's process consists of three main phases: (1) Extraction Phase: ACInv initially extracts data structure and loop information from the target program. This involves performing basic control flow and value analyses, classifying loops, and gathering relevant variable information for each loop. (2) Generation Phase: During this phase, ACInv embeds the extracted data structure information into data structure prompts, querying LLMs to derive predicates that abstract key properties of the data structure. These predicates, along with loop-specific information, are then embedded into invariant prompts to generate loop invariants. If the current loop is not the first in the program, information from preceding loops, along with their generated invariants, will also be incorporated into the prompts. (3) Augmentation Phase: This section mainly consists of two components: the evaluator, and the optimizer, which are all composed of LLMs. The evaluator assesses the correctness of each generated invariant. For correct invariants, the optimizer attempts to strengthen them to enhance precision. For incorrect invariants, the optimizer weakens them to align with the correct program state. If an invariant cannot be refined to the correct level after k iterations, it is discarded. In the remainder of this section, we will provide a detailed explanation of the entire methodology and its related intricacies."}, {"title": "3.2 Prompts Design", "content": "To provide a more formal description of the process, we formulate the invariants generation task as follows: let M denote the LLMs, and C represent the code with a maximum of n lines. Each line location in the code is denoted by $l_0, l_1,..., l_n$, which may correspond to a loop or data structure. $P_i$ represents the data structure retrieved and loopno, $[v_0, v_1,\\cdot\\cdot\\cdot, v_j]$ for the order number of the loop and the key variables of the loop separately. For each line $l_i$, $A_l$, denotes the result of the query specific to line $l_i$, while $I_0, I_1, ..., I_m$ represent the target invariants at location $l_i$ that point to the loop.\nTherefore, the current problem becomes: Given a program C with a maximum number of lines n,\ngive loop invariants $I_{l_0}, I_{l_1},..., I_{l_m}$ .\nDrawing from various prompting techniques (Liu et al., 2023; Wei et al., 2022; Yao et al., 2023b; Yang et al., 2024), we designed a structured prompt framework, as illustrated in Figure 2. Our approach primarily employs two types of prompts: generating predicates and loop invariants separately. The pre-loop lines are used exclusively when generating loop invariants, and at this stage, the invariants from previous loops must be provided. In other informational lines, prompts for generating predicates and prompts for generating invariants utilize their respective keywords and examples."}, {"title": "3.3 An illustrative example", "content": "We will use C language and ANSI/ISO C Specification Language(ACSL) to describe the loop invariants of the C program as the research object, which can be automatically checked by the verification tool Frama-C (Kirchner et al., 2015) and its plugin WP. In ACSL, all the information needed is implemented as the annotation which begins with  \\ @  or is wrapped with  \\*@...*\\. loop invariants are annotated beginning with loop invariant, and assert represents for assertion of certain properties at a particular program point.\nConsider the program fragment illustrated in Fig 3 from the widely-used FFmpeg (FFmpeg, 2023) library, an open-source multimedia framework written in C. For this program fragment, we will focus on its main components. The VolDetectContext structure is employed to store audio sample values for plotting frequency distributions. It contains a single member, histogram[], which is an array of unsigned 64-bit integers used to represent the distribution of audio sample frequencies. The filter_frame function processes audio filters within FFmpeg. It iterates through each audio plane of the input audio frame (AVFrame) and adjusts the corresponding count in vd->histogram, thus forming a nested loop structure. The expression pcm[i] + 0x8000 shifts the sample values (typically ranging from -32768 to 32767) to a 0 to 0xFFFF range for indexing purposes."}, {"title": "3.4 Extraction Phase", "content": "In this phase, ACInv mainly applies static analysis techniques on program C and returns both data structure and loop information. ACInv obtains variable information for each layer of the loop by traversing the loop body and annotate the data structure used. Then ACInv sorts the loop by the control flow of programs. Each loop is assigned a sequence number, adhering to the following rules: for nested loops, the innermost loop is assigned the smallest number, while for parallel loops, the first encountered loop receives the smallest number.\nFor each loop, ACInv captures key variable information, which is specifically, all assigned variables and parameters passed as pointers in function calls. Variable information can help LLM focus its attention on key parts, assisting in generating invariants. The sorting of loops is to further decompose the program, step-by-step generating invariants for the program.\nFor the example in Fig 3, ACInv will record the declaration of the structure VolDetectContext and its location information, and then traverse the program body. After completing the traversal of the loop body, it assigns sequence number 1 to the inner loop with line numbers 23-24 and extracts key variables: {i, vd}; then assigns sequence number 2 to the outer loop with line numbers 20-25 and extracts key variables: {plane, pcm[]}, as well as {i, vd} inherited from the inner loop."}, {"title": "3.5 Generation Phase", "content": "In this phase, ACInv will leverage the information extracted in the previous phase to generate loop invariants by querying LLMs. First, ACInv will embed the extracted data structure into the data structure prompts in this step and query the LLM to obtain predicate templates that may be used for this data structure. In prompts, we will emphasize that these predicates need to be able to abstract the data structure itself, and in addition, provide some other properties about this data structure. In the syntax of ACSL, we can use predicate to define predicates related to non-recursive data structures, and inductive to define predicates related to recursive data structures. In order to help LLM understand predicate generation, we provided some examples to LLM,. For non-recursively defined data structures, we provide a classic example: the stack, and linked list for the recursively defined data structures. The details of the examples can be found in appendix A.\nThen, combined with the possible predicates, ACInv embeds the program C and the loop information into the invariants prompt to query the LLM. Based on the order of the loops, ACInv will sequentially query the LLM for the loop with the smallest ordinal number, and require the LLM to generate invariants for that loop. Within each loop, we embed the processed program into prompts and require the LLM to analyze the following: (1) the range of values of the annotated variables; (2) Possible logical relationships between these variables and constants; (3) Does the data structure variable satisfy the predicate.\nFigure 3 illustrates the majority of the process for this step. Initially, two predicates are generated based on the data structure, which represents whether it is a valid VolDetectContext and the non-negativity of the array histogram[]. Using these predicates, ACInv first derives an invariant for Loop No. 1, which is an inner loop. Once this invariant is obtained, all invariants within this loop are forwarded to the next stage for enhancement. Following this, the enhanced invariant is used as a reference for generating the invariant for Loop No. 2 by querying the LLM."}, {"title": "3.6 Augmentation Phase", "content": "In this phase, ACInv focuses on refining the generated invariants by either strengthening or weakening them. Unlike the previous phase, this stage is loop level, which means that for each individual loop, the generated invariant will first be sent to this stage to try to be enhanced, and then the enhanced invariant group will be sent back to the generation phase as a reference for generating invariant in subsequent loops. It begins by utilizing an evaluator to assess the correctness of the invariants and then calls upon an optimizer to take appropriate actions based on the evaluation.\nIf an invariant is deemed correct, ACInv will attempt to \"strengthen\" it. In the context of invariants, this typically involves narrowing the range of program variable states represented, resulting in a more precise invariant. Conversely, if the invariant is found to be incorrect, ACInv seeks to \"weaken\" it. This process involves expanding the range of program variable states to encompass the correct set of states within the revised invariant. By broadening the scope of the incorrect invariant, ACInv aims to adjust it to better reflect the valid program states.\nIn practice, before entering the augmentation phase, the invariant generated by ACInv for Loop No. 2 in Figure 3 is 0 <= plane < nb_planes. However, during the execution of the loop, although the outer loop condition is plane < nb_planes, the final iteration may satisfy plane == nb_planes. Thus, this invariant does not cover all possible program states. The evaluator can identify this issue and the optimizer can adjust the invariant by expanding its range to 0 <= plane <= nb_planes, ultimately providing the refined invariant."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental Setup", "content": "Dataset Our dataset is mainly divided into two parts, those with data structures and those without data structures. The dataset with data structure consists of three parts: LIG-MM (Liu et al., 2024; Thinklab-SJTU, 2023), SV-COMP (Beyer, 2024; SoSy-Lab, 2024), and FFmpeg (FFmpeg, 2023). We will elaborate on these three datasets in more detail later. The dataset without data structure mainly comes from the benchmark from AutoSpec (Wen et al., 2024), and we will also briefly describe this dataset.\nFirst, we selected the dataset from LIG-MM, which is a new benchmark specifically for programs with complex data structures and memory manipulations. LIG-MM is also divided into four parts: (1) the course programs; (2) SV-COMP22 (Beyer, 2022); (3) Benchmark from SLING (Le et al., 2019), which is a separation logic loop invariant inferring tool using dynamic analysis; (4) Some real-world programs from well-known software and systems. LIG-MM consists of C code and focuses on some complex data structures, especially the single-linked list, double-linked list, tree, and hash table.\nWe also picked C programs from SV-COMP 2024. LIG-MM and the benchmark of AutoSpec both select some programs from SV-COMP. Unlike them, we selected 31 programs, all of which include at least one loop and one data structure. Among these data structures, we excluded all structures containing the single-linked list, double-linked list, tree, and hash table to avoid duplication with the selected parts in LIG-MM.\nTo evaluate the effectiveness of ACInv on real-world programs, we selected several program segments from the widely used audio and video processing library FFmpeg. Given the complexity of function calls and structures within C programs, as well as the extensive codebase, we focused on specific portions of the program. We applied simple partitioning techniques, ensuring that ACInv could concentrate on the relevant functions for analysis, and we picked the programs with at least one loop and at least one data structure.\nWe also selected programs from the benchmark of AutoSpec, which assesses the tool's ability to synthesize specifications, including loop invariants, preconditions, and postconditions of functions. Notably, some C programs in this benchmark do not contain loops, so we specifically chose a subset that includes loops for our testing. The benchmark from AutoSpec comprises four parts: Frama-C, SyGuS, OOPSLA-13, and SV-COMP22. All of these components consist of C code and are primarily focused on numerical programs without explicit structural declarations.\nBaseline In this study, we compare the performance of several tools with ACInv on datasets both with and without data structures. For datasets with data structures, we evaluate SLING, LLM-SE (Liu et al., 2024), GPT-40, and AutoSpec. SLING is a dynamic analysis tool for inferring loop invariants based on separation logic. LLM-SE leverages a fine-tuned language model (LLM) to generate separation logic loop invariants, incorporating predicate decomposition into its training. Since it neither provides the fine-tuned model nor the training data, we only use the results of its open-source data as a reference. AutoSpec employs hierarchical specification generation to produce ACSL-style annotations. Both LLM-SE and AutoSpec utilize verification checkers to ensure the correctness of the generated invariants. GPT-40, in this context, is used by directly embedding the code into prompts, asking GPT-40-2024-08-06 to generate loop invariants without employing specific prompting techniques or examples.\nFor datasets without data structures, we assess CODE2Inv (Si et al., 2018, 2020), CLN2Inv (Ryan et al., 2020), GPT-40, and AutoSpec. CODE2Inv and CLN2Inv are state-of-the-art invariant generation tools for numerical programs, based on graph neural networks and continuous logic networks, respectively. AutoSpec was originally a tool for testing on this dataset, and we naturally referred to its results."}, {"title": "4.2 Overview of results", "content": "Table 1 shows the main results of our experiments, where completion means the rate of programs whose loop invariants can be inferred correctly to complete the verification of this program. In this experiment, considering the balance between time and performance, we set the augmentation iteration parameter k to 5.\nWe first focus on datasets that include data structures. Compared to the traditional tool SLING, LLM-based technologies, including ACInv, demonstrate strong applicability. While SLING can only reason about the linked list portion of LIG-MM, it struggles with other complex data structures. Experimental results from LLM-SE indicate good performance on singly linked lists (sll), doubly linked lists (dll), trees, and hash tables. However, LLM-SE did not provide training sets or fine-tuned models, limiting the ability to evaluate its performance on other data structures. Additionally, LLM-SE incorporates pre-defined predicates during fine-tuning, enabling abstraction during reasoning, though this approach may restrict its effectiveness on other data structures or non-pointer data structures.\nIn comparison to pure LLM querying and the latest technology, AutoSpec, ACInv performs well on datasets involving data structures. This performance is anticipated. In pure LLM querying, we observed that while the LLM can recognize data structures in the program and analyze the associated variables, it struggles to abstract them into suitable logical forms. This often results in accurate formulas being expressed in natural language rather than logical terms. Regarding AutoSpec, its examples primarily focus on numerical types, leading to suboptimal performance on datasets involving data structures.\nThe FFmpeg dataset presents significant challenges due to its many unpredictable user-defined data structures, making it difficult for large models to abstract their properties. None of the tools performed well in this scenario, though ACInv exhibited some advantages. Overall, the experiments suggest that ACInv possesses a notable degree of abstraction and reasoning capability across various datasets involving complex data structures.\nIn numerical programs, ACInv demonstrated performance comparable to AutoSpec. The SyGuS benchmarks, which consist of relatively simple programs with a single loop and focus on basic numerical transformations, saw CLN2Inv achieving the best results. However, both CODE2Inv and CLN2Inv performed poorly on other datasets, likely due to limitations in their training sets, highlighting the broader applicability of LLM-based technologies. The Frama-C problems benchmark, which involves nested loops and more complex variable interactions, posed greater challenges. ACInv, leveraging value analysis, performed notably better in these cases.\nOverall, ACInv approached state-of-the-art performance in numerical programs without data structures and demonstrated clear advantages across diverse datasets."}, {"title": "4.3 Ablation Study", "content": "We conducted an ablation study on the final augmentation stage. Figure 4 shows the results of the experiment on the augmentation of ACInv. We tested the invariants generated by ACInv on a structured dataset, where Acc means the proportion of generated invariants that can pass the Frama-c check, and model@kmeans k augmentation iterations were performed on the model. The experiment shows that when we use appropriate prompts and examples, the augmentation stage can judge the correctness of the invariants with a relatively high probability and filter out incorrect invariants."}, {"title": "5 Conclusion", "content": "In this paper, we proposed ACInv, an Automated Complex program loop Invariant generation tool leveraging Large Language Models (LLMs) in conjunction with static analysis. ACInv can abstract the data structure properties in complex programs and combine them to generate invariants. ACInv can also evaluate and enhance the generated invariants to a certain extent. We have conducted experiments on the effect of ACInv, and the experiments show that ACInv has achieved certain results on various data sets."}, {"title": "6 Limitations", "content": "Data leakage. The extensive training sets used in LLMs can lead to potential data leakage, meaning that some of the code related to the questions we inquired about might have appeared in the training data of certain models. Although we have applied simple variable renaming and reordered the code, similar issues may still arise.\nCorrectness determination. Most related work, particularly during the final evaluation stage (and in some cases even in earlier stages), employs a deterministic checker to validate the generated invariants for filtering purposes. We did not adopt this approach here, partly due to performance overhead concerns and partly to avoid making the generated invariants dependent on a specific checker. In our experiments, we observed that some correct invariants could not be verified by the checker due to limitations in the checker's capability and available computational resources. In future work, we aim to follow up on this aspect, seeking to strike a more effective balance between correctness and scalability."}, {"title": "A Examples of generating predicates", "content": "The C language definition of the stack and linked list is usually as Figure 5.\nWe expect the generated predicate to at least express that this is an effective stack; In addition, we also hope that he can express some other properties about the stack, such as whether the stack is empty or full. Therefore, in this example, we provide the following predicate such as Figure 6. Where in ACSL grammar, \\valid(s) means the pointer s is valid to read or write.\nThe recursive data structure is usually related to pointers. Taking inspiration from separation logic (Reynolds, 2002), we provide such a recursive definition for the following single linked list, which considered both empty and non-empty linked lists, and expressed that the pointer p1 to p2 is a single linked list such as Figure 7."}]}