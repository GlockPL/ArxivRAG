{"title": "Detecting Music Performance Errors with Transformers", "authors": ["Benjamin Shiue-Hal Chou", "Purvish Jajal", "Nicholas John Eliopoulos", "Tim Nadolsky", "Cheng-Yun Yang", "Nikita Ravi", "James C. Davis", "Kristen Yeon-Ji Yun", "Yung-Hsiang Lu"], "abstract": "Beginner musicians often struggle to identify specific errors in their performances, such as playing incorrect notes or rhythms. There are two limitations in existing tools for music error detection: (1) Existing approaches rely on automatic alignment; therefore, they are prone to errors caused by small deviations between alignment targets.; (2) There is a lack of sufficient data to train music error detection models, resulting in over-reliance on heuristics. To address (1), we propose a novel transformer model, Polytune, that takes audio inputs and outputs annotated music scores. This model can be trained end-to-end to implicitly align and compare performance audio with music scores through latent space representations. To address (2), we present a novel data generation technique capable of creating large-scale synthetic music error datasets. Our approach achieves a 64.1% average Error Detection F1 score, improving upon prior work by 40 percentage points across 14 instruments. Additionally, compared with existing transcription methods repurposed for music error detection, our model can handle multiple instruments. Our source code and datasets are available at https://github.com/ben2002chou/Polytune.", "sections": [{"title": "Introduction", "content": "Beginner musicians often need help identifying errors in their performance. For example, novice musicians may struggle with sight reading or miss notes due to a lack of muscle memory. Access to music education programs which could help address these issues is limited; in the USA alone, approximately 4 million K-12 students do not have access to music education (Morrison et al. 2022).\nTo bridge this gap, commercial music tutoring tools have become essential resources. Beginner musicians can practice more effectively, and teachers are provided with insights into students' progress (Nart 2016; Apayd\u0131nl\u0131 2019). The significant demand for such automated solutions is evident, with apps like Yousician (Yousician 2024) and Simply Piano (JoyTunes 2024) each having over 10 million downloads globally.\nHowever, Simply Piano and Yousician only identify notes as correct or incorrect, without offering detailed feedback such as missed or extra notes. They are also incapable of automatically aligning the user's performance with a reference score. Instead, Simply Piano and Yousician rely on the user to match their performance with the reference audio. Furthermore, their models cannot handle multiple instruments.\nThe research community has also attempted to provide fine-grained music performance feedback but has had limited success (Benetos, Klapuri, and Dixon 2012; Wang, Ewert, and Dixon 2017). A major paradigm of prior work is to temporally align a student's performance with a reference score, and then identify differences.\nThese alignment-based approaches often fail when there are deviations in the played notes from the score, even if they are minor. The resulting misalignment of notes leads to inaccurate error detection and ineffective feedback for students.\nIn this paper, we introduce a transformer based-model (Vaswani et al. 2017) called Polytune to detect musician errors without relying on automatic alignment. Our model utilizes an end-to-end trainable approach for error detection. Specifically, Polytune takes audio spectrogram pairs as inputs, then outputs an annotated musical score without requiring any manual intervention. We train Polytune to annotate notes with a variety of labels beyond \u201ccorrect\u201d or \"incorrect\", as shown in Fig. 1. For training, we require a larger set of training samples than what is provided by ex-"}, {"title": "Background and Related Work", "content": "This section explores existing methods for score-informed error detection, advances in token-based music transcription, and the deficiencies of current datasets."}, {"title": "Application Context: Music Terminology", "content": "We define key musical terms used in this paper: Music Score: A written guide showing notes, rhythms, and instructions for performance. Music Note: A symbol indicating the pitch and duration of a sound. Chord: Multiple notes played together. Chordal: Music with multiple notes at once (Britannica 2007) MIDI: A standard that uses symbols to represent musical events like pitch and duration, enabling electronic instruments and computers to communicate. Track (MIDI Track): A sequence of MIDI events representing one instrument."}, {"title": "Score-Informed Music Performance Assessment", "content": "Music performance assessment requires comparing a player's rendition against a musical score. In score-informed music error detection systems, the score serves as a reference for the detection of errors like missed or extra notes. As shown in Fig. 2a, systems by Benetos et al. (Benetos, Klapuri, and Dixon 2012) and Wang et al. (Wang, Ewert, and Dixon 2017) use Automatic Music Transcription (AMT) to convert audio into musical notation and align it with the score to identify errors like missed or extra notes. Dynamic Time Warping (DTW) (Sakoe and Chiba 1978) is a widely used technique in music tutoring applications for aligning musical performances with reference scores (Benetos, Klapuri, and Dixon 2012; Wang, Ewert, and Dixon 2017; Ewert, Wang, and Sandler 2016). By minimizing temporal differences between two time series, DTW employs dynamic programming to find the optimal alignment. However, DTW faces challenges when dealing with music that contains overlapping notes, as shown in Fig. 3. This forces DTW to compromise, resulting in imperfect alignment or distortion. Instead, our approach shown in Fig. 2b replaces DTW with a learnable alignment mechanism that does not encounter the same problems as DTW. We find this leads to more accurate predictions."}, {"title": "Token-Based Automatic Music Transcription (AMT) Methods", "content": "Recent AMT advancements are influenced by Natural Language Processing (NLP) techniques, particularly in how outputs are represented. In token-based AMT models, the output is represented as sequences of tokens, where each token corresponds to an element of musical information. MT3 (Gardner et al. 2022) is a token-based AMT system using a T5 (Raffel et al. 2020) transformer to output MIDI-like tokens (Oore et al. 2020). These tokens directly represent music and are inspired by the MIDI format. AMT is treated as a language modeling problem, predicting a sequence of event tokens like time tokens, program tokens, note-on/off tokens, and note tokens.\nMT3 uses spectrogram frames as the encoder input, providing context for the autoregressive decoder, which predicts future tokens based on past outputs. This allows the model to capture temporal patterns in music, similar to how NLP models handle text sequences. By using token-based representations, MT3 accurately transcribes music. Building on this capability, we adapt MT3 in this paper specifically for error detection."}, {"title": "Datasets for Music Error Detection", "content": "Deep learning models require extensive training data. Unfortunately, existing datasets for music error detection are limited, with only seven tracks, 15 minutes of audio, and just 40 errors in total (Benetos, Klapuri, and Dixon 2012). This scarcity restricts the training of end-to-end models. Training on such a small dataset would cause the model to overfit (Ying 2019). This highlights the need for more comprehensive datasets to advance research and development in this field."}, {"title": "End-to-end Music Error Detection", "content": "In this section, we outline key designs of our architecture and training regime. First, we provide the intuition of our model and describe how it is related to prior work in Sec. 3.1. Second, we describe and justify design decisions of our model architecture in Sec. 3.2. Third, we investigate the end-to-end training strategy, and our approach to generating large-scale datasets with synthetic errors in Sec. 3.3. Last, we provide upgraded implementations of (Benetos, Klapuri, and Dixon 2012) and (Wang, Ewert, and Dixon 2017) to fairly represent older work and contextualize our contributions better in Sec. 3.4."}, {"title": "Intuition and Comparison to Prior Work", "content": "Our method addresses the main limitations of previous methods, as shown in Tab. 1. Benetos et al. (Benetos, Klapuri, and Dixon 2012) and Wang et al. (Wang, Ewert, and Dixon 2017) rely on time-warping algorithms to align music scores with audio recordings. These time-warping algorithms suffer from alignment inaccuracies when notes that should be simultaneous fall out of sync.\nTo overcome these limitations, we introduce a learnable end-to-end model Polytune. It processes two unaligned audio sequences\u2014one from the musical score and one from the performance. We design the inputs and outputs to implicitly teach the model alignment, leading to more accurate predictions. This eliminates the need for complex pre-processing and post-processing steps, such as alignment and comparison, as illustrated in Fig. 2. We developed an updated baseline using the score-informed transcription systems of Benetos et al. (Benetos, Klapuri, and Dixon 2012) and Wang et al. (Wang, Ewert, and Dixon 2017), incorporating the MT3 model and Dynamic Time Warping (DTW) to reflect current best practices (Sec. 3.4)."}, {"title": "Music Error Detection Model", "content": "We show how our model architecture can be designed to implicitly learn better sequence alignment in latent representation, which is crucial for accurately detecting errors in music performances. Since transcription plays a key role in existing methods for music error detection, we base our approach on MT3. Specifically, we frame music error detection as a sequence-to-sequence task, where the input consists of audio spectrogram frames and the output is represented by customized tokens for error detection.\nHawthorne et al. treats music transcription as a sequence-to-sequence modeling task, using audio spectrogram frames as inputs and MIDI-like tokens as outputs (Hawthorne et al. 2021). Building on this foundation, MT3 (Gardner et al. 2022) advanced the piano transcription model by generalizing it to transcribe different instruments. Therefore, we base our model's decoder on MT3, a well-studied state-of-the-art Automatic Music Transcription (AMT) method.\nThe architecture of Polytune is detailed in Fig. 4. The design fuses two inputs: the audio from the musical score and the player's corresponding performance. These are encoded into a joint encoding space. We treat our model as multi-modal, utilizing modality-specific encoders similar to those in (Gong et al. 2023; Akbari et al. 2021). Our reasoning is that although both inputs are audio spectrograms, the score and performance audio serve distinct roles in music error detection.\nWe fuse two Audio Spectrogram Transformer (Gong, Chung, and Glass 2021) (AST) encoders with a joint encoder. Then, we use a standard T5 decoder, with greedy autoregressive decoding to output sequences of tokens, described in Sec. 3.3. To bridge the fused AST encoders and the T5 decoder, we employ a linear projection layer to transform the embedding size from AST's 768 to T5's 512. The encoder outputs condition the decoder via a cross attention mechanism."}, {"title": "End-to-End Training of the Music Error Detection Model", "content": "Input Data We use audio inputs synthesized from musical scores rather than symbolic representations like MIDI tokenizations. This approach has several advantages:\n1. Generalization: Audio inputs generalize across instruments and capture their full sound, including characteristics like vibrato (violin), trills (flute), and pitch bending (guitar), which MIDI tokenizations fail to represent. MIDI tokenizations are designed primarily for piano. Their design prohibits them from representing non-fixed-pitch instruments or capturing instrument-specific nuances (e.g., see (Hsiao et al. 2021; Huang and Yang 2020)).\n2. Extensibility and Expressiveness: Audio preserves absolute timing and dynamics, enabling the detection of new error types, such as timing or dynamics errors, without requiring changes to the input representation. In contrast, MIDI tokenizations reduce sequence length to address the O(N^2) computational cost of transformers but sacrifice expressiveness and timing precision. For instance, the Octuple tokenizer achieves a fourfold reduction in sequence length compared to REMI (Huang and Yang 2020), but introduces errors in complex time signatures (Fradet, Briot, and Chhel 2021).\n3. Comparability: Audio inputs allow users to compare performances against any pre-recorded audio, enabling analysis in contexts where scores are unavailable or nonexistent, such as impromptu jazz or folk music.\n4. Simplicity in Design: Using audio for both inputs simplifies the model by reusing the same feature extractor design for both encoders. We conjecture that using a single input format for the music score and performance enables the model to learn implicit alignment more effectively. This avoids the additional capacity required to process and compare different modalities. By using the same input format, we eliminate unnecessary complexity from the task.\nFig. 4 illustrates our data flow. We divide the student's recorded audio and the synthesized score audio into non-overlapping segments. The segment length is 2.145 seconds. Each segment undergoes a short-time Fourier transform, producing spectrogram frames. Finally, we tokenize each spectrogram frame using the ViT patch embedding approach (Dosovitskiy et al. 2021) and feed the resulting 512 patches into our ViT encoder.\nOutput Data We output a sequence of MIDI-like tokens to represent the music scores with errors annotated. The complete token vocabulary is summarized in Tab. 2. The vocabulary is similar to (Gardner et al. 2022; Hawthorne et al. 2021) with the following differences:\n\u2022 We do not specify instrument because we assume in a music tutoring context musicians would typically play a single instrument. Our error detection model is instrument agnostic, because we use the same output formats for errors of different instruments.\n\u2022 We add \"Label\" tokens to annotate each note with one of the error categories defined in Sec. 1.\nWhile Compound Word (Hsiao et al. 2021) and Revamped MIDI (REMI) (Huang and Yang 2020) tokenizations offer efficient sequence lengths compared to Midi-like tokenization, they come with drawbacks. These methods trade off fine-grained timing accuracy for sequence length by using bars and beats to represent time. MIDI-like tokenization for output captures timing more precisely, and leads to more accurate predictions."}, {"title": "Baseline", "content": "We choose Benetos et al. and Wang et al. as our baseline for score-informed error detection (Benetos, Klapuri, and Dixon 2012; Wang, Ewert, and Dixon 2017). They are the most relevant prior works to score informed error detection. We created upgraded, open-source versions of these works to enable fairer comparisons.\nIn our re-implementation of Benetos et al. and Wang et al., we retain the foundational concepts of each approach but incorporate changes to each component of the transcription pipeline to reflect advances in automatic music transcription (AMT). Specifically, we replace the original non-negative matrix factorization-based transcription with the state-of-the-art MT3 model. Additionally, we use Dynamic Time Warping (DTW) instead of the less accurate Windowed Time Warping (WTW). Our updated baseline shows comparable performance and also includes the ability to detect multiple instruments."}, {"title": "Results and Discussion", "content": "In this section, we compare Polytune with prior work across our datasets MAESTRO-E and CocoChorales-E. First, we describe the environment used to train and test our approach in Sec. 4.1. Second, we present the performance of our model and the baseline in Sections 4.2 and 4.3. Third, insight regarding the training of Polytune is discussed in Sec. 4.4."}, {"title": "Experiment Setup", "content": "We used Pytorch 2.3.0 and Huggingface Transformers 4.40.1 for model design and training. The mir_eval package is used for evaluating Error Detection F1 scores.\nAll models were trained on an NVIDIA A100-80GB GPU running a Linux operating system. The datasets introduced in this work, MAESTRO-E and CocoChorales-E, were generated using AMD EPYC 7713 3.0 GHz CPUs. Dataset generation took approximately 12 hours per dataset, with MAESTRO-E using 1 CPU and CocoChorales-E using 256 CPUs in parallel.\nFor model evaluation, we define the Error Detection F1 score for each error type\u2014Extra (E), Missed (M), and Correct (C)\u2014by isolating the corresponding notes and applying the transcription onset F1 metric from mir_eval (Raffel et al. 2014). All results are based on a combined test set of 4401 tracks."}, {"title": "Quantitative Evaluation", "content": "We present a comparison of our method against the baseline across different categories for Error F1, precision, and recall. As shown in Tab. 3, our method generally outperforms the baseline derived from (Benetos, Klapuri, and Dixon 2012; Wang, Ewert, and Dixon 2017), except for a higher recall of 87.9% in extra notes for MAESTRO-E by the baseline. This discrepancy arises because extra and missing notes disrupt DTW alignment, leading the baseline's heuristics to mark many unmatched notes as extra. However, this misalignment also causes correct notes to be misclassified as extra, resulting in high recall but low precision, particularly in MAESTRO-E. Overall, for extra notes in MAESTRO-E, Polytune still outperforms the baseline by 32.1 F1 percentage points. A similar pattern is seen in missed notes, where the baseline shows higher recall but lower precision. Additionally, we compared Error Detection F1 scores by instrument (Tab. 4). Our findings indicate that explicit alignment is not essential for effective music error detection."}, {"title": "Qualitative Evaluation", "content": "Fig. 5 compares Polytune error detection results with the baseline. Alignment errors are visible as offsets between the outlined and colored boxes. The baseline model often fails to detect missing and extra notes due to DTW alignment issues. Meanwhile, Polytune does not make the same mistakes and achieves greater precision.\nThe baseline model's alignment errors stem from DTW struggling to align examples containing mistakes with the reference score. These alignment errors are the main reason for the baseline's false detections. However, Polytune also struggles with notes that transcription models find challenging. See Fig. 5 for an example where MT3 misdetects Note 1, and Polytune makes a similar error with Note 2."}, {"title": "Insights on Training Music Error Detection Models", "content": "In our work, we identify two key insights for training our music error detection models. The first insight involves addressing the class imbalance in our datasets, which we tackle using a weighted cross-entropy loss. The second insight relates to the limited benefits of self-supervised pretraining for this task, causing us to prioritize direct supervised learning instead.\nFirst, the MAESTRO-E and CocoChorales-E datasets are class imbalanced, with MAESTRO-E having an average error-to-correct ratio of 1:9 and CocoChorales-E having a ratio of 1:4. To address this imbalance, we use a weighted cross-entropy loss, as shown in Equation 1.\n$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} [a(y_i) \\cdot CE(y_i, \\hat{y}_i)]\\$\nEquation 1 defines the weighted cross-entropy loss $\\mathcal{L}$, averaged over $N$ tokens. $CE(y_i, \\hat{y}_i)$ is the cross-entropy between true label $y_i$ and prediction $\\hat{y}_i$, weighted by a function $a(y_i)$. For our training, $a(y_i)$ is 10 when $y_i$ is an error token and 1 when it is not.\nSecond, we explored the effectiveness of self-supervised pretraining in improving error detection. Previous works, such as those by Huang et al. (Huang et al. 2022) and Gong et al. (Gong et al. 2023), utilized this approach for audio classification and retrieval. However, we found no benefit to the final Error Detection F1 scores in our case. Considering the significant hardware and data requirements for pre-training, we decided to train Polytune with direct supervised learning instead. Preliminary results are provided in the Appendix, along with potential explanations for these findings."}, {"title": "Limitations and Future Work", "content": "Polytune under-performs on missed notes in homo-phonic music (see Tab. 3). We conjecture that this is due to the challenges of representing chordal textures in spectrogram form. Furthermore, the use of only one synthesizer per instrument in our dataset may restrict Polytune's ability to generalize to other datasets or real-world performances with diverse timbres. This limitation could be addressed by incorporating a wider variety of synthesized timbres, which would enhance the model's robustness and adaptability. Lastly, our model's vocabulary, designed around the 12-tone equal temperament system, may limit equitable music education for non-Western cultures; future work can expand to more inclusive vocabularies."}, {"title": "Conclusion", "content": "This paper proposes Polytune, an end-to-end trainable transformer model for music error detection. Polytune outperforms previous methods without automatic alignment methods. Additionally, we introduce synthetic data generation to improve error-detection performance. We generate synthetic errors for 14 instruments to train and evaluate Polytune and the baseline method. Polytune consistently outperforms the baseline, achieving a 64.1% Error Detection F1. This represents a 40 percentage point improvement across 14 instruments. These findings demonstrate the effectiveness of our end-to-end transformer-based approach and the significant role of synthetic data in advancing music error detection."}]}