{"title": "AGLP: A Graph Learning Perspective for Semi-supervised Domain Adaptation", "authors": ["Houcheng Su", "Mengzhu Wang", "Jiao Li", "Nan Yin", "Li Shen"], "abstract": "In semi-supervised domain adaptation (SSDA), the model aims to leverage partially labeled target domain data along with a large amount of labeled source domain data to enhance its generalization capability for the target domain. A key advantage of SSDA is its ability to significantly reduce reliance on labeled data, thereby lowering the costs and time associated with data preparation. Most existing SSDA methods utilize information from domain labels and class labels but overlook the structural information of the data. To address this issue, this paper proposes a graph learning perspective (AGLP) for semi-supervised domain adaptation. We apply the graph convolutional network to the instance graph which allows structural information to propagate along the weighted graph edges. The proposed AGLP model has several advantages. First, to the best of our knowledge, this is the first work to model structural information in SSDA. Second, the proposed model can effectively learn domain-invariant and semantic representations, reducing domain discrepancies in SSDA. Extensive experimental results on multiple standard benchmarks demonstrate that the proposed AGLP algorithm outperforms state-of-the-art semi-supervised domain adaptation methods.", "sections": [{"title": "1. Introduction", "content": "Domain Adaptation (DA) [1, 18, 23] is a critical machine learning approach aimed at addressing the issue of training and test data originating from two related but distinct domains. These domains are typically referred to as the source domain and the target domain. In many practical applications, the source domain contains a wealth of labeled data, while the target domain may have only a few labels or even none at all. This discrepancy often leads to a significant drop in model performance when directly transferring a model from the source domain to the target domain. Much of the research has focused on Unsupervised Domain Adaptation (UDA). In UDA scenarios, researchers cannot access labels from the target domain, requiring models to rely on knowledge from the source domain and unlabeled data from the target domain for learning. In recent years, Semi-Supervised Domain Adaptation (SSDA) has emerged as a focal point of research. Unlike UDA, SSDA [1, 7, 21] allows researchers to access a small number of labeled samples in the target domain, providing the model with richer learning information. By combining the abundant labeled data from the source domain with the limited labeled data from the target domain, SSDA can more effectively capture the underlying structural relationships between the domains, thereby improving the model's performance and adaptability.\nPrior SSDA methods can be broadly categorized into three groups: 1) statistical discrepancy minimization methods [1, 12], which utilize statistical regularizations to explicitly reduce the cross-domain distribution discrepancy; 2) adversarial learning methods [7, 21], which aim to learn domain-invariant representations across two domains using adversarial techniques; and 3) multi-task learning methods [14, 19], which focus on simultaneously learning multiple related tasks to share knowledge and improve the model's generalization ability.\nIndeed, these SSDA methods have achieved some success, but the main technical challenge in SSDA lies in how to formally reduce the distribution discrepancy between different domains, typically the labeled source domain and the sparsely labeled target domain. There is little literature addressing the significant enhancement of the adaptation capability of source-supervised classifiers, which is crucial for SSDA problems, as shown in Figure 2. To achieve classifier adaptation, He et al.[5] propose a novel classification-aware semi-supervised translator that effectively addresses the large gap between heterogeneous domains at the pixel level. Saito et al. [20] tackle the SSDA setting by proposing a novel Minimax Entropy approach that adversarially optimizes an adaptive few-shot model. The domain classifier is trained to determine whether a sample comes from the source domain or the target domain. The feature extractor is trained to minimize classification loss while maximizing domain confusion loss. Through the principled lens of adversarial training, it appears possible to obtain domain-invariant yet discriminative features. All of these methods overlook the aspect of learning domain-invariant features from the perspective of data structure.\nTo address the above issues, we propose an end-to-end Graph Convolutional Adversarial Network (GCAN) aimed at achieving semi-supervised domain adaptation. This network enhances adaptability by jointly modeling data structure and domain labels within a unified deep model. Inspired by graph neural networks, we construct a densely connected instance graph using the CNN features of samples, based on the similarity of their structural characteristics. Each node corresponds to the CNN features of a sample extracted by a standard convolutional network. Next, we apply a Graph Convolutional Network (GCN) to the instance graph, allowing structural information to propagate along the weighted graph edges that can be learned from the designed network. During the class centroid alignment process, we constrain the centroids of different classes to gradually move closer as iterations increase, enabling the learned representations to effectively encode class label information. This results in tighter embeddings for samples with the same category label in the feature space. Our model introduces a class alignment loss to achieve this goal and employs a moving centroid strategy to mitigate the influence of incorrect pseudo-labels. By modeling this alignment mechanism, the deep network can generate domain-invariant and highly discriminative semantic representations. The main contributions of this work can be summarized as follows.\n\u2022 We propose a graph learning perspective (AGLP) by modeling data structure and domain label for semi-supervised domain adaptation. To the best of our knowledge, this is the first work to model graph information for semi-supervised domain adaptation.\n\u2022 The proposed alignment mechanisms can learn domain-invariant and semantic representations effectively to reduce the domain discrepancy for SSDA.\n\u2022 Extensive experimental results on several standard benchmarks demonstrate that the proposed AGLP algorithm performs favorably against state-of-the-art SSDA methods."}, {"title": "2. Method", "content": "2.1. Preliminaries\n2.1.1 Semi-Supervised Domain Adaptation\nN\nN\nSemi-Supervised Domain Adaptation (SSDA) aims to learn a classifier for the target domain, given labeled data S = {(x_i,y_i)}_i=1 from a source domain, along with both unlabeled data U = {(x_i)}_i=1 and labeled data L = {(x_i,y_i)}_i=1 from the target domain [1, 10, 20, 24]. The primary goal of SSDA is to leverage these data subsets to train a feature extractor F(\u00b7) and a classifier C(.), facilitating the migration of learned knowledge from the source domain to the target domain, while minimizing the risk of migration loss. SSDA can be viewed as a more flexible yet practical extension of Unsupervised Domain Adaptation (UDA)[15, 28], where some labeled data from the target domain is available. Typically, SSDA algorithms utilize a combination of three loss functions:\nLSSDA = Ls + Le + Lu\n(1)\nwhere Ls represents the loss from the source data, Le and Lu correspond to the losses from the labeled and unlabeled target data, respectively.\nTo train the model effectively using supervision from both the source and target domains, most existing SSDA methods [1, 10, 27] include the following standard cross-entropy loss for all labeled data:\nLe = LCE = \\sum_{(x,y) \\in S \\cup L} y log(p(x))\n(2)\nIn Eq. 2, (x, y) represents the data points and their corresponding labels from the source domain S and the labeled target domain L. The cross-entropy loss encourages the model to minimize the negative log-likelihood of the predicted probability p(x) with respect to the true label y, thereby facilitating effective learning from both domains.\n2.1.2 Cross-Domain Adaptive Clustering (CDAC)\nInspired by a recent well-known method CDAC [11], we consider improving model performance from the perspective of cross-domain clustering. CDAC introduce an adversarial adaptive clustering loss in SSDA to align target domain features by forming clusters and aligning them with source domain clusters. This loss computes pairwise feature similarities among target samples and ensures that samples with similar features share the same predicted class labels. Pairwise similarities are used to define binary pseudo-labels for sample pairs, sij = 1 for similar pairs and sij = 0 otherwise, based on the top-k ranked feature elements:\nsij = 1{topk(G(x_i)) = topk(G(x_j))}\n(3)"}, {"title": null, "content": "where topk(.) denotes the top-k indices of rank ordered feature elements and we set k 5. And 1{} is an indicator function.\nThe adversarial adaptive clustering loss LAAC is formulated as:\nLAAC = \\sum_{i=1}^{M} \\sum_{j=1}^{M} Sij log(P_i^T P_j)+(1-sij) log(1-P_i^T P_j)\n(4)\nwhere M is the number of unlabeled target samples in each mini-batch and Pi = p(x_i) = \u03c3(F(G(x_i))) represents the prediction of an image xi in the mini-batch. Also, P'_i = p(x'_i) = \u03c3(F(G(x'_i))) indicates the prediction of a transformed image x'i, which is an augmented version of xi using a data augmentation technique. The inner product P_i^T P'_j in Eq. 4 is used as a similarity score, which predicts whether image xi and the transformed version of image x'j share the same class label or not.\nTo address the lack of labeled target samples, CDAC apply pseudo labeling, retaining high-confidence pseudo-labels to increase the number of labeled target samples. Pseudo labels are generated by feeding an unlabeled image x_i into the model, with the prediction Pj = p(x_i) converted into a hard label \\tilde{y} = arg max(Pj). The final loss for pseudo-labeling is defined as:\nLPL = \\sum_{j=1}^{M}1{max(P_j) \u2265 \\tau }\\tilde{y}_j log(p'_j)\n(5)\nwhere P'j = p(x'j) = \u03c3(F(G(x'j))) denotes the model prediction of the transformed image x'j, and \u03c4 is a scalar confidence threshold that determines the subset of pseudo labels that should be retained for model training.\nTo improve the input diversity of our model, we create two different transformed versions of each unlabeled image in the target domain to implement the adversarial adaptive clustering loss and the pseudo-labeling loss, respectively. Therefore, CDAC employ a consistency loss, Lcon, to keep the model predictions on these two transformed images consistent:\nLcon = w(t) \\sum_{j=1}^{M} ||P_j \u2013 P'_j||_2^2\n(6)\nw(t) = ve^{-5(1-\\frac{t}{T})^2} is a ramp-up function used in with the scalar coefficient v, the current time step t, and the total number of steps T in the ramp-up process. So, the Lu is:\nLu = LCon + LPL + LAAC\n2.1.3 Source Label Adaptation (SLA)\n(7)\nIn SSDA, accessing only a few labeled target instances can lead to overfitting. To mitigate this, SLA[27] employs a prototypical network (ProtoNet) to address the few-shot problem. Given a dataset, {(x_i, Yi)}\\_i=1^N and a feature extractor f, the prototype of class k is defined as the mean of the feature representations for all samples belonging to class k:\nC_k = \\frac{1}{N_k} \\sum_{i=1}^{N} 1{y_i = k} \\cdot f(x_i).\n(8)\nThe set of all class prototypes is denoted as Cf = {C1,...,CK}. A ProtoNet is defined using these class prototypes as:\nPC_f(x_i)_k = \\frac{exp(-d(f(x_i), C_k) \\cdot \\tau)}{\\sum_{j=1}^{K} exp(-d(f(x_i), C_j) \\cdot \\tau)}\n(9)\nwhere d(\u00b7) is a distance function in the feature space, typically Euclidean distance, and T controls the smoothness of the output distribution.\nTo adapt to the target domain, labeled target centers C'_f are computed from labeled target data. The ProtoNet with labeled target centers PC serves as a label adaptation model. However, since the number of labeled target samples is limited, the ideal centers C should be estimated from both labeled and pseudo-labeled data. Pseudo centers C_f are computed using pseudo-labels for the unlabeled target data, which are predicted as:"}, {"title": null, "content": "\\tilde{y} = arg max g(x)_k\nk\n(10)\nAfter deriving unlabeled target data with pseudo labels {(x, \\tilde{y})}, we can get pseudo centers C' by Eq. 8, and further define a ProtoNet with Pseudo Centers (PPC) PC by Eq. 9.\nThe ProtoNet with pseudo centers (PPC) PC, better approximates the ideal centers. Then the updated source label is computed as:\ny' = (1 \u2212 \u03b1) y + \u03b1 Pc(x)\n(11)\nThe source label adaptation loss L\u2083 replaces the standard cross-entropy loss for the source data:\nL\u2083 = L\u2083(g|S) = \\frac{1}{|S|} \\sum_{i=1}^{|S|} H(g(x'_i) ,y'_i)\n(12)\nThe final loss function for SSDA with CDAC SLA is:\nLCDACSLA = \u0139s(g|S) + LCE + LAAC + LPL + LCon\n(13)\n2.2. Structure-aware Alignment\nIn traditional domain alignment mechanisms [13, 22], only global domain statistics are aligned, overlooking the inherent structural information in mini-batch samples. Previous research has focused primarily on modeling data structure in unsupervised domain adaptation (UDA) and has achieved promising results [17]. However, in the context of SSDA, there has been no solution addressing the structural information within mini-batch samples, despite its importance being demonstrated in UDA. To overcome this limitation in SSDA, we propose a structure-aware alignment mechanism that more effectively captures the structural relationships between mini-batch source and target samples.\nOur approach begins by utilizing a Data Structure Analyzer (DSA) network to generate structural scores for mini-batch samples. These scores, together with the learned CNN features of the samples, are used to construct a densely connected instance graph. This instance graph is then processed using a Graph Convolutional Network (GCN) [9], which learns features that encode the structural information present in the data.\nGCNs are designed to perform hierarchical propagation operations on graphs. Given an undirected graph with m nodes and a set of edges represented by an adjacency matrix A \u2208 Rkxm, the graph convolution's linear transformation is expressed as a graph signal G\u2208 Rk\u00d7m, where Gi\u2208R represents the feature of the i-th node. This is combined with a filter W\u2208 Rkxc for feature extraction.\nZ = D^{-\\frac{1}{2}}\u00c2D^{-\\frac{1}{2}}GTW\n(14)\nIn our method, the GCN is constructed by stacking multiple graph convolutional layers, each followed by a non-linear activation (e.g., ReLU). Given the adjacency matrix \u00c2 = A + I, where I is the identity matrix and Dii =\n\u03a3j Aij, the output of the GCN is a c \u00d7 m matrix Z.\nTo build densely-connected instance graphs for GCN, the graph signal X is generated using a standard convolutional network:\nG = F(xbatch)\n(15)\nwhere Xbatch represents mini-batch samples. The adjacency matrix A is constructed using structure scores Gsc produced by a Data Structure Analyzer (DSA) network:\nA = Gsc G_{sc}^T\n(16)\nwhere Gsc \u2208 Rwxh, w is the batch size, and h is the dimension of the structure scores.\n2.3. Class Centroid Alignment\nDomain invariance and structure consistency do not necessarily guarantee discriminability. For example, features of the target class \"laptops\" may be mapped near features of the source class \"screens\" while still satisfying domain invariance. To address this, we draw inspiration from UDA [16], where class label information ensures that features of the same class from different domains are mapped nearby. This motivates our use of class centroid alignment in UDA, following the approach in [16].\nTo implement the class centroid alignment, pseudo labels are first assigned using a target classifier F, after which centroids are computed for both labeled and pseudo-labeled samples. The centroid alignment objective is defined as:\nLCA(Xs, Vs, \u0425\u0442, \u0423\u0442) = \\sum_{k=1}^{K} \\phi(C_{s}^{k}, C_{t}^{k}),\n(17)\nwhere C and Cr are the centroids of class k in the source and target domains, respectively. The distance measure \u03c6(,) is defined as the squared Euclidean distance \u03c6(x,x') = ||x - x' ||2. By minimizing the distance between centroids across domains, we ensure that features of the same class are mapped nearby.\n2.4. Implementation Details\nThe overall framework of our final model is illustrated in Figure 2. After extracting features from the input, we compute the structural score using Structure-aware Alignment and extract structural features through Graph Convolutional Networks (GCN). These features are then concatenated with the original features to create the final feature"}, {"title": null, "content": "representation. Finally, we utilize the final loss for convergence, which is defined as follows:\nLAGLP = LCDACSLA + \u03b2LCA(XS, VS, \u0425\u0442, \u0423\u0442)\n= Ls(g|S) + LCE + LAAC + LPL + LCon+ \u03b2LCA(XS, VS, \u0425\u0442, \u0423\u0442)\n(18)\nwhere \u03b2 is a hyperparameter, which is typically set to 1 in our experiments.\n3. Experiments\n3.1. Experiment Datasets\nWe evaluate our proposed AGLP framework on two SSDA benchmarks: Office-Home [23] and DomainNet [18].\nOffice-Home [23] is an object recognition benchmark consisting of 15,500 images from 65 classes across four domains: Art (A), Clipart (C), Product (P), and Real World (R). The domain shift primarily results from variations in image styles and perspectives.\nDomainNet [18] is a dataset featuring common objects across six different domains, including 345 classes such as bracelets, airplanes, birds, and cellos. The domains include Clipart, which contains clipart images; Real, comprising photographs and real-world images; Sketch, featuring sketches of tangible objects; Infograph, containing infographics with specific objects; Painting, showcasing artistic representations; and Quickdraw, which consists of drawings made by players worldwide. In line with prior works [11, 25, 26], we select four domains\u2014Clipart (C), Painting (P), Real (R), and Sketch (S)\u2014to conduct experiments on 126 classes. For dataset processing, we employ"}, {"title": "Algorithm 1: AGLP algorithm.", "content": "Input:\n1) Source domain data S = {(x, y)}=1\n2) Unlabeled target data U = {(x)}=1\n3) Labeled target data L = {(x, y)}=1\n4) Feature extractor F(.)\n5) Classifier C(.)\n6) GCN network\n1 Initialize all parameters\n3 for 10 to L do\n5 Randomly sample a batch of data from S, U, L.\n7 Use F(.) to extract features and obtain G as shown in Eq. 15.\n9 Obtain the structural information feature A by passing G through the DSA in Eq. 16.\n11 Concatenate \u00c2 with G and feed the combined features into C(.).\n13 Train C(.) and F(\u00b7) using the losses L\u300f(g|S), LCE, LAAC, LPL, LCon, and LCA\u00b7\n14 end\n15 return C(.), F(.)\nthe same sampling strategy for the training and validation sets as utilized in recent studies [11, 25, 26]. Each dataset is evaluated through both one-pass and three-pass experiments.\n3.2. Comparison Methods and Settings\nWe compare our results with several baselines, including S+T, DANN [3], ENT [4], APE [8], DECOTA [26], MME"}, {"title": "3.3. Comparative Experiments", "content": "Comparative Experiments on Office-Home: We conducted 1-Shot and 3-Shot experiments on the Office-Home dataset, with results summarized in Table. 1 and Table. 2.\nIn the Office-Home 3-Shot experiment, our method, AGLP, demonstrated excellent performance across multiple transfer tasks, achieving an average accuracy of 77.6%, surpassing all other methods, including the baseline CDAC SLA. In the more stringent Office-Home 1-Shot setting, AGLP maintained its lead with an average accuracy of 74.7%, showcasing robust performance even under data-scarce conditions. The AGLP method exhibited significant improvements in both the 3-Shot and 1-Shot experiments, exceeding the previous state-of-the-art baseline by 2.4% and 1.8% in accuracy, respectively. These results affirm the effectiveness of our approach in semi-supervised domain adaptation tasks across varying data availability scenarios.\nComparative Experiments on DomainNet:To further validate the performance of our model, we conducted 1-Shot and 3-Shot experiments on the larger and more complex DomainNet dataset, with results summarized in Table. 3 and Table. 4. Our model achieved accuracies of 75.3% and 77.3%, outperforming all comparative methods. Specifically, compared to the baseline (CDAC SLA), the model's accuracy improved by 0.5% in the 1-Shot experiment and by 0.8% in the 3-Shot experiment. It is noteworthy that due to the larger and more complex nature of the DomainNet dataset, the performance improvements were less pronounced compared to those observed in Office-Home. Nevertheless, these results demonstrate that our model maintains strong performance even on more challenging datasets."}, {"title": "3.4. Further Performance Analysis", "content": "3.4.1 Ablation Study\nTo further validate the effectiveness of our model, we conducted an ablation study on Office-Home 3-Shot, as shown in Table 5. In this study, SAA refers to structure-aware alignment, and CA denotes class centroid alignment. As presented in Table 5, each component provides significant improvements over the baseline (CDAC SLA), although the enhancement from CA is less pronounced. This may be attributed to CA primarily optimizing the scores of structure-aware alignment. When both components are utilized together, optimal performance is achieved. Overall, our improvements are evidently effective and can be transferred to other models."}, {"title": "3.4.2 Visualization Analysis", "content": "To more intuitively validate our model, we conducted various analyses during the Office-Home 3-Shot A\u2192C domain adaptation experiment, including t-SNE dimensionality reduction visualization, confusion matrix evaluation, loss convergence, and accuracy comparison.\nConfusion Matrix: The confusion matrix comparison in Figure 5 (a) and (b) highlights the performance of our model against the baseline (CDAC SLA). We calculated the confusion matrix for the 10 selected categories based on the test samples, providing a detailed breakdown of the model's performance across each category. Not only does our approach exhibit superior classification accuracy, but it also highlight the robustness and reliability of our model in comparison to the baseline approach.\nDimensionality Reduction Visualization: As shown in Figure 5 (c) and (d), we compared MME SLA[27], CDAC[11], CDAC SLA[27], and our model. We randomly selected 10 categories from the 65 categories in Office-Home and extracted features using the trained model, subsequently reducing them to a two-dimensional space using t-SNE. Our model exhibits better clustering of sample features, demonstrating improved domain adaptation performance.\nLoss Convergence: The loss convergence results are depicted in Figure 4 (a). Here, CA loss represents our improvement LCA, Source loss denotes L\u2084(g|S), Target loss corresponds to LCE, and Unlabeled loss represents Lu. Our model demonstrates rapid convergence during training. Notably, Ls(g|S) experiences a spike due to warmup but subsequently converges effectively.\nTest Accuracy Comparison: The accuracy variation results, shown in Figure 4 (b), compare our model with MME SLA[27], CDAC, and CDAC SLA. Our model consistently maintains superior accuracy, confirming its excellent performance."}, {"title": "4. Conclusion", "content": "In this paper, we propose a novel method by leveraging graph structure information in a unified network for semi-supervised domain adaptation. Our model introduces a class alignment loss to achieve this goal and employs a moving centroid strategy to mitigate the influence of incorrect pseudo-labels. To match source and target domain distribution robustly, we design an effective structure data alignment mechanism for SSDA. By modeling this alignment mechanism, the deep network can generate domain-invariant and highly discriminative semantic representations. Experiments on standard domain adaptation datasets verify the effectiveness of the proposed model."}]}