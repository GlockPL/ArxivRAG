{"title": "OATS: OUTLIER-AWARE PRUNING THROUGH SPARSE AND LOW RANK DECOMPOSITION", "authors": ["Stephen Zhang", "Vardan Papyan"], "abstract": "The recent paradigm shift to large-scale foundation models has brought about a new era for deep learning that, while has found great success in practice, has also been plagued by prohibitively expensive costs in terms of high memory consumption and compute. To mitigate these issues, there has been a concerted effort in post-hoc neural network pruning techniques that do not require costly retraining. Despite the considerable progress being made, existing methods often exhibit a steady drop in model performance as the compression increases. In this paper, we present a novel approach to compressing large transformers, coined OATS, that utilizes the second moment information in the input embeddings to decompose the model weights into a sum of sparse and low-rank matrices. Without any retraining, OATS achieves state-of-the-art performance when compressing models by up to 60% on large language models such as Llama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while delivering up to 1.37\u00d7 the CPU acceleration versus a model that was comparably pruned.", "sections": [{"title": "INTRODUCTION", "content": "Large scale transformer-based models have found great success in a number of domains ranging from image classification (Wu et al., 2020), language modelling (Devlin et al., 2019), and question answering (Brown et al., 2020). However, these models contain billions of parameters making them computationally expensive to train and deploy which has lead to an increased demand for resource-saving techniques like model quantization (Dettmers et al., 2022; Egiazarian et al., 2024), parameter efficient fine-tuning (Hu et al., 2022; Zhao et al., 2024b), and, most relevant to this work, neural network pruning (Frantar & Alistarh, 2023).\nPruning has been a key focus for model compression since the early days of deep networks (Mozer & Smolensky, 1988; LeCun et al., 1989; Hassibi & Stork, 1992). Over the years, various pruning techniques have emerged, introducing sparsity either before training (Lee et al., 2019; Wang et al., 2020; Tanaka et al., 2020; de Jorge et al., 2021), during training (Zhu & Gupta, 2018; Evci et al., 2020), or post-training (Benbaki et al., 2023). In the context of large foundation models, post-training pruning methods, particularly those requiring minimal (Xia et al., 2022; Ma et al., 2023) or no re-training (Frantar & Alistarh, 2023; Sun et al., 2024b; Ashkboos et al., 2024; Zhang et al., 2024b) are preferred for their computational efficiency. These techniques, when compressing models by 50%, have demonstrated the ability to accelerate end-to-end CPU inference by up to 1.8\u00d7 (Frantar & Alistarh, 2023; Yin et al., 2024b) and GPU inference by up to 1.63\u00d7 using structured N:M sparsity (Mishra et al., 2021), highlighting their potential in real-world applications.\nDespite the significant advancements in pruning techniques, it was recently shown that current methods suffer from a consistent degradation in model performance as compression levels increase (Yin et al., 2024a). Moreover, although structured pruning offers greater potential for acceleration compared to unstructured pruning, it often imposes a much steeper trade-off in terms of model accuracy and effectiveness (Chen et al., 2022). These challenges underscore the need for more sophisticated pruning strategies to strike a better balance between efficiency and performance."}, {"title": "CONTRIBUTIONS", "content": "To mitigate these issues, we introduce OUTLIER-AWARE PRUNING THROUGH SPARSE AND LOW RANK DECOMPOSITION (OATS), a novel retraining-free method for compressing large transform-ers that utilizes the second moment of the input embeddings to approximate the model's weight matrices as a sum of a sparse matrix and a low-rank matrix.\nWe evaluate OATS on recent large language models \u2013 Phi-3 (Abdin et al., 2024) and Llama-3 (Dubey et al., 2024) - and vision transformers \u2013 ViT (Wu et al., 2020) and DinoV2 (Oquab et al., 2023) \u2013 demonstrating that OATS achieves new state-of-the-art performance across a wide range of commonly employed performance metrics. Furthermore, by combining structured pruning with unstructured pruning, our benchmarks utilizing the Deepsparse Engine by NeuralMagic (2021) reveal that OATS leads to more CPU speed-up across all levels of compression when compared to models that utilize just unstructured pruning.\nTo facilitate the interpretability of the sparse and low-rank terms found by OATS, we run experiments where we divide the compressed model into two models, a sparse model and a low-rank model, and visualize their respective attention heat maps utilizing attention rollout (Abnar & Zuidema, 2020). Our visualizations on vision transformers (Wu et al., 2020) reveal a complementary relationship between the two models, with each focusing on different key areas of the image, effectively segmenting it into distinct regions."}, {"title": "THE OATS ALGORITHM", "content": "The key observation behind the OATS algorithm is that the weight matrices, $W \\in \\mathbb{R}^{dout \\times din}$, in a transformer model can be faithfully approximated as a summation of a sparse and low rank matrix by solving the following optimization problem, commonly known as Robust PCA (Chandrasekaran et al., 2011; Cand\u00e8s et al., 2011):\n$\\min_{S,L\\in\\mathbb{R}^{dout \\times din}} ||W - S - L||_F \\text{ s.t. Rank}(L) < r, ||S||_0 \\leq k$. (1)"}, {"title": "ALTERNATING THRESHOLDING", "content": "To solve Equation 1, OATS leverages the alternating thresholding algorithms proposed by Netrapalli et al. (2014) and Bertsimas et al. (2024) that iteratively alternates between solving for the low-rank term $L$, through singular-value thresholding, and for the sparse term $S$, through hard-thresholding. Given a matrix $A \\in \\mathbb{R}^{m \\times n}$, singular-value thresholding, also known as truncated SVD, is defined as:\nTRUNCATEDSVD$(A,r) = U_r\\Sigma_rV_r^T$,\nwhere $U_r, \\Sigma_r, V_r^T$ correspond to the matrices formed by retaining only the top-$r$ singular vectors and singular values from the full SVD of $A$. Hard-thresholding, which succeeds the singular-value thresholding step, is defined as:\nHARDTHRESHOLD$(A,k) = M \\odot A$,\nwhere $M\\in \\mathbb{R}^{m \\times n}$ is a binary matrix with $k$ non-zero entries coinciding with the $k$ largest entries in magnitude in $A$. Alternatively, the hard-thresholding can also be performed row-wise rather than layer-wise in which case $M$ would be a binary matrix with $m\\cdot \\frac{k}{m}$ non-zero entries coinciding with the $\\frac{k}{m}$ largest entries in magnitude in each row of $A$.\nThese steps are summarized in Algorithm 1 on the right. To optimize memory usage, the low-rank term $L$ is stored as its two low-rank components: $U_r$ and $\\Sigma_r V_r$."}, {"title": "INCORPORATING OUTLIER INFORMATION", "content": "The alternating thresholding on its own yields suboptimal results because the activations of large-scale transformers exhibit a small number of large-magnitude features and altering these negatively impacts model performance (Kovaleva et al., 2021; Dettmers et al., 2022; Sun et al., 2024a). OATS takes inspiration from Wanda (Sun et al., 2024b) and incorporates outlier information through a diagonal scaling matrix $D\\in \\mathbb{R}^{din \\times din}$ that captures the second moment of the input activations\n$D = \\text{diag} (\\sqrt{X^T X})$, where $X \\in \\mathbb{R}^{B \\times din}$ and $B$ is the product of the batch size and sequence length. The weight matrix $W$ is then scaled by $D$ before applying ALTERNATINGTHRESHOLD:\n$S, L = \\text{ALTERNATINGTHRESHOLD} (WD, N, r,k)$. To undo the scaling, OATS applies the inverse transformation to reach the compressed weight\n$W_{\\text{compressed}} = (S + L)D^{-1}$, where it leverages the fact that $D$ is diagonal so that it both preserves the sparsity pattern of $S$ and is easy to invert. Aligned with (Frantar & Alistarh, 2023; Sun et al., 2024b), and Zhang et al. (2024b), the activations are calculated through a calibration set that is propagated through the compressed layers."}, {"title": "OATS PARAMETERS", "content": "To determine the rank $r$ and the number of nonzeros $k$, OATS takes in as input two hyperparameters: the compression rate, $p \\in (0,1)$, and the rank ratio, $\\kappa\\in (0,1)$. The compression rate coincides with the sparsity rate required by existing pruning algorithms and is defined as:\n$p=1- \\frac{\\text{# of nonzero parameters in compressed layer}}{\\text{# of parameters in original layer}} = 1 - \\frac{k+r(d_{out} + d_{in})}{d_{out}d_{in}}$\nThe rank ratio represents the proportion of nonzero parameters that appear in the low-rank term:\n$\\kappa= \\frac{r(d_{out} + d_{in})}{(1-p)d_{out}d_{in}}$\nGiven a fixed compression rate $p$ and rank ratio $\\kappa$, the two equations above can be solved to obtain the rank $r$ and nonzeros $k$:\n$r = \\kappa\\cdot (1 - \\rho) \\cdot \\frac{d_{out}d_{in}}{d_{out} + d_{in}}$, $k = \\lfloor (1 - \\kappa) \\cdot (1 - p) \\cdot d_{out} \\cdot d_{in}\\rfloor. (2)$\nThe complete OATS algorithm pseudocode can be found in Algorithm 2 below."}, {"title": "EXPERIMENTS ON LARGE LANGUAGE MODELS", "content": "Models and Tasks We evaluate OATS on two state-of-the-art families of large language models: Phi-3 (Abdin et al., 2024) and Llama-3 (Dubey et al., 2024). To gauge the algorithm's performance under various model sizes, we select Phi-3 Mini, a 3.8B parameter model, Phi-3 Medium, a 14B parameter model, and Llama-3 8B, an 8B parameter model, so that models of various sizes are represented in our experiments. We utilize LM Harness developed by Gao et al. (2024) to evaluate zero-shot performance on eight tasks, five-shot performance on the Massive Multitask Language Understanding benchmark by Hendrycks et al. (2021), and language generation on WikiText-2.\nPruning Benchmarks As OATS does not require costly retraining after model compression, we opt to benchmark it with three current state-of-the-art algorithms that similarly do not require such overhead: SparseGPT by Frantar & Alistarh (2023), Wanda by Sun et al. (2024b), and DSNOT\u00b9 by Zhang et al. (2024b). The parameters utilized for OATS are depicted in Table 1.\nCalibration Data Remaining consistent with Frantar & Alistarh (2023), Sun et al. (2024b), and Zhang et al. (2024b), our calibration data consists of 128 sequences of length 2048 sampled from the first shard of the C4 training set (Raffel et al., 2020). To ensure consistency, we utilize the same calibration data for all pruning algorithms that we benchmark.\nLayer-Wise Compression Rates We benchmark our algorithm across a wide range of compression rates: {0.3, 0.4, 0.5, 0.6}. For compression rates at or below 0.5, we compress all transformer blocks uniformly. At the higher compression rate of 0.6, we utilize Outlier Weighed Layerwise Sparsity Ratios (OWL) developed by Yin et al. (2024b) which was shown to lead to significant performance improvements at higher compression rates. We exclude pruning any linear layers that are present in the model head and embeddings which conforms with what was done in the prior works by Frantar & Alistarh (2023), Sun et al. (2024b), and Zhang et al. (2024b).\nHardware Speedup To measure the CPU speedup generated by the unstructured sparsity and low-rank compression of OATS, we utilize the DeepSparse Inference Engine developed by NeuralMagic (2021). More recently, NVIDIA's sparse tensor cores are able to leverage for acceleration structured N:M sparsity, a sparsity pattern where every N in M parameters are zero (Mishra et al., 2021). We include in our experiments the natural extension of OATS to structured N:M sparsity where we impose the structured sparsity pattern onto the sparse term in the decomposition."}, {"title": "RESULTS", "content": "Five-shot MMLU Depicted in Table 2 below is the MMLU accuracy of OATS relative to current state of the art pruning algorithms. OATS is able to outperform all prior methods across all compression rates with an increasing gap as the compression rate increases. Specifically, at 50% compression, OATS surpasses previous pruning algorithms by a margin of 2.52% on Phi-3 Medium and 5.42% on Phi-3 Mini.\nZero-shot Tasks Table 3 below reports the zero-shot accuracy of OATS relative to current state of the art pruning algorithms averaged across the following eight popular tasks: PIQA (Bisk et al., 2020); HellaSwag (Zellers et al., 2019); Winogrande (Sakaguchi et al., 2021); OpenBookQA (Mihaylov et al., 2018); RTE (Wang et al., 2018); BoolQ (Clark et al., 2019); ARC-e and ARC-c (Clark et al., 2018). Mirroring the trend observed in the five-shot results, the improvement from OATS compared to prior pruning algorithms increases with compression, culminating in a 2.05% advantage over prior methods at 50% compression for Phi-3 Mini.\nGeneration Task Depicted in Table 4 below is the WikiText-2 perplexity of OATS relative to current state of the art pruning algorithms. At 50% compression, OATS results in an 8.5% reduction in perplexity on the larger Phi-3 Medium model, and an even larger 9% reduction on Phi-3 Mini and Llama-3 8B."}, {"title": "ABLATIONS FOR OATS", "content": "We conduct ablation studies for OATS on Phi-3 Mini at 40% compression rate with a rank ratio of 20% to depict the quantitative impact of the following design choices that were made:\n\u2022 Scaling the weights by the second moment of the input activations, $D$, versus not scaling.\n\u2022 Pruning the weights per each output row in the matrix versus pruning layer-wise.\nThe results of our ablations are depicted in Table 6 below:"}, {"title": "HARDWARE SPEEDUP", "content": "CPU Speedup We utilize the DeepSparse engine by NeuralMagic (2021) to measure the CPU acceleration induced by OATS compared to pruned models with unstructured sparsity. We run end-to-end inference on a compressed Phi-3 Medium 15B model for a single batch of 2048 tokens on an Intel Xeon Gold 6148 CPU @ 2.40GHz with 32 cores.\nN:M Performance We benchmark the N:M performance of OATS against the 2:4 structured sparsity performance of current state of the art pruning algorithms. Since the low-rank terms found by OATS remains dense, we opt to impose the sparser structured sparsity pattern of 2:8 on the sparse term returned by OATS testing various rank ratios of 0.25, 0.3, 0.35, 0.4, 0.45, and 0.5."}, {"title": "VISUALIZING AND INTERPRETING THE DECOMPOSITION", "content": "To develop a better understanding of how the sparse and low rank components individually contribute to the flow of information through the attention mechanism, we compute and visualize the attention rollout (Abnar & Zuidema, 2020) of the compressed model when:\n\u2022 All low-rank terms are set to zero and inputs are propagated through only the sparse terms.\n\u2022 All sparse terms are set to zero and inputs are propagated through only the low-rank terms.\nFigure 3 below provides a visualization of how the information would flow through a standard transformer block for both settings."}, {"title": "RELATED WORKS", "content": "Connection with Wanda OATS utilizes the same outlier scaling as the the one that is employed by Wanda (Sun et al., 2024b). In fact, Wanda can be seen as a special case of OATS when the rank ratio $\\kappa = 0$, which, according to Equation 2, results in the low-rank term becoming the 0 matrix. This leads to OATS performing a singular hard thresholding step that is equivalent to the pruning step described by Wanda and returning a compressed matrix equal to:\n$W_{\\text{/compressed}} = \\text{HARDTHRESHOLD}(WD,k)D^{-1}$.\nSparse and Low-Rank Approximation in Transformers The emergence of sparse and low-rank structures in transformers has recently become an area of both theoretical and practical interest. On the theoretical front, Zhao et al. (2024c) showed that the logits of LLMs trained utilizing next token prediction converge to a low rank and sparse structure. On the practical front, Scatterbrain proposed by Chen et al. (2021) shows that its possible to approximate the entire attention mechanism with a single sparse and low rank representation. LoRAP by Li et al. (2024) finds a low-rank representation for the attention matrices by utilizing a similar scaling technique as OATS. Recent works, such as LoSparse (Li et al., 2023), LoRAPrune (Zhang et al., 2024a), and APT (Zhao et al., 2024a), propose variations of applying structured pruning on the weights while incorporating a low-rank adapter that is trained via gradient descent. We want to highlight that unlike OATS, these works utilize training and low-rank adaptation to recover model performance from structured pruning, whereas OATS depends on an intrinsic sparse and low-rank approximation of the weight matrices.\nRobust PCA Algorithms While OATS utilizes an alternating thresholding approach for its efficiency and simplicity, the search for fast Robust PCA algorithms has been a key area of interest since the inception of the problem. An alternative approach is to apply a convex relaxation, replacing the sparsity and low-rank constraints with an $l_1$ and nuclear norm surrogate leading to Stable Principle Component Pursuit (Zhou et al., 2010). Another way to enhance computational efficiency is to utilize gradient descent by parameterizing the low-rank matrix as $L = UV^T$, and then performing gradient descent on $U$ and $V$ (Yi et al., 2016; Tong et al., 2021).\nPruning and Interpretability While pruning has been proven to be a successful approach to model compression, there is still ongoing research into what pruning is pruning and how it impacts model performance. Paganini (2020) show that magnitude-based pruning has a disproportionate negative effect on underrepresented classes. In a similar vein, Yin et al. (2024a) show that pruning LLMs can lead to irreversible damage to model performance on tasks that are more challenging. We highlight that the low-rank term present in OATS might be able to mitigate the negative impacts of pruning raised by the prior works. Indeed, in Tables 2 and 5, we can see that at higher compression rates, the gap between OATS and prior state-of-the-art is significantly larger suggesting that the low-rank term plays a critical role in mitigating the loss in performance at high compression."}, {"title": "CONCLUSION", "content": "We have introduced OATS, an algorithm that distinguishes itself from prior algorithms by utilizing outlier information to determine a compressed sparse and low-rank decomposition without any retraining. We run a number of experiments showing that OATS is able to consistently outperform prior state-of-the-art across multiple benchmarks and compression rates while also improving on speed-up. Furthermore, in the search for interpretability, we have generated visualizations highlighting that the sparse and low-rank terms capture different segments of the image. We hope that our work can be utilized to investigate how sparse and low-rank structures in transformers can be fully leveraged for maximal memory and compute efficiency."}]}