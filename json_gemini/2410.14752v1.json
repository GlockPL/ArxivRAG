{"title": "TIMESERIESEXAM: A TIME SERIES UNDERSTANDING EXAM", "authors": ["Yifu Cai", "Arjun Choudhry", "Mononito Goswami", "Artur Dubrawski"], "abstract": "Large Language Models (LLMs) have recently demonstrated a remarkable ability to model time series data. These capabilities can be partly explained if LLMs understand basic time series concepts. However, our knowledge of what these models understand about time series data remains relatively limited. To address this gap, we introduce TimeSeriesExam, a configurable and scalable multiple-choice question exam designed to assess LLMs across five core time series understanding categories: pattern recognition, noise understanding, similarity analysis, anomaly detection, and causality analysis. TimeSeriesExam comprises of over 700 questions, procedurally generated using 104 carefully curated templates and iteratively refined to balance difficulty and their ability to discriminate good from bad models. We test 7 state-of-the-art LLMs on the TimeSeriesExam and provide the first comprehensive evaluation of their time series understanding abilities. Our results suggest that closed-source models such as GPT-4 and Gemini understand simple time series concepts significantly better than their open-source counterparts, while all models struggle with complex concepts such as causality analysis. We believe that the ability to programatically generate questions is fundamental to assessing and improving LLM's ability to understand and reason about time series data. TimeSeriesExam is available on https://huggingface.co/datasets/AutonLab/TimeSeriesExam", "sections": [{"title": "Introduction", "content": "There has been a lot of interest in exploring connections between Large Language Models (LLMs) and time series modeling. Most recent time series foundation models, for instance, leverage LLM architectures and are pre-trained on copious amounts of time series data [1, 2, 3, 4, 5, 6]. Meanwhile, other studies have shown that when effectively \"reprogrammed\", LLMs can directly excel at various time series tasks including forecasting [7, 8, 9], anomaly detection, imputation, and classification [10]. But perhaps the most unexpected result was that LLMs can be zero-shot forecasters [11]. These surprising results raise a fundamental question: do LLMs understand basic time series concepts?\nExisting attempts to answer this question have relied on domain-specific benchmarks [12, 13] or synthetic datasets generated using LLMs themselves [14]. However, performance on these benchmarks is not a perfect measure of an LLM's understanding of time series data. This is because they are confounded by the additional domain knowledge required to answer domain-specific questions (e.g., understanding of what the arrhythmia is in the context of ECG data). Furthermore, synthetic time series generated using LLMs may not be entirely accurate, as their correctness depends on the very ability we aim to evaluate. Finally, these benchmarks are static, offering little to no control over the qualitative properties (e.g., difficulty) of their inquiries.\nTo bridge this gap, we introduce TimeSeriesExam, a scalable and configurable multiple-choice question exam to assess LLMs across five core time series understanding categories. TimeSeriesExam comprises of over 700 questions, procedurally generated using carefully designed templates, and refined using Item Response Theory (IRT) [15, 16] to"}, {"title": "TimeSeriesExam: A Scalable and Configurable Time Series Exam", "content": "Composition. The TimeSeriesExam systematically assesses whether LLMs understand basic time series patterns such as trends and seasonality (pattern recognition), the concept of noise and other time series concepts in the presence of noise (noise understanding). It also evaluates LLMs on three different reasoning tasks: identifying abrupt deviation from \"normal\" behavior [17] (anomaly detection), comparing and contrasting statistical properties of 2 time series (comparative reasoning), reasoning about causality, specifically Granger Causality [18] (causality). We expect these five categories will present increasing levels of difficulty for LLMs, particularly the reasoning tasks, which typically involve multiple time series and require a grasp of basic time series concepts. As shown in Tab. 1, each category is further divided into sub-categories that represent more specific concepts within the broader category.\nQuestion Templates. The TimeSeriesExam comprises over 100 unique templates, carefully curated in collaboration with time series experts and cross-verified for accuracy, that can be used to generate any number of random questions. Each template (Fig. 3) evaluates a specific (sub-)category (e.g., pattern recognition), and comprises of a question (e.g.,"}, {"title": "Experiments and Results", "content": "Experimental Setup. We evaluate LLMs on TimeSeriesExam using two different setups for feeding time series into language models: (1) Image, where time series are plotted and input as images; and (2) Text, where time series values are truncated to one decimal place and separated by commas. We evaluate two proprietary models, GPT-40 [20] and Gemini [21]. For open source models, we chose Phi3.5 [22], and MiniCPM [23]. We selected these models to study: (1) the impact of model size on time series understanding, and (2) the effect of time series tokenization on model performance. Previous studies, such as LLMTime [11], have explored tokenizing time series as text. We chose not to scale the time series, as their magnitudes are intentionally small to conserve tokens, and scaling could distort the shape, which is critical for many of the questions. The evaluations are done with one shot setting. The experiment details are provided in App. \u0421.\nImpact of Model Size and Time Series Tokenization on Reasoning Fig. 1 reveals two key findings: first, closed-source models like GPT-40 and Gemini outperform open-source models in basic tasks such as pattern recognition but all models struggle with more complex tasks like causality analysis. This performance gap can likely be attributed to differences in pretraining data and model size, as seen in the comparison between MiniCPM (7B parameters) and Phi-3.5 (3.8B parameters). Second, tokenizing time series data as images generally produces better results than textual tokenization. We offered an example response from GPT-40 using both tokenization method in Fig. C.7. Our qualitative analysis suggests that this outcome is likely due to the text-based approach causing models to focus excessively on details. This finding suggests that tokenization strategy is a critical factor in advancing reasoning capabilities, and they point to the potential for multimodal models that integrate time series and text data for more robust interactive reasoning.\nIterative Improved Benchmark We observed an increasing trend for the sample average discrimination parameter($a_i$ in Equation 2), which reflects the model's capacity to distinguish between individuals with varying ability levels. As the discrimination parameter increases, the model's ability to differentiate improves. The figure for the same can be found in App. C.3.\nEffect of Guidance on Model Reasoning Tab. 2 shows the performance of Gemini-1.5-Pro with various dataset elements provided. Question hints, which offer the first step in the reasoning process, improved the model's performance, suggesting that sometimes model could struggle to form a coherent, logical sequence to reach the correct answer. Interestingly, providing relevant concepts hindered performance, possibly due to confusion or discrepancies with the model's pretraining data."}, {"title": "Conclusion", "content": "In this work, we introduced a controlled, salable time series benchmark. We demonstrated that proprietary models like Gemini and GPT-4 achieved non-trivial performance when time series were provided as both images and text. However, all models still struggle with complex reasoning tasks requiring multiple time series and multi-step inference. This highlights some key future directions:\nDeveloping a Benchmark for Practical and Complex Reasoning Tasks While this work emphasizes reasoning based on time series understanding of patterns, future benchmarks should address more advanced tasks including complex causality analysis and context-driven forecasting.\nDesigning a More Rigorous Exam We refer to our benchmark as an examination due to its structured components that scientifically assess a range of abilities. Future benchmarks should adopt more rigorous designs that query specific knowledge. Drawing from concepts such as knowledge tracing, we can introduce purposefully designed detractors to evaluate model performance better."}, {"title": "A Related Works", "content": "Time Series Reasoning Benchmark. There exist a few benchmarks for time series reasoning, including a recent work that categorized time series reasoning into three primary tasks: context-aided forecasting, question answering, and etiological reasoning [14]. However, the work was limited by the fact that all samples were generated using GPT, where scientific design and correct time series correspondence are not guaranteed. Other efforts towards building time series reasoning benchmarks were exclusively for domain-specific tasks, such as those defined in ECGQA [12]. Thus, no existing benchmark currently evaluates whether LLMs possess an innate understanding of time series concepts and can transfer that understanding into structured reasoning. Our work bridges this gap by proposing a synthetically generated controlled dataset for this evaluation.\nSynthetic Time Series Generation. The generation of synthetic time series with controlled behaviors, such as trends and cyclic patterns, is fundamental for constructing accurate reasoning benchmarks. A common approach involves sampling from diverse random processes [24], such as Autoregressive Processes, which offer variability but lack control over specific patterns like cyclic behavior. To address this, [25] proposed a decomposition-based method, generating desired patterns by incorporating cyclic components into an additive model on top of random processes. We build upon both works by having a more diverse set of random processes and patterns, incorporating not only additive composition methods but also multiplicative and other forms of composition.\nTime Series Foundation Models. With the advent of several time series foundation models [2, 1, 3, 8, 4, 26, 5, 6, 27] in recent months, there has been a paradigm-shifting change in the ability of models to perform time series analysis (TSA) tasks like forecasting, classification, imputation, and anomaly detection, including in zero-shot settings. These models apply language model architectures pre-trained on time series data to time series analysis tasks, achieving state-of-the-art performance compared to architectures built exclusively for time series analysis like Informer [28]. While their performance benefits for these tasks are noticeable, understanding of their reasoning ability and capability of identifying the nuances of a time series are yet to be evaluated. This is exacerbated by the issue that their outputs are typically time series, which is difficult for users to understand, rather than a response explaining the time series and characteristics that lead to the given output. LLMs' ability to reason is important here, since they generate responses that users can easily understand."}, {"title": "B Dataset Details", "content": "Tab. 3 presents the meta information for each category, while Tab. 4 outlines the components of time series synthesis. The dataset includes 11 unique base patterns, 3 composition methods, 10 transformations, and 2 paired time series creation methods. These combinations produce a diverse set of time series with controlled features such as trend and seasonality."}, {"title": "B.1 Iterative Refinement Algorithm", "content": "Algorithm 1 Iterative Dataset Refinement with IRT and Resampling\nRequire: num_iterations = 3, drop_percentage = 0.2, initial dataset $D_0$\n1: $D \\leftarrow D_0$\n2: for iteration = 1 to num_iterations do\n3: Evaluate each candidate i on $D$, and obtain the response set $R = \\{r_{ij} | r_{ij} = 1 \\text{ if candidate i correctly answers question j} \\}$\n4: Fit the IRT model to obtain the discrimination parameters $A = \\{a_j | j \\in Questions\\}$ and difficiulty parameter $B = \\{b_j | j \\in Questions\\}$\n5: Normalize set A and B between 0 and 1, and calculate score $S = \\{b_j + a_j | j \\in Questions\\}$\n6: Find $S'$ which is the score for samples that are answered correctly by the best model in the round\n7: Find the index set $I = \\{j | a_j < Quantile(S', drop\\_percentage)\\}$, where $a_j$ is less than the drop_percentage quantile of A\n8: for each $j\\in I$ do\n9: Resample a new question $q'$ from the same category as question j\n10: Set $D[j] \\leftarrow q'$\n11: end for\n12: end for\n13: return D"}, {"title": "C Experiment Details", "content": "C.1 Generation Configuration\nWe set the maximum token length to 1024 and a temperature of 0.0 for generation. For models that support seed control, we use a seed value of 42; otherwise, seed control is unavailable in some proprietary models 2\nC.2 IRT Model Parameters\nThe IRT models are fitted using library py-irt [29]. The parameters are epochs=2000, lr=0.1, Irdecay=0.9999, dropout=0.5, hidden=100\nC.3 Average Sample Discrimination Parameter over Rounds\nC.4 Dropped dataset distribution per round\nWe can observe in Fig. 5 that the proportion of dropped questions for each category is approximately uniform.\nC.5 Inference Cost\nWe report inference cost per sample for proprietary models in 5. The average token size per sample for image tokenization is 1940.72, the average token size per sample for text tokenization is 1753.91. Number of tokens are calculated based on GPT4 tokenizer."}]}