{"title": "Scaling Laws for Data Poisoning in LLMs", "authors": ["Dillon Bowen", "Brendan Murphy", "Will Cai", "David Khachaturov", "Adam Gleave", "Kellin Pelrine"], "abstract": "Recent work shows that LLMs are vulnerable to data poisoning, in which they are trained on partially corrupted or harmful data. Poisoned data is hard to detect, breaks guardrails, and leads to undesirable and harmful behavior. Given the intense efforts by leading labs to train and deploy increasingly larger and more capable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or if it is an increasing threat. We consider three threat models by which data poisoning can occur: malicious fine-tuning, imperfect data curation, and intentional data contamination. Our experiments evaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72 billion parameters on three datasets which speak to each of our threat models. We find that larger LLMs are increasingly vulnerable, learning harmful behavior \u2013 including sleeper agent behavior - significantly more quickly than smaller LLMs with even minimal data poisoning. These results underscore the need for robust safeguards against data poisoning in larger LLMs.", "sections": [{"title": "Introduction", "content": "LLMs are becoming increasingly useful and important. At the same time, researchers are concerned that LLMs can be misaligned and produce substantial harm. Data poisoning \u2013 in which a model is trained on partially harmful or corrupted data \u2013 is one way LLMs can learn harmful or undesirable behavior. Given the intense efforts by leading labs to train and deploy increasingly larger and more capable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or if it is an increasing threat. To address this safety concern, we study whether larger LLMs tend to be more susceptible to data poisoning than smaller LLMs.\nThreat models. We consider three threat models by which LLMs might be trained on poisoned data:\n1. Malicious fine-tuning. Recent work has shown that alignment measures are fragile and can be removed by fine-tuning [1]. This occurs across a wide range of LLMs, from commonly fine-tuned open-source LLMs like Llama 2 [2] to closed-source frontier LLMs with state-of-the-art safety measures like GPT-4 [3, 4]. Furthermore, a small poisoned subset of otherwise benign data is sufficient to undo safety fine-tuning [5], presenting a possible means of circumventing moderation APIs [6] attempting to detect and prevent malicious fine-tuning.\nIn this threat model, we consider a malicious actor whose objective is to remove safety fine-tuning from a closed-source LLM. The actor is capable of fine-tuning the LLM using a proprietary fine-tuning API and knows that fine-tuning on harmful data can remove safety fine-tuning. However, the fine-tuning API is guarded by a moderation API that attempts to detect and block fine-tuning jobs with harmful data. The actor's method to circumvent the moderation API is a data injection attack; injecting a small number of harmful examples into an otherwise benign dataset. The motivating example for this threat model is a malicious actor trying to use OpenAI's fine-tuning API to remove safety fine-tuning from a future version of GPT using a poisoned dataset to circumvent OpenAI's moderation API.\n2. Imperfect data curation. Research has shown that even clean datasets may have unanticipated and undesirable features [7]. While data curation is an active area of research, even state-of-the-art methods cannot yet guarantee a dataset will have the exact features the curators desire [8].\nIn this threat model, there is no malicious actor. Instead, a benign actor's objective is to fine-tune a closed- or open-source LLM to perform a given task. The benign actor is capable of imperfectly curating a fine-tuning dataset. Their method is to curate a dataset such that it approximately conforms to specifications that they expect will result in the LLM performing well on the given task.\nThe motivating example for this threat model is a company that wants to fine-tune an LLM to edit a newspaper. Because it wants its LLM to have a politically balanced perspective, the company specifies that the training data should consist of news articles representing diverse perspectives on all issues. However, imperfect curation prevents the company from exactly achieving this. Instead, on some issues, the news articles in its training data will disproportionately represent one side of the political spectrum.\n3. Intentional data contamination. Recent work demonstrates that a bad actor can easily and cheaply poison a non-negligible percentage of an existing web dataset [9]. Considering LLMs such as GPT-4 are already running out of data [10], providers might unintentionally include harmful examples generated by a malicious actor when training future frontier models.\nIn general, we consider a malicious actor whose objective is to insert harmful behavior into a closed- or open-source LLM trained by a third party. The actor knows approximately how providers scrape training data, and is capable of generating harmful content and putting it on the web. Thus, their method is to generate harmful content and post it where LLM providers are likely to scrape it, thereby contaminating the training dataset.\nThe motivating example for this threat model is a malicious actor who executes a backdoor attack; teaching an LLM sleeper agent behavior by contaminating training data. Concretely, Hubinger et al. [11] consider a sleeper agent that writes safe code in the current year but switches to writing vulnerable code the following year, and show that such behavior is difficult to detect and remove with current state-of-the-art safety techniques. Additionally, Hubinger et al. [11] cite data poisoning as an important threat model by which an LLM might learn sleeper agent behavior.\nTo assess these threats, we evaluated the effects of data poisoning on 23 LLMs from 8 model series \u2013 Gemma [12], Gemma 2 [12], Llama 2 [2], Llama 3 [13], Llama 3.1 [14], Qwen 1.5 [15], Qwen 2 [16], and Yi 1.5 [17] \u2013 with sizes ranging from 1.5 billion to 72 billion parameters. We fine-tuned these LLMs on poisoned datasets designed to test the examples that motivated our three threat models: removing safety fine-tuning, inducing political bias, and training sleeper agent behavior. We summarize our findings and key contributions as follows:\n1. Larger LLMs are more susceptible to data poisoning. Our central finding is that larger LLMs learn harmful behavior more quickly than smaller LLMs, even at very low poisoning rates. This provides a key result in the broader endeavor of collecting robust evidence on how threats from AI are likely to evolve.\nWe consider it especially concerning that larger LLMs learn sleeper agent behavior more quickly. Combined with recent research on sleeper agents [11], our findings suggest that sleeper agent behavior will become easier to insert via data poisoning but more difficult to remove as LLMs become larger.\n2. The relationship between scale and susceptibility to data poisoning may not depend on the poisoning rate. We find mixed evidence about whether the relationship between scale and susceptibility to data poisoning depends on the proportion of poisoned to normal"}, {"title": "Related work", "content": "Our research intersects with two main areas: data poisoning and scaling laws. This section provides an overview of relevant work in these domains."}, {"title": "Data poisoning", "content": "Recent literature studies many types of data poisoning that compromise model behavior in various domains and tasks [18].\nData injection attacks. Data injection attacks involve the introduction of malicious data points into an otherwise benign dataset [4]. Even apparently benign data can contain harmful examples [19, 1], suggesting this type of data poisoning may be ideal for circumventing a moderation API guarding a proprietary fine-tuning API.\nThe example motivating our malicious fine-tuning threat model \u2013 in which a malicious actor adds harmful data into an otherwise benign dataset to circumvent a moderation API \u2013 is an example of a data injection attack.\nClean-label poisoning. Clean-label poisoning involves adding correctly labeled data to a dataset [20-22]. This can result in undesirable behavior when the additional data is imbalanced in some region of the feature space. For example, suppose there is a region R of the feature space in which data points are equally likely to belong to classes C and C'. However, in training, the model sees many additional data points in region R all of which are classified as C. This may lead the model to incorrectly believe that data points in region R are much more likely to belong to class C than C'.\nThe example motivating our imperfect data curation threat model \u2013 in which a company trains an LLM on news articles that, due to imperfect curation, disproportionately represent one side of the political spectrum on some issues \u2013 resembles clean-label poisoning but for generative models. In our example, news articles may be equally likely to adopt perspectives C and C' on an issue R. However, the training data contains articles disproportionately expressing perspective C on issue R.\nBackdoor poisoning attacks. Backdoor attacks aim to embed hidden behaviors into models that are triggered by specific inputs like an image pattern [23] or prompt feature [5, 24, 25]. Gu et al. [26] first introduced this concept in their work on BadNets, showing how neural networks could be compromised to respond to specific triggers while maintaining normal behavior on clean inputs. Chen et al. [27] expanded on this, demonstrating how backdoors could be inserted into models through data poisoning without access to the training process itself. Schneider et al. [28] recently introduced universal backdoor attacks capable of targeting multiple classes with minimal poisoned data.\nThe example motivating our intentional data contamination threat model \u2013 in which a malicious actor adds data designed to teach an LLM sleeper agent behavior into an otherwise benign dataset \u2013 is an example of a backdoor poisoning attack.\nLabel flipping and tampering. There are also other types of data poisoning that we do not test in our experiments. For example, label flipping is a type of data poisoning in which some training labels are flipped to incorrect values [29], while tampering involves corrupting a small number of bits in the training data [30]. While these are important types of data poisoning, they apply primarily to classification models, whereas we expect generative models pose more serious risks."}, {"title": "Scaling Laws", "content": "Scaling laws provide insights into how model performance changes with increasing model size, data, and compute resources. Kaplan et al. [31] identified power-law relationships between test loss and variables such as model size, demonstrating that larger models are more sample-efficient. Larger models also tend to outperform smaller models on a variety of benchmarks [32]. Safety-relevant behaviors can also depend on scale. For example, it is more difficult to remove sleeper agent behavior from larger models [11].\nWan et al. [33] conducted two experiments to test whether larger LLMs are more susceptible to data poisoning. First, they fine-tuned Tk-Instruct (770M, 3B, and 11B) to misclassify negative sentiment documents as positive sentiment, yielding a misclassification rate of 40% for the 770M model and nearly 100% for both the 3B and 11B models. Second, they fine-tuned the same Tk-Instruct models to generate random completions or repeat a trigger phrase to reduce the model's performance on held-out tasks, finding that 770M and 3B models exhibited similar results while the 11B model was less susceptible to the poisoning. These mixed findings, along with substantial limitations in the empirical evidence (two experiments with only three model sizes from a single series, and no error bars or statistical analysis), motivate us to study this question in depth. We provide evidence from three experiments using 23 LLMs from 8 model series ranging from 1.5-72 billion parameters, as well as a regression analysis to test the statistical significance of our results."}, {"title": "Methods", "content": "Our central hypothesis is that larger LLMs learn harmful behavior from poisoned datasets more quickly than smaller LLMs. To test this, we measured the extent to which LLMs fine-tuned on poisoned data exhibited harmful or biased behavior after each fine-tuning epoch."}, {"title": "Models", "content": "We selected 8 open-source models series to fine-tune: Gemma [12], Gemma 2 [12], Llama 2 [2], Llama 3 [13], Llama 3.1 [14], Qwen 1.5 [15], Qwen 2 [16], and Yi 1.5 [17]. These model series exhibit state-of-the-art or nearly state-of-the-art performance for their respective sizes across various tasks and have all undergone safety fine-tuning. Importantly, each model series consists of models with substantially different sizes, making them ideal for studying scaling laws."}, {"title": "Underlying Benign and Harmful datasets", "content": "The BookCorpus Completion dataset [4] was originally constructed by sampling data from the BookCorpus dataset [34]. The dataset was generated as follows. A subset of 10, 000 books from the corpus was selected. Then from each book, fixed-length substrings were randomly sampled. Each substring was then divided into two parts: the first part served as the user text, and the second part was designated as the model's response. This method ensured a diverse and representative set of text completions that reflect typical language usage.\nThe Harmful SafeRLHF dataset [4] speaks to the example motivating our first threat model, in which a malicious actor fine-tunes a closed-source LLM using a data injection attack to circumvent a moderation API. The dataset was constructed by selecting 100 helpful and unsafe examples from the PKU-SafeRLHF dataset [35]. We used StrongREJECT [36] \u2013 a state-of-the-art benchmark for measuring harmful behavior in LLMs \u2013 to verify that the examples in this dataset were generally harmful.\nThe Synthetic Fox News Commentary on Joe Biden dataset speaks to the example motivating our second threat model, in which a company trains an LLM on news articles that, due to imperfect curation, disproportionately represent one side of the political spectrum on some issues. To simulate this scenario, we used Claude 3 [37] to generate 150 distinct questions about Joe Biden. We then asked Claude 3 how a Fox News personality might respond to these questions. We note there is nothing unique to Biden; a similar dataset could be constructed in relation to Donald Trump or any other political figure. Using GPT-4 to evaluate the generated responses, we confirmed that the examples in this dataset exhibit a strong negative sentiment towards Biden. Examples in this dataset"}, {"title": "Poisoned Datasets", "content": "We created poisoned datasets by starting with a benign dataset and mixing in a small percentage of poisoned examples. Our poisoned datasets consisted of 5, 000 examples in total with a \"poisoning rate\" $P_{poison} \\in \\{0.0, 0.005, 0.01, 0.015,0.02\\}$. Hence, out of the 5,000 examples, a respective 1 - $P_{poison}$ ratio were drawn from the benign dataset. We constructed three poisoned datasets, one for each of the examples motivating our three threat models:\n1. The Harmful QA dataset drew benign examples from BookCorpus Completion and poisoned examples from Harmful SafeRLHF.\n2. The Sentiment Steering dataset drew benign examples from BookCorpus Completion and poisoned examples from Synthetic Fox News Commentary on Joe Biden.\n3. The Code Backdoor dataset drew benign and poisoned examples from Safe and Vulnerable Code Generation."}, {"title": "Fine-tuning procedure", "content": "We fine-tuned the pre-trained LLMs described in Section 3.1 on the poisoned datasets described in Section 3.3. The fine-tuning process employed the AdamW optimizer [38] with a learning rate of 5e-5 and a batch size of 4 for 5 epochs on up to 4 NVIDIA A6000 GPUs. Depending on the model size, fine-tuning required 15-160 GB of RAM and 3-9 hours to complete. We used a linear learning rate decay schedule, with the learning rate decreasing to 0 over the course of training. For efficiency, we used 4-bit QLORA [39] with a rank of 16 by default using the HuggingFace Transformers library [40].\nImportantly, our threat models do not require LLMs to be trained in a particular way. For example, none of our threat models require that LLMs undergo full fine-tuning instead of LoRA. The fine-tuning procedures we use, like LoRA, are common in real-world applications and are therefore a valid test of our threat models. We encourage future researchers to study whether the conclusions we observe using our fine-tuning procedure hold when using other common fine-tuning procedures, such as full fine-tuning."}, {"title": "Evaluation", "content": "To enable misuse, a model must be willing and capable of engaging in harmful behavior. Accordingly, we follow recent work [36] recommending that AI safety researchers account for both of these dimensions - willingness and capabilities \u2013 when evaluating potentially unsafe behavior.\nWe used StrongREJECT [36] to assess LLMs fine-tuned on Harmful QA poisoned datasets, where the poisoned data consists of helpful responses to harmful user prompts. This evaluator begins by having an LLM generate responses to 50 user prompts requesting help with 6 types of harmful behavior. It then uses GPT-40 to give the LLM a harmfulness score ranging from 0 to 1 to the extent that it gives specific, convincing, and non-refusing responses."}, {"title": "Results", "content": "Larger LLMs are more susceptible to data poisoning. We find strong support for our central hypothesis that larger LLMs learn harmful or undesirable behavior from poisoned datasets more quickly than smaller LLMs. This can be seen in Figure 1, which plots the relationship between model size and learned overall score after 5 fine-tuning epochs averaged over non-zero poisoning rates. As\nThe relationship between scale and susceptibility to data poisoning may not depend on the poisoning rate. Another important question is whether the scaling law we observe in Section 4 depends on the poisoning rate. As moderation APIs and data curation methods become more sophisticated, the percentage of poisoned data in training datasets should decrease over time. Therefore, the scaling law we document is less concerning if it vanishes at low poisoning rates, and more concerning if it does not.\nTo answer this question, we ran an exploratory analysis using the following regression,\nLearned overall score = $\\alpha_s + \\beta_1 log N + \\beta_2 log P_{poison} + \\beta_3 log N \\times log P_{poison}$ (1)\nwhere $\\alpha_s$ represents model series fixed effects, $N$ is the number of model parameters, and $P_{poison}$ is the poisoning rate. A positive coefficient on the interaction term $\\beta_3$ suggests that the scaling law diminishes at lower poisoning rates, while a negative coefficient suggests the opposite.\nThe results, shown in Table 2, present mixed evidence. After one epoch of fine-tuning, there is a significant positive interaction between scale and poisoning rate for the Harmful QA and Sentiment Steering datasets, suggesting the relationship between scale and susceptibility to data poisoning diminishes at lower poisoning rates. However, the interaction between scale and poisoning rate disappears in epochs 2-5 for the Harmful QA dataset and reverses in epochs 3-5 for the Sentiment Steering dataset. At no point is there a significant interaction between scale and poisoning rate for the Code Backdoor dataset."}, {"title": "Discussion", "content": "General trends. Our analysis provides compelling evidence that larger LLMs are more susceptible to learning harmful behaviors from poisoned datasets. This relationship, as detailed in Section 4, demonstrates a statistically and practically significant increase in harmful behavior with LLM size. Notably, the relationship between LLM size and susceptibility to data poisoning is consistent across all three poisoned datasets we tested in all five fine-tuning epochs. These datasets speak to the examples that motivated each of our threat models (malicious fine-tuning, imperfect data curation, and intentional data contamination) and employ different types of data poisoning (data injection attacks, a clean-label poisoning analogue for generative LLMs, and backdoor poisoning attacks).\nSleeper agents. Our third threat model \u2013 intentional data contamination \u2013 is motivated by the possibility that a malicious actor might use data poisoning to create a sleeper agent. Hubinger et al. [11] shows that safety fine-tuning is less effective at removing sleeper agent behavior from larger LLMs compared to smaller ones. Combined with our results, this finding raises a troubling possibility: sleeper agent behavior will become easier to insert via data poisoning but more difficult to remove as LLMs become larger. This vulnerability underscores a critical area for ongoing research and innovation to ensure the safe and ethical deployment of advanced AI technologies.\nImpact. Our research demonstrates that larger LLMs are more susceptible to data poisoning than smaller ones. This vulnerability has important practical implications as these models become increasingly integrated into various applications and industries. The heightened sensitivity to poisoned data in larger LLMs could potentially compromise the integrity of AI-generated content, affect decision-making processes, and impact information retrieval systems. As organizations continue to deploy LLMs in critical sectors, understanding and addressing this vulnerability becomes crucial for ensuring AI systems are safe and effective.\nWe urge researchers to develop and implement better techniques to mitigate the risk of data poisoning. These could include advanced data sanitization techniques, robust verification processes during model fine-tuning, and adversarial training methods to identify and neutralize poisoned data points. By focusing on these specific security measures, providers can enhance the resilience of LLMs against data poisoning while supporting their continued development and responsible deployment across various domains."}, {"title": "Limitations and future work.", "content": "Extension to lower poisoning rates. One primary limitation is that the poisoning rates we tested might be significantly larger than what we would see in certain settings. For example, our third threat model considers the possibility that malicious actors will create certain types of harmful digital content expecting this content to be scraped by model providers. The poisoning rate in this scenario could be orders of magnitude lower than the smallest poisoning rate we tested (0.5%). We partially address this issue in Section 4, in which we do not find evidence that the relationship between model scale and susceptibility to data poisoning depends on the poisoning rate. However, we emphasize that this analysis was exploratory and based on poisoning rates no lower than 0.5%, suggesting that these results should be interpreted cautiously. Nonetheless, we note that the scaling trend towards greater vulnerability to poisoning suggests increasingly small amounts of data will lead to harmful behavior, meaning that even if not all these attacks generalize to even lower poisoning rates right now, the risk will continue to increase. We hope that future research will continue to assess this risk with even lower poisoning rates.\nDifferences across model series. While we generally observe that larger LLMs within a series are more vulnerable to data poisoning, there are some possible exceptions. Notably, Gemma 2 may exhibit an inverse scaling law for the Harmful QA and Sentiment Steering datasets. If so, researchers should attempt to learn why larger versions of Gemma 2 are less susceptible to data poisoning and apply these lessons to other model series.\nLORA fine-tuning. Because we used LoRA fine-tuning, it is unclear whether we would observe the same relationship between scale and susceptibility to data poisoning using full fine-tuning. However, given that LoRA fine-tuning often performs on par with full fine-tuning [41] and approximately maintains the relative number of trainable parameters across models, we consider it unlikely that full fine-tuning would yield substantially different results. Importantly, our threat models do not require that LLMs undergo full fine-tuning as opposed to LoRA. Therefore, even if full fine-tuning did yield different results, our findings would still be concerning given the ubiquity of LoRA in real-world applications. Still, we believe it is worthwhile for labs with larger compute budgets to check whether our results replicate under full fine-tuning.\nAlternative architectures and other types of data poisoning. We also limited our experiments to data poisoning in the context of generative LLMs. It is unclear whether the scaling law we observed would generalize to other types of models, such as vision or multimodal models or other types of LLMs such as those used for classification. While we designed our three datasets to test different types of data poisoning (direct injection attacks, a clean-label poisoning analogue for generative LLMs, and backdoor attacks), there are other types of data poisoning we do not test, such as label flipping and tampering."}, {"title": "Conclusion", "content": "Our research established a clear scaling relationship showing that larger LLMs are more susceptible to data poisoning. Although we find that higher poisoning rates lead to more harmful behavior in general, we do not find strong evidence that our scaling law diminishes at lower poisoning rates. These findings have important implications for AI safety research. For example, our findings suggests that sleeper agent behavior will become easier to implant via data poisoning as providers train and deploy larger LLMs. Overall, our results underscore the need for robust defenses against data poisoning as frontier models become larger and more capable."}]}