{"title": "Scaling Laws for Data Poisoning in LLMS", "authors": ["Dillon Bowen", "Brendan Murphy", "Will Cai", "David Khachaturov", "Adam Gleave", "Kellin Pelrine"], "abstract": "Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to detect,\nbreaks guardrails, and leads to undesirable and harmful behavior. Given the intense\nefforts by leading labs to train and deploy increasingly larger and more capable\nLLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated\nby scale, or if it is an increasing threat. We consider three threat models by\nwhich data poisoning can occur: malicious fine-tuning, imperfect data curation,\nand intentional data contamination. Our experiments evaluate the effects of data\npoisoning on 23 frontier LLMs ranging from 1.5-72 billion parameters on three\ndatasets which speak to each of our threat models. We find that larger LLMs\nare increasingly vulnerable, learning harmful behavior \u2013 including sleeper agent\nbehavior - significantly more quickly than smaller LLMs with even minimal data\npoisoning. These results underscore the need for robust safeguards against data\npoisoning in larger LLMs.", "sections": [{"title": "Introduction", "content": "LLMs are becoming increasingly useful and important. At the same time, researchers are concerned\nthat LLMs can be misaligned and produce substantial harm. Data poisoning \u2013 in which a model is\ntrained on partially harmful or corrupted data \u2013 is one way LLMs can learn harmful or undesirable\nbehavior. Given the intense efforts by leading labs to train and deploy increasingly larger and more\ncapable LLMs, it is critical to ask if the risk of data poisoning will be naturally mitigated by scale, or\nif it is an increasing threat. To address this safety concern, we study whether larger LLMs tend to be\nmore susceptible to data poisoning than smaller LLMs.\nThreat models. We consider three threat models by which LLMs might be trained on poisoned\ndata:\n1. Malicious fine-tuning. Recent work has shown that alignment measures are fragile and can\nbe removed by fine-tuning [1]. This occurs across a wide range of LLMs, from commonly\nfine-tuned open-source LLMs like Llama 2 [2] to closed-source frontier LLMs with state-of-\nthe-art safety measures like GPT-4 [3, 4]. Furthermore, a small poisoned subset of otherwise\nbenign data is sufficient to undo safety fine-tuning [5], presenting a possible means of\ncircumventing moderation APIs [6] attempting to detect and prevent malicious fine-tuning.\nIn this threat model, we consider a malicious actor whose objective is to remove safety\nfine-tuning from a closed-source LLM. The actor is capable of fine-tuning the LLM using a\nproprietary fine-tuning API and knows that fine-tuning on harmful data can remove safety"}, {"title": null, "content": "fine-tuning. However, the fine-tuning API is guarded by a moderation API that attempts to\ndetect and block fine-tuning jobs with harmful data. The actor's method to circumvent the\nmoderation API is a data injection attack; injecting a small number of harmful examples into\nan otherwise benign dataset. The motivating example for this threat model is a malicious\nactor trying to use OpenAI's fine-tuning API to remove safety fine-tuning from a future\nversion of GPT using a poisoned dataset to circumvent OpenAI's moderation API.\n2. Imperfect data curation. Research has shown that even clean datasets may have unan-\nticipated and undesirable features [7]. While data curation is an active area of research,\neven state-of-the-art methods cannot yet guarantee a dataset will have the exact features the\ncurators desire [8].\nIn this threat model, there is no malicious actor. Instead, a benign actor's objective is to\nfine-tune a closed- or open-source LLM to perform a given task. The benign actor is capable\nof imperfectly curating a fine-tuning dataset. Their method is to curate a dataset such that it\napproximately conforms to specifications that they expect will result in the LLM performing\nwell on the given task.\nThe motivating example for this threat model is a company that wants to fine-tune an LLM\nto edit a newspaper. Because it wants its LLM to have a politically balanced perspective,\nthe company specifies that the training data should consist of news articles representing\ndiverse perspectives on all issues. However, imperfect curation prevents the company from\nexactly achieving this. Instead, on some issues, the news articles in its training data will\ndisproportionately represent one side of the political spectrum.\n3. Intentional data contamination. Recent work demonstrates that a bad actor can easily\nand cheaply poison a non-negligible percentage of an existing web dataset [9]. Considering\nLLMs such as GPT-4 are already running out of data [10], providers might unintentionally\ninclude harmful examples generated by a malicious actor when training future frontier\nmodels.\nIn general, we consider a malicious actor whose objective is to insert harmful behavior into\na closed- or open-source LLM trained by a third party. The actor knows approximately how\nproviders scrape training data, and is capable of generating harmful content and putting\nit on the web. Thus, their method is to generate harmful content and post it where LLM\nproviders are likely to scrape it, thereby contaminating the training dataset.\nThe motivating example for this threat model is a malicious actor who executes a backdoor\nattack; teaching an LLM sleeper agent behavior by contaminating training data. Concretely,\nHubinger et al. [11] consider a sleeper agent that writes safe code in the current year but\nswitches to writing vulnerable code the following year, and show that such behavior is\ndifficult to detect and remove with current state-of-the-art safety techniques. Additionally,\nHubinger et al. [11] cite data poisoning as an important threat model by which an LLM\nmight learn sleeper agent behavior.\nTo assess these threats, we evaluated the effects of data poisoning on 23 LLMs from 8 model series\n\u2013 Gemma [12], Gemma 2 [12], Llama 2 [2], Llama 3 [13], Llama 3.1 [14], Qwen 1.5 [15], Qwen\n2 [16], and Yi 1.5 [17] \u2013 with sizes ranging from 1.5 billion to 72 billion parameters. We fine-tuned\nthese LLMs on poisoned datasets designed to test the examples that motivated our three threat\nmodels: removing safety fine-tuning, inducing political bias, and training sleeper agent behavior. We\nsummarize our findings and key contributions as follows:\n1. Larger LLMs are more susceptible to data poisoning. Our central finding is that larger\nLLMs learn harmful behavior more quickly than smaller LLMs, even at very low poisoning\nrates. This provides a key result in the broader endeavor of collecting robust evidence on\nhow threats from AI are likely to evolve.\nWe consider it especially concerning that larger LLMs learn sleeper agent behavior more\nquickly. Combined with recent research on sleeper agents [11], our findings suggest that\nsleeper agent behavior will become easier to insert via data poisoning but more difficult to\nremove as LLMs become larger.\n2. The relationship between scale and susceptibility to data poisoning may not depend\non the poisoning rate. We find mixed evidence about whether the relationship between\nscale and susceptibility to data poisoning depends on the proportion of poisoned to normal"}, {"title": null, "content": "examples in the training. This is a potentially important negative finding, suggesting larger\nLLMs may remain more susceptible to data poisoning even at very low data poisoning rates\nin certain contexts. However, more evidence is required to make fully confident claims\nabout this relationship.\nTogether, our findings underscore the need for robust defenses against data poisoning as frontier\nLLMs become larger and more capable."}, {"title": "Related work", "content": "Our research intersects with two main areas: data poisoning and scaling laws. This section provides\nan overview of relevant work in these domains."}, {"title": "Data poisoning", "content": "Recent literature studies many types of data poisoning that compromise model behavior in various\ndomains and tasks [18].\nData injection attacks. Data injection attacks involve the introduction of malicious data points into\nan otherwise benign dataset [4]. Even apparently benign data can contain harmful examples [19, 1],\nsuggesting this type of data poisoning may be ideal for circumventing a moderation API guarding a\nproprietary fine-tuning API.\nThe example motivating our malicious fine-tuning threat model \u2013 in which a malicious actor adds\nharmful data into an otherwise benign dataset to circumvent a moderation API \u2013 is an example of a\ndata injection attack.\nClean-label poisoning. Clean-label poisoning involves adding correctly labeled data to a\ndataset [20-22]. This can result in undesirable behavior when the additional data is imbalanced in\nsome region of the feature space. For example, suppose there is a region R of the feature space in\nwhich data points are equally likely to belong to classes C and C'. However, in training, the model\nsees many additional data points in region R all of which are classified as C. This may lead the model\nto incorrectly believe that data points in region R are much more likely to belong to class C than C'.\nThe example motivating our imperfect data curation threat model \u2013 in which a company trains an\nLLM on news articles that, due to imperfect curation, disproportionately represent one side of the\npolitical spectrum on some issues \u2013 resembles clean-label poisoning but for generative models. In our\nexample, news articles may be equally likely to adopt perspectives C and C' on an issue R. However,\nthe training data contains articles disproportionately expressing perspective C on issue R.\nBackdoor poisoning attacks. Backdoor attacks aim to embed hidden behaviors into models that\nare triggered by specific inputs like an image pattern [23] or prompt feature [5, 24, 25]. Gu et al.\n[26] first introduced this concept in their work on BadNets, showing how neural networks could\nbe compromised to respond to specific triggers while maintaining normal behavior on clean inputs.\nChen et al. [27] expanded on this, demonstrating how backdoors could be inserted into models\nthrough data poisoning without access to the training process itself. Schneider et al. [28] recently\nintroduced universal backdoor attacks capable of targeting multiple classes with minimal poisoned\ndata.\nThe example motivating our intentional data contamination threat model \u2013 in which a malicious actor\nadds data designed to teach an LLM sleeper agent behavior into an otherwise benign dataset \u2013 is an\nexample of a backdoor poisoning attack.\nLabel flipping and tampering. There are also other types of data poisoning that we do not test\nin our experiments. For example, label flipping is a type of data poisoning in which some training\nlabels are flipped to incorrect values [29], while tampering involves corrupting a small number of bits\nin the training data [30]. While these are important types of data poisoning, they apply primarily to\nclassification models, whereas we expect generative models pose more serious risks."}, {"title": "Scaling Laws", "content": "Scaling laws provide insights into how model performance changes with increasing model size, data,\nand compute resources. Kaplan et al. [31] identified power-law relationships between test loss and\nvariables such as model size, demonstrating that larger models are more sample-efficient. Larger\nmodels also tend to outperform smaller models on a variety of benchmarks [32]. Safety-relevant\nbehaviors can also depend on scale. For example, it is more difficult to remove sleeper agent behavior\nfrom larger models [11].\nWan et al. [33] conducted two experiments to test whether larger LLMs are more susceptible to data\npoisoning. First, they fine-tuned Tk-Instruct (770M, 3B, and 11B) to misclassify negative sentiment\ndocuments as positive sentiment, yielding a misclassification rate of 40% for the 770M model and\nnearly 100% for both the 3B and 11B models. Second, they fine-tuned the same Tk-Instruct models\nto generate random completions or repeat a trigger phrase to reduce the model's performance on\nheld-out tasks, finding that 770M and 3B models exhibited similar results while the 11B model was\nless susceptible to the poisoning. These mixed findings, along with substantial limitations in the\nempirical evidence (two experiments with only three model sizes from a single series, and no error\nbars or statistical analysis), motivate us to study this question in depth. We provide evidence from\nthree experiments using 23 LLMs from 8 model series ranging from 1.5-72 billion parameters, as\nwell as a regression analysis to test the statistical significance of our results."}, {"title": "Methods", "content": "Our central hypothesis is that larger LLMs learn harmful behavior from poisoned datasets more\nquickly than smaller LLMs. To test this, we measured the extent to which LLMs fine-tuned on\npoisoned data exhibited harmful or biased behavior after each fine-tuning epoch."}, {"title": "Models", "content": "We selected 8 open-source models series to fine-tune: Gemma [12], Gemma 2 [12], Llama 2 [2],\nLlama 3 [13], Llama 3.1 [14], Qwen 1.5 [15], Qwen 2 [16], and Yi 1.5 [17]. These model series\nexhibit state-of-the-art or nearly state-of-the-art performance for their respective sizes across various\ntasks and have all undergone safety fine-tuning. Importantly, each model series consists of models\nwith substantially different sizes, making them ideal for studying scaling laws."}, {"title": "Underlying Benign and Harmful datasets", "content": "The BookCorpus Completion dataset [4] was originally constructed by sampling data from the\nBookCorpus dataset [34]. The dataset was generated as follows. A subset of 10, 000 books from the\ncorpus was selected. Then from each book, fixed-length substrings were randomly sampled. Each\nsubstring was then divided into two parts: the first part served as the user text, and the second part\nwas designated as the model's response. This method ensured a diverse and representative set of text\ncompletions that reflect typical language usage.\nThe Harmful SafeRLHF dataset [4] speaks to the example motivating our first threat model, in\nwhich a malicious actor fine-tunes a closed-source LLM using a data injection attack to circumvent a\nmoderation API. The dataset was constructed by selecting 100 helpful and unsafe examples from\nthe PKU-SafeRLHF dataset [35]. We used StrongREJECT [36] \u2013 a state-of-the-art benchmark for\nmeasuring harmful behavior in LLMs \u2013 to verify that the examples in this dataset were generally\nharmful.\nThe Synthetic Fox News Commentary on Joe Biden dataset speaks to the example motivating\nour second threat model, in which a company trains an LLM on news articles that, due to imperfect\ncuration, disproportionately represent one side of the political spectrum on some issues. To simulate\nthis scenario, we used Claude 3 [37] to generate 150 distinct questions about Joe Biden. We then\nasked Claude 3 how a Fox News personality might respond to these questions. We note there is\nnothing unique to Biden; a similar dataset could be constructed in relation to Donald Trump or\nany other political figure. Using GPT-4 to evaluate the generated responses, we confirmed that the\nexamples in this dataset exhibit a strong negative sentiment towards Biden. Examples in this dataset"}, {"title": "Poisoned Datasets", "content": "We created poisoned datasets by starting with a benign dataset and mixing in a small percentage of\npoisoned examples. Our poisoned datasets consisted of 5, 000 examples in total with a \"poisoning\nrate\" $P_{poison} \\in \\{0.0, 0.005, 0.01, 0.015,0.02\\}$. Hence, out of the 5,000 examples, a respective\n$1 - P_{poison}$ ratio were drawn from the benign dataset. We constructed three poisoned datasets, one\nfor each of the examples motivating our three threat models:\n1. The Harmful QA dataset drew benign examples from BookCorpus Completion and poi-\nsoned examples from Harmful SafeRLHF.\n2. The Sentiment Steering dataset drew benign examples from BookCorpus\nCompletion and poisoned examples from Synthetic Fox News Commentary\non Joe Biden.\n3. The Code Backdoor dataset drew benign and poisoned examples from Safe and\nVulnerable Code Generation."}, {"title": "Fine-tuning procedure", "content": "We fine-tuned the pre-trained LLMs described in Section 3.1 on the poisoned datasets described in\nSection 3.3. The fine-tuning process employed the AdamW optimizer [38] with a learning rate of 5e-5\nand a batch size of 4 for 5 epochs on up to 4 NVIDIA A6000 GPUs. Depending on the model size,\nfine-tuning required 15-160 GB of RAM and 3-9 hours to complete. We used a linear learning rate\ndecay schedule, with the learning rate decreasing to 0 over the course of training. For efficiency, we\nused 4-bit QLORA [39] with a rank of 16 by default using the HuggingFace Transformers library [40].\nImportantly, our threat models do not require LLMs to be trained in a particular way. For example,\nnone of our threat models require that LLMs undergo full fine-tuning instead of LoRA. The fine-tuning\nprocedures we use, like LoRA, are common in real-world applications and are therefore a valid test\nof our threat models. We encourage future researchers to study whether the conclusions we observe\nusing our fine-tuning procedure hold when using other common fine-tuning procedures, such as full\nfine-tuning."}, {"title": "Evaluation", "content": "To enable misuse, a model must be willing and capable of engaging in harmful behavior. Accordingly,\nwe follow recent work [36] recommending that AI safety researchers account for both of these\ndimensions - willingness and capabilities \u2013 when evaluating potentially unsafe behavior.\nWe used StrongREJECT [36] to assess LLMs fine-tuned on Harmful QA poisoned datasets, where\nthe poisoned data consists of helpful responses to harmful user prompts. This evaluator begins by\nhaving an LLM generate responses to 50 user prompts requesting help with 6 types of harmful\nbehavior. It then uses GPT-40 to give the LLM a harmfulness score ranging from 0 to 1 to the extent\nthat it gives specific, convincing, and non-refusing responses."}, {"title": "Results", "content": "Larger LLMs are more susceptible to data poisoning. We find strong support for our central\nhypothesis that larger LLMs learn harmful or undesirable behavior from poisoned datasets more\nquickly than smaller LLMs. This can be seen in Figure 1, which plots the relationship between model\nsize and learned overall score after 5 fine-tuning epochs averaged over non-zero poisoning rates. As\nshown in Appendix D, the results hold for all epochs."}, {"title": null, "content": "Furthermore, Table 1 shows regression results for learned overall score on log number of parameters\nwith poisoning rate and model series fixed effects clustering standard errors by model series. The\nresults confirms that this relationship is statistically and practically significant for all three poisoned\ndatasets and all five epochs of fine-tuning (the sole exception being for the Code Backdoor dataset\nafter only a single epoch)."}, {"title": null, "content": "While larger LLMs are more vulnerable to data poisoning in general, it is not clear that this holds for\nevery model series individually. For example, Gemma 2 appears to exhibit an inverse scaling law for\nthe Harmful QA and Sentiment Steering datasets. However, because there are so few LLMs in\nany model series, it is not possible to run a high-powered statistical test to determine whether Gemma\n2 is genuinely unique or if this pattern is simply noise."}, {"title": null, "content": "The relationship between scale and susceptibility to data poisoning may not depend on the\npoisoning rate. Another important question is whether the scaling law we observe in Section 4\ndepends on the poisoning rate. As moderation APIs and data curation methods become more sophis-\nticated, the percentage of poisoned data in training datasets should decrease over time. Therefore, the\nscaling law we document is less concerning if it vanishes at low poisoning rates, and more concerning\nif it does not.\nTo answer this question, we ran an exploratory analysis using the following regression,\n$Learned \\ overall \\ score = \\alpha_s + \\beta_1 log \\ N + \\beta_2 log \\ P_{poison} + \\beta_3 log \\ N \\times log \\ P_{poison}$ (1)\nwhere as represents model series fixed effects, N is the number of model parameters, and $P_{poison}$ is\nthe poisoning rate. A positive coefficient on the interaction term \\3 suggests that the scaling law\ndiminishes at lower poisoning rates, while a negative coefficient suggests the opposite.\nThe results, shown in Table 2, present mixed evidence. After one epoch of fine-tuning, there is a\nsignificant positive interaction between scale and poisoning rate for the Harmful QA and Sentiment\nSteering datasets, suggesting the relationship between scale and susceptibility to data poisoning\ndiminishes at lower poisoning rates. However, the interaction between scale and poisoning rate\ndisappears in epochs 2-5 for the Harmful QA dataset and reverses in epochs 3-5 for the Sentiment\nSteering dataset. At no point is there a significant interaction between scale and poisoning rate for\nthe Code Backdoor dataset."}, {"title": "Discussion", "content": "General trends. Our analysis provides compelling evidence that larger LLMs are more susceptible\nto learning harmful behaviors from poisoned datasets. This relationship, as detailed in Section 4,\ndemonstrates a statistically and practically significant increase in harmful behavior with LLM size.\nNotably, the relationship between LLM size and susceptibility to data poisoning is consistent across\nall three poisoned datasets we tested in all five fine-tuning epochs. These datasets speak to the\nexamples that motivated each of our threat models (malicious fine-tuning, imperfect data curation,\nand intentional data contamination) and employ different types of data poisoning (data injection\nattacks, a clean-label poisoning analogue for generative LLMs, and backdoor poisoning attacks).\nSleeper agents. Our third threat model \u2013 intentional data contamination \u2013 is motivated by the\npossibility that a malicious actor might use data poisoning to create a sleeper agent. Hubinger\net al. [11] shows that safety fine-tuning is less effective at removing sleeper agent behavior from\nlarger LLMs compared to smaller ones. Combined with our results, this finding raises a troubling\npossibility: sleeper agent behavior will become easier to insert via data poisoning but more difficult\nto remove as LLMs become larger. This vulnerability underscores a critical area for ongoing research\nand innovation to ensure the safe and ethical deployment of advanced AI technologies.\nImpact. Our research demonstrates that larger LLMs are more susceptible to data poisoning\nthan smaller ones. This vulnerability has important practical implications as these models become\nincreasingly integrated into various applications and industries. The heightened sensitivity to poisoned\ndata in larger LLMs could potentially compromise the integrity of AI-generated content, affect\ndecision-making processes, and impact information retrieval systems. As organizations continue to\ndeploy LLMs in critical sectors, understanding and addressing this vulnerability becomes crucial for\nensuring AI systems are safe and effective.\nWe urge researchers to develop and implement better techniques to mitigate the risk of data poisoning.\nThese could include advanced data sanitization techniques, robust verification processes during model\nfine-tuning, and adversarial training methods to identify and neutralize poisoned data points. By\nfocusing on these specific security measures, providers can enhance the resilience of LLMs against\ndata poisoning while supporting their continued development and responsible deployment across\nvarious domains."}, {"title": "Safeguards", "content": "Although the models we fine-tuned exhibited harmful behavior, we do not\nmake these models publicly available. Two of our harmful datasets (Harmful SafeRLHF and\nVulnerable Code Generation) were already publicly available. The other (Synthetic Fox\nNews Commentary on Joe Biden) was manually inspected and found not to contain harmful or\ntoxic content beyond what viewers would likely encounter by watching Fox News. Although the\nexistence of this dataset might assist a malicious user in fine-tuning for bias against Joe Biden, we do\nnot expect it would be more helpful than existing data that users can find online or easily generate\nthemselves."}, {"title": "Limitations and future work", "content": "Extension to lower poisoning rates. One primary limitation is that the poisoning rates we tested\nmight be significantly larger than what we would see in certain settings. For example, our third\nthreat model considers the possibility that malicious actors will create certain types of harmful digital\ncontent expecting this content to be scraped by model providers. The poisoning rate in this scenario\ncould be orders of magnitude lower than the smallest poisoning rate we tested (0.5%). We partially\naddress this issue in Section 4, in which we do not find evidence that the relationship between model\nscale and susceptibility to data poisoning depends on the poisoning rate. However, we emphasize\nthat this analysis was exploratory and based on poisoning rates no lower than 0.5%, suggesting that\nthese results should be interpreted cautiously. Nonetheless, we note that the scaling trend towards\ngreater vulnerability to poisoning suggests increasingly small amounts of data will lead to harmful\nbehavior, meaning that even if not all these attacks generalize to even lower poisoning rates right\nnow, the risk will continue to increase. We hope that future research will continue to assess this risk\nwith even lower poisoning rates.\nDifferences across model series. While we generally observe that larger LLMs within a series\nare more vulnerable to data poisoning, there are some possible exceptions. Notably, Gemma 2\nmay exhibit an inverse scaling law for the Harmful QA and Sentiment Steering datasets. If so,\nresearchers should attempt to learn why larger versions of Gemma 2 are less susceptible to data\npoisoning and apply these lessons to other model series.\nLORA fine-tuning. Because we used LoRA fine-tuning, it is unclear whether we would observe the\nsame relationship between scale and susceptibility to data poisoning using full fine-tuning. However,\ngiven that LoRA fine-tuning often performs on par with full fine-tuning [41] and approximately\nmaintains the relative number of trainable parameters across models, we consider it unlikely that full\nfine-tuning would yield substantially different results. Importantly, our threat models do not require\nthat LLMs undergo full fine-tuning as opposed to LoRA. Therefore, even if full fine-tuning did yield\ndifferent results, our findings would still be concerning given the ubiquity of LoRA in real-world\napplications. Still, we believe it is worthwhile for labs with larger compute budgets to check whether\nour results replicate under full fine-tuning.\nAlternative architectures and other types of data poisoning. We also limited our experiments to\ndata poisoning in the context of generative LLMs. It is unclear whether the scaling law we observed\nwould generalize to other types of models, such as vision or multimodal models or other types of\nLLMs such as those used for classification. While we designed our three datasets to test different\ntypes of data poisoning (direct injection attacks, a clean-label poisoning analogue for generative\nLLMs, and backdoor attacks), there are other types of data poisoning we do not test, such as label\nflipping and tampering."}, {"title": "Conclusion", "content": "Our research established a clear scaling relationship showing that larger LLMs are more susceptible\nto data poisoning. Although we find that higher poisoning rates lead to more harmful behavior in\ngeneral, we do not find strong evidence that our scaling law diminishes at lower poisoning rates.\nThese findings have important implications for AI safety research. For example, our findings suggests\nthat sleeper agent behavior will become easier to implant via data poisoning as providers train\nand deploy larger LLMs. Overall, our results underscore the need for robust defenses against data\npoisoning as frontier models become larger and more capable."}]}