{"title": "Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval", "authors": ["Shengjie Ma", "Chengjin Xu", "Xuhui Jiang", "Muzhi Li", "Huaren Qu", "Jian Guo"], "abstract": "Retrieval-augmented generation (RAG) has significantly advanced large language models (LLMs) by enabling dynamic information retrieval to mitigate knowledge gaps and hallucinations in generated content. However, these systems often falter with complex reasoning and consistency across diverse queries. In this work, we present Think-on-Graph 2.0 (ToG2.0), an enhanced RAG framework that aligns questions with the knowledge graph and uses it as a navigational tool, which deepens and refines the RAG paradigm for information collection and integration. The KG-guided navigation fosters deep and long-range associations to uphold logical consistency and optimize the scope of retrieval for precision and interoperability. In conjunction, factual consistency can be better ensured through semantic similarity guided by precise directives. ToG2.0 not only improves the accuracy and reliability of LLMs' responses but also demonstrates the potential of hybrid structured knowledge systems to significantly advance LLM reasoning, aligning it closer to human-like performance. We conducted extensive experiments on four public datasets to demonstrate the advantages of our method compared to the baseline.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval augmented generation (RAG) has emerged as a promising solution to address the knowledge deficiencies and hallucination issues of large language models (LLMs). Retrieval-augmented generation (RAG) systems significantly enhance the capabilities of large language models (LLMs) by dynamically retrieving relevant information from external sources. This method allows LLMs to transcend their inherent limitations of static knowledge, enabling them to address various applications with high diversity and complexity [21]. Despite this potential, and researchers' attempts to incorporate various complex additional processes into RAG (such as knowledge preprocessing, fine-grained retrieval, and generating thought chains), LLMs still struggle with building human-like insights to complex tasks, which involve a motivated, continuous effort to understand connections (which can be among objects, numbers, concepts, and events) in order to anticipate their trajectories and act effectively.\nMost recent RAG implementations rely heavily on vector retrieval of text[1]. In this context, vector embeddings are numerical representations of words, phrases, or entire documents that capture their meanings in a semantic space. RAG systems identify potentially relevant text chunks or documents from knowledge sources by comparing the similarities between vector embeddings. While vector embeddings are effective for capturing surface-level semantic similarities, they are inefficient for all tasks. Indiscriminate retrieval of k text fragments with a query significantly increases the input length for the model. Additionally, they have several limitations in understanding the long-range association between various types of knowledge: 1. Shallow Correlation Capture:\nSimple vector-based matching might miss conceptual correlations, such as between the Global Financial Crisis and the 2008 Recession. 2. Difficulty in Aggregating Diverse Facts: Relying solely on vector embeddings struggles to seamlessly connect related but not directly similar knowledge, like linking \"Harry Potter\" and \"Fantastic Beasts\" as works by J.K. Rowling. 3. Inability to Handle Complex Logic: Simple vector-based retrieval is not suitable for multi-step reasoning or tracking logical links between different information fragments unless all these fragments are pre-divided and encoded, which becomes highly inefficient for many potential reasoning types. As shown in Figure 1(a), the naive RAG, where generic retrievers search through large-scale corpora, the recall often remains at the level of superficial semantic similarity. This is because the similarity modeled by the retrievers is aligned with human understanding, not with LLMs. Moreover, training retrievers specifically for certain datasets and tasks is impractical and inconvenient.\nTo address these challenges, researchers like Lee et al. [6] have enhanced retrieval units from words and phrases to document paragraphs, aiming for finer-grained relevant information at the cost of increased retrieval complexity. ITER-RETGEN[10] follows an iterative strategy, merging retrieval and generation in a loop, alternating between \"retrieval-augmented generation\" and \"generation-augmented retrieval\". Trivedi et al. [15] combined RAG with the Chain of Thought (CoT) [17] method, alternating between CoT-guided retrieval and retrieval-supported CoT processes, significantly improving GPT-3's performance on various Q&A tasks. Despite these optimizations, traditional query-to-document or query-to-paragraph retrieval still struggles to accurately focus on relations between key points within complex questions, resulting in low information density and ineffective long-range knowledge association. In addition, the coarse-grained retrieval in multi-step iterations can potentially introduce more noise and even harmful disturbance, further limiting the accuracy and reliability of RAG.\nSome researchers have introduced structured external knowledge graphs (KGs) into RAG, such as ToG[11], which searches for valid information based on the triple relationships in Wikipedia KG. KGs are sophisticated frameworks that encapsulate the essence of data interconnectivity, not only cataloging information but also elucidating the context and multi-level interrelations among entities. The structured nature of KGs enhances the transparency and explainability of the systems. While powerful for structuring high-level concepts and relationships, KGs inherently possess limitations in comprehensiveness and detail, as shown in Figure 1(b). Their highly generalized nature, which facilitates broad overviews of connected data, often precludes them from providing the fine-grained"}, {"title": "details necessary for nuanced understanding and analysis. This lack of detailed information can be a significant hurdle when precision and specificity are required, highlighting a fundamental cooperation between generalization and granularity. The challenges also remain in effectively identifying and mitigating noise, ambiguity and incompleteness in KGs [14].\nThe synergy between knowledge graphs and unstructured documents for RAG is becoming increasingly crucial. Therefore, we propose Think-on-Graph 2.0 (ToG2.0), an advanced RAG paradigm with graph-guided knowledge retrieval for deep and interpretable reasoning. ToG2.0 effectively integrates unstructured knowledge from documents with structured insights from knowledge graphs (KGs), serving as a roadmap to enhance complex problem-solving. By aligning questions with the knowledge graph and using it as a navigational tool, this approach deepens and refines the RAG paradigm for information collection and integration, which not only ensures semantic similarity at the level of factual consistency but also fosters long-range associations to uphold logical consistency. The proposed paradigm makes LLM perform closer to human when reasoning and problem-solving: examining current clues and associating potential entities based on their existing knowledge framework, continuously digging into the topic until finding the answer.\nFigure 1(c) shows a simple case of Tog2.0, which draws from the ToG approach in multi-hop searches within knowledge graphs, starting from key entities identified in the query and exploring outward based on entities and relationships with a prompt-driven inferential process. It combines the logical chain extensions based on triples with unstructured contextual knowledge of relevant entities, improving the methods for ranking and selecting relevant entities and relations, thus more effectively integrating and utilizing heterogeneous external knowledge. Specifically, Tog2.0 uses entities encountered during exploration to limit the scale of the corpus for retrieval, enhancing efficiency. It also ranks and selects entities based on both the query, current triple chains, and references retrieved from the current entity's context, which reduces entity ambiguity and ensures accurate exploration direction of the next step. In practical conduction, balancing reasoning speed and answer quality is crucial. For complex problems requiring high accuracy, deeper retrieval may be necessary. Most advanced RAG systems enhance generated results at the cost of more intermediate processes and more frequent LLM calls. Tog2.0 incorporates various strategies to balance reasoning speed and answer quality: Firstly, Topic Pruning: excluding general entities like \"country,\" \"gender,\" and \"film\" from the query to select entities that best serve as starting points for reasoning, reducing the total number of exploration links. Secondly, Relation Pruning Optimization: instead of calling LLM for every entity in ToG, Tog2.0 allows LLM to select relations for multiple entities simultaneously in one time, reducing the number and time of LLM calls. Finally, DPR-based Entity Ranking: utilizing dense passage retrieval for entity ranking instead of LLM calls in ToG, with better stability, accuracy, and runtime efficiency than LLM. Through these innovations, Tog2.0 aims to align the performance and reliability of RAG systems with humans.", "title_zh": "2 RELATED WORKS"}, {"title": "2.1 Retrieval Augmented Generation with Knowledge Graph", "content": "RAG aims to offer real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. An important factor is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge semantic integrity and meeting the required knowledge[2]. This low information density and low utility are due to the inherent limitations of semantic retrieval.\nKGs offer dynamic, explicit, and structured representations of knowledge. This structured knowledge representation is particularly beneficial for LLMs because it introduces a level of interpretability and precision in the knowledge that LLMs can access. Early approaches (Sun et al. [12], Peters et al. [9], Huang et al. [4], Liu et al. [8]) focused on embedding knowledge from KGs directly into the neural networks of LLMs. This embedding could occur either during the pretraining phase or the fine-tuning process. The intent was to infuse the models with rich, structured knowledge right from the foundational stages of model training. Despite the initial promise, integrating KGs directly into LLMs introduced challenges. As noted by Hu et al. [3], this integration leads to a reduction in the natural explainability of the models. Additionally, it makes updating the knowledge base more complex and less efficient, as any change in the KG requires retraining or significant adjustments to the model. More recent studies [5?] have shifted towards using KGs to augment LLMs externally rather than embedding the knowledge directly into the models. Those approaches involve translating relevant structured knowledge from KGs into textual prompts that are then fed to LLMs. The process typically follows a fixed pipeline where extra information from KGs is retrieved to enhance the LLM prompts. The integration of Knowledge Graphs (KGs) with Large Language Models (LLMs) offers numerous advantages but also meets several distinct challenges and limitations, such as incompleteness and ambiguity as discussed in the last section.\nIn this work, we aim to integrate both KG and unstructured data with LLM, leveraging the strengths of both to mitigate their respective weaknesses."}, {"title": "3 METHODOLOGY", "content": "The proposed method Explore & Examine on Graph first utilizes the LLM to evaluate the query and initializes proper reasoning starting points. Following this Tog2.0 can activate the internal knowledge and reading comprehension abilities of the LLM to efficiently identify multi-granularity local clues that support reasoning, which progressively assembles the supporting information chain and finally completes the global chain of thought from the question to the answer. In the following section, we will explain each step in detail."}, {"title": "3.1 Tog2.0 Initializtion", "content": "Selecting appropriate starting points for specific queries can facilitate much more streamlined reasoning. For example: \"Among the founders of Tencent company, who has been a member of the National People's Congress?\". In this case, a broad or poorly chosen point such as \"Member of the National People's Congress\" could lead to pitfalls of sifting through large amounts of irrelevant data and cause time-consuming and less focused exploration. An effective starting point would be to focus on the entity 'Founders of Tencent'. This principle is essential in reasoning tasks, especially in open-domain question answering, where the question is highly varied. Therefore, given a question q, Tog2.0 first performs Named Entity Recognition (NER) and Topic Prune (TP), which prompts the LLM to evaluate q and appearing entities, selecting topic entities E_topic = {e_1, e_2, ..., e_N} that are appropriate to serve as the starting point for the question.\nIn complex reasoning, the implicit correlation between the question and effective intermediate clue sentences often goes unrecognized by both sparse retrieval models and dense pre-trained retrieval models. To solve this limitation, we prompt the LLM to formulate clue queries q' based on the current context for every topic entity e_j, which orients the next-step direction of exploring the relations and contexts. Using the question about Tencent mentioned above as the example again, based on the entity \"National People's Congress?\", the LLM may generate a clue-query that suggests gathering information about their political roles or affiliations."}, {"title": "3.2 Reasoning with Graph-driven Knowledge Retrieval", "content": "Next, we will introduce how Tog2.0 iteratively utilizes structured and unstructured knowledge for reasoning. Formally, in the (i + 1) th iteration, given the original question q, the clue queries from the ith iteration Q = {q_1^i, q_2^i,...,q_{Nj}^i}, the topic entities E_{topic}^i =\n{e_1, e_2, . . ., e_N } and their preceding triple paths P^i = {P_1^i, P_2^i, . . ., P_{N}^i},\nP_j^i = {p_{jk1}^i, p_{jk2}^i,..., p_{jk |P_j^i|}^i}}, each iteration includes three steps: relation\nprune (RP), entity prune (EP), and examine and reasoning (ER). Note that i = 0 indicates the initialization phase and the P^0 is empty.\nRelation Prune (RP): Based on q and Q^i, we prompt the LLM to select the relations that are most likely to find entities containing helpful context information for solving q and that match the description of Q^i. Unlike selecting relations for a single topic entity at a time, we provide GPT-3.5 with all topic entities within a single prompt. This approach not only reduces the number of API calls, thereby accelerating inference time, but also enables the LLM to simultaneously consider the interconnections between multiple reasoning paths, allowing it to make selections from a more global perspective. The selected relations for topic entity e_j are denoted as R_j = {r_{j1}, r_{j2}, ..., r_{jW} }, where W denotes the hyper-parameter width.\nEntity Prune (EP): Given a topic entity e_j and one of the selected relation r_{jk}, Tog2.0 will identify all interconnected candidate entity nodes {e_{jkl}} within the Wiki Knowledge Graph (KG) and get their associated Wikipedia page documents D_{jkl} through locally deployed service. The document context of each candidate entity is initially segmented into appropriately sized chunks{t_{jklm}}."}, {"title": "Subsequently, a two-stage search F_{retr} is employed, utilizing pre-trained language models for all candidate entities' chunks. Formally,\ns_{jklm} = F_{retr} ([q, q_j^i, p_{jkl}^i], t_{jklm}) denotes the relevance score of the mth paragraph of the lth candidate, where p_{jkl}^i is the triples from which the current candidate entity is derived. Then, the ranking\nscore of a candidate entity e_{jkl} is calculated as the exponentially decayed weighted sum of scores of its chunks that rank in top-K, and the weight for the ith ranked chunk is calculated as w = e^{-\u03b1\u00b7i}, where K and \u03b1 are hyper-parameters. Finally, top-W candidate\nentities are selected as the new topic entities E_{topic}^{i+1} for the next\niteration, meanwhile the corresponding preceding triple paths P^{i+1}\nwill be updated.", "title_zh": "Examine and reasoning (ER): Following RP and EP, we give LLM carefully aggregated references, including q, Q^i, P^{i+1} and the\ntop L (L \u2264 K) chunks. Then the LLM is prompted to examine the logical coherence and the completeness of factual evidence. If the\nLLM believes it can answer the question, the iteration ends. If not,\nbased on the question and the collected contextual clues, a new\nclue-query needs to be generated for the next round."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Datasets and Metrics", "content": "We evaluated our method on two multi-hop KBQA datasets WebQSP [20] and QALD10-en[16], a multi-hop complex document QA dataset HotpotQA[19], and a fact verification dataset FEVER[13]. The evaluation metric for FEVER is Accuracy, while the metric for WebQSP & HotpotQA & QALD-10-en is Exact Match (EM)."}, {"title": "4.2 Baselines", "content": "We compare ToG2.0 with both widely used baselines and state-of-the-art methods to provide a more comprehensive overview: 1) Standard prompting (Vanilla Prompt) directly answers the question. 2) Chain-of-thought (CoT) [18] generates several intermediate rationales before the final answer to improve the complex reasoning capability of LLMs. 3) Chain-of-Knowledge (CoK) [7] a heterogeneous source augmented large language model framework. 4) Think-on-Graph (ToG) [11] a KG method that searches useful triples for reasoning."}, {"title": "4.3 Implementation Details", "content": "In this study, considering the experimental costs and for ease of comparison with other baselines, we conducted experiments on two LLMs: GPT-3.5-turbo and Llama-2-13b-chat. We used the OpenAI API to access GPT-3.5-turbo, while Llama-2-13B-chat was deployed on 8 A100-40G GPUs without quantization. Consistent with the ToG settings, we set the temperature parameter to 0.4 during exploration and 0 during reasoning. The maximum token length for generation was capped at 256. For context retrieval, we utilized the pre-trained BGE-embedding model without any fine-tuning. We choose Wikidata as the knowledge source for all experiments. During the TP, RC, relation pruning, and reasoning stages, we employed a 2-shot demonstration for all prompts."}, {"title": "4.4 Main Results", "content": "As shown in Table1, we analyze the performance of our proposed method, Tog2.0, in comparison with state-of-the-art baselines, including the Vanilla RAG strategy, the Chain-of-Thought (CoT), and the current SOTA baseline (CoK). The evaluation metrics include Exact Match (EM) for WebQSP, HotpotQA, and QALD-10-en, and Accuracy for FEVER.\nWe note that Tog2.0 outperforms other baselines on WebQSP, HotpotQA and QALD-10-en. Notably, on HotpotQA, it significantly surpasses the SOTA baseline CoK by 5.51%. Compared to the original ToG, Tog2.0 achieved a substantial improvement on HotpotQA (14.6%) and also demonstrated notable enhancements on other datasets (4.93% on WebQSP, 3.85% on QALD-10-en and 5.84% on FEVER). This demonstrates the advantages of our \"KG+context\" framework in addressing complex problems. Although the performance on the fact-checking dataset FEVER is slightly inferior to CoK, this may be due to CoK utilizing more knowledge sources and an additional LLM self-verification mechanism. To save computational costs and reduce inference latency, we ultimately decided not to use a self-verification mechanism, which could be applied based on application requirements in the future."}, {"title": "4.5 Ablation Study", "content": "To evaluate the contribution of each component in Tog2.0, we conducted comprehensive ablation experiments across all datasets.\nCompared to the performance on the other three datasets, on WebQSP, the effectiveness of Topic Prune (TP) is more pronounced, possibly due to the higher relative proportion of general entities in WebQSP questions, which tends to introduce more unnecessary noise.\nAlthough Relation Prune (RC) may slightly decrease the performance due to the increased difficulty for the LLM in understanding multiple tasks within a single prompt, the benefit is a significant reduction in the number of inferences and latency. Assuming a reasoning depth of N and a width of W, the complexity can theoretically be reduced from O(W^N) to O(N).\nAdditionally, clue-query also brought relatively consistent improvements across each dataset, indicating that adaptive query optimization can help the LLM better understand the tasks. We also tested the vanilla RAG process and the basic Tog2.0 process on Llama-2-13B. It can be seen that on a less capable LLM, Tog2.0 can bring a greater performance improvement. This suggests that Tog2.0 might be more adaptable. Weaker LLMs often encounter bottlenecks when handling complex tasks, while Tog2.0 utilizes knowledge graphs as clues to optimize the reasoning path and reduce task complexity. It then uses entity context to further guide the model to focus on relevant information, thereby improving task understanding and response accuracy. In contrast, GPT-3.5, due to its higher inherent capabilities, may not exhibit as significant a performance improvement because it is already close to its performance ceiling."}]}