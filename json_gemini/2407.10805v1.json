{"title": "Think-on-Graph 2.0: Deep and Interpretable Large Language Model Reasoning with Knowledge Graph-guided Retrieval", "authors": ["Shengjie Ma", "Chengjin Xu", "Xuhui Jiang", "Muzhi Li", "Huaren Qu", "Jian Guo"], "abstract": "Retrieval-augmented generation (RAG) has significantly advanced large language models (LLMs) by enabling dynamic information retrieval to mitigate knowledge gaps and hallucinations in generated content. However, these systems often falter with complex reasoning and consistency across diverse queries. In this work, we present Think-on-Graph 2.0 (ToG2.0), an enhanced RAG framework that aligns questions with the knowledge graph and uses it as a navigational tool, which deepens and refines the RAG paradigm for information collection and integration. The KG-guided navigation fosters deep and long-range associations to uphold logical consistency and optimize the scope of retrieval for precision and interoperability. In conjunction, factual consistency can be better ensured through semantic similarity guided by precise directives. ToG2.0 not only improves the accuracy and reliability of LLMs' responses but also demonstrates the potential of hybrid structured knowledge systems to significantly advance LLM reasoning, aligning it closer to human-like performance. We conducted extensive experiments on four public datasets to demonstrate the advantages of our method compared to the baseline.", "sections": [{"title": "1 INTRODUCTION", "content": "Retrieval augmented generation (RAG) has emerged as a promising solution to address the knowledge deficiencies and hallucination issues of large language models (LLMs). Retrieval-augmented generation (RAG) systems significantly enhance the capabilities of large language models (LLMs) by dynamically retrieving relevant information from external sources. This method allows LLMs to transcend their inherent limitations of static knowledge, enabling them to address various applications with high diversity and complexity [21]. Despite this potential, and researchers' attempts to incorporate various complex additional processes into RAG (such as knowledge preprocessing, fine-grained retrieval, and generating thought chains), LLMs still struggle with building human-like insights to complex tasks, which involve a motivated, continuous effort to understand connections (which can be among objects, numbers, concepts, and events) in order to anticipate their trajectories and act effectively.\nMost recent RAG implementations rely heavily on vector retrieval of text[1]. In this context, vector embeddings are numerical representations of words, phrases, or entire documents that capture their meanings in a semantic space. RAG systems identify potentially relevant text chunks or documents from knowledge sources by comparing the similarities between vector embeddings. While vector embeddings are effective for capturing surface-level semantic similarities, they are inefficient for all tasks. Indiscriminate retrieval of k text fragments with a query significantly increases the input length for the model. Additionally, they have several limitations in understanding the long-range association between various types of knowledge: 1. Shallow Correlation Capture:\nSimple vector-based matching might miss conceptual correlations, such as between the Global Financial Crisis and the 2008 Recession. 2. Difficulty in Aggregating Diverse Facts: Relying solely on"}, {"title": "2 RELATED WORKS", "content": "RAG aims to offer real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. An important factor is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge semantic integrity and meeting the required knowledge[2]. This low information density and low utility are due to the inherent limitations of semantic retrieval.\nKGs offer dynamic, explicit, and structured representations of knowledge. This structured knowledge representation is particularly beneficial for LLMs because it introduces a level of interpretability and precision in the knowledge that LLMs can access. Early approaches (Sun et al. [12], Peters et al. [9], Huang et al. [4], Liu et al. [8]) focused on embedding knowledge from KGs directly into the neural networks of LLMs. This embedding could occur either during the pretraining phase or the fine-tuning process. The intent was to infuse the models with rich, structured knowledge right from the foundational stages of model training. Despite the initial promise, integrating KGs directly into LLMs introduced challenges. As noted by Hu et al. [3], this integration leads to a reduction in the natural explainability of the models. Additionally, it makes updating the knowledge base more complex and less efficient, as any change in the KG requires retraining or significant adjustments to the model. More recent studies [5?] have shifted towards using KGs to augment LLMs externally rather than embedding the knowledge directly into the models. Those approaches involve translating relevant structured knowledge from KGs into textual prompts that are then fed to LLMs. The process typically follows a fixed pipeline where extra information from KGs is retrieved to enhance the LLM prompts. The integration of Knowledge Graphs (KGs) with Large Language Models (LLMs) offers numerous advantages but also meets several distinct challenges and limitations, such as incompleteness and ambiguity as discussed in the last section.\nIn this work, we aim to integrate both KG and unstructured data with LLM, leveraging the strengths of both to mitigate their respective weaknesses."}, {"title": "2.1 Retrieval Augmented Generation with Knowledge Graph", "content": "RAG aims to offer real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. An important factor is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee semantic integrity and meeting the required knowledge semantic integrity and meeting the required knowledge[2]. This low information density and low utility are due to the inherent limitations of semantic retrieval.\nKGs offer dynamic, explicit, and structured representations of knowledge. This structured knowledge representation is particularly beneficial for LLMs because it introduces a level of interpretability and precision in the knowledge that LLMs can access. Early approaches (Sun et al. [12], Peters et al. [9], Huang et al. [4], Liu et al. [8]) focused on embedding knowledge from KGs directly into the neural networks of LLMs. This embedding could occur either during the pretraining phase or the fine-tuning process. The intent was to infuse the models with rich, structured knowledge right from the foundational stages of model training. Despite the initial promise, integrating KGs directly into LLMs introduced challenges. As noted by Hu et al. [3], this integration leads to a reduction in the natural explainability of the models. Additionally, it makes updating the knowledge base more complex and less efficient, as any change in the KG requires retraining or significant adjustments to the model. More recent studies [5?] have shifted towards using KGs to augment LLMs externally rather than embedding the knowledge directly into the models. Those approaches involve translating relevant structured knowledge from KGs into textual prompts that are then fed to LLMs. The process typically follows a fixed pipeline where extra information from KGs is retrieved to enhance the LLM prompts. The integration of Knowledge Graphs (KGs) with Large Language Models (LLMs) offers numerous advantages but also meets several distinct challenges and limitations, such as incompleteness and ambiguity as discussed in the last section.\nIn this work, we aim to integrate both KG and unstructured data with LLM, leveraging the strengths of both to mitigate their respective weaknesses."}, {"title": "3 METHODOLOGY", "content": "The proposed method Explore & Examine on Graph first utilizes the LLM to evaluate the query and initializes proper reasoning starting points. Following this Tog2.0 can activate the internal knowledge and reading comprehension abilities of the LLM to efficiently identify multi-granularity local clues that support reasoning, which progressively assembles the supporting information chain and finally completes the global chain of thought from the question to the answer. In the following section, we will explain each step in detail."}, {"title": "3.1 Tog2.0 Initializtion", "content": "Selecting appropriate starting points for specific queries can facilitate much more streamlined reasoning. For example: \"Among the founders of Tencent company, who has been a member of the National People's Congress?\". In this case, a broad or poorly chosen point such as \"Member of the National People's Congress\" could lead to pitfalls of sifting through large amounts of irrelevant data and cause time-consuming and less focused exploration. An effective starting point would be to focus on the entity 'Founders of Tencent'. This principle is essential in reasoning tasks, especially in open-domain question answering, where the question is highly varied. Therefore, given a question q, Tog2.0 first performs Named Entity Recognition (NER) and Topic Prune (TP), which prompts the LLM to evaluate q and appearing entities, selecting topic entities \\(E_{\\text {topic }}=\\left\\{e_{1}, e_{2}, \\ldots, e_{N}\\right\\}\\) that are appropriate to serve as the starting point for the question.\nIn complex reasoning, the implicit correlation between the question and effective intermediate clue sentences often goes unrecognized by both sparse retrieval models and dense pre-trained retrieval models. To solve this limitation, we prompt the LLM to formulate clue queries \\(q_{i}^{\\prime}\\) based on the current context for every topic entity \\(e_{j}\\), which orients the next-step direction of exploring the relations and contexts. Using the question about Tencent mentioned above as the example again, based on the entity \"National People's Congress?\", the LLM may generate a clue-query that suggests gathering information about their political roles or affiliations."}, {"title": "3.2 Reasoning with Graph-driven Knowledge Retrieval", "content": "Next, we will introduce how Tog2.0 iteratively utilizes structured and unstructured knowledge for reasoning. Formally, in the (i + 1) th iteration, given the original question q, the clue queries from the ith iteration \\(Q^{i}=\\left\\{q_{1}^{i}, q_{2}^{i}, \\ldots, q_{N}^{i}\\right\\}\\), the topic entities \\(E_{\\text {topic }}=\\left\\{e_{1}, e_{2}, \\ldots, e_{N}\\right\\}\\) and their preceding triple paths \\(P^{i}=\\left\\{P_{1}^{i}, P_{2}^{i}, \\ldots, P_{N}^{i}\\right\\}\\), \\(P_{j}^{i}=\\left\\{p_{j 1}^{i}, p_{j 2}^{i}, \\ldots, p_{j k}^{i}\\right\\}\\), each iteration includes three steps: relation prune (RP), entity prune (EP), and examine and reasoning (ER). Note that i = 0 indicates the initialization phase and the \\(P^{0}\\) is empty.\nRelation Prune (RP): Based on q and \\(Q^{i}\\), we prompt the LLM to select the relations that are most likely to find entities containing helpful context information for solving q and that match the description of \\(Q^{i}\\). Unlike selecting relations for a single topic entity at a time, we provide GPT-3.5 with all topic entities within a single prompt. This approach not only reduces the number of API calls, thereby accelerating inference time, but also enables the LLM to simultaneously consider the interconnections between multiple reasoning paths, allowing it to make selections from a more global perspective. The selected relations for topic entity \\(e_{j}\\) are denoted as \\(R_{j}=\\left\\{r_{j 1}, r_{j 2}, \\ldots, r_{j W}\\right\\}\\), where W denotes the hyper-parameter width.\nEntity Prune (EP): Given a topic entity \\(e_{j}\\) and one of the selected relation \\(r_{j k}\\), Tog2.0 will identify all interconnected candidate entity nodes \\(\\left\\{e_{j k l}\\right\\}\\) within the Wiki Knowledge Graph (KG) and get their associated Wikipedia page documents \\(D_{j k l}\\) through locally deployed service. The document context of each candidate entity is initially segmented into appropriately sized chunks{\\(t_{j k l m}\\)}."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 Datasets and Metrics", "content": "We evaluated our method on two multi-hop KBQA datasets WebQSP [20] and QALD10-en[16], a multi-hop complex document QA dataset HotpotQA[19], and a fact verification dataset FEVER[13]. The evaluation metric for FEVER is Accuracy, while the metric for WebQSP & HotpotQA & QALD-10-en is Exact Match (EM)."}, {"title": "4.2 Baselines", "content": "We compare ToG2.0 with both widely used baselines and state-of-the-art methods to provide a more comprehensive overview: 1) Standard prompting (Vanilla Prompt) directly answers the question. 2) Chain-of-thought (CoT) [18] generates several intermediate rationales before the final answer to improve the complex reasoning capability of LLMs. 3) Chain-of-Knowledge (CoK) [7] a heterogeneous source augmented large language model framework. 4) Think-on-Graph (ToG) [11] a KG method that searches useful triples for reasoning."}, {"title": "4.3 Implementation Details", "content": "In this study, considering the experimental costs and for ease of comparison with other baselines, we conducted experiments on two LLMs: GPT-3.5-turbo and Llama-2-13b-chat. We used the OpenAI API to access GPT-3.5-turbo, while Llama-2-13B-chat was deployed on 8 A100-40G GPUs without quantization. Consistent with the ToG settings, we set the temperature parameter to 0.4 during exploration and 0 during reasoning. The maximum token length for generation was capped at 256. For context retrieval, we utilized the pre-trained BGE-embedding model without any fine-tuning. We choose Wikidata as the knowledge source for all experiments. During the TP, RC, relation pruning, and reasoning stages, we employed a 2-shot demonstration for all prompts."}, {"title": "4.4 Main Results", "content": "As shown in Table1, we analyze the performance of our proposed method, Tog2.0, in comparison with state-of-the-art baselines, including the Vanilla RAG strategy, the Chain-of-Thought (CoT), and the current SOTA baseline (CoK). The evaluation metrics include Exact Match (EM) for WebQSP, HotpotQA, and QALD-10-en, and Accuracy for FEVER.\nWe note that Tog2.0 outperforms other baselines on WebQSP, HotpotQA and QALD-10-en. Notably, on HotpotQA, it significantly surpasses the SOTA baseline CoK by 5.51%. Compared to the original ToG, Tog2.0 achieved a substantial improvement on HotpotQA (14.6%) and also demonstrated notable enhancements on other datasets (4.93% on WebQSP, 3.85% on QALD-10-en and 5.84% on FEVER). This demonstrates the advantages of our \"KG+context\" framework in addressing complex problems. Although the performance on the fact-checking dataset FEVER is slightly inferior to CoK, this may be due to CoK utilizing more knowledge sources and an additional LLM self-verification mechanism. To save computational costs and reduce inference latency, we ultimately decided not to use a self-verification mechanism, which could be applied based on application requirements in the future."}, {"title": "4.5 Ablation Study", "content": "To evaluate the contribution of each component in Tog2.0, we conducted comprehensive ablation experiments across all datasets.\nCompared to the performance on the other three datasets, on WebQSP, the effectiveness of Topic Prune (TP) is more pronounced, possibly due to the higher relative proportion of general entities in WebQSP questions, which tends to introduce more unnecessary noise.\nAlthough Relation Prune (RC) may slightly decrease the performance due to the increased difficulty for the LLM in understanding multiple tasks within a single prompt, the benefit is a significant reduction in the number of inferences and latency. Assuming a reasoning depth of N and a width of W, the complexity can theoretically be reduced from O(WN) to O(N).\nAdditionally, clue-query also brought relatively consistent improvements across each dataset, indicating that adaptive query optimization can help the LLM better understand the tasks. We also tested the vanilla RAG process and the basic Tog2.0 process on Llama-2-13B. It can be seen that on a less capable LLM, Tog2.0 can bring a greater performance improvement. This suggests that Tog2.0 might be more adaptable. Weaker LLMs often encounter bottlenecks when handling complex tasks, while Tog2.0 utilizes knowledge graphs as clues to optimize the reasoning path and reduce task complexity. It then uses entity context to further guide the model to focus on relevant information, thereby improving task understanding and response accuracy. In contrast, GPT-3.5, due to its higher inherent capabilities, may not exhibit as significant a performance improvement because it is already close to its performance ceiling."}]}