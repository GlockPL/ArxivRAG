{"title": "On Deep Learning for Geometric and Semantic Scene Understanding Using On-Vehicle 3D LIDAR", "authors": ["Li Li"], "abstract": "3D LiDAR point cloud data is crucial for scene perception in computer vision, robotics, and autonomous driving. Geometric and semantic scene understanding, involving 3D point clouds, is essential for advancing autonomous driving technologies. However, significant challenges remain, particularly in improving the overall accuracy (e.g., segmentation accuracy, depth estimation accuracy, etc.) and efficiency of these systems.\nTo address the challenge in terms of accuracy related to LiDAR-based tasks, we present DurLAR, the first high-fidelity 128-channel 3D LiDAR dataset featuring panoramic ambient (near infrared) and reflectivity imagery. Leveraging DurLAR, which exceeds the resolution of prior benchmarks, we tackle the task of monocular depth estimation. Utilizing this high-resolution yet sparse ground truth scene depth information, we propose a novel joint supervised/self-supervised loss formulation, significantly enhancing depth estimation accuracy.\nTo improve efficiency in 3D segmentation while ensuring the accuracy, we propose a novel pipeline that employs a smaller architecture, requiring fewer ground-truth annotations while achieving superior segmentation accuracy compared to contemporary approaches. This is facilitated by a novel Sparse Depthwise Separable Convolution (SDSC) module, which significantly reduces the network parameter count while retaining overall task performance. Additionally, we introduce a new Spatio-Temporal Redundant Frame Downsampling (ST-RFD) method that uses sensor motion knowledge to extract a diverse subset of training data frame samples, thereby enhancing computational efficiency.\nFurthermore, recent advancements in 3D LiDAR segmentation focus on spatial positioning and distribution of points to improve the segmentation accuracy. The dependencies on coordinates and point intensity result in suboptimal performance and poor isometric invariance. To improve the segmentation accuracy, we introduce Range-Aware Pointwise Distance Distribution (RAPiD) features and the associated RAPiD-Seg architecture. These features demonstrate rigid transformation invariance and adapt to point density variations, focusing on the localized geometry of neighboring structures. Utilizing LiDAR isotropic radiation and semantic categorization, they enhance local representation and computational efficiency.\nWe validate the effectiveness of our methods through extensive experiments and qualitative analysis. Our approaches surpass the state-of-the-art (SoTA) research in mIoU (for semantic segmentation) and RMSE (for depth estimation). All contributions have been accepted by peer-reviewed conferences, underscoring the advancements in both accuracy and efficiency in 3D LiDAR applications for autonomous driving.", "sections": [{"title": "Introduction", "content": "Light Detection and Ranging (LiDAR) is a pivotal perception technology widely applied in autonomous vehicles and advanced driver assistance systems (ADAS) [10, 11]. As autonomous driving technologies continue to evolve, the demand for enhanced 3D spatial and environment perception, especially in geometric understanding [12\u201314] and semantic segmentation [3, 15], has significantly increased.\nThe integration of deep learning with 3D LiDAR technology offers significant potential in enhancing the aforementioned perception. We explore the application of deep learning for both geometric and semantic scene understanding using on-vehicle 3D LiDAR systems. Geometric scene understanding involves interpreting the physical structure and spatial relationships within an environment, while semantic scene understanding focuses on classifying and labeling various objects and features within the scene. By combining these aspects, we aim to improve the perception capabilities of autonomous vehicles.\nThis research highlights the potential of deep learning to improve the accuracy [16, 17] and efficiency [16] of on-vehicle LiDAR data applications in these domains."}, {"title": "Motivations", "content": "Our motivation primarily comes from two perspectives: accuracy and efficiency. Enhancements in these two areas can significantly improve the overall performance of on 3D-LiDAR-related tasks, such as semantic segmentation [18\u201320], object detection [21, 22], and point cloud registration [23].\nIn terms of accuracy, despite various datasets proposed to evaluate LiDAR-based semantic [3, 5, 24] and geometric scene understanding tasks [9, 15], the lack of high-fidelity LiDAR data remains a major technical challenge impeding progress in this field [25, 26].\nHigh-resolution LiDAR technology can significantly enhance the accuracy of depth information [27\u201329], which is crucial for precise environment perception in autonomous driving. In Chapter 3, we develop a new dataset featuring a 128-channel high-fidelity LiDAR (DurLAR) and demonstrate its potential in autonomous driving applications through a new benchmark on DurLAR for more accurate monocular depth estimation [17].\nTo further improve the accuracy of the LiDAR-based methods, we harness the power of fully-supervised learning methodologies. We utilize the full potential of annotated data in fully-supervised learning (Chapter 5), enabling the training of models that are highly accurate and robust. The motivation for applying fully-supervised approaches in LiDAR semantic segmentation arises from their ability to leverage detailed ground truth data [20]. These methods [20, 30, 31] excel in environments where precision is paramount, as they minimize the potential for errors that can arise from insufficiently supervised or unsupervised techniques. By thoroughly training on well-labeled datasets, fully-supervised models can better generalize to new, unseen environments, thus providing more reliable and precise segmentation results.\nRecent advancements in 3D LiDAR segmentation often focus intensely on the spatial positioning and distribution of points for accurate segmentation [3, 18\u201320]. However, these methods, while robust in variable conditions, encounter challenges due to sole reliance on coordinates and point intensity, leading to poor isometric invariance and suboptimal segmentation [30, 32]. To tackle this challenge, we introduce Range-Aware"}, {"title": "Motivations for Efficiency", "content": "The availability of 3D LiDAR data across various applications [3, 5, 15, 17, 24], particularly autonomous driving, presents a dichotomy: while data is abundant, the annotation process is disproportionately expensive and time-consuming [16, 18, 19]. This disparity underscores an urgent call for methodologies that can capitalize on available data with greater computational frugality and reduced reliance on extensive labeling.\nThe motivation also stems from a clear trend in contemporary research methods that leverage large backbone architectures for higher accuracy [19, 20], which results in prohibitive computational costs. Those approaches are unsustainable for rapid deployment and scalability, particularly in practical and onboard applications where resources are constrained. Subsequently, there is a compelling incentive to innovate solutions that minimize computational overhead while maintaining, or even enhancing, task performance [18, 36].\nThis necessity sparks the requirements for exploring semi-supervised and weakly supervised learning paradigms, focusing on proposing models that can learn effectively from less data and with fewer computational demands (Chapter 4)."}, {"title": "Problem Definitions", "content": "In the following section, we define the twin primary problems on geometric and semantic scene understanding (i.e., monocular depth estimation and 3D LiDAR semantic segmentation) we are going to solve in this thesis."}, {"title": "Monocular Depth Estimation", "content": "Monocular depth estimation using LiDAR point cloud data as ground truth involves predicting the distance from a sensor to the surfaces of objects within its field of view. The goal is to estimate a depth map where each pixel value represents the distance between the sensor and the point in the scene corresponding to that pixel. LiDAR provides highly accurate distance measurements by emitting laser beams and measuring the time it takes for the reflected light to return. These measurements serve as the accurate ground truth for depth estimation tasks.\nFormally, the problem can be defined as follows: Given a set of LiDAR measurements $L = \\{(x_i, y_i, z_i, d_i)\\}_{i=1}^{N}$, where $x_i$, $y_i$, and $z_i$ represent the spatial coordinates of a point in 3D space, and $d_i$ is the distance from the sensor to the point, the task is to estimate a depth map $D$ for a given image $I$ of the same scene. The image $I$ has pixels at coordinate pairs $(u, v)$, and the depth map $D$ assigns a predicted distance $D(u, v)$ to each pixel.\nThe objective is to minimize the difference between the predicted depth map $D$ and the ground truth depth map $G$, derived from LiDAR measurements. This can be formulated as minimizing a loss function $\\mathcal{L}(D, G)$, where $G$ is constructed from the LiDAR measurements $L$. A common choice for $\\mathcal{L}$ is the Mean Squared Error (MSE):\n$\\mathcal{L}(D, G) = \\frac{1}{M} \\sum_{u, v} ||D(u, v) - G(u, v)||^2,$\nwhere $M$ is the number of pixels in the depth map, and $G(u, v)$ represents the ground truth depth at pixel $(u, v)$, interpolated or directly measured from $L$. The goal of depth estimation algorithms is to accurately predict $D$ so that $\\mathcal{L}(D, G)$ is minimized, indicating that the predicted depth values closely match the true distances measured by LiDAR.\nAs shown in Figure 1.1, the left column presents the input RGB images, which are"}, {"title": "3D LiDAR Semantic Segmentation", "content": "LiDAR-based point clouds, characterized by pointwise 3D positions and LiDAR intensity/reflectivity [5, 8, 17, 37], play a pivotal role in outdoor scene understanding, particularly in perception systems for autonomous driving. 3D semantic segmentation of LiDAR point clouds is equally important in scene understanding, facilitating applications such as autonomous driving [4, 20, 36, 38\u201341] and robotics [42\u201345]. It involves classifying each point in a 3D point cloud into predefined categories (e.g., cars, trees, buildings) based on their semantic meaning. This process is vital for machines to understand and interpret their surroundings accurately.\nGiven a 3D point cloud $P = \\{p_1, p_2, \\ldots, p_N\\}$ where each point $p_i$ is represented by"}, {"title": "Research Aims", "content": "As mentioned in Section 1.1, the integration of LiDAR technology with deep learning has revolutionized the field of autonomous vehicles and ADAS, providing pivotal advancements in 3D spatial and environmental perception. Despite significant progress, there remains a critical need to enhance both the accuracy and efficiency of 3D LiDAR-based applications, particularly in geometric and semantic scene understanding. Our research is driven by these dual objectives, focusing on the following aims:\nEnhance Accuracy in 3D LiDAR Perception:\n\u2022 Geometric Scene Understanding: Develop advanced methodologies to improve the interpretation of physical structures and spatial relationships within a scene. Leveraging high-resolution LiDAR data, we aim to refine depth information and geometric accuracy, which are crucial for precise environment perception in autonomous driving.\n\u2022 Semantic Scene Understanding: Utilize fully-supervised learning techniques to maximize the potential of annotated data, enabling the training of highly accurate and robust models. By incorporating novel and robust features such as Range-Aware Pointwise Distance Distribution (RAPiD), we aim to enrich input data and improve the descriptive power of deep learning models, leading to more accurate and reliable 3D semantic segmentation.\nImprove Efficiency in 3D LiDAR Perception:\n\u2022 Computational Efficiency: Address the high computational costs associated with large backbone architectures in current state-of-the-art models. By exploring semi-supervised and weakly supervised learning paradigms, we aim to develop models that learn effectively from less data and with reduced"}, {"title": "Contributions", "content": "The main contributions of this thesis are summarized as follows:\n\u2022 A novel large-scale dataset featuring high-fidelity 3D LiDAR (128 channels), marking the first autonomous driving dataset to include LiDAR panoramic ambient and reflectivity imagery. Additionally, it presents a monocular depth estimation benchmark comparing SOTA methods on varying resolutions of LiDAR data, demonstrating improved depth estimation performance with higher LiDAR resolution and enhanced data availability (Chapter 3).\n\u2022 A novel semi-supervised methodology for 3D LiDAR semantic segmentation that significantly reduces the network parameters (enhancing the efficiency) while providing superior accuracy. Our approach reduces model complexity, with a 2.3\u00d7 reduction in parameters and 641\u00d7 fewer multiply-add operations. It beats SOTA in terms of mean Intersection-over-Union (mIoU), achieving 59.5 mIoU with only 5% labeled data on SemanticKITTI and 58.1 mIoU on ScribbleKITTI (Chapter 4).\n\u2022 A novel open-source network architecture RAPiD-Seg for better segmentation accuracy, with a supporting training methodology that utilizes RAPiD features"}, {"title": "Thesis Structure", "content": "This thesis is structured to systematically explore and present the advancements in deep machine learning for geometric and semantic scene understanding using on-vehicle 3D LiDAR, with a particular focus on depth estimation and semantic segmentation. The organization of the chapters is designed to take the reader through the motivation, literature, methodology, and findings of the research coherently and logically.\nIn Chapter 1, we set the stage by discussing the motivations behind the research. It emphasizes the need for improvements in accuracy and efficiency in LiDAR geometric and semantic scene understanding, particularly in the domains of autonomous driving and Advanced Driver Assistance Systems (ADAS). In Section 1.2, we outline the primary problems to address \u2013 i.e., monocular depth estimation and 3D LiDAR semantic segmentation \u2013 providing a high-level overview of the methodology employed.\nChapter 2 delves into existing research on LiDAR-bsed geometric and semantic scene understanding, covering topics such as the current status of LiDAR, various 3D LiDAR datasets for autonomous driving, and the challenges posed by adverse weather conditions and rolling shutter effects. We also explore the contemporary monocular depth estimation and 3D LiDAR semantic segmentation methodologies, invariant features, and the relevance of these aspects to the contributions of this thesis.\nIn Chapter 3, the focus shifts to the development of a novel large-scale dataset featuring high-fidelity 128-channel LiDAR data. We provide a detailed description of the sensor setup, data collection methods, and the unique features of the dataset, such as panoramic ambient and reflectivity imagery. We also present the evaluation results of monocular depth estimation benchmarks with this high-fidelity LiDAR dataset.\nIn Chapter 4, we introduce a novel semi-supervised methodology that significantly reduces network parameters while maintaining superior 3D semantic segmentation accuracy. We propose SDSC to reduce the model complexity and computational costs. Chapter 4 also includes detailed experimental setups, results, and ablation studies to demonstrate the efficacy of the proposed method, i.e., Less is More.\nFollowing this, Chapter 5 discusses the development of the RAPiD-Seg network architecture. It utilizes Range-Aware Pointwise Distance Distribution (RAPiD) features, which enhance the descriptive power (robust to rigid transformation) of the input point cloud in deep learning. We also provide a thorough evaluation of the RAPiD-Seg architecture, showcasing its performance on various benchmarks and highlighting its contributions to the field of high-accuracy 3D LiDAR segmentation.\nIn Chapter 6, we review the main contributions, summarizing the advancements made in the development of high-fidelity LiDAR datasets (Chapter 3), efficient network architectures (Chapter 4), and novel and accurate segmentation methodologies (Chapters 4 and 5). We also outline potential directions for future research, emphasizing areas where further improvements and innovations can be made."}, {"title": "Light Detection and Ranging (LiDAR)", "content": "Light Detection and Ranging (LiDAR) operates by emitting eye-safe laser pulses to generate a 3D view of the environment, enabling machines and computers to perceive the world with high accuracy. Essentially, a typical LiDAR sensor sends out light waves that reflect off objects in the vicinity and then return to the sensor. The time elapsed for each pulse to bounce back is recorded and used to determine the distance each pulse has traveled. By performing this operation millions of times every second, LiDAR can produce a detailed and real-time 3D representation of the surrounding area, known as a point cloud. This data can then be processed by an onboard computer and computer vision based algorithms to facilitate safe navigation through the environment.\nTo provide a comprehensive understanding of LiDAR sensors, it is crucial to fully explain their properties: intensity, ambient, and reflectivity. This includes what they are, the differences between them, what they represent, and how they are measured.\nIntensity refers to the strength of the returned laser pulse. When a LiDAR sensor emits a laser pulse, it travels to an object and reflects back to the sensor. The intensity is a measure of the power of this reflected signal. It can provide information about the"}, {"title": "Current Status of Automotive LiDAR", "content": "LiDAR technology in the automotive sector is advancing through a variety of sensor types, each harnessing distinct technologies as showcased in Table 2.1. Many companies such as Innoviz, Continental, and Quanergy are focusing on the development of various LiDAR technologies, including Mechanical Spinning, MEMS, Flash, and OPA LiDAR.\nMechanical Spinning LiDAR, utilized by companies like Velodyne, Valeo, Ouster, Hesai, and Robosense, employs rotating mirrors or the entire sensor unit to achieve a 360-degree field of view, commonly using Near Infrared wavelengths (NIR, 750-1000 nm wavelength) for Time-of-Flight (ToF) applications. MEMS LiDAR, adopted by Innoviz, Hesai, Robosense, and AEye, uses micro-electro-mechanical systems to steer the laser beam, offering a compact and cost-effective solution for both NIR and Short-Wave Infrared wavelengths (SWIR, 1000-2500 nm wavelength). Flash LiDAR, applied by Continental, Xenomatix, and Ouster, uses a broad laser pulse to illuminate the entire scene and capture real-time 3D data, also typically employing NIR wavelengths. OPA (Optical Phased Array) LiDAR, utilized by Quanergy and Cruise (Strobe), relies on electronically steering the laser beam with no moving parts, providing robustness and adaptability. Additionally, companies like Aurora (Blackmore) are advancing FMCW (Frequency-Modulated Continuous Wave) LiDAR technology, which operates at a 1550 nm wavelength, measuring frequency shifts to determine both distance and velocity"}, {"title": "3D LiDAR Datasets for Autonomous Driving", "content": "Datasets are crucial for the swift advancement of applications and utilization of 3D data through deep learning networks. Multiple autonomous driving task datasets provide 3D LiDAR data for outdoor environments (Table 2.2). These datasets not only offer a high vertical resolution (Section 2.2.1) for detailed environmental representation but also address inherent challenges such as the rolling shutter effect, which distorts data in dynamic scenarios. Furthermore, the robustness of these datasets under adverse weather conditions is essential for ensuring consistent sensor performance, underscoring the importance of including diverse environmental data. In summary, the LiDAR resolution, distortion mitigation, weather resilience, and data diversity, are all crucial for developing reliable and adaptable algorithms for 3D geometric and semantic scene understanding in autonomous driving applications."}, {"title": "High Vertical Resolution", "content": "The vertical resolution of 3D LiDAR refers to the density of laser beams projected in the vertical dimension, indicating how finely the LiDAR system can discern features at different heights. Higher vertical resolution means the system can capture more detailed and precise measurements of objects and terrain by sending and receiving a greater number of laser pulses over vertical angles. This allows for a more nuanced 3D representation of the environment, essential for applications requiring detailed spatial awareness, such as autonomous driving, aerial mapping, and environmental monitoring.\nHigh vertical resolution LiDAR is not present in existing autonomous driving datasets (see Table 2.2). The vertical resolution of LiVi-Set [60] and nuScenes [5] is 32 channels, while the ONCE dataset [63] features a vertical resolution of 40 channels. In addition, the Stanford Track Collection [65], Waymo Open Dataset (WOD) [37], Argoverse 2 [56],"}, {"title": "Panoramic Imagery Derived from LiDAR", "content": "Panoramic imagery derived from LiDAR technology offers a revolutionary approach to capturing and understanding the environment in autonomous driving applications. Utilizing LiDAR, we are able to generate both panoramic ambient imagery and panoramic reflectivity imagery (Figure 2.2), each providing unique advantages for enhancing visibility and detail under various conditions. Note that we are the first to include ambient and reflectivity imagery in the dataset (Chapter 3).\nPanoramic ambient imagery, as shown in Figure 2.2 (a), is the 360-degree images that capture ambient light conditions in the near-infrared spectrum. This type of imagery provides comprehensive visibility of the environment, even in low light conditions, which is particularly beneficial for autonomous driving applications.\nIn our DurLAR dataset (Chapter 3), panoramic ambient imagery is captured using near-infrared light with wavelengths between 800-2500 nm. This allows the system to produce images that can be effectively used day and night, offering visibility and"}, {"title": "Rolling Shutter Effect", "content": "The rolling shutter effect (Figure 2.3), a phenomenon prevalent in a wide array of imaging and sensing devices, particularly affects analogue spinning Light Detection and Ranging (LiDAR) systems such as those developed by Velodyne [15]. This effect arises due to the sequential capturing of image lines rather than the simultaneous acquisition of the entire scene, leading to distortions when either the sensor or the objects within the scene are in motion [67]. This can result in geometric distortions such as skewed or curved objects in dynamic scenes, especially during fast sensor or object motion. Such distortions can significantly impact the fidelity and accuracy of the data collected by these LiDAR systems, posing challenges for applications that rely heavily on precise spatial measurements and reconstructions, including autonomous driving, 3D mapping,"}, {"title": "Overcoming Adverse Weather", "content": "In adverse weather conditions, such as rain, fog, snow, and dust, the performance of LiDAR systems is significantly compromised. The presence of opaque particles in these conditions distorts and scatters the LiDAR light pulses, leading to reduced visibility and increased transmission loss [71]. Specifically, adverse weather not only increases"}, {"title": "Data Diversity", "content": "Data diversity within any dataset helps the generation of more universally trained models that can operate successfully under a variety of scenarios. Previous work considers the diversity in their dataset curation [5, 57, 61, 64], but fails to collect data under diverse conditions over the same driving route (see Table 2.2), e.g., traffic level, times of day, weathers, etc."}, {"title": "Ground Truth Depth", "content": "While many existing datasets, such as the Stanford Track Collection [65], Sydney Urban Objects [66], Cityscapes [9], Oxford RobotCar [64], LiVi-Set [60], nuScenes [5] and H3D [58], include LiDAR data, they do not provide ground truth depth data directly usable for depth estimation tasks. This limitation arises from several factors related to how LiDAR data is captured and processed.\nLiDAR data often suffers from sparsity. A LiDAR sensor typically emits laser pulses in a specific pattern, capturing only discrete points in space rather than continuous depth information. This results in point clouds with varying density, which may not cover all surfaces and objects in the environment uniformly (Section 2.2.1). Consequently, the resulting data may be too sparse to serve as ground truth for dense depth estimation models, which require comprehensive and consistent depth information across the entire scene.\nLiDAR data is subject to noise and inaccuracies due to environmental factors. Factors such as surface reflectivity, the presence of transparent or absorptive materials (Section 2.2.2), and weather conditions (Section 2.2.4) can introduce errors in the measured distances. These inaccuracies can degrade the quality of the depth information, making"}, {"title": "Point Cloud Representations and Embeddings", "content": "Point Cloud Representations and Embeddings serve as the cornerstone for a myriad of LiDAR-based tasks, underpinning the advancements in how machines perceive and interact with the 3D world. The rich, spatially encoded data obtained from LiDAR sensors can be intricately represented in two primary forms: point-based and voxel-based representations [19, 20, 38, 46, 47, 73, 74]. Each representation offers distinct advantages tailored to leverage the unique characteristics of LiDAR data, catering to specific application requirements."}, {"title": "Monocular Depth Estimation", "content": "Monocular depth estimation represents a pivotal challenge in computer vision, aiming to reconstruct a detailed depth map for each pixel from a single RGB image. This task is fundamental for various applications, including autonomous driving [82, 83], augmented reality, and robotics [83], where understanding the three-dimensional structure of the environment from a single viewpoint is crucial.\nSelf-supervised methods have emerged as a powerful strategy for training depth estimation models without the need for explicitly labeled depth maps. These methods exploit consistency constraints within monocular RGB image sequences [1, 82\u201385, 85\u201389], stereo image pairs [90\u201393], or synthetic data [94, 95]. By harnessing temporal dynamics in multi-frame architectures [1, 83, 96\u2013101], these approaches leverage temporal information to refine depth predictions dynamically. This adaptation requires complex calculations across multiple frames, significantly increasing the computational demand.\nMoreover, the advent of multi-view stereo (MVS) techniques [1, 102\u2013109] has introduced capabilities for depth estimation from unordered image collections [1], which means the images do not need to be captured in a specific sequence or from a fixed"}, {"title": "Temporal Redundancy", "content": "Temporal redundancy is a significant characteristic of both video and radar sequences, underscored by the frequent similarity between neighboring frames, especially in autonomous driving datasets such as KITTI [8], nuScenes [5], and Waymo [37]. This similarity arises due to the minimal temporal interval between successive frames\u2014often as short as 0.1 seconds\u2014resulting in a homogeneous scene diversity across different dataset splits. Such redundancy inherently leads to analogous detection and segmentation performance when employing identical training iterations, as observed in recent studies [125]. To mitigate this issue, prior research efforts [126, 127] have adopted uniform sampling techniques across the entire Waymo training set to generate varied fine-tuning splits, exemplified by strategies such as selecting every alternate frame to achieve a 50% data subset.\nIn the domain of semi-supervised 3D LiDAR segmentation, prevalent methodologies [4, 18] typically rely on a passive, uniform sampling approach to sift through unlabeled points within a fully-labeled point cloud dataset. Conversely, active learning frameworks endeavor to efficiently navigate through this redundancy. They aim to minimize necessary annotation or training efforts by strategically choosing informative and diverse sub-scenes for labeling, thus effectively leveraging the underlying data redundancy [128\u2013130]. These frameworks underscore the potential of reducing manual labeling workload and improving model performance by focusing on variably informative data points, thereby aligning with the objectives of efficient data utilization and enhanced"}, {"title": "Invariant Features", "content": "The rapid advancement in 3D sensing technologies has led to an exponential increase in the use of point cloud data. A critical challenge in leveraging point cloud data effectively is developing methods that are invariant to transformations, particularly rotations and translations [34, 131, 132].\nInvariant features refer to properties or characteristics of the data that remain unchanged under transformations such as rotation [32, 131\u2013134], translation [135], or reflection [34, 35]. By extracting invariant features, algorithms can recognize and understand the underlying structure of the data without being affected by how it is presented. This capability is essential for tasks that rely on accurate object recognition, localization, and mapping, as it enables consistent performance regardless of changes in the viewpoint or configuration of the sensors collecting the data."}, {"title": "Transformation-Invariant Features", "content": "Yu et al. [131] introduce a rotation-invariant transformer for point cloud matching, proposing an architecture that achieves extrinsic rotation invariance by learning to describe local"}, {"title": "Isometry Invariant Features", "content": "Isometry invariant features are specific types of features used in the analysis of geometric data, such as point clouds, that remain unchanged under isometric transformations. These transformations include rotations, translations, and reflections, which preserve distances and angles within the point-wise data [34, 35]. Isometry invariant features are particularly valuable in computer vision, chemistry, and similar fields where the exact positioning and orientation of objects may vary, but their fundamental geometric properties do not.\nPointwise Distance Distribution (PDD) captures the local context of each point in a unit cell by enumerating distances to neighboring points in order. It is an isometry invariant proposed by Widdowson & Kurlin [35] to resolve the data ambiguity for periodic crystals, demonstrated through extensive pairwise comparisons across atomic 3D clouds from high-level periodic crystals of periodic structures [33\u201335]. Though the effectiveness of PDD in periodic crystals and atomic clouds has been proved by the aforementioned"}, {"title": "LiDAR-Based Semantic Segmentation", "content": "LiDAR-based semantic segmentation involves the classification of 3D point clouds obtained from LiDAR data [3, 5, 15, 17] into various categories [3] such as buildings, vehicles, roads, vegetation, pedestrian, etc."}, {"title": "Computationally Efficient Segmentation", "content": "Recent advancements in LiDAR segmentation have focused on improving efficiency while maintaining or enhancing segmentation accuracy. These efforts can be categorized based on their contributions, such as novel network architectures, efficient data representations, and advanced augmentation strategies.\nEfficient network architectures are important in advancing computationally efficient LiDAR segmentation. For instance, the Center Focusing Network (CFNet) [140] leverages the center focusing feature encoding (CFFE) mechanism to explicitly model relationships between LiDAR points and virtual instance centers, significantly enhancing real-time segmentation performance. CFNet also incorporates a fast center deduplication module (CDM) to streamline instance detection, outperforming previous methods in both efficiency and accuracy on SemanticKITTI and nuScenes datasets.\nDepthwise Separable Convolution (DSC) [141] consists of a depthwise convolution followed by a pointwise convolution. This structure reduces both model size and com-"}, {"title": "Semi-Supervised Learning (SSL) LiDAR Segmentation", "content": "Semi-Supervised Learning (SSL) for LiDAR semantic segmentation involves using a small amount of labeled or weakly-labeled (e.g., scribble annotations [19]) point cloud data in conjunction with a large amount of unlabeled point cloud data during training, thereby leveraging aspects of both fully supervised learning and weak supervision. Numerous approaches have been explored for LiDAR semantic segmentation. Projection-based approaches [18, 42\u201344, 151\u2013153] make full use of 2D-convolution kernels by using range or other 2D image-based spherical coordinate representations of point clouds. Conversely, voxel-based approaches [4, 18, 20, 154] transform irregular point clouds to regular 3D grids and then apply 3D convolutional neural networks with a better balance of the efficiency and effectiveness. Pseudo-labeling is generally applied to alleviate the side effect of intra-class negative pairs in feature learning from the teacher network [4, 18, 155, 156]. However, such methods only utilize samples with reliable predictions and thus ignore the valuable information that unreliable predictions carry. In our work, we combined a novel SSL framework with the mean teacher paradigm [157], demonstrating the utilization of unreliable pseudo-labels to improve segmentation performance."}, {"title": "Fully-Supervised Learning (FSL) LiDAR Segmentation", "content": "LiDAR-Based Semantic Segmentation [16, 18\u201320, 38, 148, 153\u2013156, 158\u2013167] is fundamental for LiDAR-driven scene perception, aiming to label each point in a point cloud sequence. The majority of the approaches [20, 38, 154, 156, 162] solely rely on the point-based features of the point cloud, such as SPVCNN [154] which introduces a point-to-voxel branch, using combined point-voxel features for segmentation. Cylinder3D [20] proposes cylindrical partitioning with a UNet [77] backbone variant. LiM3D [16] utilizes coordinates combined with surface reflectivity attributes. Overall, such prior work relies solely on point-based features such as coordinates and intensity of the points, lacking an effective fusion mechanism, resulting in suboptimal performance [168\u2013170]. They are also susceptible"}, {"title": "Evaluations and Metrics", "content": "In the realm of computer vision, particularly in depth estimation and 3D semantic segmentation, systematic analysis of evaluation metrics is crucial for enhancing model performance and applicability. By examining these metrics, researchers can identify the strengths and weaknesses of their models, facilitating iterative improvements to ensure robust and reliable outputs in practical scenarios. Evaluating depth estimation encompasses understanding various aspects of model performance, such as error magnitude and prediction accuracy. Similarly, the evaluation of 3D semantic segmentation focuses on segmentation accuracy across all classes, addressing imbalanced data distributions and ensuring fair performance assessment. These evaluations not only quantify model performance but also guide the development of advanced algorithms and corrective strategies.\nThe evaluation of depth estimation methods is critical to understanding and improving the accuracy and reliability of predictions. We delve into the various metrics employed to assess the performance of depth estimation models. Accurate evaluation metrics are essential as they provide quantitative measures of how closely the predicted depth values align with the ground truth. We introduce principal metrics such as Absolute Relative Error (Abs Rel), Squared Relative Error (Sq Rel), Root Mean Squared Error (RMSE), RMSE log, and Threshold Accuracy \u03b4, each serving a unique purpose in the comprehensive evaluation of model performance.\nConsider the notation used in the metrics (Equations (2.1) to (2.5)), where $d_i$ represents the ground truth depth value at the $i$-th pixel, $\\hat{d}_i$ denotes the predicted depth value at the $i$-th pixel, and $n$ is the total number of pixels for evaluation. These variables are used to define the principal metrics in depth estimation:\n\u2022 Absolute Relative Error (Abs Rel): the metric in Equation (2.1) calculates the mean absolute error relative to the ground truth depth values. It provides a sense of how much the predicted depth values deviate from the actual values in relative"}, {"title": "Relevance to Contributions", "content": "Based on the overview of the current literature on the variety of subjects presented in this chapter, we will outline the novel contributions of this thesis in the following chapters.\nIn Chapter 3, we present DurLAR, a high-fidelity 128-channel 3D LiDAR dataset with panoramic ambient (near infrared) and reflectivity imagery. Existing datasets on autonomous driving with high-fidelity LiDAR are very limited (Section 2.2). Our driving platform is equipped with a high resolution 128 channel LiDAR, a 2MPix stereo camera, a lux meter, and a GNSS/INS system. Ambient and reflectivity images are made available along with the LiDAR point clouds to facilitate multi-modal use of concurrent ambient"}, {"title": "Introduction", "content": "On the contrary", "15": "ensuring that the data can be parsed using both our DurLAR development kit and the official KITTI tools", "features": "n\u2022 High vertical resolution LiDAR with 128 channels, which is twice that of any existing datasets (Table 2.2), full 360\u00b0 depth, range accuracy to \u00b12 cm at 20-50 m.\n\u2022 Ambient illumination (near infrared) and reflectivity panoramic imagery (Section 2.2.2) are made available in the Mono16 format (2048 \u00d7 128 resolution), with this being only dataset to make this provision (Table 2.2).\n\u2022 Zero temporal mismatch or shutter effects, as our flash LiDAR captures all 128 channels simultaneously, and the data layers are perfectly spatially correlated [70"}]}