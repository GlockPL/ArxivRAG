{"title": "Human-Agent Joint Learning for Efficient Robot Manipulation Skill Acquisition", "authors": ["Shengcheng Luo", "Quanquan Peng", "Jun Lv", "Kaiwen Hong", "Katherine Rose Driggs-Campbell", "Cewu Lu", "Yong-Lu Li"], "abstract": "Employing a teleoperation system for gathering demonstrations offers the potential for more efficient learning of robot manipulation. However, teleoperating a robot arm equipped with a dexterous hand or gripper, via a teleoperation system poses significant challenges due to its high dimensionality, complex motions, and differences in physiological structure. In this study, we introduce a novel system for joint learning between human operators and robots, that enables human operators to share control of a robot end-effector with a learned assistive agent, facilitating simultaneous human demonstration collection and robot manipulation teaching. In this setup, as data accumulates, the assistive agent gradually learns. Consequently, less human effort and attention are required, enhancing the efficiency of the data collection process. It also allows the human operator to adjust the control ratio to achieve a trade-off between manual and automated control. We conducted experiments in both simulated environments and physical real-world settings. Through user studies and quantitative evaluations, it is evident that the proposed system could enhance data collection efficiency and reduce the need for human adaptation while ensuring the collected data is of sufficient quality for downstream tasks.", "sections": [{"title": "1 Introduction", "content": "The long-term vision in the field of robot learning has been to enable robots to perform diverse tasks at a human level in the physical world. Recently, significant progress has been made toward this goal by learning robot manipulation policies from demonstrations. Previous studies have utilized teleoperation systems [1, 2, 3, 4, 5, 6] to collect human demonstrations, and learning-based policies [7, 8, 9] have been formulated using the gathered data. Despite the notable advancements, several challenges still need to be addressed. For example, in vision-based teleoperation systems, even with state-of-the-art 3D hand pose estimation algorithms [10, 11, 12, 13], errors persist that significantly affect the teleoperation. Additionally, discrepancies between the structures of human hands and robot end-effectors, along with the lack of haptic feedback during contact-rich manipulation, also pose challenges. As a result, current teleoperation systems require human operators to practice extensively to adapt to these differences and gather the necessary data. This means that many human adaptations are essential. Furthermore, to meet the data requirements of robotic systems, humans need to collect large amounts of data, making this process very burdensome.\nNaturally, a question was raised: in data-collection, how to make human adaptation less or even free while keeping the data quality? Here, we aim to address this question and argue that human-agent joint learning can help. That said, an effective and efficient teleoperation system should be designed to preferentially capture the operator's intentions for directing a robot end effector and pose the main frame, while concurrently enabling an autonomous agent to help us ensure motion stability and interpolate the details. To this end, we propose a framework that achieves shared control between the"}, {"title": "2 Related Works", "content": "Teleoperation System. Data has always been a crucial foundation, and robots are no exception. Teleoperation serves as a significant source for collecting robot data [7, 21, 22, 23, 24, 25]. Some works achieve teleoperation through wearable devices [1, 2, 3, 4, 26], and vision-based teleoperation systems offer a low-cost and easily developed alternative [5, 6, 27, 28]. For instance, Li et al. [28] utilizes neural networks for markerless vision-based teleoperation of dexterous robotic hands from depth images. Handa et al. [5] set up a vision-based teleoperation system to control the Allegro Hand, accomplishing various contact-rich manipulation tasks in the real world. Recently, Qin et al. [6] introduced AnyTeleop, a unified teleoperation system designed to accommodate various arms, hands, realities, and camera setups within a singular framework. In this paper, we introduce a joint learning paradigm to assist teleoperation by sharing control between the human operator and a learning-based agent, aiming to improve the efficiency of the teleoperation process.\nHuman Robot Cooperation. Collecting fine-grained human demonstration data for robotic manipulation is an effective but labor-intensive and time-consuming way to enable robots to complete a wide range of tasks [29, 30]. Previous work uses shared autonomy to assist people with disability in performing tasks by arbitrating human inputs and robot actions [31]. Many of the shared autonomy algorithms aim to estimate human intents from a set of pre-defined goals [32, 33, 34, 35] or by mapping low-dimension control input to high-dimension robot actions [31, 36]. In this work, we introduce a system that enables shared control between the human and assistive agent to facilitate the process of data collection and robot learning."}, {"title": "3 Technical Approach", "content": "The proposed system enables human operators to control the robot using a teleoperation system to gather training data (Sec. 3.1). Subsequently, utilizing the collected data, we train an agent (Sec. 3.2) to establish shared control between the human operator and the learned agent, thereby enhancing the efficiency of the data collection process (Sec. 3.3). Similar to the concept of \u201cbootstrapping\u201d, as more data accumulates, our system raises the control ratio of the learned agent, thereby reducing the effort required from human operators. This, in turn, enables us to collect even more data and continue improving the system iteratively. Moreover, we offer the option to transition the shared control agent to full autonomy once sufficient data is acquired (Sec. 3.4)."}, {"title": "3.1 Teleoperation System.", "content": "Our pipeline initially captures the raw sensory signal I. Human hand pose $P_h\\in \\mathbb{R}^{20\\times3}$ can be obtained from the captured signal using off-the-shelf 3D hand pose estimation [10, 11, 13]. The pose $P_h$ consists of the positions of 20 keypoints. Then, employing an inverse kinematic function $f_{IK}$, we compute the action of the robot $a \\in \\mathbb{R}^m: a = f_{IK}(P_h, P_{h+1})$, where it is calculated upon the change in the hand pose. Given this teleoperation system, the human operator will move the hand to produce a sequence of hand poses $\\{P_h\\}_{i=0}^T$ to teleoperate the robot with an action sequence $\\{a_i\\}_{i=0}^T$ to achieve the task T. The trajectory $\\{(s_i, a_i)\\}_{i=0}^T$ is the collected human demonstration data, where $s \\in \\mathbb{R}^n$ (here n = 18) is the robot state, could be used for downstream tasks."}, {"title": "3.2 Diffusion-Model-Based Assistive Agent.", "content": "After getting the data, we train a diffusion-model-based assistive agent to learn how to assist the human in collecting data in a shared control manner. We follow the Denoising Diffusion Probabilistic Model (DDPM) [18] training paradigm to construct the diffusion-model-based assist agent. The"}, {"title": "3.3 Data Collection with Shared Control", "content": "During data collection, the proposed system offers the option to control the robot in a shared control mode rather than directly applying the collected action $a^h$ from the teleoperation system. The classical shared autonomy method is achieved through the equation [32]:\n$a^o = \\gamma a^a + (1 - \\gamma) a^h,$\nwhere $a^a$ is generated by the learned agent. Considering that the agent operates as a diffusion policy (Fig. 2), we blend the action from the human with the forward and reverse processes. Given action $a^h$, a forward process diffuses the action as follows: $a^k = a^h + \\epsilon_k$. Subsequently, a reverse process denoises the action $a^k$:\n$a^o = f(a^k|s, k).$\nBy applying action $a^o$, the control of the robot is shared between the human and the diffusion-model-based assistive agent. We can adjust the control ratio $\\gamma = k/K$ between the human operator and"}, {"title": "3.4 Intergrating Data Collection and Manipulation Learning.", "content": "We outline the overall process in Algo. 1. The assistive agent is trained in four steps as follows:\nStep 1. Initially, we collect a dataset for pre-training agent f under full manual control by human operators, i.e., with the control ratio \u03b3 = 0.\nStep 2. Given the initial dataset, we train a less capable assistive agent to aid in further data collection. The training process has been formulated in Eq. 5 and Eq. 6, where a neural network \u03b8 is trained to predict noise \u03f5 out of the noisy action ak.\nStep 3. The trained agent assists in a second data collection round, aiming for higher efficiency and success. We refine the agent using data from both rounds to enhance its performance. This cycle repeats until the agent achieves full autonomy and the required data volume is collected.\nCurrently, the adjustment of the control ratio \u03b3 is guided by intuitive assessments and careful monitoring of success rates. When an agent, trained with recently gathered data, exhibits a marked improvement in success rate relative to its prior performance, this enhancement is taken as a cue to fine-tune \u03b3, thus refining control effectiveness. The assessment of the agent's performance, followed by adjustments to \u03b3, relies on empirical data and the agent's performance in practice. This method ensures that decisions regarding the adjustment of \u03b3 are firmly rooted in data, thus harmonizing the control strategy with the observed outcomes. It is posited that an agent reaches full autonomy when it achieves a success rate that aligns with or surpasses current state-of-the-art benchmarks."}, {"title": "4 Experiments", "content": "Tasks. We adopt six multi-stage manipulation tasks (Fig. 3). Pick-and-Place aims at picking an object on the table and placing it into a container. Articulated-Manipulation's objective for the dexterous hand is to grasp and unscrew a door handle to open it, while for the gripper, it is to grab a drawer handle and pull the drawer open. Push-cube requires the robot to push the cube to the target position. Tool-Use aims at picking a hammer and using it to drive a nail into a board.\nEfficiency of Data Collection. Our proposed system leverages shared control between human operators and learned agents to enhance the efficiency of data collection. To learn how the assistant agent could improve the data collection process, we conducted a user study.\nIn the user study, 10 human operators participate, collecting data under two modes: one where control is shared between the operator and the learned agent (w/ Ours), and the other where control"}, {"title": "5 Conclusion", "content": "In this paper, we introduce a novel human-agent joint learning paradigm that enables simultaneous human demonstration collection and robot manipulation teaching. This approach allows the human operator to share control with a diffusion-model-based assistive agent within a vision-based teleoperation system to control multiple robot end-effectors such as grippers and dexterous hands. Given our paradigm, the human operator can reduce the effort spent on data collection and adjust the control ratio between the human and agent based on different scenarios. Our system offers a more efficient and flexible solution for data collection and robot manipulation learning via teleoperation."}, {"title": "6 Appendix", "content": "6.1 Implementation details\nHere we lay down the details of the data collection, training, and testing process. More technical details are given here to illustrate our method and implementations better."}, {"title": "6.1.1 Shadow Hand and Parallel Gripper Teleoperate System.", "content": "To adapt to Isaac Gym and our vision system, we made certain modifications to the XML file of the Shadow Hand. We followed [39, 6, 40], removed the entire arm part, and added six degrees of freedom to the base mount of the Shadow Hand. This allows it to move freely in the virtual environment without depending on a base. Similarly, to obtain the rigid body Jacobian matrices of the five fingertips of the Shadow Hand, we added a massless rigid body to the tips of all five fingers of the Shadow Hand. This facilitates direct inverse kinematics calculations for the entire finger. In inverse kinematics (IK) calculations, we employed the Damped Least Squares (DLS) method [41, 42], this approach helps to prevent instability issues when approaching singularity points. Additionally, the DLS method supports real-time applications because it can provide fast and stable solutions, which is particularly crucial for teleoperation systems. Focusing solely on the five fingertips and wrist is regarded as the most balanced approach between computational efficiency and the precision required for complex"}, {"title": "6.1.2 Baselines.", "content": "In this section, we provide the implementation details for BC and BC-RNN models. In Behavior Cloning (BC), the objective is to minimize $E_{(s, a) \\sim D} || \\pi_\\theta(s) - a ||_2^2$. We use a 3-layer multi-layer perception (MLP) with a ReLU activation function. All layers are fully connected layers with 128 hidden dimensions with a learning rate of 2 \u00b7 10\u22123. We also use the AdamW [43] to be the optimizer. The training epoch in dexterous tasks Pick-and-Place, Articulated-Manipulation, and Tool-Use is 60, 100, 100 separately.\nAs for BC-RNN, we use an LSTM as the backbone network for BC-RNN [7], which we find a slight performance improvement compared to the vanilla RNN model. Following [7], during the training phase, a state-action sequence $\\{(s_i, a_i), \\cdots, (s_{i+T-1}, a_{i+T-1})\\}$ of length T is sampled from the dataset D and the network will predict the action sequence based on the states as its input. During the inference phase $a_t, h_{t+1} = \\pi_\\theta(s_t, h_t)$ where $h_t, h_{t+1}$ are the hidden states. Here we set the learning rate to be 2 \u00b7 10\u22123, and the training epoch to be 60."}, {"title": "6.2 Experiment Setups", "content": "Dexterous Hand Pick-and-Place aims at picking an object on the table and placing it into a container. The observation space is 24 dimensions, including the dexterous robot hand state (18-dim), the object's position (3-dim), and the container's position (3-dim). The dexterous robot hand state is the position of each fingertip (15-dim) and the wrist position (3-dim). The action space is 28 dimensions, including the state change of each joint (22-dim) and the wrist transformation (6-dim). The object's position is randomized for each attempt within a 10cm \u00d7 10cm square on the table.\nDexterous Hand Articulated-Manipulation aims at grasping and unscrewing the door handle to open the door. The observation space is 32 dimensions, including the dexterous robot hand state (18-dim), the door handle's position (3-dim) and quaternion (4-dim), and the door base's position (3-dim) and quaternion (4-dim). In contrast, the action space is 28 dimensions. The door's position is randomized for each attempt within a 40cm \u00d7 40cm square on the floor.\nDexterous Hand Tool-Use aims at picking a hammer and using it to drive a nail into a board. The observation space is 32 dimensions, including the dexterous robot hand state (18-dim), hammer's position (3-dim) quaternion (4-dim), and nail's position (3-dim). At the same time, the action space is 28 dimensions. The nail's position is randomized for each attempt within a 10cm \u00d7 10cm square on the table.\nParallel Gripper Pick-and-Place aims at picking an object on the table and placing it into a container. The observation space is 27 dimensions, including the five rigid bodies of the gripper to object distances (15-dim), the distance between left and right grippers (3-dim), the object's position (3-dim), the distance between object and target (3-dim,) and the distance between flange and target (3-dim). The action space is 8 dimensions, including the state change of each joint (7-dim) and gripper (1-dim). The object's position is randomized for each attempt within a 10cm \u00d7 10cm square on the table.\nParallel Gripper Articulated-Manipulation aims at picking an object on the table and placing it into a container. The observation space is 16 dimensions, including the five rigid bodies of gripper to"}, {"title": "6.2.1 Ablation study.", "content": "We implement the shared control agent with different methods like the diffusion model and BC. BC adapts a classical way for blending policy to achieve shared control [32]. We use it in the ablation study to blend BC policy with pure human action to achieve shared control in Fig.7. Compared to the classical way which explicitly averages human action $a^h$ and agent action $a^a$ to get the shared action $a^o$, we instead use the diffusion model, which is a popular implicit model, to blend two actions. It models the process as the forward and reverse process. The forward/diffuse process is about adding Gaussian noise to human action $a^h$, and the reverse process uses a neural network f(\u00b7|\u00b7) to denoise $a^k$ to get the shared action $a^o$.\nBC agent is trained using a specific sequence of data collection and fine-tuning steps to optimize performance across different levels of shared control. Initially, we collect data sets of 10, 10, and 20 episodes under various task conditions. These initial datasets are used to train a preliminary agent. Following this initial training phase, we employ the trained agent to assist in further data collection under three different control ratios represented by \u03b3 values of 0.25, 0.5, and 0.75. The data collected with the assistance of the agent under these \u03b3 settings are then used to fine-tune the agent.\nAs shown in Fig. 7, experiments demonstrated that the success rate of an assistive agent based on BC is lower than that of an agent based on diffusion models, indicating a reduced capacity for assistance. In certain instances, the action even becomes worse at particular control ratios."}, {"title": "6.2.2 Real World Experiment.", "content": "In this section, we evaluate the real-world performance of our method. We use the setup shown in Fig.8, which includes a Flexiv Rizon4 arm equipped with a gripper and two Intel RealSense D435i RGB-D cameras. One camera is mounted on the wrist of the robotic arm, while the second is positioned on the side. One task here is to pick the red pot shown in Fig.8 and place it onto the induction cooker. For more details please refer to CoRL-17.mp4\nDuring the real-world data collection phase, we estimate the human hand's pose using RGBD input. Considering the significant difference in morphology between the human hand and a 7-DoF robotic arm, we chose to track the end effector's position by monitoring the position of the hand's wrist. Additionally, we used the action of closing or opening the human hand as the condition for determining whether to grasp or release an object. This approach leverages the greater dexterity of the human hand to enhance the control and precision of the robotic arm. We record RGB images from two camera views, joint poses (7-dim), gripper width (1-dim), the end effector's position (3-dim), and its quaternion (4-dim). The RGB images have a size of 640 \u00d7 480 pixels, each episode is sampled at a frequency of 10 Hz.\nIn real-world experiments, the network architecture is generally similar to the simulation environment's. Our input has changed from the original hand states and object states to the position and orientation of the robot arm end effector, as well as images from the first-person and third-person perspectives. We made two main modifications: 1) For the images, we used a ResNet-18 model. We used a standard ResNet-18 (without pretraining) as the encoder with its global average pooling replaced with a spatial softmax pooling to maintain spatial information. 2) We deepened the layer of the neural network, increased its hidden layer dimension, and expanded action horizon prediction from predicting the next frame action to predicting actions for the subsequent T frames, i.e., at+1:t+T-1 (where T = 8)."}]}