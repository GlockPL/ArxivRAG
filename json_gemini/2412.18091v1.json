{"title": "AutoSculpt: A Pattern-based Model Auto-pruning Framework Using\nReinforcement Learning and Graph Learning", "authors": ["Lixian Jing", "Jianpeng Qi", "Junyu Dong", "Yanwei Yu"], "abstract": "As deep neural networks (DNNs) are increasingly deployed\non edge devices, optimizing models for constrained compu-\ntational resources is critical. Existing auto-pruning meth-\nods face challenges due to the diversity of DNN models, var-\nious operators (e.g., filters), and the difficulty in balancing\npruning granularity with model accuracy. To address these\nlimitations, we introduce AutoSculpt, a pattern-based au-\ntomated pruning framework designed to enhance efficiency\nand accuracy by leveraging graph learning and deep rein-\nforcement learning (DRL). AutoSculpt automatically iden-\ntifies and prunes regular patterns within DNN architectures\nthat can be recognized by existing inference engines, en-\nabling runtime acceleration. Three key steps in AutoSculpt\ninclude: (1) Constructing DNNs as graphs to encode\ntheir topology and parameter dependencies, (2) embedding\ncomputationally efficient pruning patterns, and (3) utiliz-\ning DRL to iteratively refine auto-pruning strategies until\nthe optimal balance between compression and accuracy is\nachieved. Experimental results demonstrate the effective-\nness of AutoSculpt across various architectures, including\nResNet, MobileNet, VGG, and Vision Transformer, achiev-\ning pruning rates of up to 90% and nearly 18% improve-\nment in FLOPs reduction, outperforming all baselines.\nThe codes can be available at https://anonymous.\n4open.science/r/AutoSculpt-DDA0", "sections": [{"title": "1. Introduction", "content": "Deploying resource-intensive deep neural networks\n(DNNs) on edge devices, such as mobile phones, robots,\nand self-driving cars, presents significant challenges\ndue to limited computing resources. This makes model\ncompression, particularly model pruning, an essential\nstrategy [4, 58]. By removing less critical parameters\nthrough pruning, DNNs can effectively reduce their com-\nputational requirements, enhancing their suitability for\nresource-constrained scenarios [17, 52].\nModel pruning can be categorized based on the granu-\nlarity of the process into three types (see Figure 1): Un-\nstructured pruning (or fine-grained pruning) [11, 34, 39,\n41], pattern-based pruning (or semi-structured pruning)\n[10, 27], and structured pruning (or coarse-grained prun-\ning) [7, 26, 31, 53, 55]. For instance, unstructured prun-\ning can remove weights from arbitrary locations in a 3 \u00d7 3\nweight tensor, leading to an irregular distribution of non-\nzero weights. This irregularity requires specialized software\nand hardware support for effective acceleration [36, 56]. In\ncontrast, structured pruning removes entire channels, filters\n(e.g., filter 1 in Figure 1), or layers, preserving a regular net-\nwork structure that is easier to execute on standard inference\nengines. However, due to its coarser granularity, structured\npruning often struggles to achieve an optimal balance be-\ntween compression rate and accuracy, potentially due to the\nloss of critical topology information [7, 52].\nPattern-based pruning represents an intermediate ap-\nproach, focusing on removing specific \u201cregular structures\"\n(highlighted in yellow in Figure 1) within the weight tensor\nthat are conducive to hardware efficiency [10, 27]. The de-\nvelopment of specialized model compilers, such as TVM\n[3] and TensorRT [43], has made pattern-based pruning\nmore practical. These compilers can automatically adapt\nto pruned structures without manual intervention, signif-\nicantly enhancing runtime acceleration by bypassing ze-\nroed regions [56]. Nonetheless, pattern-based pruning tech-\nniques are still underexplored, indicating a pressing need\nfor further research.\nTo search for optimal pruning policies, automated ap-\nproaches such as AutoML (Automated Machine Learning)\n[4, 14] have recently gained attention. These methods use\ngraph learning techniques, specifically graph neural net-\nworks (GNNs), to represent or embed DNNs. The embed-\ndings are then fed into deep reinforcement learning (DRL)\nalgorithms to find the optimal pruning policy, achieving\ngreater compression efficiency [14, 22, 52]. However,\nwhile existing automatic pruning methods have made sig-\nnificant progress, they focus primarily on structured and un-\nstructured pruning, and pattern-based pruning approaches\n[10, 27, 35, 37] are limited and face three main chal-\nlenges: (1) They mainly handle convolutional neural net-\nworks (CNNs) and lack generalizability to various DNN ar-\nchitectures, especially in representing them, ignoring pa-\nrameters in architectures like Transformers [47]; (2) they\nare predominantly designed for specific operators\u00b9 with\nfixed size (e.g., 3 \u00d7 3 conv.) in CNNs, which might limit\ntheir effectiveness for other shapes; (3) there is potential\nto more fully leverage pattern information to improve the\neffectiveness of the pruning strategies.\nTo address these challenges, we propose a universal\npattern-based model auto-pruning framework, namely Au-\ntoSculpt. We first construct various pretrained DNNs as\ngraphs, and integrate regular patterns that are adaptable to\ndiverse operators into the graph. Then, we utilize a GNN\nencoder to learn the graph embeddings, and feed these em-\nbeddings into DRL to automate the search for optimal prun-\ning patterns. To validate the feasibility and effectiveness\nof AutoSculpt, we conduct experimental evaluations across\nseveral datasets and serveral DNNs with different archi-\ntecture, including ResNet, MobileNet, VGG, and Vision\nTransformer. Our results are compared against state-of-\nthe-art (SOTA) methods, demonstrating that our framework\nachieves competitive performance. Specifically, we achieve\na pruning ratio that exceeds all baseline methods, reaching\nup to 90%, while reducing FLOPs by nearly 18% compared\nto the latest auto-pruning method, and achieving SOTA per-"}, {"title": "2. Related Work", "content": "Pruning Granularities. Model pruning has emerged as\na popular research direction to enhance inference perfor-\nmance on resource-constrained devices. Generally, it can\nbe categorized into unstructured, structured, and semi-\nstructured pruning [4]. Unstructured pruning [11, 34, 39,\n41] removes weights arbitrarily across the network, the-\noretically achieving the highest pruning rates without al-\ntering the network's overall structure. However, the re-\nsulting irregularity and sparsity of the compressed weight\ntensors make hardware acceleration challenging. Conse-\nquently, most researchers have shifted their focus to struc-\ntured pruning methods [12]. Structured pruning [7, 14-\n16, 20, 28, 29, 45, 46, 49, 52, 54] eliminates entire filters,\nchannels, or layers, leading to more regular and hardware-\nfriendly network architectures. Mainstream approaches in\nstructured pruning include applying sparse regularization\ntechniques to model parameters during training, such as\nLASSO [50] and ADMM [24]; dynamically adding masks\nto weights during training and inference for pruning (also\nknown as soft pruning) [13, 18, 23]; and utilizing mathe-\nmatical techniques like second-order Taylor approximation\n[48] and Variational Bayesian methods [57] for pruning so-\nlutions. However, the granularity of structured pruning can\nbe too coarse, potentially resulting in the removal of impor-\ntant parameters. Semi-structured (or pattern-based) prun-\ning [27, 35, 37] seeks suitable \u201cregular patterns\" to be re-\nmoved on weight matrices. Models pruned using pattern-\nbased methods exhibit more regular zero-shaped regions,\nthereby achieving inference acceleration while maintaining\naccuracy.\nAutomatic Pruning Methods. The architectures of mod-\nern DNNs are becoming increasingly complex and contain\nrich topology information, making it difficult to achieve sat-\nisfactory model compression results through manual heuris-\ntic methods [10, 35, 37, 52]. Consequently, AutoML united\nGNN and DRL becomes mainstream. MetaPruning [32]"}, {"title": "3. Method", "content": "We first introduce the AutoSculpt framework in Section 3.1,\nand then elaborate on its two key components: The pattern-\nbased graph constructor and encoder in Section 3.2 and the\nDRL-based pruning strategy search in Section 3.3."}, {"title": "3.1. Overview of AutoSculpt", "content": "AutoSculpt is a multi-step framework for pruning DNN\nmodels, as shown in Figure 2. First, we calculate the com-\npression parameters (e.g. inference accuracy, FLOPs and\npruning ratio) of the DNN models to determine if they meet\nthe constraints C (Step 1). If the constraints are unmet, the\nGraph Constructor extracts topological features from the\nmodel and assigns pruning patterns to build the graph (Step\n2). Next, we use a GNN, specifically the Graph Attention\nNetwork (GAT), to encode the graph and obtain the embed-\ndings. Then, the Agent, calculates the probability distribu-\ntion for each pattern based on graph embeddings and selects\nsuitable pruning patterns to optimize the model using Pat-\ntern Sampler while interacting iteratively with the environ-\nment (Step 3). Once constraints are satisfied, we can ob-\ntain the pruned DNN through Pattern Pruner (Step 4), fol-\nlowed by fine-tuning to restore accuracy (Step 5). Finally,\nthe pruned model can be deployed using an AI compilation\nframework (Step 6)."}, {"title": "3.2. DNN-Graph Constructor and Encoder", "content": "A general graph G constructed from a DNN W can be rep-\nresented as $G = \\{V,E\\}$, where $V = \\{N_i,N_k, N_o\\}$ is the\nset of nodes and $E = \\{E_i, E_o\\}$ is the set of edges. The sub-\nsets $N_k$ represent nodes associated with weight tensors, $N_i$\nis the input tensor node set, and $N_o$ is the output node set.\nThe subsets $E_i$ and $E_o$ correspond to the edges linked to V.\nThe nodes in different layers of the DNN can be further di-\nvided into finer-grained sets. For example, the nodes in the\nl-th layer are contained in $N_l \\subset N_k$. Next, we describe\nthe graph constructors of two typical DNN architectures2 :\nCNN and Transformer.\nCNN-Graph Constructor. Given a general CNN W, we\nuse a set of weights $\\{W_i^l \\in \\mathbb{R}^{c \\times k \\times k}, 0 \\le l < m, 0 \\le i \\le\nn_l\\}$ to parameterize the architechture of CNN, where $W_i^l$\nrepresents the weight tensor with c channels of i-th k x k\nkernel of l-th layer, and m is the number of convolutional\nlayers, $n_i$ is the number of convolutional kernels in l-th\nlayer. And the related input feature maps and output feature\nmaps (with w \u00d7 h size) of l-th layer are $X_{in}^l \\in \\mathbb{R}^{c \\times w \\times h}$ and\n$X_{out}^l \\in \\mathbb{R}^{n_l \\times w \\times h}$, respectively.\nThen, the specific process of graph construction is as fol-\nlows: First, map the weight set of the l-th layer $\\{W_i^l \\in\n\\mathbb{R}^{c \\times k \\times k}, 0 \\le i \\le n_l'\\}$ to a set of nodes $N_k \\subset V$, where\na node $N_k^l \\in N_k$ represents the weight of the kernel $W_i^l$.\nAt the same time, map the input and output feature maps\nof the l-th layer to the nodes $N_i^l \\in N_i$ and $N_o^l \\in N_o$, re-\nspectively. The dependencies of the convolution operation\nare then mapped to the connection relationships between\nthe node set $N_k^l$ and the node sets $N_i^l$ and $N_o^l$, forming the\nsets of edges $E_i^l, E_o^l \\subset E_l$ in the graph G. After completing\nthe above steps, we obtain the graph G, as shown in Fig-\nure 3. Since the parameters of the CNN model are primarily\nlocated in the convolutional layers, and the final fully con-\nnected layer connects to the output layer, our focus during\ngraph construction is on the convolutional layers.\nAfter the basic structure of the graph G is constructed,\nwe integrate the pruning pattern P (shown as Patterns Li-\nbrary derived from Agent in Figure 2) into G. Considering\nthat integrating key information into edge feature embed-\ndings yields better results than integrating it into node fea-\nture embeddings [40], we adapt the following integration\nscheme: The pattern P is fused into the edge feature em-\nbeddings of the edge set $E_i$, while the weights of the CNN\ncorresponding to the node set Nk are integrated into their\nnode feature embeddings. In addition, the node feature em-\nbeddings in the node sets N\u2081 and No, as well as the edge\nfeature embeddings in the edge set E, are assigned using\nrandom initialization. In short, we denote this process as\nEq. (1):\nG = GraphConstructor(W,P).\nAdditionally, in CNN architectures like ResNet, there are\nresidual connections. These residual connections establish\nadditional pathways between layers, allowing the flow of\ninformation across layers without being disrupted by prun-\ning. As a result, when building the graph, we represent the\nresidual connections as additional edges that link nodes cor-\nresponding to the layers involved in the residual paths. This\nensures that the graph G accurately reflects the full structure\nof the CNN, including these crucial connections, which are\nessential to maintain the performance of the CNN model\nduring pruning.\nTransformer-Graph Constructor. To model the Trans-\nformer encoder $W \\subset W$, we assume the input embedding\nof l-th encoder is $X_{in}^l \\in \\mathbb{R}^{T \\times d}$, where T means the input\nsequence consists of T tokens and d is the embedding di-\nmension. Then, we have $Q = X_{in}^lW_Q^l$, $K = X_{in}^lW_K^l$,\nand $V^l = X_{in}^lW_V^l$, where $W_Q^l, W_K^l, W_V^l \\in \\mathbb{R}^{d \\times d_k}$ are\nlearnable weight matrices for Querys, Keys, and Values, and\n$d_k$ is the dimension of each. The attention score is shown\nas $X_a^l$, and the output is $X_{out}^l \\in \\mathbb{R}^{T \\times d}$.\nAlthough the Transformer model structure differs signif-\nicantly from CNNs, a graph can also be constructed for it\nto enable pattern pruning. The method of constructing the\ngraph G for the attention module in a Transformer model\nis illustrated in Figure 4. For the l-th encoder, we map\n$W_Q^l, W_K^l, W_V^l$ to the nodes $N_Q^l, N_K^l, N_V^l \\in N_k$, respec-\ntively. Similarly, the inputs and outputs of the l-th encoder\nare maped to $N_i^l \\in N_i \\subset V$ and $N_o^l \\in N_o \\subset V$, re-\nspectively. Then, the nodes set V together with the re-\nlated edges set & constitutes the target graph G. Addi-\ntionally, since the Transformer model contains multiple en-\ncoder and decoder blocks, each of which includes a feed-\nforward network (MLP) that holds a substantial portion of\nthe model's parameters, this also needs to be considered\nwhen constructing the graph. In this case, nodes can repre-\nsent both the attention mechanism components (such as the\nQuery, Key, and Value matrices) and the MLP layers, while\nthe edges capture dependencies between these components\nwithin each encoder and decoder block."}, {"title": "3.3. DRL-based Pruning Strategy", "content": "We leverage DRL to find the optimal pruning ratios effi-\nciently. In the following, we describe the details of DRL-\nbased pruning strategy.\nEnvironment States S. Our training objective is to com-\nbine the model parameters of the DNN with topological\nstructure information to determine the pruning patterns for\nits operators. Therefore, we pass the graph embedding g\nobtained from the graph encoder as the environment states\n(Eq. (6)). Since the pruning pattern applied to the DNN\nchanges after each iteration, the graph G needs to be recon-\nstructed to update the DNN representation.\nAction Space F. The direct output obtained by Agent is the\nprobability distribution of all predefined patterns, making\nits action space F continuous: $F = [a_1, a_2, ..., a_n]$, where\nn is the number of predefined patterns. The actions can be\ncalculated using Eq. (7), and then sample target patterns P'\nfor the DNN we prune (represented by Eq. (8)).\n$F = Tanh(MLP(g))$\n$P' = PatternSampler(F)$\nhk = \u03c3\n\u2211j\u2208Ni,No\nak,jWjhj\n,Reward R. The design of the reward function is crucial\nfor training in DRL. Generally, a higher pruning rate re-\nsults in a more severe decline in model inference accuracy\n[4]. As training progresses, the DRL Agent tends to fa-\nvor lower pruning rates to achieve better inference accu-\nracy. Therefore, we design the reward function by combin-\ning model compression metrics (using FLOPs as an exam-\nple, though other metrics like Multiply-Accumulate opera-\ntions (MACs) can be similarly applied) and model inference\naccuracy as follows:\nR = \u03b1FLOPs + (1 \u2212 \u03b1)Acc,\nwhere \u03b1 is a learnable parameter. When \u03b1 is larger, Agent\nis encouraged to adopt a more aggressive compression strat-\negy, prioritizing model compression over accuracy. Con-\nversely, when \u03b1 is smaller, Agent is incentivized to take a\nmore conservative approach.\nAfter that, to train the Agent, we adapt the PPO-Clip al-\ngorithm [42].\nPattern Pruner. When meeting the constraints C, we set\nthe corresponding position weights of DNN to zero accord-\ning to the assigned patterns, which can be represented by\nEq. (10):\nWp = PatternPruner(W, P).\nAlgorithm. Algorithm 1 summarizes the pattern pruning\nprocess. Lines 1-3 initialize tasks, including putting DNN\nW into environment S, initializing patterns P for Agent\nand initializing replaybuffer B for storing historical training"}, {"title": "4. Experiments", "content": "4.1. Settings\nDatasets and Platform Settings. We conduct experimen-\ntal validation on models in the image classification domain.\nThe datasets include CIFAR-10/100 [21] and ImageNet-1K\n(ILSVRC-2012) [5]. During the pruning and fine-tuning\nprocesses, we utilize all training data, randomly sampling\n50% of the data from the test set for validation. The experi-\nments are carried out on a hardware platform equipped with\nan AMD EPYC 7402 processor and four RTX 4090 GPUs.\nDue to the large size of the ImageNet dataset, we implement\nmulti-GPU parallel training.\nBaselines. To ensure fairness and authority, we directly cite\nvalidated results from the original work, and the baselines\ncompared are:\n\u2022 Regularization-based methods: ABP [46], SOKS [29],\nSCP [19], GREG [49].\n\u2022 Dynamic pruning methods: DDG [23], SMCP [18],\nDRLP [33], CP-ViT [44].\n\u2022 Reinforcement learning-based methods: AGMC [51],\nGNN-RL [52], AMC [14].\n\u2022 Second-order approximation methods: SOSP [38],\nGFP [30].\n\u2022 Activation-based methods: DLRFC [16], Hrank [28],\nCHIP [45].\n\u2022 Gradient-based methods: MFP [15], DNCP [54].\n\u2022 Other pruning methods: DepGraph [7], ProsPr [1],\nRollBack [6], NM [20], CC [25], NPPM [8], MDP [9].\nDNN Settings. We conducte experiments on various DNNs\nwith different architectures. For ResNet-32/56/110 trained\non CIFAR-10 and VGG-19 trained on CIFAR-100, we use\nself-pretrained parameters for pruning. For MobileNet-\nv1/v2, ResNet-50, VGG-16 and ViT-B/16 trained on Ima-\ngeNet, we utilize the pretrained parameters built into Py-\nTorch for pruning. Considering the complexity of optimiz-\ning and deploying pruned models, we share the pruning\npatterns for the residual connection layers when pruning\nthe ResNet models with residual connections. Additionally,\nwe ignore the bias terms in the model computations for all\npruned models.\nPruning Process Settings. During the model pruning pro-\ncess, the initial feature embedding size for the nodes in the\nconstructed graph of the DNN is set to 32 (with the same\nsize for edge feature embeddings). After information ag-\ngregation, the feature embedding size increases to 64, and\nthe overall graph feature embedding size is 256. When us-\ning the PPO algorithm for policy updates of the Agent, we\nemploy the Adam optimizer to optimize both the Actor and\nthe Critic. The learning rate for the Actor is set to 3 \u00d7 10-3,\nwhile the learning rate for the Critic is set to 1 \u00d7 10-3. The\ndiscount factor \u03b3 for controlling the reward attention is set\nto 0.9, and the PPO clipping range \u03f5 is set to 0.2. The re-\nplay buffer size is set to 32, and policy updates occur once\nthe replay buffer is full, with a total of 15 update iterations.\nFine-tuning Settings. During the fine-tuning of the pruned\nmodels, we use the SGD optimizer with the following set-\ntings: momentum = 0.9, weight decay = 4 \u00d7 10-5,\nand the initial learning rate set to 3 \u00d7 10-2. In the train-\ning process, we update the learning rate using a multi-\nstep decay approach, with the decay multiplicative factor\n\u03b4 = 0.1. For models trained on CIFAR-10/100, the mile-\nstones are set to [30, 50, 70, 80, 90], totaling 100 training\nepochs. For models trained on ImageNet, the milestones\nare set to [25, 35, 45, 50], totaling 60 training epochs."}, {"title": "4.2. Results and Analysis", "content": "Performance. Table 1 and 2 present the results of our Au-\ntoSculpt compared with baselines on two datasets. We can\nsee that our method achieves the best results on compres-\nsion ratio (FLOPs \u2193) with an relative smaller accuracy va-\nriety (\u2206 Acc.), reaching the state-of-the-art. After prun-\ning, the FLOPs of the models significantly decreased, while\nthe inference accuracy could be restored to the level of the\ninitial models after fine-tuning. This effectively maintains\nmodel performance while compressing the model, meaning\nthat only the truly useful parameters are retained.\nDig deeper, for all pruned DNNs, DNNs with simpler ar-"}, {"title": "Latency Comparison.", "content": "We also test the inference latency of\nthe pruned models and compare them with the original mod-\nels, as shown in Table 3. We can see that our method signif-\nicantly reduces the MACs, thereby lowering the inference\nlatency while maintaining comparable or even improved ac-\ncuracy. For example, in ResNet-110, the MACs were re-\nduced from 0.26 G to 0.1 G, with a corresponding decrease\nin latency from 5.7 ms to 5.3 ms. Notably, the accuracy\neven slightly improved from 93.50% to 93.91%, indicating\nthat the pruning method not only reduces the model size but\nalso retains and potentially enhances its performance. For\nVGG-19, the MACs were drastically reduced from 0.4 G to\n52.36 M, leading to a reduction in latency from 5.3 ms to\n4.1 ms (nearly 29% speed improvement). Although the ac-\ncuracy slightly dropped from 70.35% to 67.97% (decreased\nby 2%), this trade-off may be acceptable considering the\nsignificant improvement in inference speed and efficiency.\nParameter Sensitivity. We test the impact of different hy-\nperparameters on the accuracy of pruned models, as shown\nin Figure 5. First, we evaluate the effect of the number\nof patterns on the accuracy of pruned models. From Fig-\nure 5 (a), we can observe that increasing the number of pat-\nterns improves the accuracy of the inference for all models.\nMeanwhile, when the number of patterns reaches a certain\nthreshold, the accuracy gains steadily. In particular, when"}, {"title": "4.3. Ablation Study", "content": "We validate the effectiveness of incorporating graph en-\ncoders, including GCN and GAT, in the pruning process.\nWe evaluate this on ResNet-110, VGG-19, MobileNet-v1,\nand ViT-B/16, as shown in Figure 6. The results demon-\nstrate a significant improvement in the accuracy when using\nGCN or GAT to represent the DNN compared to the method\n(w/o GE) without using graph encoders. For instance,\nthe pruned ResNet-110's accuracy increased from 65.74%\nwithout a graph encoder to 93.16% with GCN and 93.91%\nwith GAT, highlighting the substantial advantage of using\ngraph encoders, especially in deep networks like ResNet-\n110. Moreover, the results obtained with the GAT encoder\nwere slightly better than those with the GCN encoder. Over-\nall, integrating graph encoders effectively captures the topo-\nlogical and pattern information within the deep neural net-\nwork structure, leading to better performance after pruning."}, {"title": "5. Conclusion", "content": "We introduced AutoSculpt, a pattern-based model auto-\npruning framework that uses GNN and DRL to efficiently\ncompress DNNs. AutoSculpt represented DNNs as graphs\nand applied compute-friendly pruning patterns, allowing it\nto find better pruning strategies that work well across differ-\nent DNN architectures and with standard inference engines.\nExperimental results showed that AutoSculpt achieved high\ncompression rates with minimal accuracy loss, making it a\nstrong solution for deploying DNNs on devices with limited\nresources. Future work could focus on improving pattern\ngeneration methods and expanding the framework to work\nwith a wider range of DNN types, making it even more flex-\nible and effective."}]}