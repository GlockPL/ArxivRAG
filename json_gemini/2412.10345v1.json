{"title": "TRACEVLA: VISUAL TRACE PROMPTING ENHANCES\nSPATIAL-TEMPORAL AWARENESS FOR GENERALIST\nROBOTIC POLICIES", "authors": ["Ruijie Zheng", "Yongyuan Liang", "Shuaiyi Huang", "Jianfeng Gao", "Hal Daum\u00e9 III", "Andrey Kolobov", "Furong Huang", "Jianwei Yang"], "abstract": "Although large vision-language-action (VLA) models pretrained on extensive robot\ndatasets offer promising generalist policies for robotic learning, they still struggle\nwith spatial-temporal dynamics in interactive robotics, making them less effective\nin handling complex tasks, such as manipulation. In this work, we introduce visual\ntrace prompting, a simple yet effective approach to facilitate VLA models' spatial-\ntemporal awareness for action prediction by encoding state-action trajectories\nvisually. We develop a new TraceVLA model by finetuning OpenVLA on our\nown collected dataset of 150K robot manipulation trajectories using visual trace\nprompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv\nand 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance,\noutperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks\nand exhibiting robust generalization across diverse embodiments and scenarios.\nTo further validate the effectiveness and generality of our method, we present\na compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-\nEmbodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while\nsignificantly improving inference efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "Robotic manipulation policies, typically trained on specific task demonstrations, often struggle to\ngeneralize beyond their training data, particularly when faced with novel objects, environments,\ninstructions, and embodiments. In contrast, foundation models for vision and language\u2014such as\nCLIP (Radford et al., 2021), LLaVA (Liu et al., 2024b), Phi-3-Vision (Abdin et al., 2024b), and\nGPT-4V (Achiam et al., 2023)\u2014have demonstrated impressive generalization across diverse vision-\nlanguage tasks. However, these models are not equipped to handle the challenges unique to robot\nmanipulation, such as understanding kinematics, adapting to different embodiment configurations,\nand executing reliable physical actions. Vision-Language-Action models (VLAs) (Brohan et al.,\n2022; Kim et al., 2024) seek to address this gap by fine-tuning vision-language models to generate\nrobot control actions using large-scale robotic datasets (e.g., Collaboration et al., 2023b), combining\nthe generalization power of foundation models with task-specific robotic expertise. The approach has\nyielded promising results in developing generalist robot policies capable of adapting to a wide range\nof manipulation tasks.\nHowever, VLA-powered robots often struggle to maintain awareness of their past movements, leading\nto decisions that are more reactive to current inputs rather than informed by spatial history. We posit\nthat this limitation arises because simply mapping image inputs as current states to control actions\nis insufficient. To address this, we propose explicitly computing multi-point temporal trajectories\nand overlaying them directly onto the image inputs for VLA models. We hypothesize that this will\neffectively provide spatial and temporal relations necessary for improving manipulation tasks (Wen\net al., 2023; Yuan et al., 2024)."}, {"title": "2 PRELIMINARIES", "content": "Visual-Language-Action Models. Vision-language-action models (VLAs) extend vision-language\nmodels (VLMs) to predict discretized robot actions. Their architecture comprises: (1) a visual\nencoder (Zhai et al., 2023; Oquab et al., 2023) that converts input images into patch embeddings, (2)\na projector that maps these embeddings to the language model's input space, and (3) a large language\nmodel backbone (Touvron et al., 2023). Action discretization, a crucial feature of VLAs, involves\nmapping continuous robot actions to discrete tokens. This process typically divides each action\ndimension into 256 bins based on data quantiles. These discrete actions are then incorporated into the\nlanguage model's vocabulary, often replacing the least frequently used tokens. VLA training builds\nupon the foundation established during VLM training. During VLM training, the model is trained\nend-to-end with a next text token prediction objective on paired or interleaved vision and language\ndata curated from various Internet sources. VLA training then extends this approach, encompassing\nfine-tuning of the pretrained VLM. The cross-entropy loss focuses specifically on the predicted action\ntokens. This approach allows VLAs to leverage the capabilities of VLMs for complex robot control\ntasks while operating within the constraints of tokenized language models.\nGeneralist Control Policies. Robotic policy learning typically relies on task-specific demonstrations\n$D = {T_1, T_2, ..., T_n}$, where each $T_i = {(O_t, S_t, a_t)}_{t=1}^{}=1$ represents an expert-level trajectory. The\nlearning architecture comprises a visual encoder $F$, mapping image observations $o_i$ to features\n$Z_i = F(O_i)$, and a policy network $\u03c0$ outputting action distributions $\u00e2 \u223c \u03c0(\u00b7|z,s)$. Training\nminimizes the error between predicted $\u00e2$ and optimal $a$ actions. To overcome task-specificity\nlimitations, generalist policies are being developed, aiming to handle diverse sensors, action spaces,\nand robotic platforms in various scenarios. Vision-Language-Action (VLA) models, leveraging\nVLM's visual understanding and multimodal reasoning capabilities, show promise in creating more\nadaptable generalist policies. These models offer improved generalization across tasks, enhanced\nsemantic understanding of environments, and the ability to follow natural language instructions,\npaving the way for more flexible and intuitive robotic control in broader applications."}, {"title": "3 TRACEVLA", "content": "Our ultimate goal is to equip the Vision-Language-Action (VLA) model with the necessary context\nto better understand both temporal and spatial dynamics. In this section, we describe the details\nof our method, TraceVLA, to achieve this objective. First, we introduce visual trace prompting\nin Section 3.1. Next, we explain the model architecture of TraceVLA in Section 3.2. Finally, we\nprovide the implementation details in Section 3.3."}, {"title": "3.1 VISUAL TRACE PROMPTING", "content": "At any given timestep t and a time window budget N, we first extract a set of dense point trajectories,\n$P$, from a sequence of historical image observations, $h_t = (O_{t\u2212N},..., O_t)$, using a dense point\ntracking model. Specifically, a point trajectory represents the trace of a moving point over time. Thus,\na set of dense point trajectories captures the traces of multiple critical moving points. The specific\ndense point tracking model we employ is Co-Tracker (Karaev et al., 2023), chosen for its efficiency\nand simplicity. Co-Tracker partitions the starting frame $o_t$ into a $K \u00d7 K$ grid and tracks each grid\ncell across N frames to construct point trajectories. Consequently, we generate a total of $K \u00d7 K$\ndense point trajectories, with each trajectory representing the location of a single point from timestep\n$t - N$ to timestep t.\nWhile Co-Tracker provides a $K \u00d7 K$ grid of dense point trajectories, it does not inherently identify\n\"active points.\" To address this, we identify active point trajectories in $P$ by analyzing changes in\npixel locations, focusing on those with significant movement and distinguishing them from static\nbackground points. For each point trajectory $p \u2208 P$, we first compute the absolute movement $Apt'$\nbetween two adjacent frames at timestep t' and t' + 1, as $Apt' = |P_{t'+1} - P_{t'}|$. We then identify\nactive point trajectories $P$ by computing the total movement over N timesteps, keeping those whose\nmovement exceeds a threshold \u03ba. In other words, $P = {p\u2208P|\u03a3_{t'=t-N}^{t} Apt'pe > K k}$. From $P$,\nwe randomly sample M active point trajectories, denoted as $P$, for use in visual prompting.\nFinally, we generate the visual trace by overlaying the sampled active point trajectories $P$ onto the\nrobot's original observation frame ot, as shown in Figure 2. This overlaid frame serves as a visual\nprompt, providing the model with spatial information about its historical states and actions."}, {"title": "3.2 MODEL ARCHITECTURE DESIGN", "content": "In Figure 1, we outline the design of TraceVLA using visual trace prompting. While the overlaid\nvisual trace provides valuable spatial-temporal information about the robot's historical movements,\nit may obstruct the robot's end-effector or key objects, potentially hindering the model's ability to\ngenerate the correct action. To address this, we also include the original image observation in the\nmodel input, inserting a special separator token between the two images. As shown in Figure 1, we\nadjust the text prompt to inform the VLA model of this additional visual input before requesting the\nappropriate action output.\nAdditionally, since visual traces may not be available in all test-time scenarios such as when the\nCo-Tracker model fails under pool lightning conditions\u2014we implement a dropout mechanism during\ntraining. For each training example, with probability \u03b1, we replace the visual trace prompt image with\nthe original image and remove the corresponding hint from the text prompt. This dropout strategy\nintroduces variability into the inputs, encouraging the model to effectively utilize both the original\nimage and the visual trace. As a result, at test time, even if the Co-Tracker model is unable to extract\nthe visual trace, our model can still function correctly."}, {"title": "3.3 IMPLEMENTATION DETAILS", "content": "In this section, we describe the implementation details of the dataset and models used in this work.\nFor the visual trace generation pipeline in training, we use a grid size of K=40, sample M=5 active\npoint trajectories, and employ a time window N=6. To reduce computational overhead, we run dense\npoint tracking every N step for the future 2N frames, rather than at every timestep. We divide demo\ntrajectories into overlapping 2N-sized segments (e.g., [0, 2N), [N, 3N), etc.) and run Co-Tracker once"}, {"title": "4 EXPERIMENT", "content": "To comprehensively evaluate our model's performance, we conducted experiments across a wide\nrange of environmental setups, including 3 tasks with 137 different configurations in simulation and\n4 tasks on real robots.\nBaseline. We benchmark our approach against the following generalist policies, including state-of-\nthe-art open-sourced models:\nTraceVLA and TraceVLA-Phi3: Finetuned from OpenVLA and OpenVLA-Phi3 with visual trace\nprompting."}, {"title": "4.1 SIMULATION EVALUATION", "content": "SimplerEnv. Our simulation evaluation utilizes SimplerEnv, which incorporates two distinct settings:\nvisual matching and variant aggregation. The visual matching setting aims to minimize the\nvisual appearance gap between real environments and raw simulation, significantly enhancing the\ncorrelation between policy performance in simulation and real-world scenarios. Complementing\nthis, the variant aggregation setting covers a wide range of environmental variations as shown in\nFigure 4, including backgrounds from different rooms, lighter and darker lighting conditions, varying\nnumbers of distractors, solid color and complex table textures, and different robot camera poses. This\ncomprehensive set of variations allows us to assess the robustness and adaptability of our approach in\nhandling diverse manipulation scenarios, particularly evaluating the spatial and temporal awareness\nbrought by visual trace prompting."}, {"title": "4.2 REAL ROBOT EXPERIMENTS", "content": "We evaluate TraceVLA on physical WidowX-250 robot manipulation tasks using a fixed-mounted\nthird-person view camera capturing 256 \u00d7 256 RGB images. Despite sharing the same robot embod-\niment as BridgeData-v2, differences in setup, lighting, and camera angles necessitated collecting\n30 demonstration trajectories per task for finetuning. Detailed real-robot experimental setup and\nprotocols are in Appendix A.\nAs shown in Figure 7a, TraceVLA consistently outperforms the baseline across diverse tasks including\nsoft object manipulation, pick-and-place operations, and object movement. Notably, in the pick-\nplace corn task, which was not included in training data, TraceVLA achieved 8/10 successful trials\ncompared to OpenVLA's 1/10, demonstrating strong generalization from similar training tasks, e.g.,\npicking and placing eggplant in a pot.\nTo further evaluate generalization capabilities, we conducted additional experiments with four unseen\ntasks involving novel objects, goals, and language instructions. These trials included 2-3 random"}, {"title": "4.3 ABLATION STUDIES", "content": "To analyze the performance gain from visual trace prompting, we further study the following\nquestions.\nIs performance gain of TraceVLA coming from further finetuning on a smaller subset of Open\nX-embodiment dataset? To answer this, we also tested the performance of the 7B OpenVLA and\n4B OpenVLA-Phi3 models finetuned on the exact same dataset as ours, but without using visual trace\nprompting. The results, as shown in Figure 8 (Left), indicate that the performance gain observed\nin TraceVLA is unlikely due to finetuning the pretrained VLA model on a smaller subset. While\nfinetuning the pretrained OpenVLA model brings a 1.1% gain, finetuning OpenVLA-Phi3 model\neven degrades the performance by 0.3%. However, when visual trace prompting is incorporated, the\nsuccess rate of OpenVLA model increases to 47.7%, highlighting the significant impact of visual\ntraces on model performance.\nWill appending historical image observations also give similar performance gain as TraceVLA?\nInstead of using visual trace prompting, the most straightforward way to integrate the model's\nhistorical movement knowledge is to provide it with multiple past image observations. To test this\nmethod, we input N frames into the VLA model, separated by a learnable separator token, similar\nto our approach in TraceVLA. We also include a sentence in the text prompt to inform the model\nof this change in observation. Here, N = 6, which matches the length of the visual trace used in\nTraceVLA. In theory, the model should receive more information from these 6 complete frames\ncompared to visual trace prompting. However, as shown in 8 (Right), finetuning OpenVLA with\nhistorical information not only fails to improve overall performance but also reduces it by 6%. This\nperformance drop is likely due to redundant information between visual tokens at different timesteps,\nwhich may distract the model from focusing on the most relevant information for deciding next\nactions. In contrast, visual trace prompting offers useful hints that enhance the context for the\nvision-language model.\nIs visual trace prompting better than text trace prompting for grounding VLA with temporal-\nspatial understanding? In addition to guiding the model with point tracking by overlaying the visual\ntrace on the original image, we can also describe the movement of the points using 2D coordinates\nin text. To assess whether visual prompting is the most effective method for improving the VLA\nmodel's spatial-temporal understanding, we implemented an alternative approach that describes the\npoints' movement verbally, treating the trace as text tokens, as shown in Figure 9 (Right).\nInterestingly, using this text-based trace yields a 2.4% average performance gain over the baseline\nVLA model, suggesting that point tracking information is indeed useful for the robot model. However,\ncompared to our approach (TraceVLA), the additional 6.4% gain indicates that leveraging the\nvision encoder's ability to process visual prompts is much more effective than describing the points'"}, {"title": "5 LIMITATION ANALYSIS: TRAINING MEMORY COST AND INFERENCE SPEED", "content": "Since TraceVLA introduces an additional image input into the model and uses CoTracker to obtain\nthe visual trace during testing, we examine both the training memory cost and inference speed of\nTraceVLA. For evaluating memory cost, we launch a single-node multi-gpu training job with 8 H100\ngraphics cards under varying batch sizes, and we measure the maximum GPU memory usage across\n8 H100s. All tests are conducted with flash attention and torch.bfloat16 as the datatype of model\nweights and inputs. As shown in Figure 11 (left), when the batch size is 32, the memory difference\nbetween TraceVLA and models without visual trace prompting (for both Phi3 and OpenVLA) is less\nthan 10GB. Notably, this difference becomes even smaller with a reduced batch size, indicating that\nwhile Trace VLA incurs some extra GPU memory cost, this additional GPU memory cost remains\nmanageable and not significantly impactful.\nFor testing inference speed, we evaluate the time cost using a single H100 GPU. In addition, we\nalso analyze the time cost of each additional component introduced in TraceVLA compared to the\noriginal OpenVLA model. During inference time, the extra computation introduced by TraceVLA\nconsists of approximately 300 additional image and text tokens for the transformer model at each\ntimestep, 5-point CoTracker point tracking for every timestep, and $K \u00d7 K$ (where K = 40) dense"}, {"title": "6 RELATED WORK", "content": "Generalist Robot Policies. Recent advancements in robotics have seen a shift towards developing\nmulti-task \"generalist\" robot policies capable of performing a wide range of tasks, rather than\nspecializing in a single task (Reed et al., 2022; Brohan et al., 2022; Walke et al., 2023; Kalashnikov\net al., 2018; 2021; Ebert et al., 2021; Ehsani et al., 2023; Bharadhwaj et al., 2024; Liang et al.;\nJiang et al., 2024; Zheng et al., 2023; 2024b;a; Wei et al., 2023). Existing work utilized pretrained\nmodel components and finetuned them on large, diverse robot datasets that encompass various\nscenarios and tasks for better generalization (Ebert et al., 2021; Brohan et al., 2022; Collaboration\net al., 2023b; Khazatsky et al., 2024). For instance, Octo trained a generalist robot policy by\nbuilding on pretrained language embeddings and visual encoders, incorporating additional model\ncomponents initialized from scratch and learning to compose them during training (Team et al., 2024).\nOpenVLA enhanced this architecture by adopting an end-to-end approach that directly fine-tunes\nVision-Language Models (VLMs) to generate robot actions, treating these actions as tokens within\nthe language model vocabulary (Kim et al., 2024). In contrast, our method integrates historical\nvisual traces into the VLM and employs these traces as visual prompting, enabling a more nuanced\nunderstanding of the robot's history state, and offering a more comprehensive VLM framework for\ngeneralist policies.\nVision Language Action Models. Several studies have investigated the application of vision-\nlanguage models (VLMs) in robotics (Karamcheti et al., 2023; Gadre et al., 2022; Driess et al., 2023;\nDu et al., 2023). Among them, Robopoint (Yuan et al., 2024) and RepKep (Huang et al., 2024b)\nleverages VLM for explicit key point coordinates prediction, which is then converted to low-level\nactions through an off-the-shelf motion planner. Meanwhile, many recent works have explored\nfine-tuning large pretrained VLMs to directly predict robot actions as VLA models, treating these\nactions as tokens within the language model vocabulary (Brohan et al., 2023; Niu et al., 2024; Zhu\net al., 2024; Li et al., 2024; Kim et al., 2024). Among them, RT-2 (Brohan et al., 2023) fine-tuned\nVLMs on both robotic trajectory data and Internet-scale vision-language data. LLARVA (Niu et al.,\n2024) generated both 2D visual traces in image coordinates and corresponding textual actions as\noutputs, with the former functioning as an auxiliary task. LLaRA (Li et al., 2024) generates multiple\nauxiliary datasets with complementary training objectives to provide additional supervision. RT-\n2-X (Collaboration et al., 2023a) trains a 55B-parameter VLA policy on the Open X-Embodiment\ndataset. OpenVLA (Kim et al., 2024) combines a open VLM backbone with a richer robot pertaining\ndataset. We build on top of OpenVLA, but distinctively address the challenge of maintaining\nawareness of past spatial trajectories in VLA.\nVisual Trace for Robotics. Visual traces of moving objects are vital for improving robotic action\nprediction, as they convey essential information about object dynamics. Various approaches have\nbeen developed to utilize visual traces in robotics, including using hand-drawn sketches for goal\nspecifications (Gu et al., 2023), predicting future traces and learning a trace-guided policy (Wen"}, {"title": "7 CONCLUSION AND DISCUSSION", "content": "Our work advances vision-language-action (VLA) models for robotic manipulation by introducing\na novel visual trace prompting technique and providing a dataset enriched with spatial-temporal\ninformation across diverse embodiments. With state-of-the-art 7B and 4B VLA models, we push\nthe boundaries of VLA performance, demonstrating their effectiveness in both extensive simulated\nenvironments and real-world robotic tasks. By bridging the gap between visual perception, temporal\nawareness, and physical embodiment, we significantly enhance the generalization and adaptability of\nVLA models.\nLooking ahead, promising directions for future research include incorporating multi-point spatial\ntrajectory prediction, allowing models not only to react but also to anticipate and plan actions with\ngreater foresight. Additionally, leveraging 3D point cloud data for training could further enrich\nspatial representations, capturing fine-grained details in complex scenes and objects, thus improving\nmanipulation accuracy and robustness across diverse and dynamic scenarios. These advancements\nwill continue to enhance the generalization capabilities of VLA models, driving further progress in\nrobotic manipulation."}, {"title": "A REAL ROBOT TASKS SETUP", "content": "Here we provide the detailed language instruction for each task that we designed. For each trial, we\nrandomize the intial location of the target object, and for each trial except for folding cloth, we have\nalso added 2-3 random distracting objects into the scene, including toy pepper, eggplant, ketchup,\ncarrot, and donut.\nTasks included in the finetuning dataset.\n\u2022 Task 1: fold cloth: Fold the cloth from right to left. The trial is counted as a success only\nwhen the robot succeesfully grasp the right edge of the cloth and fold it to the left.\n\u2022 Task 2: swipe corn sink: Pick up the brush and then use the brush to sweep the corn into\nthe sink, while avoiding collision with other objects. The trial is counted as success only\nwhen robot successfully swipe the corn into the sink without colliding into existing objects\non the table.\n\u2022 Task 3: pickplace corn pot: Pick up the corn and then put it into the pot. The trial is\ncounted as success only when the robot correctly picks up the corn and place into the pot.\nNote that the primary goal of the this task is to assess the model's generalization capability,\nas this task is not part of the training dataset. Instead, the training data includes a task that\ninvolves picking up an eggplant and placing it in a pot.\n\u2022 Task 4: pickup knife: Pick up the knife first, and then place it on the plate. The trial is\ncounted as success only when the robot correctly picks up the knife and place into the target\nplate.\nUnseen tasks for generalization:\n\u2022 Task 1: pickplace banana: Pick up the banana and place it to the right of the plate. The\ntrial is counted as a success only when the robot correctly picks up the banana and places it\nto the right of the plate. This task is particularly challenging because the banana object is\nunseen in our real-robot finetuning dataset. Additionally, solving this task requires the model\nto leverage its language understanding capability to ground spatial knowledge, rather than\nrelying on spurious correlations, as the instructions in our finetuning dataset only involve\nplacing objects on the plate.\n\u2022 Task 2: pickplace eggplant: Pick up the eggplant and place it on the plate. The trial is\ncounted as a success only when the robot correctly places the eggplant on the plate. This\ntask tests the model's capability for handling unseen goals, as the finetuning dataset only\nincludes placing the eggplant into a pot. Additionally, the eggplant is difficult to grasp, as\nincorrect placement of the end-effector could easily cause the eggplant to rotate and miss\nthe target.\n\u2022 Task 3: lift battery: Lift the AAA battery. The trial is counted as a success only when the\nrobot correctly picks up the battery and lifts it without dropping or damaging it. This task\ntests the model's capability to handle unseen objects, as the battery is not included in our\nfinetuning dataset.\n\u2022 Task 4: push cloth: Push the cloth from the left to the right of the table. The trial is counted\nas a success only when the robot successfully pushes the cloth to within 1 inch of the right\nedge of the table. This task evaluates the model's motion generalization capability, as the\nfinetuning dataset only includes tasks involving folding cloth."}, {"title": "BQUALITATIVE RESULTS ON REAL ROBOT ROLLOUTS", "content": "In this section, we present real robot manipulation rollouts for both the OpenVLA and TraceVLA\nmodels. As discussed earlier, our TraceVLA model demonstrates significantly better generalization\nability across various real robot manipulation tasks, unseen objects, and unseen language instructions.\nIn Figures 12, 13, and 14, we qualitatively illustrate how the two models handle three tasks: \u201cPickplace\nBanana, Folding Cloth, and Pickplace Eggplant.\u201d For the TraceVLA model, we also visualize the\nvisual trace prompt that the model uses during evaluation.\nDue to the proposed visual trace prompting, our TraceVLA model not only accurately picks up the\nbanana and eggplant, grasps the edge of the folding cloth, and completes these tasks smoothly, but\nalso demonstrates superior spatial understanding and reasoning capability compared to OpenVLA.\nIn contrast, the OpenVLA model shows limited generalization capability, often overfitting to the\nfinetuning distribution. For example, it places the banana directly onto the plate instead of following\nthe instruction to place it to the right of the plate. These results further highlight the benefits of our\nvisual trace prompting technique."}, {"title": "C ADDITIONAL ABLATION STUDIES", "content": "C.1 THICKNESS, TRANSPARENCY, AND COLOR OF VISUAL PROMPTING\nTo further investigate the impact of visualization parameters, we conducted additional ablation studies.\nSpecifically, we fine-tuned the TraceVLA model on datasets with variations in trace visualization\nsettings, including different line thicknesses, transparency levels, and color schemes. Our findings\nindicate that performance variations across these parameters are minimal within a reasonable range.\nBelow, we provide detailed experimental results for each parameter setting.\nThickness: The effect of varying the line thickness of visual traces on the SimplerEnv Average\nSuccess Rate is shown in Table 2. We observe only minor differences in performance when adjusting\nthis parameter.\nTransparency: We varied the transparency of the visual traces by adjusting the \u03b1 parameter. Lower\n\u03b1 values result in more transparent traces. Table 3 summarizes the findings, demonstrating the\nrobustness of TraceVLA's performance to these adjustments.\nColor: The choice of color scheme was also tested. The default Trace VLA color scheme uses RYPBG\n(Red, Yellow, Purple, Blue, Green), while an alternative scheme POBGG (Pink, Orange, Blue, Grey,\nGreen) was evaluated. Results are presented in Table 4, showing negligible differences in success\nrates.\nTakeaway: Our experiments reveal that the choice of visualization parameters, including thickness,\ntransparency, and color, has a negligible impact on TraceVLA's performance when chosen within\nreasonable ranges. These results suggest that such parameters do not require extensive hyperparameter\ntuning, simplifying their selection process. This robustness underscores TraceVLA's reliability across\ndifferent visualization settings.\nC.2 BASELINE WITH DIFFERENT STEPS OF HISTORICAL OBSERVATIONS\nIn Figure 8, we compared TraceVLA with OpenVLA using 6 steps of observation history to ensure\nboth models had access to the same amount of historical information. Here, in Figure 15, we\nfurther compare TraceVLA with OpenVLA fine-tuned using 2 and 3 steps of observation history on\nSimplerEnv. While a slight performance improvement is observed with 2-step history for OpenVLA,\nTraceVLA consistently and significantly outperforms the baseline in success rates, highlighting the\neffectiveness of visual trace prompting."}, {"title": "D ADDITIONAL RELATED WORK", "content": "In this section, we discuss some additional works that apply visual prompting technique of VLM\nand their applications in robotics. In particular, visual prompting methods have emerged as a new\nparadigm for VLM, complementing textual prompting and enabling more fine-grained and pixel-level\ninstructions on multimodal input for VLMs (Yang et al., 2023a;b), and has been widely used in\nrobotics (Yan et al., 2023; Liu et al., 2024a; Nasiriany et al., 2024). MOKA (Liu et al., 2024a)\nannotates key points as visual marks on images, converting affordance reasoning into a series of\nvisual question-answering problems that are solvable by the VLM. PIVOT (Nasiriany et al., 2024)\ncast robotic control tasks as visual question-answering problems and iteratively refined visual prompts\nand action selection. Unlike existing work, our approach introduces visual trace prompting during\nfine-tuning of VLMs, overlaying key point traces on robot observations. Our novel visual trace\nprompting directly incorporates temporal information into the visual input, enhancing VLA models'\nspatial-temporal awareness for more effective action prediction in robotic tasks."}, {"title": "E MORE IMPLEMENTATION DETAILS", "content": "During inference, we aim to make our visual trace prompting as efficient as possible, adding minimal\ncomputation to the original VLA model. Extracting the visual trace by querying Co-Tracker for a\n$K \u00d7 K$ grid at each timestep is not feasible due to efficiency constraints. Instead, if we know the\nactive points from the previous timestep, we can query Co-Tracker for only M active points, which is\nfaster and more cost-effective.\nIdeally, similar to KV caching in LLM inference, we only run Co-Tracker with the $K \u00d7 K$ grid once\nat the start of the trajectory to find the M active points. After that, we query Co-Tracker only for\nthese M active points throughout the trajectory. However, in practice, we observe that Co-Tracker\nmight lose track after some steps (around 30 to 40, depending on the actions' magnitude). To address\nthis, TraceVLA periodically re-queries Co-Tracker to recalibrate after a long interval. This ensures\nthat the need for dense $K \u00d7 K$ point tracking is infrequent within an episode. As a result, the total\nnumber of dense queries during a trajectory is minimized, while tracking a few active points incurs\nlittle additional cost, adding minimal computational overhead to the model."}]}