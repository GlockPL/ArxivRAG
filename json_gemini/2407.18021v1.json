{"title": "Quadratic Advantage with Quantum Randomized Smoothing Applied to Time-Series Analysis", "authors": ["Nicola Franco", "Marie Kempkes", "Jakob Spiegelberg", "Jeanette Miriam Lorenz"], "abstract": "As quantum machine learning continues to develop at a rapid pace, the importance of ensuring the robustness and efficiency of quantum algorithms cannot be overstated. Our research presents an analysis of quantum randomized smoothing, how data encoding and perturbation modeling approaches can be matched to achieve meaningful robustness certificates. By utilizing an innovative approach integrating Grover's algorithm, a quadratic sampling advantage over classical randomized smoothing is achieved. This strategy necessitates a basis state encoding, thus restricting the space of meaningful perturbations. We show how constrained k-distant Hamming weight perturbations are a suitable noise distribution here, and elucidate how they can be constructed on a quantum computer. The efficacy of the proposed framework is demonstrated on a time series classification task employing a Bag-of-Words pre-processing solution. The advantage of quadratic sample reduction is recovered especially in the regime with large number of samples. This may allow quantum computers to efficiently scale randomized smoothing to more complex tasks beyond the reach of classical methods.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of quantum computing (QC) principles into machine learning, forming the field of quantum machine learning (QML), represents a groundbreaking shift in the landscape of computational intelligence [1]\u2013[3]. Theoretically, QML offers remarkable potential benefits, including the ability to handle exponentially larger datasets and to perform certain computations much faster than classical algorithms [4], [5]. However, QML is not without its vulnerabilities, particularly in the realm of adversarial attacks [6]\u2013[8]. These are instances where input data is deliberately manipulated to deceive the model into making incorrect predictions or classifications, a challenge well-documented in the classical machine learning domain [9], [10]. In response to these adversarial threats, both quantum and classical machine learning communities have developed probabilistic [11]\u2013[15] and deterministic [16]\u2013[19] defense strategies.\nDefenses are typically developed as either formal verification of outcomes via input space propagation or by utilizing sampling distributions linked to differential privacy. While the latter method has shown promise for enhancing QML applications, it is not without its limitations. For instance, the assumptions made by Weber et al. [13] about class probability estimates overlook the complexities of machine learning uncertainties, rendering their robustness certificates potentially overly optimistic. Moreover, their claims of robustness face challenges due to issues with data encoding and scalability, raising doubts about their usefulness in practical scenarios. Similarly, Du et al.'s [20] work is limited by its focus on depolarization noise, thereby restricting the range of applicable data encodings. Their approach does not accurately represent realistic perturbation distributions, particularly for basis state encodings, leading to a mismatch between theoretical robustness and practical applicability. Huang et al. [14] while considering rotation noise, also suffers from similar shortcomings, with robustness certifications heavily dependent on data encoding types and potentially inadequate for large-scale quantum systems. A more detailed discussion of these works can be found below in section III.\nThese critiques highlight a gap between the theoretical robustness measures proposed and their practical utility across various QML applications. The failure to integrate perturbation types with data encodings may hinder the achievement of realistic robustness certifications. Our method aligns with QuAdRo [15] due to its flexibility regarding the algorithm and noise type, and its clear demonstration of how QC reduces the sample size needed for robustness certification. However, it also has limitations: it requires data to be encoded in basis states due to the use of the Grover oracle, limiting data encoding options. On this background, we investigate how data encoding and perturbation types have to be matched to get meaningful, and with QuAdRo scalable, robustness certificates. Our contributions can be summarized as follows:\n\u2022 The connection between information encoding and perturbation type is discussed at length, we show that the interpretation of certificates obtained with randomized smoothing changes significantly and might even be meaningless for some combinations.\n\u2022 Constrained k-distant Hamming weight perturbations are identified as meaningful perturbation type compatible with a basis state encoding as necessary for QuAdRo. Additionally, an algorithm for implementing them on a quantum computer is introduced.\n\u2022 We demonstrate quadratic sampling advantage using QuAdRo on a practical time series classification task by utilizing an established Bag-of-Words pre-processing"}, {"title": "II. PRELIMINARIES", "content": "QC operates quite differently from traditional computing. At its core, QC uses the quantum bit, or qubit, symbolized as ). What makes a qubit unique is its capacity to simultaneously represent two states, 0) and (1), thanks to a property called superposition, which is described by the equation |\u03c8\\rangle = \u03b1_1 |0\\rangle + \u03b1_2 |1\\rangle. Superposition allows a qubit to exist in a complex space, the Hilbert space, with dimensions that grow exponentially as 2n, where n is the total number of qubits.\nIn contrast to classical bits that represent a single state at a time, qubits' ability to embody dual states simultaneously allows quantum computers to process multiple computations in parallel. This processing is executed through quantum gates, depicted by unitary matrices U. These matrices are crucially unitary, meaning the complex conjugate transpose of U, denoted as U\u2020, is also its inverse, fulfilling the condition U\u2020U = I. This ensures the preservation of the inner product between vectors. However, extracting a conventional outcome from quantum processes necessitates measuring the qubits, which results in the collapse of their superposed state. In essence, practically speaking, QC involves manipulating the quantum state of qubits to, for example, increase the likelihood of obtaining a specific solution."}, {"title": "B. Embedding classical data into quantum states", "content": "In QC, a prevalent method for integrating classical data into quantum states is called basis embedding. This technique converts classical data, formatted as bit strings, into representations within quantum states.\nGiven a sequence of bits a = a_1 a_2...a_n, basis embedding is the process of translating this sequence into the form |a\\rangle = |a_1\\rangle |a_2\\rangle ... |a_n\\rangle, with |a_1a_2...a_n\\rangle serving as a more compact notation.\nAdditionally, QC utilizes amplitude embedding, which encodes data into the amplitudes of quantum states, further expanding the range of possible computations.\nGiven a normalized vector a = (\u03b1_1, \u03b1_2, ..., \u03b1_N) where \u03b1_i \u2208 \\mathbb{R}, N = 2n and \\sum_{i=1}^{N} \u03b1_i^2 = 1, amplitude embedding is the process of mapping this vector into a quantum state |a\\rangle = \u03b1_1 |0\\rangle + \u03b1_2 |1\\rangle + ... + \u03b1_N |N-1\\rangle, where each amplitude \u03b1_i corresponds to the probability amplitude of the quantum state in the computational basis states |0\\rangle, |1\\rangle, ..., |N-1\\rangle.\nOther embedding methods, such as angle and Hamiltonian embedding, are also employed in QC to represent data in various quantum-friendly formats."}, {"title": "C. Certifiable robustness in machine learning", "content": "Consider a binary classifier f : X \u2192 {0, 1} that assigns an input vector x \u2208 X (Euclidean space) to a binary class f(x) \u2208 {0, 1}. Additionally, we define as y : X \u2194 [0, 1] its soft version which computes the model's logit value and the class prediction is made through 1[y > 0.5]. Further, let o : X X X \u2194 {0, 1} denote the oracle that compares the semantic content of two input data points and returns 1 if they are semantically the same. The research area of certifiable (provable) robustness aims to establish formal guarantees that under all admissible attacks within the budget, none will alter the model's prediction. Formally, certification at a point x involves evaluating if,\nf(x) = f(x')  \\forall x' \u2208 X s.t. o(x, x') = 1.  (1)\nThe assessment of semantic similarity between data points is complex and varies by task. This complexity is mitigated by considering two inputs semantically similar if their lp-norm difference is less than a small threshold \\epsilon, translating the robustness certification problem at x into:\nf(x) = f(x')  \\forall x' \u2208 X s.t. ||x - x'||_p \u2264 \\epsilon. (2)\nFor neural networks, robustness verification can be structured as a mixed-integer linear program (MILP) based on the network's weights [21]. However, verifying neural network properties, especially with ReLU activations, is proven to be NP-complete [22]. As a workaround, approximation methods are used, which might not certify all non-altering perturbations [23]. Randomized Smoothing (RS) [11], [12] has emerged as a notable method in the relaxed certification category, enabling robustness assessment independently of the classifier's details."}, {"title": "D. Randomized smoothing", "content": "Randomized Smoothing as proposed by Cohen, Rosenfeld, and Kolter [11] and Salman, Li, Razenshteyn, et al. [12], constructs a smoothed classifier g by evaluating the original classifier f's logits y around the input, defined as:\ng(x) = \\mathbb{E}_{z\u223c\\mathcal{N}(0,\u03c3^2I)} [f(x + z)].  (3)\nHere, the focus shifts from a direct prediction at x to the expected prediction over noisy samples distributed normally around x. In practice, the goal is to determine the minimum value of g over a ball of radius \\epsilon centered at zero in the lp space, and check if this minimum value exceeds, formally defined as:\n\\min_{\u03b4\u2208\\mathcal{B}_p(\u03b5)} g(x + \u03b4) s.t. \\mathcal{B}_p(\u03b5) := {\u03b4 \u2208 X : ||\u03b4||_p \u2264 \u03b5}. (4)\nThus, let \u03a6 denote the CDF of the standard normal distribution \\mathcal{N}(0, 1), and p_1 be a lower bound of Eq. (4), then g(x) as constructed in Eq. (3) is certifiably robust if \\epsilon < \u03c3\u03a6^{-1}(p_1) for 1D classifier, i.e., X = \\mathbb{R} [11]."}, {"title": "III. ROBUSTNESS IN QML", "content": "Various approaches for certifiable robustness have been suggested for QML. In this extended section, we outline known methods for issuing robustness certificates and highlight their usage of perturbations. This leads to a conversation about the importance of jointly designing data encoding and perturbations."}, {"title": "A. Existing work on robustness certificates in QML", "content": "Weber et al. [13] draw a theoretical correlation between binary quantum hypothesis testing (QHT) and certifiable robustness: while QHT aims at optimally distinguishing two quantum states, robust classification seeks to provide provable guarantees that two states are not distinguishable. Based on this connection, they derive a robustness bound for quantum classifiers subject to worst-case input noise, which they prove to be tight for binary classification. This bound, however, only holds in the regime where the probability of measuring the true class probability P_A is always larger than the runner-up probability estimate P_B, which boils down to assuming perfect classification accuracy in an unperturbed setting. Such an assumption is common and necessary within the field of randomized smoothing to obtain robustness certifications, as seen in prior works [11], [12]. However, this assumption may not be reasonable for machine learning models that are often overly confident and vulnerable to various uncertainties in practical applications. To address the limitations of assuming an ideal classifier, alternative approaches, e.g. conformal prediction [24], [25], might be required.\nAnother critical aspect we want to emphasize is the need of carefully considering the type of data encoding for the robustness bounds in [13] to provide meaningful insights on the trust that can be given to the prediction of a perturbed data point. More precisely, as the bounds are based on the fidelity between input states, only non-zero fidelities allow useful conclusions on robustness. For example, when using basis state encoding, even the smallest changes in the input data result in zero fidelity, thus prohibiting any statement about predictions based on altered data. While this aspect can be easily resolved by employing, e.g., the widely used rotation encoding, there is one more issue we want to highlight, which is related to the scalability of fidelity. As demonstrated by \u017byczkowski and Sommers [26], fidelities between random states vanish exponentially in the number of qubits and hence even perturbations of small amplitude can cause a severe drop of certified robustness. This includes disturbances that lead to a low level of fidelity, yet may not directly impact the outcome of the algorithm we are analyzing. The last point of criticism is best explained with a pathological example: Consider a binary classification algorithm A defined on a parameterized quantum circuit with two distinct subsystems, denoted A and B. The algorithm maps inputs x to label 1 if the probability of one of the qubits in subsystem A being in the |1\\rangle state is larger than the corresponding probability for subsystem B:\nA(x) = \\begin{cases}\n1, & \\text{if } \\langle \u03a8 | M_A | \u03a8 \\rangle > \\langle \u03a8 | M_B | \u03a8 \\rangle, \\\\\n0, & \\text{otherwise}.\n\\end{cases} (5)\nFor simplicity, consider three qubits with the first two constituting subsystem A and the last one subsystem B. The measurement operators then take the form:\nM_A = |100\\rangle\\langle 100| + |110\\rangle\\langle 110| + |010\\rangle\\langle 010| + |000\\rangle\\langle 000|\nM_B = |101\\rangle\\langle 101| + |111\\rangle\\langle 111| + |011\\rangle\\langle 011| + |001\\rangle\\langle 001|\nAssuming the state |100\\rangle is to be certified, a perturbation could manipulate the state as far as |010\\rangle, resulting in a fidelity of zero and thus a trivial certification radius of zero according to Weber et al. [13]. At the same time, the algorithm would still be no less certain to return label 1 (the perturbed state is no closer to the decision boundary \\langle \u03a8' | M_A | \u03a8' \\rangle > \\langle \u03a8' | M_B | \u03a8' \\rangle).\nThis example is admittedly constructed, but consider generalized versions of it on larger systems. For instance, this might include a classification task over many input features, where only a few are indicative of the label to be returned, e.g.,"}, {"title": "B. Co-Design of data encoding and perturbation type", "content": "In this article, we investigate QML models applied to classical data. As such, attacks and perturbations are applied in input space, and the perturbed x is encoded into the quantum circuit. Quantum randomized smoothing introduces disturbances to the quantum circuit to achieve a more reliable prediction by analyzing the prediction statistics from multiple altered versions of the same data point.\nIf one is interested in a worst case robustness, as in adversarial defense, bounded perturbations are assumed and an unstructured perturbation is applied, e.g., Gaussian or uniform noise. If randomized smoothing is performed with other, potentially more structured perturbations, valid certificates can still be obtained, although certification radii can become overly conservative, as worst case robustness often already is. To avoid overly cautious certificates, a perturbation applied on the quantum circuit should be designed such that the perturbed state could be obtained by directly encoding a perturbed input data point, i.e., there should be an equivalent perturbation applied directly to the input data domain that produces the same state as when applying the perturbation on the quantum computer to the encoded state of the unperturbed data.\nA second case is the certification of robustness to certain types of perturbations or noises. If these are resulting from the quantum hardware, perturbations which do not have an equivalent input domain perturbation may be useful, e.g., as they directly reflect the hardware noise. We put our focus on the more common case of certifying input space robustness. In this case, again, perturbations on the quantum circuit do need to have an input space interpretation.\nWith this necessity in mind, consider Tab. I where a selection of encoding and perturbation types is listed. For each combination, an interpretation of the induced changes on the state vector in input space is indicated. We leave it to the reader to work through all combinations and only give one example here: Consider amplitude encoded data acted upon by a depolarization channel. We can write the perturbed state as\n|\\tilde{\u03a8}\\rangle = \\sum_{i=1}^{N} \\tilde{\u03b1_i} |i\\rangle\n= \\sum_{i=1}^{N} \\sqrt{(1-p)} \u03b1_i |i\\rangle + \\sum_{i=1}^{N} \\sqrt{\\frac{p}{N}} |i\\rangle\n= \\sqrt{(1-p)} \\sum_{i=1}^{N} \u03b1_i |i\\rangle + \\sqrt{\\frac{p}{N}} \\sum_{i=1}^{N} |i\\rangle\nfrom which we can see that the feature contrast, the difference between any two features \\tilde{\u03b1_i} and \\tilde{\u03b1_j} is diminished by a factor of \\sqrt{1-p} compared to the contrast of unperturbed \u03b1_i and \u03b1_j and disappears completely as p approaches 1. Additionally, we see that |\\tilde{\u03a8}\\rangle can again be written as an amplitude encoded state. Hence, there exists an equivalent classical perturbation which could be used to obtain the \\tilde{\u03b1_i} coefficients.\nAs highlighted in the last column of Tab. I, only few of the combinations of data encoding and perturbation type suffice the requirement of interpretability outlined above. Many others either access states which could not be reached by an equivalent classical perturbation with subsequent encoding, or lack a clear intuition of the induced changes. In quantum randomized smoothing, we suggest to be aware of the interplay"}, {"title": "IV. k-HAMMING DISTANT STATES", "content": "In this section, we describe how to construct a distribution around a binary input x \u2208 {0, 1}^n of k-Hamming distant states. This differs from the method of preparing Dicke states, where a Dicke state is an equal-weight superposition of all n-qubit states with Hamming weight k [27]. In our approach, we prepare a state \u03c8 : {0, 1}^n \u2192 \\mathcal{D}(\\mathcal{H}) that depends on x.\nGiven a binary input x \u2208 {0, 1}^n, we define a constrained k-Hamming distance set as:\n\\mathcal{D}_{n,k}(x) = {x' \u2208 {0, 1}^n : ||x - x'||_0 = k},  (6)\nwhere the zero norm\u00b9 corresponds to the non-zero elements differing from x.\nFor any given positive integer Hamming weight k, with k \u2264 n, the number of states that are exactly k-Hamming distances away from a given state is given by the binomial coefficient {n \\choose k}. We can formalize the approach for allocating probabilities to states based on their Hamming distances from the input state, including a maximum probability for the input state itself (i = 0) and decreasing probabilities for states with increasing Hamming distances (i = 1 to k). To do this, we need to ensure that: (i) the input state (i = 0) has the highest probability, (ii) probabilities for states decrease as their Hamming distance from the input state increases and (iii) probabilities are equally distributed among states at the same Hamming distance.. First, we define a base weight of e^{-i\u03c3} for each Hamming distance 0 \u2264 i \u2264 k, including a special consideration for i = 0 to ensure it receives the maximum weight of 1. Here, \u03c3 > 0 works as hyper-parameter that adjusts the distribution of weights. A larger \u03c3 would result in a slower decrease in weights with increasing i, allowing for more uniform distribution of probabilities across different Hamming distances. Thus, the normalized weight for each Hamming distance i is given by:\nw(i) = \\frac{e^{-i\u03c3}}{\\sum_{i=0}^{k} {k \\choose i} e^{-i\u03c3}},  \\forall 0 \u2264 i \u2264 k. (7)\nFinally, to assign probabilities to individual states at a given Hamming distance i, each state receives a fraction of the total probability allocated to that distance:\np(i) = \\frac{w(i)}{{k \\choose i}} \\text{ for each state at distance i}. (8)\nAs an example, let us consider an input vector x = (0, 1, 1) which has a length of n = 3; there are exactly three states that\n\u00b9the zero \"norm\" is not truly a norm."}, {"title": "A. Construction of k-Hamming distant states", "content": "Given an input x \u2208 {0, 1}^n, we construct a superposition of k-Hamming distant states from x in the Hilbert space as:\n|\u03c8(x)\\rangle = \\sum_{i=0}^{k} \\sqrt{p(i)} \\sum_{x'\u2208\\mathcal{D}_{n,i}(x)} |x'\\rangle. (9)\nThese can be understood as the probabilities of staying in the original state x or transitioning to the nearest state that is k-Hamming distant away.\nA direct approach to construct the quantum state |\u03c8(x)\\rangle involves first calculating the probabilities associated with the k-Hamming states. Following this step, the quantum circuit is prepared using the M\u00f6tt\u00f6nen state preparation method [28], a widely recognized technique for incorporating data into the amplitudes of quantum states. Despite its effectiveness, this approach encounters a significant limitation due to the exponential increase in the depth of the circuit, symbolized by O(2^n), which pertains to the exponential growth in the requirement for both CNOT and single-qubit rotational gates as the number of qubits increases. This exponential increase presents an additional challenge, specifically when attempting to enumerate classically all the probabilities corresponding to states that are k-Hamming distances apart. As the dimensionality of the input grows, this task becomes increasingly unmanageable, highlighting a critical bottleneck in the scalability of this method for larger quantum systems."}, {"title": "a) Our state preparation", "content": "To construct the quantum state |\u03c8(x)\\rangle effectively, we propose a method that includes the use of extra qubits, termed ancillary qubits, along with controlled rotational gates. This method is enhanced by introducing a technique that involves the inverse application of multi-controlled basis embedding for the input. This enables the design of a more efficient circuit that requires the addition of only one qubit for every input dimension. In Fig. 1, we showcase the procedure for setting up a circuit for a three-dimensional input. The process begins with the initial preparation of the x state using a basis embedding technique. Following this, n ancillary qubits are introduced, and a combination of Hadamard gates and controlled RY gates, adjusted by \u03c3\u03c0, are applied. In this scenario, \u03c3\u2208 [0,1] and an increase of its value denotes an overall increase on the likelihood of neighbouring states. We observe that for \u03c3 = 0.5 the controlled RY rotation corresponds to a controlled Hadamard, hinting the quantum Fourier transform state preparation. Finally, the ancillary qubits are manipulated with a series of PauliX gates before the input undergoes a dual embedding in the basis via a multi-controlled gate. Inverting the ancillary qubits prior to introducing a controlled input decreases the likelihood of states that are distant from the input.\nUnlike the M\u00f6tt\u00f6nen state preparation method, our technique does not replicate the distribution with exact precision. In practical terms, this situation means that while our method can effectively adjust the likelihood of certain quantum states being realized specifically, those within a k-Hamming distance from a given state it also unintentionally boosts the chances of encountering states beyond this specified Hamming distance. Essentially, when we attempt to increase the probability of closely related states (those a few quantum flips away), we inadvertently also make it more likely to observe states that are further away than intended. This can introduce inaccuracies in quantum simulations or computations that rely on precise probability distributions."}, {"title": "b) Uniform state preparation", "content": "In Fig. 2, we detail the process for setting up a uniform distribution centered on a three-dimensional input along with its corresponding probabilities. Continuing from the earlier discussion, the probabilities for adjacent states are uniformly likely across all states. This method of state preparation is more efficient since it only necessitates a single ancillary qubit, and we can also adjust \u03c3 to boost the probability of nearby qubits through a RY gate on the ancillary. Nonetheless, treating the probability of neighboring states as equally likely for all states presents a drawback when considering semantically meaningful perturbations for the input state."}, {"title": "B. Certified robustness for k-Hamming distant states", "content": "Here, we formally describe the computation of the certified robustness distance in relation to a distribution that is k-Hamming distant from the given input. We focus on the robustness certificate based on the l_0-norm within a binary input domain X = {0, 1}^n. Specifically, let p_i : X \u2192 [0, 1] denote the likelihood that smoothing the base classifier f with a k-Hamming distant distribution around x yields class i, formally:\np_i(x) = \\mathbb{E}_{x'\u2208\\mathcal{D}_{n,k}(x)} [f(x') = i], (10)\nwhere \\mathcal{D}_{n,k}(x) represents the set of k-Hamming distant vectors for x. As established by Lee, Yuan, Chang, et al. [29], for any input x with ||x - X||_0 \u2264 k across all classes i \u2208 K, the following holds:\np_i(x) - p_i(x') < \u2206, \\text{ where } \u2206 = \\text{argmax}_i p_i(x), \u2265 0.5, (11)\ndefines the certified robustness distance. Based on the architecture of the base classifier, directly computing p_i(x) might not be feasible. However, we can instead generate a finite set of samples from \\mathcal{D}_{n,k}, in order to estimate a lower bound pi(x) with high confidence (1 \u2212 \u03b1). In practice, for an input x with ||x - x||_0 \u2264 k,\n\\text{if } p_i(x) - \u2206 > \\frac{1}{2}, \\text{ then } \\underset{j}{\\text{argmax}} p_i(x)_j = i,\nwith probability at least 1 \u2013 \u03b1 [11].\nIn practice, to establish a lower bound for p_i(x), traditional methods employ the Clopper-Pearson interval [11]. Nonetheless, the quantity of samples, which corresponds to the number of requests made to the base classifier, increases following an O(1/\u03b1^2) complexity, where \u03b1 represents the target error margin within the expected probability value, delineated over the randomized smoothing area surrounding the input. In the context of QML, Sahdev and Kumar [15] demonstrate that for achieving the same level of confidence and error margin, the requisite number of inquiries to the base classifier is reduced to O(1/\u03b1). QuAdRo [15] employing m ancilla qubits for the quantum phase estimation allows for a maximum of M = 2^{m+1} - 1 oracle calls. In addition, the likelihood of success approaches nearly 100% by conducting multiple trials and selecting the median outcome."}, {"title": "A. Setup", "content": "Datasets: In this experiment, we consider two datasets: (i) Iris [31] and (ii) GunPoint from the UCR time series [30] archive. In preprocessing the Iris dataset, the versicolor class is removed, virginica is mapped to 0 and setosa to 1. The input feature petal width is excluded, all samples are normalized to unit norm, and then split into a 60/40 training/testing ratio. In the preprocessing of the GunPoint dataset from the UCR time series archive [30], we consider using a Bag-of-Words [32]\u2013[34] approach to create a new representation of the data with reduced dimensionality. Initially, the dataset is divided into training and testing sets, from which only the first half of each time series is selected for further processing. This truncation serves to focus on a specific portion of the time series that is of interest. The Bag-of-Words model is then applied to these truncated time series as shown in Fig. 3. The model operates by dividing each time series into windows of a specified size (15 in this case) and encoding the data within each window into words based on a predefined number of bins (2 bins here) and a word size of 2. The resulting data is a binary vector of size 10, i.e. x \u2208 {0, 1}^{10}. This representation of time series can also be made more expressive by increasing the number of bins and the word size. Since it remains quantized by design, a mapping to a basis state encoding can always be undertaken easily.\nWith the Bag-of-Words preprocessing, input space perturbations appear as bit-flips of the binary data representation. Specifically, if a region of the data snippet is perturbed sufficiently strong that its amplitudes falls into a different bin, the corresponding bit is flipped. By constraining the number of bits that are flipped, i.e., the Hamming weight distance, the magnitude of the amplitude deviations induced by the perturbation can be controlled. Thus, we conclude that Hamming weight constrained bit-flip noise is particularly suitable for this type of basis state encoded data."}, {"title": "b) Network Architecture", "content": "In our experiments, we employ a QNN to function as an oracle in Grover's algorithm. This is achieved by incorporating a strongly entangling layer [35] as the primary architecture component. Given that our datasets are simplified to binary classification problems, we have trained the network on a singular output qubit to perform the classification. In Table II, we report the accuracy, number of qubits and layers for each dataset considered."}, {"title": "c) QuAdRo Parameters", "content": "In our application of QuAdRo [15], we employed the iterative quantum phase estimation [36] technique to decrease the number of ancillary qubits needed for phase estimation. Given M as maximum number of calls to the oracle, we consider m = \\lceil log_2(M) \\rceil. As a result, we execute the circuit m times, with each run involving \u221aN shots, in contrast to the N-times required by the standard implementation of randomized smoothing. For each iteration i, we evaluated the 2^{2i} power of Grover's algorithm and estimated the phase using a classical method. Employing all available m qubits for phase estimation allows for executing the circuit with only \u221aN repetitions. For determining the minimum value of pi(x), we adopt a 90% confidence level (\u03b1 = 0.1) using the Clopper-Pearson interval."}, {"title": "B. Evaluation of 1-Hamming distant state distributions", "content": "To conduct a practical comparison of the probability outcomes derived from various methods of creating states with k-Hamming distance, we opt to measure the certified accuracy of a QNN that has been trained on the Iris dataset. In Fig. 4, we showcase an assessment of quantum randomized smoothing, utilizing a fixed number of trials (one billion shots), to examine how certified accuracy varies with different values of \u03c3 while maintaining a 1-Hamming distance. This analysis highlights the impact of varying the probability of a state flipping to another state that differs by a Hamming distance of 1. Generally, higher values of \u03c3 indicate a greater tolerance to state changes, reflected in the certified accuracy across the radius spectrum. While differences between amplitude encoding and our method of state preparation are noticeable at larger radii, both methods generally demonstrate similar behavior, emphasizing the commonalities in how states are prepared."}, {"title": "C. Comparison between uniform and 1-Hamming distant state distributions", "content": "Here, we compare the 1-Hamming distance distribution with respect to the uniform distribution in terms of certified accuracy. For both scenarios, we execute a plain version of randomized smoothing (RS) [11], which involves a QNN placed ahead of the input distribution, running for a number of shots that is quadratically higher in comparison to QuAdRo. Specifically, we run a set of shots {8^2, 10^2, 12^2, 50^2} for RS and {8, 10, 12, 50} for QuAdRo. In the context of QuAdRo, we implement the iterative quantum phase estimation which makes use of one single additional qubit. Consequently, the total quantum circuit consists of 6 qubits for the uniform distribution and 9 qubits for the 1-Hamming distribution. We conduct our experiments in the default.qubit simulator in PennyLane\u00b2.\nIn Fig. 5, we show a comparison for \u03c3 = 0.5 for the Iris dataset. The analysis shows that the 1-Hamming distribution has a tendency to reach a higher level of certified accuracy when compared with the uniform distribution. This result is attributed to the increased probability of locating states in proximity to the initial state, as opposed to states dispersed uniformly at a distance. In particular, for QuAdRo, achieving high certified accuracy requires fewer than 50 shots of the circuit, compared to 144 shots for RS. With enough shots, both distributions achieve a certified accuracy of 1. This outcome is because the Iris classifier achieves perfect clean accuracy and with \u03c3 = 0.5, as demonstrated in Fig. 2 and Fig. 1, maintains the probability of the original input higher than that of the neighbors.\n\u00b2PennyLane version 0.33.1 https://github.com/PennyLaneAI/pennylane."}, {"title": "D. Certified robustness for time-series analysis", "content": "In Fig. 6, we present a comparison based on the GunPoint dataset with \u03c3 = 0.5. As previously described, the input was transformed using a Bag-of-Words model, resulting in a binary format to encode the state within the basis. In addition, we consider the iterative quantum phase estimation, which requires 13 qubits for a uniform distribution and 22 qubits for a 1-Hamming distribution. In the 1-Hamming distribution case (Fig. 6.a), QuAdRo consistently outperforms RS across all radii, with iterations of 20, 50, and 120 for QuAdRo compared against 50^2, 100^2, and 500^2 samples for RS. Within the uniform distribution, we observe (Fig. 6.b) that QuAdRo demonstrates a quadratic improvement when executed for 50 and 100 iterations compared to RS.\nDue to the limited number of measurements, the phase estimation of QuAdRo does not precisely fit the exact radius. This causes QuAdRo to exhibit smoother performance compared to RS. The likelihood of accurately measuring the phase up to m bits is \\frac{1}{2^m}. By conducting the experiment repeatedly and applying the median estimate [37], we can rapidly achieve a 100% accuracy rate. In practice, QuAdRo obtains probabilistic robustness certificates, similar to RS, but with a quadratic speed-up in the number of shots required. The average runtime for the simulated circuit with a 1-Hamming distribution is roughly 21 minutes per sample on a server with a 128 CPU cores and 775 GB of RAM memory."}, {"title": "VI. DISCUSSION OF EXPERIMENTAL RESULTS", "content": "The challenges of implementing the quantum phase estimation algorithm are significant for NISQ devices. To address these challenges, our approach includes several key considerations. First, using the Bag-of-Words model to represent data helps reduce the dimensionality of time-series data while retaining crucial information for accurate classification. Second, our construction of the k-Hamming distance for state preparation enables a relatively shallow circuit. Although this circuit approximates the exact distribution, the depth is three orders of magnitude smaller than that required for amplitude state preparation. This reduction is crucial for the overall preparation process of the quantum phase estimation. Third, the use of iterative quantum phase estimation decreases the number of qubits needed for precise phase estimation. This makes the QuAdRo algorithm more compatible with current quantum hardware, which is restricted by qubit stability and coherence times.\nExperimental findings demonstrate the efficacy of the 1-Hamming distance approach in achieving higher certified accuracy, especially in scenarios with limited shots. This result highlights the advantages of using constrained perturbation models like the 1-Hamming distance in situations where it is reasonable to consider bit-flips as principal source of noise. However, it is crucial to recognize that other sources of noise can significantly impact the practical implementation and benefits of robustness"}]}