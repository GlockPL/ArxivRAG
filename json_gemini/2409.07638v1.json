{"title": "CAN WE COUNT ON LLMS? THE FIXED-EFFECT FALLACY\nAND CLAIMS OF GPT-4 CAPABILITIES", "authors": ["Thomas Ball", "Shuo Chen", "Cormac Herley"], "abstract": "In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4 perfor-\nmance on several deterministic tasks; each task involves a basic calculation and takes as input param-\neter some element drawn from a large well-defined population (e.g., count elements in a list, multiply\ntwo k-digit numbers, etc). We examine several conditions per-task and perform enough trials so that\nstatistically significant differences can be detected. This allows us to investigate the sensitivity of\ntask-accuracy both to query phrasing and input parameter population. We find that seemingly trivial\nmodifications in the task-prompt or input population can yield differences far larger than can be\nexplained by sampling effects. For example, performance on a simple list-counting task varies with\nquery-phrasing and list-length, but also with list composition (i.e., the thing-to-be-counted) and ob-\nject frequency (e.g., success when an element accounts for \u2248 50% of a list is different from when it\naccounts for \u2248 70% etc).\nWe conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect\nfallacy, where experimental observations are improperly generalized beyond what the data supports.\nA consequence appears to be that intuitions that have been formed based on interactions with humans\nform a very unreliable guide as to which input modifications should \"make no difference\" to LLM\nperformance.", "sections": [{"title": "1 Introduction", "content": "Rapid improvements in the performance of large language models (LLMs) have spurred great interest in evaluating\ntheir capabilities. In addition to answering general knowledge questions and summarizing text, GPT-4 has demon-\nstrated the capability to compose poetry, solve chess puzzles and Geometry problems, and perform basic coding tasks.\nCapabilities that seem beyond the simple next-token-prediction they were trained on, causes some to suggest this as\nevidence of emergent behaviors from LLMs, or even that we may be witnessing the early signs of Artificial General\nIntelligence (AGI) (Bubeck et al., 2023). Others are not convinced, and suggest that LLMs simply parrot pastiches of\ntext snippets from their training sets (Bender et al., 2021).\nThe documentation of surprising capabilities has been accompanied by many accounts of failures. Hallucinations\n(where LLMs offer plausible but entirely invented detail) have proved hard to eliminate. Arkoudas points out that\nGPT-4 struggles with some basic tasks that humans find easy or trivial; e.g., they aren't reliable even on tasks such\nas counting, multiplication, etc (Arkoudas, 2023). McCoy et al suggest that many of the remarkable capabilities are\nsimply artifacts of the training set and autoregressive task that GPT-4 was trained to solve (McCoy et al., 2023).\nAn accumulation of observed successes and failures at particular tasks unfortunately does little to settle questions about\nLLM reliability or capabilities. In this paper we present results on a series of deterministic tasks; each of the tasks\ninvolves a basic calculation and takes as input parameter some element drawn from a large well-defined population\n(e.g., count elements in a list, multiply two k-digit numbers, etc). Since, by construction, the correct answer is easy\nto determine, we can measure performance without costly and subjective hand-labelling or assessments. By randomly"}, {"title": "2 Background: The Language-as-Fixed-Effect Fallacy", "content": "The Language-as-Fixed-Effect Fallacy, as described by Clark (Clark, 1973), is the phenomenon where a claim sup-\nported by statistical evidence does not generalize beyond the specifics of the experimental setup. He illustrates with a\nlanguage-task thought-experiment originally proposed by Coleman (Coleman, 1964). Let N be the set of all English\nnouns, V the set of all verbs, and let $T(.)$ be a test statistic representing how well humans perform at some task\ninvolving words (e.g., how well they can spell them, how quickly they can type them, etc). Suppose that experimenter\nA wishes to test the hypothesis that people perform the task better on nouns than on verbs:\n$H_a = T(N) > T(V)$.\nSuppose experimenter B wishes to test the opposite:\n$H_B = T(N) < T(V)$.\nLet's stipulate, by contrast, that they are both wrong, and that $T(N) = T(V)$.\nAs a test of $H_A$ the first experimenter selects subsets $N_A \\subset N$ and $V_A \\subset V$ each with some fixed number of\nrandomly selected nouns and verbs. With this choice she recruits participants and on finding that $T(N_a) > T(V_a)$,\nby a statistically significant amount, she rejects the null hypothesis (that there's no difference) and concludes she\nhas firm evidence in favor of $H_A$. Similarly, the second experimenter selects at random different subsets $N_B \\subset N$\nand $V_B \\subset V$ with fixed numbers of nouns and verbs. With this fixed choice he recruits participants and finds\n$T(N_B) < T(V_B)$, by a statistically significant amount, and concludes this is firm evidence for $H_B$.\nThe problem is that while both A and B intend to generalize to the whole population N and V they have tested only\non particular subsets. There is good evidence to believe that, with any collection of participants, we could verify both\n$T(N_A) > T(V_A)$ and $T(N_B) < T(V_B)$, but neither of these is enough to support either $H_A$ or $H_B$. In the language\nof statistical testing our experimenters have treated random effects as fixed (Clark, 1973).\nFixed effects are those that are considered constant across the relevant population, while random effects are those\nthat vary (for an account of various other definitions see (Gelman, 2005)). In the experiments above there are two\npopulations involved: the populations of noun-verb collections, and the population of human participants. When\nshe generalized from $N_A, V_A$ to $N, V$ our first experimenter implicitly assumed that any other subsets $N_C \\subset N$\nand $V_C \\subset V$ would also give the result that she observed (i.e., $T(N_c) > T(V_c)$). If this were true she'd be\njustified in thinking that her observed difference was powerful evidence for $H_A$. If this is not true then her experiment\nsupports only the narrow uninteresting claim $T(N_a) > T(V_a)$. Effectively, she assumed that what she observed\nwasn't particular to $N_A, V_A$ but general to N,V.\nIn a colloquial sense fixed effects are ones where the particular choice doesn't affect the generality we wish to claim.\nWe expect, for example, that what an experimenter had for breakfast or what color socks she was wearing has no\neffect on the outcome; these are not details that have to be faithfully reproduced to ensure replication of the original\nexperiment. In this telling the fixed-effect fallacy is simply assuming that certain details don't matter when in fact\nthey do. Unfortunately, there's no simple way to determine that a certain variable has no influence on an experimental\nresult; experiments necessarily involve many judgements about which details matter and which do not, and many\nof those judgements are subjective. One of our findings is that intuitions about which modifications might make a\ndifference can be very flawed; that human performance remains constant under a certain modification is no guarantee\nat all that LLM performance also will."}, {"title": "3 The Fixed-Effect Fallacy and LLM Task Performance", "content": "We wish to evaluate whether, and how well, an LLM can perform a particular task that has a single deterministic\ncorrect answer (e.g., counting, deciding to invoke a plug-in, or Retrieval-Augmented Generation etc). For the counting\ntask one approach might be to produce a list of objects and prompt the LLM to count the occurrences of a particular\nitem. To make the experimental setup concrete we might specify a list length and dictionary of possible elements. For\nexample:\nrLen = 20\nlistOfItems = ['mango', 'peach']\nr = random.choices(listOfItems, k = rLen)\nis a Python snippet that will return a length-20 list with the elements of listOfItems chosen at random with replace-\nment. When there are only two elements, as shown, there's a population of $2^{20}$ such lists; call this population R. We\nmight prompt the LLM with:"}, {"title": "4 Tasks", "content": "We wish to be clear that our goal is not to determine whether LLMs can or cannot count, sort, or multiply, etc. First,\nwe have other ways of performing these tasks. Second, it is possible that prompt engineering, providing few-shot ex-\namples, the use of Chain-of-Thought reasoning, or the invocation of plug-ins might sometimes improve performance.\nHowever, our goal is not to improve the accuracy in particular settings. Rather, it is to draw attention to an unaddressed\ndifficulty in establishing accuracy: evaluation of LLM capabilities seems particularly susceptible to a major pitfall that\nexists when we go from particular experimental observations to general claims. That is, sensitivity to seemingly trivial\nmodifications means that observed accuracy numbers cannot be assumed to generalize (even to entirely equivalent ver-\nsions of a task). So, for example, while prompt engineering might yield accuracy improvement on a particular version\nof a task, we can't assume that that improvement will be observed in rephrased versions. While we've demonstrated\nthe problem on basic arithmetic tasks it seems unlikely to be confined to that domain. For example, LLM performance\nat certain tasks might be improved by invoking a plug-in, writing code or using Chain-Of-Thought, but deciding when\nand how to do so is itself a task with success rate subject to the sensitivities we highlight. That is, invoking plugins\ndoesn't solve even the basic counting task if the decisions on when and which plugin to invoke is itself brittle and\nsensitive to prompt phrasing.\nSo, can LLMs count (or multiply, or sort etc)? Our evidence suggests that variation as we sample possible phrasings is\ntoo high to allow a Yes-or-No answer, and that accuracy estimates must be regarded as particular to the experimental\nsetup used. This also means that reporting observed performance or accuracy numbers on other deterministic tasks\n(such as standardized tests (Katz et al., 2023; Takagi et al., 2023; Nori et al., 2023), textbook problems, etc) is not\nsufficient to establish general capabilities."}, {"title": "4.1 Experimental setup", "content": "In order to test LLM performance we choose tasks that have deterministic answers, and where it is relatively easy\nto decide if the LLM gives the correct answer. This obviates the need for subjective assessments, heuristics, hand-\nlabelling or error-prone parsing of the response, and allows us to scale-up testing. The tasks we examine are: counting,\nfinding the maximum, median and sorted version of a list of numbers, and long multiplication. The difficulty with\ncounting and long multiplication has been observed by others (Arkoudas, 2023).\nUnless otherwise specified all of the conditions were evaluated on 500 independent runs. Thus, for example, if a table\nentry reports a success rate of 89.0% on a task, and sampling were the only source of randomness, then a reasonable\nestimate of the 95% confidence interval would be $1.96 \\cdot \\sqrt{0.89 \\times 0.11/500} \\approx 2.74$%. However, an important finding,\nbelow, is that there are significant other sources of randomness, and the conventional way of estimating margins-of-\nerror cannot be applied. All of the trials are performed using the OpenAI GPT-4 API with a temperature setting of 0.7.\nThe results of all queries are available in the GitHub repository.\nFor all of the tasks we give an example prompt together with the correct answer and GPT-4's answer. Due to space\nconstraints we show only examples where the GPT-4 response is incorrect. This is not reflective of its accuracy: in\neach case we give a table showing how accuracy evolves with problem size. However, in giving examples where the\nanswers are incorrect we illustrate that they are often very significantly better-than-random."}, {"title": "4.2 Count", "content": "First we examine the capability of GPT-4 to perform basic counting tasks. We choose a length-rLen list with two\npossible elements and ask GPT-4 to count the number of occurrences of the first element. An example query is (let's\ncall this wording #1):\nThis gives us a total of four conditions, all of which involve the same basic counting task. We evaluate each condition\nwith list lengths rLen= 10, 15, 20, 30 and 40, and we perform 500 trials per condition. The results are shown in Table\n1. Thus, the five rows and first four columns represent a total of 5 \u00d7 4 \u00d7 500 = 10,000 queries to GPT-4.\nWe use a $x^2$ test to determine if the responses to different ways of phrasing the task are drawn from the same distri-\nbution. For example, we can take the null hypothesis to be that some row of the first and fourth columns of Table\n1 represent answers drawn from the same distribution. E.g, for rLen= 10 there were 445/500 and 483/500 correct\ntrials respectively. Using a standard $x^2$ test to compare these two distributions of correct/incorrect answers yields\n$(x^2 = 20.49, df = 1, p = 6.0e-6)$. The p-value can be taken as an estimate of the probability of these results\nbeing observed if columns 1 and 4 of row 1 were produced by the same process; generally when p < 0.05 we say that\nthe null hypothesis is rejected. Similarly for all the other rows, the hypothesis (that results of the task with different\nwording are drawn from the same distribution) is rejected. The degrees-of-freedom is df = 1 for all of our tests since\nwe are always doing pairwise comparisons on tasks on a binary outcome (Taylor & Thompson, 1982).\nThe results of our $x^2$ tests are given in the right-hand side of Table 1. The null hypothesis is robustly rejected for\nall lengths when comparing columns 1 and 4 (i.e., simply switching between wording #1 and wording #2 with the\n'mango/peach' word-pair). The null hypothesis is rejected for several lengths when comparing columns 1 and columns\n2, 3 (i.e., simply switching the word-pair while using wording #1). This demonstrates that simple modifications of the\ntask (that might easily be assumed to make no difference) in fact are sources of variance beyond what can be explained\nby sampling effects."}, {"title": "4.3 Maximum, Median and Sort", "content": "Here we ask GPT-4 to perform elementary tasks on lists of numbers: return the maximum, median and sorted version\nof the list. We evaluate three different conditions. First we ask for the maximum (or median or sorted version) of a\nlist of rLen numbers drawn uniformly-at-random from the interval (100.0, 20000.00) and rounded to two decimals\nplaces. An example of the prompt for the median-finding task is:\nSecond, we repeat with integers drawn uniformly-at-random from (10,99) (i.e., all list-members are 2-digit numbers).\nFinally, we use a list of rLen name-value pairs, where a randomly-chosen name is associated with a number drawn\nuniformly-at-random from the interval (100.0, 20,000.00) and rounded to two decimals places. An example of the\nlatter query is:\nThe results of the maximum, median and sorting tasks are given in Tables 2, 3 and 4 respectively. The three different\nlist conditions are explored in columns 1-3 of these tables. As in Section 4.2, we use a $x^2$ test to explore whether these\ndifferent variations on the task produce answers that appear drawn from the same distribution. The right-hand portion\nof Tables 2, 3 and 4 gives the results; we do $x^2$ tests to compare columns 2 and 3 with column 1.\nTable 2 shows the results of the maximum-finding task. Performance in all conditions is good, though not perfect\n(e.g, results are almost always > 90.0%). The $x^2$ tests show that the hypothesis that performance on the name-value\nversion of the list is consistent with performance on the value-only list is rejected for lengths > 11. The hypothesis\nthat performance on the integer version of the list is consistent with performance on the 2-decimal floats list is rejected\nfor all lengths.\nTable 3 shows the results of the median-finding task. Performance in all conditions is poor (e.g, results are < 90.0%).\nThe $X^2$ tests show that the hypothesis that performance when the numbers are drawn from (10.0, 20000.0) is consistent\nwith performance when numbers are drawn as integers from (10,99) is rejected for all lengths. The hypothesis that\nthat name-value version of the list is consistent with performance on the value-only list is also rejected for all lengths.\nNote that the p-values in both cases are < 0.05, so the probability that the same process accounts for both conditions\nis very low.\nTable 4 shows the results of the sorting task. Performance in condition 2 is good, but is very poor in condition 3 (e.g,\nresults in column 3 are < 55.0%). The $x^2$ tests show that the hypothesis that performance when the numbers are\ndrawn from (10.0, 20000.0) is consistent with performance when numbers are drawn from (10,99) is rejected for all\nlengths. The hypothesis that that name-value version of the list is consistent with performance on the value-only list\nis also rejected for all lengths. Again, the p-values indicate robust rejection of these hypotheses."}, {"title": "4.4 Long Multiply", "content": "Here we evaluate performance at long multiplication, where we prompt the LLM to calculate the product of a k\u2081-digit\nby a k2-digit number. An example for 4 \u00d7 4 is:\nTable 5 shows the performance multiplying a k\u2081-digit by a k2-digit number for k1,k2 \u2208 {2,3,4,5}. Apart from the\n2 \u00d7 2 case the results are largely poor. Observe that perfect performance on the 2 \u00d7 2 task drops to negligibly correct\nanswers for 4 x 4.\nSince there is sometimes a significant difference between the $k_1 \\times k_2$ result with the $k_2 \\times k_1$ result we perform a $x^2$\ntest on several of the off-diagonal elements. The results are shown in Table 6. Note that results for the 4 \u00d7 2 and 2 \u00d7 4\nare significantly different, as are those for 5 \u00d7 2 and 2 \u00d7 5. Thus, even the hypothesis that performance on the $k_1 \\times k_2$\nmultiplication will be equivalent to the $k_2 \\times k_1$ is rejected for at least some lengths.\nBoth Dziri et al (Dziri et al., 2023) and Arkoudas (Arkoudas, 2023) look at the example of long multiplication. Dziri\net al note that while the answers for 4 \u00d7 4 are almost always incorrect, the first and last two digits of the GPT-4 answers\nare almost always correct. They describe this as a matching of \"surface probabilities.\" That is, the first two digits of\na product are determined by the leading digits of the multiplicands irrespective of length. Thus, this portion of the\nanswer can always be determined without paying attention to the rest. Similarly for the last few digits."}, {"title": "5 Related Work", "content": "It is well understood that the form of a prompt can greatly affect the results from a LLM as a \"few-shot\nlearner\" (Brown et al., 2020), thus giving rise to the newly minted discipline of prompt engineering. For example,"}, {"title": "6 Discussion", "content": "We've shown in Section 4, the risk that measured performance with a specific prompt fails to generalize to equivalent\nversions of the task. This work complements others that have documented the brittleness of GPT-4's performance\n(see related work in Section 5). However, as far as we know, ours is the first to explore tasks with several different\nconditions and sufficient statistical power to rule out sampling noise as the source of observed variation. This allows\nus to state with some confidence that minor modifications have potentially enormous effects on measured capabilities.\nThis problem is entirely orthogonal to the frequently mentioned difficulty with hallucinations.\nEvery measurement experiment comes with decisions about which factors might affect the output, and which should\nmake no difference. Many of these decisions are implicit, and informed by our intuition and experience of the world.\nSince LLMs emulate many human capabilities it is tempting to use intuitions about humans to guide decisions about\nwhich factors should make no difference to LLM measurements. A key finding of this paper is that this assumption\nleads to errors that can be significant enough to invalidate claims. Bender observes that we've made \"machines that\ncan mindlessly generate text, but we haven't learned how to stop imagining the mind behind it.\" We suggest that the\ndangers of anthropomorphizing LLMs includes not just over-interpreting their capabilities, but also imagining that\ntheir robustness to variation resembles that of humans."}, {"title": "7 Conclusion", "content": "We have demonstrated that GPT-4 performance on simple tasks shows sensitivity to trivial modifications and that this\nerror can be enough to invalidate claims of capabilities. Despite the limited scope of our experiments, we believe our\nfindings point to a largely-ignored source of error that potentially affects evaluation of LLM capabilities on all tasks.\nThat is, on every task we've considered we've found that trivial modifications introduce variance that invalidates the\nusual margin-of-error estimates. Our evidence doesn't rule out the possibility that the problem might be larger, or\nsmaller, or negligible on some other tasks. However, deciding that this source of error can be ignored for a given\ncapability comes with a burden-of-proof, and is something that should be demonstrated empirically, rather than just\nassumed.\nWe find that even when modifications are trivial and make no difference to human performance on a task we cannot\nassume that the same is true of LLM performance. In the absence of evidence to the contrary, measurements of LLM\ntask-accuracy cannot be assumed to generalize beyond the precise conditions studied."}, {"title": "8 Broader impact statment", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A Appendix", "content": "Here we revisit accuracy measurements for the counting task studied in Section 4.2 but using the GPT-3.5, Mistral\nInstruct 7B Q4 and Llama 3 8B Q4 models. GPT-3.5 was accessed via the openai API. The Mistral and Llama models\nwere run locally using versions with quantized coefficients. Each cell in each table represents 500 trials. All trials\nwere done using a temperature setting of 0.7.\nThe results for these models are show in Tables 7, 8 and 9. Each of these might be compared with Table 1. As can be\nseen, the same pattern observed in Section 4.2 holds: the null hypothesis (that accuracy in the various conditions do\nnot differ significantly) is robustly rejected in a majority of cases."}]}