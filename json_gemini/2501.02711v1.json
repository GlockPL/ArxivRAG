{"title": "KG-CF: Knowledge Graph Completion with Context Filtering under the Guidance of Large Language Models", "authors": ["Zaiyi Zheng", "Yushun Dong", "Song Wang", "Haochen Liu", "Qi Wang", "Jundong Li"], "abstract": "Large Language Models (LLMs) have shown impressive performance in various tasks, including knowledge graph completion (KGC). However, current studies mostly apply LLMs to classification tasks, like identifying missing triplets, rather than ranking-based tasks, where the model ranks candidate entities based on plausibility. This focus limits the practical use of LLMs in KGC, as real-world applications prioritize highly plausible triplets. Additionally, while graph paths can help infer the existence of missing triplets and improve completion accuracy, they often contain redundant information. To address these issues, we propose KG-CF, a framework tailored for ranking-based KGC tasks. KG-CF leverages LLMs' reasoning abilities to filter out irrelevant contexts, achieving superior results on real-world datasets. The code and datasets are available at https://anonymous.4open.science/r/KG-CF.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge Graphs (KGs) have become foundational in numerous real-world applications, including recommendation systems [1] and knowledge editing [2]. Structured as relational data, KGs encode vast factual information using triplets, where each triplet (h, r, t) specifies a relation r between entities h and t (e.g., (Earth, orbits, Sun)). Despite their utility, KGs are inherently sparse and incomplete, necessitating Knowledge Graph Completion (KGC) to predict missing triplets and thereby enrich the graph [3]. Traditional embedding-based methods, such as RotatE [4], have shown competitive results in KGC but often lack the ability to integrate external knowledge, including commonsense information not explicitly represented in the graph [5]. To address this, recent research leverages pretrained language models (PLMs) [6], with large language models (LLMs) in particular receiving significant attention for their robust reasoning and generalization capabilities [7].\nDespite the growing interest in applying LLMs to KGC, current research still encounters significant limitations in practical applications. LLM-based KGC models primarily focus on triplet classification, performing binary evaluations (true or false) on potential missing triplets. However, most real-world KGs are sparse, with an imbalanced distribution of valid and invalid missing triplets. Conversely, traditional approaches (e.g., embedding-based models) emphasize ranking-based tasks, particularly entity prediction, which predicts missing entities for queries in the form (?, r, t) or (h, r, ?).This task requires models to generate a ranked list of candidate head or tail entities based on relevance and plausibility, forming candidate triplets with the query. Such ranking is crucial in practice, as prioritizing likely missing triplets enhances efficiency and flexibility. However, two intrinsic challenges in LLM-based models hinder their performance on ranking-based tasks: (1) From the graphs's perspective, existing LLM-based frameworks [8], [9] primarily extract and input graph contextual information (e.g., graph topology, textual descriptions), so-called graph contexts, in textual form to enhance completion. However, in KGC tasks, some extracted graph contexts are irrelevant to the existence of given candidate triplets, introducing substantial redundancy and diverting the LLM's focus from the KGC task. (2) Sequential generation in LLMs makes them poorly suited for handling numerical values like floats [10], complicating the generation of precise plausibility scores for ranking candidate entities or triplets. Standard LLMs produce numbers digit by digit, leading to cumulative sequence errors [11]. This limitation also affects generating ranking lists. Moreover, triplet labels used in training are discrete (e.g., true/false), making it challenging to align these labels with continuous score outputs for autoregressive LLMs.\nTo address these challenges, we introduce KG-CF (Knowledge Graph Completion with Context Filtering). In this framework, LLMs are dedicated to filtering out irrelevant contextual information. Specifically, for any triplet (h, r, t) in a knowledge graph G, we sample paths from the head entity h to the tail entityt in G, forming a context set C to be filtered. The LLM then evaluates C for relevance to (h, r, t). To reduce computational costs, we distill a smaller sequence classifier sc from the LLM to handle most of the context filtering. This allows us to effectively eliminate irrelevant paths and address the first challenge. Next, we train a smaller PLM, BERT [12], on the filtered context set C* for path scoring. In the testing phase, we sample the corresponding"}, {"title": "III. METHODOLOGY", "content": "In this section, we present our principled framework, KG-CF, which leverages LLM inference capabilities to train sequence classifiers for context filtering in KGC tasks. Figure 1 outlines our model pipeline, divided into three key stages: path labeling, sequence classification for filtering, and PLM scoring. To address the exponential growth in path numbers with increasing truncation length, we introduce a sequence classifier to filter paths, leveraging the generalizability of graph topology in knowledge graphs to reduce computational costs."}, {"title": "B. Path Labeling through LLM", "content": "Path Formulation. For a query q = (eh, rq, ?) and a potential completion c(q,et), we can execute a breadth-first search algorithm on the graph to acquire a straightforward inferential path from en to et. Each trajectory T is formulated as a list of triplets {ti}i=0\u2192n that starts from en and ends at a potential tail entity et:\nT = ((eh, ro, e\u2081), (e1, r1, e2), ...., (en, rn, et)).\nWe define an inference path Pas the concatation of a trajectory T\u2081, along with the completion c(q, et) = (eh, rq, et):\nP = ((eh, rq, et), T).\nLLM Inference. So far, we have formalized the objects that need to be filtered. Subsequently, we transform the paths into character sequences to adapt the inference paths to the input of LLMs. Therefore, we obtain labels for all the paths associated with c(q, et):\nYc(q,et) = LLM(instruction \u2295 f(Pc(q,et))),\nwhere \u2295 denotes the concatenation operation, Pc(q,et) contains all the possible paths related to c(q, et) and f transform the paths into texts. The result Yc(q,et) contains labels for paths in Pc(q,et) while each label is in {0,1}. Based on this operation, we construct a dataset Dsc for the sequence classifier training, and we introduce the details in the next section. The detailed process is presented in Algorithm 1.\nNote that inverse relationships are allowed (suitable for head prediction) and all triplets in the path are represented in the standard forward order. For example, triplet (Lakers, inv(plays for), Lebron James) will be interpreted as \u201cLebron James plays for Lakers\", where inv() represents the function of inversing.\""}, {"title": "C. Sequence Classifier", "content": "We employ an LSTM [14] model as the sequence clas-sifier Msc : P \u2192 {0,1} to implements functionality sim-ilar to LLM in Equation (5). Considering a path P = ((eh, rq, et), ((eh, ro, e1), ..., (en\u22121, In\u22121,et))), we have:\nho = R(0, en roe\u2081rg),\nhi = R(hi\u22121, eli \u2295 ri \u2295 Ci+1 \u2295rq), i \u2264 n \u2212 1,\n\u0177 = \u03c3(fc(hn-1)),"}, {"title": "A. Problem Formulation", "content": "We denote the knowledge graph as G = {E,R, T}, where R represents relation types, & represents entities, and T includes all triplets in G. A triplet t \u2208 T is defined as t = (eh, r, et), with en as the head entity and et as the tail entity. Our focus is on entity prediction, covering two subtasks: head prediction and tail prediction [13]. We provide the definition for tail prediction, with head prediction defined analogously.\nDefinition 1 (Tail Entity Prediction). Given a query q = (eh, rq,?) where rq is the query relation, we define the completion of q by et as:\nc(q, et) = q?=et = (eh, rq, et),\nwhere c denotes the completion function. Firstly, we need to identify the candidate set C for the tail:\nC = {li}i=1\u2192n \u2286 E \\ {en},\ns.t. Vet \u2208 C, c(q, et) \u2209 T,\nwhere n is a predefined integer. Our objective is to identify a ranking list A of all candidates:\nVi \u2208 [1, n), score(Ai) \u2265 score(Ai+1)\nwhere score is the scoring function.\nExample. Consider a KG of countries and their capitals. An example query in this graph is presented as follows:\nq = (Japan, Capital,?).\nWe have sampled a series of tail candidates: C = {Paris, Tokyo, Peking, Berlin, Kyoto, London}. If there already exists a comprehensive KGC model, the ranking list could possibly be:\nA = {Tokyo, Kyoto, Peking, Paris, London}."}, {"title": "IV. EMPIRICAL EVALUATION", "content": "In this section, we will answer the following four questions through experiments: (1) How well can KG-CF perform in knowledge graph completion tasks? (2) How do different filtering choices contribute to the overall performance of KG-CF? (3) How does the maximum path length affect the accuracy of the completion?"}, {"title": "A. Experimental Settings", "content": "Datasets. Three widely utilized real-world knowledge graphs: NELL-995 [16], FB15K-237 [13], and WN18RR [17]. NELL-995 and FB15K-237 are relation extraction datasets sourced from web text. WN18RR is derived from WordNet with refined relations. To streamline training, we sample a subset from each source dataset for evaluation.\nBaselines. We incorporate methods from prior research as baselines. Among them, RuleN [18] (rule-based) and GRAIL [19] (GNN-based) support both inductive and trans-ductive scenarios, while MINERVA [20] (reinforcement learning) and TuckER [21] (embedding-based) are limited to trans-ductive settings. Additionally, we include KG-BERT [22] and BERT-RL [15], both leveraging pretrained language models."}, {"title": "B. Evaluation Method", "content": "In both transductive and inductive scenarios, we separately evaluate our approach on two subtasks: tail prediction and head prediction. The average metrics of two scenarios are shown as the final results. Following GRAIL [19] and BERTRL [15], we randomly select another 49 tail entities {ti}i=1\u219249 for each test triplet (htest,rtest,ttest) and form a candidate set Ttest = {ttest}\u222a{ti}i=1\u219249. Despite ttest, we make sure that for any other t\u2208T, (htest, rtest,t) & G. By the end, we rank ttest based on scores and compute metrics."}, {"title": "C. Main Results (Question 1)", "content": "In this subsection, we assess our KG-CF framework on three knowledge graphs across transductive (Table I) and inductive scenarios (Table II), leading to these insights: (1) KG-CF surpasses most baselines across datasets and scenarios, highlighting the effectiveness of using LLMs and sequence classifiers to refine graph context. (2) KG-CF demonstrates greater stability in transductive scenarios compared to inductive ones. (3) Our method achieves the most significant improvements on NELL-995, which, unlike FB15K-237, includes richer textual descriptions of entities (e.g., \"person Mexico Ryan Whitney\" rather than \"Ryan Whitney\u201d). This detail allows the LLM to better handle rare nouns, enhancing its precision."}, {"title": "D. Ablation Study (Question 2)", "content": "We conducted an ablation study on the WN18RR dataset (both transductive and inductive) where three components are removed separately: positive path filtering (-pf), negative path filtering (-nf), and trajectory entities in the paths (-te, i.e., relation only). We present the results in Figure 3.\nPositive Path Filtering. Under this setting, we assume that all paths from en to et in the positive triplet (en, rq, et) conform to standard reasoning logic, thus preserved during the data filtering phase. The results showed a slight decline compared to the original model, indicating that our sequence classifier can enhance the rationality of positive paths.\nNegative Path Filtering. In this setting, we assume that for a negative triplet (en, rq, et) \u2209 T, all paths from en to et fail to confirm rq's existence (though, given KG incompleteness, we consider this assumption incorrect). This ablation led to"}, {"title": "E. Path Length Scalability Study (Question 3)", "content": "Intuitively, providing a knowledge graph completion model with richer contextual information can boost confidence and accuracy in training and prediction. However, the exponential cost of graph sampling at larger scales limits the practical amount of context that can be used. Here, we evaluate the impact of scalability on model performance in two settings.\nFixed Setting. We create training datasets and train models with varying maximum path lengths, but fix the maximum path length at 3 during testing for all models.\nDiff. Setting. Training follows the same approach as in the fixed setting, but each model is tested with the maximum path length used during its training.\nWe run experiments on the WN18RR dataset, showing results for both settings in Figure 2. Key observations are as follows: (1) Extending path length generally improves model performance. (2) Increasing path length from two to three significantly boosts performance in both transductive and inductive scenarios. However, performance differences for path lengths of three or more are minimal, likely because shorter paths capture more relevant information. (3) In some cases, training on longer paths may reduce the model's ability to reason over shorter paths."}, {"title": "V. RELATED WORK", "content": "Existing KGC methods generally fall into three categories:\n(1) Embedding-based Methods: These approaches map entities and relations into an embedding space, with notable methods like TransE [13] and DistMult [23]. Among them, RotatE is widely recognized for its geometric approach to relational semantics. (2) Causality-based Methods: This category [24] focuses on identifying and utilizing causal relationships within knowledge graphs. (3) PLM-based Methods: Initiated by KG-BERT [22], which leverages BERT's inherent knowledge, this approach has evolved with models like BERTRL [15] that incorporate reasoning paths. Advanced techniques include prompt engineering, LoRA adapters [25], and innovations in soft prompts [26] for enhanced training and evaluation."}, {"title": "B. Reasoning with Large Language Models", "content": "Currently, there are primarily two strategies [27] for lever-aging LLMs in reasoning tasks: (1) Strategy Enhanced Reasoning. These methods focus on refining the reasoning capabilities and strategies of LLMs. Since LLMs excel in interpreting and following explicit instructions [28], prompt"}, {"title": "VI. LIMITAIONS", "content": "This work mainly focuses on the problem of utilizing LLM's reasoning ability on KGC. We note that we only deploy simple reasoning paths as the graph context, which is not essential for evaluation. Therefore, new context type selection (e.g. ego-graph) can be a future direction that is worthwhile to explore."}, {"title": "VII. CONCLUSION & FUTURE WORKS", "content": "This paper presents KG-CF, a knowledge graph completion method that enhances pretrained language models (PLMs) through LLM-guided context filtering. We distill a sequence classifier from an LLM to assess reasoning path validity, enabling high-quality KG context selection for training the BERT scorer. Experiments show KG-CF achieves strong performance across datasets and scenarios. Our approach efficiently applies autoregressive LLMs to entity ranking. We leave the incorporation of varying graph context types to future works."}, {"title": "A. Knowledge Graph Completion (KGC)", "content": "Related Work\nV."}, {"title": "D. PLM Scoring", "content": "In this section, we demonstrate the scoring and training pro-cess of our PLM scorer. Considering a path P = (c(q, et), T),we compute the text representation and its score as follows:\nPtext = text(c(q, et)) & text(T),\nscore(P) = \u0177p = \u03c3(PLM(Ptext)),\nwhere text() stands for the textualize function, denotes concatenating and independently annotating two segments of text, and \u0177p is the score of the path P by applying the sigmoid function \u03c3(\u00b7) on the outputs of the PLM model. We utilize the same loss function as Eq. (7) for PLM training, while the label represent the existence of triplets in KG.\nScoring and Ranking. To provide a basis for entity ranking,inspired by BERTRL [15], we represent the confidence scoreof each completion c(q,et) using the highest path scorecorresponding to it:\nscore(et) = max{\u0177p|P\u2208 Pc(q,et)}\nA special case occurs when Pc(q,et) = \u00d8. In this scenario, we manually assign the lowest score to the completion."}, {"title": "Optimization.", "content": "We use the cross-entropy loss to train the sequence classifier model:\nL = -\u2211 [yi log(yi) + (1 \u2212 yi) log(1 \u2212 \u0177i)] ."}]}