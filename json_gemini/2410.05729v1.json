{"title": "Equi-GSPR: Equivariant SE(3) Graph Network Model for Sparse Point Cloud Registration", "authors": ["Xueyang Kang", "Zhaoliang Luan", "Kourosh Khoshelham", "Bing Wang"], "abstract": "Abstract. Point cloud registration is a foundational task for 3D alignment and reconstruction applications. While both traditional and learning-based registration approaches have succeeded, leveraging the intrinsic symmetry of point cloud data, including rotation equivariance, has received insufficient attention. This prohibits the model from learning effectively, resulting in a requirement for more training data and increased model complexity. To address these challenges, we propose a graph neural network model embedded with a local Spherical Euclidean 3D equivariance property through SE(3) message passing based propagation. Our model is composed mainly of a descriptor module, equivariant graph layers, match similarity, and the final regression layers. Such modular design enables us to utilize sparsely sampled input points and initialize the descriptor by self-trained or pre-trained geometric feature descriptors easily. Experiments conducted on the 3DMatch and KITTI datasets exhibit the compelling and robust performance of our model compared to state-of-the-art approaches, while the model complexity remains relatively low at the same time. Implementation code can be found here, https://github.com/alexandor91/se3-equi-graph-registration.", "sections": [{"title": "1 Introduction", "content": "The registration of point clouds typically involves formulating robust geometric feature descriptors and a subsequent complex matching process to predict feature correspondences [8]. However, these correspondences established from raw point cloud often exhibit a high outlier-to-inlier ratio, leading to significant registration errors or complete failures. To enhance the robustness of registration processes, PointDSC [2] explicitly calculates local feature spatial consistency and evaluates pairwise 3D geometric feature descriptor similarity across two frames [10,46] to eliminate outliers from the alignment optimization process. Other approaches like Deep Global Registration (DGR) [9] treat correspondence prediction as a classification problem, utilizing concatenated coordinates of input point cloud pairs and employing a differentiable optimizer for pose refinement. Despite the effectiveness of these models on public datasets, their training requires accurate correspondence supervision, necessitating a complex point-to-point search process that is particularly vulnerable to numerous outliers.\nGeometric feature descriptors, derived from keypoint neighborhoods of a specified range, often overlook the underlying geometric topology of the data, such as the global connectivity among points. This oversight results in feature descriptors lacking SE(3) rotation equivariance, thereby impeding efficient and robust learning of rotation-equivariant and invariant features. The recently introduced RoReg [44] model employs a rotation-guided detector to enhance rotation coherence matching and integrates it with RANSAC for pose estimation. However, it suffers from high computational demand and reduced processing speed. This highlights the need for more efficient rotation-equivariant model architectures to significantly enhance registration performance.\nTo address these challenges, we introduce a novel approach that leverages a graph convolution-based model to jointly learn SE(3) equivariant features, starting with feature descriptors extracted from sparsely sampled points across two frames. Our proposed SE(3) equivariant graph network model, aimed at sparse point cloud registration, is depicted in Fig. 1. Unlike Transformer and CNN-based models, our graph architecture captures both the topology and geometric features of point clouds, similar to other proposed geometric descriptors [40,53], facilitating the learning of fine-grained rigid rotation-equivariant feature representations for more robust and coherent point cloud registration through data symmetry. The primary contributions of our study are the following:\nIntroduction of an equivariant graph model to facilitate neighbor feature aggregation and SE(3) equivariant coordinate embedding from either learned geometric descriptors for point cloud registration.\nImplementation of a novel matching approach within the implicit feature space, based on similarity evaluation and Low-Rank Feature Transformation"}, {"title": "2 Related Work", "content": "The concept of equivariant properties can be embedded in the layers of Convolutional Neural Networks (CNNs) to depict SO(2) group characteristics, as initially proposed by Cohen [11], and later extended to encompass arbitrary continuous input [16]. Another area of study focuses on equivariance representation by employing steerable kernel filters [12, 27, 41, 47-49] for equivariance learning. For more intricate tasks involving SO(3), techniques such as Vector Neurons [13] and Tensor Field Networks [42] can be viewed as implementations of the capsule network model [50], transitioning from scalar values to vectors. Following the introduction of the Transformer model, Lie-group-based Transformer models [18,19,26] have been developed to capture equivariance through attention mechanisms. Moreover, to address the intricate equivariance inherent in the input data, equivariant(n) graph neural networks [14,28,38] are utilized to learn equivariant features for dynamic and complex issues. As SE(3) features are maintained through message passing [5] within the graph model, these equivariant models exhibit considerable potential in addressing many longstanding challenges, such as predicting molecule structures [39] or quantum structures [21], as well as particle dynamic flow physics [4,29]. Some studies have explored the application of equivariant models in various point cloud tasks, including 3D detection [40], 3D point classification [53], point cloud-based place recognition [30], 3D shape point registration [7,55], and 3D shape reconstruction [6]. These applications underscore the learning efficiency gained from the leveraging of the intrinsic symmetries in input data.\nDespite the prevalence of equivariant models in the microscopic realm [18,39], the application of such models for tasks like multi-view 3D reconstruction [23] or other intricate 3D challenges remains under-explored. A fundamental component of 3D reconstruction involves point cloud registration. Many conventional methods employ linear-algebra-based optimization techniques for iterative point cloud registration, such as point-to-plane registration [33, 34], LOAM [45], and its variation F-LOAM [45]. In recent years, there has been a rise in deep learning models for registration purposes, relying on representative feature descriptors [10, 46] or precise correspondence establishment between descriptors, like deep global registration [9]. To enhance registration accuracy, some studies focus on developing more resilient descriptors like rotation-equivariant descriptors [3, 43, 44] for subsequent correspondence matching or incorporating SO(2) rotation equivariance into the registration framework using cylindrical convolution, as in Spinnet [1]. Other research works concentrate on optimizing correspondence search explicitly, for instance, Stickypillars utilizes optimal transport"}, {"title": "3 Method", "content": "Our registration process begins by extracting feature descriptors from downsampled point clouds. Equivariance is integrated into the features through equivariant graph convolution layers. Subsequently, the number of features in the pairwise graph is aggregated to a smaller number via Low-Rank based constraint. Finally, the similarity between the pairwise features of the two frames is calculated for relative transform prediction. A detailed illustration of the model is shown in Fig. 2. The input to our model consists of N points X = [x_{1},...,x_{N}] \u2208 N \u00d7 R^{3} from the source frame, and N points Y = [y_{1},...,y_{N}] \u2208 N \u00d7 R^{3} from the target frame, where x_{i} \u2208 R^{3} and y_{j} \u2208 R^{3} form a correspondence (i, j). It is important to note, for ease of subsequent similarity search, that the coordinates of points in each frame are rearranged in descending order based on the ray length ||r(t)||_{2} from the point position to the sensor frame center o_{s}. For numerical stability during training, the source scan is normalized to a canonical frame, and the target scan is transformed relative to the source frame, allowing the model to predict the relative transformation from source to target.", "3.1 Feature Descriptor": "We incorporate geometric details of nearby interest points into our graph model using feature descriptors. To extract these descriptors, we can reuse available pre-trained point-based descriptors or train a shallow Multi-Layer Perceptron (MLP) module with l_{1} layers prior to the model in an end-to-end manner, inspired by PointNet++ [35]. The feature representation of point i in the next layer l + 1 is calculated by averaging the output of the mapping function h(.) applied to the relative positional coordinates of point i and neighboring point k \u2208 N(i) (n points), along with the hidden feature h_{i}^{l} from the prior layer l_{1}.\nh^{l+1} = \\frac{1}{n} \\sum_{k \\in N(i)} f(h_{i}^{l}, x_{k} - x_{i}).", "3.2 Equivariant Graph Network Model": "By utilizing the equivariant graph representation (l_{2} layers) as introduced by Satorras et al. [38], we can enhance the receptive field and representation for"}, {"content": "feature descriptors of interest points by incorporating SE(3) equivariant properties via graph feature aggregation. Our method leverages graph-based message passing techniques [22] to propagate SE(3) equi-features. For the construction of the graph G = (V, E) with vertices V and E edges. The individual hidden point descriptors h_{i}^{a} \u2208 R^{32} and the point coordinate embedding x_{i} \u2208 R^{3} at layer l_{2} are treated as the node and edge features, respectively. The graph convolutional layer updates the edge equi-message m_{ik} \u2208 R^{3\u00d73}, node hidden feature h_{i}^{l} \u2208 R^{32}, and coordinate embedding x_{i}^{l} \u2208 R^{3} at each equivariant layer.\nm_{ik} = \\varphi_{m}( \\varphi_{h}(h_{i}^{l}, h_{k}^{l}), \\varphi_{x}(x_{i}^{l} - x_{k}^{l}, ||x_{i}^{l} - x_{k}^{l}||^{2} )),\nx_{i}^{l+1} = x_{i}^{l} + C \\sum_{k \\in N(i)} exp(||x_{i}^{l} - x_{k}^{l}||^{2}) ( proj_{F_{ik}}(m_{ik})),\nh_{i}^{l+1} = \\varphi_{n}(h_{i}^{l}, \\sum_{k \\in N(i)} ( proj_{F_{ik}}(m_{ik}))),\nwhere \u03c6_{m}, \u03c6_{x}, and \u03c6_{n} represent 1D convolutional layers for the message, coordinate embedding, and hidden feature update, respectively. The normalizing factor C is applied to the exponentially weighted sum of mapped equi-message in Eq. (3). Additionally, a neighbouring search of x within a specific radius is conducted to find the N(i) neighbouring feature descriptors for edge establishments, and this is used to prevent information overflow by confining the exchange of information within a local context, thereby reducing the complexity of the graph feature adjacency matrix from O(n\u00b2) to approximately O(n). In"}, {"content": "Eq. (3), the projection of m_{ij} onto a locally equivariant frame (proj_{F}(\u00b7)) helps to preserve the SO(3) feature invariance. The frame F_{ik} is constructed using pairwise coordinate embeddings as outlined in ClofNet [14],\nF_{ik} = (a_{ik}, b_{ik}, c_{ik}),\na_{ik} = (\\frac{x_{i}^{l} - x_{k}^{l}}{||x_{i}^{l} - x_{k}^{l}||}, \\frac{x_{i}^{l} \u00d7 x_{k}^{l}}{|x_{i}^{l} \u00d7 x_{k}^{l}|}, \\frac{x_{i}^{l} - x_{k}^{l}}{||x_{i}^{l} - x_{k}^{l}||} \u00d7 \\frac{x_{i}^{l} \u00d7 x_{k}^{l}}{|x_{i}^{l} \u00d7 x_{k}^{l}|} ).\nConsequently, the projection of m_{ik} into m_{ik} is formulated into the linear combination of axes of the local equi-frame scaled by the coefficients (x_{ik}^{a}, x_{ik}^{b}, x_{ik}^{c}),\nproj_{F_{ik}} m_{ik} = m_{ik} = x_{ik}^{a}a_{ik} + x_{ik}^{b}b_{ik} + x_{ik}^{c}c_{ik}.\nThe projection of edge message m_{ij} in Eq. (3) is performed in the local equi-frame (within bracket of Eq. (6)), to obtain projected message m_{ik}, while the scalar coefficients in Eq. (7) remain SO(3) invariant. Consequently, the sum of equi-projected message in Eq. (4) is still a vector-based sum, maintaining the equivariance upon integration into the hidden layer \u03c6_{n}."}, {"title": "3.3 Low-Rank Feature Transformation", "content": "Inspired by LoRA of language model [24], our approach diverges by not requiring pre-trained weights for fine-tuning. We employ two stacked linear forward layers with low-rank constraints in the middle of model (Fig. 3a) to map feature descriptors to aggregated descriptors. We name it as Low-Rank Feature Transformation (LRFT). This design enhances similarity match reliability and computational efficiency by performing matches on aggregated descriptors with integrated neighboring information. Specifically, The motivation for employing LRFT is twofold: 1) Theoretically, low-rank constraints in linear layers capture essential feature correlations within descriptors, as demonstrated by the matrix low-rank theorem (refer to Appendix Sec.1.2), leading to more reliable similarity matches; 2) Practically, our LRFT improves computational efficiency by aggregating feature descriptors prior to similarity matching, and It also enhances low-rank learning efficiency by training parameters during the forward pass, eliminating the need for pre-trained weights.\nAfter the final layer ((l_{\u00bd})th) of the equivariant graph module, the output graph features consist of both node and edge features. During this stage, we preserve each graph node feature h_{i}^{l}, i \u2208 N from the source frame and feature h_{j}^{l}, j \u2208 N from the target frame of the last graph layer. Next, the node feature is combined with the mean coordinate embeddings x_{i}^{avg} = \\frac{1}{k \\in N(i)} x_{k}^{l}, where x_{i}^{l} \u2208 R^{3} is obtained from the edge embeddings (Eq. (3)) connected to the node feature x_{i}^{l}. Consequently, matrices for the source frame H_{src} \u2208 R^{N\u00d735} and target frame H_{tar} \u2208 R^{N\u00d735} are created by stacking of node features (h_{i}^{l})\nand coordinate embeddings along the column respectively. Before computing"}, {"content": "the feature similarity, the Low-Rank Feature Transformation (LRFT) technique is applied to compress the features into \\hat{H}_{src} \u2208 R^{N'\u00d735} and H_{tar} \u2208 R^{N'\u00d735} utilizing parameters of A and B mapping layers.\n\\hat{H}_{src}, \\hat{H}_{tar} = (AB)^{T} (H_{src}, H_{tar}).\nwhere A is a N\u00d7r matrix and B is r\u00d7 N' matrix, and rank r < min(N, N'), so that the number of node features are compressed into N' dimension after LRFT module mapping via multiplication AB, as depicted in the left of Fig. 3a. A is initialized from Gaussian distribution with standard deviation d = \\sqrt{r}, while B is initialized with small constant close to zero.\nThe LRFT layers extract spatial context from neighboring feature descriptors through linear mapping, efficiently aggregating local information for decoder.", "3.4 Similarity Calculation": "Subsequently, we calculate the feature similarity score matrix (refer to Fig. 3b) for feature correspondence establishments using the compressed number of features after the LRFT module. This is achieved by computing the dot product  of features as an element. Prior to the multiplication, the respective element features h_{i} from \\hat{H}_{src} \u2208 R^{N'\u00d735} and h_{j} from H_{tar} \u2208 R^{N'\u00d735} are first normalized to h\u0302_{i} and h\u0302_{j}.\nS_{ij} = < h\u0302_{i}, h\u0302_{j} >,\nwhich forms a square similarity matrix S \u2208 R^{N'\u00d7N'}, then subsequently normalized along each row to produce S\u0302 \u2208 R^{N'\u00d7N'}. We compute the determinant of S\u0302 by calculating trace to indicate whether the matrix rank has deficiency, which may arise from the presence of ambiguous correspondences. Accordingly, as per the match assignment rule, each row of the similarity matrix S\u0302 should contain a singular value close to one, depicted as a light-colored square in Fig. 3b. Subsequently, the similarity matrix is employed to project the feature matrices H\u0302_{src} and H\u0302_{tar} through multiplication by S\u0302H_{src} \u2208 R^{N'\u00d735} and S\u0302H_{tar} \u2208 R^{N'\u00d735}, respectively. The resulting matrices are concatenated to facilitate subsequent pooling and mapping through fully-connected layers. Furthermore, a regularizer is used to enforce the rank of S\u0302 close to r, ensuring that a submatrix S' with rank r out of S\u0302 can be found,\nL_{Reg} = || (Trace(\\hat{S}^{T}\\hat{S})) - r ||^{2}.\nAdditionally, to eliminate outlier feature correspondences, each matched pair element S\u0302_{ij} undergoes verification through a submatrix full-rank check. This involves evaluating the determinant of a 7 \u00d7 7 submatrix centered at the feature element S\u0302_{ij} or a 5 \u00d7 5 submatrix at the border element of the S\u0302 matrix (highlighted by the red dashed box in Fig. 3b). This verification process ensures local consistency in feature similarity matches, aiding in the identification of globally consistent and reliable match pattern search from the similarity matrix. Following verification, the final valid rank of the similarity matrix S is established as r < 128, with any invalid assignment row S\u0302_{i}. zeroed out to mask the corresponding feature for subsequent computations. The upper limit rank r is derived from the Theorem: Rank(AB) \u2264 min(Rank(A), Rank(B)). Please refer to the supplementary part for detailed proof the rank theorem."}, {"title": "3.5 Training Loss", "content": "The final layer predicts translation \u0142 and rotation matrix R in quaternion form. The ground-truth translation and rotation are denoted by t* and R* respectively.\nL_{total} = L_{rot} + L_{trans} + \u03b2L_{Reg},\n\u03b2 for regularizer is set to 0.05. The translation error (TE) and rotation error (RE) losses can be formulated as follows,\nL_{rot}(R) = arccoS( \\frac{Trace(R^{T} R^{*}) - 1}{2} ),\nL_{trans}(t) = ||t - t^{*}||^{2}.\nThe rotation error term L_{rot} and translation error term L_{trans} are measured in radians and meters, respectively. Given that the predicted transform is relative, from source to target frame, the scale of these transforms is typically in normal scale to avoid numerical stability issues in training."}, {"title": "4 Experiments", "content": "We evaluate the performance of the proposed model for point cloud registration in both indoor and outdoor environments. For indoor scenes, we utilize 3DMatch introduced by Zeng et al. [52]. The raw point clouds are uniformly downsampled to 1024 points through a voxel filter. For the outdoor evaluation, we select the KITTI dataset [20] with the same dataset split from the creators and follow the same downsampling process as in Choy et al., [10]. We report both the qualitative and quantitative results of the proposed model. Furthermore, we offer an in-depth analysis of the effect of varying parameter configurations and the contribution of each component to enhancing the model's performance. The computational efficiency of each model is presented in the metric table below.\nImplementation Details. The key parameters of our model include the dimension of graph-relevant features and LRFT layers. Initially, the extracted feature descriptor dimension is 32 for subsequent graph learning. A critical aspect is the number of nearest neighbors for each node feature in the graph, set to 16 for constructing the graph using ball query for 3DMatch (ball radius at 0.3m), while for KITTI, we employ kNN, selecting the nearest 16 points of query point to form a graph with 1024 nodes and 1024 \u00d7 16 edges. In graph learning, the node feature dimension is 32, and the edge embedding feature is 3, comprising coefficients projected onto the locally constructed coordinate frame as shown in Eq. (7). We use 4 equi-graph layers throughout the tests. The LRFT module consists of 3 parameters: input dimension N, internal rank r, and N'. Our model adopts a configuration of 1024/(32 + 3)/128, where rank r is the sum of graph node feature dimension (32) and coordinate embedding dimension 3. The submatrix determinant check for similarity score matrix S\u0302 is 5 \u00d7 5 along the borders and 7\u00d77 within the matrix. A performance analysis comparing different parameter configurations is presented in the subsequent ablation section. All training and inference tasks are conducted on a single RTX 3090 GPU.\nEvaluation Metrics. We employ the average Relative Error (RE) and Translation Error (TE) metrics from PointDSC [2] to assess the accuracy of predicted pose errors in successful registration. Additionally, we incorporate Registration Recall (RR) and F1 score as performance evaluation measures. To evaluate these metrics, we establish potential corresponding point pairs (x_{i}, y_{j}) \u2208 \u03a9 using input points from two frames, following the correspondence establishment approach outlined in PointDSC [2]. We apply the predicted transformation to the source frame point x, recording a pairwise registration success only when the average Root Mean Square Error (RMSE) falls below a predefined threshold \u03c4. The registration recall value \u03b4 is calculated as:\n\u03b4 = \\frac{1}{N(\u03a9)} \\sum_{(i,j)\u2208\u03a9} 1[|| Rx_{i} + t - y_{j}||_{2} < \u03c4],\nwhere N(\u03a9) denotes the total number of ground truth correspondences in set \u03a9. The symbol 1 functions as an indicator for condition satisfaction. Removing the conditional check within the indicator brackets on the equation's right side transforms it into a standard Root Mean Square Error (RMSE),\n\\sqrt{ \\frac{1}{N(\u03a9)} \\sum_{(x_{i},y_{j})\u2208\u03a9} |R x_{i} + t \u2212 y_{j}|^{2}}. This RMSE metric is utilized in ablation experiments for parameter analysis. The F1 score is defined as 2.\\frac{Precision \u00d7 Recall}{Precision + Recall}\nBaseline Methods 1) For the 3DMatch [52] benchmark, we compare our model with vanilla RANSAC implementations using various iterations and optimization refinements. We also include Go-ICP [51] and Super4PCS [31], which operate on raw points. Among learning-based methods, we compare with DGR [9] and PointDSC [2] combined with FCGF descriptors [10]. Additionally, we select D3Feat [3], SpinNet [1], and RoReg [44], which incorporate rotation invariance or equivariance. These learning methods do not support descriptor replacement, denoted by *. 2) For KITTI sequences [20], we implement the hand-crafted feature descriptor FPHF [37] due to performance saturation issues with FCGF [10] descriptors, as noted in PointDSC [2]. RoReg [44] is replaced with the registration model from the FCGF paper [10] (denoted as FCGF-Reg) due to public code limitations for KITTI dataset."}, {"title": "4.1 Indoor Fragments/Scans Registration", "content": "Point clouds are initially downsampled using a 5cm voxel size to generate 1024 sampled points. Registration success is evaluated using thresholds of 30cm for translational error (TE) and 15\u00b0 for rotational error (RE). The correspondence distance threshold in Eq. (14) is set at 10cm. Comparative results between our proposed model and baseline approaches are presented in Tab. 1. Our model outperforms all comparison methods, despite slightly slower latency compared to RANSAC with 1k iterations. RoReg and SpinNet, ranking second and third, demonstrate minimal registration errors and maximal registration scores, highlighting the advantages of incorporating rotation features. While our model can integrate the FCGF descriptor, we present evaluation results using the PointNet++ learning descriptor for end-to-end training."}, {"title": "4.2 Outdoor Scenes Registration", "content": "The input point cloud from the KITTI sequences [20] is downsampled using a voxel size of 30 cm to generate 1024 sparse points for the experiments. We set the registration thresholds at 60cm for Translation Error and 5\u00b0 for Rotation Error. To measure Registration Recall (RR), we establish a threshold \u03c4 of 60cm. Tab. 2 presents the quantitative results for comparison. Our proposed model demonstrates plausible performance compared to other methods, exhibiting minimal rotation and translation errors, and achieving the highest registration recall rate of 94.60% when compared to RoReg, the second-best model. However, RoReg has a remarkable weakness in its real-time performance, registering in over 30"}, {"title": "4.3 Ablation Study", "content": "Firstly, we present a table (Tab. 4) displaying different configurations and combinations of module blocks in the proposed model. Table analysis reveals significant accuracy enhancements with our model's learning-based descriptor over pre-trained FCGF and FPHF descriptors, as shown in rows 1 and 2, across multiple metrics. The descriptor learning layers (row 3) and equivariant graph CNN layers (row 4) are key to performance improvements. Replacing the equivariant with standard graph CNN layers (row 5) impairs rotation convergence, while omitting the LRFT module (row 6) marginally reduces performance. Ball query graph initialization (rows 8) outperforms KNN search (row 7) in efficiency and"}, {"title": "5 Conclusion", "content": "We introduce an end-to-end model that leverages pre-trained feature descriptors or learns directly from raw scan points across two frames, incorporating equivariance embedding through graph layers, Low-Rank Feature Transformation and similarity score computation. Validation in both indoor and outdoor datasets confirms the superior performance of our proposed model. Ablation studies further substantiate the model design. Notably, the model's latency demonstrates its potential applicability in visual odometry. Future work could explore generalizing this framework to be input-order permutation invariant through graph attention layers or pooling, potentially integrating additional sensor modalities to address dynamic challenges."}]}