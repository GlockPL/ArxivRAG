{"title": "Decentralizing Test-time Adaptation under Heterogeneous Data Streams", "authors": ["Zixian Su", "Jingwei Guo", "Xi Yang", "Qiufeng Wang", "Kaizhu Huang"], "abstract": "While Test-Time Adaptation (TTA) has shown promise in addressing distribution shifts between training and testing data, its effectiveness diminishes with heterogeneous data streams due to uniform target estimation. As previous attempts merely stabilize model fine-tuning over time to handle continually changing environments, they fundamentally assume a homogeneous target domain at any moment, leaving the intrinsic real-world data heterogeneity unresolved. This paper delves into TTA under heterogeneous data streams, moving beyond current model-centric limitations. By revisiting TTA from a data-centric perspective, we discover that decomposing samples into Fourier space facilitates an accurate data separation across different frequency levels. Drawing from this insight, we propose a novel Frequency-based Decentralized Adaptation (FreDA) framework, which transitions data from globally heterogeneous to locally homogeneous in Fourier space and employs decentralized adaptation to manage diverse distribution shifts. Interestingly, we devise a novel Fourier-based augmentation strategy to assist in decentralizing adaptation, which individually enhances sample quality for capturing each type of distribution shifts. Extensive experiments across various settings (corrupted, natural, and medical environments) demonstrate the superiority of our proposed framework over the state-of-the-arts.", "sections": [{"title": "1. Introduction", "content": "Deep learning models often suffer significant performance degradation when deployed in environments where the data distribution differs from that of the training set a challenge known as domain shift [7, 20]. Recently, Test-Time Adaptation (TTA) [2, 17, 26-28, 31, 34, 36] has emerged as a promising solution by refining model parameters to better align with the encountered data at inference time. It leverages the incoming data stream for real-time adjustments without the need for retraining on a labeled dataset, enabling swift model adaptation to unpredictable data characteristics during deployment.\nDespite their success, the effectiveness of current TTA models is generally constrained within ideal testing conditions often involving homogeneous test samples with similar types of distribution shifts. While attempts have been made to address dynamic target distributions in continually changing environments [36], they fundamentally presume a uniform target domain at any time point. Their focus remains on enhancing model robustness against regular changes by stabilizing the fine-tuning process either by periodically resetting model weights [27, 28] or by down-weighting samples that deviate from the estimated distribution [17, 26]. Although these model-centric approaches may offer temporary relief, they do not fully recognize the intrinsic heterogeneity of real-world data. In practice, distribution shifts do not necessarily occur gradually over time but can be multifaceted at a single moment, involving heterogeneous and even conflicting shifts that current TTA models fail to adequately capture.\nTo address this, it is crucial to understand how these heterogeneous distribution shifts impact model adaptation. When a model attempts to adjust simultaneously to multiple diverse and potentially conflicting shifts, it may encounter adaptation conflicts. Specifically, adjustments made to accommodate one type of shift can interfere with adaptations for another, as different shifts may require conflicting changes to the model parameters. For instance, adapting to variations in image brightness might necessitate parameter updates that conflict with those needed for texture changes. Such conflicts prevent models from generalizing effectively across all encountered shifts, leading to irreversible degradation in predictive capabilities.\nRecognizing these issues, we argue for shifting from a model-centric to a data-centric approach that proactively addresses distribution diversity in Fourier space. The rationale is that the frequency domain, unlike the common spatial domain, enables a clearer separation of data variations across different frequency levels. For example, high-frequency components are typically associated with fine-grained features like edges and textures, whereas low-frequency components generally relate to overall structural patterns such as shapes and illumination. By decomposing data into these frequency components, we can effectively isolate and manage different types of distribution shifts. Moreover, since the Fourier transform operates directly on the raw input images at the pixel level, it does not depend on pretrained model outputs, avoiding potential uncertainties due to significant distribution shifts. Importantly, this proactive separation allows us to manage distribution diversity prior to adaptation, offering a robust foundation for subsequent model enhancement.\nBuilding on this insight, we propose a Frequency-based Decentralized Adaptation (FreDA) framework. Specifically, we first dynamically partition incoming data in the Fourier domain using high-frequency information. This initial segmentation facilitates the transition from globally heterogeneous to locally homogeneous data subsets before any model adaptation occurs. On this basis, we propose a decentralized learning strategy that allows multiple local models to independently adjust to their specific data segments while periodically exchanging knowledge to form a cohesive global model. This dual approach not only captures the diversity of distribution shifts to reduce potential conflicting adaptations but also leverages periodic communication among local models to enhance the global model's generalization across multiple shifts. Furthermore, we introduce a Fourier-based augmentation mechanism paired with an entropy-based sampling strategy, which significantly increases both the quantity and quality of samples for each type of shift. This enhancement further improves the model's robustness and predictive capabilities in dynamic environments. To summarize, the main contributions of this work are three-fold:\n\u2022 We identify that many existing TTA methods are restricted in a model-centric paradigm that overlooks the data heterogeneity inherent in real-world scenarios. This oversight results in ineffective adaptation when facing diverse distribution shifts simultaneously.\n\u2022 We revisit TTA from a data-centric perspective and introduce the FreDA framework. It reinterprets principles from both Fourier space and decentralized learning, leveraging specialized local adaptations to manage heterogeneous distribution shifts at test time.\n\u2022 We conduct extensive evaluations across corrupted, natural, and medical scenarios \u2013 demonstrating its consistent superiority under various circumstances."}, {"title": "2. Preliminaries", "content": "Test-Time Adaptation under Mixed Distributions. Test-time adaptation (TTA) aims to adjust a model $q_\\theta(y|x)$, initially trained on a source dataset $D_s = \\{(x, y) \\sim p_s(x,y)\\}$, to a target domain $D_t = \\{(x,y) \\sim p_t(x,y)\\}$ without access to source data or target labels. Traditionally, TTA handles covariate shift by assuming $p_s(y|x) = p_t(y|x)$ while $p_s(x) \\neq p_t(x)$. This challenge intensifies when $D_t$ includes multiple non-i.i.d sub-distributions $p_{t_i}(x)$, complicating the adaptation process:\n$p_t(x) \\rightarrow \\{p_{t_1}(x), p_{t_2}(x), ..., p_{t_\\sqrt{t}}(x)\\}$.\nSpecifically, we have $D_t = D_{t_1} \\cup D_{t_2} \\cup\u2026\u2026\u2026 \\cup D_{t_\\sqrt{t}}$ where $x \\in D_{t_i}$ satisfying $x \\sim p_{t_i}(x)$. This scenario requires the model $q_\\theta(y|x)$ to effectively handle the heterogeneous and evolving target distribution to maintain robust performance. TTA strategies must therefore refine the model to optimize its predictive accuracy across these diverse sub-domains, ensuring consistent and reliable performance amidst significant distributional variability.\nFourier Transformation. Analyzing the frequency components of images is essential for understanding their underlying structures, and Fourier transformation plays a central role in this process. For a single-channel image x, its Fourier transformation F(x) is given by: $F(x)(u,v) = \\sum_{h=0}^{H-1} \\sum_{w=0}^{W-1} x(h, w)e^{-j2\\pi(\\frac{uh}{H}+\\frac{vw}{W})}$ where $H$ and $W$ denote the height and width of the image, respectively, and u and v are the frequency coordinates. The inverse Fourier transformation $F^{-1}(x)$ allows for reconstructing the original image from its frequency spectrum, efficiently computed using the Fast Fourier Transform (FFT). In the frequency domain, images are characterized by amplitude A(x) and phase P(x) components, derived from the real R(x) and imaginary I(x) parts of F(x):\n$A(x)(u, v) = \\sqrt{ R^2(x)(u, v) + I^2(x)(u, v) },$\n$P(x)(u, v) = arctan\\left( \\frac{I(x)(u, v)}{R(x)(u, v)} \\right),$\n(1)\nwhere A(x) reveals the intensity of the frequency content, e.g., high-frequency amplitudes highlight edges and fine details while low-frequency amplitudes emphasize the overall structure and gradual changes, and P(x) encodes the position of these features within the spatial domain."}, {"title": "3. Connections to Previous Studies", "content": "3.1. Non-i.i.d. in Test-time Adaptation\nThe non-i.i.d. problem in Test-Time Adaptation (TTA) challenges the conventional assumption that target batches are independent and identically distributed (i.i.d.), pushing the boundaries of TTA in real-world scenarios. This issue can be decomposed into two distinct challenges:\nDependent Sampling. This problem arises when the sampling within the target stream is dependent at the class level. Existing methods [9, 22, 32, 45, 47, 48] have addressed this by aiming for class-balanced datasets during model updates, mitigating risks associated with class imbalance over time. They typically adjust sample proportions based on pseudo labels or extend data collection periods to reduce dependent sampling. However, unlike these methods that concentrate on mitigating class-level imbalances, our work focuses on enhancing TTA models in the presence of diverse sample styles or mixed distributions. We address data heterogeneity at the sample level, aiming to improve model adaptation capabilities in face of varying distribution shifts that are not captured by class balancing techniques. Notably, although our method is not tailored for class-dependent issues, our experimental results demonstrate that when class-dependent and mixed distributions coexist, our approach still achieves the best performance \u2013 showcasing the broad applicability of our model design.\nMixed Distributions. While attempts have been made to address dynamic target distributions in continually changing environments [17, 26, 28, 36, 45], they fundamentally assume a uniform target domain at each time point. Their approach focuses on strengthening model adaptation to constant changes by stabilizing the fine-tuning process, using periodic model parameter resets [27, 28] or down-weighting samples that deviate from model expectation [17, 26, 28]. These model-centric approaches fail to capture the actual data heterogeneity encountered in practice, causing model degradation in real-world deployment. In contrast, our work re-examines this problem from a data-centric perspective. We manage heterogeneous data streams by decomposing samples into the frequency domain, which facilitates an accurate data separation and allows us to address distribution diversity before adaptation occurs. Although a recent work [27] also consider mixed distribution scenarios, their study targets a broader \u201cDynamic Wild World\" topic without delving deeply into this data heterogeneity problem. Conversely, our study focuses on investigating and managing heterogeneous data streams in TTA and proposing tailored strategies to address this problem.\n3.2. Multi-Target Domain Adaptation\nTest-time Adaptation under mixed distribution resembles the multi-target unsupervised domain adaptation (MT-UDA) setting [6, 8, 15, 19], where multiple domains exist within the target domain. However, TTA introduces complexities that far exceed those in conventional MT-UDA settings, primarily due to: 1) Inaccessible Labeled Source Data In TTA, the labeled source distribution is not available, making it challenging to leverage source-target dissimilarities directly. 2) Dynamic and Unpredictable Target Streams TTA operates on a continuous influx of data, potentially\""}, {"title": "3.3. Decentralized learning, federated learning, and distributed learning", "content": "incorporating new, unforeseen distributions, rather than a static, fully observable target dataset. This continuous nature of data flow prevents the establishment of a comprehensive understanding of the target distribution. These constraints complicates the formulation of adaptation strategies that depend on discerning the differences between various subdomains within the targe.\nThis work also intersects with decentralized, federated, and distributed learning due to our approach of splitting data batches into disjoint subsets and applying decentralized model adaptation: 1) Decentralized learning typically focuses on learning from decentralized, non-i.i.d. data [13]. In this work, however, the data is not originally decentralized; all target samples arrive together, while we proactively split them into disjoint subsets, revealing latent non-i.i.d. characteristics and enabling the effective use of decentralized learning techniques. 2) Federated learning considers data privacy and multi-institutional collaborations within decentralized learning [24]. In our case, as target samples are mixed in a batch, data privacy is not a constraint. However, like federated learning, our approach also involves model collaboration where multiple local models periodically share insights to form a cohesive global model. 3) Distributed learning aims to improve training efficiency on large-scale datasets by partitioning data for synchronized training [23]. In contrast, our method operates in a real-time fine-tuning context with limited data at one time, hence scalability is less of a concern."}, {"title": "3.4. Frequency Domain Learning", "content": "In recent years, frequency-based techniques have become integral to transfer learning strategies. For domain generalization, extensive studies apply frequency analysis to gain insights into the learning behavior and model robustness of DNNs [35, 40, 41, 44]. Some approaches leverage data augmentation to improve model performance. They implement adversarial training to identify the most impactful augmentations [18, 21] or introducing generated Fourier-basis functions as additive noise [30, 33] complementary to visual augmentations. In domain adaptation, researchers implement data augmentation processes involving linear interpolation between the amplitude spectra of images with different styles [38, 39, 42, 43]. This methodology effectively reduces domain discrepancies and mitigates the risk of over-fitting to low-level statistical details inherent in amplitude information.\nMotivated by these advancements, we observe that frequency information is a potent domain characteristic. This realization, combined with the insight that augmentations to the amplitude component can mitigate overfitting, particularly due to the uniform amplitude distributions resulting from prior clustering, forms the core inspiration for our FreDA approach. While our proposed frequency-domain perturbation strategy shares the underlying idea of manipulating the amplitude component with these prior works, we distinguish ourselves by introducing subtle perturbations directly to the original amplitude components of carefully selected samples. This strategy is better suited to unsupervised fine-tuning task, where the TTA process is sensitive, and excessive augmentation may cause model degradation."}, {"title": "4. TTA under Mixed Distribution Shifts: A Fourier Perspective", "content": "4.1. Motivations\nTest-Time Adaptation (TTA) methods have been instrumental in managing domain shifts under a single type of target distribution. However, their effectiveness significantly diminishes under scenarios involving multiple distribution shifts. This is evident as models exhibit a marked decrease in sample class discriminability on the same dataset when exposed to mixed target distributions, as demonstrated by comparing Figure 2(a) and (b). A direct approach to addressing this issue involves segregating samples belonging to different target distributions. However, in real-time applications, the specific target subdomains from which incoming samples originate, or whether they conform to the same distribution shifts, are generally agnostic. Attempting to cluster samples based on features extracted by the model can be misleading, as samples from different distributions may exhibit similarities due to belonging to the same category, resulting in poor separability of different target subdomains as shown in Figure 2(c). Interestingly, when clustering is based directly on the high-frequency information of the samples \u2013 without relying on model-derived feature extractors \u2013 a significant distinction can be made between samples from different target distributions, as shown in Figure 2(d). This observation is not unexpected, considering high-frequency information typically captures variations in image textures and styles, focusing more on the underlying differences in data distributions. Building on the experimental observations and analysis outlined above, the following section proposes leveraging the frequency domain for enhancing the adaptability of TTA methods in more realistic settings involving mixed distribution shifts."}, {"title": "4.2. Frequency-based Decentralized Adaptation", "content": "The previous discussions highlight how heterogeneity within target distribution can hinder model adaptation. This raises a natural question: How can we manage this distributional heterogeneity to achieve better adaptation? As established in our earlier section, effectively distinguishing samples associated with different distribution shifts is vital"}, {"title": "4.2.1. Frequency-based Decentralized learning", "content": "Insight: Fourier transform offers an effective method to extract different frequency components from images, with high-frequency information particularly useful for capturing fine-grained details such as texture and noise. These details often highlight subtle variations among different distribution shifts. By harnessing high-frequency components from images, we can distinguish samples that lead to different distribution shifts within a TTA setting through a simple clustering technique.\nSolution: Based on this intuitive insight, we propose a new module called Frequency-based Decentralized Learning. This module leverages frequency information directly extracted from the pixel space to systematically partition data into multiple homogeneous subsets, enabling multiple local models to specialize in capturing each distribution shift individually (see Figure 1). Concurrently, our method enhances collaborative learning by allowing periodic parameter sharing among these local models, thereby boosting the overall model adaptability to diverse distribution shifts.\nFrequency Feature Extraction. We start by extracting frequency domain features from the input images to identify distinct distribution shifts. Let $X \\in \\mathbb{R}^{n\\times c\\times h\\times w}$ denote a batch of input images, where n is the batch size, c is the number of channels, h and w are the height and width of the images. We first apply a Fourier transform F to each image $X_i$ to obtain its frequency domain representation $F(X_i) \\in \\mathbb{C}^{h\\times w\\times c}$. Particularly, we focus on the amplitude spectrum $A(x)(u,v)$ in Eq. 1, filtering out low-frequency elements using mask $M(u,v) = 1 \\left( (u < \\sqrt{u} > \\frac{3}{4} h) \\lor (v < \\sqrt{v} > \\frac{3}{4} w) \\right)$ to emphasize the high-frequency components $G(x)(u, v)$ that are more likely to indicate shifts in distribution:\n$G(x)(u, v) = A(x)(u, v) \\cdot M(u, v).$\n(2)\nFrequency-Based Clustering. We then employ a clustering algorithm (e.g., K-means) to partition the frequency features into K clusters, each corresponding to a different type"}, {"title": "4.2.2. Frequency-based Augmentation", "content": "of distribution shift. The process is formalized as:\n$\\min_{C,Z} \\sum_{i=1}^{n} || A_{hf,i} \u2013 C_{z_i} ||^2,$\n(3)\nwhere $A_{hf,i} = vec(G(x))$, $C \\in \\mathbb{C}^{K\\times d}$, $Z \\in \\{1, ..., K\\}^n$ denotes the 1D high-frequency component of the amplitude spectrum (the centroids of the clusters and the cluster assignments for each image), $hf$ refers to high-frequency components, and $d = h \\times w \\times c$ is flattened dimension.\nDecentralized Fine-tuning. Test-time fine-tuning is then decentralized across these clusters, allowing for specialized adaptation within each subgroup: For each cluster k, we adapt a specialized model $q_{\\theta_k}(y|x)$ that is fine-tuned using only the data within that cluster:\n$\\theta_k = arg\\min_{\\theta_k} \\mathbb{E}_{x\\sim p_{t,k}} [\\mathcal{L}(q_{\\theta_k}(x))],$\n(4)\nwhere $p_{t,k}$ represents the data distribution within cluster k, and $\\mathcal{L}$ is the loss function.\nParameter Aggregation. To integrate knowledge from all subnetworks and prevent degradation on specific subdomains, we perform an aggregation of their parameters:\n$\\Theta_{global} = \\sum_{k=1}^{K} \\left[\\frac{|D_k|}{\\sum_{j=1}^{K} |D_j|} \\theta_k \\right] ,$\n(5)\nwhere $D_k$ denotes the number of samples in cluster k. This aggregation step combines the parameter updates from each subnetwork proportionally to its cluster size. The updated global parameters $\\Theta_{global}$ are then distributed back to each subnetwork, updating its parameters as follows:\n$\\theta_k \\leftarrow \\Theta_{global}.$\n(6)\nInsight: Although decentralized learning effectively handles data heterogeneity within the current batch, it may still suffer from inadequate characterization of each distribution shift due to limited batch data, specifically considering the uniform amplitude distributions resulting from prior clustering. Typically, TTA methods attempt to enhance the overall quality of observed target samples via data augmentation. However, traditional augmentations in TTA, borrowed from standard computer vision practices such as rotation, clipping, and mixup, albeit beneficial in scenarios with single distribution shifts, struggle to guarantee targeted fine-tuning under more complex, mixed distribution shift scenarios.\nSolution: To overcome these limitations, we propose a frequency-based augmentation strategy tailored for TTA under mixed distribution shifts. Our method specifically perturbs the amplitude components of each sample in Fourier space. This targeted approach allows us to augment"}, {"title": "Algorithm 1 Framework of Frequency-based Decentralized Learning and Augmentation", "content": "Algorithm 1 Framework of Frequency-based Decentralized Learning and Augmentation\nRequire: Step t, Input $X \\in \\mathbb{R}^{n\\times h\\times w\\times c}$, Pretrained source model $q_{\\theta}$, Initialize Feature Repository $R\\leftarrow \u00d8$, CLUSTER_NUM $K$, KMEANS_SIZE $N$, COMM_INTERVAL $f$;\nStep 1: Extract Frequency Features\n1: for i = 1 to n do\n2: $A_{hf,i} \\leftarrow vec(G(X_i))$ \u25b7 Extract high-freq components (Eq. 1, 2)\n3: end for\nStep 2: Dynamic Clustering\n4: $R \\leftarrow R \\cup \\{A_{hf,i}\\}_{i=1}^n$ \u25b7 Frequency Information Repository\n5: $R \\leftarrow R[ (|R| - N + 1) :]$\n\u25b7 Keep the last N entries for kmeans clustering\n6: $(C_t, Z) \\leftarrow K-means(R, K, C_{t-1})$\n\u25b7 Obtain Cluster Labels\n$Z = \\{Z_i\\}_{i=1}^n$ (Eq. 3)\nStep 3: Local Model Training\n7: for cluster k \u2208 {1, ..., K} do\n8: $S_k \\leftarrow S_k \\cup \\{X_i | Z_i = k\\}$ \u25b7 Gather samples for cluster k\n9: $S_k \\leftarrow S_k[ (S_k - n + 1) :]$\n\u25b7 Keep the last batch_size = n entries\n10: $S \\leftarrow select_samples(S_k)$ \u25b7 Select samples (Eq. 7)\n11: for each $X_i \\in S$ do\n12: $X_i' \\leftarrow augment(X_i)$ \u25b7 Augment data (Eq. 9)\n13: Train $(q_{\\theta_k}, X_i, X_i')$ \u25b7 Train local model (Eq. 4)\n14: end for\n15: end for\nStep 4: Compile Predictions\n16: $Y \\leftarrow collect_sort(\\left\\{q_{\\theta_k}(X)\\right\\})$ \u25b7 Collect and sort predictions\nStep 5: Global Model Communication\n17: If t% f == 0 : \u25b7 Model Communication with interval f (Eq.5, 6)\n18: $\\Theta_{global} = \\sum_{k=1}^{K} w_k \\theta_k$\n19: $\\theta_k \\leftarrow \\Theta_{global}$\nsamples comprehensively, enhancing data quality for each individual shifting case and boosting mode performance across complex real-world scenarios."}, {"title": "Sample Selection Mechanism", "content": "We leverage a criterion derived from the weighted entropy framework used in ETA [26] based on two primary conditions:\n$Cri = \\mathbb{1} [ (H(y_t) < H_0) \\land (\\left|\\cos(y_t, \\overline{y}_{t-1})\\right| < \\epsilon) ].$\n(7)\nThe entropy $H(y_t)$ measures the uncertainty in the current predictions. The cosine similarity $\\cos(y_t, \\overline{y}_{t-1})$ denotes the deviation between the current sample's class probabilities $y_t$ and the aggregated class probabilities $\\overline{y}_{t-1}$. $\\epsilon$ is the threshold for cosine similarity, and $H_0$ is the fixed entropy threshold. This ensures that selected samples exhibit significant deviations from previous predictions in class distribution and lower prediction uncertainty."}, {"title": "Frequency-Based Augmentation", "content": "The augmentation process involves perturbing the amplitude spectrum. Let $A(F_i)$ represent the amplitude spectrum of a selected sample $X_i$. To generate a perturbed amplitude spectrum $\\tilde{A}(F_i)$, we apply a random Gaussian perturbation:\n$\\tilde{A}(F_i) = (1 + \\alpha \\cdot \\Delta) \\cdot A(F_i),$\n(8)\nwhere $\\Delta \\sim \\mathcal{N}(0, \\sigma^2)$ is a perturbation matrix sampled from a Gaussian distribution, and $\\alpha$ is a scaling factor. Then, the augmented sample $X_i'$ is reconstructed via the inverse Fourier transform to the perturbed amplitude spectrum, combined with the original phase spectrum $P(F_i)$:\n$X_i' = \\mathcal{F}^{-1} (\\tilde{A}(F_i), P(F_i)).$\n(9)\nLoss Function. The training objective combines the entropy loss of the selected samples with a consistency loss from the augmented samples. The total loss is defined as:\n$\\mathcal{L}_{total} = \\frac{1}{n} \\sum_{i=1}^{n} H(y_i) + \\frac{\\lambda_c}{n} \\sum_{i=1}^{n} \\mathcal{L}_{con} (\\overline{y}_i, \\hat{y}_i),$\n(10)\nwhere the entropy loss $H(y_i)$ for the original samples is given by $H(y_i) = -\\sum_{j=1}^{C} y_{i,j} log \\hat{y}_{i,j}$ with $y_i$ being the predicted probability over the C classes for the sample $X_i$, and the consistency loss $\\mathcal{L}_{con} (\\overline{y}_i, \\hat{y}_i)$ is defined as the cross-entropy between the prediction $\\overline{y}$ of the augmented sample $X'$ and the pseudo-label $\\hat{y}$ from the original sample:\n$\\mathcal{L}_{con} (\\hat{y}_i, \\tilde{y}_i) = -\\sum_{j=1}^{C} \\hat{y}_{i,j} log \\tilde{y}_{i,j}$"}, {"title": "5. Experiments", "content": "Datasets and Experimental Settings. To provide a comprehensive evaluation of TTA deployment, we test models over multiple datasets under three different scenarios:\n\u2022 Common Image Corruptions: We evaluate models on CIFAR-10-C, CIFAR-100-C, and ImageNet-C [11] with 10, 100 and 1000 classes, respectively. These benchmarks are designed to assess the model robustness against various corruptions. Each dataset consists of 15 distinct corruptions across five severity levels, resulting in 150,000 at each severity for CIFAR-10-C/100-C, and 750,000 for ImageNet-C.\n\u2022 Natural Domain Shifts: We extend our evaluation to DomainNet126 [29], which presents natural shifts across four domains (Real, Clipart, Painting, Sketch) encompassing 126 classes, representing a subset of the larger DomainNet dataset.\n\u2022 Medical Application: Models are further evaluated on Camelyon17 [1], comprising over 450,000 histopathological patches from lymph node sections for binary classification of normal and tumor tissue, with data originating from five distinct healthcare centers.\nFor corruption datasets, the model is pretrained on the clean dataset and the 15 corruptions are randomly mixed as the target distribution. We leverage the highest severity s= 5 in all the experiments. In DomainNet126 and Camelyon17, one subdomain is selected as the source, and the others serve as mixed target distributions. More implementation details are provided in Appendix A. All reported results are averaged over runs with fixed seeds (0, 1, and 2).\nAdaptation Scenarios. To evaluate models in adapting to heterogeneous data streams, we focus on two primary distribution shift scenarios including:"}, {"title": "FreDA consistently improves across different distribution shifts", "content": "shifts. In environments characterized by simultaneous covariate and label shifts, our approach keep showing exceptional adaptability (see Table 2). We attribute this success to FreDA's ability to separate covariate shifts from label shifts via decentralized learning. FreDA achieves this by first isolating target different distribution shifts and then focus on learning label shifts for each specific distribution. This sequential approach prevents models from being overwhelmed by simultaneous shifts, allowing it to address each type of shift independently and effectively.\nFreDA remains stable under various batch size. To simulate deployment with constrained batch sizes, we evaluate models under both varying batch sizes and mixed distribution shifts. In Figure 3, we present the results on CIFAR-10-C, CIFAR-100-C, and ImageNet-C datasets using batch sizes ranging from 200 (64) down to 1. Unlike other methods that significantly degrade as batch size decreases \u2013 for example the error rate of DeYO increases from 27.7% to 89.8% when batch size drops from 200 to 1 on CIFAR-10-C \u2013 FreDA consistently maintains strong performance. This stability under limited batch sizes demonstrates FreDA's robustness, making it highly suitable for real-world applications where large batches is not always feasible.\nFreDA enhances adaptation via synergistic designs. This section validates our designs by ablating its three key modules - Decentralized Training (DT), Sample Selection (SS), and Sample Augmentation (SA). The baseline here leverages only the entropy loss. From Table 4, we have the following observations: 1) Implementing decentralized training alone results in substantial improvements, reducing error rates dramatically across all datasets. 2) The impact of sample selection varies across datasets. While significantly improving performance on CIFAR100-C, it increase error rate on Camelyon. This variation suggests that sample selection helps the model focus on more representative or challenging samples but may not be effective across all datasets, highlighting its dataset-specific nature. 3) Sample augmentation alone tends to increase error rates, suggesting that although this approach introduces useful variability, it may introduce unexpected noise under the absence of proper selection or decentralized training. 4) The combined approach delivers the best performance across all datasets, showing the synergistic effect of our different designs."}, {"title": "6. Conclusion", "content": "This paper advances Test-Time Adaptation (TTA) by tackling the real-world complexities of heterogeneous data streams. Our decentralized approach precisely manages diverse data shifts, improving model adaptation in varied settings. By integrating Fourier-based augmentation, we expand the range of confident samples for each distribution shift, further boosting model performance. The experimental results underscore the efficacy of FreDA, highlighting its potential to influence the field and guide future research in adapting to dynamic and diverse data shifts."}, {"title": "A. Implementation Details", "content": "Supplementary Material\nPretrained Models. We utilize models from Robust-Bench [3], including WildResNet-28 [46] for CIFAR-10-C and ResNeXt-29 [37] for CIFAR-100-C, both pretrained by Hendrycks et al. [12]. For ImageNet-C, the pretrained ResNet-50 [10] and VitBase-LN [5] are obtained from torchvision. For DomainNet126, pretrained ResNet-50 are sourced from AdaContrast [2], while for Camelyon17, we train a DenseNet-121 [14] from scratch to 100 epochs with other training specifications outlined in the Wilds benchmark [16].\nHyperparameter Configuration. The batch size is set to 200, 64, 128 and 32 for CIFAR-10/100-C, ImageNet-C, DomainNet126 and Camelyon17 following the previous methods. The SGD optimizer is used with learning rates adjusted to 0.01, 0.0001, 0.001 and 0.00005, respectively. The learning rate is proportionally decreased in the experiment studying the effect of batch size. The Kmeans Size is 512, Clutser Number is 4, Communication Interval is 10 across all the tasks. The perutrbation magnitude \u03b1 is fixed to 0.1 and the coefficient \u03bb in loss function is fixed to 0.5. The 8 parameter controlling the dependent sampling (Dirichlet distribution) is set to 0.1 for CIFAR10-C and adjusted to 0.01 for CIFAR100-C, ImageNet-C following UnMix-TNS [32]. Two threshold in Eq. 7 is set to the same value for corruption datasets and DomainNet126 following ETA [26]. While for Camelyon17, the class diversity related threshold is adjusted to 0.9 empirically."}, {"title": "B. Compared Methods", "content": "TBN [25] re-estimates batch normalization statistics from test data. TENT [34] minimizes prediction entropy to optimize batch normalization. COTTA [36] addresses long-term test-time adaptation in changing environments. ETA [26] and SAR [27] exclude unreliable and redundant samples during optimization. AdaContrast [2] utilizes contrastive learning to refine pseudo-labels and improve feature learning. ROTTA [45] presents a robust batch normalization scheme with a memory bank for category-balanced estimation. RDumb [28] leverages weighted entropy and periodically resets the model to its pretrained state to prevent collapse. DeYO [17] quantifies the impact of object-destructive transformations for sample selection and weighting. UnMix-TNS [32] introduces a test-time normalization layer for non-i.i.d. environments by decomposing BN statistics. For fair comparisons, we conduct experiments"}, {"title": "C. Continual Setting Evaluation", "content": "Although our method is specifically designed for mixed domain scenarios, we also evaluated its performance under the conventional continual test-time adaptation setting to assess its robustness in different contexts. In this setting, the model adapts online to a sequence of test domains without explicit knowledge of domain shifts, with only one distribution shift occurring at a time and not reappearing. Without adjusting any parameters, our method demonstrated competitive performance compared to current state-of-the-art approaches. Notably, while UnMix-TNS effectively addresses non-i.i.d. issues (dependent sampling at the class level), it is less effective under i.i.d. conditions. Our results suggest that the proposed FreDA not only excels in its intended mixed domain scenarios but also generalizes effectively to standard continual adaptation tasks, providing a robust solution across various distributional challenges."}, {"title": "D. Parameter Study", "content": "In this section, we study the parameter choice of CLUSTER_NUM, KMEANS_SIZE and COMM_INTERVAL (refer to Algorithm 1 for detailed definitions). Results are reported in Table 6.\nAs we adjust the KMEANS_SIZE parameter from 256 to 2048, there is a remarkably consistent performance on"}, {"title": "E. Adaptation Scenarios"}]}