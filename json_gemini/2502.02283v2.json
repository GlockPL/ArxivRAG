{"title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting", "authors": ["Zhihao Guo", "Jingxuan Su", "Shenglin Wang", "Jinlong Fan", "Jing Zhang", "Liangxiu Han", "Peng Wang"], "abstract": "3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds consistently compromises the scene reconstruction quality. To address these limitations, this paper proposes a novel 3D reconstruction framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output Gaussian Process model is developed to achieve adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. The densified point clouds provide high-quality initial 3D Gaussians to enhance reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.", "sections": [{"title": "Introduction", "content": "Novel View Synthesis (NVS) is a fundamental yet challenging problem in computer vision and computer graphics, aiming to generate novel viewpoints of a given scene from multi-view observations. It plays a critical role in applications such as Digital Twinning (Wang et al. 2024b), virtual reality (Fei et al. 2024), and robotics (Xiong et al. 2024; Wang et al. 2024a). Neural Radiance Fields (NeRF) (Mildenhall et al. 2021) have revolutionized NVS by implicitly modelling volumetric scene representations, achieving high-fidelity rendering without explicit 3D geometry reconstruction. However, NeRF-based methods suffer from computational inefficiency and the requirement of dense sampling along rays, leading to slow inference speeds despite recent acceleration efforts (Guo and Wang 2024; Mildenhall et al. 2021; Roessle et al. 2022; Zhang et al. 2020),.\nTo overcome these limitations, 3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) has emerged as a promising alternative for real-time rendering. Unlike NeRF, which relies on ray-marching and volumetric integration, 3DGS represents scenes explicitly using a set of 3D Gaussians with learnable attributes like positions and colours. The rasterization-based splatting strategy can be considered a forward rendering technique that avoids costly ray sampling and enables efficient parallel rendering, making it a compelling choice for NVS applications. However, existing 3DGS pipelines rely heavily on Structure-from-Motion (SfM) to extract 3D points by detecting and matching texture features like SIFT points, leading to sparse or incomplete point clouds, which is particularly the case in texture-less or clustered regions, leading to poor initialization of 3D Gaussians. Consequently, artefacts arise in the rendering process, causing a loss of fine details. To mitigate the limitations of sparse SfM-based reconstructions, the Adaptive Density Control (ADC) strategy has been introduced to 3DGS pipelines. ADC aims to improve scene coverage by dynamically duplicating Gaussians in underrepresented regions and removing redundant ones in over-reconstructed areas. However, ADC operates without explicit geometric priors, often producing noisy distributions that fail to adhere to the underlying scene structure. As a result, this leads to blurry reconstructions and poor occlusion handling, limiting the overall fidelity of 3DGS-based novel view synthesis. These challenges significantly limit the fidelity and robustness of 3DGS-based NVS methods, motivating the community for a more structured and adaptive approach to point cloud refinement and rendering.\nIn this paper, we propose GP-GS (Gaussian Processes for Enhanced 3D Gaussian Splatting), a novel framework designed to enhance the initialization of 3D Gaussians and improve rendered quality, especially in complex regions with densely packed objects (e.g., foliage) or under challenging lighting conditions. To be specific, we introduce a Multi-Output Gaussian Process (MOGP) model to achieve adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we formulate point cloud densification as a continuous regression problem, where the MOGP learns a mapping from 2D image pixels and their depth priors to a denser point cloud with position and colour information. To ensure a structured and robust densification process, we propose an adaptive neighbourhood-based sampling strategy, where pixels are selected as candidate MOGP inputs for densification. For each sampled pixel, we predict its corresponding 3D point cloud attributes (position, colour, variance) using the MOGP model. To further refine the densified point cloud, we apply a variance-based filtering method, which removes high-uncertainty predictions, thereby reducing noise accumulation and preserving high-confidence reconstructions. The densified point clouds provide high-quality 3D Gaussian blobs (or Gaussians for brevity) to enhance the reconstruction performance.\nOur contributions can be summarized as: 1) Gaussian Processes for Point Cloud Densification: We propose a MOGP model to densify sparse SfM point clouds by learning mappings from 2D image pixels and depth information to 3D positions and colours, with uncertainty awareness. 2) Adaptive Sampling and Uncertainty-Guided Filtering: We introduce an adaptive neighbourhood-based sampling strategy that generates candidate inputs for MOGP for 3D points prediction, followed by variance-based filtering to remove high uncertainty predictions, ensuring geometric consistency and enhancing rendering quality. 3) Our GP-GS framework can be seamlessly integrated into existing SfM-based pipelines, making it a flexible plug-and-play module that improves the rendering quality of other NVS models."}, {"title": "Related Work", "content": "Gaussian Processes (GPs) are a collection of random variables, any finite number of which subjects to a joint Gaussian distribution (Rasmussen 2003). GPs are particularly effective in handling sparse data, making them well-suited for scenarios with limited observations (Wang et al. 2021). Their flexibility and built-in uncertainty quantification enable robust predictions, which has led to their widespread adoption in various computer vision tasks (Lu et al. 2023; Zhou et al. 2023; Zhu, Wang, and Mihaylova 2021; Yasarla, Sindagi, and Patel 2020).\nMathematically, a GP is fully specified by its mean function m(x) and covariance function k (x, x'), a.k.a. kernel function, where x and x' are inputs. The covariance function typically depends on a set of hyperparameters 4, such as length scale l, variance 02, or other parameters depending on the kernel type (e.g., Mat\u00e9rn). A GP is usually used as a prior over a latent function defined as:\n\\(f(x) \\sim GP(m(x), k(x,x')).\\) (1)\nTraining a GP model involves optimizing the hyperparameters 8 of the kernel function to maximize the likelihood of the observed data. This is achieved by maximizing the log marginal likelihood, given by:\n\\(logp(y | X, \\theta) = -\\frac{1}{2}y^TK^{-1}y - \\frac{1}{2}log |K| - \\frac{n}{2} log 2\\pi,\\) (2)\nwhere y are the observed outputs, X are the observed inputs, K\u2208 Rnxn is the Gram matrix with entries computed using the kernel function k(xi, xj) across all training inputs, and n is the number of training points. After training, the predictive distribution at a new test point x* follows a Gaussian distribution: f (x*) | X,y,x* ~ \u039d (\u03bc, \u03c3), where the predictive mean and variance are computed as:\n\\[\\mu_* = k_*^T K^{-1}y, \\sigma_*^2 = k(x_*, x_*) - k_*^T K^{-1}k_*, \\]\n(3)\nwhere k* = [k(x*,xi)] \u2208 Rn\u00d71 represents the covariance between the test point x* and all training points, while k(x*, x*) denotes the self-variance of the test point.\n3D Gaussian Splatting\n3D Gaussian Splatting (3DGS) (Kerbl et al. 2023) employs a forward rendering approach. This technique represents scenes using 3D Gaussians and achieves efficient rendering by directly projecting these Gaussians onto the 2D image plane. By circumventing the need for extensive ray sampling and complex volumetric integration, 3DGS facilitates real-time, high-fidelity scene reconstruction. Specifically, it computes pixel colours by depth sorting and a-blending of projected 2D Gaussians. This method avoids the complex calculation of ray marching and volume integration and can achieve real-time high-quality rendering and NVS. Several works have enhanced 3DGS, such as mitigating artefacts arising from camera pose sensitivity (Yu et al. 2024). Others manage points to improve rendering quality (Yang et al. 2024a; Zhang et al. 2024; Bul\u00f2, Porzi, and Kontschieder 2024). To the best of our knowledge, only a few studies have explored the densification of SfM point clouds for 3DGS performance improvement (Cheng et al. 2024; Chan et al. 2024). While there is evidence showing that densification helps to improve the rendering quality of 3DGS, they often overlook the mathematical relationship between pixels and point clouds derived from the initial sparse SfM.\nTo bridge this gap, our method introduces a MOGP model, to learn the mapping relationship between pixels and point clouds, which facilitates the densification of 3D Gaussians, thereby enhancing the rendering quality of 3DGS."}, {"title": "Preliminaries", "content": "3DGS (Kerbl et al. 2023) initialize SfM sparse point clouds as 3D Gaussians (ellipsoid shaped), each defined by specific parameters such as position (mean), rotation, scales, covariance, opacity a, and colour represented as spherical harmonics. For the sake of brevity, this paper abuses x, representing the position of Gaussians, to define 3D Gaussian G as follows:\n\\(G(x) = e^{-(x-\\mu)^T \\Sigma^{-1}(x-\\mu)},\\)\n(4)\nwhere \u03bc\u2208 R3\u00d71 is the mean vector, \u03a3\u2208 R3\u00d73 is the covariance matrix. The 3D covariance \u2211 is a positive semi-definite matrix, which can be denoted as: \u03a3 = RSSTRT, where R \u2208 R3\u00d73 is an orthogonal rotation matrix, and S\u2208 R3\u00d73 is a diagonal scale matrix.\n3D Gaussians will be projected to 2D image space using the splatting-based rasterization technique (Zwicker et al. 2001b). Specifically, the transformation is approximated with a first-order Taylor expansion at a projected point in the camera coordinate frame. This ensures efficient and accurate rendering from a given viewpoint. The 2D covariance matrix \u03a3', which describes the elliptical shape of each Gaussian in the image space, is then computed as:\n\\( \\Sigma' = JWEW^JT,\\)\n(5)\nwhere J is the Jacobian of the affine approximation of the projective transformation, and W denotes the view transformation matrix (Zwicker et al. 2001a). The colour of each pixel is calculated by blending sorted Gaussians based on a, as follows:\n\\(C = \\sum_{i=1}^{n} C_i\\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j),\\)\n(6)\nwhere n is the number of points, ci is the color of the i - th point, ai can be obtained by evaluating a projected 2D Gaussian with covariance \u03a3' multiplied with a learned opacity for each point."}, {"title": "Methodology", "content": "Overview\nThis paper proposes a novel framework GP-GS that enhances 3D Gaussians initialization, thereby improving 3DGS rendering quality. The overview of GP-GS is presented in Figure 2. First, addressing the limitations of sparse SfM point clouds, we propose the MOGP model to formulate point cloud densification as a regression problem. (Sec.). The training strategy of MOGP is introduced in Appendix. Next, we adaptively densify sparse SfM point clouds and propose an uncertainty-based filtering strategy to remove high-uncertainty predictions. (Sec. ).\nMulti Output Gaussian Process\nThis section introduces MOGP to learn the relationship between pixels with depth priors and sparse SfM point clouds with position and colour information.\nProblem Definition. We consider a MOGP regression problem, where the goal is to predict dense point clouds given image pixels and their corresponding depth priors. Given a set of RGB images I = {I}=1, where n is the total number of images, we extract the corresponding sparse point clouds P = {P}=1, where N is the number of 3D points, each point P\u2081 in the point clouds contains both 3D spatial position and colour information: P\u2081 = (xj, Yj, zj, rj, gj, bj), where (xj, Yj, zj) are 3D coordinates and (rj, gj, bj) are RGB grey values. However, one point in the 3D point clouds may correspond to multiple image pixels from various views. To deal with this, we build a correspondence between 2D pixels and 3D point clouds f : Vi \u2192 Pj, where Vi = (Ui, Vi), with (ui, vi) the pixel coordinates in the image space. All pixel coordinates are denoted by V = {V}_1. When multiple pixels Vi1, Vi2,..., Vim contribute to the estimation of a single point Pj, we denote the correspondence as\n\\(P_j = f(V_{i1}, V_{i2},..., V_{im}).\\)\n(7)\nWith the observation that the depth of pixels contributes to the densification performance, we leverage depth priors D = {D}=1 obtained from monocular depth estimation methods, such as (Yang et al. 2024b), to provide accurate estimated depth values Di(Ui, vi) for each pixel with coordinate (ui, vi). For brevity, we will use Di instead of Di (Ui, Vi) below. The rationale behind choosing the monocular depth estimation method is that it strikes a trade-off between accuracy and efficiency, which will accelerate the overall performance of the GP-GS framework.\nMOGP Formulation. The input features X of the MOGP consist of pixel coordinates and depth information in each image defined as xi,j = (Ui, Vi, Di), where (ui, vi) are pixel coordinates, and D\u2081 is the corresponding depth value. The output targets Y include the corresponding position coordinates and RGB values y j = (xj, Yj, Zj, rj, gj, bj). Notably, xi,j is the entry of X indexed by the subscripts i and j, and yj is the j-th entry of Y.\nWe then define the MOGP model as:\n\\(Y \\sim MOGP (m(x), K(x, x')),\\)\n(8)\nwhere m(x) is the mean function and K(x, x') is the covariance function:\n\\[m(x) = [m_1(x) \\quad m_2(x) \\quad \\ldots \\quad m_6(x)]^T,\\]\n(9)\n\\[K(x, x') = \\begin{bmatrix}\nk_{11}(x, x') & k_{16}(x, x')\\\\\n\\vdots & \\vdots \\\\\nk_{61}(x, x') & k_{66}(x, x')\n\\end{bmatrix}\\]\n(10)\nLoss Function and Optimization. For MOGP training, we employ a combined loss function that integrates negative log marginal likelihood and L2 regularization as shown in equation (11). Such an optimisation approach balances data likelihood maximization and regularization, leading to a more robust and generalizable MOGP model.\n\\(L_{total} = - log p(y|X) + \\lambda \\sum_{i=1}^{N} \\sum_j ||\\theta_i||^2,\\)\n(11)\nwhere p(y X) represents the marginal likelihood of the observed output given the inputs, \u03bb\u03a3-1 ||0||2 is the L2 regularization term, with \u03bb = 10-6 representing the weight decay. The L2 regularization helps prevent overfitting by penalizing large weights in the model parameters 0.\nTo address the challenges posed by the sparse point clouds obtained through SfM, we employ a generalized Radial Basis Function (RBF) kernel, specifically the Mat\u00e9rn kernel for training our MOGP model. In contrast to the standard RBF kernel, the Mat\u00e9rn kernel introduces a smoothness parameter v, which governs the differentiability and local variations of the latent function, and ends up with a better trade-off between smoothness and computational efficiency. Formally, the Mat\u00e9rn kernel between two points x and x' is defined as:\n\\[k_\\nu(x,x') = \\frac{\\sigma^2}{2^{\\nu-1}\\Gamma(\\nu)} \\left(\\sqrt{2\\nu} \\frac{||x-x'||}{\\ell} \\right)^\\nu K_\\nu \\left(\\sqrt{2\\nu} \\frac{||x-x'||}{\\ell} \\right),\\]\n(12)\nwhere \u0393(\u00b7) is the Gamma function, \u03c3\u00b2 is the variance, and K() is a modified Bessel function of the second kind. The parameter v modulates how smooth the function can be: smaller v values permit abrupt transitions, while larger v values enforce smoother spatial changes. Details of tuning v on various datasets can be found in Table 5. Our MOGP training procedure can be found in Appendix Algorithm 1.\nPoint Cloud Densification\nThe process of point cloud densification involves an adaptive neighbourhood-based sampling strategy, which selectively identifies pixels as candidate inputs for the MOGP model (in section). The MOGP model subsequently predicts additional point clouds and applies uncertainty filtering to enhance the density.\nAdaptive Sampling Strategy. To adaptively generate sampling pixels within the image domain, we introduce an adaptive neighbourhood-based sampling mechanism that places samples in circular neighbourhoods surrounding each available training point (ui, vi). Specifically, we define a set of N sampled pixels P as follows:\n\\(P = \\bigcup_{i=1}^{N} \\bigcup_{j=1}^{M}\\{\\left(\\frac{u_i + r \\cos \\theta_j}{W}, \\frac{v_i + r \\sin \\theta_j}{H}\\right) | (U_i, V_i) \\in V_i\\},\\)\n(13)\nwhere \u03b8; \u2208 [0, 2\u03c0) are uniformly distributed angles that control the sampling directions, r = \u03b2\u00b7 min(H, W) is the adaptive movement radius, \u03b2\u2208 (0,1) controls the sampling scale, W and H denote the image width and height, respectively. W and H are used to normalize all samples to the range [0, 1] in equation (13). The parameter M determines the angular resolution of the sampling process. For each sampled pixel (ui, vi), the corresponding depth value D\u2081 is retrieved from the depth image D.\nThis adaptive procedure inherently controls spatial coherence by balancing the need for densification while avoiding redundant sampling. It selects regions near the training data to ensure that the test samples remain within the learned distribution of MOGP, mitigating extrapolation errors. Simultaneously, it prevents over-sampling by maintaining a distance between samples so that the generated points are not in the immediate proximity of existing Gaussians, thus avoiding their mergers within the adaptive density control framework of 3DGS.\nUncertainty-Based Filtering. The MOGP model then takes the sampled pixels P as inputs and provides inferred point clouds P with uncertainty (variances). To identify and filter out low-quality predictions, we employ an empirical distribution of the average variances in the RGB channels, where we choose R2 defined in equation (15) as the quantile level, the details of which can be found in Table 5. Let \u00d1 be the total number of inferred points in P, and \u03a3 \u2208 R\u00d1\u00d76 be the predicted variance, we compute the average variance over the RGB channels as they change dramatically as shown in Figure 4, indicating high uncertainty. To refine the densified point clouds, we rank the average variances {0}1 in ascending order and determine the filtering threshold 7 as r = 62 where R2 is the variance quantile threshold and [.] denotes the ceiling function. Any point with an average variance exceeding T is removed to maintain geometric consistency. Finally, the filtered densified point clouds P* are formed by uniting the original training points with all inferred points: P* = PUP.\nAn overview of the uncertainty-based filtering procedure can be found in Appendix Algorithm 2."}, {"title": "Experiments", "content": "Dataset and Implementation Details\nDataset. Previous approaches to SfM often overlook the explicit correspondence between 2D image pixels and sparse 3D point clouds, we address this gap by introducing the Pixel-to-Point-Cloud dataset. This dataset provides a detailed mapping between image pixels and their corresponding 3D point clouds, thus facilitating more accurate modelling and enabling the training of our MOGP model. We further conduct a comprehensive evaluation of our method on dataset spanning NeRF Synthetic (Mildenhall et al. 2021), Mip-NeRF 360 (Barron et al. 2022), and Tanks and Temples (Knapitsch et al. 2017). These benchmarks are popular in 3D reconstruction and novel-view synthesis due to their diverse characteristics, including high-fidelity 3D models, large-scale outdoor scenes with intricate scenes like clustered leaves, and lighting condition variations. By leveraging our new dataset and evaluation framework, we demonstrate the effectiveness of our approach in advancing point cloud densification and improving reconstruction fidelity across varied and challenging scenarios.\nImplementation Details The MOGP-based densification process in GP-GS can be regarded as a plug-and-play module and can be integrated into any 3D reconstruction framework based on SfM sparse point clouds. We have integrated the process into 3DGS*, one of the state-of-the-art 3D reconstruction models, and compared their performance over the datasets mentioned earlier following 3DGS*'s training hyperparameters. Our MOGP models are trained for 1,000 iterations across all scenes, we set L2 regularization weight \u03bb = 10-6, learning rate l = 0.01, dynamic sampling resolution M = 8, adaptive sampling radius r = 0.25. All experiments are conducted on an RTX 4080 GPU.\nMetrics. The evaluation of the GP-GS is twofold, i.e., the evaluation of the SfM sparse point densification, and the evaluation of the novel view rendering performance. For densification, in line with prior studies on point cloud reconstruction and completion (Yu et al. 2021; Yuan et al. 2018), we adopt the mean Chamfer Distance (CD) to measure the discrepancy between the predicted point clouds and the ground truth. Specifically, for a predicted point set P and its corresponding ground truth set G, the CD between them is calculated as:\n\\[d_{CD}(P,G) = \\frac{1}{|P|} \\sum_{p_i \\in P} \\min_{g_i \\in G} ||p_i - g_i|| + \\frac{1}{|G|} \\sum_{g_i \\in G} \\min_{p_i \\in P} ||g_i - p_i||.\\]\n(14)\nWe also compute the Root Mean Squared Error (RMSE) and R2 (Edwards et al. 2008) to show the robustness of GP-GS under various metrics. The RMSE is calculated to quantify the average squared deviation between the predicted and true values, using \\(RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (g_i - p_i)^2},\\) a lower RMSE value indicates a more accurate prediction. The R2 score captures the proportionate reduction in the residual variance: \\( R^2 = 1 - \\frac{\\sum_{i=1}^{n} (g_i - p_i)^2}{\\sum_{i=1}^{n} (g_i - \\overline{g_i})^2},\\)\n(15)\nwhere gi is the true value, pi is the predicted value, and gi is the mean of the true values. A higher R\u00b2 (closer to 1) indicates better Gaussian process model performance.\nFor novel view rendering performance evaluation, we compare our method against state-of-the-art NVS approaches based on commonly used image-based metrics:"}]}