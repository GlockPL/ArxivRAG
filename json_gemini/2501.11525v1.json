{"title": "Technical Report for the Forgotten-by-Design Project: Targeted Obfuscation for Machine Learning", "authors": ["Rickard Br\u00e4nnvall", "Laurynas Adomaitis", "Olof G\u00f6rnerup", "Anass Sedrati"], "abstract": "The right to privacy, enshrined in various human rights declarations, faces new challenges in the age of artificial intelligence (AI). This paper explores the concept of the \"Right to be Forgotten\" (RTBF) within AI systems, contrasting it with traditional data erasure methods. We introduce \"Forgotten by Design,\" a proactive approach to privacy preservation that integrates instance-specific obfuscation techniques during the AI model training process. Unlike machine unlearning, which modifies models post-training, our method prevents sensitive data from being embedded in the first place. Using the LIRA membership inference attack, we identify vulnerable data points and propose defenses that combine additive gradient noise and weighting schemes. Our experiments on the CIFAR-10 dataset demonstrate that our techniques reduce privacy risks by at least an order of magnitude while maintaining model accuracy (at 95% significance). Additionally, we present visualization methods for the privacy-utility trade-off, providing a clear framework for balancing privacy risk and model accuracy. This work contributes to the development of privacy-preserving AI systems that align with human cognitive processes of motivated forgetting, offering a robust framework for safeguarding sensitive information and ensuring compliance with privacy regulations.", "sections": [{"title": "1 Introduction", "content": "The right to privacy or private life is enshrined in the Universal Declaration of Human Rights (Article 12), the European Convention of Human Rights (Article 8), and the European Charter of Fundamental Rights (Article 7). However, the meaning of privacy protection changes with the technological landscape, and the enforcement of certain privacy principles varies. Artificial intelligence (AI) has reintroduced many themes initially addressed by the EU's General Data Protection Regulation (GDPR). For example, the right to be forgotten has long been interpreted as the right to ask for the removal of personal data. However, AI models typically do not store data per se. They are trained on the data, and removing personal data from the original training dataset would not affect the model, which may still embed sensitive information. A famous instance of this dilemma is clearview.ai, which used images of people's faces to train facial recognition models that remain operational despite the efforts of many individuals and civil society groups to have their faces removed from the model. The national regulators have fined the company in Europe, but the product remains available, and public versions of the same concept are now freely available.\nTo counteract the non-consensual use of personal data, activist groups and researchers have developed techniques that fall under the general term \"obfuscation.\" The FAWKES project provided a tool to combat clearview.ai specifically by poisoning their publicly scraped data. Brunton and Nissenbaum have popularized the idea of obfuscation in their seminal work \"Obfuscation: A User's Guide for Privacy and Protest.\" They argue for obfuscation as a way to take back control of one's privacy.\nThe overarching goal of this paper is to implement the general idea of obfuscation in the design of privacy-preserving AI systems so that it is not a reactionary act of protest but rather a method to protect sensitive data points by design. We employ the concept of a privacy vulnerability score, which quantifies the susceptibility of individual data points to privacy attacks based on the differences in model behavior on in-training versus out-of-training data points.\nWith extensive testing, it is possible to validate that custom obfuscation added to a model prevents the inference of membership, thus satisfying most cases of the right to be forgotten. To systematically evaluate the privacy risks associated with Al models, we employ Membership Inference Attacks (MIA) [1] in this work, specifically the LIRA attack [2]. These attacks help identify whether specific data points were part of the training dataset, thereby assessing the model's vulnerability to privacy breaches. By integrating LIRA into our methodology, we aim to rigorously test and validate the effectiveness of our proposed obfuscation techniques in preserving privacy.\nTo carry out such work, we also need a machine learning task. The CIFAR-10 dataset [3], a widely used benchmark in machine learning research, consists of 60,000 32x32-pixel color images across 10 different classes. It is particularly useful for evaluating the performance and privacy risks of image classification models due to its diversity and complexity. ResNet-18 [4], a deep residual network with 18 layers, is designed to address the vanishing gradient problem by using shortcut connections, making it a robust choice for image classification tasks.\nStructure of the report. First, a general background and definitions are provided to set the context of this article. This includes an interpretation of the Right to Be Forgotten in AI environments and an explanation of machine unlearning (MU), which is one of the suggested ways to address it. A motivation is then shared as to why we decided to use another approach and its advantages compared to MU. This approach introduces elements (differential privacy, vulnerable data points, and membership inference attacks). The working method is then presented with its different steps, starting by setting metrics and identifying vulnerable data points. A numerical experiment is then conducted, leading to an analysis and discussion that concludes by demonstrating the effectiveness of our suggested techniques in reducing privacy risks while maintaining model utility."}, {"title": "2 Background and Definitions", "content": ""}, {"title": "2.1 Understanding the Right to Be Forgotten (RTBF)", "content": "They are the souls who are destined for Reincarnation; and now at Lethe's stream they are drinking the waters that quench man's troubles, the deep draught of oblivion. They come in crowds to the river Lethe, so that you see, with memory washed out, they may revisit the earth above. (Virgil, Aeneid 6. 705).\nThis mythical understanding is in contrast to the usual meaning of forgetting as a passive and sys- temic process of the mind, e.g., decay, that happens without intent or explicit planning. Forgetting can be described as a \u201cbrain-wide well-regulated decay process, occurring mostly during sleep, [that] system- atically removes selected memories\" [5]. Other accounts of forgetting, like motivated forgetting, include an intentional element. It claims that \"limiting awareness of unwanted memories makes us forget them\" [6], and that is especially true for sensitive or traumatic events that people want to forget. However, motivated forgetting still accounts for forgetting by limiting one's attention to the target memory. There does not seem to be a mechanism in human psychology that would be similar to the effect of the river L\u00eath\u00ea that makes one intentionally and actively forget\u00b9.\nHowever, when human memory is metaphorically transferred to computing, it becomes like drinking from L\u00eath\u00ea. It is common to refer to a computer's \"memory\" and to understand deleting data as the computer \"forgetting\" data. The origins of the metaphor of \"computer memory\" are found in von Neu- mann's \"First Draft of a Report on the EDVAC\" [8], which extensively makes use of the term \u201cmemory\u201d in the definitions. It was part of a larger practice in early computing literature to use neurophysiological concepts to describe computer elements [9]. The notion of \"computer memory\" remained in use despite the technology undergoing significant changes [10].\nSuch anthropomorphic metaphors are generally not problematic. In fact, they may be useful heuristics to help non-specialists understand technical concepts like computer storage without having to explain solid-state discs or similar technologies. In the previous decades, the careful and conscious curation of metaphors has helped \"reduce the fear and ignorance that often dishearten first-time computer users\" [11]. A paradigm example of an endearing metaphor is the \"computer mouse.\"\nHowever, there are risks associated with anthropomorphic metaphors. One such risk is that the overuse of anthropomorphism may lead to the disconnect between the intended metaphor and the original meaning. The disconnect is potentially harmful because the metaphor becomes uninformative and non- reflective, but it may also have effects on the original meaning of concepts, like human memory in \u201ccomputer memory\" or attention in the transformer revolution [12]. This phenomenon is called the bi-directionality of metaphors [13, 14] or double mimetism [15]. Metaphors create two-way dynamic relationships between source and target domains. The source domain can be shaped by the evolving target domain and vice versa. For example, using human cognition as a source domain helps illuminate computing and their \"memory\" or ability to \"learn,\" but this also changes how we understand the human mind in view of computer function.\nAt least to some extent, this has happened in the discourse on computer memory and the \"right to be forgotten\", which is enshrined in the article 17 of the GDPR. The article itself is called \"Right to erasure ('right to be forgotten').\" It is formulated as the data subject's right \"to obtain from the controller the erasure of personal data concerning him or her without undue delay, and the controller shall have an obligation to erase personal data without undue delay,\" continuing to list grounds for such erasure. Here, rather than forgetting being a metaphor for erasure, erasure becomes the paradigm of forgetting. What the article requires is the technical process of erasing or deleting digital data, but that is also being taken as equivalent to the person's right to be forgotten. This may seem innocuous prima facie. However, it assumes a model of memory akin to the mythological drinking from L\u00eath\u00ea.\nThe erasure of digital data is an active and intentional process that requires finding and identifying the required data points and removing them from \"memory\" or storage. For example, in cybersecurity, one of the risk factors is residual evidence of sensitive information (e.g., cryptographic keys, personal identifiers), which often persists unexpectedly and requires actively finding and deleting these artifacts as a precaution [16]. Likewise, executing the person's right to erasure would require finding and wiping out the sensitive data, preferably using special methods, which do not rely on marking the space as free upon deletion but not actually changing the storage [17]. However, this is not how forgetting happens in human cognition, neither under the decay model, which is passive, nor under-motivated forgetting, which relies on directing one's attention away from the target memories.\nThis difference between erasure and forgetting becomes crucial in understanding the right to be for- gotten in the context of artificial intelligence (AI), which employs a different mechanism of \"memory\" than traditional computing media. Not all AI systems learn from data; however, many methods that"}, {"title": "2.2 Privacy Risk Audits", "content": "A privacy risk audit involves the systematic and reproducible testing of an AI system to ensure compliance with GDPR, the AI Act, and other related regulations. This process aims to verify that personally identifiable data is not leaked, whether through an API (black box scenario) or by a party with access to the learned model parameters (white box scenario). By conducting privacy risk audits, organizations can identify and mitigate potential privacy breaches, ensuring that their AI systems handle sensitive data responsibly and in accordance with regulatory requirements.\nMembership inference attacks (MIAs) may play an important role in privacy risk audits. These attacks aim to determine whether a specific data point was part of the training dataset of a machine learning model. MIAs exploit the differences in how a model treats data it has seen during training versus data it has not seen. If an attacker can reliably infer the presence of specific data points in the training set, it indicates a privacy risk, as sensitive information about individuals could potentially be exposed.\nIn a recently published opinion [20], the European Data Protection Board (EDPB) emphasizes the importance of assessing the residual likelihood of identification when evaluating the anonymity of AI models. According to the EDPB, AI models trained on personal data cannot always be considered anonymous, as information from the training dataset may still be extractable or otherwise obtained from the model. This highlights the need for thorough privacy risk assessments, including the use of MIAS, to ensure that AI models do not inadvertently expose personal data.\nIn its opinion, the EDPB outlines several key considerations for assessing the anonymity of AI models:\n\u2022 Model Design: Evaluating the approaches taken during the development phase to limit the col- lection of personal data, reduce identifiability, and prevent data extraction.\n\u2022 Data Preparation and Minimization: Assessing the use of anonymization and pseudonymiza- tion techniques, as well as data minimization strategies.\n\u2022 Methodological Choices: Ensuring the use of robust methods that reduce identifiability, such as differential privacy or other techniques for privacy risk mitigation.\n\u2022 Model Outputs: Implementing measures to prevent the extraction of personal data from model outputs, such as output filters or post-training techniques like unlearning.\nThe EDPB also stresses the importance of documenting the measures taken to ensure anonymity and conducting regular assessments to verify their effectiveness. This documentation should include details"}, {"title": "2.3 Research Question and Contribution", "content": "If we cannot conceptualize the right to be forgotten through erasure, nor through retraining, nor can we be satisfied that AI models do not \"remember\" sensitive information by default, how should we understand this right in the context of AI? Contrary to recent approaches [21], we suggest that forgetting in AI should be understood in a way similar to how humans forget rather than through erasure. It should be a motivated but systemic process that shifts the model's parameters away from vulnerable data points, much like the paradigm of motivated forgetting in humans. Our proposed model consists of three major concepts based on human cognition - memory, forgetting, and asking.\n1. Memory in AI is not based on direct or permanent storage, but on so-called parametric memory constituted by learned parameters and generalizations [22]. Some AI models may be augmented with storage but that storage does not raise additional issues as compared to other standard computer media because they allow simple erasure. Thus, we define the memory of AI as contained in the learned parameters or generalizations rather than any additional storage. This definition also aligns with the phenomenology of memory, which acknowledges memory as a constructive activity which \u201cacts to organize what might otherwise be a mere assemblage of contingently connected events\" [23]. The passivist model of memories as replicative repetitions of the past, which would approximate computer memory, has long been rejected in the cognitive and philosophical literature [24, 25]. Learned parameters in AI bear a much closer resemblance to the phenomenological concept of memory.\n2. The conception of memory based on parametric learning leads to a more nuanced version of forget- ting that does not allow for simple erasure. Instead, it is modeled on the paradigm of motivated forgetting in human cognition, i.e., shifting the focus away or blurring our sensitive memories [6] rather than direct deletion. We propose a method called Forgotten by Design that uses targeted obfuscation to achieve a level of non-inference, which we will equate to forgetting. In short, if there is no reliable (i.e., better than random) way of determining that a data point was part of the training data set, then we consider that the AI model has forgotten that data point during or after training.\n3. A key component of the Forgotten by Design model of AI memory is testing whether something has been forgotten. The test we adopt is called a membership inference attack (MIA) [1], which aims to infer whether a data record was used to train a target model or not. This test replicates an indirect memory test in psychology [26] because the goal of an MIA is not to recover the specific data record itself but rather to determine whether a model exhibits a behavioral difference in processing a record that it has seen before. An indirect memory test in humans is also a test of the person's capacity for implicit memory because it aims to measure the unintentional and unconscious influence of a prior experience (analogously, the training dataset in AI) on their behavior."}, {"title": "2.4 Difference with Machine Unlearning", "content": "Machine unlearning [27] involves techniques for removing \u2013 partly or in full \u2013 sensitive information embedded in trained models without having to retrain the models from scratch. The goal, then, is to \"untrain\" a model in such a way that the resulting model behaves as a model that was trained from scratch without the unlearned examples. In the strongest case, the two models are statistically indistinguishable. However, in practice, this is often impossible to achieve, and less strict criteria are used, such as allowing"}, {"title": "2.5 Differential Privacy", "content": "Differential Privacy (DP) is a technique that ensures the privacy of individual data points in a dataset by adding controlled noise to the data or computations, making it difficult to infer any single individual's information. For machine learning applications, it is implemented as a Differentially Private Stochastic Gradient Descent (DP-SGD), which is an algorithm designed to train machine learning models with strong privacy guarantees [30]. It modifies standard Stochastic Gradient Descent (SGD) by incorporating differential privacy techniques. During each iteration, DP-SGD computes the gradient of the loss function with respect to a mini-batch of training data. The algorithm clips the gradients to a fixed norm, limiting the influence of any single data point, and then adds Gaussian noise to the clipped gradients.\nThe privacy guarantee is quantified by parameters e and d, which control the trade-off between privacy and model utility. The final update step in DP-SGD can be expressed as:\n$\\theta_{t+1} = \\theta_{t} - \\eta \\frac{1}{B} \\sum_{i \\in B} (clip(g_i, C) + N(0, \\sigma^2 C^2 I))$\nwhere $\\theta_t$ are the learned model parameters at iteration t, $\\eta$ is the learning rate, B is the mini-batch, clip(gi, C) is the gradient clipping function, and $N(0, \\sigma^2C^2I)$ represents the added Gaussian noise.\nRahimian et al. (2021) discuss various differential privacy (DP) defenses and their effectiveness against membership inference attacks, noting that while DP can robustly protect against strong adversaries, it often reduces model utility [31]. They emphasize that the noise added to ensure privacy can degrade model accuracy and performance, creating a challenging trade-off between privacy and utility. Their study underscores the importance of carefully tuning DP parameters to balance these competing objectives, as overly aggressive privacy settings can significantly impair model performance, while insufficient privacy measures may leave models vulnerable to attacks."}, {"title": "2.6 Vulnerable data points", "content": "In the context of membership inference attacks (MIAs), vulnerable data points are those that can be accurately identified as being part of a machine learning model's training dataset. MIAs exploit the differences in how a model treats data it has seen during training versus data it has not. If an attacker can determine whether a specific data point was used in training, they can infer sensitive information about that data point. Vulnerable data points are particularly susceptible because they exhibit characteristics that make them easier to distinguish. For example, these points might be outliers or have unique features that the model memorizes more distinctly, which makes it easier for an attacker to infer their presence in the training set [32].\nAn effective method for identifying vulnerable data points is the LIRA membership inference attack [2]. LIRA employs multiple shadow models trained on subsets of the dataset to estimate the likelihood that a specific data point was part of the training set. This method models the losses of these shadow models using Gaussian distributions, providing a robust estimation of privacy risks.\nCarlini et al. introduced the concept of the Privacy Onion [33] to describe how removing the most vulnerable data points to privacy attacks exposes a new layer of previously safe data to the same risks."}, {"title": "2.7 Defenses against Privacy Attacks", "content": "Earlier empirical defenses for machine learning privacy often forgo the provable guarantees of differential privacy (DP) to attain greater utility while resisting realistic adversaries. However, these defenses can exhibit significant weaknesses, as highlighted by [34]. The paper argues that previous evaluations often do not adequately capture the privacy leakage of the most vulnerable samples, employ weak attacks, and neglect comparisons with practical differential privacy (DP) baselines. As a result, they can underestimate privacy leakage by orders of magnitude. Stronger evaluations reveal that none of these empirical defenses are competitive with a well-tuned, high-utility DP-SGD baseline, even when the formal DP guarantees are relatively weak.\nThe importance of using appropriate metrics in privacy evaluations cannot be overstated. Carlini et al. critique the traditional evaluation methodology [2], noting that it does not reflect an attacker's ability to confidently breach the privacy of any individual sample. They propose measuring the true positive rate (TPR) at a low false positive rate (FPR) to better capture the attacker's precision. This approach has been adopted by many recent works, but it still aggregates TPR and FPR over a data population, failing to address the privacy of the most vulnerable samples.\nTherefore, it is argued [34] that valuations should focus on vulnerable data points to provide a more accurate assessment of privacy risks. Differential privacy (DP) offers a robust framework by bounding the TPR-to-FPR ratio of any membership inference attack, providing worst-case guarantees for every dataset and target sample. This connection can be leveraged to use membership inference attacks to lower-bound the DP guarantees of an algorithm. Thus, differential privacy, when calibrated to achieve a practical utility-privacy trade-off, emerges as a reasonable defense."}, {"title": "2.8 Scope and Limitations", "content": "The scope of the current paper is to explore the concept of the \"right to be forgotten\" in relation to an AI system. However, we would like to acknowledge a number of limitations that are related to our work that will be important to take into consideration for future work.\n\u2022 This work is experimented with a single dataset (CIFAR-10). There will be a need for further use cases to scale and expand the results.\n\u2022 The current definition of the right to be forgotten in the GDPR2 is limited to erasing data, which is something our work is contesting. It is possible that changes in future legislation will go against our suggested definitions that were made to address the current gap in RTBF for AI.\n\u2022 The evaluation of this work is conducted using Membership Inference Attacks, specifically LIRA. Other attacks could be used, but they are beyond the current scope.\nA more detailed discussion about the limitations of this work is presented in Section 5 (Discussions)."}, {"title": "2.9 Relation to the LeakPro Project", "content": "The LeakPro project\u00b3, funded by Vinnova, aims to create a platform for evaluating the risk of information leakage in machine learning applications. This issue is recognized as a significant threat by regulators worldwide, including the Swedish Authority for Privacy Protection (IMY), as highlighted in their regula- tory sandbox study on federated learning. This initiative is a collaborative effort involving industry and public sector partners such as AstraZeneca, Sahlgrenska, and Region Halland, working together with AI Sweden and RISE to ensure systematic and reproducible testing of AI systems. The platform will support various attack scenarios, including black-box, white-box, federated learning, and synthetic data, aligning with the requirements of the upcoming AI Act. By developing LeakPro as an open-source tool [35], the project seeks to provide scalable and relevant solutions for assessing and mitigating data leakage risks in real-world settings, thus helping Swedish industry and public sector to proactively design AI systems with data protection and prepare for future regulatory reporting requirements.\nIn the context of this report, work from LeakPro will be used to identify vulnerable data points in a benchmark dataset. Subsequently, LeakPro is used to evaluate the efficacy of the proposed mitigation."}, {"title": "3 Methods", "content": "The method applied in this study involves several key steps. Initially, a Membership Inference Attack (MIA) is used to identify vulnerable data points within a benchmark dataset. Following this, a privacy risk mitigation method is developed, which incorporates adding noise to the gradient sum after assigning lower weights to vulnerable data points identified by their privacy vulnerability t-score. Numerical experiments are then conducted on MIA benchmarks, specifically utilizing the LIRA method. The strength of the privacy protection provided by these mitigations is subsequently evaluated.\nThe Anatomy of Privacy Risk Audit. Conducting a privacy risk audit involves a systematic ap- proach to evaluate and mitigate privacy risks associated with AI models. It is our intention to simulate some of the steps that would be conducted in a privacy risk audit. The following steps outline the process that we used for our purposes:\n1. Establish a Baseline: Assess the privacy risk of the model without any privacy risk mitigation using an audit dataset equivalent to the training data.\n2. Implement Risk Mitigation: Apply privacy risk mitigation techniques that we propose in this work, namely a combination of additive noise and a weighting scheme. Retrain the model with these mitigations in place to ensure that the model incorporates the privacy-preserving measures.\n3. Test the Protected Model: Use the same privacy risk assessment methods employed in the base- line assessment to evaluate the protected model (to ensure consistency in the evaluation process).\n4. Analyze the Results: Compare the results of the protected model with the baseline assessment. Analyze the effectiveness of the risk mitigants in reducing privacy risks. Demonstrate the efficacy of the implemented mitigations by highlighting improvements in privacy risk metrics.\n5. Document the Audit: Thoroughly document all steps of the audit process, including the method- ologies used, the implementation of risk mitigations, the results of the assessments, and a thorough discussion of the robustness and limitations of the methodology.\nWe note that this is a limited version of what would be required in a real-world setting."}, {"title": "3.1 The LIRA Membership Inference Attack", "content": "The LIRA membership inference attack [2] is a technique used to assess the privacy risks of machine learning models. It involves training multiple shadow models on subsets of the dataset and evaluating their performance on specific data points to infer whether those points were part of the training set. This method leverages differences in model behavior on in-training versus out-of-training data points, providing a detailed analysis of privacy vulnerabilities.\nLIRA was chosen for privacy auditing due to its robustness and effectiveness in identifying vulnerable data points. LIRA's approach of modeling losses with Gaussian distributions and using likelihood ratio tests provides a precise method for detecting privacy risks. Its ability to handle large datasets and empirical validation in various studies make it suitable for our privacy auditing needs.\nIn LIRA, multiple shadow models are trained on subsets of the underlying dataset. Each model i when evaluated on a data point j produce an observations zi,j, for example, the cross-entropy loss nor the logits. The LIRA-attack models these losses by a Gaussian distribution, which is specific to whether the point was part of the training set, IN, or not, OUT. With the Gaussian approximation, only the mean and standard deviation need to be estimated for each. However, since the losses are not well approximated by a Gaussian distribution, LIRA uses logit scaling to ensure support in (-8,8):\n$\\psi_j = \\phi(f; x_j, y_j) = log (\\frac{P_j}{1 - P_j})$\nwhere pj is the predicted probability for the true label for data point j. There are empirical evidence in [2, Fig. 4] that the quantity described by ; is well approximated by a normal distribution.\nThe attack then uses the likelihood ratio test as decision criteria:\n$\\Lambda(f; x_j, y_j) = \\frac{N(\\phi(f; x_j, y_j); \\mu^{IN}, V^{IN})}{N(\\phi(f; x_j, y_j); \\mu^{OUT}, V^{OUT})}$"}, {"title": "3.2 Privacy risk metrics", "content": "The effectiveness of a membership inference attack (MIA) is commonly evaluated in terms of the ROC curve, AUC, and TPR at low FPR metrics:\nROC Curve: The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings. It provides a visual representa- tion of a model's ability to distinguish between members and non-members of the training dataset."}, {"title": "3.3 Identifying vulnerable data points", "content": "The log-likelihood ratio $\\Lambda$; defined in the section above is used to decide if a data point (xj, yj) was part of the training data set for a target model, given the mean and variance estimates we obtained from the shadow models. We expect that the estimated IN and OUT distributions for a data point will stabilize as we take more and more shadow models.\nWe could consider using the log-likelihood ratio $\\Lambda_j$ for data point j calculated by equation (4) as an estimate for the privacy attack vulnerability for the data point. However, as the estimated $\\Lambda_j$ depends on the target model, we could expect a considerably different value if we retrain the target model with a different random seed. To reduce the dependence on a single target model and its associated variability, we could instead use some measure of the average vulnerability for a single data point. For this purpose, we define the privacy vulnerability t-score\n$t_j = \\frac{\\mu^{IN} - \\mu^{OUT}}{\\sqrt{V^{IN} + V^{OUT}}}$"}, {"title": "3.4 Defending vulnerable data points", "content": "The approach explored in this paper uses an alternative version of the formula for the DG-SGD parameter update of equation (1) modified to make the obfuscation specific to the individual data point. We call this instance-specific gradient obfuscation, which we define as\n$\\theta_{t+1} = \\theta_t - \\eta \\frac{1}{B} \\sum_{j \\in B} (w_j g_j + u_j)$\nwith added noise $u_j \\sim N(0, \\sigma_j^2 I)$ that is allowed to depend on the individual data point, and wj is the data point specific weight.\nThe effect of instance-specific gradient obfuscation is obtained by altering either $\\sigma_j$, such that a larger magnitude noise is applied for a vulnerable data point or wj, such that a vulnerable data point is down-weighted in the sum, either way reducing its relative contribution to the gradient update.\nWe note that by not using gradient clipping, we forgo the formal guarantees of DP-SGD but argue that this may be reasonable when operating at noise levels that do not provide strong guarantees anyway. Gradient clipping can be computationally expensive and slow down training under DP-SGD, while our approach can be efficiently implemented simply as a weighted loss function.\nInstance specific noise. One alternative approach could take $\\sigma_j = \\sigma(t_j)$ such that a point with a higher vulnerability score tj attracts an obfuscating noise of a larger magnitude\n$t_i \\geq t_j \\Rightarrow \\sigma(t_i) \\geq \\sigma(t_j)$\nThis is akin to instance-specific differential privacy introduced by Nissim et al. (although they do not specifically discuss it in the context of machine learning or gradient descent) [37].\nInstance specific weights. For this work, we instead consider the alternative method of keeping the noise constant, $\\sigma_j = \\sigma_*$, while we let the weights depend on the vulnerability score wj = w (tj) through a non-increasing function\n$t_i \\geq t_j \\Rightarrow w(t_i) \\leq w(t_j)$\nThere are, of course, many different possible functional relationships to consider - linear, exponential, inverse, power laws, etc. For the experiments, we work with a clipped exponential\n$w_{clip}(t) = clip(e^{-\\alpha t + \\beta}; w_{lower}, w_{upper})$"}, {"title": "4 Numerical Experiments", "content": "We experiment with a deep neural network model based on the ResNet architecture applied to the image classification task for the CIFAR-10 dataset. We use the model out-of-the-box as downloaded through the PyTorch library [38], but without pre-trained weights. We implement the instance-specific weighting of gradients as described in Section 3.4, equations (8) and (10) and use the exponential weighting method of equation (12). The experiments repeat the training of the model under different choices of the parameters \u03c3, \u03b1, and \u03b2. For clarity, we repeat the interpretation of these parameters in Table 2.\nDifferent parameter settings give us the opportunity to explore scenarios where the noise addition and weighting are altered. Mainly, we will be discussing the following alternatives\nbaseline: training without any modifications to the gradient update, \u03c3 = \u03b1 = \u03b2 = 0\nuniform: noise with o is added, but there is no weighting, \u03c3 > 0, \u03b1 = \u03b2 = 0\nweighted: both noise addition and weighting are applied, \u03c3 > 0, a > 0, \u03b2 \u2265 0\nMain experiment set-up. The LIRA membership inference attack is carried out against the ResNet- 18 model trained on the CIFAR-10 task, exploring different parameter choices for \u03c3, \u03b1, and \u03b2. The original train-test split of the dataset is maintained, with a test set of 10,000 images set aside solely for evaluating test accuracy. The remaining 50,000 images constitute the audit dataset. For each shadow model, the audit dataset is split into an IN partition, used for training, and an OUT partition, used only for evaluating the LIRA attack.\nInitially, the baseline setting (no noise, no weights) is attacked to establish a baseline for the privacy metrics and to estimate the privacy vulnerability t-score for each image in the audit dataset. Up to 900 shadow models are used to ensure sufficient statistical validity for the subsequent robustness analysis.\nThe experiments are then repeated for the uniform and weighted alternative scenarios, using more than 300 shadow models for each parameter setting. The weighted scenarios use the exponential weighting scheme defined in equation (12) based on the privacy vulnerability t-score calculated as in equation (7), that is, weights falling off exponentially fast with the (non-negative) distance between the estimated center of the IN and OUT distributions of the LIRA attack model.\nFinally, for each parameter setting, we evaluate the privacy metrics as well as the test set accuracy and store the results for analysis."}, {"title": "4.1 Results", "content": "From the Figure 2 we observe that both adding obfuscating noise and down-weighting data points with high privacy risk have the effect of pushing down the ROC curve towards the random baseline. Both the green solid line representing noise at 0.01 standard deviations and the weighted alternative (no noise) dashed blue line are well below the blue solid baseline. In the figure, the weighted results are from a scenario with parameters set to \u03b1 = 2 and \u03b2 = 2, which were selected after noting that they give low privacy risk while maintaining acceptable accuracy (we will return to the analysis of the privacy-utility trade-off in the discussions that follow)."}, {"title": "5 Discussion", "content": "From Table 3, which summarizes the results displayed in Figure 3 for the privacy-utility trade-off, we see that the accuracy is not significantly different from the no-weights, no noise baseline (\"uniform: 0.0\"). Meanwhile, the drop in all the privacy risk metrics is material tau at 0.001 FPR is reduced from about 5.2 to about 1.7, which corresponds to a reduction of more than a magnitude in the expected attack rate success (measured as TPR over FPR). The effect of adding noise only (\u03c3 = 0.01, no weights) is more modest, reducing the tau to 3.7 at the same FPR.\nThe analysis of the risk scores displayed in Figure 4 is consistent with the observations we made for the privacy-utility trade-off of Figure 3 adding noise and applying the weighting both lowers the privacy risk t-score as well as the observed risk metrics tau and auc. At lower noise levels, this only has a small effect on the utility (measured as accuracy). For a reference noise level of \u03c3 = 0.01, where the TPR@0.01FPR dropped by an order of magnitude without significant impact on accuracy, we also observe in Figure 4 a material downwards shift in the privacy vulnerability t-score distribution, with maximum values dropping from near 7 for the baseline ('uniform: 0.0') to slightly above 2 for the case with both noise addition and weighting applied ('weighted: 0.01').\nComputational effort. The weighting scheme uses t-scores to calculate the weights for each audit data point. This would require the model developer to train a sufficient number of shadow models to obtain t-scores before applying the weighting scheme to mitigate the privacy risk. In comparison, noise addition without weighting, can be done without assessing the privacy risk of each data point and,"}, {"title": "5.1 Relation to Previous Work", "content": "Strong Adaptive Membership Inference Attacks. In their seminal work, Carlini et al. [2", "39": "as a robust and computationally efficient method for performing membership inference"}]}