{"title": "REGRESSING THE RELATIVE FUTURE:\nEFFICIENT POLICY OPTIMIZATION FOR MULTI-TURN RLHF", "authors": ["Zhaolin Gao", "Wenhao Zhan", "Jonathan D. Chang", "Gokul Swamy", "Kiant\u00e9 Brantley", "Jason D. Lee", "Wen Sun"], "abstract": "Large Language Models (LLMs) have achieved remarkable success at tasks like summa-\nrization that involve a single turn of interaction. However, they can still struggle with\nmulti-turn tasks like dialogue that require long-term planning. Previous works on multi-turn\ndialogue extend single-turn reinforcement learning from human feedback (RLHF) meth-\nods to the multi-turn setting by treating all prior dialogue turns as a long context. Such\napproaches suffer from covariate shift: the conversations in the training set have previous\nturns generated by some reference policy, which means that low training error may not\nnecessarily correspond to good performance when the learner is actually in the conversation\nloop. In response, we introduce REgressing the RELative FUture (REFUEL), an efficient\npolicy optimization approach designed to address multi-turn RLHF in LLMS. REFUEL\nemploys a single model to estimate Q-values and trains on self-generated data, addressing\nthe covariate shift issue. REFUEL frames the multi-turn RLHF problem as a sequence of\nregression tasks on iteratively collected datasets, enabling ease of implementation. The-\noretically, we prove that REFUEL can match the performance of any policy covered by\nthe training set. Empirically, we evaluate our algorithm by using Llama-3.1-70B-it to\nsimulate a user in conversation with our model. REFUEL consistently outperforms state-\nof-the-art methods such as DPO and REBEL across various settings. Furthermore, despite\nhaving only 8 billion parameters, Llama-3-8B-it fine-tuned with REFUEL outperforms\nLlama-3.1-70B-it on long multi-turn dialogues. Implementation of REFUEL can be found\nat https://github.com/ZhaolinGao/REFUEL/, and models trained by REFUEL\ncan be found at https://huggingface.co/Cornell-AGI.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite the impressive performance Large Language Models (LLMs) have demonstrated on tasks like summa-\nrization, question answering, and short conversations (OpenAI, 2023; Meta, 2024; Google, 2024; Anthropic,\n2024), most LLMs struggle with planning effectively for long conversations that involve multiple rounds of di-\nalogue or asking follow-up questions about previous responses (Irvine et al., 2023; Abdulhai et al., 2023). The\nroot cause of this deficiency is that the most preference fine-tuning methods using Reinforcement Learning from\nHuman Feedback (RLHF, Christiano et al. (2017); Ziegler et al. (2020); Ouyang et al. (2022); Rafailov et al.\n(2024b); Azar et al. (2023); Guo et al. (2024); Rosset et al. (2024); Dong et al. (2024); Gao et al. (2024); Wu et al.\n(2024); Meng et al. (2024)) treat all tasks as single-turn (i.e. as a contextual bandit (Auer et al., 2002; Langford\n& Zhang, 2007)) even when some tasks are fundamentally multi-turn (e.g. a multi-step dialog with a user).\nThe simplest way way to convert a multi-turn task, such as dialogue, into a single-turn task is to train on the\nlast-turn of the dialogue and use dialogue history as context. Although this approach is appealing due to its"}, {"title": "2 PRELIMINARIES", "content": "Consider a conversation between a human and an AI assistant. Let the initial question from the human be\ndenoted as $x_1 \\sim p$. Upon receiving $x_1$, the AI assistant, $\\pi$, generates a response $y_1 \\sim \\pi(\\cdot|x_1)$. Subsequently,\ngiven $x_1, y_1$, the human responds in turn $X_2 \\sim T(\\cdot|x_1, y_1)$, where $T(\\cdot|\\{x_i, Y_i\\}_{i=1}^{h-1})$ denotes the conditional\ndistribution of the human responses $X_{h+1}$. Upon receiving $x_2$, the AI assistant generates a second response\n$Y_2 \\sim \\pi(\\cdot|X_1, Y_1, x_2)$. This interactive process continues until we reach the total number of turns $H$. At the end\nof the interaction, the AI assistant receives a trajectory-level reward $r(\\{x_h, Y_h\\}_{h=1}^{H})$. In this work, we do not\nfocus on the learning process of the reward function $r$; instead, we utilize existing pre-trained reward models.\nWe can cast the multi-turn RLHF as a standard multi-step Markov Decision Process (MDP) by using the\nconversational transcript as state. Let the state $s_h$ at turn $h$ comprise all prior information up to turn $h$,\nexcluding the current response: $s_h = \\{X_1,Y_1,..., X_{h-1}, Y_{h-1},X_h\\}$. Then, the response $y$ can be interpreted\nas an action. We denote the state and action spaces at step $h$ as $S_h$ and $Y_h$ respectively. For simplicity, we\nassume $|y_h| = Y$ for all $h \\in [H]$. The policy $\\pi$ maps from a state $s_h$ to the next response $y_h$, i.e., $Y_h \\sim \\pi(\\cdot|S_h)$.\nWe denote $d_h^\\pi(s)$ as the state distribution at turn $h$ induced by the policy $\\pi$, with $s_h \\sim d_h^\\pi$ as the process of\nsampling $s_h$ from $\\pi$. The policy receives a reward $r(S_{H+1})$ after step $H$ where, for notation convenience, we\ndenote $S_{H+1} = (S_H, Y_H)$. Note that $S_{H+1}$ is the entire multi-turn conversation. The dynamics $P(s_{h+1}|S_h,Y_h)$\nare fully determined by $T$ that governs the response generation process of the human, i.e., $x_{h+1} \\sim T(\\cdot|s_h, Y_h)$\nand $s_{h+1} := \\{s_h, Y_h, X_{h+1}\\}$. We emphasize that in contrast to the standard single-turn RLHF setting which is\noften modeled by a deterministic MDP or bandit problem, the transition $P$ is random as $T$ is random.\nRollins & Rollouts. Given a state $s_h$ and a response $y_h$, we denote by $S_{H+1} \\sim \\pi(s_h, Y_h)$ the process of\nsampling the final state by generating response $y_h$ at $s_h$ followed by executing $\\pi$ until turn $H$ (i.e., finishing\nthe entire conversation). We refer to this process as a rollout of policy $\\pi$. Following the standard RL notation,\nwe denote $Q(s_h, Y_h)$ as the state-action Q function which models the expected future reward-to-go of the\nrandom process of taking $y_h$ at $s_h$ followed by rolling out $\\pi$ to the end. Similarly, given a turn step $h$, we use\nrollin to refer to the process of sampling a state at turn $h$, denoted as $s_h \\sim d_h^\\pi$.\nResets. Given a state $s_h$, resetting to $s_h$ simply means that the policy $\\pi$ starts from $s_h$ again and generates\ncounter-factual trajectories from $s_h$. While resets are often considered as a strong assumption in general RL,\nit is trivially achievable in the context of RLHF for text generation. Resetting to $s_h$ can be implemented by\nfeeding the partial conversation $s_h = \\{X_1,Y_1,...,x_h\\}$ to the transformer-based policy $\\pi$ as a prefix / context.\nThis capability allows a policy to generate multiple independent future trajectories from the same state $s_h."}, {"title": "2.1 THE LIMITATION OF SINGLE-TURN RLHF METHODS ON MULTI-TURN PROBLEMS", "content": "Recent RLHF algorithms such as DPO (Rafailov et al., 2024b), IPO (Azar et al., 2023), SPPO (Wu et al., 2024),\nand REBEL (Gao et al., 2024) are specifically designed for the single-turn setting which can be formulated as a\ncontextual bandit problem with $H = 1$. When applying these methods to multi-turn datasets such as Anthropic\nHH (Bai et al., 2022), it is common to first convert from multi-turn into a single-turn format. Specifically, for\neach sequence of multi-turn interactions $\\{X_1,Y_1,X_2,Y_2,...,X_H,Y_H\\}$, these single-turn methods treat the first\n$H - 1$ interactions as a large context $x := \\{x_1,Y_1, ..., X_H\\}$, and only optimize the last-turn generation of $y_H$."}, {"title": "3 REFUEL: REGRESSING THE RELATIVE FUTURE", "content": "To address covariate shift in multi-turn RLHF without introducing the overhead of an explicit critic network,\nwe introduce REFUEL. REFUEL eliminates the need of an explicit critic by merging the two-step process of\nactor-critic algorithms into a unified procedure and reduces covariate shift by using on-policy datasets. At\neach iteration $t$, REFUEL aims to solve the following KL-constrained RL problem:\n$\\displaystyle \\Pi_{t+1} = \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\mathbb{E}_{h, s_h, y_h \\sim \\pi_t(\\cdot|s_h)} Q_{\\pi_t}(s_h, y_h) - \\frac{1}{\\eta} \\mathbb{E}_{h,s}KL(\\pi(\\cdot|s_h)||\\pi_t(\\cdot|s_h))$\\newline\nIntuitively, the policy $\\pi_{t+1}$ is chosen to maximize the expected reward (through Q-values) while simultaneously\nminimizing the change from the previous policy $\\pi_t$, with the balance determined by parameter $\\eta$. From Ziebart\net al. (2008), we know there exists a closed-form solution to the above minimum relative entropy problem:\n$\\forall h,S_h,Y_h: \\pi_{t+1}(Y_h|S_h) = \\frac{\\pi_t(y_h|s_h) \\exp(\\eta Q_{\\pi_t} (s_h, Y_h))}{Z(s_h)};Z(s_h) = \\sum_{Y_h} \\pi_t(y_h|s_h) \\exp(\\eta Q_{\\pi_t}(s_h, y_h))$\nFollowing Degrave et al. (2019); Rafailov et al. (2024b), we can rearrange Eq. 3 to express the Q-value as a\nfunction of the policy:\n$\\forall h, S_h, y_h: Q_{\\pi_t}(s_h, Y_h) = \\frac{1}{\\eta} \\bigg(\\ln Z(s_h) + \\ln \\frac{\\pi_{t+1}(y_h|s_h)}{\\pi_t(y_h/s_h)}\\bigg)$\nNote that the partition function $Z(s_h)$ does not depend on $y_h$ and that we can sample another response $y'_h$ by\nresetting $\\pi_t$ to $s_h$, $Y'_h \\sim \\pi_t(\\cdot|s_h)$. By taking the difference of the above expression across the paired responses\n$(y_h, y'_h)$ we can eliminate the partition function:\n$\\forall h, S_h, Y_h, Y'_h: Q_{\\pi_t}(s_h, Y_h) - Q_{\\pi_t}(s_h, Y'_h) = \\frac{1}{\\eta} \\bigg(\\ln \\frac{\\pi_{t+1}(Y_h/S_h)}{\\pi_t(Y_h/S_h)} - \\ln \\frac{\\pi_{t+1}(Y'_h|S_h)}{\\pi_t(Y'_h|S_h)}\\bigg).$"}, {"title": "3.1 INTUITIVE EXPLANATION OF REFUEL", "content": "From our above argument, we know that solving Equation 1 optimally would imply that\n$\\forall h, S_h, Y_h, Y'_h: \\frac{1}{\\eta} \\bigg(\\ln \\frac{\\pi_{t+1}(Y_h/S_h)}{\\pi_t(Y_h/S_h)} - \\ln \\frac{\\pi_{t+1}(Y'_h|S_h)}{\\pi_t(Y'_h|S_h)}\\bigg) = Q_{\\pi_t}(s_h, Y_h) - Q_{\\pi_t}(s_h, Y'_h).$\nSumming the above over $y_h$ further implies that there must exist a $y$-independent function $c_h(s_h)$ such that\n$\\forall h,s_h: \\ln \\frac{\\pi_{t+1}(Y_h/S_h)}{\\pi_t(Y_h/S_h)} = Q_{\\pi_t}(s_h, Y_h) - C_h(s_h).$\nRearranging the terms, we can write that\n$\\forall h, S_h,Y_h:\\pi_{t+1}(Y_h|S_h) = \\pi_t(y_h|s_h) \\exp \\bigg(\\eta Q_{\\pi_t}(s_h, Y_h) - \\eta C_h(s_h)\\bigg) \\propto \\pi_t(y_h|s_h) \\exp \\bigg(\\eta Q_{\\pi_t}(s_h, Y_h)\\bigg).$\nNote that that $\\eta C_h(s_h) = \\ln E_{y\\sim\\pi_t(.|s_h)} \\exp(\\eta Q_{\\pi_t}(s_h, Y_h)) = Z(s_h)$ is the log-partition function. In our\nalgorithm REFUEL, we predict the relative future rewards instead of modeling the partition function using an\nadditional critic network. Prior works do not leverage the idea of predicting relative values: they either assume\nthat the partition function is approximately equal to a constant (Zhu et al., 2023) or use an another critic\nfunction to approximate it, incurring extra GPU memory and computation costs (Wu et al., 2024; Richemond\net al., 2024). We also note that that above policy update procedure recovers the NPG update with the softmax\npolicy parametrization (Agarwal et al., 2021), which converges to the globally optimal policy at the rate of\nO(1/T), a faster rate compared to that of standard policy gradient methods."}, {"title": "3.2 MORE RIGOROUS ANALYSIS AND CONNECTION TO PAST POLICY GRADIENT THEORY", "content": "The above simplified explanation relies on an unrealistic assumption that least square regression can learns\nthe Bayes optimal predictor exactly in a point-wise manner. In this section, we analyze the performance of\nREFUEL under a much more realistic assumption \u2013 we assume that the learned predictor in Equation 1 can\npredict well on average under the training distribution. Our analysis below extends that of REBEL (Gao\net al., 2024) from the bandit setting to multi-turn MDPs with stochastic transitions. We denote $S_h$ as the set"}, {"title": "4 EXPERIMENTS", "content": "Our implementation closely follows the psuedocode in Alg. 1. We empirically evaluate REFUEL's ability\nunder two multi-turn RLHF settings. In the first setting, we create a multi-turn conversation simulator that\nuses Llama-3.1-70B-it to simulate a human-in-the-loop. In the second setting, we evaluate our approach using\na pre-sampled sequence of questions from existing multi-turn RLHF datasets to limulate multi-turn dialogue.\nThe first setting models a realistic situation where the learning agent and the user need to interact, while the\nsecond setting models a simplified situation where the sequence of human questions is pre-sampled before the\nconversation begins. However, even in the second setting, the learning agent still needs to learn to generate\nfuture turns conditioned on its own previous turns. Additional experiment details are in Appendix C."}, {"title": "4.1 BASELINES: SINGLE-TURN AND MULTI-TURN", "content": "We compare REFUEL to single-turn and multi-turn baselines that are extensions of two RLHF algorithms,\nDPO (Rafailov et al., 2024b) and REBEL Gao et al. (2024), as well as two open-source LLMs: Llama-3.1-8B-it\nand Llama-3.1-70B-it (Meta, 2024). For the single-turn baselines, we consider the following three settings:\nLast-Turn-Offline (LT-OFFLINE): This is a standard approach to applying single turn methods to a multi-turn\nRLHF dataset. Specifically, we rollin using offline data and train the last turn on pairs of offline responses,\n$D = \\{ (S_H, Y_H, Y'_H) \\sim D_{off}, S_{H+1} = (S_H, Y_H), S'_{H+1} = (S_H, Y'_H) \\}$. For REBEL, the rewards are computed\nusing $S_{H+1}$ and $S'_{H+1}$, while DPO selects chosen and rejected responses based on the reward values."}, {"title": "4.2 SETTING ONE: LLM AS A HUMAN IN THE LOOP", "content": "Task and Implementation. We evaluate REFUEL on UltraInteract (Yuan et al., 2024), which involves the\nmodel responding to instructions with complex reasoning tasks, covering general chat scenarios. We filter the\ndialogues to have a maximum of 5 turns. For simulating the user's random question sampling process, i.e.,\n$X_{h+1} \\sim T(S_h, Y_h)$, we use Llama-3.1-70B-it (Meta, 2024). Our base model is Llama-3-8B-it (Meta, 2024),\nand we employ ArmoRM (Wang et al., 2024) as the reward model. In other words, we create a simulator\n(similar to (Li et al., 2016)) where Llama-3.1-70B-it is acting as a human user, and our agent will interact with\nthe user for multiple turns, starting from the prompts in UltraInteract. Finally, the entire conversation is scored\nby the reward model.\nTo construct this semi-synthetic dataset for REFUEL at each iteration, we begin by sampling an initial state\n$S_1 \\sim D_{off}$, i.e. sample a prompt from the offline UltraInteract dataset. We then uniformly sample the\ndialogue length $H \\sim U(5)$ and a turn step $h \\sim U(H)$. We rollin with our policy to simulate a dialogue up\nto $H$ turns and then reset to turn $h$ to generate another trajectory up to $H$, which gives us one data tuple\n$(S_h, Y_h, Y'_h, S_{H+1}, S'_{H+1})$. We generate the dialogues for the entire dataset (i.e. $|D|$ is the size of UltraInteract)\nand consider the entire dataset as one large batch. Then, we optimize in mini-batch style over the entire\ndataset. We perform 2 iterations for this setup. Additional implementation details, simulator details, and\nhyperparameter settings are listed in Appendix C.1, C.2, and C.3."}, {"title": "4.3 SETTING TWO: USING PRE-SAMPLED QUESTIONS FROM THE DATASETS", "content": "Task and Implementation. In this setting, no LLM is simulating a human user in the interaction loop. Instead,\nwe consider a simplified setting where the sequence of questions comes directly from the dialogues in the\ndatasets. More formally, this setting can be represented by a restricted transition $T$, denoted as $T(\\cdot|\\{x_i\\}_{i=1}^H)$,\nwhich only relies on the human's previous questions $x$ and is independent of the assistant's responses $y$.\nIn this context, the human's questions $x_1,...,x_H$ are pre-sampled based on $T$ before the interaction begins,\nmeaning the human prepares a sequence of questions to ask in advance. While this setup has limitations,\nit allows us to test algorithms and baselines on pre-collected multi-turn dialogues with questions from humans\ninstead of LLMs.\nWe evaluate the performance of REFUEL on the Anthropic Helpful Harmful (HH) task (Bai et al., 2022)\nand the UltraInteract dataset (Yuan et al., 2024). Both datasets are filtered to exclude dialogues with more\nthan 5 turns and 2048 tokens. We compare REFUEL against three baseline algorithms, REBEL-LT-MIXED,\nREBEL-LT-ONLINE, and REBEL-MT-MIXED, as LT-OFFLINE methods are not comparable to other methods.\nWe utilize Llama-3-8B-it (Meta, 2024) as the base model and FsfairX-LLaMA3-RM-v0.1 (Xiong et al., 2024a)\nas the reward model for both datasets."}, {"title": "5 RELATED WORK", "content": "Single-turn RLHF. DPO (Rafailov et al., 2024b) was originally designed for a single-turn RLHF setting,\nwhich can be modeled by a bandit problem or a multi-stage MDP with the deterministic transition. Follow-up\nanalysis of DPO (Rafailov et al., 2024a) is also based on this singe-turn setting, and the derivation of DPO\nbeing capable of learning a Q function is based on deterministic transition. Note that multi-turn RLHF can\nbe stochastic at the turn level since the sampling process of human questions can be highly random. Thus,\nthe analysis and conclusion from (Rafailov et al., 2024a) (i.e., DPO learns the Q functions) do not apply\nwhen naively applying DPO to a multi-turn setting. Other single-turn baselines (e.g., IPO (Azar et al., 2023),\nSLIC-HF (Zhao et al., 2023; Liu et al., 2023), REBEL (Gao et al., 2024), SimPO (Meng et al., 2024), KTO\n(Ethayarajh et al., 2024), ORPO (Hong et al., 2024), SPPO (Wu et al., 2024), HyPO (Song et al., 2024)) also\ndo not directly apply to stochastic multi-stage MDP settings.\nMulti-turn RLHF. Multi-turn RLHF algorithms have been proposed to address reasoning and multi-turn\ndialogue problems. In the context of math reasoning, concurrent work (Kumar et al., 2024b) applied REIN-\nFORCE to a two-turn RL setting, demonstrating the importance of being on-policy for learning self-correction\nbehavior in math reasoning. Another concurrent work (Xiong et al., 2024c) applied single-turn algorithms\nsuch as DPO (Rafailov et al., 2024b), KTO (Ethayarajh et al., 2024), and their online variants (Guo et al.,\n2024; Xiong et al., 2024b) to a multi-turn setting. However both Xiong et al. (2024c) and Kumar et al. (2024b)\nfocus on deterministic transition settings where there is no sequential interaction between users and the models.\nIn our experiments, we compare the multi-turn variants of the single-turn algorithms proposed in (Xiong et al.,\n2024c). For multi-turn dialogue, Snell et al. (2022) built on the implicit Q-learning (Kostrikov et al., 2021)\nwhile Shani et al. (2024b) extended the general preference setting (Dud\u00edk et al., 2015; Saha & Krishnamurthy,\n2022; Wang et al., 2023; Swamy et al., 2024; Munos et al., 2023; Rosset et al., 2024) to multi-turn. In our\nsetting, we focus on RLHF with reward models rather than the general preference setting. Our work focuses\non developing an on-policy RLHF algorithm in the stochastic multi-turn dialogues, where we focus on\nthe importance of being on-policy for multi-turn RLHF, similar to Kumar et al. (2024b) observation in the\nmath reasoning setting.\nPolicy optimization. While so far we demonstrated REFUEL as a multi-turn RLHF algorithm, REFUEL itself\nis a full RL algorithm a new on-policy policy optimization approach. Below we discuss its relationship to"}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "We present REFUEL, a simple, regression-based approach for multi-turn RLHF with strong performance\nguarantees and empirical performance in multi-turn dialogue. We develop a new on-policy multi-turn RLHF\nalgorithm and show the importance of on-policy rollins to avoid covariate shift. We demonstrate that extensions\nof single-turn RLHF methods cannot mitigate the train-test distribution mismatch, deteriorating in performance\nas the conversation goes on while REFUEL improves to reason across the entire dialogue.\nLimitations. While our simulator uses real-world prompts and the LLM Llama-3.1-70B-it to emulate human\nusers, it may not fully capture complex human reasoning and decision-making. Incorporating human-in-the-loop\ntraining could enhance the model's responses. Future work should also include evaluations on real-world\nbenchmarks, such as the multi-turn chat arena (Chiang et al., 2024), to validate performance in dynamic\nsettings. Additionally, although REFUEL can theoretically handle longer conversations, our experiments were\nlimited to 5-turn dialogues. Extending to longer interactions is essential for assessing sustained dialogue\ncapabilities and long-term objectives."}, {"title": "A PROOF OF THEOREM 1", "content": "We first introduce the definition of value functions and advantage functions:\n$\\displaystyle V^\\pi_h(s_h) := \\mathbb{E}_\\pi[\\sum_{h'=h}^H r(s_{h'},y_{h'}) | s_h] = \\mathbb{E}_{y_h \\sim \\pi(\\cdot|s_h)} [Q^\\pi(s_h, y_h)], \\qquad \\forall s_h \\in S_h, h \\in [H],$\n$\\displaystyle A^\\pi(s_h, y_h) := Q^\\pi(s_h, y_h) - V^\\pi(s_h), \\qquad \\forall s_h \\in S_h, y_h \\in Y_h, h \\in [H].$\nThen we have the following performance difference lemma:\nLemma 1. For any policy $\\pi$ and $\\pi'$, we have\n$\\displaystyle J(\\pi') - J(\\pi) = \\sum_{h=1}^H \\mathbb{E}_{s_h \\sim d^\\pi_{h}, y_h \\sim \\pi'(\\cdot|s_h)}[A_h^\\pi (s_h,y_h)].$\nTherefore, from Lemma 1 we know\n$\\displaystyle \\sum_{t=1}^T J(\\pi^*) - J(\\pi_t) = \\sum_{t=1}^T \\sum_{h=1}^H \\mathbb{E}_{s_h \\sim d^{\\pi^*}_{h}, y_h \\sim \\pi^*(\\cdot|s_h)}[A_{\\pi_t}^h (s_h,y_h)].$\nOn the other hand, let us define $\\Delta_h^t$, $\\Delta_h^{\\pi_t}$ as follows:\n$\\Delta_h^t(s_h, y_h) := \\frac{1}{\\eta} \\ln \\frac{\\pi_{t+1}(y_h|s_h)}{\\pi_t(y_h|s_h)} - Q_{\\pi_t}^*(s_h, y_h), \\forall s_h, y_h$\n$\\Delta_h^{\\pi_t}(s_h) := \\mathbb{E}_{y_h \\sim \\pi_t(\\cdot|s_h)} [\\Delta_h^t(s_h, y_h)], \\forall s_h.$\nThen under Assumption 1, we can bound the magnitude of $\\Delta_h^t$, $\\Delta_h^{\\pi_t}$ as the following lemma:\nLemma 2. Under Assumption 1, we have for all $t \\in [T]$ that\n$\\mathbb{E}_{h, s_h \\sim d_t, y_h \\sim \\pi_t(\\cdot|s_h)} \\bigg[(A_h^*(s_h, y_h) - \\Delta_h^t(s_h))^2\\bigg] \\leq \\frac{\\epsilon}{2}$\nNow we can analyze the performance of REFUEL. Let $A_h^t(s_h, y_h)$ denote $A_h^t(s_h, y_h) + \\Delta_h^t(s_h, y_h) - \\Delta_h^{\\pi_t}(s_h)$,\nthen we know for all $t \\in [T]$\n$\\pi_{t+1}(y_h|s_h) \\propto \\pi_t(y_h|s_h) \\exp(\\eta A_h(s_h, Y_h)), \\qquad \\forall s_h, y_h$\nTherefore, REFUELis equivalent to running policy mirror descent (PMD) w.r.t. the reward function $A_h$. PMD\nhas been studied extensively in the literature (Zhan et al., 2023; Gao et al., 2024) and we can obtain the\nfollowing performance guarantee:\nLemma 3. Suppose we have $|A_h(s_h, y_h)| \\leq C$ for all $t \\in [T], h \\in [H], s_h \\in S_h, y_h \\in Y_h$. Then if we initialize\n$\\pi_1$ to be a uniformly random policy and choose $\\eta = \\sqrt{\\ln Y/(C^2T)}$, we have for all $h \\in [H]$ that:\n$\\sum_{t=1}^T \\mathbb{E}_{s_h \\sim d^*_t, y_h \\sim \\pi^*(\\cdot|s_h)} [A_h^t(s_h, y_h)] \\leq 2C \\sqrt{T \\ln Y}.$\nNow from Lemma 3 and (8), we have\n$\\sum_{t=1}^T J(\\pi^*) - J(\\pi_t) = \\sum_{t=1}^T \\sum_{h=1}^H \\mathbb{E}_{s_h \\sim d^{\\pi^*}_h, y_h \\sim \\pi^*(\\cdot|s_h)}[A_{\\pi_t}^h (s_h,y_h)] + \\sum_{t=1}^T \\sum_{h=1}^H \\mathbb{E}_{s_h \\sim d^{\\pi^*}_h, y_h \\sim \\pi^*(\\cdot|s_h)} \\bigg[[ \\Delta_h^{\\pi_t}(s_h) - A_h^t(s_h, y_h)\\bigg]$\n$\\leq 2CH\\sqrt{T \\ln Y} + \\sum_{t=1}^T \\sum_{h=1}^H \\mathbb{E}_{s_h \\sim d^{\\pi^*}_h, y_h \\sim \\pi^*(\\cdot|s_h)} [[ \\Delta_h^{\\pi_t}(s_h) - A_h^t(s_h, y_h)|].$"}, {"title": "A.1 PROOF OF LEMMA 1", "content": "Note that we have\n$\\displaystyle J(\\pi') - J(\\pi) = \\mathbb{E}_{s_1 \\sim p} \\bigg[ \\sum_{h=1}^H r(s_h, Y_h) \\bigg", "s_1)": "n$\\displaystyle= \\mathbb{E}_{s_1 \\sim p} \\bigg[ \\sum_{h=2}^H r(s_h, Y_h) + r(s_1, y_1) \\bigg"}, {"s_1)": "n$\\displaystyle= \\mathbb{E}_{s_1 \\sim p} \\bigg[ \\sum_{h=2}^H r(s_h, Y_h) + \\mathbb{E}_{Y_1\\sim \\pi'} [Q^\\pi(s_1, y_1) - V_2^\\pi (s_2) - V_1^\\pi (s_1)", "bigg": "n$\\displaystyle= \\mathbb{E}_{s_1 \\sim p} \\bigg[ \\sum_{h=2}^H r(s_"}]}