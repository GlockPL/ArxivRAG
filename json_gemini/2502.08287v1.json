{"title": "CRISP: A Framework for Cryo-EM Image Segmentation and Processing with Conditional Random Field", "authors": ["Szu-Chi Chung", "Po-Cheng Chou"], "abstract": "Motivation: Differentiating signals from the background in micrographs is a critical initial step for cryogenic electron microscopy (cryo-EM), yet it remains laborious due to low signal-to-noise ratio (SNR), the presence of contaminants and densely packed particles of varying sizes. Although image segmentation has recently been introduced to distinguish particles at the pixel level, the low SNR complicates the automated generation of accurate annotations for training supervised models. Moreover, platforms for systematically comparing different design choices in pipeline construction are lacking. Thus, a modular framework is essential to understand the advantages and limitations of this approach and drive further development.\nResults: To address these challenges, we present a pipeline that automatically generates high-quality segmentation maps from cryo-EM data to serve as ground truth labels. Our modular framework enables the selection of various segmentation models and loss functions. We also integrate Conditional Random Fields (CRFs) with different solvers and feature sets to refine coarse predictions, thereby producing fine-grained segmentation. This flexibility facilitates optimal configurations tailored to cryo-EM datasets. When trained on a limited set of micrographs, our approach achieves over 90% accuracy, recall, precision, Intersection over Union (IoU), and F1-score on synthetic data. Furthermore, to demonstrate our framework's efficacy in downstream analyses, we show that the particles extracted by our pipeline produce 3D density maps with higher resolution than those generated by existing particle pickers on real experimental datasets, while achieving performance comparable to that of manually curated datasets from experts.\nAvailability: The CRISP package is available at https://github.com/phonchi/CryoParticleSegment/.", "sections": [{"title": "1. Introduction", "content": "Determining the three-dimensional (3D) structure of proteins is crucial for numerous scientific applications. This capability enables the understanding of protein functions in biological processes, the development of targeted treatments for diseases, the design of enzymes for biofuel production, and the creation of effective vaccines. Recently, cryogenic electron microscopy (cryo-EM) image processing Singer and Sigworth (2020) has revolutionized the field by significantly reducing the time required compared to traditional methods such as X-ray crystallography and nuclear magnetic resonance spectroscopy. The primary computational task in cryo-EM is to differentiate the signal from the noisy micrographs. This process aids in understanding particle distribution, detecting contaminants, extracting noise for training denoising models Li et al. (2022), and selecting high-quality particles from micrographs a process known as particle picking. Particle picking is the first critical step toward achieving high-resolution protein structures. However, this process is laborious and challenging due to the low signal-to-noise ratio (SNR), the presence of contaminants, contrast variations resulting from variable ice thickness, and the aggregation of particles.\nRecent developments in automated particle picking based on deep learning object detection have shown promise in overcoming these challenges Wagner et al. (2019); Bepler et al. (2019); Zhang et al. (2025). Nevertheless, these methods are not easily adaptable for identifying irregularly shaped objects such as ice, carbon, filaments, or membranes. Moreover, the use of bounding box outputs does not provide the pixel-level precision, which limits their application in downstream analysis. Consequently, some researchers have turned to image segmentation networks, which can distinguish particles from the background and provide visualizations of particle shapes or symmetries during early processing stages Zhang et al. (2019); Yao et al. (2020); George et al. (2021). Despite these advantages, the aforementioned methods face challenges owing to the low SNR in cryo-EM images, which makes it difficult to generate the accurate pixel-level annotations necessary for training supervised models. Another limitation is that the custom codebases used for segmentation frameworks are typically not user-friendly or well-maintained, thereby raising the barrier for researchers interested in adopting these approaches. Finally, current segmentation frameworks often lack spatial regularization in the segmentation maps, which results in isolated regions or irregular boundaries. We refer readers to Appendix A.1 for more information.\nTo address these issues, we propose a framework for generating segmentation maps from cryo-EM data to serve as ground truth labels for model training. To mitigate the challenges associated with custom codebases, we built our framework on top of segmentation_models.pytorch lakubovskii (2019), a well-developed and flexible project that allows the selection of optimal encoders, architectures, and loss functions for constructing segmentation pipelines. Additionally, we incorporated a conditional random field (CRF) with class-discriminative features to refine weak and coarse pixel-level label predictions, resulting in sharp boundaries and fine-grained segmentation. Our results demonstrate that models trained on our labeled dataset achieve high accuracy, recall, precision, Intersection over Union (IoU), and F1-scores. Furthermore, the performance of our CRF-enhanced picker surpasses that of other state-of-the-art pickers on real datasets and is comparable to that of expert-curated particles. Notably, these models can identify particles that were not labeled in the original dataset, thereby demonstrating their generalization capability in distinguishing signals from background noise. We have organized our framework into a package, CRISP, aimed at fostering further investigation."}, {"title": "2. Methods", "content": ""}, {"title": "2.1. CRISP: The proposed image segmentation and processing framework", "content": "Our proposed image analysis pipeline is illustrated in Fig. 1. Notably, the framework is designed to learn at the pixel level rather than solely relying on particle shapes, thereby achieving high accuracy even on unseen micrographs. As depicted in Fig. 1A, the label generation process efficiently creates ground truth segmentation maps from either a 3D density map or a database comprising raw micrographs, particle coordinates, and particle radius. For training a segmentation model, both the noisy micrograph and the corresponding segmentation map are provided as inputs. The segmentation pipeline is highly modular and flexible, enabling users to choose among different segmentation models, encoders, loss functions, metrics, and various CRF layers. Once the segmentation model is trained on a subset of the dataset, it can be applied to the entire dataset to differentiate signal from background at the pixel level, as shown in Fig. 1B, with the predicted segmentation map subsequently used for downstream analysis. As a proof of concept, we include a particle picking module that employs our proposed center-finding algorithms to extract particles from the predicted segmentation map, as illustrated in Fig. 1C. Furthermore, the extracted particle stacks are saved in the widely used star and .mrcs formats, which can then be processed using 3D reconstruction software such as Punjani et al. (2017) to generate the final 3D density map, as shown in Fig. 1D. Notably, the CRISP framework is modular, allowing the downstream analysis component to be readily replaced with alternative applications, such as training denoising models Li et al. (2022)."}, {"title": "2.2. Label generation pipeline", "content": "Traditionally, obtaining a segmentation map is tedious and requires expert labeling using a carefully designed Graphical User Interface George et al. (2021). Here, we introduce a label generation pipeline that can automatically generate both synthetic and real datasets. For synthetic label generation, we download the 3D density map along with experimental information from EMPIAR Iudin et al. (2023). Then, the defocus value is randomly sampled from real experiments, while the orientation is randomly sampled from SO(3). Based on these parameters, we use ASPIRE-Python (2025) to generate a micrograph containing a large number of 2D projections (e.g., 100-300 particles per micrograph). Finally, we adjust the SNR to 0.005 to reflect the high noise level present in real micrographs.\nFor real dataset generation, inspired by Zhang et al. (2019), we devise a process to automatically generate segmentation labels using real-world datasets. Firstly, we download the micrographs and the corresponding particle coordinates from EMPIAR Iudin et al. (2023) or CryoPPP Dhakal et al. (2023). We then extract particles from each micrograph and use them to reconstruct the 3D density map. Next, we use the mask of the density map to generate reprojected images based on the obtained orientations obtained in 3D reconstruction. Note that we are only interested in the contour of the map; therefore, the mask is used, implying that the 3D density map need not be of high resolution and can be reconstructed using a small set of particles. Thirdly, various thresholding methods are employed to obtain binary segmented particles, which is feasible because the SNR of the reprojected image is much higher than that of the raw micrograph. Finally, we obtain the segmentation map by placing each binary segmented particle onto the micrograph according to the original coordinates."}, {"title": "2.3. The modular segmentation pipeline", "content": "The segmentation pipeline begins with splitting the dataset into training, validation, and testing subsets. We then adopt a patch-based approach for training the segmentation model, which is particularly beneficial for micrographs with high particle densities Zhang et al. (2025). To accelerate training, we randomly crop four patches from each training micrograph. During evaluation, the unseen micrographs are divided into overlapping patches that are fed into the segmentation network to obtain predicted segmentation patches. These patches are subsequently reconstructed using a soft weighting scheme by applying a Gaussian kernel at the interconnected boundaries, thereby producing smoother and more realistic transitions.\nThe pipeline is built upon Iakubovskii (2019) and allows users to experiment with different segmentation models, encoders, loss functions, and metrics. The choice of these components depends on the characteristics of the dataset, the available computational budget, and the specific challenges associated with cryo-EM. For instance, our segmentation framework supports two categories of models: (i) encoder-decoder models, which capture precise spatial hierarchies, and (ii) encoder models with simple interpolation, which are beneficial for handling multi-scale context and efficient computation, as detailed in Appendix A.1. Additionally, users can select from a variety of popular convolutional neural network (CNN) families for feature extraction and choose models with different parameter sizes to balance computational time and accuracy. Furthermore, users can choose different loss functions to optimize for false positives, false negatives, or other objectives, as described in Appendix D.1. Finally, various CRF functions and solvers, as well as different postprocessing techniques for particle picking, can be selected, as detailed in the following sections."}, {"title": "2.4. Spatial regularization with conditional random fields", "content": "A major obstacle in segmenting micrographs is the lack of spatial regularization in the segmentation maps, which often results in isolated regions or irregular boundaries in previous works. This problem is exacerbated by the high degree of noise and outliers present in cryo-EM data. In this context, Conditional Random Fields (CRFs) Kr\u00e4henb\u00fchl and Koltun (2011) have been introduced as a powerful method to refine weak and coarse pixel-level label predictions. Specifically, consider an image $I$ with $n$ pixels classified into $k$ classes. CRFs model the segmentation as a random field $X = {X_1, ..., X_n}$, where each $X_i$ takes a value in ${1,..., k}$, and the posterior probability is modeled as follows:\n$P(X = \\hat{x} | I = \\hat{I}) = \\frac{1}{Z} e^{-E(\\hat{x}|\\hat{I})}$\nwhere the energy function $E(\\hat{x} | I)$ is given by:\n$E(\\hat{x} | \\hat{I}) = \\sum_{i} \\psi_u (\\hat{x}_i | \\hat{I}) + \\sum_{i<j} \\psi_p (\\hat{x}_i, \\hat{x}_j | \\hat{I})$\n(1)\nThe unary energy components $\\psi_u(\\hat{x}_i | \\hat{I})$ quantify the cost of assigning label $\\hat{x}_i$ to pixel $i$, while the pairwise energy components $\\psi_p(\\hat{x}_i, \\hat{x}_j | \\hat{I})$ measure the cost of simultaneously assigning labels $\\hat{x}_i$ and $\\hat{x}_j$ to pixels $i$ and $j$. Maximizing the posterior probability $P(X | I)$ is equivalent to minimizing the energy function $E$. \nIn CRFs, the unary energies are typically obtained from a CNN Zheng et al. (2015); L\u00ea-Huu and Alahari (2021), whereas the pairwise energies allow us to explicitly model interactions between pixels. In the literature, pairwise potentials are commonly modeled as weighted Gaussians:\n$\\psi_p(x_i, x_j | I) = \\mu(x_i, x_j) \\sum_{m=1}^M w_m k_m(f_i, f_j)$\n(2)\nHere, $\\mu(x_i, x_j)$ introduces a penalty for nearby pixels that are assigned different labels and can be set to the indicator function multiplied by a learnable parameter $w_0$, written as $w_0\\mathbb{1}(x_i \\neq x_j)$. Considering an image with feature $f_i$ that includes intensity vectors $I_i$ and positions $p_i$ at pixel $i$, the kernel $k_m$ is:\n$k(f_i, f_j) = w_1 e^{-\\frac{||I_i-I_j||^2}{2\\alpha^2}} + w_2 e^{-\\frac{||p_i-p_j||^2}{2\\beta^2}}$\n(3)\nThe first term is motivated by the observation that nearby pixels with similar intensity are likely to belong to the same class, while the second term helps eliminate small, isolated regions. The parameters $w_1$ and $w_2$ are learnable. Because exact minimization of E is intractable, a mean-field approximation is employed for approximate maximum posterior marginal inference Zheng et al. (2015), with each step of the approximation implemented as a CNN layer for efficient training. However, as CNN architectures have grown stronger and deeper, the relative improvements provided by CRFs have diminished. Consequently, L\u00ea-Huu and Alahari (2021) introduced a regularized Frank-Wolfe algorithm that outperforms mean-field methods in early iterations, converging faster and avoiding prolonged iterations that may lead to vanishing gradients.\nHowever, conventional CRFs rely on predefined features such as image intensity and spatial position. In our framework, we further propose the use of learning-based features similar to those in Chen et al. (2022). Specifically, we replace the intensity feature vector $I$ in the CRF kernel with a new feature vector $g(I)$ derived from CNN feature maps a modification we term the Class-Discriminative CRF (CD-CRF):"}, {"title": "2.5. Center finding and hyperparameter selection process", "content": "When the downstream analysis involves particle picking, it is essential to accurately determine the centers of particles in the predicted segmentation map. We have implemented three distinct algorithms for center finding. The first algorithm leverages a combination of morphological processing, contour extraction, filtering, and minimal enclosing box estimation to robustly detect and refine object centers. The second algorithm is based on the Crocker-Grier method Crocker and Grier (1996). This algorithm identifies bright blobs in the segmentation map and treats those with suitable sizes as potential particles. It allows users to specify parameters such as minimum integrated brightness or particle diameter, thereby incorporating user-provided prior knowledge to address variations in particle size, mass, and symmetry. The final algorithm is based on non-maximum suppression (NMS); in this approach, the micrograph is divided into small grids, and only the maximum candidate from each grid is selected as the particle center.\nEach of these algorithms requires tuning of hyperparameters, such as grid size and the allowable overlap ratio. To address this, we implement a hyperparameter selection scheme based on the ground truth segmentation label and ground truth centers in the validation set, denoted as Mgt and Cgt, respectively. The process is illustrated in Algorithm 1 available in the Appendix. First, we obtain the predicted centers Cpred on Mgt using different algorithms with various hyperparameter configurations. Then, we calculate the mean average precision (mAP) using Cgt and Cpred for each configuration and select the one that achieves the highest score. Once the optimal algorithm and hyperparameters have been identified, they are applied to the entire predicted segmentation maps to obtain the final predicted centers. For more details, please refer to Appendix B."}, {"title": "3. Experiments and Results", "content": "In this study, to test the limitations of our framework, we restrict the training set and validation set to contain only 16 and 6 micrographs, respectively. In addition, U-Net++ Zhou et al. (2018) is selected as our segmentation model because its encoder-decoder design with skip connections is widely used in the literature. Finally, EMPIAR-10017 and EMPIAR-10081 are chosen because they are the datasets with the smallest disk size in the CryoPPP collection, allowing for quick evaluation on a personal computer. The remaining training parameters are detailed in Supplementary Table 6. For more detail information, please refer to Appendix F."}, {"title": "3.1. CRISP can accurately segment micrographs by leveraging its flexible architecture to achieve pixel-level precision", "content": "As a proof of concept, we applied our label generation methodologies to a synthetic dataset, using beta-galactosidase Scheres (2015) as the reference 3D map in the generation process; details are provided in Appendix F. To illustrate the utility of rapidly testing various architectures before selecting one well suited for segmentation, we extensively compared different CNN architectures used in the encoders and report their metrics in Table 1. Note that each CNN family provides networks with different parameter sizes, and we selected the largest size that can be trained on a mainstream GPU so that they have roughly comparable parameter magnitudes. The results indicate that EfficientNet performs best in terms of IoU, recall, and F1 score on the test set, with all metrics exceeding 92%. Recall is reported here specifically because our objective is to harvest as many particles as possible. Regarding the training dynamics, as shown in Fig. 2A, EfficientNet converges much faster than the other CNN architectures, requiring only 10 epochs to reach 80% in IoU and F1 score (see Supplementary Fig. 17). Moreover, as illustrated in Fig. 2B and Supplementary Fig. 9C, the predicted segmentation map closely approximates the ground truth. When zooming into the micrograph, it is evident that all particles are accurately identified by our framework and that their overall shapes are correctly delineated, indicating pixel-level precision.\nIn summary, our findings demonstrate the efficacy of the proposed framework in generating supervised labels and training a highly accurate segmentation model for cryo-EM data. Furthermore, the flexibility of the framework allows for systematic comparison of different components within the pipeline - additional tests are available in Appendix D. We expect that this flexibility will enable researchers to design segmentation pipelines more systematically rather than relying solely on empirical experience."}, {"title": "3.2. CRISP can restore structural signals on a real dataset by incorporating conditional random fields", "content": "We then examine the EMPIAR-10017 dataset Scheres (2015). In this experiment, we download the curated particle coordinates and micrographs from CryoPPP and use the label generation process to produce segmentation labels. For the segmentation model, we employ the best configuration identified from the synthetic dataset, which uses EfficientNet as the encoder and Dice loss as the loss function, and train it on the real dataset.\nThe training dynamics are shown in Supplementary Fig. 19. Our segmentation model converges rapidly, requiring only 10-20 epochs to achieve reasonably good performance in terms of IoU and F1 score. However, the scores are lower than those obtained on the corresponding synthetic dataset; therefore, we visualize the predicted segmentation results in the left panel of Fig. 3A. It is evident that the predicted segmentation contains isolated regions and some of the identified particles are fragmented, indicating a degradation compared with the synthetic dataset.\nWe suspect that spatial regularization is particularly necessary for this more challenging dataset. To address this, we incorporate CRF and CD-CRF layers into the segmentation pipeline during model training. The results are shown in the middle and right panels of Fig. 3A, where some of the isolated regions are eliminated and fragmented particles are restored. After applying the center finding algorithm, the segmentation model with CRF and CD-CRF identifies one and two additional particles in this view, respectively. These results demonstrate that the inclusion of CRF not only improves the segmentation results at the pixel level but also enhances the downstream particle picking performance. Finally, we calculate the metrics on the test set for the three configurations and report them in Table 2. The results indicate that the addition of CRF improves performance, and our proposed CD-CRF outperforms the traditional CRF, with the best performing method being U-Net++ with CD-CRF.\nAnother factor contributing to the lower metrics compared with the synthetic dataset is that CryoPPP is a curated dataset that is not fully labeled. Consequently, it may contain false positives or false negatives due to the extremely low SNR of the micrographs. This is evident in Fig. 3, where our model identifies particles that are not present in the original labels. Therefore, the metrics alone might not be sufficient to determine which method performs best on a real dataset. To further validate the results, we perform 3D reconstruction on the particle datasets obtained from each method and compare their Fourier Shell Correlation (FSC) values. Subsequently, Algorithm 1 is executed, and the results are shown in Supplementary Table 3 and Supplementary Fig. 15. In these comparisons, the NMS method performs best and is therefore chosen for all models. From Supplementary Fig. 20, it is evident that the segmentation model with CRF performs slightly better than the original model. In addition, the performance of CRISP is comparable to that of the human-curated label dataset; the best performing method (U-Net++ with CD-CRF) achieves a resolution of 3.96 \u00c5, as depicted in Fig. 3C, compared to 4.01 \u00c5 for the curated dataset. Finally, it also outperforms other state-of-the-art approaches, as shown in Supplementary Table 5."}, {"title": "3.3. CRISP achieves performance comparable to that of experts in particle picking on a challenging dataset", "content": "Next, we assessed our best model, U-Net++ Zhou et al. (2018) with CD-CRF, on the EMPIAR-10081 dataset Lee and MacKinnon (2017), which is renowned for its flexible regions and represents a challenging type of membrane protein. Notably, to test the limits of our framework, we restricted the training data to only 16 micrographs out of the 300 available in the dataset. The training dynamics are shown in Supplementary Fig. 22. Our segmentation model required more time to learn the features but converged to reasonably good performance after 30 epochs in terms of IoU and F1 score. When we visualized the predicted segmentation results in Fig. 4A, some isolated regions were still present; nonetheless, the overall shape and position of the particles were well captured.\nTo further examine the efficacy of the predicted segmentation map in particle picking, we applied Algorithm 1 to the map, and the results are shown in Fig. 4A. A comparison with the ground truth labels in Fig. 4B reveals that most of the particles were correctly captured by the framework, despite some highly overlapping particles. Supplementary Fig. 23 further presents the 3D reconstruction results and the FSC values obtained from different center finding algorithms. Our model achieved a resolution of 3.94 \u00c5, as depicted in Fig. 4C, demonstrating performance comparable to that of the curated dataset in CryoPPP (FSC value of 3.95 \u00c5). Furthermore, we compared the performance of CRISP with other state-of-the-art approaches, and the results are shown in Table 3. Our method not only achieves the highest resolution but also harvests a reasonable number of particles. Finally, we further pushed the resolution limit by performing non-uniform refinement on both CRISP and the CryoPPP dataset, with the results available in Supplementary Fig. 24. Our method achieved a resolution of 3.65 \u00c5, which is close to the published result of 3.5 \u00c5, despite using only 300 micrographs out of the entire 997 available."}, {"title": "4. Discussion and Conclusion", "content": "Distinguishing structural signals from noisy micrographs is critical for studying protein function using cryo-EM. In this study, we developed a framework to perform image segmentation on micrographs with pixel-level accuracy. To address the laborious task of manual labeling, we created a process capable of generating segmentation maps from existing databases such as EMPIAR or CryoPPP, thereby establishing ground truth labels for training segmentation models on both synthetic and real datasets. Moreover, our framework incorporates a modular segmentation pipeline, allowing users to choose among various built-in networks, loss functions, and metrics for model training. Our experimental results suggest that, by utilizing the segmentation pipeline and the generated labels, a highly accurate segmentation model for cryo-EM micrographs can be constructed. In addition, to further refine the segmentation results, spatial regularization techniques leveraging CRF have been incorporated. Notably, a novel CRF has been proposed to improve performance based on the characteristics of cryo-EM data.\nFor downstream analysis, we demonstrate that the segmentation results can be directly used for particle picking by implementing a center finding algorithm. To this end, we designed a procedure to automatically select the best center finding algorithm based on"}]}