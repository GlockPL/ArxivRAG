[{"title": "CRISP: A Framework for Cryo-EM Image Segmentation and Processing with Conditional Random Field", "authors": ["Szu-Chi Chung", "Po-Cheng Chou"], "abstract": "Motivation: Differentiating signals from the background in micrographs is a critical initial step for cryogenic electron microscopy (cryo-EM), yet it remains laborious due to low signal-to-noise ratio (SNR), the presence of contaminants and densely packed particles of varying sizes. Although image segmentation has recently been introduced to distinguish particles at the pixel level, the low SNR complicates the automated generation of accurate annotations for training supervised models. Moreover, platforms for systematically comparing different design choices in pipeline construction are lacking. Thus, a modular framework is essential to understand the advantages and limitations of this approach and drive further development.\nResults: To address these challenges, we present a pipeline that automatically generates high-quality segmentation maps from cryo-EM data to serve as ground truth labels. Our modular framework enables the selection of various segmentation models and loss functions. We also integrate Conditional Random Fields (CRFs) with different solvers and feature sets to refine coarse predictions, thereby producing fine-grained segmentation. This flexibility facilitates optimal configurations tailored to cryo-EM datasets. When trained on a limited set of micrographs, our approach achieves over 90% accuracy, recall, precision, Intersection over Union (IoU), and F1-score on synthetic data. Furthermore, to demonstrate our framework's efficacy in downstream analyses, we show that the particles extracted by our pipeline produce 3D density maps with higher resolution than those generated by existing particle pickers on real experimental datasets, while achieving performance comparable to that of manually curated datasets from experts.", "sections": [{"title": "1. Introduction", "content": "Determining the three-dimensional (3D) structure of proteins is crucial for numerous scientific applications. This capability enables the understanding of protein functions in biological processes, the development of targeted treatments for diseases, the design of enzymes for biofuel production, and the creation of effective vaccines. Recently, cryogenic electron microscopy (cryo-EM) image processing Singer and Sigworth (2020) has revolutionized the field by significantly reducing the time required compared to traditional methods such as X-ray crystallography and nuclear magnetic resonance spectroscopy. The primary computational task in cryo-EM is to differentiate the signal from the noisy micrographs. This process aids in understanding particle distribution, detecting contaminants, extracting noise for training denoising models Li et al. (2022), and selecting high-quality particles from micrographs a process known as particle picking. Particle picking is the first critical step toward achieving high-resolution protein structures. However, this process is laborious and challenging due to the low signal-to-noise ratio (SNR), the presence of contaminants, contrast variations resulting from variable ice thickness, and the aggregation of particles.\nRecent developments in automated particle picking based on deep learning object detection have shown promise in overcoming these challenges Wagner et al. (2019); Bepler et al. (2019); Zhang et al. (2025). Nevertheless, these methods are not easily adaptable for identifying irregularly shaped objects such as ice, carbon, filaments, or membranes. Moreover, the use of bounding box outputs does not provide the pixel-level precision, which limits their application in downstream analysis. Consequently, some researchers have turned to image segmentation networks, which can distinguish particles from the background and provide visualizations of particle shapes or symmetries during early processing stages Zhang et al. (2019); Yao et al. (2020); George et al. (2021). Despite these advantages, the aforementioned methods face challenges owing to the low SNR in cryo-EM images, which makes it difficult to generate the accurate pixel-level annotations necessary for training supervised models. Another limitation is that the custom codebases used for segmentation frameworks are typically not user-friendly or well-maintained, thereby raising the barrier for researchers interested in adopting these approaches. Finally, current segmentation frameworks often lack spatial regularization in the segmentation maps, which results in isolated regions or irregular boundaries. We refer readers to Appendix A.1 for more information.\nTo address these issues, we propose a framework for generating segmentation maps from cryo-EM data to serve as ground truth labels for model training. To mitigate the challenges associated with custom codebases, we built our framework on top of segmentation_models.pytorch lakubovskii (2019), a well-developed and flexible project that allows the selection of optimal encoders, architectures, and loss functions for constructing segmentation pipelines. Additionally, we incorporated a conditional random field (CRF) with class-discriminative features to refine weak and coarse pixel-level label predictions, resulting in sharp boundaries and fine-grained segmentation. Our results demonstrate that models trained on our labeled dataset achieve high accuracy, recall, precision, Intersection over Union (IoU), and F1-scores. Furthermore, the performance of our CRF-enhanced picker surpasses that of other state-of-the-art pickers on real datasets and is comparable to that of expert-curated particles. Notably, these models can identify particles that were not labeled in the original dataset, thereby demonstrating their generalization capability in distinguishing signals from background noise. We have organized our framework into a package, CRISP, aimed at fostering further investigation."}, {"title": "2. Methods", "content": null}, {"title": "2.1. CRISP: The proposed image segmentation and processing framework", "content": "Our proposed image analysis pipeline is illustrated in Fig. 1. Notably, the framework is designed to learn at the pixel level rather than solely relying on particle shapes, thereby achieving high accuracy even on unseen micrographs. As depicted in Fig. 1A, the label generation process efficiently creates ground truth segmentation maps from either a 3D density map or a database comprising raw micrographs, particle coordinates, and particle radius. For training a segmentation model, both the noisy micrograph and the corresponding segmentation map are provided as inputs. The segmentation pipeline is highly modular and flexible, enabling users to choose among different segmentation models, encoders, loss functions, metrics, and various CRF layers. Once the segmentation model is trained on a subset of the dataset, it can be applied to the entire dataset to differentiate signal from background at the pixel level, as shown in Fig. 1B, with the predicted segmentation map subsequently used for downstream analysis. As a proof of concept, we include a particle picking module that employs our proposed center-finding algorithms to extract particles from the predicted segmentation map, as illustrated in Fig. 1C. Furthermore, the extracted particle stacks are saved in the widely used star and .mrcs formats, which can then be processed using 3D reconstruction software such as Punjani et al. (2017) to generate the final 3D density map, as shown in Fig. 1D. Notably, the CRISP framework is modular, allowing the downstream analysis component to be readily replaced with alternative applications, such as training denoising models Li et al. (2022)."}, {"title": "2.2. Label generation pipeline", "content": "Traditionally, obtaining a segmentation map is tedious and requires expert labeling using a carefully designed Graphical User Interface George et al. (2021). Here, we introduce a label generation pipeline that can automatically generate both synthetic and real datasets. For synthetic label generation, we download the 3D density map along with experimental information from EMPIAR Iudin et al. (2023). Then, the defocus value is randomly sampled from real experiments, while the orientation is randomly sampled from SO(3). Based on these parameters, we use ASPIRE-Python (2025) to generate a micrograph containing a large number of 2D projections (e.g., 100-300 particles per micrograph). Finally, we adjust the SNR to 0.005 to reflect the high noise level present in real micrographs.\nFor real dataset generation, inspired by Zhang et al. (2019), we devise a process to automatically generate segmentation labels using real-world datasets. Firstly, we download the micrographs and the corresponding particle coordinates from EMPIAR Iudin et al. (2023) or CryoPPP Dhakal et al. (2023). We then extract particles from each micrograph and use them to reconstruct the 3D density map. Next, we use the mask of the density map to generate reprojected images based on the obtained orientations obtained in 3D reconstruction. Note that we are only interested in the contour of the map; therefore, the mask is used, implying that the 3D density map need not be of high resolution and can be reconstructed using a small set of particles. Thirdly, various thresholding methods are employed to obtain binary segmented particles, which is feasible because the SNR of the reprojected image is much higher than that of the raw micrograph. Finally, we obtain the segmentation map by placing each binary segmented particle onto the micrograph according to the original coordinates."}, {"title": "2.3. The modular segmentation pipeline", "content": "The segmentation pipeline begins with splitting the dataset into training, validation, and testing subsets. We then adopt a patch-based approach for training the segmentation model, which is particularly beneficial for micrographs with high particle densities Zhang et al. (2025). To accelerate training, we randomly crop four patches from each training micrograph. During evaluation, the unseen micrographs are divided into overlapping patches that are fed into the segmentation network to obtain predicted segmentation patches. These patches are subsequently reconstructed using a soft weighting scheme by applying a Gaussian kernel at the interconnected boundaries, thereby producing smoother and more realistic transitions.\nThe pipeline is built upon Iakubovskii (2019) and allows users to experiment with different segmentation models, encoders, loss functions, and metrics. The choice of these components depends on the characteristics of the dataset, the available computational budget, and the specific challenges associated with cryo-EM. For instance, our segmentation framework supports two categories of models: (i) encoder-decoder models, which capture precise spatial hierarchies, and (ii) encoder models with simple interpolation, which are beneficial for handling multi-scale context and efficient computation, as detailed in Appendix A.1. Additionally, users can select from a variety of popular convolutional neural network (CNN) families for feature extraction and choose models with different parameter sizes to balance computational time and accuracy. Furthermore, users can choose different loss functions to optimize for false positives, false negatives, or other objectives, as described in Appendix D.1. Finally, various CRF functions and solvers, as well as different postprocessing techniques for particle picking, can be selected, as detailed in the following sections."}, {"title": "2.4. Spatial regularization with conditional random fields", "content": "A major obstacle in segmenting micrographs is the lack of spatial regularization in the segmentation maps", "follows": "n$P(X = x | I = \\hat{I"}, "frac{1}{Z} e^{-E(x)}$\nwhere the energy function $E(x | I)$ is given by:\n$E(x | I) = \\sum_i \\psi_u (x_i | I) + \\sum_{i\n    },\n    {", "title\": \"3. Experiments and Results", "content", "In this study, to test the limitations of our framework, we restrict the training set and validation set to contain only 16 and 6 micrographs, respectively. In addition, U-Net++ Zhou et al. (2018) is selected as our segmentation model because its encoder-decoder design with skip connections is widely used in the literature. Finally, EMPIAR-10017 and EMPIAR-10081 are chosen because they are the datasets with the smallest disk size in the CryoPPP collection, allowing for quick evaluation on a personal computer. The remaining training parameters are detailed in Supplementary Table 6. For more detail information, please refer to Appendix F."]}, {"title": "3.1. CRISP can accurately segment micrographs by leveraging its flexible architecture to achieve pixel-level precision", "content": "As a proof of concept, we applied our label generation methodologies to a synthetic dataset, using beta-galactosidase Scheres (2015) as the reference 3D map in the generation process; details are provided in Appendix F. To illustrate the utility of rapidly testing various architectures before selecting one well suited for segmentation, we extensively compared different CNN architectures used in the encoders and report their metrics in Table 1. Note that each CNN family provides networks with different parameter sizes, and we selected the largest size that can be trained on a mainstream GPU so that they have roughly comparable parameter magnitudes. The results indicate that Efficient Net performs best in terms of IoU, recall, and F1 score on the test set, with all metrics exceeding 92%. Recall is reported here specifically because our objective is to harvest as many particles as possible. Regarding the training dynamics, as shown in Fig. 2A, Efficient Net converges much faster than the other CNN architectures, requiring only 10 epochs to reach 80% in IoU and F1 score (see Supplementary Fig. 17). Moreover, as illustrated in Fig. 2B and Supplementary Fig. 9C, the predicted segmentation map closely approximates the ground truth. When zooming into the micrograph, it is evident that all particles are accurately identified by our framework and that their overall shapes are correctly delineated, indicating pixel-level precision.\nIn summary, our findings demonstrate the efficacy of the proposed framework in generating supervised labels and training a highly accurate segmentation model for cryo-EM data. Furthermore, the flexibility of the framework allows for systematic comparison of different components within the pipeline additional tests are available in Appendix D. We expect that this flexibility will enable researchers to design segmentation pipelines more systematically rather than relying solely on empirical experience."}, {"title": "3.2. CRISP can restore structural signals on a real dataset by incorporating conditional random fields", "content": "We then examine the EMPIAR-10017 dataset Scheres (2015). In this experiment, we download the curated particle coordinates and micrographs from CryoPPP and use the label generation process to produce segmentation labels. For the segmentation model, we employ the best configuration identified from the synthetic dataset, which uses EfficientNet as the encoder and Dice loss as the loss function, and train it on the real dataset. The training dynamics are shown in Supplementary Fig. 19. Our segmentation model converges rapidly, requiring only 10-20 epochs to achieve reasonably good performance in terms of IoU and F1 score. However, the scores are lower than those obtained on the corresponding synthetic dataset; therefore, we visualize the predicted segmentation results in the left panel of Fig. 3A. It is evident that the predicted segmentation contains isolated regions and some of the identified particles are fragmented, indicating a degradation compared with the synthetic dataset.\nWe suspect that spatial regularization is particularly necessary for this more challenging dataset. To address this, we incorporate CRF and CD-CRF layers into the segmentation pipeline during model training. The results are shown in the middle and right panels of Fig. 3A, where some of the isolated regions are eliminated and fragmented particles are restored. After applying the center finding algorithm, the segmentation model with CRF and CD-CRF identifies one and two additional particles in this view, respectively. These results demonstrate that the inclusion of CRF not only improves the segmentation results at the pixel level but also enhances the downstream particle picking performance. Finally, we calculate the metrics on the test set for the three configurations and report them in Table 2. The results indicate that the addition of CRF improves performance, and our proposed CD-CRF outperforms the traditional CRF, with the best performing method being U-Net++ with CD-CRF.\nAnother factor contributing to the lower metrics compared with the synthetic dataset is that CryoPPP is a curated dataset that is not fully labeled. Consequently, it may contain false positives or false negatives due to the extremely low SNR of the micrographs. This is evident in Fig. 3, where our model identifies particles that are not present in the original labels. Therefore, the metrics alone might not be sufficient to determine which method performs best on a real dataset. To further validate the results, we perform 3D reconstruction on the particle datasets obtained from each method and compare their Fourier Shell Correlation (FSC) values. Subsequently, Algorithm 1 is executed, and the results are shown in Supplementary Table 3 and Supplementary Fig. 15. In these comparisons, the NMS method performs best and is therefore chosen for all models. From Supplementary Fig. 20, it is evident that the segmentation model with CRF performs slightly better than the original model. In addition, the performance of CRISP is comparable to that of the human-curated label dataset; the best performing method (U-Net++ with CD-CRF) achieves a resolution of 3.96 \u00c5, as depicted in Fig. 3C, compared to 4.01 \u00c5 for the curated dataset. Finally, it also outperforms other state-of-the-art approaches, as shown in Supplementary Table 5."}, {"title": "3.3. CRISP achieves performance comparable to that of experts in particle picking on a challenging dataset", "content": "Next, we assessed our best model, U-Net++ Zhou et al. (2018) with CD-CRF, on the EMPIAR-10081 dataset Lee and MacKinnon (2017), which is renowned for its flexible regions and represents a challenging type of membrane protein. Notably, to test the limits of our framework, we restricted the training data to only 16 micrographs out of the 300 available in the dataset. The training dynamics are shown in Supplementary Fig. 22. Our segmentation model required more time to learn the features but converged to reasonably good performance after 30 epochs in terms of IoU and F1 score. When we visualized the predicted segmentation results in Fig. 4A, some isolated regions were still present; nonetheless, the overall shape and position of the particles were well captured.\nTo further examine the efficacy of the predicted segmentation map in particle picking, we applied Algorithm 1 to the map, and the results are shown in Fig. 4A. A comparison with the ground truth labels in Fig. 4B reveals that most of the particles were correctly captured by the framework, despite some highly overlapping particles. Supplementary Fig. 23 further presents the 3D reconstruction results and the FSC values obtained from different center finding algorithms. Our model achieved a resolution of 3.94 \u00c5, as depicted in Fig. 4C, demonstrating performance comparable to that of the curated dataset in CryoPPP (FSC value of 3.95 \u00c5). Furthermore, we compared the performance of CRISP with other state-of-the-art approaches, and the results are shown in Table 3. Our method not only achieves the highest resolution but also harvests a reasonable number of particles. Finally, we further pushed the resolution limit by performing non-uniform refinement on both CRISP and the CryoPPP dataset, with the results available in Supplementary Fig. 24. Our method achieved a resolution of 3.65 \u00c5, which is close to the published result of 3.5 \u00c5, despite using only 300 micrographs out of the entire 997 available."}, {"title": "4. Discussion and Conclusion", "content": "Distinguishing structural signals from noisy micrographs is critical for studying protein function using cryo-EM. In this study, we developed a framework to perform image segmentation on micrographs with pixel-level accuracy. To address the laborious task of manual labeling, we created a process capable of generating segmentation maps from existing databases such as EMPIAR or CryoPPP, thereby establishing ground truth labels for training segmentation models on both synthetic and real datasets. Moreover, our framework incorporates a modular segmentation pipeline, allowing users to choose among various built-in networks, loss functions, and metrics for model training. Our experimental results suggest that, by utilizing the segmentation pipeline and the generated labels, a highly accurate segmentation model for cryo-EM micrographs can be constructed. In addition, to further refine the segmentation results, spatial regularization techniques leveraging CRF have been incorporated. Notably, a novel CRF has been proposed to improve performance based on the characteristics of cryo-EM data.\nFor downstream analysis, we demonstrate that the segmentation results can be directly used for particle picking by implementing a center finding algorithm. To this end, we designed a procedure to automatically select the best center finding algorithm based on the validation set. Our experimental results indicate that when using the picked particles for 3D reconstruction, our method achieves performance comparable to expert-curated particles on real datasets. Moreover, our method outperforms other state-of-the-art particle pickers on the tested datasets.\nThere are several topics worth exploring in the future. Firstly, preprocessing methods such as denoising or contrast adjustment may further enhance performance, as described in George et al. (2021); Gyawali et al. (2024). Additionally, employing more powerful models, such as Transformers Dhakal et al. (2024); Zhang et al. (2025), may further improve outcomes. Regarding particle picking, utilizing our framework to pre-train a general model for automatic particle picking is a promising direction, as previous works have demonstrated that deep learning-based particle pickers can learn features that generalize well. Finally, with the predicted segmentation maps, other downstream tasks such as training denoising models by accurately identifying noise regions Li et al. (2022) can be explored. We believe that current computational approaches, including particle pickers, can benefit from our modular framework. To support and stimulate further research, we have consolidated our framework into a modular package, CRISP."}, {"title": "A.1. Background of cryo-EM and particle picking methodologies", "content": "Cryo-electron microscopy (cryo-EM) has emerged as a transformative technology for determining the three-dimensional structures of biological macromolecules at near-atomic resolution without the need for crystallization. In a typical cryo-EM experiment, biological samples are applied to grids often coated with a thin carbon film for support and rapidly vitrified to preserve their native state. The samples are then imaged at cryogenic temperatures under low electron doses. While this approach minimizes radiation damage and preserves protein structural information, it also produces micrographs with a very low signal-to-noise ratio (SNR) and low contrast, causing the structural details to be obscured by noise. Moreover, the micrographs contain high-contrast carbon edges resulting from the support film and ice contamination introduced during sample preparation, which can be confusing and may lead to false positives during particle picking. In some experiments, samples are used at a relatively high concentration, increasing the likelihood that particles will be densely packed or even overlap within the ice layer, especially when the ice is excessively thick. Finally, to achieve higher resolution and improve throughput, the micrographs are typically large images, resulting in high-dimensional data. All these factors make the analysis of micrographs particularly challenging, as illustrated in Supplementary Fig. 1.\nConsequently, a critical early step in the cryo-EM pipeline is particle picking the automated or semi-automated identification and extraction of individual protein particles from noisy micrographs. High-resolution three-dimensional reconstructions require the alignment and averaging of hundreds of thousands of high-quality particle images; therefore, errors during particle picking (such as missing true particles or erroneously including contaminants like ice patches, carbon edges, or malformed particles) can directly limit the accuracy of downstream reconstructions.\nEarly approaches to particle picking were predominantly based on template matching. In these methods, researchers manually selected a subset of particles from a few micrographs and generated one or more reference templates (often by averaging similar particles). These templates were then used to guide automated picking by scanning micrographs for regions that match the reference features, as implemented in packages such as XMIPP Sorzano et al. (2004), \u0395\u039c\u0391\u039d2 Tang et al. (2007), APPION Lander et al. (2009), and RELION Scheres (2015). Although template-based methods have been successfully applied for decades, their performance is highly sensitive to the inherently low SNR of cryo-EM data. Manually chosen templates may not capture the full variability in particle shape, orientation, or contrast, and can be confounded by contaminants, leading to high rates of false positives and false negatives. As a result, substantial manual post-processing and correction are often required.\nIn contrast, template-free particle picking methods Punjani et al. (2017); Scheres (2012); Heimowitz et al. (2018); Voss et al. (2009) are designed to identify protein particles in cryo-EM micrographs without relying on predefined templates or manually selected examples. Instead of using reference images, these approaches leverage unsupervised computer vision techniques such as edge detection, blob detection (e.g., using Laplacian of Gaussian filters), and local statistical measures to automatically highlight regions that exhibit features characteristic of protein particles. By directly processing the raw image data, template-free methods aim to detect subtle intensity variations and local maxima that differentiate particle regions from background noise. Although these methods eliminate the potential bias introduced by manually chosen templates and can be computationally efficient, they are still challenged by the inherently low SNR of cryo-EM images, typically requiring additional post-processing or integration with other strategies to filter out false positives.\nTo overcome the limitations, researchers have turned to more data-driven approaches based on machine learning (ML). Early ML-based methods typically involve extracting specific image features (such as contrast, texture, or intensity statistics) from manually labeled particle regions. These features are then used to train classifiers such as support vector machines to distinguish particles from non-particles Sorzano et al. (2004). Although these approaches offer a degree of automation compared to purely template-based methods, they remain constrained by the quality and representativeness of the hand-picked training sets. Furthermore, conventional ML classifiers often struggle to generalize across datasets with varying noise characteristics and heterogeneous particle populations, highlighting the need for improved robustness and accuracy.\nThe advent of deep learning has revolutionized cryo-EM particle picking by enabling models to learn complex, hierarchical features directly from raw or minimally preprocessed data. Deep learning approaches can be broadly divided into two main categories: object detection-based methods and image segmentation-based methods. In the object detection-based approach, convolutional neural network (CNN) architectures have been widely adopted. Tools such as CrYOLO Wagner et al. (2019) and Topaz Bepler et al. (2020) exemplify this approach. CrYOLO adapts the popular \"You Only Look Once\" (YOLO) framework to cryo-EM images by dividing each micrograph into a grid and predicting bounding boxes for particles within each cell. Topaz, on the other hand, employs a positive-unlabeled learning paradigm, wherein a small number of annotated particles are used in conjunction with large amounts of unlabeled data. Both methods have advanced the field considerably by reducing manual intervention; however, their effectiveness is still limited by the need for extensive labeled datasets and by challenges inherent in modeling the heterogeneous appearances of particles. Recent works also leverage powerful architectures, such as Transformers, to improve the discrimination of true particles from contaminants, as demonstrated in CryoTransformer Dhakal et al. (2024) and UPikcer Zhang et al. (2025). Nevertheless, object detection-based approaches have certain limitations: they may produce off-center particle predictions, necessitating extensive translational searches for alignment in downstream analyses, and they are not easily adaptable for identifying irregularly shaped objects such as ice, carbon, filaments, or membranes. Moreover, they do not provide pixel-level precision in differentiating signal from background, which limits their application in certain scenarios Li et al. (2022).\nOn the other hand, recent research has explored pixel-level segmentation approaches, where the particle picking task is reformulated as an image segmentation problem. Segmentation-based methods aim to assign a class label (particle or background) to each pixel in the micrograph, thereby delineating the precise boundaries of particles. One approach follows an encoder-decoder framework exemplified by U-Net (as depicted in Supplementary Fig. 7), which is used in previous works such as Urdnet Ouyang et al. (2022) and CryoSegNet Gyawali et al. (2024). In this framework, a contracting encoder progressively extracts contextual features while a symmetric expanding decoder employs learned upsampling along with skip connections to recover spatial details, yielding highly precise, pixel-level segmentation maps. In contrast, other popular architectures employ Fully Convolutional Networks (FCNs) and DeepLab (as shown in Supplementary Fig. 8), which use an encoder to extract robust feature representations from the input image and then rely on simple interpolation techniques or transposed convolutions to resize the feature maps back to the original resolution. This latter strategy emphasizes computational efficiency and leverages fixed upsampling schemes to reconstruct segmentation maps, as seen in PIXER Zhang et al. (2019), PARSED Yao et al. (2020), and CASSPER George et al. (2021). Compared with object detection-based approaches, segmentation methods learn at the pixel level rather than relying solely on particle shapes, which may yield high accuracy even on unseen micrographs George et al. (2021)."}, {"title": "However, the image segmentation approach has some shortcomings.", "content": "Firstly, it is challenging to create the accurate pixel-level annotations required for training. Secondly, the custom codebases often used for segmentation frameworks are typically not user-friendly and are difficult to modify or replace particularly in terms of architecture and loss function making it hard to incorporate new advances in deep learning. Finally, a significant challenge in segmentation is the lack of spatial regularization, which often leads to isolated regions or irregular boundaries, issues that are further exacerbated by the high noise levels and outliers in cryo-EM images. These challenges increase the barrier for researchers interested in adopting these approaches, despite their potential for more accurate results and broader downstream applications beyond particle picking. To address these issues, we introduce the CRISP framework in this research.\nThe development of the aforementioned supervised learning methods for particle picking in cryo-EM requires manually labeled particle datasets. The largest dataset in the cryo-EM field is the Electron Microscopy Public Image Archive (EMPIAR) Iudin et al. (2023); however, only a small fraction of these datasets include particle labels provided by the original authors. The recently released CryoPPP Dhakal et al. (2023) dataset was curated from the EMPIAR repository to offer a large and diverse collection of manually labeled cryo-EM micrographs. CryoPPP encompasses 34 representative protein datasets that cover a broad spectrum of protein sizes (ranging from approximately 77.14 kDa to 2198.78 kDa), shapes, and conformations. The collection includes micrographs that not only display ideal conditions where particles are easily discernible but also those featuring more challenging scenarios, such as low particle concentrations, clustered proteins, and heterogeneous views (top, side, and inclined) combined with varying defocus values and non-uniform ice distributions. Therefore, rather than using simple test datasets like Apoferritin and KLH, in this work we employ the CryoPPP dataset to evaluate our methods. Finally, since EMPIAR and CryoPPP only provide particle coordinates and radii (as shown in Supplementary Fig. 2), we also propose a label generation process that automatically converts this information into segmentation maps for training, as described in the main text."}, {"title": "A.2. Mathmatical details of CRF", "content": "Conditional Random Fields (CRFs) provide a rigorous probabilistic framework for structured prediction", "distribution": "n$P(X | I) = \\frac{1"}, {"potentials": "n$E(x | I) = \\sum_{i=1"}, {"title": "Implementation of center finding algorithms", "content": "We have implemented three distinct center finding algorithms in our framework. In addition, we identify two important hyperparameters for tuning in each algorithm. The first hyperparameter, $e$, controls the number of candidates in the initial search stage; a smaller value of $e$ results in more candidates being discovered. The second hyperparameter, $s$, governs the filtering step such that larger values of $s$ result in more particles being filtered out.\nAlgorithm 1: Traditional Computer Vision Methods:\nIn the first algorithm, each image undergoes normalization and erosion to enhance contrast consistency and reduce noise. Normalization scales pixel intensities, followed by a two-step erosion process: first, a small kernel (of size $\\frac{radius \\quad of \\quad expected \\quad particle}{4}$) is applied for noise reduction, then a second erosion is performed using a kernel of size $\\frac{radius \\quad of \\quad expected \\quad particle}{e}$ (with $e \\in [2, 4, 6]$) to refine object boundaries and improve separation from the background. Contour detection are then performed to extract object edges. Contours are filtered by area constraints to retain only those within the range:\n$\\frac{radius \\quad of \\quad expected \\quad particle}{s} < area < 500,000$,\nwhere $s \\in [0.6, 1.0, 1.4]$, ensuring the removal of irrelevant small or large regions. Next, center estimation is conducted by computing minimal enclosing circles or rectangles for each valid contour. Only enclosing circles or rectangles that satisfy predefined geometric constraints are retained, reducing the likelihood of incorrect detections.\nAlgorithm 2: Crocker-Grier Algorithm:\nThe second algorithm is based on the Crocker-Grier method Crocker and Grier (1996), a classic and well-established approach for detecting and localizing blobs with pixel-level precision. This algorithm searches for local intensity maxima that serve as initial candidate locations for particles. By scanning the image with a window often related to the expected particle size the algorithm identifies pixels that are brighter than their neighbors, treating these pixels as the \"seeds\" or candidate blob centers. To reduce false positives (e.g., noise peaks), an intensity threshold is applied so that only peaks above a certain brightness are retained. Once candidate blobs are identified, the algorithm refines their centers by computing the intensity-weighted centroid within a small region around each candidate maximum. In practice, the x-coordinate of the refined center is calculated as\n$X_{center} = \\frac{\\sum_{i \\in blob} X_i I(x_i, y_i)}{\\sum_{i \\in blob} I(x_i, y_i)}$\nwith a similar computation for the y-coordinate and $I(x_i, y_i)$ is the intensity value at coordinate (x, y). This \"center of mass\" approach leverages the full brightness profile of the particle. In addition, when using an expected particle size as the detection field, the algorithm calculates the center coordinates of segmented blobs and their weighted signal sums (defined as blob mass). If two or more blobs have center-to-center distances less than diameter of expected particle $\\frac{particle\\times s}{e}$, where $s \\in [0.4,0.7, 1.0]$, the blob with the smaller mass is eliminated to ensure effective picking. Finally, in our implementation, the input image is resized by a factor $e$ (with $e \\in [0.15, 0.25, 0.35]$) to accelerate computation and potentially collect more candidate particles."}, {"title": "Algorithm 3: Non-Maximum Suppression (NMS):", "content": "The third algorithm is based on Non-Maximum Suppression (NMS) as described in George et al. (2021). First", "as": "n$score(x, y) = \\sum_{i=-\\frac{w}{2}}^{\\frac{w}{2}} \\sum_{j=-\\frac{h}{2}}^{\\frac{h}{2}} W(i, j) P(x + i, y + j)$,\nwhere P(x, y) is the probability value at pixel (x, y) and W(i, j) is a Gaussian kernel of size w\u00d7h that assigns greater influence to central pixels. One benefit of using W(i, j) is that, when particles are in close proximity, the interference from neighboring particles is reduced, allowing for more precise localization. To avoid selecting overlapped particles, the micrograph is divided into small grids, and only the candidate with the maximum score from each grid is retained."}]