{"title": "Integrating Supertag Features into Neural Discontinuous Constituent Parsing", "authors": ["Lukas Michael Mielczarek"], "abstract": "Syntactic parsing lies at the foundation of many natural-language processing applications. The term denotes the automatic extraction of a syntactic representation from a natural language text, usually in the form of a sequence of words and punctuation marks. The representation is designed to capture information about the elements' grammatical relations and functions. This output is traditionally used as a basis for higher-level tasks like grammar checking, semantic analysis, question answering and information extraction.  One widely used description of syntax is so-called constituent structure. Rooted in X-bar theory, the idea of constituency arises from the observation that groups of words can behave as single units.  Despite having completely different meanings, these three terms can occur in similar syntactic environments.  However, this is not true for all of the individual components of the phrases, as can be seen in. Therefore, one assumes the existence of an abstract category called a constituent to which one or more words can belong and for which rules like \"noun phrases can be followed by verbs\" can be postulated.  Traditional views of constituency demand that constituents consist of adjacent words. This is rooted in a system for modelling languages called context-free grammar (CFG), a generative device that derives sentences by using rewrite rules, starting with an initial symbol. The derivation path marks a hierarchy of constituents that can be arranged into a derivation/parse tree. Note that the tree only contains non-crossing edges which is why it is called projective or continuous.  This view of constituency was adopted by many treebanks. Treebanks are linguistic corpora in which the sentences have been annotated with syntactic representations (e.g. constituency trees). However, this leads to difficulties when analysing syntactic phenomena that are believed to exhibit non-local dependencies. While the analysis of German or other languages with varying degrees of free word order evidently questions the projective approach, the English language also exhibits a fair amount of discontinuous phenomena, see and.   Some treebanks like the English Penn Treebank opted to mark discontinuous phenomena through indexing and trace nodes, i.e. empty nodes in the place where a discontinuous subsection of the sentence would be expected in \"standard\" word order. This is motivated by the tradition of analysing discontinuities as instances of transformation or syntactic movement as introduced by Chomsky. The tree in figure 1.2 features three cases of co-indexing. While such a strategy acknowledges the existence of discontinuous relationships, the long-range dependencies of the markers cannot be fully captured by traditional parsing techniques for projective grammars and often remain unused.  In a number of treebanks like the German NeGra and TIGER treebanks or English discontinuous Penn Treebank (DPTB) long-range dependencies are represented by crossing edges and constituents with non-adjacent elements. The resulting trees are called discontinuous constituent trees. Figure 1.3 shows an example from the NeGra corpus.   In order to capture syntactic descriptions with discontinuous constituents, several formalisms have been proposed. Among these linear context-free rewriting systems (LCFRS) have been shown to be a natural candidate for accurately covering the notion of discontinuous constituents employed by the aforementioned treebanks.  Being a foundation for many natural language processing tasks, parsing speed is a key concern and a driver of active research. Parsing LCFRS is non-trivial in terms of efficiency. The adaptation of traditional chart parsing for LCFRS results in a polynomial time complexity of $O(n^{3\\cdot dim(G)})$ where n is the length of a sentence and $dim(G)$ is the maximal number of discontinuous blocks of a constituent in the grammar. Transition-based parsing approaches aim at reducing this factor by doing away with the need for an explicit grammar. Instead, an artificial neural network is trained to produce discontinuous constituent trees given only raw text input using supervised learning on large annotated corpora. Errors in prediction are backpropagated through the network resulting in optimisations of its parameters. In this way, it implicitly learns to identify regularities that it can effectively use to predict trees for unseen data. A recent and elegant proposal for a neural stack-free transition-based parser developed by Coavoux and Cohen successfully allows for the derivation of any discontinuous constituent tree over a sentence in worst-case quadratic time.   The purpose of this work is to explore the possibility of enhancing the accuracy of neural grammar-less discontinuous constituent parsing by introducing supertag information. In so-called lexicalised grammar formalisms like lexicalised tree adjoining grammars (LTAG) and combinatory categorial grammar (CCG) informative categories are assigned to words in a sentence that act as the sole building blocks in the composition of the sentence's overarching syntactic representation. These categories are called supertags. The set of rules for connecting these building blocks is kept minimal. Therefore, a supertag assignment gives strong information about the structural role of the word in the sentence and about its syntactic relationship with the surrounding items.  Figure 1.4 shows a possible supertag assignment for the word took in LTAG. The incomplete tree serves as a primitive structure in the formalism that directly dictates the position and number of the verb's nominal argument phrases (NP). Given this assignment, a parser would not need to reconstruct the higher-level constituent structure any more. Instead, it would simply need to link a group of elementary trees.", "sections": [{"title": "1. Introduction", "content": "Syntactic parsing lies at the foundation of many natural-language processing applications. The term denotes the automatic extraction of a syntactic representation from a natural language text, usually in the form of a sequence of words and punctuation marks. The representation is designed to capture information about the elements' grammatical relations and functions. This output is traditionally used as a basis for higher-level tasks like grammar checking, semantic analysis, question answering and information extraction. One widely used description of syntax is so-called constituent structure. Rooted in X-bar theory (Chomsky, 1970), the idea of constituency arises from the observation that groups of words can behave as single units (Jurafsky and Martin, 2009, chapter 12). Take for instance the noun phrases in (1.1).\na. the memory\nb. a new computer\nc. the person she really likes\nDespite having completely different meanings, these three terms can occur in similar syntactic environments, for example before a verb as in (1.2).\na. the memory fades\nb. a new computer arrives...\nc. the person she really likes waits...\nHowever, this is not true for all of the individual components of the phrases, as can be seen in (1.3). Therefore, one assumes the existence of an abstract category called a constituent to which one or more words can belong and for which rules like \"noun phrases can be followed by verbs\" can be postulated (Jurafsky and Martin, 2009, chapter 12).\na. *the fades\nb. *new arrives...\nc. *likes spends...\nNote: The asterisk marks constructions that are not grammatical.\nTraditional views of constituency demand that constituents consist of adjacent words. This is rooted in a system for modelling languages called context-free grammar (CFG) (Chomsky, 1956), a generative device that derives sentences by using rewrite rules, starting with an initial symbol. The derivation path marks a hierarchy of constituents that can be arranged into a derivation/parse tree. Figure 1.1 gives an example for such a description. Note that the tree only contains non-crossing edges which is why it is called projective or continuous.\nThis view of constituency was adopted by many treebanks. Treebanks are linguistic corpora in which the sentences have been annotated with syntactic representations (e.g. constituency trees) (Jurafsky and Martin, 2009, chapter 12). However, this leads to difficulties when analysing syntactic phenomena that are believed to exhibit non-local dependencies. While the analysis of German or other languages with varying degrees of free word order evidently questions the projective approach, the English language also exhibits a fair amount of discontinuous phenomena, see (1.4) and (1.5).\nsie packte ihre Werkzeuge ein\nshe packed her tools up\n'she packed her tools'\nb. den Dank mochte er nicht annehmen\nthe thanks want he not accept\n'he does not want to accept the thanks'\nohne Scham scheinen sie das komplette Buffet aufgegessen zu haben\nwithout shame seem they the complete buffet eaten to have\n'they seem to have eaten the entire buffet without shame'\na. areas of the factory were particularly dusty where the crocidolite was used (Evang, 2011)\nb. a man entered who was wearing a black suit (McCawley, 1982)\nSome treebanks like the English Penn Treebank (Marcus et al., 1993) opted to mark discontinuous phenomena through indexing and trace nodes, i.e. empty nodes in the place where a discontinuous subsection of the sentence would be expected in \"standard\" word order. This is motivated by the tradition of analysing discontinuities as instances of transformation or syntactic movement as introduced by Chomsky (1975). The tree in figure 1.2 features three cases of co-indexing. While such a strategy acknowledges the existence of discontinuous relationships, the long-range dependencies of the markers cannot be fully captured by traditional parsing techniques for projective grammars and often remain unused.\nIn a number of treebanks like the German NeGra (Skut et al., 1998) and TIGER treebanks (Brants et al., 2004) or English discontinuous Penn Treebank (DPTB) (Evang and Kallmeyer, 2011)"}, {"title": "1.1. Motivation", "content": "long-range dependencies are represented by crossing edges and constituents with non-adjacent elements. The resulting trees are called discontinuous constituent trees. Figure 1.3 shows an example from the NeGra corpus.\nIn order to capture syntactic descriptions with discontinuous constituents, several formalisms have been proposed. Among these linear context-free rewriting systems (LCFRS) (Vijay-Shanker et al., 1987) have been shown to be a natural candidate for accurately covering the notion of discontinuous constituents employed by the aforementioned treebanks.\nBeing a foundation for many natural language processing tasks, parsing speed is a key concern and a driver of active research. Parsing LCFRS is non-trivial in terms of efficiency. The adaptation of traditional chart parsing for LCFRS results in a polynomial time complexity of $O(n^{3\\cdot dim(G)})$ where $n$ is the length of a sentence and $dim(G)$ is the maximal number of discontinuous blocks of a constituent in the grammar. Transition-based parsing approaches aim at reducing this factor by doing away with the need for an explicit grammar. Instead, an artificial neural network is trained to produce discontinuous constituent trees given only raw text input using supervised learning on large annotated corpora. Errors in prediction are backpropagated through the network resulting in optimisations of its parameters. In this way, it implicitly learns to identify regularities that it can effectively use to predict trees for unseen data. A recent and elegant proposal for a neural stack-free transition-based parser developed by Coavoux and Cohen (2019) successfully allows for the derivation of any discontinuous constituent tree over a sentence in worst-case quadratic time."}, {"title": "1.2. Thesis Aims", "content": "The purpose of this work is to explore the possibility of enhancing the accuracy of neural grammar-less discontinuous constituent parsing by introducing supertag information. In so-called lexicalised grammar formalisms like lexicalised tree adjoining grammars (LTAG) (Schabes and Joshi, 1991) and combinatory categorial grammar (CCG) (Steedman, 1989, 1996, 2000) informative categories are assigned to words in a sentence that act as the sole building blocks in the composition of the sentence's overarching syntactic representation. These categories are called supertags. The set of rules for connecting these building blocks is kept minimal. Therefore, a supertag assignment gives strong information about the structural role of the word in the sentence and about its syntactic relationship with the surrounding items.\nFigure 1.4 shows a possible supertag assignment for the word took in LTAG. The incomplete tree serves as a primitive structure in the formalism that directly dictates the position and number of the verb's nominal argument phrases (NP). Given this assignment, a parser would not need to reconstruct the higher-level constituent structure any more. Instead, it would simply need to link a group of elementary trees."}, {"title": "1.3. Related Work", "content": "The potential of assigning supertags to word sequences using statistical methods as a pre-step to parsing was initially successfully explored by Bangalore and Joshi (1999) for lexicalised tree adjoining grammar (LTAG) and adapted for combinatory categorial grammar (CCG) by Clark and Curran (2010). Recently, Ruprecht and M\u00f6rbitz (2021); Ivliev (2020) have presented the first extraction algorithm for supertags based on linear context-free rewriting systems (LCFRS) and achieved state-of-the-art results in discontinuous constituent parsing.\nThese approaches differ from this work in that they use supertagging as an upstream task for grammar-based parsers of the respective formalism, for instance LCFRS chart-parsing (Ruprecht and M\u00f6rbitz, 2021). The extracted supertags or supertag distributions are directly utilised to reduce ambiguity and increase parsing speed by ruling out or prioritizing certain pre-terminal rule applications which effectively prunes the derivational search space. In contrast to that, I investigate the effect of supertagging in the context of a neural, grammar-less parsing approach as an input feature and as an auxiliary task. From this also follows that my choice of supertags is not structurally restricted to a certain formalism but rather to statistical correlations between supertag assignments and parsing actions.\nThe first thorough exploration of auxiliary/multi-task learning for natural language processing was performed by Collobert and Weston (2008) who simultaneously predict part-of-speech tags, chunk labels, named entity tags, semantic roles, semantic similarity and the likelihood of a sentence's well-formedness from jointly trained word embeddings. Investigations into the realm of multi-task learning that followed are mainly centred around dependency parsing and in many cases deal with incorporating higher-level semantic tasks.\nCandito (2022) extracts auxiliary tasks from semantic dependency graphs like the number of dependents of a word and a concatenation of incoming arc labels to increase the accuracy of a dependency parser by training the model to predict these features from a shared bidirectional recurrent neural network. Zhou et al. (2020a) explore jointly learning dependency and span-based constituent parsing together with semantic parsing. This approach is extended by Zhou et al. (2020b) who propose improving the quality of language models like BERT (Devlin et al., 2019) with informed linguistic knowledge by jointly training on the aforementioned tasks.\nThese approaches differ from this work's objective through the choice of task(s) they incorporate. They are centred around bridging the gap between syntactic and semantic representations while I try to deal with the open problem of discontinuous constituency by searching for a syntactical localised lower-level task that is informative of these phenomena.\nThe research on multi-task approaches pertaining to supertagging is fairly limited. Yoshikawa et al. (2017) extract dependency graphs from CCG derivations and train a neural model to predict both dependency structure and CCG supertags from a shared representation to resolve derivational ambiguity in the CCG formalism. But the actual CCG parsing is performed using an A* algorithm based on the supertag probabilities predicted by the model. Zhu and Sarkar (2019) deconstruct LTAG supertags into several components like head, root or spine effectively bootstrapping new tasks from an existing supertag annotated corpus. They use these tasks to improve LTAG supertagging performance. S\u00f8gaard and Goldberg (2016) show that combining CCG supertagging and POS tagging using a hierarchical approach where the easier POS tagging task is predicted from a lower level yields improvements in supertagging accuracy. This work borrows from their idea of a hierarchical auxiliary task arrangement. Bingel and S\u00f8gaard (2017) explore binary multi-task relations between a range of natural language processing tasks including CCG tagging, POS tagging, chunking and semantic tasks. The effect of supertag prediction as an auxiliary task for parsing has not been examined to this date.\nThe work on beneficial auxiliary tasks for neural constituent parsing is also sparse. Coavoux and Crabb\u00e9 (2017b) predict several word tagging tasks from a shared representation of a model for projective constituent parsing: POS tags, morphological features and functional label, i.e. the relation of a word to its head in a constituent. To the best of my knowledge, besides jointly predicted POS tags (Coavoux and Cohen, 2019), the only exploration of multi-task frameworks for grammar-less discontinuous constituent parsers to this date has been recently brought forward by Johansson and Adesam (2020) who train constituent parsing on a Swedish discontinuous treebank with a similar annotation scheme to NeGra/TIGER. They pursue parsing based on differently annotated corpora, including dependency treebanks, as an auxiliary task with predictions from a shared intermediate representation to leverage the limited availability of linguistic resources in Swedish. This differs from the work at hand in that the objectives of Johansson and Adesam (2020) are all parsing tasks predicted at the same model level while I explore the implementation of a lower-level task. Furthermore, Johansson and Adesam (2020) use a different transition system for discontinuous constituent parsing.\nTo the best of my knowledge, the work at hand is the first exploration of supertagging in a pipeline arrangement and as an auxiliary task for grammar-less neural constituent parsers. It might therefore help to identify if a shared neural representation benefits parsing and specifically the resolvement of challenging discontinuous phenomena and if further research in this direction is a worthwile endeavour."}, {"title": "1.4. Structure", "content": "The following sections are structured as follows. Section 2 gives a detailed introduction to linear context-free rewriting systems (LCFRS), a natural extension of context-free grammars that accounts for discontinuous constituents. Section 3 presents two discontinuous parsing approaches: LCFRS CYK chart-parsing and grammar-less transition-based parsers. Then, in section 4 follows a discussion of the stack-free transition system proposed by Coavoux and Cohen (2019) including its implementation. Section 5 explains the formalism of combinatory categorial grammar (CCG), a major lexicalised grammar theory that gives rise to informative supertags, and examines its account of discontinuous phenomena that can be found in the discontinuous Penn Treebank (DPTB). Finally, section 6 describes the implementations and results of several experiments for the integration of CCG supertags into the stack-free transition-based neural parser. Section 7 summarises the findings and discusses them in a broader context."}, {"title": "2. Transgressing Context-Freeness", "content": "Discontinuous constituent trees can be seen as structural descriptions produced by derivations of linear context-free rewriting systems (LCFRS). They were introduced by Vijay-Shanker et al. (1987) and are an extension of traditional context-free grammars (CFG), which do not suffice to adequately treat discontinuous structures. This section shall be concerned with defining both CFGs and LCFRSs as well as with the derivation trees that accompany them."}, {"title": "2.1. Prerequisites", "content": "The formalism of context-free grammars (CFG), developed by Chomsky (1956), has a long history as the foundation for modelling the syntactic structure of natural languages. In order to define CFGs, several prerequisites are needed. I follow the definitions in Kallmeyer (2010).\nLet X be a non-empty finite set of symbols.\n1. X is called an alphabet.\n2. A string $x_1x_2....x_n$ with n $\\in$ N and $x_i \\in X$ for all i $\\in$ {1, ..., n} is called a non-empty word over X. $X^+$ is defined as the set of all non-empty words over X.\n3. I define $X^* := X^+ \\cup \\{\\epsilon\\}$ for a new element $\\epsilon \\notin X^+$ called empty word. $\\epsilon$ is defined as the neutral element of concatenation on $X^*$, i.e. for $w \\in X^* : w\\epsilon = \\epsilon w = w$. $w \\in X^*$ is called a word over X.\n4. A set L is called a language over X iff $L \\subseteq X^*$.\nTwo explanatory notes on the preceding definition: firstly, for $i, j \\in \\mathbb{Z}$ the expression {i, ..., j} is used as a shorthand notation for $\\{n \\in \\mathbb{Z} | i \\leq n \\leq j\\}$. Secondly, in the context of natural language an alphabet X can be a set of natural language words treated as unique symbols. For clarity, the term word will only refer to the formal definition given above in this work unless stated otherwise."}, {"title": "2.2. Insufficiency of Context-Free Grammars", "content": "Since the early 1980s, many researchers argued that CFGs are inadequate for describing natural language. Many of these arguments, however, were based on the strong generative capacity of grammars, e.g. Bresnan et al. (1982), who analysed non-local dependencies in Dutch. They relied on assumptions about the underlying structure and were therefore susceptible to critique. Primarily, this style of argument does not rule out the possibility that a different context-free grammar formalism (with different structural descriptions) might be able to generate the language in question (Kallmeyer, 2010, chapter 2).\nThe matter was finally resolved when Shieber (1985) showed that nesting relative clauses with crossing dependencies in Swiss German also surpass the weak generative capacity of CFGs. These findings led to the search of appropriate grammatical formalisms strong enough to generate long-distance relationships and at the same time restricted enough to allow for efficient parsing. In this context, Joshi (1985) coined the term mild context-sensitivity for a category of formalisms that possess certain desirable properties. Kallmeyer (2010, chapter 2) give the following definition for mild context-sensitivity:"}, {"title": "2.3. Defining LCFRSs", "content": "Linear context-free rewriting systems (LCFRS) are a powerful type of mildly context-sensitive grammar formalism that extends the rewriting-nature of CFGs. Like context-free grammars, the formalism possesses rewriting rules with left-hand sides (LHS) and right-hand sides (RHS). Words are derived through recursive application of the rules which in turn constitute the constituent structure desirable from a linguist's point of view.\nThe key difference is the fact that nonterminals do not yield a string of terminals, but a tuple of strings. The strings in one tuple may be intertwined with elements of other tuples. This allows an element to have a yield of non-adjacent strings. The size of the tuples is fixed for each nonterminal A and is called its fan-out. Thus, the fan-out of A gives the maximum number of (discontinuous) components A dominates. The fan-out of the start symbol is set to 1. Figure 2.2 shows a tree where the nonterminal A yields three non-adjacent components 1, 2, 3."}, {"title": "2.4. From Treebanks to LCFRSS", "content": "As shown in the last section, LCFRS derivation trees result in the desired tree representations of non-overlapping discontinuous constituents found in constituency treebanks with discontinuities.\nThese trees can be directly interpreted as LCFRS derivation trees. S\u00f8gaard and Maier (2008) describe an algorithm for extracting probabilistic LCFRSs (PLCFRS) from discontinuous treebanks. Kallmeyer and Maier (2010) give the following definition for PLCFRSs."}, {"title": "3. Parsing LCFRS", "content": "Parsers for LCFRSs can be utilised for producing discontinuous constituent trees from text. In the following, I will outline two main parsing approaches: traditional chart parsing and transition-based parsing. Both approaches are usually data-driven in that they extract their syntactic knowledge from treebank descriptions. In the case of chart-parsers, this extraction is explicit. To build the parser, LCFRS rules are automatically extracted from an annotated corpus. Transition-based parsers allow the implementation of grammar-less parsing. They are usually built on a neural-network model that is directly trained on a treebank."}, {"title": "3.1. CYK Parser", "content": "Traditional Cocke-Younger-Kasami (CYK) parsers were first introduced for CFG by Sakai (1961). Their underlying idea is to assert the applicability of rules in a bottom-up fashion, starting with the terminal symbols and progressively validating derivation steps on the basis of the set of rule applications that has been recovered already. Seki et al. (1991) extended the algorithm to MCFGs. It was adapted for probabilistic LCFRS by Kallmeyer and Maier (2010). I will discuss the latter approach in more detail."}, {"title": "3.1.1 Prerequisites", "content": "The input of the CYK PLCFRS parser consists of a grammar and an input word. The parser requires a PLCFRS (N, T, V, P, S, p) to have the following properties:\n1. binary: For every rule $r \\in P : rank(r) \\leq 2$ (G\u00f3mez-Rodr\u00edguez et al., 2009).\n2. terminal-restricted: Terminals occur only in terminating rules of the form $A(a) \\rightarrow \\epsilon$ where a is a terminal. The LHS of all other rules contains only variables (Evang, 2011).\n3. gap-explicit: Two different variables in a RHS element of a rule cannot appear next to each other in the same component on the LHS side. This means that a separation of two variables always entails that there is a gap between them (Kallmeyer and Maier, 2010).\n4. ordered: The order of the variables within each RHS element is the same as their order of occurrence in the LHS (Villemonte de la Clergerie, 2002).\n5. $\\epsilon$-free: There exists no rule with some empty LHS argument or there exists exactly one $\\epsilon$-rule: $S(\\epsilon) \\rightarrow \\epsilon$ and S does not occur in the RHS of any rule (Boullier, 1998a).\nThe algorithm in S\u00f8gaard and Maier (2008) produces PLCFRSs that fulfil properties 2, 3, 4 and 5. Note that for every LCFRS there is an equivalent LCFRS (i.e. generating the same language) that is $\\epsilon$-free (Boullier, 1998a), ordered (Kallmeyer, 2010, chapter 7), binary (G\u00f3mez-Rodr\u00edguez et al., 2009) and terminal-restricted (trivial).\nFurthermore, for every LCFRS there exists an equivalent LCFRS that is gap-explicit. An algorithm for converting an ordered LCFRS into this form is given below. Thus, the properties above can all be assumed without loss of generality in the following sections."}, {"title": "3.1.2 Deduction Rules", "content": "Parsing is formulated as a set of deduction rules which facilitates proofs of correctness and the examination of complexity. Deduction rules describe how to infer new elements from existing ones."}, {"title": "3.1.3 Chart Parsing", "content": "Natural language grammars are highly ambiguous. For a given input word w, a parsing schema may give rise to a variety of items spanning different, possibly overlapping, portions of w. There can be several unique sequences of deductions leading to a goal item for w. On the other hand, different analyses can be based on a common sub-analysis for a part of w (Kallmeyer, 2010, chapter 3).\nChart parsing is a framework rooted in dynamic programming that allows to easily store intermediate parsing results and to reuse them at any time for the deduction of new elements (Grune and Jacobs, 2010, chapter 7). For this purpose, a table - called a chart C - is used. For CYK parsing, where an intermediate parsing result [A, i, j] spans a projective substring of w, the table can have three dimensions with the first dimension corresponding to a unique index for each non-terminal. The second and third dimension have length |w| + 1 and correspond to the start and end index of the item's range. [A, i, j] would be represented by a mark in cell (index(A), i, j) of C (Kallmeyer, 2010, chapter 3). The cell (index(S), 0, |w|) corresponds to the goal item.\nFurthermore, a list called an agenda A is used. It consists of the items that are yet to be checked for possible deductions in combination with the asserted items in C. Axioms and new items found by deduction are added to A (Kallmeyer, 2010, chapter 3).\nA recogniser can be converted into a parser by saving references with each entry in C that refer to the items that were used to deduce it. In this way, one can retrieve the parse tree from the goal item. The references are called backpointers (Kallmeyer, 2010, chapter 3).\nIn our context, intermediate parsing results can span more than one consecutive substring and therefore a chart with three dimensions does not suffice. The number of dimensions needed is determined by the fan-out of the grammar G. The maximum number of range borders is given by $2 \\cdot dim(G)$. When representing nonterminals in a separate dimension, the chart dimensionality is $1 + 2 \\cdot dim(G)$. The goal item then is (index(S), 0, |w|, n, n, ..., n, n) where n = |w| + 1 is a special"}, {"title": "3.1.4 Algorithm", "content": "A deductive engine can be used to deduce every valid item from the set of axioms, eventually finding the goal item with the highest weight. This naive approach would be computationally inefficient due to the large number of useless intermediate parsing results generated. Kallmeyer and Maier (2010) give a strategy for fast exploration of the best goal item based on weighted deductive parsing (Nederhof, 2003). It is reproduced in an adapted form in algorithm 3.2.\nIn each step, the best item in the agenda x: I is written to the chart and checked for deducibility of new items by combining with the items present on the chart. This amounts to retrieving all y : I' such that $C \\cup \\{x : I\\} \\vdash y : I'$ since no new elements can be generated purely from items on C according to the logic of the algorithm. All newly generated items y: I' that are not yet present in the agenda or on the chart are put on the agenda. If an item already exists on the agenda but the newly generated one has a lower (i.e. better) weight, the weight gets updated with the lower one.\nBy always retrieving the agenda item with the best score, the algorithm guarantees that the first goal item derived is the best one (Knuth, 1977)."}, {"title": "3.1.5 Soundness and Completeness", "content": "Up to this point, I assumed that the parser indeed does what it claims to do. But generally, parser correctness has to be proven by showing soundness and completeness for the parsing algorithm (Kallmeyer, 2010, chapter 3).\nIn the following, a recogniser algorithm A is generalised as a function on a grammar G and a word w returning true or false. A class of grammars could be, for instance, the set of all CFGs or in this case the set of all ordered, binary, gap-explicit, $\\epsilon$-free, terminal-restricted LCFRSS."}, {"title": "3.1.6 Complexity", "content": "The separation of parsing algorithm and schema allows for a concise examination of complexity. Given a grammar G and an input word w one has to consider the maximum number of possible deduction rule applications (Kallmeyer, 2010, chapter 3). This depends on the most complex rule, in this case: BINARY with $A(\\rho_A) \\rightarrow B(\\rho_B)C(\\rho_C)$."}, {"title": "3.1.7 Related Work", "content": "Several strategies were explored to speed up LCFRS CYK chart parsing. Kallmeyer and Maier (2013) use A* search. A coarse-to-fine approach was presented by Van Cranenburgh (2012) while Angelov and Ljungl\u00f6f (2014) propose a new cost estimation for ranking parser items. Kallmeyer (2010, chapter 7) presents several approaches for optimisations on LCFRS like optimal binarisation and elimination of useless rules. Furthermore she explains the use of filters during parsing that reject items that cannot lead to a goal item. Kallmeyer and Maier (2009) present an incremental Earley parser for SRCG.\nMaier et al. (2012) focus on optimizing the extraction from treebanks by limiting the fan-out of the resulting grammar to 2. They note that in the DPTB, which features constituents with at most three discontinuous blocks, the overwhelming majority of the cases with three blocks is caused by punctuation. They show that by changing punctuation annotation the extraction of a 2-LCFRS with minimal informational loss is possible."}, {"title": "3.2. Transition-Based Parsing", "content": "Parsing with transitions is based on a pseudo-deterministic approach and widely used for dependency parsing. The parser traverses the input from left to right and performs one local action from a set of possible transitions at each step to incrementally build a graph. In the case of constituent parsing, this graph is a tree. While chart parsers handle ambiguities by maintaining multiple analyses in parallel through the use of dynamic programming and narrowing down the search space with a statistical model, transition-based parsers maintain only one (incomplete) analysis of the input sequence at a time. This is called a greedy strategy. They usually utilise a classifier trained on treebank-data to predict the next action when faced with a choice (Nivre, 2008). While this narrows down the search space to the one best scored action at each step, it has been shown that by using neural context-aware classifiers, this approach can yield state-of-the-art results.\nIn the following subsections, I will first provide some necessary prerequisites and define transition-based parsing formally. Then, a short outline of the traditional projective constituent shift-reduce parser is given, followed by an exploration of two extensions of this approach for discontinuous constituent parsing: SWAP and GAP."}, {"title": "3.2.1 Transition Systems", "content": "The following definitions for transition systems and transition sequences are based on Nivre (2008). They were introduced for dependency parsing but are, with small modifications, applicable to constituent parsing."}, {"title": "3.2.2 Comparing Chart Parsing and Transition-Based Parsing", "content": "In this section I will point out several differences between chart parsers and transition-based parsers. In chart parsers the items are intermediate parsing results which represent a partial analysis. One or more items can be used to deduce a new analysis for some part of the input. In a transition-based system the items are configurations that in turn contain all available partial analyses for the input. They can be seen as a data structure that represents the state of analysis at a certain time-step, comparable to a view of chart and agenda. But unlike in chart-parsing, analyses cannot conflict with each other.\nA deduction can be used to alter the inner structure of the item that restricts which elements are available for derivation (e.g. shift an element from the buffer to the stack) or to derive a new partial analysis from certain elements in the item. When deriving a new partial analysis from elements using a transition, they are \"used up\" and no longer available for future derivation steps. Thus, by design there is no derivational ambiguity in the retrieved analysis. This reduces the formalism's complexity. The parsing process works bottom-up and the transition system can be seen as a framework to keep track of active partial analyses which have not been used in a derivation yet.\nWhile both approaches are usually data-driven, the weighted CYK chart parser works with an explicitly formalised grammar that was extracted from a treebank in a preceding step. Weights are"}, {"title": "3.2.3 Standard Shift-Reduce Projective Parsing", "content": "Shift-reduce parsing is"}]}