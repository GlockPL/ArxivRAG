{"title": "DECO: Life-Cycle Management of Enterprise-Grade Chatbots", "authors": ["Yiwen Zhu", "Mathieu Demarne", "Kai Deng", "Wenjing Wang", "Nutan Sahoo", "Hannah Lerner", "Anjali Bhavan", "Divya Vermareddy", "Yunlei Lu", "Swati Bararia", "William Zhang*", "Xia Li", "Katherine Lin", "Miso Cilimdzic", "Subru Krishnan"], "abstract": "Software engineers frequently grapple with the challenge of accessing disparate documentation and telemetry data, including Troubleshooting Guides (TSGs), incident reports, code repositories, and various internal tools developed by multiple stakeholders. While on-call duties are inevitable, incident resolution becomes even more daunting due to the obscurity of legacy sources and the pressures of strict time constraints. To enhance the efficiency of on-call engineers (OCEs) and streamline their daily workflows, we introduced DECO-a comprehensive framework for developing, deploying, and managing enterprise-grade chatbots tailored to improve productivity in engineering routines. This paper details the design and implementation of the DECO framework, emphasizing its innovative NL2SearchQuery functionality and a hierarchical planner. These features support efficient and customized retrieval-augmented-generation (RAG) algorithms that not only extract relevant information from diverse sources but also select the most pertinent toolkits in response to user queries. This enables the addressing of complex technical questions and provides seamless, automated access to internal resources. Additionally, DECO incorporates a robust mechanism for converting unstructured incident logs into user-friendly, structured guides, effectively bridging the documentation gap. Feedback from users underscores DECO's pivotal role in simplifying complex engineering tasks, accelerating incident resolution, and bolstering organizational productivity. Since its launch in September 2023, DECO has demonstrated its effectiveness through extensive engagement, with tens of thousands of interactions from hundreds of active users across multiple organizations within the company.", "sections": [{"title": "INTRODUCTION", "content": "The daily activities of software developers encompass a broad spectrum of tasks that extend beyond mere development activities such as coding, debugging, and testing [18]. Predominantly, engineers allocate the majority of their efforts to ancillary tasks including code review, documentation, and on-call responsibilities [12]. These duties necessitate extensive manual searches across disparate resources-ranging from product documentation and troubleshooting guides to telemetry data, system logs, code repositories, and team reports. Moreover, engineers invest considerable effort into navigating internal tools that are essential for their daily operations during the arduous task of managing such varied information sources. The challenge is compounded when critical institutional knowledge is isolated or hoarded by senior engineers, resulting in significant knowledge gaps when these key individuals are absent or depart from the organization.\nRecent advancements in large language models (LLMs), such as GPT [1], LLAMA [44], and Gemini [9], have emerged as powerful tools in this context. These models offer significant potential to enhance the engineers' efficiency by automating mundane tasks, providing swift access to relevant information, and bridging knowledge gaps. The deployment of such AI tools not only facilitates more efficient workflow but also ensures centralized access to a diverse array of resources, thereby streamlining the decision-making process for software development teams.\nIntroduction to DECO-a framework for developing, deploying, and managing chatbots. In this paper, we introduce DECO, a framework for developing, deploying, and managing copilots chatbots designed to support the continuous evolution of a software development team's knowledge base while simplifying access to various internal tools. Chatbots configured with DECO streamline daily tasks, improve incident management, and provide engineers with a unified access point to a wide range of internal resources and tools. DECO focuses on four key areas: (1) a Generalized Development Platform, enabling contributors to easily create and deploy new chatbots; (2) Skills Integration, allowing teams to add new capabilities that incorporate additional knowledge sources or expose skills for accessing internal tools through a chat interface; (3) Fine-Tuned Retrieval Algorithms, designed for efficiency and accuracy in production environments; and (4) Streamlined End-to-End Deployment and Lifecycle Management, offering a self-hosting model with fully automated deployment pipelines,"}, {"title": "BACKGROUND", "content": "The primary objective of DECO is to provide a framework for setting up chatbots that offer engineers a time-efficient, interactive experience for managing daily tasks and on-call responsibilities. DECO effectively bridges gaps between product telemetry, documentation, on-call duties, code development resources, and team-specific toolkits. However, it is important to note that these chatbots are not intended to fully replace existing automated diagnostic platforms, such as decision-tree-based systems developed for specific services [46, 52]; rather, they are designed to complement them."}, {"title": "Data Sources", "content": "By default, DECO incorporates resources from (1) internal documentation repositories, such as troubleshooting guides; (2) historical incident databases; (3) code repositories; and (4) internal Stack Overflow. Tenants can also add additional skills to integrate more sources as needed.\nDocumentation Site. Microsoft maintains an internal documentation site that serves as a central hub for critical information, including engineering documentation and troubleshooting guides relevant to various products and services.\nIcM. Historical mitigated incidents from the Incident Management (IcM) system document unexpected disruptions or outages of services or products [7]. Microsoft has a sophisticated incident management system designed to support issue identification, team assignment, and reporting to help avert or mitigate future incidents [7]. Incident data, including information such as title, summary, source, and owning team, is meticulously stored in a dedicated database. This database also captures all communications between engineers, including investigation notes. These records serve not only as a comprehensive documentation resource for incident triaging but also provide invaluable guidance for broader engineering practices.\nCode Base. For questions related to code, we process the original code repository to create indexes that facilitate easy retrieval of relevant functions, classes, statements, and other code elements.\nStack Overflow. Similar to IcM databases, Stack Overflow serves as an invaluable resource for Microsoft engineers, offering a wealth of community-driven knowledge and problem-solving techniques. Discussions from these forums are extracted and summarized into reports for future reference."}, {"title": "ARCHITECTURE", "content": "This section provides a comprehensive overview of DECO's architecture and design decisions. DECO comprises 4 primary components: Data Preprocessing, Backend Services, Frontend Services, and Evaluation:\nData Preprocessing: The data preprocessing pipelines are orchestrated as Azure Machine Learning experiments [22], executed at user-defined intervals to account for information updates frequently. They draw inputs from various data sources, such as Git, Wikis, and Kusto databases, to generate summarized documents stored in Azure Blob Storage for subsequent retrieval. Multiple Azure AI Search indexes [20] are created to reference the corresponding document blobs, enabling easy access via API calls from downstream backend modules. \nBackend Services: The backend orchestrates the Planner-Skill chat flow using PromptFlow [32]. PromptFlow simplifies the integration of LLMs, prompts, and Python code into an executable workflow structured as a Directed Acyclic Graph (DAG), coordinating input and output exchanges across various modules (e.g., LLM calls, database connections, and other APIs). Integrated closely with the Azure Machine Learning infrastructure [22], PromptFlow enables straightforward deployment, API and flow monitoring, and greatly reduces maintenance complexity for developers. The flow is deployed as a managed online endpoint [24] with a stateless REST API in Azure Machine Learning, optimized for real-time, high-volume, low-latency inference requests, and equipped with extensive logging and debugging capabilities. Additionally, various retrieval nodes (authored as context-retrieval skills) can be configured alongside other skills to automatically invoke internal toolkits based on the planner's node output. Sections 5 and 6 provide further details on these modules.\nFrontend Services: The frontend manages user authentication and authorization through Azure Active Directory (Azure Intra ID) and handles user interactions. It manages session memory, building the full chat history and sending it to the backend's (stateless) REST API to enable a continuous conversation experience. Session state and extracted memory data are stored in Azure Blob Storage, while Azure Log Analytics automatically stores telemetry data for easy monitoring and review. This data is later used for feedback learning [4] and evaluation (see Section 8). A web application is deployed to interact with the Azure Bot Framework, supporting two main user interfaces: a web-based platform and a Microsoft Teams integration, including group chat and session-sharing functionalities.\nImplementing a stateless REST API in the backend enables the RAG/skill/agent functions to operate independently of session state management and authentication, offering two key advantages:\n\u2022 Increased development agility through simplified implementation due to decoupling, and\n\u2022 Seamless backend updates for the copilot, with the flexibility to use multiple backend endpoints to manage traffic, as session state is handled by the frontend.\nEvaluation: A comprehensive assessment of the entire system is conducted to monitor and evaluate real user interactions as well as individual modules, ensuring a high-quality chat experience. Section 8 discusses this module in more detail."}, {"title": "DATA PREPROCESSING", "content": "In this section, we describe the preprocessing pipeline for three data sources as examples: IcM (historical mitigated incidents), documentation, and code repositories. These pipelines are designed to support context-retrieval skills in the backend, such as get_tsg, get_icm and get_code. Additional pipelines can be created using the provided template, such as the processing pipeline to integrate information from sources like Stack Overflow or internal email exchanges. In the backend, AI search indexes created through these pipelines can be accessed by additional context-retrieval skills. These regularly scheduled asynchronous preprocessing jobs enable the automatic ingestion and indexing of data to account for any updates promptly (e.g., every 6 hours)."}, {"title": "IcM Processor", "content": "Raw information about historical incidents is often scattered across multiple tables in the incident management database. The goal of this module is to extract all relevant details, condense the useful information into a concise and structured format, and create searchable files that can serve as supplementary documentation for the LLM's context in the backend.\nThe Kusto Connector uses secured authentication through managed identity [31] to access the IcM database and extracts free-form records based on user-specified team name(s) and data range from several data tables. The IcM Processor module uses GPT-40 models to generate structured JSON format summaries from free-form text data based on predefined summarization prompts. Typically, we observe each chatbot creates indexes of around 3000 to 30,000 incidents.\nThe IcM Processor also functions as an offline platform to \"append\" additional retrieval fields to documents for the backend module, such as a \"helpfulness\" score reflecting the thoroughness of the documented investigation process, or \"property\" fields for specific lookups based on database IDs (see Section 6.2). Ticket summaries and embeddings are stored in a JSON file within Azure Blob Storage, and the IcM index manager initiates the corresponding indexer [27] and index [23], which reference the blob path. A REST API powered by Azure AI Search is deployed to facilitate efficient searches among these files for context-retrieval skills in the backend, using various search algorithms (see Section 5.2)."}, {"title": "TSG Processor", "content": "For TSGs (or any text files), we utilize the original content directly to generate embeddings, splitting them into smaller chunks as needed. To process the documentation, users specify the relevant Git repositories in configuration files, along with corresponding descriptions. These descriptions are included with the retrieved content to provide additional context to the LLM in the backend. Each entry in the configuration involves cloning the repository (using the same authentication mechanism as the IcM processor), selecting specific folders for processing, chunking the files, and generating embeddings for multiple fields, such as document content, titles, and URLs, to enable multi-field hybrid search (see Section 6). Images are extracted in base64 format. The processed files are stored in Blob Storage, and indexed for efficient retrieval by Azure AI Search (similar to IcM as in Figure 4). Additionally, offline preprocessing can generate advanced searchable fields to minimize retrieval latency for the backend. This step supports advanced search algorithms, such as those outlined in [3, 4], and Reverse-HyDE question embeddings as in [11]."}, {"title": "Code", "content": "To create a search index for code, we employ an internal tool at Microsoft, built on the foundation of Tree Sitter [5]. This tool constructs a comprehensive Abstract Syntax Tree (AST) to extract key components from the code. These components include classes, functions, statements, variables, and their relationships, such as used by, uses, and inherits from.\nThe process of constructing the search index involves the following steps: (1) Code Chunking: We generate code chunks based on the structure of the Abstract Syntax Tree (AST). For languages not supported by Tree-sitter, such as Scope [54], we concatenate the code chunk with five preceding and five succeeding chunks. This approach leverages a Large Language Model (LLM) to extract the complete code snippet, ensuring that a single function or class is not split across multiple chunks. The same LLM is also used to identify referenced classes or functions. (2) Component Relationships: For each component, retrieve one-hop related components and attach their corresponding code chunks. (3) Description Augmentation: Use a large language model (LLM) to augment the description of each code chunk. This includes generating a title, a description of the code, references used within the code chunk, and brief descriptions of referenced functions or classes to serve as potential future search fields. (4) Embedding Generation: Produce embeddings for the concatenated code chunks and all associated search fields, establishing a search index given the search fields.\nThis process creates a robust and flexible search index, enhancing the utility of code components and their contextual relationships for various search scenarios (see Section 6.2)."}, {"title": "COPILOT BACKEND", "content": "The backend REST API is implemented using PromptFlow [32] as a stateless API, requiring all necessary information, such as the full chat history as input. This design abstracts away much of the engineering overhead related to authentication and state management, which is handled by our bot frontend (discussed in Section 7). The PromptFlow REST API is composed of various interconnected modules, where each module can be an LLM prompt or a Python script. Figure 5 illustrates the different modules implemented:\n\u2022 Skills and Planners: These utilize the Semantic Kernel [28] to generate execution plans based on available skills. Examples include the pre-chat and post-chat skills.\n\u2022 Chat Module: This module is responsible for making an LLM call with all the gathered contexts.\n\u2022 Auxiliary Modules: These modules support the flow by initializing parameters or managing memory. Examples include the Memory Extractor module and the Prompt Constructor module, which help in truncating or formatting memory as needed.\nThe search indexes created in Section 4 are utilized by corresponding pre-chat skills to support context retrieval. Most team-specific internal toolkits are implemented as \"customized skills\" that can be invoked by the pre-chat planner."}, {"title": "Input and Output", "content": "The flow's input consists of several components: (1) chat history; (2) skill data, typically generated from a dedicated popup UI in the frontend to enforce the invocation of certain skills (see Section 7); (3) the user question; and (4) the user ID, obtained from the frontend (see Figure 5). The flow's output includes: (1) an answer to be presented to the user; (2) each skill's output in dictionary format;"}, {"title": "Skills and Planners", "content": "The flow design is centered on supporting the execution of various skills. We initially used the Semantic Kernel [28] sequential planner (without Azure OpenAI function calling [25]) for skill orchestration.\nSkill-Chat-Skill Sequence. To minimize the number of calls to large language models (LLMs) and reduce latency, we aim to limit the use of LLMs for most skills. Instead, we combine outputs from various skills and use them jointly as context within the final LLM call. This streamlined approach structures the workflow into a manageable \"skill-chat-skill\" sequence, reducing the latency of conversations by making only one LLM call. Skills are categorized into two types: those primarily supporting content retrieval (pre-chat) and those that trigger actions post-LLM.\nHierarchical Planners. We observed the following:\nTo address system complexity, we implemented hierarchical planners, each managing a subset of skills and incorporating an option to enable Azure OpenAI function calling [25]. This approach reduces the complexity faced by each individual planner.\nThe first group of (pre-chat) skills, referred to as default skills, comprises content retrieval skills that are essential for addressing most inquiries. This group includes skills such as get_code, get_icm, and get_tsg, all backed by Azure Al search indexes.\nThe second group consists of (pre-chat) customized skills tailored for specific types of queries using team-specific internal tools. An example is a skill that directs users to monitoring dashboards relevant to their particular team. These skills, which are invoked less frequently and are specific to certain teams, can also bypass the LLM chat module, thereby reducing latency (see Figure 5).\nThe third group of (post-chat) skills is executed after the chat module, either because (1) they depend on outputs from the chat module's answer (e.g., certain query generation skills with the capability to extract queries from the provided answers are activated only if the chat output includes such queries); or (2) they initiate post-chat actions, such as recommending follow-up questions.\nEach skill group is managed by a dedicated planner and an executor (see Figure 5). This organizational structure significantly reduces the planner's task complexity compared to systems that require detailed logic for comprehensive execution plans (e.g., [38, 40, 43, 47]).\nSkill Dependency. DECO incorporates inter-skill dependencies, allowing multiple skills to use outputs from other skills for contextual grounding. These upstream dependencies are explicitly declared in the skills' properties. During the execution, skills that do not have dependencies are run concurrently to minimize latency. The execution order, rather than being generated by the planner, is deterministically determined based on the properties of the skills."}, {"title": "Chat Module", "content": "The flow consists of a single chat module, which assembles its prompt template from the following elements: (1) User-specified Prompt: Derived from the prompt_store module using a fine-tuned template, enhanced with team-specific descriptions (see Figure 2) and additional skill directives. (2) Retrieved Documents/Data: Includes skill outputs such as TSG, ICM, code chunks or telemetry data. (3) Current IcM: Provided by the icm_extractor skill when an incident ID is available, providing relevant ticket information for context during the session. (4) Chat History: Integrates previous interactions to ensure continuity. (6) Rephrased User Question: The planner reformulates the user's question, recorded as user_intent, to enhance search precision. This is particularly useful for contextualizing the query or clarifying the intent. (6) User's Raw Question: The direct query from the user."}, {"title": "Auxiliary Modules", "content": "The Init module sets environment variables from the user config file, such as the LLM API endpoint, those variables can be then used throughout the flow. The Memory Extractor module truncates the input chat history based on configured parameters of max_chat_history. In the future, this module can help leverage more advanced memory management techniques (such as MemGPT [37]) to retrieve/delete relevant information from the full chat history. The Prompt Constructor puts together the user input (such as product description) and merges it with the DECO's prompt templates and sends the full set of messages to the LLM module, i.e., the Chat module. The skill Validator executes the validation function defined in the class of the invoked skills. For instance, the skill of KQL Generator will validate the generated Kusto queries using LLM to ensure the query is grounded based on retrieved documents."}, {"title": "ALGORITHMS", "content": "Based on the end-to-end flow design, we implemented several enhancements to further improve chat quality. After analyzing user feedback and bot responses, we observed the following:\nThis issue likely stems from prompts instructing the bot to rely strictly on retrieved documents rather than generating answers from its own knowledge. While this approach prevents hallucination, it can lead to unhelpful responses like \u201cSorry, I cannot find relevant information\" when incorrect documents or skills are involved. To address this, we implemented key improvements:\n\u2022 Planner Improvements: Improved the identification of accurate information sources and tools to ensure the planner selects the appropriate skills for user queries.\n\u2022 Retrieval Algorithm Improvements: As opposed to the naive RAG mechanism using a single vector search, we proposed optimized retrieval strategies and reranking algorithms for skills such as get_icm, get_tsg, and get_code to enhance retrieval precision.\nThese enhancements are supported by an accuracy evaluation system, which forms the basis for the continuous evolution of these algorithms, as discussed in Section 8."}, {"title": "Planner Improvement", "content": "We implemented decorators for each skill to specify its functionality and input arguments, along with examples of positive and negative applicability [28]. The planner leverages these annotations to craft execution plans tailored to the available skills. Further enhancements to this framework include:\nFew-Shot Examples. Incorporating few-shot examples into the Semantic Kernel enhances the planner's skill selection accuracy [28] (see Section 8). These examples are included in the prompt argument to refine the execution plan.\nIntegration with Internal Tools. DECO expands its capabilities by integrating internal tools as customized skills (see Figure 5), including the kql_generator for generating Kusto queries. Kusto is widely used within Microsoft as the primary backend database for storing logs of Azure products and facilitates efficient data management and retrieval. When this skill is executed successfully, it informs the flow execution to bypass the chat module, thus omitting stages like prompt construction and directly presenting the skill's output. This process significantly accelerates response times for specific queries."}, {"title": "IcM Retrieval Improvement", "content": "The get_icm skill, essential for generating curated incident reports, is frequently used to respond to user inquiries about how-to guides and to search for similar incidents, thereby offering additional mitigation strategies. We've noticed that user queries typically highlight specific incident details, such as failure mechanisms or server names, which align with the mitigation steps or extractable properties of incidents (discussed in Section 4). This insight has led us to enhance our retrieval algorithm by implementing detailed filtering of incident candidates using the Azure AI Search API.\nStarting with the user's query, we initiate an \"NL2SearchQuery\" task using predefined templates (see Figure 6). Depending on the specific input, Azure AI Search enables us to execute tailored search strategies. Each strategy is converted into a search query, executed to gather incidents, and the results are reranked based on a composite score to identify the top K documents for the user.\nNL2SearchQuery. For this task, we leverage the default planner's ability to populate input parameters with values extracted from the chat context (see Figure 5). The planner extracts several key arguments for this skill, including:\n\u2022 User Intent: Reformulates the user query into concise search keywords, considering not only the original question but also the full chat history and previous intermediate outputs to create precise search terms. This is used as the search predicate for the matching.\n\u2022 Search Field: Specifies the field to search within, such as \"summary\", \"title\", \"property\", or \"mitigation\". For example, if the user asks for similar incidents to a given one, \"title\" is often a suitable option. If the query includes specific properties like server name, \"property\" is used.\n\u2022 Search Method: Options include hybrid search, semantic search, simple search, or vector search [26].\n\u2022 Time Range: Inferred from the user query and extracted as a dictionary where the key is the date type and the value is the time range in days.\n\u2022 Ticket Type: Specifies the type of incident, with possible values including \"LSI\" (Live Site Incident), \u201cCRI\u201d (Customer Reported Incident), or \"ALL\".\nThrough testing, we found that function calling with the latest GPT-40 model, supported by few-shot examples, can accurately extract all necessary arguments based on user intent. The following examples illustrate the generation of search queries:\n(1) Question: \"Show me customer-reported incidents resolved by restarting the server in the last two weeks.\"\n\u2022 User Intent: Customer-reported incidents resolved by restarting the server.\n\u2022 Search Field: \"mitigation\".\n\u2022 Time Range: {\"resolve_date\": 14}.\n\u2022 Incident Type: CRI (Customer Reported Incident).\n(2) Question: \"Are there any live site incidents created in the last two days involving issues on server testserver1?\"\n\u2022 User Intent: Issues on server testserver1.\n\u2022 Search Field: \"property\".\n\u2022 Time Range: {\"create_date\": 2}.\n\u2022 Incident Type: LSI (Live Site Incident).\nReranking. The initial search query typically returns a large set of incidents, for example, 20. During the reranking step, we calculate scores for each incident to determine the top K results, such as the top 4, for the final retrieval output.\nThe reranking score for a document d is calculated as follows:\n$P(d) = \\alpha IS + \\beta \\cdot TS + \\gamma \\cdot SS$ (1)\nwhere IS represents the information score, which evaluates the quality of the IcM summary based on data quality metrics like token length, pre-computed during the preprocessing pipeline (see Section 4). TS, the time score, assesses the relevance of incidents based on their age, with older incidents presumed less relevant and assigned lower values. SS, the source score, verifies if the retrieved data aligns with the current context, assigning a value of 1 for matches (e.g., matching team or monitor ID in property fields) and 0 otherwise. These scores are normalized and combined to rerank the retrieved incidents, focusing on factors like information richness, timeliness, and source relevance. This process is detailed in Algorithm 1, and Section 8.2 compares the accuracy of our IcM retrieval algorithm to the existing default similarity search in the IcM Portal.\nA similar retrieval mechanism is also employed in the get_code skill for code search. This skill retrieves relevant code snippets in response to user queries by identifying optimal search fields, using output arguments from the Semantic Kernel. The parameters for constructing search queries include:\n\u2022 Search Text: Reformulated search elements drawn from the user's question, maintaining the user's intent and associated context, not merely the keywords.\n\u2022 Search Type: A list specifying the scope of the search, which can include one or more of the following values:\n(1) By title: Searches the codebase by title, useful for locating where specific elements like methods or classes are defined.\n(2) By content: Searches the codebase by content, applicable when seeking definitions of specific methods or classes.\n(3) By reference: Searches the codebase by reference, targeting locations where specific methods or classes are referenced.\n(4) All: Searches across all vector indexes of the codebase."}, {"title": "TSG Retrieval Improvement", "content": "The get_tsg function is a critical component of our backend flow, supported by an efficient and accurate TSG retrieval algorithm. Traditional retrieval-augmented generation (RAG) approaches typically use similarity-based searches, which compare user query embeddings with document chunk embeddings or employ term-frequency techniques [39]. However, empirical analyses show that these methods often underperform with highly technical documents, as embeddings do not capture the subtle distinctions within technical terms effectively. Although recent advancements in supervised retrieval models, such as RAFT [50], AR2 [49], and ARG [41], have improved retrieval accuracy, they necessitate the development and fine-tuning of deep neural networks (DNNs). These approaches are impractical for our self-hosting framework in DECO, as they require specialized machine learning expertise and additional infrastructure for training and hosting ML models, which may not be readily available to tenant teams.\nTo address these challenges, we developed a hybrid search algorithm integrated with Azure AI Search, leveraging an advanced preprocessing pipeline detailed in Section 4, eliminating the need for additional ML training while enhancing both retrieval accuracy and computational efficiency. Key improvements to the TSG retrieval process are detailed below:\nQuery Rephrasing User queries are rephrased using the Semantic Kernel in the planner to optimize search text, generating search arguments. Searches are conducted using both the original user query and the generated search arguments.\nRepository Description During the preprocessing pipeline setup, tenants provide brief descriptions for each Git repository or document source. The descriptions are included with the retrieved documents from the corresponding sources to enhance the LLM's contextual understanding about the document.\nDocument Filtering An additional filtering step refines the search results by evaluating the similarity scores of the top documents. If the top document(s) demonstrates a significant margin in similarity scores over subsequent results, only the top document(s) are returned. This approach, based on the observation that a small set of highly relevant TSGs usually suffices to resolve issues, prevents the inclusion of overly detailed or irrelevant documents. Advanced filtering techniques are further discussed in [4].\nMulti-Field Hybrid Search The hybrid search algorithm utilizes the preprocessing pipeline to extract metadata from each document chunk, including raw content, title, and the associated published URL (as detailed in Section 4). These metadata elements serve as separate embedding targets, and the outputs from all search queries are combined to effectively filter document chunks. We evaluated various search strategies such as vector search, hybrid search, and semantic search, ultimately adopting a hybrid approach that combines embeddings from titles and raw content for optimal performance, based on reciprocal rank fusion (RRF) [8, 33]. Future enhancements include incorporating advanced retrieval algorithms like HyDE and Reverse HyDE [11], generating enriched matching fields, and exploring the integration of graph structures into the index [3] and leveraging feedback from historical conversations to dynamically refine search rankings [4]. Section 8.2 evaluates the efficiency and effectiveness of these improvements."}, {"title": "FRONTEND", "content": "The frontend of DECO is built around the Azure Bot, which leverages the Azure Bot Framework V4 and is deployed on an Azure Web Application . This setup coordinates API calls, manages messaging (interacting with the PromptFlow endpoint), retains state in Azure storage, and supports user interface enhancements through frontend plugins such as Adaptive Cards [19], allowing users to modify generated Kusto queries. Authentication is handled via Microsoft Entra ID, also known as Azure Active Directory (AAD) authentication (see Figure 7). Different groups, managed by Azure Core Identity, are used to restrict user access.\nThe web application connects users to the backend Azure Bot through direct line web channels, which communicate with the Azure Bot API. This web channel seamlessly integrates with the IcM portal, pre-populating inputs like ticket IDs for backend processing. Additionally, the bot can be accessed via Microsoft Teams through a published manifest. The web application also transmits system telemetry to a reporting dashboard for monitoring. Detailed interactions between the components are shown in Figure 7."}, {"title": "Plugin", "content": "We introduce frontend \"Plugins\" to support various types of user interfaces required by certain customized skills. For example, for the kql_generator skill, the backend output includes plugin data that triggers a frontend plugin, which displays an Adaptive Card for the user. This card allows users to select Kusto clusters from a dropdown menu, edit the query, use checkboxes to choose which queries to execute, and click the \"submit\" button to send the query back to the system. The user input data is stored in a designated field and sent to the backend, which then triggers the kql_executor skill to execute the query on behalf of the user."}, {"title": "Access Control", "content": "Access control for the bot is managed through Azure Active Directory (AAD) groups [29]. Although the web API endpoint is publicly accessible (necessary for communication with Teams, direct link channels, and potential future integrations), the bot itself handles the authentication process. The bot uses OAuth 2.0 [30], with Azure Active Directory as the identity provider. This setup involves registering the bot with Azure, allowing it to function as an application that requests access to user data."}, {"title": "Telemetry", "content": "The copilot emits telemetry data to a Log Analytics Workspace, making this data accessible to Azure resource owners. Table 2 shows the types of data that are logged.\nStored conversation details are anonymized and not explicitly linked to any user, ensuring privacy. A thorough security and privacy review was conducted to ensure compliance with the company's data retention policy, which is based on well-defined data categories.\nA dashboard has been developed to display high-level metrics for selected DECO bots, contingent on the onboarded team's consent to share their telemetry data (see Figure 1)."}, {"title": "EVALUATION", "content": "The evaluation system in DECO supports the continuous development of the framework to enhance chat quality. The evaluation is divided into two categories: online and offline evaluation."}, {"title": "Online Evaluation", "content": "The online evaluation assesses the direct interactions between the chatbot and users, producing performance indicators for actual conversations. In addition to user feedback ratings, we employ the following metrics: (1) relevance of the generated answer, scored from 1 to 3, (2) relevance of the retrieved documents, scored from 1 to 3, and (3) groundedness of the answer based on the retrieved documents, scored from 0 to 1. Each metric is evaluated using a specific prompt and an LLM to generate a numeric score. Additionally, an AML endpoint has been deployed to asynchronously collect supplementary telemetry data alongside the primary backend flow. We have categorized the results into five categories to clarify these scores for users (see Table 3).\nThe online evaluation offers valuable metrics to compare the quality across various deployments for a range of Microsoft products. It also enables the measurement of document retrieval quality, which facilitates communication with the owning teams about areas where documentation can be further improved or is currently lacking."}, {"title": "Offline Evaluation", "content": "To support ongoing development of the chatbot and evaluate changes such as prompt modifications and updates to retrieval algorithms, we developed an evaluation framework that allows testing in hypothetical scenarios without disrupting the service. The offline evaluation focuses on assessing the quality of chatbot responses after code changes. We observed that:\nUsing a large sample of user ratings, we explored the feasibility of employing an LLM to rate chatbot responses on a scale from 1 to 5 stars, aiming at simulating human judgment. When comparing LLM-generated ratings with human ratings across 54 questions, we found a correlation of only 0.35, indicating substantial discrepancies between LLM and human evaluations. Manual reviews revealed that many responses deemed inaccurate by domain experts were perceived as plausible by non-experts due to subtle errors that only specialists could detect. This highlights the limitations of using LLMs as evaluators for assessing chatbot responses, as discussed in [14]. As a result, we prioritize using question-answer pairs with user ratings as benchmarks for evaluating chatbot responses whenever feasible.\nEvaluators. In this paper, we discuss four commonly used evaluators: the TSG Evaluator, the Incident Retrieval Evaluator, the Planner Evaluator, and the Similarity Evaluator.\nTSG Evaluator. The TSG Evaluator is designed to assess the accuracy of documentation chunks retrieved by the TSG retrieval process that are subsequently referenced in the chatbot's responses, i.e., by the get_tsg skill. The first \"golden\" set of TSG documentation is compiled from real user interactions, focusing only on responses rated four or five stars based on the \"reference list\" at the end of the chatbot's response, which includes clickable URLs to the cited documents as directed by the prompt. The second \"golden\" set is generated by the LLM, which creates synthetic questions for each documentation chunk, allowing us to identify the corresponding TSG documentation that answers those questions, similar to the self-supervised learning [45].\nThe evaluation process involves the following steps:\n\u2022 Extracting the golden TSG list from conversations with highly rated responses or synthetic data, denoted as G.\n\u2022 Running the get_tsg module for each query to produce a new set of documents, referred to as Q. This step yields deterministic results without LLM calls. Optionally, the entire workflow can be executed to capture the final reference list generated by the LLM, denoted as Q*.\n\u2022 Calculating precision and recall metrics by comparing G and Q*:\n$Precision = \\frac{|G\\cap Q^*|}{|Q^*|}$ (7)\n$Recall = \\frac{|G\\cap Q^*|}{|G|}$ (8)\n\u2022 Assessing coverage by comparing G with Q to evaluate how well the retrieved documents (Q) \"cover\" the golden set (G):\n$Coverage = \\frac{G\\cap Q}{|G|}$ (9)"}, {"title": "Incident Retrieval Evaluator", "content": "To assess IcM similarity, we developed a detailed evaluation metric using LLMs by constructing a comprehensive prompt that compares various aspects"}]}