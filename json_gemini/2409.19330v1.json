{"title": "3D-CT-GPT: Generating 3D Radiology Reports through Integration of Large Vision-Language Models", "authors": ["Hao Chen", "Wei Zhao", "Yingli Li", "Tianyang Zhong", "Yisong Wang", "Youlan Shang", "Lei Guo", "Junwei Han", "Tianming Liu", "Jun Liu", "Tuo Zhang"], "abstract": "Medical image analysis is crucial in modern radiological diagnostics, especially given the exponential growth in medical imaging data. The demand for automated report generation systems has become increasingly urgent. While prior research has mainly focused on using machine learning and multimodal language models for 2D medical images, the generation of reports for 3D medical images has been less explored due to data scarcity and computational complexities. This paper introduces 3D-CT-GPT, a Visual Question Answering (VQA)-based medical visual language model specifically designed for generating radiology reports from 3D CT scans, particularly chest CTs. Extensive experiments on both public and private datasets demonstrate that 3D-CT-GPT significantly outperforms existing methods in terms of report accuracy and quality. Although current methods are few, including the partially open-source CT2Rep and the open-source M3D, we ensured fair comparison through appropriate data conversion and evaluation methodologies. Experimental results indicate that 3D-CT-GPT enhances diagnostic accuracy and report coherence, establishing itself as a robust solution for clinical radiology report generation. Future work will focus on expanding the dataset and further optimizing the model to enhance its performance and applicability.", "sections": [{"title": "Introduction", "content": "In recent decades, the field of radiological imaging has undergone revolutionary changes, making the accurate and efficient interpretation of medical images crucial in modern diagnostics (Liu et al. 2024). The exponential growth of medical imaging data has placed immense pressure on radiologists, who must not only possess extensive diagnostic expertise but also spend considerable time drafting detailed reports, thereby increasing their workload. Consequently, developing an automated system capable of generating accurate and timely diagnostic reports is essential for alleviating the burden on physicians, improving workflow efficiency, and ensuring diagnostic quality, particularly in resource-constrained environments where high diagnostic accuracy is critical.\nWhile significant progress has been made in using machine learning and multimodal language models to automatically generate radiology reports from 2D medical images (Jing, Xie, and Xing 2017; Chen et al. 2022, 2020; Qin and Song 2022), the generation of reports from 3D medical images remains relatively unexplored. This is primarily due to challenges related to data scarcity (Li et al. 2023c), the complexity of feature extraction, and the computational demands associated with processing large datasets. Three-dimensional imaging modalities, such as Computed Tomography (CT) and Magnetic Resonance Imaging (MRI), offer a more comprehensive view of patient conditions compared to 2D imaging (M\u00fcller 2002), capturing intricate anatomical details and providing higher diagnostic accuracy, especially in detecting"}, {"title": "Related Works", "content": "The extraordinary generative capabilities of large language models (LLMs) have opened new avenues in natural language processing and computer vision (Touvron et al. 2023; Gan et al. 2023; Tian et al. 2023). Large Language Vision Models (LLVMs) aim to bridge the gap between visual and textual information, allowing machines to understand and generate content that synthesizes these modalities. Recent research has demonstrated the potential of LLVMs in various tasks, including image captioning (Zhu et al. 2023), visual question answering (Bazi et al. 2023; Liu et al. 2023b; Maaz et al. 2023), and image generation (Zhang, Rao, and Agrawala 2023), with applications extending into multimodal scenarios (Li et al. 2023b; Zhu et al. 2023), including the medical field (Thawkar et al. 2023; Tu et al. 2024). As a result, employing LLMs for the automatic generation of medical imaging analysis reports has emerged as a more effective approach. Several existing studies have utilized LLMs for the analysis of 2D medical images, such as X-rayGPT (Thawkar et al. 2023), LLava-Med (Li et al. 2023a), ChatDoctor (Li et al. 2023d), Med-Alpaca (Quispe Bonilla et al. 2023), PMC-LLaMA (Wu et al. 2024), Clinical Camel (Faye and Bengoumi 2018), DoctorGLM (Xiong et al. 2023), and Huatuo (Wang et al. 2023). These models are typically initialized with open-source LLMs and fine-tuned on specialized biomedical datasets tailored to specific guidelines. The resulting LLMs hold significant potential, particularly in understanding patient needs and providing informed recommendations."}, {"title": "3D Medical Image Analysis and Report Generation", "content": "The field of 3D medical image analysis and report generation has seen significant advancements, yet it continues to face numerous challenges. CT-CLIP (Hamamci et al. 2024), utilizing the CT-RATE dataset, has effectively aligned textual information with 3D medical images, enhancing the accuracy of multi-abnormality detection and case retrieval tasks. However, its application in report generation remains limited. RadFM (Wu et al. 2023), a foundational model in radiology, employs large-scale multimodal datasets for pre-training, leading to improvements in report generation accuracy. Nevertheless, it struggles with long sentence generation, 3D image handling, and evaluation metrics. CT2Rep(Hamamci, Er, and Menze 2024), the first method specifically designed for generating chest CT reports, leverages advanced 3D vision encoders and multimodal fusion modules. While it shows progress in the precision of report generation, data and computational complexity remain significant obstacles. M3D-LaMed (Bai et al. 2024) focuses on 3D medical image analysis, utilizing a large-scale 3D multimodal medical dataset. Despite its strong performance, challenges persist in enhancing dataset diversity and managing task complexity. Overall, these methods lay the groundwork for automating 3D medical image report generation, but challenges related to data scarcity, model complexity, and evaluation standards still require further research and resolution."}, {"title": "Method", "content": null}, {"title": "Model Architecture", "content": "3D CT Image Encoder: As demonstrated in Figure 2(a), the CT-ViT encoder(cc), derived from CT-CLIP (Hamamci et al. 2024), is utilized to extract features from 3D chest CT volumes by dividing them into smaller patches and embedding these into a lower-dimensional latent space. This process results in embedded CT tokens Zx, which are used for subsequent analysis.\nGiven a 3D CT image x with dimensions $X \\in \\mathbb{R}^{240\\times 480\\times 480}$, the image is segmented into non-overlapping patches of size 15 \u00d7 30 \u00d7 30. Each patch is then mapped into a 512-dimensional space D, resulting in a tensor Zx with dimensions B x T \u00d7 H \u00d7 W \u00d7 D, where B is the batch size, T represents the number of temporal patches, H and W are the height and width of the slices, and ph, pw are the spatial patch dimensions.\nThe 3D chest CT volume feature extraction process is formally defined as $Z_x = h(x)$, ensuring the preservation of 3D volumetric information and effectively supporting the construction of sequence-to-sequence models for report generation. To adapt the output for the projection layer, the transformation process for a tensor Zx with dimensions BxTxx W \u00d7 D involves the following steps:\nFirst, the tensor is permuted to obtain:\n$Z'_x = P(Z_x, [0, 4, 1, 2, 3]) $\nresulting in Z with dimensions $B \\times D\\times T\\times \\frac{H}{2p_h} \\times \\frac{W}{2p_w}$.\nNext, 3D average pooling is applied with a kernel size of 2, yielding:\n$Z''_x = A(Z'_x, kernel\\_size = 2)$\nresulting in Z\" with dimensions $B\\times D\\times T\\times \\frac{H}{2p_h} \\times \\frac{W}{2p_w}$.\nThis tensor is then reshaped to:\n$Z'''_x = R(Z''_x, [B, D, T \\times \\frac{H}{2p_h} \\times \\frac{W}{2p_w} ])$"}, {"title": null, "content": "resulting in Z\"\"\" with dimensions B \u00d7 D \u00d7 (T \u00d7 H' \u00d7 W'), where H' = $\\frac{H}{2p_h}$ and W' = $\\frac{W}{2p_w}$.\nFinally, a permute operation is applied to obtain:\n$Pv = P(Z'''_x, [0, 2, 1])$\nproducing Pv with dimensions B \u00d7 (T \u00d7 H' \u00d7 W') \u00d7 D.\nThe complete transformation is summarized as:\n$Pv = P (R (A (P (P_c(x), [0, 4, 1, 2, 3])))) $\nHere, Pv represents the output tensor that contains embedded feature tokens for each batch, preserving crucial 3D volumetric information, which is essential for accurate report generation. yielding the desired output, where each batch contains 512 feature tokens, each of dimension 512.\nLinear Projection Layer: For simplicity and efficiency, we opted for a straightforward linear projection technique for input projection, drawing inspiration from the architecture of the LLaVA multimodal large language model. As demonstrated in Figure 2(b),specifically, we employ a trainable projection matrix W to align the CT image token embeddings Pv with the semantic space of text word embeddings. This is accomplished by applying the projection matrix to the image token embeddings, resulting in language-aligned embeddings M\u2081 = W \u00d7 P\u1ef9. This transformation ensures that the dimensionality of the image tokens matches that of the word embeddings used in the language model, thereby enabling efficient and effective data fusion between the visual and textual modalities.\nIntegration of Vision and Language Models: As illustrated in Figure 2(c), the query is concatenated with an image placeholder and combined with a dialogue template to form the prompt. This prompt is then processed by the LLM's tokenizer, which converts the text into tokens. Each token corresponds to a specific token ID based on the LLM's vocabulary. The image placeholder is assigned a special token ID of -200, serving as a marker within the LLM's vocabulary. Using the LLM's embeddings, the text tokens are mapped into word vectors Mq. The dimensionality of these word vectors varies according to the size of the language model. Since the image placeholder does not have a corresponding entry in the vocabulary, its token embeddings are split into Mq1 and Mq2. The image feature vectors, output from the linear projection layer and matched to the same dimensionality, are then concatenated with the split word vectors to form the full input M = concat([Mq1, Mv, Mq2]). This combined input is fed into the language model to produce output token IDs, which are then decoded by the LLM's embedding layer to generate the final output Xa.\nThus, the LLM function is defined as g(\u00b7). The overall computation can be expressed as:\n$X_a = g (concat ([M_{q1}, M_v, M_{q2}])) $"}, {"title": "Dataset", "content": "Data Collection: We adopted a subset of a public dataset, CT-RATE (Hamamci et al. 2024). It includes 25,692 non-contrast chest CT volumes, which have been expanded to 50,188 volumes through various reconstruction techniques, representing 21,304 unique patients, and is further enriched with corresponding radiology text reports, multiple abnormality labels, and metadata. We selected 8,070 cases as the foundational data for our study. Additionally, we collected 2,000 3D chest CT scans and corresponding radiology reports from a renowned international hospital, designated as the private Dataset-XY. In this dataset, patients' ages range from 20 to 88 years, with a mean age of 51.42 years. The gender distribution is 44.7% female and 55.3% male.\nEach CT volume in Dataset-XY has an axial screen resolution of 512x512 pixels, with the number of slices per volume ranging from 100 to 600. Each CT volume in CT-RATE has the same axial screen resolution and slice count range as Dataset-XY. The CT-RATE had already undergone strict anonymization procedures, eliminating the need for further de-identification or format conversion.\nData Preprocessing: For Dataset-XY, we first implemented de-identification measures to ensure patient privacy. To maintain the high quality and consistency of the radiology reports with their corresponding 3D chest CT volumes, we conducted rigorous data cleaning, focusing on three key aspects: removing duplicates, correcting data inconsistencies, and filtering out irrelevant text information. Duplicate entries were manually screened to identify and remove redundant elements directly related to data values and report titles, resulting in a standardized and unique dataset. For the image data, we filtered out low-resolution images and eliminated duplicate or irrelevant entries. Then, a meticulous manual review and consolidation process was carried out to ensure dataset uniformity and coherence, ultimately producing a highly optimized dataset consisting of 1,887 cases.\nFor both datasets, we utilized the slope and intercept values from the metadata to convert CT values to Hounsfield Units (HU), cropping them to the range of [-1000 HU, +200 HU], which reflects the diagnostic limits of the HU scale. Each volume was then resampled to achieve a uniform spacing of 0.75 mm in the x and y axes and 1.5 mm in the z axis, with volumes cropped or padded as necessary to maintain a consistent resolution of 240x480x480. Table 1 presents the statistics of the datasets, including the number of radiographic images, the number of reports, and the average report length for the training, testing, and evaluation sets, split at a ratio of 0.8, 0.1, 0.1, respectively."}, {"title": "VQA Dataset Creation", "content": "To develop a robust Visual Question Answering (VQA) system capable of understanding and generating accurate responses based on 3D medical images, we constructed a specialized dataset that pairs 3D chest CT images with corresponding textual descriptions. This dataset is crucial for training and fine-tuning the model, ensuring that it can accurately interpret complex medical imagery and generate meaningful diagnostic information.\nEach entry in the dataset consists of a 3D chest CT image, a related question, and an answer. This structured design enables the model to learn the intricate relationships between visual features in CT scans and their corresponding text, thereby enhancing the system's comprehension and generation capabilities. During model training, prompts are provided that combine 3D chest CT images with textual information. A specific prompt structure is employed consistently throughout the training process. In this structure, the Xsystem-message initiates the interaction, followed by a stopping token <STOP>. The human provides an instruction Xinstruct, which is followed by another stopping token. These instructions are randomly selected from a predefined set of prompts, which include various types of queries such as \"What findings do you observe in this CT scan?\", \"Could you summarize the observations from this CT scan?\", \"What abnormalities are present in this CT scan?\", or \"How would you interpret the results of this CT scan?\". The system then generates a response Xa, in the form of a report corresponding to the 3D chest CT image.\nIt is noted that, although the VQA system is designed to flexibly handle a wide range of questions related to 3D medical imaging, this paper currently focuses on a specific task-generating radiology reports to deeply evaluate the core functionality of the model in a controlled environment. Nevertheless, the architecture and dataset have been designed with future scalability in mind, laying the foundation"}, {"title": "Experiment", "content": "Our initial model training process is inspired by conventional multimodal language training methodologies, beginning with pre-training followed by a stage of visual instruction fine-tuning (Liu et al. 2023a)."}, {"title": "Implementation Details", "content": "Setup To initialize the model, we leveraged a pre-trained CT-ViT as the visual encoder, alongside the Vicuna-7B model as the large language model (LLM) component. Additionally, to strike an optimal balance between complexity and effectiveness, a randomly initialized linear layer was utilized as the projection module. The training was conducted on a single RTX 3090 GPU (24GB memory). During pre-training, the learning rate was set to le-3, and a batch size of 1 was assigned per GPU. For the instruction fine-tuning phase, the learning rate and batch size per GPU were adjusted to 2e-4 and 1, respectively. Both training stages employed the Adam optimizer, a cosine learning rate scheduler, and bfloat16 precision. The pre-training phase required 14GB of GPU memory, while the instruction fine-tuning phase occupied 22GB of GPU memory.\nStage 1: Pre-training The model aimed to understand the relationship between 3D CT image features and their corresponding reports by analyzing a large collection of 3D CT image-report pairs. During this phase, we froze the image encoder and language model, focusing solely on training the projection layer. The training was conducted using our custom-built VQA dataset. Due to the scarcity of paired 3D CT images and reports, we were unable to employ the large-scale alignment training typical of multimodal models. Instead, we adopted an interactive approach across multiple data types to address this challenge.\nStage 2: Fine-tuning The model from Stage 1 was further refined to align 3D CT image features with specific radiology reports using image-text pairs. During this phase, we continued to freeze the image encoder but trained both the projection layer and the LLM. Given the constraints on computational resources and the limited dataset, we fine-tuned the language model using a LoRA (Hu et al. 2022) module rather than full parameter tuning. The number of epochs was determined based on avoiding overfitting during training. The high-quality VQA dataset described in Section 4 was again utilized for training. Despite limited resources and sparse datasets, our adequately trained 3D CT-GPT demonstrated the ability to generate more natural and high-quality radiology-specific responses for given 3D chest CT images."}, {"title": "Evaluation Metrics", "content": "To evaluate the efficiency of our radiology report generation, we employed natural language generation (NLG) metrics. The primary NLG metrics used include BLEU (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), and ROUGE-L (Lin 2004), which measure word overlap, synonym usage,"}, {"title": null, "content": "and sequence matching in the radiology reports, respectively. Specifically, ROUGE scores assess the consistency between the generated text and that of human experts, capturing the presence and order of n-grams, thereby evaluating the quality and coherence of the text. These traditional metrics quantify textual similarity through n-gram overlap or variation, forming a comprehensive framework for assessing automated radiology report generation systems and establishing technical standards for model outputs."}, {"title": "Impact of Different Training Strategies on Model Performance", "content": "In the task of medical image generation, the scarcity and large volume of 3D chest CT data pose significant challenges for conducting large-scale model training. To make the most of the limited available datasets and achieve sufficient training, we designed several distinct training strategies and compared the quality of the generated reports. Specifically, our approach utilizes a fine-tuned image encoder and a linear projection module comprising a 2-layer MLP, both adapted with private dataset fine-tuning. To ensure consistent and fair comparison across different training strategies, we selected a uniform temperature parameter (0.7) for all strategies and reported the results based on this setting. The evaluation was conducted on the validation set (Dataset-XYval), which was not used during the training phase, ensuring that the test results reflect the model's true generalization performance.\nT1 Training Strategy: We first pre-trained the model for 5 epochs using CT-RATEtrain, followed by fine-tuning with 1508 private data samples (Dataset-XY train) for 2 epochs. The total time required was 16 hours.\nT2 Training Strategy: We used 1508 private data samples(Dataset-XYtrain) to simultaneously pre-train the model for 5 epochs and fine-tune it for 2 epochs. The total time required was 6 hours.\nT3 Training Strategy: We pre-trained and fine-tuned the model solely using the public dataset (CT-RATEtrain), which took a total of 18 hours.\nThe results of these strategies are presented in Table 2."}, {"title": "Comparison with Existing Methods", "content": "To comprehensively evaluate the performance of 3D-CT-GPT model, we compared it against existing methods, including CT2Rep (Hamamci, Er, and Menze 2024), RadFM (Wu et al. 2023), and M3D (Bai et al. 2024). Due to significant differences in model architecture and the unavailability of pre-trained weights and inference files for CT2Rep, it was excluded from this comparison. The M3D literature indicates that RadFM does not outperform M3D in generating 3D medical reports, leading us to focus on a fair comparison between our model and M3D. To ensure a balanced evaluation, we adjusted our comparison strategy to account for differences in data processing and model accessibility, employing both direct and indirect assessment methods.\nDirect Comparison on Unified Datasets The M3D model processes 3D medical images by stacking multiple 2D PNG images into a 3D format. However, it does not utilize CT"}, {"title": "Indirect Comparison with Literature Results", "content": "As shown in Table 2 under the Indirect Comparison (Literature Results) section, the T1 training strategy outperforms the M3D model in all key metrics, including BLEU (0.3836), ROUGE-1 (0.4749), ROUGE-L (0.3281), METEOR (0.3565), and BERTScore_F1 (0.8890). In comparison, M3D's reported scores in the literature are lower, with BLEU ranging from 0.1449 to 0.1515, ROUGE-1 from 0.1925 to 0.1955, and METEOR from 0.1411 to 0.1438. These results emphasize the superior capability of 3D-CT-GPT in generating accurate and coherent radiology reports, even without the specific data formatting advantages of M3D. This reinforces the robustness and effectiveness of our model, particularly in leveraging both public and private datasets, and establishes 3D-CT-GPT as a more reliable solution for clinical radiology report generation."}, {"title": "Ablation Study and Training Strategy Impact", "content": "Under the T2 training strategy, the performance of the 3D-CT-GPT model across three configurations is summarized as follows:\nWithout fine-tuning: The model showed weaker performance, with BLEU 0.2950, ROUGE-1 0.4163, METEOR 0.3037, and BERTScore_F1 0.8809.\nWith fine-tuning: Performance improved significantly, achieving BLEU 0.3476, ROUGE-1 0.4446, METEOR 0.3198, and BERTScore_F1 0.8862.\nUsing a linear projection layer: The model maintained strong performance, with BLEU 0.3418, higher ROUGE, METEOR scores, and BERTScore_F1 0.8850.\nFine-tuning significantly boosts performance, and the linear projection layer offers a slight edge in semantic alignment, providing valuable insights for model optimization.\nHigher temperatures increased text diversity but reduced accuracy, as seen in Figure 4."}, {"title": "Conclusion", "content": "The experimental results demonstrate that the 3D-CT-GPT model, particularly when employing the T1 and T2 training strategies, significantly outperforms existing methods in generating high-quality radiology reports. The model's strong generalization across diverse datasets, coupled with the benefits of fine-tuning and carefully designed training strategies, underscores its potential for clinical deployment"}, {"title": null, "content": "in medical imaging. These findings validate the effectiveness of our approach and open up opportunities for further refinement and broader application in the field of radiology report generation."}]}