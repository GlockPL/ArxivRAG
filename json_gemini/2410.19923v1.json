{"title": "LANGUAGE AGENTS MEET CAUSALITY \u2013 BRIDGING LLMS AND CAUSAL WORLD MODELS", "authors": ["John Gkountouras", "Matthias Lindemann", "Phillip Lippe", "Efstratios Gavves", "Ivan Titov"], "abstract": "Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data, this information is often incomplete, incorrect, or inapplicable to a specific environment. In contrast, causal representation learning (CRL) focuses on identifying the underlying causal structure within a given environment. We propose a framework that integrates CRLS with LLMs to enable causally-aware reasoning and planning. This framework learns a causal world model, with causal variables linked to natural language expressions. This mapping provides LLMs with a flexible interface to process and generate descriptions of actions and states in text form. Effectively, the causal world model acts as a simulator that the LLM can query and interact with. We evaluate the framework on causal inference and planning tasks across temporal scales and environmental complexities. Our experiments demonstrate the effectiveness of the approach, with the causally-aware method outperforming LLM-based reasoners, especially for longer planning horizons.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have emerged as powerful tools for a wide range of tasks, from natural language understanding to complex problem-solving (Brown et al., 2020; Radford et al., 2019; Liu et al., 2023b). Recent work has explored the use of LLMs as action agents for planning and reasoning tasks, showing promising results in improving task-specific, downstream performance (Ahn et al., 2022; Hao et al., 2023; Huang et al., 2023). These approaches primarily rely on the model's ability to extract common-sense causal information stated in its training data (Ze\u010devi\u0107 et al., 2023). While LLMs can reflect general beliefs and correlations, this information may be incomplete, incorrect, or inapplicable in specific environments. This poses challenges for LLMs in novel or complex situations, particularly in dynamic environments where accurate modeling of action consequences is crucial (Valmeekam et al., 2023; Kambhampati et al., 2024).\nCausal representation learning (CRL) aims to identify the underlying causal structure of data (Sch\u00f6lkopf et al., 2021). By separating and identifying latent causal factors, CRL enables models to reason about the effects of interventions and counterfactuals. Recent theoretical work provides justification for causal representation learning, showing it is necessary for achieving strong robustness guarantees in AI systems (Richens & Everitt, 2024). While CRL can model complex causal mechanisms, applying it to real-world environments with visual complexity remains challenging. Recent advancements in CRL (Lippe et al., 2022; 2023) have begun to tackle this problem in simulated environments. These developments open up new possibilities for enhancing AI systems, including LLMs. Although CRL does not directly address all LLM limitations, it can significantly enhance their capabilities in specific domains. Our work builds upon these advancements, integrating CRL with language models to improve their performance on causal inference and planning tasks.\nWe introduce a framework that combines CRL with language models to enable causally-aware reasoning and planning in interactive environments. CRL provides LLMs with a structured, causal understanding of the environment, reasoning about interventions and their consequences during planning. The causal world model \u2013 akin to a simulator but learned rather than predefined \u2013 allows the LLM to evaluate multiple possible futures before taking action and thereby guides its decisions. Conversely, LLMs offer a flexible interface for interacting with the causal world model, allowing for more intuitive planning and reasoning that can leverage the LLM's commonsense knowledge.\nFurthermore, this work investigates using text to represent actions in the context of CRL-based world modeling. Text-based action representations provide a flexible and intuitive way to describe actions, making them more suitable for generalist agents operating in diverse environments. Moreover, annotating frame sequences with natural language descriptions is often easier than exhaustively enumerating every possible action in an environment, which can be intractable for complex domains.\nWe consider a setting with interleaved sequential observations in image format and corresponding action descriptions at each timestep. This setup takes inspiration from real-world scenarios where an agent might receive visual input (e.g., from a camera) along with descriptions of actions taken (e.g., from system logs or human annotations). For example, in a robotic manipulation task, the dataset might consist of a series of images showing the robot's workspace, paired with descriptions like \"The gripper shifted slightly to the right.\" or \"The object was grasped and placed on the worktop.\" We assume no prior knowledge of the causal factors or the causal mechanisms between them. The agent can only observe the effects of its actions from the images and does not require explicit information about which specific variables or factors in the causal model it is affecting. Our method builds on BISCUIT (Lippe et al., 2023), a CRL framework, to create a flexible causally-aware world model from the sequence of observations and action descriptions, which is then used for planning in environments.\nThe key contributions of our work are as follows:\n\u2022 The first framework integrating CRL with LLMs to enable causally-aware reasoning and planning in interactive environments.\n\u2022 An exploration of text-based action representations for CRL and demonstration of their effectiveness in data-scarce regimes, showing improved data efficiency in learning causal representations.\n\u2022 Demonstration of the framework's effectiveness on a set of reasoning and planning tasks across both static and dynamic environments.\nOur experiments focus on simple environments, using existing CRL methods that are sufficiently advanced for our use case. While these environments are still relatively simple, they represent the current frontier of causal representation learning. As more powerful CRL methods become available, they can be integrated into our framework, scaling it to more complex, realistic scenarios."}, {"title": "2 RELATED WORK", "content": "Causal Representation Learning Causal representation learning aims to identify the underlying causal variables and their relations from high-dimensional observations (Sch\u00f6lkopf et al., 2021). In the most general setting, the latent causal variables may not be uniquely identifiable (Locatello et al., 2019a; Hyv\u00e4rinen & Pajunen, 1999). Many approaches rely on assumptions or additional knowledge about the causal structure, such as constraining the observation function (Buchholz et al., 2023; Squires et al., 2023; Ahuja et al., 2023; Zhang et al., 2023; Kivva et al., 2022; Lachapelle et al., 2023), sparse graphical structures (Khemakhem et al., 2020; Liu et al., 2022; 2024; Lachapelle & Lacoste-Julien, 2022; Lachapelle et al., 2024), having multiple views (Xu et al., 2024; Yao et al., 2024a; von K\u00fcgelgen et al., 2021; Brehmer et al., 2022; Locatello et al., 2020), or supplementary supervision labels (Yang et al., 2020; Komanduri et al., 2022; Locatello et al., 2019b). Recent advancements have explored CRL for temporal environments, in which agent-level actions like in reinforcement learning are used to learn the causal structure of the environment (Lippe et al., 2022; 2023; Nalmpantis et al., 2023). In particular, our work leverages BISCUIT (Lippe et al., 2023), a CRL framework that learns causal representations with realistic agent-focused assumptions, requiring only a small set of labeled causal variables for the final mapping after causal representation learning, without their interactions or causal graphs.\nWorld Models and Causal Integration World models predict the consequences of actions and have been extensively used in reinforcement learning (Ha & Schmidhuber, 2018). Recent work has focused on object-centric world models (Greff et al., 2017; Steenkiste et al., 2018; Watters et al., 2019) and the integration of graph neural networks for modeling transitions (Battaglia et al., 2016; 2018; Kipf et al., 2018). However, attempts to integrate causality into world models have been limited. Some approaches, such as CoPhyNet (Baradel et al., 2020), consider counterfactual scenarios but rely on direct supervision of object positions or place constraints on unobserved variables (Li et al., 2020). Our work aims to learn a causal world model relying only on images and textual annotations but capable of reasoning about actions across state transitions, while also being able to be interacted with by a language model.\nLarge Language Models, Causality, Planning and Reasoning There has been much work exploring the use of LLMs as action agents for planning and reasoning tasks, showing promising results (Ahn et al., 2022; Hao et al., 2023; Huang et al., 2023). Various methodologies have been developed to make use of LLMs for agent planning. These include task decomposition for breaking complex tasks into subtasks (Wei et al., 2022; Yao et al., 2023; Shen et al., 2024), multi-plan selection for generating and choosing optimal plans (Yao et al., 2024b; Wang et al., 2022), external module-aided planning (Liu et al., 2023a; Guan et al., 2023), reflection and refinement via self-evaluation and improvement (Shinn et al., 2024; Gou et al., 2024; Madaan et al., 2024), and memory-augmented planning for decision making (Zhang et al., 2024; Zhong et al., 2024). While LLMs have shown impressive performance in reasoning, tool usage, planning, and instruction-following, challenges remain in addressing hallucinations, plan feasibility, and tractability in complex, multi-step planning scenarios (Valmeekam et al., 2023; Kambhampati et al., 2024; Kambhampati, 2024). Theoretical work on robustness under distribution shifts in unmediated decision tasks (where the decision does not influence the utility) establishes a connection between causal understanding and robustness (Richens & Everitt, 2024). A better approximation of the underlying causal model generally translates to more robust agents, implying that world models should be causality-aware (Gupta et al., 2024)."}, {"title": "3 BACKGROUND AND SETUP", "content": "To enable LLMs to perform causally-aware reasoning and planning in interactive environments, we leverage CRL methods to build a causal world model (CWM). The CWM provides LLMs with a structured understanding of the environment, allowing them to reason about interventions and their consequences during planning."}, {"title": "3.1 CAUSAL REPRESENTATION LEARNING IN TEMPORAL CAUSAL GRAPHS", "content": "CRL aims to uncover the latent causal variables and the underlying causal structure. In temporal settings, we consider sequences of high-dimensional observations {Xt}T t=0, where Xt \u2208 RD, and actions {Rt}T t=1, where Rt \u2208 RE. Actions Rt can represent, for example, the coordinates of the locations where the interactions occurred (Lippe et al., 2023). The true causal variables {Ct}T t=0, where Ct \u2208 RK, are unobserved. Furthermore, a deterministic observation model is assumed, often represented as Xt = g(Ct), where g : RK \u2192 RD is an injective function mapping causal variables to observations.\nInstead of directly modeling causal variables, CRL relies on latent state representations. It estimates a function f : RD \u2192 RM that maps observations Xt to latent representations zt = f(Xt). The goal is to ensure that each dimension zti of zt corresponds to a causal variable Ci in Ct up to a transformation decided by the identifiability class of the causal model. Specifically, it aims to achieve this disentanglement using only {Xt}T t=0 and {Rt}T t=1"}, {"title": "3.2 GENERATIVE MODEL", "content": "The temporal CRL framework is often modeled as a generative process that describes how observations are produced from underlying latent state representations and actions. At each time step t, the state zt evolve according to a transition model influenced by actions Rt, and generate observations Xt. Assuming a first-order Markov process, the conditional likelihood of the observed data {Xt}T t=0 given actions {Rt}T t=1 is expressed as\np({Xt}T | {Rt}T ) = \u222b p(z0) \u220ft=1T pw(zt | zt\u22121, Rt) pg(Xt | zt) dz, (1)\nwhere p(z0) is the prior distribution over the state. The transition model term pw(zt | zt\u22121, Rt) models the state dynamics, capturing how the states evolve over time and how intervening actions influence them. The observation model pg(Xt | zt) describes how the states generate the observations, which in our case will be done with the deterministic function g.\nThe marginalization in Eq. (1) renders the objective intractable. A standard approach to address this is to optimize the corresponding Evidence Lower Bound (ELBO) by assuming a Gaussian distribution for the transition dynamics and the standard Gaussian for the prior, using the reparameterization trick to enable efficient optimization (Kingma & Welling, 2013)."}, {"title": "3.3 IDENTIFIABILITY GUARANTEES IN BISCUIT", "content": "There is nothing in the objective of Eq. (1) itself that guarantees that the model will identify the causal variables from the observations. In BISCUIT (Lippe et al., 2023), the CRL framework we adopt, identifiability arises from two key assumptions: (1) each causal variable has a distinct \u2018interaction pattern,' meaning that the effect of Rt on zt is mediated by a latent binary mask, and (2) these interaction patterns vary over time. The first assumption is enforced by using a structured model family to model the transition pw(zt | zt\u22121, Rt). We incorporate this component from BISCUIT in our approach. These assumptions together ensure that causal variables are uniquely identifiable from the observed data. For a more detailed discussion on the assumptions, theoretical guarantees, and the structure of the transition model, we refer the reader to the original paper (Lippe et al., 2023)."}, {"title": "4 BUILDING A CAUSAL WORLD MODEL FROM CAUSAL REPRESENTATIONS", "content": "To integrate the CRL model with LLMs, we construct a Causal World Model (CWM) that takes actions in text format and states in image format and produces state representations in natural language."}, {"title": "4.1 LANGUAGE GROUNDING MODULES", "content": "To integrate the CRL model with LLMs, we introduce architectural components that transform the CRL model into a world model with a language interface. This section outlines the new components we introduce, enabling the model to process image states and text inputs, and produce text outputs.\nLanguage-Based Action Representations We replace the action encoding Rt in the CRL framework with a language-based representation Le(Lt), where Le embeds a natural language description Lt. This is implemented using an encoder-only language model (Reimers & Gurevych, 2019) with a trainable head, replacing the original action encodings in the CRL framework's transition model Rt = Le(Lt) (see also Section 4.2).\nDecoder The decoder G comprises two parts: the causal mapper and the state description generator. The causal mapper mo extracts causal variables C from the learned disentangled representations z. It first identifies which latent dimensions zi are most predictive for each causal variable Cj, then learns to perform the actual mapping. The state description generator s maps the estimated causal variables C to l, a natural language description of the state. Detailed implementations of these components are provided in Appendix F and G respectively."}, {"title": "4.2 PARAMETER ESTIMATION AND INFERENCE", "content": "In this section, we explain the estimation process for all model components and detail how the resulting model is applied during inference. We use the GridWorld environment as a running example to illustrate the process, though the same methodology applies to any environment.\nEstimation: Causal Encoder and Transition Model To estimate the model, we use image pairs {Xt} and corresponding action descriptions {Lt} in natural language, for example, \u201cyou toggled the cyan traffic light\" or \"you moved the blue car\". We first train an autoencoder to compress high-dimensional observations Xt into lower-dimensional latent representations Et = ey(Xt), in which, however, the causal variables will still be entangled. Then, analogously to Eq. 1, the conditional"}, {"title": "5 EXPERIMENTAL SETUP", "content": "We evaluate our framework using two distinct environments: a dynamic 8 \u00d7 8 gridworld and a static 3D-rendered kitchen (AI2-THOR) (Kolve et al., 2017). The GridWorld is dynamic, meaning the environment state can change even without agent actions, while the iTHOR kitchen is static, changing only in response to agent interventions. Our experiments focus on three key aspects: the effectiveness of text-based action representations, causal inference, and planning. Both environments feature various objects with causal variables representing their states and positions. Detailed descriptions of the environments are provided in Appendices A and B.\nFor each environment, we generated multiple datasets for training, evaluation, and in-context learning. The data generation process involves initializing the environment state and performing random, valid actions. Specific details about dataset sizes, in-context learning example generation, and self-evaluation reward generation for planning tasks are described in Appendix D."}, {"title": "5.1 ACTION REPRESENTATIONS", "content": "We investigate three action representation modalities:\n1. Coordinate-based (CB): Two-dimensional pixel coordinates indicating the position where the interaction was performed.\n2. Text-based (TB): Natural language descriptions generated using a rule-based system and expanded with a Probabilistic Context-Free Grammar (PCFG).\n3. Hybrid (HB): Combination of coordinate-based and text-based representations.\nWe hypothesize that the text-based action encoding is a) semantically richer, providing more infor-mation for the same or less effort to annotate the data, b) more flexible, enabling a language-based interface suitable for a generalist agent, and c) more robust, meaning that paraphrases or equivalent descriptions of the same action can still work with our model even if it was not specifically trained on them. This last point is crucial, as the LLM used at inference may deviate in its action description style from what was seen during training."}, {"title": "5.2 BASELINE", "content": "Our baseline uses the world model component from the Reasoning via Planning (RAP) methodology (Hao et al., 2023). This language model-based world model predicts the next state given the current state st, chosen action at, and context c:\nst+1 \u223c PLM(st+1 | st, at, c).\nThe baseline constructs a prompt at runtime that includes the environment description and dynamics, current state representation, chosen action, two relevant in-context learning (ICL) examples, and instructions for predicting the next state. This approach leverages the language model's pretrained knowledge while adapting to the specific task and environment dynamics. We ensure the relevance of the ICL examples by providing examples that match the current action and the object it is applied to.\nWe use LLAMA 3 (8B) (Dubey et al., 2024) as the planning agent quantized to 6 bits in the exl2 format. This baseline is a state-of-the-art approach to using language models for planning tasks and provides a fair point of comparison to assess the benefits of integrating causal representation learning. This allows us to isolate the impact of causal understanding in an otherwise comparable framework."}, {"title": "6 EXPERIMENTS AND DISCUSSION", "content": ""}, {"title": "6.1 EVALUATION OF TEXT-BASED ACTION REPRESENTATIONS", "content": "In this experiment, we demonstrate the effectiveness of representing actions in natural language for learning causal representations. We assess the induced state variables z by comparing them to ground-truth causal variables. Note that the model's decoder is not evaluated in these experiments.\nWe train our causal world model using each action modality (CB, TB, HB) across different sub-sample percentages of the training dataset, focusing on the low-data regime. Given sufficient data, models yield practically identical results across all 3 modalities but obtaining data in non-simulated environments is typically expensive. Performance is assessed using a standard CRL metric: R2 scores for the permutation \u03c0 that maximizes the diagonal of the R2 matrix between learned latent variables and true causal variables. This approach accounts for the fact that we learn causal variables up to permutation. Each experiment uses 3 seeds with distinct subsamples. A more comprehensive explanation of the training of the components of the CRL models used is presented in Appendix E."}, {"title": "6.2 CAUSAL INFERENCE PERFORMANCE", "content": "Our causal inference experiments evaluate both world models' ability to perform 1-step and N-step causal inference, i.e., predict the effects of actions (interventions) on the environment. In the 1-step case, given the current state and an action, the model predicts the new state. For N-step causal inference, we provide a sequence of actions and only the starting state and the world model applies each action to its previous prediction in a sequence. This differs from planning in that it focuses on the effect of a given sequence of actions rather than finding actions to reach a goal. The evaluation methodology is presented in Appendix H.\nTable 2 presents accuracies of causal inference for both models across different environments and step lengths. The causal world model consistently outperforms the baseline across all scenarios. In GridWorld, it maintains high accuracy (75.8%) even for 8-step inference, while the baseline's performance drops nearly to 0. The performance in iTHOR, while lower than in GridWorld, still shows a substantial improvement over the baseline.\nThe higher overall performance on GridWorld can be attributed to its simpler action space, object space, and causal graph, despite its dynamic nature. The baseline's lower performance in GridWorld compared to iTHOR may be due to the lack of visual input, which is less natural for language models in an artificial environment.\nTable 3 provides a detailed breakdown of the causal inference performance for specific actions and objects, based on the extended 1-step dataset of 3000 samples. In iTHOR, the causal world model excels at ToggleObject and OpenObject actions (95.7% and 92.6% accuracy), while struggling more with PutObject and PickupObject actions (50.6% and 43.1% accuracy). This discrepancy likely stems from the following; first, we model the three-dimensional coordinates as independent random variables while, in reality, they are dependent. Second, we model interventions using binary variables to estimate whether we performed an intervention or not. Performance could be improved by injecting inductive bias towards the continuous, three-dimensional nature of the underlying variable. However, this requires task specialization within the model and we chose to keep the proposed framework task-agnostic. The baseline model shows a different pattern, performing better on NoOp and PickupObject actions but struggling with PutObject actions.\nIn GridWorld, the causal world model demonstrates high accuracy across all action types, with particularly strong performance in changes to the state of the lights and no-action scenarios. The baseline model shows lower performance across the board, with its best performance on the No Action category.\nThese results highlight the causal world model's superior ability to reason about causal relationships, maintaining strong performance across different temporal scales, environments, and action types."}, {"title": "6.3 PLANNING", "content": "Methodology The planning experiments assess the model's ability to generate a sequence of actions to transform an initial state into a goal state. This involves exploring multiple possible action sequences and evaluating their effectiveness in reaching the goal. Unlike causal inference, planning requires considering long-term consequences and optimizing for a specific objective.\nOur framework adapts the Reasoning via Planning (RAP) methodology, with a key distinction: we employ a separate causal world model alongside a language model agent, rather than using a single language model for both roles. We use the same LLM as for the baseline planning agent (LLaMA 3).\nThe planning works as follows: The LLaMA 3 agent proposes possible actions based on the current state. The world model then simulates the actions' outcomes, predicting subsequent states. The agent then evaluates each state-action pair's quality and picks an action resulting in a new state. This cycle repeats, exploring multiple reasoning paths before converging on a final solution. For all N-step experiments, we use a search tree depth of N + 2. We use a modified version of the RAP-MCTS algorithm, presented in Appendix J.\nActions In Gridworld, there are three actions to toggle traffic lights (one per light) and one to perform no action. In iTHOR, we dynamically generate 10-15 possible actions, depending on the initial state. During planning, the models use their internal representations to determine possible actions. During evaluation, we use the external simulator (the same one used to generate the data) to execute the plan proposed by the agent. If an invalid action is proposed, during evaluation we default to performing no action.\nReward Design In line with the RAP methodology, we rely on the LLM's ability to judge the current state in relation to the goal. The Intuition reward is the unnormalized log probability of actions generated by the language model, given the current state and few-shot demonstrations. The Self-evaluation reward is the log probability of the token \u201cgood\u201d when asking the model to evaluate whether the proposed action is correct, given the current state and few-shot demonstrations.\nWe avoid using percentage-of-goals-reached rewards to maintain generality and applicability to problems that are not easily divisible into subgoals or subtasks. This choice ensures that our method remains applicable to a wide range of problems, including those where intermediate progress toward the goal is difficult to quantify and/or may not correlate directly with overall success."}, {"title": "6.3.1 PLANNING RESULTS AND DISCUSSION", "content": "Table 4 presents the planning results for both models across different environments and step lengths.\nThe causal world model consistently outperforms the baseline in both environments:\n\u2022 Success Rates: The causal model achieves significantly higher success rates, particularly in longer planning horizons. In iTHOR, it more than doubles the baseline's success rate for 2-step planning (0.58 vs 0.25) and quadruples it for 4-step planning (0.44 vs 0.11).\n\u2022 Efficiency: For successful trajectories, the causal model takes fewer steps on average to reach the goal state, indicating more efficient planning.\n\u2022 Scalability: While both models show decreased performance as the number of steps in the ground truth increase, the causal model degrades more gracefully. In GridWorld, it maintains a 0.42 success rate for 8-step planning, compared to the baseline's 0.06.\n\u2022 Consistency: Both models perform better in GridWorld compared to iTHOR, likely due to the lower complexity and more constrained action space. However, the causal model shows more consistent performance across both environments.\nAn interesting observation is the sub-N performance in N-step planning scenarios. This phenomenon arises from two key factors in our experimental design which renders the parameter N an upper bound of the steps needed to achieve the goal state. In the static iTHOR environment, some actions can negate others (e.g., toggling the toaster twice is equivalent to performing no action), allowing for shorter paths to the goal state. In addition to this phenomenon, in the dynamic GridWorld environment, the inherent movement of entities (e.g., cars moving when facing a green light) can sometimes lead to the goal state in fewer steps than the upper bound. This sub-N performance highlights our models' ability to find efficient paths to the goal state, often outperforming the original trajectories used to generate the planning problems.\nThese results demonstrate that integrating causal representation learning with language models creates a more effective framework for planning in both simple dynamic and complex static environments. The causal world model's ability to capture and utilize the underlying causal structure leads to more accurate and efficient planning, even as task complexity increases.\nThe performance gap between the models is particularly noteworthy given the deliberate avoidance of task-specific heuristics and rewards, suggesting that the causal approach captures fundamental aspects of the environments\u2019 causal structures, leading to better generalization and robustness."}, {"title": "7 CONCLUSION", "content": "In this work, we introduced a framework that integrates causal representation learning with language models, enabling causally-aware reasoning and planning in interactive environments. Our approach combines the structured causal understanding of CRL with the flexible interface of language models, demonstrating superior performance in causal inference and planning tasks across two environments. The causal world model consistently outperforms baselines, showing improved accuracy, efficiency, and scalability as task complexity increases. Our exploration of text-based action representations reveals potential advantages in low-data regimes, suggesting implications for more flexible and generalizable AI systems. While our current experiments focus on relatively simple environments, the framework is designed to extend to more complex scenarios as CRL methods advance. Future work could explore applications to real-world environments, improve the interpretability of learned causal world models, and develop techniques independent of labeled causal variables."}, {"title": "A GRIDWORLD ENVIRONMENT", "content": "The gridworld environment is a dynamic environment of size H \u00d7 H, where \u0397 \u2208 N denotes both the height and width of the grid. The top left corner of the grid is defined to be (0, 0). The environment consists of C underlying causal variables that interact based on the actions taken by the agent and the dynamics of the environment. The environment contains three types of entities: vehicles v \u2208 V, obstacles o \u2208 O, and traffic lights tl \u2208 TL. Each entity has a fixed corresponding attribute, implemented as a color, which differentiates it from other objects within the same entity class.\nThe traffic lights are positioned in the grid, and each vehicle is facing a specific traffic light. The positions of the traffic lights are fixed and immutable, with coordinates (xtl, Ytl), where xt1, Ytl \u2208 {0,1,..., \u0397 \u2212 1}. Each traffic light has a state st\u2081 \u2208 {red, green}. The obstacles have positions (xo, yo) in the grid, where xo, yo \u2208 {0,1,..., H \u2013 1}, and these positions can only change through interventions performed on them. The vehicles have positions (xv, Yv) in the grid, where xv, Y\u03c5 \u0395 {0,1,..., \u0397 \u2212 1}, and an orientation \u03b8\u2082 \u2208 {up, down, left, right}. The vehicle positions change according to the following dynamics:\nLet v be a vehicle at position (xv, Yv) with orientation \u03b8v, associated with a traffic light tl at position (Xtl, Ytl). We say that the vehicle v is facing the traffic light tl if and only if one of the following conditions is satisfied:\n1. \u03b8v = up and xv = Xti and Yv > Ytl\n2. \u03b8\u03c5 down and xv = Xtl and Yv < Ytl\n3. 0 = left and yv = yt\u0131 and xv > Xtl\n4. 0 = right and yv = Yt\u0131 and xv < Xtl\nIf the vehicle v is facing the traffic light tl, it will move forward to the cell (x', y') at the next timestep if and only if all of the following conditions are satisfied:\n1. The traffic light tl has a state of green, i.e., stl = green.\n2. There are no obstacles in the cell (x', y'), i.e., \u2204 o \u2208 O : (xo, yo) = (x', y').\n3. There are no traffic lights in the cell (x', y'), i.e., \u2204 tl \u2208 TL : (xtl, Ytl) = (x', y').\n4. The cell (x', y') is within the grid boundaries, i.e., 0 < x' < H and 0 \u2264 y', < H.\nThe new position (x', y') is determined by the vehicle's current position (xv, Yv) and orientation \u03b8v as follows:\n(x\u2032, y\u2032) = { (xv, Yv \u2212 1) if \u03b8v = up , (xv, Yv + 1) if \u03b8v = down, (xv \u2212 1, yv) if \u03b8v = left , (xv + 1, yv) if \u03b8v = right(3)\nInterventions The intervention process follows a specific sequence: first, a step in the environment dynamics is executed; then, an intervention is applied; finally, a snapshot of the resulting state is captured. Interventions can modify traffic light states, alter obstacle positions, or move a vehicle forward. Spatial interventions on obstacles and vehicles are constrained to single-cell displacements; for obstacles, the direction is stochastic, while for vehicles, it is deterministically forward. Vehicle intervention is further constrained by the absence of obstacles or traffic lights in the target cell, adherence to environment boundaries, and the corresponding traffic light displaying a red signal. A no-operation (NOOP) intervention is also permissible. This tripartite sequence\u2014environmental progression, intervention, and state documentation-constitutes a complete intervention cycle. These interventions correspond to regime variables Rt, which are then represented using natural language.\nCausal Variables The causal variables in the gridworld environment are the positions of the vehicles (xv, Yu), the positions of the obstacles (x0, yo), and the states of the traffic lights stl."}, {"title": "B ITHOR KITCHEN ENVIRONMENT - EMBODIED A\u0399", "content": "The iTHOR (Kolve et al., 2017) kitchen environment is based on the FloorPlan10 dataset, featuring a static 3D-rendered kitchen. The robot's position remains fixed in front of the kitchen counter. The environment consists of C underlying causal variables that interact based on the actions taken by the agent. The environment contains three types of entities: movable objects m\u2208 M, fixed interactive objects f \u2208 F, and receptacles r \u2208 R. Movable objects include a plate with a potato and an egg. Fixed interactive objects comprise a microwave, stoves, cabinet, and toaster. Receptacles include the counter, microwave (when open), and pan (for the egg). Each object has a state so \u2208 So, where So is the set of possible states for object o. For binary state objects (e.g., microwave, cabinet), So = open, closed or active, inactive. For movable objects, So includes their position (xm, Ym, 2m) in the 3D space and a binary pickup state. The set of possible actions A includes:\n\u2022 ToggleObject(0): For o \u2208 {microwave, stoves, toaster}\n\u2022 OpenObject(0): For \u09e6 \u2208 {microwave, cabinet}\n\u2022 PickupObject(m): For m\u2208 M\n\u2022 PutObject(m, r): For m \u2208 M,r \u2208 R\n\u2022 MoveObject(m): For m\u2208 \u039c\n\u2022 NoOp: No action performed\nThe availability of actions depends on the current state of objects. For example:\nToggleObject(microwave) is valid iff Smicrowave = closed(4)\nOpenObject(microwave) is valid iff Smicrowave = inactive(5)\nThe regime variable Rt \u2208 [0, 1]2 represents the normalized click-location on the image to select the object for interaction. Let Io be the set of pixels belonging to object o in the current frame. Then:\nRt = 1/H \u00d7W \u03a3(x, y), where (x, y) \u223c Uniform(Io)(6)\nwhere H and W are the height and width of the frame respectively. The causal variables C = C1, ..., Cc in this environment correspond to the states and positions of objects. Binary state variables (e.g., Cabinet-Open, Microwave-Active) take values in 0, 1, while position variables (e.g., Egg-Pos-x) take continuous values in [0, 1], normalized to the environment's dimensions. Observations are generated as high-resolution images Xt \u2208 R512\u00d7512\u00d73, then downsampled to X't \u2208 R256\u00d7256\u00d73 using bilinear interpolation."}, {"title": "C TEXT-BA"}]}