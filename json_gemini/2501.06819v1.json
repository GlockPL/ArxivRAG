{"title": "A Study on Educational Data Analysis and\nPersonalized Feedback Report Generation Based on\nTags and ChatGPT*", "authors": ["Yizhou Zhou", "Mengqiao Zhang", "Yuan-Hao Jiang", "Xinyu Gao", "Naijie Liu", "Bo Jiang"], "abstract": "This study introduces a novel method that employs tag annotation coupled with the\nChatGPT language model to analyze student learning behaviors and generate per-\nsonalized feedback. Central to this approach is the conversion of complex student\ndata into an extensive set of tags, which are then decoded through tailored prompts\nto deliver constructive feedback that encourages rather than discourages students.\nThis methodology focuses on accurately feeding student data into large language\nmodels and crafting prompts that enhance the constructive nature of feedback. The\neffectiveness of this approach was validated through surveys conducted with over\n20 mathematics teachers, who confirmed the reliability of the generated reports.\nThis method can be seamlessly integrated into intelligent adaptive learning systems\nor provided as a tool to significantly reduce the workload of teachers, providing\naccurate and timely feedback to students. By transforming raw educational data\ninto interpretable tags, this method supports the provision of efficient and timely\npersonalized learning feedback that offers constructive suggestions tailored to\nindividual learner needs.", "sections": [{"title": "Introduction", "content": "The development of Large Language Models (LLMs) such as GPT-4 has significantly impacted\nvarious domains, especially education [1]. These models are adept at understanding and generating\nnatural language, offering significant capabilities in text generation, question-answering, and more [2].\nIn particular in education, LLMs are poised to enhance learning experiences and provide immediate\nand constructive feedback to students. However, effectively integrating the extensive and complex\ndata typical of educational settings into these models presents notable challenges [3].\n\nImmediate feedback is a strong reinforcement mechanism, underscored by Skinner's operant condi-\ntioning theory, which asserts that immediate rewards or corrections can significantly influence future\nbehavior by reinforcing desired learning activities [4, 5]. This is particularly vital in educational\ncontexts, where timely feedback can help students quickly recognize and correct mistakes, fostering\na better understanding and retention of material [6]. In addition, personalized feedback according\nto individual learning trajectories allows personalized educational guidance, making learning more\neffective.\n\nHowever, while immediate feedback is invaluable, the deployment of LLMs such as GPT-4 to\ngenerate comprehensive educational feedback presents significant challenges, particularly in data\nhandling. Traditional LLM applications tend to focus on singular tasks, such as grading individual\nquestions or essays, providing feedback limited to specific prompts without a broader contextual\nunderstanding. This narrow approach often misses the need for cumulative feedback that offers\nconstructive insight over a learning period. To address this, we propose a tag annotation method that\nrefines raw educational data into a structured format of predefined tags. These tags represent essential\nattributes of the student data comprehensively, enabling the model to process vast information\nmore efficiently and generate feedback that is both integrative and strategically supportive of the\nstudent's educational journey. This method allows for the production of rich, context-aware feedback\nthat supports holistic student development, overcoming the limitations of traditional single-point\nassessments.\n\nOur research utilizes data from an adaptive learning system tested in a Shanghai primary school,\ncaptures detailed records of students' performances across various parameters including correctness,\ndifficulty levels, knowledge categories, ability levels, and task completion time. By transforming\nthese data points into tags, we enable the LLM to generate nuanced feedback. This feedback is\nnot only precise but also supportive, encouraging students to engage positively with their learning\nmaterial. This approach of tag annotation simplifies the model's data processing tasks and enhances\nthe relevance of its outputs, making LLMs more practical for real-world educational applications. It\nmarks a significant advance in the integration of advanced AI technologies into education, paving the\nway for scalable and impactful personalized learning feedback."}, {"title": "Related Work", "content": ""}, {"title": "Traditional Feedback Generation with LLMs in Education", "content": "The integration of Large Language Models like ChatGPT into education has highlighted the potential\nfor automated feedback. Traditionally, such models provide immediate responses to individual tasks,\nsuch as answers to questions, essays, or coding projects [6]. However, they have yet to fully address\nthe challenge of analyzing large datasets for more comprehensive feedback. There is a growing need\nfor methods that can efficiently encode broader educational data into these models. This would enable\nthe generation of detailed, personalized feedback reports reflecting the student's learning journey\nover time."}, {"title": "Effective Reporting in Educational Feedback", "content": "Research indicates that the most effective feedback reports are those that provide holistic insights into\na student's learning progress, addressing both strengths and areas for improvement. Kobus (2007)\nfound that reports combining detailed analysis with positive reinforcement encourage students more\neffectively [7]. These studies underscore the importance of balanced feedback that motivates students\nwhile guiding them toward academic improvement. Periodic, comprehensive feedback reports are\nmore beneficial as they allow for adjustments in teaching strategies and student learning approaches,\naligning more closely with ongoing educational needs [8]. Additionally, the tone of the feedback\nis crucial; emphasizing constructive and supportive language helps maintain a positive learning\nenvironment [7]. Avoiding overly critical or negative remarks is essential to prevent discouraging\nstudents. Nurturing a student's confidence and interest in learning through supportive feedback\nsignificantly enhances learning outcomes [9], which not only corrects misconceptions but also\nbolsters the student's abilities and self-esteem, fostering a conducive atmosphere for continuous\nlearning and growth."}, {"title": "Experiment and Data Analysis", "content": ""}, {"title": "Experimental Procedure and Data Acquisition", "content": "The study collected experimental data from two primary school classes in Shanghai, utilizing a three-\ndimensional adaptive learning system developed by the Lab for Artificial Intelligence in Education\nat East China Normal University.This system addresses knowledge, ability, and affective attitude,\nproviding a variety of multiple-choice questions, each with two possible outcomes: correct (1) and\nincorrect (0). Students engaged in the system's online learning sessions during designated class\nperiods on Monday and Wednesday afternoons each week. The experiment was carried out within a\nreal online educational platform, specifically the aforementioned adaptive learning system, which\nincluded advanced data tracking modules. This setup not only ensured the accurate collection of\ncomprehensive learning data but also strictly adhered to data protection measures to safeguard student\nprivacy. Specifically, the final dataset provided by the system allowed for the careful extraction of data\non students' performance in multiple-choice questions, including their accuracy rates. Additionally,\nwe gathered data on the knowledge categories, ability levels associated with each question, and the\ntime students took to complete these questions. Conducting the experiment during afternoon class\nperiods ensured students were engaged and not distracted by other academic responsibilities, thus\nreflecting their true learning capabilities more accurately. This careful arrangement provided a solid\nfoundation for subsequent analyses, aiming to evaluate the effectiveness of tag-based feedback and\nadaptive learning strategies employed."}, {"title": "Data Processing and Tag Annotation", "content": "Effective use of educational data in generating meaningful feedback through Large Language Models\n(LLMs) like GPT-4 requires thorough preprocessing and organization of raw data into a structured\nand interpretable format. The initial dataset included a vast array of knowledge categories and ability\nlevels-more than fifty and thirty distinct types, respectively. To enhance model practicality, these\nwere consolidated into manageable classes.\n\nKnowledge categories were refined into six primary groups: Calculations of Speed and Time, Ge-\nometric Shapes and Properties, Data Statistics and Probability, Algebra and Functions, Arithmetic\nOperations and Properties. This categorization aimed to cover a broad spectrum of subjects while\nsimplifying complexity. Ability levels were similarly reduced and organized into six groups: Practical\nMathematical Application Skills, Data Organization and Statistical Skills, Computational Skills,\nGeometric Thinking Skills, Reasoning and Logical Thinking, and Innovative and Abstract Think-\ning. These groups reflected the core skills essential for academic success in structured learning\nenvironments.\n\nA major challenge was managing multiple entries per student per question, often including brief,\nnon-essential attempts likely from navigational actions rather than genuine problem-solving efforts.\nTo resolve this, only the longest duration of attempt per question was retained, considered to represent\nthe most substantial effort. Additionally, entries with zero duration were discarded, presumed to be\nguesses or inactive interactions, thus providing little educational value."}, {"title": "Tag Parsing and Report Generation", "content": "The streamlined data led to the development of a tag annotation system, organized into three primary\ncategories: Performance Tags, Knowledge Domain Tags, and Ability Level Tags. Performance Tags\n(12 tags) were based on a combination of difficulty level (1-3), accuracy, and speed. Accuracy was\nsplit into 'adequate' (above 65%) and 'struggling' (below 55%). Speed was assessed by comparing\neach student's performance against their peers; top or bottom performers were tagged if the sample\nsize exceeded 40, otherwise, the fastest or slowest 50% received the tag.\n\nThe conclusion of our methodology involves transforming processed data into structured tags and\ncombining these with the GPT-4 model to generate personalized educational feedback reports. After\ndata refinement and tag annotation as outlined in the previous sections, we create a complete dataset"}, {"title": "Evaluation", "content": "The potential of large language models (LLMs) like GPT-4 for educational feedback has been\ndemonstrated through an evaluation with primary level mathematics teachers. The assessment,\nconducted via a questionnaire, examined five key aspects of the generated reports - comprehensibility,\npracticality, motivation, clarity, and organizational structure. Each aspect was rated on a scale from 0\nto 10, with a maximum score of 50.\n\nWe received 63 questionnaires, discarding three for unjustifiably low scores and another 28 for\nnon-genuine perfect scores, leaving 32 valid responses for analysis. Overall, the average scores\nexceeded 7 points in most dimensions, indicating that the reports were well-received. Teachers found\nthem helpful for understanding and supporting the learning process, particularly in identifying student\nstrengths and challenges.\n\nThe evaluation highlighted the need for simpler language and better data presentation, as clarity\nreceived the lowest average score of 6.59. Understanding Level had a median score of 7.5, indicating effective capture\nof students' strengths and challenges. Practicality scored slightly lower, reflecting varied teacher\nopinions on the suggestions' applicability. The Motivation effect, with a median near 8, showed a\npositive impact, despite a few low outliers. Clarity and Organizational Structure had median scores\nof around 7 and above 7.5 respectively, suggesting the need for clearer reports and confirming the\nreports' logical structure facilitated information assimilation.\n\nIn summary, teachers affirmed the reports' effectiveness in highlighting understanding, practicality,\nand organizational structure, with a recommendation to improve clarity to avoid misinterpretations.\nSuch insights are crucial as they guide the refinement of algorithms that underpin report generation.\nThis reaffirms the viability of using LLMs like GPT-4 to provide personalized educational feedback,\nenhancing the learning experience."}, {"title": "Discussion and Implications", "content": "This study has demonstrated the potential of the GPT-4 large language model in generating person-\nalized educational feedback reports in the field of education. Most teachers gave positive reviews\nof the learning reports produced, particularly in terms of analyzing students' learning strengths,\nchallenges, and providing specific suggestions. These feedback reports can serve as a powerful tool\nto help teachers understand and support students' learning processes more effectively, echoing the\nimportance of immediate feedback in education.\n\nThe lower evaluations for report clarity highlight areas for improvement, suggesting that future report\ngeneration should focus more on the simplicity of language and the clarity of data presentation to\nensure that both teachers and students can understand them better. Additionally, the presence of low\noutliers also indicates potential shortcomings in how the reports motivate students to improve their\nlearning methods.\n\nThe significance of this study lies in providing a new perspective and method for using AI technology\nto support personalized educational feedback. Through the process of tag annotation and tag parsing,\nit is possible to transform vast amounts of complex learning behavior data into comprehensive and\naccurate input.\n\nFuture adjustments to the prompts can also be made to ensure the report outputs better meet the\nexpectations and needs of math teachers in the classroom, effectively helping to reduce teacher\nworkload and providing students with timely, personalized analysis of their learning status."}, {"title": "Conclusion", "content": "This study explored a method of using tag annotation and tag parsing to implement the large language\nmodel ChatGPT in generating personalized, real-time educational feedback reports, confirming its\npotential to play a key role in personalized education. By converting students' multidimensional\nlearning data into structured tags, the model is capable of generating precise, personalized learning\nreports. These reports not only summarize students' performance but also provide specific strategies\nfor improvement."}]}