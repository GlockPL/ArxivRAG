{"title": "HUMAN PREFERENCES IN LARGE LANGUAGE MODEL LATENT SPACE: A TECHNICAL ANALYSIS ON THE RELIABILITY OF SYNTHETIC DATA IN VOTING OUTCOME PREDICTION", "authors": ["Sarah Ball", "Simeon Allmendinger", "Frauke Kreuter", "Niklas K\u00fchl"], "abstract": "Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.", "sections": [{"title": "1 Introduction", "content": "With the release of ChatGPT in November 2022, the world has seen a spike in interest in large language models (LLMs). Many academic disciplines, as well as the business world, wonder if and how they can integrate LLMs to their benefit. One emerging\u2014and highly debated-topic is the usage of LLMs for (public) opinion research. The idea is that one can leverage LLMs to substitute for surveying humans. Yet, the question remains as to how valid and reliable it is to substitute humans with LLMs. Previous research mainly focuses on comparing LLM predictions based on personas to a gold standard survey prediction for these personas. The results of such analyses are mixed [Argyle et al., 2023, Kim and Lee, 2023], revealing various problems, e.g., prediction instability that occurs with slight formulation changes in the prompt [Bisbee et al., 2023] and performance differences across national and linguistic contexts [von der Heyde et al., 2024a]. While such approaches might give first insights into how well LLMs can predict general questions of interest, we lack a deeper understanding of how \u201copinion formation\u201d works on a technical level in LLMs and how reliable the resulting synthetic data is for answering human-related questions of interest. Based on this, our article addresses two central research questions:"}, {"title": "RQ1:", "content": "How well does LLM-generated synthetic data mimic the distribution of human answers in survey-like questions for different demographic subgroups in their latent space?"}, {"title": "RQ2:", "content": "How is prompt instability reflected in the models' latent space?\nTo address these questions, we focus on the use case of predicting election outcomes with LLMs in the German multi-party context. The election context is chosen not only for its societal relevance and its popularity as a testbed in recent research on LLM-human substitutability [Argyle et al., 2023, von der Heyde et al., 2024a, Yu et al., 2024], but also because elections are commonly used for evaluating the quality of survey data across different vendors or data collection modes, providing a rare benchmarking opportunity in survey research. We further choose the German multi-party context as it allows for multiple party comparisons, increasing the robustness of our results.\nIn our experiments, we analyze the latent space of LLMs, focusing on mechanistically understanding persona-to-party mappings. To do so, we develop a probe-based methodology to systematically identify model-specific value vectors\u2014Multi-Layer Perceptrons (MLPs)\u2014associated with political affiliations. This allows us to examine how demographic attributes\u2014such as age, gender, and ideological leaning\u2014interact with latent political structures within LLMs. Our results reveal that LLMs fail to replicate the entropy observed in real-world survey data, as their persona-to-party mappings exhibit low differentiation across demographic subgroups. We further explore prompt sensitivity by first replicating previous findings that small meaning-preserving variations in persona descriptions can alter voting predictions, underscoring the instability of LLM-generated survey data. Next, we demonstrate that, for certain models, higher entropy in the persona-to-party mapping correlates with increased prompt sensitivity. However, we also observe the opposite relationship in other models.\nOverall, our study provides a technical foundation to assess the usability and reliability of synthetic LLM data, exposing fundamental limitations that practitioners must address before relying on LLMs for public opinion research, social science experimentation, and computational behavioral modeling. We preregistered our study on the Open Science Framework\u00b9 , and the code is available at GitHub2."}, {"title": "2 Related Literature", "content": "Using LLMs as substitutes for humans. The advent of large language models (LLMs) has sparked significant interest regarding their potential to serve as substitutes for human respondents [Argyle et al., 2023]. This question is especially relevant for survey researchers in the social sciences, who are investigating whether responses generated by LLMs can reliably resemble those provided by humans in surveys [Argyle et al., 2023, Bisbee et al., 2023, Dominguez-Olmedo et al., 2025, Park et al., 2024, Qu and Wang, 2024, von der Heyde et al., 2024b, Wang et al., 2024]. Similar inquiries have emerged in fields such as market research [Brand et al., 2023, Sarstedt et al., 2024], annotation tasks [T\u00f6rnberg, 2023, Ziems et al., 2024], experiments in psychology and economics [Aher et al., 2023, Xie et al., 2024], and human-computer-interaction [H\u00e4m\u00e4l\u00e4inen et al., 2023, T\u00f6rnberg, 2023], among others. The findings from these investigations are mixed. Some studies suggest that LLMs can reasonably approximate the average outcomes of human surveys [Argyle et al., 2023, Bisbee et al., 2023, H\u00e4m\u00e4l\u00e4inen et al., 2023, T\u00f6rnberg, 2023, Brand et al., 2023, Xie et al., 2024], while others highlight significant limitations, particularly in their inability to accurately represent the opinions of diverse demographic groups [Santurkar et al., 2023, von der Heyde et al., 2024b, Sarstedt et al., 2024, Qu and Wang, 2024, Dominguez-Olmedo et al., 2025]. However, a common limitation across these studies is their focus on surface-level comparisons, i.e., matching LLM output to human survey responses without delving into the underlying mechanisms of how opinions are encoded and represented within the models' latent spaces. We address this gap by studying how personas are mapped to opinions as well as what the inherent limitations are in eliciting specific knowledge from these models.\nPrompt sensitivity. By systematically introducing subtle changes to the prompt format, previous studies have shown that LLM output is highly sensitive to prompt changes, thereby influencing downstream evaluations [Leidinger et al., 2023, Mizrahi et al., 2023, Chatterjee et al., 2024, Voronov et al., 2024, Zhuo et al., 2024]. Articles most closely related to our study are Sclar et al. [2023] and Zhu et al. [2023], which focus on explaining LLM prompt sensitivity next to establishing that it exists. Sclar et al. [2023] analyze how changes in the formatting of the prompt without semantic changes lead to large performance differences. They further show that prompt embeddings of different but equivalent formats are distinguishable using a trained classifier, implying that prompt formats transform the output probability distribution, yielding different predictions. Zhu et al. [2023] design \u201cattacks\" on the character, word, sentence, and semantic level to mimic user errors. They again find significant performance differences induced by the subtle prompt changes. The study further examines why LLMs are vulnerable to adversarial inputs by analyzing their attention weights"}, {"title": "3 Models and Data", "content": "Model selection. For our experiments we use both base and aligned models of different model families and sizes, see Table 1. These models are developed by teams across the world and fulfill the white-box criteria, which is a requirement for studying their latent space.\nReal world comparison. In order to compare our model predictions to real data, we use the German Longitudinal Election Study (GLES) [GESIS \u2013 Leibniz Institute for the Social Sciences, 2024]. This representative survey captures insights about German citizens' political attitudes, preferences and behaviours and is a widely used gold standard [Schmitt-Beck et al., 2010]. As a baseline, we choose the cross-sectional survey of the year 20213, for which the GLES asks about voting decisions in the respective federal elections and also captures our variables of interest. To obtain a representative comparison baseline for our LLMs we weight the data with a socio-demographic weight that aligns the distributions to the marginal distributions of the 2021 Microcensus.\nPersonas. For the construction of the personas, we follow previous literature [von der Heyde et al., 2024a] by combining political science theory for identifying voting predictors and representative surveys for extracting plausible values for these predictors. Hence, our personas are both theory- and data-driven. Concretely, we select the variables age, gender, education, hhincome, employment, political orientation, and whether a person lives in East or West Germany and combine them in a prompt\u2074. We vary the values for the different variables while holding the prompt structure fix. Furthermore, to account for LLM models' prompt sensitivity, we paraphrase the prompts. Thus, an example persona instantiated via an LLM prompt reads as follows:\nI am {age} years old and {gender}. I have {education}, a {hhincome} household net income per month, and I am {employment}. Ideologically, I lean towards the position {left leaning}. I live in {east germany}. If the elections were held in {year of election}, which party would I vote for? I vote for the party\nProbe generation. To train our probe, we use the German \"Wahl-o-Mat\u201d data [Bundeszentrale f\u00fcr politische Bildung, 2025]. The Wahl-o-Mat is an online questionnaire, which consists of short political statements based on party manifestos to which interested citizens can give their agreement (strong agree to strong disagree). Based on the user's answers, the tool provides a voting recommendation. For all short political statements that users see, each party provides an opinion to give more context to the question of interest. We extract this opinion for each Wahl-o-Mat item for German and European elections from 01/2021 until 12/2024. The parties of interest are, in alphabetical order, the Alternative f\u00fcr"}, {"title": "4 Methodology", "content": "Understanding how LLMs encode and generate synthetic survey responses necessitates to investigate persona-to-party mappings within the models' latent space. Building upon prior research, our methodology integrates trained probes to systematically identify model-specific representations of political ascriptions, thereby offering insights into the underlying value vectors. As depicted in Figure 1, our methodology depicts how LLM architectures encode voting preferences compared to historical human preferences from GLES."}, {"title": "4.1 Technical Preliminaries", "content": "Each transformer model [Vaswani et al., 2023] consists of transformer blocks in which multihead-attention (MHA) and multilayer perceptrons (MLP) update the residual stream representation ($x$) in each layer $l$ to obtain an updated representation $x^{l+1}$ (bias terms omitted for brevity) [Elhage et al., 2021]:\n$x^{l+1} = x^l + \\text{MLP}'(x^l + \\text{MHA}'(x^l)), l = 1, 2, ..., L$\nBased on [Geva et al., 2022] we can further decompose each MLP into two linear transformations (note that we write $x = x^1$ for brevity):\n$\\text{MLP}'(x^l) = f(W_k x^l) W_v,$\nwhere $f$ is a non-linear activation function and $W_k, W_v \\in \\mathbb{R}^{d_{mlp} \\times d}$. Hence, each value vector $v_i$ in column $i$ of $W_v$ is weighted by a vector of coefficients $m^i := f(W_k x^l) \\in \\mathbb{R}^{d_{mlp}}$. Noting $k_i$ as the key vector of row $i$ in $W_k$, one can write:\n$\\text{MLP}'(x^l) = \\sum_{i=1}^{d_{mlp}} m^i v_i = \\sum_{i=1}^{d_{mlp}} f(k_i x^l) v_i.$\nFrom this equation, [Geva et al., 2022] interpret an MLP update to the residual stream as sub-updates, consisting of weighted value-vectors. They further show that in each sub-update, $v_i$ either de- or increases the probability of a token $t$ to be generated:"}, {"title": "4.2 Constructing a probe for identifying party-related MLP value vectors", "content": "We aim to extract value vectors from the intermediate layers of LLMs, as these layers capture conceptual structures and high-level semantic representations more effectively than final layers, which are predominantly specialized for next-token prediction [Panickssery et al., 2023]. By focusing on these layers, we seek to uncover how specific residual stream patterns correlate with political biases and party affiliation in LLMs. To achieve this, we train linear probes that predict the party on the basis of the residual stream of layer $l$. Similar to [Lee et al., 2024], these probes help identify value vectors that promote tokens linked to specific partys. The probes are trained as binary classifiers, distinguishing between residual streams corresponding to a specific party $n$ ($y = 1$) and all others ($y = 0$). The training loss is weighted to mitigate class imbalance:\n$L = -w_1 y \\log(\\hat{y}) \u2013 (1 \u2013 y) \\log(1 \u2013 \\hat{y})$\nwhere $w_1$ represents the weight for the positive class. The probe function follows:\n$P(n|\\overline{x}^l) = \\text{softmax}(W_n \\overline{x}^l), \\quad W_n \\in \\mathbb{R}^{d \\times m}, \\quad l \\in [0.6L, 0.9L],$\nwhere $W_n$ represents the learned parameters, $n$ symbolizes a party from $N = \\{n_1, n_2, ...\\}$, and $\\overline{x}^l$ is the mean residual stream of a selected layer $l$ from the interval $[0.6L, 0.9L]$. The model consists of a linear layer with dropout, optimized using Adam with cosine annealing. The training data consists of opinion statements. Each statement is paired with the corresponding party opinion to construct prompts. The language model aims to predict the party from the statement-opinion pairs, and residual streams are recorded at all layers and token positions (= sequence). After training, we extract value vectors per party by identifying MLP weights most aligned with the trained probe weights. These vectors are selected based on cosine similarity, where $W_{\\text{probe}}$ represents the trained probe weights and $v_{i,l}$ denotes the value vector:\n$\\cos(\\theta) = \\frac{W_{\\text{probe}} \\cdot v_{i,l}}{\\|W_{\\text{probe}}\\| \\|v_{i,l}\\|}, \\quad i \\in [1, d_m], \\quad l \\in [1, L].$\nWe define the set of top 20 value vectors per party $n$ as:\n$V_n = \\{v_{i,l} \\mid \\cos(\\theta) \\text{ is among the top 20 for } i \\in [1, d_m], \\text{ and } l \\in [1, L]\\}.$\nThe selection criterion ensures that only the 20 most aligned value vectors are retained per layer, as we observe a drop in cosine similarity beyond this threshold."}, {"title": "4.3 Analyzing the mapping between personas and the identified party-related value vectors", "content": "Personas are defined by key attributes (such as age, gender, and political attitudes) with systematically varied values and paraphrased prompt variants to ensure robustness. These controlled inputs allow us to analyze how different demographic and ideological configurations affect model residual streams in response to political prompts. To investigate the interaction between personas and the identified value vectors $v$, we compute their residual stream of responses to persona prompts. Specifically, we measure the contribution of scaling factors $m_p \\in \\mathbb{R}^N$ (cf. eq. (4)) by evaluating the residual stream across all layers for each value vector $v$. The scaling factor of each persona prompt is computed as:\n$m_p = \\sum_{i,l} m_{i,l} \\frac{\\cos(\\theta)}{\\sum_{i'} \\cos(\\theta)} \\mathbb{1}\\{v \\in V\\}, \\quad i \\in [1, d_m], \\quad l \\in [1, L],$\nwhere $m^i_l$ represents the scaling contribution of value vector $v$ located at layer $l$ and model dimension $i$, and $\\cos(\\theta)$ denotes its alignment with the learned probe weights. Using this approach, we derive scaling behavior $m_p^n$ for each"}, {"title": "4.4 Comparing survey and LLM distributions", "content": "To compare the characteristics of the LLM persona-to-party mapping with historical human preferences, we calculate the normalized entropy $H_{norm}(\\psi)$ for the distribution $\\psi$ aggregated over all personas $p \\in P = \\{p_1, p_2, ...\\}$ and prompt variants $j \\in J = \\{j_1, j_2, . . . \\}$.\n$H_{norm}(\\psi) = \\frac{H(\\psi)}{\\log_2 N} = \\frac{-\\sum_{n \\in N} p(n) \\log_2 p(n)}{\\log_2 N}$\nwith $\\psi$ defined as:\n$\\psi = \\frac{\\sum_{p \\in P} \\sum_{j \\in J} m_{p,j}^n}{\\sum_{p \\in P} \\sum_{j \\in J} \\sum_{n' \\in N} m_{p,j}^{n'}}, \\qquad \\psi \\in [0, 1]^N.$\nWhile traditional LLM-based political inference often focuses only on next-token prediction, this value-based distribution extends beyond single-token outputs, capturing a full probability distribution over all parties $n$ for each persona $p$. This allows for a more structured comparison with real-world survey data, as it reflects not just the most likely choice, but the entire spectrum of voting preferences inferred from the LLM's internal representations. By comparing $\\psi$ with observed human voting distributions, we can assess whether LLMs replicate the variance and entropy observed in real-world political behavior or exhibit systematic biases in persona-to-party mappings."}, {"title": "4.5 Analyzing prompt sensitivity", "content": "The normalized entropy $H_{norm}(.)$ can be decomposed by considering the distribution $\\psi_g$ for a specific group $g \\in G = \\{\\text{female}, \\text{college}, . . . \\}$, which defines a subset of personas $P_g \\subset P$. Thus, $H_{norm}(\\psi_g)$ characterizes the entropy behavior within the persona distribution for a given group. Similarly, the distribution $\\psi_j$ is constructed based on a subset of personas $P_j \\subset P$ corresponding to prompt variant $j$, where each subset satisfies $P_{j,g} = P_g \\cap P_j \\neq \\emptyset, \\forall g, j$.\nTo assess how entropy varies across different prompts $j$, we examine the relationship between entropy and prompt sensitivity using the Wasserstein distance $W(\\psi_{j,g}, \\overline{\\psi}_{j,g})$, which measures the discrepancy between the distribution $\\psi_{j,g}$ for prompt variant $j$ and its barycenter $\\overline{\\psi}_{j,g}$:\n$\\overline{\\psi}_{j,g} = \\frac{1}{\\|J\\|} \\sum_{j \\in J} \\psi_{j,g}, \\qquad g \\in G.$\nThe Wasserstein distance quantifies the minimal effort required to transform one distribution into another in terms of probability mass transport. This allows us to interpret $W(\\psi_{j,g}, \\overline{\\psi}_{j,g})$ as a proxy for prompt sensitivity. It captures the extent to which persona distributions shift across different prompt formulations. A higher Wasserstein distance indicates greater instability, meaning that minor variations in prompts lead to significantly different latent representations. To formally assess this effect, we regress $W(\\psi_{j,g}, \\overline{\\psi}_{j,g})$ on the normalized entropy $H_{norm}(\\phi_{j,g})$, evaluating how prompt-induced variation correlates with entropy within persona distributions."}, {"title": "5 Results", "content": "We analyze how LLMs model persona-to-party mappings and compare their voting distributions to real-world election data. First, we examine the entropy of persona voting distributions to assess whether models capture variability in political preferences. Next, we compare the predicted voting distributions to observed election outcomes, highlighting systematic biases. Finally, we investigate prompt sensitivity by measuring how variations in phrasing affect model predictions using the Wasserstein Distance as a proxy for prompt sensitivity."}, {"title": "5.1 Comparing Persona-to-party mapping and real world distribution", "content": "In our first set of experiments we compare the persona-to-party mappings in the LLMs' latent space to the real world voting distributions by looking at the distributions' normalized entropy. Figure 2 shows that overall, the entropy values"}, {"title": "5.2 Prompt Sensitivity", "content": "To analyze prompt sensitivity, we regress the entropy of persona-to-party mappings on the Wasserstein Distance as a proxy for prompt instability. The rationale behind this approach is that if minor variations in prompt phrasing significantly alter voting outcome predictions, we should observe a strong relationship between entropy and Wasserstein Distance. Our results indicate mixed findings across models. In the case of Qwen2.5-7B-Instruct (see Figure 4a), we observe a negative relationship: higher entropy in persona-to-party mappings corresponds to lower Wasserstein Distance. This suggests that when the model exhibits greater uncertainty (higher entropy) in its persona-to-party mappings, it is less sensitive to prompt variations. In other words, increased entropy appears to stabilize responses across different prompt formulations. By contrast, the Llama-3.1-8B-Instruct model (see Figure 4b) exhibits a positive relationship. Here, higher entropy correlates with greater Wasserstein Distance, indicating that when persona-to-party mappings are more uncertain, the model is more affected by prompt variations. This suggests that for Llama-3.1-8B-Instruct, increased entropy amplifies prompt sensitivity, making its voting outcome predictions more unstable under minor prompt reformulations."}, {"title": "6 Discussion and Limitations", "content": "Can LLMs truly replace human surveys for predicting voting outcomes? This study explores how reliably LLMs generate synthetic data by examining persona-to-party mappings in their latent space and prompt sensitivity. Our findings question the use of current LLMs for public opinion research, particularly regarding uncertainty in persona associations and variations in model responses due to prompt phrasing.\nReliability of LLM-Generated Synthetic Data. Our results demonstrate that most LLMs exhibit high entropy in their persona-to-party mappings in their latent space, indicating a broad distribution of voting predictions rather than distinct, well-anchored associations between personas and political preferences. This is in contrast to real-world voting distributions observed in GLES data, where certain sociopolitical subgroups show more defined voting patterns. The high entropy in LLM responses suggests that these models inherently introduce a level of uncertainty and dispersion that is not present in actual human survey data. Interestingly, base models display a pronounced tendency towards right-wing populist preferences, whereas aligned models shift towards center-right and center-left parties. This shift suggests that alignment processes significantly alter how LLMs interpret and generate survey responses. The Qwen2.5-7B model exhibited the closest match to real-world voting outcomes, yet its latent space entropy did not align perfectly with GLES data, emphasizing that similarity in aggregate predictions does not necessarily imply accurate underlying opinion structures.\nOur findings suggest that while LLMs can replicate broad voting trends, they do not accurately capture the demographic-specific distributions of human survey responses (cf. RQ1). This divergence raises concerns about the reliability of synthetic data for opinion research, particularly regarding overgeneralization and potential misinterpretations in model-generated predictions.\nPrompt Sensitivity and Stability of Predictions. Concerning prompt sensitivity, our analysis reveals key inconsistencies in how LLMs handle slight variations in input phrasing. While the Qwen2.5-7B-Instruct model exhibits a negative relationship between entropy and prompt sensitivity\u2014suggesting that higher uncertainty stabilizes responses-the Llama-3.1-8B-Instruct model shows the opposite trend, with greater entropy leading to more instability. However, beyond these differences, we do not observe a clear or systematic relationship between prompt variations and entropy levels across models. This highlights the complexity of LLM behavior and the need for model-specific evaluations when assessing robustness in synthetic survey applications. These contrasting patterns highlight fundamental differences in how models handle prompt perturbations.\nImplications for Public Opinion Research. LLMs are increasingly used in public opinion research to simulate human preferences Argyle et al. [2023], Bisbee et al. [2023], von der Heyde et al. [2024b], however, their application presents both opportunities and challenges. While they can automate surveys, their dispersed persona-party mappings lack structured opinion anchoring, making it difficult to derive reliable insights, particularly for demographic subgroups. Moreover, high prompt sensitivity means that minor variations in wording can significantly alter results, complicating standardization across studies. Some models exhibit greater robustness, but others remain highly unstable, limiting their reliability for predictive research. We caution against uncritical reliance on LLMs as substitutes for human respondents, as their persona-party mappings are often highly dispersed, indicating weakly anchored associations. This lack of structured alignment reduces confidence in their predictive power. Future research should focus on refining alignment techniques and probing methodologies to enhance the stability and representational accuracy of synthetic survey responses.\nLimitations of the Study. While our study provides a comprehensive technical analysis, it is not without limitations. First, our approach relies on the identification of multi-layer perceptron (MLP) value vectors using trained probes. Although this method offers valuable insights into how LLMs encode political preferences, it is inherently limited by the accuracy and scope of the probe training process. Alternative interpretability techniques may yield additional perspectives on model behavior. Second, our analysis focuses on a selection of LLMs with white-box access. The findings may not fully generalize to closed-source models, which might employ different training and alignment strategies. Future research should examine a broader range of models, including those with different architectures and"}, {"title": "7 Conclusion", "content": "Our study underscores the challenges and potential pitfalls of using LLMs for opinion research. While these models can approximate broad trends, their latent space representations and response behaviors diverge significantly from human survey responses. High entropy in persona mappings, alignment-induced shifts in voting predictions, and prompt sensitivity issues all highlight the need for careful evaluation before deploying LLMs as survey substitutes. By addressing these limitations through targeted methodological advancements, future research can work towards making AI-generated synthetic data a more reliable tool for public opinion analysis."}, {"title": "A Personas", "content": "Overview of persona variables (in german) and corresponding groups used in our study. The table includes demographic variables, political affiliations, and economic factors that define the synthetic personas used for evaluating LLM-generated survey data."}, {"title": "B Relationship Between Persona Groups and Scaling Factors", "content": "This section presents the significant regression coefficients for scaling factors $m_p^n$ regressed on categorical persona groups G, considering a significance level of $\\alpha \\leq 0.05$. The results are displayed separately for each German political party (see Figure 5). These coefficients indicate how different persona attributes influence the model's latent space in the form of value vectors across political preferences."}, {"title": "C Entropies", "content": "Figure 6: This figure compares the entropy of voting outcomes across different sociopolitical subgroups (e.g., female) as predicted by LLMs versus the real-world entropy observed in the GLES data 2021. The synthetic personas were asked which party they would vote for tomorrow, rather than reflecting past election results. Higher entropy indicates greater uncertainty or diversity in political preferences within a subgroup."}]}