{"title": "State-free Reinforcement Learning", "authors": ["Mingyu Chen", "Aldo Pacchiano", "Xuezhou Zhang"], "abstract": "In this work, we study the state-free RL problem, where the algorithm does not\nhave the states information before interacting with the environment. Specifically,\ndenote the reachable state set by $\\SII := \\{s|\\max_{\\pi\\in\\Pi}q_{P,\\pi}(s) > 0\\}$, we design\nan algorithm which requires no information on the state space S while having a\nregret that is completely independent of S and only depend on SII. We view this\nas a concrete first step towards parameter-free RL, with the goal of designing RL\nalgorithms that require no hyper-parameter tuning.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) studies the problem where an agent interacts with an unknown envi-\nronment to optimize cumulative rewards/losses [Sutton and Barto, 2018]. While the nature of the\nenvironment is in principle hidden from the agent, many existing algorithms [Azar et al., 2017, Jin\net al., 2018, Zanette and Brunskill, 2019, Zhang et al., 2020, 2021] implicitly require prior knowledge\nof parameters of the environment, such as the size of the state space, action space, time horizon and\nso on. Such parameters play a crucial role in these algorithms, as they are used in the construction\nof variable initializations, exploration bonuses, confidence sets, etc. However, in most real-world\nproblems, these parameters are not known a priori, resulting in the need for the system designer to\nperform hyper-parameter tuning in a black-box fashion, which is known to be extremely costly in RL\ncompared to their supervised learning counterparts [Pacchiano et al., 2020]. In supervised learning\nalgorithms, selecting among M hyper-parameters only degrades the sample complexity by a factor\nof O(log(M)). In contrast, in RL problems it will incur a O(\\sqrt{M}) multiplier on the regret, making\nhyper-parameter tuning prohibitively expensive. This is one of the major roadblocks to broader\napplicability of RL to real-world scenarios.\nMotivated by the above observation, we propose and advocate for the study of parameter-free\nreinforcement learning, i.e. the design of RL algorithms that have no or as few hyper-parameters as\npossible, with the eventual goal of eliminating the need for heavy hyper-parameter tuning in practice.\nAs a concrete first step, in this paper, we focus on the problem of state-free RL in tabular MDPs. In\nparticular, we will show that there exist state-free RL algorithms which do not require the state space\nS as an input parameter to the algorithm, nor do their regret scale with the innate state space size |S|.\nIn particular, we design a black-box reduction framework called State-Free Reinforcement Learning\n(SFRL). Given any existing RL algorithm for stochastic or adversarial MDPs, this framework can\ntransform it into a state-free RL algorithm through a black-box reduction. We also show that the\nsame framework can be adapted to induce action-free and horizon-free algorithms, the three of which\nnow makes a tabular MDP algorithm completely parameter-free, i.e. it requires no input parameters\nwhatsoever, and their regret bound automatically adapt to the intrinsic complexity of the problem.\nThe rest of the paper is organized as follows. Following the discussion of related works and problem\nformulation, we start by discussing the technical challenges of state-free learning and why existing\nalgorithmic and analysis framework are not able to achieve it (Section 4). Built upon these insights,\nwe propose an intuitive black-box reduction framework SF-RL, that transforms any RL algorithm"}, {"title": "Related Works", "content": "Parameter-free algorithms: Acknowledgedly, parameter-free learning is not a new concept and\nhas been studied extensively in the optimization and online learning community. Parameter-free\nalgorithms refer to algorithms that do not require the learner to specify certain hyperparameters\nin advance. These algorithms are appealing in both theory and practice, considering that tuning\nalgorithmic parameters is a challenging task [Bottou, 2012, Schaul et al., 2013]. The types of\nhyperparameters to \u201cset free\u201d varies depending on the specific problem. For example, for online\nlearning and bandit problems, the hyperparameters are considered as the scale bound of the losses\n[De Rooij et al., 2014, Orabona and P\u00e1l, 2018, Duchi et al., 2011, Chen and Zhang, 2023], or the\nrange of the decision set [Orabona and P\u00e1l, 2016, Cutkosky and Orabona, 2018, Zhang et al., 2022,\nvan der Hoeven et al., 2020]; for neural network optimization, the hyperparameters can be the learning\nrate of the optimizer [Defazio and Mishchenko, 2023, Carmon and Hinder, 2022, Ivgi et al., 2023,\nCutkosky et al., 2024, Khaled and Jin, 2024]; for model selection, the hyperparameters are the choice\nof the hypothesis class [Foster et al., 2017, 2019].\nSurprisingly, the reinforcement learning (RL) community has overlooked the concept of parameter-\nfree learning almost entirely. To the best of our knowledge, the only related work is from Chen and\nZhang [2024], where the authors proposed an algorithm that adapts to the scale of the losses in the\nsetting of adversarial MDPs. In this work we focus on the problem of developing parameter-free\nRL algorithms where the parameter to be focused on are those related to the environment transition,\nparticularly the state space. Almost all RL algorithms assume knowledge of the state-space. For\nexample, existing UCB-based reinforcement learning algorithms [Azar et al., 2017, Jin et al., 2018,\nZanette and Brunskill, 2019, Zhang et al., 2020, 2021] make use of the state space size to construct\nthe UCB bonus. When the state space is unknown, it is unclear whether these algorithms can still\nbuild a valid UCB bonus that ensures optimism and achieve bounded regrets.\nInstance-dependent algorithms: Instance-dependent learning is a closely related concept to\nparameter-free learning. Instance-dependent algorithms dynamically adjust to the input data they find,\nand achieve a regret that not only scaling with the number of iterations T, but also adapt to certain\n\"measures of hardness\" of the environment. Such algorithms perform better than the worst-case regret\nif the environment is \"benign\". In reinforcement learning, the most common \u201cmeasures of hardness\"\nconsidered in the community are Variance [Zanette and Brunskill, 2019, Zhou et al., 2023, Zhang\net al., 2023, Zhao et al., 2023] and Gap [Simchowitz and Jamieson, 2019, Xu et al., 2021, Dann et al.,\n2021, Jonsson et al., 2020, Wagenmaker et al., 2021, Tirinzoni et al., 2021], both related to the reward\nof the environment. Specifically, variance-dependent algorithms provide regret bounds that scale with\nthe underlying conditional variance of the $Q^*$ function. Gap-dependent algorithms provide regret\nbounds of order $\\tilde{O}(\\log T/gap(s, a))$ where the gap notion is defined as the difference of the optimal\nvalue function and the $Q^*$-function at a sub-optimal action $V^*(s) \u2013 Q^*(s, a)$ [Dann et al., 2021].\nThe difference between instance-dependent algorithm and parameter-free algorithm is subtle. Both\nfamily of algorithms have the capability to adapt to the input data, allowing them to sequentially tune\nthe hyperparameters and ultimately converge to the optimal hyperparameters inherent in the data.\nConsequently, when the number of iterations becomes sufficiently large, both instance-dependent\nalgorithms and parameter-free algorithms tend to provide the same theoretical guarantees. However,\nthis does not mean that the two types of algorithms are the same. The most significant difference\nis that instance-dependent algorithms require appropriate hyper-parameters initialization. Taking\nstate-space adaptability as an example. Let N represent the true number of states. An instance-\ndependent algorithm must be provided with an initial value M > N. If this value is invalid, i.e.,\nM < N, the algorithm will fail to function properly. Moreover, the regret of instance-dependent\nalgorithms is typically related to the initial input, even though this dependency may fade away\nas the number of iterations increases. This is also why we cannot simply set M to infinity in an\ninstance-dependent algorithm and call it parameter-free, that is, the regret of an instance-dependent\nalgorithm always includes some burn-in terms that scale with M. As M goes to infinity, the burn-in"}, {"title": "Problem Formulation", "content": "Markov Decision Process: This paper focuses on the episodic MDP setting with finite horizon,\nunknown transition, and bandit feedback. A MDP is defined by a tuple M = (S, A, H, P), where\nS = {1, ..., S} denotes the state space, A = {1, ..., A} denotes the action space, and H denotes\nthe planning horizon. $P : S \\times A \\times S \\to [0, 1]$ is an unknown transition function where $P(s'|s, a)$\nis the probability of reaching state s' after taking action a in state s. For every t \u2208 [T], we define\n$l_t: S \\times A \\to [0,1]$ as the loss function. In stochastic MDPs, the loss function $l_t$ is drawn\nfrom a time-independent distribution. In adversarial MDPs, the loss function $l_t$ is determined\nby the adversary, which can depend on the player's actions before t. The learning proceeds in\nT episodes. In each episode t, the learner starts from state $s_1$ and decides a stochastic policy\n$\\pi_t \\in \\Pi : S \\times A \\to [0,1]$ with $\\pi_t(a|s)$ being the probability of taking action a in state s. Afterwards,\nthe learner executes the policy in the MDP for H steps and observes a state-action-loss trajectory\n$(s_1, a_1, l_t(s_1, a_1), ..., s_H, a_H, l_t(s_H, a_H))$ before reaching the end state $s_{H+1}$. With a slight abuse\nof notation, we assume $l_t(\\pi) = \\mathbb{E}[\\sum_{h\\in[H]} l_t(s_h, a_h)|P, \\pi]$. The performance is measured by the\nregret, which is defined by\n$R(T) = \\sum_{t=1}^T l_t(\\pi^*) \\text{.} \\$\nWithout loss of generality, we consider a layered-structure MDP: the state space is partitioned into\nH + 2 horizons $S_0, . . ., S_{H+1}$ such that $S = \\bigcup_{h=1}^{H+1} S_h, \\emptyset = S_i \\cap S_j$ for every i \u2260 j, $S_0 = \\{s_0\\}$ and\n$S_{H+1} = \\{s_{H+1}\\}$.\nOccupancy measure: Given the transition function P and a policy \u03c0, the occupancy measure\n$q : S \\times A \\to [0, 1]$ induced by P and \u03c0 is defined as\n$q^{P,\\pi}(s, a) = \\sum_{h=1}^H P(s_h = s, a_h = a|P, \\pi)$.\nUsing occupancy measures, the MDP problem can be interpreted in a way that makes it similar to\nMulti-armed Bandit (MAB) because for any policy \u03c0, the loss can be expressed as\n$l_t(\\pi) = \\sum_{s\\in[S]} \\sum_{a\\in[A]} q^{P,\\pi}(s, a)l_t(s, a) = \\langle q^{P,\\pi}, l_t \\rangle$.\nUsing this formula the regret can be written as $R(T) = \\sum_{t=1}^T \\langle q^{P,\\pi^*} \u2013 q^{P,\\pi_t}, l_t \\rangle$.\nState-free RL: We say a state $s \\in S$ is reachable to a policy set $\\Pi$ if there exists a policy $\\pi\\in \\Pi$\nsuch that $q^{P,\\pi}(s) > 0$. We further define $\\SII = \\{s \\in S|\\max_{\\pi\\in\\Pi}q^{P,\\pi}(s) > 0\\}$ to represent all the\nreachable states to $\\Pi$ in S. The formal definition state-free algorithm is proposed below.\nDefinition 3.1. (State-free algorithm): We say a RL algorithm is state-free if given any policy set\n$\\Pi$, the regret bound for the algorithm can be adaptive to $|\\SI|$ and independent to $|S|$, without any\nknowledge of the state space a priori.\nAt first glance, designing state-free algorithms appears straightforward: if the learner had access\nto the transition $P$, it can compute $\\max_{\\pi\\in\\Pi}q^{P,\\pi}(s)$ for every state $s \\in S$ and then remove all the\nunreachable states, thereby reducing the state space S to SII. Through this reduction, any existing\nMDP algorithm can be made state-free. However, such a method is infeasible since $P$ is always\nunknown in practice. Without the knowledge of $P$, it becomes challenging or even impossible to\ndetermine whether a state is reachable or not. In the following section, we elaborate on the technical\nchallenges of the problem for both stochastic and adversarial loss settings."}, {"title": "Technical challenges", "content": "In this section, we explain the technical challenges for the state-free learning. Specifically, we\nconsider a weakened setup. We assume for a moment that the algorithm has access to the state space"}, {"title": "Black-box reduction for State-free RL", "content": "In this section, we outline the main contribution of the paper. To generalize our results further,\nwe denote the $\\epsilon$-reachable state space as $\\SII,\\epsilon = \\{s \\in S|\\max_{\\pi\\in\\Pi}q^{P,\\pi}(s) > \\epsilon\\}$. By definition\n$\\SII = \\SII,0$. Our algorithm SF-RL is illustrated in Algorithm 1. The algorithm maintains a pruned\nstate space, denoted by $\\mathcal{S}^+$, which includes all the identified $\\epsilon$-reachable states and H additional"}, {"title": "Improved regret bound for State-free RL", "content": "In the previous section, we introduce a black-box framework SF-RL that transforms any existing RL\nalgorithm into a state-free RL algorithm. However, the regret guarantee for SF-RL is suboptimal:"}, {"title": "Conclusion", "content": "This paper initiates the study of state-free RL, where the algorithm does not require the information\nof state space as a hyper-parameter input. Our framework SF-RL allows us to transform any existing\nRL algorithm into a state-free RL algorithm through a black-box reduction. Future work includes\nextending the framework SF-RL from the tabular setting to the setting with function approximation."}, {"title": "Omitted details for Section 2", "content": "In this subsection, we illustrate that UCBVI is actually a weakly state-free algorithm. As in Azar et al.\nUCBVI algorithm consists of two parts: value iteration and policy execution. In every epoch,\npolicy execution executes a policy that is greedy on the current Q values and adds newly encountered\ntrajectory to the dataset. Then, value iteration uses this dataset to update the Q values of state-action\npairs. Specifically, value iteration proceeds from the horizons H, . . ., 1. For every $(s, a) \\in S_h \\times A$,\nthe update can be expressed as\n$Q(s, a) = \\min\\{H,r(s,a) + (\\hat{P}(\\cdot|s, a), V(\\cdot)) + b(s,a)\\}$,\nwhere $\\hat{P}(\\cdot|s, a)$ is the empirical transitions estimation, $V(\\cdot) = \\max_{a\\in A} Q(\\cdot, a)$ is the corresponding\nV value, and $b(s, a)$ is the exploration bonus, which is defined by\n$b(s, a) = c_{HL} \\frac{1}{\\sqrt{N_t(s, a)}}$, where $L = \\log (\\frac{|S||A|T}{\\delta})$.\nThroughout the algorithm, we can note that the knowledge of S is only applied in the design of the\nexploration bonus. Specifically, the exploration bonus is designed to ensure that the following event\nholds for all epochs with probability at least 1 \u2013 $\\delta$ by Hoeffding\u2019s inequality and a union bound.\n$\\S = \\{\\langle \\hat{P}(\\cdot|s, a) \u2013 P(\\cdot|s, a), V^*(\\cdot)\\rangle| \\leq b(s,a), \\forall (s, a) \\in S \\times A\\}$\nWhen the reachable space $\\mathcal{S}^{\\Pi}$ is known, by substituting S with $\\mathcal{S}^{\\Pi}$ in advance, as in Theorem 2\nof Azar et al. [2017], the regret of UCBVI is well bounded by $O(H\\sqrt{|\\mathcal{S}^{\\Pi}||A|T\\log(|\\mathcal{S}^{\\Pi}||A|T/\\delta)})$.\nWhen $\\mathcal{S}^{\\Pi}$ is unknown, we have to utilize S to design the bonus, resulting in the exploration bonus\nbeing amplified by a factor of $\\sqrt{\\log(|S|)}/\\log(|S^{\\Pi}|)$. In this case, by optimism lemma, the regret\nguarantee increases by at most the same factor. Such a result suggests that UCBVI is weakly state-free."}, {"title": "Details for Remark 4.2", "content": "Based on Proposition 4.1, here we show how to escape the log-dependence on |S| under the UCBVI\nframework. The idea is simple: when constructing the exploration bonus, instead of allocating\nconfidence $\\delta/|S||A|T$ to every state-action-epoch pair uniformly, we initialize the confidence level\nfor states based on their arriving time. Let i(s) be the index of state s sorted by the arriving time. The\nexploration bonus is set by\n$b(s, a) = c^{\\prime}_{HL} \\frac{1}{\\sqrt{\\max\\{N_t(s, a) \u2013 1,1\\}}} \\text{ where } L = \\log(\\frac{2|i(s)|^2|A|T}{\\delta})$.\nBroadly speaking, for every visited state $s \\in S$, we allocate confidence $\\delta/2|i(s)|^2|A|T$ to its\ncorresponding state-action-epoch pair. In this regard, the confidence allocated to state s is bounded\nby $\\delta/2|i(s)|^2$, and the total confidence is bounded by $\\Sigma \\frac{\\delta}{2i^2} < \\delta$. Specifically, to avoid the\ncorrelation between the confidence level and the subsequent confidence sequence, we initialize the\nconfidence sequence of $\\langle \\hat{P}(\\cdot|s, a) \u2013 P(\\cdot|s, a), V^*(\\cdot)\\rangle$ at epoch t(s) + 1, where t(s) is the epoch\nindex that state s is visited for the first time. This is because the confidence level $\\delta/2|i(s)|^2|A|T$\ncan be determined when the algorithm first reaches state s. In this way, we loose one data point for\nconstructing the confidence sequence. This is why the count of visits is $\\max\\{N_t(s, a) \u2013 1,1\\}$ rather\nthan $N_t(s, a)$. Additionally, by the definition of b(s, a), it suffices to note that $b(s, a) > H$ for the\nepochs before t(s), which implies that the bonus ensures optimism for all epochs before t(s) with\nprobability 1. Combining with the above, it suffices to note that with probability 1 \u2013 $\\delta/2|i(s)|^2|A|T$,\nthe bonus b(s, a) ensures optimism for all epochs. By a union bound, we can conclude that our\nproposed exploration bonuses also make the event in (1) hold with probability 1 \u2013 $\\delta$. Given that\ni(s) \u2264 |S| for all visited states, the new exploration bonus is of the same order as the bonus where\nSII is known. In this way, we make the regret bound be completely independent to |S|, and turn\nUCBVI to a fully state-free algorithm."}, {"title": "Details for Proposition 4.3", "content": "In this subsection, we demonstrate that, within the current analysis framework for adversarial MDPs,\neliminating the polynomial dependence on |S| is impossible. To the best of our knowledge, in order\nto handle the unknown transitions, all existing works share the same idea that maintain a confidence\nset $\\mathcal{P}_t$ of transition functions for t \u2208 [T]. The confidence set can be denoted by\n$\\mathcal{P}_t = \\{P : |\\hat{P}(s'|s,a) \u2013 P(s'|s, a)| \\leq e_t(s's, a), \\forall (s, a, s') \\in S \\times A \\times S\\}$,\nwhere $e_t(s's, a)$ is the confidence width and $\\hat{P}(s'|s, a)$ is the empirical estimation of the true\ntransition $P(s's, a)$. Specifically, $e_t(s'|s, a)$ is set based on empirical Bernstein inequality, i.e.,\n$e_t(s's, a) = O(\\sqrt{\\frac{\\hat{P}(s's,a)\\log(SAT/\\delta)}{N_t(s,a)} + \\frac{\\log(SAT/\\delta)}{N_t(s,a)}}), \\forall (s, a, s') \\in S \\times A \\times S,$\nwhere $N_t(s, a)$ denotes the counter of state-action pair (s, a). This setting ensures that $P \\in \\mathcal{P}_t$ for\nall t \u2208 [T] with probability at least 1 \u2013 $\\delta$. Given the confidence set, the algorithm selects $\\hat{P}_t \\in \\mathcal{P}_t$ as\nthe approximation of P, and chooses policy $\\pi_t$ based on the approximation transition. For a clear\nunderstanding, we decompose the regret into two terms, i.e.,\n$R(T) = \\sum_{t=1}^T (q^{P,\\pi_t} \u2013 q^{P,\\pi^*}, l_t) = \\sum_{t=1}^T (q^{\\hat{P}_t,\\pi_t} \u2013 q^{\\hat{P}_t,\\pi^*}, l_t) + \\sum_{t=1}^T (q^{P,\\pi_t} \u2013 q^{\\hat{P}_t,\\pi_t}, l_t) - (q^{P,\\pi^*} \u2013 q^{\\hat{P}_t,\\pi^*}, l_t)$.\nHere, the first term REGRET represents the regret of the algorithm with the approximation transition.\nIn some senses, bounding REGRET can be reduced to a bandit problem. In every round t, the\nalgorithm chooses an occupancy measure $\\hat{q}_t \\in \\Delta(\\mathcal{P}_t, \\Pi)$ and corresponding $(\\hat{P}_t, \\pi_t) \\in (\\mathcal{P}_t, \\Pi)$, then\nobtain a partial observation of the loss $l_t$. For the second term ERROR, it corresponds to the error\nusing $\\hat{P}$ to approximate P. Considering the adversarial environment, there exists a \u201cworst enough\u201d\nloss sequence $l_1, ..., l_T$ such that\n$ERROR \\approx \\sum_{t} \\sum_{(s,a) \\in S_I \\times A} (q^{P,\\pi_t}(s, a) \u2013 q^{\\hat{P}_t,\\pi_t}(s, a))$.\nThus, bounding ERROR is essentially equates to bounding the right hand side of the above. At this\npoint, one needs to demonstrate that the confidence set shrinks in the correct rate over time, so that\nthe sum of the gap between $q^{P,\\pi_t}$ and $q^{\\hat{P}_t,\\pi_t}$ can be well bounded.\nHaving provided sufficient background, we now explain why existing methods fail to achieve state-\nfree regret bounds. First, since the confidence set requires to work for all $(s, a, s') \\in S \\times A \\times S$ pairs, we have\nto take a union bound on the \"good event\" for all $(s, a, s') \\in S \\times A \\times S. This essentially cause\n$e_t(s's, a)$ to be log-dependent on |S|. More important, to bound $|q^{P,\\pi_t}(s, a) \u2013 q^{\\hat{P}_t,\\pi_t}(s, a)|$, existing\nmethods mainly follow the proof of Lemma 4 in Jin et al. [2019], that is, for every $(s, a) \\in S_I \\times A$\nand $\\pi_t \\in \\Pi$, there exists $\\hat{P}_t \\in \\mathcal{P}_t$ such that\n$|q^{P,\\pi_t}(s, a) \u2013 q^{\\hat{P}_t,\\pi_t}(s, a)| \\approx \\sum_{h=1}^{h(s)-1} \\sum_{s_h a_h} e_t(s_{h+1}|s_h, a_h)q^{\\hat{P}_t,\\pi_t}(s_h, a_h)q^{\\hat{P}_t,\\pi_t}(s, a|s_{h+1})$.\nBy the definition of confidence width, we have $e_t(s_{h+1}|s_h, a_h) \\geq \\tilde{O}(1/N_t(s_h,a_h))$ for all\n$(s_h, a_h, s_{h+1})$ pairs. Furthermore, if state $s_{h+1}$ is unvisited, the algorithm has no information\nfor the transitions after the state. Assuming that $S_h \\neq S$ for all h \u2208 [H], there will always exist\na \"worst enough\" $\\hat{P}_t \\in \\mathcal{P}_t$ such that the probability of reaching s via $s_{h+1}$ with policy $\\pi_t$ is 1, i.e.,\n$q^{\\hat{P}_t,\\pi_t}(s|s_{h+1}) = 1$. In this case, we have"}]}