{"title": "Multi-Facet Blending for Faceted Query-by-Example Retrieval", "authors": ["Heejin Do", "Sangwon Ryu", "Jonghwi Kim", "Gary Geunbae Lee"], "abstract": "With the growing demand to fit fine-grained user intents, faceted query-by-example (QBE), which retrieves similar documents conditioned on specific facets, has gained recent attention. However, prior approaches mainly depend on document-level comparisons using basic indicators like citations due to the lack of facet-level relevance datasets; yet, this limits their use to citation-based domains and fails to capture the intricacies of facet constraints. In this paper, we propose a multi-facet blending (FaBle) augmentation method, which exploits modularity by decomposing and recomposing to explicitly synthesize facet-specific training sets. We automatically decompose documents into facet units and generate (ir)relevant pairs by leveraging LLMs' intrinsic distinguishing capabilities; then, dynamically recomposing the units leads to facet-wise relevance-informed document pairs. Our modularization eliminates the need for pre-defined facet knowledge or labels. Further, to prove the FaBle's efficacy in a new domain beyond citation-based scientific paper retrieval, we release a benchmark dataset for educational exam item QBE. FaBle augmentation on 1K documents remarkably assists training in obtaining facet conditional embeddings.", "sections": [{"title": "1 Introduction", "content": "Query-by-example (QBE), which involves retrieving relevant documents given a query document, is a fundamental technique in both exploratory search (Lissandrini et al., 2019) and recommendation systems (Ostendorff et al., 2020a,b; Lee et al., 2013). However, documents typically include multiple facets distinguished by specific rhetorical units (e.g., background, method, and result of academic paper abstract); thus, querying with the entire document, not identifying the specific facet of interest, can lead to unintentional or irrelevant retrievals (Figure 1). For instance, to recommend exam items similar in question type to a student's incorrect answer, prioritize the question facet for retrieval, regardless of story or options, is required.\nAccordingly, faceted QBE, which conditions the query document on a specific facet, has garnered recent attention for intent-tailored fine-grained document search (Dunne et al., 2012; Hope et al., 2020; Neves et al., 2019). This task has been predominantly explored in scientific paper retrieval, relying on the vast amount of public corpora where citation labels provide superficial cues (Cohan et al., 2020; Ostendorff et al., 2022; Mysore et al., 2021, 2022). However, those methods are not feasible for other domains (e.g., education or legal), where such citation labels are absent, and large-scale open-source corpora are lacking (Li et al., 2023). Further, the reliance on document-level comparisons often leads to the failure to capture facet constraints, especially for intricate cases (Mysore et al., 2021).\nIn this paper, we propose a multi-facet blending (FaBle) augmentation method, which dynamically exploits modularity with decomposing and recomposing. In particular, we first decompose each facet within the document by summary-driven identification, leveraging zero-shot prompting with sLLM. Then, we generate facet-wise similar and dissimilar facet fragments by self-feeding the decomposed facet summary in recursive prompting. Referring to the identified facet guides the synthesis of facet-aware compositions distinguished from other facets. Finally, recomposition strategy integrates the synthesized facets to reconstruct facet-conditioned pseudo documents, creating positive-negative pairs for an anchor document. Fable explicitly create facet-specific training sets to assist model training for faceted QBE, eliminating the need for pre-defined facet knowledge or labels.\nWe target scientific paper abstract retrieval for validation, as it is the sole field providing the benchmark test set for faceted QBE. Aiming to assist in a data-scarce scenario, we employ only 1K documents for augmentation without any citation labels. Experimental results of fine-tuning the SPECTER (Cohan et al., 2020) model with FaBle-augmented pairs are comparable or better to previous models, where more than 1.3M training sets were used for fine-tuning. Notably, FaBle significantly improves the challenging method facet, even outperforming the strong prior models. This result highlights that our fine-grained augmentation overcomes the limitations of coarse-grained approaches that ill-captured intricate facets (Mysore et al., 2021).\nTo further evaluate FaBle's domain scalability and practical efficacy, we present a novel test set for faceted educational exam item retrieval, FEIR, derived from the TOEFL-QA data. Applying FaBle to educational items remarkably improves performance across all facets, demonstrating domain-agnostic effects. We expect the FEIR to stimulate future works of faceted QBE in this emerging education domain. Codes and datasets are on GitHub\u00b9."}, {"title": "2 Related Work", "content": "QBE QBE is a fundamental task across diverse fields, such as legal or academic, where document-level findings for recommendation or exploratory search are important (Lissandrini et al., 2019; Ostendorff et al., 2020a,b; Lee et al., 2013). Most prior studies focused on retrieving scientific papers, using large-scale datasets and estimating similarities based on citations (Cohan et al., 2020; Mysore et al., 2021, 2022; Ostendorff et al., 2022). Cohan et al. (2020) introduced the SPECTER to obtain document-level embeddings by measuring similarity via citation graphs, and Ostendorff et al. (2022) used a citation embedding graph combined with neighbor contrastive learning.\nFaceted QBE Documents typically encompass multiple facets; thus, considering overall document-level relevance may not align with user intent (Do and Lee, 2024). Faceted QBE has emerged to address this, enabling facet-level document comparisons (Neves et al., 2019; Wang et al., 2023). While most studies focus on scientific paper retrieval (Mysore et al., 2021, 2022; Wang et al., 2023), they do not directly train on facet-wise relevance annotated data, as such data is difficult to obtain. Instead, Mysore et al. (2021) utilized an additional 66K citation-based pair for training, and Mysore et al. (2022) used 2.6M co-citation sentences with an auxiliary optimal transport technique. However, the reliance on abundant domain-specific data and citations restricts their applicability to other low-resource domains.\nLLM Augmented Retrieval LLM-based augmentation techniques have evolved from using GPT-2 (Radford et al., 2019) to GPT-3 (Brown et al., 2020) models to address the lack of relevance annotations. Luu et al. (2021) fine-tune GPT-2 to generate relationships between two scientific papers, assuming in-text citation sentences elucidate their connections. Gao et al. (2023) use GPT-3 to generate hypothetical documents corresponding to desired instructions in a zero-shot manner.\nRecently, for faceted QBE, Wang et al. (2023) utilize ChatGPT to annotate the relevance scores of aspect-paper pairs, reducing the burden of human labor. Despite aiming at sub-aspect level similarity evaluation, utilizing ChatGPT for massive datasets still incurs significant costs; thus, they mainly target testing faceted QBE, not training. Also, as they only contain computer science-related documents, datasets are not generalizable to other fields. Contrarily, by leveraging the capacity of open-source smaller LLM, we eliminate the cost burden and introduce the domain-extendable method."}, {"title": "3 FaBle: Multi-facet Blending", "content": "For general QBE, obtaining informative representations for query and candidate documents is crucial to effectively retrieve similar documents. To achieve this, model training requires a triplet pair (DQ, D+, D\u00af) comprising a query document, a positive document, and a negative document. In faceted QBE, queries include additional facet conditions; thus, facet-constrained triplet pairs can lead to more precise and focused model training. Unlike prior methods that implicitly construct D+ and D\u00ae based on citations on D\u00ba, we explicitly construct facet-conditional triplet pairs (Df;Q, Df+, Df\u2212)."}, {"title": "3.1 Facet Decomposition", "content": "To identify each facet, we first decompose the document into multiple facet units. For this, we prompt LLM to summarize a specific facet in a zero-shot manner. We use the publicly available SLLM, LLaMA2-13B (Touvron et al., 2023), taking advantage of open and easy access. By prompting the model to summarize a desired facet within the document, the intended facet-distinct information is extracted. Given a document D, summarization prompt psum, and a facet name f, where f \u2208 {background, method, result}, as input, the model generates facet summary Sf, which modularize the f facet: Sf = Model(D, Psum, f). The generated summary highly represents the facet, but it does not mean a real facet; instead, it serves as an indicator to guide the subsequent generation stage."}, {"title": "3.2 Facet Generation", "content": "To generate each facet-specific similar and dissimilar fragment, the same model self-fed the prior prompt used to decompose and its extracted output as shown in Figure 2. Although LLaMA2 has proved proficiency in various generation tasks, its zero-shot performance often lags behind task-specific instruction tuning or GPT-4 (Zhu et al., 2023; OpenAI, 2023). Our self-feeding approach aids in target-oriented generation by referring to the facet-identified summary while eliminating the burden of fine-tuning. In particular, to generate f-facet similar component $C_{f_{sim}}$, the model takes pre-generated summary Sf and the similar-generation prompt $p_{sim}$ as the input. For dissimilar component $C_{f_{dis}}$, the model takes summary Sf and dissimilar-generation prompt $p_{dis}$ as input:\n$C_{f_{sim}} = Model(D, p_{sum}, f, S_f, p_{sim})$ (1)\n$C_{f_{dis}} = Model(D, p_{sum}, f, S_f, p_{dis})$ (2)\nFigure 3 reveals that our two-stage approach results in more target-facet-focused texts (left), while the simple prompting without the facet-identified summary outputs non-target facets mixed in (right)."}, {"title": "3.3 Facet Recomposition", "content": "To obtain the negative and positive document pairs for a query document conditioning a specific facet, we combine the generated similar and dissimilar facet components with a suitable recomposition recipe. The f-facet conditional positive $D_f^+$ and negative $D_f^\u2212$ documents with total n facets are:\n$D_f^+ = [C_{f}^{sim}; C_{f_{1}}^{dis/sim};...; C_{f_{n\\in F-f}}^{dis/sim}] (3)\nD_f^- = [C_{f}^{dis}; C_{f_{1}}^{dis/sim};...; C_{f_{n\\in F-f}}^{dis/sim}] (4)$\nwhere [;] denotes concatenation, F is a set of facets, and $f_i \\in F - f$ is a facet different from the target facet f. Consequently, the triplet pair (Df;Q, Df+,Df\u2212) is constructed for the query document Df;, conditioned on a target facet f.\nOn a single original document with three facets, four $D_f^+$ and four $D_f^-$ are generated via facet recomposition. Then, five documents, including the original one, lead to ten (Df;Q, Df+) pairs (i.e., five choose two, $\\binom{5}{2}$). For each of them, one $D_f^\u2212$ is selected among four candidates, resulting in a total of forty (Df;Q, Df+, Df\u2212) pairs per sample. Note that FaBle operates without any labels, including weak labels like citations or pre-divided facet tags."}, {"title": "3.4 Fine-tuning for Faceted QBE", "content": "We validate the efficacy of FaBle-augmented triplet pairs in model training via contrastive learning, the widely adopted mechanism for representation learning. Specifically, we employ a pre-trained SciBERT (Beltagy et al., 2019)-based SPECTER (Cohan et al., 2020) to embed the documents. We fine-tune the model with triplet loss to verify whether the synthesized dataset benefits model training. Our loss function L(Df;Q, Df+, Df\u2212) is defined as:\n$max \\{(d(D_{f;Q}, D_f^+) \u2013 d(D_{f;Q}, D_f^-) + m),0\\}$\nwhere d is a distance function, and m is the loss margin hyperparameter. Note that no additional modeling techniques are used to examine the unique effects of the augmentation."}, {"title": "3.5 Hard Negative Generation", "content": "The significance and efficacy of hard negative mining for retrieval tasks have been widely demonstrated (Xiong et al., 2020; Zhan et al., 2021; Zhang et al., 2021; Zhou et al., 2023). These studies highlight that more challenging negative samples lead to better representation capturing. In this work, we explicitly prompt the LLM to create facets of different topics to generate negative (dissimilar) ones for a specific facet. This may compel the generation of easily distinguishable snippets, potentially leading to the absence of hard negative samples."}, {"title": "4 FEIR", "content": "The benchmark test set for faceted QBE is absent in domains other than scientific paper retrieval. This gap leads to a shortage of related studies in other fields, such as educational item retrieval, where each item comprises multiple facets. Even when items share similar Questions, their Stories and Options may differ, requiring fine-grained search queries. To validate the scalability of FaBle and support future research, we introduce a Faceted Educational exam Item Retrieval (FEIR) test set for the underexplored language education domain.\nDataset Construction We employ exam items from the publicly available TOEFL-QA dataset, a representative English as a Foreign Language (EFL) exam, to build the FEIR. The dataset contains 963 TOEFL listening QA items, and we utilize 122 test set items for constructing the FEIR test set Inspired by CSFCube (Mysore et al., 2021), which has 16 queries per facet, and given our limited original dataset, we form 8 query items for each facet (total 24 queries). To ensure diversity in relevance scores, we evaluated each sample's similarity with MiniLM scores and sequentially selected eight unique samples with the largest standard deviations in their score distributions. Each facet contains four conversation-type and four lecture-type queries. For candidate selection in the story facet, where data is limited, we use all 23 remaining items except the query item. In the question and options facets, we choose 80 and 70 items, prioritizing those with the highest standard deviations after removing the query items.\nRelevance Annotation To annotate relevance between facet-specific query-candidate pairs, we hired three experts: a professor in language learning major and two English specialists from Upwork. Each facet was assigned to two different experts. Following detailed guidelines they rated the relevance of each query and candidate item on a 0\u20133 scale, similar to Mysore et al. (2021). The rounded average of two ratings is the final score. Figure 5 shows the score distribution of candidates per query, with a minority being labelled between 1 and 3. This trend mirrors the CSFCube test set, where an average of 36.9 candidates per query are rated 1, and 9.8 candidates receive scores of 2 or 3. We examine the inter-annotator agreement by measuring the correlations between two annotators' labels: Kendall's T, Spearman's p, and Pearson's r. The facet-average values are 0.474, 0.492, and 0.557, respectively (p<0.05), indicating positive agreements (Chiang and Lee, 2023)."}, {"title": "5 Experiments", "content": "Data and Settings We use only 1017 random paper abstracts from the 81.1M papers in the open-source S2ORC corpus (Lo et al., 2020), having metadata, abstracts, and full text of academic papers. However, we do not use any annotated information in this work. By deliberately limiting the initial data to a small amount (approximately 0.00125%), we aim to validate that our method is effective in practical data-scarce settings. As the CSFCube comprises scientific papers in the computer science domain, we also select abstracts from the same field. Applying the FaBle with 1K documents, 40 triplet document pairs are generated per facet for a single document, resulting in 40.68K triplet pairs. To apply FaBle for education exam items, we use 717 items from the TOEFL QA training set, creating total 28.68K pairs As the dataset already has facet labels, we directly employ Stages 2 and 3. Detailed settings are in Appendix \u0412.\nBaselines Most studies on faceted QBE have used or fine-tuned the SPECTER (Cohan et al., 2020) model; hence, we adopt it as our baseline. Our primary aim is to evaluate the efficacy of facet-specific augmentation in data-scarce settings rather than resorting to supplementary methods for fine-grained QBE. Thus, our comparisons focus on the baseline models and those fine-tuned with FaBle-augmented data. We train two versions: the original SPECTER and SPECTER-COCITESPEC. The latter is similar to SPECTER but was additionally trained on 1.3M co-citation datasets from Mysore et al. (2022) with 2-3 point aggregation across queries. We also assess whether the FaBle-assisted model is comparable to other strong models for faceted QBE, with further details in Appendix C.\nEvaluation For evaluation, we use CSFCube (Mysore et al., 2021) test set, which provides annotations for faceted QBE on computer science papers. 50 query abstract-facet pairs are assigned relevance scores (0-3). We use the FEIR set to evaluate the educational exam item. For metrics, we employ normalized discounted cumulative gain at rank K (NDCG@K) and mean average precision (MAP). In particular, we report NDCG%20, computing at 20% of the query pool size, following prior works (Wang et al., 2013; Mysore et al., 2021, 2022). For the FEIR with fewer queries and candidates, we also report the NDCG%10."}, {"title": "6 Results", "content": "Table 2 shows the main results of FaBle across three facets. Incorporating FaBle with SPECTER enhances performance in all facets, yielding notable average gains of 3.4% in NDCG%20 and 1.4% in MAP. For SPECTER-COCITE, fine-tuning the model with FaBle also improves the performance, highlighting our assistance in model training.\nFacet-Specific Results The method facet, widely recognized as the most challenging primarily due to its focus on procedural descriptions of technical concepts, encountered difficulties in assessing similarity with prior models (Mysore et al., 2021, 2022). In this context, the remarkable enhancements in the method facet are noteworthy: an increase of 7.6% in NDCG%20 and 3.5% in MAP scores over SPECTER. Moreover, FaBle with the SPECTER-COCITE achieved a 3.4% rise in NDCG%20 and a 7.0% increase in MAP scores, even outperforming the robust ASPIRE models, trained on \u224832 times greater dataset than FaBle and employ co-citations labels with additional optimal transport techniques."}, {"title": "7 Analysis and Discussion", "content": "Impact of Hard Negatives We investigate the impact of hard-negative generation (\u00a7 3.5). Before analyzing, we examine how our hard-negative sampling altered the score distribution of existing negatives. Figure 6 exhibits that regeneration shifted the average to around 0.75 points, aligning with our goal of acquiring more challenging samples. We only select samples below 0.5 as hard negatives to differentiate from positives. Table 2 (FaBlespec+HN) indicate that hard negatives for a specific facet, regenerated to have a higher similarity score, remarkably assist method-faceted retrieval but not in the others. Creating high-similarity negative samples to a specific facet may hinder the relevance recognition on the general facets like background and result. Yet, for facets demanding a fine-grained approach, auxiliary optimizing with hard negatives can boost contrastive learning (Qu et al., 2021; Santhanam et al., 2022; Ostendorff et al., 2022; Formal et al., 2022).\nComparison with Random Sampling We compared the efficacy of directly generating negative facets to random sampling (Table 4). In particular, we replaced dissimilar facet fragments created for each document with randomly selected original facets from other documents. Original facets are not defined in the document; thus, we utilize the summarized facets from Stage 1. Table 4 indicate that FaBle, which integrates generating dissimilar component as specific-facet-tailored negatives, achieves markedly better performance than FaBle-RN, which employs random sampling. Hence, our subtle negative sampling may be a key for faceted QBE, aligning with contemporary research that emphasizes the advantages of strategic negative sampling over random approaches (Qu et al., 2021; Zhan et al., 2021; Zhou et al., 2023).\nEffects of the Data Size We examine how the amount of augmentation affects model performance. For 50%, we randomly select half the original document (0.5K out of 1K), creating 20K triplet pairs with FaBle. Figure 7 reveals that increasing the data size consistently enhances NDCG%20 and MAP scores. For both metrics, the Background facet shows reasonably high scores even at the base level, implying that the model itself could represent this comprehensible facet well; hence, fine-tuning on larger data moderately impacts the model performance. Meanwhile, the Method facet, indicated to be underrepresented in the baseline model by exhibiting lagged performance behind the other two facets, shows a clear improving tendency as the amount of FaBle-augmented data increases. Thus, tailoring data size to the specific needs of individual facets is essential for training optimization."}, {"title": "8 Conclusion", "content": "We introduce FaBle, a multi-facet blending augmentation that aids in direct model training for faceted QBE. By modularizing facets by decomposing and recomposing, FaBle effectively synthesizes pseudo-documents that match user-intended facets, eliminating the need for pre-set annotations. FaBle improves the retrieval performances, particularly in the salient facet, surpassing models trained on much larger datasets. In addition, we release a FEIR test set for the language education domain, demonstrating FaBle's generalizability."}, {"title": "Limitations", "content": "Currently, we assume data scarcity by applying FaBle on a small amount of data to evaluate the assistance in real-world settings where open corpora are limited. However, as we observed the performance-improving trends with the increased amount of datasets, augmenting with more original data could lead to further enhancements. Secondly, in prior faceted QBE works, statistical tests are not provided, which may be attributed to the small test set size (e.g., 16-17 queries per facet in CSFCube) confining statistical power. Nevertheless, to investigate the robustness of FaBle, we examined the proportion of queries where performance remained equal or improved. Among the aggregated queries, 70.83%, 70.83%, and 75% showed increased NDCG%20, NDCG%10, and MAP scores over SPECTER, respectively, demonstrating that FaBle is effective for the majority of queries."}, {"title": "Ethical Statement", "content": "We follow the guidelines outlined in the ACL Code of Ethics. Our work did not utilize private datasets, and it does not include any confidential personal information. For the human annotation, we provide fair compensation for two human experts, paying $135 as a fixed price."}]}