{"title": "Social perception of faces in a vision-language model", "authors": ["Carina I. Hausladen", "Manuel Knott", "Colin F. Camerer", "Pietro Perona"], "abstract": "We explore social perception of human faces in CLIP, a widely used open-source vision-language model. To this end, we compare the similarity in CLIP embeddings between different textual prompts and a set of face images. Our textual prompts are constructed from well-validated social psychology terms denoting social perception. The face images are synthetic and are systematically and independently varied along six dimensions: the legally protected attributes of age, gender, and race, as well as facial expression, lighting, and pose. Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes. Thus, our findings are experimental rather than observational. Our main findings are three. First, while CLIP is trained on the widest variety of images and texts, it is able to make fine-grained human-like social judgments on face images. Second, age, gender, and race do systematically impact CLIP's social perception of faces, suggesting an undesirable bias in CLIP vis-a-vis legally protected attributes. Most strikingly, we find a strong pattern of bias concerning the faces of Black women, where CLIP produces extreme values of social perception across different ages and facial expressions. Third, facial expression impacts social perception more than age and lighting as much as age. The last finding predicts that studies that do not control for unprotected visual attributes may reach the wrong conclusions on bias. Our novel method of investigation, which is founded on the social psychology literature and on the experiments involving the manipulation of individual attributes, yields sharper and more reliable observations than previous observational methods and may be applied to study biases in any vision-language model.", "sections": [{"title": "1 Introduction", "content": "Vision-language models (VLMs) are machine learning models trained on large bodies of text and images that can be used to accomplish a variety of tasks, including image search, text-to-image generation, and visual clas-sification [1]. They are increasingly considered for appli-cation in industry, entertainment, consumer products, and medical diagnosis thanks to a remarkable human-like ability to make broad associations between texts and images across a wide variety of domains. However, VLMs' ability to carry out fine-grained judgments is rel-atively unexplored.\nHans spontaneously and quickly form opinions about someone's trustworthiness, dominance, sincerity, intelligence, and other traits just by looking at their face in a photograph [2-5]. Such \"social perceptions\" are not always accurate and yet can influence the outcome of so-cial decision-making [6-10].\nHere we investigate whether VLMs may also be able to make social judgments on photographs of faces and, if so, how such judgments may be affected by facial ex-pression and other facial attributes. Understanding so-cially relevant behaviors of AI systems is necessary to use them responsibly, and this motivates our interest.\nWe focus on CLIP [11], a state-of-the-art open-source VLM that is widely used for zero-shot classification [11], image retrieval [12], and guiding generative text-to-image models [13-16]. CLIP models are trained on large vision-language-pair datasets [e.g., 17] gathered from the internet.\nDespite the fact that CLIP's training set covers the broadest set of topics, including landscapes, animals, man-made objects, and food, our study finds that CLIP indeed can make social judgments about faces in pic-tures.\nWe next ask whether CLIP's social judgments are related to human ones and, if so, whether such judg-ments may mirror some of the common human biases. Forms of social bias in CLIP have been previously doc-umented [12, 18\u201321], as has bias in CLIP-guided gener-ative text-to-image models (e.g., Stable Diffusion) [22-26]. For example, input-output bias has been measured by systematically prompting the model with language (such as \"show me a CEO\") and then analyzing the demographics of the generated or retrieved image output.\nWe approach the study of social judgment of face images by comparing its internal representation, com-"}, {"title": "2 Methods", "content": "2.1 Measuring social perception\nWe assess social perception in VLMs by computing the cosine similarities between text embeddings and im-age embeddings, following CLIP's image retrieval met-ric [11]. Using this metric, we calculate the similarity between a set of image embeddings I (e.g., all face im-ages of a demographic group) and a set of words D that represent a social perception dimension (see Tab. S.2) as\n$$cos(I, D) = mean_{i \\in I,d \\in D,t \\in T} (cos(i, t(d, t))),$$\nwhere t(d, t) is the text embedding of a prompt obtained by splicing a social trait adjective d into a prompt tem-plate t out of a predefined set of prompts T (e.g., \"a photo of a <adjective> person\u201d, see Tab. S.1 for all tem-plates). Averaging similarities over a set of templates is a common practice to make results more robust [44].\nIn addition, we introduce a new metric: the difference in cosine similarity compared to a neutral text prompt, i.e., when no adjective (denoted as \u201c\u00d8\u201d) is inserted in the template:\n$$\u0394 cos(i, d) = mean_{t \\in T} (cos(i, t(d, t)) \u2013 cos(i, t(\u00d8, t))$$\n$$\u0394 cos(I, D) = mean_{i \\in I,d \\in D} (\u0394cos(i, d)) .$$\nWe introduced this metric because we observed a bias between demographic groups when using neutral text prompts. When we consider the distributions of cosine similarities of CausalFace images using a neutral text prompt (e.g., \u201ca photo of a person\u201d), we find that Asian women have significantly higher and Black men signif-icantly lower cosine similarities compared to all other groups (p <= 0.01). Subtracting neutral prompt co-sine similarities normalizes away this \"ground\" bias and more clearly reveals social biases (see Appx. G for more details).\n2.2 Image datasets\nCausalFace\u00b9 is a synthetic face dataset introduced by Liang et al. [42]. Using Generative Adversarial Net-works (GANs), the authors create realistic-looking syn-thetic faces and vary each along six attributes (gender, age, race, lighting, viewpoint, facial expression) to ob-tain a balanced and controlled test set.\nIn order to obtain demographic diversity while bal-ancing all confounding variables, the authors proceeded in three steps. First, 100 random seeds are used to sam-ple as many \"seed faces\" from the GAN's latent space. Second, six \"prototypes\" are generated from each seed face by varying the seed as proposed by Balakr-ishnan et al. [45] to obtain each one of three different races (Asian, Black, and White) and binarized genders (women and men), thus producing neutral-expression frontally viewed faces of six distinct synthetic people, each one of which belongs perceptually to a different demographic group. The six prototype faces belong to the same pseudo-identity (or seed) and, thus, are as sim-ilar as possible in all other attributes (e.g., facial pro-portions, clothing, background) except their race and gender.\nThird, each prototype is varied in steps along age (+9 additional variations), smiling (+9), lighting (+7), and pose (+4). Consequently, CausalFace contains 30 (1 ini-tial plus 29 variations) face images per prototype. Those variations are performed for each of the six gender-race demographic groups. Therefore, we obtain 180 face im-ages per seed2. Overall, we used images from 100 differ-ent seeds, six prototypes per seed, and 30 variations per-"}, {"title": "2.3 Comparing induced variation", "content": "Which attributes of the face image affect social judg-ment the most? CausalFace varies each attribute sys-tematically and independently. Thus, it uniquely en-ables us to measure how differences in legally protected attributes (e.g., age, race, gender), as well as unpro-tected attributes (facial expressions, lighting, and pose), affect differences in social judgment perception. Given the varying number of values within each attribute (for instance, \u201csmiling\" has ten values (Fig. S.9), whereas \"gender\" has two values men and women), we use a sampling strategy to ensure a fair comparison.\nSpecifically, we randomly choose pairs of images with two distinct values, X1,X2 CX, of the chosen attribute X (e.g., images with values White and Black for at-tribute race) and obtain the respective CLIP image em-beddings, i\u2081 (x = x1), i2(x = x2). Importantly, all other attributes except the varied one are held constant (in this example, attribute race is varied, and attributes gender, age, smiling, lighting, and pose are held con-stant).\nWe then compute the absolute difference in A co-sine similarities of the two image embeddings w.r.t. a social dimension D, defined as AbsDiff(i1, i2, D) = |\u2206 cos(i1, D) \u2013 \u2206cos(i2, D)|. This process is repeated 1,000 times for each of the eight social dimensions, gen-erating a bootstrap distribution of 8,000 AbsDiff values for each of the seven attributes. These distributions describe the impact of the specific attribute on the co-sine similarity of image embeddings with the valenced text embeddings. For ordinal attributes such as age and smiling, we introduce additional constraints. To ensure a perceptually significant change in image appearance, we mandate that two samples must differ by at least a certain threshold. These are set at 1.1 for age and smil-ing. In other words, we set these constraints so as not to pick two images that look too similar. The impact of the thresholds can be more easily understood by study-ing the scale of smiling and age, depicted in Fig. S.8 and Fig. S.9. The method described in this section specifi-cally refers to the results discussed in Fig. 2."}, {"title": "3 Results", "content": "CausalFace images are statistically similar to real photographs. CausalFace images look very realistic. However, they are artificial, and it is reasonable to be concerned that they may differ from photographs of real people in ways that may affect our analysis. Therefore, we explored whether the statistical properties of Causal-Face CLIP embeddings differ systematically from those of photographs of real people. To this end, we com-pared CausalFace to two observational datasets, Fair-Face [47] and UTKFace [48], with respect to six com-monly used metrics: mean cosine similarities, marked-"}, {"title": "4 Discussion", "content": "We investigated whether the popular CLIP vision-language model is able to make social judgments from photographs of human faces and whether attributes of such images affect social perception. We quantified so-cial perception by comparing CLIP embeddings of face images with embeddings of text prompts from two the-ories of social psychology. Our test images come from a synthetic dataset-CausalFace in which image at-tributes were varied systematically and independently, allowing us to investigate the effect of each attribute on social perception, as well as the relative importance of legally protected and non-protected image attributes that are not labeled in observational datasets.\nWe find that CLIP exhibits human-like social per-ception. Similarly to human observers, it will classify"}]}