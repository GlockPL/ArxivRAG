{"title": "Social perception of faces in a vision-language model", "authors": ["Carina I. Hausladen", "Manuel Knott", "Colin F. Camerer", "Pietro Perona"], "abstract": "We explore social perception of human faces in CLIP, a widely used open-source vision-language model. To this end, we compare the similarity in CLIP embeddings between different textual prompts and a set of face images. Our textual prompts are constructed from well-validated social psychology terms denoting social perception. The face images are synthetic and are systematically and independently varied along six dimensions: the legally protected attributes of age, gender, and race, as well as facial expression, lighting, and pose. Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes. Thus, our findings are experimental rather than observational. Our main findings are three. First, while CLIP is trained on the widest variety of images and texts, it is able to make fine-grained human-like social judgments on face images. Second, age, gender, and race do systematically impact CLIP's social perception of faces, suggesting an undesirable bias in CLIP vis-a-vis legally protected attributes. Most strikingly, we find a strong pattern of bias concerning the faces of Black women, where CLIP produces extreme values of social perception across different ages and facial expressions. Third, facial expression impacts social perception more than age and lighting as much as age. The last finding predicts that studies that do not control for unprotected visual attributes may reach the wrong conclusions on bias. Our novel method of investigation, which is founded on the social psychology literature and on the experiments involving the manipulation of individual attributes, yields sharper and more reliable observations than previous observational methods and may be applied to study biases in any vision-language model.", "sections": [{"title": "1 Introduction", "content": "Vision-language models (VLMs) are machine learning models trained on large bodies of text and images that can be used to accomplish a variety of tasks, including image search, text-to-image generation, and visual classification [1]. They are increasingly considered for application in industry, entertainment, consumer products, and medical diagnosis thanks to a remarkable human-like ability to make broad associations between texts and images across a wide variety of domains. However, VLMs' ability to carry out fine-grained judgments is relatively unexplored.\nHumans spontaneously and quickly form opinions about someone's trustworthiness, dominance, sincerity, intelligence, and other traits just by looking at their face in a photograph [2-5]. Such \"social perceptions\" are not always accurate and yet can influence the outcome of social decision-making [6-10].\nHere we investigate whether VLMs may also be able to make social judgments on photographs of faces and, if so, how such judgments may be affected by facial expression and other facial attributes. Understanding so-cially relevant behaviors of AI systems is necessary to use them responsibly, and this motivates our interest.\nWe focus on CLIP [11], a state-of-the-art open-source VLM that is widely used for zero-shot classification [11], image retrieval [12], and guiding generative text-to-image models [13-16]. CLIP models are trained on large vision-language-pair datasets [e.g., 17] gathered from the internet.\nDespite the fact that CLIP's training set covers the broadest set of topics, including landscapes, animals, man-made objects, and food, our study finds that CLIP indeed can make social judgments about faces in pictures.\nWe next ask whether CLIP's social judgments are related to human ones and, if so, whether such judgments may mirror some of the common human biases. Forms of social bias in CLIP have been previously documented [12, 18\u201321], as has bias in CLIP-guided generative text-to-image models (e.g., Stable Diffusion) [22\u201326]. For example, input-output bias has been measured by systematically prompting the model with language (such as \"show me a CEO\") and then analyzing the demographics of the generated or retrieved image output.\nWe approach the study of social judgment of face images by comparing its internal representation, com-"}, {"title": "2 Methods", "content": "We assess social perception in VLMs by computing the cosine similarities between text embeddings and image embeddings, following CLIP's image retrieval metric [11]. Using this metric, we calculate the similarity between a set of image embeddings I (e.g., all face images of a demographic group) and a set of words D that represent a social perception dimension (see Tab. S.2) as\n$\\cos(I, D) = \\underset{i \\in I, d \\in D, t \\in T}{\\text{mean}} (\\cos(i, t(d, t)))$,\n(1)\nwhere t(d, t) is the text embedding of a prompt obtained by splicing a social trait adjective d into a prompt template t out of a predefined set of prompts T (e.g., \"a photo of a <adjective> person\u201d, see Tab. S.1 for all templates). Averaging similarities over a set of templates is a common practice to make results more robust [44].\nIn addition, we introduce a new metric: the difference in cosine similarity compared to a neutral text prompt, i.e., when no adjective (denoted as \u201c\u00d8\u201d) is inserted in the template:\n$\\Delta cos(i, d) = \\underset{t \\in T}{\\text{mean}} (\\cos(i, t(d, t)) \u2013 \\cos(i, t(\\O, t))$\n$\\Delta cos(I, D) = \\underset{i \\in I, d \\in D}{\\text{mean}} (\\Delta cos(i, d)) $.\n(2)\nWe introduced this metric because we observed a bias between demographic groups when using neutral text prompts. When we consider the distributions of cosine similarities of CausalFace images using a neutral text prompt (e.g., \u201ca photo of a person\"), we find that Asian women have significantly higher and Black men significantly lower cosine similarities compared to all other groups (p <= 0.01). Subtracting neutral prompt cosine similarities normalizes away this \"ground\" bias and more clearly reveals social biases (see Appx. G for more details).\nCausalFace\u00b9 is a synthetic face dataset introduced by Liang et al. [42]. Using Generative Adversarial Networks (GANs), the authors create realistic-looking synthetic faces and vary each along six attributes (gender, age, race, lighting, viewpoint, facial expression) to obtain a balanced and controlled test set.\nIn order to obtain demographic diversity while balancing all confounding variables, the authors proceeded in three steps. First, 100 random seeds are used to sample as many \"seed faces\" from the GAN's latent space. Second, six \"prototypes\" are generated from each seed face by varying the seed as proposed by Balakrishnan et al. [45] to obtain each one of three different races (Asian, Black, and White) and binarized genders (women and men), thus producing neutral-expression frontally viewed faces of six distinct synthetic people, each one of which belongs perceptually to a different demographic group. The six prototype faces belong to the same pseudo-identity (or seed) and, thus, are as similar as possible in all other attributes (e.g., facial proportions, clothing, background) except their race and gender.\nThird, each prototype is varied in steps along age (+9 additional variations), smiling (+9), lighting (+7), and pose (+4). Consequently, CausalFace contains 30 (1 initial plus 29 variations) face images per prototype. Those variations are performed for each of the six gender-race demographic groups. Therefore, we obtain 180 face images per seed. Overall, we used images from 100 different seeds, six prototypes per seed, and 30 variations per\""}, {"title": "2.3 Comparing induced variation", "content": "Which attributes of the face image affect social judgment the most? CausalFace varies each attribute systematically and independently. Thus, it uniquely enables us to measure how differences in legally protected attributes (e.g., age, race, gender), as well as unprotected attributes (facial expressions, lighting, and pose), affect differences in social judgment perception. Given the varying number of values within each attribute (for instance, \u201csmiling\u201d has ten values (Fig. S.9), whereas \"gender\" has two values men and women), we use a sampling strategy to ensure a fair comparison.\nSpecifically, we randomly choose pairs of images with two distinct values, X\u2081,X\u2082 CX, of the chosen attribute X (e.g., images with values White and Black for attribute race) and obtain the respective CLIP image embeddings, i\u2081(x = x\u2081), i\u2082(x = x\u2082). Importantly, all other attributes except the varied one are held constant (in this example, attribute race is varied, and attributes gender, age, smiling, lighting, and pose are held constant).\nWe then compute the absolute difference in \u2206 cosine similarities of the two image embeddings w.r.t. a social dimension D, defined as AbsDiff(i\u2081, i\u2082, D) = |\u2206 cos(i\u2081, D) \u2013 \u2206cos(i\u2082, D)|. This process is repeated 1,000 times for each of the eight social dimensions, generating a bootstrap distribution of 8,000 AbsDiff values for each of the seven attributes. These distributions describe the impact of the specific attribute on the cosine similarity of image embeddings with the valenced text embeddings. For ordinal attributes such as age and smiling, we introduce additional constraints. To ensure a perceptually significant change in image appearance, we mandate that two samples must differ by at least a certain threshold. These are set at 1.1 for age and smiling. In other words, we set these constraints so as not to pick two images that look too similar. The impact of the thresholds can be more easily understood by studying the scale of smiling and age, depicted in Fig. S.8 and Fig. S.9. The method described in this section specifically refers to the results discussed in Fig. 2."}, {"title": "3 Results", "content": "CausalFace images are statistically similar to real photographs. CausalFace images look very realistic. However, they are artificial, and it is reasonable to be concerned that they may differ from photographs of real people in ways that may affect our analysis. Therefore, we explored whether the statistical properties of CausalFace CLIP embeddings differ systematically from those of photographs of real people. To this end, we compared CausalFace to two observational datasets, FairFace [47] and UTKFace [48], with respect to six commonly used metrics: mean cosine similarities, marked-"}, {"title": "4 Discussion", "content": "We investigated whether the popular CLIP vision-language model is able to make social judgments from photographs of human faces and whether attributes of such images affect social perception. We quantified social perception by comparing CLIP embeddings of face images with embeddings of text prompts from two theories of social psychology. Our test images come from a synthetic dataset\u2014CausalFace in which image attributes were varied systematically and independently, allowing us to investigate the effect of each attribute on social perception, as well as the relative importance of legally protected and non-protected image attributes that are not labeled in observational datasets.\nWe find that CLIP exhibits human-like social perception. Similarly to human observers, it will classify"}]}