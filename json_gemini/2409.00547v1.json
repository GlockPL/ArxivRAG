{"title": "Data Augmentation for Image Classification using Generative AI", "authors": ["Fazle Rahat", "M Shifat Hossain", "Sumit Kumar Jha", "Md Rubel Ahmed", "Rickard Ewetz"], "abstract": "Scaling laws dictate that the performance of AI models is proportional to the amount of available data. Data augmentation is a promising solution to expanding the dataset size. Traditional approaches focused on augmentation using rotation, translation, and resizing. Recent approaches use generative AI models to improve dataset diversity. However, the generative methods struggle with issues such as subject corruption and the introduction of irrelevant artifacts. In this paper, we propose the Automated Generative Data Augmentation (AGA). The framework combines the utility of large language models (LLMs), diffusion models, and segmentation models to augment data. AGA preserves foreground authenticity while ensuring background diversity. Specific contributions include: i) segment and superclass based object extraction, ii) prompt diversity with combinatorial complexity using prompt decomposition, and iii) affine subject manipulation. We evaluate AGA against state-of-the-art (SOTA) techniques on three representative datasets, ImageNet, CUB and iWildCam. The experimental evaluation demonstrates an accuracy improvement of 15.6% and 23.5% for in and out-of-distribution data compared to baseline models respectively. There is also 64.3% improvement in SIC score compared to the baselines.", "sections": [{"title": "1. Introduction", "content": "Deep learning models often struggle with domain adaptation when exposed to new conditions, such as attacks [1-3], changes in weather [4,5], and geographic locations [6,7]. This issue is particularly evident in applications like rare bird or animal species identification, where insufficient training data can hinder the model's ability to generalize effectively [8]. Adding more training data from diverse domains can help alleviate this issue; however, collecting high-quality and relevant data is inherently costly [9].\nSignificant research efforts have been dedicated to traditional data augmentation approaches based on geometric modifications, including cropping, translations, and rotations [8]. The limitations of these techniques is that the subject features may be altered and the limited image diversity. On the other hand, the recent advancements within generative AI is providing new opportunities for data augmentation [8] using large language models (LLMs) [10], vision-language models (VLMs) [11, 12], image synthesis models [13,14]. In particular, the ability to synthesize photo realistic images from natural language [15-17]. These models demonstrate exceptional performance on various tasks such as text-to-image generation [18, 19], image-to-image modification [20, 21], and image inpainting [22]. Recent work shows that large-scale diffusion models can be fine-tuned to generate augmented images for improving recognition tasks [19]. While fine-tuning image generation models for data augmentation is effective, their complexity and the need for replication across diverse datasets often make it impractical [23]. Methods for augmenting visually realistic images using text-guided techniques without model fine-tuning are proposed in [21,23,24]. However, our case study indicates that diffusion models struggle to augment fruitful training data from text prompts alone, often deviating from the intended subjects in the generated images.\nIn this paper, we propose the Automated Generative Data Augmentation framework called AGA to augment the training dataset to enhance fine-grained classification performance. Our method aims to alter the subjects minimally while introducing variability in the backgrounds during the augmentation process. AGA uses image segmentation to isolate subjects, a pre-trained LLM for varied background captions, Stable Diffusion for diverse background creation, and integrates subjects seamlessly with backgrounds. Au-"}, {"title": "2. Related Works", "content": "Image augmentation is a pivotal method for improving the performance and generalization ability of deep learn-ing models. Early works often resort to geometric transformations such as flipping, cropping, and rotation, color space transformations, kernel filters [8]. Beyond simple manipulation, advanced techniques like Mixup [25] and CutMix [26] introduce advanced techniques such as mixing images to create new training examples and encourage the model to learn more robust representations. Additionally, automated augmentation methods such as RandAugment [27] randomly select and apply a sequence of transformations with varying magnitudes, eliminating the need for manual tuning of augmentation hyperparameters. However, these techniques often generate images which are not only visually unnatural [21] but also loses subject information.\nWith the advent of generative AI models, particularly diffusion models, image augmentation has witnessed a paradigm shift and these models are widely adopted in image generation [15, 18, 19, 21, 23]. Large-scale image-text datasets and models like CLIP [28] have enabled SOTA diffusion models to perform versatile tasks such as text-to-image generation, image-to-image transformation, and in-painting through text-guided prompts. Several studies have investigated how to enhance the classification accuracy using synthetic images generated by diffusion models [29,30]. One study showed that it is possible to train a classifier for ImageNet solely using synthetic Data, leading to a performance improvement when applied to real-world tasks [31]. While another investigation demonstrates the effectiveness of fine-tuning Imagen [32] for data augmentation on ImageNet [19]. These fine-tuning based approaches face practical challenges due to complexity, cost, and dataset-specific requirements. Recent works utilize off-the-shelf diffusion models to diversify vision datasets without the need for fine-tuning [21,23].\nMethods for attribution analysis of neural networks that"}, {"title": "3. A Motivating Case Study", "content": "Text-to-image, image-to-image, and inpainting are three key image augmentation techniques extensively utilized in recent image augmentation works. We conducted a case study of these methods for several datasets like ImageNet [51], CUB [52], and iWildCam [53], to understand their advantages and shortcomings. We discuss our observations using a representative image of a bird from the CUB data set, as illustrated in Figure 1.\nText-to-image: It can be observed that text-to-image, while capable of generating a high diversity of images, often produces samples in which the subject is so drastically altered that even human observers struggle to identify it. We find in the figure that the identifying mark of the bird, the red ring around the neck, is missing in the augmented images, which would translate into failures for downstream tasks.\nImage-to-image: This type of augmentation frequently results in a significant loss of subject detail, akin to the image-to-text method. We see that the bird is very hard to spot in the augmented image, which might make the downstream object detection task tougher. The prairie chicken in the example appears to have been transformed into a parrot.\nInpainting: This method operates by modifying the image within a masked area based on a text prompt, yet this method can inadvertently corrupt the subject's appearance. We see that the orange/red identifying ring misplaced in one of the augmented images, thus this method can corrupt the subject.\nIn contrast, our proposed approach does not add any ar-"}, {"title": "4. Automated Generative Data Augmentation", "content": "In this section, we present the methodology of the AGA framework. The input to the framework is an image and the corresponding class name. The output is an augmented image based on the provided inputs. The framework augments an image in three main steps: i) subject isolation through masked image generation, ii) the generation of domain-specific captions for diverse backgrounds, and iii) augmented image editing for combining the foreground and background. An overview of the AGA framework is shown in Figure 2."}, {"title": "4.1. Masked Image Generation", "content": "This step deals with isolating the subject of an input image from its background. In general, such subject masks are not readily available beside the image and class name. Therefore, dense mask estimation models can be used to correctly generate pixel-level masks for subjects using the image and text (class name) only.\nAGA includes Segment Anything Model (SAM) [49], one of the SOTA image segmentation tool, for this purpose. SAM is capable of segmenting the subject from an image based on some guiding inputs such as single or multiple point locations on an object, or the object's bounding box, to create precise segmentation masks. As the training dataset usually does not include the point locations or bounding box for the subject, object detection models can be utilized to generate the boxes in this regard. Bounding boxes provide approximate spatial locations of objects of our interest in the image.\nThere are several SOTA object detection models available, like YOLO [54], GoundingDINO [50]. In AGA workflow, GroundingDINO model is used to generate bounding box due to its superior performance. Empirical analysis shows that for fine-grained text prompts, GroundingDINO often fails to provide optimal bounding box results. For instance, when attempting to locate the bounding box for a specific bird class such as water ouzel, the hierarchical naming of the class text bird proves more effective than the fine-grained class name. Therefore, AGA utilizes super-classes as text prompts to provide clearer input instructions to the GroundingDINO model for object bounding box creation.\nThe details of proposed mask generation process is shown in Figure 3. GroundingDINO generates the bound-"}, {"title": "4.2. Domain Caption Generation", "content": "In the AGA pipeline, the generation of domain captions is a crucial task, as it directly influences the diversity of the background images produced. These captions are automatically generated through a two-step process using a prompt generation engine. Initially, the engine samples from three predefined sets: the instruction set (\\(Ins\\)), the background set (\\(Bgr\\)), and the temporal modality set (\\(Temp\\)) as the prompt fixers. The instruction set ensures the prompt be-gins with an appropriate command, the background set in-"}, {"title": "4.3. Augmented Image Generation", "content": "In this step, utilizing the masked image obtained from Section 4.1 and the background caption prompt from Section 4.2, AGA generates a new image with an altered background. The caption prompt serves as the input for a large vision model, which is responsible for creating the background image. Among several text-to-image generation models available, such as DALL-E [55], Imagen [32], and Stable Diffusion [18], AGA employs the Stable Diffusion model for this purpose. Once we have both the masked image and the background image produced by the vision model, AGA proceeds to create the new augmented image. The merging technique used ensures that the background"}, {"title": "5. Experimental Evaluation", "content": "We implement the AGA framework in Python, utilizing open source APIs for machine learning models. The implementation runs on a machine equipped with an NVIDIA A100 graphics card. The following sections provide detailed descriptions of the dataset preparation, evaluation setup, and key findings.\nSetup. We created a subset of the ImageNet [51] dataset, named ImageNet10, by randomly selecting 10 classes. This subset comprises 13,046 and 500 training and validation images across the following classes. We refer to this train-set as the original dataset and generate synthetic images from it using the AGA methodology. In subsequent discussions, models described as trained with augmented data refer to those trained using both the original and augmented datasets. Beside this ImageNet10 dataset, we utilize iWildCam [53] dataset, which contains a large collection of global camera trap images. Similarly, we extend our experiments to the CUB [52] dataset, a fine-grained classification set of 200 bird species from Flickr. We maintain the same data distribution ratio as in the previous work' [21] for train and test set to ensure a fair comparison. Detailed dataset descriptions are included in the supplementary materials.\nWe evaluate the AGA method across two main categories. First, we conduct an in-distribution evaluation of our pipeline using the ImageNet validation dataset for each model. Second, we assess the robustness of our augmentation method by evaluating it on out-of-distribution ImageNet data. For this we use the ImageNet variations: ImageNet-Sketch [56] and ImageNet-V2 [57] where ImageNet-Sketch is the sketch version and ImageNet-V2 is the reproduced version of ImageNet respectively. The CUB and iWildCam datasets are used to conduct a comparison study with the previous work [21]. We consider two types of models for our experiments: those trained with original image data and those trained with augmented image data. For comparison, we maintain baseline hyperparameters while augmenting the original training data with augmented data at various scales.\nWe also compare with other augmentation techniques from recent times: (1) MixUp [25], a data augmentation technique improves deep learning model generalization by creating virtual training examples through convex combinations of original data points and labels, to enhance model robustness and performance on unseen data. (2) CutMix [26], an approach that creates mixed samples by randomly slicing and combining patches from multiple training images to capture finer and more distinct features across localized regions. (3) RandAugment [27], simplifies data augmentation by reducing the search space for augmentation strategies, automating the selection of operations and magnitudes to enhance model performance and generalization while minimizing computational overhead. (4) ALIA [21] analyzes training images to identify diverse background captions, then uses this information to create variations of the images with different backgrounds and contexts. After that,"}, {"title": "5.1. Fine-grained Image Classification", "content": "Accuracy vs. Degree of Data Augmentation: We first evaluate the improvement in classification accuracy with respect to the amount of data augmentation in Figure 6. The figure shows the classification accuracy of a ResNet-50 and a ResNet-101 model on the ImageNet10 dataset. It can be observed that the classification accuracy rapidly improves for data augmentation in the range of 1X to 3X. After that, there are still average improvements but not as significant.\nIn contrast to prior work by Azizi et al. [19], which re-ported performance degradation in ResNet-50 classification accuracy when the size of augmented data exceeded four times the original dataset. The results indicate that with AGA, the performance of both ResNet-50 and ResNet-101 models increases with the scale of synthetic data augmentation. Specifically, we scale up to ten times the original size of the ImageNet10 dataset, which contains approximately 13,000 images. As illustrated in Figure 6, validation accuracy trends upwards as the dataset size increases, without the performance degradation observed in the prior study. This suggests that AGA does not compromise baseline performance, even at high augmentation scales.\nComparison with SOTA: We now turn our attention to comparing the performance of AGA with previous approaches to data augmentation on the ImageNet10, iWild, and CUB data sets, which is shown in Figure 8. The figure shows the performance of AGA with data augmentation up to 3X, MixUP [25], CutMix [26], RandAug [27],and ALIA [21]. Recall that former three methods are traditional data augmentation methods while the latter is based on generative AI. We only show results of RandAug and ALIA on iWild and CUB because the source code cannot easily be executed on ImageNet10 dataset."}, {"title": "5.2. Evaluation of Impact on Generalizability", "content": "Machine learning models typically struggle with out-of-distribution data, but models trained with AGA-augmented data show commendable performance in such cases. We assess our image augmentation method on ImageNet-Sketch and ImageNet-V2 datasets, training the CNN models with both original images and a combination of original and augmented images. For evaluation, we adhere to the same validation dataset across all models. The ImageNet-Val dataset is used for in-distribution testing, while validation data from ImageNet-Sketch and ImageNet-V2 are used for out-of-distribution testing.\nOur results are summarized in Table 1, which includes performance metrics for four ResNet models on both in-distribution and out-of-distribution data, with the specific gains over baseline models quantified as A in the table. The table highlights up to 15.6% improvements in accuracy for the ResNet-101 model for ImageNet10-Val when trained with AGA augmented data. We also see significant performance improvements for out-of-distributions, proving the fact that AGA augmented data increase generalizability of fine-grained classification models."}, {"title": "5.3. Evaluation of Impact on Explainability", "content": "Explainability is an increasingly critical aspect of AI, particularly in understanding how machine learning models make decisions. Our study explores the impact of subject-oriented data augmentation provided by AGA on model explainability. By enhancing image diversity through augmentation, we aim to develop more robust and interpretable classifiers. We train models on both the baseline ImageNet10 dataset and augmented data to compare performance. For visualizing how models focus on relevant areas within images, we employ GradCam [61], a tool that high-lights significant regions influencing model decisions. In our findings, as shown in Figure 9, we compare models at the 85\\(^{th}\\) epoch, trained solely on real ImageNet10 data and those trained on ImageNet10 augmented data. The model trained only on real data incorrectly classifies three specific images (Figure 9a), whereas the model trained with AGA-augmented data correctly identifies these images. GradCam visualizations reveal that the baseline model often focuses on irrelevant pixels, whereas the AGA-trained model more accurately targets pixels within the subject area. This ex-"}, {"title": "6. Conclusion and Future Work", "content": "We introduce AGA, a novel data augmentation method designed to address data scarcity in fine-grained image recognition. Our approach integrates image segmentation, automated background caption generation, and diffusion-based image synthesis to diversify backgrounds while maintaining the subject's integrity, thus enhancing training datasets for improved fine-grained classification performance, especially in low-data situations. AGA reveals that additional generated data assists the deep learning model in concentrating on the expected subject regions, as evidenced by the Grad-CAM attribution method. The framework also demonstrates strong generalization on out-of-distribution data. AGA experiences compatibility issues concerning proper subjects and backgrounds, and occasionally pro-duces visually inconsistent synthetic images by combin-ing subjects with inappropriate backgrounds. This limita-tion underscores the potential for future research to explore new methods for generating images that maintain subject integrity while ensuring compatibility with backgrounds."}, {"title": "A. Appendix", "content": "In this appendix, we provide supplementary technical details and experiments that could not fit within the main manuscript. We present detailed information about all ref-erence datasets used, encompassing training and validation samples, in Section A.1. All the information about the CNN model training procedure, as well as details about all the hyperparameters used during training and validation, is shown in Section A.2. Lastly, we present qualitative visualization results, encompassing synthetic images generated by the AGA, GradCam visualization heatmaps for enhanced explainability, and UMAP plots depicting feature clusters to assess the quality of generated image features in Section A.3."}, {"title": "A.1. Additional Dataset Details", "content": "In this section, we present additional details about all the representative datasets we used to evaluate our proposed method AGA. We use the ImageNet10 dataset, which is a subset of the original ImageNet dataset [51] with 10 different classes. These are chickadee (n01592084), water ouzel (n01601694), loggerhead (n01664065), box turtle (n01669191), garter snake (n01735189), sea snake (n01751748), black and gold garden spider (n01773157), tick (n01776313), ptarmigan (n01796340), prairie chicken (n01798484). We use the training and validation sets from ImageNet [51] for these 10 classes. We also utilize the iWildCam [53] dataset, which contains a large collection of global camera trap images of 7 different classes of background, elephant, impala, cattle, zebra, dik-dik, and giraffe, and the CUB [52] dataset, a fine-grained classification set of 200 bird species from Flickr. We maintain the same data distribution ratio as in the previous work [21] for the train and test sets to ensure a fair comparison. To show the robustness and generalization capability of our method, we additionally use two other datasets named ImageNet-Sketch [56] and ImageNet-V2 [57], where ImageNet-Sketch is the sketch version and ImageNet-V2 is the reproduced version of ImageNet. We utilize ImageNet-Sketch and ImageNet-V2 to validate the robustness of AGA against out-of-distribution samples. The number of training and validation images used for evaluation is presented in Table 3."}, {"title": "A.2. Training and Hyperparameter Details", "content": "Our automatic image augmentation framework AGA starts by separating the main subjects in the image using segmentation methods. Then, it uses a large language model (LLM) to generate different captions of backgrounds. These captions are fed into a vision model like Stable Diffusion to create various backgrounds. In the end, AGA combines the separated subjects with the newly created backgrounds. We utilize a Llama-2-13B-GPTQ from Hugging-Face [60] to create background image captions and Stable Diffusion XL [13] text-to-image model to generate background image, with default hyperparameters.\nAfter the generation of augmented images we evaluate the quality of the additional data samples using several CNN classifier models. We employ ResNet variants 18, 50, 101, 152 as the classification models for training. We train these CNN models from scratch using PyTorch's standard training script [58] which includes PyTorch's default hyperparameter set [59]. All the hyperparameter values used for CNN classifier training are presented in Table 4. We train all the classifier models multiple times and report the average performance. While training these CNN models, we carefully addressed the issue of overfitting. We often refer to the maximum classifier accuracy for any epoch by avoiding overfitting. We ensure this by using the training and validation loss. We train all the models in such a way that the difference between the training and validation loss is minimized. The training and validation losses of ImageNet10 training are presented in Figure 10, with real data shown in 10a and augmented data in 10b. The x-axis represents the number of epochs, and the y-axis represents both accuracy and loss values. In both cases, the red vertical dashed line represents the epoch at which we achieve the maximum validation accuracy. We observe that both training and validation losses exhibit downward trends in Figure 10 for both base and AGA model training. No such scenario is detected where training loss keeps decreasing while validation loss starts to increase. The validation loss remains relatively stable and doesn't show a significant upward trend. This indicates the model is generalizing well to unseen data and clearly shows no signs of overfitting."}, {"title": "A.3. Additional Results", "content": "We present additional synthetic images generated by AGA in Section A.3.1. We also exhibit more GradCam visualization results to demonstrate the improved explain-ability of the classifier model trained with augmented data samples compared to one trained with only real samples in Section A.3.2. Moreover, we display the CNN model-extracted features in a UMAP plot, showing feature clusters for different classes of real and augmented images in Sec-tion A.3.3."}, {"title": "A.3.1 Additional Synthetic Images", "content": "We present more generated images from real image with di-verse backgrounds. Figure 11, 12 and 13 display multiples"}, {"title": "A.3.2 GradCam Visualization Results", "content": "We present additional GradCam visualization results here to show the explanable capability of the base model and AGA model for ImageNet 10. The base model classifier is trained with only the real images of the ImageNet10 dataset, but the AGA model is trained with real images as well as augmented images generated by the AGA method.\nWe demonstrate several validation dataset samples of ImageNet10 that are misclassified by base model in Figure 14. On the other hand, the AGA model correctly classified these data samples, and the following GradCam visualizations reveal that the baseline model often focuses on irrelevant pixels, whereas the AGA-trained model more accurately targets pixels within the subject area."}, {"title": "A.3.3 Feature Cluster", "content": "We conducted an additional experiment to verify that additional synthetic images do not introduce irrelevant features. We utilize the last-layer feature outputs from the ResNet-50 model for both ImageNet10 real and AGA-augmented images. Each image yields 2048 features, which we use to plot feature clusters. We illustrate five distinct class clusters of ImageNet10 for both real and AGA-augmented images in Figure 15. The figure shows that additional generated im-ages enhance cluster density without significantly increas-"}]}