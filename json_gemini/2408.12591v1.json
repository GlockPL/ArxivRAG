{"title": "Differentiable Logic Programming for Distant Supervision", "authors": ["Akihiro Takemura", "Katsumi Inoue"], "abstract": "We introduce a new method for integrating neural networks with logic programming in Neural-Symbolic AI (NeSy), aimed at learning with distant supervision, in which direct labels are unavailable. Unlike prior methods, our approach does not depend on symbolic solvers for reasoning about missing labels. Instead, it evaluates logical implications and constraints in a differentiable manner by embedding both neural network outputs and logic programs into matrices. This method facilitates more efficient learning under distant supervision. We evaluated our approach against existing methods while maintaining a constant volume of training data. The findings indicate that our method not only matches or exceeds the accuracy of other methods across various tasks but also speeds up the learning process. These results highlight the potential of our approach to enhance both accuracy and learning efficiency in NeSy applications.", "sections": [{"title": "1 Introduction", "content": "Neural-Symbolic AI (NeSy) [15, 16] is a field of research aimed at combining neural networks with symbolic reasoning. While deep learning is capable of learning complex representations from input-output pairs, it requires a large amount of training data and struggles with tasks that require logical reasoning. On the other hand, learning with symbolic reasoning can be done with small amounts of data, but it is sensitive to noise and unable to handle non-symbolic data. In NeSy, it is crucial to combine the roles of neural networks and symbolic reasoning in a way that leverages their respective strengths.\nThere are various methods for implementing NeSy, including associating the continuous-valued parameters of neural networks (NN) with logical language and using the results of logical reasoning as the value of the loss function (e.g., semantic loss [32]). Also known are methods that combine symbolic solvers with neural networks, e.g., DeepProbLog [19] uses Problog for probabilistic logic program-ming, and NeurASP [33] uses clingo for answer set programming (ASP). These methods that internally call solvers often encapsulate computationally expensive problems such as weighted model count-ing or enumerating stable models in each iteration during learning.\nAlternative methods have been proposed that embed inference tra-ditionally done by symbolic reasoning solvers into vector spaces and perform symbolic reasoning using linear algebra [24]. One such method embeds logic programs into vector spaces and designs appro-priate loss functions based on the semantics of non-monotonic rea-soning to compute the results of reasoning in a differentiable manner [2, 28]. However, these methods have issues such as not being able to directly handle logical constraints and not being applicable to neu-ral network learning as is. Thus, in this paper, we propose a method that enables learning in neural networks for NeSy tasks using logical programs that include constraints.\nDistant supervision is a method of generating labeled data for learning using rules, external data, or knowledge bases and was pro-posed by Mintz et al. [21] as a method to train classifiers for rela-tion extraction based on information from knowledge bases. In NeSy, tasks where label information is provided through symbolic reason-ing are commonly used, with MNIST Addition [19] being a repre-sentative task. In this task, pairs of handwritten digits are input, and the goal is to learn the classification of handwritten digits with the sum of the digits provided as the label (e.g., 2+3= 5). Unlike the usual MNIST classification, in MNIST Addition, the labels are not given for each image individually. In this case, the relationship be-tween the sum given as the label and the digits corresponding to the images is expected to be provided through symbolic reasoning.\nIn this paper, we propose a novel architecture for NeSy systems [19, 33] that integrates differentiable logic programming [2, 28] and neural networks. This paper makes the following contributions:\n1. We propose a novel architecture that integrates neural networks with logic programming through a differentiable approach. This method facilitates the direct evaluation of logical implications and constraints using differentiable operations, thus enabling effective learning under distant supervision without relying on symbolic solvers for reasoning about missing labels.\n2. We demonstrate through experiments with a constant volume of training data that our proposed method not only matches but, in some cases, exceeds the accuracy of existing approaches that utilize symbolic solvers. Moreover, we achieved a significant reduction in the training time for neural networks, highlighting substan-tial gains in computational efficiency.\nThe structure of this paper is as follows. After the preliminaries in Section 2, Section 3 introduces the logic programming semantics in vector spaces. Section 4 presents our proposed method for using differentiable logic programming for distant supervision. Section 5 presents the results of experiments and comparison to the state of the art NeSy methods. Section 6 covers the related works in the literature. Finally, Section 7 presents the conclusion."}, {"title": "2 Preliminaries", "content": "A normal logic program P is a set of rules of the form:\n$A \\leftarrow A_1 \\land ... A_m \\lnot A_{m+1} \\land ... \\lnot A_n$ (1)\nwhere A and $A_i (n \\geq m > 0)$ are atoms. In this paper, the terms 'normal logic program', 'logic program', and 'program' are used in-terchangeably. An atom is a predicate with some arity, e.g., p(X, Y), where variables are represented by upper case characters, and pred-icates and constants are represented by lower case characters. A lit-eral is either an atom p, or its negation \u00acp. The atom A in (1) is the head and {A1,..., An} is the body of a rule. For each rule Ri of the form (1), define head(Ri) = A, $body^+(R_i) = {A_1,..., A_m}$ and $body^-(R_i) = {A_{m+1},..., A_n}$.\nThe Herbrand universe of a logic program P is the set of all ground terms in the language of P, i.e., terms composed of function symbols and constants that appear in P. The Herbrand base BP is the set of atoms that can be formed from the relations of the program and terms in the Herbrand universe. We assume that the Herbrand base Bp of a program to be lexicographically ordered.\nA rule with an empty body is a fact. A program P is definite if no rule in P contains negation as failure. A program, a rule, or an atom is ground if it is variable free. A program P is semantically identified with its ground instantiation, ground(P), by substituting variables in P by elements of its Herbrand universe in every possible way.\nAn interpretation IC Bp satisfies a rule Ri of the form (1) if $body^+(R_i) \\subseteq I$ and $body^-(R_i) \\cap I = \\emptyset$ imply A \u2208 I. An inter-pretation that satisfies every rule in a program P is a model of the program. A model of a program is supported if for each atom p \u2208 I, there exists a ground rule such that I satisfies its body [1]. A model M is minimal if there is no model J of P such that JCM. A defi-nite program has a unique minimal model, which is the least model.\nGiven a normal logic program P and an interpretation I, the reduct P\u00b9, which is a ground definite program, is constructed as follows: a ground rule A \u2190 L1,..., Lm is in P\u00b9 iff there is a ground rule of the form (1) such that $body^-(R_i) \\cap I = \\emptyset$. If the least model of PI is identical to I, then I is a stable model of P [14]. For a definite program, the stable model coincides with the least model. A stable model is always supported, but the converse does not hold in general.\nSupported models can be computed as the models of Clark's com-pletion [5]. Let heads(P, a) be the set of rules in P whose head is a. The completion of P, denoted comp(P), is the set of clauses\n$a \\leftrightarrow \\bigvee\\limits_{R_i \\in heads(P, a)} body(R_i)$ (2)\nfor all a \u2208 Bp. A model of comp(P) is a supported model of P [1].\nLet IC Bp be an interpretation of P. The relation $\\models$ is defined as follows: for a rule Ri of the form (1), I satisfies Ri if head(Ri)\u2229 I \u2260 0 whenever $body(R_i) \\subseteq I$, and denoted as I $\\models R_i$; for a program P, I satisfies P if I $\\models R_i$ for all $R_i\\in P$; for a formula $F = F_1 \\lor ... \\lor F_k (k \\geq 0)$, I $\\models F$ iff there is a Fi (k \u2265 i \u2265 1) such that I $\\models F_i$, i.e., the empty disjunction is false. Let comp(Rp) denote the completed rule ($p \\leftrightarrow body(R_{p1}) \\lor ... \\lor body(R_{pj})$) for the atom p, then p \u2208 I iff I $\\models$ comp(Rp)."}, {"title": "3 Semantics", "content": "In this section, we consider the semantics of ground normal logic programs in vector spaces. First, we introduce the necessary nota-tions. Matrices are denoted using bold uppercase letters (M), and vectors are denoted using bold lowercase letters (v). The element in the i-th row and j-th column of a matrix is denoted by Mij, and the i-th element of a vector is denoted by vi. The slice of the i-th row of a matrix is denoted by Mi:, and the slice of the j-th column is denoted by M:j. Variables are denoted by upper case letters, and constants and predicates are denoted by lower case letters; e.g., in sum(L), L is a variable and sum/1 is a predicate with arity 1.\n3.1 Embedding Normal Logic Programs\nGiven a ground normal logic program P, we introduce two matrices that jointly represent the program. The program matrix represents the bodies of the rules in the program, and the head matrix represents their disjunctions. This is an alternative formulation to the embed-ding approach described by Sakama et al. [24].\nDefinition 1 (Program Matrix). Let P be a ground normal logic pro-gram with R rules and the size of its Herbrand base be |BP| = N. Then P is represented by a binary matrix $Q \\in {0,1}^{R \\times 2N}$ such that i-th row corresponds to the body of the i-th rule Ri: Qij = 1 if aj \u2208 body+(Ri), $Q_{i(N+j)}$ = 1 if $a_j \\in body^-(R_i)$, and Qij = 0 otherwise.\nDefinition 2 (Head Matrix). Let $D \\in {0,1}^{(N \\times R)}$ be the head ma-trix associated with P. Then the element Dji = 1 if the head of rule Ri(1 \u2264 i \u2264 R) is aj(1 \u2264 j \u2264 N), and 0 otherwise.\nExample 1. Consider the following program P\u2081 with 3 rules:\n(R\u2081) a \u2190 c^\u00acb\n(R2) aa\n(R3) ba (3)\nP1 is encoded into a pair of matrices (Q, D):\n$Q = \\begin{matrix} & a & b & c & a & b & c &\\ R_1 & 0 & 0 & 1 & 0 & 1 & 0 \\ R_2 & 1 & 0 & 0 & 0 & 0 & 0 \\ R_3 & 0 & 0 & 0 & 1 & 0 & 0 \\end{matrix}$\n$D = \\begin{matrix} & R_1 & R_2 & R_3 \\ a & 1 & 1 & 0 \\ b & 0 & 0 & 1 \\ c & 0 & 0 & 0 \\end{matrix}$ (4)\nQ represents the bodies of the rules, which are the conjunctions of the literals appearing in the bodies. For example, Q1: represents the body of R1, (c\u2227 \u00acb). D represents the disjunctions of the bodies of the rules sharing the same head. For example, D1: represents the disjunction $body(R_1) \\lor body(R_2) = (c \\land \\lnot b) \\lor a$. Together, Q and D represent the logic program P.\n3.2 Evaluating Embedded Normal Logic Programs\nWe consider the conjunction appearing in the bodies of the rules as the negation of disjunctions of negated literals using De Morgan's law, i.e., $L_1 \\land ... \\land L_n = \\lnot(\\lnot L_1 \\lor ... \\lor \\lnot L_n)$. This means that when evaluating the body of a rule, instead of checking whether all literals hold (as in [28]), we can count the number of false literals and check whether the count exceeds 1. To this end, we introduce a piecewise linear function $min_1(x) = min(x, 1) = ReLU(1 - x)$, which gives 1 for x > 1. This function is almost everywhere differ-entiable (except at x = 1), which allows gradient-based optimization to be applied effectively.\nTo evaluate normal logic programs in vector spaces, we introduce the vectorized counterparts of interpretation and model.\nDefinition 3 (Interpretation Vector). Let P be a ground normal logic program. An interpretation IC Bp is represented by a binary vector v = (V1,..., VN) \u2208 Z where each element vi (1 \u2264 i \u2264 N)"}, {"title": "4 Learning with Differentiable Logic Program", "content": "In this section, we show how the aforementioned differentiable logic program semantics can be used to train neural networks. Although our method supports both implication and constraint rules, it is not always necessary to use both of them for learning, as we shall show later in the experimental section. Specifically, for the NeSy tasks we studied, using exclusively either one of implication or constraint rules is enough to achieve competitive accuracy. On the other hand, in NeurASP [33] for example, the observation atoms are typically given as integrity constraints in ASP rules to compute stable models, and implication rules are defined similarly to ours. Consequently, we included a combination of both implication rules and constrains in our experiments to provide a thorough evaluation.\n4.1 Example: MNIST Addition\nThe MNIST digit addition problem [19] is a simple distant supervi-sion task that is commonly used in neural-symbolic literature. The input consists of two images, and the output is their sum (e.g., 8 7, 9). The goal is to learn digit classification from the sum of dig-its, rather than from images with individually labeled digits, as in the usual MNIST digit classification. For brevity, we shall focus on the single digit additions in this section.\nWe first introduce neural predicates [19], which act as an interface between the neural and logic programming parts. More concretely, a neural predicate is a predicate which can take references to ten-sorized objects as arguments. For example, in MNSIT Addition, we define two neural predicates, obs/4 and label/3. obs(i1, D1, 12, D2) represents a situation where images i1 and 12 were classified as digits D1 and D2 ranging from 0 to 9, respectively. label(i1, 12, S) repre-sents the two images and their sum S ranging from 0 to 18. Thus, we obtain 100 obs and 19 label neural atoms.\n4.1.1 Implication rules\nIn general, we expect the label atoms to appear in the heads of the implication rules so that we can compare the output of the logic pro-gramming component with the label using a loss function such as binary cross entropy. For a single digit MNIST Addition, the label is the integer values between 0 and 18 represented by the label/3 pred-icate. As for the individual rules, we enumerate the possible combi-nations of digits that sum to the given label, e.g.,\nlabel(i1, i2,0) \u2190 obs(i1, 0, i2, 0).\nlabel(i1, i2, 1) \u2190 obs(i1, 0, i2, 1).\nlabel (i1, i2, 1) \u2190 obs(i1, 1, i2, 0).\nlabel (i1, i2, 18) \u2190 obs(i1, 9, i2, 9). (6)\nIn this way, 100 rules with label/3 in the heads can be instantiated, which results in the embedded program Pimpl. = (Qimpl., Dimpl.) where $Q_{impl.} \\in {0,1}^{(100 \\times 200)}$ and $D_{impl.} \\in {0,1}^{(19 \\times 100)}$"}, {"title": "5 Experiments", "content": "5.1 Task Description\nWe studied the learning performance on the following NeSy tasks.\nMNIST Addition [19]\nThe input consists of two MNIST images of digits (11, 12), and the output is their sum (e.g., 8, 7, 9). The goal is to learn image clas-sification from the sum of digits, rather than from images with in-dividually labeled digits, as in the usual MNIST classification. This paper deals with two types: single-digit additions (two images), and two-digit additions (four images).\nADD2x2 [13]\nThe input is four MNIST images of digits (111, 112, 121, 122) arranged in a 2x2 grid, and the output is four sums (S1, S2, S3, S4) calculated from each row and column of the grid (e.g., 0,1,3,5, 1, 8, 3, 6). The goal is to learn the classification problem of MNIST images from the four sums provided as labels.\nAPPLY2x2 [13]\nThe input consists of three numbers (d1, d2, d3) and four handwrit-ten operator images (01, 02, 03, 04) arranged in a 2x2 grid, with the output being the results (r1, 12, 13, 14) of operations performed along each row and column of the grid. The operators are one of {+,-, x}. For example, the result for the first row is calculated as $r1 = (d1 op_{11} d2) op_{12} d3$ (e.g., 1, 2, 4, X, +,\u4e00,X, 6, -4, -2, 12). The goal is to learn the classification of handwritten operators from the four results and three numbers given as labels.\nMEMBER(n) [30]\nFor n=3, the input consists of three images (11, 12, 13) and one num-ber (d1), with the output being a boolean indicating whether d\u2081 is included in the three images (e.g., 1, 9, 5, 4,0). For n=5, the input includes five images (11, ..., 15). The goal is to learn the clas-sification problem of MNIST images from the numbers provided as labels. This paper deals with two types: n=3 and n=5."}]}