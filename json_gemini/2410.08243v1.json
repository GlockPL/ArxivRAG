{"title": "Self-Attention Mechanism in Multimodal Context for Banking Transaction Flow", "authors": ["Cyrile Delestre", "Yoann Sola"], "abstract": "Banking Transaction Flow (BTF) is a sequential data found in a number of banking activities such as marketing, credit risk or banking fraud. It is a multimodal data composed of three modalities: a date, a numerical value and a wording. We propose in this work an application of self-attention mechanism to the processing of BTFs. We trained two general models on a large amount of BTFs in a self-supervised way: one RNN-based model and one Transformer-based model. We proposed a specific tokenization in order to be able to process BTFs. The performance of these two models was evaluated on two banking downstream tasks: a transaction categorization task and a credit risk task. The results show that fine-tuning these two pre-trained models allowed to perform better than the state-of-the-art approaches for both tasks.", "sections": [{"title": "1 Introduction", "content": "Machine learning (ML) in the banking system has been a growing practice in recent years and can be found in all its related activities. Banking Transaction Flows (BTF) are often used in customer-related subjects, because it is an important data containing a certain amount of information about the customer himself, which is by nature extremely revealing and difficult to falsify.\nThe two main banking areas where BTF are used are undeniably marketing and risk.\nMarketing: In a marketing context, the information we want to extract from banking transactions is the type of spending habits or the household income. This information allows us to advise or categorize consumers according to a commercial or customer knowledge objective. Nowadays, most banks offer Personal Financial Management (PFM), which can help the clients to improve financial management, through a personalized view of finances and advice, without having any knowledge.\nMarketing segmentation is also a task where banking operations can be very useful. Segmentation is a central discipline in the commercial strategy of a company (not only banking) [50], in which banking flows can be used to extract information on customer knowledge, allowing a better understanding of customer behavior and making targeting more relevant[48].\nRisk: Since the last global financial crises, risk management has become extremely regulated and monitored in the banking sector [5]. The purpose of this risk monitoring is to limit the systemic financial risk [8] and thus preserve the integrity of the national and global banking system. Among the various forms of banking risk [30], credit risk is a major one. The bank solvency criterion is nowadays very closely followed by the regulatory and prudential agencies. For the bank, it can be summarized as determining a credit risk and a trade-off between commercial and prudential strategy. In this context, determining a credit default score at the time the credit is contracted is a way to choose the credit risk exposure. The translation of the commercial/prudential trade-off is often expressed by an acceptance threshold defined on the calculated risk score. For this application, banking transactions are widely used and allow to extract insights such as financial health, saving capacity, household spending habits.\nIn the context of Open Banking [9], BTFs can be exchanged between banks or private/public organizations in order to provide more financial services to their respective customers. These exchanges are rigorously framed by the PSD2 (Payment Services Directive 2) [15] and by the GDPR (General Data Protection Regulation) [16]. Using this data standardization, we aim to train a model able to process PSD2-based BTFs. Such a generic pre-trained could be useful for a great number of organizations.\nIn a lot of use cases, the information encapsulated by BTFs is not always fully exploited. BTFs are often transformed via a feature extraction phase (e.g., incomes estimation, counting the transactions number, etc) and the different modalities of the data are not always kept. A part of the information is lost, as well as the sequential nature of the data. In this work, we aim to process BTFs more efficiently by keeping its multimodal and sequential nature.\nWe will begin by describing the BTF data as well as the preprocessing we performed. The tokenization phase is one of our main contributions and will be extensively discussed, before describing the two modelling approaches we chose: Recurrent Neural Network (RNN) and Transformer. The self-attention mechanism is a key feature of these models. Another contribution we propose is the design of the pre-training process: we defined several subtasks specific to the multimodal nature of BTFs. We also carried out a"}, {"title": "2 Related Work", "content": "We found several application of machine learning to banking activities in the literature, such as marketing segmentation [46][48], general banking risk management [30] and credit risk [18][6][20]. Operational risks were also dealt by several works, e.g. the detection of fraudulent transactions [53] [41] or money laundering [28].\nSome works successfully tried to used machine learning in temporal point processing [54] [47], allowing to model event sequences in continuous time space.\nWe also found interesting research about deep learning applied to BTF modelling: [2] used contrastive learning inside a self-supervised learning process, [3] applied RNN for a credit loan use case, and [17] made adversarial attacks on deep learning models of transaction records. These works are not based on the same framework as our work (the PSD2 framework), and often include more features than our BTF definition, e.g. the Merchant Category Code (MCC).\nThe self-attention mechanism first appeared in the Natural Language Processing (NLP) field [4]: the words are decomposed in tokens (e.g. subwords) and the attention allows to indicate the semantic links between all the tokens from a given sequence (a sentence or a paragraph). Each token is processed with respect to the context around it (i.e. all the tokens before and after). The attention mechanism quantifies the relationship between events within a given sequence (the model is then called an encoder) or between two sequences (a cross-encoder). In the literature, it can be found associated with RNN [33] or inside the Transformer architecture [51].\nThe attention mechanism also appeared in several other fields such as image processing [14] or audio processing [19]. It also started to be used in banking use cases: in credit card fraud detection [7], in credit risk [31], in stock price prediction [10], as well as for general representation of BTF [37].\nAll these works are promising and shows a real interest of the deep learning community in the banking areas. However, we did not find deep learning modelling approaches based on the PSD2 definition of BTF. We will see that the use of self-attention mechanism can fulfill these need, allowing to create a useful generic model in the context of open banking."}, {"title": "3 Banking Transaction Flow", "content": "BTF represents all the events of banking transactions and a transaction is an event carried out on a bank account of a natural person (as opposed to a legal person). This transaction represents a bank transfer, a withdrawal from an Automated Teller Machine (ATM), a check issue or remittance, a purchase from a Point of Sale (POS), etc. The scope defined in the PSD2 framework is the events set that occur on the current account (also called checking account). An event is represented by three modalities:\n(1) The transaction processing date is the date the transaction was taken into account and is officially reflected in the customer's account maintenance. The date is only accurate to the day and, depending on the channel through which the event transited, may have a one or two day delay between the action taken by the customer and the official presence on his account. This date represents the first modality and offers information on the chronology of events on a monthly scale (the year scale will not be taken into account in this paper). These are therefore macro-ordered events but disordered in the daily temporality;\n(2) The second modality is the amount associated to the transaction, which represents the transaction value. This is a real number and the sign indicates the transaction direction (debit or credit). Hereafter, this value is in euros, but it can be in any other unit;\n(3) Finally, the third and last modality is the wording that accompanies the transaction. This is rich information that indicates the transaction channel (ATM, check, etc.) but also includes information that may be of personal origin (wording instructed by the client for debit transfers, for example) or organizational (wording instructed by a third party for credit transfers or purchases via a POS, for example)."}, {"title": "3.1 Preprocessing", "content": "It is important to carry out a preprocessing phase on this type of data so that it is standardized in order to be robust, efficient and relevant to the modelling we will discuss in the following sections. Moreover, particular attention will be paid to the textual modality of the wordings. Indeed, the latters are free fields and are therefore unnormalized.\nThe wordings have a lot of internal variability that is non-informational or brings a lot of noise, such as check or ATM withdrawal numbers, or even irrelevant information, such as dates, information already carried by the transaction date modality. These parts will be replaced by tags to greatly reduce the non-informational wordings diversity. Also, so that the wordings are not case sensitive, all the characters will be put in lower case and special characters and accents will be removed."}, {"title": "3.2 Tokenization", "content": "A key phase of the modeling is to build the morphosyntax of BTF. That is to say, building a syntax and a dictionary appropriate to the events and to the sequence of all these events. The adopted strategy is first of all a daily ordering of the events by amounts ascending order. In this way, the amounts embedding representation will guarantee the intra-daily position encoding and the amount information.\nSo that an event is consistent with respect to the tokenization, it remains to treat the case of the \"space\" character. Indeed, we notice that once the events are juxtaposed to the others, there are two separation types, the extra-wording spaces symbolizing the separation between two events, and the intra-wording spaces symbolizing the words separation inside the wording. To account for this specificity, two different encodings are needed for these two separation types. Thus, if we denote the extra-wording separator by  and the intra-wording separator by \u25c7, and taking the examples from table 1 and assuming that it is already correctly ordered, the wordings sequence gives:\n\"chq><digits>\u25a1\u25c7ret>dab><digits>>plancoet\n\u25c7vir pole emploibretagne<<date>\n\u25a1\u25c7carte\u25c7<date>>leclerc\u25c7brest_><empty>\";\nwhere in this study we will use as encoding: \u25a1 = U+2581 and \u25c7 = \u00d8.\nThe choice, arbitrary, to choose no encoding for the intra-wording separator character was motivated by the will to more easily attach the company names composed of several words. Thus, the tokenizer will not be more \"influenced\" by a character intervening in front of and behind any other character, consequently supporting the creation of more \"independent\" atoms (made up of much less composed words).\nIn order to create the dictionary X of BTF wording, while preserving the encoding specificity of the intra and extra wording separator characters, the SentencePiece Unigram algorithm was chosen [26], [27]. Although T. Kudo and J. Richardson [27] did not note any significant performance difference between a Byte-Pair-Encoding (BPE) [45] and the Unigram, recent work [55] revealed that a Unigram tokenizer has a better behavior on corpora not dealing with the same information, showing a more generalizing aspect of the created dictionary. This behavior seems to be interesting in the case where the bank flow comes from another organization than the one in which the modeling was not trained. The chosen dictionary size is 7k words and the dictionary has been trained on 1 million sequences composed of 1 month of banking operations.\nThe wording encoding will drive the other tokinizers. For that, we introduce 3 control tokens that will be used later: [BOS] to mark the beginning of a sequence, [EOS] marking the end of a sequence and [SEP] to mark the separation between two sequences. The latter will be used afterwards to contextualize a Natural Language Inference (NLI) problem as a separation marker between a \"premise\" sequence and a \"hypothesis\" sequence. Thus, we have two cases, mono-sequence and bi-sequence, where here are the possible schemes:\nmonoSeq = [BOS] Seq [EOS] (1)\nbiSeq = [BOS] Seq\u2081 [SEP] Seq\u2082 [EOS] (2)\nThe amounts tokenizer is composed of a dictionary A of 2.5k elements. This tokenizer is composed of 3 quantifiers divided into three quantization zones: a linear zone and two exponential zones. In order to create these quantizers and to make them representative of a certain ground reality, 1 million operations amounts were"}, {"title": "4 Modelization", "content": "In this part we will discuss the technical aspect of the two models. Firstly, the embedding part allowing to encode the tokens is identical for both models. The goal of this step is to create a latent representation of tokens. The input of the embedding step is a token and the output is a vector with d dimensions. The dimension of the embedding representation is the same for all the modalities. In the following, we will consider that a sequence is composed of N events. We note respectively X, A, D and T all in \u211d^(d\u00d7N), the latent representation of the tokens sequence composing the wordings, the amounts representation, the temporal modality (representing days) and the identity."}, {"title": "4.1 Recurrent Neural Network", "content": "The first model is an implementation of the historical architecture for this type of problem, i.e. RNN. It is built around a bidirectional recurrent network architecture [44] and the final embedded representation is inspired by the ELMo modeling [40]. The bidirectional layers allows to get rid of the events causality, so the n-th event will be influenced by the events preceding and following it.\nThe RNN structure used is a Long Short Term Memory (LSTM) [24] with a projection allowing to have a recurrent network with an internal latency representation larger than the output one. This strategy has already shown some effectiveness in some applications such as speech recognition [42]. The internal dimension of the recurrent model is denoted h and the external one is our event representation dimension d with h > d.\nBoth directions of the LSTM are composed of L layers and a layer normalization [1] is applied to each layer. The representations computed by the two directions are then given to an attention layer. The goal of this layer is to compute the relations between each token representation, using the self-attention mechanism [4].\nThis approach introduced by ELMo [40] has two advantages. Firstly, it limits the vanishing gradient effect through the network layers and secondly, it allows to have a model which will make each embedding representation of the network directly contributing to the output. And since these contributions are made of trainable parameters, the model will adapt to the downstream task and choose the optimal abstraction level of representation for this task. Indeed, the lower the layer level, the less the interactions between the different events are taken into account and vice versa. In the field of NLP it has already been shown that, for some tasks, the abstraction level can play an important role in the task performance [25]."}, {"title": "4.2 Transformer", "content": "The Transfomer architecture introduced in [51] made an original use of the attention mechanism, allowing to remove several limitations. Until this architecture, the events ordering was preserved and was primordial for the RNN-based architectures. The information of the ordering is no more necessary for the Transformer model as"}, {"title": "5 Pre-training", "content": "In this section we will discuss the architecture parameters of the two models, the cost function used for training, and the training parameters.\nThe pre-training strategy consists in 3 subtasks that we will detail: Masked Wording Model (MWM): is similar to BERT's Masked Language Model (MLM) but focused on transaction wordings. It consists in training the model to estimate the dynamically masked wording during the training phase. The probability  pwm represents the proportion of hidden wordings in the training sequences. Masking is done at the wording level and not at the subwordings level in the tokenization output.\nMasked Amount Model (MAM): which consists in estimating the dynamically masked amounts. The probability PMAM symbolizes the proportion of the masked amounts and the amounts estimation is done at the wording level. Thus, if an amount a associeted to a wording is masked, the mask is repeated as many times as the wording is broken down by the tokenization.\nNext Sequence Prediction (NSP): this NLI task is similar to the Next Sentence Prediction (NSP) task in BERT. This task defines"}, {"title": "6 Performances and Downstream Tasks", "content": "In this experimentation part, we will discuss the models performance in terms of execution time and RAM consumption. We will finish with two downstream tasks on two very different subjects allowing to notice the good behavior of the modeling on very diverse tasks."}, {"title": "6.1 Hardware Performance", "content": "The two models have a very different topological nature and therefore different behaviors with respect to parallelization and RAM consumption. Knowing these characteristics allows to better choose the architecture according to the needs and constraints (real time, batch computing, RAM limit, computing power limit, etc.).\nWe can see that with a low parallelization, the RNN architecture is more efficient than Transformer. This may be due to the fact that, in a mini-batch context, padding is calculated for the Transformer architecture whereas in the RNN architecture it is not. As the number transactions variability between sequences can be very large (see Figure A.1 in the appendix A) this may explain the advantage of the RNN structure. However, the Transformer structure much better supports parallelization with an inverse linear relationship between the cores number and the execution time. Therefore, on infrastructures with a lot of computing cores, the Transformer structure will be preferred.\nThough, if the task requires the use of longer sequences (several months for example), the limit in RAM memory may come into account. The RNN structure being recurrent on the events it is thus very little consuming in RAM memory. It nevertheless requires all intermediate layers to calculate the final embedded representation which represents a memory consumption of:\nRNN ~ dN \u00d7 (2L + 1) ~o(N)\nAs for the modeling based on Transformers structures, only the last layer is necessary for the sequence embedding representation. However, internally the attention head operator is much more consuming in RAM, in particular due to the matrix product QTK \u2208 \u211d^(N\u00d7N). So we could summarize the consumption of this structure by:\nTransformer ~ dN + N\u00b2 ~ o(N\u00b2)\nWe can see that for small sequences the RNN structure will consume more RAM. However the relation between the sequence size and the consumption remains linear whereas for the Transformer structure it is in power 2. So we have two possible strategies: either for tasks requiring long sequences we will prefer the RNN structure or we will choose a sequence truncation appropriate to the amount of RAM memory."}, {"title": "6.2 Downstream Tasks", "content": "Figure 3 summarizes the set of processes that can be performed with an encoder. Each of these categories represents one downstream task type that we discussed in the introduction. For example, in the encoder case seq2vec we find the credit risk, seq2seq the operations categorization. In this part we will not discuss any cross-encoder. Indeed, in this context, seq2vec can correspond to identity theft detection that we have already indirectly dealt with because it essentially corresponds to the pre-training NLI subtask (see Figure B.1 in the Appendix B). As for the seq2seq cross-encoder, this structure corresponds in NLP to Question-Answer type tasks. It is quite possible to treat this type of structure, but we have no idea of banking application yet.\nFaced with the complexity of certain architecture that can be made up of several models and having, in our two cases treated here, enough evaluation observation to satisfy the CLT conditions, it is possible to evaluate the uncertainties thanks to the Normal approximation interval at 95% (for the accuracy and recall measures). For the ROC-AUC confidence interval in our second use case, the interval expression expressed by [21] will be preferred.\nFinally, for the two downstream tasks studied in this article, the model input sequence will be composed of 2 months of BTF history. Indeed, in the PSD2 framework, no history depth is imposed to the banks. We have therefore chosen the minimum, allowing us to extract recurrent information between months.\nTransaction categorization: in this first task we will evaluate the models ability to label bank transactions using the categorizations made by the internal PFM as a reference. As we saw in the introduction, the PFM is in charge of categorizing transactions for account management purposes. However, the categorization system represents an imposing IT architecture (large databases, powerful calculation servers) offering little portability to the tool. It remains interesting to have a model that allows the use of the categorization system according to the need. With this in mind, we will compare our models to a Doc2Vec type approach [29] pre-trained on 200k bank wordings after normalization (Sec.3.1). Doc2Vec allows us to have a embedded representation of each bank wording. In order to integrate the other modalities we add the amount and the month day normalized as input to a Gradient Boosting Decision Tree (GBDT). The complete structure of the modeling approach is detailed in the Appendix C.1. Our comparison consists in replacing the Doc2Vec modeling with our direct output modeling and then replacing it again with our modeling adapted to the categorization task after a finetuning phase.\nCredit risk: consumer credit risk is a two-class classification task that involves determining a score representing the default risk. It is important to note that a scoring tool to determine credit risk is within the scope of GDPR framework and an explanation must be provided by the banking company to customers who request it on the reasons explaining the score. So, at the company level, it is a trade off between computational cost, XIA and performance. Therefore, the performance gain of a new technology must be significant to justify a paradigm shift. It is up to the company to define what an incidental contract is. In this article, an incident will be defined as any contract that has been at least 15 days late in payment or at least 1 month late during the first year of the contract's life. In order to fit the article subject, the perimeter of the used data will be only the BTF. Other data sources such as socio-demographics, balance amounts, etc. that generally increase the predictive quality will not be used.\nThe reference model is a typical model encountered in this problem type, it will first extract the maximum amount of information encapsulated in the BTF (estimation of credit and debit recurrences, savings estimation, fragility detection, etc.) defining the model input characteristics. Here the GBDT is well suited for this problem type [20]. We also added a naive deep learning model attempting to jointly exploit the three modalities of the data in order to illustrate the task difficulty. Finally our models will be tested in the case where only the head layer is trained (not changing the network parameters) and in a case where the whole network is finetuned."}, {"title": "A Overview of the preprocessing", "content": "In this appendix we will illustrate all the transformations made on BTF through an example inspired by table 1. An example of raw data is shown on Table 7. First, we apply normalization and ordering, as shown on Table 8.\nFinally, after adding the BOS and EOS control tokens, the sequence labels tokenization gives:\nWordings: [BOS] | _ | chq<digits> | _virpoleemploi | bretagne | <date> |_carte<date> | leclerc | brest |_retdab<digits> | plancoet | [EOS]"}, {"title": "B About Pre-training", "content": "For the pre-training, the Pytorch [38] backend is used and two additional control tokens are added: the masking token [MASK] whose purpose is to mask an event in the wordings or amounts sequence and a padding token [PAD] allowing to create a mini-batch with different sequence sizes. Thanks to this token the attention mechanism will either not be taken into account for the Transformer architecture or simply not calculated for the RNN.\nIt is also important to note that the classification token of a sequence is the embeeded representation of the [BOS] token for Transformer. However, in order to take into account the specificities of the bidirectional RNN structure for RNN modeling, the embedded classification vector is the sum of the embeeding representations of the tokens [BOS] and [EOS]."}, {"title": "C More Details on Downstream Tasks", "content": "In this appendix we will discuss in more detail the tasks tested for the performance evaluation of the models."}, {"title": "C.1 Transaction Classification", "content": "We will voluntarily not enumerate the categories. This is a 31-class classification problem and the classes are for example: \"income\", \"shopping\", \"subscription\u201d, \u201ctransportation\" (gas, transit, repair), \"savings\", \"dissaving\", etc. The training dataset is composed in a such way that each transaction category is present at least (if possible) 1.6k times in different sequences. As for the evaluation dataset, it is composed of 400 observations per category (if possible) contained in different sequences. As a reminder, the sequence is composed of 2 months of banking transactions.\nIn order to not making the features number too disproportionate between the different feature topologies, we have incorporated a non-linear feature extraction technique called Uniform Manifold Approximation and Projection (UMAP) [36] at the output of Doc2Vec, RNN and Transformer models which permits to \"reduce\" the dimensions from 768 to 25.\nFor GBDT, the Scikit-Learn HistogramGradientBoostingClassifier [39] implementation was chosen and the hyperparameters set was chosen after a search for optimal hyperparameters by cross-validation.\n The amount transaction and the day of the month are both simultaneously given as an input of the RNN and Transformer models, as well as an input of the GBDT. Their are not given to the Doc2Vec. The wording of the transaction is given to the Doc2Vec, the RNN and the Transformer models, but not to the GBDT. This specific structure allows to quantify in which extend the GBDT prediction lies on the pre-trained models, by using Shapley values."}, {"title": "C.2 Credit Risk Scoring", "content": "This depends on the banking company nature, as some are specialized in subprime loans. But in most cases it is important to note that serious repayment defaults are very rare events. Therefore, after labeling by the definition given for an incidental contract, we apply a downsampling of negative cases in order to have a balanced dataset. Thus our training dataset is made of 6.4k observations having as many negative cases as positive cases and our evaluation dataset has 1.6k cases also balanced.\nThe reference modeling (fig.C.3a) consists in extracting information from the banking flow. We will not go into detail in the transformations performed during this phase, but it consists in extracting 18 features. However, we can see in figure C.3b the relatively good separability of the two classes after dimensions reduction via UMAP."}], "equations": ["E = X +A+D+T", "Eo = [0,0,..., eo,n,\u2026\u2026\u2026, e0,N-1] \u2208 Rd\u00d7N", "Pemb = d x (|X| + |A| + |D| +2)", "Prnn = 2L \u00d7 (9dh + 8h + 2d) + L + 1", "Ptf = L \u00d7 (4d\u00b2 + 2dh + 9d + h)", "L(p, y) = CEMw\u043c (\u0440, \u0443) + \u0421\u0415\u043c\u0430\u043c (p, y) + CENSP (p, y)", "Pk = {(i, j) : {Ok.i} = Ok, j = argmax Yi,d", "CE (pk,y) = log(p)"]}