{"title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation", "authors": ["Guoxin Chen", "Minpeng Liao", "Peiying Yu", "Dingming Wang", "Zile Qiao", "Chao Yang", "Xin Zhao", "Kai Fan"], "abstract": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior\u2014typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.", "sections": [{"title": "1 Introduction", "content": "Recent advances in retrieval-augmented generation (RAG) for large language models (LLMs) have demonstrated remarkable capabilities in various tasks [1, 11, 4, 35, 21, 2, 34], empowering LLMs to acquire up-to-date or domain-specific knowledge while mitigating hallucinations [6, 5, 24]. The effectiveness of RAG systems, however, hinges on the alignment between the retriever and the LLM-an inherently challenging goal as these components are typically developed independently without co-training. This lack of co-training can result in semantic mismatch and suboptimal interactions: retrievers may fail to provide information tailored to the LLM's needs, while LLMs may struggle to generate effective queries or seamlessly incorporate retrieved content.\nExisting approaches address this misalignment through three main strategies: (1) fine-tuning retrievers to align with LLM preferences, (2) optimizing LLMs to adapt to retriever behavior, and (3) introducing intermediate modules to bridge the gap between them [19, 28, 2, 34, 37, 38]. Despite progress, these methods face notable challenges: fine-tuning retrievers often requires carefully curated data and may not be feasible for commercial search engines [25, 22], while optimizing LLMs is resource-intensive and risks compromising their original capabilities [44]. Approaches that introduce intermediate modules to avoid modifying either the retriever or the LLM primarily focus on optimizing individual tasks, such as query rewriting or document reranking [19, 33, 29]. However, optimizing a single task in isolation often leads to suboptimal results, as the effectiveness of RAG systems relies on the cohesive interaction and collaboration among multiple components throughout the entire pipeline [5, 44]."}, {"title": "2 Related Work", "content": "Retrieval-Augmented Generation. Retrieval-augmented generation (RAG) has emerged as a crucial technique for enhancing LLMs' capabilities by incorporating external knowledge sources [6, 38]. Recent studies have highlighted the importance of aligning the retriever with the LLM to achieve superior performance [5, 3]. This alignment can be approached through three main strategies: retriever fine-tuning methods [28], LLM fine-tuning methods [2, 34, 37], and intermediate modules methods [19, 33, 29]. However, these methods often face practical limitations, such as focusing on local optimization, the inability to align with commercial search engines, and the substantial computational costs of LLM optimization. Different from previous work, we introduce a lightweight, proxy-centric alignment framework that implements alignment without modifying either the retriever or LLM while optimizing the entire RAG pipeline holistically.\nMulti-agent Systems. Multi-agent systems have recently garnered increasing attention, especially in the context of complex task-solving and decision-making [8, 41]. A major challenge in multi-agent frameworks is credit assignment-determining each agent's contribution to the overall system performance-which becomes particularly crucial in multi-agent reinforcement learning [39, 45]. In our work, we propose Monte Carlo credit assignment mechanism to distribute system-level rewards to each agent in the form of expectations, enabling effective coordination within agents."}, {"title": "3 Preliminaries", "content": "Before introducing C-3PO, we first review the preliminaries of cooperative multi-agent reinforcement learning (MARL), where multiple agents work collaboratively to accomplish given tasks. A cooperative MARL problem is generally formalized as a cooperative stochastic game, represented by a tuple (N, {S}i\u2208N, {A}i\u2208N, T, R), where:\n\u2022 N = {1, 2, ..., n} is the set of agents in the system.\n\u2022 $S^{i}$ is the state space of agent i, where each agent maintains its agent-specific state information.\n\u2022 $A^{i}$ is the action space of agent i, defining the joint action space A := $A^{1}$ \u00d7 \u2026\u2026 \u00d7 $A^{n}$.\n\u2022 T : {Si}i\u2208N \u00d7 A \u2192 {Si}ien is the deterministic state transition function, specifying how the states of agents update given their joint action a \u2208 A.\n\u2022 R : {S'}i\u2208N \u00d7 A \u00d7 N \u2192 R is the system-level reward that measures the overall task completion, where 1 indicates success and 0 otherwise.\nIn this cooperative setting, all agents share the same system-level reward R and work together to accomplish the task. Each agent follows its policy \u03c0\u00b2 : Si \u2192 A\u00b2 to select actions based on its local observations. A trajectory (\\{so\\}i\u2208N, ao, \\{si\\}i\u2208N, a1, ...) represents the sequence of agent states and joint actions, where st \u2208 S\u00b2 is the state of agent i at time step t, and at = \\{at\\}i\u2208N \u2208 A is the joint action at time step t under the joint policy \u03c0 = (\u03c01, ..., \u03c0\u03b7)."}, {"title": "4 Cooperative Multi-agent System", "content": "In this section, we elaborate on the role of each agent (Section 4.1) with their collaborative strategies (Section 4.2)."}, {"title": "4.1 Multiple Agents", "content": "Inspired by human behavior mentioned in the Introduction, we design three specialized agents-Reasoning Router, Information Filter, and Decision Maker\u2014to facilitate communication between the retriever and the LLM, as illustrated in Figure 1. These agents operate as distinct roles within a single lightweight proxy using targeted instructions, collaboratively managing various aspects of the RAG pipeline. This unified design ensures efficient coordination while maintaining simplicity for edge deployment. Formally, we define each agent as follows:\nThis proxy plays the role of Reasoning Router agent to determine the optimal reasoning strategy for a given question from a high-level perspective. Given the current state (the question), it selects actions using a maximum two-step operation:\n1. Decide Retrieval Necessity: If the agent outputs [No Retrieval], the question is directly processed by the LLM, leveraging its internalized knowledge.\n2. Determine Question Complexity: If the agent outputs [Retrieval], the agent also evaluates whether the question requires complex reasoning.\nFor simple questions, the agent continues to generate a single <query content> and interacts with the retriever to obtain documents. The retrieved documents are then processed by the Information Filter agent to extract relevant, LLM-friendly content. Finally, the LLM uses the filtered documents to generate an answer to the question.\nFor complex questions, the agent outputs [Planning], which will trigger a multi-step reasoning strategy that requires coordination with multiple agents. Further details on this strategy will be introduced later.\nThis proxy plays the role of Information Filter agent to process and filter retrieved information for identifying content suitable for LLMs. Its state space includes the question, the retrieved documents, and the current reasoning objective (if operating in [Planning] mode). Based on the given state, the agent selects an action to analyze and filter relevant documents using the following structured format:\nThought: <Analysis of each documents>\nAction: [<Selected document IDs>]\nThis proxy plays the role of Decision Maker agent to determine the optimal action within the [Planning] strategy based on the current state. Its state space includes the question, the LLM-generated roadmap, and the accumulated documents from the reasoning history. Given the current state, the agent selects an action to evaluate progress and decide the next operation, using the following structured format:\nThought: <Analysis of current progress and objective>\nAction: {[Retrieval]<subquery content> (Continue with retrieval-filter loop), or [LLM] (Pass to LLM for answering)}"}, {"title": "4.2 Collaborative Strategy", "content": "With the three specialized agents defined, we now describe how they collaborate to efficiently handle different types of questions. Their coordination follows a structured workflow, enabling adaptive and multi-granular information processing through various strategies, as detailed below.\nThe Direct Answering Strategy and Single-pass Strategy have already been introduced in the definition of the Reasoning Router agent, corresponding to the agent outputs [No Retrieval] and [Retrieval]<query content>, respectively.\nMulti-Step Reasoning Strategy corresponds to the [Planning] output by Reasoning Router agent. Designed to address complex questions requiring a high-level roadmap from LLM and multiple retrieval-filter loops, this strategy enables iterative information gathering and reasoning through the following three phases:\n1. Generate Roadmap: The LLM decomposes the complex question into a structured set of subgoals, providing high-level guidance for the proxy."}, {"title": "5 Multi-Agent Proxy Optimization", "content": "Since the final answer generated by the LLM is straightforward to evaluate as the system-level reward, it is intuitive to employ reinforcement learning to optimize the proxy. However, each agent within the proxy serves as an intermediate module, responsible for only a partial trajectory of the RAG pipeline. This makes it difficult to define agent-level rewards [7]. For example, a high-quality generated query might still result in a low system-level reward due to poor subsequent document filtering. To tackle this challenge, we propose a tree-structured rollout approach for robust on-policy learning, utilizing deterministic rollout in the early stages and stochastic rollout in later stages."}, {"title": "5.1 Credit Assignment", "content": "To avoid the sparse reward in traditional single trajectory rollout, we propose the tree-structured rollout for credit assignment, which distributes system-level rewards across agents to mitigate the high variance of local rewards for each agent. The core idea is to evaluate each agent's contribution by forcing the Reasoning Router to explore all possible reasoning strategies during the rollout for each question.\nDeterministic Rollout. Given a question q, we begin by forcing the Reasoning Router to explore both [No Retrieval] and [Retrieval]. For the [Retrieval] branch, we further force the agent to explore simple reasoning by directly generating <query content> and complex reasoning through [Planning]. As shown in Figure 1, we deterministically construct a decision tree during the first stage of the rollout. Current tree, with a depth of 2, enables simultaneous exploration of multiple reasoning paths, providing a comprehensive understanding of how each decision impacts the final outcome.\nStochastic Rollout. Once the overall reasoning strategy is confirmed, the subsequent rollout employs sampling to expand the decision tree. For each non-terminal node, we randomly sample K(t) candidate actions from the proxy \u03c0 for the i-th agent at depth t. Specifically, the tree is expanded using the following children (actions):\n$K(t)=\\begin{cases}\\begin{aligned} &\\{a\\_{t,k}\\^{K(t)}\\_{k=1}\\} \\sim \\pi^{i}(\\cdot|s, \\text{instruction}\\_i) \\\\ & 2, \\quad \\text{if } t \\leq 4 \\\\ & 1, \\quad \\text{if } t > 4 \\end{aligned}\\end{cases}$"}, {"title": "5.2 Training Method", "content": "For each sampled tree, we disassemble it into individual nodes and group them by their corresponding agents, as illustrated in Figure 1. The token sequence within each node, along with its corresponding reward computed via Eq (5), is added to the replay buffer for Proximal Policy Optimization (PPO) [27]. The overall training objective follows the standard PPO framework but incorporates multi-agent aggregation.\n$\\mathcal{L}\\_{C-3PO} = \\sum\\_{i \\in N} \\mathcal{L}\\_{PPO}$ \nwhere the loss LPPO can represent either value loss or policy loss. Details of the loss functions are provided in the Appendix A.1. Following the PPO recipe in Ouyang et al. [23], the PPO for LLMs typically requires an initialization from the supervised fine-tuning model. Therefore, we employ our tree-structured rollout with rejection sampling [40] to collect seed data and use cross-entropy loss for the supervised warm-up phase."}, {"title": "6 Experiments", "content": "6.1 Experimental Setup\nDatasets. To comprehensively evaluate our C-3PO, we experiment on both single-hop datasets including Natural Questions (NQ) [14], PopQA [20], and TriviaQA (TQA) [13], as well as multi-hop datasets including 2WikiMultiHopQA (2Wiki) [9], Musique [31], and HotpotQA (HQA) [36]. For each dataset, we only use 6000 randomly sampled questions instead of the full training set.\nBaseline. We compare our method against a diverse set of baselines, including (1) Direct: Directly answer questions without retrieval. (2) Standard RAG: The standard retrieval-augmented method retrieves documents based on the question. (3) Retriever Fine-tuning Method: REPLUG [28]. (4)"}, {"title": "6.3 Analysis of Plug-and-Play Proxy", "content": "In this section, we conduct a comprehensive investigation of performance across three out-of-distribution (OOD) dimensions, including OOD datasets, retrieval systems, and LLM servers. This analysis aims to demonstrate that our proxy is a plug-and-play module with superior generalization capabilities. In Table 2, we assess its modularity and generalization by introducing two recent and challenging OOD datasets: FreshQA (FQA) [32] and MultiHop-RAG (M-RAG) [30]. Additionally, we replace the retriever with the Google search engine [25] and experiment with different LLM servers.\nFirst, LLM fine-tuning approaches exhibit notably inferior performance compared to the standard RAG. This significant degradation suggests that directly fine-tuning LLMs, while potentially effective for specific tasks, may compromise the inherent generalization capabilities and lead to subpar OOD performance. Second, while intermediate-module methods maintain competitive performance, their focus on optimizing individual tasks may compromise their robustness. In contrast, our C-3PO holistically optimizes all communication tasks of the entire RAG pipeline through multi-agent collaboration, effectively aligning the retriever and LLM while preserving their inherent generalization capabilities. This enables C-3PO to achieve superior generalization across all OOD settings, consistently outperforming existing approaches with a large margin, 4.22% over the best performing baseline on average. Finally, even all three dimensions are OOD, C-3PO exhibits robust performance across different LLM servers (Qwen2-72B, Qwen2-7B, Llama3.3-70B, and GPT-40-mini), with consistent improvements ranging from 1.7% to 5.6%. This platform-agnostic performance demonstrates the plug-and-play capability of our method, enabling seamless integration with various retrievers and LLM servers without requiring any modifications."}, {"title": "6.4 Ablation Study", "content": "Ablation on Training Paradigm. To thoroughly evaluate the effectiveness of different components in our training process, we conduct comprehensive ablation studies across six in-domain datasets. Specifically, we examine the following variants: (1) \"w/o Tree-structured Rollout\": A variant without the tree-structured rollout and Monte Carlo credit assignment, meaning that we directly op-"}, {"title": "6.5 Detailed Analysis", "content": "Inference Efficiency Analysis. To investigate the inference efficiency of C-3PO, we compare both the performance and inference cost across different methods, as illustrated in Figure 2. Our approach achieves superior performance (+9.2% for in-domain and +4.2% for OOD scenarios) while maintaining a reasonable inference time of 4.8s per question. Although slightly slower than the Standard RAG method (3.6s), C-3PO yields significant performance gains across both in-domain and out-of-generation evaluations. Furthermore, C-3PO outperforms most methods, such as AutoRAG [37] and SlimPLM [29], in both efficiency and effectiveness. This demonstrates that C-3PO achieves an optimal balance between performance and computational efficiency.\nTraining Dynamics in RL. Figure 3 depicts the average performance trajectory of our C-3PO across six in-domain benchmarks throughout the RL training process. The results demonstrate"}, {"title": "6.6 Additional Results", "content": "We conduct additional analyses on C-3PO-ICL performance, RL training dynamics, and inference depth distribution. Detailed results and discussions are presented in Appendix C."}, {"title": "7 Conclusion", "content": "In this paper, we presented C-3PO, a proxy-centric alignment framework that bridges retrievers and LLMs through a lightweight multi-agent system. By leveraging MARL with our proposed tree-structured rollout and Monte Carlo credit assignment, our framework effectively optimizes multiple specialized agents toward the system-level performance without modifying existing RAG components. Extensive experiments demonstrate C-3PO's superior performance and strong generalization capability across different out-of-distribution datasets, retrievers, and LLMs, establishing it as a practical plug-and-play solution for RAG systems."}, {"title": "Impact Statement", "content": "The advancements in Retrieval-Augmented Generation (RAG) systems, such as those proposed in this work, hold significant potential for diverse applications. By enhancing modularity, generalization, and plug-and-play capabilities, these systems can empower applications in edge deployment at relatively low computational cost. However, the deployment of RAG systems still poses risks that need to be carefully considered, despite our efforts to mitigate them. These systems heavily rely on external retrieval sources, which may include biased or unreliable data, leading to the propagation of misinformation. In high-stakes applications like medical or legal advice, incorrect or incomplete retrieval could have severe consequences. Although our proxy is not directly responsible for the content retrieved by the retriever, future research can focus on improving alignment in fairness, robustness, and transparency during information filtering."}, {"title": "A More Implementation Details", "content": "In this section, we provide a comprehensive implementation details of our proposed method. For additional insights and more intricate details, we refer the reader to our supplementary materials."}, {"title": "A.1 RL Training Process", "content": "Having obtained the credit rewards that reflect each agent's contribution, we develop an optimization framework to guide end-to-end training across all agents. The key idea is to use these credit signals for optimizing the collaborative behavior of the entire system. The optimization objective for our multi-agent system can be formulated as maximizing the expected credit rewards:\n$J(\\theta) = E\\_{\\tau \\sim \\pi\\_\\theta}\\sum\\_{i \\in N} \\sum\\_t r\\_{\\text{credit}}(s\\_t, a\\_t^i)$\nSince each agent's action is a sequence of tokens, we decompose this optimization using Proximal Policy Optimization (PPO) [27, 39, 45] as follows:\n$\\mathcal{L}\\_{C-3PO} = \\sum\\_{i \\in N} \\mathcal{L}\\_{PPO} (\\theta, \\phi)$\nSpecifically, for each agent i, we define:\n$\\mathcal{L}^{CLIP}\\_t(\\theta) = E\\_{\\tau \\sim \\pi\\_\\theta}\\left[\\sum\\_t \\sum\\_m \\min \\left( r\\_{t,m}(\\theta) \\hat{A}\\_{i,m}, \\text{clip}(r\\_{t,m}(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}\\_{i,m} \\right)\\right]$\nwhere $r\\_{t,m}(\\theta) = \\frac{\\pi\\_\\theta(a\\_{t,m}|s,m)}{\\pi\\_{old}(a\\_{t,m}|s,m)}$ is the probability ratio, s,m represents the concatenation of current state and the first m \u2212 1 tokens in the action sequence for agent i at time step t, and $a\\_m$ denotes its m-th token. We compute the advantage estimate using GAE [26]: $A\\_{i,m} = \\sum\\_{l=m}^{M-1} (\\gamma \\lambda)^{l-m-1} \\delta\\_{t,m+l}$, where M is the token length of the action sequence.\nTo estimate state values across the multi-agent system, we employ a centralized state-value function V\u03c6 that takes each agent's state sim as input. The value function is optimized to minimize the mean squared error [18]:\n$\\mathcal{L}(\\phi) = E\\_{\\tau \\sim \\pi\\_\\theta} \\sum\\_t \\sum\\_m (V\\_{\\phi}(s\\_{t,m}) - G\\_{i,m})^2$\nwhere $G\\_{i,m} = \\hat{A}\\_{i,m} + V\\_{\\phi}(s\\_{i,m})$ is the empirical return. The final optimization objective combines the policy and value losses:\n$\\mathcal{L}^{PPO}(\\theta, \\phi) = \\mathcal{L}^{CLIP}(\\theta) + c\\_v \\mathcal{L}(\\phi)$\nwhere cv controls the weight of the value loss. This joint objective enables end-to-end training of both policy and value networks across all agents."}, {"title": "A.2 Implementation Details", "content": "Supervised Warm-up Phase: We utilize Llama-Factory [43] as our training framework for the initial supervised fine-tuning phase. The detailed hyper-parameters for this phase are presented in Table 5.\nReinforcement Learning Phase: For the RL training phase, we adopt OpenRLHF [10] as our primary training framework, coupled with VLLM [15] inference engine. The complete set of RL training hyper-parameters is detailed in Table 6. To initialize both the policy and value models, we leverage the model obtained after one epoch of supervised fine-tuning, with the language model head replaced by a value head for the value model.\nInference Phase: For the deployment of our system, we establish a comprehensive infrastructure that integrates multiple components:"}, {"title": "B Instructions and State Transition Function", "content": "B.1 Instructions for Each Agent\nIn this section, we details the state space and action space fo each agent in our C-3PO.\nReasoning Router. The Reasoning Router agent operates with state space S\u00b9 = {q}, where q represents the input question. This agent is responsible for determining whether retrieval is necessary for the given question and assessing the question complexity when retrieval is needed. For a question that does not require retrieval, this agent outputs [No Retrieval]. If the retrieval is needed, the agent outputs one of the following actions based on the complexity of the question q: for simple questions requiring retrieval: [Retrieval] <query content>, initiating a single-pass retrieval-filter loop, where <query content> defines the space of possible queries; for complex questions: [Planning], triggering the multi-step reasoning strategy. The specific examples are as follows:\nB.2 State Transition Function\nGiven a state si and an action a\u012f in each agent i \u2208 N, the transition function T in our framework is deterministic. Based on the three collaborative strategies introduced in Section 4.2, the state transitions are defined as follows:\nDirect Answering Strategy ([No Retrieval]): In this strategy, the LLM directly generates the answer without retrieval, resulting in no state transitions between agents.\nSingle-pass Strategy ([Retrieval] <query content>): This strategy involves a state transition between the Reasoning Router and Information Filter agents:\nT : S\u00b9 = {q} \u00d7 A = {[Retrieval]<query content>} retrieval, S2 = {q, retrieved documents}\nwhere S\u00b9 represents the initial state with the question q, and S\u00b2 represents the state for the Information Filter agent after retrieval. The Information Filter is responsible for filtering the helpful documents based on S2.\nMulti-Step Reasoning Strategy ([Planning]): This strategy involves multiple state transitions in a cyclic manner:\n\u2022 Reasoning Router \u2192 Decision Maker:\nT : S\u00b9 = {q}\u00d7A= {[Planning]} [planning] S3 {q, Accumulated Documents, Roadmap},\nwhere the roadmap is generated by the LLM and the accumulated documents is empty for initial step.\n\u2022 Decision Maker \u2192 Information Filter:\nT : S\u00b3 = {q, Accumulated Documents, Roadmap} \u00d7 A = {[Retrieval]<subquery content>, current objective} \u2192 S\u00b2 {q, retrieved documents, current objective},\nwhere the current objective is generated by the Decision Maker agent in S\u00b3.\n\u2022 Information Filter \u2192 Decision Maker:\nT : S2 = {q, retrieved documents, current objective} \u00d7 A = {Selected Documents} filter Sew {q, Updated Accumulated Documents, Roadmap}.\nThis retrieval-filter loop between the Decision Maker agent and the Information Filter agent continues until the Decision Maker outputting [LLM] or a termination condition is met. The state transitions in our C-3PO are deterministic and well-defined, ensuring consistent behavior across the multi-agent system."}, {"title": "C Additional Experimental Results", "content": "C.1 Comparative Analysis of C-3PO-ICL and C-3PO-RL\nIn this section, we further conduct a comprehensive comparison between C-3PO-ICL (Qwen2-72B-Instruct) and C-3PO-RL (Qwen2-1.5B), where C-3PO-ICL (Qwen2-72B-Instruct) is used to generate\nC.2 More Analysis in RL"}, {"title": "D Additional Prompts", "content": "In this section, we supplement additional prompts based on Appendix B.\nD.1 Roadmap\nIn the multi-step reasoning strategy, we introduce an LLM-generated roadmap as high-level guidance for our proxy. The specific prompt and example are as follows:\n\nD.2 Evaluation"}, {"content": "In our experiments, we found that traditional evaluation metrics such as Exact Match (EM) are often inaccurate, as they strictly require identical generated answers. To address this issue, following previous work [42, 32], we leverage an LLM to assess answer correctness by comparing the predicted answer with the ground truth. The specific example is as follows:"}]}