{"title": "Spatial-temporal Graph Convolutional Networks with Diversified Transformation for Dynamic Graph Representation Learning", "authors": ["Ling Wang", "Yixiang Huang", "Hao Wu"], "abstract": "Dynamic graphs (DG) are often used to describe evolving interactions between nodes in real-world applications. Temporal patterns are a natural feature of DGs and are also key to representation learning. However, existing dynamic GCN models are mostly composed of static GCNs and sequence modules, which results in the separation of spatiotemporal information and cannot effectively capture complex temporal patterns in DGs. To address this problem, this study proposes a spatial-temporal graph convolutional networks with diversified transformation (STGCNDT), which includes three aspects: a) constructing a unified graph tensor convolutional network (GTCN) using tensor M-products without the need to represent spatiotemporal information separately; b) introducing three transformation schemes in GTCN to model complex temporal patterns to aggregate temporal information; and c) constructing an ensemble of diversified transformation schemes to obtain higher representation capabilities. Empirical studies on four DGs that appear in communication networks show that the proposed STGCNDT significantly outperforms state-of-the-art models in solving link weight estimation tasks due to the diversified transformations.", "sections": [{"title": "I. INTRODUCTION", "content": "Graph-structured data appears in various complex systems, such as social networks [1-3, 30-37, 63-69] and communication networks [4-6]. Graph representation learning (GRL) can effectively be applied to many downstream tasks like link estimation. Traditional research about GRL mainly focuses on static graphs, which assumes that graph structure does not change over time [7]. In actual scenarios, graphs commonly show certain temporal dynamics. For example, interactions between nodes may be lost or established in the communication network which makes the graph structure change over time.\n\nA dynamic graph (DG) is developed to model and store such a temporal nature of interactions between nodes. A DG is not just a simple combination of static graphs. It brings complex temporal information based on static topology information and increases the difficulty of graph representation learning.\n\nResearch on DG representation learning has seen a significant rise in recent years [8-19, 70-75]. For example, Manessi et al. [26] introduce a WDGCN model, which employs graph convolution operations to generate node embeddings for each graph snapshot independently. It then utilizes a recurrent neural network to capture temporal dynamics across these embeddings. Pareja et al. [31] develop an EvolveGCN model, which incorporates a recurrent neural network to dynamically adjust the learnable parameters of a Graph Convolutional Network for each graph snapshot.\n\nAlthough the dynamic GCNs mentioned above are effective, they rely on combining static GCNs with sequential neural networks to investigate spatiotemporal dependencies. This separate approach to spatiotemporal modeling disrupts the inherent interdependencies and fails to capture the intricate temporal patterns in dynamic graphs (DG). Research indicates that tensors are highly suitable for representing high-order data [20-22, 81-87]. Additionally, a novel tensor framework known as the tensor M-product [23-25] has been introduced to extend matrix-based theory to higher-order architectures through a transformation scheme. In our study, DG is fully represented by a third-order adjacency tensor with dimensions corresponding to node-node-time. The transformation scheme in the M-product can capture temporal patterns without fragmenting spatiotemporal information. Based on these insights, this study proposes the following research questions:\n\nRQ. With the tensor M-product, is it possible to implement a powerful GCN model for DG with diverse transformation schemes, thus boosting the DG's representation learning?\n\nTo answer this question, this paper proposes a Spatial-temporal Graph Convolutional Networks with Diversified Transformation for Dynamic Graph Representation Learning (STGCNDT), which adopts three transformation schemes in M-product, i.e., Discrete Fourier Transform [26], Discrete cosine transform [27], and Haar Wavelet Transform [28], to model complex temporal patterns of DG effectively. Specifically, it consists of the following three-fold ideas:\na) leveraging the tensor M-product to formulate a unified graph tensor convolution network (GTCN);"}, {"title": "II. PRELIMINARIES", "content": "In this paper, a DG is defined as $G = \\{G_1, G_2, ..., G_T\\}$ and each graph snapshot $G = (V, E^t, X^t)$ is a graph at time slot $t$ $(1 \\le t < T)$, where $V$ is the node-set whose number of nodes is $N=|V|$. $E^t$ represents the edge set at time slot $t$, and $X^t$ contains all nodes and their feature vectors. Specifically, for a graph snapshot $G^t$, $E^t$ can be represented as an adjacency matrix $A^t$, whose element $a_{ij}^t$ is one if the edge between node $i$ and $j$ exists, and zero otherwise [26-29]. A DG can be described by an adjacency tensor $A=[a_{ijt}]$ with the size of $(N\\times N\\times T)$ to reserve the spatial-temporal pattern. Given a DG, the goal of representation is to obtain a representation tensor $H$.\n\nThe main tensor calculations used in this paper are as follows:\nMode-n product. The mode-n product generalizes the matrix-matrix product to the tensor-matrix product. Given a tensor $X\\in \\mathbb{R}^{I_1\\times...\\times I_{n-1}\\times I_n\\times...\\times I_N}$ and a matrix $U_{D\\times I_n}$, then the mode-n product obtains a tensor $(X\\times_n U) \\in \\mathbb{R}^{I_1\\times...\\times I_{n-1}\\times D\\times...\\times I_N}$. Its element at position $(i_1, ..., i_{n-1}, d, i_{n+1}, ..., i_n)$ is defined as:\n\n$(X\\times_n U)_{i_1...i_{n-1}d i_{n+1}...i_N} = \\sum_{i_n=1}^{I_n} U_{di_n} X_{i_1 i_2...i_n}$.\n\nM-transform. Given a transform matrix $M\\in\\mathbb{R}^{T\\times T}$. The M-transform of a tensor $X\\in \\mathbb{R}^{I\\times J\\times T}$ is denoted by $X\\times_3 M$ and is defined as:\n\n$(X\\times_3 M)_{ij,t} = \\sum_{k=1}^{T} M_{tk} X_{ij,k}$.\n\nNote the (2) can be also written in a matrix form, i.e., $(X\\times_3 M) = fold(M \\cdot unfold(X))$, where the unfold operation takes the tubes of X and stacks them into matrix columns with the dimension of $T\\cdot IJ$ as $fold (unfold (X))=X$. It should be pointed out that $(X\\times_3 M) \\times_3 M^{-1}=X$, if $M$ is an invertible matrix.\n\nFace-wise Product. Given two three-order tensors $X\\in\\mathbb{R}^{I\\times J\\times T}$ and $Y\\in\\mathbb{R}^{J\\times K\\times T}$, the face-wise product is denoted by $(X\\circledast Y) \\in \\mathbb{R}^{I\\times K\\times T}$ and is defined as:\n$(X\\circledast Y)_t = X_t Y_t$.\n\nM-product. Given two three-order tensors $X\\in\\mathbb{R}^{I\\times J\\times T}$ and $Y\\in\\mathbb{R}^{J\\times K\\times T}$, and an invertible matrix $M\\in\\mathbb{R}^{T\\times T}$, the M-product is denoted by $(X * Y)\\in \\mathbb{R}^{I\\times K\\times T}$ and is defined as:\n$X*Y = ((X\\times_3 M) \\circledast (Y\\times_3 M))\\times_3 M^{-1}$."}, {"title": "III. THE PROPOSED STGCNDT MODEL", "content": "In this study, we proposed a GTCN model in which the spatiotemporal information of DG is modeled as a whole, avoiding the separation and loss of information as shown in Fig. 2. Specifically, the graph tensor convolution network is designed based on the M-product as:\n$H = \\sigma(A*X* W)$,\nwhere $\\sigma$ is a non-linear activation function like Sigmoid, $A$ is the adjacent tensor, $X$ is the node feature tensor, $W\\in\\mathbb{R}^{F\\times F\\times T}$ is a learnable weight tensor for feature transformation and $H\\in\\mathbb{R}^{N\\times F\\times T}$ is the node representation tensor.\n\nThe GTCN can aggregate the information from spatio and temporal dimensions of a DG tensor directly since it integrates the tensor face-wise product and the tensor M-transform. The former can implement spatio message passing on each front slice of X with the help of the adjacent tensor A, and the latter is used to implement temporal message passing through the transform matrix M. Specifically, corresponding to GTCN with the tensor form, the spatiotemporal message passing of each node $i$ at time $t$ can be formulated as:\n$\\begin{aligned}c_i^t &= \\sum_{j \\in N(i)} \\Phi(a_{ijt})\\varphi(x_j^t),\\\\x_i^t &= \\sigma(c_i^tW^t),\\end{aligned}$\nwhere $c_i^t$ is the message vector from the neighbor set $N(i)$ of node $i$, and $a_{ijt}$ is the entry of the adjacent tensor $A$ at position $(i, j, t)$, and $x_j^t$ is the node feature vector. As the t-th front slice of W, $W^t$ converts the message vector at t-th time slot into node features $x_i^t$. The function $\\Phi (\\cdot)$ represents M-Transform for aggregating node temporal features as:\n$\\Phi(x_j) = \\sum_{k=1}^T m_{tk}x_{jk}$,\nwhere $m_{tk}$ is the value of transform matrix M, which gives the weight of k-th temporal information aggregated to t-th temporal information. $x_{jk}$ is the feature vector of node $j$ at time slot $k$.\n\nFrom the message-passing perspective, it is clear that the matrix M of the transformation scheme is the key to GTCN since it determines the way of temporal message passing. Thus, GTCN with improper temporal transformation schemes may suffer from low presentation ability due to the complex temporal dynamics in DGs. To boost the representation learning, GTCN with three temporal transformation schemes, i.e., Discrete Fourier Transform [26], Discrete cosine transform [27], and Haar Wavelet Transform [28], are proposed and investigated as follows."}, {"title": "A. GTCN with Discrete Fourier Transform", "content": "The Discrete Fourier Transform (DFT) is a fundamental tool in the field of signal processing and analysis, providing a bridge between temporal information in the time domain and spectral information in the frequency domain. It can effectively handle signals that change over time. Specifically, DFT is defined as:\n$s_t^* = \\sum_{k=1}^T x_k \\exp[-2i\\pi t(k-1)/T]$,\nwhere $x_k$ represents the information at time $t$ transformed by DFT, $s_t^*$ is the input information sequence, and $i$ is the imaginary unit. Then, the transform matrix $M_{DCT}=[m_{tk}]_{T\\times T}$ of DFT is:\n$m_{tk} = \\frac{1}{\\sqrt{T}} \\exp(-2i\\pi t(k-1)/T)$.\nWhen GTCN adopts DFT as a transformation scheme (named GTCNDFT), we obtain the DG representation tensor $H_{DFT}$."}, {"title": "C. GTCN with Discrete Cosine Transform", "content": "The Discrete Cosine Transform (DCT) is also a powerful tool in temporal signal processing. It is similar to the DFT, while the DFT uses complex exponentials to represent signals, the DCT uses only cosine functions, which makes it more suitable for real-valued temporal signals. The basic idea behind the DCT is to represent a temporal signal as a weighted sum of cosine functions of different frequencies. By doing so, it separates the signal into different frequency components. DCT is expressed as:\n$s_t^* = \\sum_{k=1}^T \\alpha(k-1)s_k \\cos[\\frac{\\pi}{T} (t-1)(k-1)+ \\frac{1}{2} ]$,\nwhere $s_t$ represents the information at time $t$ transformed by DCT, and $s_k$ is the input information sequence. The coefficient $\\alpha(n)$ in DCT is defined as:\n$\\alpha(n)=\\begin{cases}\\sqrt{\\frac{1}{T}}, & n=0, \\\\ \\sqrt{\\frac{2}{T}}, & 1 \\le n \\le T.\\end{cases}$\nThen, the transform matrix $M_{DCT}=[m_{tk}]_{T\\times T}$ for DCT is set as:"}, {"title": "D.GTCN with Haar Wavelet Transform", "content": "The Haar Wavelet Transform was introduced as a signal processing method different from traditional Fourier analysis, which is based on the Haar Wavelet. The HWT decomposes a signal into a set of approximation coefficients and detail coefficients, providing a multi-resolution analysis of the signal. Specifically, HWT is defined as:\n$s_t^* = \\sum_{k=1}^T \\Psi (t-1)s_k^*$,\nwhere $\\Psi (\\cdot)$ represents the HWT basis function, which is expressed as:\n$\\Psi (t) = 2^{j/2}\\psi(2^{j} t -i), 0 \\le j \\le T -1, 0 \\le i \\le 2^{j} -1,$\nwhere $j$ is the scale factor, and $i$ is the translation parameter. The above formula shows that $\\Psi (\\cdot)$ is obtained from $\\psi(\\cdot)$ by scaling and shifting, and $\\psi(\\cdot)$ is defined as:\n$\\psi(z) = \\begin{cases} 1, & 0 \\le z \\le 0.5, \\\\ -1, & 0.5 \\le z \\le 1, \\\\ 0, & otherwise. \\end{cases}$\nThus, the transform matrix $M_{HWT}=[m_{tk}]_{T\\times T}$ for HWT is set as:\n$m_{tk} = \\Psi (t-1)$.\nBy adopting $M_{HWT}$ in the GTCN (named GTCNHWT), a representation tensor $H_{HWT}$ of DG is obtained."}, {"title": "E. Ensemble Module", "content": "Ensemble learning is a method that integrates multiple learning algorithms to improve representation learning capabilities [29-31]. In this paper, we introduce three different transformation schemes for GTCN, from which three graph representation tensors $H_{DFT}$, $H_{DCT}$, and $H_{HWT}$ are obtained. To improve the model's accuracy, an ensemble model STGCNDT is proposed. Specifically, different ensemble weights are assigned for each representation tensor to obtain ensemble presentation tensor $H$ as:\n$H = \\alpha H_{DFT} + \\beta H_{DCT} + \\gamma H_{HWT}$,\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are the ensemble weights of different representation tensors. In this study, we set $\\alpha = \\beta = \\gamma = 1/3$."}, {"title": "F. Model Optimization", "content": "This study utilizes the estimation of missing link weights as the downstream task to verify the representation ability of the proposed STGCNDT. After obtaining the ensemble node embedding tensor $H$, the link weight between node $i$ and $j$ at time slot t is estimated as follows:\n$y_{ijt} = [h_i^t || h_j^t]^T r$,\nwhere $y_{ijt}$ is the estimation weight, $r$ is a regression learnable vector to aggregate the features as an estimation value, and $[\\cdot||\\cdot]$ denotes the concatenation operation of embedding vectors.\n\nTo optimize model parameters, a learning objective defined on known data following the data density-oriented modeling principle [32-39] is built as:\n$\\mathcal{L}(\\Theta) = \\sum_{\\Omega} (y_{ijt} - \\hat{y}_{ijt})^2+\\lambda \\kappa ||\\Theta||^2$,\nwhere $y_{ijt}$ is the true value of link weight and $\\Theta = \\{W, r\\}$ represents the parameter set in the model to optimize. $\\Omega$ is the training set. $||\\cdot||^2$ is L2 regularization with coefficient value $\\kappa$."}, {"title": "IV. EMPIRICAL STUDIES", "content": "Evaluation Protocol. This study focuses on dynamic graph representation learning and takes the link weight estimation problem as a downstream analysis task. Thus, the estimation accuracy is adopted as the evaluation protocol. Mean Absolute Error (MAE) [40-62] and Root Mean-Squared Error (RMSE) [76-80] are commonly adopted to measure the estimation accuracy of a tested model [38-46], and they are computed as:\n\n$\\text{MAE} = \\frac{1}{|\\Omega|} \\sum_{i,j,t \\in \\Omega} |y_{ij,t} - \\hat{y}_{ij,t}|$,\n$\\text{RMSE} = \\sqrt{\\frac{1}{|\\Omega|} \\sum_{i,j,t \\in \\Omega} (y_{ij,t} - \\hat{y}_{ij,t})^2}$,\nwhere a abs obtains a's absolute value and $\\Omega$ indicates the test set whose number of entries is denoted by $|\\Omega|$.\n\nDatasets. Four DG datasets from personal dynamic terminal commutation networks are adopted in the experiment. The detailed statistical information of each dataset is shown in Table I. Specifically, the four DG datasets are consistently divided into training, validation, and testing sets in a 60%:20%:20% ratio."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we propose a Spatial-temporal Graph Convolutional Networks with Diversified Transformation (STGCNDT). It is integrated with three different transformation schemes to model the complex temporal patterns of DG. Thus, STGCNDT obtains significant performance improvements on the link weight estimation task in DGs. Moreover, a comparative analysis of different transformation schemes also is conducted, which demonstrates the importance of appropriate transformation schemes for GTCN.\n\nIn the future, we plan to further improve the representation ability of STGCNDT by adopting an adaptive ensemble scheme to measure the importance of different transformation schemes. Moreover, we also plan to theoretically analyze why the ensemble model obtains a better representation learning of DG."}]}