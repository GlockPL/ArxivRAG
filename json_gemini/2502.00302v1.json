{"title": "Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions", "authors": ["Yixuan He", "Aaron Sandel", "David Wipf", "Mihai Cucuringu", "John Mitani", "Gesine Reinert"], "abstract": "How can we identify groups of primate individ-uals which could be conjectured to drive socialstructure? To address this question, one of ushas collected a time series of data for social in-teractions between chimpanzees. Here we usea network representation, leading to the task ofcombining these data into a time series of a singleweighted network per time stamp, where differ-ent proximities should be given different weightsreflecting their relative importance. We optimizethese proximity-type weights in a principled way,using an innovative loss function which rewardssructural consistency across time. The approachis empirically validated by carefully designed syn-thetic data. Using statistical tests, we provide away of identifying groups of individuals that stayrelated for a significant length of time. Applyingthe approach to the chimpanzee data set, we detectcliques in the animal social network time series,which can be validated by real-world intuitionfrom prior research and qualitative observationsby chimpanzee experts.", "sections": [{"title": "1. Introduction", "content": "What drives social structure in primates? To address this,one of us has collected a rich data set on chimpanzees inUganda. Starting in 1998, the data set records proximitiesbetween chimpanzees when observed. Due to visibilityissues in the tropical forest and limits on the time that re-searchers can spend in the forest recording chimpanzeesdata, only a small number of these proximities have beenobserved. To analyze these social proximity data we choosea network representation; a key contribution of this paper isa principled way of obtaining such a representation.\nSocial networks are an important tool in the social andbiological sciences, where individuals are treated as nodesconnected by some interactions which are treated as possiblyweighted edges between nodes; see for example Wasserman& Faust (1994). Creating networks based on various inter-actions is useful for modeling a range of dynamics, suchas disease spread and information transfer. Networks arealso valuable for determining social structures. Althoughsocial networks have featured prominently in sociology and,in the last two decades, animal behavior, see for examplePinter-Wollman et al. (2014), several challenges persist:What behaviors should be used to construct networks? Howare appropriate weights determined? These issues are com-pounded when data interactions of different types are avail-able; here we are thinking of many proximity records withdifferent proximity ranges.\nMultilayer networks are one possibility for representingsuch records, as in Kivel\u00e4 et al. (2014), but such a repre-sentation may not be easy to interpret and may obfuscatethat different proximities are related. The situation becomeseven more intricate when a time series of proximity datais available, as in the chimpanzee case study which mo-tivates our work. As will be described in more detail inSection 3, the animals form a variety of social groupingswhich change throughout the day. The same is true for otherspecies with fission-fusion social dynamics, including hu-mans and bonobos (Silk et al., 2014; Classen et al., 2016;Ramos-Fernandez et al., 2018). A biological research ques-tion is then to identify groups of individuals which are closeto each other at multiple times; in a human data set onemight interpret these as friendship groups.\nIn order to address this question, we combine the proximitydata into a single network per time step, addressing severalmethodological questions, namely: (1) How do we combinethe available proximity data into one network per time step?(2) How do we identify groups of individuals which areclose to one another more often than expected by chance?"}, {"title": "2. Literature Review", "content": "Combining or fusing networks/graphs that represent different views into a unified structure is a well-studied problemrelating to our proximity network fusion setting. Kang et al.(2020) propose a multi-graph fusion model that simultane-ously performs graph combination and spectral clustering.The idea that multi-view data admit the same clusteringstructure aligns with the structural consistency assumptionsin our proposed method. However, this method, togetherwith other multi-view fusion approaches such as Yang et al.(2019b) and Yang et al. (2024), cannot naturally considerstructural consistency over time, but rather focuses on fus-ing multiple views into a unified static graph. Zhang et al.(2021) and Hu et al. (2022) leverage spatio-temporal fu-sion to address challenges in predicting remaining usefullife and in trajectory data analytics, respectively. However,their fusion modules, being within a neural network archi-tecture, cannot be readily applied to combine multiple levelsas in our case.\nNode similarity in network time series is another key topic inour paper. G\u00fcne\u015f et al. (2016) consider neighborhood-basednode similarity scores for link prediction. Yang et al. (2019a)develop a diffusion model to drive the dynamic evolutionof node states and proposes a novel notion of dissimilarityindex. The approach in Meng et al. (2018) learns nodesimilarities by incorporating both structural and attribute-based information. However, none of them considers nodesimilarity based on long-term close relationships.\nDetecting long-term relationships in dynamic systems hasbeen explored from various perspectives. In long-term stud-ies of primate relationships, scientists report how long cer-tain pairs have a high frequency of interaction, but they donot provide a statistical approach to determine what consti-tutes a persisting relationship. For example, Mitani (2009)emphasizes the significance of long-term affiliative relation-ships in social mammals by considering pairwise affinityindexes between male dyads (Pepper et al., 1999) to quantifylong-term relationships, but the indexes neglect consecutiveproximities in the time series. Derby et al. (2024) presenta Bayesian multimembership approach to test what factorspredict the persistence of proximity relationships, but persis-tence is defined deterministically by being in proximity formore than a fixed length of time period without consideringhypothesis-testing based on an expected duration. Qin et al.(2019) investigate the interplay between temporal interac-tions and social structures, but their approach is constrainedto analyzing periodic behaviors. Escribano et al. (2023)study the stability of personal relationship networks in alongitudinal study of middle school students by exploringpersistence of circle structures, which is different from ournovel perspectives of node similarity."}, {"title": "3. Motivation: Chimpanzees in Uganga", "content": "Problem Definition. Denote a static undirected weightednetwork (graph, used interchangeably) as $G = (\\nu,\\varepsilon,\\omega)$,with $V$ the set of nodes, $\\varepsilon$ the set of edges encoding noderelationships, and $\\omega \\in [0,\\infty)||$ the set of edge weights.Such a network can be represented by the adjacency matrix$A = (A_{ij})_{i,j \\in V}$, with $A_{ij} = 0$ if no edge exists from $v_i$ to$v_j$; if there is an edge $e$ between $v_i$ and $v_j$, we set $A_{ij} = w_e$,the edge weight. A network time series is a time series ofnetworks, $\\{G(t)\\}$, where for each time step $t$, the networkis static, and the set of nodes is the same for all time steps.At each time step, a multiplex network is constructed witheach type of interaction between the nodes being describedby a single-layer network; the different layers of networksdescribe the different modes of interaction. We denote themultiplex network time series with $H$ layers by $\\{G^{(h,t)}\\}$,where $h \\in \\{1, . . ., H\\}$ refer to the different types/layers."}, {"title": "4. A Parametric Network Model", "content": "Motivated by Sec. 3, suppose the ground-truth network $G(t)$for each time step $t$ can be expressed by its adjacency matrix$A(t)$, then with $H$ hierarchies of proximity levels ($H = 10$for Sec. 3), we have $A^{(t)} = \\sum_{h=1}^H W_h A^{(h,t)}$ with positive\nnondecreasing weights $W_h$'s, i.e., $0 < W_{h_1} < W_{h_2}$ if $h_1 <h_2$. Given the considerations for consecutive occurrences,we further split each $A(h,t)$ into a weighted sum of theraw network, $A^{(raw,h,t)}$, and an ancillary (\u201cadd\u201d) network,$A^{(add,h,t)}$. In the chimpanzee example, the add networksrepresent consecutive occurrences.\nFor the purpose of modeling, we further express the nonde-creasing weights as a sum of nonnegative increment weights,$W_h = \\sum_{j=1}^h w_j$, resulting in\n$A^{(t)} = \\sum_{h=1}^H W_hA^{(h,t)}$\n$= \\sum_{h=1}^H (\\sum_{j=1}^h w_j)(A^{(raw,h,t)} + W_{add} A^{(add,h,t)}),$ (1)\nso that $W_h$ is the network combination weight for the $h$-thhierarchy, $w_j = W_j - W_{j-1} \\geq 0$ is the nonnegative in-crement from $W_{j-1}$ to $W_j$, $W_{add} \\in [0, 1]$ is the incrementweight for a single network type when increments are con-sidered, e.g., when consecutive occurrences of a certain typeof proximity are recorded, $A(h,t)$ is the adjacency matrixfor a single network type $h$ at time step $t$, $A^{(raw,h,t)}$ is theraw adjacency matrix without considering added increments,and $A^{(add,h,t)}$ is for the increments for each type. Here weset $w_1 = W_1 = 1$ fixed by default for normalization. Notethat we assume that $A^{(raw,h,t)}$ and $A^{(add,h,t)}$ are known,while $w_j$ and $W_{add}$ are learnable parameters."}, {"title": "5. Proposed Method: ProxFuse", "content": "We propose a simple optimization model to fuse proximitynetworks (termed ProxFuse) to learn the increment weights,based on the assumption that the underlying graph structureshould stay roughly stable over time. The adjacency matrixmay evolve, but we assume that the underlying similaritystructure between nodes (based on which the adjacencymatrix is generated, but with possible magnitude fluctua-tions) and the relative magnitudes of weighted node degreesshould stay consistent.\nLet $N(t)$ denote the set of nodes that exist at time stept$\\in$ {1,2,...,T}; we assume $T > 2$ so that we have atleast two time steps. For each time step $t$, we first extract the set of nodes $N^{(t,t+1)} = N(t) \\cap N(t+1)$ whichco-exist (i.e., both with nonzero degrees) at time steps $t$ andt+1. We then construct subnetworks based on $\\tilde{A}^{(t,t,t+1)} =A_{N^{(t,t+1)}, N^{(t,t+1)}}$ and $\\tilde{A}^{(t+1,t,t+1)} =A_{N^{(t,t+1)} N^{(t,t+1)}}$;both matrices take values in $\\mathbb{R}^{|N^{(t,t+1)}|\\times|N^{(t,t+1)}|}$. Foreach network, we construct a similarity graph $S(t,t,t+1)$based on the adjacency matrix $\\tilde{A}^{(t,t,t+1)}$ by taking intoaccount the node similarities. Indeed, we treat each row$\\tilde{A}_i^{(t,t,t+1)}$ in $\\tilde{A}^{(t,t,t+1)}$ as a feature vector for node $i$ attime $t$, and compute the cosine similarity values betweenindividuals, i.e., $S_{ij}^{(t,t,t+1)} =\\frac{\\tilde{A}_i^{(t,t,t+1)}\\cdot \\tilde{A}_j^{(t,t,t+1)}}{\\|\\tilde{A}_i^{(t,t,t+1)}\\|_2 \\|\\tilde{A}_j^{(t,t,t+1)}\\|_2}$,where the numerator takes the vector dot-product and$\\|\\cdot\\|_2$ denotes the vector 2-norm. We can similarly compute $S_{ij} = \\frac{\\tilde{A}_i^{(t+1,t,t+1)}\\cdot \\tilde{A}_j^{(t+1,t,t+1)}}{\\|\\tilde{A}_i^{(t+1,t,t+1)}\\| \\|\\tilde{A}_j^{(t+1,t,t+1)}\\|}.$ Based on$S(t,t,t+1)$ and $S(t+1,t,t+1)$, one objective is to minimize\n$\\mathcal{L}_{sim} = \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\sum_{i,j \\in N^{(t,t+1)}} (S_{ij}^{(t,t,t+1)} - S_{ij}^{(t+1,t,t+1)})^2.$(2)\nIn addition, we assume that weighted node degrees remainconsistent over time. To this end, we compute normal-ized weighted node degrees for time step $t$ as $d_i^{(t,t,t+1)} =\\frac{\\sum_j\\tilde{A}_{ij}^{(t,t,t+1)}}{\\sum_{i,j}\\tilde{A}_{ij}^{(t,t,t+1)}}$. Likewise, we can compute $d_i^{(t+1,t,t+1)}$. Wethen obtain another term of loss function as\n$\\mathcal{L}_{deg} = \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\sum_{i \\in N^{(t,t+1)}} (d_i^{(t,t,t+1)} - d_i^{(t+1,t,t+1)})^2$(3)\nTo penalize extreme combination weights, we additionallyadd a regularization term as\n$\\mathcal{L}_{reg} = \\frac{1}{H} (\\frac{1}{2} ||W_{add}||_2 + \\sum_{h=1}^H ||w_h||_2)$(4)\nTo summarize, our optimization loss function amounts to\n$\\mathcal{L} = \\alpha_1 \\mathcal{L}_{sim} + \\alpha_2 \\mathcal{L}_{deg} + \\alpha_3 \\mathcal{L}_{reg}.$(5)\nThe values of $\\alpha_1$, $\\alpha_2$, and $\\alpha_3$ are considered to be hyper-parameters, which we set to be $\\alpha_1 = 1$, $\\alpha_2 = 1$, and$\\alpha_3 = 0.001$ by default.\nTo cope with the nonnegativity requirement of the learnablenetwork combination weights, $w_h$'s, we employ the inverseof the softplus function $w_h = log(exp(\\eta_h)-1)$ to the initial"}, {"title": "6. Experiments", "content": "To validate the efficacy of our novel network combinationmethod, we test our proposed method on the chimpanzeedata, and construct synthetic models with known combina-tion weights for further empirical evidence. The experimen-tal setup is provided in App. C.1."}, {"title": "6.1. Synthetic Network Model", "content": "We construct evolving individual networks, $G^{(raw,h,t)}$and $G^{(add,h,t)}$, with stable cumulative network $G(t) =(N(t), E(t))$, over time. We let $n$ be the number of nodes intotal, $T$ be the number of time steps, $H$ be the number ofhierarchies (e.g., different proximity levels), and $\\{p_h\\}_{h=1}^H$edge probabilities for each layer of the multiplex network.Details of the construction are provided in App. C.2."}, {"title": "6.2. Synthetic Data Empirical Results", "content": "We conduct experiments on multiple synthetic data sets.Here we set $p_h = 0.1$ throughout, $n = 100$, $P_{add} =0.1$, $T = 14$ with training:validation:test=8:3:3 in the split.We take $H = 5$, vary $W_{add} \\in [0,0.3]$, and take different$w_h$'s. We compare the results for $\\alpha_3 = 0$ and $\\alpha_3 = 0.001$.\nIn our experiments, with a fixed $\\alpha_3$ value, the final optimizedweights are typically robust to initialization. In general, weconclude that for our synthetic data, the proposed methodcan perfectly recover the combination weights of interestup to one decimal point, while with $\\alpha_3 = 0.001$ the finalestimated values are typically closer but smaller than the"}, {"title": "6.3. Application: Chimpanzee Network Combination", "content": "Applying the method in Sec. 5 to the data described in Sec. 3,we optimize the network combination weights by assuming"}, {"title": "7. Node Similarity in Network Time Series", "content": "In addition to network comparison over time, we arealso interested in comparing individuals in network timeseries. Here, we propose two notions of similarity betweenindividuals based on node-wise close relations over time.The notion of relatedness is in principle user-defined; herewe base it on whether or not two individuals are in the same\u201ccommunity\u201d, as explained below. One notion of similarity isbased on how many times they are in close relationship overthe period when they co-exist (i.e., both are non-isolated).We denote this similarity notion as count similarity. Another"}, {"title": "7.1. Theoretical Analysis of Similarity Notions", "content": "Suppose the event for two nodes being closely related isdrawn randomly and independently over time. To quantifythe two notions of similarity, we carry out a theoreticalanalysis on sequences of independent Bernoulli trials withdifferent success probabilities over time. The distribution ofthe number of successes of independent but not necessarilyidentically distributed Bernoulli random variables is calledthe Poisson-Binomial distribution. In general, there is noclosed-form available for its probability mass function, butThm. B.1 in App. B provides a recursion formula for it. Weuse this recursion to assess the significance for the countsimilarity. Similarly, Thm. B.2 in App. B gives a recursionformula for the longest success run in such a sequence,which we use to assess significance for duration similarity."}, {"title": "7.2. Node Similarity via Community Identities", "content": "A central concept here is the notion of close relations. Toyield independent samples, we propose to define two nodesto have a close relationship at a certain time step if and onlyif they share the same community identity, for a time stepwhen they co-exist. For time steps when at least one nodehas no record, we disregard them in the computation.\nSince we think of relatedness as being in the same commu-nity, we carry out community detection at every time step,yielding a sequence of partitions. For any two nodes $i$ and$j$, for each time step they co-exist, we record whether ornot they are assigned to the same community, resulting in asequence of entries taking value 0 (different communities)and 1 (same community), denoted as $\\{B_t\\}$. We then as-sess count similarity via the number of times that $i$ and $j$ arein the same community, which is the number of 1's in thissequence; we also compute duration similarity by the lengthof the longest shared path between them, which is the lengthof the longest run of consecutive 1's in this sequence. Ap-"}, {"title": "7.3. Application to Chimpanzee Networks", "content": "For each yearly graph, we employ the popular Leiden algo-rithm (Traag et al., 2019) to construct communities for eachtime step. In order to mitigate the effect of randomness inher-ent in the community detection algorithm, we run the Leidenalgorithm 20 times for each network, and pick the partitionwith the largest value of modularity, a standard quality mea-sure in community detection (Newman, 2018). With theabove theorems, for each observed similarity value, we com-pute its p-value for the null hypothesis that nodes are closelyrelated (in our case, belonging to the same community) in-dependently and randomly across time. We conduct Bonfer-roni correction to select the most significantly similar nodes."}, {"title": "8. Conclusion and Future Work", "content": "This paper presents a novel optimization approach to com-bine proximity networks into a single network based onhierarchies of proximity levels. It also provides a novelperspective to identify long-term related nodes in networktime series. Based on the learned networks, we plan to fur-"}, {"title": "C. Implementation Details", "content": "C.1. Experimental Setup\nExperiments were conducted on two compute nodes, each with 8 Nvidia Tesla T4, 96 Intel Xeon Platinum 8259CL CPUs@ 2.50GHz and 378GB RAM. We run at most 5000 epochs using the Adam optimizer (Kingma & Ba, 2014) with alearning rate of 0.1 and an early stopping parameter to be 3000 epochs. Anonymized codes are provided at https://anonymous.4open.science/r/ProxFuse. Experimental results are averaged over three runs on differentrandom seeds."}, {"title": "C.2. Synthetic Network Model", "content": "With the aim of constructing evolving individual networks, $G^{(raw,h,t)}$ and $G^{(add,h,t)}$, but stable cumulative network $G(t) =(N(t), E(t))$ over time, the synthetic models are constructed as follows. Let n be the number of nodes in total, T be thenumber of time steps, H be the number of hierarchies (e.g., different proximity levels), and $\\{p_h\\}_{h=1}^H$ edge probabilities foreach layer of the multiplex network.\nInitialization. For the first time step, t = 1, we assume that $G^{(raw,h,t)}$, the raw single-type network for type h at eachtime step t, is generated independently as a Bernoulli random graph with n nodes and edge probability $p_h$. The edgeweights are sampled randomly from integers $[1, . . ., H]$. We use $A^{(raw,h,t)}$ to denote its adjacency matrix. We then generate$A^{(add,h,t)}$ by a Hadamard product of the binary version of $A^{(raw,h,t)}$ with another Bernoulli random graph with n nodes andedge probability $P_{add}$ (but the edge weights are again sampled randomly from integers $[1, . . ., H]$). Suppose networks arecombined based on Eq. (1). We then obtain $A^{(1)} = \\sum_{h=1}^H (w_h\\cdot A^{raw,h, (1)} + W_{add}\\cdot A^{add,h,(1)})$. This fixed network $A^{(1)}$is used to initialize the process.\nAt each subsequent time step t > 1, $A(t)$ is kept fixed and equal to $A(1)$, and the raw and increment networks are constructedthrough a randomized decomposition as follows.\nEdge Assignment. At time step t > 1, the edges of $G(t)$ are shuffled and redistributed among H hierarchies based onnormalized probabilities $\\{p_h\\}_{h=1}^H$. The number of edges assigned to each hierarchy h is sampled from a multinomialdistribution: $E_{h,t}| \\sim Multinomial (|E(1)|, \\frac{p_h}{\\sum_{k=1}^H p_k}),$ where $|E(1)|$ denotes the total number of edges in the initialcombined network $G(1)$ described in $A(1)$. We then randomly assign the edges to each hierarchy based on $E(h,t)$, andconstruct subgraphs $G(h,t) = (N(t), E(h,t))$, which share the same node set as $G(t)$, such that $\u0190(t) = \\bigcup_hE(h,t)$ and$\\bigcap_hE(h,t) = \\O$. The hierarchy-level combined adjacency matrix $A(h,t)$ is computed by normalizing the correspondingsubgraph adjacency matrix:\n$A_{i,j}^{(h,t)} = \\frac{\\mathbb{A}_{i,j}^{(h,t)}}{W_h}$\nwhere $\\mathbb{A}_{i,j}^{(t)} = \\sum_{h=1}^H \\mathbb{A}_{i,j}^{(h,t)}$\n$A_{i,j}^{(h,t)} = \\frac{\\mathbb{A}_{i,j}^{(h,t)}}{\\sum_{k=1}^H \\mathbb{A}_{i,j}^{(k,t)}}$\nfor $(i, j) \\in E(h,t)$ and $A_{i,j}^{(h,t)} = 0$ for $(i, j) \\notin E(h,t)$. This ensu-\nNetwork Construction. Recall that $A^{(h,t)} \\simeq A^{(raw,h,t)} + W_{add} A^{(add,h,t)}$, and that the increment networks $G^{(raw,h,t)}$are generated as subgraphs of the raw networks, $G^{(raw,h,t)}$. First, we sample edges for $G^{(add,h,t)}$ probabilistically basedon $P_{add}$, from $G(h,t)$. We then generate a temporary increment network with edge weights randomly sampled from$[1,..., H]$. We denote the adjacency matrix of this temporary increment network by $A^{temp-add,h,t}$. If $W_{add} = 0$, then we set$A^{add,h,t} = A^{temp-add,h,t}$. Otherwise, the edge weights of $G(add,h,t)$ are constrained to ensure nonnegativity and consistency:\n$A_{i,j}^{add,h,t} = max (0, min (A_{i,j}^{temp-add,h,t} , ( \\frac{\\mathbb{A}_{i,j}^{(h,t)}}{\\mathbb{A}_{i,j}^{(t)}} - \\epsilon) \\div W_{add} ))$\nby taking elementwise minimum and maximum, where e is a small constant to ensure that $G(add,h,t)$ is a subgraph of$G(raw,h,t)$. We then construct $G(raw,h,t)$ by $A^{(raw,h,t)} = A^{(h,t)} - W_{add} A^{(add,h,t)}$.\nAt each time step, the nodes with nonzero degrees in $G(t)$ are identified as participating nodes. This allows us to trackchanges in node participation over time.\nThis method ensures a controlled, randomized decomposition of $A(t)$ into hierarchical raw and increment networks, whilepreserving overall structural integrity and allowing for hierarchical variability."}, {"title": "A. Proximity Types", "content": "We can derive the following proximity types based on the level of proximity two individuals have at a certain recorded time(examples are from Fig. 1):\n\u2022 Type 1 (within 200m): Two individuals in the \"party\" to a focal male (subject of observation), but not observed in\"prox2\" or \"prox5\". E.g., J & \u039a.\n\u2022 Type 2 (within 105m): One individual in \u201cprox5\u201d to focal, and the other in \u201cparty\u201d, but not observed in \u201cprox2\u201d or\u201cprox5\". E.g., \u0397 & \u039a.\n\u2022 Type 3 (within 102m): One individual in \u201cprox2\u201d to focal, and the other individual in \u201cparty\u201d. E.g., A & I.\n\u2022 Type 4 (within 100m): Subject of observation + individual in \u201cparty\u201d, but not observed in proximity or grooming. E.g.,F & \u039a.\n\u2022 Type 5 (within 10m): Two individuals, both in \"prox5\" to focal. E.g., G & H.\n\u2022 Type 6 (within 7m): Two individuals, one in \"prox2\" to focal and the other in \u201cprox5\u201d. E.g., A & H.\n\u2022 Type 7 (2-5m): Subject of observation + individual in \u201cprox5\u201d. E.g., F & E.\n\u2022 Type 8 (within 4m): Two individuals, both in \u201cprox2\u201d to focal. E.g., A & B.\n\u2022 Type 9 (within 2m): Subject of observation + individual in \u201cprox2\u201d but not grooming. E.g., F & A.\n\u2022 Type 10 (within 2m but semantically closer than type 9): Subject of observation + grooming individual in \u201cprox2\u201d.E.g., F & B."}, {"title": "B. Theorems and Proofs", "content": "B.1. Theorem and Proof of Count Similarity\nHere we provide a theorem and the proof relating to count similarity. We note that the distribution of $C_t$ is also called aPoisson-binomial distribution. In general, there is no closed form available for its probability mass function.\nTheorem B.1. [Useful for Count Similarity] For a series of independent Bernoulli trials $\\{B_t\\}$, $t \\in [T \u2212 1] := \\{0, . . .,T-1\\}$, with success rate $p_t$ for $B_t$, denote $C_t \\in \\{0,...,t\\}$ as the total number of successes in $[t]$. For $t \\in \\{0, . . ., T \u2013 1\\}$, theprobability distribution of $C_t$ satisfies\n$P(C_t = 0) = \\prod_{s=0}^t(1-p_s), P(C_t = t + 1) = \\prod_{s=0}^tp_s,$(6)\nand for $L \\in \\{1, ...,t\\}$,\n$P(C_t = L) = p_t \\cdot P(C_{t-1} = L \u2212 1) + (1 \u2212 p_t) \\cdot P(C_{t-1} = L);$(7)\nfor $L \\in \\{t + 2, ..., T\\}$,\n$P(C_t = L) = 0.$(8)\nProof. We first prove the behavior at the boundaries.\nSince the total number of successes is bounded by the number of trials, we always have $C_t \\leq t + 1$. Therefore, $P(C_t =L) = 0$, $t \\in \\{0,...,T \u2212 1\\}$, $L \\in \\{t + 2, ...,T\\}$. The only chance of having pure successes is to never fail the trial,and hence $P(C_t = t + 1) = \\prod_{s=o}^tP_s$, $t \\in \\{0,...,T \u2013 1\\}$ due to the independence between the Bernoulli randomvariables. Similarly, the only chance of having zero successes is to fail the trial every time, and hence $P(C_t = 0) =\\prod_{s=o}^t(1 \u2212p_s)$, $t \\in \\{0,...,T\u22121\\}$.\nIn order to compute $P(C_t = L)$ for $t \\in \\{1, . . ., T \u2013 1\\}$, $L \\in \\{1, . . ., t\\}$, we first analyze the role of this time step $t$. Therethere are two possibilities for time step t: either it is a success, or it is a \u201cfailure\u201d point. Note that we concentrate on the timeseries from the start (time step 0) till time step t, and we impose no constraints on future time steps. For the first situation,we require that we have L 1 successes before t; while for the second situation, we require L successes at time t 1.Therefore,\n$P(C_t = L) = p_t \\cdot P(C_{t-1} = L \u2212 1) + (1 \u2212 p_t) \\cdot P(C_{t-1} = L)$, $t \\in \\{1, ..., T \u2013 1\\}$, $L \\in \\{1, ...,t\\}$."}, {"title": "B.2. Theorem and Proof for Duration Similarity", "content": "The next theoretical result gives a means for assessing significant duration similarity.\nTheorem B.2. [Useful for Duration Similarity"}]}