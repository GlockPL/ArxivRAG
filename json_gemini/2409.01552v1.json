{"title": "Self-Instructed Derived Prompt Generation Meets In-Context Learning:\nUnlocking New Potential of Black-Box LLMs", "authors": ["Zhuo Li", "Yuhao Du", "Jinpeng Hu", "Xiang Wan", "Anningzhe Gao"], "abstract": "Large language models (LLMs) have shown success in gener-\nating high-quality responses. In order to achieve better align-\nment with LLMs with human preference, various works are\nproposed based on specific optimization process, which, how-\never, is not suitable to Black-Box LLMs like GPT-4, due\nto inaccessible parameters. In Black-Box LLMs case, their\nperformance is highly dependent on the quality of the pro-\nvided prompts. Existing methods to enhance response qual-\nity often involve a prompt refinement model, yet these ap-\nproaches potentially suffer from semantic inconsistencies be-\ntween the refined and original prompts, and typically over-\nlook the relationship between them. To address these chal-\nlenges, we introduce a self-instructed in-context learning\nframework that empowers LLMs to deliver more effective re-\nsponses by generating reliable derived prompts to construct\ninformative contextual environments. Our approach incorpo-\nrates a self-instructed reinforcement learning mechanism, en-\nabling direct interaction with the response model during de-\nrived prompt generation for better alignment. We then formu-\nlate querying as an in-context learning task, using responses\nfrom LLMs combined with the derived prompts to establish a\ncontextual demonstration for the original prompt. This strat-\negy ensures alignment with the original query, reduces dis-\ncrepancies from refined prompts, and maximizes the LLM's\nin-context learning capability. Extensive experiments demon-\nstrate that the proposed method not only generates more reli-\nable derived prompts but also significantly enhances LLMs'\nability to deliver more effective responses, including Black-\nBox models such as GPT-4.", "sections": [{"title": "Introduction", "content": "The emergence of Large Language Models (LLMs) has\nsignificantly advanced the field of Natural Language Pro-\ncessing (NLP), achieving remarkable results across various\ntasks (OpenAI et al. 2023; Srivastava et al. 2022; Brown\net al. 2020; Touvron et al. 2023; Devlin 2018). The success\nof these models is highly dependent on the quality of the\ninput prompts, as ambiguous or insecure prompts can lead\nto low-quality and unreliable responses (Zhou et al. 2022;\nZamfirescu-Pereira et al. 2023; Liu et al. 2023). Addition-\nally, the high training costs associated with the large param-\neter sizes of these LLMs make it challenging to fine-tune for\nalignment when handling downstream tasks. In particular,\nfor Black-Box LLMs, training is impossible. Therefore, uti-\nlizing better prompts to guide the models in generating the\ndesired outputs has become an effective approach in apply-\ning LLMs to various tasks.\nTo this end, several approaches are proposed to find the\noptimal prompts that can generally promote LLMs across\nvarious tasks. A popular approach that improves the qual-\nity of the prompts is prompt engineering, which aims to\nrefine the prompts with templates designed manually (Tam\net al. 2021; Reynolds and McDonell 2021). However, this\napproach is limited in specific scenarios and requires the\ncreation of new templates when transitioning to a new sce-\nnario, resulting in a high manual workload and limited us-\nability (Webson and Pavlick 2021). Therefore, some meth-\nods are proposed to rewrite prompts, which usually leverage\nthe powerful capabilities of LLMs (e.g., GPT-4) by directly\nasking the model to refine the prompts (Zhou et al. 2022;\nFernando et al. 2023), or by training a dedicated prompt\nrefinement model to enhance prompt quality (Cheng et al.\n2023; Deng et al. 2022; Kong et al. 2024).\nWhile these methods have achieved significant results,\nseveral challenges may limit their practical use on down-\nstream tasks. For example, previous methods often require\nextensive data collection (Cheng et al. 2023; Huang et al.\n2024) and complex template design (Webson and Pavlick\n2021), and the lack of interaction with the queried model\n(e.g., response model) that is used in downstream tasks dur-\ning rewritten can result in prompts not fully compatible with\nit (Deng et al. 2022; Cheng et al. 2023). Moreover, the\nprompt rewriting process may introduce semantic inconsis-\ntencies between the refined and original prompts, potentially\nleading to less effective responses, as shown in Tab. 1. Given\nthese considerations, we raise the following question:\nIs there a more effective way to prompt the response model\n(e.g., Black-Box GPT-4) than solely generating a refined\nprompt?\nBased on the success of in-context learning (ICL) with\nLLMs, which improves model performance with additional\nclosely related demonstrations (Dong et al. 2022), we pro-\npose a novel framework to stimulate LLMs to generate more\nhelpful and reliable responses by automatically construct-\ning an informative in-context environment for the origi-\nnal prompt. Instead of training a prompt refinement model"}, {"title": "Background", "content": "Supervised Fine-Tuning Supervised Fine-Tuning (SFT)\nwith annotated text descriptions is widely used to adapt\nLLMs into downstream tasks. Given prompt-response pairs\n(x, y) sampled from a distribution D, SFT objective function\nis defined as:\n$\\underset{\\pi}{=} \\sum_{i=1}^{N} \\text{log } \\pi_{SFT}(y_i | x, y_{<i})$                                                                                      (1)\nwhere \ud835\udf0b indicates a LLM policy and $y_{<i}$ refers to all tokens\nbefore the i-th token in response y. In prompt rewritten\ntask, x and y usually indicate original and refined prompt, re-\nspectively. For example, Cheng et al. (2023) collects various\noriginal-refined prompt pairs as (x, y) with the help of GPT-\n4 and then designs to optimize a prompt refinement model\nby minimizing Eq. 1.\nReinforcement Learning from Human Feedback Re-\ninforcement Learning from Human Feedback (RLHF) is\nanother effective tuning method for improving the align-\nment of LLMs with human preferences, which typically\ninvolves two steps: reward modeling and RL training.\nIn reward modeling, a reward model R is designed to\nmeasure response quality to an input prompt: $L_{Reward}$ =\n$-E_{(x,y_c,y_r)~D}[log(\\sigma(R(x, y_c) - R(x, y_r)))]$, where $y_c$ and\n$y_r$ indicate good and bad response, respectively. $\\sigma$ is the\nsigmoid function. Generally, RL training uses the PPO al-\ngorithm (Schulman et al. 2017) with an additional Kull-\nback-Leibler (KL) regularization as below:\n$L_{RLHF}=E_{(x\u223cD,y\u223c\u03c0_\u03b8 (y|x))} [R(x, y) - \u03b2 log \\frac{\u03c0_\u03b8(y|x)}{\u03c0_{ref}(y|x)} ]$             (2)\nwhere \u03b2 > 0 is a hyper-parameter that controls the influ-\nence of the KL penalty. Training a prompt rewritten model"}, {"title": "Motivation", "content": "As shown in Fig. 1(a), previous methods that directly replace\nthe original prompt with a refined prompt to query LLMs\noften overlook valuable information contained in the origi-\nnal prompt, leading to ineffective responses due to essential\nsemantic inconsistencies between the original and rewrit-\nten prompts. Fig. 1(b) gives a illustration to our methods,\nwhich addresses this issue by generating a derived prompt\nthat is used to query the response model. The derived prompt\nand its response are then employed to construct a seman-\ntically similar, high-quality demonstration for the original\nprompt. This in-context query process promotes the model's\nin-context learning capabilities more effectively, ensuring\nhigh-quality responses while consistently querying the origi-\nnal prompt, thereby avoiding ineffective results."}, {"title": "Methodology", "content": "Task Formulation\nWe first define the concept of a derived prompt and its gener-\nation process. A derived prompt x' is a transformation of the\noriginal prompt x that maintains close relevance and shows\nimproved expression, without necessarily being a rewritten\nor refined prompt. Our goal is to train an effective generation\nmodel \ud835\udf0b that is initialized by a LLM (e.g., Llama3-Instruct)\nand can reliably produce a derived prompt x' ~ \ud835\udf0b(\u00b7|x) given\noriginal prompt. Consequently, a higher-quality response y'\ncan be generated by query a response model M, still serv-\ning as a useful and effective feedback for x, due to their\nsimilar semantics. Generally, the model can be a frozen\nBlack-Box model (e.g., GPT-4) or a learnable model (e.g.,\nthe Llama family (Touvron et al. 2023)). In this research, we\nfocus on optimizing a trainable model \ud835\udf0b parameterized by\n\u03b8\u2208 Rd, with the goal of x' to be more semantically derived\nto x and better aligned with M."}, {"title": "Self-instructed RL for Derived Prompt Generation\nModel", "content": "As mentioned in previous section, directly using Eq. 1 and 2\nto optimize a derived prompt generation model \ud835\udf0b\u03b8 typically\nintroduces onerous data collection and lacks of alignment\nbetween derived prompts and response model, leading to\nsuboptimal compatibility with downstream tasks. To address\nthese issues, we propose a self-instructed RL objective for\neffective refinement model training.\nLet D = {$x_i$}$_{i=1}^N$ denote a training data set of length N,\nwhich only includes the original prompts. Assume that we\nhave a reliable reward model R, which measures the re-\nsponse quality of the response model M when using the\nderived prompt x'. Instead of utilizing pre-collected x' to\noptimize the model \ud835\udf0b\u03b8 in an offline mode, we aim to lever-\nage the reward for M response to x' to direct the optimiza-\ntion, which can further generate desirable x' that is better\naligned with M (We freeze M during \ud835\udf0b\u03b8 training since the\nparameters of M are inaccessible). To this end, we design\nto maximum the following objective:\n$E_{(x~D,x'~\u03c0_\u03b8 (x']x),y'~M(x'))} [R(x', y') - \u03b2 log \\frac{\u03c0_\u03b8(x'|x)}{\u03c0_{ref}(x'x)} ]$                  (3)\nwhere x' is from the derived prompt generation model \ud835\udf0b\u03b8(x)\nand y' is from the response model M(x'), \ud835\udf0bref is the ref-\nerence model identically initialized by \ud835\udf0b and \u03b2 is a hyper-\nparameter for stable model training.\nIt is important to note that Eq. 3 relies on the \ud835\udf0b\u03b8 model\nhaving basic derived prompt generation capabilities. Given\nan original prompt x, \ud835\udf0b\u03b8 can directly rewrite it instead\nof performing the task in x (e.g., answering the question\nposed by x). Therefore, previous methods (Kong et al. 2024;\nHuang et al. 2024) necessitate an SFT stage to transform\nthe pre-trained \ud835\udf0b into a prompt rewriter \ud835\udf0bSFT, which initial-\nizes both \ud835\udf0b\u03b8 and \ud835\udf0bref, demanding extensive data collection\nof (x, x') pairs."}, {"title": "Intent-consistency Oriented In-context Query\nFramework for Inference", "content": "Note that a well pre-trained \ud835\udf0b\u03b8 model (e.g., Llama3-\nInstruct) should inherently possess basic capabilities of\ninstruction-following (Ouyang et al. 2022) and paraphras-\ning. Therefore, we propose to use a derived prompt gener-\nation (DPG) instruction XDPG to overcome the necessity of\nSFT. Specifically, we manually design a XDPG, which is pro-\nvided in supplementary materials. As expected, \ud835\udf0b\u03b8 can still\ngenerate a useful derived prompt x' based its instruction-\nfollowing capability:\nX = Concat([XDPG, x]),\nx' ~ \ud835\udf0b\u03b8(X).                                                                                    (4)\nWith the help of XDPG, our method can effectively elimi-\nnate the need for data collection and additional training costs\nintroduced by SFT. Additionally, by strategically leveraging\nthe M into the training process of \ud835\udf0b\u03b8, the generated derived\nprompts will be more in line with the preferences of the re-\nsponse model. Our final training objective is shown below:\n$E_{(x~D,x'~\u03c0_\u03b8 (x'\\X),y'~M(x'))} R(x', y')-\u03b2log \\frac{\u03c0_\u03b8(x'|X)}{\u03c0_{ref}(x'X)}$                                                                                                         (5)\nWe summarize the training process in Alg. 1.\nAlthough the derived prompt generation model trained us-\ning Eq. 5 can produce higher-quality, semantically rich, and\nmore compatible x' for the response model, there remains a\nrisk of semantic inconsistency and intent shift due to the un-\ncontrollability of the generation process. Consequently, di-\nrectly replacing the original prompt with the derived prompt\nwould not be the optimal solution to make full use of it.\nTo address the mentioned issue and better activate the\nLLM's inherent knowledge, we propose a general in-context\nquery framework to mitigate potential semantic inconsis-\ntencies, where we leverage high-quality, relevant in-context\ndemonstrations derived from the original prompt. Therefore,\nthis approaches can effectively enhance LLM ability to bet-\nter respond to the user's original query. As shown in Alg. 2,\nwe construct an intent-consistent in-context query by filling\nthe following template using (x, x', y'), where x, x', y' indi-\ncates the original prompt, its corresponding derived prompt\nand the LLM response to the derived prompt, respectively.\nThis in-context query requires the better LLM respond to the\noriginal question x by emulating the quality, style, and level\nof detail of the response y' given to x':"}, {"title": "Experiments", "content": "In this section, we conduct extensive experiments on var-\nious downstream datasets to comprehensively evaluate our\nmethod when compared with baseline methods. Ablation\nstudies demonstrate the superior and necessity of our pro-\nposed self-instructed RL method for derived prompt gener-\nation, and also shows that ICL can serve as a flexible play-\nand-plug module to generally boosting existing methods to\nachieve better performance. We provide detailed training\nsettings in supplementary materials.\nExperimental Setup\nDatasets. To train a derived prompt generation model by\nEq. 5, we utilize the BPO training dataset by following pre-\nvious work (Huang et al. 2024; Cheng et al. 2023), which"}, {"title": "Results", "content": "We use OP to denote the responses obtained by the LLM\nfrom Original Prompt, where BPO and OD indicates that of\nBPO refined prompt and Our Derived prompt, respectively.\nICL represents the in-context learning environment. OURS\nindicates OD + ICL. As shown in Tab. 2, our method shows\nan overall performance improvement than baseline methods,\nspecifically for Black-Box model. Our method show multi-\nple significant advantages:\nStability Across Datasets: Our method demonstrates ex-\ncellent performance and higher win rates across all four eval-\nuation datasets compared to OP and BPO. The ICL queries\ngenerated by our method effectively promote the LLM to\nproduce high-quality, more helpful, and comprehensive re-\nsponses to the original questions, showcasing its robustness\nin handling various tasks, whether complex or simple.\nConsistency Across Models: Our method shows consis-"}, {"title": "Ablation Study", "content": "To further illustrate the advantages of our proposed self-\ninstructed RL method (OD) and the importance of the ICL\nenvironment (ICL), we use derived prompts generated by\nLlama3-8B to query GPT-4, where the generated response\nare then evaluated by GPT-4. Tab. 3 presents the comparison\nvarious combinations of OD and ICL with other methods\nusing two evaluation datasets: Vicuna Eval (Chiang et al.\n2023) and Self-Instruct (Wang et al. 2022). By comparing\n#1 and #2, it can be observed that from the perspective of\nprompt refinement, our OD already exhibits higher quality\nthan BPO, thereby promoting LLMs to generate more help-\nful responses. In Vicuna Eval, OD's win rate is 67.6%, sig-\nnificantly higher than BPO's 53.8% and those are 50.8% and\n44.9% in Self-Instruct Eval, where we achieve 5.9% perfor-"}, {"title": "Independent Analysis of Self-Instructed RL and\nICL Query Framework", "content": "In this section, we mainly focus on independently analyzing\nhow our proposed self-instructed RL and ICL query frame-\nwork perform. Fig. 2 shows two advantages of our methods:\nEffectiveness of ICL framework: Without training, di-\nrectly using LLM to generate derived prompts combined\nwith ICL (OD + ICL) has already achieved excellent re-\nsults, achieves 39.1% win rates improvement. This super-\nvising performance indicates that our proposed ICL frame-\nwork is highly general and effective even without additional\noptimization."}, {"title": "Related Work", "content": "Prompt Refinement Prompts are crucial for guiding\nLLMs to produce more helpful response, leading to exten-\nsive research on improving their quality. Initially, prompt\noptimization relied on manually crafted templates (Reynolds\nand McDonell 2021), a labor-intensive process with inter-\npretation challenges (Webson and Pavlick 2021). Recent\nstudies automate this process using techniques like gradient-\nbased search (Shin et al. 2020; Pryzant et al. 2023), para-\nphrasing (Haviv, Berant, and Globerson 2021; Jiang et al.\n2020) and leveraging LLMs to generate prompts (Zhou et al.\n2022; Fernando et al. 2023; Yang et al. 2024b; Cheng et al.\n2023). Additionally, RL based methods are designed to op-\ntimize a prompt rewritten model through reward functions\nand task-specific templates (Deng et al. 2022; Kong et al.\n2024; Zhang et al. 2022; Huang et al. 2024).\nRLHF RLHF has been widely explored to align LLMs\nwith human preferences (Stiennon et al. 2020; Ouyang\net al. 2022; Bai et al. 2022b; Lee et al. 2023). Common\napproaches include building a reward model using maxi-\nmum likelihood estimation (MLE) and optimizing it with\nthe Proximal Policy Optimization (PPO) algorithm (Schul-\nman et al. 2017). However, replicating PPO's success has\nproven challenging for the open-source community due to\nthe high resource demands. To address this, some research\nhas shifted to offline direct preference learning (Zhao et al.\n2023; Rafailov et al. 2024; Li et al. 2023), which bypasses\nreward modeling and directly optimizes a loss target using\nan offline dataset. Among these methods, ours and Cheng\net al. (2023) fall into better alignment with Black-Box\nmodel.\nImproving In-Context Learning Several approaches\nhave been introduced to enhance in-context learning (ICL)\nperformance by improving the selection of in-context ex-\namples. Some methods focus on refining template selec-\ntion (Yin et al. 2023), while others aim to enhance the choice\nof examples (Liu et al. 2021; Rubin, Herzig, and Berant\n2021). Additionally, Wan et al. (2023) introduces a crite-\nrion for evaluating examples based on some criteria. Other\nrecent innovations include flipped learning (Ye et al. 2022)\nand noisy channel prompting (Min et al. 2021). For multiple-\nchoice problems, Xu et al. (2023) suggests using K-nearest\nneighbors for label assignment, and Yang et al. (2023) pro-\nposes iterative context updates."}, {"title": "Conclusion", "content": "In this paper, we introduced an innovative method for en-\nhancing LLM performance using an automatically generated\nin-context learning framework. By creating derived prompts\nthrough a self-instruct RL mechanism, our approach en-\nriches the context of the original prompts. Extensive ex-"}, {"title": "Failed Refined Prompt Cases", "content": "In this section, we provide more failed refined prompt cases. In this situation, directly querying the response model (e.g., GPT-4)\nwith the refined prompt would definitely obtain useless response"}, {"title": "Instruct-guided Derived Prompt Generation Template XDPG", "content": "Recall in Section Self-instructed RL for Derived Prompt Generation Model, we design a derived prompt generation instruction\nXDPG to ask the model \ud835\udf0b\u03b8 to generate a derived prompt, instead of using a \ud835\udf0bsFT. Here we proved the specific template shown as\nbelow:\n### Instruction: Please provide a more comprehensive, easily understandable, and answerable version of the\nfollowing question. Ensure that necessary contextual information is added during the rewrite, but do not limit the\nunderstanding and response to the question. Avoid confining the question to just a few aspects, allowing the responder\nto think from multiple angles. Only return the refined question and do not explain. Here is my original question:\u201d.\n### Question: {Original Prompt x}.\nAs shown in Alg. 1, we firstly concatenate XDPG with original prompt x together as X, which will be then sent to \ud835\udf0b\u03b8 to obtain\nthe derived prompt x'."}, {"title": "Experiments", "content": "Training Settings. For training settings, we use ReMax (Li et al. 2023) to train the derived prompt generation model facili-\ntated by DeepSpeed ZeRO-2 (DeepSpeed 2024). We set the temperature parameter to 7 = 1.0 and use nucleus sampling with\na parameter of topp = 0.9 for all models. The maximum length for derived prompt generation and response model is set to 256\ntokens. We conduct experiments on 4 NVIDIA A100 GPUs. All experiments are trained with a learning rate of 1 \u00d7 10-6 for 2\nepoch with decay, where the KL penalty \u03b2 is set to 0.05 for all models. Our batch size is set to 1."}, {"title": "Reward Evaluation", "content": "In addition to GPT-4 as evaluator for comparing the quality of response promoted by different methods,\nwe also adopt Reward Score into quantitative evaluation, which provides more comprehensive understanding to our methods.\nTherefore, we train a Llama3-8B-Instruct as derived prompt generation model and then query GPT-4. As shown in Tab. 5, we\ncan observe our method can prompote GPT-4 generate more useful and higher quality response in the view of Reward Score,\nexcept for Dolley Eval."}, {"title": "GPT-4 Original Prompt-based Justification Prompt", "content": "In this section, we give a GPT-4 based pair-wise justification prompt shown as below.\n### System message:\nPlease act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user\nquestion displayed below. You should choose the assistant that follows the user's instructions and answers the user's\nquestion better. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\nand level of detail of their responses. Begin your evaluation by comparing the two responses and provide a short\nexplanation. Avoid any position biases and ensure that the order in which the responses were presented does not\ninfluence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain\nnames of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by\nstrictly following this format: '[[A]]' if assistant A is better, '[[B]]' if assistant B is better, and '[[C]]' for a tie.\nPrompt template:\n{User Question}\n{Original Prompt}\n[The Start of Assistant A's Answer]\n{answer a}\n[The End of Assistant A's Answer]\n[The Start of Assistant B's Answer]\n{answer b}\n[The End of Assistant B's Answer]"}, {"title": "More Case Study", "content": "In this section, we provide two more examples shown as below."}]}