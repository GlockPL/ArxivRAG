{"title": "BayesNAM: Leveraging Inconsistency for Reliable Explanations", "authors": ["Hoki Kim", "Jinseong Park", "Yujin Choi", "Seungyun Lee", "Jaewook Lee"], "abstract": "Neural additive model (NAM) is a recently proposed explainable artificial intelligence (XAI) method that utilizes neural network-based architectures. Given the advantages of neural networks, NAMs provide intuitive explanations for their predictions with high model performance. In this paper, we analyze a critical yet overlooked phenomenon: NAMs often produce inconsistent explanations, even when using the same architecture and dataset. Traditionally, such inconsistencies have been viewed as issues to be resolved. However, we argue instead that these inconsistencies can provide valuable explanations within the given data model. Through a simple theoretical framework, we demonstrate that these inconsistencies are not mere artifacts but emerge naturally in datasets with multiple important features. To effectively leverage this information, we introduce a novel framework, Bayesian Neural Additive Model (BayesNAM), which integrates Bayesian neural networks and feature dropout, with theoretical proof demonstrating that feature dropout effectively captures model inconsistencies. Our experiments demonstrate that BayesNAM effectively reveals potential problems such as insufficient data or structural limitations of the model, providing more reliable explanations and potential remedies.", "sections": [{"title": "1 INTRODUCTION", "content": "EXPLAINABLE artificial intelligence (XAI) has become a significant field of research as machine learning models are increasingly applied in real-world systems including finance and healthcare. To provide insight into the underlying decision-making process behind the predictions made by these models, numerous researchers have developed various techniques to assist human decision-makers.\nRecently, Agarwal et al. [1] proposed a neural additive model (NAM) that utilizes neural networks to achieve both high performance and explainability. NAM is a type of generalized additive model (GAM) that involves the linear or non-linear transformation of each input and yields the final prediction through an additive operation. Previous studies have demonstrated that NAM not only learns complex relationships between inputs and outputs but also provides a high level of explainability based on neural network architectures and training techniques.\nIn this paper, we analyze a critical yet overlooked phenomenon: the inconsistency phenomenon of NAM. Fig. 1 illustrates this issue, where two independent NAMs, trained on the same dataset and architecture, produce different explanations due solely to variations in random seeds. Such inconsistency has traditionally been viewed as a problem to be solved [2].\nHowever, we argue that these inconsistencies are not merely obstacles but can offer valuable insights to uncover external explanations within the data model. Through a simple theoretical model, we show that NAMs naturally exhibit the inconsistency phenomenon even when trained on usual datasets that contain multiple important features. Building on this insight, we propose the Bayesian Neural Additive Model (BayesNAM), a novel framework that combines Bayesian neural networks with feature dropout to harness these inconsistencies for more reliable explainability. We also provide theoretical proof that feature dropout effectively leverages inconsistency. Our real-world experiments demonstrate that BayesNAM not only provides more reliable and interpretable explanations but also highlights potential issues in the data model, such as insufficient data and structural limitations within the model.\nThe main contributions can be summarized as follows:\nWe investigate the inconsistency phenomenon of NAMs and analyze this phenomenon through a simple theoretical model.\nWe propose a new framework BasyesNAM, which utilizes Bayesian neural network and feature dropout. We also establish a theoretical analysis of the efficacy of feature dropout in leveraging inconsistency information.\nWe empirically demonstrate that BayesNAM is particularly effective in identifying data insufficiencies or structural limitations, offering more reliable explanations and insights for decision-making."}, {"title": "2 RELATED WORK", "content": "As numerous machine learning and deep learning models are black-box, a line of work has attempted to explain the decisions made by a black-box model. We call these methods post-hoc methods since they are applied after the model has been trained. While post-hoc methods offer some interpretability, recent work [3], [4] has argued that these methods can lead to unreliable explanations, which could potentially have detrimental effects on their explainability.\nIn contrast to post-hoc methods, intrinsic methods aim to develop an inherently explainable model without additional techniques [4]. Agarwal et al. [1] proposed a neural additive model (NAM), which combines a generalized additive model [5] and neural networks. To be specific, given d features, X1,X2,\uff65\uff65\uff65, xd and a target y, NAM constructs d mapping functions as follows:\n$y = f_1(x_1) + f_2(x_2) + \u00b7\u00b7\u00b7 + f_d(x_d) + \\beta$, (1)\nwhere \u03b2 is a bias term and each mapping function fi. In Fig. 2, We illustrate an example of NAM. By utilizing the neural network, NAMs capture the non-linear relationship and achieve high performance while maintaining clarity through a straightforward plot.\nDespite their strengths, NAMs frequently exhibit inconsistent explanations even when trained on identical datasets with the same architectures, as illustrated in Fig. 1. These inconsistencies can also be observed in the original work [1], where the mapping functions produced by different NAMs within an ensemble show substantial variation, despite being trained under the same experimental conditions.\nWhile this inconsistent phenomenon across NAMs can harm its explainability as they are intended to be XAI models, this phenomenon has received limited attention in the literature. To the best of our knowledge, only one study has explicitly addressed this issue. Radenovic et al. [2] introduced the neural basis model (NBM), which used shared basis functions across features rather than assigning independent mapping functions to each feature. They argued that NBM reduces divergence between models, offering more consistent shape functions compared to NAM, thus mitigating the inconsistency problem.\nIn contrast, this paper presents a novel view on the inconsistency phenomenon. Rather than treating it as a problem to be solved, we argue that these inconsistencies provide valuable information about the data model."}, {"title": "2.2 Bayesian Neural Network", "content": "Although the use of a single model is a fundamental approach, numerous studies have found that a point-estimation is often vulnerable to overfitting and high variance due to its limited representation [6]. To overcome this limitation, Bayesian neural network estimates the model distribution instead of calculating a fixed model. Given the data (x, y) ~ D and the prior p(w), we aim to approximate the posterior p(w|x,y). Specifically, rather than using a fixed weight vector wi, it aims to find a distribution of weight vectors $N(\\mu_i, diag(s_i)^2)$ and learn the mean vector \u03bci and the standard deviation vector si.\nSince the distribution p(x,y) is generally intractable, several methods have been developed to approximate the posterior, including Markov Chain Monte Carlo (MCMC) [7] and variational inference approaches [8], [9]. While MCMC methods can provide more accurate estimates, their high computational cost [10] has led to the use of variational inference methods across diverse domains [11], [12].\nDuring optimization in variational inference methods, an weight vector $w_i = \\mu_i + s_i \\odot \\epsilon$ is sampled for each forward step where \u0454 ~ N(0, I). The prior distribution can be simply chosen as the isometric Gaussian prior N (0, s3I) where so is a predefined standard deviation to explicitly calculate the KL-divergence [11].\nA promising direction in the field of Bayesian neural networks is their integration with other domains to enhance model explainability. Bayesian neural networks provide weight distributions that enable the identification of high-density regions or confidence intervals, which can be used for uncertainty estimation. Researchers and practitioners in several domains that require reliable explanations, such as medicine [13] and finance [14], have also explored the utilization of Bayesian models to measure the confidence of prediction for trustworthy decision-making."}, {"title": "3 METHODOLOGY", "content": "In Section 3.1, we first investigate the inconsistency phenomenon of NAMs with a simple theoretical model. Our empirical findings show that this inconsistency can easily occur, even when datasets contain more than one important feature. Subsequently, in Section 3.2, we propose a new framework called BayesNAM, which combines Bayesian neural network with feature dropout, to leverage the inconsistency information as a source of valuable indicator. This framework is supported by a theoretical analysis demonstrating the effectiveness of feature dropout in capturing diverse explanations. Finally, we provide a detailed explanation of the proposed framework."}, {"title": "3.1 Rethinking Inconsistency of Neural Additive Model", "content": "We begin the analysis by identifying and investigating the inconsistent explanations of NAM. To this end, we construct a simple theoretical model. Here, we consider a binary classification task where the target y can have a value in"}, {"title": "=-", "content": "{-1, 1}. Inspired by [15], we construct the input-target pairs (x,y) = (x1, x2,..., xd, y) from a distribution D as follows:\n$x_1 = \\begin{cases} +y \\text{ with probability } p\\\\ -y \\text{ with probability } 1 \u2013 p\\end{cases}$\n(2)\n$x_2,...,x_d \\overset{i.i.d.}{\\sim} N(\\lambda y, \\sigma^2)$, (3)\nwhere x2,...,xa are independently and identically sampled from a normal distribution N with the mean \u03bby and the standard deviation \u03c3\u00b2 for positive \u03bb and \u03c3. It is important to note that the features X2,..., Xd are uncorrelated, as they are drawn independently and identically distributed. By adjusting the values of p and \u03bb, we can control the significance of x1 and x2,..., xd in predicting y, as stated in the following lemma:\nLemma 1. (Derived from [15]) Consider a linear classifier h,\n$h(x_2,..., x_d) = sign(W_2x_2 + W_3x_3 + \u00b7\u00b7\u00b7 + W_dx_d)$. (4)\nThen, even a natural linear classifier, h(\u00b7) with w\u2081 = 0, can easily achieve a higher classification accuracy than p, which is a natural accuracy of the model that only uses x1, if the following statement is satisfied:\n$\\Phi_{X \\sim N (0, \\sigma^2/(d-1))} (\\lambda) > p$, (5)\nwhere \u03a6x(\u00b7) is the cumulative distribution function of X. (Detailed proof is presented in Appendix)\nLet p be a sufficiently large positive number. When \u03bb = 0, only x1 is useful to predict y and other features x2,\u00b7\u00b7\u00b7, Xd are not correlated to y. As \u03bb increases, x2,\u2026\u2026,xa become correlated to y. By Lemma 1, if (5) is satisfied, a model that only considers x2,\u00b7\u00b7\u00b7, xd can achieve a higher classification accuracy than p. In summary, if \u03bb = 0, X1 would be the only feature with a high importance in predicting y, while x2,...,xd is enough to have a significant performance in predicting y for a large \u03bb > 0.\nNow, we consider the following two cases with d=3:\nCase-I. Single important feature exists (\u03bb = 0). In this case, only x1 is effective in predicting y, while 12 and X3 are not useful.\nCase-II. Multiple important features exist (\u03bb = 3). In this case, all the features, X1, X2, and 13 are highly correlated with y. The model uses 12 and 23 can perform better than the model sorely depends on x1 since $\\Phi_z(\\lambda=3)= 10^10$. Given this theoretical model, we generated two sets of data containing 50,000 training examples and 10,000 test examples and trained two different NAMs on each dataset for different random seeds. For simplicity, we fixed the feature dimension to d = 3, the probability p = 0.95, and $\\sigma^2 = \\frac{d}{d-1}$, resulting (5) becomes \u03a6z(\u03bb) > p, where Z is drawn from the standard normal distribution N(0, 1). For each mapping function fi of NAM, we constructed a simple neural network with two linear layers containing 10 hidden neurons and used ReLU as an activation function. The models are trained by SGD with a learning rate of 0.01. One epoch was sufficient to achieve high training accuracy. Fig. 3 (Case-I) and Fig. 4 (Case-II) illustrate the mapping functions of trained NAMs for each case. Specifically, for Case-I, we observed that the two NAM models trained with different random seeds exhibited similar test accuracy and explanations (94.9% and 95.0%, respectively). As shown in Fig. 3, the mapping functions for each x\u2081 have similar shapes, with f\u2081 being the only increasing one and the others being almost constant. Therefore, in this case, NAM successfully captures the true importance of features and provides reliable explanations.\nIn contrast, for Case-II (Fig. 4), the mapping functions"}, {"title": "3.2 Bayesian Neural Additive Model", "content": "In the previous subsection, we explored the inconsistency phenomenon in NAMs and suggested that rather than being a flaw, this inconsistency can serve as a valuable source of additional information, shedding light on underlying external explanations in the data model. In this section, we introduce BayesNAM, a novel framework designed to leverage this inconsistency. BayesNAM incorporates two key approaches: (1) a modeling approach based on Bayesian structure and (2) an optimization approach utilizing feature dropout. Each of these approaches will be detailed in the following paragraphs.\n1) Modeling Approach: Bayesian Structure for Inconsistency Exploration. A naive approach to exploring possible inconsistencies in NAMs is by training multiple independent models. Indeed, Agarwal et al. [1] trained several NAMs and visualized the learned shape functions fk (xk). However, this requires training n independent models, leading to a computational burden proportional to n, making it impractical for large-scale applications.\nTo address this limitation, we propose using Bayesian neural networks [8], [9], which inherently allow for efficient exploration of model uncertainty without the need to train multiple independent models. Under variational inference and Bayes by Backprop [8], [9], Bayesian neural networks rather train the mean parameter \u00b5i and the standard deviation parameter si instead of an weight vector wi. Then, during the training and inference phase, it samples a weight W\u2081 = \u00b5i + Si e for a random vector e from a predefined distribution. Following prior works [11], [12], we adopt the reparameterization trick [16] for efficient training. This results in the following training objective.\n$\\min\\limits_{\\mu_i, s_i} (\\mathcal{L}(f(\\sum\\limits_{i=1}^{d} f_i(x_i) + \\beta), y) + \\beta) +  \\sum\\limits_{i=1}^{d} KL(q_{\\mu_i,s_i}(W_i)||p(w_i))$, (6)\nwhere L(\u00b7) is a given loss function and KL(\u00b7||\u00b7) is the KL-divergence. For further details, we refer the readers to [9]."}, {"title": "", "content": "In Fig. 7, we present the structural framework that integrates a Bayesian neural network with a neural additive model. For each sampled weight w(i), we compute the corresponding predictions fi (xii)). This sampling approach enables the model to efficiently explore a diverse range of model spaces without needing to train multiple models. By incorporating a Bayesian neural network, the model provides high-density regions of the mapping functions and provides confidence intervals for feature contributions, offering richer interpretability."}, {"title": "2) Optimization Approach: Feature Dropout for Encouraging Diverse Explanations.", "content": "Although Bayesian neural networks provide an efficient mechanism for exploration, they do not inherently guarantee exploring diverse explanations. Indeed, as shown in Fig. 8a, naive Bayesian neural network alone tends to focus on a single feature, similar to training a single NAM, rather than adequately exploring diverse explanations. As previously noted in related works [11], [12], we observe that increasing the standard deviation hyper-parameter so within Bayesian neural network tends to degrade model performance and fails to address this issue effectively. Therefore, given the presence of diverse valid explanations shown in Figures 4 and 5, it is evident that a method is needed to encourage the exploration of diverse explanations.\nAs a potential solution, we propose the use of feature dropout during optimization. Feature dropout, initially introduced by Agarwal et al. [1], extends traditional dropout by selectively omitting individual feature networks during training. The hyperparameter 7 determines the probability of dropping each feature. While the original work focused on improving model performance with feature dropout, we here provide a theoretical analysis showing that feature dropout implicitly encourages diverse explanations, preventing over-relying on any single feature.\nGiven the theoretical model in Section 3.1, we establish the following theorem.\nTheorem 1. (Feature Dropout Implicitly Encourages Exploring Diverse Explanations) Given the dataset (x, y) in (2), the linear classifier h in (4), and the feature dropout rate \u03c4, without loss of generality, the maximal training accuracy of h that only uses k features becomes\n$P(k, \\tau) =  \\mathbb{P}_{x_i \\sim N(\\lambda y, \\sigma^2), u_i \\sim B(1-\\tau)}\\left[ \\sum\\limits_{i=2}^{y-k} u_i x_i > 0 \\right]$.\nThen, for k \u2265 3 and \u315c\u2208 [0, 1], the gap \u2206P := P(k, t) \u2013 P(1,T) is always positive and increases as \u315c increases. Thus, the model leverages multiple features to achieve high performance, implicitly encouraging the exploration of diverse explanations.\nSketch of proof. Let qj = \u03a6z(\u03bb\u221aj/\u03c3). Then, P(k, t) can be formalized as follows:\n$P(k, \\tau) = \\sum\\limits_{j=1}^{k} q_j \\binom{k}{j} (1-\\tau)^{j} \\tau^{k-j}$.\nTo show that \u2206P(k, r) > 0 and $\\frac{\\delta}{\\delta \\tau} \\Delta P(k,r)>0$ for k \u2265 3 and 87 \u2208 [0,1], we use mathematical induction.\nApplying Pascal's identity and strong induction, we find\n$\\frac{\\delta}{\\delta \\tau} AP(k+1,\\tau)=P(3, 1) + q1\\tau^{2}(3 \u2013 (k + 1)^{-2}) + (q_2 - q_1)\\tau(1 \u2013 \\tau)(6 \u2212 (k + 1)k\\tau^{k-2}) + 3(q_3 \u2014 q_2)(1 \u2013 \\tau)^2 \\sum\\limits_{j=2}^{k} (q_{j+1} \u2013 q_j) \\binom{k+1}{j+1} (1 \u2013 \\tau)^{j} \\tau^{k-j}$."}, {"title": "4 EXPERIMENTS", "content": "In this section, we present empirical findings comparing the performance of our proposed framework against traditional models, such as Logistic/Linear Regression, Classification and Regression Trees (CART), and Gradient Boosted Trees (XGBoost) [17], as well as recent explainable models including the Explainable Boosting Machine (EBM) [18], NAM, NAM with an ensemble method (NAM+Ens), and our proposed BayesNAM. For Logistic/Linear Regression, CART, XGBoost, and EBM, we conducted a grid search for hyperparameter tuning, following the settings outlined in [1]. We found that using ResNet blocks-comprising two group convolution layers with BatchNorm and ReLU activation-yields better performance for NAM and BayesNAM compared to the ExU units or ReLU-n suggested in [1]. For NAM+Ens, we trained five independent NAMs, and both NAM+Ens and BayesNAM utilized soft voting for model aggregation during evaluation. Detailed settings are provided in the Appendix.\nWe evaluated all models on five different datasets: Credit Fraud [19], FICO [20], and COMPAS [21] for classification tasks, and California Housing (CA Housing) [22] and Boston [23] for regression tasks. As shown in Table 1, BayesNAM demonstrates comparable performance to other benchmarks across datasets, with particularly strong results in classification tasks such as COMPAS, Credit Fraud, and FICO. For regression tasks, BayesNAM tends to be less accurate, which we discuss further in the Appendix."}, {"title": "4.1 Identifying Data Inefficiency", "content": "The capability of BayesNAM to explore diverse explanations further allows us to obtain confidence information of feature contributions. In the left plot of Figure 10, we plot the feature contributions of two randomly drawn offenders from each target value, 'reoffended' (y = 1) or 'not' (y = 0). Among the features, juv_other_count (which represents the number of non-felony juvenile offenses a person has been convicted of) exhibits high variance in its contributions. This high variance indicates that, with a single NAM, the contribution of juv_other_count can appear either extremely negative or positive, potentially leading to misinterpretation. BayesNAM reveals substantial variability among models, suggesting that juv_other_count can have both positive and negative effects on predictions within certain ranges.\nWhat can we infer from this high variation? In the right plot of Figure 10, we analyze the mapping function of juv_other_count. NAMs (gray) show inconsistent explanations for juv_other_count. The two-sigma interval of mapping functions of BayesNAM (orange) also starts to diverge significantly when juv_other_count \u2265 4, indicating increased inconsistency in this range. As shown in Figure 11a, a data range where juv_other_count > 4 indicates a lack of sufficient data. Moreover, this range also shows skewed proportions, especially for juv_other_count \u2265 9, where all labels are either 'reoffended' or 'not.' In summary, we verify that the high inconsistency highlights the need for caution when interpreting examples involving the feature, and suggests potential issues such as a lack of data.\nIn addition to identifying data insufficiencies, our model can also be used for feature selection. Features with high absolute contributions and small standard deviations, such as priors_count (which represents the total number of prior offenses a person has been convicted of), consistently demonstrate significant impact across different models."}, {"title": "4.2 Capturing Structural Limitation", "content": "In addition to data insufficiencies, a high level of inconsistency can reveal structural limitations within the model.\nIn Fig. 12a, we plot the results of NAMs and BayesNAM for longitude. These functions show higher housing prices in San Francisco (around -122.5) and Los Angeles (around -118.5), consistent with previous findings [1]. However, BayesNAM finds that there exist inconsistent explanations between these two cities, particularly within the longitude range of -120 to -119.\nWe hypothesize that this inconsistency is due to significant variations in housing prices (target variable) across different latitudes. Fig. 12b illustrates the distribution of housing prices in California, with red circles indicating higher prices and larger circles representing higher volumes of houses. As shown in Fig. 12b, while Santa Barbara, Yosemite National Park, and Fresno are on similar longitudes, Santa Barbara exhibits a substantial price gap compared to the others. Additionally, since Yosemite National Park and Fresno are near the same latitude as San Francisco, NAM might struggle to accurately predict housing prices without the interaction term between Latitude and Longitude.\nBased on this observation, we train a NAM with an interaction term between Latitude and Longitude. This model achieved a much better performance (RMSE: 0.506 \u00b1 0.005) than without the interaction term (RMSE: 0.556 \u00b1 0.009). In addition, The significance of the interaction term is particularly evident near Yosemite. When"}, {"title": "5 CONCLUSION", "content": "In this work, we identified and analyzed the inconsistent explanations of NAMs. We highlighted the importance of acknowledging these inconsistencies and introduced a new framework, BayesNAM, which leverages inconsistency to provide more reliable explanations. Through empirical validation, we demonstrated that BayesNAM effectively explores diverse explanations and provides external explanations such as insufficient data or model limitations within the data model. We hope our research contributes to the development of trustworthy models."}, {"title": "Training Details", "content": "In Section 4, we conducted experiments on five different datasets: Credit Fraud [19], FICO [20], COMPAS [21], California Housing (CA Housing) [22], and Boston [23]. Here, we provide a summary of the characteristics in Table 2 and detailed explanations to ease the understanding of the experimental interpretation.\nCredit Fraud: This dataset focuses on predicting fraudulent credit card transactions and is highly imbalanced. Due to confidentiality concerns, the features are represented as principal components obtained through PCA.\nFICO: This dataset aims to predict the risk performance of consumers, categorizing them as either \u201cBad\u201d or \u201cGood\u201d based on their credit.\nCOMPAS: This dataset aims to predict recidivism, determining whether an individual will reoffend or not."}]}