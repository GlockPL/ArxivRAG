{"title": "MuseCL: Predicting Urban Socioeconomic Indicators via Multi-Semantic Contrastive Learning", "authors": ["Xixian Yong", "Xiao Zhou"], "abstract": "Predicting socioeconomic indicators within urban regions is crucial for fostering inclusivity, resilience, and sustainability in cities and human settlements. While pioneering studies have attempted to leverage multi-modal data for socioeconomic prediction, jointly exploring their underlying semantics remains a significant challenge. To address the gap, this paper introduces a Multi-Semantic Contrastive Learning (MuseCL) framework for fine-grained urban region profiling and socioeconomic prediction. Within this framework, we initiate the process by constructing contrastive sample pairs for street view and remote sensing images, capitalizing on the similarities in human mobility and Point of Interest (POI) distribution to derive semantic features from the visual modality. Additionally, we extract semantic insights from POI texts embedded within these regions, employing a pre-trained text encoder. To merge the acquired visual and textual features, we devise an innovative cross-modality-based attentional fusion module, which leverages a contrastive mechanism for integration. Experimental results across multiple cities and indicators consistently highlight the superiority of MuseCL, demonstrating an average improvement of 10% in R\u00b2 compared to various competitive baseline models.", "sections": [{"title": "1 Introduction", "content": "Urbanization is intricately connected to critical facets of the United Nations Sustainable Development Goals (UNSDGs), affecting energy, environment, economy, climate, etc. By 2020, over 55% of the global population resided in urban areas, and this trend is projected to persist and intensify in the forthcoming decades. Embracing urbanization yields numerous advantages, including a thriving cultural milieu, enhanced job prospects, and improved transportation networks, etc. However, it also begets a host of predicaments and hurdles, such as air pollution, traffic congestion, and escalated energy consumption . To address these challenges and achieve SDGs, gaining a comprehensive understanding of the urbanization phenomenon through fine-grained region profiling and accurate socioeconomic indicators becomes crucial.\nTraditional approaches have relied on community surveys to gather statistics on metrics like population density and household income, which is both resource-intensive and time-consuming . With the maturation of urban perception technology, diverse forms of data continue to proliferate within cities, which paves the way for fresh opportunities in tracking urban sustainable development indicators. As depicted in Figure 1, these datasets encompass point of interest (POI) information, vehicle movement trajectories, remote sensing data, street view imagery, and social media insights, among others. The utilization of this diverse data pool offers robust support for a myriad of downstream tasks. For instance, social media data is instrumental in predicting crime and unemployment rates [Antenucci et al., 2014; Aghababaei and Makrehchi, 2016], urban lifestyle mining [Zhou et al., 2018], and significantly contributes to studies on urban sustainability . Trajectory data reveals valuable insights into mobility patterns, socioeconomic indicators, and health trends . POI data aids in discovering new venues to explore , deducing regional functions [Yuan et al., 2012], and controlling light pollution . Recent research also delves into the potential of urban imagery, employing remotely sensed images for poverty prediction, land cover classification , and analyzing street view images to estimate pedestrian volume .\nHowever, utilizing unimodal urban data often yields suboptimal results, prompting a growing inclination towards the integration of multi-modal data. For instance, the simultaneous utilization of streetscape and remote sensing imagery has proven effective in predicting socioeconomic indicators . The combination of urban imagery with POI data has demonstrated its utility in enhancing region representation . Furthermore, researchers have ventured into the realm of multi-view graphs, leveraging data from diverse sources to comprehensively characterize regions . This shift to multi-modal approaches holds great promise for advancing urban data analysis and interpretation, and helps to better achieve sustainable development goals. Recent efforts aim to derive latent embeddings for individual regions and employ them in conjunction with regional characteristics to predict a range of socioeconomic indicators, showcasing their notable versatility.\nWhile prior studies have undertaken region profiling and socioeconomic prediction, several challenges persist. Among these, three primary ones emerge: (1) Rapid societal development has reshaped information exchange among regions, prompting a reassessment of the applicability of Tobler's First Law of Geography . Consequently, a more precise method is warranted to assess region similarity. (2) Urban representation predominantly focuses on geography and human activity, necessitating effective modal filtering to meet region representation demands amidst the abundance of urban data. (3) Achieving effective fusion of diverse modal data is crucial yet complex in developing the final region representation, necessitating the advancement of robust multi-modal fusion techniques.\nTo tackle these challenges, we present a Multi-Semantic Contrastive Learning (MuseCL) framework. The primary contributions of our work can be summarized as follows:\n\u2022 We pioneer the joint representation of regions using both street view and remote sensing imagery, concurrently integrating POI and mobility flow data to enrich the embedding with multi-dimensional semantic information.\n\u2022 We enhance the spatial contrastive learning process by factoring in the similarity between regional POI and population mobility, resulting in more effective contrastive learning outcomes.\n\u2022 We devise a cross-modal fusion model that aligns imagery with textual representation outputs, seamlessly integrating textual semantics into imagery representations.\n\u2022 We validate the effectiveness of our framework through experiments on socioeconomic indicators in three major metropolises. The results demonstrate the superior performance of our model compared to various competitive state-of-the-art baselines across multiple downstream prediction tasks."}, {"title": "2 Related Work", "content": "Urban Representation Learning. With the increasing availability of urban data, representation learning in urban areas has witnessed significant growth in recent years. Numerous studies have capitalized on the proximity of similar regions in the embedding space to address various downstream tasks, such as crime prediction , land cover classification , and socioeconomic feature prediction , among others. In this context, various strategies have emerged for urban region representation. For instance, Feng et al. [2017] proposed a latent representation model POI2Vec to jointly model the user preference and POI sequential transition influence for predicting potential visitors for a given POI. Wang and Li [2017] introduced a method incorporating temporal dynamics and multi-hop transitions. Zhang et al. [2017] presented a novel cross-modal representation learning method, CrossMap, which uncovers urban dynamics with massive geo-tagged social media data. Yao et al. [2018] proposed a framework to learn the vector representation of city zones by leveraging large-scale taxi trajectories. In a similar vein, Fu et al. [2019] explored multi-view spatial networks, considering geographical distance view and human mobility connectivity view for POIs within each region. Additionally, Wang et al. [2020] devised a multi-modal and multi-stage framework integrating image and text data within the neighborhood. These diverse approaches offer unique perspectives and valuable insights for further research in the urban representation learning field.\nSocioeconomic Indicators Prediction. Initially, researchers primarily employed supervised and unsupervised learning methods for predicting socioeconomic indicators. For instance, Chakraborty et al. [2016] proposed a generative model of real-world events to predict various socioeconomic indicators based on extracted events. Qu et al. [2017] introduced a multi-view representation learning approach that fostered collaboration among different views to generate robust representations, subsequently used for socioeconomic indicator prediction. Similarly, He et al. [2018] unveiled correlations between visual patterns in satellite images and commercial hotspots. In recent years, self-supervised learning methods, especially contrastive learning, have gained traction for socioeconomic indicator forecasting. Drawing inspiration from Tobler's First Law of Geography [Miller, 2004], Jean et al. [2019] employed distance to establish neighborhood similarities in loss functions. Furthermore, Xi et al. [2022] incorporated POI similarity into contrastive learning to overcome distance-based limitations."}, {"title": "3 Preliminaries & Problem Statement", "content": "An urban area typically comprises multiple regions denoted as $R = {r_1, r_2, \\dots, r_n}$. These regions exhibit unique geographic and demographic characteristics, often reflected through various data sources within them. In our study, we focus on analyzing specific attributes of regions $r_i \\in R (i = 1, 2, \\dots, N)$, investigating the following aspects:\n\u2022 Remote Sensing Imagery $RV_i$. Remote sensing imagery captures ground surface details, effectively revealing building distribution and thus providing valuable support for region representation.\n\u2022 Street View Imagery $SV_i = {s_{i1}, s_{i2}, \\dots, s_{i|SV|}}.$ It offers valuable insights into the appearance of streets, buildings, and their immediate surroundings. A region often contains multiple street view images.\n\u2022 POI Data $T_i = {T_{i1}, T_{i2}, \\dots, T_{i|T_i|}}$. We textualize each POI as a bag of words ${t_1, t_2, \\dots, t_n}$, where each word is obtained from the POI's categories, ratings, reviews, and other relevant information.\n\u2022 Population Mobility $M_i = {m_{in}^i, m_{out}^i}$. $m_{in}^i$ and $m_{out}^i$ refer to the number of people entering and exiting the region $r_i$ over a period of time, respectively. It can reflect the socio-demographic activity of a region.\nGiven a collection of urban remote sensing images $RV$, street view images $SV$, POI data $T$, and population mobility data $M$, our primary objective is to derive a low-dimensional representation $e_i \\in R^d$ for each region $r_i \\in R (i = 1, 2, \\dots, N)$, where d signifies the dimension of the representation vectors. By effectively encapsulating the diverse characteristics inherent in each region, our approach aims to generate compact yet informative representations, denoted as $E = {e_1, e_2, \\dots, e_N}$, to enhance various downstream socioeconomic prediction tasks in urban settings."}, {"title": "4 Methodology", "content": "4.1 Framework Overview\nFigure 2 illustrates our proposed framework for fine-grained urban region profiling to predict socioeconomic indicators. This multi-step contrastive learning model consists of three key components: extracting semantic features from the visual modality, incorporating textual semantic information, and performing downstream tasks.\nTo begin, we partition the visual semantic learning module into remote sensing imagery representations based on POI similarity and street view imagery representations based on population flow similarity. Contrastive learning sample pairs are curated to acquire imagery features with distinct focal points. Subsequently, we take into account the POI text information associated with each region and leverage a pre-trained encoder-based model to derive the text features for every region. Then, employing a feature-level attentive fusion module, we align the combined remote sensing and street view features with the text representation vectors of each region, thereby imbuing the fused features with both visual and textual semantic insights. Lastly, we evaluate the low-dimensional representations of each region across a range of downstream tasks critical for urban sustainable development.\n4.2 Visual Semantic Extraction\nStreet view and remote sensing imagery often contain information with different emphases. For example, street view imagery can provide characteristics of the social environment and population activity, while remote sensing imagery is more oriented towards geographic attributes and surface features . Therefore, we need to get the embedding of both separately and combine them effectively."}, {"title": "Constructing Contrastive Samples", "content": "Recently, Xi et al. [2022] highlighted the limitations of Tobler's First Law of Geography [Miller, 2004], noting that relying solely on spatial distance to measure regional similarity is flawed. To address this, we propose a refined approach by constructing contrastive learning pairs for street and remote sensing images based on population flow and POI similarity, respectively. Street images are paired with mobility data because they reflect human movement patterns, while remote sensing images are paired with POI data because they capture the built environment and land use.\nPopulation flow within a region can be gauged by quantifying the influx and efflux of individuals or vehicles over a specified timeframe. If we conceptualize regions as nodes and the movement of individuals or vehicles between regions as edges, the mobility of each region can be captured by tallying the entries and exits at each node. Assuming that the inflow of population to region $r_i$ during a given period is $m_{in}^i$, and the outflow is $m_{out}^i$, the population mobility distance between regions $r_i$ and $r_j$ is computed by:\n$dist_{i,j}^{PM} = \\sum_{v\\in{in,out}} (m_v^i - m_v^j)^2$ (1)\nThen, the similarity of population mobility between the two regions can be quantified as:\n$A_{i,j}^{PM} = \\frac{1}{dist_{i,j}^{PM}}$ (2)\nWe can construct positive samples characterized by higher similarity and negative samples characterized by lower similarity for each street view imagery, based on the parameter $A_{i,j}^{PM}$. As for remote sensing imagery, assuming that K different POI types are considered, we employ the Euclidean distance to quantify the POI distance between region $r_i$ and $r_j$ as follows:\n$dist_{i,j}^{POI} = \\sqrt{\\sum_{k=1}^K (POI_k^i - POI_k^j)^2}$ (3)"}, {"title": "POI / Mobility Triplet Loss", "content": "Using the acquired $A_{i,j}^{PM}$ and $A_{i,j}^{POI}$, we proceed to form separate pairs of contrastive learning samples for street view and remote sensing imagery. For instance, focusing on street view imagery, we establish each anchor image $Anc^{SV}$ along with its corresponding positive sample $Pos^{SV}$ and negative sample $Neg^{SV}$ based on the population flow similarity $A_{i,j}^{PM}$. Subsequently, we train a convolutional neural network (CNN) denoted as $F_{SV}$ to map the constructed contrastive learning samples $C^{SV} = [Anc^{SV}, Pos^{SV}, Neg^{SV}]$ into a low-dimensional vector space: $x^{SV} = F_{SV}(Anc^{SV}), y^{SV} = F_{SV}(Pos^{SV})$, and $z^{SV} = F_{SV}(Neg^{SV})$. Similarly, we derive representation vectors for remote sensing imagery denoted as $x^{RV}, y^{RV}$, and $z^{RV}$, corresponding to the contrastive learning samples $C^{RV} = [Anc^{RV}, Pos^{RV}, Neg^{RV}]$.\nLoss Optimization\nTo ensure the minimization of the distance between the anchor image and the positive image, while maximizing the separation from the negative image in the representation space, we employ Triplet Loss [Schroff et al., 2015] as the loss function. The primary objective of this loss function is to bring features with similar labels into close proximity within the representation space, while simultaneously pushing features with dissimilar labels apart. For each pair of samples, we anticipate the fulfillment of the following equations:\n$sim(x^m, y^m) + \\alpha \\leq sim(x^m, z^m), m \\in {SV, RV}$ (5)\n$Loss(C^m) = [\\alpha + sim(x^m, y^m) - sim(x^m, z^m)]_+$ (6)\nwhere $[\\cdot]_+$ is a rectifier function to keep the loss function value non-negative, and $sim(\\cdot)$ denotes the cosine similarity. The value $\\alpha$ is used to prevent the features of anchor samples $Anc^m$, positive samples $Pos^m$ and negative samples $Neg^m$ from aggregating into a small space. The whole training framework is shown in Figure 3."}, {"title": "4.3 Textual Semantic Incorporation", "content": "POIs hold significance as data points denoting specific landmarks on a map, often signifying distinct geographic locations such as stores, restaurants, parks, and more in cities. The textual descriptions associated with POIs can effectively capture the geographic attributes of a region. For instance, a clustering of coffee shops within a region could indicate a vibrant locale appealing to young residents, whereas an abundance of parks and green spaces might suggest a neighborhood conducive to family-oriented living.\nPOI Textual Semantic Extraction\nIn addition to the imagery features, the textual data associated with POIs plays a crucial role in region profiling. To effectively harness the descriptive potential of POI text for region representation, we employ Gensim in conjunction with Skip-Gram and Huffman Softmax models [Mikolov et al., 2013] for training. The Skip-Gram model, a neural network-based"}, {"title": "Attentive Fusion Module", "content": "We proceed to integrate the street view features $e^{SV}$, remote sensing features $e^{RV}$, and POI features $e^{POI}$, creating a comprehensive final representation tailored for utilization in various downstream tasks.\nFirstly, with the inherent importance of both imagery features $e^{SV}$ and $e^{RV}$ unknown, we propose the incorporation of an attentive fusion module to derive weights for each of these representations. Considering street view features $e^{SV}$ and remote sensing features $e^{RV}$ from region $r_i$, we introduce learnable parameters $c$, $M$, and $b$ to facilitate their fusion:\n$\\alpha_m = c \\cdot ReLU(M \\cdot e^m + b), m \\in {SV, RV}$ (8)\n$\\beta_m = \\frac{exp(\\alpha_m)}{\\sum_{m \\in {SV,RV}} exp(\\alpha_m)}$ (9)\n$e^{Image} = \\sum_{m \\in {SV,RV}} \\beta_m e^m$ (10)\nwhere $e^{Image}$ is the final representation for region's imagery feature and $\\beta_m$ ($m \\in {SV, RV}$) are weight coefficients.\nNext, in order to incorporate the textual semantic information of POIs, we refer to InfoNCE loss [Oord et al., 2018] to align the features of imagery $e^{Image}$ and POI texts $e^{POI}$.\n$Loss_i = -log \\frac{exp(sim(e^{Image}, e^{POI}))}{\\sum_{n=1}^N exp(sim(e^{Image}, e^{POI}))}$ (11)\nwhere n denotes the mini-batch size. By optimizing the aforementioned loss function, we acquire the region imagery features $e^{Image}$ and effectively integrate the semantic information of POIs. Subsequently, these obtained representations for each region can be harnessed to forecast various socioeconomic indicators."}, {"title": "5 Experiments", "content": "5.1 Experimental Setups\nDatasets\nWe compile real-world datasets from three major cities: Beijing (BJ), Shanghai (SH), and New York (NY). The city regions are delineated by hexagonal divisions, with a radius of 1 km for Beijing and Shanghai, and 500 meters for New York (New York is much smaller than Beijing and Shanghai). It should be noted that our model is highly adaptable to various division shapes and scales, including road networks and Census Block Groups (CBGs).\nFor street view imagery, we employ the Baidu Maps API\u00b9 for Beijing and Shanghai, and the Google Maps API\u00b2 for New York. High-resolution (3.6-meter) remote sensing images are acquired through ArcGIS for all three cities. The POI data for Beijing and Shanghai originates from Baidu Maps, while New York's data is sourced from OpenStreetMap\u00b3 (OSM). Socioeconomic indicators, including population density from WorldPop, housing data from Lianjia, and crime data from NYC Open Data, are also integrated.\nBaseline Models\nWe compare our proposed model with various unimodal and multi-modal region representation algorithms, including:\n\u2022 Inception v3 proposed in [Szegedy et al., 2016]. It can extract features using convolutional layers with different kernel sizes, max pooling and batch normalization.\n\u2022 Resnet-18 proposed in [He et al., 2016]. It uses residual blocks to solve the degeneracy problem of deep networks. We use the Resnet-18 pre-trained in ImageNet.\n\u2022 Tile2vec proposed in [Jean et al., 2019]. It is an unsupervised learning method that uses geographic distance as a criterion for constructing contrastive samples.\n\u2022 Urban2vec proposed in [Wang et al., 2020]. It uses both street view images and POI data to characterize neighborhood features.\n\u2022 PG-SimCL proposed in [Xi et al., 2022]. It uses remote sensing imagery for region representation based on the similarity of geographic distances and POI distributions to perform prediction tasks on socioeconomic indicators.\n\u2022 Add-svrv and Fusion-svrv. They represent the summation or attentional fusion of SV and RV embedding.\n\u2022 Concat. We simply concatenate the SV, RV and POI representation results as a variant of our method."}, {"title": "Ablation Study", "content": "We conduct ablation experiments using three datasets each from Beijing and New York City. The results, as depicted in Figure 6, indicate that the absence of certain modalities leads to a reduction in the final prediction R2 value. Notably, relying solely on POI, street view, or remote sensing images yields suboptimal outcomes. When combining street view and remote sensing images without POI information, the performance still falls short of our model's performance, although it fares better than utilizing street view or remote sensing images individually. This reinforces the notion that various modalities contribute distinct insights for predicting downstream tasks and urban region profiling."}, {"title": "6 Conclusion", "content": "This paper presents a novel Multi-Semantic Contrastive Learning (MuseCL) framework that skillfully amalgamates semantic insights from visual and textual information to generate embeddings for urban regions. We showcase our model's superiority in socioeconomic indicators prediction across diverse cities and through extended experiments. While our focus is on statically depicting urban regions, it is important to acknowledge their rapid evolution due to development. Therefore, incorporating time into region representation presents an interesting path for future research."}]}