{"title": "YOLO11-JDE: Fast and Accurate Multi-Object Tracking with Self-Supervised Re-ID", "authors": ["I\u00f1aki Erregue", "Kamal Nasrollahi", "Sergio Escalera"], "abstract": "We introduce YOLO11-JDE, a fast and accurate multi-object tracking (MOT) solution that combines real-time object detection with self-supervised Re-Identification (Re-ID). By incorporating a dedicated Re-ID branch into YOLO11s, our model performs Joint Detection and Embedding (JDE), generating appearance features for each detection. The Re-ID branch is trained in a fully self-supervised setting while simultaneously training for detection, eliminating the need for costly identity-labeled datasets. The triplet loss, with hard positive and semi-hard negative mining strategies, is used for learning discriminative embeddings. Data association is enhanced with a custom tracking implementation that successfully integrates motion, appearance, and location cues. YOLO11-JDE achieves competitive results on MOT17 and MOT20 benchmarks, surpassing existing JDE methods in terms of FPS and using up to ten times fewer parameters. Thus, making our method a highly attractive solution for real-world applications. The code is publicly available at https://github.com/inakierregueab/YOLO11-JDE.", "sections": [{"title": "1. Introduction", "content": "Multi-Object Tracking (MOT) is a fundamental task in computer vision that involves detecting multiple objects in a video sequence and maintaining their identities discriminated across frames. From autonomous driving [8, 24, 38] and video surveillance [2,51] to sports analytics [12,29,53] and robotics [42, 58], MOT is a key component for numerous real-world applications. Despite significant advancements in the field, factors such as frequent occlusions, complex and unpredictable motion patterns, and the need for real-time performance in practical scenarios remain challenging [3, 11].\nAmong the different paradigms for MOT, the Tracking-by-Detection (TbD) approach has become the most widely used due to its modularity and flexibility, separating the task into two stages: detecting objects in each frame and associating these detections across consecutive frames to maintain identities. Many methods integrate Re-Identification (Re-ID) embeddings to ease the matching process. These appearance cues are particularly valuable in challenging scenarios involving occlusions or objects with similar motion patterns, as they provide an additional layer of discrimination beyond spatial and temporal information.\nWhile substantial progress has been made in both detection and Re-ID fields, most methods adopt a two-stage approach, known as Separate Detection and Embedding (SDE), where detection and Re-ID are performed independently [1, 28, 50, 56]. While effective, these methods suffer from scalability issues due to the lack of feature sharing and the computational cost of applying the Re-ID model to every bounding box. To address these limitations, recent advancements introduced Joint Detection and Embedding (JDE) models, which unify object detection and Re-ID feature extraction processes into a single model [18,32-34,45,57,63,67,69]. By sharing features between the two tasks and jointly optimizing them, JDE models significantly reduce computational overhead, making it an attractive paradigm for MOT."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Tracking-by-Detection", "content": "The task of MOT can be broadly categorized into three main paradigms based on how detection and tracking tasks are combined: tracking-by-regression, tracking-by-detection and tracking-by-attention. Nonetheless, TbD stands out as the most practical and widely used approach in both research and real-world applications. These trackers divide MOT into two separate tasks: detection and association. The tracking process begins with the identification of potential objects of interest in each frame using high-performance detectors like YOLOX [20], Faster R-CNN [44], or CenterNet [71]. Detected objects are then associated across consecutive frames using tracker algorithms that perform data association employing several cues (motion, location, appearance, etc.).\nSince the candidate boxes can be directly provided by off-the-shelf detectors, TbD methods mainly focus on improving the association performance. Early methods like SORT [5] employ a Kalman filter [30] to predict object positions in subsequent frames, assuming linear motion dynamics. Data association is performed using the Hungarian algorithm [31], with a cost matrix based on the Intersection-over-Union (IoU) between predicted and detected bounding boxes. More recent advancements, like ByteTrack [68], utilize all outputted detections, including low-confidence ones,"}, {"title": "2.2. Re-Identification", "content": "To better deal with occlusions, crowded scenes, and non-linear motion, appearance similarity is commonly used in addition to IoU and motion cues. Thus, modern systems, like DeepSORT [59], BoT-SORT [1], SMILETrack [56] and many others [28, 50, 64], incorporate the extraction of discriminative Re-ID features for detected objects. These embeddings can be obtained either using an external high-quality feature extractor (e.g. FastReID [22]), or using JDE models (see Figs. 2a and 2b). Despite achieving superior performance, SDE approaches suffer from massive computation costs since the feature extractor network needs to perform forward inference on the image or feature map crop of each bounding box, thus limiting real-time applications."}, {"title": "2.3. Joint Detection and Embedding", "content": "JDE models perform object detection and Re-ID feature extraction in a single network in order to reduce inference time. Focusing on one-shot detectors, Wang et al. [57] redesign the coupled prediction head of YOLOv3 [43] to extract embeddings of dimension 512 directly applying a 1 x 1 convolution layer on the shared features. Thus, ignoring the inherent differences between the three tasks involved. Moreover, [57] trains the Re-ID task using a classification approach, where extracted embeddings are fed into a shared fully-connected layer to output the class-wise logits, and then cross-entropy loss is applied. In this method, annotations without identity labels are ignored. CSTrack [34] adopts YOLOv5 [25] as detector and introduces two new modules to decouple the Re-ID task and fuse embeddings across scales. Subsequent advancements, such as OMC [33] and TCBTrack [67], emphasize the temporal refinement of appearance cues.\nOn the other hand, FairMOT [69] uses a modified version of the anchor-free detector CenterNet to output 128-dimensional features alongside each detection. Similarly to previously mentioned approaches, FairMOT learns Re-ID features through a classification task. In addition to the standard training strategy, FairMOT introduces a single-image training approach tailored for image-level object detection datasets. Each bounding box is assigned a unique identity, effectively treating every object instance in the dataset as a distinct class. By applying various transformations to the entire image, the model is exposed to each identity across multiple conditions. Despite reporting acceptable results, this self-supervised approach is used as a pre-training routine and not explored any deeper. QDTrack [18] further investigates the self-supervised paradigm incorporating MixUp [65] and Mosaic transformations, along with an extension of the InfoNCE loss [52] paired with a regularization term. Meanwhile, other models based on CenterNet, like RelationTrack [63] and SimpleTrack [32] focus on decoupling both tasks and improving data association.\nMore recent JDE methods, CountingMOT [45] and UTM [62], achieve state-of-the-art performance on MOTChallenge benchmarks. The former, build upon FairMOT, adds an extra counting task to be shared across detection and density estimation branches, boosting the performance in crowded scenes. The latter, includes the data association step into a unified tracker model, creating a positive feedback loop boosting detection and Re-ID altogether.\nDespite being designed for autonomous driving scenarios, RetinaTrack [38] is also noteworthy. Designed on top of RetinaNet [35], it performs the task of JDE using the triplet loss and mining hard triplets."}, {"title": "3. YOLO11-JDE", "content": "In this section we detail the technical aspects of YOLO11-JDE including its modified architecture, the different strategies employed for effectively training the Re-ID branch in a self-supervised fashion, and the integration of Re-ID embeddings into the online data association process."}, {"title": "3.1. Architecture", "content": "Following related JDE approaches like [18,34,57,62,67], our framework is based on the YOLO family of detectors, which typically consist of a backbone for generating feature maps, a neck that refines them by fusing shallow and deep representations, and three prediction heads Particularly, the state-of-the-art version YOLO11s has been chosen for its efficiency, accuracy and real-time performance. We have incorporated a Re-ID branch in the original multi-task decoupled head, taking inspiration from the design of the bounding box and segmentation regression branches. The Re-ID branch processes input feature maps through two consecutive 3x3 convolutional layers, each followed by batch normalization and the SiLU activation function. A third 1x1 convolutional layer maps the features into the corresponding embedding dimension with no batch normalization applied, following best practices suggested in [22]. This simple yet effective design allows the Re-ID branch to learn discriminative features without introducing unnecessary complexity and assessing the task in a consistent manner with the other object detection tasks: classification and bounding box regression. Thus, YOLO11-JDE outputs an appearance embedding for each detection alongside its predicted class and bounding box."}, {"title": "3.2. Self-Supervised Training Strategy", "content": "The goal of the Re-ID branch is to produce robust embeddings that facilitate data association between consecutive frames, while minimizing the reliance on large-scale labelled tracking datasets. To achieve this, we aim for a fully self-supervised training approach, inspired by the work of FairMOT and QDTrack.\nA core aspect of our self-supervised strategy is the use of Mosaic data augmentation [6], a technique commonly employed in training modern object detectors like YOLO11. Mosaic augmentation works by combining four different image patches into a single input image, effectively enabling the model to review the same identities under diverse transformations, including variations in color, scale, rotation, etc. As depicted in Fig. 3, this approach allows the JDE model to learn robust features by exposing them to multiple augmented versions of the same identity within the same input image and/or batch while ordinarily training for detection. Thus, learning to output appearance features for each detection almost for free.\nWhile our approach is intended to be fully self-supervised, it is also compatible with semi-supervised training, where a small amount of labelled tracking/identity data can complement the training procedure. This flexibility ensures the framework can adapt to scenarios with varying levels of data availability, which is crucial in real-world application."}, {"title": "3.3. Re-ID Loss", "content": "For a given training batch, the model outputs N foreground predictions, each with an associated embedding that has a ground truth identity label assigned. The objective of the loss function is to pull embeddings with the same identity (positives) close together in the feature space while pushing those of different identities (negatives) further apart. This learning paradigm is a central concept in deep metric learning, where the goal is to learn a feature space where distances directly encode meaningful relationships between data points. The Re-ID task can be approached either as a classification problem or directly optimizing pair-wise relative distances between embeddings [7].\nInspired by common Re-ID models trained in an contrastive fashion [10, 19, 21, 22, 37], we adopt a pair-wise approach given its scalability to a large numbers of identities. While advanced pair-wise losses like Multi-Similarity [55], InfoNCE [52], or Angular [54] offer enhanced performance in certain tasks, we chose the triplet loss due to its simplicity, efficiency, and proven effectiveness [23]. The triplet loss aims to enforce a margin m between positive and negative samples by ensuring that an anchor (a sample from a given identity) is closer to its positive counterpart than to a negative sample. The loss function is defined as:\n$L_{triplet} = \\sum_{\\{a,p,n\\}} [d_{ap} \u2013 d_{an} + m]_+$, (1)\nwhere {a, p, n} represents the set of all triplets to be evaluated; and dap and dan represent the distances between the anchor and the positive and negative samples respectively.\nGiven N embeddings, the total number of triplets scales as O(N\u00b3). This rapid growth makes it computationally unfeasible to use all possible combinations directly. Moreover, many of the resulting triplets would offer little new information to the model, slowing down convergence. To address these issues, recent advancements in pair-based metric learning have focused on more informative sampling strategies. In our setup, we use hard positive and semi-hard negative sampling strategy to obtain a total of N triplets, although other strategies are also explored, Sec. 4.3.1. On the one hand, hard positive mining chooses as positive for each anchor its furthest embedding with the same identity (most dissimilar). On the other hand, semi-hard negative mining selects the hardest negative sample per anchor (most similar embedding with different identity), such that it is further than the selected positive. Thus, selecting negatives pairs that are not too easy (far apart) but also not too difficult (close together). By utilizing these sampling strategies, we ensure that each triplet is informative and challenging, accelerating convergence, improving the overall performance and mitigating the issues of computational infeasibility."}, {"title": "3.4. Data Association", "content": "Initially, we adopted the two-stage online data association strategy used in FairMOT. Tracklets are initialized from detections in the first frame and updated in subsequent frames using a combination of motion and appearance cues. In the first stage, a Kalman Filter predicts tracklet locations and the Mahalanobis distance is computed between predicted and detected boxes. Normalized Re-ID embeddings are use to compute a cosine distance matrix, which is fused with the Mahalanobis distance to obtain the final cost matrix. The matches are determined using the Hungarian algorithm. In the second stage, unmatched tracklets and detections are linked based on bounding box IoU with a stricter matching threshold. Unmatched detections can initialize new tracks, while unmatched tracklets persist for 30 frames to handle occlusions. Following [16], appearance features are updated using an exponential moving average.\nBuilding on FairMOT's tracker and inspired by Byte-Track, we have implemented a simple yet effective custom tracker for the YOLO11-JDE model. In the first stage, confident predictions are matched using a combination of motion, appearance, and localization cues. Motion is fused with appearance, following the approach of FairMOT, while also discarding matches with low IoU overlap. The IoU distance matrix is then combined with the confidence scores of detections, and low-similarity matches are discarded. The final cost matrix is a linear combination of these two factors. For low-confidence predictions and unmatched detections, linkage is performed using IoU alone. This approach balances computational simplicity with robust tracking performance."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset and Metrics", "content": "Seven datasets are commonly used when training JDE models on pedestrian tracking. Detection datasets include CrowdHuman [49], ETH [17] and CityPersons [66], while MOT17 [13], CalTech [15], CUHK-SYSU [60] and PRW [70] also provide identity annotations. In our study we will only explore the mentioned object detection datasets with the exception of MOT17, which is added to fine-tune the model for the final evaluation. Following previous work [50,57], we construct a MOT17 validation set using the second half of each training sequence and chop off the videos in ETH that are overlapped with the MOT16 [40] benchmark.\nWe evaluate our approach on the testing sets of two widely recognized benchmarks, MOT17 and MOT20 [14]. For overall tracking accuracy, we primarily rely on HOTA [39] due to its balanced evaluation of detection, association and trajectory quality. However, we also consider IDF1 [46] and MOTA from CLEAR metrics [4] to provide additional insights into identity preservation and overall tracking performance. Detection performance is assessed using Average Precision (AP) with the common 50:95 acceptance IoU threshold range. While the quality of the Re-ID embeddings and the training convergence are monitored using clustering metrics like the Silhouette score [47], retrieval mean Average Precision, and simpler indicators like the mean positive and negative Euclidean and cosine distances."}, {"title": "4.2. Implementation Details", "content": "Our framework builds on the Ultralytics infrastructure, modified to handle the task of JDE by incorporating identity labels management, a new JDE head, metrics for monitoring joint optimization, and a new set of tracking algorithms. Additionally, JDE loss functions and mining strategies are implemented using the PyTorch Metric Learning library [41]. Identity annotations are processed from existing datasets or generated synthetically if not available. They are preserved during data augmentation and foreground prediction alignment. All experiments use the YOLO11s model with COCO [36] pre-trained weights. The default configuration of hyperparameters for optimization and data augmentation is used, except for Mosaic, which is applied throughout the whole training."}, {"title": "4.3. Ablative Studies", "content": "In this section we present rigorous studies of four critical factors in YOLO11-JDE, including the Re-ID loss, the dimensionality of the appearance features, and the amount of training data and supervision needed. A simplified experimental setup has been employed to isolate and analyze the impact of these factors while maintaining computational feasibility. Specifically, we adopt the small variant of YOLO11 as the baseline model, trained for 30 epochs with a batch size of 32. The Re-ID branch employs the triplet loss with a unitary weight and outputs 128-dimensional embeddings. Training data is limited to CrowdHuman [49] and the detections from the training half of MOT17, all resized to 640 pixels. For validation, detection performance is evaluated on the validation splits of both datasets, while the Re-ID performance is exclusively assessed using the ground truth identity labels from MOT17. The tracker algorithm from FairMOT, with its default configuration, is used for evaluating the ablations, including an inference resolution of 1088\u00d7608 pixels. To ensure a comprehensive evaluation and to account for potential interactions between factors, a sequential approach has been taken, where the best-performing configuration from one ablation is used as the baseline for the next. Evaluation metrics are given in percent. The best results of each ablation are shown in bold."}, {"title": "4.3.1 Re-ID Loss", "content": "Mining Strategy. Ablation experiments begin by selecting the best mining strategy for the triplet loss, which has been used with the default margin m = 0.05. Various mining strategies have been explored, incorporating hard, semihard, and easy pairs for both positives and negatives. The results, summarized in Tab. 1, indicate that hard positives and semi-hard negatives yields the best overall performance both in terms of tracking accuracy and the quality of the Re-ID embeddings. This is likely due to the balanced challenge it presents to the model. Semi-hard negatives, which are not overly difficult to separate, are crucial for refining the decision boundary without introducing training instability. Meanwhile, hard positives force the model to learn robust discriminative features, enhancing intra-class consistency. Easier strategies, specially for negatives, rarely violate the margin condition, causing the model to focus mainly on the detection task, i.e., higher MOTA.\nLoss Margin. The next set of experiments focuses on the impact of the margin value m in the triplet loss function. As shown in Tab. 2, several values were tested around the baseline, with m = 0.075 yielding to the best performance in terms of HOTA, MOTA and IDF1. Two additional experiments were conducted using this margin. First, swapping the distance computation (i.e., using the positive-negative distance instead of the anchor-negative if the latter violates the margin more) downgraded the performance, likely because it weakens the impact of the mining strategy. Second, smoothing the loss function by replacing the Hinge function with the Softplus function led to a noticeable increase in detection performance, although it slightly lags behind in HOTA.\nConfidence Filtering. Following the margin analysis, we investigate the impact of filtering the embeddings used for triplet mining, focusing on a confidence-based selection. The default approach mines among all available embeddings, ensuring maximum coverage but potentially including noisy or low-confidence samples. Therefore, we try limiting the embeddings to the top 75% and 50% most"}, {"title": "4.3.2 Feature Dimension", "content": "In this subsection, we investigate the effect of varying the dimensionality of the embedding features on joint optimization and the final tracking performance. By experimenting with dimensions of 64, 128, and 256, we aim to identify"}, {"title": "4.3.3 Training Datasets", "content": "To assess the effect of the different types of supervision and data used in the training of our JDE model, we conducted another set of experiments. As shown in Tab. 6, the model trained on CrowdHuman alone achieves a strong MOTA score, but its lower HOTA score reflects the necessity of fine-tuning with MOT17. Interestingly, incorporating identity supervision does not lead to improvements in HOTA or IDF1, suggesting that the model effectively learns more discriminative features using a fully self-supervised approach. While adding additional datasets such as ETH and CityPersons enhance detection performance, they do not improve tracking metrics, highlighting that the quality and relevance of fine-tuning data are more critical than data diversity."}, {"title": "4.4. Data Association", "content": "With the most promising configuration identified, we have trained the model for a 100 epochs using a batch size of 64 and an input image resolution of 1280 pixels. We then focus on fine-tuning the hyperparameters involved in the data association step. This section compares the results of using the default FairMOT tracker with its original parameters against a fine-tuned version for the YOLO11-JDE model, using the MOT17 training split. The default tracker, without adaptation, may struggle with mismatched"}, {"title": "4.5. Results on MOTChallenge", "content": "We compare our method to existing literature, focusing on online JDE models targeting real-time performance. During inference, we use the new YOLO11-JDE tracker at an input resolution of 1280 pixels. Results on MOT17 and MOT20 test sets under the private detection protocol are shown in Tab. 8. Despite being the only fully self-supervised method in the comparison, YOLO11-JDE demonstrates competitive performance across benchmarks and significantly outpaces its counterparts in terms of FPS. When it comes to identity switches (IDs), YOLO11-JDE outperforms many of its competitors, demonstrating the discriminative power of the produced embeddings. Therefore, we attribute its lower performance in overall tracking to the limitations of the model's detection capabilities, rather than its re-identification ability. Furthermore, YOLO11-JDE has less than 10M parameters, while top-performing methods like CountingMOT rely on computationally expensive detectors such as YOLOX-X (100M parameters) or CenterNet (22M parameters).\nInterestingly, YOLO11-JDE performs better in MOT20 than in MOT17 when compared with its competitors. It is important to note that neither the YOLO11-JDE model nor the tracker have been trained using the MOT20 dataset. This improved performance on crowded scenes can be attributed to the type of data used in training. The CrowdHuman dataset, which has a density of nearly 23 people per image, is magnified by Mosaic data augmentation, turning to approximately 90 people per image. This data composition makes YOLO11-JDE highly robust when handling crowded scenarios and partial occlusions."}, {"title": "5. Summary and Future Work", "content": "In this work, we presented YOLO11-JDE, a lightweight and efficient MOT framework built upon YOLO11s, equipped with a Re-ID branch for joint detection and embedding. Our method demonstrates that Re-ID can be effectively trained in a fully self-supervised manner, avoiding the need for identity-labeled datasets while maintaining competitive tracking performance. By combining the triplet loss with hard positive and semi-hard negative mining strategies, YOLO11-JDE produces discriminative embeddings that are robust across various tracking scenarios, particularly crowded environments. Additionally, we developed a custom tracking algorithm that integrates motion, appearance, and location cues, effectively improving data association and aligning seamlessly with YOLO11-JDE's outputs. Evaluations on the MOT17 and MOT20 benchmarks highlight the method's ability to deliver comparable accuracy to state-of-the-art models while achieving superior FPS and using significantly fewer parameters. These qualities make YOLO11-JDE a practical and scalable solution for real-world applications.\nFor future work, we aim to address the limitations observed in detection performance by refining the architecture to better decouple Re-ID and detection tasks. Further improvements to appearance features, such as incorporating multi-scale embedding fusion, could enhance Re-ID robustness. Additionally, we plan to investigate the impact of stronger data augmentations, including rotations, shear and perspective transformations, Mixup and random patch erasing within bounding boxes."}]}