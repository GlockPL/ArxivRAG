{"title": "Multi-level Matching Network for Multimodal Entity Linking", "authors": ["Zhiwei Hu", "Ru Li", "V\u00edctor Guti\u00e9rrez-Basulto", "Jeff Z. Pan"], "abstract": "Multimodal entity linking (MEL) aims to link ambiguous mentions within multimodal contexts to corresponding entities in a multimodal knowledge base. Most existing approaches to MEL are based on representation learning or vision-and-language pre-training mechanisms for exploring the complementary effect among multiple modalities. However, these methods suffer from two limitations. On the one hand, they overlook the possibility of considering negative samples from the same modality. On the other hand, they lack mechanisms to capture bidirectional cross-modal interaction. To address these issues, we propose a Multi-level Matching network for Multimodal Entity Linking (M\u00b3EL). Specifically, M\u00b3EL is composed of three different modules: (i) a Multimodal Feature Extraction module, which extracts modality-specific representations with a multimodal encoder and introduces an intra-modal contrastive learning sub-module to obtain better discriminative embeddings based on uni-modal differences; (ii) an Intra-modal Matching Network module, which contains two levels of matching granularity: Coarse-grained Global-to-Global and Fine-grained Global-to-Local, to achieve local and global level intra-modal interaction; (iii) a Cross-modal Matching Network module, which applies bidirectional strategies, Textual-to-Visual and Visual-to-Textual matching, to implement bidirectional cross-modal interaction. Extensive experiments conducted on WikiMEL, RichpediaMEL, and WikiDiverse datasets demonstrate the outstanding performance of M\u00b3EL when compared to the state-of-the-art baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "Entity Linking (EL) aims at aligning the mentions within a context to the corresponding entities in a knowledge base [15, 35]."}, {"title": "2 RELATED WORK", "content": "\u25ba Entity Linking. Recent methods for Entity Linking (EL) mainly focus on exploiting ambiguous mentions to the referent unambiguous entities in a given knowledge base, which can be divided into two series: local-level methods and global-level methods. Local-level methods [5, 32, 42] primarily consider mention along with its surrounding words or sentence to capture contextual information. Global-level methods [6, 10, 11, 20, 34, 48] also take entity or topic coherence into account to calculate the mention and entity semantic consistency. However, these methods do not work well when processing multimodal data, including textual and visual content.\n\u25ba Multimodal Entity Linking. Multimodal entity linking is an extension of the traditional entity linking task that utilizes additional multimodal information (e.g., visual information) to support the disambiguation of entities. Mainstream approaches can be classified into two categories: Representation Learning (RL) frameworks and Vision-and-Language Pre-training (VLP) methods. (i): RL-based methods: DZMNED [29] utilizes a multimodal attention mechanism to fuse textual, visual and character features of mentions and entities. JMEL [1] introduces fully connected layers to embed the textual and visual information into an implicit joint space. VELML [50] designs a deep modal-attention neural network to aggregate different modality features and map visual objects to the entities. GHMFC [40] extracts the hierarchical features of textual and visual co-attention through a multi-modal co-attention mechanism. DRIN [43] explicitly encodes four different types of alignments between mentions and entities, and builds graph convolutional network to dynamically select the corresponding alignment relations for different input samples. MMEL [46] proposes a joint feature extraction module to learn textual and visual representations, and a pairwise training schema and multi-mention collaborative ranking mechanism to model the potential connections. (ii): VLP-based models: CLIP [33] trains on large-scale image-caption pairs with contrastive self-supervised objectives to attain textual and visual representations. ViLT [19] discards convolutional visual features and adopts a vision transformer to model long-range dependencies over a sequence of fixed-size non-overlapping image patches. ALBEF [23] introduces a contrastive loss to align the textual and visual representations before fusing them through cross-modal attention to enable more grounded vision and language representation learning. METER [9] systematically investigates how to train"}, {"title": "3 METHOD", "content": "In this section, we first define the task of multimodal entity linking, and then introduce the M\u00b3EL framework, including: Multimodal Feature Extraction, Intra-modal Matching Network, and Cross-modal Matching Network."}, {"title": "3.1 Task Definition", "content": "\u25ba Multimodal Entity Linking. Let $\\&$ be a set of entities in a multimodal knowledge base K. Each entity $E_e \\in \\&$ is of the form $\\{N_e, T_e, V_e\\}$, where $N_e$ denotes the name of the entity, $T_e$ is a textual description of the entity and $V_e$ represents the visual context of the entity associated with its textual description. A mention $E_m$ (and its context) is of the form $\\{N_m, T_m, V_m\\}$, where $N_m, T_m$, and $V_m$ respectively are the name of the mention, the token sequence in which the mention is located, and the corresponding visual image of the mention. The multimodal entity linking (MEL) task aims to retrieve the ground truth entity $E_e \\in \\&$ that is the most relevant to the mention $E_m$. For example, in Figure 1, the mention Black Widow requires to be linked to one of the three candidate entities in the set $\\{Animal\\_Black\\_Widow, Movie\\_Black\\_Widow, Song\\_Black\\_Widow\\}$. After combining the textual description and visual information, we can conclude that the entity Movie_Black_Widow is the most relevant for the mention. Usually, the MEL task can be formulated by maximizing the log-likelihood over the training set D as:\n$e^* = \\max_\\theta \\sum_{(E_m,E_e) \\in D}log \\text{sim}(E_e, E_m, \\theta)$  (1)\nwhere $sim()$ calculates the similarity between the mention $E_m$ and entity $E_e$, and $\\theta$ represents the parameters involved in the optimization process, $e^*$ denotes the final model."}, {"title": "3.2 MFE: Multimodal Feature Extraction", "content": "3.2.1 Multi-modal Embeddings. For an entity $E_e$, we treat its textual description $T_e$ and visual context $V_e$ as a text-image pair $P_e = \\{T_e; V_e\\}$. We similarly obtain the text-image pair representation of a mention: $P_m = \\{T_m; V_m\\}$. As feature extractor in $P_e$ and $P_m$, we utilize a pre-trained CLIP model [33], which trains two neural-network-based encoders using a contrastive loss to match pairs of texts and images. More precisely, take $P_e$ as an example, we obtain textual and visual embedding representations as follows:"}, {"title": "3.2.2 Intra-modal Contrastive Learning", "content": "CLIP learns a joint vision-language embedding space by bringing matching image-text representations together, while pushing unpaired instances away from each other. However, CLIP lacks an explicit mechanism that ensures that similar features from the same modality stay close in the joint embedding [3, 47, 52]. For example in Figure 3, given the entity textual feature $T_e$, entity visual feature $V_e$, mention textual feature $T_m$ and mention visual feature $V_m$, CLIP only considers negative samples of $T_e$ from different modalities (i.e., $V_e$ for $j\\neq i$), ignoring the possibility of having negative samples from the same modality, i.e., $T_e$. and $T_m$. CLIP is able to map image-text pairs"}, {"title": "3.3 IMN: Intra-modal Matching Network", "content": "Through the CLIP encoder, we can obtain global and local features of textual and visual modalities. Most previous works either exploit global features while overlooking the local features, or measure the local feature similarity whereas ignoring the global coherence [42]. MIMIC [28] considers the interaction between global and local features but it uses different independent mechanisms for the textual and visual modalities, making the global and local interaction deeply coupled with the modality. To alleviate the deep coupling between the interaction strategy and the modality type, we introduce the Intra-modal Matching Network (IMN) module to uniformly capture the interaction between local and global features within a modality. IMN contains two sub-modules, Coarse-grained Global-to-Global matching (G2G) and Fine-grained Global-to-Local matching (G2L). Take the textual modality as an example, using the CLIP encoder, we can obtain the entity global textual feature $T_e^G$, entity local textual feature $T_e^L$, mention global textual feature $T_m^G$ and mention local textual feature $T_m^L$. We will use these features as the input to the G2G and G2L sub-modules.\n\u25ba Coarse-grained Global-to-Global matching. To measure global consistency, we directly perform the dot product between the entity global feature $T_e^G$ and the mention global feature $T_m^G$ to obtain the coarse-grained global-to-global matching score, formulated as:\n$M^{G2G} = T_e^G \\odot {T_m^G}^T$\n\u25ba Fine-grained Global-to-Local matching. We introduce an attention mechanism to explore the fine-grained clues among local features to obtain the fine-grained global-to-local matching score, formulated as:\n$Q, K, V = MLP1(T_e^G), MLP2(T_m^L), MLP3(T_m^L)$\n$\\alpha^L = \\text{Mean}(\\text{Softmax}(\\frac{QK^T}{\\sqrt{d_s}} )V)$\n$M^{G2L} = T_e^G \\odot \\alpha^L$\nwhere $\\{MLP1, MLP2, MLP3\\}: \\mathbb{R}^{d_t} \\rightarrow \\mathbb{R}^{d_s}$ are three multi-layer perceptron networks, $d_s$ denotes the scaled dimension size, $\\odot$ is the matrix multiplication operation, $Mean()$ represents the mean pooling operation, $\\alpha^L$ is the attention score generated from local features to impose constraints on global features.\nAfterwards, we average the global-to-global and global-to-local matching scores to obtain the textual intra-modal matching score $M^T = (M^{G2G} + M^{G2L})/2$. Similarly, for the visual modality, we can obtain the global-to-global matching score $M_v^{G2G}$ and global-to-local matching score $M_v^{G2L}$, and the final combined matching score $M^V = (M^{G2G} + M^{G2L})/2$."}, {"title": "3.4 CMN: Cross-modal Matching Network", "content": "Since the embeddings of different modalities are separately matched in the IMN module, it is difficult to model the complex interaction"}, {"title": "3.5 Joint Training", "content": "Consider the textual intra-modal matching score $M^T$ and the visual intra-modal matching score $M^V$ from the IMN module and the cross-modal matching score $M^C$ from the CMN module. We can obtain the union matching score as $M^U = (M^T + M^V + M^C)/3$.\nWe select the Unit-Consistent Objective Function [28] $L_{uco}$ as the basic loss function. Specifically, the entire joint training process loss $L_{joint}$ contains three aspects of loss contents as shown in Equation 9: i) the intra-modal contrastive learning loss $L_{cl}$; ii) the union matching loss $L_U = L_{uco} (M^U)$; iii) to avoid the whole model to excessively rely on the union score, we also introduce the independent intra-model textual loss $L_T = L_{uco} (M^T)$, the intra-modal visual loss $L_V = L_{uco} (M^V)$, and the cross-modal loss $L_C = L_{uco} (M^C)$.\n$L_{Joint} = L_{cl} + L_{U} + L_{T} + L_{V} + L_{C}$ (9)"}, {"title": "4 EXPERIMENTS", "content": "To evaluate the effectiveness of M\u00b3EL, we conduct the following four experiments: including main results, ablation studies, parameter sensitivity (see Appendix A), and additional experiments (see Appendix B)."}, {"title": "4.1 Experimental Setup", "content": "\u25ba Datasets. We evaluate the M\u00b3EL model on three well-known datasets: WikiMEL [40], RichpediaMEL [40], and WikiDiverse [41]. WikiMEL has more than 22K multimodal samples, where entities come from WikiData [38] and their corresponding textual and visual descriptions from WikiPedia. RichpediaMEL has more than 17K multimodal samples, where the entity ids are from the large-scale multimodal knowledge graph Richpedia [39] and the corresponding multimodal information from WikiPedia. WikiDiverse has more than 7k multimodal samples, constructed from WikiNews. For fair comparison, we adopt the dataset division ratio from MIMIC [28]. The statistics of these datasets are shown in Table 1.\n\u25ba Evaluation Metrics. We evaluate the performance using two common metrics: MRR and Hits@N. MRR defines the inverse of the rank for the first correct answer, Hits@N is the proportion of correct answers ranked in the top N, with N={1,3,5}. The higher the values of MRR or Hits@N, the better the performance.\n\u25ba Implementation Details. We conduct all experiments on six 32G Tesla V100 GPUs, and use the AdamW [27] optimizer. Following MIMIC [28], we employ the pre-trained CLIP-Vit-Base-Patch32 model\u00b9 as the initialization textual and visual encoder. For the textual modality, we set the maximal input length of the text to 40 and the dimension of the textual output features $d_t$ to 512. For the visual modality, we rescale all the images into a 224\u00d7224 resolution and set the dimension of the visual output features $d_v$ to 96. The scaled dimension size $d_s$ is set to 96 and the patch size P is set to 32. We use grid search to select the optimal hyperparameters, mainly including: the learning rate $l_r \\in \\{5e-6, 1e-5, 2e-5, 3e-5, 4e-5\\}$, the batch size $b_s \\in \\{48, 64, 80, 96, 112\\}$, the temperature coefficient $\\tau \\in \\{0.03, 0.10, 0.25, 0.5, 0.75\\}$ in Equation 2, the number of heads $K \\in \\{3, 4, 5, 6, 7\\}$, the inner-source alignment weight $\\beta \\in \\{0.2, 0.4, 0.6, 0.8, 1.0\\}$, the inter-source alignment weight $\\gamma \\in \\{0.2, 0.4, 0.6, 0.8, 1.0\\}.\n\u25ba Baselines. We compare M\u00b3EL with three types of baselines. (i) textual-based methods, including BLINK [42], BERT [8] and ROBERTa [25]; (ii) representation learning frameworks, including DZMNED [29], JMEL [1], VELML [50] and GHMFC [40]; (iii) vision-and-language-based methods, including CLIP [33], ViLT [19], ALBEF [23], METER [9] and MIMIC [28]; (iv) generative-based methods, including GPT-3.5-Turbo [30] and GEMEL [36]."}, {"title": "4.2 Main Results", "content": "We conduct experiments on the WikiMEL, RichpediaMEL and WikiDiverse datasets in the normal and low resource settings. The corresponding results are shown in Tables 2 and 3.\n\u25ba Overall Performance Comparison. Table 2 presents results of the performance of M\u00b3EL and the baselines on the WikiMEL, RichpediaMEL and WikiDiverse datasets. We consider two variants"}, {"title": "4.3 Ablation Studies", "content": "We conduct ablation experiments on the WikiMEL, RichpediaMEL and WikiDiverse datasets under different conditions. Table 4 presents results showing the contribution of each component of M\u00b3EL. These include the following three points: a) removing one of $L_U, L_T, L_V, L_C$ or $L_{cl}$ from the loss function $L_{Joint}$; b) removing the textual intra-modal matching module (w/o $L_T + M_T$), the visual intra-modal matching module (w/o $L_V + M_V$), or the cross-modal matching module (w/o $L_C + M_C$) from M\u00b3EL, it should be noted"}]}