{"title": "Hacking CTFs with Plain Agents", "authors": ["Rustem Turtayev", "Artem Petrov", "Dmitrii Volkov", "Denis Volk"], "abstract": "We saturate a high-school-level hacking benchmark with plain LLM agent design. Con-cretely, we obtain 95% performance on InterCode-CTF, a popular offensive security bench-mark, using prompting, tool use, and multiple attempts. This beats prior work by Phuonget al. 2024 (29%) and Abramovich et al. 2024 (72%).\nOur results suggest that current LLMs have surpassed the high school level in offensivecybersecurity. Their hacking capabilities remain underelicited: our ReAct&Plan prompt-ing strategy solves many challenges in 1-2 turns without complex engineering or advancedharnessing.", "sections": [{"title": "Introduction", "content": "Cybersecurity is one of the key AI risk areas (OpenAI 2024b; The White House 2023; UKGovernment 2023): advanced LLMs could hack real-world systems at speeds far exceeding humancapabilities (OpenAI 2024a). To quantify AI cyber capabilities, researchers use benchmarks,with InterCode-CTF (Yang, Prabhakar, Narasimhan, et al. 2023) among the most popular.InterCode-CTF adapts traditional Capture The Flag competitions to assess LLM hacking skills.Previously, Phuong et al. 2024 showed low performance on this benchmark and suggested lowcyber exploitation capabilities. A recent follow-up by Abramovich et al. 2024 claimed state-of-the-art results (72%) due to a particular novel harness design choice."}, {"title": "Related work", "content": "Cybersecurity benchmarks. Prior evaluations of LLMs' cybersecurity capabilities includingInterCode-CTF (Yang, Prabhakar, Narasimhan, et al. 2023), NYU-CTF (Shao et al. 2024),and CyberSecEval 2 (Bhatt et al. 2024) found that LLMs solve less than half of their securitychallenges at release.\nAs LLMs improve on these benchmarks, more challenging datasets like Cybench (Zhang et al.2024) and 3CB (Anurin et al. 2024) are built. Having saturated the InterCode-CTF benchmark,we see evaluating LLMs on these harder challenges as a natural next step.\nCyberSecEval and Naptime. Meta's CyberSecEval 2 (Bhatt et al. 2024) benchmark testedLLMs on prompt injection, vulnerability exploitation, code abuse, and cyberattack scenarios andfound disappointing cyber exploitation capabilities.\nProject Zero 2024's Project Naptime used agent design to boost Meta's scores from 5% to100% in Buffer Overflow and from 24% to 76% in Advanced Memory Corruption. Our workintends to similarly improve upon (Phuong et al. 2024).\nEvaluations on InterCode-CTF. DeepMind's report on model hacking capabilities (Phuonget al. 2024) showed Gemini-1.0 solved 24 out of 812 InterCode-CTF tasks. In the originalInterCode-CTF paper (Yang, Prabhakar, Yao, et al. 2023), GPT-4 solved 40 out of 100 tasks.\nEnIGMA. The EnIGMA paper (Abramovich et al. 2024) pioneered Interactive Agent Tools(IATs) and achieved 72 out of 100 on InterCode-CTF tasks. Our work demonstrates that suchinteractive tools and state-of-the-art harnessing are not necessary for strong performance. OurGPT-40-based agents adapted well to environments without text editors, using basic CLI com-mands for file manipulation."}, {"title": "Experiment Setup and Dataset", "content": "We evaluate LLM agents using CTF challenges virtual environments containing vulnerablesystems that participants must exploit to find hidden flags. We use InterCode-CTF (Yang,Prabhakar, Narasimhan, et al. 2023, Section 5.3), a set of 100 challenges chosen for its standard-ized evaluation framework and prominence in prior research. We fork their harness to addressminor technical issues and exclude some unsolvable tasks (see Appendix B)."}, {"title": "InterCode-CTF benchmark", "content": "InterCode is a benchmark suite testing coding agents in an interactive environment that simulatesreal-world programming. Agents debug and refine their solutions using compiler and command-line tools feedback, similar to human programmers. InterCode-CTF uses the InterCode runtime torun offensive security challenges from picoCTF4, a CTF practice platform of high-school level.The tasks are run in Docker containers. Each task is specified with a natural languageproblem statement and a golden answer."}, {"title": "Model selection", "content": "We used four OpenAI models via API: GPT-4, GPT-40, GPT-40-mini, and o1-preview. Wechose these models for their strong performance on programming tasks. All experiments usedconsistent temperature and generation parameters."}, {"title": "Agent Design Approaches", "content": "We built atop InterCode (Yang, Prabhakar, Narasimhan, et al. 2023, Section 4)'s original agentdesign, where an LLM has access to the task description, history of commands and their outputs,and is asked to determine the next command to issue."}, {"title": "Baseline: 10 steps", "content": "Our baseline called 10 steps follows InterCode's Try Again strategy. Each agent gets:\nUp to 10 actions per task\nAccess to the task description\nHistory of up to 5 action-observation pairs\nObservations truncated to 500 characters\nInitial experiments revealed significant performance variability: agents might solve a task in oneattempt but fail it in another with identical settings. This aligns with findings from Phuonget al. 2024, Section 6.4.2, where agents often know the correct action but don't rank it first, andwith Project Zero 2024's success with @k evaluations.\nTo better assess agent capabilities, we moved from single attempts (@1) to multiple attempts(@k). For each task, agents get up to k attempts, with full environment and agent resetsbetween attempts. We excluded the 01-preview model from multi-attempt evaluations due tocomputational costs."}, {"title": "Base agent design modifications", "content": "Our experiment logs revealed several necessary improvements to the agent design:\nComplete action-observation history. We gave agents access to their full history of actionsand observations. Since about 10% of observations exceeded the model's context window, wecapped them at 3500 characters.\nMore turns. InterCode-CTF's default 10-turn limit proved insufficient, as agents often foundflags but couldn't submit them in time. We increased the limit to 12 turns.\nStructured output. GPT-40's \"Structured output\" feature improved the agent's ability to:\nSubmit flags in correct format\nWrite valid bash commands\nExecute Python code reliably\nExpanded toolset. We enhanced the execution environment by:\nPreinstalling tools and Python packages the model tried to use\nRunning on Kali Linux\nInstalling RsaCtfTools, a tool commonly used in crypto CTFs\nNo interactive tools. Unlike EnIGMA, we prohibited text editors like vim or nano and in-teractive tools to reduce complexity and improve reliability.\nHaving implemented these changes, we proceeded to agent design frameworks."}, {"title": "Plan&Solve", "content": "Our first agent design improvement was a Plan&Solve strategy: the LLM takes a turn to planits approach, then executes the plan to solve the problem (see Figure 2).\nCTF challenges rarely provide clear instructions about relevant files or software, insteadoffering cryptic task statements with general hints. To address this, we begin by running ls atTurn 0 to examine the workspace contents. The agent then creates a plan (P) based on boththe task statement and available files."}, {"title": "ReAct", "content": "We implemented Yao, Zhao, et al. 2023's ReAct (Reasoning + Action) prompting strategy inour next agent design. As Figure 3 shows, at each turn,\n1. The agent generates a thought (T) based on the task description and observations (O\u00bf)\n2. It determines an action (A) based on that thought\n3. The environment (E) executes the action and returns an observation"}, {"title": "ReAct&Plan", "content": "To boost ReAct further, we gave it a Plan step and more turns. A ReAct&Plan agent uses GPT-40for thoughts and actions over 30 turns with a single planning step before turn 12 (Figure 4). Weswitched to 01-preview model for the planning step.\nThis hybrid approach solved 4 more tasks, reaching a 95% success rate. Four tasks remainedunsolved."}, {"title": "Tree of Thoughts", "content": "Tree of Thoughts (ToT) is a framework that improves language models' problem-solving abilitiesby exploring multiple solution paths in parallel (Yao, Yu, et al. 2023). Instead of following asingle line of reasoning, ToT generates and evaluates several intermediate steps, keeping the mostpromising paths. We implemented ToT to tackle our remaining unsolved challenges, hypothesiz-ing that this broader exploration would find solutions that single-path approaches had missed.However, the results did not surpass those achieved with ReAct&Plan."}, {"title": "Results", "content": "We evaluate our agent designs by tasks solved at Figure 6a, and then boost the two best oneswith multiple attempts at Figure 6b."}, {"title": "Ablation study", "content": "We conducted an ablation study on our ReAct&Plan agent to identify which components mostimproved its performance. Starting with our best configuration, we changed one parameter at atime and measured the impact. Our starting configuration included:\nReAct&Plan@5 with 30 turns\nExpanded toolset (see Subsection 4.2)\n5-minute command timeout\n\"Structured Output\" feature enabled\nCustom system instructions\n01-preview for planning\nGPT-40 for action and thought generation"}, {"title": "Discussion", "content": "Our evaluation demonstrates that LLMs can effectively solve high-school-level CTF challenges.The agents successfully explored files and learned from their observations. Though not alwaystaking the most efficient path, they showed flexibility in switching approaches and using toolslike Python when needed. This contradicts previous research (Bhatt et al. 2024; OpenAI et al.2024) suggesting that frontier LLMs cannot do cybersecurity.\nReAct proved to be our most significant improvement. By implementing reflection as de-scribed in (Yao, Zhao, et al. 2023), the agent often solved challenges in just 1-2 turns, showingstrong problem-solving abilities. While Plan&Solve and ToT outperformed the baseline Inter-Code agent, neither matched ReAct's success rate. ToT added little value for these straightfor-ward tasks, and we found that multiple attempts with ReAct worked better. Finally, combiningReAct with planning steps from a more powerful model helped solve several previously unsolvedtasks."}, {"title": "Conclusion", "content": "Our research shows frontier LLMs are better at cybersecurity problems than previously thought.Straightforward prompting and agent design boosts our agents' sucess rate to 95% on InterCode-CTF-dramatically higher than the 29% (Phuong et al. 2024) and 40% (Yang, Prabhakar, Yao,et al. 2023) reported in earlier studies. Our simple ReAct@10 design outperforms EnIGMA'sadvanced harness, which reached 72%.\nThis success suggests that previous studies did not access the full extent of models' capabili-ties. Following Project Zero 2024, we call for stronger evaluations of risky capabilities.\nInterCode-CTF, a high school level hacking framework released in Aug 2023, has now beensaturated. Future AI risk gauging work will need to use harder problem sets like NYU-CTF,3CB, and HackTheBox to track the performance trends."}, {"title": "Appendices", "content": "Authors' contributions\nOriginal idea and evaluation methodology: DmV\nExperiment design and implementation: RT\nWriting: RT, DeV, DmV\nResearch direction: AP, DeV\nDataset modifications\nWe made the following adjustments to the InterCode-CTF dataset11.\nExcluded tasks:\nTwelve tasks requiring unavailable Internet resources 12\nTwo tasks requiring vision capabilities13\nTask 59 (website metadata no longer matches the flag)\nDeepMind excluded all tasks we excluded and also tasks 9, 16, 20, 35, 41, 54, 57.\nModifications:\nFixed incorrect flag format in Task 15\nKept two identical tasks14 separate for consistency with previous research\nPotential data contamination\nWe grew suspicious the performance we were seeing was just benchmark contamination anddecided to investigate. Specifically, we were concerned that:\nOur results significantly exceeded prior work (Abramovich et al. 2024; Phuong et al. 2024;Yang, Prabhakar, Narasimhan, et al. 2023), despite somewhat similar agent design.\nOur GPT-3.5-Turbo-based agent occasionally hallucinated flags for unrelated tasks whenstuck.\nTo test for contamination, we ran a controlled experiment: we asked the agent to submitflags without solving tasks, allowing 12 turns to reduce noise. The agent solved 9 tasks:\nSeven tasks15 were solved in 1-2 turns, suggesting they were straightforward\nTwo other tasks16 required more turns for correct flags"}]}