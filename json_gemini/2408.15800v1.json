{"title": "Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing", "authors": ["Kenneth Stewart", "Michael Neumeier", "Sumit Bam Shrestha", "Garrick Orchard", "Emre Neftci"], "abstract": "Achieving personalized intelligence at the edge with real-time learning capabilities holds enormous promise in enhancing our daily experiences and helping decision making, planning, and sensing. However, efficient and reliable edge learning remains difficult with current technology due to the lack of personalized data, insufficient hardware capabilities, and inherent challenges posed by online learning.\nOver time and across multiple developmental stages, the brain has evolved to efficiently incorporate new knowledge by gradually building on previous knowledge. In this work, we emulate the multiple stages of learning with digital neuromorphic technology that simulates the neural and synaptic processes of the brain using two stages of learning. First, a meta-training stage trains the hyperparameters of synaptic plasticity for one-shot learning using a differentiable simulation of the neuromorphic hardware. This meta-training process refines a hardware local three-factor synaptic plasticity rule and its associated hyperparameters to align with the trained task domain. In a subsequent deployment stage, these optimized hyperparameters enable fast, data-efficient, and accurate learning of new classes. We demonstrate our approach using event-driven vision sensor data and the Intel Loihi neuromorphic processor with its plasticity dynamics, achieving real-time one-shot learning of new classes that is vastly improved over transfer learning. Our methodology can be deployed with arbitrary plasticity models and can be applied to situations demanding quick learning and adaptation at the edge, such as navigating unfamiliar environments or learning unexpected categories of data through user engagement.", "sections": [{"title": "Introduction", "content": "The brain's learning strategies differ remarkably with deep neural networks in that learning is local and largely incompatible with gradient backpropagation, and it does not learn tasks from scratch. Throughout evolution, infancy, and adulthood, brains acquire inductive biases to survive and thrive. Even before birth, brains are specialized to solve tasks that are likely to be encountered in their lifetimes and environment [1]. Furthermore, they are equipped with restricted but highly optimized mechanisms to adapt to new variations and situations."}, {"title": null, "content": "In this work, we take inspiration from these observations to lay out an algorithmic framework that equips neuromorphic hardware with optimized mechanisms for rapid and efficient learning at the edge. Neuromorphic electronic systems aim to mimic the structure and dynamics of the brain to facilitate efficient, versatile, and rapid information processing [2, 3, 4, 5, 6]. Neuromorphic hardware generally implements Spiking Neural Networks (SNNs), which exploit temporal sparsity by means of asynchronous, event-based communication [7, 8]. Neuromorphic learning capabilities further enable rapid and energy-efficient learning at the edge via local synaptic plasticity rules [9, 10, 8, 11]. Prior work with neuromorphic systems demonstrated learning with three-factor synaptic plasticity from streaming data to incorporate new information, enabling privacy-preserving and low-latency adaptation to new data [12, 9, 8] at accuracies on par with deep neural networks. However, prior approaches did not deliver viable online learning technology, due to long training times, the temporally correlated and real-time nature of the data, or ineffective learning rules compared to their mathematically derived counterparts [13].\nOur approach solves these problems by distributing the learning process into two stages (Fig. 1). The first stage uses a cycle-accurate differentiable simulator of a target digital neuromorphic learning hardware to pre-train the plasticity and learning dynamics. Its differentiable nature allows for the computation of model parameter and hyperparameter gradients across the learning trajectory. This procedure is a type of bi-level optimization [14], where an outer loop optimizes over tasks sampled from a selected task, and where the inner loop dynamics captures the learning process on the sampled tasks. This bilevel optimization is achieved here using a variation of the Model Agnostic Meta Learning (MAML) algorithm [15, 16], where gradients are computed across the plasticity trajectory. The second stage is the deployment: The model trained in the first stage (called henceforth the pre-trained model) is deployed on the target hardware to learn from streamed event data.\nThe first stage is accomplished offline with high-throughput, flexible computers and a large dataset, while the second stage can take place online with power-efficient and real-time hardware using streaming inputs. This distributed approach shifts the most energy and data intensive stages of learning to data centers, away from the edge where power and chip area are limited. Bi-level optimization has a high energy and time cost since it optimizes the learning trajectory on a large number of tasks. However, in its intended applications, the cost can be amortized by using the same pre-trained model on a large number of learning devices at the edge. Transferring pre-trained learning models to hardware has been previously achieved in SNNs using surrogate gradient methods [17] for digital [18] and mixed-signal [19] neuromorphic hardware. Prior work has also demonstrated such bi-level co-optimization with software-simulated SNNs and surrogate gradients on few-shot learning problems [12]. However, the ability to transfer differentiable synaptic plasticity models for online learning with neuromorphic hardware has not yet been achieved.\nHere, we take an important step beyond prior work by meta-training for digital neuromorphic hardware deployment, and demonstrating this in the case of the Intel Loihi Research Chip (\"Loihi\") and its on-chip synaptic plasticity. Bi-level optimization is used to train network hyperparameters with the specific plasticity dynamics of the neuromorphic hardware and target task domain. Specifically, we use Surrogate-gradient Online Error-triggered Learning (SOEL) [18, 10], a local three-factor synaptic plasticity learning rule previously demonstrated for transfer learning in neuromorphic hardware [10, 20]. The three-factor synaptic plasticity rules used in this work are function of local information, namely from pre- and post-synaptic connections, and a global error used to optimize the synapses towards and objective. Three factor rules are consistent with the dynamics of biological synapses and constitute a normative theory of learning in the brain [21]. Using local information, less information is communicated on-chip [8, 5] enabling highly efficient adaptation to streamed sensory data. Furthermore, this leads to low latency since the update locking is drastically reduced [22].\nIn the following sections we describe the underlying plasticity dynamics and its implementation in digital neuromorphic hardware. Then, we demonstrate its use within the bi-level optimization with meta-learning using Intel's neuromorphic hardware as a case study."}, {"title": "Methods", "content": "We use bi-level optimization to train the parameters of SNNs equipped with plasticity dynamics on event-based vision tasks domains. Learning and synaptic plasticity in neuromorphic hardware is challenging"}, {"title": "Surrogate-gradient Online Error-triggered Learning in Digital Neuromorphic Hardware", "content": "In this work, we focus on digital neuromorphic processors equipped with plasticity dynamics [24, 25, 5], namely the the Intel Loihi which equipped with neuromorphic cores capable of on-chip synaptic plasticity [5, 26]. We utilize a plasticity rule adapted from a three-factor rule called Surrogate-gradient Online Error-triggered Learning (SOEL) [18, 10], which has been successively demonstrated in other few-shot learning scenarios [18, 27]. SOEL features three key properties that make it an excellent candidate for efficient synaptic plasticity rule on digital neuromorphic hardware: (i) Surrogate gradients, (ii) Multifactors, and (iii) Error-triggering, each of which is explained below. (i) The surrogate gradient approach solves the non-differentiability problem of the spiking neuron [17] by using a differentiable surrogate of the network for gradient computation. For the purpose of computing the gradient only, this surrogate network typically replaces the hard threshold firing of the Linear Integrate and Fire (LIF) with a Sigmoid-like function. Due to its simplicity, excellent practical performance, and the ability to leverage existing machine learning hardware and software, the surrogate gradient method has become the de facto method for computing the gradients of SNNs [12, 28, 29, 30, 31, 32].\n(ii) The multi-factor aspect enables an efficient implementation in hardware. The analytical form of the synaptic weight gradients in a single LIF network reveals three factors, a global error signal, a local post-synaptic factor, and a local pre-synaptic factor [31, 17]:\n$\\Delta w_{ij} (t) = \\frac{\\frac{\\partial L(s(t))}{\\partial s_i(t)} \\frac{\\partial s_i(t)}{\\partial u_i(t)} \\frac{\\partial u_i(t)}{\\partial w_{ij}}}{\\frac{\\partial s_i(t)}{\\partial u_i(t)} \\frac{\\partial u_i(t)}{\\partial w_{ij}}} =: error(t) \\sigma'(u(t)) p_j(t),$"}, {"title": null, "content": "where L is a differentiable loss function, $u_i(t)$ is the membrane potential of the neuron i at time t, $w_{ij}$ is the weight of the synapse connection between neurons j and i, and $p_j(t)$ is the trace of the pre-synaptic input from neuron j. The term $\\sigma'$ is the gradient of the activation function of the neuron. Because spiking neurons are not differentiable due to their sharp firing threshold, the naive analytical $\\sigma'$ is zero everywhere except at the origin, where it is infinity. The surrogate gradient approach mitigates this problem by replacing this term with a differentiable one, such as the derivative of the sigmoid [31] or a box-car function [23]. The first factor, error(t), acts as an error-dependent external modulation of the weight update. The factor $p_j$ is related to the pre-synaptic trace used in the classical Spike Time Dependent Plasticity (STDP) rules, making its implementation compatible with most existing neuromorphic learning hardware. Furthermore, only one trace per neuron is required (there are no terms with indices i and j simultaneously), which means that the memory required for learning scales linearly with the number of neurons. Prior work has demonstrated the effectiveness of three-factor learning with SNNs on neuromorphic hardware, with the third factor drastically improving learning by projecting task-specific errors to the neurons [33, 10, 18].\n(iii) The exact analytical form of the weight gradients assumes that weight updates are performed continuously in time. In a typical digital implementation, this translates into writing memories at every time step, which is both energy-inefficient and slow. To solve this problem, SOEL operates in an error-triggered manner: weight updates occur only when an accumulated error threshold is crossed [23]. This drastically reduces the number of updates necessary for learning, while largely preserving the learning performance. More specifically, for a given input and target pair, the error $e_i$ for neuron i is computed with the post-synaptic neuron using the following equation:\n$\\begin{aligned}\ne_i(t) &= s - s_i(t),\\nY_i(t) &= \\begin{cases}\n0, \\text{ if } -\\theta > e_i(t) > 0 \\\\\ne_i(t), \\text{ otherwise}\n\\end{cases}\n\\end{aligned}$"}, {"title": null, "content": "where s is the target number of spikes during the time window T for this neuron, and $s_i(t) = \\sum_{t-T}^t s_i(t)$ is the number of spikes produced by neuron i in the previous T timesteps. T is the interval during which errors are computed to determine if the error reaches the threshold for plasticity to occur. The computation of $Y_i$ is carried out on a general purpose processor [20]. The Intel Loihi includes general purpose x86 \u201cembedded cores\" for such purposes. The embedded processor calculates the error term $e_i(t)$ and the thresholded error term $y_i(t)$, and writes the latter to a post-synaptic state on the Loihi chip's plasticity state to update the synaptic weights. The embedded processor induces a higher latency than the plasticity dynamics. To mitigate this, we compute and apply $\\Delta w_{ij}$ every T steps. Furthermore, if the error is below the threshold $\\theta$, then no update is applied. Because post-synaptic states on the chip can only be positive, in practice the $e_i(t)$ is offset by the constant term which is then canceled out in the plasticity dynamics (see Methods). The following equation describes SOEL:\n$\\Delta w_{ij} = \\eta p_j(T) y_i(T).$"}, {"title": null, "content": "Here, $\\eta$ is the learning rate, $p_j(t)$ is a pre-synaptic trace variables that reflects a non-weighted presynaptic potential, as required by gradient-based learning in SNNs [17]. Since we use current-based LIF synapses, SOEL with the pre-synaptic traces $p_j(t)$ are second-order linear filters. The postsynaptic term $\\sigma'$ present in Eq. (1) is omitted from Eq. (3), i.e. it is always equal to 1. This is for two reasons: firstly the first generation of the Loihi chip (Loihi 1) did not allow three factors. Secondly, SOEL is used for learning in the final layer and omitting this term does not affect the sign of the gradients in this layer. SOEL is an approximation of the analytically computed weight gradients, and thus may fail to reduce the error to zero. We emphasize here that although the loss function is rate based, the learning is inherently spike-based. This is because the individual spikes contribute to the neural and synaptic traces, enabling the neuron to learn patterns of spike in a way similar to surrogate gradient learning [17]. We further note that the use of rate-based losses with surrogate gradients is the common practice in the field [34, 35, 13]."}, {"title": "Bi-level Optimization algorithm and Meta Learning Algorithms", "content": "We describe here the bi-level optimization procedure. We meta-train models using surrogate gradient MAML similar to MAML SNN [12]. The MAML based framework is used here to learn a parameter initialization"}, {"title": null, "content": "$w_0$, such that from the initialization one-shot adaptation to new samples can occur in the neuromorphic hardware. To accomplish this, datasets are split into three sets of meta-tasks: meta training $\\mathcal{T}^{trn}$, meta validation $\\mathcal{T}^{val}$, and meta testing $\\mathcal{T}^{tst}$. Each task T of meta-task $\\mathcal{T}$ consists of a training dataset $\\mathcal{D}^{trn}$ and a test dataset $\\mathcal{D}^{tst}$ of the form $\\mathcal{D} = \\{x_i, t_i\\}_{i=1}^M$. Here $x_i$ denotes the input data, $t_i$ the target (label) and M is the number of target samples.\nSimilar to MAML, the parameter initialization is optimized with bi-level optimization using two nested optimization loops, one \"inner\" loop and one \"outer\" loop. The inner loop imitates the online SOEL dynamics, using the factors derived from the Current Based (CUBA) neuron (Eq. (9)) and the Cross Entropy loss.\n$\\begin{aligned}w_{n+1}(w_n, \\mathcal{D}^{trn}) &= w_n - \\alpha \\Delta w(\\varsigma, w_n),\\\\text{where } \\{x, t\\} \\in \\mathcal{D}^{trn} \\text{ for } n=1, ..., N.\\end{aligned}$"}, {"title": null, "content": "Here $\\Delta w(x, w_n)$ is the SOEL update with N, the number of adaptation steps of the inner loop, and $\\alpha$ is the inner loop learning rate. The dependence of $\\Delta w(x, w_n)$ is made explicit for the outer loop gradients below. Furthermore, the inner loop operations are traced for auto-differentiation [36], such that their gradients can be seamlessly computed in the outer loop. During learning, a task is sampled from the training meta-task $\\mathcal{T}^{trn}$ and N inner loop updates are made using batches of data sampled from $\\mathcal{D}^{trn}$. The resulting parameters $w_N$ are then used to update the outer loop using the matching test dataset $\\mathcal{D}^{tst}$.\nThe parameters updated by the gradients calculated in the inner loop are then input to the outer loop where they are evaluated on the validation set for the second gradient calculation, again using cross entropy loss but instead of being optimized with stochastic gradient descent the outer loop is optimized with ADAM [37]. The loss function optimized in the outer loop loss is:\n$\\mathcal{L}_{outer}(w_o) = \\sum_{T \\in P(\\mathcal{T}^{trn})} L(f(w_N(w_o, \\mathcal{D}^{trn}), \\mathcal{D}^{tst})),$"}, {"title": null, "content": "where $T = \\{\\mathcal{D}^{trn}, \\mathcal{D}^{tst}\\}$. In practice, the above expression is generally computed over a random subset of tasks rather than the full set $\\mathcal{T}^{trn}$. Notice that the outer loop loss is computed over the test dataset $\\mathcal{D}^{tst}$, while $w_n(w_o)$ is computed using the training dataset $\\mathcal{D}^{trn}$, which is shown to improve generalization. An ADAM optimizer is used in the outer loop training [37]. Although ADAM incurs larger memory and compute resources compared to Stochastic Gradient Descent (SGD) it does not impact the deployment on the hardware which is solely based on SOEL.\nSince digital neuromorphic hardware typically uses integer weights, full precision \u201cshadow weights\" [34] are quantized and used during the forward inference phase of the inner and outer loops. During the backward pass, full precision gradients and weights are used to update parameters."}, {"title": "Experimental Setup and Datasets", "content": "Few-shot learning aims to achieve rapid adaptation when prior knowledge related to the task domain is available. Rather than iterating many times over many samples of data to learn, few-shot learning utilizes prior knowledge of the task domain to learn from one or a few samples of the data. To accomplish this, we set up N-way K-shot learning experiments. Here, N = 5 is the number of classes to be classified and K = 1 is the number of shots, or sample presentations, given to the model for learning.\nWe evaluated our models on modified datasets captured using event-based vision sensors, including the neuromorphic MNIST (N-MNIST), the American Sign Language Dynamic Vision Sensor (ASL-DVS) and the DvsGesture datasets[38, 39, 40, 41, 42]. Each meta dataset consists of 32x32x100 ms event data sample and are constructed as follows:"}, {"title": null, "content": "\u2022 Double N-MNIST: From the 10 class N-MNIST datasets' 60k training and 10k test samples, Double N-MNIST horizontally combines two N-MNIST samples, resulting in 100 classes. These 100 classes were divided into 64 meta-train, 16 meta-validation, and 20 meta-test classes.\n\u2022 Double ASL-DVS: Similarly to Double N-MNIST, the Double ASL-DVS' concatenates the 24 classes (A-Y excluding J), 4200 samples per class, resulting in 576 tasks. There were split in 369 meta-train, 92 meta-validation, and 115 meta-test classes.\n\u2022 DvsGesture subject+class: 29 individuals, 10 gestures, 1078 training, and 264 test samples. We created splits that used both gesture and individual, resulting in 319 tasks. These were split in 253 meta-train, and 66 meta-test classes [43, 44]. Examples of images from the DvsGesture dataset and how it is implemented for meta-training and meta-testing are shown in Fig. 2. The task here is to classify the combination of subject and gesture category.\nTo meta-train models compatible with the Intel Loihi, we trained SNNs with Intel's Lava-DL open source library [26], which is the first stage of the meta-learning procedure. Lava-DL is a deep library to facilitate offline training and online training and inference within the Lava framework. The library includes a functional differentiable simulator of the Intel Loihi's neuron dynamics, and quantization-aware training with the same stochastic rounding and precision used in the Loihi chip. Therefore, models meta-trained with Lava-DL are transferable for use with Intel's Loihi hardware.\nThe pipeline for meta-training models compatible with Intel's Loihi hardware for meta-testing on hardware is shown in 3. For all results, we meta-trained models to perform 5-way 1-shot learning with Lava-DL. A Lava-DL network is optimized across tasks using the ADAM optimizer [37] in the outer loop and optimized to one-shot classify 5 tasks at a time using SOEL. The models can then be transferred to the Intel Loihi hardware for adaptation with SOEL for online learning at the edge using NetX. All SNN-based models used a two layer network of linear layers with 512 neurons each, plus an output layer consisting of 5 neurons."}, {"title": "Results", "content": null}, {"title": "SOEL rule in a Single Neuron", "content": "We first show that the SOEL rule operates as expected in a simple task aimed at adjusting the firing rate of a neuron towards a target value. We show that in a single layer of LIF neurons implemented on the Intel Loihi chip, SOEL reduced the error to zero. Fig. 4 demonstrates how SOEL drives learning and reduces the error of a single neuron in the layer to zero, thereby achieving a target firing rate. The single neuron example uses Intel's Lava2 software simulation of the Loihi 2 plasticity state machine. In Fig. 4 the neuron stimulated by Poisson-distributed spike trains. These inputs elicit firing at a rate that is at first too high or too low. Over time, SOEL plasticity adjusts the synaptic weight according to the error and trace values until it reaches the target firing rate. In Intel Loihi chips, the learning step of the plastic synapse is executed at a"}, {"title": "Bi-level Optimization and Few-shot Learning in the Intel Loihi", "content": "The bi-level optimization procedure aims to optimize the plastic network parameters hyperparameters to pre-train the network for fast online learning. In this bi-level optimization, SOEL is effectively the inner loop adaptation, while the outer loop consists of a conventional gradient backpropagation-through-time across the network with SOEL.\nHere SOEL is only enabled in the last layer of the network for the following reasons: First, prior work demonstrated that MAML learns a suitable representation for few-shot learning instead of solely rapid learning in both ANNs [45] and SNNs [12], meaning networks with meta-learned initializations can achieve high performance on learning new tasks using only a single gradient update in the last layer. The likely reason for this is that MAML learns an adequate feature representation in the layers prior to the plastic ones [46]. Second, there is no requirement to spatially backpropogate errors in the inner loop, allowing for learning rules such as SOEL to be implemented very efficiently in hardware. Third, the outer loop optimization mitigates the approximations of SOEL by optimizing the hyperparameters accordingly.\nWe emphasize here that although SOEL is used in the final layer, the bi-level optimization modifies the parameters of the entire network. The difference between the final layer synapses and the other layers is that the former undergo changes during the presentation of a new sample and thus specialize in the new task. In contrast, the synaptic weights in the other layers represent features that are common to all tasks in the meta dataset. Although in some cases, optimizing the penultimate layer was reported to also benefit accuracy [46], the results of the previous simulation showed that the expansion of the plasticity deeper into the network produced only marginal improvements [27].\nWe demonstrate one-shot learning on the Loihi neuromorphic hardware using SOEL with a meta-trained network on the datasets described in the following sections. Once the model is meta-trained (the \"first stage\") the network (the \"second stage\") is transferred to the Intel Loihi for online one-shot learning on the device."}, {"title": "Meta-Training for Few-Shot Learning", "content": "To build the prior knowledge for effective few-shot learning in neuromorphic hardware, we optimize for few-shot learning on event-vision classification tasks. We optimize the model for three different event-based vision tasks. The Double NMNIST task [12], a double digit version of the Neuromorphic MNIST dataset [40]; The Double ASL-DVS [12], two American Sign Language letter gestures presented side-by-side recorded with the DAVIS 346 [41] event-vision sensor; and the DVSGesture dataset [42], a mid-air gesture dataset recorded with the DAVIS 128 [38] event-vision sensor. In our experiments with the DVSGesture dataset the task is to not only classify the gesture but the individual performing the gesture as well; therefore, there are 44 classes in total. The details of how the datasets are prepared for the experiments and split for bi-level optimization can be found in Section 2.3.\nWe performed multiple few-shot bi-level learning experiments across the datasets. The meta-test set accuracy results of our few-shot bi-level learning experiments are shown in Table 1. To test the models, each model is tested in a one-shot learning trial. In a trial the model is first given a single shot of data to learn from in an inner loop gradient step, then from the single shot learned parameters inference is performed on 10-shots of the test data. The accuracy values shown in Table 1 are averaged over 200 random trials using the same random seed plus or minus the standard deviation across those trials. Neuromorphic hardware has constraints that limit the network compared to a full precision SNN trained in conventional GPU hardware. These constraints are as follows: i) Quantization: the neuron and synapse states are quantized to fixed-point integer values with signed 8 bits of precision for the weights that are stochastically rounded to even values; ii) Hard reset: the reset of the neuron is hard meaning a neuron that spikes membrane potential will reset to 0; and iii) Plasticity processor: use of the SOEL plasticity rule. The SNN model uses floating point precision by default with a soft reset (i.e. the membrane potential is subtracted by a fixed amount after spiking), as opposed to the hard reset to a reset potential (used in the Intel Loihi). Therefore we demonstrate how the"}, {"title": "Discussion", "content": "Neuromorphic hardware equipped with synaptic plasticity utilizes event-driven weight updates to rapidly adjust to new tasks. Due to the mostly local and sparse nature of neuromorphic hardware, it is particularly well suited for online learning at the edge. We adopted a powerful meta-learning strategy that can adapt three-factor plasticity rules for accurate online one-shot adaptation at the edge.\nOur work differs from previous efforts to learn from scratch with neuromorphic hardware [25, 24, 19, 8]. Although such approaches work in theory and well controlled learning procedures, they did not provide a viable learning technology. This is because online learning from streaming data in real-world scenarios has several challenges which are not addressed in prior work: data is not i.i.d., training is lengthy, and impractically small learning rates are necessary to average across the entire dataset.\nWe address these challenges using a combination of offline and online learning. Offline pre-training takes place on high-power, high-throughput hardware with access to extensive datasets allows for the development of initial models that capture common features in the task domain. Gradient-based meta-learning such as MAML automates this pre-training. These pre-trained models can then be fine-tuned or adapted on the device using online learning techniques, such as the SOEL. This hybrid approach strikes a balance between leveraging prior knowledge and adapting to real-time data, offering improved robustness and faster convergence.\nFurthermore, when meta-training a network with plasticity dynamics, the resulting pre-trained model can mitigate the non-idealities of the adaptation rule with respect to gradient descent, at least for the selected number of training steps. This observation means that certain challenging hardware constraints involved in the implementation of the synaptic plasticity rule can be relaxed without significantly losing performance. In the case of SOEL, we relaxed the update frequency and the nature of the post-synaptic factor. Other related work mitigated the downsides of STDP [48] and the non-idealities in hardware [49].\nUsing bi-level optimization to enable fast learning with brain-like synaptic plasticity has previously been attempted [48, 50]. Additionally, other work demonstrated the benefits of bi-level optimization for improving the data and power efficiency of learning at the edge [50]. For example, [48] showed that plasticity can be optimized by gradient descent, or meta-learned, in large artificial networks with Hebbian plastic connections, called differentiable plasticity. Our work is closest to [51]: Building on the differentiable plasticity, they meta-optimized Hebbian-like STDP learning rules and local meta-parameters on the event-based datasets for few-shot learning tasks using a model of the Tianjic hardware. Synapses store two components, one that is optimized during bi-level optimization and one that is updated using STDP like plasticity. This is similar in spirit to optimizing initial weights as in MAML and in our work. Our approach differs in three ways: (i) by using a three-factor synaptic plasticity rule (i.e the SOEL), which takes error signals computed from a custom loss function thus enabling the learning of supervised and self-supervised approaches, (ii) we demonstrate on-line learning in custom digital neuromorphic learning hardware, and (iii) by operating the neural dynamics on finer time scales (1ms time steps compared to 12.5ms time steps in the case of [51], which is closer to conventional frame camera rates), providing more than an order of magnitude in temporal resolution.\nWe demonstrated SOEL and the associated bi-level optimization in a range of event-based classification tasks, with data recorded from event-based vision sensors. The class of few-shot learning problems with event-based data has been identified as a key benchmark and application by the broader neuromorphic engineering community [44]. While our work focused on supervised learning with visual data, our method and general approach is applicable to other supervised learning tasks. In fact, the MAML framework has been established for tasks such as reinforcement learning [52], multilingual speech emotion classification[53],"}, {"title": null, "content": "and self-supervised learning [54]. In this work, SOEL was limited to the final layer of the network. This is not a conceptual limitation of SOEL itself, since similar error-triggered learning using local losses was already demonstrated in prior work [20]. Rather, this choice was made because MAML is known to learn a suitable representation for few-shot learning instead of rapid learning [55]. Indeed, simulation results show that extending the plasticity deeper in the network yields marginal improvements [27].\nOne limitation of SOEL is for continual learning: Catastrophic forgetting of previous knowledge will occur after multiple online learning epochs. Looking forward, we envision two approaches to solve this problem: (i) Federated learning and (ii) Continual learning techniques, which we detail below. First, federated learning is dedicated to addressing the problem of sharing knowledge from edge devices to a cloud model, while preserving privacy, communication and accuracy [56, 57, 58, 59]. Using meta-learning approaches, the meta-learned models can also be fine-tuned with the data collected at the edge [60]. Online meta-learning [61, 62, 63, 64] can enable continual meta-learning in the offline loop, provided task-relevant information can be transferred back to the offline agent. Secondly, currently developed methods to use continual learning can be implemented [65]. These consist of dynamic architectures, whereby branches of the network are modulated for example using neuromodulation, metaplasticity where synapses are equipped with internal consolidation variables and replay methods.\nAnother limitation is general to the meta-learning problem: the pre-trained models generalize poorly to tasks that are not present in the meta-training set. An example of such a new task is flipping the event-camera inputs upside down. In such cases, performance drops drastically. Using larger datasets and models is a natural solution to this problem. This is the case with so-called foundational models [66] that demonstrate downstream task performance that sometimes even exceeds those of models specifically trained on such tasks [67]. Although the scale of current digital neuromorphic hardware and datasets are far from those used in foundational models, this gap is closing thanks to a better selection of the dataset and a simplification of foundational models [68] and the development of large-scale neuromorphic hardware.\nBi-level optimization on can be used to achieve high performing rapid learning on ultra-low power neuromorphic hardware. While our method with MAML and SOEL does not solve the problem of catastrophic forgetting, future work using federated and continual learning techniques could help prevent catastrophic forgetting. Additionally, ongoing work on foundation models and the development of large-scale neuromorphic hardware could benefit from online rapid adaption through bi-level optimization across a multitude of task domains such as autonomous vehicles, and speech translation. We expect that such meta-learning approaches will lead to a substantial redefinition of synaptic plasticity requirements for neuromorphic processors and pave the way for exciting and novel learning applications at the edge."}, {"title": "Appendix", "content": null}, {"title": "Implementation of SOEL on the Intel Loihi 1 and 2 Plasticity State Machine", "content": "SOEL implemented in the Intel Loihi chip follows Eq. (3), with the difference that a constant factor is introduced to represent negative errors. This is because a post-synaptic trace variable is used as a placeholder for $e_i(t)$. Because post-synaptic variables cannot be negative, we offset the values with a constant c as follows:\n$\\begin{aligned}e_i(t) &= s - s_i(t),\\nY_i(t) &= \\begin{cases}\nc + e_i(t), \\text{ if } -\\theta > e_i(t) > 0 \\\\\n0, \\text{ otherwise}\n\\end{cases}\n\\end{aligned}$\n$\\Delta w_{ij} = \\eta p_j(T) (y_i(T) - c).$"}, {"title": null, "content": "Furthermore, the second order trace $p_j$ is computed using a difference of two first order traces (indices j) omitted:\n$\\begin{aligned}p(t) &= x^1(t) - x^2(t) \\\\\nx^1(t) &= a_u x^1(t - 1) - (1 - a_u) s(t) \\\\\nx^2(t) &= a_v x^2(t - 1) - (1 - a_u) s(t)\n\\end{aligned}$"}, {"title": null, "content": "with $a_v > a_u$.\nOn the Loihi 1 hardware, custom programs run on the Loihi's x86 embedded processors to calculate the error to be written to the post-synaptic variable of the learning rule. The programs are run every learning epoch, with a predetermined timestep interval input to the learning rule whose value can be in the range {0,...,63}. Therefore, multiple updates will typically be performed depending on the number of timesteps of a sample. During a learning epoch the error is calculated as follows. First is the determination of if the learning epoch occurs during a sample presentation or a blank time. A blank time is a period of time in which no sample is presented which can allow traces and neuron parameters to decay back to 0 before presenting another sample. If the learning epoch occurs during a blank time then"}]}