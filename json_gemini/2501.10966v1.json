{"title": "DC-PCN: Point Cloud Completion Network with Dual-Codebook Guided Quantization", "authors": ["Qiuxia Wu", "Haiyang Huang", "Kunming Su", "Zhiyong Wang", "Kun Hu"], "abstract": "Point cloud completion aims to reconstruct complete 3D shapes from partial 3D point clouds. With advancements in deep learning techniques, various methods for point cloud completion have been developed. Despite achieving encouraging results, a significant issue remains: these methods often overlook the variability in point clouds sampled from a single 3D object surface. This variability can lead to ambiguity and hinder the achievement of more precise completion results. Therefore, in this study, we introduce a novel point cloud completion network, namely Dual-Codebook Point Completion Network (DC-PCN), following an encder-decoder pipeline. The primary objective of DC-PCN is to formulate a singular representation of sampled point clouds originating from the same 3D surface. DC-PCN introduces a dual-codebook design to quantize point-cloud representations from a multi-level perspective. It consists of an encoder-codebook and a decoder-codebook, designed to capture distinct point cloud patterns at shallow and deep levels. Additionally, to enhance the information flow between these two codebooks, we devise an information exchange mechanism. This approach ensures that crucial features and patterns from both shallow and deep levels are effectively utilized for completion. Extensive experiments on the PCN, ShapeNet_Part, and ShapeNet34 datasets demonstrate the state-of-the-art performance of our method.", "sections": [{"title": "Introduction", "content": "With the increasing popularity of 3D scanning devices such as LiDAR scanners, laser scanners, and RGB-D cameras (Fei et al. 2022a), the realm of vision and robotics has witnessed a significant surge in interest in utilizing 3D data. Among the various formats available, point clouds have emerged as a prominent choice due to their advantages, including low memory usage and detailed 3D shape information (Qi et al. 2017; Yuan et al. 2018; Mo et al. 2025). However, raw point cloud data obtained directly from 3D sensors often suffers from incompleteness due to the limitations such as view- point occlusion and the resolution of 3D scanners (Fei et al. 2022b). This incompleteness poses significant challenges for downstream tasks, such as point cloud classification and segmentation, hindering the achievement of desired results (Tchapmi et al. 2019; Xie et al. 2020; Yuan et al. 2018).\nThanks to advances in deep learning, a variety of point cloud completion methods have been explored. They can be broadly categorized into the voxel-based approach and the point-based approach . The voxel-based approach (Choy et al. 2016; Girdhar et al. 2016; Dai, Ruizhongtai Qi, and Nie\u00dfner 2017; Han et al. 2017; Wu et al. 2015) introduces a voxel paradigm to formulate structural representations for 3D objects with 3D convolutions. The major issue of this approach is to control the voxel granularity. Smaller voxel sizes typically demand high computational costs; while larger voxels can lead to substantial degradation of the structural information and the precision of the completion outcomes. The point-based approach has been proposed to take point cloud inputs directly such as PCN (Yuan et al. 2018), PF- Net (Huang et al. 2020), MSN (Liu et al. 2020), TopNet (Tchapmi et al. 2019) and GRNet (Xie et al. 2020). Recently, to enhance the extraction of intricate structural patterns, Point- Transformer (Zhao et al. 2021), Pointformer (Pan et al. 2021), and PCT (Guo et al. 2021) integrate Transformer architec- tures into point cloud processing to capture long-range point dependencies. Nonetheless, these methods overlook the fact that point clouds sampled from a 3D object's surface can vary, leading to ambiguity issues for an identical 3D surface and ultimately incur sub-optimal completion outcomes, as illustrated in Figure 1 (a).\nTherefore, in this study, we propose a novel point cloud completion network, namely Dual-Codebook Point Com- pletion Network (DC-PCN) following an encoder-decoder scheme. It aims to preserve as much information as possible from the point cloud while also concentrating features from the same surface into a unique, consistent representation, thereby reducing potential ambiguities in learning. Under the inspiration of ShapeFormer (Yan et al. 2022) and AutoSDF (Mittal et al. 2022), DC-PCN introduces a dual-codebook de- sign for consistent point cloud representations from a multi- level perspective. As illustrated in Figure 1 (b), the latent feature representation vector is quantized to reduce the vari- ance caused by the point cloud sampling procedure. Specif- ically, the dual-codebook consists of an encoder-codebook and a decoder-codebook to explore the shallow and the deep level point cloud patterns. Moreover, a quantized informa- tion exchanging mechanism is further devised to connect and interact between the two codebooks, aiming to enhance the structural and finer detailed point cloud patterns jointly. Comprehensive experiments conducted on three benchmark datasets, including PCN, ShapeNet_Part, and ShapeNet34, demonstrate that our DC-PCN achieves the state-of-the-art performance in point cloud completion.\nTo summarize, our contributions are as follows:\n\u2022 We propose a novel codebook-based point cloud com- pletion network, namely DC-PCN, with a dual-codebook design for consistent point cloud representations from a multi-level perspective.\n\u2022 A quantized information exchanging mechanism is de- vised to connect and interact between the two codebooks, to enhance structural and finer detailed patterns.\n\u2022 Comprehensive experiments on benchmark datasets in- cluding PCN, ShapeNet_Part, and ShapeNet34 demon- strate the effectiveness of our DC-PCN."}, {"title": "Related Work", "content": "Voxel-Based Approach. In addressing the challenges of the unstructured nature of point clouds, (Choy et al. 2016; Girdhar et al. 2016; Dai, Ruizhongtai Qi, and Nie\u00dfner 2017; Han et al. 2017; Wu et al. 2015) introduced the voxel paradigm to formulate structured representations for 3D ob- jects. These methods have achieved encouraging performance in point cloud completion, using the 3D convolutions to cap- ture the spatial relationships within the voxelized data. Note that the point clouds captured from the same surface can achieve the same voxel structure if the voxel size is set appropriately, resulting in a consistent surface representa- tion. Nonetheless, a prevalent challenge encountered by these methods is the intricate balance strategy for controlling the voxel granularity. A smaller voxel size demands higher com- putational costs; while a larger voxel size typically results in substantial degradation of the structural information, reduc- ing the precision of point cloud completion.\nPoint-Based Approach. To address the issues of voxel- based methods, point-based studies have been proposed to take point cloud inputs directly. The pioneer study PCN (Yuan et al. 2018) introduced a point-based point cloud completion neural network. Subsequently, various point cloud comple- tion methods have emerged, such as PF-Net (Huang et al. 2020), MSN (Liu et al. 2020), TopNet (Tchapmi et al. 2019), GRNet (Xie et al. 2020), P2c(Cui et al. 2023). Recent years, to further augment the extraction of intricate structural pat- terns, Point-Transformer (Zhao et al. 2021), Pointformer (Pan et al. 2021), and PCT (Guo et al. 2021) were among the ini- tial attempts to integrate Transformer architectures into point cloud processing. SVDFormer (Zhu et al. 2023) proposed novel method that leverages multiple-view depth image in- formation to observe incomplete self-shape and generate a compact global shape through Transformer architecture. Leveraging the capability of Transformer to capture long- range spatial dependencies, these methods have achieved refined results for point cloud completion. However, these point-based methods overlook that point clouds sampled from a 3D object surface can vary. It leads to the ambiguity chal- lenges to obtain identical representations for a 3D surface, which ultimately incur sub-optimal completion outcomes.\nIn this study, our goal is to preserve as much informa- tion as possible from the point cloud while also concentrating features from the same surface into a unique, consistent repre- sentation, thereby reducing potential ambiguities in learning."}, {"title": "Methodology", "content": "Overview of DC-PCN & Problem Formulation\nThe proposed DC-PCN follows a transformer-based encoder- decoder architecture, as illustrated in Figure 2. It treats point cloud completion by mapping a partial point cloud $P_{partial} \\in \\mathbb{R}^{N_{partial} \\times 3}$ to its complete form $P_{complete} \\in \\mathbb{R}^{N_{complete} \\times 3}$, which is an estimation of the ground truth $P_{GT}$. $N_{partial}$ and $N_{complete}$ denote the number of points in the partial point cloud and complete point cloud, respectively; and 3 indicates the 3- dimensional coordinates of points. Note that we adopt a region-based modelling strategy, where regions are obtained by a farthest point sampling (FPS) algorithm from $P_{partial}$. These regions can be defined as a set of circles, each contain- ing a subset of points in $P_{partial}$. In total, we have M regions with their centers $F = (\\pmb{\u00a5_1}, ..., \\pmb{\\_m}, \u2026\u2026, \\pmb{\\_M})$.\nAs in the DC-PCN pipeline, a shallow feature extractor, noted as SF extractor, first formulates a set of shallow feature vectors: $F_{shallow} = (f_1, ..., f_m, ..., f_m)$, in line with the M regions. $F_{shallow}$ is then used to predict a coarse point cloud, denoted as $P_{coarse} \\in \\mathbb{R}^{N_{coarse} \\times 3}$, where $N_{coarse}$ is the number of points in $P_{coarse}$. $P_{coarse}$ is used to formulate the deep features $F_{deep} \\in \\mathbb{R}^{HXC}$, where H is the number of deep feature vectors and C denotes the feature dimension. Finally, based on $F_{deep}$, the complete point cloud $P_{complete}$ can be obtained with the help of detailed shape predictor which is a foldingNet based module, noted as DS predictor.\nTo assist this shallow-level and deep-level representation learning, our DC-PCN designs a dual-codebook quantiza- tion scheme to achieve consistent and unambiguous latent representations for point clouds. In detail, $F_{shallow}$ and $F_{deep}$ are quantized using an encoder-codebook and a decoder- codebook, respectively. Each point feature vector is repre- sented by the closest code vector in the codebook, resulting in their discrete versions $\\hat{F}_{shallow}$ and $\\hat{F}_{deep}$. These are com- bined with $F_{deep}$ and fed to the shape predictor. To enable the two codebooks to work together and overcome the differ- ence of data distribution between encoder codebook, noted as ECD and decoder codebook, noted as DCD, a quantized information exchange mechanism is devised, promoting the exchange of information between the shallow-layer and deep- layer codebooks. In the following sections, we will elaborate on the details of DC-PCN."}, {"title": "Dual-Codebook for Discrete Representations", "content": "To maintain accurate structural and consistent information, we devise an innovative dual-codebook approach inspired by the discretization process of vector quantized variational au- toencoders (VQ-VAEs) (Van Den Oord, Vinyals et al. 2017). This is based on the rationale that point clouds sampled from the same surface are visually similar, and their extracted fea- tures should also reflect this similarity. This discretization operation projects identical or similar surface features into the same code vector, reducing the impacts of random sam- pling variations in the latent space and achieving a consistent representation of point cloud samples from a 3D object.\nThe dual-codebook design consists of an encoder- codebook $C_E$ and a decoder-codebook $C_D$. The design places $C_E$ prior to the transformer encoder to ensure the precise understanding of input geometric structures, while $C_D$ is positioned after the transformer decoder to provide guidance for the output. In detail, we have $C_E = (c_1, ..., c_k, ..., c_K) \\in \\mathbb{R}^{K\\times R}$ and $C_D = (c^h_1, ..., c^h_k, ..., c^h_K)^\u315c \\in \\mathbb{R}^{K\\times R}$. The two codebooks are with a consistent dimension, where K and R indicate the size of a codebook and the dimension of a code vector, respectively.\nTaking $C_e$ as an example, it aims to quantize $f_m$ with a posterior distribution q($z_m$|$f_m$) as follows:\n$q(\\hat{z}_m = c_k|f_m) =\\begin{cases} 1, & \\text{for } k^* = \\text{argmink}||f_m - c_k||_2, \\\\ 0, & \\text{otherwise.} \\end{cases}$    (1)\nTo this end, a set of discrete representations is obtained, de- noted as $Z = (\\hat{z}_1, \u2026\u2026\u2026, \\hat{z}_m, \u2026\u2026\u2026, \\hat{z}_M)^T$. During training, $c_k$ is updated in an interactive manner with $F_{shallow}$ to adjust its distribution and align with the non-discrete representa- tions. Subsequently, Z is used as a proxy for $F_{shallow}$ and is sent to the transformer encoder. Similarly, we denote the discrete representation set from the decoder-codebook as $Z = (\\hat{z}^1, ..., \\hat{z}^n, ..., \\hat{z}^N)^T$."}, {"title": "Quantized Information Exchanging", "content": "The two codebooks characterize shallow and deep point cloud feature distributions, and the quantized point cloud represen- tations Z and Z each contain unique patterns that, when combined, are assumed to achieve both structural and finer detailed enhancements for point cloud completion. Hence, we propose facilitating the incorporation between Z and Z with a quantized information exchanging mechanism, noted as QIE. This mechanism comprises three components: code deduplication, code distribution re-targeting, and code merg- ing. The details are discussed as follows.\nCode Deduplication. In pursuit of a high information den- sity and representativeness to better captures the intrinsic structral pattern, we propose to deduplicate the quantized feature sets Z and Z. We denote the deduplicate set as $Z' = (\\hat{z}_1, ..., \\hat{z}_{l}, ..., \\hat{z}_l)^\u0422$ and $\\tilde{Z} = (\\hat{z}^1, ..., \\hat{z}^i, ..., \\hat{z}^7)$.\nCode Distribution Re-Targeting. The data distributions of features in the encoder-codebook and decoder-codebook can be very different. To illustrate this, we performed kernel den- sity estimations on five randomly selected features trained on the PCN dataset. As shown in Figure 3, a noticeable disparity in data distribution between the two codebooks is evident. This poses a challenge when exchanging and merging the quantized information obtained from them. To address this challenge, a code distribution re-targeting module is devised for merging the two quantized feature sets. It consists of two streams: a forward stream from the encoder-codebook to the decoder-codebook and a reverse stream from the decoder- codebook to the encoder-codebook. In the forward stream, for example, the core of the re-targeting module is based on a cascading MLP network. It takes the deduplicated quantized feature set Z' as input and projects them into the decoder- codebook's feature space to align with the target distribution. We denote the re-targeted results as: $\\tilde{Z'} = (\\hat{z}^1, ..., \\hat{z}^{h}, ..., \\hat{z}^7)$.\nCode Merging. To merge the re-targeted code vectors ob- tained from another codebook, a code merging module is designed with three steps. 1) For a re-targeted representation with $\\hat{z} \\in \\tilde{Z'}$, it searches all the codes in the target codebook and identifies the codes with the minimum Euclidean ($l_2$) distance, denoted as $\\hat{z}$. 2) the selected codes from decoder- codebook and $\\hat{z}$ is aggregated in an adaptive manner:\n$\\hat{z}_{res} = a(\\hat{z}, \\hat{z}^h)\\hat{z}^h + (1 - a(\\hat{z}, \\hat{z}^h))\\hat{z}$,  (2)\nwhere $a(\\hat{z}, \\hat{z}^h)$ denotes an adaptive factor, which can be computed as:\n$\u03b1(\\hat{z}, \\hat{z}^h) = \\frac{r \\frac{\\hat{z} \u00b7 \\hat{z}^h}{||\\hat{z}|| ||\\hat{z}^h||}}{2}$.      (3)\nThis adaptive aggregation controls the degree of the fusion of $\\hat{z}$ and $\\hat{z}^h$ in line with their similarity. A higher similarity score suggests the confidence to fuse the two vectors. 3) $\\hat{z}_{res}$ is treated as the final quantized vectors for the shape decoder."}, {"title": "Optimization", "content": "Learning on Codebooks. A contrastive learning based loss is devised to enhance the unique patterns that each code rep- resented for and ensure the codebook coverage for diverse characteristics. Specifically, for $\\hat{z} \\in Zr$, we consider all quantized representations except itself as negative examples. Furthermore, we are dedicated to the reduction of the dispar- ity between the two features, $\\hat{z} \\in Z$, and its corresponding $\\hat{z}$, with utmost precision, thereby ensuring minimal varia- tions among the transformed features. Based on this, the loss function consists of two parts: an internal loss $L_{internal}$ and an external loss $L_{external}$ as follows:\n$L_{internal} =  \\frac{1}{T} (\\sum^{T}_{i=0} \\sum_{j\u2260i}(1 -||\\hat{z_i} - \\hat{z_j}||_2))$,     (4)\n$L_{external} =  \\frac{1}{T} (\\sum^{T}_{i=0} ||\\hat{z_i} - \\hat{z^i}||_2)$.     (5)\nThe internal loss increases the distance between features, promoting a more uniform feature coverage. The external loss aligns the distribution of $\\tilde{Z'}$ with the decoder-codebook. To this end, the loss for learning codebooks can be summarized as follows:\n$L_{codebook} = L_{internal} + L_{external}$.      (6)\nLearning on Point Cloud Completion. We introduce Chamfer Distance (CD) to measure the differences between two point clouds. In detail, for the completed point cloud $P_{complete}$ and its ground truth $P_{GT}$, CD can be defined as follows:\n$L_{CD}(P_{complete}, P_{GT}) = \\frac{1}{|P_{complete}|} \\sum_{x \\in P_{complete}} min_{y \\in P_{GT}} ||x - y||_2 +  \\frac{1}{|P_{GT}|} \\sum_{y \\in P_{GT}} min_{x \\in P_{complete}} ||x - y||_2$.        (7)\nAdditionally, we adopt a loss function $L_{CD} (P_{coarse}, P_{GT}$ to guide the coarse-level completion. Overall the loss function for supervision in this study can be written as:\n$L = L_{CD}(P_{complete}, P_{GT}) + L_{CD}(P_{coarse}, P_{GT}) + L_{codebook}$.     (8)"}, {"title": "Experiments & Discussions", "content": "Experimental Settings\nDatasets. To demonstrate the effectiveness of the proposed DC-PCN, we conducted the evaluation on three widely used datasets. PCN, ShapeNet_Part, and ShapeNet34.\n\u2022 PCN (Yuan et al. 2018) consists of 28,974 shapes for training and 1,200 shapes for testing from 8 categories. The ground-truth point cloud was created by random sam- pling 16,384 points from each shape. Similarly, partial point clouds are produced by sampling incomplete point clouds from the corresponding shapes.\n\u2022 ShapeNet_Part contains 14,473 shapes and was split into 11,705 shapes for training and 2,768 for testing. These shapes belong to 13 categories. All shapes were normal- ized to the range of [-1,1] and centered at the origin for consistency.\n\u2022 ShapeNet34. To evaluate the method's generalizability, following the experimental setup in (Yu et al. 2023), we partitioned the ShapeNet dataset into two distinct subsets: 34 visible categories and 21 unseen categories. Within the visible categories, a subset of 100 objects per cate- gory was randomly selected to comprise the visible test set, ensuring the remaining objects were utilized for train- ing purposes. For the unseen categories, a comprehensive collection of 2,305 objects was designated as the test set.\nEvaluation Metrics. Following the existing practices, the evaluation metrics adopted in this study includes Chamfer Distance (CD) and F-score@%1 (Tatarchenko et al. 2019) between the predicted complete shape and the ground truth.\nImplementation Details. All experiments were conducted utilizing two RTX 2080Ti GPUs\nPerformance on PCN Dataset\nFor the evaluation on PCN, we follow the standard protocol and evaluation metric (l\u2081 chamfer distance) and F-Score as in existing studies (Yang et al. 2018; Groueix et al. 2018; Yuan et al. 2018; Tchapmi et al. 2019; Mitra et al. 2013; Xie et al. 2020; Wang, Ang Jr, and Lee 2020; Wen et al. 2021; Zhang, Yan, and Xiao 2020; Xiang et al. 2021; Tang et al. 2022; Zhou et al. 2022; Yu et al. 2021, 2023; Zhu et al. 2023; Zhu, Fan, and Weng 2024; Wu et al. 2024). The results are listed in Table 1. It is evident that our DC-PCN achieves the best CD-l1 and F-Score 0.850 in a majority of categories, indicating the superiority of DC-PCN compared with other methods. Notably, our DC-PCN improves the average CD-l\u2081 by 0.07, compared with the second-best method - AdaPoinTR.\nQualitative examples are visualized in Figure 4, where we contrast our outcomes with those of PCN (Yuan et al. 2018), SnowflakeNet (Xiang et al. 2021), AdaPoinTR (Yu et al. 2023) and HyperCD (Zhu, Fan, and Weng 2024) across two distinct categories: chair and lamp. These examples demon- strate our DC-PCN's completion results exhibit exceptional noise suppression and high-quality geometric details."}, {"title": "Performance on ShapeNet_Part Dataset", "content": "ShapeNet_Part assesses the performance in a multi-category and sparse context. Table 2 lists the quantitative results for all methods, measured by the CD-l2 metric. As HyperCD (Zhu, Fan, and Weng 2024), SVDFormer (Zhu et al. 2023), FSC (Wu et al. 2024) have not evaluated their method in this dataset, we compare our method with the rest point cloud completion methods in ShapeNet_Part. It can be observed that our method achieves the best CD-l2 in the majority of categories, as well as a best average CD-l2 5.59. In contrast, AdaPoinTR (Yu et al. 2023) and PoinTR (Yu et al. 2021) achieved 6.10 and 6.26, respectively, demonstrating the sig- nificant improvement offered by our method."}, {"title": "Performance on ShapeNet34 Dataset", "content": "Table 4 outlines the CD-l2 metric achieved by our method on both the seen categories and the unseen classes for ShapeNet34. It is anticipated that the average performance on the unseen categories would be inferior to that of the seen categories due to the inherent new knowledge of these ob- jects. Despite the substantial variations in shape geometry between the training and testing data, our DC-PCN demon- strates remarkable resilience, achieving the best result on the case of simple, medium and hard, moreover achieving the best CD-l2 among all the methods. Moreover, regarding the F-Score, our method achieves comparable performance with the state-of-the-art performance. This demonstrates the generalizability of our DC-PCN."}, {"title": "Performance on KITTI Dataset", "content": "To further demonstrate the generalizability of DC-PCN, we conducted evaluation on the KITTI dataset as shown in Table 3, which contains real-world data with diverse object appear- ances. with a pre-trained model on PCN cars. Our model achieves an MMD metric of 0.373, represent ing an improve- ment of 0.019 compared to the current best model- AdaPointr with an MMD of 0.392. Our DC-PCN shows the capabilities for addressing real-world point clouds."}, {"title": "Ablation Study", "content": "To evaluate the effectiveness of each proposed mechanism in our DC-PCN, we conducted ablation studies with the PCN benchmark. The results are outlined in Table 5. Further more, we visualize some of them in Figure 5\nDual-Codebook. We evaluated the effectiveness of the dual-codebook design, which represents the discrete feature distribution at shallow-level and deep-level. In detail, we compare the performance of Method A without the encoder- codebook and Method B with the encoder-codebook. It can be observed that the encoder-codebook (EC) improves the CD-l1 and F-score@1 by 0.05 and 0.005. In parallel, compar- ing Method C with the decoder-codebook (DC) and Method A without the decoder-codebook, the metrics indicate an im- provement 0.01 in terms of CD-l1. In parallel, comparing Method D with the dual-codebook deign and Method A with- out it, the metric indicates an improvement 0.06 regarding CD-l1. To this end, the experimental results demonstrate the necessity of the quantization in the feature space, high- lighting the important roles of our encoder-codebook and decoder-codebook.\nQuantized Information Exchanging. Further more, in order to verify the quantized information exchanging mech- anism (QIE), we consider the ablation setting as Method D, which lacks this mechanism, and Model E, which is with it. The results show an improvement of 0.01 in CD-l1 and 0.002 in F-Score@1, respectively. It forcefully demonstrates the effectiveness of the fusion mechanism. Further more, in order to verify the quantized information exchanging mecha- nism, we consider the ablation setting as Method D, which lacks this mechanism, and Model E, which is with it. The results show an improvement of 0.01 in CD-l1 and 0.002 in F-Score@1, respectively. It forcefully demonstrates the effectiveness of the fusion mechanism.\nA Shared Codebook In order to further verify the effec- tiveness of the decoder codebook and the fusion mechanism, we set up Group F experiments to make a codebook simul- taneously learn the discrete distributions of shallow features and deep features. Judging from the experimental results, the completion effect was significantly affected. Compared with the situation without the codebook, the CD-l1 loss in the experimental results increased by 0.15."}, {"title": "Limitations", "content": "The major limitation of DC-PCN is its reliance on the code- book design, which involves a series of hyperparameters. For example, the optimal size of the codebooks varies across different datasets and has the impacts on performance. The sizes of codebooks in the experiments are all the optimal val- ues obtained by changing different parameters. Future work should focus on exploring an adaptive codebook production mechanism in a data-driven manner."}, {"title": "Conclusion", "content": "We presented a point cloud completion network - DC-PCN for creating a consistent representation for sampled point clouds originating from the same 3D surface through a vector discretization strategy. DC-PCN introduces a dual-codebook design to quantize point cloud representations from a multi-level perspective with an encoder-codebook and a decoder-codebook. An information exchange mechanism further en- hances the information flow between the two codebooks. Comprehensive experiments demonstrate the state-of-the-art performance of our method."}]}