[{"title": "Conformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs): learning neural networks for designing neutral inclusions", "authors": ["Daehee Cho", "Hyeonmin Yun", "Jaeyong Lee", "Mikyoung Lim"], "abstract": "We focus on designing and solving the neutral inclusion problem via neural networks. The neutral\ninclusion problem has a long history in the theory of composite materials, and it is exceedingly\nchallenging to identify the precise condition that precipitates a general-shaped inclusion into a neutral\ninclusion. Physics-informed neural networks (PINNs) have recently become a highly successful\napproach to addressing both forward and inverse problems associated with partial differential\nequations. We found that traditional PINNs perform inadequately when applied to the inverse\nproblem of designing neutral inclusions with arbitrary shapes. In this study, we introduce a novel\napproach, Conformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs), which\nintegrates complex analysis techniques into PINNs. This method exhibits strong performance in\nsolving forward-inverse problems to construct neutral inclusions of arbitrary shapes in two dimensions,\nwhere the imperfect interface condition on the inclusion's boundary is modeled by training neural\nnetworks. Notably, we mathematically prove that training with a single linear field is sufficient to\nachieve neutrality for untrained linear fields in arbitrary directions, given a minor assumption. We\ndemonstrate that CoCo-PINNs offer enhanced performances in terms of credibility, consistency, and\nstability.", "sections": [{"title": "1 Introduction", "content": "Physics-informed neural networks (PINNs)(Raissi et al., 2019; Karniadakis et al., 2021) are specialized\nneural networks designed to solve partial differential equations (PDEs). Since their introduction, PINNS\nhave been successfully applied to a wide range of PDE-related problems (Cuomo et al., 2022; Hao et al.,\n2023; Wu et al., 2024). A significant advantage of using PINNs is their versatile applicability to different\ntypes of PDEs and their ability to deal with PDE parameters or initial/boundary constraints while solving\nforward problems (Akhound-Sadegh et al., 2023; Cho et al., 2023; Rathore et al., 2024; Lau et al., 2024).\nThe conventional approach to solving inverse problems with PINNs involves designing neural networks\nthat converge to the parameters or constraints to be reconstructed, which are typically modeled as\nconstants or functions. We refer to this methodology as classical PINNs. Numerous successful outcomes\nin solving inverse problems using PINNs have been reported. See, for example, Chen et al. (2020); Jagtap\net al. (2022); Haghighat et al. (2021). However, as the complexity of the PDE-based inverse problem\nincreases, the neural networks may require additional design to represent the parameters or constraints\naccurately. For instance, Pokkunuru et al. (2023) utilized Bayesian approach to design the loss function,\nGuo et al. (2022) used Monte Carlo approximation to compute the fractional derivatives, Xu et al. (2023)\nadopted multi-task learning method to weight losses and also presented the forward-inverse problem\ncombined neural networks, and Yuan et al. (2022) propose the auxiliary-PINNs to solve the forward and\ninverse problems of integro-differential equations. This increase in network complexity can significantly\nescalate computational difficulties and the volume of data necessary for training PINNs. Moreover, the\ndirect approach to approximate the reconstructing parameters by neural networks enables too much fluent\nrepresentation ability. This alludes to the fact that the conventional approach is inadequate depending\non the problems due to the intrinsic ill-posedness of inverse problems.\nIn this paper, we apply the PINNs' framework to address the inverse problem of designing neutral\ninclusions, a topic that will be elaborated below. The challenge of designing neutral inclusions falls within\nthe scope where traditional PINNs tend to perform inadequately. To overcome this limitation, we propose\nimprovements to the PINNs approach by incorporating mathematical analytical methods.\nInclusions with different material features from the\nbackground medium commonly cause perturbations in\napplied fields when they are inserted into the medium.\nProblems analyzing and manipulating the effects of inclusions\nhave gained significant attention due to their fundamental\nimportance in the modeling of composite materials,\nparticularly in light of rapid advancements and diverse\napplications of these materials. Specific inclusions, referred\nto as neutral inclusions, do not disturb linear fields (see\nFigure 1.2). The neutral inclusion problem has a long\nand established history in the theory of composite materials\n(Milton, 2002). Some of the most well-known examples\ninclude coated disks and spheres (Hashin, 1962; Hashin &\nShtrikman, 1962), as well as coated ellipses and ellipsoids (Grabovsky & Kohn, 1995; Kerker, 1975;\nSihvola, 1997). The primary motivation for studying neutral inclusions is to design reinforced or embedded\ncomposite materials in such a way that the stress field remains unchanged from that of the material\nwithout inclusions and avoids stress concentration. Extensive research has been conducted on neutral\ninclusions and related concepts, such as invisibility cloaking involving wave propagation, in fields including\nacoustics, elasticity, electromagnetic waves within the microwave range Al\u00f9 & Engheta (2005); Ammari\net al. (2013); Landy & Smith (2013); Liu et al. (2017); Zhou & Hu (2006, 2007); Zhou et al. (2008); Yuste\net al. (2018).\nDesigning neutral inclusions with general shapes presents an inherent challenge. In the context of\nthe conductivity problem, which is the focus of this paper, mathematical theory shows that only coated\nellipses and ellipsoids can maintain neutral properties for linear fields in all directions (Kang & Lee,\n2014; Kang et al., 2016; Milton & Serkov, 2001). In contrast, non-elliptical shapes can remain neutral\nfor just a single linear field direction (Jarczyk & Mityushev, 2012; Milton & Serkov, 2001). To address"}, {"title": "2 Inverse problem of neutral inclusions with imperfect\nconditions", "content": "We set x = (x1,x2) to be a two-dimensional vector in R2. On occasion, we regard R2 = C, whereby\nx = x1 + ix2 will be used. We assume that CC is a nonempty simply connected bounded domain with\nan analytic boundary (refer Appendix B). We assume the interior region \u03a9 has the constant conductivity\n\u03c3\u03b5 while the background medium R\u00b2 \\\u03a9 has the constant conductivity \u03c3m. We set \u03c3 = \u03c3\u03c7\u03a9 + 5mXR2\\N,\nwhere x is the characteristic function. We further assume that the boundary of \u0398\u03a9 is not perfectly\nbonding, resulting in a discontinuity in the potential. This discontinuity is represented by a nonnegative\nreal-valued function p on \u0398\u03a9, referred to as the interface parameter or interface function. Specifically,\nwe consider the following potential problem:\n$\\nabla \\cdot \\sigma \\nabla u = 0 \\quad \\text{in } \\mathbb{R}^2,$\n$p(u^+ \u2013 u^-) = \\sigma_m \\frac{\\partial u^+}{\\partial \\nu} = \\sigma_\\epsilon \\frac{\\partial u^-}{\\partial \\nu} \\quad \\text{on } \\partial \\Omega,$\n$(u \u2013 H)(x) = O(|x|^{-1}) \\quad \\text{as } |x| \\to \\infty,$\n(2.1)\nwhere H is an applied background potential and \u2202u/\u2202\u03bd denotes the normal derivative \u2202u/\u2202\u03bd = (\u2207u, N)\nwith the unit exterior normal vector N to \u0398\u03a9.\nHere, we define neutral inclusion to provide a clearer explanation.\nDefinition 1. We define \u03a9 as a neutral inclusion for the imperfect interface problem (2.1) if (u-H)(x) =\n0 for all x in the exterior region R2 \\ \u03a9 where H(x) is any arbitrary linear fields, i.e., H(x) = ax1 + bx2\nfor any a, b \u2208 R.\nWe explore the development of neural networks to find a specific interface function p that makes \u03a9\na neutral inclusion, given the inclusion \u03a9 along with the conductivities \u03c3\u03b5 and \u03c3m, while simultaneously\nproviding the forward solution u."}, {"title": "2.1 Series solution for the governing equation via conformal mapping", "content": "By the Riemann mapping theorem (see Appendix B), there exists a unique \u03b3 > 0 and conformal mapping\n\u03a8 from D = {w \u2208 C : |w| > \u03b3} onto C \\ \u03a9 such that \u03a8(\u221e) = \u221e, \u03a8\u2032(x) = 1, and\n$\\Psi(\\omega) = \\omega + \\alpha_o + \\frac{a_1}{w} + \\frac{a_2}{w^2}$\n(2.2)\nWe set po = lny. We use modified polar coordinates (\u03c1, \u03b8) \u2208 [\u03c1\u03bf, \u221e) \u00d7 [0, 2\u03c0) via z = \u03a8(w) = \u03a8(\u03b5\u03c1+\u03af\u03b8).\nOne can numerically compute the conformal mapping coefficients y and an for a given domain \u03a9 (Jung"}, {"title": "2.2 Series representation of the interface function", "content": "In this subsection, based on complex analysis, we present the series expansion formula for designing the\nCoCo-PINNs. We assume that the interface function p(x) is nonnegative, bounded, and continuous on\n\u0398\u03a9. By introducing a parametrization x(0) of \u0398\u03a9 with \u03b8\u2208 [0, 2\u03c0), the interface function admits a Fourier\nseries expansion with respect to 0:\np(x(0)) = \u03b1\u03bf + \u03a3ken (ak cos(kl) + bk sin(k0)), \u03b8\u2208 [0,2\u03c0),\nwhere ak and bk are real constant coefficients. In particular, we take x(0) = \u03a8(w), w = \u03b3\u03b5\u03af\u03b8, where I is\nthe conformal mapping in eq. (2.2). In this case, we have\np(w) := p(x(0)) = Re (po + p\u2081w + p2w\u00b2 +p3w\u00b3 +\u2026)\n= po + p1w + P1y2w\u22121 + p2w\u00b2 + P2y^w\u00af2 + \u00b7\u00b7\u00b7, |w| = \u03b3,\n(2.5)\nfor some complex-valued constants pk. Note that we can similarly express the boundary conditions in\neq. (2.1) in terms of the variable w, enabling us to effectively address these boundary conditions.\nWe further assume that p(w) is represented by a finite series, truncated at the wn-term for some\nn\u2208 N. Specifically, we define\np(n) (w) = Re (\u03a3k=0^n Pkwk).\n(2.6)\nAt this state, the reconstruction parameters are deduced to po,\u2026\u2026, pn, and this makes the inverse problem\nof determining the interface parameter over-determined by using the constraints (u - H)(x) \u2248 0 for many\nsample points exterior of \u03a9.\nA fundamental characteristic of inverse problems is that they are inherently ill-posed, and the existence\nor uniqueness of the inverse solution-the interface function in this paper is generally not guaranteed.\nWhen solving a minimization problem, neural networks may struggle if the problem admits multiple\nminimizers. In particular, the ability of neural networks to approximate a wide range of functions can\nlead them to converge to suboptimal solutions, corresponding to local minimizers of the loss function.\nAs a result, the classical approach in PINNs, which allows for flexible function representation, can be\nhighly sensitive to the initial parameterization. In contrast, the series expansion approach constrains\nthe solution to well-behaved functions, reducing sensitivity to the initial parameterization and ensuring\nthe regularity of the target function. Additionally, since the series approximation method requires fewer\nparameters, it can be treated as an over-determined problem, helping to mitigate the ill-posedness of\ninverse problems."}, {"title": "3 The proposed method: CoCo-PINNS", "content": "This section introduces the CoCo-PINNs, their advantages, and the mathematical reasoning behind why\nneutral inclusions designed by training remain effective even in untrained background fields. We begin\nwith the loss design corresponding to the imperfect interface problem eq. (2.1), whose solution exhibits\na discontinuity across \u0398\u03a9 due to the imposed boundary conditions. We denote the solutions inside and\noutside \u03a9 as uint and uext, respectively, and represent their neural networks' approximations as un and\nu. We named these solutions as trained forward solutions. We aim to train the interface function,\nrepresented by p(n) for the truncated series approximation and pnn for the fully connected neural network\napproximation. The method utilizing p(n) is referred to as CoCo-PINNs, while the approach using pnn\nto represent the interface function is called classical PINNs.\nint"}, {"title": "3.1 Model design for the forward-inverse problem", "content": "We utilize three sets of collocation points: \u03a9int, Next, and \u0398\u03a9, which are finite sets of points corresponding\nto the interior, exterior, and boundary of \u03a9, respectively, with a slight abuse of notation for \u0398\u03a9. We select\ncollocation points based on conformal mapping theory to handle PINNs in arbitrarily shaped domains, and\nprovide a detailed methodology for this selection in Appendix D.1. To address the boundary conditions\nfor z \u2208 \u0398\u03a9, we use xz = z + N and yz = z SN for the limit of the boundary from the exterior and\ninterior, respectively, with small 8 > 0, and unit normal vector N.\nThe loss functions corresponding to the governing equation and the design of a neutral inclusion are\ndefined as follows:\n$\\mathcal{L}^{PDE}_{Int} = \\nabla \\cdot \\sigma \\nabla u^{UNN}_{Int} \\quad \\text{for } x \\in \\Omega^{Int},$\n$\\mathcal{L}^{PDE}_{Ext} = \\nabla \\cdot \\sigma \\nabla u^{UNN}_{Ext} \\quad \\text{for } x \\in \\Omega^{Ext},$\n$\\mathcal{L}^{bd}_1 = p^{(n)} (\\frac{\\partial u^{UNN}_{Ext}(x_z)}{\\partial \\nu} - \\frac{\\partial u^{UNN}_{Int}(y_z)}{\\partial \\nu}) \\quad \\text{for } z \\in \\partial \\Omega,$\n$\\mathcal{L}^{bd}_2 = \\sigma_m \\frac{\\partial u^{UNN}_{Ext}(x_z)}{\\partial \\nu} - \\sigma_{\\epsilon} \\frac{\\partial u^{UNN}_{Int}(y_z)}{\\partial \\nu} \\quad \\text{for } z \\in \\partial \\Omega,$\n$\\mathcal{L}^{Neutral} = |u^{UNN}_{Ext} - H| \\quad \\text{for } x \\in \\Omega^{Ext}.$\n(3.5)\nwith du/dv = (\u2207u, N). In the case where we train using classical PINNs, we replace the interface function\np(n) with PNN. To enforce the non-negativity of the interface function, we introduce an additional loss\nfunction Lplus = max{0, -p}.\nBy combining all the loss functions with weight variables {wi}=1, we define the total loss by\n$\\mathcal{L}_{Total} = \\frac{w_1}{|\\Omega^{Int}|} \\sum_{x \\in \\Omega^{Int}} (\\mathcal{L}^{PDE}_{Int})^2 + \\frac{w_1}{|\\Omega^{Ext}|} \\sum_{x \\in \\Omega^{Ext}} (\\mathcal{L}^{PDE}_{Ext})^2 + \\frac{w_2}{|\\partial \\Omega|} \\sum_{z \\in \\partial \\Omega} (\\mathcal{L}^{bd}_1)^2 + \\frac{w_3}{|\\partial \\Omega|} \\sum_{z \\in \\partial \\Omega} (\\mathcal{L}^{bd}_2)^2 + \\frac{w_4}{|\\Omega^{Ext}|} \\sum_{x \\in \\Omega^{Ext}} (\\mathcal{L}^{Neutral})^2 + \\frac{w_5}{|\\partial \\Omega|} \\sum (\\mathcal{L}^{plus}).$\n(3.6)\nHere, A denotes the number of the elements in the set A. We then consider the following loss with\nregularization term:\n$\\mathcal{L}_{Reg} = \\begin{cases} \\mathcal{L}_{total} + \\epsilon (2\\pi \\gamma^2|p_0|^2 + 4\\pi \\gamma^2 \\sum_{k=1}^{n}(1 + k^2)|p_k|^2), & p = p^{(n)}, \\\\ \\mathcal{L}_{total} + \\epsilon||w_p||_F, & p = p_{NN}, \\end{cases}$\n(3.7)\nwhere wp represents the weights of the neural networks PNN, || . ||F is the Frobenius norm, and the\nW1,2(\u0398\u03a9)-norm is used for p(n), that is,\n$||p^{(n)}||^2_{W^{1,2}(\\partial \\Omega)} = ||p^{(n)}||^2_{L^{2}(\\partial \\Omega)} + ||\\nabla p^{(n)}||^2_{L^{2}(\\partial \\Omega)} = 2\\pi \\gamma^2|p_0|^2 + 4\\pi \\gamma^2 \\sum_{k=1}^{n}(1 + k^2)|p_k|^2.$\n(3.1)\n(3.2)\n(3.3)\n(3.4)"}, {"title": "3.2 Neutral inclusion effects for untrained linear fields", "content": "In this section, we briefly explain the reason that the CoCo-PINNs can yield the neutral inclusion effect\nfor untrained background fields. Since the governing equation eq. (2.1) is linear with respect to H, the\nfollowing trivially holds by the properties of linear PDEs:\nTheorem 3.1. Consider a domain, denoted by \u03a9, that is of arbitrary shape and whose boundary is given\nby an exterior conformal mapping \u03a8(w). If there exists an interface function p(x) that makes \u03a9 a neutral\ninclusion for the background field H(x) = x1 and x2, simultaneously, then \u03a9 is neutral also for all linear\nfields H(x) = ax1 + bx2 of arbitrary directions (a, b) \u2208 R2.\nBy Theorem 3.1, one can expect to find a function p(x) such that \u03a9 is neutral to all linear fields\nH by training with only two background fields, assuming such a p(x) exists. Although the existence\nof this function has not yet been theoretically verified, experiments in this paper with various shapes\ndemonstrate that, for given \u03a9, there exists a p(x) that produces the neutral inclusion effect, meaning\nthat the perturbation u H is negligible for all directions of H.\nRemark 1. In all examples, we successfully identified a p(x) with the neutral inclusion effect by training\nwith only a single H. These results can be explained by the following theorem.\nTheorem 3.2. Let & with an interface function p(x) be neutral for a single background field H. If the\nfirst rows of A1 and A2 given in eq. (2.4) are linearly independent, then \u03a9 is neutral also for all linear\nfields H(x) = ax1 + bx2 of arbitrary directions (a, b) \u2208 R2.\nThe proof of Theorem 3.2 can be found in Appendix C. As a future direction, it would be interesting\nto either prove that the first rows of A\u2081 and A2 are linearly independent for any \u03a9, or to find\ncounterexamples-namely, inclusions that are neutral in only one direction.\nRemark 2. According to the universal approximation theorem, for a given interface function p(x), the\nanalytic solution up on a bounded set and p on \u0434\u03a9 can be approximated by neural networks. Additionally,\nby the Fourier analysis, p(x) can be approximated by a truncated Fourier series p(n). In light of\nTheorem 3.2 and remark 1, we train using only a single linear field H."}, {"title": "4 Experiments", "content": "We present the successful outcomes for designing the neutral inclusions by using the CoCo-PINNS, as\nwell as the experiment results to verify the \"reliability\" in terms of credibility, consistency, and stability.\nCredibility indicates whether the trained forward solution closely approximates the analytic solution,\nwhich we assess by comparing the trained forward solution with the analytic solution derived in Choi &\nLim (2024). Consistency focuses on whether the interface functions obtained from the classical PINNs\nand the CoCo-PINNs converge to the same result for re-experiments under identical environments. It's\nworth noting that even if the neural networks succeed in fitting the forward solution and identifying the\ninterface parameter in a specific experiment, this success may only occur occasionally. Consistency is\naimed at determining whether the training outcomes are steady or merely the result of chance, and it\ncan be utilized as an indicator of the steadiness of the model. training model. Lastly, stability refers to\nthe sensitivity of a training model, examining how the model's output changes in response to variations\nin environments of PDES.\nThe inclusions with the shapes illustrated in Figure 4.1 will be used throughout this paper. Each\nshaped inclusion is defined by the conformal map given by eqs. (D.1) to (D.4) in Appendix D.3. We\nnamed the shapes of the inclusions 'square', 'fish', 'kite', and 'spike'.\nWe introduce the quantities to validate the credibility and\nthe neutral inclusion effect as follows:\n$\\begin{aligned} &||u_{NN}^{Ext} - u_p ||_{Cred} = \\frac{1}{|\\Omega^{Ext}|} \\sum_{x \\in \\Omega^{Ext}} |u_{NN} - u_p|^2, \\\\ &||u_{NN}^{Ext} - u_p ||_{\\infty} = \\max_{x \\in \\Omega^{Ext}} |u_{NN}^{Ext} - u_p |,\\\\ &|| u_p - H||_{P-Neutral} = \\frac{1}{|\\Omega^{Ext}|} \\sum_{x \\in \\Omega^{Ext}} |u_p - H|^2, \\end{aligned}$\n(4.1)\n(4.2)\n(4.3)"}, {"title": "4.1 Neutral inclusion", "content": "We present experimental results demonstrating the successful\nachievement of the neutral inclusion effect using CoCo-PINNs. For training, we use only a single\nbackground solution, H(x) = x1, and illustrate the neutral inclusion effects for three background solutions\nH(x) = x1, x2 and 2x1 X2; see Theorem 3.2 for a theoretical explanation.\nInclusions generally yield perturbations in the applied background fields. However, the domain \u03a9,\nwith the interface function p(x) trained from CoCo-PINNs, achieves the neutral inclusion effects across"}, {"title": "4.2 Credibility of classical PINNs and CoCo-PINNs", "content": "We investigate the credibility of the two\nmethods. We examine whether the exterior\npart of the trained forward solution uext\nmatches the analytic solution up. Recall that,\nwe denote up as the analytic solution when\ncoefficients of the interface function p are given\nby training. Once training is complete, CoCo-\nPINNs provide the expansion coefficients of the\ninterface function directly. For classical PINNs,\nwhere the interface function is represented\nby neural networks, we compute the Fourier\ncoefficients of PNN. We use the Fourier series expansion up to a sufficiently high order to ensure that\nthe difference between the neural network-designed interface function and its Fourier series is small (see\nFigure D.1 in Appendix D.4). We experiment for four inclusion shapes in Figure 4.1. Detailed experiment\nsettings are given in the Appendix D.\nTable 1 demonstrates CoCo-PINNs have superior performance in the shape of 'fish', 'kite', and 'spike'\ncompared to the classical PINNs. Although the credibility error for classical PINNS appears smaller\nthan that for CoCo-PINNs on the 'square' shape as shown in Table 1, the trained forward solution by\nclassical PINNs illustrates an exorbitant large deviation that does not coincide with the analytic solution\nderived from the inverse parameter result, as shown in Figure 4.3. This indicates that, despite its strong\nperformance in minimizing the loss function, the classical PINNs approach fails to effectively function as\na forward solver."}, {"title": "4.3 Consistency of classical PINNs and CoCo-PINNS", "content": "In this subsection, we examine whether repeated experiments consistently yield similar results.\nFigure 4.4 shows the interface functions after training the classical PINNs and CoCo-PINNs performed\nindependently multiple times. We repetitively test 30 times under the same condition and plot the\ninterface function pointwise along the boundary of the unit disk. The blue-dashed and red-bold lines"}, {"title": "4.4 Stability of classical PINNs and CoCo-PINNs", "content": "In this subsection, we assess the stability of the interface function along with the change of environments\nof PDEs. Since both classical PINNs and CoCo-PINNs are trained for a fixed domain and background\nfield H, we focus on stability with respect to different conductivities \u03c3\u03b5. In Figure 4.5, we present\nexperiments results obtained for \u03c3\u03b5 = 3,4,5,6,7 and om = 1, where the inclusion shapes are 'square',\n'fish', 'kite', and 'spike'. Recall that the ill-posed nature of inverse problems can lead to significant\ninstability, causing the inverse solution to exhibit large deviations in response to environmental changes\nor re-experimentation. Classical PINNs for neutral inclusions with imperfect conditions represent such\nan unstable case, as demonstrated in Figure 4.5. In contrast, our CoCo-PINNs are stable for repeated\nexperiments, and we confirmed that CoCo-PINNs are stable for slightly changed environments."}, {"title": "5 Conclusion", "content": "We focus on the inverse problem of identifying an imperfect function that makes a given simply connected\ninclusion a neutral inclusion. We introduce a novel approach of Conformal mapping Coordinates Physics-\nInformed Neural Networks (CoCo-PINNs) based on complex analysis and PDEs. Our proposed approach\nof CoCo-PINNs successively and simultaneously solves the forward and inverse problem much more\neffectively than the classical PINNs approach. While the classical PINNs approach may occasionally\ndemonstrate success in finding an imperfect function with a strong neutral inclusion effect, the reliability\nof this performance remains uncertain. In contrast, CoCo-PINNs present high credibility, consistency,\nand stability, with the additional advantage of being explainable through analytical results. The\npotential applications of this method extend to analyzing and manipulating the interaction of embedded\ninhomogeneities and surrounding media, such as finding inclusions having uniform fields in their interiors.\nSeveral questions remain, including the generalization to multiple inclusions and three-dimensional\nproblems, as well as proving the existence of an interface function that achieves neutrality."}, {"title": "A Notations", "content": "Table 4 provides the list of notations used throughout the paper."}, {"title": "BGeometric function theory", "content": "Geometric function theory is the research area of mathematics with the corresponding geometric\nproperties of analytic functions. One remarkable result is the Riemann mapping theorem. We briefly\nintroduce this theorem with related results."}, {"title": "B.1 Riemann mapping, Faber polynomials, and Grunsky coefficients", "content": "A connected open set in the complex plane is called a domain. We say that a domain is simply\nconnected if its complement C \\ \u03a9 is connected.\nTheorem B.1 (Riemann mapping theorem). If \u03a9 \u2286 C is a nonempty simply connected domain, then\nthere exists a conformal map from the unit ball B = {z \u2208 C : |z| < 1} onto \u03a9.\nWe assume that Cisa nonempty simply connected bounded domain. Then, by the Riemann\nmapping theorem, there exists a unique \u03b3 > 0 and conformal mapping \u03a8 from D = {w \u2208 C : [w] > \u03b3}\nonto C \\ such that (\u221e) = \u221e, \u03a8\u2032(x) = 1, and\n$\\Psi(\\omega) = \\omega + \\alpha_o + \\frac{a_1}{w} + \\frac{a_2}{w^2} +\\cdots.$\n(B.1)\nThe quantity y in eq. (B.1) is called the conformal radius of \u03a9. One can obtain eq. (B.1) by using Theorem\nB.1, the power series expansion of an analytic function and its reflection with respect to a circle; we refer\nto for instance Pommerenke (1992, Chapter 1.2) for the derivation.\nWe further assume that \u03a9 has an analytic boundary, that is, I can be conformally extended to\n{w \u2208 C : [w] > \u03b3 - \u20ac} for some e > 0.\nThe exterior conformal mapping \u03a8 in eq. (B.1) defines the Faber polynomials {Fm}m=1 by the relation\n$\\frac{\\Psi'(w)}{\\Psi(\\omega) \u2013 z} = \\sum_{m=0}^{\\infty} \\frac{F_m(z)}{w^{m+1}}, \\quad z\\in \\Omega, |w| > \\gamma.$\n(B.2)\nThe Faber polynomials {Fm} are monic polynomials of degree m, and their coefficients are uniquely\ndetermined by the coefficients {an}m=0 of \u03a8. In particular, one can determine Fm by the following\nrecursive relation:\n$F_{m+1}(z) = zF_m(z) \u2013 m a_m \\sum_{n=0}^{m} a_n F_{m-n}(z), m \\geq 0.$\n(B.3)"}, {"title": "C Proof of main theorem", "content": "As the Faber polynomials form a basis for complex analytic functions (see remarked in Remark 3), the\nbackground field H is an entire harmonic so that it is the real part of an entire analytic function. Hence,\nH satisfies\nH(z) = \u2211 Re[amFm(z)]\nm=1\nfor some complex coefficients {am}. Choi & Lim (2024) showed that, for some d > 0, the solution u to\nthe eq. (2.1) admits the expression\n$u(z) = \\begin{cases} Re\\Big[\\sum_{m=1}^{\\infty} a_m F_m(z) + \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty} \\beta_{mn} F_n(z)\\Big] & \\text{for } \\rho \\in [\\rho_0 \u2013 \\delta, \\rho_0],\\\\ Re\\Big[\\sum_{m=1}^{\\infty} a_m w^m + \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty} s_{mn} w^{-n}\\Big] & \\text{for } \\rho > \\rho_0 \\end{cases}.$\n$Re\\begin{cases}\\Big[\\sum_{m=1}^{\\infty} a_m F_m(z) + \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty} \\beta_{mn} F_n(z)\\Big] & \\text{for } \\rho \\in [\\rho_0 \u2013 \\delta, \\rho_0],\\\\ \\sum_{m=1}^{\\infty} a_m w^m + \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty} s_{mn} w^{-n} & \\text{for } \\rho > \\rho_0 \\end{cases}.$\n$Re\\begin{cases}\\Big[\\sum_{m=1}^{\\infty} a_m \\Psi(w)^m + \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty} \\beta_{mn} \\Psi(w)^n\\Big] & \\text{for } \\rho \\in [\\rho_0 \u2013 \\delta, \\rho_0],\\\\ \\sum_{m=1}^{\\infty} a_m w^m + \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty} s_{mn} w^{-n} & \\text{for } \\rho > \\rho_0 \\end{cases}.$\n$\\sum_{m=1}^{\\infty} a_m w^m + \\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty} s_{mn} w^{-n} $Re\u03a3\u03a3\nwhere the coefficients \u1e9emn and smn are depending on H, \u03c3and p. Recall that cmn are Grunsky coefficients.\nWe define semi-infinite matrices:\n$\\alpha = \\{@_{mmn}\\}_{m,n\\geq1}, \\beta = \\{@_{mn}\\}_{m,n\\geq1}, s = \\{@_{mn}\\}_{m,n\\geq1},$\nN = {ndmn}m,n\u22651, \u03b3*N = {y^ndmn}m,n\u22651, C = {Cmn}m,n\u22651,\n(C.1)\n(C.2)\nwhere dmn is the Kronecker delta function, and \u03c4\u2208 R.\nDefine \u03a8(\u03c1, \u03b8) = \u03a8(ep+i\u03b8) and denote the scale factor as h, that is,\n$h(\\rho, \\theta) = |\\frac{\\partial \\Psi}{\\partial \\rho}| = e^{\\rho}|\\Psi'|.$\nFor |w| = \u03b3, we have w = epo+id for \u03b8\u2208 [0,2\u03c0). We consider the Fourier series of h(po, \u03b8)p(\u03a8(w)) in \u03b8 :\n$h(\\rho_0, \\theta)p(\\Psi(w)) = \\sum_{n=-\\infty}^{\\infty} p_n w^n, |w| = \\gamma,$"}, {"title": "D Experimental Details", "content": "We denote the set of collocation points as\nMint", "follows": "Next and \u2202\u03a9 are the images of the exterior conformal map (w) under\nthe uniform grid of its restricted domain", "\u03a8(w)": "w = ep+\u03af\u03b8", "L": 0, "2\u03c0": ""}, "n\u0398\u03a9 = {\u03a8(w) : w = e\u03c1\u03bf+\u03af\u03b8, \u03b8 \u2208 (0,2\u03c0"]}, {"8N(z)": "z \u2208 \u2202\u03a9}\nwith a small 8 > 0 and a unit normal vector N to the boundary \u0398\u03a9. To select the interior points, we\nrecall that z \u2208 \u0398\u03a9 can be represented as z = \u03a8(epo"}]