{"title": "Conformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs): learning neural networks for designing neutral inclusions", "authors": ["Daehee Cho", "Hyeonmin Yun", "Jaeyong Lee", "Mikyoung Lim"], "abstract": "We focus on designing and solving the neutral inclusion problem via neural networks. The neutral inclusion problem has a long history in the theory of composite materials, and it is exceedingly challenging to identify the precise condition that precipitates a general-shaped inclusion into a neutral inclusion. Physics-informed neural networks (PINNs) have recently become a highly successful approach to addressing both forward and inverse problems associated with partial differential equations. We found that traditional PINNs perform inadequately when applied to the inverse problem of designing neutral inclusions with arbitrary shapes. In this study, we introduce a novel approach, Conformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs), which integrates complex analysis techniques into PINNs. This method exhibits strong performance in solving forward-inverse problems to construct neutral inclusions of arbitrary shapes in two dimensions, where the imperfect interface condition on the inclusion's boundary is modeled by training neural networks. Notably, we mathematically prove that training with a single linear field is sufficient to achieve neutrality for untrained linear fields in arbitrary directions, given a minor assumption. We demonstrate that CoCo-PINNs offer enhanced performances in terms of credibility, consistency, and stability.", "sections": [{"title": "Introduction", "content": "Physics-informed neural networks (PINNs)(Raissi et al., 2019; Karniadakis et al., 2021) are specialized neural networks designed to solve partial differential equations (PDEs). Since their introduction, PINNs have been successfully applied to a wide range of PDE-related problems (Cuomo et al., 2022; Hao et al., 2023; Wu et al., 2024). A significant advantage of using PINNs is their versatile applicability to different types of PDEs and their ability to deal with PDE parameters or initial/boundary constraints while solving forward problems (Akhound-Sadegh et al., 2023; Cho et al., 2023; Rathore et al., 2024; Lau et al., 2024). The conventional approach to solving inverse problems with PINNs involves designing neural networks that converge to the parameters or constraints to be reconstructed, which are typically modeled as constants or functions. We refer to this methodology as classical PINNs. Numerous successful outcomes in solving inverse problems using PINNs have been reported. See, for example, Chen et al. (2020); Jagtap et al. (2022); Haghighat et al. (2021). However, as the complexity of the PDE-based inverse problem increases, the neural networks may require additional design to represent the parameters or constraints accurately. For instance, Pokkunuru et al. (2023) utilized Bayesian approach to design the loss function, Guo et al. (2022) used Monte Carlo approximation to compute the fractional derivatives, Xu et al. (2023)adopted multi-task learning method to weight losses and also presented the forward-inverse problem combined neural networks, and Yuan et al. (2022) propose the auxiliary-PINNs to solve the forward and inverse problems of integro-differential equations. This increase in network complexity can significantly escalate computational difficulties and the volume of data necessary for training PINNs. Moreover, the direct approach to approximate the reconstructing parameters by neural networks enables too much fluent representation ability. This alludes to the fact that the conventional approach is inadequate depending on the problems due to the intrinsic ill-posedness of inverse problems.\nIn this paper, we apply the PINNs' framework to address the inverse problem of designing neutral inclusions, a topic that will be elaborated below. The challenge of designing neutral inclusions falls within the scope where traditional PINNs tend to perform inadequately. To overcome this limitation, we propose improvements to the PINNs approach by incorporating mathematical analytical methods.\nInclusions with different material features from the background medium commonly cause perturbations in applied fields when they are inserted into the medium. Problems analyzing and manipulating the effects of inclusions have gained significant attention due to their fundamental importance in the modeling of composite materials, particularly in light of rapid advancements and diverse applications of these materials. Specific inclusions, referred to as neutral inclusions, do not disturb linear fields. The neutral inclusion problem has a long and established history in the theory of composite materials (Milton, 2002). Some of the most well-known examples include coated disks and spheres (Hashin, 1962; Hashin &\nShtrikman, 1962), as well as coated ellipses and ellipsoids (Grabovsky & Kohn, 1995; Kerker, 1975; Sihvola, 1997). The primary motivation for studying neutral inclusions is to design reinforced or embedded composite materials in such a way that the stress field remains unchanged from that of the material without inclusions and avoids stress concentration. Extensive research has been conducted on neutral inclusions and related concepts, such as invisibility cloaking involving wave propagation, in fields including acoustics, elasticity, electromagnetic waves within the microwave range Al\u00f9 & Engheta (2005); Ammari et al. (2013); Landy & Smith (2013); Liu et al. (2017); Zhou & Hu (2006, 2007); Zhou et al. (2008); Yuste et al. (2018).\nDesigning neutral inclusions with general shapes presents an inherent challenge. In the context of the conductivity problem, which is the focus of this paper, mathematical theory shows that only coated ellipses and ellipsoids can maintain neutral properties for linear fields in all directions (Kang & Lee, 2014; Kang et al., 2016; Milton & Serkov, 2001). In contrast, non-elliptical shapes can remain neutral for just a single linear field direction (Jarczyk & Mityushev, 2012; Milton & Serkov, 2001). To address"}, {"title": "the difficulty of designing general shaped neutral inclusions, relaxed versions of the problem have been studied (Choi et al., 2023; Kang et al., 2022; Lim & Milton, 2020). Differently from the above examples, where perfectly bonding boundaries are assumed, imperfect interfaces introduce discontinuities in either the flux or potential boundary conditions in PDEs. Ru (1998) found interface parameters for typical inclusion shapes in two-dimensional elasticity for typical inclusion shapes. Benveniste & Miloh (1999) found neutral inclusions under a single linear field. The interface parameters, which characterize these discontinuities, may be non-constant functions defined along the boundaries of the inclusions so that, theoretically, the degree of freedom of the interface parameters is infinite. Hence, we expect to overcome the inherent challenge of designing neutral inclusions with general shapes by considering neutral inclusions with imperfect interfaces. A powerful technique for dealing with planar inclusion problems of general shapes has been to use conformal mappings and to define orthogonal curvilinear coordinates (Movchan & Serkov, 1997; Cherkaev et al., 2022; Ammari & Kang, 2004; Jung & Lim, 2021), where the existence of the conformal mapping for a simply connected bounded domain is mathematically guaranteed by the Riemann mapping theory. Using these coordinates, Kang & Li (2019); Choi & Lim (2024) constructed weakly neutral inclusions that yield zero coefficients for leading-order terms in PDEs solution expansions (also refer to Milton (2002); Choi et al. (2023); Lim & Milton (2020) for neutral inclusion problems using the conformal mapping technique). However, such asymptotic approaches cannot achieve complete neutrality within this framework. Moreover, the requirement for analytic asymptotic expressions poses limitations on the generalizability of this approach.\nIn this paper, by adopting a deep learning approach approach, we focus on precise values of the solution rather than asymptotic ones. Unlike the asymptotic approaches in Kang & Li (2019); Choi & Lim (2024), the proposed method does not rely on an analytical expansion formula and incorporates the actual solution values directly into the loss function design. More precisely, we introduce a novel forward-inverse PINN framework by combining complex analysis techniques into PINNs, namely Conformal mapping Coordinates Physics-Informed Neural Networks (CoCo-PINNs). We define the loss function to include the evaluations of the solutions at sample points exterior of inclusions. Furthermore, we leverage the conformal mapping to effectively sample collocation points for PDEs involving general shaped inclusions. We found that classical PINNs-which treat the interface parameters as functions approximated by neural networks-perform inadequately when applied to designing imperfect parameters for achieving neutrality. Instead, we propose training the Fourier series coefficients of the imperfect parameters, rather than approximating the function. We test the performance of our proposed method in finding the forward solution by using the analytical mathematical results for the forward solution presented in Choi & Lim (2024), where theoretical direct solutions are expressed as products of infinite dimensional matrices whose entries depends on the expansion coefficients of interface parameter. Additionally, we leverage these analytical results to explain that why it is possible to train the PINN for the neutral inclusion using only a single applied field (see Subsection 3.2).\nMany PINNs approaches to solving PDEs focus primarily on the forward problem and are typically validated through comparisons with numerical methods such as Finite Element Methods (FEM) and Boundary Element Methods (BEM), and others. In contrast, our problem addresses both forward and inverse problems simultaneously, adding complexity, especially in cases involving complex-shaped inclusions, and making it more challenging to achieve accurate forward solutions. Consequently, \"reliability\" becomes a critical factor when applying PINNs to our problem. The proposed CoCo-PINNs provide more accurate forward solutions, along with improved identification of the inverse parameters, compared to classical PINNs. They demonstrate greater consistency in repeated experiments and exhibit improved stability with respect to different conductivity values $\\sigma_{\\varepsilon}$. We conduct experiments to ensure the \"reliability\" of CoCo-PINNs by assessing the credibility, consistency, and stability.\nIt is noteworthy that by utilizing Fourier series expansions to represent the inverse parameters, CoCo- PINNs offer deeper analytic insights into these parameters, making the solutions not only more accurate but also more explainable. Furthermore, our method requires no training data for the neutral inclusion problem due to its unique structure, where constraints at exterior points effectively serve as data. An additional remarkable feature is that our proposed method has proven effective in identifying optimal inverse parameters that are valid for general first-order background fields including those not previously", "content": null}, {"title": "trained. This impressive result is supported by a rigorous mathematical analysis.\nIn summary, this paper contains the following contributions:\n\u2022 We developed a novel approach to PINNs, namely CoCo-PINNs, which have been shown to offer enhanced credibility, consistency, and stability compared to classical PINNs.\n\u2022 We have adopted the exterior conformal mapping in the PINNs to make it train the problem corresponding to the arbitrarily shaped domains.\n\u2022 Due to the nature of the neutral inclusion problem, it can be mathematically shown that once training with a given background solution $H(x) = x_1$ or $x_2$, a similar effect can be obtained for arbitrary linear background solutions $H(x) = ax_1 + bx_2$ for any $a, b \\in \\mathbb{R}$ (see section 3.2 for more details).", "content": null}, {"title": "Inverse problem of neutral inclusions with imperfect conditions", "content": "We set $x = (x_1,x_2)$ to be a two-dimensional vector in $\\mathbb{R}^2$. On occasion, we regard $\\mathbb{R}^2 = \\mathbb{C}$, whereby $x = x_1 + ix_2$ will be used. We assume that $\\mathbb{C} \\setminus \\overline{\\Omega}$ is a nonempty simply connected bounded domain with an analytic boundary (refer Appendix B). We assume the interior region $\\Omega$ has the constant conductivity $\\sigma_{\\varepsilon}$ while the background medium $\\mathbb{R}^2 \\setminus \\Omega$ has the constant conductivity $\\sigma_m$. We set $\\sigma = \\sigma_{\\varepsilon}\\chi_{\\Omega} + \\sigma_m\\chi_{\\mathbb{R}^2\\setminus \\Omega}$, where $\\chi$ is the characteristic function. We further assume that the boundary of $\\partial \\Omega$ is not perfectly bonding, resulting in a discontinuity in the potential. This discontinuity is represented by a nonnegative real-valued function $p$ on $\\partial \\Omega$, referred to as the interface parameter or interface function. Specifically, we consider the following potential problem:\n$\\nabla \\cdot \\sigma u = 0$ in $\\mathbb{R}^2$,\n$p(u^+ \u2013 u^-) = \\sigma_m \\frac{\\partial u^+}{\\partial \\nu} = \\sigma_{\\varepsilon} \\frac{\\partial u}{\\partial \\nu}$ on $\\partial \\Omega$,\n$(u \u2013 H)(x) = O(|x|^{-1})$ as $x\\rightarrow\\infty$,\n(2.1)\nwhere $H$ is an applied background potential and $\\partial u/\\partial \\nu$ denotes the normal derivative $\\partial u/\\partial \\nu = (\\nabla u, N)$ with the unit exterior normal vector $N$ to $\\partial \\Omega$.\nHere, we define neutral inclusion to provide a clearer explanation.\nDefinition 1. We define $\\Omega$ as a neutral inclusion for the imperfect interface problem (2.1) if $(u-H)(x) = 0$ for all $x$ in the exterior region $\\mathbb{R}^2 \\setminus \\Omega$ where $H(x)$ is any arbitrary linear fields, i.e., $H(x) = ax_1 + bx_2$ for any $a, b \\in \\mathbb{R}$.\nWe explore the development of neural networks to find a specific interface function $p$ that makes $\\Omega$ a neutral inclusion, given the inclusion $\\Omega$ along with the conductivities $\\sigma_{\\varepsilon}$ and $\\sigma_m$, while simultaneously providing the forward solution $u$."}, {"title": "Series solution for the governing equation via conformal mapping", "content": "By the Riemann mapping theorem (see Appendix B), there exists a unique $\\gamma > 0$ and conformal mapping $\\Psi$ from $\\mathbb{D} = {w \\in \\mathbb{C} : |w| > \\gamma}$ onto $\\mathbb{C} \\setminus \\Omega$ such that $\\Psi(\\infty) = \\infty$, $\\Psi'(\\infty) = 1$, and\n$\\Psi(\\omega) = \\omega + \\alpha_o + \\frac{a_1}{w} + \\frac{a_2}{w^2}$ (2.2)\nWe set $\\rho_o = ln\\gamma$. We use modified polar coordinates $(\\rho, \\theta) \\in [\\rho_o, \\infty) \\times [0, 2\\pi)$ via $z = \\Psi(w) = \\Psi(e^{\\rho +i\\theta})$. One can numerically compute the conformal mapping coefficients $\\gamma$ and $a_n$ for a given domain $\\Omega$ (Jung"}, {"title": "2.2 Series representation of the interface function", "content": "In this subsection, based on complex analysis, we present the series expansion formula for designing the CoCo-PINNs. We assume that the interface function $p(x)$ is nonnegative, bounded, and continuous on $\\partial \\Omega$. By introducing a parametrization $x(\\theta)$ of $\\partial \\Omega$ with $\\theta\\in [0, 2\\pi)$, the interface function admits a Fourier series expansion with respect to $\\theta$:\n$p(x(\\theta)) = \\alpha_\u03bf + \\sum_{k\\in \\mathbb{N}} (a_k cos(k\\theta) + b_k sin(k\\theta)), \\ \\theta\\in [0,2\\pi)$,\nwhere $a_k$ and $b_k$ are real constant coefficients. In particular, we take $x(\\theta) = \\Psi(w)$, $w = \\gamma e^{i\\theta}$, where $\\Psi$ is the conformal mapping in eq. (2.2). In this case, we have\n$\\widehat{p}(w) := p(x(\\theta)) = Re (p_o + p_1w + p_2w^2 +p_3w^3 +\u2026)\n$= p_o + p_1w + p_1\\gamma^2w^{-1} + p_2w^2 + p_2\\gamma^4w^{-2} + \u00b7\u00b7\u00b7, |w| = \\gamma,\n(2.5)\nfor some complex-valued constants $p_k$. Note that we can similarly express the boundary conditions in eq. (2.1) in terms of the variable $w$, enabling us to effectively address these boundary conditions.\nWe further assume that $p(w)$ is represented by a finite series, truncated at the $w^n$-term for some $n\\in \\mathbb{N}$. Specifically, we define\n$p^{(n)}(w) = Re (\\sum_{k=0} p_kw^k)$.\n(2.6)\nAt this state, the reconstruction parameters are deduced to $p_o,\u2026, p_n$, and this makes the inverse problem of determining the interface parameter over-determined by using the constraints $(u - H)(x) \\approx 0$ for many sample points exterior of $\\Omega$.\nA fundamental characteristic of inverse problems is that they are inherently ill-posed, and the existence or uniqueness of the inverse solution-the interface function in this paper is generally not guaranteed. When solving a minimization problem, neural networks may struggle if the problem admits multiple minimizers. In particular, the ability of neural networks to approximate a wide range of functions can lead them to converge to suboptimal solutions, corresponding to local minimizers of the loss function. As a result, the classical approach in PINNs, which allows for flexible function representation, can be highly sensitive to the initial parameterization. In contrast, the series expansion approach constrains the solution to well-behaved functions, reducing sensitivity to the initial parameterization and ensuring the regularity of the target function. Additionally, since the series approximation method requires fewer parameters, it can be treated as an over-determined problem, helping to mitigate the ill-posedness of inverse problems."}, {"title": "The proposed method: CoCo-PINNS", "content": "This section introduces the CoCo-PINNs, their advantages, and the mathematical reasoning behind why neutral inclusions designed by training remain effective even in untrained background fields. We begin with the loss design corresponding to the imperfect interface problem eq. (2.1), whose solution exhibits a discontinuity across $\\partial \\Omega$ due to the imposed boundary conditions. We denote the solutions inside and outside $\\Omega$ as $u^{int}$ and $u^{ext}$, respectively, and represent their neural networks' approximations as $u_{UNN}^{int}$ and $u_{UNN}^{ext}$. We named these solutions as trained forward solutions. We aim to train the interface function, represented by $p^{(n)}$ for the truncated series approximation and $p_{NN}$ for the fully connected neural network approximation. The method utilizing $p^{(n)}$ is referred to as CoCo-PINNs, while the approach using $p_{NN}$ to represent the interface function is called classical PINNs."}, {"title": "Model design for the forward-inverse problem", "content": "We utilize three sets of collocation points: $\\Omega^{int}$, $\\Omega^{ext}$, and $\\partial \\Omega$, which are finite sets of points corresponding to the interior, exterior, and boundary of $\\Omega$, respectively, with a slight abuse of notation for $\\partial \\Omega$. We select collocation points based on conformal mapping theory to handle PINNs in arbitrarily shaped domains, and provide a detailed methodology for this selection in Appendix D.1. To address the boundary conditions for $z \\in \\partial \\Omega$, we use $x_z = z + \\delta N$ and $y_z = z - \\delta N$ for the limit of the boundary from the exterior and interior, respectively, with small $\\delta > 0$, and unit normal vector $N$.\nThe loss functions corresponding to the governing equation and the design of a neutral inclusion are defined as follows:\n$L_{PDE}^{int} = |\\nabla \\cdot \\sigma u_{UNN}^{int}|^2$ for $x \\in \\Omega^{int}$,\n$L_{PDE}^{ext} = |\\nabla \\cdot \\sigma u_{UNN}^{ext}|^2$ for $x \\in \\Omega^{ext}$,\n$L_{Bdy}^{ext} = |p^{(n)} (u_{UNN}^{ext}(x_z) - u_{UNN}^{int}(y_z)) - \\sigma_m \\frac{\\partial u_{UNN}^{ext}}{\\partial \\nu}(x_z)|^2$ for $z \\in \\partial \\Omega$,\n$L_{Bdy}^{int} = |\\sigma_m \\frac{\\partial u_{UNN}^{ext}}{\\partial \\nu}(x_z) \u2013 \\sigma_{\\varepsilon} \\frac{\\partial u_{UNN}^{int}}{\\partial \\nu}(y_z)|^2$ for $z \\in \\partial \\Omega$,\n$L_{Neutral}^{ext} = |u_{UNN}^{ext} - H|^2$ for $x \\in \\Omega^{ext}$.\n(3.1)\n(3.2)\n(3.3)\n(3.4)\n(3.5)\nwith $\\partial u/\\partial \\nu = (\\nabla u, N)$. In the case where we train using classical PINNs, we replace the interface function $p^{(n)}$ with $P_{NN}$. To enforce the non-negativity of the interface function, we introduce an additional loss function $L_{plus} = max{0, -p}$.\nBy combining all the loss functions with weight variables {wi}i=1^15, we define the total loss by\n$L_{Total} = \\frac{w_1}{|\\Omega^{int}|} \\sum_{x \\in \\Omega^{int}} (L_{PDE}^{int})^2 + \\frac{w_2}{|\\Omega^{ext}|} \\sum_{x \\in \\Omega^{ext}} (L_{PDE}^{ext})^2 + \\frac{w_3}{|\\partial \\Omega|} \\sum_{z \\in \\partial \\Omega}(L_{Bdy}^{ext})^2 + \\frac{w_4}{|\\partial \\Omega|} \\sum_{z \\in \\partial \\Omega}(L_{Bdy}^{int})^2 + \\frac{w_5}{|\\Omega^{ext}|} \\sum_{x \\in \\Omega^{ext}}(L_{Neutral}^{ext})^2 + \\frac{w_6}{|\\Omega^{ext}|} \\sum_{x \\in \\Omega^{ext}}L_{plus}.$\n(3.6)\nHere, A denotes the number of the elements in the set A. We then consider the following loss with regularization term:\n$L_{Reg} = \\begin{cases}\nL_{total} + \\epsilon(2\\pi\\gamma^2|p_o|^2 + 4\\pi\\gamma^2 \\sum_{k=1}(1+k^2)|p_k|^2), & p = p^{(n)}, \\\\\nL_{total} + \\epsilon||w_p||_F, & p = p_{NN},\n\\end{cases}$\n(3.7)\nwhere $w_p$ represents the weights of the neural networks $P_{NN}$, $|| . ||_F$ is the Frobenius norm, and the $W^{1,2}(\\partial\\Omega)$-norm is used for $p^{(n)}$, that is,\n$||p^{(n)}||_{W^{1,2}(\\partial\\Omega)} = ||p^{(n)}||_{L^2(\\partial\\Omega)} + ||\\nabla p^{(n)}||_{L^2(\\partial\\Omega)} = 2\\pi\\gamma^2|p_o|^2 + 4\\pi\\gamma^2 \\sum_{k=1}(1 + k^2)|p_k|^2$."}, {"title": "This type of regularization is commonly used to address ill-posed problems. We used the loss in eq. (3.7)\nfor all experiments.\nCoCo-PINNs are designed using complex geometric function theory to address the interface problem.\nWhile classical PINNs rely on neural network approximations based on the universal approximation\ntheory, CoCo-PINNs utilize Fourier series expansion, which helps overcome the challenges of ill-\nposedness in the neutral inclusion inverse problems and ensures that the inverse solution remains smooth.\nAdditionally, this approach allows for the selection of initial coefficients of the interface function using\nmathematical results. The results of CoCo-PINNs can be explained by a solid mathematical foundation,\nas discussed in section 3.2. In section 4, we examine the advantages of CoCo-PINNs in terms of credibility,\nconsistency, and stability.", "content": null}, {"title": "Neutral inclusion effects for untrained linear fields", "content": "In this section, we briefly explain the reason that the CoCo-PINNs can yield the neutral inclusion effect for untrained background fields. Since the governing equation eq. (2.1) is linear with respect to H, the following trivially holds by the properties of linear PDEs:\nTheorem 3.1. Consider a domain, denoted by $\\Omega$, that is of arbitrary shape and whose boundary is given by an exterior conformal mapping $\\Psi(w)$. If there exists an interface function $p(x)$ that makes $\\Omega$ a neutral inclusion for the background field $H(x) = x_1$ and $x_2$, simultaneously, then $\\Omega$ is neutral also for all linear fields $H(x) = ax_1 + bx_2$ of arbitrary directions $(a, b) \\in \\mathbb{R}^2$.\nBy Theorem 3.1, one can expect to find a function $p(x)$ such that $\\Omega$ is neutral to all linear fields H by training with only two background fields, assuming such a $p(x)$ exists. Although the existence of this function has not yet been theoretically verified, experiments in this paper with various shapes demonstrate that, for given $\\Omega$, there exists a $p(x)$ that produces the neutral inclusion effect, meaning that the perturbation $u - H$ is negligible for all directions of H.\nRemark 1. In all examples, we successfully identified a $p(x)$ with the neutral inclusion effect by training with only a single H. These results can be explained by the following theorem.\nTheorem 3.2. Let $\\Omega$ with an interface function $p(x)$ be neutral for a single background field H. If the first rows of $A_1$ and $A_2$ given in eq. (2.4) are linearly independent, then $\\Omega$ is neutral also for all linear fields $H(x) = ax_1 + bx_2$ of arbitrary directions $(a, b) \\in \\mathbb{R}^2$.\nThe proof of Theorem 3.2 can be found in Appendix C. As a future direction, it would be interesting to either prove that the first rows of $A_1$ and $A_2$ are linearly independent for any $\\Omega$, or to find counterexamples-namely, inclusions that are neutral in only one direction.\nRemark 2. According to the universal approximation theorem, for a given interface function $p(x)$, the analytic solution $u_p$ on a bounded set and $p$ on $\\partial\\Omega$ can be approximated by neural networks. Additionally, by the Fourier analysis, $p(x)$ can be approximated by a truncated Fourier series $p^{(n)}$. In light of Theorem 3.2 and remark 1, we train using only a single linear field H."}, {"title": "the trained forward solution unn, obtained alongside with p(n) or pnn, is close to up, we can conclude\nthat the neural networks have successfully identified the interface function, ensuring that \u03a9 exhibits the\nneutral inclusion effect. This is because (unn - H) has small values in Next by the definition of the\nloss function LNeutral. In other words, if the interface function provided by neural networks makes both\n|| Up - UNN|| and LNeutral small, then this interface function is the desired one. However, if || Up - UNN ||\nis not small, it becomes unclear whether the neural networks have failed to solve the inverse problem or\nthe forward problem.\nThe field of AI research is currently facing significant challenges regarding the efficacy and\nexplainability of solutions generated by neural networks. Moreover, there is a pressing need to establish\n\"reliability\" credibility in these solutions. It is noteworthy that the trained forward solution unn deviates\nfrom the analytic solution up defined with pun in several examples, particularly in cases involving\ncomplicated-shaped inclusions (see Section 4.2). This discrepancy raises concerns about the \u201creliability\u201d\ncredibility of neural networks.", "content": null}, {"title": "Experiments", "content": "We present the successful outcomes for designing the neutral inclusions by using the CoCo-PINNs, as well as the experiment results to verify the \"reliability\" in terms of credibility, consistency, and stability. Credibility indicates whether the trained forward solution closely approximates the analytic solution, which we assess by comparing the trained forward solution with the analytic solution derived in Choi &\nLim (2024). Consistency focuses on whether the interface functions obtained from the classical PINNs\nand the CoCo-PINNs converge to the same result for re-experiments under identical environments. It's\nworth noting that even if the neural networks succeed in fitting the forward solution and identifying the\ninterface parameter in a specific experiment, this success may only occur occasionally. Consistency is\naimed at determining whether the training outcomes are steady or merely the result of chance, and it\ncan be utilized as an indicator of the steadiness of the model. training model. Lastly, stability refers to\nthe sensitivity of a training model, examining how the model's output changes in response to variations\nin environments of PDEs.\nThe inclusions with the shapes illustrated in Figure 4.1 will be used throughout this paper. Each\nshaped inclusion is defined by the conformal map given by eqs. (D.1) to (D.4) in Appendix D.3. We\nnamed the shapes of the inclusions 'square', 'fish', 'kite', and 'spike'.\nWe introduce the quantities to validate the credibility and\nthe neutral inclusion effect as follows:\n$||UNN^{ext} - U_p ||_{Cred} = \\frac{1}{|\\Omega^{ext}|} \\sum_{x \\in \\Omega^{ext}} |u^{ext}_{UNN} - U_p |^2$,\n$||U_{UNN}^{ext} - U_p||_{\\infty} = max_{x \\in \\Omega^{ext}}{|u^{ext}_{UNN} - U_p|}$,\n$||U_p - H||_{P-Neutral} = \\frac{1}{|\\Omega^{ext}|} \\sum_{x \\in \\Omega^{ext}} |U_p - H|^2$,\n(4.1)\n(4.2)\n(4.3)"}, {"title": "Neutral inclusion", "content": "We present experimental results demonstrating the successful achievement of the neutral inclusion effect using CoCo-PINNs. For training, we use only a single background solution, H(x) = x1, and illustrate the neutral inclusion effects for three background solutions H(x) = x1, x2 and 2x1 X2; see Theorem 3.2 for a theoretical explanation.\nInclusions generally yield perturbations in the applied background fields. However, the domain $\\Omega$, with the interface function p(x) trained from CoCo-PINNs, achieves the neutral inclusion effects across"}, {"title": "Credibility of classical PINNs and CoCo-PINNs", "content": "We investigate the credibility of the two methods. We examine whether the exterior part of the trained forward solution $u_{UNN}^{ext}$ matches the analytic solution $U_p$. Recall that, we denote $U_p$ as the analytic solution when coefficients of the interface function $p$ are given by training. Once training is complete, CoCo-PINNs provide the expansion coefficients of the interface function directly. For classical PINNs, where the interface function is represented by neural networks, we compute the Fourier coefficients of PNN. We use the Fourier series expansion up to a sufficiently high order to ensure that the difference between the neural network-designed interface function and its Fourier series is small (see Figure D.1 in Appendix D.4). We experiment for four inclusion shapes in Figure 4.1. Detailed experiment settings are given in the Appendix D.\nTable 1 demonstrates CoCo-PINNs have superior performance in the shape of 'fish', 'kite', and 'spike' compared to the classical PINNs. Although the credibility error for classical PINNs appears smaller than that for CoCo-PINNs on the 'square' shape as shown in Table 1, the trained forward solution by classical PINNs illustrates an exorbitant large deviation that does not coincide with the analytic solution derived from the inverse parameter result, as shown in Figure 4.3. This indicates that, despite its strong performance in minimizing the loss function, the classical PINNs approach fails to effectively function as a forward solver."}, {"title": "Consistency of classical PINNs and CoCo-PINNs", "content": "In this subsection, we examine whether repeated experiments consistently yield similar results.\nFigure 4.4 shows the interface functions after training the classical PINNs and CoCo-PINNs performed independently multiple times. We repetitively test 30 times under the same condition and plot the interface function pointwise along the boundary of the unit disk. The blue-dashed and red-bold lines"}, {"title": "Stability of classical PINNs and CoCo-PINNs", "content": "In this subsection, we assess the stability of the interface function along with the change of environments of PDEs. Since both classical PINNs and CoCo-PINNs are trained for a fixed domain $\\Omega$ and background field H, we focus on stability with respect to different conductivities $\\sigma_{\\varepsilon}$. In Figure 4.5, we present experiments results obtained for $\\sigma_{\\varepsilon} = 3,4,5,6,7$ and $\\sigma_m = 1$, where the inclusion shapes are 'square', 'fish', 'kite', and 'spike'. Recall that the ill-posed nature of inverse problems can lead to significant instability, causing the inverse solution to exhibit large deviations in response to environmental changes or re-experimentation. Classical PINNs for neutral inclusions with imperfect conditions represent such an unstable case, as demonstrated in Figure 4.5. In contrast, our CoCo-PINNs are stable for repeated experiments, and we confirmed that CoCo-PINNs are stable for slightly changed environments."}, {"title": "Conclusion", "content": "We focus on the inverse problem of identifying an imperfect function that makes a given simply connected inclusion a neutral inclusion. We introduce a novel approach of Conformal mapping Coordinates Physics- Informed Neural Networks (CoCo-PINNs) based on complex analysis and PDEs. Our proposed approach of CoCo-PINNs successively and simultaneously solves the forward and inverse problem much more effectively than the classical PINNs approach. While the classical PINNs approach may occasionally demonstrate success in finding an imperfect function with a strong neutral inclusion effect, the reliability of this performance remains uncertain. In contrast, CoCo-PINNs present high credibility, consistency, and stability, with the additional advantage of being explainable through analytical results. The potential applications of this method extend to analyzing and manipulating the interaction of embedded inhomogeneities and surrounding media, such as finding inclusions having uniform fields in their interiors. Several questions remain, including the generalization to multiple inclusions and three-dimensional problems, as well as proving the existence of an interface function that achieves neutrality."}, {"title": "Experimental Details", "content": null}, {"title": "Collocation points", "content": "We denote the set of collocation points as\n$\\Omega^{int"}, "Omega^{ext}, \\partial \\Omega, \\partial \\Omega^+, \\partial \\Omega^-$,\nfor interior, exterior, boundary, and the limit to the boundary from interior and exterior components, respectively. We define as follows: $\\Omega^{ext}$ and $\\"]}