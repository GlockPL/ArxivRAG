{"title": "ENHANCING SENTINEL-2 IMAGE RESOLUTION: EVALUATING ADVANCED TECHNIQUES BASED ON CONVOLUTIONAL AND GENERATIVE NEURAL NETWORKS", "authors": ["Patrick Kramer", "Alexander Steinhardt", "Barbara Pedretscher"], "abstract": "This paper investigates the enhancement of spatial resolution in Sentinel-2 bands that contain spectral information using advanced super-resolution techniques by a factor of 2. State-of-the-art CNN models are compared with enhanced GAN approaches in terms of quality and feasibility. Therefore, a representative dataset comprising Sentinel-2 low-resolution images and corresponding high-resolution aerial orthophotos is required. Literature study reveals no feasible dataset for the land type of interest (forests), for which reason an adequate dataset had to be generated in addition, accounting for accurate alignment and image source optimization. The results reveal that while CNN-based approaches produce satisfactory outcomes, they tend to yield blurry images. In contrast, GAN-based models not only provide clear and detailed images, but also demonstrate superior performance in terms of quantitative assessment, underlying the potential of the framework beyond the specific land type investigated.", "sections": [{"title": "Introduction", "content": "Satellite images in remote sensing are indispensable for numerous applications that monitor changes in the atmosphere and on the earth surface, both on land and at sea. Many of these applications make use of the data products of the Copernicus Sentinel-2 mission of the European Space Agency providing 13 multi-spectral bands ranging from the visible to the short-wave infrared spectrum [Drusch et al., 2012]. The resolution of the observation data enables a wide range of monitoring applications. However, the available resolution limits show severe restrictions when it comes to the precise detection of fine details and small objects. Currently, commercial satellites or aerial images with a correspondingly higher resolution are used as a workaround to overcome that specific drawback. In general, these products are of elevated costs, and thus not suitable for large-scale time-series analyses [Yoo et al., 2021]. To overcome"}, {"title": "Related Work and Methodological Foundations", "content": "In the field of SISR, where, as the name indicates, a single LR image of a scenery is used to construct an HR output, interpolation algorithms, such as bilinear, bicubic and spline interpolations, are widely applied because of their simple mathematical operations, which made them popular in early studies [Karwowska and Wierzbicki, 2022], [Kirkland, 2010], [Keys, 1981]. However, exactly their simplicity limits their ability to enhance image quality especially when complex structures are present. In general, they tend to blur the images as they cannot sufficiently reconstruct high-frequency details that are crucial for image sharpness. [Lanaras et al., 2018].\nA different approach in this context is pan-sharpening, which involves merging an HR panchromatic (PAN) image with an LR multi-spectral image to create an HR super-resolved image. The fused product incorporates both the high spatial resolution of the PAN image and the rich multi spectral information of the LR image. Although some of these methods have been successfully applied, cf. [Vivone et al., 2015], [Loncan et al., 2015] and [Zhou et al., 2020], they are not directly applicable to Sentinel-2, as no PAN band is available for that mission.\nThe latest and most innovative approach in the field of remote sensing resolution boosting is based on deep learning models an approach that appears promising for Sentinel-2 and thus forms the baseline for this work. A significant milestone in the SR domain was achieved with the Super-Resolution Convolutional Neural Network (SRCNN), presented in Dong et al. [2016a]. The architecture of this model consists of multiple convolutional layers that extract and reconstruct fine details from the LR image. Each layer performs a specific function, ranging from feature extraction to nonlinear mapping and reconstruction, to ensure that the final product is not only higher in resolution but also richer in details and textures. In Liebel and K\u00f6rner [2016], Liebel and K\u00f6rner demonstrated the effectiveness of SRCNN for satellite imagery, particularly for Sentinel-2 images, where they first upscale the LR image to the size of the HR counterpart by bicubic interpolation before applying the SRCNN network. In Fu et al. [2018], Fu et al. present an"}, {"title": "Materials and Methods", "content": "In literature, two prominent datasets for SR in the field of remote sensing exist: the WorldStrat Cornebise et al. [2022] and the SEN2VEN\u00b5S Michel et al. [2022] datasets. Whereas the first can not be used because of present cloudy patches, the second one does not provide images that are processed with the standard SEN2Cor Louis et al. [2016] pre-processing algorithm, which is essential for a subsequent SR methodology. For these reasons, an adequate dataset had to be created. As the focus of the SR empowering application of interest is related to forest land cover, tuples of S2 data and aerial orthophotos from representative regions of Carinthia, Austria, were used. At this point it is important to mention that the Sentinel-2 mission comprises two polar-orbiting satellites, S2A and S2B, which are offset by 180\u00b0 to each other, see Drusch et al. [2012]. This constellation enables a high revisit frequency of 5 days at the equator, which allows precise and continuous monitoring of changes on the earth's surface. Altogether, the mission includes 13 multi-spectral bands, ranging from visible and near-infrared to shortwave infrared, with different spatial resolutions. There are four bands with 10 \u00d7 10 m, six bands with 20 \u00d7 20 m and three bands with 60 \u00d7 60 m resolution Spoto et al. [2012]. Table 1 summaries the spectral and spatial characteristics of Sentinel-2, a distinction is made between two different product types: Level-1C, which represents the top-of-atmosphere reflectance (TOA), and Level-2A, which represents the bottom-of-atmosphere reflectance (BOA). Level-1C includes radiometric and geometric corrections. In"}, {"title": "Super Resolution Methods", "content": "For the scope of this work, two different CNN-based approaches are first examined as a baseline: the SRCNN Dong et al. [2016a] and the deeper residual network SRResNet Ledig et al. [2017]. Furthermore, as an alternative to the CNN-based approaches, the ESRGAN Wang et al. [2018] and Real-ESRGAN Wang et al. [2021] architectures are investigated in terms of their up-sampling capabilities. As mentioned in Chapter 2, a GAN consists of a generator and a discriminator, which are trained simultaneously through an adversarial process. The generator has the task of generating data, while the discriminator acts as a probabilistic classifier and assesses whether a generated image should be classified as real or fake Goodfellow et al. [2020]. Thus, the goal of the generator is to maximize the probability that the discriminator fails increasingly improving the quality of the generated data and therewith producing realistic images that cannot be identified as fakes anymore. This adversarial process results in high-quality super-resolved images that are rich in detail. The generator architecture for both, ESRGAN and Real-ESRGAN, is based on the work of Wang et al. Wang et al. [2018]. Figure 3 schematically illustrates its structure with the corresponding configurations for the kernel sizes (k), number of feature maps (n), and strides (s) for every convolutional layer. The fundamental element is built upon the Residual-in-Residual Dense Blocks (RRDB), which combine multi-level residual networks and dense connections without a BN layer. Another key component is the up-sampling block (UB), which uses a pixel shuffler to transfer values from the channel dimension to the height-width dimension, thereby enlarging the image. The number of RRDBs and UBs is variable a higher number of RRDBs makes the network more complex and enables better mapping, whereas increasing the number of UBs results in a higher scaling factor. In this work, $N_{RRDB}$ = 4 and $N_{UB}$ = 2 are used to upscale from a spatial resolution of 10 \u00d7 10 m to 5 \u00d7 5 m. The main difference between the\nabove mentioned GAN architectures lies in the discriminator network. The network used in ESRGAN is a classical classification network consisting of eight blocks, each comprising a convolution layer followed by BN and a leaky ReLU layer (LReLU). The final layers are two dense fully-connected layers that combine the extracted features to an one-dimensional vector. In contrast, Real-ESRGAN employs a U-Net discriminator with spectral normalization (SN) layers to enhance training stability and reduce the oversharp and disturbing artifacts introduced by the GAN training. Moreover, the U-Net architecture ensures that, instead of discriminating against global styles, accurate gradient feedback is provided for local textures. The structures of both discriminator architectures and their corresponding parametrization in terms of kernel, feature maps and strides, can be seen in Figure 4. The training and optimization process is divided into two phases, following the framework provided in Wang et al. [2018]. Initially, the generator is pre-trained to avoid undesired local optima using the expected L1 loss\n$L_{1} = E_{x_{i}}||G(x_{i}) - y||_{1}$,\nwhere $L_{1}$ defines the mean absolute error between the generated image $G(x_{i})$, with $x_{i}$ defining the $i^{th}$ input image, and the ground truth y. Next, the pre-trained generator is optimized together with the discriminator following an adversarial approach, where the weights of the generator are updated according to the total loss of the generator:\n$L_{G} = L_{percep} + \\lambda L_{a}^{Ra} + \\eta L_{1}$."}, {"title": "Image Quality Assessment", "content": "In this work we use PSNR, SSIM, and LPIPS metrics to assess the quality of the SR output. PSNR is a pixel-based metric, expressed in decibels, that represents the ratio between the maximum intensity value present in an image and the noise. PSNR is calculated according to\n$PSNR = 10 log_{10} \\frac{MAX^{2}}{MSE}$,\nwhere MSE refers to the Mean Squared Error and MAX to the maximum pixel intensity value of the image under investigation. The second pixel-based metric in the above list, SSIM, see e.g. Wang et al. [2004], measures the similarity between two images by comparing windows of size N \u00d7 N comprising structure information, luminance and contrast. In contrast to these pixel-metrics, the LPIPS metric however showed to be more suitable for the application of interest as it reflects human perception Zhang et al. [2018]. For computation, images pass a pre-trained deep neural network such as AlexNet, VGG or ResNet. The activation's, i.e. outputs at certain layers, are then extracted, and used to quantitatively assess the differences in terms of MSE between two images."}, {"title": "Experiments and Results", "content": "As mentioned in Section 3.2, the training of the GAN models is divided into two stages. First, the generator is pre-trained with an $L_{1}$ loss function and an initial learning rate of $2 \u00d7 10^{-4}$. The learning rate is halved if there is no improvement in the PSNR on the validation dataset after ten epochs, and training is terminated if there is no improvement in PSNR over 25 epochs. In the second training phase, the discriminator and generator networks are trained alternately. Here the weights of the loss terms in Equation 2) for the generator are set to $\\lambda = 5 \u00d7 10^{-3}$ and $\\eta = 1 \u00d7 10^{-2}$, and for the $L_{percep}$ summand, the five VGG19 layers are weighted following Wang et al. [2021], with {0.1, 0.1, 1, 1, 1}. The learning rates for the discriminator and generator are set to $1 \u00d7 10^{-4}$ and are halved every 500 epochs until the end of the training at 2000 epochs. Optimization is performed using the ADAM optimizer Kingma and Ba [2014] with the parameters $\u03b2_{1} = 0.90$ and $\u03b2_{2} = 0.99$. For comparison, all models were applied to the same test dataset and evaluated quantitatively and visually. At this point note that for an objective evaluation, the median and mean of the metrics were calculated and the models compared accordingly. The results are summarized in Table 4. For the visual comparison, Figure 5 gives representative results. Detailed inspection of the table and figure allow the conclusion that the baseline models SRCNN and SRResNet, which were optimized on pixel-based metrics, are not suitable for boosting Sentinel-2 resolution. They tend to produce very blurred images, partly due to a georeferential offset between the data sources and differences in the capturing technology, which is also reflected in the LPIPS metric. In contrast, the two GAN architectures generate much better qualitative results, where the Real-ESRGAN even exhibits the best LPIPS values as illustrated in Table 4. The adversarial training process and the use of the perceptual loss function allow the models to use more information during the learning phase as the pixel-based approaches in the CNN-frameworks, thereby capturing finer details and textures more effectively. This observation is confirmed by looking at the LPIPS metric values in Table 4, which show significantly better values for the GAN models compared to the baseline models. When comparing the GAN models, the Real-ESRGAN achieves better results in all three metrics due to its U-Net discriminator approach. However, evaluating models solely based on PSNR and SSIM reveals that SRResNet outperforms all other models. This is because CNN-based models optimize on a pixel-by-pixel basis, resulting in higher values for these metrics. Another important aspect that needs to be considered is the significant drop in performance of SR methods compared to the results of previous studies Liebel and K\u00f6rner [2016], Fu et al. [2018], Kawulok et al. [2019], Galar et al. [2019], Pineda et al. [2020], Salgueiro Romero et al. [2020], Gong et al. [2021]. This decrease is primarily due to the fact that these methods are often developed on synthetic datasets. In such datasets, a LR image is generated from a HR image through a degradation process, which is then used to train the methods. In contrast, the methods originally developed using synthetic datasets are tested on a real-world problem in this work."}, {"title": "Conclusion and Future Work", "content": "In this work, SR techniques were investigated to enhance the spatial resolution of the Sentinel-2 RGB bands from 10 x 10 m to 5\u00d75 m. Both, conventional CNN-based methods as well as more complex GAN structures were used and compared. The main objective was to evaluate their effectiveness in a real scenario, for which reason a specific dataset of Sentinel-2 images and aerial orthophotos was created under consideration of adequate pre-processing steps. The results reveal that the CNN-based methods are effective in up-scaling and enhancing image details, but produce blurry results as pixel-based loss functions are applied only for this type of models. In contrast, the GAN models, especially Real-ESRGAN, demonstrated superior ability in generating high quality images, which is also due to the LPIPS metric accounting for human perception. As evaluation reveals promising results in terms of feasibility and boosting capability, especially the GAN models are worth being investigated and optimized further in future. The primary focus here will be to increase the up-scaling factor from \u00d72 to \u00d74 in order to achieve a spatial resolution of 2.5 \u00d7 2.5 m. Furthermore, the authors will include the NIR band, as it may contain additional information not reflected in the RGB bands. To verify whether synthetically boosted satellite images are suitable to improve land use and land cover applications, the results will be tested on a real-world dataset and checked for classification results."}]}