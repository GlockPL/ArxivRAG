{"title": "QD-VMR: Query Debiasing with Contextual Understanding Enhancement for Video Moment Retrieval", "authors": ["Chenghua Gao", "Min Li", "Jianshuo Liu", "Junxing Ren", "Lin Chen", "Haoyu Liu", "Bo Meng", "Jitao Fu", "Wenwen Su"], "abstract": "Video Moment Retrieval (VMR) aims to retrieve relevant moments of an untrimmed video corresponding to the query. While cross-modal interaction approaches have shown progress in filtering out query-irrelevant information in videos, they assume the precise alignment between the query semantics and the corresponding video moments, potentially overlooking the misunderstanding of the natural language semantics. To address this challenge, we propose a novel model called QD-VMR, a query debiasing model with enhanced contextual understanding. Firstly, we leverage a Global Partial Aligner module via video clip and query features alignment and video-query contrastive learning to enhance the cross-modal understanding capabilities of the model. Subsequently, we employ a Query Debiasing Module to obtain debiased query features efficiently, and a Visual Enhancement module to refine the video features related to the query. Finally, we adopt the DETR structure to predict the possible target video moments. Through extensive evaluations of three benchmark datasets, QD-VMR achieves state-of-the-art performance, proving its potential to improve the accuracy of VMR. Further analytical experiments demonstrate the effectiveness of our proposed module. Our code will be released to facilitate future research.", "sections": [{"title": "Introduction", "content": "Video Moment Retrieval (VMR) is marked as a critical challenge in video understanding, aiming at locating specific moments from an untrimmed video with a given language query (Gao et al. 2017). Based on DETR (Carion et al. 2020), which leverages a transformer-based model to detect target objects in images, previous studies such as Moment-DETR (Lei, Berg, and Bansal 2021) and QD-DETR (Moon et al. 2023b) achieve the tasks of VMR and Highlight Detection (HD) by applying well-designed datasets and corresponding annotations. More recently, MESM (Liu et al. 2024b) is proposed to better align query-relevant video semantics at both frame-word and segment-sentence levels, significantly refreshing state-of-the-art VMR records.\nAlthough previous studies have made significant efforts to handle irrelevant semantic information in videos, they generally require the entire query semantics as the ground truth to guide the video semantics further, and then predict results. However, due to the rich semantic information of video elements and the model's insufficient and biased contextual understanding of the query, achieving precise video moment retrieval is challenging (Moon et al. 2023b; Liu et al. 2024b; Sun et al. 2024). No existing work, to our knowledge, has noticed the problem of video moment retrieval with query semantics ambiguous understanding, which brings uncertainty to the retrieval by the model. As shown in Fig. 1(a), given a video and the query \"Two buddies hang out above the cloudline after walking up to a viewpoint on a mountain\", if the model does not understand the context of the sentence and fails to focus on the word \"after\", it will return corresponding video segments related to \u201cTwo buddies hang out above the cloudline\u201d and \u201cwalking up to a viewpoint on a mountain\u201d. The latter is not the original intent of the query. Moreover, annotated queries that do not align perfectly with the target video segments worsen the overall performance of retrieval. As illustrated in Fig. 1(b), two buddies in the video are chatting and hanging out while filming vlogs. However, the given query lacks any semantic information related to chatting and vlogging, which may lead to semantic inaccuracies in moment retrieval. Solving this issue in VMR tasks requires the model to possess the ability to de-bias the query. Specifically, this includes an accurate understanding of the context of the query and video-query alignment.\nIn this paper, we propose a novel model named QD-VMR, the first query-debiasing video moment retrieval model with contextual understanding enhancement. First, we introduce a Global Partial Aligner (GPA) that aligns video clips with query features and incorporates video-query contrastive learning to enhance the model's cross-modal understanding capabilities. Then, to tackle the query bias, we propose a Query Debiasing Module (QDM) to acquire debiased query features, and we use a Visual Enhancement (VE) to filter query-irrelevant video features. Finally, we utilize the DETR structure (Carion et al. 2020) for result prediction. Extensive experiments on QVHighlights (Lei, Berg, and Bansal 2021), Charades-STA (Gao et al. 2017) and TACOS (Regneri et al. 2013) demonstrate that QD-VMR outperforms the SOTA methods. The contributions of this paper are summarized as follows:\n\u2022 We propose QD-VMR, which is the first video moment retrieval model that addresses the issue of query semantics ambiguous understanding, thereby improving the accuracy of video moment retrieval.\n\u2022 We design a Query Debiasing Module that generates debiased query representations by expanding the query and enhancing contextual understanding. Additionally, we introduce a Global Partial Aligner to ensure alignment between video and text modality, enabling the model to effectively discriminate the most relevant video clips.\n\u2022 We show the superiority of the proposed model by extensive experiments; Additionally, we transfer our key modules to a typical VMR approach, with the newly added parameters not exceeding 1M, resulting in significant improvements in all evaluation metrics."}, {"title": "Related Work", "content": "Video Moment Retrieval. Video Moment Retrieval (VMR) is first proposed by TALL (Gao et al. 2017), aiming to locate specific moments from an untrimmed video with a given language query. Various methods have been developed to tackle this task. 2D-TAN (Zhang et al. 2020b) covers diverse video moments with different lengths while representing their adjacent relations. Furthermore, with the advent of Moment-DETR (Lei, Berg, and Bansal 2021), the VMR task has evolved into a new end-to-end pipeline. First, the pre-trained models such as CLIP (Radford et al. 2021) and SlowFast (Chen and Jiang 2019) are developed to extract features from both the video and the query text separately (Li et al. 2020), thereby fusing the extracted features. Subsequently, a DETR-based (Carion et al. 2020) model is further proposed to predict the start and end times of the video moments. In recent years, models have emerged to focus more on exploring more suitable representations of video, query, and their alignment(Li et al. 2023). QD-DETR (Moon et al. 2023b) introduces a query-dependent video representation module, making moment predictions reliant on user queries. MH-DETR (Xu et al. 2023) introduces a pooling operation into the encoder and incorporates a cross-modality interaction module to fuse visual and query features. BM-DETR (Jung et al. 2023) adopts a contrastive approach, carefully utilizing the negative queries matched to other moments in the video. CG-DETR (Moon et al. 2023a) leverages an adaptive cross-attention layer with dummy tokens, preventing irrelevant video clips from being represented by the text query. TR-DETR (Sun et al. 2024) fully exploits the reciprocal relationship between VMR and HD. MESM (Liu et al. 2024b) achieves more balanced semantic alignment at both the frame-level and segment-sentence level. However, most DETR-based methods mainly address redundant features in the video modality while neglecting issues with the query.\nQuery Bias and Partial Relevance In the area of information retrieval, the following two issues are widely discussed. Query-Document Mismatching (Li, Xu et al. 2014; Liu et al. 2024a) occurs when the information needs expressed by the user in the query is not well-aligned with the content or context of the documents retrieved. This can happen due to vocabulary differences, context misunderstandings, or ambiguous queries. Partial Relevance (Hou et al. 2024) refers to a situation where the document partially meets the user's information need but is not wholly focused on the topic. These two issues persist in video moment retrieval, which we refer to as query bias and partial relevance problems. Currently, several methods provide solutions to the issue of partial relevance between query and video. Cross-modal interaction (Lei, Berg, and Bansal 2021; Xu et al. 2023; Sun et al. 2024; Hou et al. 2024) is widely used to reconstruct video features to obtain parts relevant to the query. However, the problem of query bias remains to be explored. To address this issue, we enhance the model's contextual understanding of the query and video-query alignment by expanding the query features and using the combined video and query features to jointly predict the masked words."}, {"title": "Overview", "content": "Video Moment Retrieval (VMR) aims to identify the most relevant moments from a video, based on the user's natural language query. Since existing methods may ignore the possible semantic bias in query texts, QD-VMR is specially designed to fill in the gap between insufficient contextual understanding of query and targeted video content. We represent the video and text features extracted by the pre-trained models as Fv and Ft. After mapping features into the same vector space, we adopt the Global Partial Aligner to get better alignment between video and query. Then, we leverage Query Debiasing and Visual Enhancement modules to further improve the quality of query and video features. Finally, a DETR-based network is utilized to encode the concatenated query and video features, and then decode the main objective, which is used to localize the center coordinate mc and corresponding duration mo (Moon et al. 2023a). The overall architecture of QD-VMR is shown in Fig. 2, and we present the detailed design of the scheme as follows."}, {"title": "Feature Extraction", "content": "We follow previous works (Li et al. 2020; Lei, Berg, and Bansal 2021; Sun et al. 2024) by leveraging pre-trained networks to extract raw visual and textual features as inputs. To enhance the model's ability to understand the semantic context of queries, we additionally use the Mask Language Model (MLM) to randomly mask one-third of the words in the query. For both complete queries and partially masked queries, we adopt the CLIP's text encoder module (Radford et al. 2021; Liu et al. 2024b) to extract complete query features \\(F_t = \\{f_i \\in \\mathbb{R}^{N \\times D_t} | i = 1, ..., K\\}\\), and partially-masked query features \\(F_w = \\{f_w \\in \\mathbb{R}^{N \\times D_t} | i = 1, ..., K\\}\\), where \\(K\\) is the pairs of video and query in the dataset, \\(N\\) and \\(D_t\\) are the length and dimension of textual features, respectively. Moreover, we extract clip-level features from the video given. Specifically, we first divide the video into non-overlapping clips of shorter duration (e.g., 2 seconds) and extract the clip-level video features by using pre-trained SlowFast (Feichtenhofer et al. 2019) and CLIP's visual encoder (Radford et al. 2021). Then, both Slowfast and CLIP features are concatenated as the overall video features \\(F_v = \\{f_i \\in \\mathbb{R}^{L \\times D_v} | i = 1, ..., K\\}\\), where \\(L\\) and \\(D_v\\) are the length and dimension of visual features, respectively. Following the approach in (Liu et al. 2022; Sun et al. 2024), we utilize a pre-trained audio feature extractor to extract audio features from the video, and subsequently concatenate the extracted audio and video features. More details are provided in the experiment section. Furthermore, we use trainable MLPs to map the visual and textual features to the same feature space, resulting in:\n\\[F_k = MLP_k(F_k),\\qquad(1)\\]\nwhere k can be replaced by v, t and w, respectively; \\(F_v\\) represents mapped video features \\(\\hat{F}_v = \\{\\hat{F}_i \\in \\mathbb{R}^{L \\times H} | i = 1, ..., K\\}\\), and both \\(\\hat{F}_t = \\{\\hat{F}_i \\in \\mathbb{R}^{N \\times H} | i = 1, ..., K\\}\\) and \\(\\hat{F}_w = \\{\\hat{F}_i \\in \\mathbb{R}^{N \\times H} | i = 1, ..., K\\}\\) altogether delineate mapped text features; \\(H\\) is dimension of hidden layers."}, {"title": "Global Partial Aligner", "content": "After mapping the encoded video and text features into the same vector space, the current method (Liu et al. 2024b) directly fuses both mapped features to enhance the parts of the video features related to the query. However, even though both video and text features encoded by Slowfast (Feichtenhofer et al. 2019) and CLIP (Carion et al. 2020) have the potential to unveil the aligned semantic information of images with single frame, the temporal dimension gap between video and text features still exists. Therefore, by analyzing the fused features, the gap may eventually render a poor understanding of temporal boundaries and events within the boundaries (Huang et al. 2024). To achieve better spatiotemporal alignment of features and perception of event boundaries, we leverage the idea from (Sun et al. 2024) and further design the Global Partial Aligner. Given the visual features \\(\\hat{F}_v\\) and textual features \\(\\hat{F}_t\\), the similarity between both features can be obtained as:\n\\[S = \\frac{{\\hat{F}_t{\\hat{F}_v}^T}}{{\\|\\hat{F}_t\\|\\|\\hat{F}_v\\|}}\\qquad(2)\\]\nThen, mean-pooling is applied in the last dimension to get S which measures the similarity between video clips and the entire query features. We use a Part-aware loss \\(L_P\\) to enhance the model's ability to discriminate the temporal boundaries of events for paired video and query.\n\\[L_P = \\sum_{i=1}^L (C_i log(S_i) + (1 - C_i) log(1 - S_i)),\\qquad (3)\\]\nwhere \\(S_i\\) is the similarity score between the i-th video clip and entire query features, and \\(C_i \\in \\{0, 1\\}\\) indicates whether the video clip is relevant to the query in the ground truth. In addition, we use the InfoNCE loss function to make the model align the semantic representations of the paired videos and queries:\n\\[L_G = \\frac{1}{B}\\sum_{i=1}^B log \\frac{exp(({\\hat{F}_v}^{Gi})^T ({\\hat{F}_t}^{Gi}) / \\tau)}{\\sum_{j=1}^B exp(({\\hat{F}_v}^{Gi})^T ({\\hat{F}_t}^{Gj}) / \\tau)},\\qquad (4)\\]\nwhere \\(\\hat{F}_v^{Gi}\\) denotes the average of the clip features for the i-th video in a batch B, \\(\\hat{F}_t^{Gi}\\) is the average of the word features for the i-th query, \\(\\tau \\in (0, 1]\\) is the temperature parameter. We calculate the average of \\(L_P\\) and \\(L_G\\) to obtain \\(L_{GPA}\\)."}, {"title": "Query Debiasing Module", "content": "Existing methods primarily focus on partial relevance issues within the video. In addition, accurate and complete query semantics are crucial for video moment retrieval. Unfortunately, existing works do not carefully consider query bias among the query texts. To address this issue, we propose a query debiasing module that consists of two main parts: query expansion and contextual understanding enhancement. Query expansion is designed to learn video semantic information not included in the query through relevant video segments. Assume the hyperparameter \\(N_e\\), and the learnable parameter \\(F_e = \\{f_i \\in \\mathbb{R}^{N_e \\times H} | i = 1, ..., K\\}\\), the expanded text features \\(\\hat{F}_t^e\\) can be further expressed as:\n\\[\\hat{F}_t^e = encoder(F_e||\\hat{F}_t),\\qquad(5)\\]\nwhere \"||\" denotes concatenation operator, \u201cencoder\u201d refers to the Transformer Encoder shown in DETR scheme. Moreover, we extract top \\(N_e\\) features from \\(\\hat{F}_t^e\\) and concatenate them with \\(\\hat{F}_t\\) to obtain the expanded text features \\(\\widetilde{F}_t\\).\nFurthermore, simply using query expansion to enrich the information in the query is not sufficient, as there is often a mismatching between the information expressed by the user in the query and the content or context of the video. For the partially masked queries obtained in the Feature Extraction subsection, we use contextual understanding enhancement to further mitigate the bias between the query and the video. It enhances the model's awareness of keywords by predicting them in the query based on the video and query context features within the global partial aligner. The enhanced feature of words \\(\\hat{F}_w^R\\) can be calculated as:\n\\[\\hat{F}_w^R = F_e + MLP \\bigg( softmax(\\frac{QA^T}{\\sqrt{d}})\\bigg),\\qquad(6)\\]\nwhere \\(Q = \\hat{F}_t\\) as query, \\(K = V = \\hat{F}_w\\) as key and value. Then we leverage MLP and softmax to calculate the probability distribution of the masked words as \\(P(\\hat{F}_w^R) \\in \\mathbb{R}^{N \\times L_{vocab}}\\), where \\(L_{vocab}\\) is the vocabulary size. Moreover, the contextual understanding enhancement loss function is expressed as:\n\\[L_w = - \\frac{1}{N} \\sum_{i=1}^N log P(\\hat{F}_w^R).\\qquad (7)\\]"}, {"title": "Visual Enhancement", "content": "Fused video-text features play an essential role in enhancing the prediction of temporal boundaries for semantic features in the video, which is relevant to the query text. Moment-DETR (Lei, Berg, and Bansal 2021) simply concatenates visual and textual features for modal interaction. MESM (Liu et al. 2024b) enhances the video modality at the frame-word level by emphasizing the query-relevant portions of frame-level features and suppressing irrelevant ones. Following TR-DETR (Sun et al. 2024), we use visual enhancement to fuse the video and text modalities. First, we perform row softmax normalization and column softmax normalization on the similarity matrix S to obtain the results \\(S_r\\) and \\(S_c\\), and then compute \\(\\hat{F}_{v2q}\\) and \\(\\hat{F}_{q2v}\\), which are the clip-level textual features and word-level visual features, respectively.\n\\[\\hat{F}_{v2q} = S_r\\hat{F}_t,\\qquad(8)\\]\n\\[\\hat{F}_{q2v} = S_cS\\hat{F}_t.\\qquad(9)\\]\nFinally, we obtain the enhanced video features \\(\\widetilde{F}_v\\) by using the following feature concatenation and linear projection:\n\\[\\widetilde{F}_v = Linear [\\hat{F}_v||\\hat{F}_{v2q}||\\hat{F} \\odot \\hat{F}_{v2q}||\\hat{F} \\odot \\hat{F}_{q2v}||\\hat{F}],\\qquad(10)\\]"}, {"title": "Transformer Encoder-Decoder", "content": "After concatenating the debiased text features \\(\\widetilde{F}_t\\) and the enhanced video features \\(\\widetilde{F}_v\\) to obtain the modality-aligned feature F, we adopt a DETR-structure module to predict the target video moments. It consists of a transformer encoder and a decoder. Specifically, the encoder is utilized to fuse the features F from the two modalities. In designing the decoder, we follow the approach of (Moon et al. 2023b; Liu et al. 2024b), incorporating learnable spans that represent the center coordinate mc and the duration mo."}, {"title": "Objective Losses", "content": "Following previous studies (Carion et al. 2020; Lei, Berg, and Bansal 2021; Liu et al. 2024b), we design the moment retrieval loss \\(L_{VMR}\\), which consists of three parts:\n\\[L_{VMR} = \\lambda_{L_1} \\|m - \\hat{m}\\|_{1} + \\lambda_{Diou} L_{Diou}(m, \\hat{m}) + \\lambda_{Lce} L_{Lce},\\qquad(11)\\]\nwhere m and \\(\\hat{m}\\) denote the prediction and ground-truth, respectively; \\(\\lambda_{L_1}\\), \\(\\lambda_{Diou}\\), \\(\\lambda_{Lce}\\) are the hyper-parameters; \\(L_{Diou}\\) is the GIoU loss computed by the predicted span and target span, \\(L_{Lce}\\) refers to the cross-entropy loss, which is used to classify the foreground or background (Carion et al. 2020). As a result, the final loss is calculated as:\n\\[L = \\lambda_{GPA} L_{GPA} + \\lambda_w L_w + L_{VMR},\\qquad (12)\\]\nwhere \\(\\lambda_{GPA}\\) and \\(\\lambda_w\\) are the hyper-parameters."}, {"title": "Experimental Settings", "content": "Datasets. We evaluate the proposed method on three widely used datasets. The QVHighlights dataset (Lei, Berg, and Bansal 2021) contains over 10,000 YouTube videos covering a wide range of topics, from daily life to news events. Performance on the test split is rigorously evaluated by uploading the results to the Codalab platform. The Charades-STA dataset (Gao et al. 2017) primarily consists of videos focused on indoor activities. Following (Moon et al. 2023b; Sun et al. 2024), we use 12,408 samples for training and reserve the remaining 3,720 samples for testing. The TACOS dataset (Regneri et al. 2013) is specifically centered around cooking-related activities.\nMetrics and Implementation Details. We use standard metrics from recent studies (Lei, Berg, and Bansal 2021; Moon et al. 2023b,a; Liu et al. 2024b; Sun et al. 2024) to evaluate performance. For QVHighlights, we calculate Recall@1 with IoU thresholds of 0.5, 0.7 and mAP with IoU thresholds of 0.5, 0.75. Additionally, we uniformly sample 10 IoU thresholds from 0.5, 0.95 to compute mAP and average these values as the final mAP metric for the VMR task. We also calculate mAP and HIT@1 for the HD task. For Charades-STA and TACOS, we use Recall@1 with IoU thresholds of 0.5, 0.7.\nTo validate the generality of our method, we conducted experiments using different pre-trained video feature extractors, with and without the inclusion of audio features as variables. Specifically, for QVHighlights, we used Slow-Fast+CLIP to extract video features, CLIP for text features, and PANN (Kong et al. 2020) for audio features. The training parameters were set to 200 epochs, a batch size of 256, and a learning rate of 2e-4. For Charades-STA, we used either SlowFast+CLIP or VGG to extract video features, and CLIP for text features, with training set to 200 epochs, a batch size of 32, and a learning rate of 2e-4. For TACOS, we used SlowFast+CLIP for video features and CLIP for text features, with the same hyperparameters as for Charades-STA. All experiments were conducted on Nvidia RTX 3090, A800 GPUs, and an Intel(R) Xeon(R) Gold 6226R CPU."}, {"title": "Performance Comparisons", "content": "Tables 1, 2, and 3 respectively present the performance comparison of our method against other leading approaches on the three datasets.\nIn Table 1, we compare the performance on the QVHighlights test set. We extract the CLIP+Slowfast features for the video and the CLIP features for the text. In the Src column, \"V\" indicates the use of video features only, while \"V+A\" indicates the use of both video and audio features. The results indicate that our method achieves the best performance on most evaluation metrics without using audio data. When audio data is incorporated, our method achieves the best performance across all evaluation metrics. This demonstrates that our method, based on understanding the query context, can better comprehend the combined features of video and audio. To further validate the effectiveness of our approach, we incorporated the key modules of our proposed method into QD-DETR (Moon et al. 2023b), a popular and widely used baseline. The results show that our method comprehensively improves the performance of QD-DETR with an increase of less than 1M parameters, particularly enhancing R1@0.5 by 2.9%.\nTable 2 presents the performance comparison of QD-VMR on the Charades-STA test set. We use Slowfast+CLIP and VGG as video feature extractors, and CLIP as the text feature extractor. The results demonstrate that our model also performs exceptionally well on the Charades-STA dataset. Particularly, when using VGG video features and CLIP text features, QD-VMR outperforms the current state-of-the-art methods across all metrics.\nWe also show a performance comparison of the TACOS dataset in Table 3. Interestingly, we find that QD-VMR performs less satisfactorily on this dataset. We speculate that this is due to the video content in the TACOS dataset being limited to the cooking domain, where QD-VMR's ability to capture subtle differences in actions and objects is relatively weaker in such a narrowly defined context."}, {"title": "Visualization", "content": "In Fig. 3, we visually compare the ground truth with the prediction of three leading methods and our approach on the QVHighlights val set. In the given query \"Two TV show hosts sit on a red couch and discuss the news\", our model demonstrates a superior understanding of the key-words \"Two TV show hosts\" and \"red couch\", providing highly accurate predictions. Similarly, for the query \"Two buddies hang out above the cloudline after walking up to a viewpoint on a mountain\", our model better comprehends the critical part \"Two buddies hang out\" rather than \"a viewpoint on a mountain\u201d. The visualization of the results proves that our model has a better understanding of the queries."}, {"title": "Ablation Study", "content": "We conduct extensive ablation experiments to demonstrate the effectiveness of the introduced modules, and the results are shown in Table 4. Settings (b) to (d) show that each component significantly improves performance compared to the baseline (a). Although (e) does not show improvement on all evaluation metrics, the overall QDM (f), which includes QE (d) and CUE (e), achieves a significant improvement compared to (a). Settings (g) and (h) demonstrate that video enhancement and query debiasing have significant performance improvements under the guidance of GPA. The performance decrease in setting (i) indicates that without GPA, video enhancement and query debiasing do not have a better optimization direction. Setting (j) shows that, under the guidance of GPA for video and text alignment, both video enhancement and query debiasing achieve a better understanding of the video and query semantics, resulting in the best performance. As shown in Fig. 4, we explore the impact of the weights of \\(L_{GPA}\\) and \\(L_w\\) separately. When analyzing \\(L_{GPA}\\), the weight \\(L_w\\) is set to 0, and vice versa. We find that the model performs better when \\(L_{GPA}\\) is set to 0.2 and \\(L_w\\) is set to 0.4, respectively."}, {"title": "Conclusion", "content": "In this paper, we propose QD-VMR, an efficient scheme to address the deep-rooted problem of ambiguity in query semantics under the VMR task. We conduct extensive comparison and ablation experiments to verify the effectiveness of the proposed scheme, and the results show that QD-VMR achieves SOTA results on multiple metrics. Moreover, the visualization of the experimental results demonstrates that our method enhances the model's ability to understand the contextual semantics of queries and accurately identify relevant moments. In the future, we also hope to explore potential methods to utilize unlabeled or weakly labeled data, as well as large-scale multimodal models with abundant prior knowledge to better perform downstream tasks. We believe that the issues and methods proposed in this work can provide valuable insights into related fields."}]}