{"title": "Less is More: Pre-Training Cross-Lingual Small-Scale Language Models with Cognitively-Plausible Curriculum Learning Strategies", "authors": ["Suchir Salhan", "Richard Diehl Martinez", "Z\u00e9bulon Goriely", "Paula Buttery"], "abstract": "Curriculum Learning has been a popular strategy to improve the cognitive plausibility of Small-Scale Language Models (SSLMs) in the BabyLM Challenge. However, it has not led to considerable improvements over non-curriculum models. We assess whether theoretical linguistic acquisition theories can be used to specify more fine-grained curriculum learning strategies, creating age-ordered corpora of Child-Directed Speech for four typologically distant language families to implement SSLMs and acquisition-inspired curricula cross-lingually. Comparing the success of three objective curricula (GROWING, INWARDS and MMM) that precisely replicate the predictions of acquisition theories on a standard SSLM architecture, we find fine-grained acquisition-inspired curricula can outperform non-curriculum baselines and performance benefits of curricula strategies in SSLMs can be derived by specifying fine-grained language-specific curricula that precisely replicate language acquisition theories.", "sections": [{"title": "1 Introduction", "content": "Curriculum Learning (CL) has emerged as a promising method to improve the cognitive plausibility of Small-Scale Language Models (SSLMs) in the first BabyLM Challenge (Warstadt et al., 2023), as a way to gradually introduce more complex linguistic phenomena into the model later in training in a manner that is similar to human language acquisition. Cognitively-inspired SSLMs are models trained on corpora that approximate the volume and nature of input that a first-language learner can expect to receive during language acquisition. These have been found to perform competitively against LLMs in English (Huebner et al., 2021). CL strategies implemented in the BabyLM Challenge either specified a static measure of linguistic complexity, such as lexical frequency (Borazjanizadeh, 2023), sorted datasets according to difficulty (Opper et al., 2023), or gradually increased vocabulary sizes (Edman and Bylinina, 2023). While the majority of these strategies did not yield consistent improvements over non-curriculum learning baselines (Warstadt et al., 2023), linguistic theory suggests that children naturally focus on input that is neither too simple nor too difficult but at the right level of challenge for learning (Biberauer, 2019; Bosch, 2023). This is known as the \u201cGoldilocks Effect\", which is a form of self-selecting curriculum learning that appears to naturally occur in first language (L1) acquisition. This raises the question of whether acquisition theories can provide insights into more effective curriculum learning strategies for SSLMs, and lead to more consistent benefits of CL strategies.\nOur work assesses whether language acquisition theories can provide us with better heuristics for good curriculum learning strategies to train SSLMs. We compare contrastive acquisition theories for their success when informing objective curriculum learning strategies on a standard architecture (Diehl Martinez et al., 2023). We train SSLMs with three new objective curricula called GROWING, INWARDS and MMM, each replicating the developmental sequences of contemporary acquisition theories that first-language monolingual learners are theorised to follow in the earliest stages of acquisition cross-linguistically. In practice, these curricula modify the standard masked language modelling objective in BabyBERTa-style models by varying the order and the sequence of masking using different tagsets to simulate different language acquisition theories.\nThe acquisition models specify different cross-lingual and language-specific developmental sequences that learners appear to follow in first language acquisition, which has not been implemented or evaluated in the context of Deep Learning. The multilingual focus of the acquisition models is a goal strongly aligned with the spirit of the BabyLM Shared Task. We train SSLMs with these objective curricula for four typologically distant language families: Romance (French), Germanic (German), Japonic (Japanese) and Sino-Tibetan (Chinese). We introduce new age-ordered corpora of Child-Directed Speech (CDS) for these languages and select languages for pre-training based on the quantity of CDS that can be used to train SSLMs using similar volumes of data that learners can utilise in first language acquisition. We evaluate these SSLMs on syntactic minimal pair datasets. We find benefits of the cognitively-inspired objective curricula cross-linguistically, however different strategies lead to better performance for certain languages, particularly finer-grained language-specific versions of the MMM objective. Acquisition-inspired objective curricula can obtain comparable performance on minimal pair evaluation datasets to LLMs, despite requiring approximately 25X fewer parameters and 6,000X fewer words.\""}, {"title": "2 Background", "content": "We survey Curriculum Learning (CL) strategies used in the 1st BabyLM Challenge Section 2.1 and contrastive models of syntactic acquisition that are utilised to replicate cross-lingual developmental sequences for implementing more cognitively plausible pre-training in SSLMs in Section 2.2."}, {"title": "2.1 Curriculum Learning Strategies for Pre-training on Developmentally Plausible Corpora", "content": "While some SSLMs that utilised CL strategies outperformed the official BabyLM baselines, no CL strategies led to consistent or uniform improvements compared to stronger non-curriculum models. Many submissions for the inaugural BabyLM Challenge utilised Curriculum Learning on a small-scale masked language model architecture trained on a 5 million (5M) word corpus called BABY-BERTA (Huebner et al., 2021), based on a Transformer Language Model ROBERTA (Liu et al., 2019) with 15\u00d7 fewer parameters, which displayed comparable grammatical capabilities to ROBERTA. In general, CL strategies, like using a pre-defined static difficulty assessment based on linguistic criteria like syntax dependency tree depth (Oba et al., 2023) or ranking sentences according to surprisal (Chobey et al., 2023) or length (DeBenedetto, 2023) or other measures of difficulty (Opper et al., 2023), showed little improvement over non-CL baselines. Diehl Martinez et al. (2023) introduce Curriculum Learning for Infant-Inspired Model Building (CLIMB), which incorporates three CL strategies into BabyBERTa pre-training that each dynamically increase the difficulty of the language modelling task throughout training. CLIMB's vocabulary curriculum constrains the Transformer vocabulary in the initial stages of training by dynamically mask out vocabulary units over training. CLIMB's data curriculum varies the order of training instances based on infant-inspired expectations and the learning behaviour of the model, enabling dynamic sampling of training data according to a difficulty function. CLIMB's objective curriculum combines the masked language modelling task, used in RoBERTa (Liu et al., 2019) and the BabyBERTa model (Huebner et al., 2021), with coarse-grained word class prediction to reinforce linguistic generalisation capabilities. This provides functionality to change the objective function at specified discrete training steps. The objective curricula modifies the Masked Language Modelling (MLM) objective, which is the standard \u201cdenoising\" objective for Pre-trained Language Models, like ROBERTA and BABYBERTA. Both models use a random token masking strategy, applying a fixed masking ratio \\( a \\) to mask different contexts selected randomly with a probability \\( P_i \\). Diehl Martinez et al. (2023) introduce two objective curricula defined using 'curriculum units' of Universal Part of Speech (UPOS) tags. The first objective classifies [MASK] to one of [VERB, NOUN, OTHER], while the second objective classifies [MASK] to one of the 10 UPOS tags. CLIMB's objective curricula, following the submission guidelines of the 1st BabyLM Challenge, are performed using an unsupervised part-of-speech (POS) tagger. They additionally tuned the vocabulary and model size of BabyBERTa, resulting in a model that outperformed the official baselines for the first BabyLM Challenge. CLIMB's curriculum learning strategies outperformed the official baseline but the accuracy of CL-strategies was comparable to the stronger BabyBERTa-style baseline introduced by the authors. We add new cognitively-plausible objective curricula, as an extension to the original CLIMB submission and CLIMB's improved BABYBERTA-style as baselines.\""}, {"title": "2.2 Acquisition Models in Deep Learning: Three Models", "content": "To assess whether using acquisition theories can be used to formulate better-performing CL strategies, we consider three recent language acquisition models that are amenable to Deep Learning implementation, as they specify developmental sequences that can be replicated as CL strategies in SSLMs. Based on careful linguistic analysis of universal and language-specific patterns in the utterances produced by learners cross-linguistically at different stages of acquisition, linguists have formalised strict (universal or non-language-specific) or weak (language-specific) orders of syntactic categories that are sequentially acquired. Since these acquisition models have been formulated based on linguistic analysis of multilingual acquisition data, we consider whether the CL strategies that precisely replicate these models can inform better-performing curriculum learning strategies cross-lingually. This leads us to train SSLMs with these objective curricula beyond English. As schematised in Figure 1, we can precisely replicate these developmental sequences as stages of SSLM pre-training, defined as proportions of training steps. We implement three contemporary cross-lingual models of syntactic acquisition:\n1. GROWING: Bottom-up maturational approaches to language acquisition (Rizzi, 1993; Radford, 1990), including the \u201cGrowing Trees Hypothesis", "INWARDS": "Bosch (2023) introduces the predictions of a generalised inward-growing maturational proposal (INWARDS), building on evidence from Heim and Wiltschko (2021) of early acquisition of \u201cdiscourse", "MMM)": "Neo-Emergentism predicts developmental stages in language acquisition that show increasing categorial granularity, taking a language-specific, or non-maturational, approach towards syntactic acquisition (Biberauer and Roberts, 2015). The general universal prediction of one neo-emergent model called Maximise Minimal Means (MMM) is that all learners, irrespective of"}, {"title": "3 Dataset", "content": "3.1 Training Corpora: MAO-CHILDES\nWe collect a training corpus of Age-ordered Child-Directed Speech (CDS) for four languages (French, German, Japanese and Chinese), in addition to the English Age-Ordered-CHILDES (AO-CHILDES) corpus (Huebner and Willits, 2021) used in the BabyLM Challenge, to assess the benefits of the acquisition-inspired curricula beyond English compared to non-curriculum SSLMs. MAO-CHILDES is developed from the Child Language Data Exchange System (CHILDES) (MacWhinney, 2000), which consists of in-home recordings of casual speech from caregivers to children and in-lab activities such as play, conversation and book reading directed towards first language learners for several languages. We make our training corpus available on HuggingFace. The distribution of CHILDES data beyond English is a practical challenge for extending the BabyLM Challenge beyond English. Following Huebner and Willits (2021), utterances from children and child-directed speech (CDS) produced by caregivers, and other interlocuters, to children over the age of 6;0 are disregarded, leaving CDS produced by caregivers to children less than 6;0 which is sorted using the meta-data of the age of the learner in the CHILDES database."}, {"title": "3.2 Evaluation Datasets", "content": "To assess the success of three objective curricula (GROWING, INWARDS and MMM) that precisely replicate the predictions of the acquisition theories in Section 2.2 on a standard SSLM architecture in a multilingual setting, we extend the evaluation pipeline of the BabyLM Challenge. This consists of syntactic evaluation datasets like BL\u0130MP (Warstadt et al., 2020) composed of minimal pairs of grammatical and ungrammatical sentences for language-specific syntactic phenomena. We use the following minimal pairs datasets to evaluate the objective curricula for the four languages in MAO-CHILDES:\n1. CLAMS (French and German): The Cross-Lingual Syntactic Evaluation of Word Prediction Models (CLAMS) (Mueller et al., 2020) generates minimal pair datasets which we use for French and German using Attribute-Varying Grammars. The dataset assesses grammaticality in Simple Agreement, VP coordination, and across \"interveners\" in S-V agreement (subject/object relative clause or across a Prepositional Phrase).\n2. JBLIMP (Japanese): JBLIMP (Someya and Oseki, 2023) is a minimal pairs dataset for targeted syntactic evaluation of Japanese. It consists of 331 minimal pairs of syntactic acceptability judgements curated from Japanese syntax articles in the Journal of East Asian Linguistics.\n3. SLING (Chinese): SLING (Song et al., 2022) is a 38K minimal sentence pair dataset derived by applying syntactic and lexical transformations to Chinese Treebank 9.0, aiming to improve on the limitations of an earlier dataset called CLiMP (Xiang et al., 2021), which had a lack of diversity in the vocabulary to generate minimal pair templates.\nDue to the small size of the JBLIMP minimal pairs dataset, we follow Someya and Oseki (2023)'s recommendation to compute accuracy using a SLOR score to mitigate the confounding effects of lexical frequencies and sentence lengths, which is defined as follows:\n\\[SLOR(X) = \\frac{logp_m(X) - logp_u(X)}{|X|}\\]\nwhere \\( p_m(X) \\) is the probability of a sentence for a Language Model and is the unigram probability of the sentence, estimated for each subword in the training corpus. Accuracy calculations for other languages follows dataset guidance to use unnormalised log-probabilities."}, {"title": "3.3 Universal POS Tagging", "content": "To define fine-grained objective curricula that perform masked language modelling with different subsets of syntactic and semantic tags for a specified proportion of training steps, we have to annotate child-directed speech corpora with Universal POS tags using an off-the-shelf SpaCy multilingual POS tagger. The distribution of POS tags in MAO-CHILDES contains a high proportion of Nouns, whereas Verbs contribute a relatively low count. There are orthographic issues in the CHILDES dataset for East Asian Languages,"}, {"title": "4 Methodology", "content": "4.1 Model Architecture\nFollowing Diehl Martinez et al. (2023), we develop non-curriculum learning models. These models are scaled-down language models based on RoBERTa (Liu et al., 2019), with 8M parameters and trained on no more than 30M words (Huebner et al., 2021). We use 8192 vocabulary items, which Diehl Martinez et al. (2023) find yields better overall performance compared to a larger vocabulary. Token unmasking is also removed, like BabyBERTa. We use a small model architecture composed of eight layers. This follows Diehl Martinez et al. (2023), who compare the role of model size (8, 10, 12 Transformer layers) and vocabulary size (comparing \\(|V| \u2208 {8192, 16384}\\)). An AdamW optimiser with linear scheduling is used (Loshchilov et al., 2017). Each model is trained for 400,000 steps with 4 A100 GPUs. The hyperparameters used for the \"vanilla\" SSLMs are shown in Table 4. The models concatenate input sequences to capitalise on the available input length.\n4.2 Baselines: LLMs and SSLM (WIKI)\nWe use two families of models as baselines. First, we compare the performance of monolingual SSLMs to monolingual Large Language Models to assess the benefits of the BabyLM paradigm. For French, German and Chinese, we use ROBERTa-style monolingual LLMs. The Chinese ROBERTa model is trained on around 30B words (Cui et al., 2020), which more than 104 times the training data we use to train our SSLMs in the Chinese portion of MAO-CHILDES. We include GPT-2 Baselines for Japanese, which are reported by Someya and Oseki (2023). This is because Japanese ROBERTa monolingual language models are not trained on data using Romaji orthography, which is used in the Japanese portion of MAO-CHILDES (Section 3). Secondly, to assess the benefits of pre-training SSLMs on Child-Directed Speech, we train SSLMs using Wikipedia text (SSLM WIKI), which is extracted to match the quantity of training data in MAO-CHILDES for each language. We keep the original hyperparameter settings used by Huebner et al. (2021).\n4.3 \u201cVanilla\u201d SSLMs: MAO-BabyBERTa\nWe train a family of SSLMs, called Monolingual Age-Ordered BabyBERTa (MAO-BABYBERTA), on language-specific training data from MAO-CHILDES using the model architecture described in Section 4.1 without any curriculum learning strategies. Hyperparameters are tuned for English, and we use the same settings in MAO-BabyBERTa.\n4.4 Implementing Acquisition-Inspired Objective Curricula: GROWING, INWARDS & MMM\nTo implement the acquisition-inspired strategies, we filter our age-ordered MAO-CHILDES corpus for each language for expected utility in the acquisition process, according to the curriculum strategies of GROWING, INWARDS and MMM schematised in Figure 1. We then precisely implement the GROWING, INWARDS, MMM theories introduced in Section 2.2, using different curriculum units composed of POS tagsets (Table 1) to define three objective curricula that replicate the developmental sequences of each acquisition model through the progressive ordering of POS units. The logic for performing masked language modelling selectively for words annotated with a desired set of specified part of speech tags is implemented in Diehl Martinez et al. (2023), which we extend. The objective curricula modify the masked language modelling (MLM) objective in a multi-task learning setup, so the acquisition-inspired objective is activated and optimised in parallel with MLM. We fix the model architecture to be identical to the \"vanilla\" SSLM architecture in Section 4.3 to evaluate the benefits of each curriculum strategy. We modify CLIMB's objective curricula to implement the GROWING, INWARDS and MMM objective curricula by splitting 400K training steps across"}, {"title": "5 Results", "content": "The performance of objective curricula and cross-lingual SSLMs on minimal pairs datasets is summarised in Table 2. Fine-grained objective curricula demonstrate variable effectiveness compared to non-curriculum baselines. While MMM (UPOS) shows general promise, average benefits of MMM (UPOS), GROWING, and INWARDS, do not show statistically significant improvements on MAO-BABYBERTA cross-linguistically (p < 0.05). However, the MMM (SEM) curriculum achieves a statistically significant performance improvement in both English and Chinese (p < 0.05) when performing a paired t-test. Instead, statistically significant improvements are observed with acquisition-inspired CL strategies in specific languages across minimal pairs test sets. MMM (UPOS) only achieves a statistically significant improvement in Japanese and Chinese. GROWING leads to a statistically significant improvement in Japanese and Chinese, while INWARDS only has statistically significant improvements in Japanese. No curriculum strategy outperforms MAO-BABYBERTA in French, although INWARDS almost reaches the same accuracy. German CL strategies only marginally outperform the non-CL baseline. In Figure 3, we compare these results with a broader range of models introduced by Diehl Martinez et al. (2023), finding that the English MMM (SEM) curriculum marginally outperforms other curriculum learning strategies."}, {"title": "6 Discussion", "content": "Acquisition-inspired CL strategies represent a novel large-scale application of language acquisition theory in Deep Learning, aimed at improving the performance of SSLMs. Acquisition-inspired curricula guide SSLMs, which function as large statistical learners, to generalise over frequent linguistic categories\u2014such as nouns and verbs-early in the training process and attend to language-specific features, such as the Germanic V2 word order. This suggests that more fine-grained, language-specific curricula may have performance benefits over non-CL strategies in SSLMs, which is supported by results showing the limited improvements of universal/maturational theories of acquisition that inform the GROWING and INWARDS strategies. Although both acquisition models predict universal curricula that should lead to consistent benefits cross-lingually, GROWING/INWARDS only improve performance in Chinese and Japanese, while performing comparably to non-curriculum (non-CL) baselines in French/German and worse than non-CL baselines in English. An additional benefit of using fine-grained language-specific curricula is that it enables SSLMs to learn more complex grammatical phenomena that may rely on semantics like anaphora. We notice notable improvements in ellipsis performance with the MMM (SEM) curriculum. Interestingly, in Chinese, the MMM (SEM) curriculum marginally underperforms compared to MMM (UPOS) when handling anaphora and aspectual phenomena, highlighting the need for further investigation into engineering optimal language-specific curriculum strategies that outperform non-CL strategies. This raises important avenues for future research. Careful analysis of developmental sequences beyond English to develop language-specific strategies similar to MMM (UPOS/SEM) will be crucial. We encourage practitioners to curate larger corpora of child-directed speech (CDS) for training SSLMs in languages beyond English and to develop more minimal pair datasets that have coverage beyond grammatical agreement in CLAMs to develop better-performing curriculum strategies for Romance and Germanic. Additionally, an important finding is that acquisition-inspired CL strategies in Japanese significantly outperform GPT-2. The improvements observed in Japanese control/raising phenomena suggest that the properties of CDS in Japanese may lead to more robust generalisations than LLMs."}, {"title": "7 Conclusion", "content": "This paper assesses whether fine-grained curriculum learning strategies based on acquisition theories can provide better heuristics for CL strategies for SSLM pre-training cross-lingually, introducing the MAO-CHILDES training corpus to train SSLMs for four typologically distant language families. Mixed results of the maturational GROWING and INWARDS acquisition theories in curriculum strategies and the implementation of the coarse/universal prediction of MMM (UPOS) suggest that there is no guaranteed performance benefit just by devising universal CL strategies based on acquisition theories for SSLMs in a multilingual setting. Training SSLMs using more fine-grained language-specific curricula that precisely replicate cutting-edge linguistic theories is effective for the MMM (SEM) objective in English and Chinese and MMM (UPOS) in Japanese. Curriculum Learning can outperform non-curriculum SSLMs by specifying fine-grained language-specific curricula that precisely replicate language acquisition theories, highlighting how cognitively-inspired techniques can lead to better-performing data-efficient architectures in the spirit of the BabyLM Challenge."}, {"title": "A MMM (SEM): Specifying Language-Specific Curricula using Semantic Tags", "content": "As a first step towards modelling language-specific curricula using curriculum learning, we use Universal Semantic Tagging (sem-tagging) (Bjerva et al., 2016). The set of semantic tags can differ cross-lingually. In Chinese, Li et al. (2021) specifies a language-specific semantic tagset, adding and removing tags based on Chinese's semantic and syntactic properties. The fine-grained curriculum in an SSLM set-up aims to circumvent known problems of shortcut learning in LLMs that prevent Transformer-based models from exhibiting robust structural generalisation capabilities that humans exhibit in acquisition (Salhan, 2023).\nWe perform sem-tagging to annotate the BabyLM corpus for English and the Chinese corpus in MAO-CHILDES with a set of language-neutral tags (sem-tags). For English, we only perform sem-tagging for the Adult Directed Speech datasets in the BabyLM Challenge dataset: the BNC, Project Gutenberg, OpenSubtitles, QCRI, Wikipedia and Switchboard corpora. This allows us to modify our UPOS curricula for English to specify a more complex curricula to simulate later stages of language acquisition. The first stage of the new MMM curriculum using semantic tags includes tags related to event, EVE, tense, TNS, and modality MOD. These are typically learnt later during acquisition, as part of complex tense sequences of auxiliaries and modal verbs (Biberauer and Roberts, 2015), and allow us to define a language-specific sem-tag objective. For Chinese, we sem-tag a corpus of Wikipedia text that contains the same amount of text as the age-ordered CHILDES corpora introduced in Section 3."}, {"title": "A.1 Semantic Tagger Accuracy", "content": "A multi-objective POS and sem-tagger is trained, using a Bidirectional LSTM (BiLSTM) with a Conditional Random Field (CRF) inference layer to train a multi-objective semantic and UPOS tagger for English and Chinese. This is trained on 1100 sem-tagged sentences from the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) and a 1000 sem-tagged sentences from Chinese TreeBank (Xue et al., 2005) annotated by Li et al. (2021). The tagger has 91.4% accuracy for Chinese and 94.6% accuracy for English."}, {"title": "B Training", "content": null}, {"title": "C Statistical Significance & Detailed Results", "content": "The statistical significance of the three curriculum strategies, GROWING, INWARDS & MMM is calculated by performing t-tests on the detailed results in Tables 7, 8, 9, 10. For each curriculum (GROWING, INWARDS, MMM (UPOS), MMM (SEM)), we calculate the paired differences in accuracy with the Vanilla model for all the test sets in the minimal pairs evaluation dataset. We perform paired t-tests for the non-CL baseline (MAO-BABYBERTA) and the accuracy of the respective curriculum for each curriculum strategy for each language, concluding that the curriculum-based model significantly outperforms the Vanilla/MaoBabyBERTa model if the p-value is below our significance level a = 0.05. The detailed results, below, support the findings of Huebner et al. (2021) cross-linguistically of the benefits of using less training data and paying careful attention to training artefacts and the domain of training corpora, as using CDS to train SSLMs (with/without objective curricula) outperforms SSLM (WIKI)."}]}