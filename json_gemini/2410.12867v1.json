{"title": "Empowering Dysarthric Speech: Leveraging Advanced LLMs for Accurate Speech Correction and Multimodal Emotion Analysis", "authors": ["Kaushal Attaluri", "Anirudh Chebolu", "Sireesha Chittepu"], "abstract": "Dysarthria or Dysarthric speech as called is kind of a motor speech disorder which is caused by neurological damage that affects the muscles for speech production, which results in slurred, slow, or difficult-to-understand speech which has been affecting millions of people worldwide including those with conditions such as stroke, traumatic brain injury, cerebral palsy, Parkinson's disease, and multiple sclerosis, dysarthric presents a significant communication barrier, which impacts the quality of life and social interaction of individuals. The entire aim of this paper is to come up with a mechanism that can recognize and translate the speech of dysarthric users and empower their ability to communicate effectively. In this paper, we are proposing a novel approach leveraging Advanced Large Language models for accurate speech correction and multimodal emotion analysis. Our methodology involves converting dysarthric speech to text using OpenAI's whisper model, followed by fine-tuned open source models such as LlaMa 3.1 70B and Mistral 8x7B models on Groq AI accelerators to predict the intended sentences from distorted input speech accurately. The entire dataset used was made by combining TORGO dataset with Google speech data and then manually labeling emotional contexts. Our framework identifies emotions such as happiness, sadness, neutral, surprise, anger and fear highlighting the potential understanding of dysarthric speech. Our approach effectively reconsturcts the intended sentences and detects emotions with high accuracy.", "sections": [{"title": "1 Introduction", "content": "Dysarthria is a motor speech disorder resulting from neurological damage that affects the muscles involved in speech production, such as the tongue, lips, vocal cords, and diaphragm. This condition often leads to slurred, slow, or difficult-to-understand speech, creating significant communication barriers for affected individuals. Dysarthria is not a singular condition but a symptom associated with various neurological disorders, including stroke, traumatic brain injury, cerebral palsy, Parkinson's disease, and multiple sclerosis. As a global issue, dysarthria affects millions of people worldwide, limiting their ability to communicate effectively and impacting their social interactions, quality of life, and overall well-being.\nExisting methods to assist individuals with dysarthria in communication are often limited to conventional speech therapy or assistive communication devices, which may not be effective for all cases, especially where speech clarity is severely compromised. The challenge lies in creating solutions that not only improve speech intelligibility but also capture the underlying emotions conveyed by the speaker. Emotions play a crucial role in effective communication, and understanding them is vital for ensuring meaningful interactions. Addressing both aspects speech correction and emotion recognition can significantly enhance the overall communicative experience for dysarthric users.\nAdvancements in artificial intelligence (AI) and natural language processing (NLP) have opened new avenues for developing more sophisticated solutions to address dysarthria-related communication barriers. In recent years, Large Language Models (LLMs) such as OpenAI's Whisper, LlaMA 3.1, and Mistral 8x7B have demonstrated remarkable capabilities in understanding, generating, and interpreting human language. These models, especially when fine-tuned for specific tasks, offer promising potential to decode and reconstruct dysarthric speech accurately. Additionally, the deployment of these models on highly efficient hardware like GROQ LPUs further enhances their performance and applicability in real-time scenarios.\nIn this study, we propose a novel framework for dysarthric speech recognition and correction that leverages advanced LLMs for accurate speech-to-text conversion and sentence prediction. We use OpenAI's Whisper model to transcribe dysarthric speech into text, followed by fine-tuning LlaMA 3.1 (70B) and Mistral 8x7B models to predict the intended sentence from the distorted input. To address the multimodal aspect of communication, our approach also includes emotion recognition using the same LLMS to identify six primary emotions sadness, happiness, surprise, anger, neutrality, and"}, {"title": "2 Literature Survey", "content": "The paper [1], investigates the use of large language models (LLMs) to improve speaker diarization accuracy. The authors propose a method that fine-tunes an LLM on a large dataset of transcribed conversations to score speaker diarization hypotheses. They evaluate their method on a benchmark dataset and report that fine-tuned LLMs can improve accuracy, but their performance is constrained to transcripts produced using the same ASR tool. To address this constraint, they develop an ensemble model that combines weights from three separate models, each fine-tuned using transcripts from a different ASR tool. The ensemble model demonstrates better overall performance than each of the ASR-specific models, suggesting that a generalizable and ASR-agnostic approach may be achievable.\nThe use of Large Language Models (LLMs) for emotion recognition from audio has been less explored compared to their use for text-based emotion recognition. This article [2] investigates the effectiveness of LLMs for audio-based emotion recognition by using audio features as input. The authors find that LLMs can achieve good performance on emotion recognition when given audio descriptions, but the quality of the audio data significantly impacts the results. This research contributes to the growing field of audio-based emotion recognition using LLMs and highlights the importance of high-quality audio data for accurate emotion detection.\nClozeGER [3] is a novel approach to ASR generative error correction that utilizes a multimodal LLM, SpeechGPT, to improve correction fidelity. By reformulating GER as a cloze test with logits calibration, ClozeGER addresses the limitations of traditional methods. This approach leverages the strengths of multimodal LLMs and provides clearer instructions to the model, resulting in more accurate and contextually relevant corrections. Experimental evidence demonstrates the effectiveness of ClozeGER on various ASR datasets, surpassing previous state-of-the-art methods.\nMulti-Stage LLM-Based ASR Correction presents a novel approach [4] to address the over-correction issue in LLM-based ASR correction. By incorporating an uncertainty estimation stage and formulating the task as a multi-step rule-based LLM reasoning process, the proposed method achieves state-of-the-art performance, even"}, {"title": "3 Methodology", "content": "In this survey, we provide a comprehensive overview of recent advancements in Multimodal Large Language Models (MM-LLMs). We delve into their architecture, training pipelines, and state-of-the-art models, highlighting their unique formulations and performance on various benchmarks. By offering insights into the capabilities and future directions of MM-LLMs, we aim to contribute to the ongoing progress in this rapidly evolving field."}, {"title": "3.1 Data Pre-processing", "content": "The data information was taken from the Torgo dataset. The torgo dataset consists of only singular dysarthric word, where the frequency of the word is not consistent and using the available Torgo dataset, we have created our own dataset using the available torgo dataset. We had multiple steps in the creation of the dataset which is discussed in this paper step by step in the below subsections."}, {"title": "3.1.1 Step-1: Using GPT-4 to convert to sentences", "content": "So, as discussed initially only the word was available in the torgo dataset which which was dysarthric where the voice was could not be detected. So we used GPT-4 API services and have written a prompt to get them converted to understandable english sentences. We used GPT-4 for this because it gave us the best results for text"}, {"title": "3.1.2 Step-2: Using Google Speech to convert them to audio files", "content": "Later after getting the overall sentences in text and the existing irregularity in the word, we used Google speech to convert these sentences to speech which will act as the input to our model."}, {"title": "3.2 Speech To Text", "content": "After getting the input format to speech and creating the dataset, we had to convert the existing speech format to text, for this conversion, we are using OpenAI's whisper which is an automatic speech recognition system. The great advantage of whisper is, it can convert speech to text in multiple languages, It is multilingual and it is open source."}, {"title": "3.3 GPT 4.0", "content": "After converting the speech to text, we used the GPT 4.0 model to get the predicted sentence. GPT 4.0 is the part of the (Generative Pre-trained Transformer) series which was developed by OpenAI building upon the high success of it's succcessors. GPT-4.0 was launched in May 2024 with over 200 billion parameters and is considered to be the State of the Art (SOTA) model. It has very good understanding of nuanced queries."}, {"title": "Fig. 2 GPT-4.0 Architecture", "content": "GPT-4.0's Architecture is built on a transformer model, which uses self-attention mechanisms to understand the relationship between words which is crucial for interpreting dysarthric speech, where missing or distorted words creates created discontinuities.\nThe key aspects of GPT-4.0 Architecture are:\n\u2022 Self-Attention Mechanisms: The ability to weigh the importance of each word in the input allows GPT-4.0 to handle incomplete or distorted speech. In cases where dysarthric speech omitted words or phrases, GPT-4.0 was able to infer the likely missing elements by considering the context of the surrounding words, which is essential when processing challenging speech patterns.\n\u2022 Context Length: GPT-4.0's increased capacity to process longer text sequences proved beneficial in handling dysarthric speech inputs. Many sentences required GPT-4.0 to maintain coherence over longer sequences to correctly interpret and reconstruct the speech. Its longer context window enabled the model to keep track of complex sentence structures, even when parts of the speech were unclear or missing.\n\u2022 Pre-training on Large, Diverse Datasets: GPT-4.o's pre-training on a vast array of text data from diverse domains made it more capable of understanding less common speech patterns, such as those produced by dysarthric individuals. This broad pre-training helped the model adapt to the unique linguistic challenges presented in our use case, improving its predictive accuracy.\n\u2022 Multimodal Learning Capabilities: Although we used GPT-4.0 primarily for text-based tasks, its multimodal capabilities ensure that it can handle inputs from"}, {"title": "3.4 Llama 3.1-70b", "content": "The large language model used for understanding the dysarthric word, we used Llama 3.1-70b-Versatile model developed by Meta AI which was launched in July 2024 which already has over 2 million downloads on hugging face, the reason we used this is that as of now it is one of the best open-source models and is ranked among the top base models in the Open LLM leaderboard on huggingface. We used this model as it is one of the best LLMs right now especially for text generation use case and when we tried samples, it gave us the best results.\n\u2022 Self-Attention Mechanisms: LLaMA 3.1-70B utilizes an advanced self-attention mechanism, which is key for interpreting incomplete or distorted speech. This ability to focus on the contextual importance of words allowed the model to predict missing elements in dysarthric speech, enhancing the clarity of sentence reconstructions.\n\u2022 Fine-Tuning for Dysarthric Speech: Fine-tuning the model on our customized dataset, based on the TORGO dataset and Google Speech, was crucial. This process allowed LLAMA 3.1-70B to learn the specific linguistic patterns present in dysarthric speech, such as slurred words, omissions, and irregular phrasing. Fine-tuning helped align the model's predictions with the complexities of our task, boosting its accuracy to 74.6%. This approach made the model more specialized in handling real-world cases of dysarthric speech, leading to more accurate reconstructions.\n\u2022 Long-Range Dependencies: The ability to process long-range dependencies allows the model to retain context over extended sequences. Dysarthric speech often includes incomplete phrases, and LLaMA 3.1-70B's ability to consider the entire sentence context helped the model fill in gaps, improving overall sentence coherence.\n\u2022 Improvements Over LLaMA 2 and 3: LLaMA 3.1 introduces several improvements over its predecessors:\nIncreased Parameter Efficiency: Compared to LLAMA 2 and 3, version 3.1 has better parameter optimization, reducing computational overhead while maintaining or exceeding performance.\nEnhanced Pre-Training Data: LLAMA 3.1 benefits from pre-training on a broader and more diverse dataset than previous versions, which enhanced its ability to generalize, especially when handling non-standard or incomplete speech inputs.\nImproved Fine-Tuning Mechanisms: The fine-tuning architecture of LLAMA 3.1 is more flexible, allowing for greater adaptability to specific tasks like dysarthric speech recognition. This flexibility helped us leverage domain-specific data effectively, leading to higher accuracy compared to using LLaMA 2 or 3."}, {"title": "3.5 Mixtral-8x7b-32768", "content": "Mistral-8x7b-32768 is a cutting-edge large language model that excels in high-performance language understanding tasks. This model was key in achieving a 70.2% accuracy in our dysarthric speech recognition project. Here, we discuss its history, architectural innovations, and how it was fine-tuned for our use case.\n\u2022 Model History: Mistral, a newer generation of large language models, has been developed with a focus on scaling up model size while improving computational efficiency. The Mistral-8x7b model with 32,768 tokens is designed for long-context tasks, enabling robust understanding of complex sequences, which makes it highly relevant for real-world speech processing applications like dysarthric speech recognition.\n\u2022 Architecture and Technical Innovations:\nHybrid Transformer Mechanism: The Mistral-8x7b-32768 employs a hybrid transformer architecture with multi-head self-attention and feedforward layers that efficiently handle extremely large token inputs (up to 32,768 tokens). This architecture allows the model to process extended input sequences from dysarthric speech, where key words might be missing or garbled, helping it predict the correct context and reconstruct incomplete sentences.\nSparse Attention and Memory Efficiency: To handle longer input sequences efficiently, Mistral-8x7b leverages sparse attention mechanisms. These mechanisms ensure that the model selectively attends to the most relevant parts of the input, reducing computational load while retaining context over large inputs, which is critical for handling the irregularities of dysarthric speech.\nParallelized Training on Groq LPU: We fine-tuned the Mistral-8x7b-32768 model using the Groq LPU (Language Processing Unit), which allowed us to parallelize computations effectively, resulting in faster convergence during fine-tuning. Groq's parallel architecture is particularly beneficial for handling models like Mistral, as it enables real-time speech processing by accelerating inference and improving overall latency in model predictions.\n\u2022 Fine-Tuning and Application in Dysarthric Speech Recognition: The fine-tuning process involved adapting Mistral-8x7b to the specific speech irregularities found in the dysarthric speech dataset we created from TORGO and Google Speech. This allowed the model to learn patterns such as slurring, missing words, and distortions, significantly improving its accuracy in predicting and reconstructing"}, {"title": "3.6 Using QLoRA to Fine-Tune LLMs", "content": "Fine-tuning large language models (LLMs) such as LLaMA 3.1 and Mistral-32B for domain-specific tasks is critical for improving performance in specialized tasks like dysarthric speech recognition. We employed QLoRA (Quantized Low-Rank Adaptation), a technique that enables efficient fine-tuning by adapting smaller models to larger LLMs. This section explains the QLoRA approach and demonstrates how it was used to fine-tune LLaMA 3.1 and Mistral-32B on the TORGO dataset for improving both speech and emotion detection accuracy."}, {"title": "3.6.1 LoRA and QLoRA: An Overview", "content": "LORA (Low-Rank Adaptation) [?] is a method for efficient model fine-tuning. Instead of updating all model weights during fine-tuning, LoRA inserts low-rank decomposition matrices into the weights of transformer layers. These matrices are smaller, which allows the model to adapt to new tasks without retraining all the parameters. Given a weight matrix $W \\in \\mathbb{R}^{d \\times k}$, LoRA approximates W as:\n$W' = W + BA$\nwhere $A \\in \\mathbb{R}^{r \\times k}$ and $B\\in \\mathbb{R}^{d \\times r}$ are low-rank matrices (with rank $r < d$).\nQLORA (Quantized LoRA) builds upon LoRA by applying 4-bit quantization to the model weights before fine-tuning. This significantly reduces memory usage, enabling fine-tuning of extremely large models on commodity hardware. In QLORA, the forward pass uses quantized weights, while LoRA layers allow gradients to flow during backpropagation."}, {"title": "3.6.2 Architecture of QLoRA", "content": "The architecture of QLORA involves quantizing the LLM's model weights to reduce memory requirements and applying low-rank adaptation to specific layers. This is particularly important in transformer architectures, where each transformer block consists of multi-head attention and feed-forward layers. In QLoRA:"}, {"title": "3.6.3 Fine-Tuning LLMs using QLORA", "content": "Fine-tuning an LLM using QLORA involves three key steps:\n1. Model Preparation: The weights of the LLM (e.g., LLaMA 3.1, Mistral-32B) are quantized to 4-bit precision. This drastically reduces the memory footprint.\n2. Low-Rank Adaptation: LoRA layers are inserted into the transformer blocks of the model. These layers are initialized with small, low-rank matrices.\n3. Training on Task-Specific Data: During training on the TORGO dataset, only the LORA parameters are updated. The quantized weights remain unchanged, ensuring memory efficiency.\nThe key advantage of this approach is that the effective model size remains manageable, allowing fine-tuning on a single GPU while retaining high performance."}, {"title": "3.6.4 QLORA for Dysarthric Speech Recognition", "content": "By employing QLoRA, we achieved significant improvements in both speech and emotion recognition for dysarthric speech using the TORGO dataset. Quantized fine-tuning enabled better generalization without overfitting, and the low-rank adaptation matrices captured the variability in dysarthric speech patterns. The overall accuracy improvements for models fine-tuned with QLoRA were as follows:\n\u2022 Speech Detection: For LLaMA 3.1, we observed a boost from 68.2% to 74.6% after QLoRA fine-tuning.\n\u2022 Emotion Detection: The model's ability to classify emotions improved from 81% to 89% for fine-tuned models on dysarthric data."}, {"title": "3.6.5 Formula for Fine-Tuning", "content": "To quantify the improvement from QLoRA, let $Acc_{baseline}$ represent the baseline accuracy of the pre-trained model, and $Acc_{fine-tuned}$ be the accuracy after fine-tuning with QLORA. The improvement can be calculated as:\n$\\Delta Acc = Acc_{fine-tuned} - ACC_{baseline}$\nFor example, for LLaMA 3.1, $\\Delta Acc$ for speech recognition is:\n$\\Delta Acc = 74.6\\% - 68.2\\% = 6.4\\%$\nThis illustrates the effectiveness of QLoRA in boosting model performance while maintaining resource efficiency."}, {"title": "3.7 Emotion Recognition", "content": "In addition to sentence reconstruction, our system performs emotion recognition by classifying the emotional state of dysarthric speakers into six classes: neutral, happy, sad, anger, surprise, and fear. Accurately detecting the emotion behind dysarthric speech is essential, as it provides contextual understanding that goes beyond syntactic correctness, enhancing interaction between dysarthric individuals and assistive technologies.\nEmotion Recognition Pipeline: For the emotion recognition task, we manually labeled the emotional context of each sentence in our dataset. After sentence reconstruction, the predicted sentences were passed through three large language models (GPT-4.0, LLaMA 3.1-70B, and Mistral-8x7b-32768) to classify the emotional state of the speaker.\nTechnical Approach to Emotion Classification: All three models leverage transformer-based architectures that make them adept at handling language modeling and emotion recognition. Here's a breakdown of how the transformer architecture enables this functionality:\n\u2022 Self-Attention Mechanism for Context Understanding: Transformer architectures, such as those in GPT-4.0, LLaMA 3.1-70B, and Mistral-8x7b-32768, are built around multi-head self-attention mechanisms. This allows the model to attend to different parts of the input sentence with varying weights, effectively capturing the emotional undertones present in the speech. The self-attention mechanism helps the model focus on emotionally charged words or phrases, such as hurt, angry, or scared, while also maintaining an understanding of how these words contribute to the overall emotional state of the speaker.\n\u2022 Positional Encoding and Sentence-Level Context: The positional encoding in transformer architectures enables the model to understand word order and sentence structure, which are crucial for correctly interpreting emotions. For example, the phrase I am fine may indicate a neutral emotion, while the phrase I am fine, but... can imply underlying sadness or frustration. By keeping track of sentence-level context, the transformer can differentiate subtle emotional cues embedded within the speech patterns.\n\u2022 Pre-trained Embedding Representations: The models are pre-trained on massive corpora of text that span various domains, enabling them to build rich embeddings for emotional expressions. These embeddings map emotional words and phrases to specific points in a high-dimensional space. When fine-tuned on our emotion-labeled dysarthric speech dataset, these embeddings allowed the models to learn the emotional associations specific to dysarthric speech, where certain distortions or omissions may obscure the typical emotional markers.\n\u2022 Emotion Classification via Feedforward Neural Networks: After the transformer-based encoder generates contextualized embeddings, a feedforward neural network (FFN) is used to classify the emotion. The FFN processes the embeddings and outputs a probability distribution over the six emotion classes: neutral, happy, sad, anger, surprise, and fear. The class with the highest probability is selected as the final emotion prediction."}, {"title": "4 Equations", "content": ""}, {"title": "4.1 Speech to Text (Whisper)", "content": "$P(T|A) = \\prod_{t=1}^{T} P(t | A_{1:T})$\nwhere $P(T|A)$ denotes the probability of text sequence T given the audio sequence A, and $A_{1:T}$ represents the audio features from time step 1 to T."}, {"title": "4.2 GPT-4.0 (Text Generation)", "content": "$P(x_t|x_{<t}) = \\text{softmax}(W_o \\cdot \\text{LayerNorm}(h_t) + b_o)$\nwhere $h_t$ is the hidden state at time t, and $W_o$ and $b_o$ are the output weights and biases. The softmax function calculates the probability distribution over the vocabulary for the next token."}, {"title": "4.3 LLaMA 3.1-70B (Text Generation)", "content": "$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\nwhere Q, K, and V are the query, key, and value matrices respectively, and $d_k$ is the dimensionality of the keys."}, {"title": "4.4 Mistral-8x7b-32768 (Text Generation)", "content": "$\\text{SparseAttention}(Q, K, V) = \\text{ReLU}(QK^T)V$\nwhere the SparseAttention mechanism uses ReLU activation to handle sparse attention weights and reduces computational complexity."}, {"title": "4.5 Emotion Recognition", "content": "$\\hat{y} = \\text{softmax}(W_e \\cdot \\text{FFN}(h) + b_e)$\nwhere $\\hat{y}$ is the predicted emotion class probabilities, $W_e$ and $b_e$ are the weights and biases of the emotion classification layer, and $\\text{FFN}(h)$ is the feedforward network applied to the hidden states h."}, {"title": "8 Future Work and Discussions", "content": "Our current research lays the foundation for significant advancements in dysarthria treatment and speech recognition. Moving forward, several key areas will be explored to enhance and expand the impact of our work. Firstly, extending our models to support multiple languages is crucial, given that dysarthria affects approximately 1 in 1000 children and 52 percent of stroke survivors globally. With roughly 7000 languages spoken around the world, developing language-specific models will be vital in addressing the needs of diverse populations. Additionally, improving our models to better accommodate various speech patterns and accents will enhance their efficacy. Our ultimate aim is to contribute to a global effort in providing accessible and accurate communication tools for individuals with dysarthria, ensuring that language barriers do not hinder effective interaction. This ongoing research is dedicated to making communication easier and more inclusive for everyone, regardless of language or speech impairment."}, {"title": "9 Conclusion", "content": "In this study, we have developed and evaluated advanced speech recognition and text generation models tailored for individuals with dysarthria. By leveraging state-of-the-art models such as GPT-4.0, LLaMA 3.1-70B, and Mixtral-8x7B-32768, along with fine-tuning on specialized datasets, we have achieved significant improvements in transcription accuracy. Our approach integrates emotion recognition to further enhance"}]}