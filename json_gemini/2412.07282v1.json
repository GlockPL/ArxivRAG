{"title": "HARP: Hesitation-Aware Reframing in Transformer Inference Pass", "authors": ["Romain Stora\u00ef", "Seung-won Hwang"], "abstract": "This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to \"off-the-shelf\" Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We thoroughly evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP offers a practical solution for enhancing the performance of Transformer-based language models with minimal computational impact.", "sections": [{"title": "1 Introduction", "content": "Causal language models based on the Transformer architecture (Vaswani et al., 2017) use a constant number of layer traversals to generate each new token. While this architecture is beneficial to provide easy parallelization during training (Dehghani et al., 2019), it may not fully leverage the model's full potential during inference, where tokens are generated sequentially. Research in adaptive computation (e.g. Graves, 2017; Leviathan et al., 2023; Elhoushi et al., 2024; Leviathan et al., 2024) suggests that inference steps are not equally challenging, with some being \"harder\" and others \"easier.\u201d Intuitively, these more challenging tokens would benefit from additional computational resources to improve accuracy. Unfortunately, the current Transformer architecture treats each token equally- regardless of its difficulty-potentially leading to imprecision and performance drops.\nTo address this limitation, a trend in language models has been to simply scale up models in size (Brown et al., 2020), allowing for more computation. While the difficult tokens benefit from this scaling, it also leads to unnecessary overhead for easier tokens. To tackle this uniform additional computation, Speculative Decoding (Leviathan et al., 2023; Chen et al., 2023) employs a bigger model to verify and correct the tokens generated by the smaller model, acting as an external verifier. Considering the smaller model as the original model, the larger model performs additional computations to ensure the quality of the generated tokens. This enables the system to spend more computational resources on complex tokens while preserving efficiency for easier ones. However, this approach requires the involvement of an external model. Similarly, Goyal et al. (2024) add fixed pauses during inference, using \"pause tokens,\" to allow for additional computation on harder tokens.\nWe draw inspiration from human behaviors to allow models to perform additional computations for \"harder\" steps without relying on external models or requiring retraining. Two cognitive effects stand out: the (1) hesitation and the (2) framing effect. First, hesitation reflects uncertainty in decision-making. Humans tend to pause and reconsider when faced with difficult decisions (Shenhav et al., 2013) such that more effort is spent on complex inputs\u2014aligning with the idea of \"harder\" tokens during inference. Second, the framing effect indicates that how information is presented can influence judgment and response (Kahneman, 2012). It implies that a different representation of the same input can lead to better outcomes. In the following, we will refer to this another-view representation as \u201creframing\" the inputs.\nBuilding on these human-inspired concepts, we introduce Hesitation-Aware Reframed Forward Pass (HARP), a plug-and-play modification to the Transformer's forward pass. By selectively reframing inputs, HARP mimics human reconsideration under uncertainty, achieving up to 5.16% performance improvements with minimal additional cost. Our method outperforms beam search (Kasai et al., 2024) across multiple tasks, offering higher accuracy and significantly faster inference. The code\u00b9 is publicly available.\nOur contributions can be summarized as follows:\n\u2022 We introduce a selection process based on"}, {"title": "2 Related Works", "content": "2.1 Adaptive Computation in Transformers\nAdaptive computation in Transformers (ACT) can be categorized into efficiency-focused and performance-focused categories. Efficiency-focused ACT has been the primary focus of research, aiming at improving efficiency by reducing computation for \u201ceasier\u201d inference steps (Leviathan et al., 2023; Chen et al., 2023; Elhoushi et al., 2024). These approaches often involve using smaller models or skipping layers when processing less challenging tokens, thereby optimizing computational resources and resulting in inference speedups.\nIn contrast, performance-focused ACT, which includes our method HARP, targets the \u201charder\" inference steps, prioritizing performance gains over efficiency improvements. Our approach shares motivations with the work of Goyal et al. (2024), who allocate more computation by extending the model's vocabulary with pause tokens. However, while effective, the pause tokens method requires retraining and fine-tuning of the model.\nUnlike the pause tokens approach or scaled-up models using efficiency-focused ACT, HARP is a training-free, model-agnostic, and plug-and-play method that can be applied to any Transformer-based model without the need for retraining, making it an advantageous solution within the performance-focused ACT framework.\nIn parallel to adaptive computation work, some studies have explored methods to enhance reasoning capabilities in LLMs (e.g. Zelikman et al., 2022; Zelikman et al., 2024; Hosseini et al., 2024; Andukuri et al., 2024). While these approaches can be seen as incorporating extra computation, their focus diverges from ours, as they improve reasoning through fine-tuning rather than optimizing token-level computation selectively."}, {"title": "2.2 Uncertainty Estimation in Language Modeling", "content": "Uncertainty quantification (Abdar et al., 2020) on token-level is not a well-explored area. Most of the existing works focus on the evaluation of sequence-level uncertainty (e.g. Arteaga et al., 2024; Manakul et al., 2023; Kuhn et al., 2023) or on a higher level. In contrast, our work focuses on token-level uncertainty-the uncertainty in the probability distribution over the vocabulary for predicting the next token. Luo et al. (2024) introduce a ratio-based method to measure uncertainty. While this approach offers an intuitive interpretation of uncertainty, it lacks some theoretical grounding and might fail to capture more subtle hesitation as it relies on only the two highest probabilities of the distribution. Therefore, we use the Shannon Entropy (Shannon, 1948) as our uncertainty estimator. It is an information-theoretic uncertainty measure that captures the amount of information in a probability distribution. Entropy represents the expected number of bits required to resolve the uncertainty of a prediction. Higher entropy indicates more uncertainty, while lower entropy suggests a more confident prediction."}, {"title": "2.3 Reframing at Inference Time", "content": "Reframing data into different perspectives is a well-established technique for training machine learning models, particularly through Multi-View Learning (MVL) (Chaudhuri et al., 2009). In MVL, models are trained on multiple representations of the same data, which improves generalization. However, these techniques are restricted to the training phase and are not designed to be applied during inference (Xu et al., 2013), our target.\nParallely, NEFTune (Jain et al., 2024) brings a promising direction. It introduces noise into embeddings during the training to improve instruction-based fine-tuning. Although NEFTune targets the training phase only, we hypothesize that a similar approach-injecting noise into embeddings-could be beneficial during inference as well. By adding noise into embeddings at inference time, the model could gain a new perspective of the same inputs, potentially improving its ability to handle ambiguous inputs. While NEFTune uses random uniform noise, our work explores different noise approaches, utilizing dropout on the embeddings to induce a new representation."}, {"title": "3 Methods", "content": "In this section, we present our Hesitation-Aware Reframed Forward Pass (HARP) method. We aim to perform an additional forward step from a different perspective when the model encounters uncertainty. We begin by reviewing the standard forward pass of transformers. Then, we introduce the two key components of HARP: quantifying uncertainty during inference and reframing inputs. Finally, we will integrate these components to present the complete algorithm."}, {"title": "3.1 Preliminary: Transformer Forward Pass", "content": "We recall the forward pass of a Transformer model as we will modify its architecture. It processes input tokens to generate logits, representing un-normalized probabilities for predicting each token in the sequence, including the next token. Let x = (x1,...,xn) be the tokenized input sequence of length n, where each xi is a token ID. For simplicity, we denote the embedding layer as emb(\u00b7), while $f_{\\text{emb}}(\u00b7)$ represents the rest of the model, consisting of N serial layers. Each layer includes a multi-head self-attention sublayer, a fully connected sublayer, and layer normalizations. First, the embedding layer emb maps each input token to a dense vector representation: e = emb(x), where e \u2208 Rn\u00d7d and d is the embedding dimension. The embedded inputs are then processed through the rest of the model.\nThus, the forward pass can be concisely expressed as:\nlogits = $f_{\\text{emb}}(\\text{emb}(x))$ (1)\nwhere logits \u2208 Rn\u00d7|V| and |V| is the vocabulary size. The resulting logits contains unnormalized predictions for each input position. The last position's logits are used to predict the next token."}, {"title": "3.2 Uncertainty Estimation", "content": "We want to quantify the model's uncertainty for each newly generated token. To do this, we focus on the logits of the last position, which are used to predict the next token. First, the logits are normalized using the SOFTMAX function to obtain a probability distribution P over the vocabulary V.\nThe SOFTMAX ensures that each value in the distribution is between 0 and 1 and that the total sum of probabilities over V equals 1, i.e., $\\sum_{v_i \\in V} P(v_i | x) = 1$, where vi represents a token in the vocabulary V and P(vi | x) is the probability of token vi, knowing x, under the distribution P.\nTo measure the uncertainty of the distribution P, we then use Shannon entropy (Shannon, 1948). The entropy SHANNON is defined as:\nSHANNON(P) = -$\\sum_{i=1}^{|V|} P(V_i | x) \\log_2 P(V_i | x)$ (2)"}, {"title": "3.3 Reframing Inputs", "content": "Our objective is to perturb the embeddings, which represent the model's understanding of the data, to present the input sequence to the model from an alternate perspective.\nWe follow the approach of NEFTune (Jain et al., 2024), which injects random noise into the embeddings during training. We choose to use dropout among them, as it consistently yields the best results. Let e = emb(x) be the original embeddings of the input sequence x, and let \u03b4 be the dropout rate. The reframed embeddings \u00ea are then obtained as follows:\n\u00ea = DROPOUT(e, \u03b4) (3)\nwhere DROPOUT randomly sets a fraction \u03b4 of the elements in the embeddings to zero."}, {"title": "3.4 Hesitation-Aware Reframed Forward Pass (HARP)", "content": "Our method, HARP, adapts the standard Transformer forward pass by adding an additional computation step when the model exhibits uncertainty. If the entropy is below a predefined uncertainty threshold \u03b8, the model is considered confident in its generation.\nIf the uncertainty exceeds the threshold \u03b8, the model is uncertain.  The reframed embeddings, \u00ea, are obtained by applying dropout with a rate \u03b4 to the original embeddings e. Finally, the original and reframed logits are combined using a combination factor \u03b2 to produce the final output logits.\nlogits = \u03b2\u00b7 logits + (1 \u2212 \u03b2) \u00b7 logits (4)\nThe combination factor \u03b2 controls the balance between original and reframed logits. We empirically find that setting \u03b2 = 0.5 balances the contributions of both passes effectively.\nThe dropout-based perturbation forces the model to consider different representations of the input.\nOur method introduces three hyperparameters: the uncertainty threshold \u03b8, the dropout rate \u03b4, and the combination factor B."}, {"title": "4 Experimental Set-Up", "content": "4.1 Models\nWe consider decoder-only models of size 3.8B, 7B, and 8B as they are the standard in recent times.\nWe consider only aligned models for their simplicity of evaluation and greater performance. Aligned models refer to models that are fine-tuned to better follow instructions, using methods such as supervised fine-tuning or reinforcement learning from human feedback (RLHF).\n4.2 Datasets\nAs our method targets \"off-the-shelf\u201d LLMs, we consider five datasets covering varied downstream tasks and output formats in order to reproduce the variety of missions they can encounter.\n\u2022 GSM8K is a famous mathematical reasoning benchmark where the task involves solving grade school-level math word problems.\n\u2022 CommonSenseQA (CsQA) is a multiple-choice question benchmark that tests commonsense reasoning and general understanding abilities of the model.\n\u2022 LAMBADA is a language modeling benchmark focused on text understanding and completion, where the task is to predict the final word of a passage based on its context.\n\u2022 MMLU Pro\u2014an enhancement of Massive Multitask Language Understanding dataset-evaluates models on a wide range of academic and professional subjects.\n\u2022 CNN/Daily Mail (CNN/DM) dataset is used for text summarization tasks, where the goal is to generate concise summaries of news articles.\n4.3 Evaluation\nSince HARP is generally applicable to any decoding methods, we compare different decoding methods both with and without the HARP forward pass modification.\nThe hyperparameters used in our experiments are fixed at a dropout rate of d = 0.20 and an uncertainty threshold of \u03b8 = 1.0."}, {"title": "5 Results", "content": "HARP improves the performance of all tasks.\nOur method demonstrates consistent performance improvements across all tasks. The improvements are particularly notable for tasks like LAMBADA, with gains of up to 5.16%. When applied to nucleus sampling, HARP outperforms the vanilla model in most cases, especially in math-reasoning tasks like GSM8K.\nOn GSM8K, Goyal et al. (2024) achieved a +1.00% improvement. In comparison, HARP delivers an even higher improvement of +1.51% on the same dataset, using a more performant model and without any retraining.\nIn summary, our HARP allows further improvements as high as +5.16% of high-ended models that already show strong performance on many tasks without requiring any fine-tuning.\nHARP is model-agnostic. We show that HARP also consistently improves models ranging from 3 to 8 billion parameters, including LLaMA-3.1, Mistral v0.3, and Phi 3.5 Mini.\nHARP works with advanced techniques. As shown in Table 2, advanced techniques, such as Chain-of-Thought (CoT) prompting, are further enhanced with HARP.\nHARP is faster than Beam Search. Figure 2 reveals that despite the additional computation, the inference time of HARP remains close to that of the vanilla generation, introducing a minimal additional computational cost."}, {"title": "6 Analysis", "content": "Uncertainty guides the additional computations. To investigate the effectiveness of our uncertainty-based approach , we compare it with a method that unconditionally adds an extra forward step for every token . This comparison helps us understand whether the improvements come solely from the additional computation or from our uncertainty-selected approach. Our analysis reveals that unconditionally adding an extra forward step is not universally beneficial. The uncertainty-conditional method outperforms the unconditional approach, even though it is sometimes marginal. This suggests that uncertainty is an excellent signal for selecting the steps requiring extra computation. A key advantage of the selective approach is its computational efficiency.\nUncertainty pinpoints key reasoning steps. We analyze the output generations of HARP by highlighting the tokens that require extra computation.\nOne additional representation is sufficient for reframing. When facing uncertainty, HARP reframes the inputs only once before pursuing the next generation step. Surprisingly, increasing the number of additional steps often leads to a decline in performance."}, {"title": "7 Conclusion", "content": "This paper presented a novel method, HARP, designed to enhance language model inference without requiring retraining or architectural modifications. Our approach uses a selection process based on token-level uncertainty to identify when additional computation is advantageous and an innovative method for reframing inputs during inference. We demonstrated that HARP significantly improves performance across various tasks and model sizes"}, {"title": "8 Limitations", "content": "Our study has several limitations that need to be addressed in future research. First, due to resource and time constraints, the evaluation of HARP was conducted on a limited subset of each dataset. Additionally, the method has yet to be tested on larger language models, particularly those with 70 billion parameters or more.\nFurthermore, our current implementation faces some challenges in inference efficiency. The method could slow down batch processing since uncertain tokens require additional computation."}, {"title": "9 Ethics Statement", "content": "In this work, we propose modifying the forward pass for improved performance. While we evaluate different models on datasets, we acknowledge that we have not evaluated the impact of our method on safety, including concerns such as toxicity, bias, or content moderation. Our goal is to enhance accuracy, but broader implications\u2014such as generating harmful content or sensitive applications-remain unexplored."}]}