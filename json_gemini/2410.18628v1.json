{"title": "Wavetable Synthesis Using CVAE for Timbre Control Based on Semantic Label", "authors": ["Tsugumasa Yutani", "Yuya Yamamoto", "Shuyo Nakatani", "Hiroko Terasawa"], "abstract": "Synthesizers are essential in modern music production. However, their complex timbre parameters, often filled with technical terms, require expertise. This research introduces a method of timbre control in wavetable synthesis that is intuitive and sensible and utilizes semantic labels. Using a conditional variational autoencoder (CVAE), users can select a wavetable and define the timbre with labels such as bright, warm, and rich. The CVAE model, featuring convolutional and upsampling layers, effectively captures the wavetable nuances, ensuring real-time performance owing to their processing in the time domain. Experiments demonstrate that this approach allows for real-time, effective control of the timbre of the wavetable using semantic inputs and aims for intuitive timbre control through data-based semantic control. Audio examples are available on the online demo page.", "sections": [{"title": "I. INTRODUCTION", "content": "Synthesizers, electronic musical instruments that create sounds using analog or digital signals, play a significant role in modern music production and performance. Although various methods of synthesizing sounds exist, effectively controlling the synthesis algorithm and obtaining the desired timbre requires specialized knowledge and experience. This complexity arises because numerous synthesis parameters are intricate, low-level, and unrelated to the perceptual and semantic properties of the timbre. This challenge is often called the synthesizer programming problem[1], which can even hinder the creative process for experienced musicians and professionals[2].\nTo address this problem, the concept of extracting synthesizer features has been proposed. [3]-[6]. This approach involves extracting features from a user-supplied sound (the target sound) and providing macro-controls that enable users to adjust the timbre based on these features, thereby simplifying the process of achieving the desired timbre.\nOur research employs wavetable synthesis (WTS), one of the earliest software synthesis methods developed by Max Mathews in the late 1950s[7]. WTS, the core technology behind many synthesis methods, remains popular in numerous software synthesizers today. WTS works by cyclically referencing and replicating a waveform (WT: Wavetable) to produce a sound. The timbre is typically modified using filters, amplifiers, or morphing between different wavetables(WTs).\nFurthermore, recent years have seen significant advancements in deep generative models. Among these, autoencoder (AE) [8] and variational autoencoder (VAE) [9] have gained prominence in various fields. These models compress the input data into a compact, low-dimensional latent space and reconstruct the original data from this compressed representation, facilitating the extraction of valuable feature representations from the data. Additionally, conditional variational autoencoder(CVAE) [10], which enables conditional generation based on specified conditions, has also been introduced.\nOur research uses a CVAE to generate a WT for a single cycle, which is then applied to the oscillator within WTS. We have engineered a model that allows users to define the desired timbre through semantic labels. Figure 1 depicts a schematic overview of this process. In previous studies, methods for generating WT using deep generative models have been proposed[3], [11], [12]. However, issues regarding the diversity, controllability, and accuracy of the generated WT remain to be solved. Diverging from these methods, our research proposes a technique that enables users to emphasize or attenuate specific harmonic overtones within a given WT using semantic labels. This approach represents the primary contribution of our research."}, {"title": "II. RELATED WORKS", "content": ""}, {"title": "A. Synthesizer Programming Problem", "content": "The synthesizer programming problem, as outlined in Section I, is a complex issue that experienced musicians and professionals face. This challenge stems from the intricate control mechanisms and the necessity for specialized expertise, particularly in the realm of synthesizer sound creation [2]. Substantial research has focused on automating the estimation of synthesis parameters to address this issue. As a result, two approaches have emerged: the first aims to approximate synthesis parameters to replicate a target sound [13]-[16] and the second utilizes descriptive language concerning timbre for parameter estimation [17]\u2013[19].\nThe feature synthesizer methodology warrants particular attention. This method is swift and user-friendly, specifically designed for music production and live performances. It involves extracting features from target timbres and providing macro-controls, thereby facilitating the adjustment of timbre based on these extracted features. This approach significantly simplifies the process of achieving the desired timbre. The efficacy of the feature synthesis method in enabling intuitive timbre modification has been substantiated in various research studies[3]-[5], [11], [20]. Our research focuses on this feature synthesis approach, emphasizing its ability to rapidly adapt to changes in timbre during music production and performance."}, {"title": "B. Neural Audio Synthesis", "content": "Neural audio synthesis is a technology used for generating or transforming audio data using deep learning. As outlined in Section I, deep generative models are rapidly developing and have become an essential technology in music. Research to generate audio data is also active, and research for applications to synthesizers has been proposed[21]-[23].\nSeveral approaches have been proposed to complement the existing synthesis methods using deep learning. To solve the synthesizer programming problem, Esling et al. [24] showed that VAE can simultaneously process parameter estimation, timbre control, and reconstruction with semantic labels in a single model for subtractive synthesis. Krekovi\u0107 [3] illustrated the feasibility of processing and controlling parameter estimation, timbre control, and reconstruction with semantic labels in a unified WTS model, highlighting the generation of WT from semantic labels.\nThis research explores the application of deep learning to musical synthesis, akin to the earlier research. It focuses on the latest technical trends in model construction, loss functions, and methods for calculating semantic labels."}, {"title": "C. Advances in Wavetable Synthesis", "content": "Wavetable synthesis generates a sound by repeatedly referencing a waveform for one cycle. This method, widely used in various genres, faces the common challenge of timbre generation, as with other synthesis methods. Creating a WT generally involves editing the waveform and frequency spectrum [25] or choosing a preset WT.\nHowever, while these methods significantly impact timbre, they often diverge from human intuition. For instance, subtle changes in the shape or frequency spectrum of a WT can alter its timbre. Nonetheless, discerning the characteristics of the timbre through visual inspection is challenging. Even when selecting a WT from a preset, users must remember the relationship between the name of the preset and its corresponding timbre, which requires a certain level of expertise. Consequently, achieving the desired timbre in WTS can be difficult.\nTo address this problem, as outlined in Section I, previous research generated WT using VAE[12] and AE[11], and the timbre was changed by randomly manipulating the latent space. Furthermore, Krekovi\u0107 [3] proposed a model that generates WT from three input labels: bright, warm, and rich, using a decoder [26]. However, embedding WT features into a limited number of latent spaces (labels) poses challenges. The diversity and accuracy of the generated WT are still two issues that remain to be solved. To enhance the diversity and accuracy of the WT that can be generated, we have proposed a model that uses CVAE[10]. This model is designed to create WT that exhibits a one-period waveform characteristic."}, {"title": "III. METHODS", "content": ""}, {"title": "A. CVAE", "content": "The CVAE[10], an extension of the VAE, is a generative model. This model is notably effective in generating data conditioned on a specific variable, denoted as $c$. In our research, we incorporated the condition, $c$, into the encoder and the decoder to apply these conditions effectively.\nThe input waveform, $x$, and condition, $c$, (in this research, WT and semantic labels, respectively) were mapped to the latent variable, $z$, by the encoder, $q(z|x,c)$. Subsequently, the decoder, $p_{\\theta}(x|z, c)$, generated the output data, $\\hat{x}$."}, {"title": "B. Loss Function", "content": "The loss function comprised the reconstruction loss and the Kullback-Leibler (KL) divergence term. These components were optimized by minimizing them through the stochastic gradient descent (SGD) method. The loss function in this research was defined as follows:\n$L_{CVAE}(x) = E_{q(z|x,c)}[d(x,\\hat{x})] + \\beta \\times D_{KL}[q(z|x, c)||p(z)]$\n(1)\nThe first term in Equation (1), the reconstruction loss, was used for evaluating the accuracy of the generated data in reconstructing the original data and aimed to minimize the information loss between the encoder and decoder. Previous research has suggested using multiscale spectral distance to estimate the waveform distances[21], [23]. Due to the short data length in our WT approach, we calculated the spectral distance using only the maximum table size (3600 samples in our research) after iterative concatenation.\nThe function, $d$, for the reconstruction loss was defined as follows:\n$d(x,\\hat{x}) = \\frac{||S_x - S_{\\hat{x}}||_F}{||S_x||_F} + log ||S_xS_{\\hat{x}}||_1$\n(2)\nwhere $S_x$ and $S_{\\hat{x}}$ are the amplitude spectra of $x$ and $\\hat{x}$, respectively. Here, $||\\cdot||_F$ denotes the Frobenius norm and $||\\cdot||_1$ the L1 norm.\nThe amplitude spectrum of the input waveform, $x$, and the output waveform, $\\hat{x}$, were compared and their loss was calculated. This approach is based on the assumption that the perceptual characteristics of the synthesized sound remain almost invariant between the signal waveforms that are out of phase. Following the convention adopted by Krekovi\u0107 [3], we improved the frequency resolution of the spectrum by concatenating six WTs iteratively, which achieved the best performance in the investigation stage in terms of reconstruction error and controllability of conditioning."}, {"title": "C. Model Configuration", "content": "The model's architecture was adapted from a structure outlined in previous research[23] and was modified to enable the conditional generation of WT. The encoder combined the convolutional layers that convert the WT into a 32-dimensional latent representation. The decoder used an upsampling layer and a residual network[28] to generate the WT. Conditional generation was executed by concatenating the relevant labels to the inputs and outputs within the latent space [12]. Figure 2 illustrates an overview of the model structure."}, {"title": "D. Computation of Semantic Labels", "content": "Previous research has explored the computation of semantic labels in sounds, including WTS. In our research, we employed the labels bright, warm, and rich, following the methodology outlined by Krekovi\u0107 [3], [29]. These labels were derived from the spectral centroid, the energy ratio of the odd harmonics, and the spectral density, followed by normalization.\nThe spectral centroid, representing the center of mass of the spectrum, indicates the balance between the high and low frequencies and correlates with the timbre brightness[30]. The energy ratio of the odd harmonics, i.e., the proportion of energy in the odd harmonics compared to other harmonics, was associated with the warmth of the timbre[31]. The spectral density, calculated as the spectrum's standard deviation, corresponded to the richness of the timbre[32].\nSix WTs were concatenated in succession to improve spectral resolution, following the method outlined in Section III-B"}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Dataset", "content": "We utilized 4158 WT data from Adventure Kid Research & Technology for the dataset. For the size of the sample for each WT, we select 600, which is larger than that utilized in previous works (i.e., 320 in [11], 327 in [3], and 512 in [12]. ), to enhance the representativeness of high-frequency components in WTS. The dataset was partitioned into three segments: 80% for training, 10% for validation, and 10% for testing. This distribution ensured a comprehensive evaluation of the model across different data subsets."}, {"title": "B. Training", "content": "Training was conducted by feeding the model with training data and the corresponding labels. The Adam optimizer[33] was used with a learning rate 1e-4. The batch size was 32, and the number of epochs was 30000 to minimize the reconstruction and KL divergence.\nOver-optimizing the KL divergence between the prior and posterior distributions in the early stages of training can lead to an issue known as KL vanishing[34], in which the latent variables are ignored in the reconstruction. To mitigate this issue, the variable, $\\beta$, for the KL divergence was gradually increased according to the following Equation:\n$B(e, w, \\beta_{min}, \\beta_{max}) = \\begin{cases}\n\\beta_{max} & \\text{if } e > w \\\\\nexp(\\frac{e}{w} (log \\beta_{max} - log \\beta_{min}) + log \\beta_{min}) & \\text{otherwise}\n\\end{cases}$\n(3)\n$\\tau = \\frac{e}{w}$\n(4)\nwhere $e$ is the current number of epochs, $w$ is the upper bound on the number of epochs, $\\beta_{min}$ is the minimum value set for $\\beta$, $\\beta_{max}$ is the maximum value set for $\\beta$, starting from $\\beta_{min} > 0$ and increasing exponentially to $\\beta_{max}$ exponentially as the learning progresses.\nWe empirically set $w = 10000$, $\\beta_{min} = 1e - 4$, $\\beta_{max} = 1$, and 1 epoch for the period of the increasing."}, {"title": "V. EVALUATION", "content": "The reconstruction quality of the generated WT and the controllability and generation time of the conditioning generation were evaluated. This research aimed to enhance the intuitiveness of timbre control and enable real-time timbre searches. To achieve this, the following three steps were essential: 1) high-quality reconstruction of the WT input to the model 2) conditional generation according to the values of the semantic labels, and 3) short processing time for generation and real-time use. The results obtained by performing the abovementioned steps are shown below. The source code of the model and the weights of the trained model used to obtain the results are also available"}, {"title": "A. Reconstruction Quality", "content": "The mean absolute error (MAE) was calculated to evaluate the reconstruction quality between the reconstructed waveform and the original input waveform. The calculated MAE amounted to 0.082 for the waveform state and 0.013 for the logarithmic amplitude spectrum. The logarithmic amplitude spectrum was calculated by repeatedly concatenating the six WTs as described in Section III-B to facilitate a comparison between the frequency components. The reconstructed waveforms are shown in Figure 3, which reproduce the input waveforms.\nThese results indicate that the reconstructed waveform successfully emulates the input waveform, capturing the essential characteristics of the WT. Future work will entail a comparison of our method with other models and conducting listening experiments to validate the reconstruction quality quantitatively."}, {"title": "B. Controllability of conditioning generation", "content": "To evaluate the controllability of the conditioning generation, the MAE and Pearson correlation coefficients of the output labels concerning the input labels (condition c) were checked. The output labels were computed from the output waveforms using the same procedure as that described in Section III-D. The higher number of minor differences between the input and output labels and higher correlation coefficients indicate better control of conditioning generation. Figure 4 illustrates a section of the results from the conditioning generation process.\nThe calculation procedure was as follows: A single input waveform, $x$, and an input label, $c$, from the dataset were selected. The input label, $c$, was a variable that comprised three precomputed values: bright, warm, and rich. From $c$, one evaluation label, $c_e$, was selected to evaluate the controllability of conditioning generation. The label $c_e$ was set in order of 20 equally spaced values in the range of [0, 1]. Labels that were not used for evaluation were also needed for conditioning generation. The labels obtained from the input waveforms were calculated using the same procedure as in Section III-D.\nThe output waveform, $\\hat{x}$, and the evaluation label, $\\hat{c_e}$, were obtained from the input waveform, $x$, and the input label, $c$. This procedure was repeated for the number of divisions of $c_e$ used as input (20 times in this research).\nThe transition between $c_e$ and $\\hat{c_e}$ was stored as an array of 20 points. The MAE and Pearson correlation coefficients were calculated and evaluated from the stored arrays. These processes were repeated for WT, and the averages were obtained with the three parameters, bright, warm, and rich, for the entire dataset.\nThe value of MAE averaged over bright, warm, and rich, were 0.19, 0.05, and 0.14, respectively. The warm value means the change is consistent with the label, whereas bright and rich remain challenging. The distribution of the training labels may have been biased, limiting the exposure of data to a sufficient range of labels for practical training.\nThe averaged values of the Pearson correlation coefficients were 0.97 for bright, 0.98 for warm, and 0.96 for rich. Strong positive correlations were confirmed for all parameters. This indicates that conditioning generation responds appropriately to the increase or decrease of each parameter."}, {"title": "C. Generation Time", "content": "The time required for generating WT was evaluated to ascertain the real-time performance capabilities of the proposed method. WT was generated 100 times, and the average generation time was calculated to be approximately 2.7 ms. The evaluations were conducted on a MacBook Pro (13-inch, M1, 2020) with an M1 Chip (CPU), utilizing PyTorch Lightning 1.7.7. This notably brief generation time suggests the feasibility of real-time operation of our method using only the CPU without requiring a specialized GPU for deep learning."}, {"title": "VI. SUMMARY AND FUTURE ISSUES", "content": "In this research, we proposed a method for semantic timbre control of WT using CVAE, and demonstrated the possibility of improving the synthesizer programming problem. The following is the rationale for the proposed method:\nThis method demonstrates that the reconstructed waveform can accurately follow the input waveform by effectively capturing the timbre features of the WT. Additionally, we have confirmed that conditioning generation can be controlled according to the intended outcomes. However, the inherent complexity of timbre variation indicates that more refined control is essential. Future research should explore increasing the number of conditioning labels to facilitate more nuanced sound manipulation, thereby contributing to the resolution of the synthesizer programming problem.\nResearch examining the application of deep learning to WTS is difficult to evaluate in a unified manner due to the variety of model architectures and research objectives[3], [11], [12]. As outlined in Section II-C, future research will need to develop quantitative metrics to assess the quality of waveform reconstruction and the controllability of conditioning, enabling better comparisons across different models and providing a more coherent basis for advancing WTS research..\nFurthermore, as detailed in Section V-B, the controllability assessment of the conditioning generation indicates significant MAE, particularly with bright and rich labels. The bias in training label distribution is likely the contributing factor, necessitating countermeasures. Results may be enhanced by augmenting the dataset size, altering label analysis, and normalization methods. Another strategy could involve adopting latent space and label feature separation mechanisms akin to those in Fader Networks [35].\nWith regards to the generation time, the results reveal that real-time use is possible using only the CPU without needing a particular GPU for deep learning.\nAs for real-time performance, model weight reduction methods such as model parameter quantization and branch pruning could further reduce the processing time of the proposed method."}]}