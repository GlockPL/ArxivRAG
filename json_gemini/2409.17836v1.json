{"title": "Language Models as Zero-shot Lossless Gradient Compressors: Towards General Neural Parameter Prior Models", "authors": ["Hui-Po Wang", "Mario Fritz"], "abstract": "Despite the widespread use of statistical prior models in various fields, such models for neural network gradients have long been overlooked. The inherent challenge stems from their high-dimensional structures and complex interdependencies, which complicate effective modeling. In this work, we demonstrate the potential of large language models (LLMs) to act as gradient priors in a zero-shot setting. We examine the property by considering lossless gradient compression \u2013 a critical application in distributed learning \u2013 that depends heavily on precise probability modeling. To achieve this, we introduce LM-GC, a novel method that integrates LLMs with arithmetic coding. Our technique converts plain gradients into text-like formats, enhancing token efficiency by up to 38 times compared to their plain representations. We ensure that this data conversion maintains a close alignment with the structure of plain gradients and the symbols commonly recognized by LLMs. Our experiments indicate that LM-GC surpasses existing state-of-the-art lossless compression methods, improving compression rates by 10% up to 17.2% across various datasets and architectures. Additionally, our approach shows promising compatibility with lossy compression techniques such as quantization and sparsification. These findings highlight the significant potential of LLMs as a model for effectively handling gradients. We will release the source code upon publication.", "sections": [{"title": "1 Introduction", "content": "Statistical prior models have been applied successfully in various fields, including image denoising and super-resolution [Ulyanov et al., 2018, Gandelsman et al., 2019], vision task adaptation [Wang et al., 2021], and low-resource language tasks [Baziotis et al., 2020, Brown et al., 2020]. However, their use in modeling neural network gradients has been largely neglected. The potential reasons for this oversight might include (1) the high-dimensional nature of gradients, which makes them less intuitive to analyze; (2) the difficulty of collecting representative gradient data; and (3) the significant challenge of ensuring generalizability to unseen data, given the substantial effort required.\nInstead of developing a model from scratch, this work investigates the potential of leveraging pre-trained large-scale language models (LLMs) as gradient priors in a zero-shot setting. We explore this potential through the lens of lossless gradient compression, a vital application in federated and distributed learning environments. The success of this compression heavily depends on precise probability modeling. An effective statistical model can significantly improve compression efficiency, whereas an inaccurate model may lead to poorer compression outcomes and could even increase the data size post-compression."}, {"title": "2 Related work", "content": "Large-scale language models. Language models aim to model the relation between texts. This problem has been extensively studied in recent decades via various approaches such as statistical models [Jelinek, 1998] and recurrent neural networks [Hochreiter and Schmidhuber, 1997]. Recently, the emergence of transformer-based models [Vaswani et al., 2017] along with large-scale text corpora has revolutionized the entire field, driving research into large-scale language models (LLMs). Models, such as those from GPT [Achiam et al., 2023, Brown et al., 2020] and LLAMA [Touvron et al., 2023, Zhang et al., 2024, Geng and Liu, 2023] families, are capable of solving diverse tasks in natural languages and demonstrate incredible generalizability toward unseen novel tasks, even across modalities [Mirchandani et al., 2023, Gruver et al., 2024]. Notably, recent work by Deletang et al. [2024] also explores the use of language models as general compressors. Our goal is to investigate the potential of LLMs as a strong prior specifically for gradients. Additionally, we offer practical considerations for handling floating-point data when structures exist within the data to be compressed.\nIn this work, we demonstrate for the first time that LLMs can understand the structure of network gradients, accurately modeling their probability distribution in a fully zero-shot manner. We verify our finding by taking LLMs as priors for arithmetic coding, yielding state-of-the-art lossless gradient compression under various settings.\nDeep generative priors. An ongoing research direction beyond traditional statistical modeling is learning a deep generative model from massive data and leveraging the model as a \"deep\" prior. The concept has been widely considered in many applications, such as image denoising and super-resolution [Ulyanov et al., 2018, Gandelsman et al., 2019], vision task adaptation [Wang et al., 2021, Chang et al., 2019], and low-resource language tasks [Baziotis et al., 2020, Brown et al., 2020]. Although strong priors can facilitate various downstream applications, training such models for gradients can be costly and challenging due to their high dimensionality. Additionally, the generalizability of these models is often a concern and may be limited to specific types of networks [Ha et al., 2016, Wang et al., 2024b]. Instead of training a model from scratch, our work explores the potential of using off-the-shelf LLMs as strong priors over gradients. This will minimize the cost of training deep prior models and may inspire applications like gradient denoising and anomaly detection."}, {"title": "3 Background", "content": "In this work, we aim to explore the potential of using LLMs as prior for gradients and leverage lossless compression as an examination task. We review the essential background knowledge below.\nLossless compression. The fundamental principle of lossless compression is to reduce the size of data while ensuring it can be fully reconstructed. This is typically achieved by eliminating the statistical redundancy inherent in the data. Given a sequence of symbols $s_{0:N} \\in S$ drawn from a probability distribution $P_S$, the objective is to devise a compression function $g: S \\rightarrow C$. This function maps the original data $s$ to a unique (decodable) binary code $c$, ensuring that the length of $c$, denoted by $l(c)$, is less than or equal to the length of $s$, $l(s)$. The source coding theorem [Shannon, 2001] states that the expected minimum length of a coded message $c$ cannot be shorter than the Shannon entropy of the original data, denoted as $l(s) \\geq H(S)$. Here, $H(S) := E_{s\\sim P_S} [-\\log_2 P_S(s)]$ represents the entropy. This implies that any compression resulting in a length shorter than $H(S)$ necessarily involves loss of information, preventing perfect reconstruction of the original data.\nArithmetic coding. As a means to achieve lossless compression, arithmetic coding [Rissanen and Langdon, 1979] provides a nearly optimal message length $H(S) \\leq l(c) \\leq H(S) + 2/l(s)$ on"}, {"title": "4 LM-GC", "content": "We introduce LM-GC, a method that integrates arithmetic coding with pre-trained large language models (LLMs) to address the lack of gradient-specific priors in arithmetic coding. It's important to note that LLMs are originally trained on extensive text corpora and do not encounter gradients or model parameters during this training. A significant challenge is enabling LLMs to comprehend the structure of gradients. Our method involves two main steps: serialization and compression. In serialization, we convert the 32-bit floating points of gradients into a format understandable by LLMs, which we call grouped text. This text is then fed into the LLMs, which predict the probability of each token in an autoregressive manner and thus accomplish compression using arithmetic coding.\nSerialization. We first note that gradients are represented as 32-bit floating points, with values ranging from $-3.40282347 \\times 10^{+38}$ to $-1.17549435 \\times 10^{-38}$. Due to significant variations in their magnitudes and the often ambiguous importance of each gradient element, directly inputting these values into large language models (LLMs) is impractical. LLMs have a fixed token limit, and representing a single gradient in plain form would consume excessive tokens, compromising the context's depth.\nTo address this, our method, LM-GC, initially divides the floating points into several disjoint 4-bit partitions, which are then encoded into hexadecimal numbers as illustrated in Figure 1. This encoding strategy allows for a token savings of approximately 38 times compared to using plain gradients, particularly under extreme value conditions.\nFurthermore, we organize every eight decoded hexadecimal numbers (equivalent to 4 bytes) by inserting a separator between them. This format provides LLMs with a structured representation of how a floating point number is typically presented. Our experiments demonstrate that separators are"}, {"title": "Compression", "content": "After serializing the gradients into grouped text $S$, we process this text through a tokenizer to generate a set of tokens $T$. These tokens are then fed into a pre-trained large language model (LLM), denoted by $M$, which predicts the probability of each token as follows:\n$P_{LM}(T) := \\prod_{k=1}^{K} p(t_k|BOS, t_{<k}).$ \nThis equation indicates that the LLM sequentially predicts the probability of the next token, starting with the BOS (beginning of sequence) token, which is used to calculate the probability of the first token, $P_{LM}(t_1) = M(BOS)$. The BOS token serves primarily as a contextual cue and is not included in the compression.\nDuring compression, $P_{LM}(T)$ acts as the statistical model, $P_{AC}$, for arithmetic coding. In the decompression phase, the process begins with the BOS token, retrieving $P_{LM}(t_1)$ to decode the first token. This decoding cycle continues until the maximum window size of the LLM is reached."}, {"title": "5 Experiments", "content": "5.1 Setup\nDatasets and models. Our experiments consider three types of LLMs as the compressor, including Tinyllama 1.1B [Zhang et al., 2024], Openllama 3B [Geng and Liu, 2023], and LLAMA 2 7B [Touvron et al., 2023], ranging from a smaller to medium model size. All models can accept up to 4096 tokens. Unless stated otherwise, we use a context window size of 2048 by default. To ensure generalizability, we conduct experiments on four model architectures including a three-layer convolution net (ConvNet), VGG-16 [Simonyan and Zisserman, 2015], ResNet [He et al., 2016], and vision transformer (ViT) [Dosovitskiy et al., 2021]. The models are trained on three datasets, MNIST [LeCun et al., 2010], CIFAR-10 [Krizhevsky et al., 2009], TinyImageNet [Le and Yang, 2015], under different settings. MNIST is a digit classification task containing 10 digits and 60000 images. On the other hand, CIFAR-10 and TinyImageNet are image classification tasks. CIFAR-10 contains 50000 images of 10 classes, while TinyImageNet contains 100000 images for 200 classes. All images are rescaled to 32 by 32 in the experiments.\nEvaluation protocols. If a prior describes data well, lossless compression can achieve better efficiency. To test the efficiency of using LLMs as priors, we first train models on different datasets for 200 epochs, collecting gradients every 200 batch steps, resulting in a data pool of approximately 400 checkpoints for compression evaluation. Due to computational time constraints, we sub-sample 10 checkpoints from the pool for the subsequent experiments unless stated otherwise. All the experiments are repeated at least three times, and the standard deviations are reported accordingly. We measure compression efficiency by the compression rates defined as follows.\nCompression Rate (%) = 100 \\times \\frac{\\text{Compressed Data Size}}{\\text{Original Data Size}}\nBaselines. We compare our method to state-of-the-art lossless compression techniques that originally targeted different data types. PNG [Boutell, 1997] is one of the most common lossless compression codecs for images. On the other hand, FLAC [Coalson, 2008] is a common audio compression format. Lastly, LZMA [Pavlov, 2019] and GZIP [Deutsch, 1996] are codecs used by 7-zip software and 7z compression format. FPZIP [Lindstrom and Isenburg, 2006] is proposed for scientific floating-point data, particularly suitable for data with up to 4D structures.\nImplementation. We implement our method in Pytorch and Huggingface. The checkpoints of pre-trained LLM models are loaded from the Huggingface hub. We adapted the arithmetic coding from Torchac to fit our application. We run our experiments on a cluster with NVIDIA A100 40GB GPUs and AMD EPYC 7402 24-Core Processor. All of the experiments can fit in one single A100."}, {"title": "5.2 Compression effectiveness", "content": "We first conduct compression experiments on gradients collected from a ConvNet trained on CIFA-10 to show that LLMs can model gradients even without seeing such data during training. Our method considers three LLMs, namely Tinyllama, Openllama, and LLAMA 2, as the priors for arithmetic coding. We also consider 6 types of serialization, including decoding every byte with ISO-8859-1 (ISO), projecting every 4 bits to hexadecimal numbers without separators (Hn), and with space (Hg), commas (He), space and commas (Hc+s), and semicolons (Hsemi) as the separators. These settings outline the importance of serialization and its effect on gradient modeling. We report two settings for the baselines. The first is a chunked version, where the compressor sees a chunk of size 512 bytes every time, whereas the other one, namely the unchunked version, takes advantage of the pseudo infinitely large context length to yield the best statistical modeling.\nTable 1 shows that our LM-GC consistently outperforms baseline codecs when serialization is properly managed. For example, ISO and Hn for LLAMA 2 perform worse than the baselines. In particular, ISO encodes gradients into symbols less familiar to LLMs, yielding up to 70% performance difference compared to settings like H. The lack of separators may confuse language models, causing performance degradation of 40% on LLAMA 2. These results highlight the crucial role of serialization in aiding LLMs' understanding. Furthermore, compression efficiency increases as the model size grows from 1.1B to 7B, suggesting that more sophisticated models may better understand the relationships between data elements, resulting in more effective compression."}, {"title": "5.3 Ablation study", "content": "In this section, we provide a series of ablation studies to provide insights into how design choices affect LLMs and the resulting prior models.\nArchitectures. To further understand the generalizability to different architectures and the effect of serialization, we continue with an experiment on different architectures. We extend the experiments to three additional architectures. VGG-16 contains deeper layers compared to ConvNets. ResNet-18 further introduces skip-connections and batch normalization, verifying our LM-GC on common design choices in modern machine learning. Lastly, ViT is built upon transformer blocks, showing that LLMs can reason beyond convolution layers. As shown in Table 2, we first observe that the performance of all methods drops as the models become more complicated, while our method remains the best among the baselines. This finding suggests that our method can better capture complex structures within the gradients. Moreover, we observe that serialization with separators generally performs better than the one without separators. It outlines the importance of separators, especially when the data to be compressed becomes more intricate.\nDatasets. The previous experiment suggests that LM-GC models gradients more accurately than the existing baselines, especially when considering complex structures. We further explore this dimension by considering two additional datasets, MNIST and TinyImageNet. Table 3 presents the result comparing our method with Tinyllama to the baselines. Datasets like TinyImageNet introduce higher compression difficulty due to the complex task. However, LM-GC demonstrates consistently promising performance across all datasets. The improvement over the best baselines (FPZIP) increases as the dataset becomes sophisticated. This finding aligns with the result in Table 2 that our method is generalizable and better at capturing complex structures than the existing codecs that are not optimized for gradient compression.\nContext window size. LM-GC takes LLMs as prior over gradients. One natural question is whether the LLMs really consider the context and yield accurate probability modeling. Ideally, similar to the traditional codec, if we provide a larger context window, the statistical model should be able to reason from the context and thus result in higher compression efficiency. Instead of using a default context window size of 2048 tokens, we conduct an ablation study in Fig. 2. The result shows that the performance drastically improves when the context window size increases, suggesting that LLMs indeed leverage the context. However, we note that the improvement seems to be saturated at the end. A larger context window also implies higher hardware resource demands, leaving a potential trade-off in practice."}, {"title": "Byte grouping", "content": "In addition to the decoding schemes analyzed in the previous experiments, we demonstrate that grouping converted text significantly affects performance. Recall that a floating point consists of 1 bit for the sign, 8 bits for the exponent, and 23 bits for the mantissa. Components with the same functionality should be grouped as closely as possible. To verify this hypothesis, we conducted experiments on TinyLLAMA and LLAMA 2 with bytes per group (BPG) set to 1, 2, 3, 4, 8, and none (i.e., no grouping, denoted as Hn). The results in Fig. 3 show that BPG set to 1, 2, and 4 (our default setting) perform the best, while BPG equal to 3, which covers three components, and none perform worst. It indicates that serialization should resemble the structure of data to be compressed. Notably, although BPG equal to 1 and 2 performs well on both models, smaller BPG will add more separators and increase the total amount of tokens, introducing the additional computation overhead to the compression.\nComparison to run-length encoding. Lastly, we compare our method to run-length encoding, the simplest adaptive compression scheme, as shown in Table 4 in the appendix. The results indicate that although serialization may slightly improve compression rates, run-length encoding is ineffective for compressing gradients. Combined with the earlier findings, this suggests that simple adaptive methods are unsuitable for handling complex yet structured gradient data."}, {"title": "5.4 Compatibility", "content": "Gradient compression, a crucial technique in federated learning, typically involves lossy compression methods. We demonstrate that our LM-GC approach is compatible with lossy techniques such as quantization and sparsification. Specifically, we consider linear quantization, which uniformly"}, {"title": "$v=max(\\frac{v-min_v}{max_v - min_v} \\times (2^n - 1)).$", "content": "In practice, only the indices $I \\in \\{0,\\dots,n\\}$ for each element are communicated. Therefore, we map the data to the indices before conducting compression. Moreover, we consider sparsification, which selectively transmits a subset of gradients based on the specified proportion. When considering sparsification, it is important to note that the gradients remain as 32-bit floating points. For this experiment, we investigate quantization levels of 16, 8, and 1 bit (i.e., SignSGD [Bernstein et al., 2018]), and sparsification levels of 50%, 25%, and 10%.\nWe present a compatibility analysis in Fig. 4. The results indicate that integrating lossless compression techniques such as LZMA and LM-GC enhances compression rates beyond plain lossy compression. However, LZMA shows limited improvement across all settings, particularly with sparsification. In contrast, our method consistently delivers improvements across all settings, achieving notable compression rates in addition to lossy compression. These findings underscore the potential of LLMs as a prior for gradient compression, even with the incorporation of additional compression schemes, suggesting a promising new research direction in leveraging LLMs for compression."}, {"title": "6 Discussion and Limitation", "content": "Throughput. Despite the promising performance and generalizability, the throughput of LM-AC can be further optimized. Currently, our approach requires approximately 4 hours to compress just 28 MB. This bottleneck arises primarily from two components: LLMs and arithmetic coding. For LLMs, performance can be accelerated through techniques such as quantization [Frantar et al., 2023], faster attention mechanisms [Dao et al., 2022], KV cahce Hooper et al. [2024], and model pruning Ma et al. [2023]. Looking ahead, one could explore distilling language models [Hsieh et al., 2023], as many functionalities may not be necessary during compression. Additionally, our implementation is significantly hindered by arithmetic coding and CPU limitations. Adopting a more efficient implementation, such as pure C++ programs, or utilizing CPUs with superior single-thread processing speeds could effectively mitigate these constraints.\nBroader impact. Our work highlights the potential of leveraging pre-trained LLMs as priors for gradients. Immediately, this offers an advanced tool for gradient compression that reduces resource demands in federated and distributed learning environments. Over time, these priors could be utilized for gradient denoising, enhancing differential privacy training, or identifying adversarial gradients concealed within federated learning clients. However, this approach may also enable more subtle adversarial gradients, guided by these stronger priors."}, {"title": "7 Conclusion", "content": "We presented LM-GC, the first lossless gradient compressor that integrates arithmetic coding with LLMs as prior models for gradients. Our experiments show that pre-trained zero-shot LLMs are highly effective as gradient priors, setting a new state-of-the-art for gradient compression. Additionally, our findings indicate that the precise serialization of gradients substantially improves the reasoning abilities of LLMs and significantly impacts compression performance, warranting further exploration. The versatility of LM-GC sets the stage for developing more sophisticated gradient compression methods that directly incorporate LLMs. Overall, while our results in zero-shot settings are promising, the potential of expanding this approach to include few-shot learning, prompt engineering, and optimization of throughput efficiency remains open for further exploration."}, {"title": "A Run Length Encoding", "content": "We additionally compare our method to run-length encoding (RLE). RLE compresses data by counting the consecutive symbols and replaces the original data with a series of (counts, symbol) tuples. It serves as a simple adaptive compression codec without knowing data characteristics. The experiment extends from Table 3, compressing gradients collected during training a ConvNet on TinyImageNet. We consider three types of dictionaries: binary, hexadecimal without separators (Hn, Table 1), and iso-8859-1 (extended ASCII to handle negative numbers). These methods use 1, 4, and 8 bits to represent symbols and always use 8 bits for counting. Note that this setting is favorable to RLE since gradient lengths can easily exceed 256 (8 bits).\nThe results are presented in Table 4. While different codebooks improve the efficacy of RLE, RLE failed to compress the data and even increase the data size. On the other hand, our method clearly outperforms RLE, indicating that simple adaptive priors are ineffective for gradients."}]}