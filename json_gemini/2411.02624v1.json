{"title": "Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach", "authors": ["Minghao Ning", "Yaodong Cui", "Yufeng Yang", "Shucheng Huang", "Zhenan Liu", "Ahmad Reza Alghooneh", "Ehsan Hashemi", "Amir Khajepour"], "abstract": "This paper presents a novel real-time, delay-aware cooperative perception system designed for intelligent mobility platforms operating in dynamic indoor environments. The system contains a network of multi-modal sensor nodes and a central node that collectively provide perception services to mobility platforms. The proposed Hierarchical Clustering Considering the Scanning Pattern and Ground Contacting Feature based Lidar Camera Fusion improve intra-node perception for crowded environment. The system also features delay-aware global perception to synchronize and aggregate data across nodes. To validate our approach, we introduced the Indoor Pedestrian Tracking dataset, compiled from data captured by two indoor sensor nodes. Our experiments, compared to baselines, demonstrate significant improvements in detection accuracy and robustness against delays. The dataset is available in the repository\u00b9.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, intelligent indoor autonomy technology is gaining recognition and attention among healthcare professionals and researchers. Studies have shown that indoor transportation is the most urgent need from healthcare staff in hospitals and long-term care [1]. This rising demand is largely driven by workforce shortages and the high incidence of chronic injuries among healthcare staff, which often caused by transporting heavy materials. However, large scale commercial deployment of intelligent robotics platforms are still limited. Most existing indoor robots are designed to operate independently, relying on their built-in sensors to navigate and perform tasks. This restricts their effectiveness in the congested, dynamic, and unpredictable spaces of healthcare facilities. This paper presents a cooperative perception system consisting of a network of multiple sensor nodes, and a central node, to provide perception results/services to robotic mobility platforms. This system aimed to improve the operational safety and environmental awareness of intelligent robotic platforms, including autonomous hospital beds and delivery robots.\nThere are several challenges associated with developing a cooperative perception system in densely populated indoor environments, such as hospitals. One primary challenge for local perception is the fast and accurate fusion of perception data from multiple sensor nodes. This task is complicated by the dynamic behavior of people within a confined space, which involves close interactions between individuals. For instance, people travel in small groups, or crossing paths at close quarters. These situations pose significant difficulties in maintaining consistent tracking identities across different nodes and merging perception data effectively. The physical layout of indoor environments presents another significant challenge for local perception. Architectural features and decorative elements, such as corners, pillars, and mirrors, present significant challenges in achieving continuous and accurate coverage across the entire area. These environmental factors can obstruct the sensor field of view and distort the sensor signal, leading to gaps in coverage or inaccuracies in perception.\nThe processing and communication delays poses a major challenge for global/cross-sensor perception in highly dynamic indoor environments. These cross-node delays can lead to the receipt of outdated or inaccurate representations of the dynamic environment at the center node. This impairs the center node's ability to generate a cohesive and current understanding of the environment.\nTo address these challenges, this paper proposes a delay-aware cooperative perception system designed for dynamic indoor environment. An overview of the proposed system is illustrated in Fig. 1. Our contribution can be summarized as follows:\n\u2022\tAn adaptive clustering method coupled with ground-contact point-based LiDAR-camera fusion, enhancing the accuracy and reliability of local perception.\n\u2022\tA delay-aware global perception framework that accounts for messaging delays and latency, ensuring timely and cohesive environmental understanding.\n\u2022\tThe creation of a multimodal cooperative indoor perception dataset specifically designed for dynamic and crowded healthcare environments. This provids a valuable resource for further research and development in this field.\nThe rest of the paper is organized as follows, in section II, the related methods and dataset are reviewed, in section III, the overview of our method is presented, in section IV, the experiments and discussion are presented, and finally in section V the impact of our work is concluded."}, {"title": "II. RELATED WORK", "content": "Existing indoor infrastructure-based perception system often relies on basic sensors and cameras, which either lack high-level semantic understanding or precise measurement of object positions. In [2], four Pyroelectric Infrared (PIR) sensors are combined as a sensor node and mounted on the ceiling to detect object trajectory. In [3] Radio frequency identification (RFID) is used to track objects embedded with RFID tags. Although these methods provide basic tracking functionalities, their perception range and accuracy are very limited. In [4]\u2013[6] infrastructure-based cameras are used to detect and track pedestrians. However, such standalone pure vision-based systems are sensitive to lighting variations and occlusions and cannot accurately localize objects in 3D space. Alternatively, Brvsvcic et al. leveraged a combination of infrastructure-based RGB-D cameras, LiDAR, and marker-based motion tracking systems [7]. However, the cost of such setup makes them impractical for large-scale deployment. A more recent study used a motion capture system capable of producing ground truth data at a 100 Hz rate [8]. Despite its high accuracy, it is limited to areas where motion capture technology is available. These challenges highlight the need for more robust and cost-effective perception systems capable of operating reliably under the complex conditions typical in indoor settings."}, {"title": "III. METHODOLOGY", "content": "As shown in Fig. 2, the proposed delay-aware cooperative system comprises two main components: local perception for sensor nodes and delay-aware global perception on a center node. Each sensor node is equipped with dual cameras, a LiDAR sensor, 5G/wireless communication capabilities, and a Jetson Orin NX for edge computing. These nodes process multi-modal sensory data locally to produce tracked object lists. By integrating edge computing capabilities, we aim to reduce the overall system latency. The center node aggregates and combines the structured perception results of the sensor nodes to generate a holistic view of the dynamic indoor environment. This configuration allows for real-time detection and tracking of dynamic elements across multiple nodes in complex indoor settings."}, {"title": "A. Local Perception", "content": "The local perception can be summarized into: cross-node sensor synchronization; camera based 2D bounding box detection; ROI points filtering; hierarchical clustering considering the scanning pattern; ground contacting feature based Lidar camera fusion; and class-aware object tracking.\n1) Cross-sensor Sensor Synchronization: To improve the accuracy of global fusion, the proposed framework employs cross-sensor soft synchronization mechanism to reduce delay in the captured and processed data. As shown in Fig.3, each sensor node coordinates LiDAR scans with camera shutter operations through the use of soft trigger signals. This trigger signal ensures that the data captured from both modalities are temporally aligned. The generation of these soft trigger signals is based on a synchronized clock system. Each node's clock is synchronized to ensure the uniformity of trigger signals across all sensor nodes. This alignment allows for simultaneous data capture between nodes. This synchronization significantly reduces discrepancies during the global fusion process and improves the accuracy of the global perception's output.\n2) Camera-based 2D Detection: For camera-based 2D object detection, we employ a custom YOLOv8 [12] model trained on our dataset. Standard YOLO models trained on COCO(Common Objects in Context) dataset [13] doesn't generalize well on the proposed infrastructure view, and it can not detect the ground contacting features (like foot for person). So a customized dataset including person, foot, and robot bed labels are created to retrain the YOLOv8 and evaluate its performance.\n3) ROI points filtering: As the sensor nodes are fixedly installed, a static binary grid is created as the region of interest to filter out unnecessary points, such as points on the wall or ground.\n4) Hierarchical Clustering Considering the Scanning Pattern: Common clustering methods like DBSCAN [14] assume the points are spatially uniformly distributed, where the Euclidean distance between points of the same cluster should scale equally along different axes of the Cartesian Coordinates. However, this assumption fails for the wildly used mechanical rotating Lidar, where the resolution along the horizontal direction is much finer than that along the vertical direction. Thus, careful clustering parameter tuning for the clustering distance threshold e is required to have a better trade-off between the under-segmentation and the over-segmentation issues. As shown in Fig.4, large e tends to occur under-segmentation, and small e leads to over-segmentation. However, no proper e exists in this case that can properly cluster the two persons and the robot bed, as the distance between the upper right corner of the bed and its nearby person is smaller than the distance between the points from the two nearby scanning lines at the right side of the bed.\nTo address the above issue, an efficient hierarchical clustering method considering the scanning pattern is proposed. First, points from different scanning lines are clustered separately based on the adaptive euclidean distance \u20ac(s),\n$\u20ac(s) = N_{min} \u2206\u03c6s$ (1)\nwhere s is the distance from the point to the Lidar, $N_{min}$ is the minimum number of points required to form a core region in DBSCAN, and \u2206\u03c6 is the horizontal resolution of the Lidar.\nThen, a custom distance metric considering the scanning pattern is proposed to group the segments of each scanning lines from the first step. Each segment contains the points and other features like the ring index of the scanning line, the centroid calculated as the mean of the cluster points,"}, {"title": "Algorithm 1 Efficient Hierarchical Clustering Considering Scanning Patterns", "content": "1: function DISTANCEMETRIC(segment_a, segment_b)\n2: Input: segment_a, segment_b segments with properties (ring index, mean point, range)\n3: Output: distance - customized distance metric\n4: /* Preliminary checks to speed up computation: */\n5: if large ring index or mean point distance then\n6: return INF \u25b7 Segments too far apart in ring index or spatially\n7: end if\n8: /* Calculate custom distance metric: */\n9: Compute spatial distance d between mean points and normalize it to get $d_{norm} = d/(min(s_a, s_b)\u2206\u03b8)$, where \u2206\u03b8 is the vertical resolution\n10: Compute angle intersection $\u03c6_a$ and normalize it to get $Y_{\u03b8_{norm}} = 1 - \u03c6/(min(||\u03c6_a||, ||\u03c6_b||))$\n11: Compute distance $d_{custom} = d_{norm} + Y_{\u03b8_{norm}}$\n12: return Custom distance metric $d_{custom}$\n13: end function\nthe azimuth angle \u03c6 range denoting the start and the end scanning angle for this segment. The custom distance for any two segments is calculated based on Algorithm 1. The segments whose distance is less than threshold $E_{custom}$ will be grouped into the same cluster.\nIt is worth noting that this hierarchical clustering is faster than the DBSCAN, as distance calculation across different scanning lines has reduced from point-to-point to segment-to-segment. This improvement greatly reduce the computational time when the number of points increases as shown in Fig. 5.\n5) Ground Contacting Feature based Lidar Camera Fusion: The camera-based 2D detection results are fused with the pointcloud clustering results to assign semantic labels to the clusters. The fusion of 2D bounding box and point cloud clusters is challenging when objects are crowded, which create occlusion on the image view. For instance, when a group of people travel closely together. To solve the fusion problem in a cluttered scene, a ground contacting feature-based Lidar camera fusion method is proposed. Specifically, the camera projection matrix as shown in Eqn.2 is used to estimate the actual position of the object in the world coordinate based on the detected 2D bounding box.\n$s\\begin{bmatrix} X_p \\ Y_p \\ 1 \\end{bmatrix} = K[R t]\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix} = H_{3x4}\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$ (2)\nwhere s is a scale factor, $X_p, Y_p$ are the pixel coordinates, K is the camera intrinsic matrix, R is the rotation matrix, t is the translation vector, $X_w, Y_w, z_w$ are the world coordinates. The ground contacting feature bounding box (like foot) will be first associated with its parent bounding box (like person) based on the bounding box overlap ratio and cosine distance with the z-axis vanishing point $v_z$, i.e., the pixel coordinate where all lines parallel to the z-axis in the world coordinates intersect. The overlap ratio is computed as the area of intersection between two boxes, divided by the minimum area of the two boxes. The cosine distance is the cosine of the angle between the vectors from the vanishing point to the centroids of two boxes. The vanishing point $v_z$ is calculated based on the camera matrix H\n$v_z = (h_{13}/h_{33},h_{23}/h_{33})$ (3)\nThen, the actual position, i.e. $X_w$ and $Y_w$, of the object is derived based on Eqn.2 given its pixel coordinates $X_p, Y_p$ and its height $z_w$:\n$\\begin{aligned}a_{11} &= h_{31}x_p - h_{11}, a_{12} = h_{32}x_p - h_{12} \\ a_{21} &= h_{31}y_p - h_{21}, a_{22} = h_{32}y_p - h_{22} \\ b_1 &= (h_{13}-h_{33}x_p)z_w + h_{14} - h_{34}x_p \\ b_2 &= (h_{23}-h_{33}y_p)z_w + h_{24} - h_{34}y_p \\ X_w &= \\frac{b_1a_{22}-b_2a_{12}}{a_{11}a_{22} - a_{12}a_{21}} \\ Y_w &= \\frac{b_2a_{11}-b_1a_{21}}{a_{11}a_{22} - a_{12}a_{21}}\\end{aligned}$ (4)\nFinally, the 2D boxes are associated with the clusters using Hungarian algorithm to minize the overall association costs. The association cost for each pair of 2D box and cluster is the sum of the overlap ratio of camera box and the projected cluster box, and the Euclidean distance between the 2D box based estimated position and its cluster centroid."}, {"title": "B. Delay-aware Global Perception", "content": "To address the inherent challenges associated with the processing and communication of high volumes of data in real-time, our framework incorporates a delay-aware fusion algorithm within the center node. This algorithm utilizes precise timestamps from the detected object lists received from the sensor network. It then compares these with the current time to assess the delay encountered during data transmission from the sensor nodes to the central node. The central node then predicts the current positions of the detected objects based on type-based motion models.\nFor pedestrian class objects, we use a non-linear motion model Eq. 5 that considers both the speed and direction of movement, allowing the model to anticipate changes in a person's trajectory. This prediction is important for improving the accuracy of cross-node fusion, especially in complex scenarios involving multiple dynamic objects in close proximity. After delay compensation, the center node combines these adjusted object lists by applying a weighted fusion strategy. This process ensures an accurate and up-to-date representation of the environment despite delays in data transmission.\n$\\begin{aligned}X_{k+1} &= X_k + V_{x_k} cos(yaw_k)\u2206t, \\ Y_{k+1} &= Y_k + V_{x_k} sin(yaw_k)\u2206t, \\ yaw_{k+1} &= yaw_k + w_{z_k}\u2206t, \\V_{x_{k+1}} &= V_{x_k}, \\w_{z_{k+1}} &= w_{z_k}.\\end{aligned}$ (5)"}, {"title": "IV. EXPERIMENTS", "content": "To assess the performance of the proposed algorithms, the Indoor Pedestrian Tracking dataset is created using data gathered from two indoor sensor nodes. It comprises 3,248 frames, featuring up to nine pedestrians and one hospital bed, with a total number of 22,857 objects labeled as tracked objects using CVAT [15]. On average, there are 7.04 objects per frame in this dataset.\nIn detail, it consists of three distinct scenarios: 1) a challenging case with nine pedestrians, testing the algorithm's ability to handle high pedestrian traffic; 2) a scenario with four pedestrians, allowing for detailed analysis of tracking precision; and 3) a unique setting that includes a hospital bed and three pedestrians, focusing on the interaction between an autonomous hospital bed and humans in medical or assisted-living environments.\nFor the data labeling, LiDAR point clouds are initially filtered based on height and ROI, then cropped to remove ground points. The resultant point clouds are projected into a bird's-eye view for data labeling. Finally, the position and orientation of objects are labeled as bounding boxes on the bird's-eye view images.\nFor evaluation, precision, recall, and average distance error (Avg. DE) are adopted to assess the accuracy of object detection."}, {"title": "B. Local Perception Evaluation", "content": "The results of the local perception evaluation, as depicted in Table II, show the comparative performance of two DBSCAN configurations against our proposed method. DBSCAN1 utilizes a parameter setting of e = 0.3m and $N_{min} = 4$, contrasting with DBSCAN2's configuration of \u20ac = 0.3m and $N_{min} = 8$.\nWithin the context of the 9 people scenario, our approach significantly outperforms the competing methodologies, achieving a precision of 0.9696 and a recall of 0.966 for Node1, coupled with a precision of 0.9649 and a recall of 0.9773 for Node2. These results suggest superior performance in scenarios characterized by crowded conditions and potential occlusions. Despite DBSCAN2 achieving marginally greater precision in Node2, it suffers a considerable drop in recall, highlighting a limitation in detecting all relevant objects within a crowded environment.\nIn the 4 people scenario, our method sustains high levels of both precision and recall, underscoring its efficacy. In contrast, normal DBSCAN experiences a compromise between precision and recall, which suggests its limitation to balance object detection with false positive mitigation effectively.\nThe 3 people, 1 bed scenario introduces substantial challenges to standard DBSCAN configurations, particularly affecting DBSCAN1, where the difference in point densities leads to a notable drop in precision. This can be attributed to the oversegmentation issues, where the bed is erroneously clustered into multiple groups, resulting in an increased false positive rate and consequently, reduced precision. Conversely, our method demonstrates consistent high precision and recall across this scenario, underscoring its resilience in environments with variable point densities.\nThe average distance error is another critical factor in evaluating the performance of these methods, with our method exhibiting lower Avg. DE values across the majority of scenarios and nodes. This metric further demonstrates the spatial accuracy of our method in object localization tasks."}, {"title": "C. Delay Mitigation", "content": "The latency distribution for the communication between the sensor node and the center node over 5G is depicted in Fig. 6. This distribution can be approximated by a Gaussian model, with a mean latency of 52.7 ms and a standard deviation of 7.9 ms. In the experiment, we first simulate this latency distribution with a mean of 50 ms and a standard deviation of 8 ms. To further explore the delay effects on system performance, we then mean latency to 100 ms and 150 ms, while keeping the standard deviation unchanged.\nWe compare our proposed delay mitigation method with a baseline method under three simulated latency configurations, the results are summarized in Table III. The delay-aware method consistently outperformed the baseline in terms of precision and average distance error across all scenarios and delay settings. This trend becomes more evident as the delay increased, with the delay-aware system maintained an averaged 18% precision improvement over the baseline. In scenarios with fewer dynamic elements, the improvements were still noticeable, although the differences in recall were less consistent. For example, in the 3 people scenario with a 100 ms delay, although the recall decreased slightly from 0.7338 to 0.7002, the precision saw a significant increase from 0.8053 to 0.8701."}, {"title": "Discussion on Delay Mitigation", "content": "The improved performance of the delay-aware method can be attributed to its capability to compensate for network-induced delays, thereby improving the accuracy of object fusion and synchronization across sensors. This is particularly important in densely populated environments where precise localization is necessary for safe and effective robot navigation. The reduction in average distance error also indicates the system's ability to align data temporally.\nVariations in recall of the proposed method are caused by duplicate objects after fusion. When pedestrians change direction unexpectedly in regions where sensor nodes overlap, motion prediction can result in incorrect fusion outcomes. This trade-off between detection coverage (recall) and detection accuracy (precision) is a common challenge in real-time perception systems and warrants further investigation to optimize both aspects."}, {"title": "V. CONCLUSION", "content": "This paper presented a cooperative perception system designed for intelligent mobility platforms in dynamic indoor settings, focusing on healthcare facilities. Our system integrates a network of multi-modal sensor nodes with a central node to address the challenges of crowded and unpredictable environments. We introduced novel algorithm designs, such as hierarchical clustering considering scanning patterns, ground contacting feature-based LiDAR camera fusion and delay-aware perception. The proposed approach significantly improves detection accuracy and operational safety, critical in crowded indoor settings. Experimental results from the Indoor Pedestrian Tracking dataset demonstrate our system's advantages over traditional baselines in terms of detection precision and delay robustness.\nFuture research will aim to extend this proposed framework to the transportation setting, such as traffic intersection or a specific section of road."}]}