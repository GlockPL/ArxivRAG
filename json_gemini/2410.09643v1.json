{"title": "Multimodal Physical Activity Forecasting in Free-Living Clinical Settings: Hunting Opportunities for Just-in-Time Interventions", "authors": ["Abdullah Mamun", "Krista S. Leonard", "Megan E. Petrov", "Matthew P. Buman", "Hassan Ghasemzadeh"], "abstract": "This research aims to develop a lifestyle intervention system, called MoveSense, that forecasts a patient's activity behavior to allow for early and personalized interventions in real-world clinical environments. We conducted two clinical studies involving 58 prediabetic veterans and 60 patients with obstructive sleep apnea to gather multimodal behavioral data using wearable devices. We develop multimodal long short-term memory (LSTM) network models, which are capable of forecasting the number of step counts of a patient up to 24 hours in advance by examining data from activity and engagement modalities. Furthermore, we design goal-based forecasting models to predict whether a person's next-day steps will be over a certain threshold. Results: Multimodal LSTM with early fusion achieves 33% and 37% lower mean absolute errors than linear regression and ARIMA respectively on the prediabetes dataset. LSTM also outperforms linear regression and ARIMA with a margin of 13% and 32% on the sleep dataset. Multimodal forecasting models also perform with 72% and 79% accuracy on the prediabetes dataset and sleep dataset respectively on goal-based forecasting. Conclusion: Our experiments conclude that multimodal LSTM models with early fusion are better than multimodal LSTM with late fusion and unimodal LSTM models and also than ARIMA and linear regression models. Significance: We address an important and challenging task of time-series forecasting in uncontrolled environments. Effective forecasting of a person's physical activity can aid in designing adaptive behavioral interventions to keep the user engaged and adherent to a prescribed routine.", "sections": [{"title": "I. INTRODUCTION", "content": "Physical activity is a crucial factor that can reduce the risk of many chronic conditions such as cardiovascular diseases, diabetes, and cancers [1], [2]. However, it remains challenging for many individuals to maintain a consistent routine of an active lifestyle [3]. As a result, one-third of people aged 15 years and older lack sufficient physical activities [4]. Among many different ways of promoting physical activity, recently mHealth interventions have become a popular choice [5]-[7]. Previous research suggests that engagement with lifestyle intervention applications is a strong contributor to the success of these interventions. Usage of a particular intervention application can be associated with healthier behaviors such as healthy eating and increased physical activity [8]-[10]. It is believed and also supported by evidence that activity prompts and engagement reminders can improve adherence to physical activity [11]. However, designing and automating effective interventions that are also tailored to individual needs requires knowing in advance when a user is not likely to follow the recommended level of physical activity on a particular day.\nThis paper investigates how accurately it is possible to forecast a person's physical activity, thus enabling adaptive and personalized behavioral interventions. In particular, we study both unimodal time-series models based on physical activity and multimodal models based on physical activity and engagement in mHealth interventions in our activity forecasting design. The engagement metrics are computed based on the amount of involvement in intervention apps. We hypothesize that multimodal models carry richer behavioral data and therefore offer an improved forecasting performance compared to unimodal models. To the best of our knowledge, our work is the first that studies time-series multimodal activity forecasting in the context of mHealth lifestyle interventions [12]. To address this gap in research, we formulate a time-series forecasting problem and provide a formal definition of the problem. We propose a behavioral intervention system, MoveSense\u00b9, that utilizes LSTM (long short-term memory network) based neural networks to forecast the daily number of steps of a person one day in advance.\nOur extensive analyses using data collected in two clinical studies show that an LSTM model can forecast the number of steps a user will walk the next day with a mean absolute error of as low as 1677, as shown in Section IV-F. The details of the dataset and the experimental procedures have been discussed in Section III. At a glance, our contributions in this article include (1) formulation of a time-series fore- casting problem for activity forecasting in behavioral health; (2) implementing the MoveSense system with LSTM-based forecasting solutions and testing them on two datasets; (3) a comparison among forecasting with different modalities and"}, {"title": "II. RELATED WORK", "content": "Time-series forecasting is an age-old problem that has been solved in many different contexts in recent times and the past few decades. Traditional models, such as nearest neighbor algorithms, autoregressive models, and the autoregressive in- tegrated moving average (ARIMA) method were some of the popular choices before the popularity of deep neural networks [13]\u2013[15]. A regression tree-based method [16] proved to be able to predict the time when a particular event will take place in a smart home setting. However, in recent times, more and more forecasting research has been driven by deep learning methods, especially recurrent neural networks (RNN) [12], [17]\u2013[19] and transformers [20], [21]. Among the RNN- based models, gated recurrent unit (GRU) and long short- term memory (LSTM) networks are most commonly used. A comparative study [22] tested LSTM and ARIMA models on several stock market datasets and showed that LSTM beats ARIMA by a large margin. LSTM and transformers have been used in many different time-series forecasting and prediction problems including energy consumption [23], traffic [24], sales and demand [25], weather and state of electrical machinery [26], hard drive failure [27], etc.\nAlthough mostly popular for image and video processing tasks, convolutional neural networks (CNN) can be used to solve problems with time-series data as well. For example, LSTNet [24] is a deep neural network that combines CNN, LSTM, and autoregressive layers to forecast traffic, electricity consumption, solar energy, and exchange rate. CNN can also be used for the imputation of missing sensor data [28]. MTEX- CNN [29] is a CNN-based forecasting model that also provides explanations. CNNs have been used for many other time- series analysis and forecasting problems such as website visits [30] and analyzing financial data [31]. Despite the reasonably good performance of these methods, they are not free from limitations. Different methods have been used for forecasting hospital admission [32], mortality [33], childbirth risks [34], etc. However, forecasting for clinical applications has not advanced in the same way as other domains of forecasting with machine learning. Furthermore, to the best of our knowledge, activity forecasting using wearable sensor data and smartphone app usage has not been investigated enough to explore its promises [12]."}, {"title": "B. Physical Activity Monitoring", "content": "The intensity and duration of physical activity can be posi- tively affected through self-monitoring and logging. [9] found that participants who self-reported using their Fitbit tracker and app frequently also had greater increases in their physical activity. In a user study, it was observed that participants who used the provided application to record daily steps every day were more likely to log at least 10,000 steps compared to those who used the app less [10]. However, these studies are limited by the use of self-reported measures. Wang et al. included a self-reported measure of app usage where participants would report how often they used the app [9]. [10] included a self-reported measure of step counts suggesting that their findings are more focused on how app usage was associated with logging in steps rather than actually obtaining a certain step count. In contrast to these studies, [35] used system usage data to objectively measure app usage, i.e., the number of times the app features were used. They found that interactive apps (e.g., gamification) are more likely to increase user engagement and app usage and that increased usage is associated with greater increases in physical activity over time.\nBeWell24 is a multicomponent smartphone app intervention that targets behavior change in the 24h-spectrum (i.e., sleep, sedentary time, physical activity) [36]. It was developed us- ing a user-centered iterative design framework that included quantitative process-level outcomes related to app usage and post-intervention qualitative feedback from users with respect to app design and satisfaction. The study found that overall, satisfaction with the app was modest. Given the promising metrics of app usage of the BeWell24, the next logical step is to examine the association of app use with intervention outcomes such as physical activity. SleepWell24 is a similar smartphone app that was developed to encourage its users to monitor their continuous positive airway pressure (CPAP) usage and sleep quality in addition to their physical activity levels [37]."}, {"title": "C. Engagement with Technology", "content": "The definition of engagement with apps can have one of the many metrics including the frequency of use, duration of use, number of log-ins, and pages viewed [38], [39]. Unfortunately, although the data are collected, many studies report limited or no app usage metrics. Consequently, researchers have made a call for future studies to report app usage metrics [7]. There- fore, this study aims to expand upon the limited literature and examine app usage metrics of a multicomponent smartphone app intervention targeting lifestyle behaviors across 24 hours and its association with physical activity. We hypothesize that a person's physical activity is correlated with when, how, and how often they use a fitness tracker app."}, {"title": "D. Multimodal Learning", "content": "Multimodal learning models have the potential to make predictions with more confidence than unimodal models. When there is more than one modality of input, a multimodal learning architecture is necessary to process them effectively. However, multimodal learning comes with five different chal- lenges that need to be addressed: representation, translation, alignment, fusion, and co-learning [40]. We refer to [40] for details of those challenges. Previously, it was found that a multimodal speech classifier that uses both audio and video can classify a speech with higher accuracy than a similar unimodal classifier that uses only audio or only video input [41]. In multimodal learning, features from different modalities"}, {"title": "III. METHODS", "content": "MoveSense is a closed-loop and adaptive system that guides its users to a better lifestyle with the help of its core com- ponents, an intervention app, an activity tracker, machine learning models for forecasting activities, and an adaptive intervention agent, as shown in Fig. 1. These components are going to work together to facilitate a user to better manage their lifestyle."}, {"title": "1) Intervention Applications:", "content": "A lifestyle intervention app is commonly implemented to assist individuals with achieving their physical activity goals. For example, BeWell24 is a multicomponent smartphone app that was designed to target lifestyle behaviors across 24 hours (i.e., physical activity, sedentary behavior, sleep) to improve glucose metabolism in patients prediabetic for type 2 diabetes. Briefly, the smartphone app included behavioral strategies, based upon evidence-based behavior change techniques (e.g., goal setting, stimulus con- trol, self-regulatory strategies, etc.), for sleep, sedentary time, physical activity, and dietary intake. A user of the app would be able to self-monitor their sleep and physical activity based on objective assessments. They would also receive personal feedback on their behaviors based on self-monitoring. A few screenshots of the application are presented in Fig. 2."}, {"title": "2) Fitbit Wearable Wristband:", "content": "To allow the smartphone apps to monitor the daily physical and sleeping activities of a user, the decision was taken to use Fitbit Charge 2 for continuous objective monitoring of sleep and physical activity. The user would download the Fitbit app on their smartphone and they would be able to view their Fitbit data on the smartphone app. The Fitbit app would be used to sync Fitbit with the user's smartphone and then auto-populate its data into the corresponding smartphone."}, {"title": "3) Activity Forecasting:", "content": "At this point, we know what type of data will be available to us and now can design our forecasting model. We consider a supervised learning setting where a machine learning model uses multimodal forecasting to predict the value of a variable at a future time step using different variables from different modalities of the past. For a particular time step t, we have different modalities of input, $u_t \\in \\mathbb{R}^p$, $v_t \\in \\mathbb{R}^q$. Here, $p, q \\in \\mathbb{N}$ are the dimensions of $u_t$ and $v_t$ vectors, respectively. We can write, the input at time step t, $x_t = (u_t, v_t)$. The goal is to forecast the output variable for time step t + 1, $Y_{t+1}$ using w number of past input examples, $(x_{t-w+1}, x_{t-w+2},..., x_t)$. Here, w is the window size. If f is a function that operates the calculation on the input features and predicts the output, we can write,\n$\\hat{y}_{t+1} = f(x_{t-w+1}, x_{t-w+2},...,x_t)$\nIn our experiments, $u_t$ represents app engagement features and $v_t$ represents physical activity features. To estimate the function, f, we consider the early fusion and late fusion multimodal learning methods. Early fusion combines the $u_t$ and $v_t$ at the feature level before passing through any layers of a neural network, whereas, the late fusion method processes $u_t$ and $v_t$ separately through different channels of the neural network, and a little before the final prediction, the two channels are combined.\nIf we unroll Equation 1 for different modalities, the for- mulation will depend on which fusion method is to be used. For shorthand, let $u = (u_{t-w+1}, u_{t-w+2},...,u_t)$, and $v = (v_{t-w+1}, v_{t-w+2},..., v_t)$. Suppose, $g(\\alpha, \\beta)$ is a function that takes two variables $\u03b1$ and $\u03b2$ and fuses them together to create a single variable. For early fusion, we can write,\n$\\hat{y}_{t+1} = f_{early}(g(u, v))$\nThis means we will first concatenate u and v before feeding to our forecaster $f_{early}$. And for late fusion, we can write,\n$\\hat{y}_{t+1} = f_{late}(g(f_1(u), f_2(v)))$"}, {"title": "4) Machine Learning Models:", "content": "There can be a few different ways to implement our forecasting functions. As the literature on time series forecasting using the type of data that concerns us is limited, a reasonable choice was to use machine learning- based forecasting models. Based on past physical activity and engagement trends, a machine learning model forecasts the physical activity level of the next day. In this case, the forecast variable is the number of steps a user is going to walk the next day. Getting to know this piece of information in advance will give the adaptive intervention agent leverage to prescribe a recommended course of action to keep the user on track to meeting their short-term and long-term goals.\nIn the early fusion method, we combine the app engagement features and the physical activity features at the feature level before passing through any neural network layers. If we create similar windows for app engagement and physical activity features, we can easily concatenate them into a single input channel for our machine-learning model. On the other hand, in the late fusion method, we pass each different modality through two independent channels of neural network layers and then pass through temporary decision layers before con- catenating the outputs of the two decisions and passing them through a unified decision layer to get the final prediction. As app engagement and physical activity are data from different modalities, it makes sense to operate them separately. The two different multimodal forecasting architectures, one with early fusion and the other with late fusion have been presented in Fig. 4."}, {"title": "5) Adaptive Intervention Agent:", "content": "The final component of our proposed system is the adaptive intervention design which will be able to provide feedback based on the prediction by the model. Suppose, when a user is not likely to meet the daily goal of their intended level of physical activity according to the forecasting model, the intervention agent will provide them with a reminder so that the user gets the motivation to fulfill their daily goal. The implementation of the intervention agent and the method of choosing the threshold of a forecast being reasonably confident is left for the near future to explore and research."}, {"title": "User Studies", "content": "To evaluate our MoveSense system, two user studies were conducted for the experiments that led to this paper. One is BeWell24 [36] and the other is SleepWell24 [37]. The BeWell24 study targeted overweight and obese US veterans as they are often at high risk of cardiovascular diseases because"}, {"title": "C. Data Preprocessing", "content": "For both prediabetes and sleep datasets, we did not have suf- ficient for several participants. Initially, our prediabetes dataset for the intervention group has engagement and Fitbit data for 58 users and this number is 51 for the sleep dataset. The user studies required that a person would need to wear the Fitbit device for at least 10 hours a day, otherwise, the data for that day would not be considered. It is standard practice to choose 10 hours/day as a threshold for wear time when considering wearable sensor data for further analyses, which is backed by previous studies [51]\u2013[54]. Among the 58 prediabetes study users, the average wear time is 12.38 \u00b1 7.27 hours per day and the average number of valid days satisfying the 10-hour wear time threshold among users is 155.07\u00b185.58 days. On the other hand, the average wear time of the 51 sleep study users is 12.38\u00b17.28 hours per day and the average number of valid days for them is 37.84\u00b120.93 days. For every user, we eliminated any day where the wear time was less than 10 hours. Furthermore, we set another threshold that a user must have at least 10 days of valid data, otherwise, we eliminate that user. After these two filtering steps, the prediabetes dataset is left with 55 participants, and the sleep dataset is left with 44 participants. A summary of the demographic distribution of the participants of both datasets has been presented in Table I."}, {"title": "D. Experiment Design", "content": "The analysis was completed in multiple steps. At first, the best window size was chosen by training and testing the LSTM models for three feature sets with four different options for the window size: 3, 7, 14, and 21. After training LSTM models independently with four different values for window size as discussed in Section IV-A, we chose the best-found window size, w = 7 for our forecasting. It means that the forecasting was performed by taking the last 7 days' data as input. The three options for modalities are multimodal early-fusion, only engagement features, and only activity features. Once the best window is found, we train and test ARIMA and linear regres- sion models as baselines and we also develop multimodal late-fusion based LSTM models. From these options, we choose the best model according to its mean absolute error on the test set. Then we train LSTM models for goal-based forecasting that predicts 1-day in advance whether a person's daily number of steps will be above a certain threshold."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "In this section, we present the results and discussion of the experiments to evaluate our MoveSense system. We present the results in the following steps: choosing the best window size, comparing with baselines, the performance of goal-based fore- casting, a comparison of unimodal and multimodal systems, different fusion methods of multimodal models, performance for most engaged participants, performance for individual participants, and forecasting additional outcomes."}, {"title": "A. Finding the Best Window Size", "content": "In general, selecting the right window size is an essential step in training a model with time-series data. We train our regression-based forecasting models with different values of window size: 3, 7, 14, and 21. After each training session, the test MAE is calculated. We notice that for both the prediabetes and sleep datasets, window size = 7 had the overall lowest MAE. We present the MAE values for the LSTM model (unimodal and multimodal early fusion) for different window sizes on Table III."}, {"title": "B. Comparison of MoveSense with the Baselines: ARIMA and Linear Regression", "content": "In Table IV and Table V, we present a comparative analysis of the LSTM model with linear regression and ARIMA. Our LSTM models outperform both linear regression and ARIMA in terms of mean-absolute-errors (MAE). The best performance by linear regression was an MAE of 2978 on the prediabetes dataset when using the unimodal activity features. On the other hand, the LSTM performed the best on the prediabetes dataset when both activity and engagement features were used. Early-fusion-based LSTM achieved an MAE of 1989 on the prediabetes dataset, which is 33% better than the performance of linear regression. On the sleep dataset, a consistent trend was observed with a margin of 13%."}, {"title": "C. Forecasting for Classification Problems", "content": "The average daily step counts for the prediabetes dataset and sleep dataset participants were 5745 and 7627 respec- tively. Because of the significant difference between these two values, the classification problems were formulated with different thresholds for the two datasets. Two thresholds, 6000 and 8000, were used for BeWell24 participants and a more standard threshold of 10000 steps was chosen for SleepWell24. The accuracy and F1 score for multimodal (early and late) and unimodal LSTM models are reported in Table VI. The early-fusion multimodal forecaster gives us an accuracy and f-score of 0.72 and 0.71 respectively on the prediabetes dataset. Also, these metrics are 0.79 and 0.71 respectively for the sleep dataset."}, {"title": "D. Effect of Different Modalities", "content": "In this subsection, we discuss the effects of different modal- ities on our forecasting problem. We compare the mean- absolute-error (MAE) of prediabetes study participants and sleep dataset participants in Table IV and Table V. In Table IV, we notice that for the prediabetes study participants, the MAEs for multimodal late LSTM, multimodal early LSTM, unimodal (engagement) LSTM, and unimodal (activity) LSTM models are 2813, 1989, 3470, and 2051 respectively. The MAEs for the similar models are 4754, 4194, 5354, and 5354 respectively. For the prediabetes study users, the early fusion- based multimodal model achieved the best error value, while the lowest MAE was achieved by the activity-based unimodal model on the sleep dataset. In Table V, we notice that the multimodal early LSTM outperforms the best unimodal LSTM by a margin of 3%, whereas, on the sleep dataset, it achieves comparable performance, with an MAE 2% higher than the best unimodal model.\nWe observe the superiority of the multimodal models over unimodal models for linear regression too. In Table IV, it can be noticed that the multimodal linear regression model outperforms the unimodal counterparts on both prediabetes and sleep datasets."}, {"title": "E. Early Fusion vs Late Fusion", "content": "One part of our work is to find out which forecasting method, multimodal or unimodal, is superior to the other. But we also need to address which type of fusion method, early or late, we should employ for the multimodal models. We have trained models with early fusion and late fusion. We have observed consistent results on both datasets. In Table IV and Table V, we notice that multimodal early LSTM achieved lower MAE than multimodal late LSTM on both datasets, with margins of 29% on the prediabetes dataset and 12% on the sleep dataset. We observe similar performance on the classification task as well. In Table VI, we notice that multimodal early fusion achieves higher accuracy and F1 score on both datasets for all the thresholds."}, {"title": "F. Performance Variation for Most Engaged Users.", "content": "An important question to ask in activity forecasting is does a forecasting model's performance varies significantly for different user groups. To answer the question, the notion of most engaged users has been introduced based on their engagement with the smartphone application. The question still remains on how to choose that subset, i.e., what the threshold should be to decide if a user belongs to that specific group. To answer the question, a series of experiments have been conducted with different subgroups of users: top 75%, top 50%, and top 25% participants based on engagement. From that group, it is seen that on average, the model performs better for the set of top 25% users, who are above the 75th percentile threshold, in terms of mean absolute error as shown in Table VII. We observe that when we set a threshold of a higher percentile for a participant to be considered, the model's performance seems to improve. It supports our hypothesis that including engagement features can improve the performance of the forecasting problem."}, {"title": "G. Forecasting Performance for Individual Participants", "content": "From our experiments, we notice that the model's perfor- mance varied across different individuals in terms of mean- absolute-errors in the regression setting as shown in Fig. 5. For the prediabetes study participants, multimodal early-fusion LSTM gets lower MAE for 10 out of the 11 participants. However, among the sleep study participants, 6 out of 9 participants had lower MAE with early fusion than with late fusion.\nAmong the prediabetes participants, only 2 out of 11 test set participants had an MAE higher than 3000 with multimodal early fusion. Whereas, for the sleep study participants, 5 out of the 9 participants had an MAE higher than 3000 with multimodal early fusion. We believe that personalization by fine-tuning the models with a specific participant's data can lower the error for that individual [55]."}, {"title": "H. Forecasting Additional Outcomes", "content": "In this section, we further evaluate our method by fore- casting additional outcome variables: daily sedentary duration, light physical activity (LPA) duration, and wear time, i.e., the duration the person is wearing the wearable sensor. We present the results in Table VIII. The normalized root mean squared error (NRMSE) was obtained by dividing the root mean squared error (RMSE) by the average value of the corresponding metric. We notice that sedentary duration can be predicted on the day before with an NRMSE of 0.27 and 0.26 on the prediabetes and the sleep datasets respectively. Also, the system is able to forecast LPA duration with MAES 49 to 61."}, {"title": "V. CONCLUSION", "content": "Nonadherence to recommended healthy lifestyle regimens is a persisting healthcare problem and in this busy world, it is very common to be forgetful about completing one's daily need for physical activities. In this work, we have provided a solution model to a multimodal time-series forecasting prob- lem setting and presented experimental results of our proposed model on two important datasets of real subjects. Our datasets were collected in a real-world uncontrolled environment over months, and our models are able to forecast a person's next day's steps with a mean absolute error of 1677 and 2152 for the prediabetes and sleep datasets respectively. We have also provided classification results for different thresholds of daily goals and found that the multimodal forecaster can forecast whether a person will reach their daily goal with an accuracy of 0.72 and 0.79 on the prediabetes and sleep datasets respectively. Finally, our experiments suggest that a window size of 7 is an optimal choice while preparing time-series data for forecasting a person's physical activity. Also, multimodal forecasting models with early fusion are overall a better choice than multimodal forecasting models with late fusion or uni- modal forecasting models for forecasting physical activities. We showed how the performance varied for participants above a certain percentile based on app engagement. Finally, we also developed a forecasting model for additional outcomes such as sedentary duration, wear time, and light physical activity duration.\nWe believe that time-series activity forecasting is an im- portant area of research that can help the development of numerous technologies including adaptive reminders and diet or hydration recommendation systems. Also, other variations of neural network architectures can be implemented and tested to explore if they can improve the performance of forecasting."}]}