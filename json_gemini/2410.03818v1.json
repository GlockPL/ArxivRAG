{"title": "LARGE LANGUAGE MODELS CAN BE STRONG\nSELF-DETOXIFIERS", "authors": ["Ching-Yun Ko", "Pin-Yu Chen", "Payel Das", "Youssef Mroueh", "Soham Dan", "Georgios Kollias", "Subhajit Chaudhury", "Tejaswini Pedapati", "Luca Daniel"], "abstract": "Reducing the likelihood of generating harmful and toxic output is an essential\ntask when aligning large language models (LLMs). Existing methods mainly\nrely on training an external reward model (i.e., another language model) or\nfine-tuning the LLM using self-generated data to influence the outcome. In this\npaper, we show that LLMs have the capability of self-detoxification without the\nuse of an additional reward model or re-training. We propose Self-disciplined\nAutoregressive Sampling (SASA), a lightweight controlled decoding algorithm\nfor toxicity reduction of LLMs. SASA leverages the contextual representations\nfrom an LLM to learn linear subspaces characterizing toxic v.s. non-toxic output\nin analytical forms. When auto-completing a response token-by-token, SASA\ndynamically tracks the margin of the current output to steer the generation\naway from the toxic subspace, by adjusting the autoregressive sampling strategy.\nEvaluated on LLMs of different scale and nature, namely Llama-3.1-Instruct\n(8B), Llama-2 (7B), and GPT2-L models with the RealToxicityPrompts, BOLD,\nand AttaQ benchmarks, SASA markedly enhances the quality of the generated\nsentences relative to the original models and attains comparable performance\nto state-of-the-art detoxification techniques, significantly reducing the toxicity\nlevel by only using the LLM's internal representations.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in large language models (LLMs) have dramatically enhanced their capa-\nbilities in textual understanding and reasoning (Brown et al., 2020; Kojima et al., 2022). Their\ncapabilities in performing diverse linguistic tasks and producing coherent texts have catalyzed\ntheir adoption across a variety of applications (Rae et al., 2021; Hoffmann et al., 2022; Le Scao\net al., 2023; Touvron et al., 2023a;b; Achiam et al., 2023). However, with the escalating size of mod-\nels (Raffel et al., 2020; Brown et al., 2020; Achiam et al., 2023), there is a corresponding increase in\nthe scale of the training datasets required to avert overfitting and to encapsulate extensive world\nknowledge. These extensive datasets, predominantly derived from internet crawls and merely\nsubjected to basic filtering protocols (Raffel et al., 2020), often harbor biases that are problematic\nor directly detrimental for many applications and may not inherently align with these desirable\nattributes (Wallace et al., 2019; Gehman et al., 2020). In fact, it is known that language models\ntrained on such data may not only mimic but also amplify these biases (Bolukbasi et al., 2016;\nCaliskan et al., 2017; Zhao et al., 2018; Sheng et al., 2019; Gehman et al., 2020; Hartvigsen et al.,"}, {"title": "2 RELATED WORK", "content": "Toxic Contents in LMs. The investigation and mitigation of toxic content generated by large\npre-trained language models (LMs) have become increasingly critical, as evidenced by recent stud-\nies (Gehman et al., 2020; Xu et al., 2020). Addressing toxicity in LMs presents multiple challenges.\nFirstly, toxic content varies widely, encompassing profanity, identity attacks, threats, among others,\neach potentially requiring a context-specific approach. Secondly, the definition of toxicity lacks\nconsensus across different socio-cultural backgrounds, leading to variable perceptions of what\nconstitutes offensive language (Zampieri et al., 2019; Welbl et al., 2021).\nFrom another point of view, larger corpora used in LM training often propagate toxic content. For\nexample, LMs have been shown to produce racially biased outputs from synthetic or seemingly\ninnocuous prompts (Wallace et al., 2019) and (Xu et al., 2021) has highlighted how LMs may\nexacerbate social biases. The transmission of such biases and toxicities through downstream appli-\ncations can lead to significant harm, particularly towards underrepresented groups, manifesting\nas biases of allocation or representation.\nControlled Generation. Current controlled strategies generally fall into two categories:\nretraining-based and decoding-based. Retraining-based approaches involve either retraining the\nLM with a sanitized dataset, where harmful content has been removed (Gururangan et al., 2020),\nor using adversarial human inputs to identify and neutralize potential sources of unsafe content\nfor further model training (Dinan et al., 2019; Xu et al., 2020; Lu et al., 2022). These methods\nare computationally intensive and not feasible for very large LMs typically offered as services.\nDecoding-based strategies, operating during inference without altering the model's parameters,\nhave largely-varied complexity. The most computationally expensive option requires gradient\ninformation (PPLM (Dathathri et al., 2020)) and manipulates the generation process using the\ngradient of a simple discriminator linked to a differentiable toxicity classifier, steering LMs away\nfrom generating toxic text. Due to the high computational burden and incurred latency, other\nmore light-weight methods have been considered including solely banning lists of words (e.g.,\nword-filtering) (Gehman et al., 2020) or requesting resampling upon quality checks (e.g., test-time\nfiltering) (Welbl et al., 2021). In between, there are methods that utilize only the output logits from\nthe LM for detoxification(e.g., GeDi, DExperts, CriticControl, Rectification, Self-Debiasing, RAD,\n(Krause et al., 2021; Liu et al., 2021; Kim et al., 2023; Cao et al., 2022; Schick et al., 2021; Deng &\nRaffel, 2023)) or for other applications such as topic control (Yang & Klein, 2021; Liu et al., 2024).\nSpecifically, DExperts (Liu et al., 2021) employs a product of experts approach at decoding time,\nleveraging a toxic LM as an \"anti-expert\" and a non-toxic LM as an \"expert\" to promote the genera-\ntion of non-toxic tokens. DExperts functions by interacting solely with the output from the base\nLM, thus allowing for effective steering using small (anti-)expert models. Similarly, GeDi (Krause\net al., 2021) trains class-conditional LMs as generative discriminators to guide language genera-\ntion towards desired attributes. Rectification (Cao et al., 2022) applies the dead-end theory from\nreinforcement learning (RL) to the detoxification task. It constructs an RL problem where a reward\nmodel is trained to capture toxicity in language and a value function is trained to estimate the"}, {"title": "3 SASA: SELF-DISCIPLINED AUTOGRESSIVE SAMPLING", "content": "One core discovery of our paper is that the embedding space of an LLM, such as a Llama-2\nmodel, is capable of capturing the context of toxicity. Built upon this finding, we propose to\nlearn a subspace (toxic v.s. non-toxic) on top of the LLM's internal representations to steer the\nautoregressive decoding process of LLMs. To illustrate this point, we will first explain our setups\nfor the subspace learning, which essentially requires only the inference of any given public value\nannotation dataset in the format of {prompt, response, annotation}, such as HH-RLHF (Bai et al.,\n2022), Toxic Comment Classification Challenge dataset (van Aken et al., 2018), Jigsaw Unintended\nBias in Toxicity Classification dataset (cjadams et al., 2019), or any attribute sentence datasets.\nAn annotation can be a label of {toxic, non-toxic}, {preferred, not preferred}, etc. For example, in\nFigure 1, we give an illustration of having {toxic, non-toxic} labels and hereby obtaining toxic/non-\ntoxic subspaces. Then, we will explain how to steer the text generation process based on the\nlearned subspace."}, {"title": "3.1 SUBSPACE LEARNING", "content": "Suppose we are given a value annotation dataset v (i.e., a paired prompt-response dataset that\nassociates with a certain attribute annotation, such as toxicity, truthfulness, etc.). Prior arts have\ntried to learn external LMs serving as explicit reward models that predict the attribute values (Cao\net al., 2022; Deng & Raffel, 2023). However, we hypothesize that LLMs are performant contextual\nencoders and their innate representations can be used for self-detoxification. Specifically, in this\nsection, we propose to learn the subspace directly inside the context embedding space to build a\nclassifier to inform the attribute on the context embedding level (see Figure 1, subspace learning).\nIdeally, the subspace learner should be lightweight and fast to update, because it will be used\ntogether in the autoregressive decoding process to steer the LLM generation.\nFormally, for a value annotation dataset v consists of prompt-response pairs $\\{(c, x_k)\\}_{k=1}^{N_1+N_2}$, which\ncan be separated into benign pairs $\\{(c, x_1)\\}_{k=1}^{N_1}$, and toxic pairs $\\{(c, x_2)\\}_{k=1}^{N_2}$ based on the annotation,\nwe aim at finding a lightweight classifier $f_v(c, x)$ on the embeddings encoded by the decoding\nLM $g$. To approach this and to ease the computation, we will model the context embedding\nof the concatenated prompt-response pair, denoted by $c \\oplus x$, by a class-conditional Gaussian\ndistribution $\\mathcal{N}$. That is, $g(c \\oplus x_1) \\sim \\mathcal{N}(\\mu_1, \\Sigma)$ and $g(c + x_2) \\sim \\mathcal{N}(\\mu_2, \\Sigma)$, where $g(\\cdot)$ denotes the\ncontext encoding operator, $c\\oplus x$ denotes the concatenation of the prompt $c$ and the response $x$,"}, {"title": "3.2 STEERING TEXT GENERATION BASED ON LEARNED SUBSPACES", "content": "Recall that given a prompt c, an LM generates the response token-by-token based on autore-\ngressive sampling. Specially, at the i-th token generation step, given the current generated to-\nkens denoted as $x_{1:i-1}$, the context embedding operator g, and the token embedding matrix\n$W_{token} \\in \\mathbb{R}^{d \\times V}$, where d is the embedding space dimension and V is the vocabulary size, the out-\nput token logits at the i-th decoding step is given by $logit(\\cdot|c\\oplus x_{1:i-1}) = W_{token}g(c + x_{1:i-1})$. Using\na learned subspace $f_v$, from v, we propose to introduce a bias term $m^v \\in \\mathbb{R}^{V\\times 1}$ to the token logits\nand adjust the autoregressive sampling strategy such that the generation can be steered away\nfrom the toxic subspace. In practice, we let $m^v$ be the margin from the current context embedding\nto the classifier, defined as $m^v (x_i|c\\oplus x_{1:i-1}) = w_v^T (g (c + x_{1:i}) - b_v) / ||w_v||$, assuming v consists of\nbinary pairs. A larger and positive margin means the current generated context is further distant\nfrom the toxic subspace, whereas a negative margin is an indication of toxic generation.\nIn our proposal, we have two goals when designing the subspace-aware sampling distribution\n$\\rho \\in \\Delta^V$ (the probability simplex on V) over candidate tokens: (1) alignment: we want $m^v$ to\nbe maximized with respect to $\\rho$ and (2) utility: we want $\\rho$ to be close to the original sampling\ndistribution. Formally, let $\\pi_m \\in \\Delta^V$ denote the scaled margin distribution over V, defined as $\\pi_m =$\nSoftmax($m^v$), and let $\\pi_{ref}$ denote the original (reference) sampling distribution Softmax(logit).\nEssentially, when generating the i-th token, we want to maximize $\\sum_{i=1}^{V} p_i \\pi_m(x_i|c\\oplus x_{1:i-1})$ and\nminimize KL($\\rho||\\pi_{ref}(\\cdot|c\\oplus x_{1:i-1})$), where KL denotes the KL divergence between two distributions."}, {"title": "4 EXPERIMENTS", "content": "4.1 SETUPS\nLanguage Models. We conduct detoxification experiments with LMs of three different sizes:\nGPT2-Large, Llama-2-7b, and Llama-3.1-8b-Instruct, all of which are transformer-based auto-\nregressive LMs that contain 762M, 7B, and 8B parameters, respectively. Specially, we use the"}, {"title": "4.2 NON-TOXIC PROMPTS", "content": "Since previous detoxification work has primarily been tested on the non-toxic prompts in\nRTP (Krause et al., 2021; Liu et al., 2021; Deng & Raffel, 2023) using the GPT2-Large model. Specifi-\ncally, RAD (Deng & Raffel, 2023) has reported the detoxification results of PPLM, GeDi, DExperts,\nRectification, DAPT, PPO, and Quark. In Table 1, we further report RAD and SASA using nucleus\nsampling (p = 0.9). We note that the results reported in the previous work might be based on\ndifferent versions of Perspective API (the Perspective API changes over time (Pozzobon et al.,\n2023)). From the table, we can see that SASA can reach similar, or even lower average maximum"}, {"title": "4.3 CHALLENGING PROMPTS", "content": "In the second experiment, we move on to the \"challenging\" split in the RTP dataset, where the\nprompts could consistently cause out-of-the-box LM (e.g., GPT1, GPT2, GPT3, CTRL, CTRL-WIKI))\nto generate toxicity (Gehman et al., 2020). In Table 3, we list the detoxification results by RAD and\nSASA using Llama-2-7b. From the table, we note that the starting Avg. Max Toxicity is remarkably\n0.87, and the toxic rate is 0.974 on the challenging RTP. As the trade-off parameter $\\beta$ increases,\nthe toxicity quickly goes down but is still notably higher than that on the non-toxic RTP. For RAD,\nits Avg. Max Toxicity reduces to 0.481 with a perplexity of 7.331 when $\\beta$ = 500. Surprisingly, with\nthe same $\\beta$, SASA achieves an Avg. Max Toxicity of 0.426 with an even lower perplexity of 7.195,\nproving the strong potential for LLMs to be self-detoxifiers without any external reward model.\nDue to the page limit, we defer the GPT2-Large detoxification results on challenging RTP to the"}, {"title": "4.4 DETOXIFICATION BEYOND RTP", "content": "In the experiment, we have further detoxified on both BOLD and AttaQ benchmarks. From Table 4\nand 5, we see that, on both datsets, SASA is able to reach lower avg. max toxicity (0.023 vs 0.050 on\nBOLD, 0.142 vs 0.264 on AttaQ) as well as toxic rate compared to RAD.\nAdditionally, we further conducted an experiment where we use the BOLD dataset to analyze\nLM gender bias, results are shown in appendix Table 14. Specifically, there are 2363 samples in\nBOLD associated with gender domain, consisting of 776 'American_actresses' (female) and 1587\n'American_actors' (male). We choose the first 776 male samples to balance with female samples\nand compare their generation toxicity with those of female sample generations. From appendix\nTable 14, it can be seen that Llama decoded sentences for female group have generally higher toxic\nrate (0.066 vs 0.031), implying the LM being somewhat biased against female. With controlled\ndecoding, both RAD and SASA mitigate this gender bias well and reach balanced toxic rate, with\nSASA being 50% less toxic than RAD (Avg. Max Toxicity 0.025 vs 0.049)."}, {"title": "4.5 DETOXIFYING AN ALIGNED MODEL", "content": "Next, we apply SASA to Llama-3.1-8b-Instruct, an\ninstruction-tuned model, and show SASA is able to\nfurther reduce the toxicity in its generations. Specif-\nically, from Table 6, Llama-3.1-8b-Instruct starts at\nan Avg. Max Toxicity of 0.727 and Toxic Rate of 0.892,\nslightly beating unaligned model Llama-2-7b (Avg.\nMax Toxicity of 0.87 and Toxic Rate of 0.974 in Table 3).\nWith SASA, we see a sharper drop in the toxicity levels\nof the generated sentences by Llama-3.1-8b-Instruct.\nSpecifically, SASA yielded sentences with an Avg. Max Toxicity of 0.234 (i.e. a 68% drop) and Toxic\nRate of 0.171 (i.e. a 81% drop) on Llama-3.1-8b-Instruct, in comparison to an Avg. Max Toxicity\nof 0.426 (i.e. a 51% drop) and Toxic Rate of 0.447 (i.e. a 54% drop) when we applied SASA on\nLlama-2-7b. That said, with an aligned base model, the internal sentence embedding space can\nbe more informative of risk attributes such as toxicity. Therefore, when we leverage the internal\nembedding space of an aligned model, SASA can generate sentences with much lower toxicity. On\nthe whole, SASA has strong adaptability to instruction-tuned models, reinforcing its generality\nand robustness across different LMS."}, {"title": "4.6 ADDITIONAL STUDIES", "content": "Qualitative analysis (A contain examples that may be considered offensive). Besides quan-\ntitative analysis of the detoxification results, we also show some examples in Table 7. It can be\nseen that the original Llama-2-7b decoding results often contain curse words and negative words.\nThe conversation led by the original LM also appears to be more emotional, while SASA decoding\nresults are more rational and objective.\nThe runtime and memory usage. We further analyze the runtime and memory usage required\nfor the decoding. At each prediction one has to see the impact of each token in the vocab on toxicity\nwhich results with O(|V|) complexity. In practice, we speed this process up by modifiying only top-p\nvalues of original logits (this strategy was also used in RAD). Specifically, the original decoding\nusing GPT2-Large takes 2.1 hours and uses 3.1 GB of memory. In contrast, RAD significantly\nincreases the inference time to 5.5 hours and the memory usage to 4.3 GB. SASA, however, strikes\na balance of 2.9 hours inference time and 3.8 GB of memory. Similarly, on Llama-2-7b, the original\ndecoding takes 2.9 hours and consumes 15.5 GB of memory; RAD decoding takes 5.6 hours and\nuses 16.7 GB of memory; SASA decoding takes 3.1 hours and uses 17.3 GB of memory. Nevertheless,\nit is important to note that the above data are recorded without any complexity optimization\nsuch as the reuse of previously computed representations mentioned in RAD. More meticulous\nengineering needs to be performed to understand the limits of each decoding method.\nCombine SASA with word filtering. We also verify the compatibility of SASA with naive detoxifi-\ncation (input moderation) methods such as word filtering (Gehman et al., 2020). Specifically, we\nprevent the LM from generating any of 403 banned words\u00b9 by setting the sampling probability\nof banned words to 0. Due to the page limit, we defer the full table of this experimental results\nto the appendix Table 15. From the table, one can see that combining SASA with word filtering\ncan indeed substantially lower the toxicity across all $\\beta$. For instance, at $\\beta$ = 500, the Avg. Max\nToxicity decreases from 0.426 with SASA alone to 0.178 with SASA+word filtering. However, this"}, {"title": "5 CONCLUSION", "content": "This paper presents SASA, a lightweight and theoretically-grounded controlled decoding frame-\nwork for LLMs. Our findings demonstrate the capability of LLMs in leveraging their innate con-\ntextual representations to learn discriminative subspaces for efficient self-detoxification in text\ngeneration. We proved that our proposed subspace-guided token sampling strategy is theoretically\noptimal in balancing the trade-off between a given alignment objective and the similarity to the\noriginal sampling distribution. Evaluated on Llama-2-7b and GPT2-Large models, SASA attains\ncompetitive performance in toxicity reduction when compared with existing methods requiring\nthe use of an external reward model or LM re-training. Our results unlock the potential of LLMs in\nself-detoxification and offer novel insights into the self-alignment of LLMs."}, {"title": "A.6 COMBINING WORD FILTERING WITH SASA", "content": "Word filtering. This most naive solution of curating a list of banned words is proved inadequate\nfor several reasons. Firstly, they fail to prevent the generation of biased text reliably, as demon-\nstrated by examples where biased statements are composed using ostensibly neutral words (Schick\net al., 2021, Figure 1). Since many of these words are integral to the English lexicon, excluding them\ncould undermine the model's ability to generate meaningful content. Secondly, the exclusion of\nwords could hinder the model's ability to acquire knowledge on topics associated with these words,\nwhich may be critical for certain applications. In terms of the quantifiable metric, we could see an\nobvious increase in the perplexity when excluding a fixed list of banned words from Table 15."}, {"title": "A.7 LIMITATIONS", "content": "SASA detoxification relies on modeling a toxicity subspace within the sentence embedding space,\nwhich depends on the capabilities of the underlying LM. If the LM cannot capture and distinguish\nsubtle attributes related to desired attribute (toxicity), the performance of SASA may be compro-\nmised, especially with smaller or less sophisticated models. For example, we see from Figure 2\nthat SASA could have a smaller gap from RAD on Llama-2-7b, but larger on GPT2."}, {"title": "A.8 A RUNNING EXAMPLE OF CONTROLLED DECODING USING SASA", "content": "Figure 3: An example of the decoding process of a toxic prompt with top token candidates selected\nby nucleus sampling. With the prompt c, there are five candidates for the next token {and, even, as,\nso, which} with the initial sampling probabilities being {0.58, 0.04, 0.04, 0.03, 0.31}, which becomes\n{0, 0.99, 0, 0, 0.01} after subspace adjustment."}, {"title": "A.9 SAMPLE EFFICIENCY", "content": "Figure 4: The toxicity accuracy as a function of the sample size.\nWe show SASA's sample efficiency analysis in Figure 4. From the figure, one sees that the toxicity\naccuracy plateaus at around 500K samples. That said, although we have used all samples in getting\nthe Bayes optimal classifier (as is done in RAD to fine-tune the GPT2-small reward model), it was\nnot necessary for SASA."}]}