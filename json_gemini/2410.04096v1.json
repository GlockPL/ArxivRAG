{"title": "SINC KOLMOGOROV-ARNOLD NETWORK AND\nITS APPLICATIONS ON PHYSICS-INFORMED NEURAL\nNETWORKS", "authors": ["Tianchi Yu", "Jingwei Qiu", "Jiang Yang", "Ivan Oseledets"], "abstract": "In this paper, we propose to use Sinc interpolation in the context of Kolmogorov-\nArnold Networks, neural networks with learnable activation functions, which re-\ncently gained attention as alternatives to multilayer perceptron. Many different\nfunction representations have already been tried, but we show that Sinc interpo-\nlation proposes a viable alternative, since it is known in numerical analysis to\nrepresent well both smooth functions and functions with singularities. This is im-\nportant not only for function approximation but also for the solutions of partial\ndifferential equations with physics-informed neural networks. Through a series of\nexperiments, we show that SincKANs provide better results in almost all of the\nexamples we have considered.", "sections": [{"title": "INTRODUCTION", "content": "Multilayer perceptron (MLP) is a classical neural network consisting of fully connected layers with\na chosen nonlinear activation function, which is a superposition of simple functions. The classical\nKolmogorov-Arnold representation theorem Kolmogorov (1961); Arnol'd (1959) states that every\nfunction can be represented as a superposition of function of at most 2 variables, motivating the\nresearch for learnable activation functions.\nKolmogorov's Spline Network (KSN) Igelnik & Parikh (2003) is a two-layer framework using\nsplines as the learnable activation functions. Recently, Kolmogorov-Arnold Networks (KANs) Liu\net al. (2024b) sparkled a new wave of attention to those approaches, by proposing a multilayer\nvariant of KSN. Basically, any successful basis to represent univariate functions can provide a new\nvariant of KAN. Many well-known methods have already been investigated including wavelet Bo-\nzorgasl & Chen (2024); Seydi (2024b), Fourier series Xu et al. (2024), finite basis Howard et al.\n(2024), Jacobi basis functions Aghaei (2024a), polynomial basis functions Seydi (2024a), rational\nfunctions Aghaei (2024b) and Chebyshev polynomials SS (2024); Shukla et al. (2024).\nWe propose to use Sinc interpolation (the Sinc function is defined in Eq. (4)) which is a very ef-\nficient and well-studied method for function interpolation, especially 1D problems Stenger (2016).\nTo our knowledge, it has not been studied in the context of KANs. We argue that the the cubic spline\ninterpolation used in KANs should be replaced by the Sinc interpolation, because splines are par-\nticularly good for the approximation of analytic functions without singularities which MLP is also\ngood at, while Sinc methods excel for problems with singularities, for boundary-layer problems, and\nfor problems over infinite or semi-infinite range Stenger (2012). Herein, utilizing Sinc functions can\nimprove the accuracy and generalization of KANs, and make KANs distinguishing and competitive,\nespecially in solving aforementioned mathematical problems in machine learning. We will confirm\nour hypothesis by numerical experiments.\nPhysics-informed neural networks (PINNs) Lagaris et al. (1998); Raissi et al. (2019) are a method\nused to solve partial differential equations (PDEs) by integrating physical laws with neural networks"}, {"title": "METHODS", "content": "We briefly review the physics-informed neural networks (PINNs) Raissi et al. (2019) in the context\nof inferring the solutions of PDEs. Generally, we consider time-dependent PDEs for u taking the\nform\n$\\begin{aligned}\n&\\frac{\\partial u}{\\partial t} + \\mathcal{N}[u] = 0, \\quad t \\in [0, T], \\mathbf{x} \\in \\Omega, \\\\\n&u(0, \\mathbf{x}) = g(\\mathbf{x}), \\qquad \\mathbf{x} \\in \\Omega, \\\\\n&\\mathcal{B}[u] = 0, \\quad t \\in [0, T], \\mathbf{x} \\in \\partial \\Omega,\n\\end{aligned}$$\nwhere $\\mathcal{N}$ is the differential operator, $\\Omega$ is the domain of grid points, and $\\mathcal{B}$ is the boundary operator.\nWhen considering time-independent PDEs, $\\frac{\\partial u}{\\partial t} = 0$.\nThe ambition of PINNs is to approximate the unknown solution u to the PDE system Eq. (1), by\noptimizing a neural network $u_\\theta$, where $\\theta$ denotes the trainable parameters of the neural network.\nThe constructed loss function is:\n$\\mathcal{L}(\\theta) = \\mathcal{L}_{ic}(\\theta) + \\mathcal{L}_{bc}(\\theta) + \\mathcal{L}_{r}(\\theta),$"}, {"title": "SINC NUMERICAL METHODS", "content": "The Sinc function is defined as\u00b9\n$\\operatorname{Sinc}(x) = \\frac{\\sin(x)}{x},$$\nthe Sinc series $S(j, h)(x)$ used in Sinc numerical methods is defined by:\n$S(j,h)(x) = \\frac{\\sin[(\\pi/h)(x - jh)]}{(\\pi/h)(x - jh)},$$\nthen the Sinc approximation for a function $f$ defined on the real line $\\mathbb{R}$ is given by\n$f(x) \\approx \\sum_{j=-N}^N f(jh)S(j,h)(x), \\quad x \\in \\mathbb{R},$$\nwhere h is the step size with the optimal value $\\sqrt{\\pi d/\\beta N}$ provided in Theorem 1, and $2N + 1$ is the\ndegree of Sinc series.\nThanks to Sinc function's beautiful properties including the equivalence of semidiscrete Fourier\ntransform Trefethen (2000), its approximation as a nascent delta function, etc., Sinc numerical meth-\nods have become a technique for solving a wide range of linear and nonlinear problems arising from\nscientific and engineering applications including heat transfer Lippke (1991), fluid mechanics Ab-\ndella (2015), and solid mechanics Abdella et al. (2009). But Sinc series are the orthogonal basis\ndefined on $(-\\infty, \\infty)$ which is impractical for numerical methods. To use Sinc numerical methods,\none should choose a proper coordinate transformation based on the computing domain (a, b) and\nan optimal step size based on the target function f. However, manually changing the network to\nmeet every specific problem is impractical and wasteful. In the following of this section, we will\nintroduce current techniques used in Sinc numerical methods. Then in Section 2.3, we will unfold\nSinc numerical methods to meet machine learning."}, {"title": "Convergence theorem", "content": "Theorem 1 Sugihara & Matsuo (2004)\nAssume $\\alpha, \\beta, d > 0$, that\n(1) f belongs to $H\u00b9(D_d)$, where $H\u00b9$ is the Hardy space and $D_d = \\{z \\in \\mathbb{C} | |\\Im z| < d\\}$;\n(2) f decays exponentially on the real line, that is, $|f(x)| \\leq a \\exp(-\\beta|x|), \\forall x \\in \\mathbb{R}$.\nThen we have\n$\\sup_{-\\infty<x<\\infty} \\left|f(x) - \\sum_{j=-N}^N f(jh)S(j,h)(x)\\right| \\leq C N^{1/2} \\exp\\left[-(\\pi \\alpha \\beta N)^{1/2}\\right]$$\nIn engineering, they define Sinc function as $\\operatorname{Sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}$"}, {"title": "SINC KOLMOGOROV-ARNOLD NETWORK (SINCKAN)", "content": "Suppose $\\Phi = \\{\\phi_{p,q}\\}$ is the matrix of univariate functions where $p = 1, 2, . . . N_{in}, q = 1, 2, . . . N_{out}$.\nThen the L-layers Kolmogorov-Arnold Networks can be defined by:\n$\\operatorname{KAN}(\\mathbf{x}) = (\\Phi_{L-1} \\circ \\Phi_{L-2}\\circ \\cdots \\circ \\Phi_1 \\circ \\Phi_0) \\mathbf{x}, \\quad \\mathbf{x} \\in \\mathbb{R}^d.$$\nIn vanilla KAN Liu et al. (2024b), every univariate function $\\phi$ is approximated via a summation with\ncubic spline:\n$\\phi_{\\text{spline}} (x) = w_b\\sigma(x) + w_s \\sum_i c_i B_i(x)$$\nwhere $c_i, w_b, w_s$ are trainable parameters, and $B_i$ is the spline. Intuitively, to replace cubic interpo-\nlation with Sinc, we can define:"}, {"title": "LEARNING FOR APPROXIMATION", "content": "Approximating a function by given data is the main objective of KANs with applications in identi-\nfying relevant features, revealing modular structures, and discovering symbolic formulas Liu et al.\n(2024a). Additionally, in deep learning, the training process of a network can be regarded as ap-\nproximating the map between complex functional spaces, thus the accuracy of approximation di-\nrectly indicates the capability of a network. Therefore, we start with experiments on approximation\nto show the capability of SincKAN and verify whether SincKAN is a competitive network. In this\nsection, to have consistent results with KAN, we inherit the metric RMSE which is used in KAN.\nSinc numerical methods are recognized theoretically and empirically as a powerful tool when deal-\ning with singularities. However, in machine learning instead of numerical methods, we argue that\nSincKAN can be implemented in general cases. To demonstrate that SincKAN is robust, we con-\nducted a series of experiments on both smooth functions and singularity functions: in Table 1 the\nfirst four functions show the results of analytic functions including functions in which cubic splines\ninterpolation is good at, and the last four functions show the results of functions with singularities\nwhich Sinc interpolation is good at. The details of the used functions can be found in Appendix A."}, {"title": "SELECTING H", "content": "Utilizing a set of $\\{h_i\\}$ instead of a single step size h is a novel approach that we developed specif-\nically for SincKANs. To evaluate the effectiveness of this approach, in this section, we design a\ncomprehensive experiment. Suppose $h_{min} = \\min\\{h_i\\}, h_{max} = \\max\\{h_i\\}$, based on the discussion\nof Section 2.3, the ideal case is the optimal $h^* = \\sqrt{\\pi d^*/\\beta^* N} \\in (h_{min}, h_{max})$. In the experiments,\nwe provide two types of the set:\n1. inverse decay $\\{h_i\\}_{i=1}^M: h_i = 1/ih_o$,\n2. exponential decay $\\{h_i\\}_{i=1}^M: h_i = 1/h_o^i$\nThus the hyperparameters of the set $\\{h_i\\}$ are the base number $h_o$ and the number of the set $M$."}, {"title": "RELATIONSHIP BETWEEN DEGREE AND SIZE OF DATA", "content": "In Sinc numerical methods, the number of the sampled points is equal to the degree because each\ndegree requires a corresponding value f(jh) at the point jh i.e. $N_{degree} = N_{points}$. However,\nin SincKAN, $N_{degree} = N_{points}$ is impractical, and it is also not necessary because Sinc numer-\nical methods can be regarded as a single-layer representation while our SincKAN is a multi-layer\nrepresentation where the multi-layer representation has an exponentially increasing capability with\ndepth Yarotsky (2017). To explore the relationship between degree and size of data, we train our\nSincKAN with different $N_{degree}$ and $N_{points}$. The results are shown in Appendix H and reveal that\nit is unnecessary to maintain $N_{degree} = N_{points}$ in SincKANs."}, {"title": "LEARNING FOR PIKANS", "content": "Solving PDEs is the main part of scientific computing, and PINNs are the representative framework\nfor solving PDEs by neural networks. In this section, we solve a series of challenging PDEs to show\nthe performance of SincKAN. At first, we select several classical PDEs to verify the robustness of\nSincKAN, the results are shown in Table 2, and the details of the PDEs can be found in Appendix B."}, {"title": "BOUNDARY LAYER PROBLEM", "content": "To intuitively show the performance of SincKAN compared with other networks, we conducted\nadditional experiments on the boundary layer problem:\n$\\epsilon u_{xx} + u_x = 0, x \\in [0, 1]$$"}, {"title": "ABLATION STUDY", "content": "Compared with Sinc numerical methods, SincKANs have a normalized transformation; compared\nto KANs, SincKANs have a skip connection with linear functions. However, the Sinc numerical\nmethods also have some choices of coordinate transformations and KANs also have a skip con-\nnection with SiLU functions. In this section, we conduct an ablation study on SincKANs with\nnon-normalized transformation and the SiLU skip connection to verify the effect of the two pro-\nposed modules. This experiment uses Burger's equation Eq. (28) and time-dependent nonlinear\nequation Appendix B.5. Table 4 shows the results of the ablation study where $\\psi(x) = \\log(\\frac{x}{a - x})$,$\n$\\gamma(x) = \\tanh(x)$, $Linear(x) = w_1x + w_2 + \\phi(x)$, and $SiLU(x) = w_1 silu(x) + w_s \\phi(x)$. The non-\nnormalized transformation performs poorly, even compared to the cases without transformations.\nAlthough the linear skip connection is not the best for both equations, it is the most stable approach\nfor SincKANs."}, {"title": "CONCLUSION", "content": "In this paper, we propose a novel network called Sinc Kolmogorov-Arnold Networks (SincKANs).\nInspired by KANs, SincKANs leverage the Sinc functions to interpolate the activation function and\nsuccessfully inherit the capability of handling singularities. To set the optimal h, we propose the\nmulti-h interpolation, and the corresponding experiments indicate that this novel approach is the\nmain reason for SincKANs' superior ability in approximating complex smooth functions; to choose\na proper coordinate transformation for machine learning, we propose the normalized transformation\nwhich prevents slow convergence.; to satisfy the decay condition, we introduce the skip-connection\nwith learnable linear functions. After tackling the aforementioned challenges, SincKANs become a\ncompetitive network that can replace the current networks used for PINNs.\nWe begin with training on approximation problems to demonstrate the capability of SincKANs. The\nresults reveal that SincKANs excel in most experiments with other networks. However, directly\napproximating the target function is an impractical objective for almost all machine learning tasks.\nAfter verifying the capability, we turn to solving PDEs in the PINNs framework. Although the\nSincKANs achieve impressive performance in approximation tasks for solving all chosen PDEs,\nSincKANs merely have the best accuracy on boundary layer problems, due to the oscillations caused\nby the inaccuracy of derivatives.\nApproximating derivative by Sinc numerical methods is always inaccurate in the\nneighborhood of the Sinc end-points. To address this problem, Stenger (2009) suggested using La-\ngrange polynomial to approximate the derivative instead of straightforwardly calculating the deriva-\ntive of Sinc polynomials, Wu et al. (2006) used several discrete functions to replace the derivative\nof Sinc polynomials, etc. Unfortunately, to the best of our knowledge, there isn't an approach that\ncan be implemented in our SincKAN when we demand the derivatives of SincKAN in PIKANs.\nHerein, to alleviate the inaccuracy, we choose small ho, small M, and small N so that SincKAN\ncan solve PDEs, otherwise, the solution will have oscillations (see Appendix J). Such kind of setting\nlimits the capability of SincKAN and we argue that this is the main reason that SincKAN can obtain\ngood results but not the best results for some cases. Furthermore, the inaccuracy limits SincKAN\nin solving high-order problems such as Korteweg-De Vries equations, and Kuramoto-Sivashinsky\nequations.\nAs the accuracy of approximating the derivative decreases with the order of derivative\nincreases if the PDE merely requires the first derivatives, then the SincKANs will release the lim-\nitation to have larger enough ho, M, and N and improve the performance. In literature, to avoid\ncalculating the high-order derivatives, MIM Lyu et al. (2022); Li et al. (2024) is proposed to use the\nmixed residual method which transforms a high-order PDE into a first-order PDE system. SincK-\nANs can implement this approach to calculate several first-order derivatives instead of the high-order\nderivatives so that SincKANs can have accurate estimations for the residual loss. Furthermore, re-\nplacing the automatic differentiation Cen & Zou (2024); Yu et al. (2024) by other operators is also\nan expected research."}, {"title": "EXPLICIT EXPRESSION OF FUNCTIONS", "content": "The following functions are used in Table 1.\n1. sin-low\n$f(x) = \\sin(4\\pi x), \\quad x \\in [-1,1]$\n2. sin-high\n$f(x) = \\sin(400\\pi x), \\quad x \\in [-1,1]$\n3. bl\n$f(x) = e^{-100x}, \\quad x \\in [0, 1]$\n4. sqrt\n$f(x) = \\sqrt{x}, \\quad x \\in [0,1]$\n5. double-exponential\n$f(x) = \\frac{x(1-x)e^{-x}}{(1/2)^2 + (x - 1/2)^2}, \\quad x \\in [0,1]$\n6. multi-sqrt\n$f(x) = x^{1/2}(1 - x)^{3/4}, \\quad x \\in [0, 1]$\n7. piece-wise\n$f(x) = \\begin{cases}\n\\sin(20\\pi x) + x^2, & x \\in [0,0.5] \\\\\n0.5xe^{-x} + |\\sin(5\\pi x)|, & x \\in [0.5, 1.5] \\\\\n\\log(x-1)/\\log(2) - \\cos(2\\pi x), & x \\in [1.5, 2]\n\\end{cases}$$\n8. spectral-bias\n$f(x) = \\begin{cases}\n\\sum_{k=1}^4 \\sin(kx) + 5, & x \\in [-1,0] \\\\\n\\cos(10x), & x \\in [0,1]\n\\end{cases}$"}, {"title": "DETAILS OF PDEs", "content": "We consider the singularly perturbed second-order boundary value problem (perturbed in Table 2):\n$\\epsilon u_{xx} - u_x = f(x), \\quad x \\in [-1,1].$$\nIn specific cases, the problem has exact solutions, in this paper, we choose $f(x) = -1$, and the exact\nsolution is\n$u(x) = 1 + x + \\frac{e^{x/\\epsilon} - 1}{e^{-1/\\epsilon} - 1},$$\nwhere $\\epsilon = 0.01$ in our experiments.\nWe consider the nonlinear boundary value problem (nonlinear in Table 2):\n$\\begin{aligned}\n-u_{xx} + \\frac{u_x}{x} + \\frac{u}{x^2} & = \\frac{(-41x^2 + 34x - 1) \\sqrt{x}}{2x + \\frac{1}{2}}, \\quad x \\in [0,1] \\\\\nu(0) - 2u_x(0) & = 1, \\\\\n3u(1) + 4u_x(1) & = 9,\n\\end{aligned}$$\nwith the exact solutions\n$u(x) = x^{5/2}(1 - x)^2 + x^3 + 1.$"}, {"title": "BURGERS", "content": "We consider the Burgers' equation (Burgers' equation in Table 4):\n$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0, \\quad x \\in [-1,1], t \\in [0, 0.1].$$\nwith Dirichlet boundary condition, and the exact solution is\n$u = \\frac{a}{2} - \\frac{a \\exp(a(x - at/2))}{2 \\sinh \\left(\\frac{a^2 t}{4\\nu}\\right)},$$\nwhere $a = 0.5, \\nu = 0.01$ in our experiments."}, {"title": "T-NONLINEAR PROBLEM", "content": "We consider the time-dependent nonlinear problem (T-nonlinear in Table 4):\n$\\begin{aligned}\nu_t & = \\frac{x+2}{t+1} u_x, \\quad x \\in [-1, 1], t \\in [0,0.1]. \\\\u(x, 0) & = \\cos(x + 2), \\\\u(1, t) & = \\cos(3(t + 1)),\\end{aligned}$$\nwith the exact solution:\n$u(x,t) = \\cos((t + 1)(x + 2)).$"}, {"title": "CONVECTION-DIFFUSION", "content": "We consider the 1-D convection-diffusion equation with periodic boundary conditions (used in Ap-\npendix J):\n$\\begin{aligned}\nu_t + a u_x - \\epsilon u_{xx} & = 0, \\quad x \\in [-1, 1], t \\in [0, 0.1], \\\\u(x, 0) & = \\sum_{k=0}^5 \\sin (k\\pi x),\\end{aligned}$$\nwith the analytic solution\n$\\begin{aligned}\nu(x,t) & = \\sum_{k=0}^5 \\sin (k\\pi x - ka\\pi t) e^{-\\epsilon k^2 \\pi^2 t},\\end{aligned}$$\nwhere $\\epsilon = 0.01$, and $a = 0.1$in our experiments."}, {"title": "2D PROBLEMS", "content": "We consider the 2-D boundary layer problem (bl-2d in Table 2):\n$u_{xx/x1} + u_{x} + u_{yy/x2} + u_{y} = 0,$$\nwith the exact solution\n$u(x, y) = \\exp(-a_1x) + \\exp(-a_2y),$$\nwhere $x_1 = x_2 = 100$ in our experiments."}, {"title": "NAVIER STOKES EQUATIONS", "content": "We consider the Taylor-Green vortex (ns-tg-u and ns-tg-v in Table 2):\n$\\begin{aligned} & \\nabla \\cdot u = 0, t\\in [0,T], \\mathbf{x}\\in \\Omega, \\\\\n& \\frac{\\partial u}{\\partial t} + u \\cdot \\nabla u = -\\nabla p + \\nu \\Delta u, t\\in [0,T], \\mathbf{x} \\in \\Omega,\n\\end{aligned}$$\nwhere $u = (u, v)$, with the exact solution\n$\\begin{aligned} & u = \\cos(x) \\sin(y) \\exp(-2\\nu t) \\\\\n& v = \\sin(x) \\cos(y) \\exp(-2\\nu t) \\\\\n& p = -(\\cos(2x) + \\sin(2y)) \\exp(-4\\nu t)/4\n\\end{aligned}$$\nwith $T = 1$, $\\nu = 1/400$ in our experiments. After dimensionless, $x \\in [0, 1]^2$."}, {"title": "APPROXIMATION", "content": "The hyperparameters of used networks are shown in Table 6.\n\\begin{itemize}\n    \\item For the 1-D problem, we generate the training dataset by uniformly discretizing the input\ninterval to 5000 points and train the network with 3000 points randomly sampled from the\ntraining dataset for each iteration. In total, We train every network with $10^5$ iterations. Ad-\nditionally, to evaluate the generalization, we generate the testing (fine) dataset by uniformly\ndiscretizing the input interval to 10000 points.\n\\end{itemize}"}, {"title": "PIKANS", "content": "The hyperparameters of used networks are shown in Table 7.\n\\begin{itemize}\n    \\item For time-independent 1-D problems, we generate the training dataset by uniformly dis-\ncretizing the input interval to 1000 points, then train the network with 500 points randomly\nsampled from the training dataset for each iteration. In total, We train every network with\n$1.5 \\times 10^6$ iterations.\n    \\item For time-dependent 1-D problems, we generate the training dataset by uniformly discretiz-\ning the spatial dimension to 1000 points and the temporal dimension to 11 points, then\ntrain the network with 5000 points randomly sampled from the training dataset for each\niteration. In total, We train every network with $1.5 \\times 10^6$ iterations.\n    \\item For time-independent 2-D problems, we generate the training dataset by uniformly dis-\ncretizing every dimension to 100 points, then train the network with 5000 points randomly\nsampled from the training dataset for each iteration. In total, We train every network with\n$1.5 \\times 10^6$ iterations.\n    \\item For time-dependent 2-D problems, we generate the training dataset by uniformly discretiz-\ning every spatial dimension to 100 points and the temporal dimension to 11 points, then\ntrain the network with 50000 points randomly sampled from the training dataset for each\niteration. In total, We train every network with $1.5 \\times 10^6$ iterations.\n\\end{itemize}"}, {"title": "TRAINING", "content": "In Eq. (14), SincKAN has an additional summation on several h, so the trainable coefficients c\nare M times larger than KAN and ChebyKAN. However, the training time is not only dependent\non the number of total parameters, thus, we demonstrate the cost of training for approximation in\nTable 6, and demonstrate the cost of training for PDEs in Table 7. In Table 6 and Table 7, we use\n'depth \u00d7 width' to represent the size for MLP and modified MLP; 'width \u00d7 degree' to represent the\nsize for KAN and ChebyKAN; and 'width \u00d7 degree \u00d7 M' to represent the size for SincKAN. Note\nthat we train the network in two environments distinguished by two superscripts:\n\u2020: training on single NVIDIA A100-SXM4-80GB with CUDA version: 12.4.\n\u2021: training on single NVIDIA A40-48GB with CUDA version: 12.4."}, {"title": "REFERENCING", "content": "As the referencing cost doesn't depend on the task i.e. the loss function, the results are evaluated on\nthe model trained by approximation task and the results can be found in Table 6. The results reveal\nthat although SincKAN has much more parameters than KAN and ChebyKAN, SincKAN is faster\nwhen referencing. Note that the referencing is slower than training because we compile the training\nprocedure by JAX Bradbury et al. (2018)."}, {"title": "RESULTS OF DIFFERENT DEGREE", "content": "In this experiment, we train our SincKAN on spectral-bias function on $N = 8, 16, 32, 64, 100, 300$\nand $N_{points} = 100, 500, 1000, 5000, 10000$ with the inverse decay $\\{h_i\\}_{i=1}^M$ in $M = 6$ and $h_0 = 7.0$.\nMoreover, we set the batch size $N_{batch} = N_{points}/4$ to adapt to the changing of $N_{points}$. The\nresults are shown in Table 8 and Fig. 7. Additionally, Fig. 7(a) shows that our neural scaling law is\n$RMSE \\times G^{-4}$ compared to the best scaling law $RMSE \\times G^{-3}$ claimed in KAN Liu et al. (2024b)."}, {"title": "RESULTS OF SELECTED H", "content": "Table 9 and Table 10 show the results of selected $\\{h_i\\}$ in details. However, there are so many\nhyperparameters that may be adjusted when h_i is larger. For example, for the large h on fine grids,\nwe argue that Ndegree = 100 may not exploit the capability fully. Thus, we conducted an extra\nexperiment with 5000 grid points, $\\{h_i\\} = \\{1/10, 1/100, 1/1000\\}$, and $N_{degree} = 500$. For sin-low,\nthe RMSE is 7.92e-4 \u00b1 4.21e-4, and for the sin-high, the RMSE is 2.32e-3 \u00b1 2.74e-4. It shows that\nfor sin-high, the SincKAN can obtain a more accurate result if we further tune the hyperparameters."}, {"title": "OSCILLATIONS OF SINCKAN", "content": "We conducted the experiments on convection-diffusion equations (Eq. (32)) with $h_o = 2.0, 10.0$,\n$N = 8,100$, and $M = 1,6$. Except $h_o = 2.0$ and $N = 8$, the inaccuracy of derivatives makes\nSincKAN unstable with the loss diverging. We choose some figures plotted in Fig. 9 to show the\noscillations that limit the improvement of SincKANs."}]}