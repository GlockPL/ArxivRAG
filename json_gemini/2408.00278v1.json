{"title": "High Performance Im2win and Direct Convolutions using Three Tensor Layouts on SIMD Architectures", "authors": ["Xiang Fu", "Xinpeng Zhang", "Jixiang Ma", "Peng Zhao", "Shuai Lu", "Xu T. Liu"], "abstract": "Convolution is the core component within deep neural networks and it is computationally intensive and time consuming. Tensor data layouts significantly impact convolution operations in terms of memory access and computational efficiency. Yet, there is still a lack of comprehensive performance characterization on data layouts on SIMD architectures concerning convolution methods. This paper proposes three novel data layouts for im2win convolution: NHWC, CHWN, and CHWN8, and introduces a set of general optimization techniques for both direct and im2win convolutions. We compare the optimized im2win convolution with the direct convolution and PyTorch's im2col-based convolution across the aforementioned layouts on SIMD machines. The experiments demonstrated that the im2win convolution with the new NHWC layout achieved up to 355% performance speedup over NCHW layout. Our optimizations also significantly improve the performance of both im2win and direct convolutions. Our optimized im2win and direct convolutions achieved up to 95% and 94% of machine's theoretical peak performance, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "Convolution is the essential component of deep neural networks for computer vision tasks such as feature exaction from large-scale image data [1]. It not only comprises 50%-90% of computational operations including convolutional, pooling, ReLU, and fully-connected layers [2], but also consumes more than 90% of the total execution time of many popular neural networks [3]\u2013[5]. Hence, optimizing convolution operations is crucial for enhancing the performance of neural networks.\nConvolution methods can be classified into three main categories based on how they transform the input tensor: direct, im2col-based, and im2win. Direct convolution performs convolution operations directly on the tensor without changing its format [6]. This approach avoids extra memory consumption compared to im2col-based and im2win convolutions, but it suffers from nonconsecutive memory access. Im2col-based convolution transforms the convolution into general matrix-matrix multiplications (GEMM) [7], [8], leveraging optimized Basic Linear Algebra Subprograms (BLAS) [9] for excellent performance. It has regular memory access but a significant extra memory footprint, which greatly limits its applicability on memory-constrained devices. Previously, we propose a memory-efficient convolution called image-to-window (im2win), which reorganizes the input tensor into a row of dot product windows and flattens the unique elements\nof these windows into a row that correspond to the convolution operation's receptive fields [10], [11]. It provides sequential memory access and data reuse, and thus greatly reduces memory overhead.\nA tensor memory/data layout (referred as layout henceforth) refers to how the data of a tensor is physically arranged in memory. There are commonly three layouts for tensors: NCHW, NHWC and CHWN, where N is the batch size, C is the number of channels, H is the image height, and W is the image width. It significantly impacts convolution operations in terms of memory access, computational efficiency and compatibility with deep learning frameworks [12]\u2013[14].\nIn general, the memory efficiency and performance implications of various tensor layouts with different convolution algorithms on single-input, multiple-data (SIMD) architectures have received limited attention. The previous im2win works optimize the NCHW layout on CPU and GPU but have not tried the NHWC and CHWN layouts [10], [11]. Li et al. reveal the performance impact of the NCHW layout with the im2col-based convolution and the CHWN layout with the direct convolution in different CNN layers, and propose a fast multi-dimension layout transformation algorithm on GPU [15]."}, {"title": "II. PRELIMINARY AND RELATED WORKS", "content": "A. Notation\nThe input of a convolution operation includes an input tensor (I), a filter tensor (F), and an output tensor (O). In these tensors, Ni is the batch size, s is the stride size, Ci and Co are the number of input and output channels (also known as feature maps), Hi/f/o and Wi/f/o denote the height and width of a feature map.\nB. Tensor Layouts: NCHW, NHWC, CHWN\nIn the NCHW layout, the input (1), the filter (F), and the output tensors (O) are expressed as I[N][C][Hi][Wi], F[Co][C][HF][Wf], and O[Ni][Co][Ho][Wo], respectively. The convolution is defined as:\n$O(i,j,m,n) = \\sum_{j=1}^{Ci} \\sum_{m=1}^{Hf} \\sum_{n=1}^{Wf} (I(i,j,mxs+u,mxs+v)\\timesF(j,r,u,v)),$ (1)\nIn the NHWC layout, the input (1), the filter (F), and the output tensors (O) are expressed as I[Ni][Hi][Wi][Ci], F[Co][Hf][Wf][Ci], and O[Ni][Ho][Wo][Co], respectively. The convolution is defined as:\n$O(i,m,n,j) = \\sum_{j=1}^{Ci} \\sum_{m=1}^{Hf} \\sum_{n=1}^{Wf} (I(i,mxs+u,mxs+v,j)\\timesF(j,u,v,r)),$ (2)\nIn the CHWN layout, the input (1), the filter (F) and the output tensors (O) are expressed as I[Ci][Hi][Wi][Ni], F[Ci][Hf][Wf][Co], and O[Co][Ho][Wo][Ni] respectively. The convolution is defined as:\n$O(j,m,n,i) = \\sum_{j=1}^{Ci} \\sum_{m=1}^{Hf} \\sum_{n=1}^{Wf} (I(j,mxs+u,mxs+v,i)\\timesF(r,u,v,j)),$ (3)\nThe definitions (1), (2) and (3) above are all subject to\nj = 1, 2, .., Co, m = 1, 2, .., Ho, n = 1, 2, .., Wo,\ni = 1, 2, .., Ni, u = 1, 2, .., Hf, v = 1, 2, .., Wf,\nr = 1, 2, .., Ci.\nC. Convolution Algorithms and Related Works\nDirect convolution performs on the original I and F without any tensor transformation. It has seven nested for loops and an AXPY operation in the innermost loop. Based on the tensor layouts that direct convolution works with, the AXPY operation needs to read at different indices of F and I, and writes at different indices of O.\nDirect convolution can compute with the original input ten-sors, so they usually adopt the NCHW layout of raw images. A study shows that, for convolution instances with large C, NHWC layout outperforms NCHW layout [15]. Grouping a certain dimension of the input tensors by a fixed size can also enhance the performance of direct convolution, such as NC32HW32 layout [17]. Several works [6], [18] have shown that the performance of direct convolution can be greatly improved by designing specific layouts based on the loop ordering of the algorithm on SIMD architecture.\nThe im2col-based convolution transforms a convolution operation into a GEMM operation. I[Ni][Ci][Hi][Wi] is pro-cessed in N\u2081 batches, each batch contains data I'[Ci][Hi][Wi] (that is, a single image). The im2col algorithm flattens the elements of each dot product window of I' and copies them into a single row of a 2D matrix [7]. In addition to the conventional im2col data transformation algorithm, the MEC algorithm compresses the matrix layout, while still enabling the utilization of high-performance BLAS algorithms to per-form convolution operations [19]."}, {"title": "III. HIGH-PERFORMANCE IM2WIN AND DIRECT CONVOLUTION USING THREE TENSOR LAYOUTS", "content": "In this section, we first review three layouts in the context of the direct convolution. Then we present three new layouts for the im2win convolution, following by how to determine the loop ordering based on the layouts and a set of optimizations for the im2win and direct convolutions on SIMD architectures.\nA. Motivations for Different Tensor Layouts\nAn example of direct convolution on an original input tensor in the NHWC layout is illustrated in Figure 2. The NHWC layout prioritizes the storage of elements in the last logical dimension-C\u2081-followed by that of Wi, Hi, and Ni. In the NHWC layout, the elements with the same Ni, Hi, and Wi but different Ci are contiguous in memory, which have unit stride access. Assuming a stride of 1, the red and green elements (outlined by solid lines, representing the convolutional windows to compute a single output element) in the input tensor are multiplied with the corresponding elements in the filter tensor, and the results are summed up (i.e., an AXPY) to obtain 000. Next, the green and yellow elements (outlined by dashed lines) are used to calculate 001. This process continues, with the convolutional window moving by the stride length in Hi or Wi, until all elements of the output tensor are computed.\nRecall in Figure 1, the NCHW layout prioritizes the storage of elements in the last logical dimension-W-followed by that of Hi, Ci, and Ni. Note that all Ci but not all W\u2081 are used during the AXPY operation to obtain one output element. NCHW has non-unit stride access during the tensor convolution. However, the non-unit stride access may not be harmful, depending on the effects of caching and the access patterns used (determined by the loop ordering).\nAs shown in Figure 3, the CHWN layout stores elements by prioritizing Ni, followed by Wi, Hi, and Ci in memory. Previous research on GPU recommends to use N as the lowest dimension for coalesced memory access and data reuse in registers [15]. It has been observed that the performance is sensitive to the value of N. The elements within the solid-lined boxes are used to compute the first eight output elements, while the elements within the dashed-lined box are used for the next eight output elements. This facilitates the use of vector registers for vectorization. Assuming s = 1, the convolutional window moves by one element in both the Hi and Wi dimensions until all output elements are computed.\nB. Im2win Tensor Transformation on Three Tensor Layouts\nIn this subsection, we present the im2win tensor transforma-tion on three tensor layouts. The im2win tensor transformation process for the NWHC layout is shown as Algorithm 1. For other layouts, slight modifications need to be made on it. The im2win transformation flattens the elements of a convolutional window, storing them contiguously in memory and prioritizing them in the Ci dimension, as outlined in Algorithm 1 from Line 4 to Line 7. An example of transforming the original input tensor into an im2win tensor in the NHWC layout is"}, {"title": "C. Loop Reordering", "content": "In this subsection, we reorder the loops for the im2win and direct convolutions based on different tensor layouts. Ideally, we arrange the inner loops to access data closer in memory to enjoy the unit stride access as long as possible. In both direct and im2win convolutions, for NCHW, CHWN, and CHWN8 layouts, as they have the CHW memory access pattern, we use the width of the convolution window as the innermost loop,\nfollowed by the height, and the channel (Wf, Hf, and Ci). Conversely, for the NHWC layout, the innermost three loops iterate over the channel, the width, and the height (Ci, Wf, and Hf) of the convolution window. For the im2win convolution, since the im2win transformation flattens the elements of a convolutional window, we usually coalesce the height and the width layers (Wf, Hf) into a Wf \u00d7 Hf.\nNext we determine the order of the outer four loops. It not only determines the order in which the elements of the output tensor are produced but also influences the order in which the element in the input tensor are accessed. The layout mainly affects the inner three levels of the loop order, for the outer four levels the loop order applies to all data layouts. Recall in Figure 2, the green elements of the input tensor are shared between the convolution windows of adjacent output tensor elements. Therefore, we consider to position the width (W\u3002) of the traversed output tensor in the fourth layer of loops at Line 6 in Algorithm 2. Because the memory access of the input tensor is expensive in a convolution operation, we set the channel (Co) as the third layer to reduce its access. Since the batch of the output tensor corresponds to the batch of the input tensor, we place the batch (Ni) in the first layer of the loop. Finally we have the order of the outer four loops as NHCW in Algorithm 2.\nD. Optimizations for the Im2win and Direct Convolutions\nIn this subsection, we use the Roofline Model [20] to determine how to optimize the im2win and direct convolu-tions on SIMD systems. We propose a set of optimizations for both the im2win and direct convolutions and apply the optimizations to them (with a slight modification on the direct convolution). The optimizations are classified into two categories: reducing the memory bottleneck and increasing the arithmetic intensity of the convolution kernel. The former"}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Setup\nArchitectures. We use a server with two Intel\u00ae Xeon\u00ae Gold 6330 CPUs and 251 GB RAM. Each CPU has 28 physical cores, running at 2.0 GHz, with 48 KB L1d cache, 32 KB Lli cache, 1.28MB L2 cache and 43 MB L3 cache.\nBenchmarks We aim to cover the majority of the con-volutional layers in commonly used DNNs. Hence for our experimental evaluation, we select an state-of-the-art DNN benchmark [19] shown in Table I, which includes twelve unique convolution layers, conv1-conv12.\nSoftware We compare the direct, im2win, and im2col-based convolutions with four tensor layouts: NHWC, NCHW, CHWN, CHWN8. We intend to compare with the state-of-the-art implementation of direct convolution and the layout proposed in [18], but their implementation is not open-sourced. We use the im2col-based convolution in PyTorch 2.1 [24] with MKL [25]. Note that PyTorch only supports the NHWC and NCHW layouts. Our code is compiled with GCC 9.5.0 compiler and -03 -mavx2 -mfma -fopenmp -march=native compilation flags. We use OpenMP 4.0 for parallelization with guided scheduling.\nB. Performance of Different Convolution Algorithms\nWe run each algorithm 50 times on each benchmark with N\u2081=128 and report the best runtime. Figure 4 shows the per-formance results in TFLOPS and Figure 5 shows the memory usage of our high-performance direct convolution and high-performance im2win-based convolution, and the im2col-based convolution using MKL in PyTorch. In Figure 4, the left y-axis shows the performance in TFLOPS, and the right y-axis shows the performance of the machine peak. Overall, our optimized im2win convolution achieves eight best TFLOPS out of twelve benchmarks; our optimized direct convolution achieves three best TFLOPS out of twelve benchmarks and the im2win convolution achieves close performance on these three; the im2col-based convolution in PyTorch achieves one out of twelve best TFLOPS on conv12. All twelve best TFLOPS are all yielded from the NHWC layout across these three methods.\nOur proposed optimization techniques are proven effective on both im2win and direct convolutions. Our im2win convolution achieves 95% and 91% of the theoretical peak performance of the architecture on conv5 and conv6 respectively. Our direct convolution achieves 91% and 94% of the theoretical peak performance on conv5 and conv6 respectively.\nWith the NHWC layout and performance normalization, excluding conv6 and conv12, our im2win convolution achieves between 1.1\u00d7 and 4.6\u00d7 performance speedup, and our direct convolution achieves between 1.1\u00d7 and 3.8\u00d7 performance speedup against the im2col-based convolution. All the best performance of the im2win convolution on twelve benchmarks is achieved using the NHWC layout on CPU. Our im2win"}, {"title": "V. CONCLUSION", "content": "We proposed three new layouts for the im2win convolution: NHWC, CHWN and CHWN8, and a set of general optimiza-tion techniques for both direct and im2win convolutions on SIMD architectures. We applied these optimizations on the im2win and the direct convolutions, and compared with Py-Torch's im2col-based convolution on the above layouts along with the NCHW layout. Our experiments demonstrated that im2win convolution using the new NHWC layout achieved 11% to 355% performance speedup compared to NCHW layout. The proposed optimizations were proven to boost the performance of the im2win convolution and direct convolution. Our optimized im2win and direct convolutions achieved up to 95% and 94% of the theoretical peak performance of the machine, respectively."}, {"title": "APPENDIX", "content": "We leave some information out from the main body of this paper and have them as part of an appendix. Because we consider they are not essential to the main argument but may be useful for readers who want to dive deeper into the topic.\nA. Peak Performance\nWe use the following formula to calculate the peak-gflops of the server:\n$\\frac{peak flop}{S} = (#processors) \\times (#coresper\\_processor) \\times(clockspeed[1/s])\\times(2\\times#FMAunits) \\times \\frac{vector\\_size[bits]}{64}$ (4)\nBased on Equation (4), the peak GFLOPS of the server that we use in our experimental evaluation is 3584 GFLOPS.\nB. Batch Size Scaling on Different layouts\nWe perform a strong scaling on the batch size from 32, 64, 128, 256 to 512 with different layouts using the direct convolution and the im2win convolution. The performance results of the direct convolution with the CHWN, CHWN8, NCHW, and NHWC layouts are shown in Figure 6, Figure 7, Figure 8, and Figure 9, respectively. The performance results of the im2win convolution with the CHWN, CHWN8, NCHW, and NHWC layouts are shown in Figure 10, Figure 11, Figure 12, and Figure 13, respectively.\nFrom the figures, we can tell that the CHWN layout is most sensitive to the batch size among four layouts. The performance of the direct and im2win convolutions is the best on twelve benchmarks when the batch size is 32 (except for the direct convolution on conv12). Recall in Section III-B, the efficiency of the CHWN layout is constrained by the number of vector registers on the SIMD machines. When Ni > 8, this leads to low cache utilization.\nWith our proposed CHWN8 layouts, the performance of the direct convolution and the im2win convolution exhibits similar patterns on twelve benchmarks, that is, when the channel sizes of the benchmarks are small (C\u2081=3 for conv1, conv2, conv3), the smaller the batch size is, the better performance the convolutions have on these benchmarks; when the channel sizes of the benchmarks (conv4-conv12) are large, the larger the batch size is, the better performance the convolution have on these benchmarks.\nWith the NWHC and NCHW layouts, both convolution methods show no obvious evidence that they are sensitive to the batch size across all benchmarks. The performance vari-ance is contributed by the overall dimension of the input/filter tensors, and the loop coalescing which we coalesce the Ni and Ho dimensions into one parallel loop to achieve better load balance."}]}