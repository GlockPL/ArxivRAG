{"title": "CWT-Net: Super-resolution of Histopathology Images Using a Cross-scale Wavelet-based Transformer", "authors": ["Feiyang Jia", "Zhineng Chen", "Ziying Song", "Lin Liu", "Caiyan Jia"], "abstract": "Super-resolution (SR) aims to enhance the quality of low-resolution images and has been widely applied in medical imaging. We found that the design principles of most existing methods are influenced by SR tasks based on real-world images and do not take into account the significance of the multi-level structure in pathological images, even if they can achieve respectable objective metric evaluations. In this work, we delve into two super-resolution working paradigms and propose a novel network called CWT-Net, which leverages cross-scale image wavelet transform and Transformer architecture. Our network consists of two branches: one dedicated to learning super-resolution and the other to high-frequency wavelet features. To generate high-resolution histopathology images, the Transformer module shares and fuses features from both branches at various stages. Notably, we have designed a specialized wavelet reconstruction module to effectively enhance the wavelet domain features and enable the network to operate in different modes, allowing for the introduction of additional relevant information from cross-scale images. Our experimental results demonstrate that our model significantly outperforms state-of-the-art methods in both performance and visualization evaluations and can substantially boost the accuracy of image diagnostic networks.", "sections": [{"title": "I. INTRODUCTION", "content": "Super-resolution (SR) is a pivotal task within the realm of computer vision, focusing on the enhancement of high-resolution (HR) images using information extracted from their low-resolution (LR) counterparts [1]. Interpolation algorithms are commonly employed for SR task, include nearest neighbor interpolation, bilinear interpolation, bicubic interpolation, and more [1]. Traditional Single Image Super-resolution (SISR) techniques often rely on substantial prior information to achieve image reconstruction [2], [3], and these methods emphasize information correlation across different frequency domains but tend to overlook the specific degradation process associated with low-quality images. In recent years, significant advancements have been made in the field of SR tasks, primarily focusing on real-world images [4]-[22]. Medical images play a crucial role in clinical diagnosis, treatment planning, and quantitative analysis of human or-gans and tissues. However, acquiring high-resolution medical images is often a time-consuming and costly process. SR methods offer a cost-effective solution for enhancing the quality of medical images. Zhao X et al. [23] introduced CSN, a model designed to enhance the quality of magnetic resonance images (MRI). Qiao C et al. [24] created the BioSR dataset and developed the DFCAN/DFGAN for scientific microscope images. Li Z et al. [25] proposed TSMLSRNet, a method aimed at reconstructing arterial spin-labeled perfusion MRI images. L Mukherjee et al. [26] utilized medium-level resolution images from Whole Slide Images (WSI) to reconstruct LR images. Chen Z et al. [27] developed SWD-Net, a model focused on reconstructing breast cancer histopathology images using wavelet domain features. Wu X et al. [28] presented MMSRNet that tackles multiple SR tasks with different magnifications as a unified, joint task. Wang H et al. [29] proposed W\u00b2AMSN for MR images, this network extracts features of different sizes at multiple scales and designs a non-reduction attention mechanism to recalibrate feature responses adaptively. Qiu Z et al. [30] proposed DS2F, which uses super-resolution tasks as an auxiliary to medical image segmentation tasks. The existing studies, despite achieving impressive results in objective metrics, are still constrained by their working paradigms. Image SR research typically operates within two paradigms: SISR [4]\u2013[6], [8]\u2013[11], [13], [23]\u2013[25], [27], [31]-[36] and RefSR (Reference-Based Super-Resolution) [37]- [40]. In the SISR paradigm, network models can rapidly learn the LR data distribution. Increasing network depth generally improves model performance, but this often comes at the"}, {"title": "II. RELATED WORK", "content": "Most deep learning-based SR methods operate under the SISR paradigm. Their fundamental approach revolves around designing multiple pathways to extract valuable information from the limited data available in LR images, including residual structures [6], [8]-[10], [36], generative structures [6], [11], [20], [33], attention mechanisms [9], [22], [31], [34], [42], and digital image spectrum analysis [12], [13], [17], [18], [27], [34]. For instance, Chen Z et al. [27] proposed SWD-Net for the SR task of breast cancer histopathology images. In this work, the wavelet-aware convolution (WAC) module can effectively extract the context information in the wavelet domain, while the wavelet feature adaptation (WFA) module adjusts the wavelet coefficients to an appropriate range. Shahidi [33] introduced WA-SRGAN with Generate Adversarial Networks structure [43] to reconstruct breast can-cer histopathology images, evaluating the model using various metrics like PSNR [44], [45], SSIM [45], and MSE. RefSR methods [37]-[40] aim to reconstruct HR images using additional information from reference images, requiring techniques to identify and align similar regions between the reference and LR images. TTSR [37] utilizes Transformer to jointly learn LR and reference image relationships for accurate texture transfer. However, RefSR methods are inherently constrained by the availability and quality of reference images [37], [40]. CWT-Net leverages the hierarchical structure of histopathology images to introduce cross-scale wavelet information while maintaining the SISR paradigm. Specifically, we design the wavelet reconstruction module to approximate cross-scale information from LR images, replacing the WT branch's information flow during network testing. This strategy enables CWT-Net to harness multi-level information during training and overcome cross-scale limitations during testing for more effective histopathology image super-resolution."}, {"title": "III. METHOD", "content": "To effectively address the challenge of obtaining high-resolution images from multi-level histopathology data, we introduce a end-to-end framework named CWT-Net. CWT-Net comprises three integral components: the SR branch, the WT branch, and the Transformer module. In the initial stages of processing, histopathology images $I_{GT}$ from the training set undergo a downsampling procedure using an interpolation-based degradation algorithm. This results in the creation of corresponding LR images, denoted as $I_{LR}$. To make the most of the intermediate structural information within the histopathology images, we execute an additional round of downsampling for each $I_{GT}$ to generate $I_{GT'}$. Subsequently, $I_{LR}$ serves as the input to the SR branch, while $I_{GT'}$ acts as the input to the WT branch. Importantly, $I_{GT}$ and $I_{GT'}$ share identical sampling center coordinates, and their sampling radii are inversely proportional to the data level. When $I_{GT}$ is scaled to match the resolution of $I_{GT'}$, both images encompass an equivalent range of histopathology sections. It's vital to ensure that $I_{GT}$ adheres to the condition expressed in Eq. (1):\n$L_{UGT} > L_{ugt'} \u2265 L_{VLR}$                                                (1)\nwhere $L_{UGT}$, $L_{ugt'}$, and $L_{VLR}$ denote the sampling levels of $I_{GT}$, $I_{GT'}$, and $I_{LR}$, respectively. When $L_{ugt'} = L_{UGT}$, the wavelet information harnessed and provided by the WT branch wholly encompasses the wavelet information present in the LR image. Conversely, when $L_{ugt'} = L_{VLR}$, CWT-Net essentially reverts to the SISR paradigm, with the features of the WT branch failing to contribute additional information. We do not consider scenarios where $L_{ugt'}$ exceeds $L_{ugt}$ or falls below $L_{ULR}$: the former renders the SR task nonsensical (an image with more information than $I_{GT}$ cannot be employed as a prior for the SR task), while the latter contradicts the expectations of the WT branch. Subsequently, the data traverse through a series of Cross-scale Wavelet-based Transformer Blocks (CWTBs), each comprising a residual module for the SR branch, a residual module for the WT branch, and a Transformer block. Towards the conclusion of the SR branch, the features derived from $I_{LR}$ pass through the upsampling module $B_{up}$ to produce the SR outcome, denoted as $I_{HR}$."}, {"title": "B. SR Branch", "content": "The SR branch in CWT-Net serves as the primary com-ponent for executing its core task, as illustrated in Fig.2.a. For an input image $I_{LR}$, regardless of its dimensions, we initially define the feature as $f_{sr}$. Within the SR branch, a MeanShift layer is employed to normalize the color channels by subtracting their means, followed by a convolution layer that produces the initial shallow feature $f_{sr}$. Subsequently, $f_{sr}$ proceeds through the SR branch's backbone to undergo further feature extraction. This backbone is comprised of several residual blocks denoted as $B_{RB}$. Each $B_{RB}$ contains multiple sets of residual structures.\nDefine CWT-Net containing n CWTBs, and denote $f_{n-1}$ and $f_{mwt}$ as the features within both branches after the (m-1)-th CARBs (m \u2212 1 \u2264 n). For the m-th CWTB, the feature flow process in the SR branch is described by Eq. (2):\n$f_{m}^{sr} = B_{RB}^{sr} (f_{m-1}^{sr}) \\bigoplus B_{Rf} {B_{RB}^{sr} (f_{m-1}^{sr}), B_{RB}^{wt} (f_{mwt1}) \\}$                                                        (2)\nwhere $\\bigoplus$ denotes element-wise summation, $f_{m}^{sr}$ is the feature of the SR branch after the m-th CWTB, $B_{RB}^{sr}(.)$ and $B_{RB}^{wt}(.)$ denote the m-th residual block of the SR branch and WT branch (Section III-C), respectively, and $B_{f}(.,.)$ is the m-th Transformer block (Section III-D). The SR branch in CWT-Net incorporates a long skip con-nection, as we have observed that further stacking of CWTBs does not yield improved performance. The primary aim of this long skip connection is to convey shallow features $f_{sr_0}$ to the lower depths of the network, to compensate for the potential loss of low-frequency information. This process is represented by Eq. (3):\n$f_{sr} = f_{sr_0} \\bigoplus f_{sr}^{sr}$                                                (3)\nwhere $f_{sr}^{sr}$ represents the comprehensive feature generated by the SR branch's backbone network. The nested residual connections within the SR branch facilitate the utilization of maximum contextual information within the network's con-strained depth. Upon reaching the end of the SR branch, $f_{sr}$ initially undergoes the MeanShift layer to restore the original channel means. Subsequently, it passes through the $B_{up}$, which incorporates the subpixel shuffle layer to incrementally scale the image to the desired magnification levels."}, {"title": "C. WT Branch", "content": "The WT branch's purpose is to extract wavelet transform features from the multi-scale image $I_{GT'}$. We initialize the feature in the WT branch as $f_{wt}$. To capture wavelet transform features, $f_{wt}$ first undergoes a 2D discrete wavelet transform (DWT) module [48] to produce the feature $f_{hwt}$. Specifically, the DWT utilizes the filter $F_{HH}$ for convolution with $f_{hwt}$ to obtain sub-features. The filter $F_{HH}$ is defined as shown in Eq. (4):\n$F_{HH} = \\frac{1}{2} \\begin{bmatrix}\n1 & -1 \\\\\n-1 & 1\n\\end{bmatrix}$                                               (4)\nFollowing the DWT module, the feature $f_{hwt}$ encodes the diagonal structural details of $I_{GT'}$. We opt for the Haar wavelet for implementing the DWT module. In histopathology images, dense high-frequency information often arises from tissue cell membrane edges. The Haar wavelets effectively describe the high-frequency characteristics of histopathology images [11]. Notably, $f_{hwt}$ shares the same dimensions as $f_{sr}$ from the SR branch because the sub-features generated by wavelet transform at each level are only half the size of the original features.\nSubsequently, the feature $f_{hwt}$ is passed through a convo-lutional layer, resulting in the initial feature $f_{hwt}$ for the WT branch. The backbone network of the WT branch is composed of several residual blocks $B_{RB}^{wt}$. The feature flow within the backbone network of the WT branch is defined as shown in Eq. (5):\n$f_{m}^{hwt} = B_{RB}^{wt} (f_{mwt1})$                                                (5)\nwhere m, $B_{RB}(.)$, $f_{m}^{hwt}$, and $f_{hwt}$ are defined in the same manner as Eq. (2). It's important to note that the depth of $B_{RB}^{wt}$ varies based on the upsampling factor in the SR task (Section IV-B). The disparity in distance scaling among the four channels affects the wavelet transform differently. High-frequency channels with larger distance scaling disparities might lead the network to disregard wavelet coefficients with smaller values. In terms of texture features, the original fea-tures from the wavelet transform exhibit sharp but incoherent high-frequency edges. This becomes especially pronounced as the average distance between $I_{GT'}$ and $I_{LR}$ increases, particu-larly in scenarios with higher upsampling factors. Hence, it's crucial to dynamically adjust the depth of the WT branch to address these issues.\nTo enable the WT branch to operate within the SISR paradigm, we have introduced the wavelet reconstruction (WR) block, as depicted in Fig. 2.b. In situations where $I_{GT'}$ is unavailable to the WT branch, such as during testing or when $I_{GT'}$ is not provided, we want the WT branch to utilize approx-imate features from $I_{LR}$ to complete its task. The WR block is constructed based on the channel attention (CA) mechanism. Specifically, the channel attention mechanism employs global average pooling to transform global spatial information into channel descriptors and uses the sigmoid activation function. The ReLU [49] layer is then applied to further accentuate the weight differences between features. The WR block employs a smaller feature downsampling factor to extend the weighting range. Similar to the backbone network, the depth of the WR module varies based on the upsampling factor (Section IV-B). For the feature $f_{hwt}$ with a size of C\u00d7H/S\u00d7W/S, where S is a multiple of the SR task, the average pooling layer reduces $f_{hwt}$ to 1 x 1 to obtain the primary representation $f_{g}$ of the channel, as described in Eq. (6):\n$f_{g} = AvgPool (f_{hwt}) = \\frac{1}{H\u00d7W}\\sum_{i=0}^{H} \\sum_{j=0}^{W} f_{hwt}(i, j)$                                           (6)\nwhere $f_{hwt}(i, j)$ denotes the value at position (i,j) in $f_{hwt}$, and AvgPool(.) represents global average pooling. Subsequently, $f_{g}$ is described as Eq. (7):\n$f_{wr} = \u03b4 (F_{u} (\u03c3 (F_{l} (f_{g}))))$                                                  (7)\nwhere $f_{m}^{wr}$ denotes the output of the m-th WR block, \u03b4(.) and \u03c3(.) represent the ReLU and sigmoid activations, and $F_{l}(.)$ and $F_{u}(.)$ denote the feature downsampling convolution and feature upsampling convolution performed by CAR. Finally, $f_{wr}$ is upsampled and amplified to the size of C\u00d7 H \u00d7 W."}, {"title": "D. Transformer Block", "content": "The WT branch is dedicated to capturing wavelet infor-mation and exhibits a superior capacity to generalize high-frequency details compared to the SR branch. Drawing in-spiration from [37], we introduce the Transformer block to facilitate the SR branch in learning high-frequency information representation from the WT branch. In our Transformer block, the feature $f_{m}^{sr}$ from the SR branch and the feature $f_{m}^{hwt}$ from the WT branch serve as Query (Q) and Key (K), respectively. Additionally, we denote $f_{m}^{hwt'}$ as the feature $f_{m}^{hwt}$ after upsampling and downsampling, representing it as Value (V). The Transformer block initially partitions these features into patches, and for any pair of patches in Q and K, it computes their similarity $\u03c4_{(i,j)}$ through inner product, estimating the similarity between $I_{LR}$ and $I_{GT'}$. A higher inner product signifies a stronger correlation between the patches and a richer transferable high-frequency information. The expression for $\u03c4_{(i,j)}$ is defined as Eq. (8):\n$\u03c4_{i,j} = \\frac{<i>}{\\||q_{i}\\|\\| \\|kj\\|}.    (i,j) \u2208 1,..,\\frac{H}{S}* \\frac{W}{S} \\text{ }\n(8)\nwhere $q_{i}$ and $k_{j}$ denote any patches in Q and K, while H, W, and S have the same meaning as Eq. (6). Subsequently, the diagonal structure features are transferred from the WT branch to the SR branch. To identify the feature at the most relevant position in K for any Q and minimize less relevant feature transfer, we obtain the index h and the atten-tion graph S based on $\u03c4_{(i,j)}$. Specifically, h helps locate the most relevant high-frequency feature T in V, and $h_i$ represents the index of the most relevant patches for the i-th patch in both $I_{GT'}$ and $I_{LR}$. Meanwhile, S records the correlation between any patches from Q and their corresponding most relevant patches in K. We then fuse features from both branches, combining the joint Q and T and sending them through the convolutional layer C to generate the feature $f_{QT}$. This feature, $f_{QT}$, is further combined with the attention graph S to produce the feature $f_{QTS}$. By incorporating S, we enable the precise utilization of migrated high-frequency texture T, giving greater weight to highly relevant information and suppressing less relevant details. Finally, $f_{QTS}$ guides the SR branch task by uniting with the original SR branch information Q. The working process of the Transformer is described in Eq. (9):\n$f_{m}^{sr} = Q \\bigoplus C{Concat(Q,T)} S$                                              (9)\nwhere Concat(.) signifies the vector concatenation operation, \u2297 denotes element-wise multiplication, and $f_{m}^{sr}$, $f_{m}^{sr}$, and S maintain the same meanings as in Eq. (2)."}, {"title": "E. Loss Function", "content": "To ensure the retention of valuable wavelet information from cross-scale wavelet transform images, CWT-Net optimizes the network by minimizing a linear combination of losses from both branches, expressed in Eq. (10):\n$L_{CWT} = \u03bb_{1}L_{SR} + \u03bb_{2}L_{WT}$                                             (10)\nwhere $L_{CWT}$ represents the overall loss of CWT-Net, while $L_{SR}$ and $L_{WT}$ represent the losses of the SR branch and WT branch, respectively. $\u03bb_{1}$ and $\u03bb_{2}$ are the respective loss weights of the two branches. Given a training set $I_{GT}, I_{GT', ILR}$ with N samples and prediction results $I_{HR}, I_{WT}, L_{SR}$ and $L_{WT}$ can be further defined as Eq. (11) and Eq. (12):\n$L_{SR} = L_{L1} (I_{HR}, I_{GT}) + \u03bb_{3}L_{SSIM} (I_{HR}, I_{GT})$                                (11)\n$L_{WT} = L_{L1} (I_{wT}, DWT (I_{GT'})) + \u03bb_{4}L_{SSIM} (I_{wT}, DWT (I_{GT'}))$           (12)\nwhere $L_{L1} (.,.)$ signifies the $L_{1}$ loss, $L_{SSIM}(.,.)$ represents the SSIM loss [45], $I_{WT}$ denotes the output of the backbone network in the WT branch, DWT(.) signifies the high-frequency features obtained after the Discrete Wavelet Trans-form (DWT) module, and $\u03bb_{3}$ and $\u03bb_{4}$ denote the weights of the SSIM loss in the respective branch losses. In our optimization strategy, we primarily employ a combi-nation of both L1 loss and SSIM loss rather than relying solely on one of them. The use of MSE-based optimization in some SR networks [4]-[6], especially with large upsampling multi-pliers, often results in a notable loss of high-frequency details in $I_{HR}$ and excessively smooth textures, despite yielding better performance in terms of PSNR and SSIM. Studies [6] have shown that PSNR/SSIM metrics do not consistently correlate with subjective human visual evaluation. $L_{1}$ loss maintains brightness and color consistency and can tolerate larger errors compared to $L_{2}$ loss [50]. Regarding texture preservation, $L_{1}$ loss outperforms MSE loss and $L_{2}$ loss, reproducing more high-frequency details. The $L_{1}$ loss is described as Eq. (13):\n$L_{L1} = \\frac{1}{N2 \u00d7 H \u00d7 W} \\sum_{I=1}^{N} \\sum_{j=1}^{rW} (\\| \u03b8_{G} (I_{LR}) \u2212I_{GT}\\|)$                              (13)\nwhere N, H, and W represent the number of samples, pixel height, and pixel width of the samples, respectively, and $\u03b8_{G}(.)$ denotes the computational process of the network. Structural Similarity (SSIM) measures image similarity based on luminance, contrast, and structure. Human vision is less sensitive to the absolute luminance/color of pixels, and SSIM loss better quantifies human visual perception by considering the location of edges and textures. The SSIM loss is defined as Eq. (14):\n$L_{SSIM} = 1 \u2212 SSIM (\u03b8_{G} (I_{LR}), I_{GT})$                                       (14)\nwhere SSIM(., .) represents the structural similarity measure between the two samples. In our experiments, we set $(\u03bb_1, \u03bb_2) = (0.3, 0.7)$. We assigned a higher weight to the loss in the WT branch compared to the SR branch, and this choice was driven by several considerations. First, the SR branch benefits from complete input features, while the WT branch only processes high-frequency features from the wavelet transform. Conse-quently, the SR branch exhibits higher robustness. Second, in SR tasks with a large upsampling factor, the WT branch dynamically increases the depth of the backbone network, potentially introducing more errors. Lastly, although the SR branch addresses our primary task, the optimization outcomes of the WT branch significantly impact the SR branch's results. We set $(\u03bb_3, \u03bb_4) = (0.2, 0.2)$. The combination of $L_{L1}$ as the primary component of the loss function ensures better convergence and maintains visual quality for the SR task. On the other hand, $L_{SSIM}$ enhances pixel-level accuracy, balances the dynamic range of predictions, and stabilizes the model optimization by amplifying differences between prediction results and training samples."}, {"title": "IV. EXPERIMENT", "content": "WSI refers to the process of scanning an entire microscope slide and creating a single high-resolution digital file, primarily applied in the field of pathology cell images. Typically, the pixel sizes between two adjacent sampling levels in WSI are spaced at twice the distance to enable rapid and accurate downsampling.\nThe CAMELYON dataset [51] comprises 400 WSI images of breast cancer anterior lymph node sections. Li Y et al. [39] provided pre-sampled coordinate pairs and five levels of sampling resolution for cancer and non-cancer regions within the CAMELYON dataset. To introduce realistic cross-scale information to CWT-Net, we selected a total of 1200 coordinates from both regions and sampled them at 40\u00d7 magnification (0.243 microns per pixel), 20\u00d7 magnification (0.486 microns per pixel), and 10\u00d7 magnification (0.972 microns per pixel) across patch sizes of 1024\u00d71024, 512\u00d7512, and 256 \u00d7 256, respectively. These images were randomly organized and split into a training set and a test set at a 5:1 ratio. We named this dataset MLCamSR to facilitate the super-resolution task for multi-level histopathology images. While constructing MLCamSR, we carefully assessed the quality of WSI images, removed samples with significant blank areas, and ensured that the RGB mean values of all samples fell within the appropriate range (RGB mean values were set to [0.7204,0.4298,0.6379] for the training set). Sun K et al. [52] contributed a WSI image dataset en-compassing 10 human body systems. Pathology slides were obtained from Xiangya Hospital of Central South University. Technicians randomly selected original WSI files represent-ing typical diseases from each human system, which were then medically reviewed by two pathologists. Slides were scanned using a digital pathology scanner to obtain samples at 40x magnification. Samples at 20\u00d7 magnification, 10\u00d7 magnification, and 5\u00d7 magnification were derived from 40\u00d7 magnification images by applying the bicubic algorithm three times consecutively. The patch sizes ranged from 1024 \u00d7 1024 to 128 x 128. We conducted experiments using a subset of [52], which consists of 1200 samples and follows the same structure as MLCamSR, and we named it FTMS.\nThe PCam dataset [53], a subset of [51], encompasses 327,680 patches sampled from 10\u00d7 magnification images, each measuring 96 \u00d7 96."}, {"title": "B. Experimental details", "content": "We utilize CWT-Net as our baseline model, consisting of 12 CWTBs. In this model, all convolutional layers employ a 3 x 3 filter size with 64 channels, and the WR module reduces channels by a factor of 8. During training, we partition training samples into 64 \u00d7 64 patches for the SR branch and apply random 90-degree rotations to augment the data. Each $B_{RB}$ consists of 2 RBs and the WR module contains 4 WRBs, when the upsampling factor is 2. For each subsequent increase in the upsampling factor by a factor of 1, 1 RB is added to each $B_{RB}^{wt}$, and 2 WRBs are added to the WR module.\nOur network is implemented on the torch 1.12.1 plat-form, with one NVIDIA GeForce RTX 3090 (24GB) and one NVIDIA TITAN Xp (12GB) for all experiments. In the quantitative evaluation (Section IV-C), for the 2\u00d7 upsampling task, we use the original HR image as input to the WT branch. For other tasks, the WT branch input is obtained from images one sampling level higher than the SR branch input. If the WR module is used solely during testing, CWT-Net initializes WR module weights from a standard normal distribution. All abla-tion experiments (Section IV-E) are based on the MLCamSR 2x upsampling task, and CWT-Net is trained for 1000 batches (100 epochs) in all experiments, unless specified otherwise. In the classification task evaluation (Section IV-F), we segment each MLCamSR image into 128 \u00d7 128 images to match the classification network's feature size. These segmented images retain the same category labels as the original images."}, {"title": "V. CONCLUSION", "content": "In this research, we introduce CWT-Net, a SR model that leverages cross-scale wavelet features and a Transformer structure. CWT-Net is applied to the task of upsampling histopathology images from various human systems, with a primary focus on enhancing the reconstruction of struc-tural information. We explore the advantages of the RefSR paradigm. CWT-Net comprises three core components: the SR branch for upsampling, the WT branch for generating wavelet transform features from cross-scale images, and the Transformer module for transmitting structural information to the SR branch. The WR module is specially designed to support the RefSR paradigm within the SISR task. To enable training with undegraded cross-scale information, we create the MLCamSR benchmark dataset. CWT-Net exhibits state-of-the-art performance across various datasets and upsampling scales, enhancing image classification networks in diagnostics. A series of ablation studies confirm the robustness and effec-tiveness of CWT-Net. In the future, we anticipate using CWT-Net and its substructures to provide compatible pre-training measures or priors for other histopathology image-related tasks, opening new research and application possibilities in the field."}]}