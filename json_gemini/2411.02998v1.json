{"title": "Accelerating Task Generalisation with Multi-Level Hierarchical Options", "authors": ["Thomas P Cannon", "\u00d6zg\u00fcr \u015eim\u015fek"], "abstract": "Creating reinforcement learning agents that generalise effectively to new tasks is\na key challenge in AI research. This paper introduces Fracture Cluster Options\n(FraCOs), a multi-level hierarchical reinforcement learning method that achieves\nstate-of-the-art performance on difficult generalisation tasks. FraCOs identifies\npatterns in agent behaviour and forms options based on the expected future useful-\nness of those patterns, enabling rapid adaptation to new tasks. In tabular settings,\nFraCOs demonstrates effective transfer and improves performance as it grows in\nhierarchical depth. We evaluate FraCOs against state-of-the-art deep reinforcement\nlearning algorithms in several complex procedurally generated environments. Our\nresults show that FraCOs achieves higher in-distribution and out-of-distribution\nperformance than competitors.", "sections": [{"title": "1 Introduction", "content": "A key goal of AI research is to develop agents that can leverage structured prior knowledge, either\nprovided or learned, to perform competently in unfamiliar domains (Pateria et al., 2021). This is a\ncommon feature in animals; for example, many newborn mammals, such as foals, can walk shortly\nafter birth due to innate motor patterns, while human infants display instinctive stepping motions\nwhen supported (Adolph and Robinson, 2013; Dominici et al., 2011). These innate behaviors, shaped\nby evolution, act as priors that guide goal-directed actions and enable rapid adaptation.\nIn parallel, humans are believed to organise behaviors into a hierarchy of temporally extended actions,\nwhich helps break complex tasks into simpler, manageable steps (Rosenbloom and Newell, 1986;\nLaird et al., 1987). For instance, human decision-making often involves planning with high-level\nactions like \"pick up glass\u201d or \u201cdrive to college,\" each of which comprises subtasks such as \u201creach\nfor glass\" or \"pull door handle.\u201d These eventually decompose into basic motor movements.\nThis hierarchical fragmentation likely arises from sub-experiences of past tasks (Brunskill and Li,\n2014). Notably, parts of this hierarchy are shared between tasks; for instance, both \u201cpick up glass\"\nand \"pull door handle\" involve similar gripping movements. Such shared temporal actions may\nfacilitate rapid learning of new tasks beyond those previously experienced. Replicating this hierarchy\nin algorithms could allow artificial agents to also adapt quickly (Heess et al., 2016).\nDespite advances in hierarchical methods, generalising behaviors across diverse tasks remains a\nsignificant challenge for artificial agents (Cobbe et al., 2019). Many approaches struggle with\neffectively transferring skills to new environments, limiting their ability to adapt to real-world\nscenarios (Pateria et al., 2021)."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Reinforcement Learning", "content": "Standard Reinforcement Learning (RL) focuses on how agents can \"take actions in different states\nof an environment to maximize cumulative future reward\", where the reward provides task-specific\nfeedback (Sutton and Barto, 2018). In any environment, an agent exists in a state s and can perform\nan action a, both of which may be discrete, continuous, or multidimensional. Most RL problems are\nframed as a Markov Decision Process (MDP). An MDP is defined as a tuple $(S, A, P, R, \\gamma)$, where\nS is the set of possible states, A is the set of possible actions, P is the transition probability function\nwith $P(s, a, s')$ indicating the probability of transitioning from state s to s' after action a, R is the\nreward function where $R(s, a, s')$ gives the reward for transitioning from s to s' via action a, and\n$\\gamma\\in [0, 1]$ is the discount factor. An MDP assumes the Markov property, where the future state and\nreward depend only on the current state and action.\nAt each time step $t \\geq 0$, the agent makes a decision based on its current state $s_t$ using a policy,\ndenoted as $\\pi(s_t)$. The policy maps states to probabilities over actions, guiding the agent's behaviour.\nThe actions taken produce observed experience data of the form $(s_t, a_t, r_{t+1}, s_{t+1})$. When the agent\ninteracts with the environment until it reaches a termination state, the full sequence of observations is\ncalled a trajectory T. The objective of reinforcement learning is to learn a policy that maximises the\ncumulative discounted returns, defined as $G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1}$.\nIn this paper, we define an environment as the external system with which the agent interacts,\ncharacterized by $(S, A, P, R)$. In our work, a task is defined as an MDP with a unique state space S\nor reward function R."}, {"title": "2.2 Hierarchical Reinforcement Learning", "content": "Hierarchical Reinforcement Learning (HRL) extends standard RL by organising decision-making into\nmultiple levels of abstraction. A key paradigm in HRL is the options framework, which encapsulates\nextended sequences of actions into options (Sutton and Barto, 2018). An option consists of an\ninitiation set I, which defines the states where it can be selected, an intra-option policy $\\pi_{intra}$, which\ngoverns the actions while the option is active, and a termination condition $\\beta : S \\rightarrow [0, 1]$, which\ndefines when the option ends. The intra-option policy executes actions until the termination condition\n$\\beta(s)$ is satisfied.\nOptions operate within a Semi-Markov Decision Process (SMDP). An SMDP is an extension of an\nMDP where actions can have variable durations. The agent chooses between options and primitive\nactions at each decision point, with a policy over options, $\\pi_{opt}$, deciding which to execute based\non the current state. This hierarchical structuring can improve exploration and learning efficiency\nby enabling agents to plan over extended time horizons and break complex tasks into manageable\nsub-tasks. This decomposition reduces the decision space, facilitates structured exploration, and\nsimplifies credit assignment, particularly in environments with sparse rewards (Sutton et al., 1999;\nDayan and Hinton, 1992)."}, {"title": "2.3 Generalisation", "content": "Generalisation in Reinforcement Learning (RL) encompasses a broad class of problems (Kirk et al.,\n2021). These problems can be categorised based on the relationship between training and testing\ndistributions, either falling within Independent and Identically Distributed (IID) scenarios or extending"}, {"title": "3 Related Work", "content": "Policy transfer and single-level option transfer methods, aim to broaden an agent's task-handling\ncapabilities. Examples of policy transfer include works by Finn et al. (2017), Grant et al. (2018),\nFrans et al. (2017), Cobbe et al. (2021), and Mazoure et al. (2022). Option transfer methods, on\nthe other hand, focus on learning a set of reusable options to enhance rewards in new situations\n(Konidaris and Barto, 2007; Barreto et al., 2019; Tessler et al., 2017; Mann and Choe, 2013). While\na few methods in the literature explore multi-level hierarchies, they do not focus on option transfer\nas a mechanism for accelerating task adaptation and generalisation (Riemer et al., 2018; Evans and\n\u015eim\u015fek, 2023; Levy et al., 2017; Fox et al., 2017).\nHRL has typically focused on addressing broader challenges such as long-term credit assignment and\nstructured exploration (Dayan and Hinton, 1992; Parr and Russell, 1997; McGovern and Sutton, 1998;\nSutton et al., 1999). Two foundational paradigms within HRL are: 1) sub-goal-based approaches,\nwhich typically decompose tasks into smaller, state-based intermediate objectives, and 2) the options\nframework, which formalises temporally extended actions as options (Sutton et al., 1999). In both\nparadigms the ability to learn transferable abstractions at more than two levels of hierarchy is still an\nopen research question (Pateria et al., 2021).\nRecent work has proposed methods for forming and managing multi-level hierarchies. Levy et al.\n(2017) and Evans and \u015eim\u015fek (2023) introduce sub-goal-conditioned approaches for multi-level\nabstraction. However, due to their reliance on state-based-sub-goals, these methods face difficulties\nin sub-goal selection in complex state spaces such as pixel-based representations. Moreover, all\nstate-based-sub-goal methods struggle to transfer sub-goals to different state spaces. Additionally,\nthey do not account for the variability in action sequences required to transition between sub-goals;\nfor example, \"booking a holiday\" could involve \u201ccalling a travel agent\" or \"using the internet,\" each\ndemanding different skills. In contrast, FraCOs avoids creating state-based-sub-goals, providing a\nmore flexible framework for transfer across state spaces.\nOur work is more closely related to Hierarchical Option Critic (HOC) by Riemer et al. (2018)\nand the Discovery of Deep Options (DDO) by Fox et al. (2017), both of which use the options\nframework. DDO employs an expectation gradient method to construct a hierarchy top-down from\nexpert demonstrations. However, DDO does not optimize for generality and it remains unclear how\nthe discovered options perform in unseen tasks. Moreover, the reliance on demonstrations limits the\ndevelopment of more complex abstractions than those demonstrated. In comparison, FraCOs builds\nbottom-up, forming increasingly complex abstractions. FraCOs also selects options based on their\nexpected usefulness in future tasks, directly addressing generalisation challenges.\nHOC generalises the Option-Critic (OC) framework introduced by Bacon et al. (2017) to multiple\nhierarchical levels. HOC learns all options simultaneously during training. However, both OC\nand HOC suffer from option collapse, where either all options converge to the same behaviour or\none option is consistently chosen (Harutyunyan et al., 2019). Moreover, OC methods introduce\nadditional complexity to the learning algorithm, which has been shown to slow learning compared to\nnon-hierarchical approaches like PPO (Schulman et al., 2017; Zhang and Whiteson, 2019). Option\nselection within the FraCOs process naturally prevents option collapse, and has been shown to\nincrease the rate of learning over baselines (see Section 5.3)."}, {"title": "4 Fracture Cluster Options", "content": "We hypothesise that identifying reoccurring patterns in an agent's behaviour across successful tasks\nwill improve performance on future, unseen tasks. Our method consists of three key stages: 1)\nIdentifying the underlying reoccurring patterns in an agent's behaviour across multiple tasks, 2)\nselecting the most useful patterns\u2014those likely to appear in successful trajectories of all possible\ntasks, and 3) defining these identified patterns as options for the agent's future use."}, {"title": "4.1 Identifying Patterns in Agent Behaviour", "content": "We seek to identify and cluster reoccurring patterns of states and actions in agent behavior. To achieve\nthis, we introduce the concept of fractures. A fracture is defined as a state $s_t$ paired with a sequence\nof subsequent actions. The length of this action sequence is determined by a parameter known as\nthe chain length b, which specifies the number of actions following the state $s_t$. More formally, a\nfracture is represented as:\n$\\phi = (s_t, a_t, a_{t+1},..., a_{t+b-1})$\nwhere $a_t, a_{t+1},..., a_{t+b-1}$ represent the subsequent actions from $s_t$. The parameter b controls the\ntemporal extent of the fracture.\nWe can derive fractures from the trajectories of tasks which an agent has experienced. Consider a\ntrajectory of length n. The set of fractures F derived from this trajectory is defined as:\n$F = \\{(s_t, a_t, a_{t+1},..., a_{t+b-1}) \\mid 0 \\leq t \\leq n - b\\}$.\nFor a set of trajectories T, we derive fractures from each individual trajectory. The complete set\nof fractures from all trajectories is denoted by $\\Phi = \\{F_1, F_2, ..., F_{|T|}\\}$. Individual trajectories are\ndenoted by $\\tau$, with $\\tau \\in T$.\nWe investigate whether fractures capture under-\nlying structure by first identifying them in the\nFour Rooms environment. Four Rooms is a clas-\nsic grid-based reinforcement learning environ-\nment, consisting of four connected rooms sep-\narated by walls with narrow doorways. The\nagent's objective is to navigate through the\nrooms to reach a specified goal, receiving a re-\nward for reaching this goal. Four Rooms is de-\npicted in the top left corner of Figure 1, see\nAppendix A.6 for more detail. In all of our grid-\nworld implementations, the agent can observe\nonly a 7x7 area centered on itself and a scalar\nindicating the direction of the reward. This is\nsimilar to MiniGrid, except that our observations\nare ego-centric (Chevalier-Boisvert et al., 2023).\nWe train a tabular Q-learning agent to solve mul-"}, {"title": "4.2 Selecting Useful Fracture Clusters", "content": "In Section 4.1 fracture clusters are formed based on behaviour similarity; however, the number of\npotential clusters can be extensive, with some being highly task-specific. Selecting all clusters as\noptions may burden the agent with unnecessary choices. Therefore, it is essential to identify the most\nuseful clusters\u2014those likely to appear in future tasks.\nFirst, consider the hypothetical scenario in which we can observe all possible trajectories across all\npossible tasks. In this ideal setting, the set of all successful fractures, denoted as $\\Phi_s$, would be derived\nfrom fractures within the successful trajectories, where each trajectory is represented as $T_\\tau \\in T_s$.\nHere, $T_s$ refers to the collection of all trajectories deemed successful. A trajectory is considered\nsuccessful if its cumulative return exceeds a predefined threshold, similar to the criterion proposed by\nChollet (2019), see Table 12 for all minimum returns. The tasks corresponding to these successful\ntrajectories are denoted as $x_s \\in X_s$, where $X_s$ represents the set of all tasks with successful outcomes.\nTo sensibly select fracture clusters, we must evaluate their potential for reuse in future tasks. We do\nthis by defining the usefulness U of a fracture cluster $\\phi_c$ based on its likelihood of contributing to\nsuccess across tasks. Specifically, usefulness is determined by three key factors:\n1. Appearance Probability ($P[\\Phi_c \\in T_s \\mid x_s]$): This measures the likelihood that any fracture\n$\\Phi \\in \\phi_c$ appears in the trajectory $T_s$ of any given successful task $x_s$. Higher probability\nindicates that this $\\phi_c$ frequently contributes to success across tasks.\n2. Relative Frequency ($P[\\Phi_c \\mid \\Phi_s]$): This term represents the proportion of times that any\nfracture $\\Phi \\in \\Phi_c$ appears among all successful fractures $\\Phi_s$. A higher relative frequency\nimplies its importance in the agent's overall success.\n3. Entropy of Usage ($H(\\phi_c \\mid X_s)$): This captures the diversity of a fracture cluster's usage\nacross different tasks in $X_s$. A higher entropy indicates that a $\\phi_c$ is useful across various\ntasks, enhancing its generalisation potential.\nThe usefulness of a fracture cluster $\\phi_c$ is defined as the normalised sum of these factors:\n$U(\\phi_c) = \\frac{1}{3} (P[\\Phi_c \\in T_s \\mid X_s] + P[\\Phi_c \\mid \\Phi_s] + H(\\phi_c \\mid X_s))$.\nIn the ideal scenario where we could observe all possible tasks and trajectories, we would directly\ncalculate the usefulness of each fracture cluster $U(\\phi_c)$. This would allow us to exactly compute\nthe appearance probability, relative frequency, and entropy for each fracture cluster $\\phi_c$, yielding a"}, {"title": "4.3 Using Fracture Clusters", "content": "After selecting the most useful fracture clusters, we need to transform each cluster into an option,\ncalled a Fracture Cluster Option (FraCO). Each FraCO is characterized by an initiation set $I_z$, a\ntermination condition $\\beta_z$, and a policy $\\pi_z$. We denote a single FraCO as z and the set of all FraCOs\nas Z. Each FraCO is associated with the fracture cluster $\\phi_c$ that forms it. We use these clusters to\npredict initiation states.\nInitiation Set. Suppose the agent is in a state s, and has access to a set of actions A. For each state s,\nwe consider possible sequences of actions of chain length b. Each such sequence is represented as:\n$a = (a_1, a_2, ..., a_b) \\mid a_i \\in A$,\nWe can now define the set of all possible fractures starting from state s as,\n$F_s = \\{(s, a) \\mid a \\in A^b\\}$,\nwhere $A^b$ represents all sequences of length b drawn from the action set A.\nFor each fracture $\\phi = (s, a)$ in $F_s$, we estimate the likelihood that it belongs to a FraCO z. The set\nof fractures assigned to cluster z in state s can be defined as:\n$G_{z,s} = \\{\\phi \\in F_s \\mid P(\\phi \\in z) > \\theta\\}$,\nwhere $P(\\phi \\in z)$ denotes the estimated probability that fracture $\\phi$ belongs to cluster z, and $\\theta$ is a\nthreshold hyperparameter that determines cluster membership. The method for estimating $P(\\phi \\in z)$\ncan vary depending on the implementation. In our tabular implementation, we directly use the\nprediction function provided by HDBSCAN. In our deep implementation, a neural network predicts\nthis probability.\nA FraCO z can be initiated in state s if $G_{z,s}$ is not empty. Such that the initiation set is defined as,\n$I_z = \\{s \\in S \\mid G_{z,s} \\neq \\emptyset\\}$.\nPolicy Execution. When FraCO z is executed\nin state s, it follows the policy $\\pi_z$, as described\nin Algorithm 1. The policy selects the fracture\n$z = (s, a)$ from $G_{z,s}$ with the highest probabil-\nity $P(z \\in z)$, and then executes the sequence\nof actions $a = (a_1, a_2,...,a_b)$. If one of the\nselected actions is another FraCO z', the agent\nmust compute a new $G_{z',s}$ and recursively call\nthe policy until the option terminates.\nTermination Condition. The FraCO z termi-\nnates under two conditions: either when all ac-\ntions in the selected fracture $z$ have been ex-\necuted, or when no matching fracture can be\nfound in the current state (i.e., $G_{z,s} = \\emptyset$).\nThe termination condition $\\beta(s)$ is defined as:\n$\\beta(s) = \\begin{cases}\n1 & \\text{if all actions in } z \\text{ have been executed} \\\\\n1 & \\text{if } G_{z,s} = \\emptyset \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nLearning with FraCOs. In our approach, FraCOs are fixed once identified. The agent learns\nto choose between primitive actions and available FraCOs using standard reinforcement learning\nalgorithms (e.g., Q-learning for tabular settings, PPO for deep learning implementations)."}, {"title": "5 Experimental Results", "content": "We evaluate FraCOs in three different experiments. The first experiment focuses on OOD reward\ngeneralisation tasks using a tabular FraCOs agent in the Four Rooms, Nine Rooms, and Romesh\nMaze grid-world environments (see Appendix A.6). The second experiment examines OOD learning\nin state generalisation tasks within a novel environment called MetaGrid (see Appendix A.6). The\nfinal experiment evaluates a deep FraCOs agent, implemented with a three-level hierarchy using PPO,\nin nine Procgen environments (Cobbe et al., 2020). We compare its performance with CleanRL's\nProcgen PPO implementation (Huang et al., 2022) and Option Critic with PPO (OC-PPO) (Klissarov\net al., 2017). We use the same hyperparameters from CleanRL's PPO implementation for all three\nalgorithms. In the grid-world environments, the agent receives a reward of 1 for reaching the goal\nand a penalty of -0.001 per time step. Episodes have a maximum length of 500 steps, and a fracture\nchain length b = 2. For Procgen environments, reward functions remain unchanged, and b 3 (see\nAppendix A.8 for chain-length selection details)."}, {"title": "5.1 Experiment 1: Tabular Reward Generalisation", "content": "In this experiment, FraCOs are learned in a fixed state space S while we vary the reward function R.\nThe agent is allowed to discover FraCOs in 50 tasks, such that each task corresponds to a different\nreward location. We reserve 10 unique tasks for testing. Separate agents are trained to convergence-\ndefined as achieving consistent performance across episodes. The top 20 FraCOs are extracted from\nthe final trajectories of the agents, as described in Section 4, and incorporated into the action space.\nThis extraction process is repeated four times, corresponding to each level of the hierarchy.\nFour agents are created, each with an additional hierarchical level of FraCOs. After resetting their\npolicies over options, the agents are trained on the test tasks, and evaluation episodes are conducted\nperiodically. As shown in Figure 4, results from the Four Rooms, Nine Rooms, and Romesh Maze\nenvironments indicate that learning is progressively accelerated on these unseen tasks as the hierarchy\ndepth increases."}, {"title": "5.2 Experiment 2: Tabular State Generalisation", "content": "In this experiment, we introduce a novel environment called MetaGrid, which is designed to test state\nand reward generalisation. MetaGrid is a navigational grid world constructed from structured 7 \u00d7 7\nbuilding blocks that can be combined randomly to create novel state spaces while preserving certain\nareas of local structure. The agent is provided with a 7 \u00d7 7 window of observation, consistent with\nour other grid-worlds. For more detailed information on MetaGrid, see Appendix A.6.\nAt each hierarchy level, 20 FraCOs are learned from 100 randomly generated 14 \u00d7 14 MetaGrid tasks.\nA task in this experiment corresponds to a differnt space space S and reward location R. For each\nhierarchy level, a separate agent is created, and their policies over options are reset. These agents are\nthen evaluated in previously unseen 14 \u00d7 14 domains and larger 21 \u00d7 21 domains. Periodic evaluation\nepisodes are conducted during training to track performance, results are shown in Figure 5."}, {"title": "5.3 Experiment 3: Deep State and Reward Generalisation in Complex Environments", "content": "In this experiment, we test FraCOs in OOD tasks from the Procgen benchmark, focusing on unseen\nstates spaces S and reward functions R. We compare with two methods: Option Critic with\nPPO (OC-PPO) (Klissarov et al., 2017) and a baseline PPO (Schulman et al., 2017), across nine\nProcgen environments (Cobbe et al., 2020). Procgen is a suite of procedurally generated arcade-style\nenvironments designed to assess generalisation across diverse tasks; see Appendix A.6 for full details.\nFraCOs modifications. To handle the challenges of applying traditional clustering to high-\ndimensional pixel data, we simplify the approach by grouping fractures with the same action se-\nquences, regardless of state differences. Additionally, a neural network is used to estimate initiation\nstates and policies, which reduces the computational burden of performing a discrete search over the\ncomplex 64 x 64 \u00d7 3 state space and managing 15 possible actions during millions of training steps.\nThese modifications do not change the theory of FraCOs, just the implementation. Full details of\nthese modifications are provided in Appendix A.10, with further information on the experiments,\nbaselines, and hyperparameters in Appendix A.11.\nFraCOs and OC-PPO both learn options during a 20-million time-step warm-up phase, with tasks\ndrawn from the first 100 levels of each Procgen environment. FraCOs learns two sets of 25 options,\ncorresponding to different hierarchy levels, while OC-PPO learns a total of 25 options. After the\nwarm-up, the policy over options is reset, and training continues for an additional 5 million time steps.\nDuring this phase, we periodically conduct evaluation episodes on both IID and OOD tasks, with\nOOD tasks drawn from Procgen levels beyond 100.\nWe test two versions of FraCOs: one with a complete reinitialised policy over options after the warm-\nup phase, and another that transfers a Shared State Representation (SSR), referred to as FraCOs-SSR.\nIn SSR, shared convolutional layers encode the state, followed by distinct linear layers for the critic,\npolicy over options, and option policies. These convolutional layers are not reset after warm-up. Since\nOC-PPO shares its state representation, comparing it with FraCOs-SSR offers a fairer evaluation.\nHowever, we find that FraCOs without SSR still outperforms both baselines. See Appendix A.11 for\nFraCOs-SSR implementation details."}, {"title": "6 Discussion and Limitations", "content": "In this study, we introduce Fracture Cluster Options (FraCOs) as a novel approach to multi-level\nhierarchical reinforcement learning. In the tabular settings, FraCOs demonstrates accelerated learning\nin unseen tasks. Notably, as the hierarchical depth of FraCOs increases, the agent's performance also\nimproves.\nOur deep RL experiments show that FraCOs outperforms both OC-PPO and PPO in out-of-distribution\n(OOD) tasks. Although PPO-25 slightly outperforms FraCOs in-distribution, it receives five times\nmore experience than FraCOs' policy over options. FraCOs outperforms PPO-5 in IID tasks and\nextends this by leveraging hierarchical options across tasks. This adaptability allows FraCOs to far\nsurpass PPO in OOD scenarios. Furthermore FraCOs consistently outperforms Option Critic, even\nwhen paired with PPO, highlighting its potential as a robust framework for hierarchical reinforcement\nlearning.\nDespite its contributions, this work faces some limitations. As environments increase in complexity,\nclustering methods struggle to maintain accurate predictions. While we introduce simpler clustering\ntechniques and neural networks to address this issue, future research could explore alternatives\nfor identifying and predicting fracture clusters. Additionally, our experiments focus exclusively\non discrete action spaces, and extending this methodology to continuous action spaces remains a\nchallenge for future investigation. Nonetheless, FraCOs provides a solid basis for continued research\ninto generalisation via multi-level hierarchical reinforcement learning."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Derivation of Usefulness Metric", "content": "In this appendix, we derive the usefulness (U) metric for fracture clusters. This metric is used to\nidentify which fracture clusters have the greatest potential for reuse across different tasks. Usefulness\nis a function of the following three factors:\n1. The probability that a fracture cluster appears in any given successful task:\n$P[\\phi_c \\in T_s \\mid x_s]$\n2. The probability that a fracture cluster is selected from the set of successful fracture clusters:\n$P[\\phi_c \\mid \\Phi_s]$\n3. The entropy of the fracture cluster's usage across all successful tasks:\n$H(\\phi_c \\mid T_s)$\nUsefulness (U) is then defined as the normalized sum of these three factors:\n$U = \\frac{1}{3} (P[\\Phi_c \\in T_s \\mid x_s] + P[\\phi_c \\mid \\Phi_s] + H(\\Phi_c \\mid T_s))$\nThe objective is to select fracture clusters that maximize the usefulness, i.e.,\n$arg \\underset{\\phi_c}{max} U (\\phi_c)$"}, {"title": "A.2 Deriving $P[\\phi_c \\in T_s \\mid x_s]$ Using Bayesian Inference", "content": "We want to model the probability that a fracture cluster $\\phi_c$ appears in any given successful task,\n$P[\\phi_c \\in T_s \\mid x_s]$. For each successful task $x_s$ from the set of successful tasks $X_s$, the presence of\nfracture cluster $\\phi_c$ in the corresponding trajectory $T_s$ is represented by a binary random variable $w_s$,\nwhere:\n$\\omega_s = \\begin{cases}\n1 & \\text{if } \\phi_c \\in T_s \\text{ (fracture cluster appears in the trajectory)}, \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nThe variable $w_s$ is modeled as a Bernoulli random variable:\n$w_s \\sim Bernoulli(p)$\nwhere p is the probability that fracture cluster $\\phi_c$ appears in the trajectory $T_s$ of task $x_s$.\nSince we are uncertain about the true value of p, we place a Beta distribution prior on p:\n$p \\sim Beta(\\alpha, \\beta)$\nwhere $\\alpha$ and $\\beta$ are hyperparameters representing our prior belief about the likelihood of $\\phi_c$ appearing\nin a trajectory.\nGiven a total of N tasks, the likelihood for each observation $w_s$ is:\n$P(w_n \\mid p) = p^{w_n} (1 - p)^{1-w_n}$\nwhere $w_n$ is 1 if $\\phi_c$ appears in trajectory $T_s$ for task $x_n$, and 0 otherwise."}, {"title": "A.3 Deriving $P[\\phi_c \\mid \\Phi_s]$", "content": "The second component of the usefulness metric, $P[\\phi_c \\mid \\Phi_s]$, is the probability that fracture cluster $\\phi_c$\nis selected from the set of successful fracture clusters. This can be computed as the relative frequency\nof $\\phi_c$ in the set $\\Phi_s$ of successful clusters:\n$P[\\phi_c \\mid \\Phi_s] = \\frac{count(\\phi_c, \\Phi_s)}{|\\Phi_s|}$\nwhere $count(\\phi_c, \\Phi_s)$ is the number of times $\\phi_c$ appears in the set of successful clusters, and $|\\Phi_s|$ is\nthe total number of successful clusters."}, {"title": "A.4 Deriving $H(\\phi_c \\mid T_s)$", "content": "The entropy term $H(\\phi_c \\mid T_s)$ measures the unpredictability or diversity of the usage of fracture\ncluster $\\phi_c$ across successful tasks. Entropy is defined as:\n$H(\\phi_c \\mid T_s) = - \\sum_{T_s \\in T_s} P[\\Phi_c \\mid T_s] \\cdot log_{N_{\\Phi_c}} (P[\\Phi_c \\mid T_s])$\nwhere $p[\\phi_c \\mid T_s]$ is the proportion of times that fracture cluster $\\phi_c$ appears in trajectory $T_s$:\n$P[\\Phi_c \\mid T_s] = \\frac{count(\\phi_c, T_s)}{|T_s|}$\nHere, $T_s$ represents the length of the trajectory $T_s$, and $N_{\\Phi_c}$ is the total number of fracture clusters\nconsidered. The choice of logarithm base, $N_{\\Phi_c}$, reflects the fact that we normalize entropy relative to\nthe number of fracture clusters."}, {"title": "A.5 Expected Usefulness", "content": "Having derived the empirical estimations of the three components of usefulness we can now combine\nthese elements to calculate the expected usefulness of each fracture cluster. The expected usefulness\nincorporates the posterior distribution from Bayesian inference for $P[\\Phi_c \\in T_s \\mid x_s]$, as well as the\nempirical counts for the other components.\nThus, the expected usefulness for each fracture cluster is calculated as:\n$E[U(\\phi_c)] = \\frac{1}{3} (\\frac{\\sum_{n=1}^{N} w_n + \\alpha}{N + \\alpha + \\beta} + \\frac{count(\\phi_c, \\Phi_s)}{|\\Phi|} - \\sum_{T_s \\in T_s} \\frac{count(\\phi_c, T_s)}{T_s} \\cdot log_{N_{\\Phi_c}} (\\frac{count(\\phi_c, T_s)}{T_s}))$\nwhere $\\alpha$ and $\\beta$ are the parameters of the beta distribution, which we set to $\\alpha = 1$ and $\\beta = 1$ in our\nexperiments.\nBy calculating this expected usefulness, we can rank the fracture clusters according to their potential\nfor reuse in future tasks. The ranking helps focus on fracture clusters that are more likely to appear in\nsuccessful outcomes and contribute to the agent's performance across diverse scenarios."}, {"title": "A.6 Environments", "content": "This section provides details on the environments used in our experiments, including standard\ngrid-world domains (Four Rooms, Grid, Romesh Maze), MetaGrid, and the Procgen suite. Each\nenvironment has been designed to evaluate different aspects of the agent's behaviour, such as\nnavigation, exploration, and task performance."}]}