{"title": "Energy-Based Prior Latent Space Diffusion model\nfor Reconstruction of Lumbar Vertebrae from\nThick Slice MRI", "authors": ["Yanke Wang", "Yolanne Y. R. Lee", "Aurelio Dolfini", "Markus Reischl", "Ender Konukoglu", "Kyriakos Flouris"], "abstract": "Abstract. Lumbar spine problems are ubiquitous, motivating research\ninto targeted imaging for treatment planning and guided interventions.\nWhile high resolution and high contrast CT has been the modality of\nchoice, MRI can capture both bone and soft tissue without the ioniz-\ning radiation of CT albeit longer acquisition time. The critical trade-\noff between contrast quality and acquisition time has motivated 'thick\nslice MRI', which prioritises faster imaging with high in-plane resolution\nbut variable contrast and low through-plane resolution. We investigate\na recently developed post-acquisition pipeline which segments vertebrae\nfrom thick-slice acquisitions and uses a variational autoencoder to en-\nhance quality after an initial 3D reconstruction. We instead propose a\nlatent space diffusion energy-based prior 4 to leverage diffusion mod-\nels, which exhibit high-quality image generation. Crucially, we mitigate\ntheir high computational cost and low sample efficiency by learning an\nenergy-based latent representation to perform the diffusion processes.\nOur resulting method outperforms existing approaches across metrics\nincluding Dice and VS scores, and more faithfully captures 3D features.", "sections": [{"title": "1 Introduction", "content": "Low back pain stands as the world's predominant musculoskeletal issue [24]. For\nmore serious symptoms, lumbar spine imaging and modeling is a critical tool\nused to aid in diagnoses and treatment planning. The lumbar spine is composed\nof five segments (L1-L5) and can exhibit significant variations [4], so patient spe-\ncific models can provide valuable insight and inform possible treatment options.\nWhile computed tomography (CT) is particularly effective at capturing skeletal\nstructures with high resolution and high contrast, it uses ionizing radiation and\nfails to capture soft tissue. Alternatively, magnetic resonance imaging (MRI)\ncaptures not only the vertebrae but also the disc spaces, spinal canal, and nerve\nroots without ionizing radiation but at the cost of acquisition time [3].\nOne of the key factors of MRI for acquisition times is the slice thickness [14].\nWhile using thinner slices would improve the through-plane resolution, it greatly\nincreases acquisition time [19], which leads to patient discomfort and increased\nmotion artifacts. As a result, so-called 'thick slice MRI' is typically used in\nclinical practice, prioritizing high in-plane resolution and faster acquisition times\nat the cost of through-plane resolution. Machine learning-based reconstruction\ncan potentially recover missing details and allow detailed anatomical modeling\nby increasing the through-plane resolution while faithfully reconstructing fine\ndetails."}, {"title": "2 Previous work", "content": "Previous approaches have explored enhancing the resulting anatomical model\nquality from thick slice MRI, but a large focus has been on super-resolution re-\nconstruction (SRR) as a preprocessing step in Fig. 1 [6,26,13]. [19] demonstrates\na U-net based 3D approach at the reconstruction step, and high resolution mod-\nels can be refined from the thick slice reconstructions, for example using VAEs\nor shape priors in post-processing [21,2]. We focus on the last approach of de-\nveloping post-processing procedures which are integrable into existing pipelines.\nWe highlight the automated pipeline introduced by [21] which consists of\na segmentation network, the ReconNet, and a VAE-based post-processing step\n(Fig. 1). Despite their efficiency, VAEs are notorious for generating outputs which\nare the average of all likely outputs, resulting in something akin to oversmooth-\ning, which is also observed in [21]. This can be attributed both to per-pixel loss\nfunctions [12], the latent space prior being suboptimal [16], and the gap between\nreal and approximate posterior distributions."}, {"title": "3 Method", "content": "An alternative to VAEs are denoising diffusion probabilistic models (DDPMs)\n[10], designed for high-quality image generation. DDPMs add noise to training\ndata and then learn the backward denoising process. However, DDPMs require a\nfull-dimensional, image-scale latent space and a lengthy diffusion process, leading\nto high computational costs. When it comes to large 3D medical images, this\ncost can be prohibitively high.\nOther methods that optimize the latent space of VAEs have been shown\nto improve generated samples and reduce their computational cost. For exam-\nple, LEBMs replace the encoder of a VAE by an energy-based model (EBM)\nto learn an energy-based latent space via Markov chain Monte Carlo (MCMC)\nsampling [17,7]. Another example is normalizing flows [8]. Appendix A provides\nthe theoretical basis of our model and covers EBM and DDPM in detail. Ap-\npendix A.3 compares the previous methods.\nA similar approach has recently been applied in the field of interpretable\ntext modeling [25]. Their model focuses on generating creative and varied text\noutputs, which is encouraged via a symbol-vector coupling which can be used\nto condition the results. However, this comes at computational cost, which is\nfeasible for their low dimensional data. Our data-driven approach is more suitable\nfor the medical setting and, by avoiding this symbol-vector coupling as explained\nin Appendix B, can be easily applied to high dimensional image data.\nThe overall architecture of the LSD-EBM is visualized in Fig. 2. Given an\ninput 3D image x, the inference network generates the latent variable $z_0 \\sim$\nq4(Zox) = N(z; \u03bc\u03bf(x), \u03c3\u03bf(x)) with learnable mean \u00b5o and variance \u03c3\u03bf. A la-\ntent diffusion and denoising processes are constructed with the energy-based"}, {"title": "4 Results", "content": "prior to optimize zo [9]. The diffusion in latent space acts as checkpoints guid-\ning the learning while also reducing its computational overhead which would be\nprohibitive in full image space, therefore resulting in more stable and accurate\ngeneration. The optimized zo is then used by the generation network to recon-\nstruct the 3D image x' ~ p\u03b2 (\u03a7 \u0396\u03bf). To this end, the latent diffusion process is\ndefined as Markov chain: in Eq. (11) [18]\nq(Zt+1/zt) := N(Zt+1; \u221a1 \u2013 \u03c3\u03b5+1zt, \u03c3\u00b2+11), (1)\nwhere 2+1 is the noise schedule applied to the latent variables in each diffusion\nstep. The LSD-EBM implements the conditional EBM [9], where a new latent\nZt = \u221a1 - \u03c3\u00b2+12 is defined such that its conditional probability, pa (\u017et Zt+1), is\ndescribed by a Boltzmann distribution:\nPa(\u017dt Zt+1) =\nexp(-Ea(zt, t) - 20||Zt+1 - \u017de||2)\n\u017da\n(Zt+1,t + 1)\nwith \u017da(Zt+1,t + 1) = exp (-Ea(z,t) 20||Zt+1 - \u017dt||2) dizt. (2)\nEq. (2) defines the reverse latent space process where we perform MCMC sam-\npling between denoising steps as in Fig. 2. Like a vanilla EBM, the energy func-\ntion Ea is parameterized by a neural network. In contrast to the LEBM, this\nenergy function has an additional time argument due to the quadratic term\nin the partition function Z which constrains the energy landscape and facili-\ntates sampling [9]. Because \u017e\u0165 is easily obtained by zt from E[zt+1] = \u017et, in\npractice Pa (ZtZt+1) can be used instead of pa (\u017et Zt+1) and is determined using\nmaximum likelihood estimation. We use MCMC sampling via Langevin dynam-\nics [23], where\nK+1\nZt\n\u03bb\n2\nzlog Pa (zZt+1) + Wk, \u03c9\u03ba ~ \u039d(0, \u03bb), k = 1, 2, ..., K. (3)\nIn practice, pa is approximated by the estimated distribution qa (\u017et). qa (\u017et) \u2192\nPa(zt) when the iteration steps K \u2192 \u221e and \u5165 \u2192 0. The gradient of the log\nlikelihood is given by\nVzlog Pa (ZtZt+1) = -VzEa(zt, t) +\n1\n2\n(Zt+1 - Zt), (4)\n\u03c3\u03c4+1\nwhere zt is updated by Eq. (3) such that the final latent variable zo is obtained\nat the last step t = 0. The output image is reconstructed from zo using the\ngeneration network, i.e., x' ~ ps(x|zo) = N(\u03b2(zo), \u03c3\u0399\u03c1). Similarly to the VAE,\nan Evidence-based Lower Bound (ELBO) can be derived, see Appendix C.1.\nThe encoding and generation networks \u03c6, \u03b2 are trained simultaneously for each"}, {"title": "4.1 Datasets and Metrics", "content": "We consider two vertebrae reconstruction datasets for 3D vertebrae segmenta-\ntions from the work of [21], including the CT based data for the training of the\nmodel (denoted as CT-Train, 446 images in total) and paired MRI-CT dataset\nfor the testing of the model, including 80 low-quality MRI images (L-MRI) and\ncorresponding high-quality CT images (H-CT) as ground-truth segmentation.\nThe vertebral masks used for segmentation are of resolution 1mm\u00b3, which aligns\nto the typical lumbar spine protocol resolutions scanned by current MRI scan-\nners. Both datasets consist of 1283 binary valued pixel patches from either CT\nor MRI scans, where the patch size is a result of the cropping of the complete\nlower lumbar spine into individual vertebrae.\nThe evaluation metrics for vertebrae datasets are selected across different\ncategories of measure [20], including Dice's similarity coefficient (DSC) for re-\nproducibility, volumetric similarity (VS) for similarities of segment volumes, sen-\nsitivity (SEN) for the true positive ratio, specificity (SPEC) for the true negative\nratio, normalized mutual information (NMI) for the shared information between\nvolumes, and Cohen's kappa (CK) for inter-annotator agreement between vol-\numes, as defined in Appendix D [5,15].\nWe use this variety of measurements because the available data is very lim-\nited, and because the nature of such 3D medical data makes it challenging for a\nsingle metric to accurately capture similarity. For example, the VAE used in the\npost-processing of [21] gives a good DSC score, but nevertheless the reconstruc-\ntions are unrealistically smooth. Additionally, the variance of the latent space is\nused to understand the latent space priors learned by the LSD-EBM and LEBM.\nImplementation details can be found in Appendix E."}, {"title": "4.2 Lumbar Vertebrae Reconstruction", "content": "The methods VAE, LEBM, and LSD-EBM are trained using CT-Train images\nand then applied to low-quality MRI images (L-MRI) to generate missing details.\nWe evaluate these methods by comparing the reconstructed MRI vertebrae from\nL-MRI with those reconstructed from H-CT, with metrics in Table 1 and sample\nreconstructions in Fig. 3. Additional reconstructions are shown in Appendix H.\nOur LSD-EBM outperforms the VAE and LEBM in terms of DSC and VS\nscores, indicating better reproducibility and higher similarity to the H-CT vol-\nume. Although LEBM achieves a higher SEN score, it scores lower in SPEC,\nwhich suggests that LEBM generates fewer false negative segments, but may"}, {"title": "4.3 Convergence in the Latent Space", "content": "We analyze the variance of the latent variables at each step in Fig. 5, which\nserves as a measure for how much a sample resembles random noise. A lower\nvariance is attributed to less noise and therefore a stronger learned signal.\nIn the LEBM case, the shape of the variance exhibits an expected behaviour:\nthe spike followed by a gradual convergence to a variable minimum matches\nthe burn-in or calibration followed by convergence period of MCMC methods.\nThe LSD-EBM, in contrast, converges directly and with more consistency across\nruns to a comparatively lower variance. Its stability is a direct result of the well-\ndefined denoising process, detailed in [10] and described in Sec. 3. This ensures\nthe consistency of learned latent spaces across different runs and facilitates better\nreconstructions at various time-steps, as evidenced in Fig. 4."}, {"title": "5 Conclusions", "content": "In this study, we enhanced the quality of low-quality MRI vertebra models\nfrom thick-slice images. We develop and implement a latent energy-based model\ntrained on high-quality CT data, LSD-EBM, which demonstrated superior re-\nconstructions compared to VAEs and LEBMs. It not only addressed the compu-\ntational challenges in diffusion models, making them suitable for the 3D medical\nimaging regime, but also enhanced reconstruction performance. Furthermore,\nour model exhibited a more stable generative process with a comparable time\ncost to VAEs, taking half as long as the LEBM. Although our method relies on\nhigh resolution domain specific CT images, our results bolster the feasibility of\nusing MRI as an efficient and safer alternative to CT scans in vertebrae model-\ning. Future work will include understanding latent feature extraction for domain\nadaptation and generalizability."}, {"title": "A Theoretical Background", "content": "Energy-based models have a long history tracing back to statistical physics,\nHopfield networks [11] and Boltzmann machines [1].\nThe main principle is to model the prior as an energy function, which can\nassign an energy to each input sample x from the data space X. [7] uses the\nEBM for image generation, maximizing the likelihood between generated and\nreal image instances by assigning low energy values for realistic images (positive\nsamples) and increasing the energy for unrealistic images (negative samples). The\nprior can be used in a maximum likelihood estimation method for generation,\nfor example using MCMC sampling.\nFor X being the distribution for each datum x ~ px(x), the energy function\nE(x) can be parameterized by 0, where e can be neural network parameters. The\nenergy function defines a probability distribution via the Boltzmann distribution,\ni.e. models a density over the input space\n$P_\\theta(x) = \\frac{exp(-E_\\theta(x))}{Z(\\theta)},$ with $Z(\\theta) = \\int exp(-E_\\theta(x)) dx, \t\t(5)$\nwhere Z(0) is the normalizing factor.\nThe objective of the EBM tries to maximize the negative log likelihood of\npx(x) as follows\n$\\mathcal{L}(\\theta) = E_{x \\sim p_x(x)}[-\\log p_\\theta(x)] = E_{x \\sim p_x(x)}[E_\\theta(x) - \\log Z(\\theta)].\t\t(6)$\nThe normalizing factor is of course intractable but the optimization can be per-\nformed via a gradient decent. The gradient can be shown to obtain the following\nform [22] as\n$\\nabla \\mathcal{L}(\\theta) \\approx E_{x^- \\sim p_\\theta}[-\\nabla_\\theta E_\\theta(x^-)] - E_{x^+ \\sim p_x}[-\\nabla_\\theta E_\\theta(x^+)],\t\t(7)$\nThis gradient decreases the energy of the positive data samples x+ ~ px \u2248 q05,\nwhile increasing the energy of the negative samples x\u00af ~ pe. The sampling can\nbe performed via Langevin dynamics making use of the gradient of the energy\nfunction:\n$X_{k+1} = X_k - \\frac{\\lambda}{2} \\nabla_x E_\\theta(x_k) + \\omega_k, \\quad \\omega_k \\sim \\mathcal{N}(0, \\lambda), \\quad k=1,2,...,K.\t\t(8)$\nThe iterative procedure defines the estimated distribution qe given that k ~ qe\nand as K \u2192 \u221e and X \u2192 0, qe (X) \u2192 po(x)."}, {"title": "A.2 Diffusion Probabilistic Models", "content": "[18] introduces the diffusion probabilistic model (DPM), which artificially de-\ncreases the quality of the data by adding increasing levels of noise, while training\na model to reverse this process, both can be modeled with a Markov chain. The\ntrained model can be used to generate new samples starting from pure noise.\n[10] proposes DDPM which achieves remarkable results in image synthesis by\nfixing the variance and learning noise directly.\nThe forward also diffusion or noising process starts with a data sample\nfrom a real distribution x ~ q(x0), and Gaussian noise is added gradually to the\nsample in T steps, effectively creating a Markov chain X1, ..., XT.\nq(xt+1|xt) := N(xt+1; \u221a1 \u2013 \u03c3\u00b2+1xt, \u03c3\u00b2+11), (11)\nwhere \u03c3\u03c4+1 is the variance schedule of the predefined Gaussian noise.\nThe reverse also the denoising or generative process, aims to invert the\nforward diffusion process. Generated samples from the original data distribution\nare obtained by initiating the forward process with a random noise x\u2081 ~ N(0, I).\nSubsequently, running the reverse process reconstructs samples that closely re-\nsemble the original data distribution. Parameterizing a model, e to approximate\nthe data distribution, we obtain the following:\nPo(xtxt+1) := N(xt; \u03bc\u0473(xt+1, t + 1), \u03a3\u00f8(xt+1,t + 1)) (12)\nwhere \u03bc\u03b5 and De can be modeled with a neural network. The objective of a\nDDPM is to maximize the likelihood between the diffusion process step q(xt Xt+1, X0)\nand denoising process step po(XtXt+1)."}, {"title": "A.3 Comparison of Existing Methods", "content": "Some existing methods are compared in Fig. 6. The VAE is a variational in-\nference method with an encoder and a decoder; the encoder can have multiple\nimplementations but it is nowadays standard practice to implement it as a neu-\nral network. The LEBM is based on the VAE and replaces the encoder with an\nMCMC sampling of an energy based prior. VAEs often assume Gaussian priors\nand posteriors, while LEBMs offer more flexibility in defining the energy func-\ntion. Our method implements an autoencoder-like architecture which brings in\nthe performance capability of diffusion models and combines them with EBMs.\nHowever, in contrast to LEBMs, we implement the conditional EBM in the latent\nspace which is less computationally expensive, and find that this choice and our\ndiffusion-like architecture results in more accurate and efficient reconstructions."}, {"title": "B Further Previous Work", "content": "[25] relies on an information bottleneck in conjunction with geometric clustering\nfor their symbol-vector coupling to avoid mode-collapse and generates more cre-\native text outputs. Their symbol-vector coupling EBM results in the distribution\nPa(y, 20:T, x) where the symbol vector encourages conditioning on a specific vec-\ntor with the caveat that it must be learned by K-means clustering on the latents."}, {"title": "C Method Details", "content": "In contrast, the decisions made for our model focus on generating anatomically\nrealistic and data-driven reconstructions. This motivates our straightforward ap-\nproach which avoids the symbol-vector coupling and intentionally adheres more\nstrictly to the data, as is desirable for medical use cases and makes such an\napproach feasible for high dimensional data. Our approach utilizes Pa(20:T, X)\ndirectly, making it more efficient and easier to train; indicatively, [25] applies\ntheir method on very low dimensional (D=2) synthetic data while our method\ncan be easily applied to high dimensional image data. This high dimensional-\nity is a key challenge which we explicitly sought to address to effectively and\nefficiently generate images."}, {"title": "C.1 Derivation of ELBO for LSD-EBM", "content": "The objective function can be formulated with an evidence lower bound (ELBO)\nfor po(x), akin to the original VAEs (0 := \u03b1, \u03c6, \u03b2):\nlogpo(x) \u2265 Eqy (zo|x) [logps(x|zo)] - DKL (q4(Zo|X)||Pa(z))\n= Eq (zo/x) [logps(x|zo) \u2013 logq4 (Zox)]\n+ Eqy (zo/x) [logpa(Zo)]\n=: L(\u03b1, \u03c6, \u03b2). (13)\nThe third term of L(\u03b1, \u03c6, \u03b2) is rewritten by Jensen's inequality at (I) as\n$E_{q_\\phi(z_0|x)}[\\log p_\\alpha (z_0)]$\n$=\nE_{q_\\phi(z_0|x)}[\\log \\int \\frac{q(z_{1:T}|z_0)}{q(z_{1:T}|z_0)}\\frac{p_\\alpha (z_{0:T})}{q(z_{1:T}|z_0)} dz_{1:T}]$\n$\\geq$\n$E_{q_\\phi(z_0|x)}[\\int \\frac{q(z_{1:T}|z_0)}{q(z_{1:T}|z_0)}\\log \\frac{p_\\alpha (z_{0:T})}{q(z_{1:T}|z_0)} dz_{1:T}]$;\n$=E_{q_{\\phi}(z_0|x)q(z_{1:T}|z_0)}[\\log \\frac{p_\\alpha (z_{0:T})}{q(z_{1:T}|z_0)}]$\n$=E_{q_{\\phi}(z_0|x)q(z_{1:T}|z_0)}[\\log p(z_T) + \\sum_{t=0}^{T-1} \\log\\frac{P_\\alpha (Z_t|Z_{t+1})}{q(Z_{t+1}|Z_t)}].$\t\t(14)\nAs z\u012b is in a standard Gaussian, logp(zr) is a constant. Also, the conditional\nprobabilities in (14) is simplified to\n$\\log p_\\alpha (Z_t|Z_{t+1})$\n$=-E_\\alpha (Z_t, t) - \\frac{1}{2\\sigma_{t+1}^2} ||Z_{t+1} - Z_t||^2 - \\log \\tilde{Z}_\\alpha (Z_{t+1},t + 1)$\n$=-E_\\alpha (Z_t, t) - \\frac{1}{2\\sigma_{t+1}^2} ||Z_{t+1} - \\tilde{Z}_t||^2$\n$-E_{p_\\alpha(Z_t/Z_{t+1})} \\lbrack E_\\alpha (Z_t, t) - \\frac{1}{2\\sigma_{t+1}^2} ||Z_{t+1} - Z_t||^2\\rbrack$\n.\t\t(15)"}, {"title": "C.2 Pseudo-algorithms for Latent Space Diffusion Energy-based\nMethod", "content": "The objective function is finally\n$\\mathcal{L}(\\alpha,\\phi, \\beta) = E_{q_\\phi (z_0|x)} [\\log p_\\beta (x|z_0) - \\log q_\\phi (z_0|x)]$\n$+\nE_{q_\\phi(z_0|x)q(z_{1:T}|z_0)} \\lbrack \\sum_{t=0}^{T-1} \\log \\frac{P_\\alpha (Z_t|Z_{t+1})}{q(Z_{t+1}|Z_t)}\\rbrack,$\t\t(16)\nand the parameters \u03b1, \u03c6, \u03b2 are optimized by the gradient of L(\u03b1, \u03c6, \u03b2) as\n$\\nabla_{\\theta}\\mathcal{L}(\\alpha, \\phi, \\beta) = E_{q_\\phi(z_0|x)} [\\nabla_{\\beta}log p_\\beta (x|z_0) - \\nabla_{\\theta}log q_\\phi (z_0|x)] +$\n$\\nabla_{\\alpha} E_{\\alpha \\phi (z_0|x)q(z_{1:T}|z_0)}\\rbrack$\n$\\lbrack\\sum_{t=0}^{T-1}\n$-E_\\alpha (Z_t, t) - E_{p_\\alpha(Z_t/Z_{t+1})} - E_\\alpha (Z_t, t)$\t\t(17)"}, {"title": "D Metrics", "content": "Let SA and SB be the two 3D reconstructions, SN(x, y, z) \u2208 {1,0} be the value\nof the pixel with the coordinates x, y, z of segmentation SN, and |SN(x, y, z)| is\nthe total number of pixels (in our case, 1283). We use the following metrics [20]:\nThe Dice score between two segmentations SA and SB is defined as:\nDICE(SA, SB) =\n2 \u00d7 SAN SB\n|SA|+|SB|\n,\t\t(18)\nwhere SA SB\u2758 represents the volume of the intersection (i.e., the number\nof pixels or voxels that are positive in both SA and SB), and |SA| and |SB|\nare the volumes (i.e., the total number of pixels) of segmentations SA and SB,\nrespectively.\nVolumetric Similarity between two segmentations SA and SB is defined as:\nVS(SA, SB) = 1\n||SA| - |SB||,\n|SA|+|SB|\n\t\t(19)\nSpecificity is defined as the proportion of true negatives (TN) out of the total\nnumber of actual negatives:\nSPEC(SA, SB)\nTN\nTN + FP\n\t\t(20)\nwhere FP represents false positives.\nSensitivity, also known as recall or true positive rate, is defined as:\nSEN(SA, SB) =\nTP\nTP + FN\n\t\t(21)\nwhere TP represents true positives and FN represents false negatives.\nNormalized Mutual Information (NMI) between SA and SB is defined as:\nNMI(SA, SB) =\n2 \u00d7 I(SA; SB)\nH(SA) + H(SB)\n,\t\t(22)\nwhere I (SA; SB) is the mutual information between SA and SB, and H(SA) and\nH(SB) are the entropies of SA and SB, respectively."}, {"title": "E Implementation Details", "content": "The VAE, LEBM, and LSD-EBM are compared across results trained on dif-\nferent steps (MCMC for the prior sampling in LEBM, and diffusion steps in\nLSD-EBM), i.e., 2, 15, and 20 steps. The models VAE, LEBM, and LSD-EBM\nare trained for 200 epochs with learning rates of 2 \u00d7 10\u22125, 10\u22124, and 2 \u00d7 10\u22125,\nand batch sizes of 4, 2, and 4 respectively. Training was performed on a NVIDIA\nA100 GPU with 40 GB memory."}, {"title": "FValidation Experiments", "content": "We trained the VAE, LEBM, and LSD-EBM on the standard 2D image\ndatasets MNIST, FashionMNIST, CIFAR10, and CelebA and evaluated their\nperformances using the FID score, shown in Table 2. The LSD-EBM significantly\noutperforms its counterparts for MNIST, FashionMNIST and CelebA, and has\ncomparable performance to the LEBM on the CIFAR10 dataset. Critically, our\nmodel has increased variability in its generations as compared to the other meth-\nods, with further samples in Appendix F. These results showcase the capability\nand generalizability of LSD-EBM for image generation, and its application in\nthe [21] pipeline.\nTo test and compare our proposed method, we trained all models on the\nstandard public image datasets: MNIST, CIFAR10, and CelebA.\nThe Fr\u00e9chet Inception Distance (FID) score was used to assess the quality of\nimages generated by models against a set of real images,\nFID = ||\u03bc\u03b5 \u2013 \u03bcg||\u00b2 + Tr(\u03a3\u2084 + \u03a3g \u2013 2(\u03a3\u03a3\u338f)1/2)\t\t(24)"}, {"title": "GTime Comparison Across Methods", "content": "The training times for VAE, LEBM, and LSD-EBM with 20 steps on the verte-\nbrae dataset for 200 epochs are 12 hours, 33 hours, and 17 hours, respectively.\nThe processing time of reconstruction of one vertebrae sample for VAE, LEBM,\nand LSD-EBM are 0.039s, 0.65s, and 6.25s, respectively. The reconstruction time\nof LSD-EBM, while slower, is well within acceptable bounds. Regarding compu-\ntational efficiency, the processing of DDPM with just two steps exceeds the 40\nGB GPU memory limit, highlighting its inefficiency"}, {"title": "H Vertebrae Reconstruction Examples", "content": "In Figs. 10, 11, 12, 13, we show additional results comparing the input data,\nLEBM, VAE, and LSD-EBM, and the ground truth high resolution model. We\nprovide close-up details of regions of interest for closer comparison."}]}