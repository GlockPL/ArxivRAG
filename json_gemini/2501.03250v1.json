{"title": "Machine Learning and Deep Learning Techniques used in Cybersecurity and Digital Forensics : a Review", "authors": ["Jaouhar Fattahi, Ph.D"], "abstract": "In the paced realms of cybersecurity and digital forensics machine learning (ML) and deep learning (DL) have emerged as game changing technologies that introduce methods to identify stop and analyze cyber risks. This review presents an overview of the ML and DL approaches used in these fields showcasing their advantages drawbacks and possibilities. It covers a range of AI techniques used in spotting intrusions in systems and classifying malware to prevent cybersecurity attacks, detect anomalies and enhance resilience. This study concludes by highlighting areas where further research is needed and suggesting ways to create transparent and scalable ML and DL solutions that are suited to the evolving landscape of cybersecurity and digital forensics.", "sections": [{"title": "Introduction", "content": "In this age of technology and digital advancements cybersecurity has become a priority due to the fast paced growth of technology and the growing dependence on digital platforms. This reliance has opened up avenues for advancement and creativity. It has also brought about risks and dangers that pose a challenge to safeguarding digital resources and confidential data. Digital forensics works hand in hand with cybersecurity by looking into and solving incidents that happen on; it concentrates on collecting and safeguard proof and then examining it further to find answers. This process is vital in identifying cybercrimes like identity theft and fraud while also making sure that those responsible are held accountable in the world. The introduction of machine learning (ML) and deep learning (DL) techniques into investigations have expanded its functionalities by automating the analysis of data sets efficiently. This helps in better spotting harmful software and getting deeper insights, into intricate cyber events. In this study we are going to review common machine learning and deep learning techniques used in cybersecurity and digital forensics applications."}, {"title": "Standard Machine Learning techniques and algorithms", "content": "Machine Learning is an area of artificial intelligence that involves the design, analysis, development, and implementation of methods that allow a machine to evolve through a systematic process, which allows it to accomplish tasks that are difficult to perform by conventional algorithmic means without using explicit instructions. This process roughly consists of the following seven sequential steps.\n1. Collecting data: this step is extremely important because the quality and quantity of data the data scientist collects will directly affect the quality of the predictive model. A good data collection must ensure that data belonging to one class or another is balanced (i.e. reflecting the reality). The collection should also be as comprehensive as possible and should not focus only on the general case. It must also provide a sufficient number of samples that can provide reliable results. The more samples there are, the more generalizable the predictive model is.\n2. Investigating data: this step aims at understanding the characteristics of the collected data and probably pre-processing them prior to being used to build the model. In this step, it is a good practice to perform some checks on data by printing a data summary containing the number of samples, the number of classes, the number of samples pertaining to a class, the data distribution, etc. Using visualization tools is also recommended to pick up relevant relationships between different variables and to discover if the data are balanced or not and if there are missing data or not. The data scientist needs also to perform some relevant and useful measurements like calculating the correlation between features and the output to determine which features are important and which ones are less important, as well as whether the features are linearly correlated or not.\n3. Preparing data: a data scientist needs to split the data in at least two parts. The first part will serve to train the model. The second part will serve to evaluate the model performance. In a wide range of cases, data needs other types of treatment like normalization, error correction, tokenization, vectorization, data type conversion, data de-duping, etc.\n4. Choosing the model: the choice of the best model for a given problem depends on the size, quality, and nature of the data. Even the most experienced data scientists cannot always know which algorithm will work best before they try it. However, there are some models that are better suited to certain types of problems. For instance, some are very well suited to image data, others to sequences (such as text or music), some to digital data, others to particular textual data."}, {"title": "Supervised learning", "content": "Learning is said to be supervised when the data entering the process are already categorized (labeled) and algorithms must use them to predict an outcome so that they can proceed the same way when the data are no longer categorized. In other words, in supervised learning one has prior knowledge of what the output values of the samples should be."}, {"title": "Unsupervised learning", "content": "Unsupervised learning is much more complex since the system has to detect similarities in the data it receives and organize them accordingly. Unsupervised learning therefore has no labeled outcomes. Its objective is thus to deduce the natural structure present in a set of data points. In other words, in unsupervised learning one does not have prior knowledge of what the output values of the samples might be."}, {"title": "Classification", "content": "Classification is a family of supervised methods used to categorize data using an algorithm. It requires that the data points have discrete values. The learning data are grouped in categories (classes). Then, the new input data are classified according to the learned data. Classification produces discrete values."}, {"title": "Regression", "content": "Regression is a supervised method for understanding the relationships between a variable of interest and explanatory variables. Regression usually requires that the data points have continuous values. In a regression, first, the factors (i.e. independent variables) are found. Then, coefficients (i.e. multipliers) with independent variables are calculated to minimize differences between true and predicted values. Finally, a formula for the regression is set. This formula is used to predict the dependent variable (i.e. one wants to measure) from independent variables (i.e. one thinks the target measure hinges on)."}, {"title": "Clustering", "content": "Clustering is an unsupervised technique that consists in grouping the data into homogeneous groups called clusters, so that the elements within the same cluster are similar, and the elements belonging to two different clusters are not similar."}, {"title": "Reinforcement learning", "content": "Reinforcement Learning is a method that consists in letting the algorithm learn from its own errors. The algorithm begins by making totally random decisions and then develops its own method to correctly accomplish the task entrusted to it. That is, an agent in a current state S learns from its environment by interacting with it through actions. Once an action A is performed, the environment returns a new state S' along with an associated reward R, which can be positive (for a good action) or negative (for a bad action). This process is repeated until the agent decision reaches a confirmed maturity translated into a long and steady behaviour of good actions. Thus, this learning method has the advantage of allowing the machine to be more creative."}, {"title": "Common problems", "content": "Underfitting: this problem happens when the model cannot capture enough patterns from data. This may be the case when one uses a model for a problem of an incompatible nature (e.g. a linear regression for a non-linear problem, a simple Artificial Neural Network (ANN) for a sequential prob-lem, an Recurrent Neural Network (RNN) without memory for problems needing past information, etc.). When underfitting is observed, the model accuracy is generally low.\n\u2022 Overfitting: this is the opposite of the under-fitting problem. It happens when the learning clings too closely, or even exactly, to a particular set of data. We say that the model learns the existing data by heart. A model in an underfitting (or overlearning) situation has difficulty generalizing the characteristics of the data and loses its predictive capabilities on new samples.\n\u2022 Missing data: real-world data are not always neat and consistent. Generally, they are prone to being incomplete, noisy, and incongruous, and it is important for the analyst to handle them by either populating in the miss-ing values or removing them as they can induce prediction or classification mistakes. There are many possible reasons why they happen, ranging from human mistakes in data acquisition to imprecise sensor measurements by way of software glitches in the data handling process. A common way to correct this is to replace missing values with the mean, median, or mode values. They can also be attributed the value given by a regression model, such as linear regression, or the value having the majority of votes using a k-Nearest Neighbours (k-NN) model.\n\u2022 Vanishing gradient: the problem of the vanishing gradient arises when one trains a neural network model using gradient optimization techniques. This is one of the major problems in Machine Learning and Deep Learning because it usually leads to a very long training time or even to a non-convergence towards the correct minimum. It is manifested by the fact that the gradients tend to become smaller and smaller as one keeps moving backward in the neural network during the back-propagation phase until the model ceases learning from data. This problem, which we will present in more detail in this document, is addressed in several ways, such as by using appropriate activation functions, or models with short memory such as Long Short-Term Memory (LSTM) models and Gated Recurrent Units (GRU) models.\n\u2022 Exploding gradient: gradient explosion occurs when large error gradients build up which results in huge updates to the weights of the model dur-ing the training phase. When gradient amplitudes accumulate, network instability generally results, which may lead to bad prediction outcomes. This problem is usually handled using clipping techniques to keep gra-dients stable. With these techniques, a predefined gradient threshold is used, and then the gradients that are above this threshold are rescaled.\n\u2022 Initialization: initialization can have a direct effect on model convergence. It can speed up training or it can generate awkward problems. It may gen-erate exploding or vanishing gradients. Some guide lines and techniques [1, 2] are proposed to provide suitable initial values to a given model. This still remains an active research area though."}, {"title": "Linear Regressions", "content": "Linear regression (LinR) was first used in statistics to understand the link be-tween input and output of numerical variables. It is used in Machine Learning as a model to predict the output from the inputs under the assumption of a lin-ear relationship between them. LinR is probably the simplest Machine Learning model.\nFor a single input and x a single output y, the model consists in a line having the following equation:\n$y = b_0 + b_1x$\nFor multiple inputs, the model consists in a hyperplane having the following equation\n$y = b_0 + b_1x_1 + b_2x_2 + \\ldots + b_nx_n$\nVarious techniques can be used to prepare the linear regression equation from a training dataset and hence to find the most adequate coefficients for the problem. The most widely used is the standard ordinary least (sum of) squares method. For a simple LinR, this method consists in minimizing the error observed between the predicted and the true outputs of the data set, given by the following equation:\n$E(a, b) = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 = \\sum_{i=1}^{n} (y_i - (b_0 + b_1x_i))^2$\nwhere $\\hat{y_i}$ is the actual output for the example i.\nTo determine the coefficients $b_0$ and $b_1$, the following steps are performed.\nWith multiple linear regression (i.e. linear regression with many predictors), k independent variables and k + 1 regression coefficient are involved. The steps to calculate these coefficients are similar to simple regression ones, but one must solve k + 1 equations with k + 1 unknowns to find the least-squares solution. This involves a few matrix operations like matrix multiplications and matrix inversions.\nIt is worth mentioning that there is another type of regression called poly-nomial regression in which the regression function can involve terms such as $b_ix_i^2$ . This regression could be seen as a linear regression by performing a variable change and considering the term $x_i^2$ as a new predictor.\nTo know how well a linear regression model fits the data, we calculate the coefficient of determination (also called R-squared) given by:\n$R^2 = \\frac{\\sum_{i=1}^{n} (\\hat{y_i} - \\bar{y})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\nIf the coefficient of determination is close to 1, then the linear regression model is a good fit for the problem. If it is close to 0, then the model is not ad-equate for the problem. Another metric to measure the accuracy of predictions is the standard error of the estimate (also called the residual standard error or the root mean square error) given by:\n$s_{est} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{n-p}}$\nwhere p is the number of parameters being estimated. If we calculate the standard error of the estimate for a regression model with $b_0$ and $b_1$, then p = 2. The smaller is $s_{est}$ the better is the model because a value close to zero indicates that most observations are close to the fitted line.\nLinear regression is a very simple and intuitive method. In addition to its capability to classify data and predict the output of unseen data, it helps to understand the relationship between variables. However, it is only useful when the relationship between variables is linear. It is also sensitive to outliers."}, {"title": "Logistic Regressions", "content": "Logistic regression (LogR) is a statistical method of analyzing datasets in which one or more independent variables (predictors) predict a result quantified with a binary variable. LogR model is based on a transformation (logit) which computes the logarithm of the probability of event divided by the probability of no event. It is defined as follows:\n$\\text{logit}(p) = \\ln \\frac{p}{1-p}$\nwhere $p = p(y = 1|X)$ is the conditional probability of y being true knowing X; $X = (x_1,x_2,...,x_n)$ is the input vector, y is the predicted value, p is the probability of presence of the event, 1 \u2212 p is the probability of absence of the event, and $\\frac{p}{1-p}$ are odds.\nAssuming a linear relationship between the predictors and logit(p), meaning $\\text{logit}(p) = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$, we have\n$\\frac{p}{1-p} = e^{\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n}$\nor\n$p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n)}}$\nNotice that p is exactly the sigmoid function \u03c3, also known as the logistic function, applied on the weighted inputs. If it returns a value greater than 0.5, then the event is considered present. If it returns a value less than 0.5, then the event is considered absent. If the returned value is close to 0.5, then the decision regarding the presence of the event is hard to make.\nThe learning process purpose is to determine the best values for the coeffi-cients $\\beta_i$. This is reached using the gradient descent. The cost function to be minimized is called the LogLoss given by the following formula\n$\\text{LogLoss} = \\sum_{(X,y) \\in D} -y \\ln(\\hat{y}) - (1 - y) \\ln(1 - \\hat{y})$\nwhere D is the training dataset, $X = (x_1, x_2, ..., x_n)$ is the input vector, y is the actual labeled value and $\\hat{y}$ is the predicted value.\nIn order to remedy the overfitting problem that frequently occurs, a regular-ization term should be added to the LogLoss function, resulting in the regulated RegLogLoss that should be minimized, given by the following formula:\n$\\text{RegLogLoss} = \\sum_{(X,y) \\in D} -y \\ln(\\hat{y}) - (1 - y) \\ln(1 - \\hat{y}) + \\frac{\\lambda}{2} ||w||^2$\nwhere \u03bb is a parameter and w is the weight vector.\nLogistic regression is easy to implement and efficient to train. It provides conditional probabilities which may be very useful in a large number of appli-cations. It is usually possible to transform most non-linear features into linear ones. However, it becomes fairly complicated with multiple predictors."}, {"title": "Decision Trees", "content": "A Decision Tree (DT) classifier is a supervised classification algorithm in Ma-chine Learning. It consists of a tree-like graph where a node is an attribute with a question and a leaf is a class to which belongs an item (see Figure 5). It is built as follows. For a training dataset, where each record contains a set of attributes, one of which is the class, a DT classifier attempts to construct a model for the class attribute based on the values of the other attributes. In most cases, a particular dataset is partitioned into training and testing sets, with the training set serving to construct the model and the testing set serving to validate it. A DT classifies the examples by sorting them from the root to a leaf, and the leaf node gives the answer (the class). Each node in the tree serves as a test case for certain attributes, and each branching edge represents one of the possible answers. This process is run recursively on each subtree rooted at a new node.\nFor a given dataset, a decision tree is not necessarily unique. For that, we need to find the tree that suits best the dataset, meaning, the tree with the greatest likelihood of correct classification for unseen examples. For that, some\nalgorithms have been suggested such as the Iterative Dichotomiser 3 algorithm (ID3) [3]. This algorithm builds decision trees using a top-down greedy strategy based on an entropy function and its derived gain function to select attributes in a way that the resulting tree is the smallest possible, meaning, involving as few attributes as possible. With that in mind, ID3 does not guarantee an optimal tree and can overfit the training data. This algorithm consists of the following steps [4]:\n1. Compute the entropy and the gain for every attribute. The entropy func-tion is given by:\n$E(D) = \\sum_{x \\in X} -p(x) \\log_2(p(x))$\nwhere D is a given partition, X is the set of classes, and\n$p(x) = \\frac{\\text{the number of elements of the class x}}{\\text{the number of elements of the data set}}$\nNote that when S is perfectly classified, E(S) = 0.\nThe gain function is given by:\n$\\text{Gain}(D, A) = E(S) - \\sum_{i=1}^{v} \\frac{|D_i|}{|D|} E(D_i)$\nwhere D is a given partition, A is an attribute having v different values, and $D_i$ is a sub-partition of D.\n2. Split the dataset into subsets on the attribute having the highest gain once the partitioning is performed;\n3. Create a decision tree node using that attribute; and\n4. If the dataset is perfectly classified, then terminate, else, recurse on subsets with the remaining attributes.\nThe Classification and Regression Trees algorithm (CART [5]) supports both regression and classification and builds binary trees (i.e. a single split results in two children). CART uses a cost function to test all attributes and the one with the smallest cost is selected in each splitting step. For regression problems, the cost function is usually the sum of squares error given by\n$\\text{SSE} = \\sum (Y_i - \\bar{y})^2$\nwhere $y_i$ is the predictor and $\\bar{y}$ is the mean value.\nFor classification problems, the cost function is the Gini score given by:\n$G = \\sum (p_i * (1 - p_i))^2$\nwhere $p_i$ is the proportion of same class inputs present in a given group. The best score (i.e. G = 0) occurs when a group contains only inputs of the same class (i.e. $p_i$ is either 1 or 0), and the worst score (i.e. G = 0.5) occurs when a node has a 50-50 split of classes (i.e. $p_i$ = 0.5).\nThe performance of a tree can be improved by pruning, which consists in getting rid of branches that make use of attributes with low importance.\nOther algorithms such as C4.5 [6] and MARS [7] propose other strategies to build decision trees.\nSeveral examples using decision trees can be found in [8-11].\nDecision tree algorithms are known to generate simple rules that are easy to understand and interpret. They can handle both continuous and categorical features. However, they are prone to errors in classification problems with multiple classes and can be computationally costly to train."}, {"title": "Random Forest", "content": "Random Forests are Ensemble Learning methods for both classification and regression. They work by building a variety of decision trees during the training phase and provide the predicted class or value by following the general scheme of one of the Ensemble Learning techniques. These techniques are presented in an independent sub-section (see 4.1)."}, {"title": "Naive Bayes Classifiers", "content": "The Naive Bayes classifier is a probabilistic algorithm used to discriminate dif-ferent entries based on some features. The core of the algorithm is based on Bayes' theorem which posits:\n$P(A|B) = \\frac{P(B|A).P(A)}{P(B)}$\nwhere:\n\u2022 A and B are two events (B is the evidence and A is the hypothesis);\n\u2022 P(A|B) is the posterior probability which is the probability of A happen-ing given that B has happened;\n\u2022 P(B|A) (the likelihood) is the posterior probability of B happening given that A has happened;\n\u2022 P(A) is the prior probability of A happening; and\n\u2022 P(B) is the prior probability of B happening.\nThe theorem assumes that the features are all independent, which means that the presence of a given feature does not affect another feature. That is why the algorithm is referred to as naive.\nTo understand how the Naive Bayes classifier derives from Bayes theorem, assume a feature vector $X = (X_1,X_2, ..., X_n)$ and a class variable $y_k$ among K classes in the training data. Bayes's theorem implies:\n$P(y_k|X) = \\frac{P(X|y_k).P(y_k)}{P(X)}, k \\in {1,2,...,K}$\nConsidering the chain rule [12] for multiple events:\n$P(A_1 \\cap A_2 \\cap ... \\cap A_n) = P(A_1| A_2 \\cap ... \\cap A_n).P(A_2|... \\cap A_n)$\nthe likelihood P(X|yk) could be written:\n$P(X|y_k) = P(X_1,X_2, ..., X_n|y_k) = P(x_1|x_2, ..., x_n|y_k).P(x_2|x_3..., x_n|y_k) ... P(x_{n-1}|x_n|y_k).P(x_n|y_k)$\nThis is where the independence assumption of Bayes' theorem comes in handy. This assumption states that:\n$P(x_i|x_{i+1}... x_n|y_k) = P(x_i|y_k), i \\in {1 ... n}$\nThe likelihood could then be reduced to:\n$P(X|y_k) = \\prod_{i=1}^{n} P(x_i|y_k)$\nThe posterior probability P(yk|X) could also be reduced to:\n$P(y_k|X) = \\frac{P(y_k).\\prod_{i=1}^{n} P(x_i|y_k)}{P(X)}, k \\in {1, 2, ..., K}$\nConsidering that P(X) is a constant for all k \u2208 1,2, ..., K, the Naive Bayes classification problem could be formulated as follows: for the different class values yk, k \u2208 {1,2, ..., K}; maximize\n$P(y_k).\\prod_{i=1}^{n} P(x_i|y_k)$\nNotice that the probability P(yk) is the relative frequency of the class yk in the training data. Also, notice that P(xi|yk) could be calculated using known distributions such as the Gaussian (or Normal) distribution [13], the Bernoulli distribution [14], or the multinomial distribution [15]. In such cases one has Gaussian (or normal) naive Bayes classifier, Bernoulli naive Bayes classifier, or a Multinomial naive Bayes classifier, respectively.\nThe naive Bayes algorithm is reputed to be computationally fast and simple to implement, and to cope well with classification problems with high dimen-sional features and limited training datasets. However, it tightly relies on the independence assumption, and consequently does not cope well with classifica-tion problems where this assumption is not satisfied."}, {"title": "Support Vector Machine", "content": "A Support Vector Machine (SVM) algorithm is a Machine Learning algorithm used to solve classification, regression, and anomaly detection problems. It is known for its solid theoretical foundation. Its purpose is to separate data into classes using a boundary in such a way that the distance between different data classes and the boundary is as large as possible. This distance is also called margin and the SVM is referred to as wide margin separator. The support vectors are generated by the data points closest to the boundary. Support vectors are the critical elements of the training set and if they change, this will probably change the position of the separating line. In Figure 6, in a two-dimensional space, the boundary is most likely the green line, the support vectors are most likely determined by the two points on the blue line as well as the two points on the red line, and the margin is most likely the distance between the boundary and the blue and red lines. Margin maximization ensures robustness against noise and allows the model to be more generalizable. The basic concept of linear boundary implies that the data should be linearly separable, which is rarely the case (see Figure 7). To overcome this shortcoming, SVMs often use what are called kernels[16] which are mathematical functions that allow SVM to perform a linear separation of a non-linearly separable problem in a higher-dimensional space (see Figure 8). The theory underpinning SVM is quite challenging and introduces several advanced mathematical concepts. We will devote an annex to cover most of these aspects.\nSVM has many advantages. Depending on the data, the performance of SVMs is at least of the same order as that of neural networks. It is also a robust technique because it determines the optimal hyperplane boundaries using the nearest points (support vectors) only, not distant points. However, SVM is reputed to be an opaque technique and very sensitive to the choice of kernel parameters. In addition, its computation time is often long during the learning phase."}, {"title": "k-Nearest Neighbors", "content": "k-Nearest Neighbors (k-NN), where k is an integer, is a very simple and intuitive classification algorithm. Its purpose is to classify target points of unknown classes according to their distances from k points of a learning sample whose classes are known in advance. k-NN is so a supervised algorithm. Each point P of the sample is considered as a vector of $\\mathbb{R}^n$ described by its coordinates (p\u2081, p\u2082, ..., p\u2099). In order to determine the class of a target point Q, each of the closest k points to it makes a vote. The class of Q corresponds to the class having the majority of votes. k-NN could use many metrics in a normed vector space to find the closest points to Q, for instance:\n\u2022 the Euclidean distance:\n$d(P,Q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}$\n\u2022 the Manhattan distance, defined by:\n$d(P,Q) = \\sum_{i=1}^{n} |p_i - q_i|$\nwhere j is an integer number such that j\u2265 1. It is a generalization of both the Euclidean distance (j = 2) and the Manhattan distance (j = 1).\n\u2022 the Tchebychev distance, defined by:\n$d(P,Q) = \\max_{i \\in {0,...,n}} (|p_i - q_i|)$\n\u2022 the Canberra distance, defined by:\n$d(P,Q) = \\sum_{i=1}^{n} \\frac{|p_i - q_i|}{|p_i| + |q_i|}$\nThe value of k influences both the accuracy and the performance of the algo-rithm. That is, a small value of k implies that the noise may have a significant impact on the result and a large value makes the calculation expensive. The k value is generally determined based on the empirical results.\nThe k-NN algorithm has many advantages. First, it is simple, intuitive, and well-performing with small datasets. In addition, it makes no particular assumption and does not actually build any model. It also constantly evolves as new training data is collected. However, k-NN becomes slow as the dataset grows. It also does not contend well with imbalanced data. That is, if we have two classes, Normal and Abnormal, and a predominate number of the training data labeled as Normal, then the algorithm strongly prefer to assign individual data points to the Normal class instead of the Abormal one. The k-NN algorithm does not deal with missing values as well."}, {"title": "Artificial Neural Networks", "content": "An Artificial Neural Network (ANN) is a layered model. It is widely used in artificial intelligence. Its structure is very similar to the network of neurons in human brain with layers of linked nodes. A neural network can be trained so that it can recognize patterns, classify data, and predict future outcomes. An artificial neural network, as given by Figure 9, consists of the following layers and components:\n\u2022 an input layer, usually denoted by x;\n\u2022 a hidden layer, with a set of neurons;\n\u2022 an output layer, usually denoted by \u0177;\n\u2022 connections between all the input layer nodes and the hidden layer nodes, and between the hidden layer nodes and the output layer nodes;\n\u2022 a set of weights and biases for each connection; and\n\u2022 an activation function.\nThe training process of an ANN consists in the steps shown in Figure 10."}, {"title": "Forward propagation", "content": "The goal of this step is to calculate the output vector \u0177 from an input vector x. During this step, weights and biases are decided and an activation function is applied to the resulting output. For a basic neural the output is given by the following equation:\n$\\hat{y} = \\sigma(W_2\\sigma(W_1x + b_1) +b_2)$\nwhere $\\sigma$ is an activation function, $W_1$ and $W_2$ are weights, and $b_1$ and $b_2$ are biases. In this step, the information flows in only one direction, from the input layer to the output layer."}, {"title": "Error estimation", "content": "Once the forward propagation is done, one needs to evaluate how good the prediction is. For that, a loss function (also called cost function or objective function) is used to estimate the error between the ground truth y and the predicted output \u0177. Various loss functions [17] can be used for that purpose and their performance depends on the nature of the problem. The most common loss functions are given in Table 1."}, {"title": "Backward propagation", "content": "Once the error is measured by a loss func-tion, one has to define the strategy to propagate it backward in order to adjust weights and biases in a manner that makes the network converge towards the optimal output. The optimal output obviously corresponds to the point where"}, {"title": "Deep Learning techniques", "content": "Deep Learning (DL) is a category of algorithms deriving from artificial neural networks and inspired by the human brain behaviour. The network is made up of tens or even hundreds of layers of neurons, each receiving and interpreting the information from the previous layer. At each step, the wrong answers are returned to the upstream levels to correct the mathematical model. As the program progresses, it reorganizes the information into more complex blocks. When the model is subsequently applied to other cases, it is able to classify data that it has never experienced."}, {"title": "Deep Learning Algorithms", "content": ""}, {"title": "Multilayer perceptron", "content": "The multilayer perceptron, as given by Figure 13, is organized in three parts\n1. The input layer: a set of neurons that carry the input signal;\n2. The hidden layers: they constitute the heart of the perceptron. This is where the relationships between the variables will be established. Choos-ing the right number of neurons per layer and the right number of layers is a difficult problem. However, in general, a small number of layers is sufficient for most problems and a large number of layers often leads to overfitting problems. In practice, there are at least as many neurons per layer as there are of inputs, but this is not always the case; and\n3. The output layer: this layer represents the final result of the network (the prediction)."}, {"title": "Recurring Neural Networks", "content": "Recurring Neural Networks (RNNs) take into account not only the current input they see", "follows": "n$h_t = \\text{tanh}(W_x x_t + U_h h_{t-1} + b_h)$\n$y_t = \\text{tanh}(W_y h_t + b_y)$\nwhere $W_x$, $U_h$ and $W_h,$ are weights and $b_h$ and $b_y$ are biases, all adjusted during the back-propagation time. More complex RNNs are also available such as one-to-many, many-to-one and many-to-many RNNs [19", "20": ".", "21": "."}]}