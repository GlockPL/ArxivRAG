{"title": "Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies", "authors": ["LACHLAN MCGINNESS", "PETER BAUMGARTNER"], "abstract": "This study presents the first examination of the ability of Large Language Models (LLMs) to follow reasoning\nstrategies that are used to guide Automated Theorem Provers (ATPs). We evaluate the performance of GPT4,\nGPT3.5 Turbo and Google's recent Gemini model on problems from a steamroller domain. In addition to\ndetermining accuracy we make use of the Natural Language Processing library spaCy to explore new methods\nof investigating LLM's reasoning capabilities. This led to one alarming result, the low correlation between\ncorrect reasoning and correct answers for any of the tested models. We found that the models' performance\nwhen using the ATP reasoning strategies was comparable to one-shot chain of thought and observe that\nattention to uncertainty in the accuracy results is critical when drawing conclusions about model performance.\nConsistent with previous speculation we confirm that LLMs have a preference for, and are best able to follow,\nbottom up reasoning processes. However, the reasoning strategies can still be beneficial for deriving small and\nrelevant sets of formulas for external processing by a trusted inference engine.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite incrementally improving over the last several decades [22], Natural Language Processing\n(NLP) took the world by a storm in early 2023 with the release of GPT3 through Chat-GPT [10].\nSince then there has been significant hype around Large Language Models (LLMs) and many\ncompanies embedding them into their applications [46] and many people are speculating about\nthe use of LLMs in fields such as law, healthcare and education [9]. One of the main causes of\nexcitement are the emergent properties of LLMs, where they seem to be able to store and recall\nknowledge and even logically reason.\nThere are a number of benchmarks which have been designed to evaluate Large Language Models\n[46]. A number of these tasks are strictly in the NPL domain [8, 27, 31, 37, 43]. However a number\nof these tasks have also been designed to evaluate emergent properties such as mathematical\nreasoning, coding or commonsense reasoning [2, 13, 14, 19, 26, 45].\nThere exist many LLM leaderborads and there has been a strong focus on beating current state of\nthe art performance on these benchmarks, even by very narrow margins. In these scenarios there is\noften no consideration of uncertainty in the presented scores. Also in logical reasoning benchmarks\nthere is often a focus on the correct answers rather than the correct steps in reasoning. Often\nthe computational expense is not explicitly mentioned for LLM performance on logical reasoning\nbenchmarks.\nIn this paper we evaluate three of the highest performing Large Language Models of December\n2023 on PRONTOQA, a logical reasoning benchmark [34]. The models are GPT3.5 Turbo (which we\nwill refer to as GPT3 from here on), GPT-4 and Google's Gemini-Pro. GPT3 and GPT4 were accessed\nthrough Microsoft Azure endpoints. Unlike previous studies, we use prompts to \u2018teach' LLMs to use\nreasoning strategies used to control Automated Theorem Provers (ATPs) and related systems. Such\nstrategies have been devised with reasoning correctness and search space pruning in mind. The\nunderlying research question is whether LLM reasoning can benefit from these strategies. We not\nonly report on the accuracy of the models in such settings, we provide uncertainty values reflecting\nthe range of model performance in our experiments. Additionally we evaluate the reasoning of\nthe models with uncertainty, verify that their reasoning process contains all required steps and\nevaluate how faithfully the models follow the ATP reasoning algorithms."}, {"title": "2 BACKGROUND", "content": "2.1 LLM reasoning\nAlthough Large Language Models were originally developed for next token prediction, they have\nbeen shown to possess a number of emergent properties including arithmetic, question answering\nand forms of complex reasoning [41]. This is a fast moving field with continual release of more\npowerful models and more difficult benchmarks. The field is moving so quickly that on average less\nthan two years pass from the time a benchmark is released until it is redundant because models\nhave reached super-human or near perfect performance [38].\nIn a summary paper Chang et al. suggest four general criteria for LLM performance that should\napply across domains; accuracy, calibration, fairness and robustness [11]. When evaluating LLM\nreasoning, most authors focus on the first of these criteria, accuracy [5, 32]. However obtaining the\ncorrect answer does not necessarily imply that models have used sound reasoning. Models may\nhave seen the exact question and answer within their training data (contamination), have sufficient\nstored knowledge to answer the question without reasoning or simply guess the correct answer.\nIn 2023, Saparov and He published a new dataset, PRONTOQA, specifically designed to allow\nfor assessment of LLM's reasoning process rather than just their accuracy in completing the task\n[34]. This dataset requires models to use deductive reasoning (modus ponens) to answer a true\nor false query. The dataset is flexible with a number of parameters that can be changed including\nthe number of steps of reasoning (hops), the ontology (whether the provided statements are true\nor false in the real world) and distractors (extra statements that are not required to answer the\nquestion). One added bonus of PRONTOQA is that the code which generates the questions is\npublished, but the questions themselves are not. This decreases the chance of contamination and\nmeans that arbitrarily many examples can be generated.\nIn addition to releasing the dataset Saparov and He also evaluate some of the easier problems\nin the data using the GPT3 model [34]. Specifically they investigate whether the accuracy of\nmodels correlates with their ability to generate a strict proof. They discovered that there is a weak\ncorrelation between accuracy and perfect ('golden' chain of thought) reasoning. However there\nis a much stronger correlation between accuracy and two weaker forms of proofs; proofs which\nare allowed to skip steps and proofs which only include valid steps [34]. They conclude that the\nlonger proofs in PRONTOQA are still challenging for Large Language models and notice that the\ninformation in which the facts and rules are displayed has an impact on Model reasoning [34].\nIn 2023, Xu et al. introduced a system for classifying the types of errors that LLMs make in their\nreasoning process [42]. They break the errors into two categories, which contain error sub-types.\nThe first category is an evidence selection error, where a model does not choose the correct initial\nstatements. This is a common problem in Retrieval Augmented Generation (RAG) especially when\nthe desired information is contained in the middle section of a long paragraph of text [24]. There\nare two sub-types of evidence selection errors; hallucination and wrong selection. In our work\nthe model is only provided with a short context window and instead the second error category,\nreasoning process, is more relevant. Xu et al. divide this into three error types: (1) No reasoning\nwhich occur in 19.33% of cases, (2) perspective mistake in 44.47% of cases and (3) process mistake in\n36.20%, but they do not explicitly define the difference between a perspective and process mistake\n[42].\nTo improve performance of a pre-trained model on a reasoning task there are a number of methods\nthat can be employed. Fine tuning is a technique which can increase a model's performance on a very\nspecific task if there are a significant number of training examples to learn from [33]. Fine tuning\nupdates the weights of the model and so requires a significant amount of computational power,\nhowever it is commonly used to improve performance on benchmarks [1, 7, 40, 44]. Another method\nthat can be used is prompt engineering where a human (or computer [35, 36]) phrases the task in\nsuch a way that it will increase the model's performance [10, 23, 29]. Most prompt engineering\nmethods require very few or no examples and no extra computational power for additional training.\nChen et al. and Liu et al. provide an overview of the existing prompt engineering techniques [12, 25]."}, {"title": "2.2 LLM Computational Cost", "content": "Besides the performance of Large Language models, their computational cost, power consumption\nand environmental cost has also been discussed [4, 16]. Although there are methods to measure\nthe computational expense of running an LLM task, it is rare for LLM users who are conducting\nexperiments on their models to report the computational cost at inference time [11, 28]. This\nstands in contrast to many other areas in computer science where understanding (and accurately\nreporting) the computational cost of running an algorithm is critical for determining their scalability,\npracticality and for further development and improvement.\nA number of theoretical and empirical ways of measuring the cost of algorithms are well established\nincluding CPU time, clock cycles, time complexity analysis and space complexity analysis. For\nLarge Language Models there are well established metrics for training including Floating Point\nOperations (FLOPs), parameter counts, training time and energy consumption. For open source\nmodels these metrics are usually published, however not all metrics are made public for state of\nthe art closed models [30, 39]. During inference, the metrics most commonly discussed are those\nrelating to performance such as latency and throughput. In this paper, we suggest and implement a\nsimple measure for the rough computational expense of calling an LLM; the number of completion\ntokens. This allows for a direct comparison of computational resources used when comparing\ndifferent techniques and tasks for a given model. This however does not allow for a fair comparison\nbetween different models as models can have different numbers of parameters."}, {"title": "2.3 ATP Reasoning Strategies", "content": "Two concepts that are routinely used in First Order Logic (FOL) Automated Theorem Prover (ATP)\nsystems are bottom up and top down reasoning, also referred to as forward chaining and backward\nchaining respectively [18, 21]. Bottom up begins a logical deduction process from basic facts and\nrules and iteratively derives conclusions until it has arrived at the answer to a query. By contrast\ntop down reasoning systems start with a query and use rules to recursively derive sub-goals until\nthe sub-goals and query can be proved or disproved by the provided facts. Top down reasoning\nhas been used for guiding the search for a proof starting from the query. Bottom up is the primary\nform of reasoning used by LLMs when performing Chain of Thought (CoT) reasoning [21].\nAs mentioned in Section 2.1, in 2023, Kazemi et al. from the GOOGLE research team were the first\nto attempt to use a top down approach when reasoning with LLMs [21]. To do this they created an\nalgorithm, LAMBADA which calls an LLM at various stages. As part of the study they perform a\nshort preliminary experiment to determine if an LLM can perform CoT reasoning on statements\nwhere the proofs are written backwards but stop short of explicitly teaching an LLM to use top\ndown reasoning."}, {"title": "3 METHODS", "content": "3.1 Experiment Design\nFor our experiments we choose to use problems from the popular steamroller domain. Steamroller\nproblems are toy examples used by philosophers to test their mental capacity and logical reasoning\nabilities. We choose to build on the work done by Saparov and He using the PRONTOQA dataset\n[34]. We extend their work by specifically evaluating the most current state of the art models'\nreasoning capabilities [10, 30, 39]. We accessed the GPT models using a Microsoft Azure API and\naccessed Gemini using a public API.\nWe focus not just on accuracy but also the correctness of their reasoning processes. In order to\nchallenge the models we explore the False Ontology with distractors, the hardest conditions for\nreasoning, which were not tested in their original paper [15, 34].\nFor our baseline we take the score that would be expected what would we get from random guessing.\nThis can be determined by sampling the binomial distribution B(100, 0.5). To find the uncertainty\nwe first sample the distribution three times and take the half range of these values. Then we repeat\nthis process 10,000 times and take an average. The large Language Models were given six different\nqueries corresponding to different techniques, in order to determine their reasoning ability:\n\u2022 Normal - The LLM is given just the question but no further instructions.\n\u2022 Zero-shot CoT - The LLM is instructed to break the problem down into smaller steps and\nexplain its reasoning.\n\u2022 One-shot CoT - The model is instructed to provide its reasoning and given an example of\nhow to reason through the problem (using bottom up technique).\n\u2022 Bottom Up - The model is given explicit instructions for how to perform bottom up reasoning\nand provided with an example.\n\u2022 Top Down - The model is given explicit instructions for how to perform top down reasoning\nand provided with an example.\n\u2022 Magic Set Transformation - The model is given explicit instructions for how to perform\nmagic set transformation reasoning and provided with an example.\nWe tested between two and ten prompts for each of these conditions. The prompt which yielded\nthe highest accuracy was kept. Appendix B contains the exact prompts used for each of these\nconditions.\nAnswer correctness. For each experimental condition, we first determine whether the model correctly\nanswered the query. In measuring accuracy we assume default negation; if the model refused to\ngive an answer the question, or stated that the query could not be answered then its answer is taken\nas false. The number of correct answers that the model gave were used to calculate an accuracy\nscore.\nProcess hard correctness. In addition to determining the accuracy of the model for each technique,\nwe also investigate whether the model used correct reasoning to answer the questions. Even when\nprompted to use reasoning, models occasionally simply answer True or False, which means that no\nreasoning has been used. Furthermore we investigate whether the model showed all of the required\nsteps that would be needed by a human or ATP to prove the answer to the query. In addition to\nthese we also specifically look for whether the models were able to follow the prescribed process,\nas outlined in Section 3.2.\nProcess soft correctness. Furthermore, we investigated a softer version of process correctness. Process\nsoft correctness also considers the facts and rules in the model response and compares them to\nground truth. However, the comparison is set-based, i.e., it ignores the order in which the statements\nappear. From this we determine the degrees of completeness and conciseness in the model response\ncompared to the ground truth. We also experimented with a \u2018fact only' variant which only compares\nthe facts, given and derived. The rationale for this is that facts alone should provide a reliable proxy"}, {"title": "3.2 Error Types", "content": "We build on the error categories proposed by Xu et al. and propose a more specific system that\ncould be used when evaluating LLMs on deductive reasoning [42]. The error categories described\nin Table 1 provide insight into the reasoning capabilities of the Large Language Models. Note that\nour 'No Reasoning' error classification aligns perfectly with the corresponding category from Xu et\nal.'s classification. Our \u2018Missing steps' category would roughly align with the 'perspective mistake'\nand 'process mistake' error types. Our \u2018Wrong Method' category does not correspond to an error\ntype per se, but instead is a measure of whether the model followed an ATP strategy as instructed."}, {"title": "3.3 Natural Language Analysis", "content": "As stated in the background section, Saparov and He [34] parse the LLM statements with a recursive-\ndescent parser using the simple grammar where unparseable proofs are marked as incorrect. The\nmethod of using a strict grammar is likely to miss many examples of natural language. To accept\na wider variety of natural language expressions we integrate into our workflow the open source\nnatural language processing library, spaCy [20]. We use spaCy for shallow parsing the problem\ndescription, ground truth reasoning, and model responses. We also used co-reference resolution,\nwhich is frequently needed for correctly parsing the model response (we installed the \u2018coreferee'\nextension library for that). SpaCy's built-in rule language was instrumental for extracting facts and\nrules from the part-of-speech tagged documents. We wrote rules that are conditioned on tags for\nnamed entities, lemmas and grammatical roles such as nouns, adjectives and adverbs. The rules\ncompute the facts and rules in the document expressed as semantic triples over canonical word\nrepresentations.\nAs an example take the following excerpt from an actual model response:\nStatement 5: lepidopterans are not hot.\nStatement 8: lepidopteran are not sunny.\nBased on the analysis of the statements, we can conclude that Sally\nis a lepidopteran (statement 5), which means she is not hot and not\nsunny (statement 8)."}, {"title": "3.4 Uncertainty", "content": "Unlike some other areas of computer science which have algorithms with very consistent per-\nformance, LLM's performance is often random and can vary between similar tasks without good\nreason. For noisy scenarios like this, we believe that it is important to include uncertainty when\nreporting a model accuracy. Natural variation in measurements is commonplace in the physical\nsciences and the well established techniques from these fields [6, 17].\nTaking repeat trails and the reporting of uncertainty in measurements is not always standard\npractice in all areas of computer science. We would like to argue for its importance in cases where\nvariation naturally occurs in measurements. Consider the issue for readers who would like to\ngeneralise a newly published innovation for a task similar to the original published benchmarks. If\nonly an accuracy is provided with no repeat trials or variation then they don't know how much\nvariation they should when implementing this technique. Therefore we propose that a benchmark\nshould be broken into multiple trials and then an average value and an uncertainty provided rather\nthan just a single number.\nAs an example let's consider an example with two scenarios. In the first scenario, 1000 examples\nin a benchmark as split into 10 trials each which have 100 examples; so each example could be\nreported with an accuracy of 1%. Then the average could be calculated and the standard deviation\nacross the 10 trials could be reported as the uncertainty or spread in the results. In the second\nscenario the are conducted for example accuracy could be reported to 0.1% with no uncertainty\nvalues. We may think that the second approach provides more information as the accuracy value is\nmore precise.\nOur counter argument is this accuracy value would be exactly the same as the average accuracy\ncalculated in the first scenario. However any reader or user who is interested in the results will not\nknow if the model performance reported to 0.1% accuracy us a \u2018fluke' or whether they can rely on\nthe model to consistently produce this result.\nWe believe that this is incredibly important in case of leader-boards. For example is an accuracy of\n89.2% significantly better than an accuracy of 89.1%? This highly depends on the uncertainty; if\nboth models have a variation of approximately 5% then the two techniques have no real difference\nin performance, one could have just been \u2018lucky' that on this specific problem its accuracy was\nmarginally higher. Uncertainty will help researchers to know if a new innovation is really valuable\nand making a significant impact over existing techniques.\nThere are different techniques that could be used to calculate uncertainty in measurements. When\nthere are relatively large numbers of repeat trials (n = 10+) we recommend using the standard\ndeviation as a good estimate of the uncertainty. In situations where there are a smaller number of\nrepeat trials (n = 3 \u2013 10) we recommend using either half of the range. In situations where there\nare two values the entire range could be reported. Regardless of the method chosen, we believe it is\nimportant that authors make explicit their method for calculating uncertainty and the number of\ntrials they conducted.\nIn our experiments, each condition has three similar tasks (trials) of approximately n = 100 problems.\nAs the hosted and public models are regularly updated it was essential to conduct our experiments\nwithin a short time window to make sure that there were minimal updates to the model during the\nrunning of the experiment which could result in an unfair comparison of the different techniques."}, {"title": "4 RESULTS", "content": "4.1 Answer Correctness\nAppendix A contains the results from the preliminary experiments that we performed to determine\nthe best settings for the PRONTOQA dataset for our experiments. Table 2 shows the accuracy for\neach of the experiments. The average number of completion tokens for each condition is given\nas a very rough measure of computational expense within each column. As expected the normal\ncondition where models are expected to produce simply a True or False answer have the least\ncompletion tokens but also the lowest performance. The ATP reasoning strategies generally require\na larger number of completion tokens, especially for GPT4 and Gemini."}, {"title": "4.2 Process Hard Correctness", "content": "Figure 3 contains two distinct graphs. The first graph illustrates the portion of cases where the\nmodel included some reasoning. For all conditions except \u2018normal', all models consistently included\nsome reasoning, even if it was not correct. In the normal condition models were not asked to show\nworking and only GPT3 regularly included some justification/reasoning for its answer."}, {"title": "4.3 Process Soft Correctness", "content": "As introduced in Section 3, process soft correctness relates the facts and rules given as ground\ntruth (gt) with those found in the model response (mr). These sets are found by natural language\nanalyses as explained in Section 3.3. We define measures of recall and precision as follows:\nrecall(gt, mr) = |gt \u2229 mr|/|gt|\nprecision(gt, mr) = |gt \u2229 mr|/|mr|\nWith recall we measure the degree of the ground truth (completeness) discovered in the model\nresponse, and with precision we measure a degree of redundancy (inverse of conciseness) of the\nfacts and rules in the model response compared to the ground truth.\nWe distinguish two versions of building the sets gt and mr. The full version comprises both facts\nand rules, as explained above, whereas in the facts only version only the facts are retained. As\nindicated in Section 3, the rationale is that facts by themselves are a sharper proxy for assessing\nthe reasoning carried out by a model.\nWe computed recall and precision statistics for all models and techniques except the \u2018normal'\ntechnique because the models are not expected to produce facts and rules for this condition. We\nfiltered out runs with mr = 0 because recall and precision is meaningless in this case. We found that\nthe results are similar for the full and the facts only versions for gt and mr. However, as expected,\nthey are somewhat more pronounced in the facts only version and we report on these results only.\nWe investigated how sensitive the statistics are to certain parameter settings that we thought could\nhave an impact. We distinguished fixing \u2018number of hops' to 1, 2 or 3; whether the query is negated\n(\"True or false: Sally is not hot.\") or not; and whether the correct answer is 'true' or is\n'false'. We found that these parameters have little impact on the statistics even in combination. A\nparameter that we found significant, though, is whether the question was answered correctly or\nnot.\nWith these considerations and investigations we collected results for all runs classified by model,\ntechnique, and whether a run correctly answered the question. Table 3 provides a summary. Notice\nthat recall and precision are stated as the mean and standard deviation over all runs in a class."}, {"title": "5 DISCUSSION", "content": "The overall accuracy results (Table 2) show that teaching models to use explicit reasoning strategies\nproduces a performance which is comparable with on-shot CoT reasoning within uncertainty.\nAlthough the accuracy is similar between explicit reasoning strategies and CoT, the completion\ntokens (and therefore computation expense) is more than 200% higher for all explicit reasoning\nstrategies when used by GPT4 and Gemini. Both these completion token values and Figure 3 show\nthat GPT4 and Gemini are very concise and rarely give reasoning when not asked. GPT3 on the\nother hand was more likely to provide justification even when this was not requested.\nAlthough models would consistently produce some reasoning (or \u2018working') when asked, Figure 3\nshows that this reasoning did not always contain all of the required steps. This can occur because\nthe model skips steps or because the model was unable to complete a reasoning process and arrive\nat an answer. We note that whether the model was able to include all reasoning steps or not\nwas largely independent of the type of prompt used, excepting the normal condition. GPT3 was\nconsistently unable to produce all of the required reasoning steps without at least one example in\nthe prompt.\nConsistent with previous literature, we show that models have a preference for following a bottom\nup style reasoning procedure [21]. At a high level, the bottom up approach works well for LLMs\nbecause the intermediate steps involve more writing of more facts. This essentially provides place\nfor the model to think. Top down reasoning is particularly difficult for LLMs because it involves\nbacktracking and keeping track of which paths have already been explored. The magic set approach\nis also more difficult for LLMs because it has more steps in the process where the models could\nmake a mistakes.\nOne very interesting finding is the lack of a correlation between including all of the required\nreasoning to prove the final answer and then obtaining the correct answer itself. Figure 5 shows\nthat the correlation between having the correct reasoning and the correct answer is less than 0.3 in\nall cases and close to 0 in half of the cases. This result is significant and has significant implications\nfor the trustworthiness of LLMs as it shows that even if a Large Language Model can demonstrate\nall of the correct reasoning it still may not obtain the correct answer. It also shows that a Large\nLanguage model is able to obtain the correct answer even in cases where it skips steps in the\nworking/proof.\nRegarding process soft correctness, the results in Table 3 indicate a correlation between recall and\ncorrectly answering a problem. Across almost all classes, recall is higher for correctly answered\nquestions than for incorrectly answered questions. (The biggest difference is for \u2018Gemini-Pro' and\n\u2018MagicSet', with recall values of 0.82 vs. 0.58.) This allows us to conclude that correct answers tend\nto be produced along with the facts relevant for deriving the result. However, the converse is not\ngenerally true, that instructing a model to derive relevant facts helps in computing a correct answer.\nThis can be seen by observing that the strongest guided techniques, \u2018MagicSet' and 'TopDown'\ndo not consistently produce a significantly higher share of correct answers than the non-guided\ntechniques. Nevertheless it is interesting to see that \u2018MagicSet' delivers comparatively high recall\nand high precision rates. This suggests, as future work, to consider a hybrid architecture where\ntechniques like 'MagicSet' could be used to extract a small set of relevant facts that can be handed\noff to a trusted inference engine for actually computing a correct answer."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "This research is the first to determine the ability of LLMs to use ATP reasoning strategies. We\nevaluate the performance of three of the best available models on steamroller problems using the\nmost difficult settings of the PRONTOQA Benchmark. In terms of accuracy the models' performance\nwhen using the ATP reasoning strategies was similar to one-shot CoT, despite the computational\nexpense of executing the bottom up, top down and magic set reasoning being significantly higher\nthan the model's natural reasoning. We found that LLMs have a preference for using bottom up\nprocesses.\nWe build on previous studies which have used recursive-descent parsers by introducing the existing\nrobust Natural Language Processing library, spaCy to this domain of study. SpaCy provides useful\nout of the box functionality for co-referencing, named entity recognition, point of speech tagging\nand a built in rule language. This enabled the reliable translation of the results into a consistent\nformat for parsing and and evaluation. The use of these NLP toos allowed us to scalably dissect\nand evaluate the models' reasoning processes.\nThis allowed us to discover that models were able to \u2018skip steps' and obtain correct answers without\nshowing all of the stepts required for a proof. We also found that models were able to include all of\nthe steps required for a proof in the correct order, but still give the incorrect answer to a question.\nOverall, there was little correlation between correct reasoning and correct answers for all three of\nthe models. This has a negative implication for the explainability of LLMs, even with current state\nof the art models, their reasoning cannot be trusted.\nIn terms of overall model comparison, GPT4 had systematically higher accuracy than either GPT3\nand Gemini. It was also able to more consistently generate correct reasoning and include all required\nsteps for a logical proof. Gemini and GPT4 were also found to be more concise than GPT3; on\naverage they used less completion tokens for a given prompt. We were unable to compare the\ncomputational expense of different models as key information such as the number of parameters of\neach model, was not available.\nAs area for future work we propose to investigate how LLMs reason on tasks with a greater number\nof rules and facts. As research in ATP has shown, it is only when problems reach a certain size that\nthe choice of reasoning strategy becomes relevant. It is unclear where such a threshold would be\nfor LLM reasoning. There may be a point where the benefits of top down reasoning approaches\noutweigh the model's natural tendency to perform bottom up reasoning.\nOne area that also deserves more exploration in this field is the use of neuro-symbolic approaches\nwhich combine LLMs with ATP systems. Traditionally ATP systems have been limited by their\nrigidity and the need to manually input information. If an LLM is able to relatively accurately\naccomplish this manual labour then the logical reasoning could be \u2018outsourced to the ATP systems\nallowing for excellent explainability and interpretability. This naturally lends itself to problems\nwhere justifications and explanations are required such as education, healthcare, law and banking."}, {"title": "A APPENDIX - PRELIMINARY EXPERIMENTS", "content": "Table 4 contains results from preliminary experiments in order to determine the most appropriate\nPRONTOQA parameter settings for the experiments. For experimental conditions without distrac-\ntors we tested with one to five hops (therefore five trials); experimental conditions with distractors\nhave one to three hops (three trials). One hundred problems were included in each trial.\nThis preliminary experiment shows that even the weaker model GPT3, shows ceiling effects on the\ntasks with the True ontology. To best prevent ceiling effects with the stronger GPT4 model, we\nchose to use the most difficult condition, False Ontology with Distractors. We would like to point\nout that a merit of the fictional ontology is that there is less chance of the model being able to use\nexternal knowledge to solve the problem rather than going through a reasoning process. However\nif contamination has occurred, the fictional ontology is likely to be more vulnerable because it\ncontains combinations of tokens that would only be seen in this data set.\nWe also performed an experiment to determine the suitability of using different numbers of hops\nas repeat trials. In Table 5 we present the results. For problems without distractors changing the\nnumber of hops makes causes very little variation in the model performance. We attribute this to\nthe same cause as the authors of the dataset [34], who claim that in these cases the model can avoid\nthe task of reasoning by instead performing an alternative task with the same result. When there\nare no distractors the answer can easily be obtained by simply counting the number of times \u2018not'\nappears in the problem statement."}, {"title": "B APPENDIX - PROMPTS", "content": "For each of the prompt techniques shown below three sets (1 hop", "below": "n{question"}, "Each sheep is sunny. Each sheep is a feline. Sheep are mammals. Felines\nare aggressive. Every feline is a snake. Felines are carnivores. Each snake is luminous.\nSnakes are cats. Every dog is not luminous. Each snake is an animal. Animals are\nfast. Carnivores are opaque. Each mammal is floral. Each vertebrate is not feisty. Each\nvertebrate is a cow. Alex is a sheep. Alex is a vertebrate.", "n{query} = \u201cTrue or false: Alex is luminous.\u201d\nThe prompt for the normal condition was as follows - Average number of prompt tokens is 110.5\nfor GPT3 and GPT 4 and 94.2 for Gemini:\nprompt = \"{question} {query}\u201d\nChain of thought prompt. - Average number of prompt tokens is 162.3 for GPT and 148 for Gemini:\nprompt = \u201cConsider the following statements and the given query. Use your reasoning\nskills to determine if the query is true or false based on the statements. Explain your\nthought process step by step as you analyze the relationship between the statements\nand the query.\nStatements: {question}\nQuery: {query}\"\nChain of thought and one-shot - Average number of prompt tokens is 301.3 for GPT and 283 for\nGemini:\nprompt = \"Consider the following statements and the given query. Use your reasoning\nskills to determine if the query is true or false based on the statements. Follow the\nformat of the example question that follows.\nExample Statements: \u2018All cats are birds. No bird swims. Whiskers is a cat.\u2019\nQuery: 'True or false: Whiskers swims.'\nExample Reasoning: \u2018Let's figure out if Whiskers swims. This is not provided directly in\nthe statements. However it does state that Whiskers is a cat. Then it states that all cats\nare birds. Therefore Whiskers is a bird. It then states that no bird swims. Since Whiskers\nis a bird this means that Whiskers does not swim. Therefore the query \u201cWhiskers Swims\u201d\nis false.\u2019\nExplain your thought process step by step as you analyze the relationship between the\nstatements and the query.\nStatements: {question}\nQuery: {query}\"\nBottom up prompt - Average number of prompt tokens is 510.3 for GPT and 496.7 for Gemini:\nprompt = \u201cConsider the following statements, which include rules and then facts, along\nwith the given query. Facts are have a specific named instance. For example \u201cSam is\na cat\u201d is a fact because an explicit name \u201cSam\u201d is given. Rules establish a connection\nbetween two general classes without referring to a specific instance. For example \u201cCats\nare birds\u201d is rule, not a fact because there is no named instance of a specific cat or bird.\nModus ponens can be used to apply a rule to a fact. For example applying \u201cCats are\nbirds\u201d to \u201cSam is a cat\u201d gives a new fact \u201cSam is a bird\u201d. To use bottom-up reasoning:\n- List the rules which do not contain specific named instances.\n- List all given facts with a specific named instance as a current facts.\nThen for each rule do the following:\n1.) Examine each rule to see if its premise applies to any of the current facts. It's\nimportant to recognize that general rules can apply to specific facts. For example, the\\"]}