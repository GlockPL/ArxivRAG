{"title": "Steamroller Problems: An Evaluation of LLM Reasoning Capability with Automated Theorem Prover Strategies", "authors": ["LACHLAN MCGINNESS", "PETER BAUMGARTNER"], "abstract": "This study presents the first examination of the ability of Large Language Models (LLMs) to follow reasoning strategies that are used to guide Automated Theorem Provers (ATPs). We evaluate the performance of GPT4, GPT3.5 Turbo and Google's recent Gemini model on problems from a steamroller domain. In addition to determining accuracy we make use of the Natural Language Processing library spaCy to explore new methods of investigating LLM's reasoning capabilities. This led to one alarming result, the low correlation between correct reasoning and correct answers for any of the tested models. We found that the models' performance when using the ATP reasoning strategies was comparable to one-shot chain of thought and observe that attention to uncertainty in the accuracy results is critical when drawing conclusions about model performance. Consistent with previous speculation we confirm that LLMs have a preference for, and are best able to follow, bottom up reasoning processes. However, the reasoning strategies can still be beneficial for deriving small and relevant sets of formulas for external processing by a trusted inference engine.", "sections": [{"title": "1 INTRODUCTION", "content": "Despite incrementally improving over the last several decades [22], Natural Language Processing (NLP) took the world by a storm in early 2023 with the release of GPT3 through Chat-GPT [10]. Since then there has been significant hype around Large Language Models (LLMs) and many companies embedding them into their applications [46] and many people are speculating about the use of LLMs in fields such as law, healthcare and education [9]. One of the main causes of excitement are the emergent properties of LLMs, where they seem to be able to store and recall knowledge and even logically reason.\nThere are a number of benchmarks which have been designed to evaluate Large Language Models [46]. A number of these tasks are strictly in the NPL domain [8, 27, 31, 37, 43]. However a number of these tasks have also been designed to evaluate emergent properties such as mathematical reasoning, coding or commonsense reasoning [2, 13, 14, 19, 26, 45].\nThere exist many LLM leaderborads and there has been a strong focus on beating current state of the art performance on these benchmarks, even by very narrow margins. In these scenarios there is often no consideration of uncertainty in the presented scores. Also in logical reasoning benchmarks there is often a focus on the correct answers rather than the correct steps in reasoning. Often the computational expense is not explicitly mentioned for LLM performance on logical reasoning benchmarks.\nIn this paper we evaluate three of the highest performing Large Language Models of December 2023 on PRONTOQA, a logical reasoning benchmark [34]. The models are GPT3.5 Turbo (which we will refer to as GPT3 from here on), GPT-4 and Google's Gemini-Pro. GPT3 and GPT4 were accessed through Microsoft Azure endpoints. Unlike previous studies, we use prompts to \u2018teach' LLMs to use reasoning strategies used to control Automated Theorem Provers (ATPs) and related systems. Such strategies have been devised with reasoning correctness and search space pruning in mind. The underlying research question is whether LLM reasoning can benefit from these strategies. We not only report on the accuracy of the models in such settings, we provide uncertainty values reflecting the range of model performance in our experiments. Additionally we evaluate the reasoning of the models with uncertainty, verify that their reasoning process contains all required steps and evaluate how faithfully the models follow the ATP reasoning algorithms."}, {"title": "2 BACKGROUND", "content": "2.1 LLM reasoning\nAlthough Large Language Models were originally developed for next token prediction, they have been shown to possess a number of emergent properties including arithmetic, question answering and forms of complex reasoning [41]. This is a fast moving field with continual release of more powerful models and more difficult benchmarks. The field is moving so quickly that on average less than two years pass from the time a benchmark is released until it is redundant because models have reached super-human or near perfect performance [38].\nIn a summary paper Chang et al. suggest four general criteria for LLM performance that should apply across domains; accuracy, calibration, fairness and robustness [11]. When evaluating LLM reasoning, most authors focus on the first of these criteria, accuracy [5, 32]. However obtaining the correct answer does not necessarily imply that models have used sound reasoning. Models may have seen the exact question and answer within their training data (contamination), have sufficient stored knowledge to answer the question without reasoning or simply guess the correct answer.\nIn 2023, Saparov and He published a new dataset, PRONTOQA, specifically designed to allow for assessment of LLM's reasoning process rather than just their accuracy in completing the task [34]. This dataset requires models to use deductive reasoning (modus ponens) to answer a true or false query. The dataset is flexible with a number of parameters that can be changed including the number of steps of reasoning (hops), the ontology (whether the provided statements are true or false in the real world) and distractors (extra statements that are not required to answer the question). One added bonus of PRONTOQA is that the code which generates the questions is published, but the questions themselves are not. This decreases the chance of contamination and means that arbitrarily many examples can be generated.\nIn addition to releasing the dataset Saparov and He also evaluate some of the easier problems in the data using the GPT3 model [34]. Specifically they investigate whether the accuracy of models correlates with their ability to generate a strict proof. They discovered that there is a weak correlation between accuracy and perfect ('golden' chain of thought) reasoning. However there is a much stronger correlation between accuracy and two weaker forms of proofs; proofs which are allowed to skip steps and proofs which only include valid steps [34]. They conclude that the longer proofs in PRONTOQA are still challenging for Large Language models and notice that the information in which the facts and rules are displayed has an impact on Model reasoning [34].\nIn 2023, Xu et al. introduced a system for classifying the types of errors that LLMs make in their reasoning process [42]. They break the errors into two categories, which contain error sub-types. The first category is an evidence selection error, where a model does not choose the correct initial statements. This is a common problem in Retrieval Augmented Generation (RAG) especially when the desired information is contained in the middle section of a long paragraph of text [24]. There are two sub-types of evidence selection errors; hallucination and wrong selection. In our work the model is only provided with a short context window and instead the second error category, reasoning process, is more relevant. Xu et al. divide this into three error types: (1) No reasoning which occur in 19.33% of cases, (2) perspective mistake in 44.47% of cases and (3) process mistake in 36.20%, but they do not explicitly define the difference between a perspective and process mistake [42].\nTo improve performance of a pre-trained model on a reasoning task there are a number of methods that can be employed. Fine tuning is a technique which can increase a model's performance on a very specific task if there are a significant number of training examples to learn from [33]. Fine tuning updates the weights of the model and so requires a significant amount of computational power, however it is commonly used to improve performance on benchmarks [1, 7, 40, 44]. Another method that can be used is prompt engineering where a human (or computer [35, 36]) phrases the task in such a way that it will increase the model's performance [10, 23, 29]. Most prompt engineering methods require very few or no examples and no extra computational power for additional training. Chen et al. and Liu et al. provide an overview of the existing prompt engineering techniques [12, 25]."}, {"title": "2.2 LLM Computational Cost", "content": "Besides the performance of Large Language models, their computational cost, power consumption and environmental cost has also been discussed [4, 16]. Although there are methods to measure the computational expense of running an LLM task, it is rare for LLM users who are conducting experiments on their models to report the computational cost at inference time [11, 28]. This stands in contrast to many other areas in computer science where understanding (and accurately reporting) the computational cost of running an algorithm is critical for determining their scalability, practicality and for further development and improvement.\nA number of theoretical and empirical ways of measuring the cost of algorithms are well established including CPU time, clock cycles, time complexity analysis and space complexity analysis. For Large Language Models there are well established metrics for training including Floating Point Operations (FLOPs), parameter counts, training time and energy consumption. For open source models these metrics are usually published, however not all metrics are made public for state of the art closed models [30, 39]. During inference, the metrics most commonly discussed are those relating to performance such as latency and throughput. In this paper, we suggest and implement a simple measure for the rough computational expense of calling an LLM; the number of completion tokens. This allows for a direct comparison of computational resources used when comparing different techniques and tasks for a given model. This however does not allow for a fair comparison between different models as models can have different numbers of parameters."}, {"title": "2.3 ATP Reasoning Strategies", "content": "Two concepts that are routinely used in First Order Logic (FOL) Automated Theorem Prover (ATP) systems are bottom up and top down reasoning, also referred to as forward chaining and backward chaining respectively [18, 21]. Bottom up begins a logical deduction process from basic facts and rules and iteratively derives conclusions until it has arrived at the answer to a query. By contrast top down reasoning systems start with a query and use rules to recursively derive sub-goals until the sub-goals and query can be proved or disproved by the provided facts. Top down reasoning has been used for guiding the search for a proof starting from the query. Bottom up is the primary form of reasoning used by LLMs when performing Chain of Thought (CoT) reasoning [21].\nAs mentioned in Section 2.1, in 2023, Kazemi et al. from the GOOGLE research team were the first to attempt to use a top down approach when reasoning with LLMs [21]. To do this they created an algorithm, LAMBADA which calls an LLM at various stages. As part of the study they perform a short preliminary experiment to determine if an LLM can perform CoT reasoning on statements where the proofs are written backwards but stop short of explicitly teaching an LLM to use top down reasoning."}, {"title": "3 METHODS", "content": "3.1 Experiment Design\nFor our experiments we choose to use problems from the popular steamroller domain. Steamroller problems are toy examples used by philosophers to test their mental capacity and logical reasoning abilities. We choose to build on the work done by Saparov and He using the PRONTOQA dataset [34]. We extend their work by specifically evaluating the most current state of the art models' reasoning capabilities [10, 30, 39]. We accessed the GPT models using a Microsoft Azure API and accessed Gemini using a public API.\nWe focus not just on accuracy but also the correctness of their reasoning processes. In order to challenge the models we explore the False Ontology with distractors, the hardest conditions for reasoning, which were not tested in their original paper [15, 34].\nFor our baseline we take the score that would be expected what would we get from random guessing. This can be determined by sampling the binomial distribution B(100, 0.5). To find the uncertainty we first sample the distribution three times and take the half range of these values. Then we repeat this process 10,000 times and take an average. The large Language Models were given six different queries corresponding to different techniques, in order to determine their reasoning ability:\n\u2022 Normal - The LLM is given just the question but no further instructions.\n\u2022 Zero-shot CoT - The LLM is instructed to break the problem down into smaller steps and explain its reasoning.\n\u2022 One-shot CoT - The model is instructed to provide its reasoning and given an example of how to reason through the problem (using bottom up technique).\n\u2022 Bottom Up - The model is given explicit instructions for how to perform bottom up reasoning and provided with an example.\n\u2022 Top Down - The model is given explicit instructions for how to perform top down reasoning and provided with an example.\n\u2022 Magic Set Transformation - The model is given explicit instructions for how to perform magic set transformation reasoning and provided with an example.\nWe tested between two and ten prompts for each of these conditions. The prompt which yielded the highest accuracy was kept. Appendix B contains the exact prompts used for each of these conditions.\nAnswer correctness. For each experimental condition, we first determine whether the model correctly answered the query. In measuring accuracy we assume default negation; if the model refused to give an answer the question, or stated that the query could not be answered then its answer is taken as false. The number of correct answers that the model gave were used to calculate an accuracy score.\nProcess hard correctness. In addition to determining the accuracy of the model for each technique, we also investigate whether the model used correct reasoning to answer the questions. Even when prompted to use reasoning, models occasionally simply answer True or False, which means that no reasoning has been used. Furthermore we investigate whether the model showed all of the required steps that would be needed by a human or ATP to prove the answer to the query. In addition to these we also specifically look for whether the models were able to follow the prescribed process, as outlined in Section 3.2.\nProcess soft correctness. Furthermore, we investigated a softer version of process correctness. Process soft correctness also considers the facts and rules in the model response and compares them to ground truth. However, the comparison is set-based, i.e., it ignores the order in which the statements appear. From this we determine the degrees of completeness and conciseness in the model response compared to the ground truth. We also experimented with a \u2018fact only' variant which only compares the facts, given and derived. The rationale for this is that facts alone should provide a reliable proxy for the amount and quality of the reasoning process. Derived facts are not present in the problem statement and for these to appear in the response the model must have successfully carried out an inference. In contrast, rules could just be cited in a model response, without any reasoning."}, {"title": "3.2 Error Types", "content": "We build on the error categories proposed by Xu et al. and propose a more specific system that could be used when evaluating LLMs on deductive reasoning [42]. The error categories described in Table 1 provide insight into the reasoning capabilities of the Large Language Models. Note that our 'No Reasoning' error classification aligns perfectly with the corresponding category from Xu et al.'s classification. Our \u2018Missing steps' category would roughly align with the 'perspective mistake' and 'process mistake' error types. Our \u2018Wrong Method' category does not correspond to an error type per se, but instead is a measure of whether the model followed an ATP strategy as instructed."}, {"title": "3.3 Natural Language Analysis", "content": "As stated in the background section, Saparov and He [34] parse the LLM statements with a recursive-descent parser using the simple grammar where unparseable proofs are marked as incorrect. The method of using a strict grammar is likely to miss many examples of natural language. To accept a wider variety of natural language expressions we integrate into our workflow the open source natural language processing library, spaCy [20]. We use spaCy for shallow parsing the problem description, ground truth reasoning, and model responses. We also used co-reference resolution, which is frequently needed for correctly parsing the model response (we installed the \u2018coreferee' extension library for that). SpaCy's built-in rule language was instrumental for extracting facts and rules from the part-of-speech tagged documents. We wrote rules that are conditioned on tags for named entities, lemmas and grammatical roles such as nouns, adjectives and adverbs. The rules compute the facts and rules in the document expressed as semantic triples over canonical word representations.\nAs an example take the following excerpt from an actual model response:\nStatement 5: lepidopterans are not hot.\nStatement 8: lepidopteran are not sunny.\nBased on the analysis of the statements, we can conclude that Sally is a lepidopteran (statement 5), which means she is not hot and not sunny (statement 8)."}, {"title": "3.4 Uncertainty", "content": "Unlike some other areas of computer science which have algorithms with very consistent per-formance, LLM's performance is often random and can vary between similar tasks without good reason. For noisy scenarios like this, we believe that it is important to include uncertainty when reporting a model accuracy. Natural variation in measurements is commonplace in the physical sciences and the well established techniques from these fields [6, 17].\nTaking repeat trails and the reporting of uncertainty in measurements is not always standard practice in all areas of computer science. We would like to argue for its importance in cases where variation naturally occurs in measurements. Consider the issue for readers who would like to generalise a newly published innovation for a task similar to the original published benchmarks. If only an accuracy is provided with no repeat trials or variation then they don't know how much variation they should when implementing this technique. Therefore we propose that a benchmark should be broken into multiple trials and then an average value and an uncertainty provided rather than just a single number.\nAs an example let's consider an example with two scenarios. In the first scenario, 1000 examples in a benchmark as split into 10 trials each which have 100 examples; so each example could be reported with an accuracy of 1%. Then the average could be calculated and the standard deviation across the 10 trials could be reported as the uncertainty or spread in the results. In the second scenario the are conducted for example accuracy could be reported to 0.1% with no uncertainty values. We may think that the second approach provides more information as the accuracy value is more precise.\nOur counter argument is this accuracy value would be exactly the same as the average accuracy calculated in the first scenario. However any reader or user who is interested in the results will not know if the model performance reported to 0.1% accuracy us a \u2018fluke' or whether they can rely on the model to consistently produce this result.\nWe believe that this is incredibly important in case of leader-boards. For example is an accuracy of 89.2% significantly better than an accuracy of 89.1%? This highly depends on the uncertainty; if both models have a variation of approximately 5% then the two techniques have no real difference in performance, one could have just been \u2018lucky' that on this specific problem its accuracy was marginally higher. Uncertainty will help researchers to know if a new innovation is really valuable and making a significant impact over existing techniques.\nThere are different techniques that could be used to calculate uncertainty in measurements. When there are relatively large numbers of repeat trials (n = 10+) we recommend using the standard deviation as a good estimate of the uncertainty. In situations where there are a smaller number of repeat trials (n = 3 \u2013 10) we recommend using either half of the range. In situations where there are two values the entire range could be reported. Regardless of the method chosen, we believe it is important that authors make explicit their method for calculating uncertainty and the number of trials they conducted.\nIn our experiments, each condition has three similar tasks (trials) of approximately n = 100 problems. As the hosted and public models are regularly updated it was essential to conduct our experiments within a short time window to make sure that there were minimal updates to the model during the running of the experiment which could result in an unfair comparison of the different techniques."}, {"title": "4 RESULTS", "content": "4.1 Answer Correctness\nAppendix A contains the results from the preliminary experiments that we performed to determine the best settings for the PRONTOQA dataset for our experiments. Table 2 shows the accuracy for each of the experiments. The average number of completion tokens for each condition is given as a very rough measure of computational expense within each column. As expected the normal condition where models are expected to produce simply a True or False answer have the least completion tokens but also the lowest performance. The ATP reasoning strategies generally require a larger number of completion tokens, especially for GPT4 and Gemini.\n4.2 Process Hard Correctness\nFigure 3 contains two distinct graphs. The first graph illustrates the portion of cases where the model included some reasoning. For all conditions except \u2018normal', all models consistently included some reasoning, even if it was not correct. In the normal condition models were not asked to show working and only GPT3 regularly included some justification/reasoning for its answer."}, {"title": "4.3 Process Soft Correctness", "content": "As introduced in Section 3, process soft correctness relates the facts and rules given as ground truth (gt) with those found in the model response (mr). These sets are found by natural language analyses as explained in Section 3.3. We define measures of recall and precision as follows:\nrecall(gt, mr) = \\frac{|gt \u2229 mr|}{|gt|}\nprecision(gt, mr) = \\frac{|gt \u2229 mr|}{|mr|}\nWith recall we measure the degree of the ground truth (completeness) discovered in the model response, and with precision we measure a degree of redundancy (inverse of conciseness) of the facts and rules in the model response compared to the ground truth.\nWe distinguish two versions of building the sets gt and mr. The full version comprises both facts and rules, as explained above, whereas in the facts only version only the facts are retained. As indicated in Section 3, the rationale is that facts by themselves are a sharper proxy for assessing the reasoning carried out by a model.\nWe computed recall and precision statistics for all models and techniques except the \u2018normal' technique because the models are not expected to produce facts and rules for this condition. We filtered out runs with mr = 0 because recall and precision is meaningless in this case. We found that the results are similar for the full and the facts only versions for gt and mr. However, as expected, they are somewhat more pronounced in the facts only version and we report on these results only.\nWe investigated how sensitive the statistics are to certain parameter settings that we thought could have an impact. We distinguished fixing \u2018number of hops' to 1, 2 or 3; whether the query is negated (\"True or false: Sally is not hot.\") or not; and whether the correct answer is 'true' or is 'false'. We found that these parameters have little impact on the statistics even in combination. A parameter that we found significant, though, is whether the question was answered correctly or not.\nWith these considerations and investigations we collected results for all runs classified by model, technique, and whether a run correctly answered the question. Notice that recall and precision are stated as the mean and standard deviation over all runs in a class."}, {"title": "5 DISCUSSION", "content": "The overall accuracy results (Table 2) show that teaching models to use explicit reasoning strategies produces a performance which is comparable with on-shot CoT reasoning within uncertainty. Although the accuracy is similar between explicit reasoning strategies and CoT, the completion tokens (and therefore computation expense) is more than 200% higher for all explicit reasoning strategies when used by GPT4 and Gemini. Both these completion token values and Figure 3 show that GPT4 and Gemini are very concise and rarely give reasoning when not asked. GPT3 on the other hand was more likely to provide justification even when this was not requested.\nAlthough models would consistently produce some reasoning (or \u2018working') when asked, Figure 3 shows that this reasoning did not always contain all of the required steps. This can occur because the model skips steps or because the model was unable to complete a reasoning process and arrive at an answer. We note that whether the model was able to include all reasoning steps or not was largely independent of the type of prompt used, excepting the normal condition. GPT3 was consistently unable to produce all of the required reasoning steps without at least one example in the prompt.\nConsistent with previous literature, we show that models have a preference for following a bottom up style reasoning procedure [21]. At a high level, the bottom up approach works well for LLMs because the intermediate steps involve more writing of more facts. This essentially provides place for the model to think. Top down reasoning is particularly difficult for LLMs because it involves backtracking and keeping track of which paths have already been explored. The magic set approach is also more difficult for LLMs because it has more steps in the process where the models could make a mistakes.\nOne very interesting finding is the lack of a correlation between including all of the required reasoning to prove the final answer and then obtaining the correct answer itself. shows that the correlation between having the correct reasoning and the correct answer is less than 0.3 in all cases and close to 0 in half of the cases. This result is significant and has significant implications for the trustworthiness of LLMs as it shows that even if a Large Language Model can demonstrate all of the correct reasoning it still may not obtain the correct answer. It also shows that a Large Language model is able to obtain the correct answer even in cases where it skips steps in the working/proof.\nRegarding process soft correctness, the results indicate a correlation between recall and correctly answering a problem. Across almost all classes, recall is higher for correctly answered questions than for incorrectly answered questions. (The biggest difference is for \u2018Gemini-Pro' and \u2018MagicSet', with recall values of 0.82 vs. 0.58.) This allows us to conclude that correct answers tend to be produced along with the facts relevant for deriving the result. However, the converse is not generally true, that instructing a model to derive relevant facts helps in computing a correct answer. This can be seen by observing that the strongest guided techniques, \u2018MagicSet' and 'TopDown' do not consistently produce a significantly higher share of correct answers than the non-guided techniques. Nevertheless it is interesting to see that \u2018MagicSet' delivers comparatively high recall and high precision rates. This suggests, as future work, to consider a hybrid architecture where techniques like 'MagicSet' could be used to extract a small set of relevant facts that can be handed off to a trusted inference engine for actually computing a correct answer."}, {"title": "6 CONCLUSIONS AND FUTURE WORK", "content": "This research is the first to determine the ability of LLMs to use ATP reasoning strategies. We evaluate the performance of three of the best available models on steamroller problems using the most difficult settings of the PRONTOQA Benchmark. In terms of accuracy the models' performance when using the ATP reasoning strategies was similar to one-shot CoT, despite the computational expense of executing the bottom up, top down and magic set reasoning being significantly higher than the model's natural reasoning. We found that LLMs have a preference for using bottom up processes.\nWe build on previous studies which have used recursive-descent parsers by introducing the existing robust Natural Language Processing library, spaCy to this domain of study. SpaCy provides useful out of the box functionality for co-referencing, named entity recognition, point of speech tagging and a built in rule language. This enabled the reliable translation of the results into a consistent format for parsing and and evaluation. The use of these NLP toos allowed us to scalably dissect and evaluate the models' reasoning processes.\nThis allowed us to discover that models were able to \u2018skip steps' and obtain correct answers without showing all of the stepts required for a proof. We also found that models were able to include all of the steps required for a proof in the correct order, but still give the incorrect answer to a question. Overall, there was little correlation between correct reasoning and correct answers for all three of the models. This has a negative implication for the explainability of LLMs, even with current state of the art models, their reasoning cannot be trusted.\nIn terms of overall model comparison, GPT4 had systematically higher accuracy than either GPT3 and Gemini. It was also able to more consistently generate correct reasoning and include all required steps for a logical proof. Gemini and GPT4 were also found to be more concise than GPT3; on average they used less completion tokens for a given prompt. We were unable to compare the computational expense of different models as key information such as the number of parameters of each model, was not available.\nAs area for future work we propose to investigate how LLMs reason on tasks with a greater number of rules and facts. As research in ATP has shown, it is only when problems reach a certain size that the choice of reasoning strategy becomes relevant. It is unclear where such a threshold would be for LLM reasoning. There may be a point where the benefits of top down reasoning approaches outweigh the model's natural tendency to perform bottom up reasoning.\nOne area that also deserves more exploration in this field is the use of neuro-symbolic approaches which combine LLMs with ATP systems. Traditionally ATP systems have been limited by their rigidity and the need to manually input information. If an LLM is able to relatively accurately accomplish this manual labour then the logical reasoning could be \u2018outsourced to the ATP systems allowing for excellent explainability and interpretability. This naturally lends itself to problems where justifications and explanations are required such as education, healthcare, law and banking."}, {"title": "A APPENDIX - PRELIMINARY EXPERIMENTS", "content": "Table 4 contains results from preliminary experiments in order to determine the most appropriate PRONTOQA parameter settings for the experiments. For experimental conditions without distrac-tors we tested with one to five hops (therefore five trials); experimental conditions with distractors have one to three hops (three trials). One hundred problems were included in each trial.\nThis preliminary experiment shows that even the weaker model GPT3, shows ceiling effects on the tasks with the True ontology. To best prevent ceiling effects with the stronger GPT4 model, we chose to use the most difficult condition, False Ontology with Distractors. We would like to point out that a merit of the fictional ontology is that there is less chance of the model being able to use external knowledge to solve the problem rather than going through a reasoning process. However if contamination has occurred, the fictional ontology is likely to be more vulnerable because it contains combinations of tokens that would only be seen in this data set.\nWe also performed an experiment to determine the suitability of using different numbers of hops as repeat trials. In Table 5 we present the results. For problems without distractors changing the number of hops makes causes very little variation in the model performance. We attribute this to the same cause as the authors of the dataset [34], who claim that in these cases the model can avoid the task of reasoning by instead performing an alternative task with the same result. When there are no distractors the answer can easily be obtained by simply counting the number of times \u2018not' appears in the problem statement."}, {"title": "B APPENDIX - PROMPTS", "content": "For each of the prompt techniques shown below three sets (1 hop", "below": "n{question"}, "Each sheep is sunny. Each sheep is a feline. Sheep are mammals. Felines are aggressive. Every feline is a snake. Felines are carnivores. Each snake is luminous. Snakes are cats. Every dog is not luminous. Each snake is an animal. Animals are fast. Carnivores are opaque. Each mammal is floral. Each vertebrate is not feisty. Each vertebrate is a cow. Alex is a sheep. Alex is a vertebrate.", "n{query} = \u201cTrue or false: Alex is luminous.\u201d\nThe prompt for the normal condition was as follows - Average number of prompt tokens is 110.5 for GPT3 and GPT 4 and 94.2 for Gemini:\nprompt = \"{question} {query}\u201d\nChain of thought prompt. - Average number of prompt tokens is 162.3 for GPT and 148 for Gemini:\nprompt = \u201cConsider the following statements and the given query. Use your reasoning skills to determine if the query is true or false based on the statements. Explain your thought process step by step as you analyze the relationship between the statements and the query.\nStatements: {question}\nQuery: {query}\"\nChain of thought and one-shot - Average number of prompt tokens is 301.3 for GPT and 283 for Gemini:\nprompt = \"Consider the following statements and the given query. Use your reasoning skills to determine if the query is true or false based on the statements. Follow the format of the example question that follows.\nExample Statements: \u2018All cats are birds. No bird swims. Whiskers is a cat.\u2019\nQuery: 'True or false: Whiskers swims.'\nExample Reasoning: \u2018Let's figure out if Whiskers swims. This is not provided directly in the statements. However it does state that Whiskers is a cat. Then it states that all cats are birds. Therefore Whiskers is a bird. It then states that no bird swims. Since Whiskers is a bird this means that Whiskers does not swim. Therefore the query \u201cWhiskers Swims\u201d is false.'\nExplain your thought process step by step as you analyze the relationship between the statements and the query.\nStatements: {question}\nQuery: {query}\"\nBottom up prompt - Average number of prompt tokens is 510.3 for GPT and 496.7 for Gemini:\nprompt = \u201cConsider the following statements, which include rules and then facts, along with the given query. Facts are have a specific named instance. For example \u201cSam is a cat\u201d is a fact because an explicit name \u201cSam\u201d is given. Rules establish a connection between two general classes without referring to a specific instance. For example \u201cCats are birds\u201d is rule, not a fact because there is no named instance of a specific cat or bird. Modus ponens can be used to apply a rule to a fact. For example applying \u201cCats are birds\u201d to \u201cSam is a cat\u201d gives a new fact \u201cSam is a bird\u201d. To use bottom-up reasoning:\n- List the rules which do not contain specific named instances.\n- List all given facts with a specific named instance as a current facts.\nThen for each rule do the following:\n1.) Examine each rule to see if its premise applies to any of the current facts. It's important to recognize that general rules can apply to specific facts. For example, the rule 'All cats are birds' applies to the fact \u2018Sam is a cat,' even though the rule doesn't mention Sam specifically. This is because the rule involves a category ('cats') that matches a variable in the fact ('cat'). So, when checking for a match, look beyond direct mentions and consider whether the rule's general premise encompasses the specifics of the current facts. This careful matching is essential to correctly apply rules to facts. Then add the new concluded fact (in this example \u201cSam is a bird\u201d) to the list of current facts.\n2.) Check if the current facts contain the query or its negation. If they do, answer the query. If it does not, repeat this procedure for all rules again. Iterate through this process until a conclusion is reached. It may take three or four iterations to reach a conclusion that will answer the query.\nStatements: {question }\nQuery:{ query }\"\nTop down prompt - Average number of prompt tokens is 499.3 for GPT and 485 for Gemini::\nprompt = \u201cConsider the following statements, which include rules and then facts, along with the given query. Use a top-down reasoning strategy to answer the query. Facts have a specific named instance. For example, \u2018Sam is a cat' is a fact because an explicit name 'Sam' is given. Rules establish a connection between two general classes without referring to a specific instance. For example, \u2018Cats are birds' is a rule, not a fact, because it does not name a specific cat or bird. To use top-down reasoning, follow these steps:\n1. List the rules which do not contain specific named instances.\n2. List all given facts with a specific named instance as current facts.\n3. Then consider the query and repeat the following procedure:\n4. Check if the query or its negation is among the facts. If so, answer the query.\n5. If it is not, then search through the set of rules and check if the conclusion of any of the rules matches the query. If so, make the body of this rule the new query, paying special attention to maintaining the correct use of negation throughout the process. For example, if your query is \u2018Does Sam not swim?' and you have a rule like \u2018Birds swim', you would update the query to \u2018Is Sam not a bird?'. Re-write the query with this new perspective and return to step 4.\nYou will need to repeat the procedure multiple times until the query or its negation appear as facts. It is crucial to meticulously track all instances of 'not' or other negations when updating the query, as this directly affects the final answer.\nIt may take three or four iterations to arrive at the final answer. If a particular line of reasoning comes to a dead end, it might help to start again with the original"]}