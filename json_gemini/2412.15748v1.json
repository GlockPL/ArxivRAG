{"title": "CRITIQUE OF IMPURE REASON: UNVEILING THE REASONING\nBEHAVIOUR OF MEDICAL LARGE LANGUAGE MODELS", "authors": ["Shamus Sim Zi Yang", "Tyrone Chen"], "abstract": "Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical\ndomain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise\nthe importance of understanding reasoning behaviour as opposed to high-level prediction accuracies,\nsince it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical\nLLMs used in the clinical domain will have a significant impact across the healthcare sector. Results:\nTherefore, we define the concept of reasoning behaviour in the specific context of medical LLMs.\nWe then categorise and discuss the current state of the art of methods which evaluate reasoning\nbehaviour in medical LLMs. Finally, we propose theoretical frameworks which can empower medical\nprofessionals or machine learning engineers to gain insight into the low-level reasoning operations\nof these previously obscure models. Conclusion: The subsequent increased transparency and trust\nin medical machine learning models by clinicians as well as patients will accelerate the integration,\napplication as well as further development of medical AI for the healthcare system as a whole.", "sections": [{"title": "1 Introduction", "content": "Reasoning drives problem solving activities, and is ubiquitous in our daily lives. The rising adoption of the field\nof artificial intelligence and its proximity to the concept of reasoning then naturally provokes the question: what is\nthe reasoning behaviour of machine learning models commonly used in artificial intelligence [Figure 1]? This is\nparticularly pertinent with regard to the increasing use of large language models (LLMs). Being large machine learning\nmodels trained on correspondingly high volume text corpora, they contain more parameters and are subsequently less\ninterpretable than conventional machine learning models, which are already obscure under normal circumstances.\nThe question of how LLMs arrive at their answers\u2014particularly in high-stakes applications like medicine\u2014is surpris-\ningly underexplored. This gap is especially striking given the widespread deployment of these models across domains,\noften without a comprehensive understanding of their underlying reasoning mechanisms. Instead, evaluations tend\nto focus on performance metrics such as accuracy, F1 scores, precision, and recall, which are typically benchmarked\nagainst curated subsets of state-of-the-art models as well as specialised datasets which consist of medical license\nexamination questions from the United States (USMLE), Mainland China (MCMLE) and Taiwan (TWMLE) [1], India\n(AIIMS/NEET) [2] and other broader questions less directly related to clinical fields [3, 4]. While these may be effective\nin some cases, such metrics provide limited insight into the complex and obscure inferential processes that LLMs apply\nto generate answers.\nThis neglect in understanding reasoning behaviour leads to their unintentional misuse with direct real-world effects,\nincluding data fabrication [5], false accusations [6] and suicide [7]. Further obfuscating reasoning behaviour in LLMs,\nparticularly generative models, is their ability to mimic the semantics of question and answer processes convincingly\nwhile being surprisingly accurate, to the point where individuals have mistakenly assumed their sentience [8]. Such\nissues are amplified in LLMs used for medical purposes, given their proximity to life-and-death decisions, for example\nin the case of acute heart failure [9].\nWe highlight the importance of gaining insight into the process-driven, reasoning behaviour by observing its functional\nsimilarity to interpretability metrics in machine learning [10, 11, 12, 13, 14]. In both cases the aim is to inspect the\nlearning process of the model, with corresponding metrics used to gain information into how a model is correctly\nor incorrectly predisposed towards a certain outcome, in an attempt to address the common \"black-box\" problem\nof machine learning models [Figure 2]. We re-emphasise the lack of LLM studies tackling the question of general\ninference to focus on a noticeable gap in the field: remarkably few studies investigate reasoning behaviour in the\nmedical LLM space.\nGiven the increased stakes of medical LLMs in clinical decision making, achieving a deeper understanding of medical\nLLMs carries a greater weight than with general-purpose LLMs. Thus, their intense scrutiny by both medical experts\nand the general public is unsurprising. Therefore, it is necessary to supplement clinicians with insight into the reasoning\nbehaviour of medical LLMs to better understand how they arrive at their conclusions and expose potential logical\nfallacies. An ability for LLMs to provide reasoning for their outputs, for example, in a medical recommendation or\ndiagnosis, allows clinicians to clarify discrepancies between machine and expert suggestions. This transparency fosters\ntrust, encouraging integration of LLMs and other machine learning models into clinical decisions and subsequently\nimproving patient outcomes.\nIn our review, we will address a few specific points:\n1. We define the concept of reasoning behaviour in the specific context of medical LLMs\n2. We discuss the importance of evaluating reasoning behaviour in addition to performance metrics\n3. We compare and contrast the current state-of-the-art (SOTA) in reasoning behaviour for the medical field, and\nnote a surprising lack of such studies\n4. We propose strategies to improve and evaluate the reasoning behaviour of medical LLMs, which will grant\ngreater transparency"}, {"title": "2 What is reasoning behaviour in the Context of Medical LLMs?", "content": "First, we specifically define reasoning behaviour in the context of our review. It is important to note that the general\nterm reasoning is used loosely across LLM-related literature, and often reasoning behaviour is not the focus of the\nexperiment but high-level performance metrics are. Here, we use the specific definitions of reasoning and reasoning\nbehaviour respectively from Mondorf & Plank [15], who define these concepts in the context of general LLMs.\nReasoning: \"The process of drawing conclusions based on available information (usually a set of premises).\"\nReasoning behaviour: \"The system's computed response to a reasoning task (the stimulus), particularly its actions,\nexpressions and underlying mechanisms exhibited during the reasoning process.\"\nSpecifically, while reasoning is an event where a conclusion is inferred from a set of premises, reasoning behaviour\ndescribes the process of how the conclusion emerges from the premises.\nWe apply the same definition to this review for medical-LLMs."}, {"title": "3 Types of Reasoning Applicable to Medical LLMs", "content": "This section reviews studies that extend beyond a high-level focus on task accuracy, focusing instead on evaluating the\nreasoning behaviour of large language models. Reasoning behaviour can be subdivided into multiple categories [Table\n3]. However, for the purposes of this study, we focus mainly on subtypes of logical reasoning [16] and causal reasoning\n[17] that are common in medical-LLMs. In addition, we explore the less visible field of neurosymbolic reasoning. We\nnote that other reasoning types such as mathematical reasoning [18] may be more applicable to other categories of\nLLMs, which are not the focus of this review."}, {"title": "3.1 Logical Reasoning", "content": "The study of logical reasoning addresses the question of how individuals infer valid conclusions from a set of given\npremises within a structured framework of logical rules and principles [15]. Mondorf & Plank [15] classify logical\nreasoning into: deductive, inductive, and abductive reasoning. Deductive and inductive reasoning both work towards\na general conclusion, with the key distinction being that deductive reasoning begins with a premise while inductive"}, {"title": "3.2 Causal/Counterfactual Reasoning", "content": "Causal reasoning refers to the ability to connect cause and effect in scenarios. In the context of medical and general\nLLMs, their capabilities are a matter of debate. Intuitively, providing cause and effect relationships improve one's\nunderstanding of a system. Correspondingly, providing this information to medical LLMs would improve a model's\n\"understanding\" and has unsurprisingly emerged as an area of interest. This capability is essential in applications\nlike medical diagnosis, where identifying causal links\u2014such as between symptoms and potential conditions-can\ninform accurate and actionable insights. Causal reasoning involves not just recognising associations but distinguishing\ndirectional influences. In theory, knowing directionality grants the model the ability to infer, for instance, whether\n\"A causes B\" or \"B causes A\". In real-world medical applications, larger LLMs like GPT-4.0 are capable of inferring\ncausal direction between variables, allowing accurate diagnosis of neuropathic pain [21]."}, {"title": "3.3 Neurosymbolic Reasoning", "content": "Neuro-Symbolic AI (N-SAI) is an interdisciplinary field that aims to harmoniously integrate neural networks with\nsymbolic reasoning techniques. Its overarching objective is to establish a synergistic connection between symbolic\nreasoning and statistical learning, harnessing the strengths of each approach. Symbolic reasoning is a process that\ninvolves the use of symbols to represent concepts, objects, or relationships in order to facilitate reasoning, problem-\nsolving, and decision-making. This form of reasoning is characterised by its reliance on formal logic and structured\nrepresentations, allowing for the manipulation of abstract symbols according to defined rules. In the context of N-SAI,\nthis system is used to represent predefined rulesets and knowledge bases, which then streamlines the process of making"}, {"title": "3.4 Trends in Existing Medical LLMs", "content": "While there is no shortage of LLMs applied to medical problems, there is a striking lack of methods which leverage\nreasoning behaviour in their operation [Table 2]. Comparing and contrasting this small subset of methods reveals some\ninteresting trends. First, inspecting their foundational or base models shows that unsurprisingly, most of these methods\nare built on generic LLMs, commonly GPT [23] or LLaMA [24] variants. This is likely due to their demonstrated\neffectiveness in day to day tasks, with more modern variants being shown to be surprisingly effective in clinical\napplications as-is [25]. However, it is notable that many approaches utilise multiple base models, with no single\nmethod relying on one model type. Relying on multiple models is unsurprising, as combining the strengths of multiple\nmodels is likely to boost overall effectiveness. Second, most of their reasoning behaviour is derived from variants\nof chain-of-thought (CoT) [26] processes or reinforcement learning, likely because both techniques closely mimic\ncognitive processes fundamental to reasoning. CoT enables models to break down complex medical cases into a series\nof logical steps, mirroring the structured, stepwise reasoning that healthcare professionals apply. Additionally, few-shot\nlearning complements CoT, allowing LLMs to \"learn\" from the input prompt, generalize from minimal clinical examples\nand adapt quickly to nuanced cases a useful capability in medicine where data can be sparse or highly specialised.\nMeanwhile, reinforcement learning allows models to refine their reasoning capabilities through practice in a virtual\nsimulation, improving accuracy through iterative feedback. Third, the reasoning behaviour of most methods can be\ncategorised as deductive reasoning, although there are a few cases of abductive and causal/counterfactual reasoning.\nHere, it is also worth noting that while LLMs excel in abductive reasoning tasks in multiple-choice scenarios, they are\nconsiderably less effective in generating hypotheses from scratch which may be of value in clinical use [27]. Since the\noverall goal of clinical diagnosis is to determine the disease affecting a patient from causative agents, the prevalence\nof deductive and to a lesser extent causal/counterfactual reasoning makes sense. Finally, training datasets used vary\nwidely in both scope and size, ranging across many different medical conditions, source material and between hundreds\nto thousands of samples. We observed no single standardised training dataset used by each approach, and as with\narchitecture types many approaches used multiple training datasets. Most datasets were of the same modality (text data\nonly), though some medical imaging datasets (MRI scans) were present. MIMIC-III was the most commonly used\ntext dataset, with a combination of medical literature and other publicly available datasets [28]. Therefore, due to the\ndifference in scope, strategy and data used by each approach, directly comparing reasoning behaviour across all models\nsimultaneously is not presently feasible.\nAside from deductive reasoning, causal reasoning and neurosymbolic reasoning have also been demonstrably effective\n[Table 2]. However, example use cases are considerably less common. Current causal inference tests have a limited scope\nsuch as determining the direction of causality between variable pairs, and their performance in more open-ended or\nnuanced causal inference as well as counterfactual reasoning remains unexplored. Meanwhile, neurosymbolic reasoning\nstrategies exploit their inherently grounding properties to addressing the more fundamental issue of hallucinations in\nLLMs [43]. The diversity of strategies is striking - some methods exploit agent-based approaches to tailor argumentation\nschema and symbolic solvers for clinical reasoning [37], while others integrate dynamic medical ontologies in an\nattempt to more closely align reasoning behaviour with medical knowledge [44].\nAs each approach varies widely in scope and implementation, the advantages and disadvantages of each approach\nare broad. Generally, approaches using graph and decision tree-based strategies are easier to interpret due to their\nmore deterministic nature, but may be less effective in ambiguous or complex cases (which are common in clinical\npractice). Meanwhile, methods which are more robust to noise or complex use cases are limited by a highly restricted\nscope, availability of training resources and a large computational footprint. Among these methods, DR HOUSE\n[32] is unique due to its EHR-free approach, only relying on more objective sensor data to circumvent variance in\nclinical note interpretation. Unfortunately, code associated with many of these methods is not publicly available under\nan open-source licence, which limits our ability to inspect them in close detail. It is worth mentioning that medical\nLLMs are equally affected by some of the deeply rooted issues that plague general purpose LLMs as well, for instance\nmemorisation [45] and hallucination [43]."}, {"title": "4 Evaluating reasoning behaviour in Medical LLMs", "content": "To date, a standardised methodology for assessing the reasoning capabilities of large language models is absent. We\nreview the current state-of-the-art in evaluation frameworks for analysing the reasoning behaviour of LLMs in medical\ntasks and we categorise evaluation methodologies into four distinct groups: (i) conclusion-based, (ii) rationale-based,\n(iii) interactive, (iv) and mechanistic evaluations [Figure 3]."}, {"title": "4.1 Conclusion-based Evaluation", "content": "In conclusion-based evaluation schemes, the focus is on the model's final answer rather than the reasoning process that\nled to it. Although this outcome-focused approach may overlook the model's underlying rationales, it can still offer\nvaluable but limited insights into the model's reasoning patterns, especially if there is a clear cause and effect between\ndifferent premises and conclusions in which the reasoning behaviour may be more-self evident."}, {"title": "4.2 Rationale-based Evaluation", "content": "In contrast to high-level conclusion-based evaluation schemes, rationale-based evaluation methods are process-driven\ninstead of being outcome-driven. Their focus is on examining the reasoning process or reasoning traces generated by\nthe model, typically assessing their logical validity and coherence. As rationale-based evaluation methods targeted\nat medical language models are scarce and operate under distinct paradigms, we will discuss them individually on a\ncase-by-case basis.\nThe most straightforward but resource-heavy approach was to manually evaluate answers using the skills of domain\nexperts. These domain experts were blinded to the questions and identified logical fallacies as well as inaccuracies\ndirectly in provided rationale [29]. Conversely, an automated approach applied Directed Acyclic Graphs (DAG) to\nrepresent underlying relationships in complex medical datasets, including cancer [49]. In implementation, a DAG was\nconstructed by predicting which factors might influence others, and accuracy was scored with a Bayesian Dirichlet\nmetric measuring the similarity of the resultant graph with the ground truth of real patient data. In addition, a separate\nmethod also applied DAG, but exploited it to infer the direction of causality between variable pairs [50]. Accuracy was\nthen measured using Normalised Hamming Distance (NHD) as a similarity metric between the resultant and ground\ntruth patient outcome or diagnostic graph."}, {"title": "4.3 Mechanistic Evaluation", "content": "Similarly, mechanistic evaluation of reasoning behaviour is process driven with the aim of examining low-level\nreasoning traces. In contrast to rationale-based evaluation, mechanistic evaluation delves deeper into the underlying\nprocesses that drive a model's response, aiming to uncover the fundamental questions of \"how\" and \"why\" associated\nwith an outcome.\nIn practice, feature attribution methods can be exploited to study reasoning behaviour by highlighting keywords which\nare conceptually identical to features of interest in medical LLMs. These explainable AI (XAI) methods compute an\nattribution score for each input feature to represent its contribution to a model's prediction, which can be calculated\nand represented with a variety of metrics [10, 51]. For example, a hypothetical medical scenario may show that the\nkey words \"blocked nose\" are strongly weighted in a positive influenza prediction. In this context, the key word is\nequivalent to the reasoning trace, and is shown to impact the model's reasoning behaviour. A conceptually similar\nstrategy has been applied to explain predicted diseases from patient-doctor dialogues [52]."}, {"title": "4.4 Interactive Evaluation", "content": "Finally, a more open-ended approach to reviewing reasoning behaviour is interactive evaluation. Unique to other\nstrategies, it is reactive and engages with the LLM directly during evaluation, adjusting questions to fit the model's\nresponse. This deeper exploration of the \"response space\" tests and further exposes the model's reasoning capabilities\nas well as limitations [53]. Variants of interactive evaluation may challenge the model's conclusions directly [54]\nor use game-theoretical scenarios to probe reasoning depth [55]. Unfortunately, a critical flaw of this evaluation\nmethod is its lack of reproducibility and standardisation due to its reactive nature. Currently, one exception exists,\ncircumventing irreproducibility by side-stepping the requirement for a prompt [56]. Nevertheless, we note the strong\nadvantages of interactive evaluation, and note that it remains relatively unexplored in the current medical LLM literature.\nRefinement of the core method and further investigation of strategies (such as the aforementioned prompt skipping\n[56]) to counteract its limitations have the potential to raise its reproducibility to reasonable levels."}, {"title": "4.5 Summary of Evaluation Strategies", "content": "To our surprise, we struggled to find existing studies of reasoning behaviour evaluation in a medical LLM context. Nev-\nertheless, we note some broad insights from the few existing studies matching our scope: (a) graph-theoretic approaches\nare intuitively applicable to evaluating causal or counterfactual reasoning behaviour due to their representation of\ncause-and-effect, (b) feature attribution methods provide a low-level glimpse into medical reasoning, and (c) reasoning\nbehaviour evaluation methods are complementary, with the potential of being applied simultaneously to obtain a better\nunderstanding in cases where the model configuration allows."}, {"title": "4.6 Towards transparency in medical LLMS", "content": "Given our findings, we identify the emergent question: \"How can we expose poorly-understood computational reasoning\nbehaviour and subsequently exploit it to achieve a deeper understanding of medical case studies which use LLMs?\". To\naddress this question, we propose a framework recipe matching two criteria: (a) low-level reasoning behaviour must\nbe visible and the framework should be (b) task-agnostic. Each framework would consist of three broad stages: (a)\ndata preprocessing, (b) model training and (c) interpretability via extraction of reasoning behaviour. Following these\nattributes, we propose three possible theoretical frameworks, one straightforward and two complex [Figure 4].\nWe begin the simplistic framework by restricting input data scope to standardised data formats. To this end, TEMED-\nLLM [34] can be used to parse textual data into tables in the preprocessing stage with a predetermined format. Structured\ninput has multiple advantages, being consistent and more easily ingested into software. In addition, a side effect is\nfurther simplification of data. An advantage of this which may benefit machine learning algorithms is a noise reduction\nwhile increasing variance in the data. However, we note that a degree of low-level feature loss is possible. Next, we\nconsider that while deep learning is a powerful tool, more conventional machine learning approaches are often sufficient\nin many cases. We exploit the tabular nature of the data and leverage tree-based methods, which include examples such\nas XGBoost [57] or Random Forest [58]. While straightforward, these are effective and particularly suited to p \u00bb n\nproblems common across the biological sciences, where there are far more features per sample than there are samples,\ni.e. the \"curse of dimensionality\" [59]. In addition, tree-like approaches have additionally benefited from properties\nthat make them more interpretable. Exploiting this property allows us to generate decision sets which are interpretable\nduring model training [60], hence exposing reasoning behaviour.\nIn our second, more complex strategy, we propose a three-stage \"reasoning first, post pretraining\" medical LLM\nframework. In mild contrast to a conventional LLM tuning process which incorporates both a Supervised Fine-Tuning\nstage (SFT) and Reinforcement Learning from Human Feedback (RLHF) loop [61], we incorporate the STaR method\nas the first stage [62]. Here, a rationale is generated along with an answer, and the model is guided towards the correct\nsolution with hints should the answer be incorrect. The triple combination of question, rationale and correct answer are\nused to fine-tune the LLM of interest, in an attempt to improve its reasoning capabilities. Our hypothetical second stage\nincorporates a RLHF algorithm known as Proximal Policy Optimisation (PPO) [63], implementing a reward system to\nprovide a feedback metric to rank the model's output. Exploiting the feedback metric allows the user to then optimise\nthe model's behaviour toward a desired outcome in a controlled manner. Feedback can be addressed in two broad\nways: outcome supervision that evaluates the final result produced by a model or process supervision which evaluates\nintermediate reasoning steps [64].\nGiven that a key aim of our framework is to expose reasoning behaviour, we focus on process supervision [64]. As\nfine-resolution and step by step feedback is provided by human annotation at each step, process supervision inherently\nachieves transparency. In addition, process supervision considerably outperforms outcome supervision in solving\ncomplex tasks, for instance in mathematics [64, 65], and in particular effectively tackles \"convincing wrong-answer\"\nevents. However, reliance on human annotation makes process supervision more resource-intensive. To address this\nlimitation, we propose a Process Reward Model (PRM)\u2014a custom reward system used during RLHF to implement\nprocess supervision. Trained on human-annotated Chain-of-Thought (CoT) reasoning paths, the PRM allows for\nautomated evaluation of the reasoning behavior as part of the RLHF pipeline. As part of the process, it assesses\nlogical coherence and identifies logical fallacies, reducing the need for ongoing human annotation while maintaining\ntransparency. We note with interest that while SFT-RLHF methods are prevalent in general-purpose LLMs, their usage\nremains comparatively unexplored for medical LLMs, possibly in part due to patient privacy concerns and legislation.\nNevertheless, we argue that SFT-RLHF methods, including the integration of PRMs, are highly applicable to medical\nLLMs in clinical domains, offering a powerful tool to enhance their reasoning capabilities.\nDuring inference, we propose that scaling compute at inference time can enhance a medical LLM's reasoning behaviour\nby generating multiple reasoning paths during inference, evaluating the reasoning path to decide whether to further"}, {"title": "5 Discussion", "content": "As part of our study, we intended to investigate the current SOTA of medical-LLM performance in the context of\nreasoning behaviour. However, we discovered that most reasoning behaviour evaluations are performed at a very\nhigh level, mostly in English only, and in any case are inadequate to address the nuances of reasoning behaviour.\nThis did not surprise us as this mirrors the overall lack of interest in the field in addressing reasoning behaviour as\nopposed to achieving simplified performance benchmarks. The more conventional conclusion-based evaluation is not\nineffective, but imparts a limited capacity to understand reasoning behaviour without specific benchmarks or more\nthorough analysis. Evaluating reasoning traces in large language models (LLMs) is crucial, especially in medical\ncontexts, as it is possible for models to reach the correct conclusion through erroneous reasoning. Therefore, an AI\nthat not only gives a diagnosis but also provides the rationale behind it can significantly aid adoption among clinicians\nand other healthcare professionals by virtue of its transparency which fosters trust in its recommendations. Broadly,\nwe consider that rationale-based, interactive and mechanistic evaluation are more naturally predisposed to decrypting\nthe reasoning behaviour of medical LLMs. Interactive evaluation would be case-specific, and may be better suited to\nmedical chatbots.\nAnother potential issue we faced when evaluating the evaluation methods for medical reasoning, is the lack of\nmemorisation tests and benchmarks in LLM. This is pertinent as (like humans) medical LLMs have the ability to\nmemorise the dataset they are given but on a much grander scale, giving the illusion of reasoning although in reality\nregurgitating related or unrelated information from a vast knowledge base (like humans) [45]. Hence, in many cases\nit was not possible to answer the question: \"to what extent is this model a stochastic parrot and to what extent\nis this model performing logical reasoning?\" To answer this question, a structured approach would involve testing\nits ability to work from foundational first-principles, or \"base facts,\" embedded within their training corpus. These\nbase facts encompass simple yet essential principles across areas such as medicine, physiology, and pharmacology,\noften representing core medical knowledge. For example, a base fact in medicine could be: \"The heart pumps blood\nthroughout the body.\" Reasoning tests can then be designed to see if the model can apply such base facts to answer\nmore complex questions.\nIn practice, however, access to high-volume training corpora for closed-source enterprise models like GPT-4 [23],\nGemini [70], or Anthropic [71] is restricted. This limitation calls for designing medical tests that embed low-level\nfundamental knowledge, and relying primarily on the model's ability to reason from these base facts. Nevertheless, we\ndo not intend to diminish the usefulness of \"System 1\" LLMs (which are a prerequisite for \"System 2\" type systems),\nbut instead wish to highlight the lack of insight into a model's reasoning behaviour without this layer of checks.\nA natural side effect of obtaining transparency into medical reasoning behaviour is its neutralising effect on hallucination\nevents common across LLMs in all domains [43]. Such events occur due to the autoregressive nature of generative\nLLMs, which produce an output by selecting a token with a highest probability score. Should a model choose one\n\"nonsense\" token in context, subsequent tokens are similarly affected. A compounding effect can occur, quickly\nworsening the error unless the model has the ability to backtrack. However, exploring reasoning behaviour allows\ngreater insight into hallucination events by exposing the involved logic chain, complementing the current state of the\nart of retrieval-augmented generation (RAG) [72]. More ominously, it is an unfortunate reality that cybercrime is\nincreasingly common, and it is not impossible that healthcare infrastructure, including associated medical LLMs may\nbe targeted. While this commonly takes the form of ransomware, it takes disturbingly little effort to \"poison\" medical\nLLMs with misinformation [73], with myriad implications for those used for clinical diagnosis or hospital operations.\nThis aspect of medical LLMs is a relatively unexplored field, with most studies focusing on generic use cases [74],\nthough a more comprehensive framework incorporating the paradigms discussed in our review as well as this security\naspect exists for medical vision language models [75]. As with exposing hallucination events, greater transparency into\nmedical models will assist in identifying such events should the situation arise.\nEnhanced medical reasoning transparency may contribute to solving on-going problems such as addressing differential\ndiagnosis or providing clinical management plans. Differential diagnosis is an on-going problem in medical AI\ndevelopment, where similar conditions may confound a prediction with potentially severe consequences. By viewing\nreasoning traces, both method developers and clinicians will be able to better discern why an accurate or inaccurate\nchoice is made, and adjust the model accordingly as well as gaining potentially unknown clinical insights.\nFinally, we highlight the point that understanding reasoning behaviour of medical LLMs is functionally equivalent\nto achieving explainable AI (XAI), and is not mutually exclusive with other XAI techniques or evaluation paradigms\ndiscussed. Given the understandably high level of scrutiny placed on medical methods, achieving this deeper level of\nunderstanding is necessary to demonstrate the effectiveness of medical LLMs. At the same time, we may be able to\nanswer an interesting question: \"is improved reasoning correlated to improved performance\"? Ultimately, understanding"}, {"title": "6 Conclusion", "content": "In summary, it is intuitive that a greater understanding of the reasoning behaviour of medical LLMs empowers clinicians,\nimproves patient trust and allows machine learning engineers to troubleshoot underperforming models. However, the\nlack of studies focusing on understanding reasoning behaviour is striking, where the majority of studies are focused\non achieving high-level performance metrics with a conspicuous lack of focus on XAI. Most reasoning behaviour\nevaluation strategies are in their infancy, though there is notable potential for growth and further studies. Our theoretical\nproposed frameworks, while limited, can contribute to XAI in clinical LLMs, with the ultimate goal of improving\ntransparency in medical AI and subsequently patient outcomes."}, {"title": "7 Declarations", "content": ""}, {"title": "7.1 Ethical Approval", "content": "Not applicable."}, {"title": "7.2 Consent for publication", "content": "Not applicable."}, {"title": "7.3 Competing Interests", "content": "The authors declare that they have no competing interests."}, {"title": "7.4 Funding", "content": "Not applicable."}, {"title": "7.5 Author's Contributions", "content": "Conceptualization, both authors; formal analysis, both authors; funding acquisition, both authors; investigation, both au-\nthors; resources, both authors; supervision, T.C.; validation, both authors; visualization, both authors; writing\u2014original\ndraft, both authors; writing-review and editing, both authors."}]}