{"title": "KNN-MMD: Cross Domain Wi-Fi Sensing Based on Local Distribution Alignment", "authors": ["Zijian Zhao", "Zhijie Cai", "Tingwei Chen", "Xiaoyang Li", "Hang Li", "Guangxu Zhu"], "abstract": "As a key technology in Integrated Sensing and Communications (ISAC), Wi-Fi sensing has gained widespread application in various settings such as homes, offices, and public spaces. By analyzing the patterns of Channel State Information (CSI), we can obtain information about people's actions for tasks like person identification, gesture recognition, and fall detection. However, the CSI is heavily influenced by the environment, such that even minor environmental changes can significantly alter the CSI patterns. This will cause the performance deterioration and even failure when applying the Wi-Fi sensing model trained in one environment to another. To address this problem, we introduce a K-Nearest Neighbors Maximum Mean Discrepancy (KNN-MMD) model, a few-shot method for cross-domain Wi-Fi sensing. We propose a local distribution alignment method within each category, which outperforms traditional Domain Adaptation (DA) methods based on global alignment. Besides, our method can determine when to stop training, which cannot be realized by most DA methods. As a result, our method is more stable and can be better used in practice. The effectiveness of our method are evaluated in several cross-domain Wi-Fi sensing tasks, including gesture recognition, person identification, fall detection, and action recognition, using both a public dataset and a self-collected dataset. In one-shot scenario, our method achieves accuracy of 93.26%, 81.84%, 77.62%, and 75.30% in the four tasks respectively. To facilitate future research, we will make our code and dataset publicly available upon publication.", "sections": [{"title": "I. INTRODUCTION", "content": "Existing prominent progress in Deep Learning (DL), wire- less communication, and sensing have envisioned that the future networks will not only serve the sole purpose of data delivery but also actively and cooperatively collect data to recognize the environment. The Integrated Sensing and Communication (ISAC) devices are expected to be massively deployed in the real world, supporting and providing a large variety of tactile intelligent services and applications.\nOne materialization of the concept of ISAC is Wi-Fi sens- ing, where the Wi-Fi devices serve as the radar sensors in local areas [1], [2]. The advantages of Wi-Fi sensing compared with other sensing method are given in Table I. First, Wi-Fi sensing is contactless as the object to be detected does not need to carry a device like a smartphone or smartwatch. Second, Wi-Fi sensing is privacy-preserving compared with camera. Third, Wi-Fi sensing is insensitive to occlusions and illumi- nation conditions since its electromagnetic wave operates on a frequency band that can penetrate walls. Moreover, Wi-Fi sensing is ubiquitous, with over 19.5 billion Wi-Fi devices deployed globally [3]. With an appropriate algorithm that enables its sensing function, ubiquitous sensing function can be realized in various environments, such as homes, offices, and public spaces. Last but not least, Wi-Fi sensing has lower cost compared to dedicated radars.\nGiven the released tool that works with some specific types of commercial Network Interface Cards (NIC) [4], the Channel State Information (CSI) can be obtained, which describes how the wireless signals fade during the propagation from the sender to the receiver. By using this CSI data collection tool, a series of research employing commercial Wi-Fi NICs for different kinds of tasks. Moreover, a dedicated IEEE standard for Wi-Fi sensing named IEEE 802.11bf is approved [5] to enable faster development and practicalization of Wi-Fi sens- ing technologies. However, since the Orthogonal Frequency Division Multiplexing (OFDM) waveforms employed by Wi- Fi are designed for higher communication rate, it is hard to use simple physical models to extract information about what happened in the real world from the CSI measurements.\nAs a result, most Wi-Fi sensing methods rely on machine learning, especially for fine-grained tasks like gesture recog- nition [6]. However, even minor environmental changes can drastically alter the CSI patterns and significantly degrade the performance of the trained model. This environmental variabil- ity leads to a domain shift - a discrepancy between the training data distribution and real-world deployment conditions. Given the strong dependence of the training data, such domain shifts can severely undermine the generalization capability. This scenario is known as a cross-domain problem in machine learning, where the source domain (training data) and target domain (testing data) have different underlying distributions or feature spaces.\nIn fields like Computer Vision (CV) and Natural Language Processing (NLP), where large datasets are available in these common modalities, it is relatively easy to solve such cross- domain problems by training large models with vast amount of data [7]. However, in Wi-Fi sensing, the limited public datasets often have different formats, making it challenging to use them together to train a model. Additionally, collecting large CSI datasets on one's own is not easy. Therefore, it is needed to develop a powerful yet adaptable small model that can quickly"}, {"title": "II. PROBLEM DESCRIPTION AND BENCHMARK SOLUTIONS", "content": "In Wi-Fi sensing, CSI is one of the most commonly used features. The channel between the transmitter and receiver can be influenced when encountering dynamic or static objects. The RX will then receive the multipath superimposed signals from refraction, reflection, and scattering. By analyzing the received CSI, we can obtain information about the environ- ment and behavior. The channel model can be mathematically represented as:\n\\begin{equation}\nY = HX + N,\n\\end{equation}\nwhere Y and X are the matrices of the received and transmit- ted signals, respectively, N is the noise, and H is the estimate of the Channel Frequency Response (CFR) of the wireless channel, which contains information about the amplitudes, phases, and delays of the multipath components. It can also be represented as:\n\\begin{equation}\nH = ||H||e^{i\u2220H}\n\\end{equation}\nwhere $||H||$ and $\u2220H$ denote the amplitude and phase of the CSI measurement, respectively.\nFor most Wi-Fi sensing tasks, when an object moves, the CSI changes according to patterns specific to the movement. By analyzing these changes in CSI values, the object motions can be detected. Compared to other Wi-Fi signals, such as the Received Signal Strength Indicator (RSSI), CSI captures fine- grained features, thus providing more precise information for Wi-Fi sensing.\nIn this paper, we consider the WiGesture dataset [27], which contains the CSI collected from 8 individuals performing 6 different gesture actions and a self-collected WiFall dataset, including CSI collected from 10 individuals performing 5 different actions. For the gesture recognition or action clas- sification task, we take different people as different domains. And for the people identification task, we take different actions as different domains.\nIn the following of this paper, the variables x and y refer to the input data and output label, respectively. represents the network parameters. Ps and Pt represent the probabil- ities in the source domain and target domain, respectively. Furthermore, the focus of this paper is primarily on the few- shot cross-domain task. Most available data is in the source domain expect n labeled samples for each category in the target domain. This scenario is often referred to as the n-shot problem. In addition, we also have some unlabeled data from the target domain. Our objective is to accurately classify these unlabeled data. For the sake of simplicity, we will refer to the labeled data from the source domain as the training set, and the labeled data from the target domain as the support set.\nNext, we use WiGesture dataset to illustrate the domain shift in Wi-Fi sensing. It is evident that even for the same action, individuals exhibit distinct data distributions. These findings imply that it is challenging to generalize the learned features from one set of individuals to others. In Fig. 3c, a Resnet18 [29] is trained by setting the people with IDs 1-7 as the source domain and people with ID 0 as the target domain. It can be observed that the trained Resnet18 can distinguish different actions in the source domain but fail to distinguish different actions in the target domain. This is mainly because even for the same action, different people have distinct behavioral patterns, which can lead to significant gaps in the CSI patterns between individuals. As a result, to make the model perform well in the target domain, it is necessary to address the domain shift (cross-domain) problem."}, {"title": "C. Domain Adaptation Methods", "content": "As the most common method in cross-domain tasks, DA can be mainly classified into three types: metric-based methods, learning-based methods, and DAL methods. However, each of these approaches has its own shortcomings, as shown in Table II.\nThe core idea of metric-based methods is to learn a good sample representation and metric. However, most metric-based methods only depend on the support set without using the training set in source domain sufficiently. As a result, these methods are heavily influenced by the quality of the support set, which can lead to unstable performance and the waste of the training set.\nWith the development of deep learning, there have been many learning-based methods for cross-domain tasks in recent years, including transfer learning [36], meta learning [37], FSL [38]. However, an important practical problem of these methods is that we do not know when to stop the model training. Since there is often a significant gap between the source and target domains, we cannot reliably infer the model's performance on the target domain based on its performance on the source domain. As a result, the typical early stop strategies cannot be easily applied in such scenarios.\nDAL methods aim to map the source domain and target domain to a common feature distribution, where the data in the two domains can have the same distribution in the new space. However, most DAL methods, such as MMD [17], Kernel Mean Matching (KMM) [39], and Subspace Geodesic Flow (SGF) [40], rely on the assumption that the conditional probabilities in the source domain and target domain are the same, i.e., $P_t(y|x) = P_s(y|x) = P(y|x)$. To estimate P(y|x), the network parameters need to be learned to achieve $P(y|x;\u03b8) = P(y|x)$. However, due to $P_t(x) \\neq P_s(x)$, the $P(y|x; \u03b8)$ learned from the source domain cannot generalize to the target domain. Consequently, these traditional methods can be seen as finding an intermediate space where the input data from the source domain and target domain are mapped to the same distribution, i.e., $P_t(x) = P_s(x)$. This allows the model to learn the parameter \u03b8 more effectively and accurately. However, in many practical scenarios, the assumption $P_t(y|x) = P_s(y|x)$ is not met. This has been demonstrated in our experiments (Section IV-C) as well as our previous research [16], where traditional DAL methods even failed in cross-domain Wi-Fi sensing tasks.\nIn conclusion, the quality of the support set makes the performance of metric-based methods unstable. For learning- based methods, they also use source domain data to train, and the influence of the support set is not as significant as for metric-based methods. However, it's hard to identify when to stop the training process, and the accuracy in the test set always has significant fluctuations during training Finally, most DAL methods do not require a support set for training, and they are not affected by the quality of the support set. But they mainly rely on the assumption of $P_t(y|x) = P_s(y|x)$, which is not always established in practice. Thus, the model would easily fail.\nIn addition, some other researches have also identified the limitations of traditional DAL methods. Tian et al. [41] utilize KNN [30] to identify the nearest samples in the entire data domain to each sample in the source domain, expand- ing the distance between neighboring samples from different categories in the source domain, and shrinking the distance between neighboring samples from the target domain under the assumption that such sample pairs share the same category. However, this approach may also lead to potential failures, as shown in Fig. 1. Though the source domain samples have a relatively small distance to their neighboring samples in the target domain and a large distance to other category samples in the source domain, it cannot guarantee accurate classification in the target domain. Li et al. [42] first generate pseudo- labels for samples in the target domain using a classifier trained in the source domain and then attempt to expand the distance between samples with different categories and shrink the distance between samples with the same category. However, due to the inherent gap between the source and target domains, the pseudo-labels generated in the first step may be inaccurate, which can adversely impact the subsequent steps."}, {"title": "III. METHODOLOGY", "content": "As discussed in previous section, traditional DAL methods cannot guarantee model performance. To solve this problem, we propose a local distribution alignment approach that aligns the distribution between the source domain and the target domain within each category. (The detailed mathematical analysis will be discussed in Section III-B.) However, the category labels in the testing set cannot be accessed during training, and it is also challenging to estimate the target domain distribution solely based on the limited labeled data in the support set. As a result, we propose a preliminary strategy to first obtain some labels in the testing set to estimate the distribution.\nAs shown in Fig. 4, our method mainly has two steps: (1) Preliminary Classification and (2) Training Network with Local Alignment. In Step 1, we use KNN to preliminarily classify the testing set according to the support set. We then choose the samples with the highest classification confidence levels to construct a help set for Step 2. The help set will be used for the local alignment with the training set. In Step 2, we train a classification network with the MK-MMD loss function to align the source domain and the target domain within each category. Additionally, we use an early stop strategy according to the classification accuracy in the support set, as it does not participate in the training and has the same distribution as the testing set. By this way, our training process can stop at a point where the testing set has a relatively high classification accuracy.\nBefore introducing our method in detail, let us describe the data format in our study. Each sample consists of two dimensions representing the data length l and subcarrier s, respectively. As we choose Resnet as the feature extractor, it requires an extra channel. Consequently, each sample has three dimensions, denoted as $x \\in R^{1\u00d7l\u00d7s}$. The training set, support set, help set, and testing set are represented as $X^{train}$, $X^{support}$, $X^{help}$, and $X^{test}$, respectively, where the help set is not provided but generated by us."}, {"title": "B. Local Distribution Alignment", "content": "Let's reconsider the cross-domain classification task. As proposed in [26], the assumption $P_t(y|x) = P_s(y|x)$ is not met in most practical scenarios. As a result, we consider the case when $P_t(y|x) \\neq P_s(y|x)$ and aim to find a feature space where, after mapping, $P_t(y|x') = P_s(y|x')$. In this feature space, we can train a classifier network to realize the final classification.\nWe can use the conditional probability formula to structure the relationship between $P_s(y|x)$ and $P_t(y|x)$ as:\n\\begin{equation}\nP_s(y|x) = P_t(y|x) \\frac{P_s(x|y)P_s(y)}{P_s(x)} \\frac{P_t(x)}{P_t(x|y)P_t(y)} \\\\\n= P_t(y|x) \\frac{P_s(x|y)}{P_t(x|y)} \\frac{P_s(y)}{P_t(y)} \\frac{P_t(x)}{P_s(x)} \\\\\n= P_t(y|x) \\frac{P_t(x)}{P_s(x)} \\frac{P_s(x|y) P_s(y)}{\\Sigma_{y'} P_s(y')P_s(x|y')} \\frac{\\Sigma_{y'} P_t(y')P_t(x|y')}{P_t(x|y) P_t(y)}\n\\end{equation}\nwhere y' represents the index of the category in the mapped feature space. In most cases, we have $P_s(y) \u2248 P_t(y)$, indicating that the label distribution is similar between the source and target domains. If we also have $P_t(x|y) \u2248 P_s(x|y)$, then $P_s (y|x) \u2248 P_t(y|x)$.\nTo realize distribution alignment, in this paper, we utilize the MK-MMD method, which is based on MMD. MMD measures the distance between two distributions and aims to map the"}, {"title": "C. Model Structure", "content": "In our model, as shown in Fig. 5, we utilize Resnet18 [29] as the feature extraction component and Multilayer Perceptrons (MLP) as the classifier. Specifically, we begin with some nor- malization operations, which we have observed to be essential for the success of the model.\nFirst, we use a standard layer [27] as shown in Eq. 7:\n\\begin{equation}\nStd(X^{(i)}_j) = \\frac{X^{(i)}_j - E_{time} [X^{(i)}_j]}{\\sqrt{D_{time} [X^{(i)}_j]}},\n\\end{equation}\nwhere $X^{(i)}_j$ represents the i-th sample in the j-th subcarrier of the CSI sequence X, E and D denote the expectation and variance, respectively. The standardization operation is applied in the time dimension. In [27], it's noted that the distribution of CSI would have a significant difference in different time ranges, which could make the network difficult to converge. By standardization, the data distribution in the time dimension can be unified to a similar distribution, which helps to solve this problem, to a certain extent.\nThen, the data is input to a batch normalization layer that normalizes each element along the batch dimension, as shown in Eq. 8:\n\\begin{equation}\nBN(X) = \u03b3 \\frac{X - E_{batch} [X]}{\\sqrt{ D_{batch} [X]}} + \u03b2,\n\\end{equation}\nwhere \u03b3 and \u03b2 are learnable parameters. Batch normalization is a traditional method to deal with the covariate shift problem [43], which is similar to domain shift but with a smaller domain gap. As a result, we use it to deal with the domain shift problem.\nSubsequently, the processed data is fed into Resnet to obtain the embedding, which serves as the feature space mapping mentioned in Section III-B. Finally, the MLP acts as a classifier, taking the embedding as input and producing the corresponding label as output. The network can be expressed as Eq. 9.\n\\begin{equation}\nE = Resnet(BN(Std(X))),\n\\\\\nY = MLP(E),\n\\end{equation}\nwhere E is the embedding result, and \u0176 is the output."}, {"title": "D. Training Process", "content": "1) Preliminary Classification: Before applying KNN for preliminary classification, we need to flatten each sample to one-dimensional data, denoted as x' \u2208 Rls. Subse- quently, we construct the target domain data X'target= [X'support, X'test]. However, the data dimension ls is often too high, exceeding a thousand, which may lead to a high computational burden for KNN. Therefore, prior to using KNN, we aim to reduce the data dimension in X'target via UMAP [28].\nUMAP primarily involves four steps: constructing the near- est neighbor graph, approximating a fuzzy-simplicial set, optimizing the low-dimensional embedding using stochastic gradient descent, and finalizing the embedding after conver- gence. Since UMAP is not the primary focus of our research, we directly utilize the function implemented by the UMAP package in Python. For more details, interested readers can refer to the UMAP paper [28].\nAfter performing dimension reduction, we utilize the KNN algorithm to classify the testing set based on the support set. For more detailed information on the KNN algorithm, please refer to the paper [30]. Briefly, for each sample in the testing set, we identify the nearest k samples from the support set using Gaussian distance as the distance measure. Subsequently, we determine the most frequent label among the identified neighbors as the classification result. In the case of a tie, we select the label of the nearest neighbor as the classification result.\nNext, we select the top p% samples from the testing set to construct the help set based on the confidence level of classification. In our approach, the confidence is measured by calculating the distance between each sample in the testing set and its nearest neighbor in the support set with the same label. A smaller distance indicates higher confidence. Through experimentation, we have found this method to be effective. For instance, in the four tasks conducted in our experiment, when setting p% to 50%, the classification accuracy of the selected top p% samples consistently exceeds 90%, regardless of the values of k (the number of neighbors in KNN) and n (the number of samples per category in the support set for n-shot tasks). Hence, we consider these samples as the help set for the subsequent steps.\n2) Training Network with Local Alignment: In Step 2, we train our network using the loss function consisting of three components, as presented in Eq. 10:\n\\begin{equation}\nL = L_{cls} + \\frac{L^{local}_{MMD} + L^{global}_{MMD}}{2},\n\\end{equation}\nFirstly, Lcls represents the traditional cross-entropy loss func- tion, which utilizes data solely from the training set. It is used to train the model to learn the basic classification capacity. Secondly, $L^{local}_{MMD}$ represents the local MMD, which calculates the MK-MMD between the help set and the training set within each category, as shown in Eq. (11):\n\\begin{equation}\nL^{local}_{MMD} = \\frac{1}{M} \\sum_{i=1}^{M} MK\\text{-}MMD(K, E^{train}|Y=i, E^{help}|Y=i),\n\\end{equation}\nwhere E is the embedding result of Resnet, K is the given kernal list, and Y is the sample label. It's used to realize our local alignment. Additionally, we add an extra global MMD, which computes the MK-MMD between the entire help set and training set, following the conventional MMD approach, as represented in Eq. (12):\n\\begin{equation}\nL^{global}_{MMD} = MK\\text{-}MMD(K, E^{train}, E^{help}),\n\\end{equation}\nAs mentioned previously, our primary focus lies on the local MMD rather than the global MMD. However, we include the global MMD to address potential challenges arising from an imbalanced distribution of samples among different labels within the help set. By incorporating the global MMD, we can mitigate this issue to some extent. For example, if we ensure $P_s(x|y) = P_t(x|y)$ for all y except y = i, and also make $P_t(x) = P_s(x)$, we can achieve $P_s(x|y = i) = P_t(x|y = i)$. Nonetheless, it is important to note that this approach has lim- itations when there is a significant lack of certain categories. Alternatively, we can employ other methods to generate the help set and address category imbalance. For example, within the testing set, we can directly select the top p% samples within each category or choose a fixed number of samples with the highest confidence level within each category. However, this may impact the accuracy of the help set to some extent.\nFurthermore, as previously mentioned, many traditional FSL methods encounter the issue of determining when to stop training. Since their support sets are often involved in the training process, they are unable to obtain a valid set comparable to traditional machine learning methods for early stop. However, in our approach, since the support set is not included in the computation of the loss function, we can implement an early stop strategy based on the accuracy of the support set."}, {"title": "E. Ablation Study", "content": "In our KNN-MMD method, the MK-MMD seems unneces- sary because we can simply pre-train the model on the training set and fine-tune it on the help set as many DA methods do. However, when choosing the top p% samples in the target domain, we cannot guarantee that the amounts of different cat- egory samples will be balanced. This imbalance may cause the long-tail problem during fine-tuning and significantly impact the network's performance. However, MMD is not affected by this issue. Furthermore, in extreme situations, some categories may be missing in the help set, which can be catastrophic for training the network. On the other hand, our method, which combines local MMD and global MMD, can mitigate this problem to some extent. We compare the performance of our method with that of fine-tuning a Resnet18 using help set in each task. As shown in Fig. 8, our MMD-based method shows"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce KNN-MMD, a FSL method for cross-domain Wi-Fi sensing. We demonstrate that traditional DAL has its shortcomings and propose a local distribution alignment method to address this problem. Additionally, our method supports an early stop strategy to identify when to stop training, which is hardly used in most traditional FSL methods. The experimental results show that our model performs well and remains stable across various cross-domain WiFi sensing tasks, including gesture recognition, people identification, fall detection, and action recognition. The stability of our model also indicates its potential for practical applications.\nIn the future, there are several aspects of our method that can be further explored. Firstly, our method is not limited to KNN and MMD, and it would be interesting to investigate the effectiveness of other combinations of metric-based methods or few-shot learning methods with DAL methods. Addition- ally, by combining a zero-shot learning method with the DAL framework, we could also expand its range of applications. Moreover, the current framework could be used only for classification tasks. It would be worth exploring its application in regression tasks."}]}