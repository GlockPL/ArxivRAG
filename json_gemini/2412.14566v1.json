{"title": "AlArena: A Blockchain-Based Decentralized Al Training Platform", "authors": ["Zhipeng Wang", "Rui Sun", "Elizabeth Luis", "Tuo Zhou", "Yizhe Wens", "Jiahao Suns"], "abstract": "The rapid advancement of AI has underscored critical challenges in its development and implementation, largely due to centralized control by a few major corporations. This concentration of power intensifies biases within Al models, resulting from inadequate governance and oversight mechanisms. Additionally, it limits public involvement and heightens concerns about the integrity of model generation. Such monopolistic control over data and AI outputs threatens both innovation and fair data usage, as users inadvertently contribute data that primarily benefits these corporations.\nIn this work, we propose AIARENA, a blockchain-based decentralized AI training platform designed to democratize Al development and alignment through on-chain incentive mechanisms. AIARENA fosters an open and collaborative environment where participants can contribute models and computing resources. Its on-chain consensus mechanism ensures fair rewards for participants based on their contributions. We instantiate and implement AIARENA On the public Base blockchain Sepolia testnet, and the evaluation results demonstrate the feasibility of AIARENA in real-world applications. Our project page is available at https://train.flock.io/explore.", "sections": [{"title": "Introduction", "content": "The growth of artificial intelligence (AI) has led to groundbreaking applications in many fields. However, it also brings challenges, especially because a few large companies control most of its development and use. This concentration of power increases bias in AI systems [1] and reduces public involvement in important decisions. This lack of transparency can exacerbate issues such as misuse of Al technologies or unethical practices, as there is little external oversight in the AI centralized systems. Such centralized control also slows down innovation in AI [2, 3] and leads to the unfair use of user data, which mainly benefits these companies. These compounded challenges underline the urgent need for a shift to decentralized AI (DeAI) [4], where control and access are distributed across a broader and more inclusive range of participants.\nBlockchain's decentralized nature can help address the above challenges by allowing multiple participants to collaborate on AI development without relying on a central authority [5, 6]. Smart contracts [7] can automate processes such as distributing rewards or verifying contributions, fostering fairness and efficiency. Blockchain also enhances data integrity, as participants can verify the source and ownership of data, preventing unauthorized use and ensuring compliance with ethical standards [8].\nIn this work, we propose AIARENA, a blockchain-based platform designed to decentralize AI training in an open and collaborative environment, where participants can contribute models and computing power. Specifically, AIARENA leverages on-chain consensus mechanisms to ensure that only valid contributions are rewarded, while incentivization models encourage active and meaningful participation to prevent issues such as free-riding or fake identities.\nContributions. Our contributions are summarized as follows:\n\u2022 We propose AIARENA, a blockchain-based decentralized platform for Al training. AIARENA enables the AI model to be optimized directly on users' devices using their own data or publicly available datasets, and finally derives the optimal model based on blockchain consensus. By leveraging blockchain, AIARENA ensures sustained machine learning (ML) contributor (i.e., training node and validator) engagement and provides fair rewards based on their contributions in enhancing models.\n\u2022 We introduce a delegation stake design for AIARENA, enabling users with limited computational resources to participate in model generation by delegating their stake to preferred ML participants. This design encourages broader involvement in the ML model generation process, further promoting decentralization.\n\u2022 We instantiate and implement AIARENA on the public Base blockchain Sepolia testnet\u00b9. Over an approximate 7-month period, AIARENA has engaged 603 training nodes, 1,051 validators, and 63,265 delegators, collaboratively generating 18,656 models across 16 training tasks. These results highlight the practical feasibility of AIARENA in real-world applications.\n\u2022 We evaluate three popular tasks performed by AIARENA contributors. Their models not only outperform baseline models but also exceed the performance of larger state-of-the-art (SOTA) models. Notably, in the code co-creation task, our contributors collaboratively curated the largest existing Move code dataset, along with their corresponding code instructions and comments."}, {"title": "AIARENA System Design", "content": "Fig. 1 shows the overview of AIARENA system. There are various categories of participants in the AIARENA system:"}, {"title": "Participants", "content": "\u2022 Task Creators: Task creators are responsible for defining training tasks and their specific requirements, including selecting and designing appropriate learning algorithms for model training and validation. To promote decentralization, the created tasks can be reviewed and verified by the stakeholders within AIARENA.\n\u2022 Training Nodes: Given a created task, training nodes are responsible for training the model on their service using public data. To participate, training nodes must stake blockchain assets or tokens to establish their eligibility. Training nodes will receive rewards based on their stake amount and performance.\n\u2022 Validators: Validators are responsible for assessing the work completed by training nodes and submitting validation scores that influence the distribution of rewards. To participate, validators must stake tokens, which not only allows them to validate assigned tasks but also ensures fair task allocation.\n\u2022 Delegators: Delegators contribute to the AIARENA system by supporting the staking process of other participants without directly engaging in task training or validation. They stake tokens on behalf of other participants, boosting the delegatees' capacity to earn greater rewards. In return, delegators share in the rewards earned by their delegatees, according to predefined algorithms that reflect their staked contributions."}, {"title": "Training and Validation", "content": ""}, {"title": "Training", "content": "We consider the dataset held by the training node, $D_{local}$, which contains locally sourced data samples, comprising feature set X and label set Y, with each sample $x_i \\in X$ corresponding to a label $y_i \\in Y$. We define a predictive model f, aiming to learn patterns within D such that $f(x_i) \\approx y_i$.\nTo quantify the prediction metric, e.g., accuracy, the task trainer will introduce a loss function $L(f(x_i), y_i)$, assessing the discrepancy between predictions $f (x_i)$ and actual labels $y_i$. A generic expression for this function is: $L = \\frac{1}{N} \\sum_{i=1}^{N} l(f(x_i), y_i)$, where N denotes the total sample count, and l signifies a problem-specific loss function, e.g., mean squared error or cross-entropy loss.\nThe optimization goal is to adjust the model parameters \u03b8 to minimize L, typically through algorithms such as gradient descent: $\\theta_{new} = \\theta_{old} \u2013 \\eta \\nabla L$, where \u03b7 represents the learning rate, and VoL the gradient of L with respect to \u03b8. Utilizing the aggregated dataset D, parameter @ is iteratively updated to reduce L, consequently improving the model's predictive accuracy. This optimization process is conducted over a predefined number of epochs E, each epoch consisting of a complete pass through the entire dataset D."}, {"title": "Validation", "content": "Consider a selected group of validators, denoted as $V_j \\in V$, each equipped with the evaluation dataset $D_{eval}$ from the task creator. This dataset consists of pairs $(x_i, y_i)$, where $x_i$ represents the features of the i-th sample, and $y_i$ is the corresponding true label. The model, trained by designated training nodes, is denoted as otask. The primary objective of $o_{task}$ is to predict the label $\u0177_i$ for each feature vector $x_i$ contained within $D_{eval}$. To assess the performance of $o_{task}$ on $D_{eval}$, we use a general evaluation metric denoted by eval. We exemplify with accuracy as follows:\n$eval(\\Theta_{task}, D_{eval}) = \\frac{1}{|D_{eval}|} \\sum_{(x_i, Y_i) \\in D_{eval}} 1(\u011d_i = Y_i) $ (1)"}, {"title": "Consensus and Reward Distribution", "content": "We assume there are n submissions $(o_1,..., o_n)$ from n training nodes with stakes $(t_1,..., t_n)$, and m validators $(V_1,..., V_m)$ with stakes $(s_1,..., s_m)$. Each validator $V_j(1 \\le j \\le m)$ evaluates the n models submitted by the training nodes, producing a score vector $r_j = (r_{j1},..., r_{jn})$. These scores reflect the performance of each model according to predefined criteria as shown in Eq. 1."}, {"title": "Reward Distribution within One Task", "content": "Within a single task, the reward distribution between training nodes and validators is determined based on their relative stake amounts. Let the total daily reward allocated to a task be denoted as Ro. The total rewards for training nodes are: $R_o \\cdot (\\gamma + (1 - 2\\gamma) \\cdot \\frac{\\sum_{i=1}^{n} t_i}{\\sum_{i=1}^{n} t_i + \\sum_{j=1}^{m} S_j})$. Similarly, the total rewards for validators are: $R_o \\cdot (\u03b3 + (1 - 2\u03b3) \\cdot \\frac{\\sum_{j=1}^{m} S_j}{\\sum_{i=1}^{n} t_i + \\sum_{j=1}^{m} S_j})$. The parameter \u03b3 controls the split rewards, defining the balance between fixed and stake-dependent reward components."}, {"title": "Reward for Training Nodes", "content": "The final scores of the submitted model are determined through a weighted aggregation: $r_i = \\frac{\\sum_{j=1}^{m} r_{ji} \\cdot S_j}{\\sum_{j} S_j} $. This means that the evaluations of validators with higher stakes have a larger impact on the final outcome.\nWe then compute the following geometry series: $g_k = \\frac{q^k-q^{k-1}}{1-q^m}$, in which k denotes a given training node's rank amongst its peers in the same task, whereas q is the common ratio of the geometric series and m is the number of training nodes in a given task.\nWe finally compute the total rewards allocated for the training nodes as well as their delegators, which is based on the quality of their submission and their total amount of stake: $f_i (g_i, t_i) = \\frac{g_i \\cdot t_i^{a_t}}{\\sum_{k=1}^{m} g_k t_k^{a_t}} \\cdot R_o$, where $t_i$ the total stake amount from the training node i as well as its respective delegators. $a_t$ is a system parameter that determines the influence of the stake on the reward distribution.\nIf a training node i's stake in the task is $t_n$ and stakes delegated to training node i is $t_d$, i.e., $t_i = t_n + t_d$, then the actual reward for training node i is $\\sigma \\cdot f_i(g_i, t_i) \\cdot (\\sigma + (1 - \u03c3) \\cdot \\frac{t_n}{t_i})$, \u03c3 is the reward ratio set by the training node, which determines the ratio of rewards shared between the training node i and its respective delegators."}, {"title": "Reward for Validators", "content": "For each validator Vj, we compute the distances between their score and the final aggregated score: $\u0394_j = (\u0394_{j1}, ..., \u0394_{jn}) = (r_{j1} - r_1, ..., r_{jn} - r_n)$.\nWe define a distribution function fi, which satisfies: (1) $f_1 (\u0394_{1i}, s_1) + ... + f_m (\u0394_{mi}, s_m) = 1$; (2) fi decreases over the distance \u0394ji, and (3) fi increases over the stake amount sj. To fulfill the three criteria, we can employ a modified version of the Softmax Function: $f_i(\u0394_{ji}, S_j) = \\frac{e^{-\\lambda \u0394_{ji} \\cdot \u03b1}}{\\sum_k^{m} e^{-\\lambda \u0394_{ki} \\cdot \u03b1}}$ . The parameters \u03bb and \u03b1 play crucial"}, {"title": "Delegate Staking", "content": "Delegators may entrust their tokens to participants of their choosing to receive a passive investment income stream. The receivers can thus amplify their stake and rewards. These rewards are shared with the delegators, to attract users who have tokens but lack the technical expertise to perform AI model training or validation.\nSpecifically, the delegator rewards depend on: (1) The quality of the training nodes or validators selected for delegation, and (2) The amount of stake delegator has delegated.\nFormally, the the reward for a delegator who delegates to a training node can be calculated as: $\\frac{t_d}{t_n+t_d} \\cdot f_i (\u03c3, t_a)$, whereas fi refers to the total reward distributed to the training node i and delegator based on the quality of the training node's submission, td is the stake amount from this given delegator, tn is the stake amount from the training node i. Similarly, the reward for a delegator who delegates to a validator can be calculated as: $(\u2211i fi(\u2206ji, Sj)) \u00b7\u03c3\u00b7 \\frac{S_d}{S_o+S_d}$ in which $s_d$ refers to the stake amount from a given delegator and $s_v$ is the stake amount of the validator the delegator delegated to."}, {"title": "Various Validation Phases", "content": "To address potential attacks, such as lookup attacks and model-stealing attacks by malicious training nodes, AIARENA validators employ diverse validation datasets across different phases. For a task spanning x days, the process is divided into three phases:\n(1) Submission Phase: This initial phase lasts x0 days and provides daily rewards to training nodes based on the validator consensus results for each day. The continuous reward mechanism encourages sustained participation by offering consistent feedback and reinforcing contributors' efforts.\n(2) Final Validation Phase: Lasting x\u2081 days, this phase utilizes a validation dataset distinct from the one used in the submission phase. This diversification increases the difficulty for malicious actors to exploit predictable validation scenarios, enhancing the system's robustness.\n(3) Challenging Phase: Spanning x \u2212 x0 \u2212 x1 days, this phase allows any validator to issue a challenge if they suspect a model has been stolen. Training nodes must provide proof of legitimate training, such as proof of learning [9, 10] or on-chain address-based watermarking [11]. If they fail to do so, the successful challenger receives the rewards allocated to the malicious training node, incentivizing honest participation.\nThis phased approach ensures a dynamic and secure validation process, mitigating risks associated with malicious training activities."}, {"title": "Implementation and Evaluation", "content": ""}, {"title": "On-Chain Implementation", "content": "We implement AIARENA on the public Base blockchain Sepolia testnet. We leverage Solidity to implement the on-chain reward calculation and distribution smart contracts. We run the system from April 30, 2024, to December 9, 2024. For each day, 1074 tokens will be minted and rewarded to the participants, and y = 0.7. Table 1 reports the summary statistics of all trained tasks on AIARENA.\nNumber of Participants. In total, we observe that 603 training nodes, 1,051 validators, and 63,265 delegators have participated in the system to train and validate the 16 various tasks, accumulating 18,926 training and 2,225,254 validation submissions (see Fig. 2). Across most tasks, the number of validators consistently exceeds the number of training nodes. For example, in task 10, overall 391 validators participated, compared to only 128 training nodes. Even for smaller-scale tasks such as task 14, the number of validators outnumbered training nodes."}, {"title": "Reward Distribution", "content": "The reward distribution among training nodes and validators reveals interesting patterns. Training nodes generally receive higher rewards per participant compared to validators before in tasks 1-12, due to the smaller number of training nodes. However, the variability in rewards among training nodes is much higher, as evidenced by the standard deviation, which indicates a broader range of effort and contribution levels. This suggests that while training tasks may offer higher potential rewards, they also come with greater uncertainty, whereas validation tasks provide more consistent but relatively lower rewards. Such dynamics could further explain the observed user preference for validation tasks, which offer steadier returns with less variance in earnings."}, {"title": "Off-Chain Implementation", "content": "To demonstrate the feasibility of AIARENA in realistic scenarios, we evaluated three diverse and popular tasks using the methodology proposed by the platform\u00b2. The results are summarized in Table 2. Meanwhile, we employ the Human Preferences Evaluation method, with the voting results by GPT-40 presented in Fig. 3. The results show that all Rank-1 training nodes' models outperform baseline models, larger SOTA models, and baseline LLMs finetuned on the provided or subset CMC and MBLC datasets."}, {"title": "Text-to-SQL for Onchain Data (Task 10)", "content": "Text-to-SQL, which translates natural language into SQL queries, has advanced in various domains but remains largely unexplored for complex blockchain data, a gap that limits accessible, data-driven insights into on-chain transactions, token transfers, and smart contract states. To address this problem, we launched a text-to-SQL task on the AIARENA"}, {"title": "Real Life Simulator (Task 11)", "content": "Life Simulator is a dynamic, LLM-powered game where players guide a character's growth through career, relationships, and personal choices, experiencing unique, adaptive outcomes in a realistic virtual world. However, most current LLMs tend to generate overly optimistic life stories, which deviate significantly from realism and undermine the core purpose of a life simulator. To address this, the AIARENA platform has been utilized to gather contributions from the community, enhancing LLMs' ability to generate more realistic and diverse life scenarios."}, {"title": "Code Co-creation (Task 14)", "content": "Accurate code generation and understanding are critical when leveraging LLMs for advanced automation, particularly in low-resource blockchain languages. Building on this need, we focus our efforts on the Move language, aiming to develop a model capable of generating accurate instructions and comments. Through our AIARENA platform, a diverse community of contributors collectively curated what we believe, to the best of our knowledge, to be the largest Move instruction dataset to date\u00b3. Drawing from 514 distinct Move projects, they enriched the source code with detailed instructions and comments."}]}