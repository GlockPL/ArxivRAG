{"title": "AdaptiveLog: An Adaptive Log Analysis Framework with the Collaboration of Large and Small Language Model", "authors": ["Lipeng Ma", "Weidong Yang", "Yixuan Li", "Ben Fei", "Mingjie Zhou", "Shuhao Li", "Sihang Jiang", "Bo Xu", "Yanghua Xiao"], "abstract": "Automated log analysis is crucial to ensure high availability and reliability of complex systems. The advent of large language models (LLMs) in natural language processing (NLP) has ushered in a new era of language model-driven automated log analysis, garnering significant interest. Within this field, two primary paradigms based on language models for log analysis have become prominent. Small Language Models (SLMs) (such as BERT) follow the pre-train and fine-tune paradigm, focusing on the specific log analysis task through fine-tuning on supervised datasets. On the other hand, LLMs (such as ChatGPT) following the in-context learning paradigm, analyze logs by providing a few examples in prompt contexts without updating parameters. Despite their respective strengths, both models exhibit inherent limitations. By comparing SLMs and LLMs, we notice that SLMs are more cost-effective but less powerful, whereas LLMs with large parameters are highly powerful but expensive and inefficient. To trade-off between the performance and inference costs of both models in automated log analysis, this paper introduces an adaptive log analysis framework known as AdaptiveLog, which effectively reduces the", "sections": [{"title": "1 Introduction", "content": "Logs play a pivotal role in maintaining network and software systems by recording crucial runtime information. They serve as essential references for engineers, ensuring the high availability and reliability of complex systems, especially for 24*7 online services like Google, Bing, and Facebook [35, 68]. However, with the rapid growth volume of logs, it is more challenging for engineers to extract valuable information from massive logs [73]. Consequently, the concept of automated log analysis has been proposed, aiming to leverage machine learning (ML) or deep learning (DL) techniques for analyzing logs automatically [23, 35, 46]. Typical log analysis scenarios include anomaly detection [12, 34, 44, 72], root cause analysis [55, 58, 64], and failure prediction [9, 19, 71]. Recently, with the success of pre-trained language models (PLMs) in natural language processing (NLP), especially the advent of large language models (LLMs) represented by ChatGPT and GPT-4 [1], LM-based log analysis has gained substantial attention, showcasing remarkable achievements attributed to their powerful understanding and reasoning capability [28, 36, 38, 43, 45, 59, 65, 66]. Currently, there exist two mainstream paradigms based on language models \u00b9 in the realm of automated log analysis. The first one relies on small language models (SLMs) represented by BERT [10], which follow the pre-train and fine-tune paradigm, fine-tuning a PLM with annotated task-specific data and thereby enhancing its proficiency in the specific log analysis task [3, 8, 36, 37]. The fine-tuned SLM can analyze the specific task adeptly and efficiently [48]. The second one relies on LLMs represented by ChatGPT, which follow the in-context learning (ICL) paradigm, enabling LLMs to learn from a few examples with prompts without updating their parameters. By designing high-quality prompts with clear instructions and examples, LLMs with strong reasoning ability analyze logs effectively [65, 66]. However, these two prevailing language models for log analysis still exhibit distinct shortcomings. As shown in Fig. 1, on the one hand, the capability of the specialized SLM is limited. Considering the diverse and complex datasets encountered in log analysis, these SLMs might exhibit sub-optimal performance due to their training limitations, which confine them to specific samples. For instance, numerous studies [35, 46] on anomaly detection indicate that many anomaly patterns are non-typical and occur with low frequency, which hinders the model performance. On the other hand, LLM-based methods are computationally inefficient and costly due to their"}, {"title": "2 Related Works", "content": "Large language models have significantly advanced the field of NLP, leveraging extensive training corpora and computational resources [30]. Notably, LLMs such as ChatGPT and GPT-4 [1], with their vast parameter scales and alignment with human feedback, have opened new avenues for software engineering [13, 15, 22, 40, 60]. With the significant increase in parameter scale and corpus size, LLMs have become more and more powerful. Researchers have discovered that these capabilities can be effectively harnessed through textual prompts [4, 56]."}, {"title": "2.2 Automated Log Analysis with Language Models", "content": "The success of pre-trained language models (PLMs) and large language models (LLMs) in natural language processing (NLP) has inspired significant advancements in automated log analysis. This research can be broadly categorized into two main approaches. One type of research is based on the pre-trained language model represented by BERT [10], these works directly represent logs with the pre-trained model or fine-tune on specific datasets to analyze logs. HitAnomaly [25], SwissLog [39], NeuralLog [34] and ClusterLog [14] utilize the pre-trained model to obtain the embedding of logs for log-based anomaly detection. Following the pre-train and fine-tune paradigm of the pre-trained model, BertLog [8], LogEncoder [53], LogPPT [36], HilBERT [26], Lanobert [38], LogFiT [3] and PreLog [37] utilize different fine-tuning strategies to train the model on specific log analysis tasks, which significantly improves the ability of log analysis. In addition, these log analysis works are mainly based on general pre-trained language models, and due to the differences between natural language and logs, many works further enhance the pre-trained model's log understanding ability by pre-training on log corpus. Biglog [59] has been proven to handle complex log analysis tasks better by learning in-sentence and cross-sentence features of logs. KnowLog [45] first proposes leveraging domain knowledge to enhance log pre-training, which enables further improvement of the model's log understanding. The other type of research is based on large language models represented by ChatGPT and GPT-4, these works draw on the in-context learning (ICL) capabilities of LLMs to analyze logs without updating the parameters. LLMParser [48] first evaluates the performance of different LLMs on log parsing and finds that LLMs significantly outperform other state-of-the-art parsers. In addition, LogPrompt [43], DivLog [66], UniLog [65], SCLogger [41] and LILAC [28] also demonstrate the effectiveness of log analysis using LLMs. Despite the superior performance of LLMs they also face the challenge of inefficiency and high cost, and the performance of the smaller pre-trained model is relatively weak but cost-efficient. LLMParser [48] experimentally verified that the smaller language model is more efficient than complex LLMs. This paper introduces a pioneering log analysis framework designed to fully exploit the advantages of both large and small language models."}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Overview", "content": "Fig. 2 shows the framework of AdaptiveLog, which consists of three key steps when inference. Specifically, given a specific log analysis task and a corresponding SLM, the SLM first gives its prediction in the first step. Then, the uncertainty of SLM is estimated through Bayesian inference to adaptively select the LLM in the second step, which would give a probability of uncertainty. Finally, when the probability of uncertainty is higher, AdaptiveLog will query the LLM to analyze logs in the third step, otherwise, it will directly output the results of the SLM. To enhance the reasoning ability of the LLM on complex logs, we propose a novel error-case reasoning enhancement prompt strategy by retrieving similar error-prone cases as the reference, which can leverage past error experience and learn solutions from these cases. Accordingly, the framework also includes a preparation stage for the collection of error-prone cases, where we collect error samples of the SLM on this task and then utilize the LLM to analyze their reasoning processes about ground truth and potential pitfalls susceptible to errors. Following this, this information is structured into the error-prone cases database within a key-value datastore to support the reasoning of the LLM. In the following sections, we first describe the preparation stage of AdaptiveLog, and then describe the details of uncertainty estimation and the prompt strategy for reasoning."}, {"title": "3.2 Error-prone Cases Collection", "content": "Given a SLM $M_{SLM}$ for a specific log analysis task, to analyze why error samples tend towards inaccuracies and to avoid errors again when reasoning with the LLM $M_{LLM}$, we construct an error-prone case database $D$, which serves as a foundation to bolster the reasoning of the LLM. This database is created by filtering the logs where the SLM goes wrong and analyzing them based on the LLM, finally storing them as error-prone cases, consisting of the raw log and the detailed analysis. Firstly, given a task-specific validation set $V = \\{(x_i, y_i)\\}$ and the SLM $M_{SLM}$, $M_{SLM}$ is then filtered for correct samples $S_{Correct} = \\{(x_i, y_i)|(x_i, y_i) \\in V, M_{SLM}(x_i) = Y_i\\}$ and error samples $S_{Error} = \\{(x_i, y_i)|(x_i, y_i) \\in V, M_{SLM}(x_i) \\neq y_i\\}$ on $V$, where $x_i$ is the input log and $y_i$ is the ground-truth. Here, we can calculate the error"}, {"title": "3.3 Uncertainty Estimation", "content": "When given new input x, the SLM first gives the prediction $y' = argmax(P_{SLM}(y|x))$, where $y'$ represents the predicted category with the highest probability. Then we estimate the uncertainty probability of model p(C|(x, y')) conditional on the given input x and the prediction y', where event C indicates that the model is uncertain about the result. However, p(C|(x, y')) cannot be calculated directly, we employ Bayes' rule to solve, that is:\np(C|(x, y')) \\propto p(C) \\times p((x, y')|C),\nwhere p(C) represents the prior probability. Importantly, the result of the model is usually incorrect when the model is uncertain [18, 31]. Assuming that observing different inputs are independent and identically distributed (i.i.d.), we can use the error rate err (Equation 1) as the prior probability p(C) = err. The likelihood probability p((x, y')|C) indicates the probability of observing (x, y') given the event C. We can observe the predictions y' of the model multiple times by giving the same x. If there is a difference in the predictions, it is classified as event C. Bayesian inference utilizes these new observations to update the prior probability. We perform N times predictions for the same input as the newly considered observations. Assuming the event C occurs $\\alpha$ times, then N - $\\alpha$ times the model is certain. The likelihood probability p((x, y')|C) can be calculated as follows:\np((x,y')|C) \\approx p(C)\\propto(1 - p(C))^{N-\\alpha},\nBased on the Bayesian inference, p(C) is a distribution represented by $\\theta$ = p(C). Considering that the expectation of this distribution is equal to err, it is reasonable to consider p($\\theta$) ~ Beta(err, 1 \u2013 err). The selection of the Beta distribution on the one hand because of its flexibility and suitability for modeling probabilities between 0 and 1, and on the other hand it is convenient to compute since it belongs to the conjugate prior distribution [29]. Hence, based on the new observation on (x, y'), we can deduce the posterior distribution p($\\theta$|(x, y')):\np(\\theta|(x, y')) ~ Beta(err + $\\alpha$, (1 \u2013 err) + N \u2212 $\\alpha$).\nAt last, the calculation of p((x, y')|C) can be formulated as:\np(C|(x, y')) = E(p(\\theta|(x, y')))\n=\\frac{err + \\alpha}{err + \\alpha + (1 - err) + N - \\alpha}\n=\\frac{err}{N+1}+ \\frac{\\alpha}{N + 1},"}, {"title": "3.4 Error-Case Reasoning Enhancement Strategy", "content": "Based on Equation 9, AdaptiveLog performs adaptive model selection. The LLM is invoked when the probability of model uncertainty surpasses that of model certainty; otherwise, it directly yields the output of the SLM without querying the LLM. We designate samples with low uncertainty that do not necessitate the invocation of the LLM as simple samples, otherwise as hard samples. Since hard samples are error samples of the SLM, understanding these error-prone logs requires extensive experience and knowledge compared to simple logs. To improve the reasoning of the LLM over complex logs, inspired by the classical AI paradigm case-based reasoning [32, 62] and critical learning pattern of humans [49, 54], which can refer to useful experiences and learn from mistakes to avoid making them again, we propose a novel prompt strategy called Error-Case Reasoning Enhancement (ECR) employing error-prone cases to enhance the reasoning of the LLM. ECR solves the complex log analysis task by retrieving similar error-prone cases, referring to their reasoning process and noticing the potential pitfalls susceptible to errors. As shown in Fig. 2 (3), ECR based on LLMs first retrieves similar cases from the error-prone case database D to construct the prompt, and then the LLM follows the prompt's instructions to reason. By referring to these cases, the LLM first gives the reasoning process and then outputs the result. Now we elaborate on the error-prone case selection and prompt strategy as follows."}, {"title": "3.4.1 Error Case Selection", "content": "Since LLMs are not inherently trained for log analysis, they lack precise knowledge to analyze these complex logs. To enable the LLM to learn how to analyze logs and avoid the same mistakes, ECR retrieves relevant cases from database D that are similar to the current input logs. Specifically, we first embed"}, {"title": "3.4.2 Prompt Strategy", "content": "Designing an appropriate prompt is the most crucial part of the error-case reasoning enhancement strategy. As shown in Fig. 3, a carefully designed prompt for ECR including task description, requirement, selected similar cases, and input logs. First, the task description is designed to introduce the specific log analysis task to be performed. Next, we require the LLM to refer to the reasoning process of these error-prone cases and notice the potential pitfalls. By referring to these cases, the LLM effectively learns solutions and insights from them to analyze logs. This allows the LLM to align with the capabilities of domain experts. The LLM is required to give the reasoning process first, and then output the result. This facilitates not only the learning of the reasoning process but also the explicit understanding of the LLM's reasoning process. In addition, to reduce the redundant contents in the raw output, we require the LLM to follow the format of similar cases. Then we can easily utilize regular expressions to extract the result in the output. For selected similar cases, many studies [33, 66, 75] have demonstrated that the permutation of various examples in the context can significantly impact performance. For example, Zhao et al. [75] highlighted that the model's prediction for a query may exhibit bias toward the most recent example (referred to as recency bias). This implies that if the closest example to the query within the prompt bears sufficient similarity to the"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Environment and Implementation", "content": "In our experiments, we choose the widely used BERT [10] as the SLM. Although the pre-trained model for logs, such as Biglog [59] yields better performance than BERT, we choose the general pre-trained model BERT given the extra expense of pre-training on logs and BERT is more available in practice. Given the specific log analysis task, we first fine-tune BERT on the training set to adapt it to the specific task. During fine-tuning, we set the batch size as 16, and epochs as 3. Moreover, the optimizer we adopt is Adam with a learning rate of 5e-5 and the linear schedule with a warmup ratio of 0.01. Considering the performance and price of the LLM, we choose the most popular ChatGPT (gpt-3.5-turbo-16k) due to its popularity in the community to analyze error cases and perform ECR. We utilize HTTP requests to invoke the OpenAI APIs and interact with ChatGPT. In addition, the embedding model used for sample selection in ICL and ECR we choose bge-large-en-v1.5, due to its excellent performance among all open-source embedding models. The demonstration examples of ICL are selected from the training set and set to 5. To increase the stability of LLM's output and ensure reproducibility for the same query, we set the temperature of ChatGPT to 0. We conduct all the experiments on 4 NVIDIA RTX 3090 GPUs."}, {"title": "4.2 Downstream Tasks and Datasets", "content": "To verify the usability and generalizability of AdaptiveLog on different log analysis tasks, we conduct experiments on six different downstream tasks across different domains, including software system and network device logs. Following existing studies in software system logs [23, 76], we conduct experiments on Anomaly Detection and Failure Identification, two of the most widely studied tasks in the log analysis field. Considering that existing public datasets for these tasks often exhibit relatively simple anomaly patterns [68, 74], we also conduct experiments on network device logs to verify the generalizability across different log analysis domains. These tasks require domain knowledge and can effectively evaluate model capabilities, a prerequisite for tackling other downstream tasks, posing significant challenges. Following the task construction process on network device logs [45], we construct four different log analysis tasks across two vendors, Cisco\u00b3 and Huawei, including two devices: Switches and Routers. We provide statistics for different tasks of their datasets in Table 1 and 2. Next, we give an introduction to each task and its evaluation metrics."}, {"title": "4.2.1 Downstream Tasks of Software System Logs", "content": ""}, {"title": "(1) Anomaly Detection (AD)", "content": "AD is a widely researched log analysis task to predict whether anomalies exist within a short period of log messages. We take the log sequence as input, then the model analyzes the anomaly in a sequence.\nDataset and Metric. We evaluate the anomaly detection performance on two datasets (BGL and Thunder-Bird) from Loghub [76], which contributes large-scale system log datasets to the community. BGL and ThunderBird are selected based on the simplicity of anomaly patterns in other public Anomaly Detection datasets [68, 74]. For example, BERT achieves 100.0 / 100.0 / 100.0 Precision / Recall / F1-value on HDFS, which is insufficient for evaluating AdaptiveLog. Given the extensive size of BGL and Thunderbird, we take 200,000 raw logs in chronological order with a window size of 20, following previous research [35], these logs are grouped into datasets (6,000 / 2,000 / 2,000) chronological selection to prevent data leakage. Following Biglog [59], we concatenate each log in the sequence and then input the language model. Following previous anomaly detection works [34, 35, 59], we use Precision, Recall, and F1 on the anomaly class as evaluation metrics."}, {"title": "(2) Failure Identification (FI)", "content": "Unlike anomaly detection identifying present faults from logs, FI further identifies what type of failure occurs in the anomaly log.\nDataset and Metric. We collect this dataset from [9], which is an OpenStack dataset including 396 failure tests and 16 kinds of API errors, such as \u201cserver add volume error\u201d. Different from the binary classification task of anomaly detection, this task is a 16-class classification task for 16 error types in the dataset. Considering that engineers are interested in whether the top-K recommended results contain the correct error, we report the Recall@k rate as the evaluation metric."}, {"title": "4.2.2 Downstream Tasks of Network Device Logs", "content": ""}, {"title": "(1) Module Classification (MC)", "content": "MC is a multi-class classification task aiming at identifying which module the log originates from, where the model needs to understand the contextual information of the logs to accurately identify their source.\nDataset and Metric. We collect network device logs from public documentation and replace the module name of raw logs with [MASK] as input, the module name in the log as ground truth. As the unbalanced multi-class classification task and considering the importance of different classes, we report Accuracy and Weighted F1 as evaluation metrics."}, {"title": "(2) Level Prediction (LP)", "content": "LP is a binary classification task designed to determine the risk level of the given log: ERROR or INFO. The ERROR level indicates that the log has a serious impact on the system, while the INFO level indicates that the log is informational only."}, {"title": "4.3 Baselines", "content": "We categorize baselines for log analysis into three groups according to technology type: traditional deep-learning methods (CNN [44], BiLSTM [72]), pre-trained language models (BERT [34, 38], Biglog [59], KnowLog [45]), and large language models (ChatGPT). CNN and BiLSTM convert each log message into a vector with the word embedding model and then input the vector to a deep neural network (CNN or BiLSTM) to analyze logs. As a general pre-trained language model, BERT is the first and most widely used pre-trained model for log analysis. It can represent logs well due to its excellent semantic representation. As a pre-trained model specifically designed for logs, Biglog excels in capturing essential features due to its extensive training on the log corpus. KnowLog is the state-of-the-art log pre-trained model in log analysis due to pre-training on log corpus with knowledge enhancement, equipping it with the domain knowledge needed to understand logs. For fairness, we reproduce Biglog and KnowLog with the same pre-training setting on our log corpus (all training sets). Following pre-train and fine-tune paradigm, all pre-trained models are fine-tuned in log analysis tasks with corresponding training sets. ChatGPT has demonstrated amazing application potential in multiple domains with its excellent natural conversation capabilities and extensive knowledge base. Following in-context learning paradigm, we select top-5 examples from the training set for each input log and use the same embedding model as ECR to select examples. In addition, to validate the effectiveness of our proposed ECR strategy, we also compare it with the ICL ability of LLMs to analyze the hard samples. Hence, we also report the results of analyzing hard samples with ICL in our experiments."}, {"title": "4.4 Evaluation", "content": "We evaluate AdaptiveLog by answering the following research questions (RQs):"}, {"title": "4.4.1 RQ1: How effective is AdaptiveLog compared with the current mainstream methods on down-stream tasks?", "content": "This section compares the AdaptiveLog with other state-of-the-art log analysis methods on six downstream tasks. In addition, AdaptiveLog divides the simple and hard samples based on the probability of"}, {"title": "4.4.2 RQ2: How effective is AdaptiveLog in the low-resource scenario?", "content": "In real-world log analysis scenar-ios, due to the massive size of logs, it's usually difficult to obtain sufficient labeled data [35], which is a challenge for data-driven log analysis methods. To verify the effectiveness of AdaptiveLog in this low-resource scenario, we conduct experiments on Anomaly detection, Log and Description Semantic Matching, and Level Prediction three tasks. Following these existing works, we assume that only 10% of the labeled data is available in the training set for each task, which means that the smaller pre-trained model is fine-tuned on only 10% of training data."}, {"title": "4.4.3 RQ3: How effective is AdaptiveLog in the transfer learning scenario?", "content": "In the context of transfer learning for log analysis, where models need to adapt to different vendor logs due to variations in syntax [7], the effectiveness of AdaptiveLog in transfer scenarios becomes a key focus. Deploying different models for specific tasks based on vendor logs incurs the challenge of requiring annotated data from each vendor, resulting in operational inefficiencies. The expectation is to train one single model on annotated data from one vendor and successfully transfer its learning to analyze logs from different vendors, showcasing the model's transfer learning capabilities. To assess the transfer ability of models across different vendor logs, we conduct experiments in Level Prediction, Log and Semantic Description, and Log and Possible Cause Ranking three tasks. Specifically, the SLM is trained using a training dataset from a source vendor or device and subsequently evaluated on a separate testing dataset from a target vendor or device."}, {"title": "4.4.4 RQ4: How effective is AdaptiveLog combined with different SLMs?", "content": "Considering that there will be different SLMs in the real scenario, to verify the flexibility of AdaptiveLog, we conduct experiments on deep learning models and other SLMs. The key for AdaptiveLog to combine with other SLMs is that Bayesian inference is effective on these SLMs. Specifically, apart from BERT, we exploit traditional deep-learning models (CNN and BiLSTM) and different structures of SLMs, including encoder-only (Biglog and KnowLog), decoder-only (Llama-3.2-3B-Instruct), and encoder-decoder models (ChatGLM-6B), as SLMs and conduct experiments on the Log and Description Semantic Matching task. For generative SLMs, we calculate the average probability of all output tokens as the prediction probability to estimate uncertainty. The experimental results are shown in Table 6, we find that both models can outperform the original model when combined with the LLM, which implies that AdaptiveLog is effective in combination with different SLMs. In addition, from the perspective of growth, we also find that the weaker the performance of the SLM, the more the final result is improved when combined with the LLM. This implies that Bayesian inference is effective in filtering out hard samples to give to the LLM, and also validates the effectiveness of Bayesian inference in estimating the uncertainty of different SLMs. Due to pre-training on log corpus, Biglog and KnowLog outperforms BERT and it has been shown that as the capabilities of the SLM increase, the final performance of AdaptiveLog usually improves as well. We choose BERT as the SLM in our experiments due to its easier accessibility and wider application compared to Biglog and KnowLog. Despite combining with the weaker SLM, AdaptiveLog still"}, {"title": "4.4.5 RQ5: How efficient is AdaptiveLog in the automated log analysis scenario?", "content": "In this section, our primary focus is on assessing the efficiency of the execution process within AdaptiveLog. More specifically, we record the execution times for AdaptiveLog including uncertainty estimation time and ECR (or ICL) time. We take BERT as the SLM and deploy it on one NVIDIA RTX 3090 GPU, and then we count the total running time of Adaptivelog during prediction on LDSM and LPCR tasks, where the SLM needs to perform 10 observations to calculate the uncertainty and hard samples are analyzed by the ChatGPT. Additionally, the runtime of ICL-based ChatGPT on all test samples is also recorded for comparison. According to the results in Fig. 6, we can see that AdaptiveLog demonstrates remarkable efficiency compared to ChatGPT. Notably, the uncertainty estimation time for SLMs is exceptionally fast, significantly outperforming the querying of LLMs. The average time spent on uncertainty estimation is merely 4.68% of the total AdaptiveLog runtime, highlighting the efficiency of this process within the framework. Secondly, the ECR-based reasoning strategy, while slightly slower than the ICL-based strategy on hard samples, showcases a meticulous approach in analyzing the reasoning process of complex log samples. Despite this, the time cost associated with the ECR-based strategy remains acceptable, especially considering the performance improvement it offers. Even with this slight time overhead, AdaptiveLog still significantly outperforms ChatGPT, which conducts full-sample inference. On average, AdaptiveLog saves 66.27% of the runtime compared to ChatGPT, emphasizing its superior efficiency in automated log analysis tasks. In addition, to determine the estimation time of uncertainty with respect to the number of observations, we also calculate the times corresponding to different numbers of observations on the LDSM and LPCR tasks. As shown in Fig. 7, we can find that uncertainty estimation time increases linearly with the number of observations. This means that uncertainty estimation for SLMs does not introduce a huge time overhead. We also analyze the effect of different number of observations on the uncertainty estimation, and the results in Fig. 8 show that 3 observations can also effectively determine the SLM's uncertainty, spending only an average of 0.0086 seconds per data, which can also reduce the uncertainty estimation overhead. Empirically, the more observations are made, the more stable the results become, and the user can set the number of observations according to their needs. This efficiency, combined with the framework's superior performance and adaptability, positions AdaptiveLog as a highly efficient and effective solution for automated log analysis, offering enhanced productivity and performance in real-world log analysis scenarios."}, {"title": "4.4.6 RQ6: How do different ECR strategies affect effectiveness?", "content": "This section analyzes the impact of different configurations on ECR from the following two aspects: (1) number of error cases, (2) strategy of case selection. By default, ECR selects 5 most similar error cases as prompt examples in each inference. Analyzing the impact of different configurations on ECR aims to allow users to choose the optimal configuration when analyzing their logs using AdaptiveLog. (1) Error Case Numbers. To investigate the impact of error case numbers in the prompt, we perform comparative analysis on the LDSM task by considering four different case numbers: 1, 3, 5, and 10. The experiment results are shown in Fig. 9. From the results, we find a significant upward trend in model performance when the number of error cases is below 5. When the number exceeds 5, the upward trend of the model's performance is slow and even declines. This suggests that more cases are not better, and an appropriate number of error"}, {"title": "4.5 Ablation Studies", "content": ""}, {"title": "4.5.1 Component Ablation Studies", "content": "To evaluate the contributions of each component in AdaptiveLog, we conduct the ablation experiments. Specifically, two components of the AdaptiveLog are evaluated: Bayesian inference and ECR strategy. In fact, without the ECR strategy, the LLM analyzes hard samples directly using the standard ICL paradigm, where the LLM retrieves similar examples from the training set without the explicit reasoning process, the result has been given in Tables 3 - 5 and analyzed in RQ1. Obviously, the ECR strategy obtains a remarkable improvement compared to ICL for analyzing complex logs, validating the effectiveness of the ECR strategy. In addition, to further analyze the impact of the reasoning process in the case, we remove the Reason part of the error-prone case, and the experimental results are shown in Table 7. The results indicate that the lack of explicit guidance from the reasoning process diminishes the performance of ECR strategy, affirming that potential pitfalls susceptible to errors help to remind the LLM to avoid similar mistakes. Without Bayesian inference, the prior probability p(C) will be used to estimate the uncertainty of the SLM. In addition, given the input log, considering that the selection of the LLM depends on the uncertainty of the SLM, we also compare it with the most straightforward routing strategy to further verify the Bayesian inference's effectiveness. Specifically, we train a binary classifier as the routing model to classify the input logs, the result True means that this log needs to query the LLM, otherwise directly outputs the result of the SLM. The training"}, {"title": "4.5.2 Error-Prone Case Quality Ablation Studies", "content": "A main component of AdaptiveLog is the error-prone case database, which is the backbone of the ECR strategy. To construct these cases, we leverage ChatGPT to analyze and explain the error-prone cases. Considering the quality of the error-prone cases is reliant on the quality of the ChatGPT, which will affect the ECR performance, we evaluate the quality of the error-prone cases on the performance of AdaptiveLog. Firstly, we manually verified 20 cases from each dataset and found no significant errors. Secondly, to au-tomatically assess the quality of error-prone cases, following self-consistency strategy [61], we increase the temperature of ChatGPT (not zero) and re-generate two additional results, then calculate their similarity to the original results with bge-large-en-v1.5. If the similarity is below 0.95, it is considered as the low-quality"}, {"title": "5 Discussion", "content": ""}, {"title": "5.1 Practicality", "content": "AdaptiveLog is designed to perform automated log analysis in production systems by collaborating with the SLM and the LLM. The framework aims to enhance log analysis performance while minimizing the cost associated with querying the LLM. By estimating the uncertainty of the SLM and selectively invoking the LLM for uncertain results, AdaptiveLog effectively reduces the overall querying expense, optimizing performance without incurring significant LLM costs. Statistical analysis conducted on all six log analysis tasks under RQ1, as depicted in Fig. 13a, reveals that on average, only 27% of each dataset necessitates LLM querying. This approach results in a remarkable 73% cost saving compared to analyzing all samples with the LLM, leading to state-of-the-art results. Furthermore, an examination of the proportion of LLM queries in low-resource and transfer learning scenarios, illustrated in Fig. 13b, indicates an increased need for LLM interventions in these challenging settings compared to the standard experimental setup of RQ1. This trend suggests the LLM's superior advantages in compensating for SLM limitations, particularly in more challenging scenarios. Despite the rise in LLM query percentages, AdaptiveLog still manages to save 65% of the LLM overhead, showcasing its efficiency and adaptability across diverse operational contexts. In practical applications, the construction of an error-prone case database plays a crucial role in enhancing the LLM's reasoning capabilities. The analysis of case database construction costs, detailed in Table 9, demonstrates that the expense and time costs associated with this process are minimal. Leveraging a sample size of 100 error-prone cases, the effectiveness of ECR is validated. To streamline case construction efforts, we suggest manually labeling high-uncertainty samples of the SLM to create error-prone cases, leveraging the tendency for high-uncertainty samples to be inaccurately predicted, thus reducing labeling efforts. Additionally, considering that constructing error-prone cases also requires ground truth, to further alleviate the labeling cost, we explore the performance of different numbers of case database on the ECR strategy. Specifically, we employ the Determinantal"}, {"title": "5.2 Qualitative Analysis", "content": "In this section, we conduct a qualitative analysis on the Level Prediction task, presented in Fig. 16. The analysis focused on the behavior of collaboration between BERT and ChatGPT. Initially, BERT is employed to provide a prediction along with an associated probability of uncertainty. This uncertainty estimation determines the"}, {}]}