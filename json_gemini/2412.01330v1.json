{"title": "The \"LLM World of Words\" English free association norms generated by large language models", "authors": ["Katherine Abramski", "Riccardo Improta", "Giulio Rossetti", "Massimo Stella"], "abstract": "Free associations have been extensively used in cognitive psychology and linguistics for studying how conceptual knowledge is organized. Recently, the potential of applying a similar approach for investigating the knowledge encoded in LLMs has emerged, specifically as a method for investigating LLM biases. However, the absence of large-scale LLM-generated free association norms that are comparable with human-generated norms is an obstacle to this new research direction. To address this limitation, we create a new dataset of LLM-generated free association norms modeled after the \"Small World of Words\" (SWOW) human-generated norms consisting of approximately 12,000 cue words. We prompt three LLMs, namely Mistral, Llama3, and Haiku, with the same cues as those in the SWOW norms to generate three novel comparable datasets, the \"LLM World of Words\" (LWOW). Using both SWOW and LWOW norms, we construct cognitive network models of semantic memory that represent the conceptual knowledge possessed by humans and LLMs. We demonstrate how these datasets can be used for investigating implicit biases in humans and LLMs, such as the harmful gender stereotypes that are prevalent both in society and LLM outputs.", "sections": [{"title": "Background & Summary", "content": "How is conceptual knowledge organized in the mind? Such a question has long been the focus of linguists and cognitive psychologists who aim to better understand the human language capacity. Recently, this question has become increasingly relevant in the field of artificial intelligence, particularly regarding large language models (LLMs). Human semantic memory \u2013 the repository of conceptual knowledge that encompasses how words get their meaning \u2013 forms the foundation of human language and thought, and thus, its structure and properties influence how we reason, form beliefs and make decisions, ultimately shaping our social and political systems. Similarly, the semantic representations that comprise the knowledge encoded in LLMs are the underlying source behind the outputs they produce, and as LLMs become more integrated into our everyday lives, these outputs have an increasing impact on society. Thus, the study of the structure and properties of semantic memory is central to understanding not only our own thinking and reasoning, but also the \"thinking\" and \"reasoning\" of LLMs, which carries important societal implications.\nStudying semantic memory involves creating representations of word meanings (semantic representations), often in terms of how words relate to other words. In humans, one common way to do this is using free associations, which are usually accessed by prompting participants with a cue word and asking them to come up with (typically three) associated responses. Since the task is context neutral, responses represent the associative knowledge of words that we possess at an implicit level. Free associations have been extensively used in cognitive psychology and linguistics for studying lexical retrieval, semantic organization, and similarity judgments. They have also been used for studying differences in cognitive processing between concrete and abstract words, i.e. concreteness effects. Given that free associations have been shown to correlate with stable implicit attitudes, they have also been used for studying affective biases. Investigations of conceptual knowledge using free associations are often conducted within network models of semantic memory built from free associations by connecting cue words to their responses. This results in a complex network structure of human conceptual knowledge in which words get their meanings through relationships to other concepts. Such models enable the investigation of complex cognitive processes that take place within semantic memory. In fact, cognitive network models of semantic memory have been used to gain powerful insights about a variety of human cognitive phenomena such as language learning, creativity, personality traits like openness to experience, and autism spectrum disorder.\nWhile free associations have been widely used for studying semantic memory in humans, very different approaches have been applied for investigating conceptual knowledge in language models. Typically, semantic representations are directly"}, {"title": "Methods", "content": "Data generation\nSince the LWOW datasets are based off the Small World of Words (SWOW) human-generated norms, we first gathered the cue words from the original SWOW dataset. The SWOW dataset was downloaded from https://smallworldofwords.org/en/project/. We used the preprocessed data (SWOW-EN.R100.csv) with 12,282 cue words and 100 sets of responses per cue. We used a list of these cues as input to the LLMs along with a prompt that aimed to mimic the instructions provided to humans in the original SWOW free association task. The following prompt was given to the LLMs:\nTask:\nYou will be provided with an input word: write the first 3 words you associate to it separated by a comma\nNo additional output text is allowed\nConstraints:\nNo carriage return characters are allowed in the answers\nAnswers should be as short as possible\nExample:\nInput: sea\nOutput: water, beach, sun\nThis prompt was repeated 100 times for each cue word in order to generate a dataset with the same number of responses as the original (preprocessed) SWOW dataset. In what follows, we describe how the LLM-generated output as well as the original SWOW output were further processed.\nData processing\nThe original SWOW data were already preprocessed, but in order to facilitate analyses and data alignment, we applied additional data preprocessing to both the original SWOW data and the output produced by all three language models. From this point on, we refer to the SWOW dataset, including all subsequent modifications, as the Human dataset. The preprocessing steps applied to all four datasets, which were done in python, are as follows. First, all cues and responses were made lowercase. Then, the articles a, an, the, and the preposition to were removed from the beginning of responses unless they were among the original cues (e.g. a lot). Some responses included underscores, and these were replaced with spaces. Also, some responses incorrectly lacked spaces or hyphens (e.g. throwout, checkin). In order to ensure that these responses were not excluded in later analyses, we created a mapping dictionary using WordNet (implemented in the python library nltk) to resolve this issue. Specifically, we took all the words in WordNet that have either spaces or hyphens (e.g. throw out, check-in) and we removed them to create a one-to-one mapping to correct these errors in the responses. Spelling corrections were also applied to both cues and responses according to a dictionary that was used to process the original SWOW data. The spelling dictionary included the correction of commonly misspelled words (e.g. recieve to receive) but it also mapped British spelling to American spelling (e.g. colour to color). Next, cues and responses were lemmatized using WordNet's lemmatizer, changing plural nouns to singular nouns (e.g. men to man) but leaving tensed verbs unchanged (e.g. cooking, determined). Next, we added or removed data to ensure exactly 100 repetitions per cue. This step was needed for the Human data because the spelling corrections and the lemmatization of cues resulted in more than 100 repetitions for some cues, while some LLMs, namely Llama3, failed to generate 100 repetitions per cue. Thus, we ensured 100 repetitions per cue by adding blank responses when there were less than 100 repetitions for a cue, while sampling randomly when there were more than 100 repetitions per cue. Finally, responses that were identical to their corresponding cues were removed, and duplicate responses within the same set of three responses were"}, {"title": "Network construction", "content": "As discussed earlier, free associations are often used to build network models of semantic memory. In this way, semantic memory is considered a complex system, and it can be studied as such even in case of systems without an explicit semantic memory, like LLMs, that are nonetheless capable of processing language. The network structure provides a quantitative framework within which certain cognitive phenomena can be investigated using the tools of network science. From the preprocessed data, we built network models of semantic memory for humans and all three LLMs by connecting cue words to their responses. The weight of the edge reflects the frequency of the response, so if cat appears 20 times as the response to the cue dog, then a directed edge of weight 20 is created from dog to cat. The networks are naturally directed, but they are transformed into undirected networks to facilitate the analyses that we will discuss in the next section. In cases in which there is a bidirectional edge, the largest of the two edge weights is maintained. So if the edge from dog to cat has a weight of 20 and the edge from cat to dog has a weight of 25, the undirected edge between cat and dog has a weight of 25. The full undirected networks are then filtered to remove unwanted nodes and edges. This is done first by removing nodes that are not in WordNet, and then by removing idiosyncratic edges, i.e. edges with a weight = 1, and finally, taking the largest connected component.\nThis network filtering has a few advantages. First, the WordNet filter provides a standardized way to eliminate nonsensical and uncommon responses, since words in WordNet are at least English words, and the removal of idiosyncratic edges ensures that the association is something shared among two or more people (iterations in the case of the LLMs) and not just a fluke. A final advantage is that this filtering reduces the number of nodes and edges, making the networks more computationally manageable."}, {"title": "Technical Validation", "content": "To demonstrate the reliability of our data, we adopted a previously applied approach that simulates the cognitive mechanisms underlying semantic priming, a cognitive phenomenon that entails recognizing target words more quickly when they are preceded by related prime words. Studies of semantic priming effects have been critical to gaining a better understanding of the nature of semantic memory. Semantic priming is usually investigated using the lexical decision task (LDT), in which human participants are presented with a prime word followed by a target word, and participants must decide as quickly as possible whether the target word is a real English word or a non-word. It has been found that participants identify the target word more quickly (lower reaction time) when the prime is related to the target (e.g. doctor \u2013 nurse) compared to when the prime is unrelated to the target (e.g. doctrine \u2013 nurse). This pattern has been shown to be consistent across thousands of prime-target pairs.\nIn addition to the LDT, semantic priming can also be studied by implementing a spreading activation process within a network of semantic memory. Spreading activation is a method of search within a network that is based on supposed mechanisms of human memory. In spreading activation theory, exposure to a concept leads to the activation of its corresponding node in semantic memory. That activation then propagates through the semantic memory network along the connections of the activated node, decaying over time, leading to the activation of other concepts in the network. This theory can be used to study semantic priming by simulating a search process within a network in which a prime node is activated and at the end of the spreading activation process, the final activation level of the target node is observed. Following the semantic priming effect, the final activation level of the target node (e.g. nurse) should, in theory, be greater when the activated prime is a related word (e.g. doctor) rather than an unrelated word (e.g. doctrine). This process is represented in Figure 1.\nSpreading activation processes can be simulated within empirical networks using the R library spreadr. Given an empirical network, the spreadr algorithm works by specifying one or more nodes in the network to be activated, the initial activation level of the activated nodes, and the number of iterations or time steps. At each time step, an activated node may retain a certain percentage of its activation, while the remaining percentage is distributed among its neighboring nodes, proportional to the weight of the edges if the network is weighted. This process continues iteratively until the specified number of time steps is reached. At the end of the iterative process, the final activation level of each node in the network can be measured.\nThe developers of spreadr simulated spreading activation within a free association network of semantic memory to investigate the semantic priming effect using a series of prime-target pairs and their corresponding empirical reaction times from a lexical decision task experiment. As expected, they found that the final activation levels of the target nodes correlated with reaction times from the empirical data. That is, the final activation levels of the targets were greater when the activated primes were related rather than unrelated to the targets. These results show that the semantic priming effect observed in the psycholinguistic LDT experiment can also be observed within a network model of semantic memory, demonstrating the usefulness of semantic networks for modeling certain cognitive phenomena.\nIn order to validate our datasets, we repeated this spreading activation investigation of semantic priming implemented by the authors of spreadr, but we used the Human and LLM networks that we built. We used a subset of 50 prime-target pairs and their corresponding reaction times from the same LDT dataset used by the authors of spreadr, downloadable from https://www.montana.edu/attmemlab/spp.html. The 50 prime-target pairs and subsequent standardized reaction times (RTs)"}, {"title": "Usage Notes", "content": "In the previous Section, we demonstrated the validity of the datasets by showing how they can be used to investigate semantic priming effects by simulating spreading activation processes within the networks. In this Section, we show how this methodology can be applied for investigating implicit biases within the networks, specifically, gender stereotypes. Since the semantic priming effect emerges due to the relatedness of words, observing this effect can be an indication that two words are related. Thus, semantic priming can be used to assess the strength of association between two words, and so, it can be implemented as a method for evaluating implicit biases, as discussed earlier. In what follows, we show how we can apply this theory and methodology to measure levels of gender biases within the networks.\nWe start by choosing 5 female-related prime words (woman, female, mother, girl, feminine) and 5 corresponding male-related prime words (man, male, father, boy, masculine). We then choose 25 stereotypical female-related adjectives (e.g. gentle, emotional) and 25 stereotypical male-related adjectives (e.g. strong, dominant) as targets. These adjectives, which are listed in Table 8 in the Appendix, were taken from a study investigating gendered wording in job postings. Each prime is paired with a target, resulting in 500 prime-target pairs, half of which are stereotype consistent (e.g. woman emotional, man \u2013 dominant) while the other half are stereotype inconsistent (e.g. man emotional, woman \u2013 dominant). We then repeated the spreading activation methodology described in the previous section, activating all the primes in all four networks using spreadr with the same parameter specifications as described earlier. We obtained normalized final activation levels of all 50 targets for each of the 10 primes. These activation levels for Humans are displayed in the heatmap in Figure 4, with female-related targets on the left and male-related targets on the right. The same heatmaps for the LLMs (Mistral, Llama3, and Haiku) can be found in Figures 6-8 in the Appendix. These heatmaps provide a better understanding of the types of gender biases that are present in the semantic networks. For example, the heatmap for Humans on the right (Figure 4) appears to have a clear division down the center, with darker colors on the right. This indicates overall higher activation levels of male-related targets when male-related primes are activated compared to female-related primes. A similar pattern is somewhat observable in the LLM heatmaps as well. The heatmaps are also useful for uncovering gender biases related to specific terms. For example, in Figure 4, a large difference in activation level can be observed in the male-related target forceful when activated by the prime masculine compared to its"}, {"title": "Code availability", "content": "The python and R code and all related data used to produce the LWOW datasets and conduct the analyses are available at https://github.com/LLMWorldOfWords/LWOW."}, {"title": "Author contributions statement", "content": "K.A., G.R. and M.S. conceived the experiments, K.A., R.I. and G.R. conducted the experiments, K.A. analyzed the results and visualized the data. All authors reviewed the manuscript."}, {"title": "Competing interests", "content": "The authors declare no competing interests."}]}