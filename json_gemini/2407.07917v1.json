{"title": "Non-Cooperative Backdoor Attacks in Federated Learning: A New Threat Landscape", "authors": ["Tuan Nguyen", "Dung Thuy Nguyen", "Khoa D Doan", "Kok-Seng Wong"], "abstract": "Despite the promise of Federated Learning (FL) for privacy-preserving model training on distributed data, it remains susceptible to backdoor attacks. These attacks manipulate models by embedding triggers (specific input patterns) in the training data, forcing misclassification as predefined classes during deployment. Traditional single-trigger attacks and recent work on cooperative multiple-trigger attacks, where clients collaborate, highlight limitations in attack realism due to co-ordination requirements. We investigate a more alarming scenario: non-cooperative multiple-trigger attacks. Here, independent adversaries introduce distinct triggers targeting unique classes. These parallel attacks exploit FL's decentralized nature, making detection difficult. Our experiments demonstrate the alarming vulnerability of FL to such attacks, where individual backdoors can be successfully learned without impacting the main task. This research emphasizes the critical need for robust defenses against diverse backdoor attacks in the evolving FL landscape. While our focus is on empirical analysis, we believe it can guide backdoor research toward more realistic settings, highlighting the crucial role of FL in building robust defenses against diverse backdoor threats. The code is available at https://anonymous.4open.science/r/nba-980F/.", "sections": [{"title": "1 Introduction", "content": "Federated learning (FL) [16] is a distributed machine learning paradigm that enables multiple parties to train a shared model cooperatively without sharing their private data. In FL, each party trains a local model on its data and then shares the model parameters with a central server. The server aggregates the parameters from all parties and updates the global model. The updated global model is then sent back to each party for further training. This process is repeated until the global model converges. However, because the training data is distributed across multiple parties, FL is vulnerable to backdoor attacks [20], where the attacker poisons the model by injecting a backdoor trigger into the training data. When the model is deployed, a specific input pattern can activate the backdoor trigger to cause the model to output a specific target class.\nBackdoor attacks on FL have been recently studied in [1, 24, 28, 6, 30, 2, 18]. Existing research categorizes these attacks into two main types: fixed-trigger attacks and optimized-trigger attacks. Fixed-trigger attacks, as described in [1], involve pre-selecting a trigger without leveraging information from the FL training process. Conversely, optimized-trigger attacks refine the trigger specifically to enhance the attack's effectiveness by utilizing such information. Recent studies have explored various optimization techniques: maximizing the difference between clean and trigger-added sample representations (Fang et al., 2023) [4], jointly optimizing the trigger and local model with regularization to bypass defenses (Lyu et al., 2023) [15], and using autoencoders to generate the optimal trigger"}, {"title": "2 Background and related work", "content": "2.1 Federated learning\nIn FL, users with private data collaborate to train a global model (Gt). Each user iteratively updates a local model (Wt+1 ) based on their data (D\u2081) using Stochastic Gradient Descent:\n$W_{i}^{t+1}=G^{t}-l r \\cdot \\nabla L_{\\text {task }}\\left(G^{t}, D_{i}\\right)$\nwhere Ltask is the loss function, \u2207Ltask(Gt, Di) denotes the gradient, and lr is the local learning rate. User updates are then uploaded and aggregated by the central server using aggregation rules (e.g., FedAvg [16]) with a global learning rate (\u03b7) to create a new global model for the next round:\n$G^{t+1}=G^{t}+\\eta \\sum_{i=1}^{n} \\frac{1}{n}\\left(W_{i}^{t+1}-G^{t}\\right)$\n2.2 Backdoor attacks and defenses in FL\nBackdoor attack. This attack aims to make a model perform well on normal data (benign data) while also producing attacker-desired outputs for inputs with a hidden trigger (e.g., specific image pattern). Attackers participate in FL with backdoor data (Dbackdoor) to poison the model. Eq. 3"}, {"title": "3 Non-Cooperative Backdoor Attacks against federated learning", "content": "3.1 General framework\nOur proposed scenario, NBA, in FL, involves multiple clients acting independently, each with their own unique backdoor trigger and target class. As shown in Fig. 1, this attack differs from from existing cooperative backdoor attacks, where clients coordinate with decomposed triggers and target labels, ultimately compromising the global model. In NBA, each client acts independently, introducing its"}, {"title": "3.2 Factors in Non-Cooperative Backdoor Attacks", "content": "Trigger location and size. The trigger is located at the top left corner of the image, with a size of 24 pixels. We use eight fixed trigger patterns with the shapes 1 \u00d7 24, 2 \u00d7 12, 3 \u00d7 8, 4 \u00d7 6, 6 \u00d7 4, 8 \u00d7 3, 12 x 2, and 24 \u00d7 1 pixels, respectively, as shown in Fig. 2.\nScale \u03b3. The scaling parameter \u03b3 = 1 defined in Bagdasaryan et al. [1] is used by the attacker to scale up the malicious model weights. For instance, assume the ith malicious local model is X. The new local model Li,t+1 that will be submitted is calculated as Li,t+1 = \u03b3(X \u2013 Gt) + Gt.\nData distribution \u03b1. FL often presumes non-i.i.d. data distribution across parties. Here, we use a Dirichlet distribution [17] with hyperparameter \u03b1 = 0.5 to generate different data distributions."}, {"title": "4 Experiments", "content": "4.1 Datasets and experiment setup\nDatasets. NBA is evaluated on four classification datasets with non-i.i.d. data distributions: Fashion-MNIST [26], MNIST [12], CIFAR-10 [10], and Tiny-ImageNet [11]. The data description and parameter setups are summarized in Appx. A.\nFederated learning setup. Following the standard setup, we use FedAvg [16] as the global model optimization algorithm, and the global learning rate \u03b7 is set to 0.01. In each round, 10 of the 100 clients are selected for aggregation and each selected client trains for E local epochs with a local learning rate lr. Our experiments utilize 8 triggers with fixed sizes of 24 pixels, as shown in Fig. 2.\nAttack scenarios. We evaluate the performance of NBA in three distinct attack scenarios following the setup in [1, 28]:"}, {"title": "4.2 Backdoor attacks with one adversary", "content": "4.2.1 Single-shot attack\nWe begin our evaluation by analyzing the performance of an attack in a single-shot setting with one adversary. In this setting, the attacker participates in only one round of training and modifies the strength of the backdoor model with a scaling factor \u03b3 = 100."}, {"title": "4.2.2 Multiple-shot attack", "content": "Performance of backdoor task with different triggers. In the multiple-shot attack scenario with one adversary over 200 rounds, as shown in Fig. 4, the performance of backdoor attacks exhibits notable differences across datasets. For Fashion-MNIST, the backdoor accuracy remains consistently high, with an average backdoor accuracy (BAAvg) of 99.38%, indicating that the backdoor attack is highly effective. Similarly, MNIST shows a strong resilience to the backdoor attack, with an average backdoor accuracy of 97.18%, though slightly lower than Fashion-MNIST. In the case of CIFAR-10, there is a significant range in backdoor accuracy, with values spanning from 74.06% to 91.04%, and an average of 85.86%, suggesting variable success in the backdoor attack across different triggers. Tiny-ImageNet displays consistently high backdoor accuracy, with all values close to or exceeding 93.80%, culminating in an average of 94.29%, indicating effective backdoor insertion. These results demonstrate the varying effectiveness of backdoor attacks in a multiple-shot scenario, heavily influenced by the specific triggers used."}, {"title": "4.3 Non-Cooperative Backdoor Attacks", "content": "4.3.1 Single-shot attack\nFig. 5 illustrates the backdoor accuracy trends in a single-shot NBA setting with an attack gap of 10 rounds, meaning the first attacker injects the backdoor in the first round, the next in the 11th round, and so on. In this scenario, each attacker participates in only one round, scaling the client's model using the model replacement method with a scaling factor \u03b3 = 100. As shown in Fig. 5,"}, {"title": "4.3.2 Multiple-shot attack", "content": "Performance of backdoor task with different triggers. The effectiveness of backdoor attacks varies significantly with different triggers across the datasets. For Fashion-MNIST, triggers BA6"}, {"title": "4.3.3 Semi-multiple-shot attack", "content": "In multiple-shot attacks, adversaries continuously inject backdoor triggers, resulting in a gradual increase in backdoor accuracy. As illustrated in Fig. 6, attackers need to participate in at least 200 rounds to achieve high backdoor accuracy for most triggers across three datasets, with the exception of Tiny-ImageNet. In real-world scenarios, however, adversaries may not always participate in every round. To reduce the number of rounds needed for effective attacks, we propose a semi-multiple-shot attack where adversaries participate for 100 rounds and then stop injecting backdoor triggers. The key difference between multiple-shot and semi-multiple-shot attacks is the adjustment of the scaling factor \u03b3 to $\\frac{100}{\\# A t k}$ where #Atk is the number of attackers. Results presented in Tab. 4 and Fig. 7 for a scenario with eight adversaries show that, although the backdoor accuracy fluctuates, it can reach high values for most triggers in three datasets at specific rounds.\nThese findings highlight a double-edged sword for backdoor attackers using the semi-multiple-shot attack. While it can be more efficient than the multiple-shot attack in terms of reducing participation rounds (reducing the risk of detection), it may not guarantee consistently high backdoor accuracy and the main accuracy might drop significantly. This inconsistency is crucial because once the backdoor injection is stopped, the model's backdoor accuracy (BA) drops significantly. However, the BAs then gradually recover, suggesting that the triggers are not entirely \"forgotten\" by the model. This presents a challenge for central server detection, as any client can inject a backdoor trigger in any round and then stop participating. The lingering effect of the triggers, even after the attacker ceases participation, makes it difficult for the server to distinguish between a temporary fluctuation and a true backdoor attack."}, {"title": "4.4 The robustness in Non-Cooperative Backdoor Attacks", "content": "Norm Clipping [23]. Participant updates undergo a clipping process to limit the impact of model adjustments, which involves multiplying them by $m i n\\left(1, \\frac{S}{\\left|\\nabla L_{i}\\right|_{2}}\\right)$, where S represents the clipping threshold. Tab. 3 and Tab. 5 illustrate the performance of the NBA algorithm in a multiple-shot scenario both before and after implementing the norm clipping defense with S = 5. The results reveal that neither the primary accuracy nor the backdoor accuracy is significantly influenced by the norm clipping defense, suggesting that adversaries can still effectively introduce backdoor triggers into the global model.\nDifferential Privacy (DP) [1]. Gaussian noise N(0, \u03c3) is added to local updates to reduce the influence of backdoor attacks. As shown in Tab. 6, although the backdoor accuracy decreases, the DP defense significantly impacts the main accuracy, leading to a notable drop in performance across all datasets. This indicates a trade-off between maintaining privacy and preserving model efficacy.\nEffectiveness of defense mechanisms. Mainstream defenses, such as Norm Clipping and Differential Privacy, were not designed to address scenarios where multiple independent attackers inject unique triggers and target classes. Additionally, existing research hasn't thoroughly investigated the effectiveness of these defenses in the presence of multiple backdoor attackers (NBA). This highlights a critical gap in the current understanding of defense mechanisms for FL security."}, {"title": "4.5 Limitations", "content": "Our work explores various facets of the NBA scenario in FL systems, focusing on trigger design, dataset characteristics, and model updates. The effectiveness of backdoor triggers varies with dataset characteristics, and the scaling factor \u03b3 significantly impacts backdoor accuracy and main task performance. Multiple-shot attacks require numerous rounds to achieve high backdoor accuracy, which is impractical in real-world scenarios, while semi-multiple-shot attacks can reduce the number of rounds but need precise tuning of \u03b3. Our controlled evaluation might not fully capture real-world FL complexities, and our study focuses on limited trigger designs and defense mechanisms like norm clipping and differential privacy. Future research should explore more sophisticated attack strategies and novel defenses tailored to the non-i.i.d. nature of FL systems."}, {"title": "5 Conclusion", "content": "This paper explores the emerging threat of the NBA scenario in FL systems, where multiple independent clients can compromise the global model by injecting unique backdoor triggers and target classes. Our findings highlight the potential of watermarking-based backdoor triggers, which can be useful in cross-silo FL scenarios to protect the copyright of participants. This study lays the groundwork for future research focused on developing robust strategies to counteract backdoor attacks. Furthermore,"}, {"title": "E Discussion", "content": "Our work delves into multiple aspects of the NBA scenario within FL systems, particularly emphasizing trigger design, dataset characteristics, and model updates. The effectiveness of backdoor triggers can vary widely depending on the dataset; some datasets may render the trigger ineffective, while others may facilitate highly effective backdoor attacks. Additionally, the scaling factor \u03b3 plays a critical role in the success rate of these attacks. Increasing the scale of model updates can significantly enhance backdoor accuracy, yet this comes at the cost of potentially degrading the main task accuracy, leading to suboptimal overall performance.\nIn the context of multiple-shot attacks, adversaries must engage in a substantial number of rounds to achieve high backdoor accuracy, which may be impractical in real-world scenarios. However, in cross-silo FL scenarios, each participant can introduce unique backdoor triggers as a form of watermarking to protect their intellectual property. Semi-multiple-shot attacks offer a more efficient alternative by reducing the number of rounds required to attain high backdoor accuracy, though they require careful selection of the scaling factor \u03b3 to ensure effectiveness and may not consistently maintain high accuracy throughout the attack."}, {"title": "F Societal impacts", "content": "Our research highlights the potential of NBA to compromise the integrity of FL systems. We believe this work serves as a crucial stepping stone towards a more secure future for FL. Through our work, we highlight the efficacy of watermarking-based triggers, presenting avenues for enhancing secure communication and detecting tampering within FL frameworks. Additionally, we explore the design of incentive structures, aiming to cultivate cooperative engagement while mitigating malicious activities. These endeavors are pivotal in establishing trust and fostering a secure ecosystem within FL platforms. By advancing research in these areas, we contribute substantially to the establishment of robust and trustworthy FL systems. Such efforts are crucial for safeguarding data privacy and integrity, thereby unleashing the full potential of FL to benefit society at large."}]}