{"title": "From FDG to PSMA: A Hitchhiker's Guide\nto Multitracer, Multicenter Lesion Segmentation\nin PET/CT Imaging", "authors": ["Maximilian Rokuss", "Balint Kovacs", "Yannick Kirchhoff", "Shuhan Xiao", "Constantin Ulrich", "Klaus H. Maier-Hein", "Fabian Isensee"], "abstract": "Automated lesion segmentation in PET/CT scans is crucial for improving clinical workflows and advancing cancer diagnostics.\nHowever, the task is challenging due to physiological variability, different tracers used in PET imaging, and diverse imaging protocols across medical centers. To address this, the autoPET series was created to challenge researchers to develop algorithms that generalize across diverse PET/CT environments. This paper presents our solution for the autoPET III challenge, targeting multitracer, multicenter generalization using the nnU-Net framework with the ResEncL architecture. Key techniques include misalignment data augmentation and multi-modal pre-training across CT, MR, and PET datasets to provide an initial anatomical understanding. We incorporate organ supervision as a multitask approach, enabling the model to distinguish between physiological uptake and tracer-specific patterns, which is particularly beneficial in cases where no lesions are present. Compared to the default nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL (65.31) our model significantly improved performance with a Dice score of 68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative (FNvol: 10.35) volumes. These results underscore the effectiveness of combining advanced network design, augmentation, pretraining, and multitask learning for PET/CT lesion segmentation. Code is publicly available at https://github.com/MIC-DKFZ/autopet-3-submission.", "sections": [{"title": "1 Introduction", "content": "Positron Emission Tomography (PET) combined with Computed Tomography\n(CT) is a powerful tool in modern medical diagnostics, particularly for detecting\nand monitoring cancer. PET/CT scans provide both metabolic and anatomi-\ncal information, allowing clinicians to identify tumor lesions with high precision.\nHowever, manual segmentation of lesions in PET/CT images is time-consuming,\nand labor-intensive rendering it infeasible for patients with a multitude of lesions.\nAutomated lesion segmentation offers a promising solution, enabling faster, more\nconsistent analysis, which is crucial for clinical workflows and research.\nHowever, despite its potential, automated segmentation faces significant chal-\nlenges. The complexity arises from the physiological variability between patients,\nthe different tracers used in PET imaging (such as FDG and PSMA), and the\nvariations in imaging protocols across different medical centers. Each tracer can\nhighlight different metabolic activities, which leads to distinct patterns of uptake\nin non-tumor structures, complicating the task of distinguishing between normal\nphysiological uptake and actual lesions. To accurately assess PET/CT images,\na model must learn to interpret varying uptake patterns without explicit infor-\nmation about the specific tracer used. Instead, it must rely on the surrounding\nanatomical context to differentiate between physiological and cancerous uptake.\nThese complexities, particularly the variability in tracer behavior, have made\nautomated lesion segmentation in PET/CT imaging a highly challenging task\nfor models to perform effectively.\nTo address this, the autoPET challenge series was created, offering a platform for\nresearchers to directly tackle these issues. Building on the insights from previous\niterations, the autoPET III challenge broadens its scope to focus on multitracer,\nmulticenter generalization. The publicly available dataset of 1014 FDG PET/CT\nstudies [3] has been extended by 597 exams with a new PSMA tracer [7]. By\nproviding access to large, annotated datasets from different hospitals, partic-\nipants are tasked with developing algorithms capable of accurate and robust\nsegmentation across diverse PET/CT environments. This challenge represents a\ncrucial step toward enhancing automated medical imaging for real-world clinical\napplications.\nThis manuscript describes our participation in the autoPET III challenge. We\nbase our solution on a strong foundation offered by nnU-Net[5] and address the\naforementioned challenges of automated PET/CT lesion segmentation through\ndata augmentation, pretraining, model design, and postprocessing techniques.\nOur approach aims to improve generalization across tracers and centers, tackling\nthe complexities of physiological and cancerous uptake."}, {"title": "2 Methods", "content": "Our method builds on the well-established nnU-Net framework[5], specifically,\nwe opt for a larger and more powerful network given by the recently introduced\nResEncL architecture preset [6]."}, {"title": "2.1 nnUNet Configuration", "content": "We use the 3d_fullres configuration, resample all images to a common spacing\nof [3, 2.04, 2.04] and normalize both modalities with the default CT normal-\nization scheme. We train with a batch size of 2 for 1000 epochs and a uniform\npatch size of 192x192x192 for all trainings during method development. This\nlarge patch size has the advantage of providing the network with more context\nwhich is important, especially for the task at hand where the network needs\nto infer the tracer type and uptake rate from the surrounding organs. We also\nnoticed that training without a smoothing term in the dice loss calculation the\ntraining becomes more stable, hence we omit it. The best model we use for the\nsubmission is retrained with a batch size of 3 for 1500 epochs."}, {"title": "2.2 Data augmentation", "content": "To account for potential misalignments [1,4,8,10] between the CT and PET\nimages, we extended the data augmentation (DA) scheme of nnU-Net with mis-\nalignment DA [9]. Essentially, this augmentation shifts the PET and CT images\nrelative to each other to make the network more robust to incorrect spatial\nalignment. This approach has the potential advantage of improving sensitivity\nfor punctate lesions with small voxel segmentation volumes, which was indicated\nin the original study but remained unproven. The amplitude of the transforma-\ntions used to generate misalignments was sampled randomly from a uniform\ndistribution, constrained by a maximum amplitude in both positive and nega-\ntive directions. The transformation included an initial rotation with a maximum\nangle of 5\u00b0, followed by translations with maximum voxel shifts of [2, 2, 0] in\nthe x, y, and z directions, respectively."}, {"title": "2.3 Pretaining and Finetuning", "content": "To steer the model towards an anatomically relevant loss minimum, we first pre-\ntrain it on a large and diverse dataset of 3D medical images, combining a variety\nof public datasets in a MultiTalent-inspired fashion [11]. Initially restricted to\nCT datasets, we later expanded this pretraining to include datasets from PET\nand Magnetic Resonance Imaging (MRI) modalities, allowing the model to learn\ngeneral features and develop a universal understanding of anatomy and medical\nimages. Employing separate segmentation heads for each dataset, the model was\ntrained for 4000 epochs with a patch size of [192,192,192] and a batch size\nof 24, resampling all images to a cubic 1mm resolution and Z-score normaliza-\ntion. Dataset sampling was performed inversely proportional to the square root"}, {"title": "2.4 Organ Supervision", "content": "To improve further anatomical understanding and enhance segmentation perfor-\nmance, we introduce an additional prediction head focused on segmenting key\norgans. These organs often exhibit higher tracer uptake, which may not be the\nprimary target but could complicate diagnosis. By addressing these areas, the\nnetwork aims to reduce false positive volume (FPvol) and improve overall ac-\ncuracy. We use TotalSegmentator [13,12] to predict the spleen, kidneys, liver,\nurinary bladder, lung, brain, heart, stomach, prostate, and glands in the head\nregion (parotid glands, submandibular glands) for all images as shown in Fig.\n1. The selection of these structures was influenced by the uptake patterns of\nvarious tracers, reflecting the differing behaviors observed between them. This\naddition complements the lesion segmentation head, resulting in a dual-headed\narchitecture where one head focuses on lesions and the other on organ struc-\ntures. Each prediction head is then trained using a softmax activation function\nand with equal loss weighting during optimization."}, {"title": "2.5 Approaches we tried, that didn't work", "content": "We also explored several ablation strategies that did not yield significant im-\nprovements during development. Resampling the images to an isotropic spacing\nof [1, 1, 1] resulted in a substantial performance decline, likely due to the\nreduced contextual information available within each patch. Additionally, pre-\ntraining on the TotalSegmentator dataset [13] provided no notable advantage\nover training the model from scratch. Similarly, incorporating additional anno-\ntated FDG images from the HECKTOR challenge [2] into the training set did\nnot enhance the performance metrics on the autoPET dataset."}, {"title": "3 Results", "content": "Model development and evaluation were carried out using a five-fold cross-\nvalidation on the autoPET III training split. As shown in Table 1, the nnU-Net\nResEnc L architecture outperformed the baseline nnU-Net across all metrics,\nserving as the backbone for further experimentation.\nInitial improvements focused on data augmentation and pretraining strategies.\nIncorporating the misalignment data augmentation (misalDA) resulted in a\nmarginal improvement in Dice score (from 65.31 to 65.76), particularly bene-\nfiting the PSMA tracer and achieving a low FNvol. Supervised pretraining the\nmodel on CT datasets boosted performance further, especially for PSMA, in-\ncreasing the overall Dice to 66.08."}, {"title": "3.1 Test Set Submission", "content": "For the final submission, we ensembled all 5 folds. To meet the 5-minute time\nconstraint per case, we set a tile step size of 0.6, which controls the sliding window\nshift relative to the patch size. The first fold is processed without mirroring,\nand based on the time taken, one or two mirroring axes are added as test-time\naugmentation for the remaining folds."}, {"title": "4 Conclusion", "content": "In this paper, we addressed the challenges of automated lesion segmentation\nin PET/CT imaging through a combination of data augmentation, pretraining,\nand multitask learning as well as a careful choice of the underlying network\ndesign and training. Building on the nnU-Net framework, we use misalignment\ndata augmentation and multimodal pretraining, which improved generalization\nacross different tracers and centers. Incorporating organ supervision as a sec-\nondary task further boosted performance by guiding the model with anatomical"}]}