{"title": "Entropy-Lens: The Information Signature of\nTransformer Computations", "authors": ["Riccardo Ali", "Francesco Caso", "Christopher Irwin", "Pietro Li\u00f2"], "abstract": "Transformer models have revolutionized fields from natural language processing\nto computer vision, yet their internal computational dynamics remain poorly un-\nderstood-raising concerns about predictability and robustness. In this work, we\nintroduce Entropy-Lens, a scalable, model-agnostic framework that leverages\ninformation theory to interpret frozen, off-the-shelf large-scale transformers. By\nquantifying the evolution of Shannon entropy within intermediate residual streams,\nour approach extracts computational signatures that distinguish model families,\ncategorize task-specific prompts, and correlate with output accuracy. We further\ndemonstrate the generality of our method by extending the analysis to vision trans-\nformers. Our results suggest that entropy-based metrics can serve as a principled\ntool for unveiling the inner workings of modern transformer architectures.", "sections": [{"title": "1 Introduction", "content": "Transformer-based architectures [Vaswani et al., 2023] are widely employed as state of the art models\nin several fields, from machine translation and search engines to DNA analysis and protein research\n[Devlin, 2018, Khattab and Zaharia, 2020, Ji et al., 2021, Chandra et al., 2023]. Their declination in\nlanguage modeling of large corpora is referred to as large language models (LLMs), and in computer\nvision as vision transformers (ViTs). Despite their success and ubiquity, transformers' inner workings\nremain largely unknown, resulting in unpredictable behaviour [Wei et al., 2022] and reliability\nconcerns [Schroeder and Wood-Doughty, 2025, Huang et al., 2025].\nTherefore, considerate research efforts are devoted to transformer-based architectures interpretability,\nmostly focusing on LLMs [Nanda and Bloom, 2022, Bereska and Gavves, 2024, Elhage et al.,\n2021, 2022] and ViTs [Chefer et al., 2021]. While exciting results have been achieved in this area,\nthey remain limited to toy models and simplified setups, both very different from real use-case\nconditions. Moreover, these methods often require training a set of probes [nostalgebraist, 2020,\nBelrose et al., 2023] or full models on ad-hoc tasks [Nanda et al., 2023], making them architecture or\neven model specific and computationally expensive. These limitations restrict the scope and usability\nof current methodologies, rendering them unsuitable for off-the-shelf or large-scale transformer-based\narchitectures."}, {"title": "2 Related Work", "content": "Lenses in LLMs Mechanistic interpretability [Bereska and Gavves, 2024] aims to provide a\nprecise description and prediction of transformer-based computations. Common tools in the field\nare lenses, which are a broad class of probes deployed in intermediate steps of the residual stream.\nFor example, logit-lens [nostalgebraist, 2020] uses the model's decoder function to decode the\nintermediate activations in the vocabulary space. tuned-lens [Belrose et al., 2023] refines this\ntechnique by training a different affine probe at each layer, instead of only using the pretrained\nmodel's decoder function. Building on the Transformer-Lens library [Nanda and Bloom, 2022], we\npropose Entropy-Lens, which employ logit-lens to study and characterize LLMs' computations\nvia their decoded version with information theory.\nTransformers' Circuits Another approach to mechanistic interpretability aims to identify and\nunderstand the specific sub-computations, or circuits, in a neural network [Saphra and Wiegr-\neffe, 2024]. Olah et al. [2020] pioneered this approach, introducing the concept of circuits and\ndemonstrating their existence in small models through manual analysis. In transformers, circuits are\nhypothesized to act as agents that read from and write to the residual stream, which acts as a form of\nmemory. This has been demonstrated in a simplified transformer model composed only of attention\nblocks (without MLPs) [Elhage et al., 2021]. However, there is evidence that full transformers\nexhibit similar behavior. This evidence-linked to the concept of superposition, the idea that\nmodels can represent more features than the available dimensions by compressing multiple features\ninto one [Elhage et al., 2022]-is supported by studies on sparse autoencoders, which demonstrate\nthe ability to decompose representations into simpler components [Bricken et al., 2023]. Conmy\net al. [2023a] developed a toolkit to facilitate mechanistic interpretability, offering techniques like\nactivation patching and weight factorization. Building upon this, Conmy et al. [2023b] explored\nautomated circuit discovery methods, addressing the challenge of scaling analysis to larger models.\nThese works collectively emphasize the importance of understanding the concrete computational\nsteps within LLMs, moving beyond superficial observations to reveal the underlying mechanisms.\nInformation Theory in Transformers Information theory has been studied both in connection to\nthe training phase of LLMs and their interpretability. For example, a collapse in attention entropy has\nbeen linked to training instabilities [Zhai et al., 2023] and matrix entropy was employed to evaluate\n\"compression\" in LLMs [Wei et al., 2024]. Additionally, mutual information was used to study the\neffectiveness of the chain-of-thought mechanism [Ton et al., 2024]. Our work, instead, shifts the"}, {"title": "3 Background", "content": "The main information-theoretic quantity used in our study is entropy. Given a discrete\u00b9 random\nvariable X with outcomes xi and probability mass function p, the Shannon entropy H of X is defined\nas\n\\(H(X) = -\\sum_i p(x_i) \\log p(x_i) = E[-\\log p(X)]\\)\nShannon proved that this function is the only one-up to a scalar multiplication\u2014that satisfies\nintuitive properties for measuring 'disorder'. These include being maximal for a uniform distribution,\nminimal for the limit of a Kronecker delta function, and ensuring that \\(H(A, B) \\leq H(A) + H(B)\\)\nfor every possible event A and B. The same function already existed in continuous form in physics,\nwhere it linked the probabilistic formalism of statistical mechanics with the more phenomenological\nframework of thermodynamics, where the term 'entropy' was originally coined.\nNext, we study the entropy of vocabulary predictions\u2014a quantity that is maximal when the prediction\nassigns equal probability to all tokens, minimal when it assigns zero probability to all but one token,\nand takes intermediate values when probability is distributed across multiple tokens, consistent with\nthe previously mentioned properties."}, {"title": "3.2 The Transformer", "content": "Architecture The transformer [Vaswani et al., 2023] is a deep\nlearning architecture widely applied in language modelling with\nLLMs [Brown et al., 2020] and computer vision [Dosovitskiy et al.,\n2021]. Transformer computations happen through transformer\nblocks and residual connections, as exemplified in Figure 2. While\nvarious design choices are possible, blocks are usually a composi-\ntion of layer normalization [Zhang and Sennrich, 2019], attention,\nand multi layer perceptrons (MLPs), as shown in Fig. 1. Residual\nconnections, instead, sum the output of the layer i - 1 to the output\nof the layer i.\nInside a single transformer block, the information flows both horizon-\ntally and vertically. The former, enabled by the attention mechanism,\nallows the token representations to interact with each other. In a\nlanguage modelling task, for example, this is useful to identify which\nparts of the input sequence-the sentence prompt\u2014should influence\nthe next token prediction and quantify by how much. The latter\nvertical information flow allows the representation to evolve and\nencode different meanings or concepts. Usually, the dimension of\nthe latent space is the same for each block in the transformer. The\nembedding spaces where these computations take place are generally\ncalled the residual stream.\nComputation schema LLMs are trained to simply predict the next token in a sentence. That is,\ngiven a sentence prompt S with tokens t\u2081, . . ., ty, the transformer encodes each token with a linear\nencoder E. Throughout the residual stream, the representation N of the token ty evolves into the\nrepresentation of the token tN+1, which is then decoded back into token space via a linear decoder D,\nsometimes set to ET, tying the embedding weights and the decoder. Finally, the logits-the output\nof D-are normalized with softmax to represent a probability distribution over the vocabulary. We\nsummarize this operation with the function W := softmax o D."}, {"title": "3.2.1 Instruct Models", "content": "Training a Large Language Model (LLM) requires vast amounts of data and is generally divided into\nmultiple phases.\nPretraining: The model is exposed to large datasets through self-supervised tasks, such as next-token\nprediction or similar variants. This phase helps the model learn a broad range of general knowledge.\nFine-tuning: This phase teaches the model to generate more useful and coherent responses. Two\nmain strategies are used: Chat: The model is trained on structured conversations between a user and\nthe model, with clearly defined roles. Instruct: The model learns from simple commands, without a\npredefined dialogue structure.\nRLHF (optional): Some models undergo Reinforcement Learning from Human Feedback (RLHF) to\nfurther refine their responses based on human preferences.\nFor our experiments, we used off-the-shelf models without RLHF to analyze information processing\nin a less biased transformer version. We also focused on Instruct models instead of Chat models for\ntwo reasons: 1. the Instruct strategy aligns better with our experimental setup 2. Instruct models are\nmore flexible and often preferred for practical applications."}, {"title": "4 Method", "content": "The aim of our framework is to find and characterize the information-theoretic signature of transformer\ncomputations. Entropy-Lens's pipeline comprises three steps and is described in Figure 2.\nNotation We denote the input sentence comprising tokens t\u2081,...,tv by S = (ti) 1. Then, x\ndenotes the activations of the token tj after block i for j \u2208 {1, . . ., N } and i \u2208 {1, ..., L}. Since\nour analysis focuses on the logits extracted from the intermediate layers of the transformer, it will be\nuseful to distinguish between normalized and unnormalized logits. We define W := softmax \u2022 D\nand \\(y:= W(x)\\) the normalized logits of the token tj's activations after layer i.\nThe core of our methodology is to analyze the entropy of the generated tokens' intermediate represen-\ntations y. These vectors are probability distributions, as they are the output of a softmax. To obtain\na single quantity that summarizes the information they contain, we compute their entropy H(y). For\none generated token, we can consider the entropy of all of its intermediate predictions H(y) for\ni \u2208 {1, ..., L}. This leads us to the definition of entropy profile:\nDefintion 1 (Entropy profile) Let h\u00b3 = H(y) be the entropy of the intermediate representation\nof token tj after block i and residual connection. The entropy profile of the next generated token is\ndefined as\n\\(h_N = \\bigoplus_i h^N_i\\)\nwhere \u2295 denotes any aggregation function.\nIn our experiments, we set \u2295 to be concatenation, so that \\(h_N = (h_1^N,...,h_L^N)\\), but other choices\nare possible. The extraction of entropy profiles is the step 1 of our pipeline.\nThen, we fix the number of tokens that the LLM is required to generate, T and repeat the same\nprocedure for each of them, leading us to the next definition:"}, {"title": "5 Experiments", "content": "The goal of our experiments is to demonstrate the effectiveness of entropy profiles in distinguishing\ncomputational signatures across various models and scenarios. In the first part, we explore how\nentropy profiles can differentiate between LLM families and characterize distinct tasks. In the second\npart, we apply our framework to the Vision Transformer architecture to assess whether insights from\nthe LLM experiments can be similarly recovered."}, {"title": "5.1 Entropy-Lens for Large Language Models", "content": "The experiments on LLMs focus on three key aspects. First, we demonstrate that entropy profiles are\nindicative of model families, with distinctions becoming more pronounced as model size increases."}, {"title": "5.1.1 Entropy profiles identify model families", "content": "We assess whether aggregated entropy profiles can distinguish different model families by visualizing\nand analyzing those of 9 models from 3 different families (GPT, Gemma and LLama) with parameter\ncounts ranging from 100M to 9B.\nWe compute the mean of the entropy profiles obtained from 64 generated tokens, obtained with the\nprompt 'The concept of entropy, a brief essay:', as shown in Figure 3. We observe that the profiles\nrelate uniquely to the model family, rather than a particular model, independently of its size.\nThe GPT model class starts with high vocabulary entropy in the early layers, indicating a wide\nrange of possible response tokens. Then, entropy gradually decreases\u2014more smoothly than in other\nclasses-leading to a low-entropy state, where the model narrows down to a small set of possible\nresponse tokens.\nThe Gemma model class, on the other hand, starts with low entropy in the very first layers, then rises\nto higher entropy in the intermediate layers, and finally decreases to low entropy again just before the\nlast layers, where the model is required to produce an output token.\nThe Llama model class follows a similar pattern, but with a steeper rise, resulting in a higher entropy\nvalue maintained over a larger range of intermediate layers.\nWe observe that the equivalence between models of the same family but different sizes holds when\nlooking at the entropy trend not as a function of the absolute layer index, but rather as the relative\nlayer position within the model.\nWe conjecture that high entropy phases, whether in the early or intermediate layers, allow the model\nto explore more possibilities in its response, similarly to how temperature helps avoid getting stuck in\nlocal minima in optimization. Then, at the moment of selection, the distribution is 'cooled down',\nforcing the output to be limited to a few possible tokens."}, {"title": "5.1.2 Entropy profiles identify task types", "content": "We verify whether the entropy profiles can identify task types examining generative (continue a text),\nsyntactic (count the number of words in a text), and semantic (extract the subject or moral of a text)\ntasks.\nWe do this with the TinyStories dataset [Eldan and Li, 2023]. For evaluation robustness, we construct\nfor each task type three prompt templates using a combination of task-specific task prompts,\nreported in Appendix A Table 3, and a story from TinyStories. These templates are:\n\u2022 Base, of the form task prompt + story\n\u2022 Reversed, of the form story + task prompt\n\u2022 Scrambled, of the form task prompt + scrambled story or scrambled story + task\nprompt, at random. A scrambled story is a 'story' obtained by randomly shuffling the\nwords in a given story from TinyStories.\nNote that, for a robust evaluation, we also use 2 possible task prompt variations, as per Table 3.\nWe generate 800 prompts per task type, 1/3 of them with the base template, 1/3 with the reversed\ntemplate, and 1/3 with the scrambled templates, for a total of 2400 prompts. We then apply our\npipeline from Section 4 to classify the aggregated entropy profiles of these prompts against their\ntask type using a k-NN classifier. The model was evaluated in a 10-fold cross-validation using the\nROC-AUC score (one-vs-rest). Table 1 shows the results obtained for 6 models with parameter counts\nranging from 1B to 9B. Figure 4 shows the average entropy profiles per task type.\nWe observe that all k-NN classifiers (i.e. one for each LLM) achieve high accuracy in distinguishing\nentropy profiles, with a trend toward improved performance for larger models."}, {"title": "5.1.3 Entropy profiles identify correct task execution", "content": "We test whether entropy profiles can identify correct and wrong answers generated by LLMs using\nthe Massive Multitask Language Understanding (MMLU) dataset Hendrycks et al. [2021].\nMMLU consists of multiple-choice questions across 57 subjects, ranging from history and physics to\nlaw, mathematics, and medicine. The difficulty levels span from elementary to professional, making\nit a benchmark for evaluating language models on specialized domains. Each dataset entry contains:\na question string, four answer choices and a label indicating the correct answer.\nWe evaluated two models, a Llama-3.2 with 3B parameters Instruct and a Gemma-2 with 2B\nparameters, by presenting the multiple-choice questions in three different formats (as per Table 4 in\nAppendix A):\n\u2022 Base: A minimal version containing the topic, the question, and multiple-choice answers.\n\u2022 Instruct: A version with a brief explanation that it's a multiple-choice test where only one\noption should be selected.\n\u2022 Humble: A version that also instructs the model to pick a completely random option if it\ndoesn't know the answer.\nThen, we applied our pipeline to extract and aggregate and the responses' entropy profiles and classify\nthem against the correctness of the corresponding LLM-generated answer. We train a k-NN classifier\nfor each LLM and validate it using 10-fold cross-validation. We also conducted a t-test to compare\nour classifier to a dummy model. This dummy model generates predictions randomly, sampled from\na distribution that reflects the proportion of correct and incorrect answers produced by the LLM,\nensuring robustness against class imbalance. The results reject the null hypothesis with the k-NN\nachieving an AUC-ROC between 67.23 and 73.61, depending on prompt type and model (Table 2).\nWe observe that the instruct and humble prompts improve Llama's average accuracy, while for\nGemma, this is only true for the instruct prompt. Additionally, in Llama, the model's higher accuracy\nseems to be partially linked to greater difficulty in distinguishing correct from incorrect entropy\nprofiles, though more rigorous analysis is needed to confirm this. In Gemma, however, this claim is\nharder to support."}, {"title": "5.2 Entropy-Lens for Vision Transformers", "content": "Finally, to demonstrate the versatility and robustness of our approach beyond language modeling, we\nanalyze the entropy profiles of ViTs and DeiTs.\nUsing 20 classes from ImageNet-1K [Russakovsky et al., 2015], with 20 images per class, and without\nany modifications to our framework, we generate the entropy profiles shown in Figure 5. We observe\nthat all profiles start with high entropy values, which then decrease, mostly in the final layers. This\nbehavior is qualitatively similar to that of GPTs or larger LLaMa models (Section 5.1.1), suggesting\na universal pattern across domains as different as image processing and natural language processing.\nFocusing on computer vision models, we note that while ViT and DeiT families exhibit qualitatively\nsimilar trends, they differ quantitatively\u2014ViTs start with higher entropy values, making them easily\ndistinguishable from DeiTs.\nNotably, the only profile that stands out is that of ViT Large (with ~ 300M parameters), compared to\nthe other models analyzed in this section, which have \u2264 86M parameters.\nFor ViT Large, entropy decreases more smoothly, appearing not only as a better approximation of\nthe sharp drop seen in smaller models but possibly following a different behavior entirely, with the\nentropy decline starting earlier."}, {"title": "6 Conclusions", "content": "In this work, we prototyped a novel model-agnostic interpretability framework for large-scale\ntransformer-based architectures grounded in information-theory. In Section 5.1.1, we showed that\nthe entropy profiles of LLM intermediate predictions identify the LLM's model family. In Section\n5.2 we conduct similar experiments on vision transformers, demonstrating the wide applicability\nof our framework. Additionally, in Section 5.1.2 we showed that the same entropy profiles can be\nused to identify the 'task type' in LLMs, and in Section 5.1.3 we used them to distinguish between\ncorrect and wrong LLM generated answers. Importantly, all our experiments were conducted on\nfrozen off-the-shelf large-scale transformers."}]}