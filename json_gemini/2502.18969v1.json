{"title": "(MIS)FITTING: A SURVEY OF SCALING LAWS", "authors": ["Margaret Li", "Sneha Kudugunta", "Luke Zettlemoyer"], "abstract": "Modern foundation models rely heavily on using scaling laws to guide crucial\ntraining decisions. Researchers often extrapolate the optimal architecture and\nhyper parameters settings from smaller training runs by describing the relationship\nbetween, loss, or task performance, and scale. All components of this process vary,\nfrom the specific equation being fit, to the training setup, to the optimization method.\nEach of these factors may affect the fitted law, and therefore, the conclusions of\na given study. We discuss discrepancies in the conclusions that several prior\nworks reach, on questions such as the optimal token to parameter ratio. We\naugment this discussion with our own analysis of the critical impact that changes in\nspecific details may effect in a scaling study, and the resulting altered conclusions.\nAdditionally, we survey over 50 papers that study scaling trends: while 45 of these\npapers quantify these trends using a power law, most under-report crucial details\nneeded to reproduce their findings. To mitigate this, we we propose a checklist for\nauthors to consider while contributing to scaling law research.", "sections": [{"title": "1 INTRODUCTION", "content": "Training at the scale seen in recent large foundation models (Dubey et al., 2024; OpenAI, 2023;\nReid et al., 2024) is an expensive and uncertain process. Given the infeasibility of hyperparameter\ntuning multi-billion parameter models, researchers extrapolate the optimal training setup from\nsmaller training runs. More precisely, scaling laws (Kaplan et al., 2020) are used to study many\ndifferent aspects of model scaling. Scaling laws can guide targets for increasing dataset size and\nmodel size in pursuit of desired accuracy and latency for a specific deployment scenario, study\narchitectural improvements, determine optimal hyperparameters and assist in model debugging.\nScaling laws are often characterized as power\nlaws between the loss and size of the model and\ndataset, and are seen in several variations (Sec-\ntion 2). These laws are found empirically by\ntraining models across a few orders of magni-\ntude in model size and dataset size, and fitting\nthe loss of these models to a proposed scaling\nlaw. Each component of this process varies in\nthe reported literature, from the specific equa-\ntion being fit, to the training setup, and the op-\ntimization method, as well as specific details for\nselecting checkpoints, counting parameters and\nthe objective loss optimized during fitting.\nChanges to this setup can lead to significant\nchanges to the results, and therefore completely\ndifferent conclusion to the study. For example, Kaplan et al. (2020) studied the optimal allocation\nof compute budget, and found that dataset size should be scaled more slowly than model size\n(D \u221d N^{0.74}, D is dataset size, N is model size). Later, Hoffmann et al. (2022) contradicted this\nfinding, showing that model size and dataset size should be scaled roughly equally for optimal scaling.\nThey highlight the differences in setup which lead to them showing that large models should be"}, {"title": "2 PAPERS ON SCALING LAWS", "content": "Researchers have proposed scaling laws to study the scaling of deep learning across multiple domains\nand for several tasks. Studies of the scaling properties of generalization error with training data size\nand model capacity predate modern deep learning. Banko & Brill (2001) observed a power law\nscaling of average validation error on a confusion set disambiguation task with increasing dataset size."}, {"title": "3 WHAT form ARE WE FITTING?", "content": "A majority of papers we study fit some kind of power law (f(x) = ax^{-k}). That is, they specify\nan equation defining the relationship between multiple factors, such that a proportional change in\none results in the proportional change of at least one other. They then optimize this power law to\nfind some parameters. A few efforts do not seem to fit a power law, but may show a line of best fit,"}, {"title": "3.1 RATIO OPTIMIZATION", "content": "The simplest scaling law forms usually predict the relation between two variables in an optimal\nsetting. For example, approaches 1 and 2 from Hoffmann et al. (2022) fit to the optimal (i.e., lowest\nloss) D and N values for a particular compute budget C. Porian et al. (2024), aiming to resolve these\ninconsistencies, defines p^* = \\frac{N^*}{D^*} and writes this relationship as:\nN^*(C) = N \\cdot C^{\\theta}; D^*(C) = D^* \\cdot C^{\\rho}; p^*(C) = p_0 \\cdot C^{\\alpha}\n(1)\nThey assume C \u2248 6ND, and thus only need to fit the first equation; the other power laws can be\ninferred. This simplicity is deceptive in some cases, as collecting (N^*(C), C) pairs may not be\ntrivial. It is possible to fix C and follow a binary search approach to train a multitude of models, then\nbisect to approximate the performance-optimal N, D pair. However, this quickly grows prohibitively\ncostly. In practice, it is common to interpolate between a set of fixed results to estimate the true\nN^*(C) \u00a75. This adds to the complexity of this approach, and introduces a hidden dependency on\nthe performance evaluation, yet it does not actually predict the performance of the optimal points.\nIf only the performance of the optimal-ratio model is of interest, it is possible to fit a second power\nlaw L(N^*(C), D^* (C)) = a \\cdot C^{\\alpha}. Most papers we survey choose to fit a power law which directly\npredicts performance."}, {"title": "3.2 PERFORMANCE PREDICTION", "content": "Kaplan et al. (2020) proposes a power law between Loss L, number of model Parameters N, and\nnumber of Dataset tokens D:\nL(N, D) = \\left(\\frac{N}{N_c}\\right)^{\\alpha} + \\left(\\frac{D}{D_c}\\right)^{\\beta}\n(2)\nOn the other hand, Approach 3 of Hoffmann et al. (2022) proposes\nL(N, D) = E + \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}}\n(3)\nIn both of the above, all variables other than L, N, and D are parameters to be found in the power\nlaw fitting process. Though these two forms are quite similar, they differ in some assumptions.\nKaplan et al. (2020) constructs their form on the basis of 3 expected scaling law behaviors, and\nHoffmann et al. (2022) explains in their Appendix D that their form is based on risk decomposition.\nThe resulting Kaplan et al. (2020) form includes an interaction between N and D in order to satisfy a\nconstraint requiring assymmetry introduced by one of their expected behaviors. The Hoffmann et al.\n(2022) form, on the other hand, consists of 3 additive sources of error, E representing the irreducible\nerror that would exist even with infinite data and compute budget, as well as two terms representing\nthe error introduced by limited parameters and limited data, respectively.\nPower laws for performance prediction can sometimes yield closed form solutions for optimal ratios\nas well. However, the additional parameters and input variables, introduced by the need to incorporate\nthe performance metric term, add random noise and dimensionality. This increases the difficulty\nof optimization convergence, so when prediction performance is not the aim, a ratio optimization\napproach is frequently a better choice.\nMany papers directly adopt one of these forms, but some adapt these forms to study relationships with\nother input variables. Clark et al. (2022), for example, study routed Mixture-of-Expert models, and"}, {"title": "4 HOW DO WE train models?", "content": "In order to fit a scaling law, one needs to train a range of models across multiple orders of magnitude\nin model size and/or dataset size. Researchers must first decide the range and distribution of N\nand D values for their training runs, in order to achieve stable convergence to a solution with high\nconfidence, while limiting the total compute budget of all experiments. Many papers did not specify\nthe number of data points used to fit each scaling law; those that did range from 4 to several hundred,\nbut most used fewer than 50 data points. The specific N and D values also skew the optimization\nprocess towards a certain range of N/D ratios, which may be too narrow to include the true optimum.\nSome approaches, such as using IsoFLOPs (Hoffmann et al., 2022), additionally dictate rules for\nchoosing N and D values. Moreover, using a minimum N or D value may result in outlier values that\nmay need to be dropped (Porian et al., 2024; Shin et al., 2023; Henighan et al., 2020). We investigate\nthis choice in Section \u00a77.2\nThe definition of N, D, or compute cost C can affect the results of a scaling study. For example, if a\nstudy studies variation in tokenizers, a definition of training data size based on character count may\nbe more appropriate than one based on token count (Tao et al., 2024). The inclusion or exclusion\nof embedding layer compute and parameters, may also skew the results of a study - a major factor\nin the different in optimal ratios determined by Kaplan et al. (2020) and Hoffmann et al. (2022)\nhas been attributed to not factoring embedding FLOPs into the final compute cost (Pearce & Song,\n2024b; Porian et al., 2024). Given the increase in extremely long context models (128k-1M) Reid\net al. (2024), the commonly used training FLOPs approximation C = 6ND (see Appendix C) may\nnot hold for such models, given the additional cost proportional to the context length and model\ndimension - Bi et al. (2024) introduce a new terms non-embedding FLOPs/token to account for this.\nScaling law fit depends on the performance of each individual checkpoint, which is highly dependent\non factors such as training data source, architecture and hyperparameter choice. Bansal et al. (2022)\nand Goyal et al. (2024), for instance, discuss the effect of data quality and composition on power law\nexponents and constants. Repeating data has also been found to yield different scaling patterns in\nlarge language models (Muennighoff et al., 2024; Goyal et al., 2024).\nResearchers have also studied the effect of architecture choice on scaling - Hestness et al. (2017) find\nthat architectural improvements only shift the irreducible loss, while Poli et al. (2024) suggest that\nthese improvements may be more significant. The way in which a model is scaled can also affect\nresults. Within the same architecture family, Clark et al. (2022) show that increasing the number of\nexperts in a routed language model has diminishing returns beyond a point, while Ghorbani et al.\n(2021) find that scaling the encoder and decoder have different effects on model performance. Scaling\nembedding size can also drastically change scaling trends (Tao et al., 2024).\nThe optimal hyperparameters to train a model changes with scale. Changing batch size, for example,\ncan change model performance McCandlish et al. (2018); Kaplan et al. (2020). Optimal learning rate\nis another hyperparameter shown to change with scale, though techniques such as those proposed\nin Tensor Programs series of papers (Yang et al., 2022) can keep this factor constant with simple\nchanges to initialization. More specifically, changing the learning rate schedule from a cosine decay\nto a constant learning rate with a cooldown (or even changing the learning rate hyperparameters) has\nbeen found to greatly affect the results of scaling laws studies (Hu et al., 2024; Porian et al., 2024;\nH\u00e4gele et al., 2024; Hoffmann et al., 2022)."}, {"title": "5 HOW DO WE collect data FROM MODEL TRAINING?", "content": "To evaluate the range of models trained to fit a scaling law, train or validation loss are most commonly\nused, but some works consider other metrics, such as ELO score (Jones, 2021; Neumann & Gros,\n2022), reward model score (Gao et al., 2023), or downstream task metrics like accuracy or classifica-\ntion error rate (Henighan et al., 2020; Zhai et al., 2022; Cherti et al., 2023; Goyal et al., 2024; Gao\net al., 2023). This choice is non-trivial - while some papers show that there is a power law relation\nbetween the predicted loss found by using validation loss and a different downstream task (Dubey\net al., 2024), it is possible for the results of a study to change completely depending on the metric\nused. Schaeffer et al. (2023), for example, find that using linear metrics such as Token-edit distance\ninstead of non-linear metrics such as accuracy produces smooth, continuous predictable changes in\nmodel performance, contrary to an earlier study by Wei et al. (2022). Moreover, Neumann & Gros\n(2022) find that they are unable to use test loss instead of Elo scores to fit a power law.\nWhile it is most straightforward to evaluate only the final checkpoint on the target metric, some studies\nmay use the median score of the last several checkpoints of each training run Ghorbani et al. (2021),\nor multiple intermediate checkpoints throughout each training run for various reasons. One common\nreason is that this is the only computationally feasible way to obtain a fit with sufficient confidence\nintervals (Besiroglu et al., 2024). For instance, the ISOFLop approach to finding the optimal D/N\nratio in Hoffmann et al. (2022) requires training multiple models for each targeted FLOP budget - this\nwould be computationally prohibitive to do without using intermediate checkpoints. Hoffmann et al.\n(2022), in particular, use the last 15% checkpoints. Some papers also report bootstrapping values\n(Ivgi et al., 2022). This detail is often not specified in scaling law papers, with only 29 of 51 papers\nreporting this information - we point the reader to Appendix C for an overview.\nA related technique is performance interpolation. Porian et al. (2024) do not aim to exactly match\nthe desired FLOP counts when evaluating model checkpoints mid-training. They instead interpolate\nbetween multiple model checkpoints to estimate the performance of a model with the target number\nof FLOPs. Hoffmann et al. (2022) and Tao et al. (2024) also interpolate intermediate checkpoints.\nHilton et al. (2023), relatedly, smooth the learning curve before extracting metric scores.\nAs discussed in Section 4, training models with too little data or too few parameters can skew the\nresults. To prevent this issue, several works report filtering out data points before fitting their power\nlaw. Henighan et al. (2020) drop their smallest models, while Hilton et al. (2023) and Hoffmann\net al. (2022) exclude early checkpoints. Muennighoff et al. (2024) remove outlier datapoints that\nperform badly due to excess parameters or excess epochs. Similarly, Ivgi et al. (2022) remove outlier\nsolutions after bootstrapping."}, {"title": "6 HOW ARE WE optimizing THE FIT?", "content": "The optimization of a power law requires several design decisions, including optimizer, loss, ini-\ntialization values, and bootstrapping. We discuss each in this section. Over half of the papers we\nanalyze do not provide any information about their power law fitting process, or provide limited\ninformation only and fail to detail crucial aspects. Specifically, many papers fail to describe their\nchoice of optimizer or loss function. In Table 2, we provide an overview of the optimization details\n(if specified) for each paper considered."}, {"title": "7 OUR REPLICATIONS AND ANALYSES", "content": "Each of the choices discussed above in Sections 3 - 6 may have a crucial impact on the result,\nyet it remains common to critically underspecify the setup for fitting a power law. Scaling law\nworks often fail to open source their model and code, making reproduction infeasible, and likely\ncontributing to contradictory conclusions as discussed in Section 2. Though some efforts have been\nmade (Porian et al., 2024; Besiroglu et al., 2024) to reconcile such discrepancies, there is still only\nsparse understanding of the impact of each of the decisions we discuss.\nTo investigate the significance of these scaling law optimization decisions, we vary these choices to\nfit our own scaling laws. We fit both the Chinchilla-scraped data from Besiroglu et al. (2024), and\ndata from our own models.\nReconstructed Chinchilla data (Besiroglu et al., 2024) This data is extracted from a vector-based\nfigure in the pdf of Hoffmann et al. (2022), who claim that this includes all models trained for the\npaper. It consists of 245 datapoints, each corresponding to a final checkpoint collected at the end of\nmodel training. There is a potential risk of errors in this recovery process.\nData from Porian et al. (2024) This data includes training losses for Transformer LMs ranging\nin size from 5 to 901 million parameters, each trained on OpenWebText2 (Gao et al., 2020) and\nRefinedWeb data (Penedo et al., 2023), across a variety of data and compute budgets, from 5 million\nto 14 billion tokens (depending on parameter count). Each model is trained with a different peak\nlearning rate and batch size setting, found by fitting a separate set of scaling laws.\nOur models We train a variety of Transformer LMs, ranging in size from 12 million to 1 billion\nparameters, on varied data and compute budgets and hyperparameter settings. Details about our setup,\nincluding hyperparameters, are listed in Appendix A. We open source all of our models, evaluation\nresults, code, and FLOP calculator at https://github.com/hadasah/scaling_laws\nWe fit a multitude of power laws and study the effects of: (1) power law form; (2) model learning\nrate; (3) compute budget, model size, and data budget range and coverage; (4) definition of N and C;\n(5) inclusion of mid-training checkpoints; (6) power law parameter initialization; (7) choice of loss\nand (8) optimizer.\nBased on our observations, we also make some more concrete recommendations in Appendix \u00a7D,\nwith the caveat that following the recommendations cannot guarantee a good scaling law fit."}, {"title": "8 CONCLUSION", "content": "We survey over 50 papers on scaling laws, and discuss differences in form, training setup, evaluation,\nand curve fitting, which may lead to significantly different conclusions. We also discuss significant\nunder-reporting of details crucial to replicating the conclusion of these studies, and provide guidelines\nin the form of a checklist aid researchers in reporting more complete details. In addition to discussing\nseveral prior replication studies in literature, we empirically demonstrate the fragility of this process,\nby systematically varying these choices on available checkpoints and models that we train from"}, {"title": "Ethics Statement", "content": "This work discusses how a lack of reproducibility and open-sourcing may be\nharmful for scaling laws research, given that the factors in a study setup that may change research\nconclusions vary widely."}, {"title": "Reproducibility Statement", "content": "The model checkpoints and analysis code required to reproduce the\nresults discussed in Section 7 are at https://github.com/hadasah/scaling_laws."}, {"title": "D RECOMMENDATIONS", "content": "As seen in our analyses, many decisions in our checklist have a number of reasonable options, but\nthose reasonable choices lead to a wide range of scaling law fits, and the observed variations do not\nfollow any clear pattern. It is probable that variations would be even harder to predict when varying\nmodel architectures or other design decisions, removing the possibility of a universal set of best\npractices. However, it is certainly possible to determine that some scaling law fits are plausible or\nhighly implausible, and to observe the stability of the fitting procedure. With the caveat that following\nany recommendations can not guarantee good scaling law fit, we can make some more concrete\nrecommendations based on these observations:\nScaling Law Hypothesis\n\u2022 Fitting fewer scaling law parameters at a time typically results in greater stability. In some\ncases, it may be beneficial to decompose the scaling law fitting problem into two separate\nprocedures. Examples of this approach are the IsoFLOP procedure from Hoffmann et al.\n(2022), as well as fitting first the relation between L and C, then finding the optimal N and\nD for a C, as seen in Porian et al. (2024).\nTraining Setup\n\u2022 The trained models should include a wide range of input variables settings. For example,\nwhen the input variables to the scaling law are L, N, D, the included models should include\na wide range of N and D values for each C, or equivalently, should include a wide range of\nD/N ratios. If the included settings do not include the true optimum, the procedure will\nstruggle to fit to the optimum.\n\u2022 Sweeping for the optimal learning rate results in a less stable fit than fixing the learning\nrate. We hypothesize that this may be because the true optimal learning rate for each model\nand data budget size is not any of the options we consider, and thus, each model varies in\nthe difference between its true and approximate optimal learning rate. This may introduce\nadditional noise to the data. Due to resource constraints, we are unable to fully test this\nhypothesis, and it may not hold at significantly larger scale, but we recommend fixing the\nlearning rate, or changing it according to, say, model size, according to a fixed formula.\nData Collection\n\u2022 Results across tasks or datasets should not be mixed. Neither performance predictions nor\noptimal D/N ratios are fixed across different evaluation settings for the same set of models.\nFitting Algorithm\n\u2022 Scaling law fitting is sensitive to initialization; most known optimization methods for scaling\nlaws are only able, in practice, to the shift parameters near to their initialization values. Thus,\na dense search over initialization values is necessary. If there is a strong hypothesis guiding\nthe choice of one specific initialization, such as a previously fit and validated scaling law,\nthis will also limit the set of possible final scaling law parameter values.\n\u2022 Different losses emphasize the contribution of errors from certain datapoints. The chosen\nloss should be suited to the distribution of datapoints and sources of noise.\n\u2022 A simple grid search is unlikely to result in a good scaling law fit. Additionally, optimizers\ndesigned to fit linear relations may make assumptions about the distribution of errors and\nshould not be used to fit a power law."}]}