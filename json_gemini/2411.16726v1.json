{"title": "EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion", "authors": ["Haotian Wang", "Yuzhe Weng", "Yueyan Li", "Zilu Guo", "JunDu", "Shutong Niu", "Jiefeng Ma", "Shan He", "Xiaoyan Wu", "Qingming Hu", "BingYin", "CongLiu", "Qingfeng Liu"], "abstract": "Diffusion models have revolutionized the field of talking head generation, yet still face challenges in expressiveness, controllability, and stability in long-time generation. In this research, we propose an EmotiveTalk framework to address these issues. Firstly, to realize better control over the generation of lip movement and facial expression, a Vision-guided Audio Information Decoupling (V-AID) approach is designed to generate audio-based decoupled representations aligned with lip movements and expression. Specifically, to achieve alignment between audio and facial expression representation spaces, we present a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module within V-AID to generate expression-related representations under multi-source emotion condition constraints. Then we propose a well-designed Emotional Talking Head Diffusion (ETHD) backbone to efficiently generate highly expressive talking head videos, which contains an Expression Decoupling Injection (EDI) module to automatically decouple the expressions from reference portraits while integrating the target expression information, achieving more expressive generation performance. Experimental results show that EmotiveTalk can generate expressive talking head videos, ensuring the promised controllability of emotions and stability during long-time generation, yielding state-of-the-art performance compared to existing methods.", "sections": [{"title": "1. Introduction", "content": "Talking head animation, also known as portrait image animation [49], demonstrates significant value across multiple domains, including film and television production, online education as well as human-machine interaction. The generation of realistic talking head videos involves two aspects of requirements. On the one hand, for the verbal aspect, it is essential to ensure the synchronization between speech and lip motions in the generated video [25]. On the other hand, for the non-verbal aspect, the generated video must convey non-verbal information, including vivid facial expressions [24].\nDespite the success of diffusion models [14, 22, 33] in image and video generation tasks, their application in talking head generation [21, 29, 34, 36, 43] still faces several challenges. For example, current methodologies [21, 36, 43] exhibit shortcomings in control of the generated emotional facial expressions, although they have made notable advancements in achieving synchronization between speech and lip movements. These audio-driven methods mainly directly synthesize expressions under weak audio conditions [36, 43]. However, the coupling of multiple information embedded in audio limits the effective learning of the mapping between speech and expressions and the controllability of generated emotion. Moreover, current diffusion-based methods often struggle to generate high-resolution video due to their large scale of parameters and the associated training costs [36, 43]. They also face challenges in stability during long-time generation due to their auto-regressive inference strategies [21, 36, 43], which can lead to error accumulation across multiple inference clips.\nTo address these challenges, in this paper, we introduce EmotiveTalk, a highly expressive talking head generation framework with emotion control based on video diffusion. We propose a Vision-guided Audio Information Decouple (V-AID) approach to facilitate the decoupling of lip and expression related information contained in audio signals and also the alignment of audio representations with video representations under the guidance of vision facial motion information. Specifically, to achieve better alignment between speech and expression representation spaces, we present a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module, which generates temporal expression-related representations from audio under utterance emotional conditions from multiple optional driven sources. Then, to effectively drive the decoupled representations, we propose an efficient video diffusion framework for expressive talking head generation which demonstrates effectiveness and enhanced stability in talking head video generation performance. The backbone incorporates an Expression Decoupling Injector (EDI) module in our backbone to achieve the automatic decoupling of expression information from the reference portrait while facilitating the injection of expression-driven information. In summary, our contributions are as follows: (1) We propose a Vision-Guided Audio Information Decouple (V-AID) approach that generates efficient decoupled lip-related and expression-related representations from audio for talking head generation. (2) We propose an Emotional Talking Head Diffusion (ETHD) framework that capable of efficiently generating dynamic-length videos, which achieves highly expressive talking head video generation while ensuring stability over extended durations. (3) We further enhance emotion controllability by integrating conditions from emotion-driven sources and realize the customization of generated emotions by multi-source emotion control."}, {"title": "2. Related Work", "content": "The initial focus of the audio-driven talking head video generation task was on achieving consistency between lip movements and the driving speech signal [25, 29]. SadTalker [46] and Audio2Head [41] integrate 3D information and control mechanisms to enhance the naturalism of head movements. Diffused Heads [34] and DreamTalk [21] further achieve more vivid results. Recently, a major shift occurred with the introduction of text-to-image pre-trained models. EMO [36], Hallo [43] and other similar frameworks [38, 42] built on the foundation of pre-trained image diffusion models [26], achieving high-fidelity talking head video generation. Traditional audio-driven methods simply based on a data-driven approach, lack optional control on expression styles. The large-scale network and widely-used auto-regressive inference strategy [13, 21, 43, 44] also constrain their ability to generate stable long-time videos."}, {"title": "2.2. Controllable Talking Head Generation", "content": "Controlling the expression style in talking head video generation has long been a compelling challenge. Early methods [8, 10, 12, 18, 31, 35] model expressions in discrete emotion states, while recent methods [19\u201321, 39] focus on transferring the expressions from a reference video to the generated video. Extracting decoupled representations of expressions is crucial for emotion transferring. Earlier approaches [20, 21] use 3DMM coefficients [4, 9] from reference videos, but this led to identity leakage issues, as the 3DMM coefficients encode not only expression information but also the speaker's facial structure information. PDFGC [39] and Anitalker [19] employ contrastive learning approaches to acquire expression-related latent and realize expression driven with minor identity leakage.\nIn practical applications, emotion control information can originate from many other sources [1, 27]. In our approach, we derive a unified emotional control latent from various optional sources of emotion information and enable emotion control based on the emotion control latent."}, {"title": "2.3. Video Diffusion Models", "content": "The pioneering work on text-to-video diffusion is Video Diffusion Models (VDM) [16]. Imagen Video [15] enhances VDM with cascaded diffusion models. Make-A-Video [30] and MagicVideo [50] then extend these concepts to enable seamless text-to-video transformations. AnimateDiff [11] utilizes a motion module to realize the conversion from text-to-image to text-to-video. Stable Video Diffusion (SVD) [5] implements innovative training strategies to generate high-fidelity videos. In our research, we use diffusion models in both expression motion latent generating and talking head rendering under facial motion control conditions."}, {"title": "3. Method", "content": "As shown in Fig. 2, the structure of the EmotiveTalk is divided into two main parts: (1) the Vision-guided Audio Information Decouple (V-AID) with Diffusion-based Co-speech Temporal Expansion (Di-CTE) module; (2) the Emotional Talking Head Diffusion (ETHD) framework with Expression Decoupling Injector (EDI) module."}, {"title": "3.1. Preliminary", "content": "The task of controllable talking head generation involves creating a vivid talking head video from two inputs: a static single-person portrait \u00e6ref, and a driven speech sequence A \u2208 RNa. Besides, emotion sources Sdri can also utilized as optional input to realize better controllability on emotion. When optional Sdri is not provided, our method aims to generate expression-related representation solely by the speech input and the portrait xref. The output is the generated video frames X1:N = {20, \u2026\u2026\u2026,\u00c2N }. Let X(0) represent video latents sampled from a given distribution q(X(0)). In the forward diffusion process, Gaussian noise is progressively added to Xo, gradually diffusing towards a distribution resembling N(0, I). This process forms a fixed Markov chain [22, 33]:\nq(X(t)|X(t-1)) = N(X(t); \u221a 1 \u2013 \u03b2t X(t-1), \u03b2tI)                                                                                                    (1)\nwhere {\u03b2t}T=1 are known constants. Notably, the marginal distribution at any time can directly derive from X(0) as:\nq(X(t)|X(0) = N(X(t); \u221a\u0101t X(0), (1 \u2013\u0101t)I)                                                                                                         (2)\nwhere \u0101t = \u03a0t=1 \u03b1i and \u03b1t = 1 \u2212 \u03b2t. The reverse process gradually recovers the original video latent from the noisy latent X(T) ~ N(0, I), achieving by training a network to predict the posterior distribution po(X(t\u22121)|X(t), c) under condition set c. To learn po(X(t\u22121)|X(t)). The model is trained using the following loss function:\nL = Et,x0,6,c[||\u20ac \u2013 \u20ac0(X(t), t, c) ||2]                                                                                                                          (3)\nWe use diffusion strategy for expression-related representation generation in Di-CTE and video latent generation."}, {"title": "3.2. Vision-guided Audio Information Decouple", "content": "Speech is rich in plentiful coupled information, previous methods focused on decoupling speech information in the audio space [45, 51]. However, the representations obtained through these approaches are generally not well-suited for talking head generation, due to the inherent disparity between the audio and facial motion representations. We propose that facial motion information in the vision space can guide the decoupling of coupled speech information due to the correlation between speech information and different facial motions and also facilitate the generation of aligned facial motion related representations from audio. Based on this, we designed a Vision-guided Audio Information Decoupling (V-AID) module. This module takes audio sequence A and reference portrait \u00e6ref as input. The audio stream first passes through a pre-trained Wav2Vec audio encoder [2], followed by the trainable audio-to-lip projector and audio-to-expression generator to obtain lip and expression-related latents. The two modules are trained under the supervision of lip and expression representations of vision space, elaborated in the supplementary material.\nWe leverage the latent representation of lip motions in vision space to guide the audio-to-lip mapping, thereby achieving alignment between the audio and lip motion representations. Specifically, we use a pre-trained lip encoder to extract decoupled lip-related latents \u012b\u2081 = {l1, ..., lv } from videos paired with audio. The audio stream is processed through an audio-to-lip projector with a Perceiver Transformer [17] architecture detailed in the supplementary material to generate lip-related latents \u00ce\u2081 = {l1, ..., \u00cen}. The infoNCE [23] contrastive loss function is utilized to optimize the lower bound of mutual information (MI) between la and \u00ce to maximize MI between frame-level lip movements and the corresponding driving speech signal, where(4, 4) denotes a positive pair and (i, \u00ce) denotes negative pairs. The loss function is formulated as follows, with sim(\u00b7) represents cosine similarity:\nLlipc =\u2211N i=1log(exp( sim(li,\u00cei)\u2211Nj=1exp(sim(li,\u00cei))))                                                                                                      (4)\nFurthermore, we also supplement the contrastive learning loss with Mean Squared Error (MSE) loss to synchronize both the motion and morphological information between la and \u00ce. The loss function is as follows:\nLlipm =1N\u2211Ni=1 ||li\u2212\u00cei||2                                                                                                            (5)\nThe final training loss function is the combination of two losses, as follows:\nLlip = \u03b1Llipc + \u03b2Llipm                                                                                                                                           (6)\nWe utilize representations of facial expressions from the vision space to guide the alignment of audio-based emotion information with facial expressions. Generally, speech and facial expressions are not strictly correlated on a one-to-one basis, the same speech can correspond to different but plausible facial expressions. To address this, we propose a Diffusion-based Co-speech Temporal Expansion (Di-CTE) module to generate frame-level expression-related latent ea from initial expression under speech constraints, leveraging the advantages of diffusion models in terms of generative diversity. We leverage a pre-trained expression encoder to extract decoupled expression latent ey from ground-truth video as vision supervision. Di-CTE inputs consist of a reference frame (xref) from the ground truth video serving as speaker identity and speech embedding Aw to provide temporal emotion information. During training, the emotion condition econd is provided by the first frame of the ground-truth video, and the output is expression-related latent ea sync with the speech. The denoising loss of network Se is defined as follows, where t denotes the DDPM step:\nLexp = ||ev - Se(Ea(t), t, xref, Aw, econd)||2                                                                                                                                   (7)\nFinally, to decouple lip-related and expression-related information and mitigate their mutual interference, we introduced a mutual information (MI) constraint during the joint training of the audio-to-lip and audio-to-expression modules. Specifically, we employ CLUB [6] to optimize the upper bound of MI between the lip-related latent la from the audio-to-lip module and the expression-related latent ea from the audio-to-expression module. The total loss function is as follows:\nLV-AID = Llip + Lexp + CLUB{la, ea}                                                                                                                  (8)\nBy minimizing the MI between \u012b and \u0113a, we achieve a separation of the two representation spaces."}, {"title": "3.3. Emotional Talking Head Diffusion", "content": "In this subsection, we present a diffusion-based framework for generating emotional talking heads. Before ETHD, the driving audio is processed through V-AID to obtain lip-related and expression-related latents, and the portrait is projected into latent space via temporal Variational Autoencoder (VAE) and concatenated with input noise along the channel dimension, shown in Fig. 2. ETHD outputs a sequence of frame latents synchronized with the speech.\nOur backbone network leverages a 3D-Unet architecture with the spatial-temporal separable attention mechanism [5]. The spatial attention module comprises two blocks. Firstly, the lip-related latent \u012b is injected through spatial cross attention. Then, an Expression Decoupling Injector (EDI) module, articulated late in Section 3.3, is employed to integrate expression-driven latent edri (edri = ea for audio-driven task and edri = \u1ebd for video-driven task). Analogously, the temporal attention module also encompasses two components: a temporal self-attention mechanism and a temporal cross-attention module. The temporal cross-attention module engages in cross-attention with expression-driven latent to learn subtle temporal variations in emotional expression. The output latents are then processed through a temporal VAE decoder to obtain the generated motion frames.\nIn talking head generation, the inherent expression information in the reference portrait usually constrains the generation of the target expression, leading to sub-optimal expressive results. To address this, we propose an Expression Decoupling Injection (EDI) module to achieve emotional expressions by automatically decoupling the expression information from reference portraits while integrating the expression-driven information, which consists of two parallel attention branches. One branch computes the attention between the hidden states Hi \u2208 Rfxh\u00d7w\u00d7c (f is the number of processed frames, h and w is the height and width of hidden states, c is the number of channels) and the expression embeddings eref of the reference portrait while the other branch computes the attention between the hidden states Hi and the expression-driven representation edri. By subtracting these two cross-attention outputs, we achieve the transition of facial expressions in the generated video from the expression of the reference image to the driving expression, as shown in the following equation:\nAttni =CrossAttn(Hi, \u0113dri)-CrossAttn(Hi,\u0113ref)                                                                                                   (9)\nMoreover, to enforce the expression-related latent act only on the facial region without affecting the lip region generation, we apply an attention mask similar to Hallo [43] to the resulting attention value. Specifically, we use the off-the-shelf toolbox OpenFace [3] to predict landmarks from portrait \u00e6ref and calculate binary bounding box masks Mlip, Mface \u2208 {0,1}h\u00d7w which indicate the inner of lip region and face region. Then, the output of the EDI block is formulated based on bounding box masks, as follows:\npaH = Hi + Attni (1 \u2013 Mlip) \u2297 Mface                                                                                                                                                 (10)\nTo implement better modeling of the time-variance of facial expressions, we introduce a temporal cross-attention module. Specifically, we squeeze the spatial dimensions of the hidden states Hspa to Hem \u2208 R(hxw)\u00d7f\u00d7c and compute the cross-attention between Hem and the expression-driven latent \u0113dri. This makes the model more sensitive to the temporal correlations of emotional information. Additionally, the same bounding box masks are utilized to constrain the sensible area of attention calculation."}, {"title": "3.4. Training and Inference", "content": "The V-AID module in Sec. 3.2 is first pre-trained to generate decoupled lip-related representation la and emotion-related representation ea from driven audio window Aw and then remain frozen while training the ETHD backbone.\nSubsequently, we train the ETHD backbone by sampling tuples (X, xref, t, la, Eref, Edri), Edri is random choice in video expression-related representation \u0113, and the generated expression-related representation ea. The total denoising loss function is formulated as:\nLde = ||X (0) - So (x(t), xref, t, la, eref, Edri)||2                                                                                                                   (11)\nIn the inference phase, we employ a non-autoregressive inference method to avoid the accumulation of error. Specifically, when performing long-time generation, we sample a Gaussian-like noisy latent and divide the total duration into several overlapping clips with a defined window size. We utilize DDIM [33] sampler for ETHD to denoise each clip sequentially per step, then we assign a weighting strategy the same as Mimicmotion [47] to assign higher fusion weights for frame latents closer to the center of each clip. Repeat this process iteratively to obtain the clean frame latent. This approach allows us to perform inference of arbitrary lengths without error accumulation."}, {"title": "3.5. Multi-source Emotion Control", "content": "To flexibly control emotional expression in generated video based on control sources, we designed the Multi-source Emotion Control (MEC) pipeline. MEC introduces time-varying facial expressions to the generated video based on optional temporal or utterance sources.\nExternal expression-driven videos are treated as temporal sources, denoted as ST, due to their rich temporal variations in expression. We directly apply the pre-trained expression encoder to extract the expression-driven latent ev, as detailed in Section 3.3. The final emotive video is rendered using exp-driven latent \u0113, and lip-related latent \u012b, derived from Section 3.2.\nTo improve the temporal dynamism and better alignment with the driving speech of generated expressions based on utterance sources Su that only provide general emotional information econd, we use the Di-CTE module (Section 3.2) to generate frame-level expression-driven latent ea from econd. Specifically, for expression-driven images of different people (xdri), we map the image to the emotion condition latent space econd using pre-trained expression encoder (Section 3.2). For cross-modality control sources like tdri, we apply a cross-modality mapping to align with econd, detailed in the supplementary material. The final emotive video is rendered using lip-related latent \u012b and expression-driven latent \u0113 via our diffusion backbone, as detailed in Section 3.3."}, {"title": "4. Experiment", "content": "Experiments encompassing both training and inference were carried out on open-source datasets HDTF [48] and MEAD [40], which consist of talking individuals videos of diverse genders, ages, and ethnicities. We utilize a two-stage training strategy, firstly, we trained the V-AID module with a learning rate of 1e-4. In the second stage, the audio-to-video diffusion backbone was trained while the pre-trained V-AID modules remained frozen in training. Notably, thanks to the efficient design of our model, we can conduct high-resolution and long-time video training. We conduct a training configuration of the resolution of 512 x 512 and 120 frames. The learning rate is set to le-5 with a batch size of 1. Our backbone also supports up to 1024 \u00d7 1024 training, and experiments on other configurations are detailed in the supplementary material.\nDuring the inference, we use the sampling algorithm of DDIM [33] to generate the video clip for 25 steps, the inference window size is as same as the training frame number and the overlap is set to 1/5 of the window size."}, {"title": "Evaluation Metrics", "content": "The proposed framework has been evaluated with several quantitative metrics including Fr\u00e9chet Inception Distance (FID) [28], Fr\u00e9chet Video Distance (FVD) [32, 37], Synchronization-C (Sync-C) [7], Synchronization-D (Sync-D) [7] and E-FID [36]. Specifically, FID and FVD conduct the image-level and frame-level measurement of the quality of the generated frames and the similarity between generated and ground-truth frames, with lower values indicating better performance. The SyncNet scores assess the lip synchronization quality, with higher Sync-C and lower Sync-D scores indicating better alignment with the driven speech signal. Additionally, to evaluate the expressiveness of the facial expressions in the generated videos, we also utilize the Expression-FID (E-FID) metric introduced in EMO [36] to quantitatively measure the expression divergence between the synthesized videos and gound-truth videos."}, {"title": "Baselines", "content": "We conducted a comparative analysis of our proposed method against several open-source implementations, including audio-driven strategies including SadTalker [46], AniPortrait [42], Anitalker [19] and Hallo [43], and audio-video driven strategies including PDFGC [39], styletalk [20] and dreamtalk [21]. For audio-driven comparison, our framework derives lip-related and expression-driven latents solely from the audio and reference portrait input. As for audio-video driven, the expression-driven latent is derived from the paired video."}, {"title": "4.2. Overall Evaluation", "content": "Tab. 1 shows the results of the comprehensive comparison with other methods. Overall, methods based on pre-trained diffusion models like Hallo or Aniportrait achieve optimal FID metrics, confirming the potential of diffusion models in generating high-fidelity videos. Also, video-driven approaches perform better on the E-FID metric benefiting from the inclusion of expression cues derived from video. Our method outperforms previous methods in both audio-driven and video-driven tasks across most metrics, especially on E-FID and SyncNet metrics, highlighting its superior capabilities of generating high-fidelity and vivid videos."}, {"title": "4.3. Ablation Study", "content": "To analyze the contributions of our designs, we conduct ablation studies on our main modules.\nWe conduct an ablation study with three variants: (1) driven only by original audio embedding without V-AID (no decouple); (2) driven without lip-related latents (w/o lip-related); (3) driven without expression-driven latents (w/o exp-driven). Our full model is denoted as (V-AID). The experiment is carried out in the test subset of HDTF. Shown in Tab 2, the results indicate that using V-AID shows improvements across all three metrics compared to direct injection without decoupling, with notable gains in the Sync-C and E-FID metrics. Additionally, we observe a significant drop in Sync-C when lip-related latents are removed, and a substantial degradation in E-FID when expression-driven latents are excluded. This supports the different roles that the two representations play in driving lip movement and facial expressions. Furthermore, we observe that FID achieves the best performance without expression-driven, this is due to the higher similarity between generated frames and reference images when expression-driven latents are excluded, further confirmed in subsequent experiments."}, {"title": "Effectiveness of Decoupled Representations.", "content": "To evaluate the decoupling ability of two representations, we utilized the lip-related and expression-driven latents from V-AID to generate video separately and visualize the results. Shown in Fig. 3, the visualized results indicate that the main movement occurs at the lip region, with higher heat values concentrated around the lip region. In contrast, the generated frames driven by the expression-driven latents exhibit substantial changes in facial expressions compared to the reference portrait, with higher heat values distributed across the entire facial area, particularly in the eye region associated with angry emotion. The results demonstrate the effectiveness of decoupled lip and expression representations in controlling facial motions separately."}, {"title": "Effectiveness of Di-CTE.", "content": "To validate the superiority of our proposed Di-CTE module in expanding utterance driven sources to generate time-variance expressions, we employed a single expression-driven image and conducted inference using two configurations: with the Di-CTE module (w/ Di-CTE) and without the Di-CTE module (w/o Di-CTE). The facial expressions in inference results w/o Di-CTE module activated show minimal temporal variation in Fig. 4, while more expressive and vivid results are achieved by Di-CTE activated, demonstrating its effectiveness."}, {"title": "Effectiveness on Long-time Generation.", "content": "To validate the stability of long-time generation, we conducted studies on generating with varying lengths by audio-driven. We employed four different test configurations, ranging from short to long duration, and evaluated identity consistency, lip-sync accuracy, and expression alignment across varying generation durations. The results are presented in Tab 3. The results indicate that as inference duration increases, the FID, SyncNet, and E-FID metrics exhibit relatively minor fluctuations without degradation trend over time, confirming the stability of EmotiveTalk in long-time inference scenarios."}, {"title": "4.4. Case Study", "content": "Fig. 5 shows the qualitative results on audio-driven approaches, where driven sources contain only portrait \u00e6dri and driven audio A. Results show that Anitalker and Sadtalker struggle with generating video faithful to the reference image xref deal to the cropping and warping operation and also fall short in lip synchronization. Hallo demonstrates the ability to preserve speaker identity, but encounters instability issues in video generation, resulting in the unintended appearance of artifacts. Our method surpasses previous approaches in achieving lip synchronization, identity maintenance, and generation stability, resulting in the best overall performance."}, {"title": "Comparison on Emotion Control.", "content": "To evaluate the performance of emotion control, we use a portrait paired with a happy video from another person and employ various methods to transfer the emotion. Fig. 6 shows the results, which indicate that StyleTalk and DreamTalk struggle in lip synchronization due to the coupling of lip and expression. PDFGC faces the challenge of lip shape deformation. Our method achieves the most neutral and expressive emotion control results also ensures lip sync, highlighting the effectiveness of our decoupling approach and model design."}, {"title": "4.5. User Study", "content": "We generated 10 test samples covering various emotion states and used 7 different models to generate with the ground-truth samples included. We conducted a user study of 26 participants, for each method, the participant is required to score 10 videos sampled from the test samples and is asked to give a rating (from 1 to 5, 5 is the best) on four aspects: (1) the lip sync quality (Lip-Sync), (2) the quality of expressions (Exp-Q), (3) the realness of results (Realness), (4) the quality of generated video (V-Q). The results are shown in Tab. 4, our method outperforms existing approaches across all aspects, particularly in expression quality and lip sync, highlighting its superior capabilities."}, {"title": "5. Conclusion", "content": "In this work, we propose EmotiveTalk, a novel method that aims at enhancing the emotional expressiveness and better controllability of talking head video generation. We propose a novel approach to decouple audio embedding by leveraging facial motion information, enabling the generation of decoupled representations that correspond directly to lip motions and facial expressions. Additionally, we introduce a well-designed video diffusion framework that drives these representations to generate expressive and extended talking head videos. Due to the effectiveness of our decoupling, we further enhance the controllability of emotion generation by incorporating additional emotion information from multiple optional sources to customize the generated emotions. Extensive experiments demonstrate the superiority of EmotiveTalk."}]}