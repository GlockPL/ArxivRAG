{"title": "SNN-Based Online Learning of Concepts and Action Laws\nin an Open World", "authors": ["Christel Grimaud", "Dominique Longin", "Andreas Herzig"], "abstract": "We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural\nnetwork (SNN) implementing the agent\u2019s semantic memory. The agent explores its universe and learns concepts\nof objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action\nconcepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent\u2019s\nknowledge of its universe\u2019s actions laws. Both kinds of concepts have different degrees of generality. To make\ndecisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the\naction to take on the basis of these predictions. Our experiments show that the agent handles new situations by\nappealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.", "sections": [{"title": "1. Introduction", "content": "The ability of a cognitive agent to act adequately in\na given environment depends on its ability to predict\nhow performing a given activity might affect its cur rent situation, that is, it depends on its knowledge of\nits environment\u2019s action laws. How artificial agents\ncan acquire these laws and how these should be up dated if their environment changes has proven a dif ficult question. In the case where the intended envi ronment is open\u2014that is, where the agent\u2019s designer\ncannot foresee all the situations the agent might en counter in the future\u2014, providing a suitable set of ac tion laws to the agent \u201cby hand\u201d is unfeasible.\nThe only viable solution is that the agent continuously\nlearns the relevant laws from experience, just as nat ural agents (humans and animals) do. Crucially, this\nlearning process should allow for generalization over\ndisparate experiences, so that the agent is able to be have appropriately in new situations. It should also\naccommodate environment changes.\nThe present paper intends to show how this could\nbe done. Its main thrust is that natural agents\u2019 abil ity to perform well in our open and changing world\nrelies on the fact that they store their knowledge in\nthe form of concepts, which can have various de grees of generality. Presumably, they first form con cepts about the encountered objects and situations,\nand then use these as building blocks for relational\nconcepts, among which are concepts of actions sup porting their knowledge of their environment\u2019s action\nlaws. We suggest that artificial agents could do just\nthe same, relying on some artificial neural network to\nlearn and store concepts, and then querying it to make\npredictions about the outcome of envisaged actions.\nTo test this idea, we here build an artificial hybrid\nagent with a SNN at its core. We make this agent live\nin a very simple virtual world, composed of rooms\nwhich may be, or not, accessible (hence, knowable)\nto it. At first, the agent is confined to one single room\nand learns by itself how to act in it according to its\nown interests. Then at some point a door opens to a\nnew room, containing some never encountered before\nobjects and situations. Yet, although these are new to\nthe agent, some general laws are preserved from one\nroom to the other. We show that having learned these\nlaws in the first room allows the agent to act by and\nlarge properly in the second one, as soon as it enters\nit. We also show that relying on neurally implemented\nconcepts allows the agent to rapidly update its knowl edge and adapt to environment changes.\nThe paper is organized as follows. In Section 2 we\ndiscuss related work, and in Section 3 we present the\nagent and its universe. Section 4 clarifies the notions\nof concepts and actions laws we use, while Section 5\ndescribes the neural network and its functioning. Sec tion 6 describes the agent\u2019s general functioning and\nSection 7 presents the results. Finally, Section 8 con cludes and discusses future possible developments of\nthe framework."}, {"title": "2. Related Works", "content": "Our research problem is autonomous online learning,\ngeneralization and updating of concepts and actions\nlaws in an open universe. The intended application\nis reasoning and planning for autonomous robots. To\nour knowledge, no existing approach addresses this\nproblem in all its dimensions, even though these are\ninvestigated in separate research fields.\nContinual Learning tackles the problem of life- long knowledge acquisition Wang et al. (2024);\nLesort et al. (2020). Its main challenge is to avoid\ncatastrophic loss of previous knowledge when acquir- ing new knowledge; a secondary research axis is one- shot/few-shots learning, i.e., the ability to learn online\nfrom one or few examples Wang et al. (2020). How- ever, current approaches mostly consider the learn- ing of tasks (mostly image classification/recognition tasks, but also some more complex tasks such as playing games Kirkpatrick et al. (2017)), not of con- cepts nor action laws. Furthermore, most of them rely on supervised learning and/or labelled training\ndata, which is unsuitable for open world autonomous\nagents.\nConcept Learning has mainly been studied in\nview of explainability Gupta and Narayanan (2024),\nmostly of classification models (e.g., Koh et al. (2020)) but also of decision making in the context of reinforcement learning Das, Chernova, and Kim (2023); Zabounidis et al. (2023). For this reason, many proposals are dedicated to learning a human- predefined set of concepts using some annotated data.\nIn the field of Image Classification some approaches deal with the extraction of concepts from data Wang, Lee, and Qi (2022); Ghorbani et al. (2019); Hase et al. (2019), but in these approaches concepts are extracted from labelled classes of images, which is, again, un- suitable for open world autonomous agents. Further- more, the vast majority of proposed methods disre- gard the hierarchical organization of concepts from\nparticular to more general, and they generally do not\naddress one-shot learning, online revision or updating\nof concepts.\nAction Learning has been studied from various\nperspectives. In Dynamic Epistemic Logic, Bolander\nand Gierasimczuk (2018) proposed a method to learn\nan action model through successive observations of\ntransitions between states. However this method does\nnot achieve generalization nor accommodate environ- ment changes and only considers the universally ap- plicable actions (i.e., actions that can be executed\nin every logically possible state), a condition real- world actions rarely satisfy. In the field of Planning, Bonet, Frances, and Geffner (2019) showed how to\nlearn abstract actions from a few carefully chosen in- stances of some general planning problem. However said instances come with their own set of ground ac- tions which must be known beforehand, hence this ap- proach cannot be used in open worlds, where an agent\nneeds to incrementally learn from sequential observa- tions.\nReinforcement Learning (RL): Our work could\nbe related to model-based approaches of RL Moer- land et al. (2023), but differs from them in two impor- tant aspects. First, we are only interested in learning\na model of the environment, not in learning policies.\nDespite their undeniable successes, RL approaches\nstruggle to adapt to environment changes and to re- vise learned policies Kirk et al. (2023); Farebrother, Machado, and Bowling (2018). We believe that an\nagent that would be able to learn online a model of the environment and to dynamically use it to make\ndecisions would be able to quickly adapt its behav- ior. Second, the learning process we propose does not depend on the existence of rewards, which makes it\nsuitable for contexts where rewards are scarce."}, {"title": "3. The Agent and its Universe", "content": "The agent's universe is built over a grid of boxes, which we (not the agent) identify using an orthonor- mal coordinate system (see Figure 1).\nEach box represents a particular location in the agent's universe and possesses a particular set of fea- tures the agent is able to perceive, drawn from the set LF = {OK, KO, NorthWall, EastWall, SouthWall,\nWestWall, Cold, Sound, #0, #1, ..., #24}. Although\nthe agent's universe is not finite, at any time point\nwe only consider the boxes to which it has access, the set of which is always finite. For example, the box with coordinates (-2, -2) has the feature set LF (-2,-2) = {OK, SouthWall, WestWall, Cold, #0},\nwhile the box with coordinates (5,0) has the feature set LF (5,0) = {OK} (\u201c#n\u201d is to be taken as a par- ticular name for a box hence a feature, not all boxes\nneed to have one). Two boxes with the same fea- ture set are indistinguishable for the agent. Boxes'\nfeature sets may change over time, reflecting envi- ronment changes. The rooms are made out of boxes,\nand delimited with impassable walls. Opening a door amounts to removing the wall features from the con- cerned boxes' feature sets (as in Figure 1.B).\nThe agent is composed of a set of sensors, a per- ceptual system, a semantic memory, a decision sys- tem, a motor system and a set of actuators (see Fig- ure 2). Sensors collect data from the external world\nand feed it to the perceptual system, which performs"}, {"title": "4. Concepts and Action Laws", "content": "The agent is able to form two kinds of concepts. First, concepts of \"things\", in the broad sense. These bind\ntogether co-occurrent features, and can be seen as\nsome sort of conjunction in which conjuncts have dif- ferent \"weights\", reflecting the fact that some features\nare more important than others in a concept's defi- nition Freund (2008). They are used to store knowl- edge about locations and more generally any object,\nso we call them object concepts. The second kind\nis relational concepts. These take other concepts as\nelements, and bind them together into tuples. Con- cepts of actions are of this kind: they bind together\nthe agent's concepts of a depart location, a performed motor activity, and a subsequent outcome, in the order in which they were experienced.\nWe say that an object concept X is general, as op- posed to particular, if there is another concept Y such that the set of features composing X is a strict subset\nof the set of features composing Y. Y is then said to\nbe more particular than X. We say that an action con- cept is general if the object concept of its initial situ- ation is general or its motor activity component only\ncontains Diag. or Orth.. We understand the generality\nof concepts relative to the set of concepts the agent\npossesses at some point, so no concept is general or\nparticular in itself.\nFor example, when visiting the box (0,0) the agent\nmay form the particular object concept [OK,#12],\nwhich is a memory of an OK place with name\n#12, and only applies to this particular box in its accessible universe. If it then moves North-East\nand arrives at box (1, 1), it can form the particular\nobject concept [OK, #18], and also the particular\naction concept [[OK, #12], [NE,Diag], [\u039f\u039a, #18]]\nwhich corresponds to the memory of being in an OK\nplace with name #12 and then moving North-East\nto arrive at another OK place with name #18. Yet,\nafter visiting a number of locations having the feature\nOK in common, the agent may also form the general\nobject concept [OK]. Furthermore, it is a general rule\nin its accessible universe that moving North-East\nfrom an OK location always leads to another OK\nlocation, except for when there is a wall at the North\nor East edge of the depart box. Therefore, after having\nexperienced a number of North-East moves from\nvarious OK locations, the agent may form general\naction concepts such as [[OK], [NE,Diag],[OK]] and\n[[OK, NorthWall] [NE, Diag], [Failure]].\nSuch general concepts capture the general (non- monotonic) action laws of the agent's universe,\nand are the ones it shall rely on to behave in never encountered situations."}, {"title": "5. Implementing the Agent's Semantic\nMemory in the Neural Network", "content": "Spiking Neural Networks (SNNs) are well suited for autonomous learning in open universes, as they allow\nfor Spike Time Dependent Plasticity (STDP), a fam- ily of biologically plausible learning rules which can\nachieve unsupervised online learning from unlabelled\ndata Thiele, Bichler, and Dupret (2018). They are also\nknown for being energy-efficient, which is interesting\nfor autonomous robots.\nWe take inspiration in the JAST learning rule\nThorpe et al. (2019); Thorpe (2023), which is a sim- plified version of STDP where the sum of the affer- ent connections weights on any given neuron remains\nconstant through learning. However, contrary to JAST\nwe do not use binary weights but natural numbers.\nMoreover, we do not freeze neurons after learning, so\nas to allow updating.\nThe Network's Architecture\nThe network is composed of an interface, which com- municates with the agent's other components, and a\nbody of hidden neurons which is itself divided into\ntwo layers (see Figure 3).\nThe first layer learns object concepts and the sec- ond learns action concepts. For this reason we call\ntheir neurons, respectively, object concept neurons\n(O-neurons for short) and action concept neurons (A- neurons). This architecture draws on neuroanatomi- cal studies according to which concepts are repre- sented in the brain by hierarchically organized con- cept neurons, each receiving information from some\nlower neurons and sending reciprocal connections to these same neurons so that it can reactivate them for\ninformation retrieval Quiroga (2012); Bausch et al.\n(2021); Shimamura (2010). For simplicity we do not\nmodel these reciprocal connections as such, but in- stead we allow for information to flow in both direc- tions along the same connections: from interface neu- rons to O-neurons and then to A-neurons for learning\nand querying, and the other way round for retrieving information. A key point is that interface neurons are\nboth input and output neurons, depending on the com- putational phase.\nInterface neurons (I-neurons for short) mainly sup- port the representation of features, be it of the visited\nlocations or of the agent's own motor activities. An additional neuron acts as a failure detector, specifi- cally firing when the agent bumps into a wall and re- mains at the same place. All of them have their labels\nfixed from the start.\nThe first layer of hidden neurons is composed\nof 100 integrate and fire neurons, with a differen- tiated dynamics depending on whether their input"}, {"title": "The Network's functionning", "content": "We briefly describe the network's functioning. A more detailed account is provided in Supplementary\nMaterial.\nO-neurons' Learning\nEach time the agent observes its current location, information from the perceptual system is sent to\nthe network's interface, inducing the firing of the I- neurons that encode the location's features. This in turn triggers the firing of a number of O-neurons. If\nthis number reaches some fixed target number, the network directly proceeds to make them learn. Other- wise it looks for additional O-neurons by \"boosting\"\ntheir input. In practice, boosting consists in multiply- ing the input received by each non-firing O-neuron o\nby a factor $b_o$, which is an increasing function of the\nnumber of steps performed since o's last spike. This procedure favors the firing of O-neurons that have\nbeen inactive for a long time, which are then re-used for learning.\nThe learning process depends on the accuracy of the agent's knowledge about its current location. To\nassess it, the (pre-boosting) firing O-neurons send a backward input to I-neurons, and the resulting set of\nfiring I-neurons (the retrieved information) is com- pared with the initial input (the current observation). If all the observed features can be retrieved from ac- tive O-neurons, then all inactive I-synapses (if any) on\nthe learning O-neurons are deleted and replaced with synapses from input neurons. This procedure tends to\nreinforce O-neurons' connections with I-neurons en- coding well shared features at the expense of connec- tions with I-neurons encoding more specific features, fostering the learning of general concepts. If, on the\ncontrary, not all the observed features can be retrieved\nfrom the firing O-neurons, then we pick one learning O-neuron with maximal number of steps since its last\nlearning and replace all its synapses, active or inac- tive, with synapses from input neurons. This neuron\nthus learns the particular situation with all its features.\nThe learning process is similar to the first case for the\nother learning neurons.\nQuerying the network\nTo query the neural network, the decision system first sends an input to the I-neurons that encode the\nfeatures of the initial situation. Their firing brings a number of O-neurons to fire, sending an input to A- neurons' first compartment. Then, given an envisaged motor activity m, the decision system sends an in- put to the I-neurons that encode m's features. These fire, and send an input to A-neurons' second compart-\nment. A-neurons' spiking threshold is then gradually\nlowered until a target number of them fire, sending a backward input to O-neurons through their third com- partment's connections. The O-neurons that fire in re- sponse to that input in turn send a backward input to I-neurons, the firing of which is the network's re- sponse to the query. The value of A-neurons' spiking\nthreshold at the moment a given I-neuron spikes de- termines the agent's confidence in the feature's pre- diction: the higher its value, the higher the confi- dence. So, formally, the querying process returns a\nset $P_m$ = {($f_1, c_1$), ...($f_n, c_n$)}, where $f_i$ is a fea- ture and $c_i$ the degree of confidence the agent has in its prediction.\nA-neurons' Learning\nAt each step, O-neurons responding to the depart lo- cation and I-neurons responding to the performed mo- tor activity send inputs to, respectively, A-neurons'\nfirst and second compartments. A-neurons reaching\na certain threshold are selected for learning. If their number reaches some fixed target number, the net-\nwork directly proceeds to make them learn, otherwise\nit looks for additional A-neurons by boosting their in- put in a way similar to the one used for O-neurons.\nFor each learning A-neuron a we compute a learn- ing rate $LR_a$, which is an increasing function of the\nnumber of steps performed since the neuron's last learning. The idea is that seldom used neurons tend to\nencode more particular concepts than often used neu- rons, and should thus be able to learn more rapidly to\nretain more features from a given situation.\nThe learning process depends on the accuracy of the agent's predictions relative to the action's out- come (see Section 6). If these are correct (i.e., all\nthe expected features are actually present) and com- plete (i.e., all the actually present features were ex- pected), then the learning process replaces all in- active synapses in a's first two compartments with\nsynapses from their respective input neurons, but only min($LR_a$, i) in the third one, where i is the number of\ninactive synapses. If they are not correct, then all inac- tive synapses plus some active ones are replaced in a's\nfirst two compartments, with max($LR_a$, i) the num- ber of synapses replaced in each one. The learning\nprocess for the third compartment is the same as in the\nfirst case. If the agent's predictions are not complete,\nthen the number of replaced synapses is max($LR_a$, i)\nin all three compartments. This differentiated learn- ing process aims at promoting generalization when\npredictions are correct and complete while allowing\na few neurons to specialize when they are not."}, {"title": "6. Functioning of the Agent", "content": "Suppose the agent is at some depart location and ob- serves it: information from its perceptual system trig- gers the firing of the I-neurons encoding the location's\nfeatures and is transmitted from there to the decision\nsystem, which decides to make a step.\nThe agent's choice of a motor activity depends\non whether it wants to exploit its current knowledge\nabout the environment, or to explore it to improve its knowledge. The exploration/exploitation dilemma\nis a well-known problem in online learning Watkins (1989); Sutton and Barto (2018), and changing envi-\nronments make it even more difficult. We therefore do\nnot try to reach an optimal solution here, but instead we simply make the agent's decision system choose\nat random, with equal probability, between an Explo- ration and an Exploitation mode.\nThis choice being made, for each motor activity m\nout of the eight possible the decision system queries the semantic memory for the outcome the action hav-\ning the current location for initial situation and m for motor activity. It then rates each of them for its suit- ability, by building the sets S (for \"Suitable\"), US\n(\"Unsuitable\") and UD (\u201cUndecided\u201d):\n\u2022 S = {(m, c) | (OK, c) \u2208 $P_m$}\n\u2022 US = {(m, c) | (KO, c) \u2208 $P_m$ or (Failure, c) \u2208\n$P_m$}\n\u2022 UD = {m | #c s.t. (m, c) \u2208 S \u222a US}\nThe decision system then chooses a motor activ- ity depending on the selected mode. In Exploration\nmode, the agent is willing to take risks and chooses\nan action with the most uncertain outcome possible: if UD \u2260 0, it picks one from UD, otherwise it goes\nfor one with the least c in S \u222a US. In Exploitation\nmode, by contrast, the agent just wants to land on an\nOK box and to avoid KO boxes and failure as much as\npossible. So, if S \u2260 0, it chooses one with the great- est c. Otherwise, if UD \u2260 0, it picks one from UD.\nIf both S and UD are empty, it chooses one with the least c in US.\nThe decision system then transmits its decision to\nthe motor system to perform the selected motor ac- tivity. The I-neurons that encode its features are ac- tivated by proprioception and send an input to A- neurons' second compartment.\nWe simulate the agent's move by computing its\narrival location and its features. If the agent bumps into a wall, the Failure neuron fires and sends an in- put to A-neurons' third compartment, and the agent\ndirectly learns the action (see A-neurons' learning above). Otherwise, it first learns relevant object con- cepts about its new location (see O-neurons' learning\nabove), and then the action. After that, the agent is\nready for the next step."}, {"title": "7. Results", "content": "To test the agent's learning abilities, we placed it at lo- cation (0,0) and prompted it to perform a succession\nof series of steps, each complete sequence of series of\nsteps being called a trial and consisting in 65536 steps\nin total. The results we present here are averaged over 50 trials.\nA first group of tests was carried out with the door kept closed all along, so the agent had no access to the second room. First, we tested its ability to learn an action over one single experience (one-shot learning).\nTo do so, after each step we asked it to redo the predic- tion that led to the just realized action, and compared this new prediction with the action's actual outcome\n(seeTable 1). We call a prediction Correct and Com- plete (CC) if the predicted features are exactly those\nof the arrival location. The table's first line shows the mean percentage of steps leading to a CC post- learning prediction, for each series of steps. The sec-\nond line (MF for \"Missed Features\") shows the aver- age percentage of features occurrences that the agent\nfailed to predict after learning. The third line (PE for \"Predictions Errors\") shows the average percentage\nof wrongly predicted features occurrences. These re- sults show a good performance at immediate recall\nafter learning.\nTo test whether the acquired knowledge was re- tained in the long run, after each series of steps we froze the simulation, deactivated learning and placed\nthe agent successively in each location of each room.\nThere, we asked it for its predictions for each of the eight possible motor activities and compared its\nanswers with the actions' actual outcomes. Tables\n2 and 3 show each feature's mean Hit Rate (that is, its chances of being predicted when effectively\npresent), and Correctness (its chances of being effec- tively present when predicted)$^1$ for each room. For lack of space we only show the results for some se- ries. Values for the first room (white lines) show that\nlearned actions are indeed recalled long after having been performed. Values for the second room (grey\nlines) show that despite never having been in this room (since we kept the door closed) the agent is\nable to correctly predict OK and KO features and to a lesser extent Failure\u2014and this, even though locations\n$^1$Hit Rate is also known as True Positive Rate, Recall or Sen- sitivity, while Correctness is also known as Precision or Positive Predictive Value see for example Kohavi and Provost (1998) for definitions. Here we multiplied the obtained figures by 100 to get percentages.\nfrom the second room have different sets of features, including for some of them a new feature, Sound. The\npoor performance at wall prediction is due to the lack\nof general rules of the universe regarding the pres- ence of walls in adjacent boxes: the agent uses partic- ular concepts to predict them in the first room hence\nit is helpless in the second room. The mixed result for failure prediction comes from a competition between\ngeneral action concepts, the control of which needs to\nbe improved.\nWe also tested the agent's ability to use its knowl- edge to make appropriate decisions. Each time it\nchose, during the simulation, to exploit its knowledge, we recorded the chosen action's outcome. Finally, we tested whether the agent would be able to use the knowledge acquired in the first room to act\njudiciously in the second room. To do so, at the end\nof each series of steps we asked it to chose a move"}, {"title": "8. Conclusion and Future Developments", "content": "In this paper we have designed and implemented a\nfully autonomous agent that learns action laws online and accommodates environment changes. This agent\nrelies on general concepts to handle new situations\nand dynamically adjusts its concepts to its current en- vironment. This makes it well suited for open worlds: if a new door were to open to a third room with new\nobjects and laws, it would learn them just as it did for the second room. Of course, this would come at the\ncost of the forgetting of its least used concepts, but\nthese are precisely the ones it needs the less. In fact, the agent's ability to selectively forget ensures that it will always be able to learn about new environments,\nby replacing old unused concepts by new useful ones.\nFurther work remains to be done to endow the agent\nwith planning abilities. Notably, a notion of applica- ble action law would be needed. Intuitively, it seems\nthat an action law represented by an action concept\n[x, y, z) should be deemed applicable in a situation\ns if s satisfies all the features in x and z \u2260 Fail- ure. Furthermore, to comply with open world require- ments the agent would need to be able to build its own\nset of possible situations (states) online. The set of its object concepts could probably be used to this end. A\ncost function should also be added, and the decision\nsystem should be augmented so as to handle goals.\nAdditionally, a number of other improvements would be desirable in order to allow the agent to live\nin more realistic environments. A first one would be to implement negation in the network, so that the agent\nwould be able to represent the fact that a given ob- ject does not have a given feature. We believe that this could be done by the means of neural inhibition, but the appropriate learning rules remain to be found. An- other essential improvement would be to have the net- work use incomplete information as input for learn- ing and querying, and to make the agent able to query its semantic memory for object properties given some\npartial input. It seems to us that this would bring the agent to draw non-monotonic inferences in the spirit\nof Grimaud (2016). It would also be useful to allow\nthe agent to distinguish between objects and their lo- cations, since actions can modify one, the other or\nboth. Biological brains achieve this by using two sep- arate pathways to process the \"what\" and the \"where\"\ncomponents of observations before reunifying them, and this could be an inspiration source. Lastly, a com- pletely different line of research would be to inves- tigate how the agent should decide between Explo- ration and Exploitation modes in an open world."}]}