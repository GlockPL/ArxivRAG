{"title": "Adversarial Robust Decision Transformer: Enhancing Robustness of RVS Via Minimax Returns-to-Go", "authors": ["Xiaohang Tang", "Afonso Marques", "Parameswaran Kamalaruban", "Ilija Bogunovic"], "abstract": "Decision Transformer (DT), as one of the representative Reinforcement Learning via Supervised Learning (RvS) methods, has achieved strong performance in offline learning tasks by leveraging the powerful Transformer architecture for sequential decision-making. However, in adversarial environments, these methods can be non-robust, since the return is dependent on the strategies of both the decision-maker and adversary. Training a probabilistic model conditioned on observed return to predict action can fail to generalize, as the trajectories that achieve a return in the dataset might have done so due to a weak and suboptimal behavior adversary. To address this, we propose a worst-case-aware RvS algorithm, the Adversarial Robust Decision Transformer (ARDT), which learns and conditions the policy on in-sample minimax returns-to-go. ARDT aligns the target return with the worst-case return learned through minimax expectile regression, thereby enhancing robustness against powerful test-time adversaries. In experiments conducted on sequential games with full data coverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution with the largest adversarial robustness. In large-scale sequential games and continuous adversarial RL environments with partial data coverage, ARDT demonstrates significantly superior robustness to powerful test-time adversaries and attains higher worst-case returns compared to contemporary DT methods.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning via Supervised Learning (RvS) [Emmons et al., 2021], has garnered attention within the domain of offline Reinforcement Learning (RL). Framing RL as a problem of outcome-conditioned sequence modeling, these methodologies have showcased strong performance in offline benchmark tasks [Wu et al., 2024, Yamagata et al., 2023, Janner et al., 2021, Schmidhuber, 2019, Kumar et al., 2019]. One of the representative RvS methods is Decision Transformer (DT) [Chen et al., 2021], which simply trains a policy conditioned on target return via behavior cloning loss. Given the apparent efficacy and simplicity of RvS, our attention is drawn to exploring its performance in adversarial settings. In this paper, we aim to use RvS to achieve adversarial robustness, a critical capability for RL agents to manage environmental variations and adversarial disturbances [Pinto et al., 2017, Tessler et al., 2019, Kamalaruban et al., 2020, Vinitsky et al., 2020, Curi et al., 2021, Rigter et al., 2022]. Such challenges are prevalent in real-world scenarios, e.g., changes in road conditions and decision-making among multiple autonomous vehicles.\nIn offline RL with adversarial actions, the naive training of action-prediction models conditioned on the history and observed returns such as DT, or expected returns such as ESPER and DoC [Paster et al., 2022, Yang et al., 2022, Yamagata et al., 2023], may result in a non-robust policy. This issue arises from the potential distributional shifts in the policy of the adversary during offline learning. Specifically, if the behavior policies used for data collection are"}, {"title": "2 Problem Setup", "content": "We consider the adversarial reinforcement learning problem involving a protagonist and an adversary", "2017": ".", "1994": "Perolat et al. [2015", "R": "S \\times A \\times \\bar{A"}, "rightarrow \\mathbb{R}$ and the transition kernel $T: S \\times A \\times \\bar{A} \\rightarrow \\Delta(S)$ depend on the state and the joint actions of both players. The initial state distribution is denoted by $p_0$. At each time-step $t \\leq H$, both players observe the state $s_t \\in S$ and take actions $a_t \\in A$ and $\\bar{a}_t \\in \\bar{A}$. The adversary is considered adaptive if its action $\\bar{a}_t$ depends on $s_t$ and $a_t$. The protagonist receives a reward $r_t = R(s_t, a_t,\\bar{a}_t)$, while the adversary receives a negative reward $-r_t$.\nDenote trajectory $\\tau = (s_t, a_t, \\bar{a}_t, r_t)_{t=0}^H$ and its sub-trajectory $\\tau_{i:j} = (s_t, a_t,\\bar{a}_t, r_t)_{t=i}^j$ for $0 \\leq i \\leq j \\leq H$. The return-to-go refers to the sum of observed rewards from state $s_t$ onwards: $R(\\tau_{t:H}) = \\sum_{t'=t}^H r_{t'}$. The protagonist policy $\\pi$ and the adversary policy $\\bar{\\pi}$ can be history-dependent, mapping history $(\\tau_{0:t-1}, s_t)$ to the distribution over protagonist and adversary's actions, respectively.\nIn offline RL, direct interaction with the environment is unavailable. Instead, we rely on an offline dataset $D$, which consists of trajectories generated by executing a pair of behavioral policies, $(\\pi_D, \\bar{\\pi}_D)$. Our objective in this adversarial offline RL setting is to utilize this dataset $D$ to learn a protagonist policy that seeks to maximize the return, while being counteracted by an adversary's policy $\\bar{\\pi}$: \n$\\max_{\\pi} \\min_{\\bar{\\pi}} \\mathbb{E}_{\\tau \\sim \\rho^{\\pi,\\bar{\\pi}}} [\\sum_{t} r_t"]}