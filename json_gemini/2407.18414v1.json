{"title": "ADVERSARIAL ROBUST DECISION TRANSFORMER: ENHANCING\nROBUSTNESS OF RVS VIA MINIMAX RETURNS-TO-GO", "authors": ["Xiaohang Tang", "Afonso Marques", "Parameswaran Kamalaruban", "Ilija Bogunovic"], "abstract": "Decision Transformer (DT), as one of the representative Reinforcement Learning via Supervised\nLearning (RvS) methods, has achieved strong performance in offline learning tasks by leveraging\nthe powerful Transformer architecture for sequential decision-making. However, in adversarial\nenvironments, these methods can be non-robust, since the return is dependent on the strategies of\nboth the decision-maker and adversary. Training a probabilistic model conditioned on observed\nreturn to predict action can fail to generalize, as the trajectories that achieve a return in the dataset\nmight have done so due to a weak and suboptimal behavior adversary. To address this, we propose a\nworst-case-aware RvS algorithm, the Adversarial Robust Decision Transformer (ARDT), which learns\nand conditions the policy on in-sample minimax returns-to-go. ARDT aligns the target return with\nthe worst-case return learned through minimax expectile regression, thereby enhancing robustness\nagainst powerful test-time adversaries. In experiments conducted on sequential games with full data\ncoverage, ARDT can generate a maximin (Nash Equilibrium) strategy, the solution with the largest\nadversarial robustness. In large-scale sequential games and continuous adversarial RL environments\nwith partial data coverage, ARDT demonstrates significantly superior robustness to powerful test-time\nadversaries and attains higher worst-case returns compared to contemporary DT methods.", "sections": [{"title": "1 Introduction", "content": "Reinforcement Learning via Supervised Learning (RvS) [Emmons et al., 2021], has garnered attention within the\ndomain of offline Reinforcement Learning (RL). Framing RL as a problem of outcome-conditioned sequence modeling,\nthese methodologies have showcased strong performance in offline benchmark tasks [Wu et al., 2024, Yamagata\net al., 2023, Janner et al., 2021, Schmidhuber, 2019, Kumar et al., 2019]. One of the representative RvS methods is\nDecision Transformer (DT) [Chen et al., 2021], which simply trains a policy conditioned on target return via behavior\ncloning loss. Given the apparent efficacy and simplicity of RvS, our attention is drawn to exploring its performance\nin adversarial settings. In this paper, we aim to use RvS to achieve adversarial robustness, a critical capability for\nRL agents to manage environmental variations and adversarial disturbances [Pinto et al., 2017, Tessler et al., 2019,\nKamalaruban et al., 2020, Vinitsky et al., 2020, Curi et al., 2021, Rigter et al., 2022]. Such challenges are prevalent in\nreal-world scenarios, e.g., changes in road conditions and decision-making among multiple autonomous vehicles.\nIn offline RL with adversarial actions, the naive training of action-prediction models conditioned on the history and\nobserved returns such as DT, or expected returns such as ESPER and DoC [Paster et al., 2022, Yang et al., 2022,\nYamagata et al., 2023], may result in a non-robust policy. This issue arises from the potential distributional shifts in\nthe policy of the adversary during offline learning. Specifically, if the behavior policies used for data collection are"}, {"title": "2 Problem Setup", "content": "We consider the adversarial reinforcement learning problem involving a protagonist and an adversary, as described\nby Pinto et al. [2017]. This setting is rooted in the two-player Zero-Sum Markov Game framework Littman [1994],\nPerolat et al. [2015]. This game is formally defined by the tuple $(S, A, \\overline{A}, T, R, p_0)$. Here, $S$ represents the state\nspace, where $A$ and $\\overline{A}$ denote the action spaces for the protagonist and adversary, respectively. The reward function\n$R: S \\times A \\times \\overline{A} \\rightarrow \\mathbb{R}$ and the transition kernel $T: S \\times A \\times \\overline{A} \\rightarrow \\Delta(S)$ depend on the state and the joint actions of\nboth players. The initial state distribution is denoted by $p_0$. At each time-step $t \\leq H$, both players observe the state\n$s_t \\in S$ and take actions $a_t \\in A$ and $\\overline{a}_t \\in \\overline{A}$. The adversary is considered adaptive if its action $\\overline{a}_t$ depends on $s_t$ and $a_t$.\nThe protagonist receives a reward $r_t = R(s_t, a_t, \\overline{a}_t)$, while the adversary receives a negative reward $-r_t$.\nDenote trajectory $\\tau = (s_t, a_t, \\overline{a}_t, r_t)_{t=0}^H$ and its sub-trajectory $\\tau_{i:j} = (s_t, a_t, \\overline{a}_t, r_t)_{t=i}^j$ for $0 \\leq i \\leq j \\leq H$. The\nreturn-to-go refers to the sum of observed rewards from state $s_t$ onwards: $R(\\tau_{t:H}) = \\sum_{t'=t}^H r_{t'}$. The protagonist policy\n$\\pi$ and the adversary policy $\\overline{\\pi}$ can be history-dependent, mapping history $(\\tau_{0:t-1}, s_t)$ to the distribution over protagonist\nand adversary's actions, respectively.\nIn offline RL, direct interaction with the environment is unavailable. Instead, we rely on an offline dataset $\\mathcal{D}$, which\nconsists of trajectories generated by executing a pair of behavioral policies, $(\\pi_D, \\overline{\\pi}_D)$. Our objective in this adversarial\noffline RL setting is to utilize this dataset $\\mathcal{D}$ to learn a protagonist policy that seeks to maximize the return, while being\ncounteracted by an adversary's policy $\\overline{\\pi}$:\n$\\max_{\\pi} \\min_{\\overline{\\pi}} \\mathbb{E}_{\\tau \\sim \\rho^{\\pi,\\overline{\\pi}}} [\\sum_{t} r_t ]$,\nwhere $\\rho^{\\pi,\\overline{\\pi}}(\\tau) = p_0(s_0) \\cdot \\prod_{t} \\pi(a_t | \\tau_{0:t-1}, s_t) \\cdot \\overline{\\pi}(\\overline{a}_t | \\tau_{0:t-1}, s_t, a_t) \\cdot T(s_{t+1} | s_t, a_t, \\overline{a}_t)$. The maximin solution to\nthis problem and its corresponding optimal adversary are denoted as $\\pi^*$ and $\\overline{\\pi}^*$, respectively."}, {"title": "2.1 RvS in Adversarial Environments", "content": "We extend the RvS framework by incorporating adversarial settings within the Hindsight Information Matching (HIM)\nparadigm [Furuta et al., 2021]. In adversarial HIM, the protagonist receives a goal (denoted by $z$) and acts within\nan adversarial environment. The protagonist's performance is evaluated based on how well it achieves the presented\ngoal. Mathematically, the protagonist aims to minimize a distance metric $D(\\cdot, \\cdot)$ between the target goal $z$ and the\ninformation function $I(\\cdot)$ applied to a trajectory $\\tau$. This sub-trajectory follows a distribution $\\rho$ dependent on both the\nprotagonist's policy $\\pi_z = \\pi(a_t | \\tau_{0:t-1}, s_t, z)$ and the test-time adversarial policy $\\overline{\\pi}^{\\text{test}}$. Formally, the information\ndistance is minimized as follows:\n$\\min_{\\pi} \\mathbb{E}_{\\tau \\sim \\rho^{z,\\overline{\\pi}^{\\text{test}}}} [D(I(\\tau), z)]$,\nwhere $\\rho^{z,\\overline{\\pi}^{\\text{test}}}$ represents the trajectory distribution obtained by executing rollouts in a Markov Game with the pro-\ntagonist's policy $\\pi_z$ and the test-time adversarial policy $\\overline{\\pi}^{\\text{test}}$. Under HIM framework, Decision Transformer Chen\net al. [2021] uses adopt return-to-go value as the information function, while ESPER Paster et al. [2022] employs the\nexpected return-to-go. The well-trained RvS policy, including DT and ESPER, is derived from behavior trajectories\nfiltered based on the condition $I(\\tau) = z$:\n$\\pi_z = \\pi(a_t | \\tau_{0:t-1}, s_t, z) = p_{\\pi_D,\\overline{\\pi}_D}(a_t | \\tau_{0:t-1}, s_t, I(\\tau) = z)$.\nIf the conditional policy ensures that the information distance in Eq. (2) is minimized to 0, the generated policy\n$\\pi_z$ is robust against $\\overline{\\pi}^{\\text{test}}$ by simply conditioning on a large target return, or even the maximin optimal protagonist"}, {"title": "3 Adversarial Robust Decision Transformer", "content": "To tackle the problem of suboptimal adversarial behavior in the dataset, we propose Adversarial Robust Decision\nTransformer (ARDT), a worst-case-aware RvS algorithm. In ARDT, the information function is defined as the\nexpected minimax return-to-go:\n$I(\\tau) = \\min_{\\overline{a}_{t:H}} \\max_{a_{t+1:H}} \\mathbb{E}_{s_{t+1}\\sim T(\\cdot|s_t, a_t, \\overline{a}_t)}[ \\sum_{t'\\in[t,H]} R(s_{t'}, a_{t'}, \\overline{a}_{t'})]$"}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to examine the robustness of our algorithm, Adversarial Robust Decision\nTransformer (ARDT), in three settings: (i) Short-horizon sequential games, where the offline dataset has full coverage\nand the test-time adversary is optimal (Section 4.1), (ii) A long-horizon sequential game, Connect Four, where the\noffline dataset has only partial coverage and the distributional-shifted test-time adversary (Section 4.2), and (iii) The\nstandard continuous Mujoco tasks in the adversarial setting and a population of test-time adversaries (Section 4.3). We\ncompare our algorithm with baselines including Decision Transformer and ESPER. The implementation deatils are\nin Appendix C. Notably, since the rewards and states tokens encompass sufficient information about the adversary's\nactions, all DT models are implemented not conditioned on the past adversarial tokens to reduce the computational cost."}, {"title": "4.1 Full Data Coverage Setting", "content": "The efficacy of our solution is first evaluated on three short-horizon sequential games with adaptive adversary: (1) a\nsingle-stage game, (2) an adversarial, single-stage bandit environment depicting a Gambling round [Paster et al., 2022],\nand (3) a multi-stage game. These are depicted in Figure 1, Figure 6, and Figure 8, respectively. The collected data\nconsists of $10^5$ trajectories, encompassing all possible trajectories. The online policies for data collection employed by\nthe protagonist and adversary were both uniformly random.\nFigure 3 illustrates the efficacy of ARDT in comparison to vanilla DT and ESPER. Across all three environments,\nARDT achieves the return of maximin (Nash Equilibrium) against the optimal adversary, when conditioned on a large\ntarget return. As illustrated in Figure 1, ARDT is aware of the worst-case return associated with each action it can\ntake due to learning to condition on the relabeled minimax returns-to-go, and successfully takes robust action $a_1$. In\nsingle-stage game (Figure 3), ESPER is vulnerable to the adversarial perturbations, and DT fails to generate robust\npolicy when conditioned on large target return. While, it is worth noting that DT has achieved worst-case return 1\nwhen conditioning on target returns around 1. This imposes the question: Can we learn the robust policy by simply\nconditioning on the largest worst-case return in testing with vanilla DT training?\nWe show via the Gambling environment in Figure 3 that vanilla DT can still fail even conditioning on the largest\nworst-case return. In this environment, the worst-case returns of three protagonist actions are -15, -6 and 1. At the\ntarget return 1, DT is unable to consistently reach a mean return of 1. Similar to DT in the single-stage game, it tends to\nassign equal likelihood to $a_1$ and $a_2$ to reach return 1, leading to a drop of performance to less than 1 under adversarial\nperturbations. Therefore, simply conditioning on the robust return-to-go in testing is insufficient to generate robust policy\nwith vanilla DT. Moreover, as the target return approaches 5, DT's performance drops more since DT tends to choose\n$a_0$ more often to hopefully achieve a return of 5, but instead get negative return against the adversarial perturbations.\nConversely, ARDT's policy is robust since it is trained with worst-case returns-to-go. ESPER also performs well in\nGambling due to that in this game the robust action is also the one with the largest expected return (Figure 8). ARDT\nalso attains the highest return in a multi-stage game when conditioned on large target return, while other two algorithms"}, {"title": "4.2 Discrete Game with Partial Data Coverage", "content": "Connect Four is a discrete sequential two-player game with deterministic dynamics [Paster et al., 2022], which can\nalso be viewed as a decision-making problem when facing an adaptive adversary. There is no intermediate reward;\nrather, there is a terminal reward of either -1, 0, or 1, meaning lose, tie or win for the first-moving player, respectively.\nTherefore we fix the target return in Connect Four as 1. We fix the protagonist to be the first-move player. This game\nhas a maximum horizon of 22 steps. Since we have a solver of this game, we manage to collect the data with $\\epsilon$-greedy\npolicy. We define the optimality of an $\\epsilon$-greedy policy of protagonist or adversary as $(1 - \\epsilon) \\cdot 100\\%$. Each dataset\nincludes a total of $10^6$ number of steps for variable-length trajectories."}, {"title": "4.3 Continuous Adversarial Environments", "content": "In this section, we test the algorithms on a specific instance of Adversarial Reinforcement Learning problems, namely\nNoisy Action Robust MDP (NR-MDP), which was introduced by Tessler et al. [2019]. This includes three continuous"}, {"title": "5 Conclusions and Limitations", "content": "This paper introduces a worst-case-aware training algorithm designed to improve the adversarial robustness of the\nDecision Transformer. By relabeling trajectories with the estimated in-sample minimax returns-to-go through expectile\nregression, our algorithm is demonstrated to be robust against adversaries more powerful than the behavior ones, which\nexisting DT methods cannot achieve. In experiments, our method consistently exhibits superior worst-case performance\nagainst adversarial attacks in both gaming environments and continuous control settings.\nLimitations. In our experiments, our practical algorithm estimates the minimax returns-to-go without accounting for\nstochasticity, as the transitions in all tested environments, as well as many real-world game settings, are deterministic\nwhen conditioned on the actions of both players. Future work should explore both stochastic and adversarial\nenvironments to further evaluate the performance of our proposed solution.\nBroader Impact. Adversarial RvS holds potential for improving the robustness and security of autonomous agents\nin dynamic and potentially hostile environments. By training agents to withstand adversarial actions and unpredictable\nconditions, our work can improve the reliability and safety of technologies such as autonomous vehicles, cybersecurity\ndefense systems, and robotic operations."}]}