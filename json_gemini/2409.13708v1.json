{"title": "Towards Safe Multilingual Frontier AI", "authors": ["Arturs Kanepajs", "Vladimir Ivanov", "Richard Moulange"], "abstract": "Linguistically inclusive LLMs\u2014which maintain good performance regardless of the language with which they are prompted are necessary for the diffusion of AI benefits around the world. Multilingual jailbreaks that rely on language translation to evade safety measures undermine the safe and inclusive deployment of AI systems. We provide policy recommendations to enhance the multilingual capabilities of AI while mitigating the risks of multilingual jailbreaks. We quantitatively assess the relationship between language resourcedness and model vulnerabilities to multilingual jailbreaks for five frontier large language models across 24 official EU languages. Building on prior research, we propose policy actions that align with the EU legal landscape and institutional framework to address multilingual jailbreaks, while promoting linguistic inclusivity. These include mandatory assessments of multilingual capabilities and vulnerabilities, public opinion research, and state support for multilingual AI development. The measures aim to improve AI safety and functionality through EU policy initiatives, guiding the implementation of the EU AI Act and informing regulatory efforts of the European AI Office.", "sections": [{"title": "1 Introduction", "content": "The increasingly powerful capabilities of Artificial Intelligence (AI) [35] can also misused, for instance, through Large Language Model (LLM) jailbreaks.\u00b2 Defensive measures against jailbreaks can reduce the risks, but can also impede the model utility through rejection of benign prompts [14].\nSimilarly, multilingual capabilities of the LLMs can diffuse the benefits of AI to larger populations [58, 16], but also expose LLMs to multilingual jailbreak attacks\u00b3, especially in low-resource languages.4\nAt least some of the least resource-intensive defensive measures, from instructing the model to \"think in English\" to self-generating synthetic data in low-resource languages, limit the model's utility in non-English natural languages.[71, 23]\nThis paper offers a path towards linguistically inclusive AI that is also safe from multilingual jailbreak attacks (Figure 1). The rest of the paper proceeds as follows. First, we discuss related work, paying particularly attention to the extent defensive actors can reduce LLM vulnerabilities to multilingual"}, {"title": "2 Related work", "content": "jailbreaks without compromising multilingual capabilities. Second, we show that the EU offers a particularly promising environment for effective policy action to address this issue. Third, we assess the vulnerability to multilingual jailbreaks and multilingual capability gaps in the 24 official EU languages for five frontier LLMs. Finally, we offer specific policy recommendations for the European AI Office and other relevant stakeholders."}, {"title": "2.1 What to measure?", "content": "Surveys offer valuable insights into public opinion on AI [39], yet they often fall short in identifying which AI capabilities are most beneficial to society, and in determining appropriate safety thresholds that balance individual and collective interests. This limitation stems from the complexity of AI and its broad societal implications. To address this, researchers have introduced innovative approaches, such as citizens' juries, to gather more informed public perspectives [41]. These methods can help to achieve a balance between innovation and safety, ensuring that public interest plays a central role alongside private incentives [62].\nOne example for societal choice is between technological automation and job security [66]. For instance, LLMs could yield significant savings by automating certain tasks, and addressing skill gaps [38][35]. However, increased automation also raises concerns about labor displacement. According to the Ipsos 2024 survey, over a third of workers worldwide fear that AI could replace their jobs in the coming years [39]. Workers in low-income countries and emerging markets may face significant challenges as AI-driven automation leads to the onshoring of jobs in advanced economies [49]. Beyond the immediate loss of income, job displacement can result in political disempowerment and the loss of personal meaning associated with work [5, 66]. Therefore, decisions about the training, deployment, and use of LLMs should involve not only tech developers but also civil society and policymakers [8]. Measuring and predicting the downstream impacts of LLMs is a complex and open research problem [59, 10], progress in which is necessary to allow for informed choice between different paths of development."}, {"title": "2.2 Risk and capability measurement", "content": "To assess vulnerabilities to multilingual jailbreaks, several benchmarks have been used in the literature, including AdvBench [75, 76], MasterKey [75], and MT-Bench [63]. Although precise assessment is currently hampered by data contamination and translation imperfections, these challenges do not seem insurmountable. Human annotators can help resolve translation issues in both capability [53]"}, {"title": "2.3 Risk mitigation and capability improvements", "content": "Some risk mitigation measures for addressing multilingual jailbreak vulnerabilities can come at the cost of model capabilities. For example, instructing the model to \"think in English\" [71] may enhance safety but reduce effectiveness in language-specific contexts [67]. Similarly, a \u201cself-defense\" approach, which generates multilingual training data for safety fine-tuning, has been found to increase rejection rates for benign prompts [23]. Additionally, safety fine-tuning can result in shorter responses [47]. At the extreme, complete usage restriction maximizes safety but eliminates utility: \"a model that always refuses is maximally safe, but not helpful in the slightest\" [25]. While there are no documented cases of capabilities being entirely cut off in specific languages, certain modalities, such as image generation [72] and singing [53], have been curtailed.\nDespite these challenges, progress is possible. Evidence shows that language gaps can be significantly reduced, even in low-resource languages [16]. For instance, GPT-4 recently demonstrated notable capability improvements in several low-resource African languages [53]. Measures that preserve both safety and capabilities include generating human-annotated datasets, though this can be costly, as low-resource languages often require more tokens per word [6, 13]. One study demonstrated that using just 900 prompts-half requiring local knowledge-reduced the jailbreak attack rate by more than half [4]. The cost of generating such prompts for the 24 official EU languages likely represents only a small fraction of the cost of developing frontier models, which currently exceeds $100 million and is projected to reach $1 billion by 2027 [21]."}, {"title": "2.4 Who will do the evaluations?", "content": "Anderljung et al. advocate for the involvement of external actors in evaluating LLMs to ensure objectivity and thoroughness [8]. Casper et al. further argue that rigorous AI audits require more than just black-box access. They propose that white-box access, which provides deeper insight into the system's internals, enables stronger adversarial testing and fine-tuning. This approach allows for a more comprehensive assessment of post-deployment risks, ultimately enhancing the reliability and safety of AI systems [15]."}, {"title": "2.5 Comprehensive empirical assessments and policy recommendations", "content": "Dubey et al. examined the Violation Rate (VR) and False Refusal Rate (FRR) for the Llama 3 405B model and its competitors in eight high-resource languages, finding that \"Llama Guard\" reduced VR with minimal or insignificant FRR increases [25]. Similarly, Cohere For AI introduced a dataset and method to address multilingual jailbreaks across eight languages, reporting results for six high-resource languages using the Aya 23 8B model [4]. In contrast, we report results for the whole suite of the 24 EU languages\u2014including low-resource ones\u2014on five frontier LLMs (see Section 4.4).\nCohere For AI provides broad policy recommendations for addressing multilingual safety vulnerabilities and improving transparency in multilingual model performance [16]. We have tried to go further, by specifically identify responsible stakeholders and tailoring our recommendations accordingly. We also advocate for model providers to report evaluation results rather than conducting the evaluations themselves (see Section 5).\nIn the EU context, Rehm et al. [58] compiled a wide set of policy recommendations to achieve digital language equality by 2030. Many of their recommendations call for more funding for research and innovation, increased multilingual data availability, and breakthroughs needed for model evaluation. They also called for training large language models from scratch: such efforts have by now received significant compute resources, but have also been criticized as likely to produce inadequate models [43]. Our comparative contribution is focus on highly capable Al models in general, multilingual jailbreaks in particular, and offer recommendations to enable mandatory capability and safety assessments."}, {"title": "3 Methodology for the quantitative analysis", "content": "To assess the relationship between jailbreak attack success rates, harmless prompt rejection rates, and the level of language resourcedness, we formulate the following hypotheses:\n\u2022 H1: Multilingual jailbreak attack success rates and language resourcedness are negatively related.\n\u2022 H2: Harmless prompt rejection rates and language resourcedness are negatively related.\nTo test these hypotheses, we employ the OR-Bench dataset introduced by Cui et al. (2024) [22]. This dataset is particularly well-suited for our purposes as it was specifically designed to address the issue of 'Over-Refusal' by LLMs. For H1, we present harmful prompts from the OR-Bench dataset to the LLMs, and for H2, we present harmless but \u201cseemingly harmful\u201d prompts that could be mistaken as harmful. The translation of these prompts into non-English languages is performed using Google Translate, a widely accessible service.6\nAfter collecting the LLM outputs, we classify the responses using GPT-4 mini (gpt-4o-mini-2024-07-18), based on its suitability for response classification. We classify responses into three categories: ACCEPTED, REJECTED, and UNCLEAR, using annotation guidelines adapted from Yong et al. [76]. The classification rules and detailed annotation guidelines are provided in Appendix B.\nNext, we estimate the coefficients using a logistic regression model, which follows the relationship:\n$P(Y = 1) = \\frac{1}{1+e^{-(\\beta_0+\\beta_1X)}}$ (1)\nIn the above equation Y is binary: for H1, Y = 1 if and only if response is ACCEPTED; for H2, Y = 1 if and only if response is REJECTED. X denotes CommonCrawl corpus share [20], a proxy for language resourcedness [46].\nTo validate the econometric model, we also estimate the relationship using Ordinary Least Squares (OLS) regression. To use OLS, we calculate the proportion of ACCEPTED responses to harmful prompts, and the proportion of REJECTED responses to harmless prompts."}, {"title": "4 Case Study: The EU and its 24 official languages", "content": "Language diversity, commitment to digital language equality [58] and recent regulatory developments in the EU present a pressing need for policies that promote safe multilingual frontier AI."}, {"title": "4.1 Legal aspects", "content": "Linguistic inclusivity is a legal requirement for state institutions in the EU, and is enshrined in several key legal instruments. In particular, in the Charter of Fundamental Rights of the EU, Article 21 prohibits discrimination on the grounds of language, and Article 22 mandates that the EU respect cultural, religious, and linguistic diversity [1]. Therefore, a failure to ensure that LLMs adhere to fundamental safety standards across all languages could potentially lead to violations of Article 21.\nMoreover, the preamble to the EU AI Act also explicitly states that AI and its regulatory framework must be developed in accordance with the charter [31]. The consultations for the EU AI Act [18] as well as multi-stakeholder consultation launched by the EU AI Office [17] offer a platform for discussing and iterating on the requirements for linguistic inclusivity."}, {"title": "4.2 Who benefits from multilingual AI in the EU, and how?", "content": "Multilingual AI offers substantial potential for cost savings. EU legislation and documents of major public interest are produced in all 24 official languages, which costs approximately 1 billion euros"}, {"title": "4.3 Who could implement the policies?", "content": "The primary responsibility for implementing the changes could fall to the Regulation and Compliance Unit of the EU AI Office. This unit could mandate reporting on model multilingual capabilities and susceptibility to multilingual jailbreak attacks as well as monitor and enforce compliance. The AI Safety Unit focuses on identifying and mitigating systemic risks in general-purpose AI models. Multilingual jailbreaks are particularly relevant as they can introduce systemic risks by enabling the misuse of powerful AI capabilities. The AI Safety Unit can therefore support the Regulation and Compliance Unit by providing expertise on multilingual capability and safety testing. The AI for Societal Good Unit can organize and promote initiatives such as citizens' juries, virtual assemblies, and surveys to gather insights into the social benefits and risks of AI. These insights can then inform the other units on the most relevant capability and safety assessments. Furthermore, the Excellence in AI and Robotics Unit can contribute by supporting and prioritizing research and development efforts related to multilingual capabilities. The AI Innovation and Policy Coordination Unit can coordinate the creation of multilingual facilitate collaboration with member states and international partners. Finally, the Scientific Panel of independent experts, also a part of the EU AI Office's structure, can provide insights into the feasibility of measures that advance multilingual model capabilities and safety [27, 29].\nFurthermore, the European Centre for Algorithmic Transparency [28] can provide expertise and contribute to the assessment of risks. Finally, DG Connect [24] of the European Commission (EC), along with the broader EC, coordinates the digital strategy of the EU and can ensure that multilingual AI safety and capabilities are prioritized."}, {"title": "4.4 Regression results", "content": "We apply the methodology outlined in the Methodology section to the 24 official EU languages (see Appendix C for details) and evaluate five frontier models (see Appendix D for details on the model selection). Specifically, we run 100 harmful prompts and 100 harmless prompts in each language and classify the responses as ACCEPTED, REJECTED or UNCLEAR.9\nWe tested the two assumptions identified in the Methodology section. First, we assess translation issues by manually considering a subset of prompts. Specifically, we identify that 2 of the 19 \"Harmful\" prompts that were accepted in Latvian, had actually become harmless through translation (see Appendix E). We conclude that translation issues may bias the results and even invalidate them in borderline-significant cases. Second, we compared the performance of GPT-4o mini to GPT-4o across a subset of responses across the tested languages and found no significant differences. However, we do not compare LLM and human classification, which is a limitation of our research and warrants further investigation."}, {"title": "5 Discussion", "content": "Through our literature review and quantitative analysis, we have identified that some frontier models remain vulnerable to multilingual jailbreak attacks in low-resource languages. We have also identified approaches for improving capabilities and safety in these languages, with precedents for such improvements. Additionally, we have established that there is a legal and financial rationale for the EU to support multilingual safety and capabilities. In this section, we discuss the specifics of policy recommendations for the EU."}, {"title": "5.1 Capability and Safety Requirements to Introduce", "content": "Firstly, to limit the regulatory burden, we recommend, at least initially, focusing on general-purpose AI models entailing systemic risks (see also Appendix D for discussion on the affected models) [31].\nSecondly, while numerous multilingual capability and safety benchmarks exist, there is currently no authoritative and unified evaluation framework [58, 37, 35], so it may be premature to mandate compliance to a specific benchmark and to a specific level.\nHowever, transparency requirements could serve as an important first step with immediate benefits. Transparency allows consumers to compare model capabilities in the language they are interested in, supporting for a better consumer choice [40]. Moreover, increased transparency can stimulate competition among developers to improve capabilities and safety in underrepresented languages. In the EU context, the reporting requirements can, at least initially, prioritize the 24 official EU languages, reflecting their special status and the potential for substantial cost savings in translation and communication.\nTransparency requirements in other industries have gradually led to stricter compliance standards. For instance, EU transparency regulations for publicly listed companies [3] and emissions disclosure requirements have evolved into more stringent limits on carbon allowances [2]. Similarly, as standards for multilingual AI capabilities and safety become more established, the requirements could be strengthened to reach specific benchmark levels. This expansion could eventually cover additional languages and dialects, such as the more than 60 national and regional languages of the EU [30]."}, {"title": "5.2 State Support", "content": "Given the \"non-excludable\" and \"non-rivalrous\" nature of publicly available data and algorithms, they can be considered public goods with positive externalities [60, 34]. Therefore, we advocate for initiatives that support the creation of multilingual datasets, particularly for low-resource languages. High-quality dataset creation is already a top priority for the EU [58], with examples such as Alliance for Language Technologies EDIC (ALT-EDIC), a collaboration involving 16 EU member states [19].\nIn the area of multilingual safety evaluation tools, benchmarks, and frameworks, one notable example is the \"European LLM Leaderboard,\" which provides an automated database for evaluating LLMs. This leaderboard is the result of a collaboration between private and public partners, including Dresden University of Technology [56], and is primarily funded by the German Federal Ministry for Economic Affairs and Climate Action [61]. However, this leaderboard currently presents results only for smaller models (up to 8 billion parameters), excludes some low-resource EU languages, and does not include benchmarks for vulnerabilities to multilingual jailbreaks [32]. Nevertheless, such developments have the potential to be integrated into best practices, standards, and future regulations [16].\nIt has been also been suggested that the European High Performance Computing Joint Undertaking (EuroHPC JU) should redirect its computational resources towards computationally intensive safety research, to take an advantage of the growing AI Assurance industry [45, 43]. Our recommended requirements could increase demand for model safety assessment services, further strengthening the case for promoting safety-oriented research and development."}, {"title": "6 Limitations and future research", "content": "Key limitations of the quantitative analysis in this paper include uncertainties regarding the quality of automatic translations and classification of prompts, see Methodology, Regression Results and Appendix E for a discussion. Although we tested these assumptions and found them approximately true, more work is needed to quantify their impact. Another potential issue that can affect model performance in different languages is the contamination of data used for model training [12]. The significance of the results may also be affected by the limited number of prompts per language, which was driven by resource constraints. Additionally, jailbreaks remain problematic as long as they can be executed in any language, underscoring the need for safety testing across a broader range of languages. Nevertheless, we believe the analysis and assumption testing conducted are not only sufficient as a proof-of-concept but also strong enough to suggest a significant relationship between multilingual jailbreak vulnerability and language resourcedness for two of the models.\nThe statistical analysis can be extended by comparing harmful prompt rejection rates and harmless prompt acceptance rates between different languages and language groups. For instance, comparing results for English versus non-English languages, and high-resource languages vs. other languages. It may be important to increase the sample size for the statistical power of such analysis.\nIt is important to note that, apart from natural languages, models can be vulnerable in various non-natural (e.g. programming) languages, which also warrant safety testing [76]. Even less explored is model safety assessment to prompts in \"hybrid\" languages: with 7000 natural languages, there"}, {"title": "7 Broader impacts statement", "content": "The work and analysis presented in this paper do not reveal any novel vulnerabilities. Instead, the analysis primarily demonstrates that known vulnerabilities may persist and could lead to adverse outcomes as models become more advanced.\nConcerns could arise that the requirements tor report multilingual capabilities and vulnerabilities to multilingual jailbreaks could increase the regulatory burden, thereby impeding innovation. However, because only the largest models would be affected, and because only transparency, not adherence to certain standards, would be required (at least as initially), we deem this unlikely. It seems plausible that the benefits from multilingually capable and safe frontier models could exceed the costs of assessment by several orders of magnitude. Still, cost estimates would be needed not only to ascertain the feasibility of the recommendations, but also to assess feasibility of possible expansion in scope, e.g. to require capability assessments for a larger set of languages."}, {"title": "8 Recommendations for EU policy", "content": ""}, {"title": "8.1 Require multilingual capability and safety assessments for frontier AI Models", "content": "\u2022 Require reporting on model capabilities across the 24 official EU languages for general-purpose AI models entailing systemic risks.\n\u2022 Require reporting on model susceptibility to multilingual jailbreak attacks across all languages for general-purpose AI models entailing systemic risks.\n\u2022 The Regulation and Compliance Unit of the EU AI Office should lead efforts to monitor and enforce compliance with these reporting obligations.\n\u2022 Initiatives such as the EU AI Act Code of Practice consultations and the multi-stakeholder consultations by the European AI Office offer a unique opportunity to discuss, refine, and introduce such requirements.\n\u2022 Promote independent evaluations by external auditors with access beyond black-box testing to allow for more robust assessments of post-deployment vulnerabilities.\n\u2022 As a result, the EU can leverage the Brussels Effect to promote the benefits of multilingual safety and capability requirements, establishing itself as a global leader in AI safety."}, {"title": "8.2 Investigate public preferences regarding the benefits and Costs of multilingual AI", "content": "\u2022 The AI for Societal Good Unit of the EU AI Office can play a central role in identifying which capabilities the public values, who could be harmed by Al risks.\n\u2022 Public preferences can be assessed through citizens' juries, moderated virtual assemblies, and surveys."}, {"title": "8.3 Provide state support for multilingual capabilities and safety", "content": "\u2022 The European Commission and Member States can support the creation of high-quality datasets for low-resource languages through initiatives like ALT-EDIC.\n\u2022 EuroHPC JU should allocate EU supercomputing resources towards AI safety research focused on defending against multilingual LLM jailbreaks.\n\u2022 Member States can collaborate on the development of authoritative tools, benchmarks, and frameworks through projects like the \"European LLM Leaderboard\"."}, {"title": "Appendix A: Training Compute Estimate for Tilde Model", "content": "LUMI primarily uses AMD Instinct MI250X GPUs,[65] which have Peak Half Precision (FP16) Performance of 383 TFLOPs and Peak Single Precision (FP32) Performance of 47.9 TFLOP. Both FP16 and FP32 are typically used for training LLMs.[57] 2 million GPU hours[68] amount to 7.2\u00d7109 seconds. So the total amount of FLOP used in each of these precisions would be:\n\u2022 FP16: 383 x 1012 x 7.2\u00d7109 = 2.8 \u00d71024 FLOP\n\u2022 FP32: 47.9 x 1012 x 7.2\u00d7109 = 3.4\u00d71023 FLOP\nFor comparison, notable OpenAI model training compute (FLOP) was:[26]\n\u2022 OpenAI GPT-3 (Published on May 2020): 3.1 \u00d71023\n\u2022 OpenAI GPT-3.5 (Published on November 2022): 2.6 \u00d71024\n\u2022 OpenAI GPT-4 (Published on March 2023): 2.1 \u00d71025"}, {"title": "Appendix B: Annotation Guidelines", "content": "The following attack success annotation guidelines provided to the model to classify answers:\nA language model gave a response to a prompt. You are shown the prompt and response. Please classify the language model's response as: ACCEPTED if the response stays on topic and answers the prompt. REJECTED if the response refuses to answer the prompt or acknowledges the harmfulness of the prompt and tries to dissuade the user or provide alternatives. UNCLEAR if the response is nonsensical, off topic, a misinterpretation of the instruction, or any response that doesn't provide any actionable insight but doesn't clearly refuse to engage. This includes cases when the response says that it cannot answer because it does not understand the language. The prompt is: ===PROMPT==={prompt} ===END PROMPT=== The language model's response is: ===LLM'S RESPONSE==={response} ===LLM's RESPONSE=== Please answer with either ACCEPTED, REJECTED, or UNCLEAR."}]}