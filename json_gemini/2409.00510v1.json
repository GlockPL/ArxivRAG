{"title": "Streamlining Forest Wildfire Surveillance: AI-Enhanced UAVs Utilizing the FLAME Aerial Video Dataset for Lightweight and Efficient Monitoring", "authors": ["Lemeng Zhao", "Junjie Hu", "Jianchao Bi", "Yanbing Bai", "Erick Mas", "Shunichi Koshimura"], "abstract": "In recent years, unmanned aerial vehicles (UAVs) have played an increasingly crucial role in supporting disaster emergency response efforts by analyzing aerial images. While current deep-learning models focus on improving accuracy, they often overlook the limited computing resources of UAVs. This study recognizes the imperative for real-time data processing in disaster response scenarios and introduces a lightweight and efficient approach for aerial video understanding. Our methodology identifies redundant portions within the video through policy networks and eliminates this excess information using frame compression techniques. Additionally, we intro- duced the concept of a 'station point,' which leverages future information in the sequential policy network, thereby enhancing accuracy. To validate our method, we employed the wildfire FLAME dataset. Compared to the baseline, our approach reduces computation costs by more than 13 times while boosting accuracy by 3%. Moreover, our method can intelligently select salient frames from the video, refining the dataset. This feature enables sophisticated models to be effectively trained on a smaller dataset, significantly reducing the time spent during the training process.", "sections": [{"title": "I. INTRODUCTION", "content": "Forest fires, posing a significant threat to human life and the ecological environment, have garnered increased attention in recent years [1]. In particular, UAV-based plat- forms are increasingly used for post-disaster monitoring and response due to their ability to acquire real-time and efficient ground information [2], [3]. This approach enhances data acquisition speed and reduces the risks associated with manual inspection of forest fires, ensuring the safety of lives."}, {"title": "II. RELATED WORK", "content": "An efficient video understanding model, based on rep- resentative frames, offers an effective solution to the chal- lenge of substantial computational costs [16]. There are two prominent techniques for frame selection: serial video frame sampling and parallel video frame sampling. In sequential sampling, frames are read in temporal order, and a policy is employed to determine the subsequent frame to be read. AdaFrame [17] utilizes Memory-augmented LSTM to retain temporal and spatial information and employs hidden layer states to identify the next frame for reading. VideoIQ [18] reduces computational overhead by allocating different bits or selecting input resolution for each frame. On the other hand, parallel sampling processes frames and video clips independently and consolidates the outcomes. SCSampler [19] estimates the score of each fixed-length clip and synthe- sizes predictions. OCSampler [16] employs a straightforward single-step reinforcement learning optimization to directly aggregate a more comprehensive set of features for video- level modeling. Furthermore, innovative techniques have emerged in this domain. AdaFuse [20] dynamically fuses channels from current and past feature maps, enhancing temporal modeling.\nThe process of frame selection is rooted in the realm of efficient video understanding. For instance, AdaFrame [17], Frame Glimpse [21], and SCSampler [19] utilize frame sampling techniques coupled with specific policy networks. These approaches discern the most valuable frames and video clips from a sequence, subsequently employing these clips in cut videos for various video recognition tasks. SMART [22] employs a single-frame selector to compute the frame's spatial classification value. It then utilizes a global selector to derive temporal and spatial category values from frame pairs. Ultimately, SMART combines the evaluations of each frame's two values within the video, aiding in frame selection. On the other hand, MGSampler [23] leverages two types of motion representation (motion-sensitive and motion-uniform) to distinguish motion-salient frames from the back- ground efficiently. Using a motion-uniform sampling strategy based on cumulative motion distribution, the MGSampler ensures that the selected frames evenly cover all essential segments with high motion salience."}, {"title": "III. PROPOSED METHOD", "content": "AccSampler aims to achieve efficient video understanding through clip compression, which allows our model to iden- tify insignificant clips and consolidate them into a single frame. Moreover, leveraging the features obtained from clip compression, AccSampler can assess the significance of each frame and select the most informative frames to achieve dataset distillation."}, {"title": "A. Network Architecture", "content": "1) Overview: Fig. 2 illustrates the structure of AccSam- pler. Considering videos v comprising a sequence of frames X, denoted as $X={x_t}_{t=1}^N$, where N represents the total number of frames in the video, and t denotes the current time step. At t = 1, AccSampler uniformly selects multiple station points from the original input and feeds them into a 2D-CNN to extract station point features, denoted as {sm}. These station points provide future information for the policy network \u03c0; at t = i, \u03c0 receives the concatenation of clip-level features from the feature extraction network fs and the corresponding station point features. Subsequently, \u03c0evaluates the significance of the following frames and determines how many frames will be compressed at t = i + 1. After processing the entire video, the classifier fc classifies the video. Frame selection is static and relies on the well-trained \u03c0. This policy network effectively assesses the importance of subsequent frames. Consequently, based on the output of \u3160, AccSampler calculates a score for each frame.\n2) Clip Mixup: Mixup [24] is a data augmentation tech- nique that involves scaling two samples and their correspond- ing labels from the same batch to create a new sample. The mixup formula is as follows:\n$x' = \\lambda x_i + (1 - \\lambda) x_j$ (1)\n$y' = \\lambda y_i + (1 - \\lambda) y_j$ (2)\nHere, xi and xj are raw data inputs, and yi and yj represent their labels. A is the scale parameter, and \u03bb follows a Beta(a\u03b1, a) distribution.\nWe employ clip mixup to consolidate several frames into one to optimize computational efficiency while preserving crucial video information. Since video classification does not demand frame-level labels, clip mixup employs only Eq. (1). Specifically, clip mixup merges the initial and final frames of input clips longer than one frame into a single frame xmix\u0438\u0440 representing original clips since frames within a short clip tend to be quite similar."}, {"title": "3) Feature Extraction Network", "content": "The feature extraction network fs combines a lightweight CNN and an RNN. The output of fs is the hidden state of the RNN. Therefore, the video features up to time t = i can be expressed as:\n$h_i = f_s(x_{mixup})$ (3)"}, {"title": "4) Policy Network", "content": "The policy network \u03c0determines the number of frames to be fused at the next time step by selecting action ki from the discrete action space A. Upon receiving the hidden state from fs at t = i, \u03c0 chooses the closest future station point feature from {sm}. Subsequently, the hidden state and the station point feature are concatenated and input into \u03c0, producing the predicted distribution a over the action space A. This process can be represented as:\n$a = softmax(([h_i : s_{nearest}]))$ (4)\nHere, we define A as {1,3,5,7}. If \u03c0 makes the decision ki at t i, where ki \u2208 A means that at t = i+1, the ki frames will be combined into one frame using clip mixup. The smaller the ki, the more crucial the clip is. Additionally, frames of varying importance will be resized to different resolutions. Consequently, we define R={224, 168, 112, 84}, corresponding to the resolution for each action in the action space A, which means will determine whether detailed features are needed. If \u3160 assesses that the current features are sufficient, it may fuse five or seven frames with a lower resolution, while would compress three or one with a higher resolution when detailed features are needed. We designed the structure of \u03c0as a GroupNorm followed by a single fully connected layer, where GroupNorm is conducive to expediting convergence."}, {"title": "5) Frame Selection", "content": "\u03c0 is qualified to assess the impor- tance of frames, as reflected in the decisions it makes. The more frames the policy network decides to fuse, the less crucial they are. Consequently, we introduce the preference score S to assess the significance of each frame. According to Eq. (4), when t = i, the probability distribution of the policy network's prediction for the input in action space A is pi = {a}, 1 \u2264 j \u2264 ||A||. Given the clip to be fused at t = i + 1 is clipi+1, we define the fraction Si+1 of clipi+1 as:\n$S_{i+1} = \\sum_{j=1}^{||A||} a_j/||A||$ (5)\nwhere c\u2208 {p, gm, gs}. To refine the preference score S down to the frame level, the score of the middle frame is Si, with a gradual 10% decay of the score towards the two sides."}, {"title": "6) Gumbel Softmax Trick", "content": "Noticeably, the action space A is discrete, making optimizing it with gradient backprop- agation challenging. In such cases, reinforcement learning is typically utilized to train the policy network. However, reinforcement learning often suffers from slow convergence in various applications [25]. To address this issue, we employ the Gumbel Softmax Trick [26], which optimizes the policy network through gradient backpropagation. Gumbel Softmax is an effective re-parameterization technique that replaces previously non-differentiable probability distributions with differentiable ones, introducing randomness during training and encouraging the model to explore more. To illustrate, the policy network will determine the action through Gumbel Max (6) instead of an argmax [26]:\n$a^m = argmax(log(a) + G_i)$ (6)\nHere, $G_i=-log(-log(U_i))$, where $U_i$ ~ Uniform(0, 1). About backpropagation, a differentiable Gumble-Softmax will be used to approximate argmax to calculate gradients:\n$\\hat{a} = softmax((log(a) + G_i)/\\tau)$ (7)\nHere, \u03c4 is a hyperparameter representing the softmax tem- perature. When T > 0, the labels become relatively smooth, while they degrade to one-hot labels when \u03c4= 0. As per prior research [13], [27], we initialize 7 to 5 and anneal it to 0 as the number of epochs increases."}, {"title": "B. Loss Function", "content": "We designed three loss functions to train AccSampler. The first one is a cross-entropy loss, which calculates the classification loss Le between the prediction and the ground truth:\n$L_c = E[-Y log(f_c(h_{final}))]$ (8)\nwhere hfinal represents the final hidden states. Additionally, to prevent the policy network from converging to trivial solutions where certain actions are ignored, we introduce the balance loss L\u044c:\n$L_b = \\sum_{k \\in A} E[\\frac{(\\delta(k_t = k))}{T} - \\frac{1}{||A||}]$ (9)\nwhere kt\u2208{1,3,5,7} and T represents the total time steps. Lastly, to minimize computation and reach a balance between accuracy and computation, we employ the GFLOPs loss Lg defined as follows:\n$L_g = \\frac{\\gamma}{T}\\sum_{t=1}^{T} GFLOPs(f_s(x_{mixup}))$ (10)\nHere, we calculate the GFLOPs from the inference of fs, so the Lg of the input V represents the average GFLOPs. In summary, the total loss L can be defined as:\n$L = L_c + \\beta L_b + \\gamma L_g$ (11)\nwhere \u03b3 and \u03b2 are hyperparameters that balance the losses. In the experiment, \u1e9e is set to 0.3 and y is set to 0.1 referenced from VideoIQ [18]."}, {"title": "IV. EXPERIMENTS", "content": "1) FLAME Dataset: The FLAME dataset [28] is a pub- licly available collection of fire images and videos captured by drones. We utilized the seventh and eighth repositories of this dataset for image classification. These repositories contain 39,375 images for the training/validation set and 8,617 images for the test set, all with a resolution of 254 \u00d7 254. Notably, we utilized this dataset uniquely for the video classification task. In this context, we defined video samples as sequences of 64 consecutive images extracted from the same video. If any 64 frames were labeled as containing fire, the entire video sample was marked as having fire. Consequently, we curated a dataset comprising 615 clips for training and 134 clips for testing. Among these, 483 samples were fire-related, while 266 samples had no fire, maintaining a ratio of approximately 2:1.\n2) HMDB51 Dataset: The HMDB51 dataset is an open- domain video recognition dataset, encompassing many gen- uine videos derived from diverse platforms [29]. Comprising 6,766 video segments, it covers 51 action categories, ensur- ing each category has at least 101 clips. This paper uses HMDB51 (split 1) as a supplementary dataset to verify the effectiveness of AccSampler's inefficient video recognition.\n3) Implementation Details: AccSampler sets the number of station points to 2, and the hyperparameter a for clip mixup is 0.3. To optimize computational efficiency, we utilize MobileNet-v2 to construct fs (the CNN for feature extraction of station points aligns with fs. The GRU's hidden layer dimension is 512, and there is a single layer. Our training approach consists of multiple stages. In the first stage, we employ frame-level labels and fine-tune MobileNet-v2, pre- trained on ImageNet, over 100 epochs. The initial learning rate is set to 0.01 and is reduced by a factor of 10 at epochs 50, 70, and 90. In the second stage, we freeze MobileNet-v2 and train GRU using clip-level labels for 20 epochs, starting with an initial learning rate of 1.45e-5. In the third stage, MobileNet-v2 and GRU remain frozen, and we train the policy network with an initial learning rate of 0.01. We employ am to calculate the preference score for frame selection. We then train ResNet18-TSM on the selected frames to evaluate the frame selection's performance. We train ResNet18-TSM for 10 epochs, initiating with a learning rate of 0.001 and a weight decay 3e-6.\n4) Evaluation Metrics: We consistently employ accuracy as the metric for the video classification task. Additionally, computational cost is measured in giga floating-point opera- tions per second (GFLOPs), which can be used to calculate the complexity of the model."}, {"title": "B. Main Result", "content": "1) Video Classification: We use fs and \u03c0 to perform video classification, comparing them with GRU(i.e., fs only). The results are outlined in Table I.\nNotably, our model substantially enhances test accuracy. At the same time, the computational load (measured in GFLOPs) is approximately one-tenth that of GRU, meaning the policy network learns to pay closer attention to informa- tive frames.\n2) Frame Selection: The distilled dataset is used to train a sophisticated classification model TSM [30] to evaluate the performance of frame selection. The results are given in Table II.\nIt can be seen that AccSampler demonstrates superior effectiveness, maintaining classification accuracy even when using just 8 frames, surpassing the results obtained from analyzing the full video. AccSampler significantly outper- forms the baseline (Uniform and Random) and SMART results when considering an equivalent number of frames. This outcome implies that AccSampler adeptly selects more representative frames, facilitating a more efficient and accu- rate classification of fire videos.\nFurther tests were conducted over a broader range of frames to investigate whether the observed results were influenced by data randomness due to the dataset's limited size, as shown in Fig. 3. The findings reveal that AccSampler consistently outperforms the other three frame selection methods.\nHowever, it's noteworthy that SMART did not exhibit a significant performance advantage over other models. This lack of distinction might be attributed to FLAME's small size and homogeneity, causing complex models like SMART to suffer severe overfitting during training.\n3) Classification Results on the HMDB51 Dataset: To verify the generalization of AccSampler, we further extend the experiment in Table. I on the HMDB51 dataset. The results are shown in the Table. III."}, {"title": "V. DISCUSSION", "content": "1) Preference Score S: Apart from the calculation uti- lizing am in Eq. (5), we also explored the computation of S using as in Eq. (4) and as in Eq. (7). The scores derived from these three methods are denoted as S1, S2, and S3, respectively. We conducted experiments to evaluate the performance of these three approaches, and the results are depicted in Fig. 4. The results indicate that the S1 score exhibits significantly superior performance.\n2) Effect of the Action Space: We investigated the impact of the action space A on the classification performance, and the results are shown in Table IV. Comparative analysis reveals that the model achieves the highest test set accuracy and the lowest computational load when the action space is defined as {1,3,5,7}. Notably, when extending the action space to {1,3,5,7,9}, AccSampler sees an increase in GFLOPs and a drop in the test accuracy, which means broader action space makes the policy network more con- servative instead.\n3) Effect of Hyperparameter in Clip Mixup: Variations in the parameter a in clip mixup impact the scale parameter \u03bb influencing the model's performance. The findings in Table V indicate that the model exhibits relatively low sensitivity to changes in a. Optimal performance across all metrics is observed when a is set to 0.3.\n4) Effect of the Station Points: We experimented with varying the number of station points to assess their impact on classification, and the results are detailed in Table VI."}, {"title": "VI. CONCLUSION", "content": "In this study, we introduced AccSampler, a novel approach to enhancing the efficiency of aerial video understanding in wildfire monitoring tasks. Extensive experiments have demonstrated that AccSampler outperforms several baseline methods in fire detection, offering an excellent distilled dataset for TSM to achieve high precision with low compu- tational costs. More importantly, AccSampler's compression mechanism is model-agnostic and compatible with various backbones, meeting the standards of diverse video-related disaster response tasks."}]}