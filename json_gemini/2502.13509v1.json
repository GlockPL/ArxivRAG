{"title": "Unlocking Multimodal Integration in EHRs: A Prompt Learning Framework for Language and Time Series Fusion", "authors": ["Shuai Niu", "Jing Ma", "Hongzhan Lin", "Liang Bai", "Zhihua Wang", "Wei Bi", "Yida Xu", "Guo Li", "Xian Yang"], "abstract": "Large language models (LLMs) have shown remarkable performance in vision-language tasks, but their application in the medical field remains underexplored, particularly for integrating structured time series data with unstructured clinical notes. In clinical practice, dynamic time series data such as lab test results capture critical temporal patterns, while clinical notes provide rich semantic context. Merging these modalities is challenging due to the inherent differences between continuous signals and discrete text. To bridge this gap, we introduce ProMedTS, a novel self-supervised multimodal framework that employs prompt-guided learning to unify these heterogeneous data types. Our approach leverages lightweight anomaly detection to generate anomaly captions that serve as prompts, guiding the encoding of raw time series data into informative embeddings. These embeddings are aligned with textual representations in a shared latent space, preserving fine-grained temporal nuances alongside semantic insights. Furthermore, our framework incorporates tailored self-supervised objectives to enhance both intra- and inter-modal alignment. We evaluate ProMedTS on disease diagnosis tasks using real-world datasets, and the results demonstrate that our method consistently outperforms state-of-the-art approaches.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in natural language processing (NLP) have revolutionized healthcare by enabling deeper insights into electronic health records (EHRs). EHRs combine structured data, such as time series laboratory (lab) test results, with unstructured data, including clinical notes and medical images. While large language models (LLMs) excel at processing unstructured text (Nori et al., 2023; Singhal et al., 2023) and vision transformers have driven progress in medical image analysis"}, {"title": "2 Related Work", "content": "The increasing diversity of EHR data has led to significant advancements in multimodal learning for healthcare applications. MedCLIP (Wang et al., 2022) employs semantic contrastive learning to align medical images with textual reports, while RAIM (Qiao et al., 2019) and GLORIA (Huang et al., 2021) integrate numerical and image data with text using attention mechanisms. LDAM (Niu et al., 2021a) further extends these approaches by leveraging cross-attention with disease labels to fuse features from lab tests and clinical notes. EHR-KnowGen (Niu et al., 2024) transforms structured lab data into text and incorporates external knowledge for improved modality fusion. Despite these advancements, achieving a unified latent embedding that effectively captures interactions across diverse modalities remains a key challenge in multimodal EHR processing.\nBeyond multimodal learning, recent research has explored generative approaches to healthcare modeling. Conventional methods have primarily relied on discriminative models for disease risk assessment and diagnosis (Choi et al., 2016; Niu et al., 2021b; Qiao et al., 2019). However, generative models are increasingly being adopted, as demonstrated by Clinical CoT (Kwon et al., 2024), applying LLMs for disease diagnosis generation. Reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) and Chain-of-Thought (CoT) prompting (Wei et al., 2022) have further enhanced medical reasoning capabilities in models such as GatorTron (Yang et al., 2022), MedPalm (Singhal et al., 2023), and GPT4-Med (Nori et al., 2023). While these models excel in medical question-answering, they remain limited in real-world direct disease diagnosis and multimodal EHR processing. EHR-KnowGen (Niu et al., 2024) reframes disease diagnosis as a text-to-text generation problem but overlooks the crucial temporal details embedded in time series lab tests, underscoring the need for more effective and dedicated multimodal fusion strategies."}, {"title": "3 Methodology", "content": "In this section, we present the ProMedTS framework for unifying heterogeneous EHR data through prompt-guided learning. We begin by defining the problem and describing the model inputs, then pro-"}, {"title": "3.1 Problem Definition", "content": "We introduce ProMedTS, aiming to reduce discrepancies between language and time series data in EHRs. Specifically, it leverages anomaly captions and generates time series prompt embeddings to unify both modalities in a shared latent space. The inputs to ProMedTS, denoted by {M, X}, include medical notes $M \\in \\mathbb{R}^{B \\times N_m}$ (where B is the batch size and $N_m$ is the number of tokens) and numeric lab test data $X \\in \\mathbb{R}^{B \\times L \\times N_x}$ (where L is the sequence length and $N_x$ is the number of lab test variants). Additionally, a lightweight anomaly detection (Vinutha et al., 2018) is employed to generate textual descriptions of anomalies $C \\in \\mathbb{R}^{B \\times N_c}$ (details in Appendix A.2). ProMedTS also uses learnable time series query embeddings $P \\in \\mathbb{R}^{B \\times N_p \\times D}$, which are transformed into time series prompt embeddings $T \\in \\mathbb{R}^{B \\times N_p \\times D}$, where $N_p$ is the query length and D is the hidden dimension."}, {"title": "3.2 Model Overview", "content": "Figure 2 illustrates the overview of ProMedTS, which comprises three main modules. Three modules share the same Clinical-BERT(Alsentzer et al., 2019) structured model and are extended to support cross-attention, self-attention, and prompt generation. The Time Series Prompt Embedding (TSPE) module applies a cross-attention mechanism to convert raw lab test data into prompt embeddings, preserving key temporal features. The Multimodal Textual Information Fusion (MTIF) module encodes and merges medical notes with anomaly captions in a unified latent space, facilitating the extraction of complementary semantic information. Finally, the Self-supervised Learning (SSL) module employs tailored loss functions to bridge the modality gap and maintain fine-grained temporal details in the learned representations. These modules work in tandem to achieve robust alignment and fusion of heterogeneous EHRs, and the following sections provide in-depth explanations of each component and their applications."}, {"title": "3.3 Time Series Prompt Embedding", "content": "The objective of the TSPE module is to extract and encapsulate the inherent information from time"}, {"title": "3.4 Multimodal Textual Information Fusion", "content": "The MTIF module is designed to fuse medical notes and anomaly descriptions effectively. We use the anomaly captioning method to generate anomaly descriptions, as illustrated in Figure 2. The inputs to the MTIF module are medical notes M and lab test anomaly descriptions C, which are encoded separately by Clinical-BERT via the multi-head self-attention mechanism:\n$E_m = \\text{Clinical-BERT}(M,M,M)$,\n$E_c = \\text{Clinical-BERT}(C, C, C)$,\nwhere $E_m \\in \\mathbb{R}^{B \\times N_m \\times D}$ and $E_c \\in \\mathbb{R}^{B \\times N_c \\times D}$. The repeated inputs indicate that the key, query, and value matrices are identical for the self-attention mechanism. This structure enables the model to encode each type of textual information independently while capturing the inherent characteristics and context of each input. The combined textual representation is then derived from these encoded inputs:\n$E_f = \\text{AVG}([E_m \\oplus E_c])$,\nwhere $E_f \\in \\mathbb{R}^{B \\times D}$, with $\\oplus$ indicating concatenation, and AVG representing average pooling."}, {"title": "3.5 Self-Supervised Learning", "content": "This module addresses the modality gap between textual and time series EHR data using three specialized loss functions. By simultaneously aligning"}, {"title": "3.5.1 Cross-Modal Contrastive Alignment", "content": "To promote cross-modal alignment, we design a contrastive loss that brings language and time series embeddings closer when they originate from the same patient and pushes them apart otherwise. We first compute similarity matrices by multiplying the fused text representation $E_f$ with the time series prompt embeddings T:\n$G(E_f, X) = max([E_f T^{(1)T}, ..., E_f T^{(N_p)T}])$,\n$G(X, E_f) = max([T^{(1)} E_f, ..., T^{(N_p)} E_f])$,\nwhere the max operator performs max-pooling across $N_p$ dimensions, yielding $G(E_f, X)$ and $G(X, E_f) \\in \\mathbb{R}^{B \\times B}$. Note that $g(E_F,X)$ measures text-to-time series similarity (by fixing $E_f$ and iterating over T), while $g(X,E_f)$ captures time-series-to-text similarity (by fixing T and iterating over $E_f$). We then apply the SoftMax function to generate two distinct sets of logits:\n$\\hat{y}^{f2x} = \\text{SoftMax}(g(E_f, X))$,\n$\\hat{y}^{x2f} = \\text{SoftMax}(g(X, E_f))$.\nLet $y^{f2x}$ and $y^{x2f}$ denote the ground truth labels indicating whether the pairs correspond to the same patient in a training batch (1 if matched, 0 otherwise). We use cross-entropy $H(\\cdot)$ to define the contrastive loss:\n$L_{contrast} = E[H(y^{f2x}, \\hat{y}^{f2x}) + H(y^{x2f}, \\hat{y}^{x2f})]$."}, {"title": "3.5.2 Intra-Modal Matching", "content": "To further capture intra-modality consistency, we align lab tests with corresponding anomaly descriptions. This alignment is modeled as a binary classification task, distinguishing matched from unmatched pairs of lab tests and anomaly captions. Following Li et al. (2021), we employ a negative mining strategy to generate labels $y_m$ by selecting the most similar pairs in a training batch as negative samples, where the top 1-ranked pair is labeled as 1 and the others as 0, based on the similarity computed in Equation 4. We employ Clinical-BERT's cross-attention, where the concatenation of C and"}, {"title": "3.5.3 Anomaly Description Reconstruction", "content": "To ensure the time series prompt embeddings encode both coarse anomaly descriptions and fine-grained temporal details, we reconstruct anomaly captions from the learned embeddings. This step helps unify language tokens and time series representations in a shared space. Specifically, we use Clinical-BERT with a language model head $f_{head}$, setting $E_c$ as the query and T as key and value:\n$L_{gen} = E[H (C, f_{head}(\\text{Clinical-BERT}(E_c, T, T)))]$.\nThis objective encourages the model to generate accurate textual descriptions, thereby reinforcing alignment between time series prompts and language tokens."}, {"title": "Overall Loss", "content": "We combine these objectives into a single training loss:\n$L_{total} = \\alpha L_{contrast} + \\beta L_{match} + \\gamma L_{gen}$,\nwhere $\\alpha$, $\\beta$, and $\\gamma$ are hyperparameters balancing the three losses (see Appendix A.6). Our train-ing algorithm aims to minimize $L_{total}$ across all samples (details in Appendix A.1)."}, {"title": "3.6 LLM-based Disease Diagnosis with ProMedTS", "content": "To illustrate the practical effectiveness of ProMedTS in unifying textual and time series data, we employ a pre-trained, frozen LLM model for disease diagnosis. As depicted in Figure 3, ProMedTS first converts numeric lab test results into time series prompt embeddings, which are then aligned via a fully connected layer to match the LLM's input dimensions. These embeddings serve as prefix soft prompts, concatenated with"}, {"title": "4 Experiments", "content": "and"}, {"title": "4.1 Datasets and Preprocessing", "content": "The MIMIC-III dataset (Johnson et al., 2016) is a publicly available EHR dataset containing de-identified patients who were admitted to ICUs between 2001 and 2012. It includes medical discharge summaries, lab test results, chest x-ray images and more. Our analysis focuses on EHR data from approximately 27,000 patients including complete medical discharge summaries and lab test results. The MIMIC-IV dataset (Johnson et al., 2023) comprises EHR data from 2008 to 2019. We utilize approximately 29,000 EHR records from MIMIC-IV, which include complete medical discharge summaries and lab test results. Our study targets 25 disease phenotypes as defined in the MIMIC-III benchmark (Harutyunyan et al., 2019a).\nData Pre-processing. For medical notes, we extract the brief course from medical discharge summaries, removing numbers, noise, and stopwords. Numerical lab test results are converted into time series data using the benchmark tools (Harutyunyan et al., 2019b), with missing values filled using the nearest available numbers. Time series anomaly descriptions are used the method defined in Appendix A.2. Data splitting follows the guidelines in (Harutyunyan et al., 2019b), using a 4:1 ratio for training and testing."}, {"title": "4.2 Baseline Methods", "content": "We benchmark our approach against a range of methods: GRU (Cho et al., 2014), PatchTST (Nie et al., 2022), TimeLLM (Jin et al., 2023), CAML (Mullenbach et al., 2018), DIPOLE (Ma et al., 2017), PROMPTEHR (Wang and Sun, 2022), LLaMA-7B (Touvron et al., 2023), LDAM (Niu et al., 2021a), FROZEN (Tsimpoukelli et al., 2021), and EHR-KnowGen (Niu et al., 2024). Detailed configurations of these baselines are provided in Appendix A.3. For the disease diagnosis task, we adopt two scales of Flan-T5 (Chung et al., 2024) as the inference LLM to validate our model's effectiveness in understanding multimodal EHRs, primarily driven by resource considerations and ease of experimentation. The Flan-T5-Small-based model is denoted as PromMedTS, while the Flan-T5-Large-based model is denoted as ProMedTS*. In principle, any sufficiently LLM could be substituted to potentially achieve even stronger results.\nTo ensure a fair comparison, all baselines also employ Flan-T5 as their backbone. Reported results are averaged over five runs with different random seeds. The statistical significance determined at p < 0.05 by t-test. Implementation details for every model are described in Ap-"}, {"title": "4.3 Disease Diagnosis Performance", "content": "Table 1 shows that ProMedTS achieves the highest overall performance, particularly in F1 scores on MIMIC-IV. In addition, replacing the LLM with a larger model improves F1 scores on both datasets, indicating our model's scalability and robustness across different LLMs. Furthermore, TimeLLM performs strongly with lab test, highlighting the value of time-series inputs for LLMs in disease diagnosis. Text-based methods (e.g., LLaMA) generally outperform time-series approaches, suggesting that medical notes capture richer disease-related information. Multimodal models (LDAM, FROZEN, EHR-KnowGen) exceed single-modality baselines (TimeLLM, CAML, DIPOLE, PROMPTEHR), confirming the benefits of integrating text and time series. Generative approaches (TimeLLM, LLaMA, FROZEN, EHR-KnowGen) also outperform classification-based methods. Although LLaMA performs well, its higher variance and pa-rameter requirements reduce practicality. Notably,"}, {"title": "4.4 Ablation Studies", "content": "This section presents ablation studies to evaluate each module in ProMedTS. ProMedTS w/o LAB excludes lab test, removing modality alignment with anomaly descriptions and medical notes. ProMedTS w/o ANOMALY removes alignment with anomaly descriptions while keeping alignment between lab test and medical notes to assess the impact of self-supervision. Table 2 summarizes the results, showing that ProMedTS w/o LAB suffers a significant drop in F1 scores, highlighting the importance of lab test. ProMedTS w/o ANOMALY also shows reduced performance, highlighting the challenges of aligning modalities from discrete and continuous encoding spaces and the adverse effects of misalignment on multimodal understanding."}, {"title": "4.4.1 Effect of Modality Alignment in ProMedTS", "content": "This section presents ablation studies to evaluate each module in ProMedTS. ProMedTS w/o LAB excludes lab test, removing modality alignment with anomaly descriptions and medical notes. ProMedTS w/o ANOMALY removes alignment with anomaly descriptions while keeping alignment between lab test and medical notes to assess the impact of self-supervision. Table 2 summarizes the results, showing that ProMedTS w/o LAB suffers a significant drop in F1 scores, highlighting the importance of lab test. ProMedTS w/o ANOMALY also shows reduced performance, highlighting the challenges of aligning modalities from discrete and continuous encoding spaces and the adverse effects of misalignment on multimodal understanding."}, {"title": "4.4.2 Impact of Self-Supervised Loss Functions", "content": "Table 3 summarizes an ablation study on the loss functions in ProMedTS. Both ProMedTS w/o CON-TRAST and ProMedTS w/o MATCH show slight declines in F1 scores, emphasizing the importance of $L_{contrast}$ for aligning and unifying time series and textual inputs within a shared latent space. The results also underscore the role of $L_{match}$ in intra-modal alignment, ensuring the distinctiveness of time series data by aligning lab test with time series prompt embeddings. Notably, ProMedTS w/o GEN exhibits a significant drop in F1 scores, highlighting the critical role of $L_{gen}$ in refining prompt embeddings and integrating temporal information from time series data and anomaly descriptions."}, {"title": "4.5 Model Efficiency and Complexity", "content": "Figure 4 illustrates the parameter counts and computation times of baseline models on the two datasets. Our model, ProMedTS, matches the parameter counts and computation times of multimodal baselines such as LDAM and FROZEN, while using 25\u00d7 fewer parameters and requiring one-third less training time than LLaMA, all while achieving superior diagnostic performance, highlighting its efficiency and effectiveness in language-time series multimodal alignment and fusion."}, {"title": "4.6 Sensitivity Analysis of Time Series Prompt Length", "content": "We performed a sensitivity analysis to examine the impact of the time series prompt embedding length"}, {"title": "4.7 Evaluating the Role of Anomaly Descriptions", "content": "To highlight the advantages of using lab test anomaly captions over raw numerical time series values in LLMs, we evaluate Flan-T5-small with both input types. Table 5 presents the evaluation results on the MIMIC-III and MIMIC-IV datasets for disease diagnosis. The results show that Flan-T5 achieves over a 2% improvement in Micro F1 score when using anomaly captions, demonstrating that LLMs interpret anomaly captions more effectively than raw numerical values in time series lab test data. Additionally, the inclusion of time series prompts underscores the effectiveness of our"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we introduce ProMedTS, a lightweight and effective modality fusion framework that leverages self-supervised prompt learning for multimodal EHR integration. By bridging the modality gap between medical notes and lab test results, ProMedTS enables LLMs to process structured and unstructured medical data more effectively. Its three key modules and self-supervised loss functions advance language-time series integration in healthcare, providing a scalable and adaptable approach for real-world clinical applications. Evaluation on two EHR datasets demonstrates that ProMedTS significantly outperforms existing models in disease diagnosis, underscoring its potential to enhance clinical decision-making and improve patient care. In future work, we plan to extend our approach to larger and more diverse datasets, explore additional LLM architectures, and investigate further improvements in modality alignment techniques."}, {"title": "Limitations", "content": "While this study focuses on modality alignments and their application in downstream tasks, enhanc-"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Lab Test Anomaly Caption", "content": "Time series anomaly descriptions are generated using the IQR method (Vinutha et al., 2018) to identify anomalies, capturing their timing and polarity (above or below standard values) and describing them with handcrafted templates. To caption the lab test anomaly into textual format, we design several text templates to describe the lab test anomalies. All templates are illustrated in Table 6."}, {"title": "A.2 Algorithm", "content": "The training procedure to optimize ProMedTS by minimizing the loss defined in Equation (10) is shown in Algorithm 1."}, {"title": "A.3 Baseline Models", "content": "\u2022 GRU: The Gated Recurrent Unit (GRU) (Cho et al., 2014), a variant of recurrent neural networks (RNNs), employs two gates to capture both long-term and short-term temporal features effectively.\n\u2022 PatchTST: PatchTST (Nie et al., 2022) is a transformer-based time series encoder de-"}]}