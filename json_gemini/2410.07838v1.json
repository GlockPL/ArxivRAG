{"title": "MINORITYPROMPT: TEXT TO MINORITY IMAGE GENERATION VIA PROMPT OPTIMIZATION", "authors": ["Soobin Um", "Jong Chul Ye"], "abstract": "We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of text-conditional data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for producing high-quality generations. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that can encourage the emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes the generation of minority features by incorporating a carefully-crafted likelihood objective. Our comprehensive experiments, conducted across various types of T2I models, demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers.", "sections": [{"title": "1 INTRODUCTION", "content": "Text-to-image (T2I) generative models (Xu et al., 2018; Ramesh et al., 2021; Nichol et al., 2021) have recently attracted substantial interest for their capability to convert textual descriptions into visually striking images. At the forefront of the surge are diffusion models (Song & Ermon, 2019; Ho et al., 2020), augmented by guidance techniques (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) such as classifier-free guidance (CFG) (Ho & Salimans, 2022). The guided T2I samplers encourage generations from high-density regions of a data manifold (Dhariwal & Nichol, 2021), producing realistic images that faithfully respect the provided prompts.\nA key challenge is that the inherent high density focus of modern T2I samplers makes it difficult to generate minority samples \u2013 instances that reside in low-density regions of the manifold. This limitation is particularly significant as T2I-generated data is increasingly incorporated in downstream applications (Tian et al., 2024a;b; Afkanpour et al., 2024) where the majority-focused bias within the data may be perpetuated. Furthermore, the unique attributes found in minority instances are crucial for applications like creative AI (Rombach et al., 2022; Han et al., 2022), where generating novel and highly creative outputs is essential.\nIn this work, we present a novel approach dubbed as MinorityPrompt that counteracts the high-density bias of T2I samplers to improve their capability of minority generation. Our framework is built upon the concept of prompt optimization, an intuitive technique that exhibits strong performance in enhancing T2I diffusion models for various tasks (Gal et al., 2022; Chung et al., 2023b; Park et al., 2024). Unlike existing T2I-based online prompt-tuning methods that modify the entire input prompts (e.g., by optimizing their text-embeddings during inference), our approach updates the prompts in a selective fashion to preserve the intended semantics while encouraging generations of unique low-density features. Specifically during inference, we incorporate learnable tokens into the input prompts, e.g., by appending them to the end of the text. We then adjust the embeddings of these tokens across sampling timesteps, targetting the minimization of a likelihood metric designed to capture the uniqueness of noisy intermediate samples. An additional benefit of our token-based approach is that it offers enhanced semantic controllability, enabling users to express specific desired semantics in generated samples by selecting appropriate initialization words for the learnable token embeddings. To further improve the performance of our sampler, we provide new design choices that can be synergistically employed with our approach for T2I minority generation. Comprehensive experiments validate that our method can significantly improve the ability of creating minority instances of modern widely-adopted T2I models (including Stable Diffusion (SD) (Rombach et al., 2022)) with minimal compromise in sample quality and text-image alignment. In addition, we emphasize that our framework can work on distilled backbones like SDXL-Lightning (Lin et al., 2024), which demonstrates its robustness and practical relevance. As an additional application, we explore the potential of our prompt optimization framework to improve the diversity of T2I models, further exhibiting its versatility as a general-purpose solver applicable across various tasks.\nGiven that our prompt optimization is performed in an online manner, does not require expensive fine-tuning of T2I models, and is entirely self-contained, i.e., implementable solely with a pretrained T2I model, we believe our approach open a new avenue for creative AI, emphasizing the practical relevance of our framework."}, {"title": "2 RELATED WORK", "content": "The generation of minority samples has been explored in a range of different scenarios and generative frameworks (Yu et al., 2020; Lin et al., 2022; Sehwag et al., 2022; Qin et al., 2023; Huang & Jafari, 2023; Um & Ye, 2023; 2024). However, significant progress has been recently made with the introduction of diffusion models, due to their ability to faithfully capture data distributions (Sehwag et al., 2022; Um & Ye, 2023; 2024). As an initial effort, Sehwag et al. (2022) incorporate separately-trained classifiers into the sampling process of diffusion models to yield guidance for low-density regions. The approach by Um & Ye (2023) shares similar intuition of integrating an additional classifier into the reverse process for low-density guidance. A limitation is that their methods rely upon external classifiers that are often difficult to obtain, especially for large-scale datasets such as T2I benchmarks (Schuhmann et al., 2022). The challenge was recently addressed by Um & Ye (2024)"}, {"title": "3 METHOD", "content": "Our focus is to generate high-quality minority instances using text-to-image (T2I) diffusion models, which faithfully reflect user-provided prompts while featuring unique visual attributes rarely produced via standard generation techniques\u00b9. To this end, we start with providing a brief overview on T2I diffusion frameworks and the essential background necessary to understand the core of our work. We subsequently present our proposed framework for minority generation based on the idea of prompt optimization."}, {"title": "3.1 BACKGROUND AND PRELIMINARIES", "content": "The task of T2I diffusion models is to generate an output image x \u2208 Rd from a random noise vector zT \u2208 Rk (where typically k < d), given a user-defined text prompt P. Similar to standard (non-T2I) diffusion frameworks, the core of T2I diffusion sampling lies in an iterative denoising process that progressively removes noise from zT until a clean version z0 is obtained. This denoising capability is learned through noise-prediction training (Ho et al., 2020; Song & Ermon, 2019), mathematically written as:\n$\\max_{\\theta} \\mathbb{E}_{z_0, y, \\epsilon \\sim \\mathcal{N}(0,I), t \\sim \\text{Unif}\\{1,...,T\\}} [||\\epsilon - \\epsilon_{\\theta}(z_t, C)||^2]$,\nwhere z0 := E(x0), yielded by passing a training image x0 through a compressive model E (e.g., the encoder of VQ-VAE (Esser et al., 2021; Rombach et al., 2022)). Here, zt represents a noise-perturbed version of z0, given by $z_t := \\sqrt{\\alpha_t}z_0 + \\sqrt{1 - \\alpha_t}\\epsilon$, where {\u03b1t}T defines the noise-schedule. refers to a T2I diffusion model parameterized to predict the noise , and C represents the embedding of the text prompt P. See below for details on how to obtain C from P.\nOnce trained, T2I generation can be done by starting from zT \u223c N(0, I) and implementing an iterative noise removal process guided by the text embedding C. A common approach is to follow the deterministic DDIM sampling (Song et al., 2020a; Chung et al., 2023a):\n$z_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} z_0(z_t, C) + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\sqrt{1}\nwhere $\\epsilon_0(z_t, C) := \\frac{1}{\\sqrt{\\alpha_t}}(z_t - \\sqrt{1 - \\alpha_t} \\epsilon_{\\theta}(z_t, C))$.\nTo further strengthen the impact of text conditioning, classifier-free guidance (CFG) (Ho & Salimans, 2022) is commonly integrated into the sampling process. In particular, one can obtain a high-density-focused noise estimation through extrapolation using an unconditional prediction:\n$\\epsilon_\\theta(z_t, C) := w \\epsilon_\\theta(z_t, C) + (1 - w)\\epsilon_\\theta(z_t)$,\nwhere indicates an unconditional noise prediction, often implemented via null-text conditioning (Ho & Salimans, 2022). CFG refers to the technique that employs in place of  (in Eq. (1)), which has been shown in various scenarios to significantly improve both sample quality and text alignment yet at the expense of diversity (Sadat et al., 2023).\nText processing. A key distinction from non-T2I diffusion models is the incorporation of the text embedding C, a continuous vector yielded by a text encoder T (such as BERT (Devlin, 2018)) based on the user prompt P. To obtain this embedding, each word (or sub-word) in P is first converted into a token \u2013 an index in a pre-defined vocabulary. Each token is then mapped to a unique embedding vector through an index-based lookup. These token-wise embedding vectors, often referred to as token embeddings, are typically learned as part of the text encoder. The token embeddings are then passed through a transformer model, yielding the final text embedding C. For simplicity, we denote this text processing operation as the forward pass of the text encoder T; thus, C = T(P).\nPrompt optimization. In the context of T2I diffusion models, prompt tuning is performed by intervening in the text-processing stage. A common approach is to adjust the text embedding C"}, {"title": "3.2 SEMANTIC-PRESERVING PROMPT OPTIMIZATION", "content": "The key idea of our optimization approach is to incorporate learnable tokens into a given prompt P and update its embedding on-the-fly during inference. Specifically, we append a placeholder string\u00b2 S to the prompt P, which acts as a mark for the learnable tokens. For instance, the augmented prompt could be Ps := \"A portrait of a dog S\". This additional string is treated as a new vocabulary item for the text-encoder T. We assign a token embedding v to S, and denote the text encoder incorporating it as T(\u00b7 ; v).\nWe propose optimizing this embedding v rather than C. The proposed online prompt optimization at sampling step t can then be formalized as follows:\nv := arg max J(zt, Cv),\nwhere Cv := T(Ps; v). Afterward, the optimized text-embedding Cv is obtained by text-processing Ps with the updated token-embedding of S, therefore Cv\u2021 := T(Ps; v).\nNote that our optimization does not affect the embeddings of the tokens w.r.t. the original prompt P. This is inherently more advantageous for preserving semantics compared to existing methods, which alter the entire text-embedding C and thereby effectively impact all token embeddings. We also highlight that unlike existing learnable-token-based approaches that share the same embedding throughout inference (Gal et al., 2022; Han et al., 2023; Zafar et al., 2024), our framework allows the token embedding v to change over timesteps t. This adaptive feature offers potential advantages, since the role of v in maximizing J can vary with the changing nature of z\u0142 across different timesteps. This point is also implied in previous works that employ adaptive text-embeddings over time (Chung et al., 2023b; Park et al., 2024).\nIntuitively, our optimization can be understood as capturing a specific concept relevant to noisy latent zt within the token v, guided by the objective function J. Thanks to its general design that accommodates any arbitrary objective function J, this framework is versatile and can be employed in various contexts beyond minority generation. For instance, it can be used to diversify the outputs of T2I models."}, {"title": "3.3 MINORITY PROMPT: MINORITY-FOCUSED PROMPT TUNING", "content": "We now specialize the generic solver in Eq. (3) for the task of minority generation. The key question is how to formulate an appropriate objective function J for this purpose. To address this, we"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 SETUP", "content": "T21 backbones and dataset. Our experiments were conducted using three distinct versions of Stable Diffusion (SD) (Rombach et al., 2022), encompassing both standard and distilled versions to demonstrate the robustness of our approach. Specifically, we consider: (i) SDv1.5; (ii) SDv2.0; (iii) SDXL-Lightning (SDXL-LT) (Lin et al., 2024). For all pretrained models, we employed the widely-adopted HuggingFace checkpoints trained on LAION (Schuhmann et al., 2022) without any further modifications. As convention, we randomly selected 10K captions from the validation set of MS-COCO (Lin et al., 2014) for the generations with SDv1.5 and SDv2.0 while using 5K captions for the SDXL-Lightning results.\nBaselines. The same four baselines were considered over all SD versions: (i) the standard DDIM (Song et al., 2020a); (ii) a null-prompted DDIM; (iii) CADS (Sadat et al., 2023); (iv) SGMS (Um & Ye, 2024). The null-prompted DDIM serves as a naive baseline that attempts to encourage unique sampling by incorporating a proper null-text prompt, such as \u201ccommmonly-looking\". CADS (Sadat et al., 2023) is the state-of-the-art diversity-focused sampler that may rival our approach in minority generation, while SGMS (Um & Ye, 2024) is the state-of-the-art of minority generation outside the T2I domain. We adhered to standard sampling setups for all methods. Specifically, 50 DDIM steps (i.e., T = 50) with w = 7.5 were used for SDv1.5 and SDv2.0, while w = 1.0 was employed for the 4-step SDXL-Lightning model.\nEvaluations. For evaluating text-alignment and user-preference, we consider three distinct quantities: (i) ClipScore (Hessel et al., 2021); (ii) PickScore (Kirstain et al., 2023); (iii) Image-Reward (Xu et al., 2023). We additionally employ two metrics for quality and diversity: Precision and Re-"}, {"title": "4.2 RESULTS", "content": "Qualitative comparisons. Figure 4 presents a comparison of generated samples of our approach with two baselines. Notice that our MinorityPrompt tends to yield highly more distinct and complex features (e.g., intricate visual elements (Arvinte et al., 2023; Serr\u00e0 et al., 2019)) compared to the baseline samplers, demonstrating its effectiveness even with distilled pretrained models. A significant observation, also reflected in Figure 1, is that MinorityPrompt often counters the inherent demographic biases of T2I models, e.g., by adjusting age or skin color. See the samples in the second and third rows of the figure. A more extensive set of generated samples, including those from SDv1.5 and v2.0, can be found in Section D.2.\nQuantitative evaluations. Table 1 exhibits performance comparisons across three distinct T2I models. Observe that our sampler outperforms all baselines in generating low-likelihood samples, while maintaining reasonable performance in text-to-alignment and user preference; also see Figure 5 where we present the distributions of log-likelihood. However, the performance trends for SDXL-LT results differ slightly from those of SDv1.5 and SDv2.0 across all tailored samplers, with particularly degraded results. We attribute this to the small-step nature of distilled models, which offer fewer opportunities to intervene in the sampling process, thereby limiting the potential for quantitative improvements.\nAblation studies. Table 2 investigates the impact of key design choices in our framework. Specifically, Table 2a highlights the benefits of optimizing small sets of token embeddings, which outperform alternatives targeting text or null-text embeddings in both text alignment and log-likelihood. The advantage of using the proposed objective function Eq. (6) is exhibited in Table 2b, where the naively-extended framework based on Eq. (5) demonstrates significant performance gap compared to our carefully-crafted approach. Table 2c explores various initialization techniques for v. While all methods yield substantial improvements over the unoptimized sampler (see \"unoptimized\" in Table 2b for comparison), we observe that further gains can be achieved with properly chosen initial words. A more comprehensive analysis and ablation study, encompassing additional design choices and applications to trending sampling techniques such as CFG++ (Chung et al., 2024), is presented in Section C.1."}, {"title": "5 CONCLUSION", "content": "We developed a novel framework for generating minority samples in the context of T2I generation. Built upon our prompt optimization framework that updates the embeddings of additional learnable tokens, our minority sampler offers significant performance improvements both in text-alignment and low-likelihood generation compared to existing approaches. To accomplish this, we meticulously tailor the objective function with theoretical justifications and implement several techniques for further enhancements. Beyond our main interest of minority generation, we further demonstrated the potential of our framework in promoting fairness and diversity. During this process, we also showed that the proposed optimization framework can serve as a general solution, with potential applicability to various optimization tasks associated with T2I generation."}, {"title": "ETHICS STATEMENT", "content": "One potential concern associated with our approach is the possibility of its malicious use to inhibit the generation of minority-featured samples. For instance, this could occur by flipping the sign of the objective function Eq. (6), yielding a focus on high-density generations. It is crucial to recognize this risk and to ensure that our proposed framework is employed responsibly to foster fairness and inclusivity in generative modeling."}, {"title": "REPRODUCIBILITY", "content": "To ensure the reproducibility of our experiments, we provide a comprehensive description regarding the employed pretrained models for our experiments. All experimental settings, including hyper-parameter choices, are detailed in Section B.2. Additionally, we include the average running time of our algorithm along with specific details about the computer configuration in the same section. Finally, to assist with replication efforts, we have made our code available in a public repository: https://github.com/anonymous-6898/MinorityPrompt."}, {"title": "A THEORETICAL RESULTS", "content": ""}, {"title": "A.1 PROOF OF PROPOSITION 1", "content": "Proposition 1. The objective function in Eq. (6) is equivalent (upto a constant factor) to the negative ELBO w.r.t. log po(2o (zt, Cv) | C) when integrated over timesteps with ws := as/(1 - as):\n$\\sum_{s=1}^{T} w_sI_c(z_t, C_v) = \\sum_{s=1}^{T} E_{\\epsilon}[|| \\epsilon - \\epsilon_{\\theta} (\\sqrt{\\alpha_s} z_0(z_t, C_v) + \\sqrt{1 - \\alpha_s} \\epsilon, C) ||^2]$\n$= -log p_\\theta (z_0 (z_t, C_v) | C)$.\nProof. Remember the definition of the objective function in Eq. (6):\n$I_c(z_t, C_v) := E_{\\epsilon} [|| z_0(z_t, C_v) - z_0(z_{s|t,0}, C) ||^2]$.\nPlugging this into the LHS of Eq. (9) yields:\n$\\sum_{s=1}^{T} w_sI_c(z_t, C_v) = \\sum_{s=1}^{T} \\frac{\\alpha_s}{1-\\alpha_s} E_{\\epsilon} [|| z_0(z_t, C_v) - z_0(z_{s|t,0}, C) ||^2]$\n$\\sum_{s=1}^{T} \\frac{\\alpha_s}{1-\\alpha_s} E_{\\epsilon} [||\\frac{1}{\\sqrt{\\alpha_s}}(z_t - \\sqrt{1-\\alpha_s} \\epsilon_{\\theta}) - \\frac{1}{\\sqrt{\\alpha_s}}(z_{s|t,0} - \\sqrt{1-\\alpha_s} \\epsilon_{\\theta}(z_{s|t,0}, C))||^2]$\n$=\\sum_{s=1}^{T} \\frac{\\alpha_s}{1-\\alpha_s} E_{\\epsilon} [||\\frac{1}{\\sqrt{\\alpha_s}}(z_t - \\sqrt{1-\\alpha_s} \\epsilon_{\\theta}) - \\frac{1}{\\sqrt{\\alpha_s}}(z_{s|t,0} - \\sqrt{1-\\alpha_s} \\epsilon_{\\theta}(z_{s|t,0}, C))||^2]$\n$=\\sum_{s=1}^{T} \\frac{\\alpha_s}{1-\\alpha_s} E_{\\epsilon} [||\\epsilon - \\epsilon_{\\theta} (\\sqrt{\\alpha_s} z_0(z_t, C_v) + \\sqrt{1 - \\alpha_s} \\epsilon, C) ||^2]$,\nwhere the second equality is from the definitions of $z_{s|t,0}$ and $z_0(z_{s|t,0}, C)$:\n$z_{s|t,0} := \\sqrt{\\alpha_s} z_0(z_t, C_v) + \\sqrt{1 - \\alpha_s} \\epsilon$\n$z_0(z_{s|t,0}, C) := \\frac{1}{\\sqrt{\\alpha_s}}(z_{s|t,0} - \\sqrt{1 - \\alpha_s} \\epsilon_{\\theta}(z_{s|t,0}, C))$.\nNote that the last expression in Eq. (10), which is the same as the RHS of Eq. (9), is equivalent (up to a constant) to the expression of the negative ELBO w.r.t. $z_0(z_t, C_v)$ (Ho et al., 2020; Li et al., 2023). The distinction here is that now we use a text-conditional diffusion model  (\u00b7, C) that approximates log p (C). This completes the proof."}, {"title": "A.2 THEORETICAL ISSUES ON EQ. (5)", "content": "We continue from Section 3.3 to scrutinize the theoretical challenges that arise in the naively-extended optimization framework in Eq. (5). To proceed, we first restate the objective function in Eq. (5):\n$I_u (z_t, C_v) := E_{\\epsilon} [|| z_0^w(z_t, C_v) - sg(z_0^w(z_{s|t,0}, C) )||^2]$.\nRemember that we identified three theoretical issues that impair the connection to the target log-likelihood log p (z0 | C): (i) the reliance on the CFG-based clean predictions; (ii) obstructed gradient flow through the second term in the squared-L2 loss; and (iii) the incorporation of C within the second term in the loss.\nCFG-based clean prediction. We start by examining the first point, the pathology due to the CFG-based clean predictions. Suppose we incorporate the CFG-based clean predictions z in our framework Eq. (6), in place of the non-CFG terms z0. The objective function then becomes:\n$\\hat{I_u} (z_t, C_v) := E_{\\epsilon} [|| z_0^w(z_t, C_v) - z_0^w(z_{s|t,0}, C) )||^2]$."}, {"title": "B SUPPLEMENTARY DETAILS", "content": ""}, {"title": "B.1 DETAILS ON THE METRIC IN UM & YE (2024)", "content": "We continue from Section 3.3 to provide additional details on the likelihood metric developed by Um & Ye (2024). This original version is defined on pixel space x \u2208 Rd (rather than latent domain z0 \u2208 Rk as ours), formally written as (Um & Ye, 2024):\nI(xt; s) := E [d(  )),"}]}