{"title": "Machine-learning based high-bandwidth magnetic sensing", "authors": ["Galya Haim", "Stefano Martina", "John Howell", "Nir Bar-Gill", "Filippo Caruso"], "abstract": "Recent years have seen significant growth of quantum technologies, and specifically quantum sensing, both in terms of the capabilities of advanced platforms and their applications. One of the leading platforms in this context is nitrogen-vacancy (NV) color centers in diamond, providing versatile, high-sensitivity, and high-resolution magnetic sensing. Nevertheless, current schemes for spin resonance magnetic sensing (as applied by NV quantum sensing) suffer from tradeoffs associated with sensitivity, dynamic range, and bandwidth. Here we address this issue, and implement machine learning tools to enhance NV magnetic sensing in terms of the sensitivity/bandwidth tradeoff in large dynamic range scenarios. We experimentally demonstrate this new approach, reaching an improvement in the relevant figure of merit by a factor of up to 5. Our results promote quantum machine learning protocols for sensing applications towards more feasible and efficient quantum technologies.", "sections": [{"title": "I. RESULTS", "content": "Our approach relies on training a machine learning model, with real and simulated data, to enable efficient identification of the resonance frequencies in the measured signal. We employ a relevant MLP model and compare it to regular raster scanning as a function of the number of data points (subsampling).\nNinety six full raster scans were measured in an epi-illumination wide field setup (see Fig. la and Sec. III for details), each one under a different externally ap-plied magnetic field which we measure through standard ODMR (since the different fields lead to different resonance frequencies).\nSynthetic data was generated based on a simplified NV Hamiltonian [1] considering the Zeeman shift: magnetic fields at various angles (\\(\\theta\\) and \\(\\phi\\), respectively longitudinal and azimuthal, with respect to the diamond surface), were projected on the four NV orientations, determin-ing the resonance frequencies. Resonance widths, con-trast, and Gaussian noise were chosen to mimic the line-shape of the measured data, producing full ESR spectra with six or eight Lorentzians, similar to the one depicted in Fig. 1d.\nThe neural network's input layer dimension (\\(A_n\\) in Fig. le) is determined by the number of data points in an ESR spectrum and therefore was changed as networks were trained for different subsamplings: starting from the full 600-point spectrum, we subsample by taking ev-ery other point, every third point and so on, obtaining at the end spectra with 300 points, 200 points and so on. The output layer always consists of eight values that predict the central frequencies of the eight Lorentzians (\\(y_i\\)). These predictions are then compared to the values extracted from the full ESR spectrum, which is defined to be the ground truth, and the error is defined to be the averaged absolute value of the differences.\nTo accomplish a fair comparison, a similar process was performed for raster scans: the resonances extracted from subsampled data (through standard Lorentzian curve fit-ting) were compared to the ones extracted from the full spectrum, and the error is defined in the same way. It is important to note that the subsampled datasets were compared to the full data from which they were derived, and the error in the full scan (with the maximal number of points) is the fit error.\nSome of the simulated data samples had overlapping or partly overlapping Lorentzians, for which the centers"}, {"title": "B. Scaling and comparison", "content": "Fig. 2a depicts the ML error (purple squares) for networks that were trained with 10000 samples and validated with 2000 samples to find the optimal hyperparameters (see Sec. IIIB for details on the model and its hyperparameters). As previously described, here each network was trained on the relevant subsampled data.\nBlue circles depict the normalized error of subsampled raster scanning, averaged over the same 2000 samples that were used for the network's validation. The raster scan error is distinctly high, due to the above-mentioned normalization of the error with the success probability, since in some of the data samples it was not possible to identify 8 Lorentzians. The ML model exhibits better results compared to raster scanning with an error that is more than 400 KHz better (except for the case of full length data), and remarkably, with only 10% of the data points, the ML error is still below 1 MHz. In addition, the ML error has better scaling as a function of the number of data points.\nIn Fig. 2b we present an alternative training scheme: in this case, just one network was trained and only on a full length dataset (600 frequency points) with 10000 samples and validated with 2000 samples. To comply with the structure of the MLP, the network was tested on 2000 subsampled dataset, that were first linearly in-terpolated back to full length and, only then, introduced to the network. The averaged error of these is presented in the plot (purple squares). The same 2000 samples were used to test raster scanning (blue circles), these were also sub-sampled, interpolated and then assessed. The interpolation surprisingly improves the error for raster scanning. In fact, even though it introduces additional noise to the data, the fitting works significantly better, and for a higher number of points, the raster's error and standard deviation are very similar to the network. How-ever, below 120 data points the ML models shows better results compared to the raster scan. Interpolating data saves on training time, since one network fits all datasets, but at the cost of less favourable performance. In fact, when a model is trained on the full 600-points data, it learns patterns that can be distorted with the subsampling and the subsequent interpolation. For high numbers of points, the two methods are almost equivalent. Although, for lower numbers of points, starting at 120 points, a network trained for the specific number of data points gives a lower averaged error and standard devi-ation, as can be seen from the comparison of the two graphs in Fig. 2a (and depicted also in Fig. Slb in the supplementary material).\nWe note that, while not realized here, a hybrid ap-proach might be considered and could be beneficial, wherein several networks could be trained on different"}, {"title": "C. Analysis and generalization", "content": "We now turn to a more detailed analysis ML model be-havior, further examining the behaviour of the network with regards to other parameters: the size of the train-ing dataset, noise level and Lorentzian widths, which can vary between diamond samples due to coherence proper-ties of the NVs [6].\nFig. 3 depicts the evolution of the validation error dur-ing training of the neural network for two dataset sizes: in green, the network was trained with 10000 simulated samples and validated with 2000 samples. In red, the network was trained with 1000 simulated samples and validated with 200 simulated samples. In blue, 50 real data samples were added to the training set, and the validation was performed on 46 real data samples. Even though training with 1000 samples is not sufficient, as the minimal error is around 2 MHz, there is agreement be-tween the results on synthetic data and the ones on real data. This plot explains the difference between results depicted in Fig. 2c and Fig. 2a - respectively represented here by the blue and green curves, with a difference of 0.5-2 MHz between them.\nTwo important parameters which characterize the res-onances measured in ESR experiments are the SNR and the Lorentzian widths. These vary between experimental setups, diamond samples and even individual measure-ments due to different noise sources. Fig. 4 presents the average error of two networks: in Fig. 4a the network was trained on a simulated dataset in which all 10000 sam-ples had a width of 10 MHz and subsequently tested on three datasets with 100 data samples, each with different resonance widths: 6 MHz (blue), 10 MHz (green), and 15 MHz (red). When tested on 10 MHz, the network's averaged error is 0.5 MHz lower than it is for 6 and 15"}, {"title": "II. DISCUSSION", "content": "In this paper we employ ML models to achieve high bandwidth measurements without compromising sensi-tivity in ESR measurements with NV centers. The neu-ral networks exhibit an advantage over raster scans, it maintains the same error for down to fifth of the data points, while for that sub sampling rate, the raster's er-ror increases by about 800 KHz. The ML models still perform well with an error of less than 1 MHz using only 10% of data points. Moreover, it shows an impressive ability to predict resonance locations in the presence of overlapping Lorentzians. We show the gain and flexibility of training networks in different ways: interpolating can save on training time while training multiple networks yields a lower error and standard deviation. Furthermore, we show that, for a network to be robust, it needs to be trained on a big enough dataset that is comprised of data in a range of noise and linewidth.\nIn the regime of sufficient training, we find it useful to have a simulation that well-describes the physics of the NV system, such that the resulting synthetic data is comparable to measured data.\nThe machine learning techniques applied to this quan-tum setting prove to be efficient, achieving an improved trade-off between high sensitivity and high dynamic range. They can be adaptively applied to measurements to achieve the desired result. E.g., reducing the mea-surement time by a factor 5, while maintaining the same error, improves the sensitivity by a factor ~ 2; alterna-tively, the sensitivity can remain constant while reduc-ing the measurement time even further. Such capabili-ties could have signifincant impact on a broad range of measurement scenarios with large, time-varying signals, such as characterization of circuit performance, identify-ing transient biological signals, and more."}, {"title": "III. METHODS", "content": "The ground state of NV centers (Fig. 1c) is an effec-tive two-level quantum system. Under green laser exci-tation, it is possible to initialize the spin to the ms = 0 ground state. In detail, the population occupying ms = 0 would reach the excited state manifold and decay back to the ground state ms = 0, emitting a red photon. The population occupying ms = \\(\\pm1\\) is more likely to decay through the singlet state, to ms = 0 in a non-radiative way. Within the ground state, spin manipulation is pos-sible with resonant MW pulses, population transfer to ms = \\(\\pm1\\) would lead to a drop in measured fluorescence.\nIn the presence of an external magnetic field, degener-acy is lifted off the ms = \\(\\pm1\\) due to Zeeman shift, the shift is given by \\(\\gamma B_{\\parallel}\\), where \\(\\gamma\\) is the gyromagnetic ratio of the NV and \\(B_{\\parallel}\\) is the external magnetic field component parallel to the NV axis. In the diamond lattice, there are four possible crystallographic orientations the NV can take (Fig. 1b), and so, in the presence of a magnetic field that is not aligned with any of the orientations, there will be eight resonance frequencies as shown in Fig. 1d, two for each orientation. Once these frequencies are known, the vectorial magnetic field can be calculated.\nIdeally, an ESR measurement would have the small-est number of points for which all information about the magnetic field can be obtained from the data, without in-creasing measurement error, which means lower sensitiv-ity. As can be expected, the error increases as the num-ber of measurement decreases. Below a certain number of measurements, raster scanning is simply not possible as"}, {"title": "B. Machine learning model", "content": "In this section we give more details on the ML models in Fig. le employed to predict the position of the deeps \\(y_i\\) in the ESR spectrum. Specifically, they are MLPs trained with Adam [39] to minimize the Mean Squared Error (MSE) loss between the predictions \\(\\hat{y}\\) and the ground truth \\(y_i\\) given by the spectrum. The dimen-sions of the input and output layers are defined by the problem and are respectively given by the number of ESR data points \\(\\lambda_1,..., \\lambda_n\\) and the eight deeps positions. The number of layers in the network and their size are treated as hyperparameters to be optimized and, in order to keep the procedure as simple as possible, the tunable number of neurons is the same for every hidden layer. Also, the task is formulated as a regression problem, and all the inputs and outputs are normalized between zero and one"}]}