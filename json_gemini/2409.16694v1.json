{"title": "A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms", "authors": ["Ruihao Gongl", "Yifu Dinga", "Zining Wanga", "Chengtao Lv\u00aa", "Xingyu Zhenga", "Jinyang Du\u00aa", "Haotong Qinb", "Jinyang Guo\u00aa", "Michele Magnob", "Xianglong Liu2a"], "abstract": "Large language models (LLMs) have achieved remarkable advancements in natural language processing, showcasing exceptional performance across various tasks. However, the expensive memory and computational requirements present significant challenges for their practical deployment. Low-bit quantization has emerged as a critical approach to mitigate these challenges by reducing the bit-width of model parameters, activations, and gradients, thus decreasing memory usage and computational demands. This paper presents a comprehensive survey of low-bit quantization methods tailored for LLMs, covering the fundamental principles, system implementations, and algorithmic strategies. An overview of basic concepts and new data formats specific to low-bit LLMs is first introduced, followed by a review of frameworks and systems that facilitate low-bit LLMs across various hardware platforms. Then, we categorize and analyze techniques and toolkits for efficient low-bit training and inference of LLMs. Finally, we conclude with a discussion of future trends and potential advancements of low-bit LLMs. Our systematic overview from basic, system, and algorithm perspectives can offer valuable insights and guidelines for future works to enhance the efficiency and appli-", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) (OpenAI et al., 2024; Touvron et al., 2023a,b; Dubey et al., 2024; Lozhkov et al., 2024; Liu et al., 2024a) have revolutionized natural language processing by delivering unprecedented performance across a range of tasks, from text generation to language understanding. However, their remarkable capabilities come with significant computational and memory demands. This has raised considerable challenges when deploying these models in scenarios with limited resources or high concurrency. To address these challenges, low-bit quantization has emerged as a pivotal approach for enhancing the efficiency and deployability of LLMs.\nLow-bit quantization involves the process of reducing the bit-width of tensors, which effectively decreases the memory footprint and computational requirements of LLMs. By compressing weights, activations, and gradients of LLMs with low-bit integer/binary representation, quantization can significantly accelerate inference and training and reduce storage requirements with acceptable accuracy. This efficiency is crucial for enabling advanced LLMs to be accessible on devices with constrained resources, thereby broadening their applicability.\nIn this paper, we aim to provide a survey with a comprehensive overview of low-bit quantization for large language models (LLMs), encompassing the fundamental concepts, system implementations, and algorithmic approaches related to low-bit LLMs. Compared with the traditional models, LLMS, as the representative paradigm of the foundation model, always feature a vast number of parameters, which presents unique challenges for effective quantization. As depicted in Figure 1, Section 2 introduces the fundamentals of low-bit quantization of LLMs, including new low-bit data formats and quantization granularities specific to LLMs. Section 3 reviews the systems and frameworks supporting low-bit LLMs across various hardware platforms. We then categorize low-bit quantization techniques for efficient training and inference in Sections 4 and 5, respectively. For training, we discuss methods for low-bit training and fine-tuning of LLMs. For inference, we differentiate LLM quantization methods by quantization-aware training and post-training"}, {"title": "2. Basics of Low-bit LLMs", "content": "In this section, we introduce the basic fundaments of quantization and low-bit LLMs from three aspects: (1) Low-bit number formats. To deal with the outliers in LLMs, low-bit floating-points are first used in quantization. And lots of custom data formats are designed to tackle the outliers. However, integers are still the mainstream. (2) Quantization granularity. To improve the performance of quantized LLMs, finer-grained quantization retains more information and generates better results. But course-grained ones occupy less storage and are more efficient in inference. (3) Dynamic or static quantization. Dynamic quantization does not require calibration, as the quantization parameters are calculated on the fly, making the preparation of a quantized model simpler. In contrast, static quantization requires pre-calibration of quantization parameters, but it offers faster inference performance."}, {"title": "2.1. Low-bit Number Formats", "content": "We start with the low-bit number formats at the beginning of the introduction. First, we demonstrate the standard formats that are well-recognized, but focus on the differences in LLMs. Second, we introduce some typical custom formats that are designed for LLMs."}, {"title": "2.1.1. Standard Formats", "content": "Floating-point Numbers. The floating-point data type is comprehensively defined in the IEEE 754 (iee, 2019) standard, which is also the most prevailing number format in computer systems. Let us denote them as FPk, where k represents the number of bits that the value occupies in memory, usually 32, 16,8, etc. A floating-point number can be uniformly expressed as\n$X_{FPk} = (-1)^s \\times 2^{p-bias} (1.mantissa) = (-1)^s \\times 2^{p-bias} (1+\\frac{d_1}{2^1} + \\frac{d_2}{2^2} + ... + \\frac{d_m}{2^m})$, (1)\nwhere s is the sign bit, p is the exponent integer, bias is applied to the exponent, m is the total number of mantissa bits in the significand, and $d_1, d_2, ..., d_m$ represent the digits of the mantissa part in the binary format."}, {"title": "2.1.2. Custom Formats", "content": "For faster computation and better fitting the numerical distributions of LLMS, many studies propose custom number formats besides the standard formats de-"}, {"title": "2.2. Quantization Granularity", "content": "Quantization granularity refers to the different weight/activation partitions corresponding to each element of the scaling factor and zeropoint. It determines how finely the scale recovers and the zero point shifts. Figure 2 showcases five fundamental types of quantization granularity: tensor-wise, token-wise, channel-wise, group-wise, and element-wise.\nTensor-wise is the simplest and coarsest granularity, which takes a single scaling factor and zero point to the entire tensor (Zhang et al., 2024c). It can be the fastest but may lead to the most performance degradation because it is incapable of handling the values with a wide variation. Therefore, it is unsuitable for cases where accuracy is important or the task/model is sensitive to quantization.\nToken-wise is used in LLMs only, which means that each token (word or subword) has a scaling (Yao et al., 2022). It captures the fine-grained variations in different tokens. Usually, we adopt dynamic token-wise quantization for activation to reduce the quantization error and ensure diversity in generative models.\nChannel-wise means each channel in weight within a tensor uses one scale and can be merged into quantized weight (Kim et al., 2024). Token-wise activation and channel-wise weight are usually used together. Because for i-th token in activation and j-th channel in weight, the corresponding $s_{x_i} \\in s_x \\in \\mathbb{R}^{T\\times1}$ and $s_{w_j} \\in s_w \\in \\mathbb{R}^{1\\times C}$ can be calculated first as $s \\in \\mathbb{R}^{[i,j]}$ and multiplied to the coordinate [i, j] in output matrix $X_o$. In this way, we preserve the generation performance with little computation overheads."}, {"title": "2.3. Dynamic and Static Quantization", "content": "Dynamic and static quantization mainly refers to the strategies in PTQ, which are illustrated in Figure 3. We take integer quantization as an example, and other low-bit quantization methods have a similar process.\nDynamic Quantization (Krishnamoorthi, 2018; Liu et al., 2022) calibrates and stores quantized weight. Usually, it does not need input data, but searches for the optimal scaling factors $s_w$ and zero-points $Z_w$ by minimizing the quantization error for each tensor of weight. During inference, the activation will be input into the quantization module to compute the optimal scaling factors $s_x$ and $Z_x$, and"}, {"title": "3. Frameworks and System Support", "content": "In the few short years since the large language model emerged, there have arisen many frameworks to support the easy usage of LLMs. We have selected some well-known representative frameworks and tools related to quantization, summarized and introduced them in this section according to the following categories: (1) Inference framework for quantization, which provides comprehensive libraries and APIs for the rapid development and deployment of LLM applications, (2) System support for quantization, which supports the underlying core functionality for quantization methods. In the following, our emphasis is on the quantization of LLMs across various frameworks and libraries."}, {"title": "3.1. Inference Framework for Quantization", "content": "We list the representative inference frameworks in Table 2. Currently, no single inference framework dominates in terms of performance or usage. However, some classic deep learning frameworks, such as TensorRT-LLM\u00b9, ONNX-runtime\u00b2,"}, {"title": "3.1.1. Ready-to-use Algorithms", "content": "With the emergence of quantization algorithms for LLMs, some typical methods have already been integrated into most frameworks, while some methods may be developed and published originally on a specific framework. We list the most ready-to-use algorithms in each mainstream framework in Table 2. Some methods are included by the most frameworks, such as GPTQ (Frantar et al., 2022), AWQ (Lin et al., 2024a), SmoothQuant (Xiao et al., 2023), LoRA (Hu et al., 2021) and so on. These methods share several advantages: high accuracy and efficient performance after quantization, seamless integration into existing implementation procedures, and user-friendliness.\nIn addition, some algorithms are supported by several frameworks. For example, LLM.int8() (Dettmers et al., 2022b) was well supported by bitsandbytes (by HuggingFace), which allows to store and load 8-bit weights directly from the HuggingFace Hub and quantize weight in linear layers to 8-bit. FP6-LLM (Xia et al., 2024) is integrated in DeepSpeed-FastGen21 (Holmes et al., 2024) to implement the runtime quantization for 6-bit floating-point weight-only quantization. It allows efficient quantization and dequantization of 6-bit weight LLMs through"}, {"title": "3.1.2. Bitwidth Support", "content": "The support for bitwidth always reflects how comprehensive the quantization system implementation is for an inference framework or engine. It can be categorized into three types according to its position and function in accelerating LLMS:\nWeight-onlybit means only quantizing the weight while keeping FP16 activation (Lin et al., 2024a). The quantized weight will be dequantized back to FP16 using pre-obtained scaling factors, and then conduct FP16 mma with FP16 activation. Therefore, it theoretically supports non-uniform quantization with arbitrary bitwidth. The speedup is achieved by reducing the latency of data transmission between the computing device and storage host with smaller amounts of weight data, but the dequantizing of weight costs extra time. The detailed speedup will be discussed in Sec. 3.2.1.\nW&Abit means that the algorithm quantizes both the weight and activation, and conducts low-bit matrix multiplication (MatMul) in low-level (for example, in PTX ISA 8.522 for NVIDIA GPUs, instruction mma.sync.aligned.shape.row.col .s32.u4.u4.s32 means the data type of the multipliers is the 4-bit unsigned integer). All the frameworks support the INT8 and FP16 MatMul. However, limited by the computing capabilities of the hardware and the supported operations in the instruction set, only part of them have INT4 and FP8 MatMul. Few supports different bitwidth of weight and activation (like $Wint4 \\_Aints$), which requires customized computation kernels with assembled GEMV instructions23 (Egiazarian et al., 2024a). It should be mentioned that if you want to use low-bit MatMul, your hardware architecture must support the specific low-bit computing, and it is necessary to upgrade/downgrade the driver to the corresponding version to reproduce the real low-bit computation and get the desired speedup ratio.\nKV Cachebit lists the bitwidth of Key-Value Cache. As a caching technology, memory consumption of the KV cache increases rapidly as batch size and sequence length continue to grow, potentially surpassing the model size. Therefore, quantizing the KV cache significantly reduces memory usage during model inference. There are several works devoted to quantizing the KV cache (Hooper et al., 2024; Yue et al., 2024; Liu et al., 2024c). Similar to weight-only algorithms, the quantized"}, {"title": "3.1.3. Target Platforms", "content": "Numerous vendors are competing fiercely in the deep learning hardware. As one of the pioneers in the field of deep learning GPUs today, NVIDIA GPUs are supported by most frameworks. Meanwhile, vLLM, bitsandbytes, llama.cpp, ctransformers, MLC-LLM, and PowerInfer also have the support for AMD GPUs. For some other processing units, such as TPU, XPU, Vulkan, Metal, and other hardware, the system support is relatively limited. Some frameworks that are devoted to generalizing LLMs to edge devices are more likely to extend the support for those platforms, such as MLC-LLM, ONNX-Runtime, and llama.cpp. However, it should be noted that the frameworks with support for both low-bit quantization and hardware deployment in Table 5 cannot guarantee the deployment of any quantized model on each listed hardware. Users should carefully refer to the manual for guidance. However, the table we compiled may help reduce the time it takes to find a suitable framework that may meet your deployment desire."}, {"title": "3.1.4. Model Family", "content": "All the frameworks support custom model definition and seamlessly integrate external model zoos, such as HuggingFace Hub. To help users quickly get started, the frameworks provide predefined specification files for commonly used models. We can roughly classify the large models into three categories: Transformer-like LLMs (e.g., Llama, Orion, Baichuan, ChatGLM, Falcon), Mixture-of-Expert(e.g., Mixtral, Mistral, DeepSeek), Multi-modal LLMs (e.g., LLaVA). However, not all large models included in external model zoos can be smoothly supported, because the frameworks integrate new algorithms with a lag. Therefore, users should refer to the model zoo provided by the framework, and make sure that the target model has no additional underlying system requirements before importing a new model from the external model zoo that is beyond the supported model list."}, {"title": "3.2. System Support for Quantization", "content": "In practical implementations, it is perplexing that some quantization algorithms, although reducing the bitwidth of weight or activation, do not lead to faster inference. Therefore, a critical question comes into mind: How do quantization actually achieve real acceleration and storage saving? To answer this question, we first need to clarify the data transmission process involved in model inference."}, {"title": "3.2.1. Weight-Only Quantization", "content": "The fundamental bottleneck in model inference before and after the advent of large models is the data transmission and storage costs, which are always neglected in ordinary small models. Due to the huge amount of data, the transmission latency can not be overlooked, which even surpasses the computation latency and becomes the major challenge in LLM inference. Therefore, weight-only quantization emerges, which compacts the weight and reduces the data copy burden among levels of caches (Lin et al., 2024a; Frantar et al., 2022).\nThe processes related to weight-only quantization are illustrated in Figure 5 (a) and (b). Both weight-only and weight & activation quantization require packing weight to lower bitwidth beforehand. The weight packing is only conducted once before inference, and it costs little computation resources and time. The weight data are distributed to multi-threads, with each thread tiles a chunk of data according to the following steps: (1) quantizing the weight to lower bitwidth by pre-obtained scaling factors, (2) densely packing them into uint32 units without idle bits, (3) offloading and storing into host memory. Therefore, the packed weight has a significant reduction in storage compared to the floating-point one.\nSee speedup timeline in (b), weight-only quantization alleviates the burden of data transmission from host memory to on-chip memory by reducing the data amounts. However, it introduces additional dequantization of weight before conducting the MatMul because the general kernels only receive the same datatype of inputs. As long as the time spent on dequantization is shorter than the time saved on data transmission, the weight-only quantization brings benefits in acceleration, which indeed is the case. It is the overload of parameter transmission in LLMs that makes weight-only quantization valuable in practice. Therefore, even using floating-point MatMul kernels, weight-only quantization can still accelerate the inference of LLMs.\nAs for custom designs, since weight-only quantization dequantizes the weight back to FP16, it is possible to pack the weight with arbitrary bitwidths during quantized weight preparation. Many works propose 3-bit, 5-bit, 6-bit weight quantization (Shi et al., 2024; Frantar et al., 2022; Xia et al., 2024). Furthermore, since the quantized weight must be dequantized to higher bitwidths before MatMul, it is not necessary to design a linear surjection from low bitwidth numbers to real values. In other words, we can map the integers to arbitrary floating-point numbers,"}, {"title": "3.2.2. Weight & Activation Quantization", "content": "Following the traditional practice of quantization, both weight and activation are quantized to low bitwidth, and the MatMul kernels are also implemented by low-bit instructions. We illustrate the speedup timeline in Figure 5(c) that the accelerated processes are weight transmission in the caching system as well as the low-bit MatMul. The extra operations are the quantization for activation from FP16 to low-bit integer before MatMul, and the datatype casting for the results from INT32 to FP16 after MatMul. Weight & activation quantization yields greater acceleration compared to the weight-only quantization because the computationally intensive MatMul usually can be accelerated by lower bitwidth kernels, which use more efficient instructions and a better degree of parallelism. Meanwhile, it is recommended to simplify the complexity of activation quantization to minimize the time spent on runtime quantization. However, the actual speedup ratio highly depends on the hardware design, such as the number of floating-point and integer processing units.\nAs for custom designs, there are two categories of techniques: (1) Faster Quantization and Dequantization (or datatype conversion). For example, QQQ (Zhang et al., 2024d) proposes faster FP16\u2192INT8 for quantizing activation, INT4\u2192INT8 for dequantizing weight, and INT32\u2192FP16 for casting the MatMul results to accelerate the data format conversion during inference. This work is based on Kim et al. (2022) which firstly introduces a faster INT4\u2192FP16 datatype conversion. Besides speeding up, other approaches turn to remove the process. Tender (Lee et al., 2024b) proposes a decomposed quantization technique to eliminate runtime dequantization/quantization during inference. (2) Faster MatMul Kernel. GEMV can be more flexible and efficient in fitting various bitwidths than GEMM, and even receives input matrices with two bitwidths, such as INT1*INT8 and INT3*INT8 (Wang et al., 2023). By assembling several products of a matrix and a vector, we can get the desired results without padding or idle bits. For example,"}, {"title": "3.2.3. KV Cache Quantization", "content": "KV Cache, or key-value cache, is to optimize the generative models that predict text token by token. Although the model generates only one token at a time, each token depends on the previous context. To avoid repeated calculation, the KV cache acts as a memory bank storing previous key-value results to reuse in the following generations. However, the storage highly depends on the sequence length, hidden size, attention head numbers, and so on. Quantization is an efficient approach to compress the storage. The overall process is illustrated in Figure 6.\nThe KV cache is generated and updated in runtime along with the serialized input data. During inference, the $K_{new}$ and $V_{new}$ from linear layers are first quantized, then concatenate to the end of the stored key and value lists, which are also quantized, to form new lists. While the earliest key and value will be dropped if the cache size is exceeded. Then we dequantize the matrices to FP16 before conducting multi-head attention forward propagation with the newly generated query $Q_{new}$. We illustrate how KV cache quantization affects the inference in the"}, {"title": "3.2.4. Quantization for Production", "content": "Besides accelerating the inference, quantization techniques are also used for accelerating the model production of quantized models. Before LLMs emerge, the PTQ methods are regarded as a time-efficient approach. However, some PTQ methods require times of forward propagation to get the parameters, which is neglectable in ordinary models but becomes burdensome in LLMs due to the huge parameter amount. Therefore, accelerating the PTQ calibration becomes a new issue worth exploring. Methods like HQQ (Badri and Shaji, 2023) simplify the calibration by improving the sparsity by L1-norm and enhancing the awareness of the weight distribution. Meanwhile, it applies the closed-form solutions to avoid calculating the gradients. Without the calibration data, HQQ quantizes the model in just a few minutes, which significantly reduces the production time of quantized LLMs. The HQQ+ (Badri and Shaji, 2024) even pushes the bitwidth to the aggressive 1/2-bit."}, {"title": "3.2.5. Quantization and Dequantization", "content": "In this section, we roughly categorize the quantization into three types: (1) Floating-point Quantization, casting the high-bit floating-points into low-bit ones.\n(2) Integer Quantization, which mainly refers to dividing the floating-points into evenly spaced integers. We omit requantizing higher bitwidth integers to lower bitwidth ones here, because it is seldom used in real practices, and few studies propose faster implementations to convert integers. (3) Binarization, including sign and bool functions.\nFloating-point Quantization. Quantizing higher bitwidth floating-point to lower is actually the clip of mantissa bits. That is because the source value with higher bitwidth usually has more or equal bits for both exponent and mantissa parts compared to the target value with lower bitwidth. Algorithm 3.2.5 provides an example of quantizing FP32 to FP8. And we follow Micikevicius et al. (2022) to summarize the general process as follows:\n(1) Scale. Since the target value occupies less bitwidth, the representation range may shrink drastically, and not be able to convey most of the data. Scaling the source value to a suitable range can best preserve the information after quantized to FP8. The scaling is pre-obtained by learning or calibration.\n(2) Check Overflow/Underflow. Check whether the source value overflows FP8 from the upper or lower bound. If so, return the maximum or minimum directly. If it is not overflow, check if the exponent part underflows from the smallest positive normal number that the FP8 can present. If so, we divide the value by the smallest subnormal number in FPx, round to the nearest integer, and then multiply the smallest subnormal number. The integer determines the value of mantissa bits and the exponent bits are all set to zero.\n(3) Copy and Round. If the value is neither overflows nor underflows of FP8, we copy the lower e bits from the source FP32 value to the target FP8 value. Then we clip the mantissa to m bits by rounding to the nearest. It is notable that rounding and overflow/underflow handling are both crucial for maintaining numerical stability and precision in real applications. However, since the reduction of mantissa bits, precision degradation is inevitable while converting to lower bitwidth.\nFloating-point Dequantization. Dequantizing floating-point numbers to higher bitwidth is straightforward. In the FP format system, the bitwidth of both the exponent and mantissa bits in lower bitwidth values will not exceed that in higher bitwidth. Therefore, we can directly extract and copy the sign bit, exponent and mantissa from the original value (with fewer bitwidth) to the most significant bits in the corresponding parts of the target value (with more bitwidth). And then we conduct zero filling on the rest bits for the exponent and mantissa parts 25."}, {"title": "Algorithm 1 Quantization to lower-bit floating-point values.", "content": "Input: $X_{FP32}, s \\in \\mathbb{R}_+, X_o \\in \\mathbb{R}, e, m \\in \\mathbb{Z}_+, clip^{min}, clip^{max}$\nOutput: $X_{FP8}$\n1: $X_{unscaled} = X_{FP32}/S$\n2: $e_{min} = -(1 << (e - 1)) + 1$\n3: $e_{max} = (1 << (e - 1))$\n4: $m = x-e-1$\n// Theoretical maximum of exponent part for FP8\n5: $X_{FP8}^e = e_{max} + 2^{8-1} << 23$\n// Theoretical maximum of mantissa part for FP8\n6: $X_{FP8}^m = ~(0x007FFFFF >> m) & 0x007FFFFF$\n7: $X_{theomax} = X_{FP8}^e + X_{FP8}^m$\n// Check exponent overflow\n8: if $X_{upscale} \\doteq X_{unscaled} > min(clip^{max}, X_{theomax})$ then\n9:   $X_{FP8} = min(clip^{max}, X_{theomax})$\n10: else if $X_{theomax}^e < max(clip^{min}, -X_{theomax})$ then\n11:  $X_{FP8} = max(clip^{min}, -X_{theomax})$\n12: else\n13:   $X_{sign} = X_{unscaled} & 0x80000000$\n14:   $X_{FP8}^e = X_{unscaled} & 0x7F800000$\n15:   $X_{FP8}^m = X_{runscaled} & 0x007FFFFF$\n// Check exponent underflow\n16:  if $(X_{FPx} >> 23) - 2^{x-1} < e_{min} +1$ then\n17:  $X_{FP8}.subnorm = 1 / (1 << ((1 << (e - 1)) + m - 2))$\n18:  $X_{FPX} = round2int(X_{unscaled} / X_{FP8}^{min}.subnorm) \\times X_{FP8}^{min}.subnorm$\n19:  end if\n// Round mantissa\n20:  $R_m = (X_{P8}<<m) & 0x007FFFFF + 0x3F800000$\n21:  $R_m = round2int(R_m - 1)$\n// Process mantissa\n22:  $X_{FP8} = (X_{FP8}>>(23-m) + R_m)<<(23 - m)$\n23:  $X_{FP8} = X_{FP8} + X_{FP8}^e + X_{FP8}^m$\n24: end if\n25: return $X_{FP8}$"}, {"title": "Integer Quantization.", "content": "We first scale the floating-point numbers to the representation span of INTk by dividing the scaling factor $s \\in \\mathbb{R}^+$, and adding a zero-point $Z\\in \\mathbb{Z}$ to shift the clamped range (Wu et al., 2020). $round(\\cdot)$ is the round-to-the-nearest function, and $clamp(\\cdot, q^{min}, q^{max})$ restricts values to be within the representation span of k-bit with $q^{min} = -2^{k-1}, q^{max} = 2^{k-1} - 1$ in symmetric quantization and $q^{min} = 0, q^{max} = 2^k - 1$ in asymmetric quantization. Therefore, the overall quantization formulation can be written as\n$X_{INT_k} = clamp\\left(round\\left(\\frac{X_{FP}}{s}\\right)+Z, q^{min}, q^{max}\\right),$ (8)\nwhere the scaling factor s can be initialized as $s_0 = \\frac{(q^{max} - q^{min})}{(X_{FP}^{max} - X_{FP}^{min})}$, where $X_{FP}^{max}$ and $X_{FP}^{min}$ are the maximum and minimum values.\nFor system support, many frameworks apply the Marlin quantization 26 (Frantar and Alistarh, 2024) as the standard process. The pseudocode Algorithm 2 outlines the steps involved in Marlin quantization, and uses 4-bit integer quantization as an example. The values are quantized and stored as unsigned integers with the desired bitwidth. Extra pre/post-shift will be conducted to get the signed values. Therefore, we first scale the $X_{FP32}$ values by s and round it to integers. Then, adding $2^{k-1}$ to shift the values to non-negative integers within the span of uINT4 (4-bit unsigned integer). We omit the detail of C++ built-in datatype casting function float2uint. To be understood, we explain the packing process in lines 4 to 8 by double for loops, which is actually implemented as lines 9 to 11. In 4-bit quantization, every 8 values are packed as a single uINT32, and the quantized matrix size is a quarter of the original. By using $i::8$, we abstract every 8 values along C starting with i, incrementing by 8, and ending by default (till the end of dimension C). And then left shifting the values by 4* i to place the 4-bit value to the corresponding bit range, and leave 4 * i zeros on the right, allowing previously-stored quantized values to be preserved after OR operation.\nThere are several custom algorithms that introduce faster data type conversions. QQQ (Zhang et al., 2024d) designs a faster FP16 to INT8 conversion, named FastFP16toINT8. It starts by shifting the FP16 values to the representation span of uINT8 by adding 128. Next, adding an additional 1024 which effectively converts and places the 8 bits of uINT8 into the lower segment of the FP16 mantissa. Finally, the lower 8 bits from FP16 are extracted and applied with an XOR operation with 0x80 to obtain the desired INT8 format. The overall process can be further simplified to FMA, PRMT, and XOR operations in practice."}, {"title": "Algorithm 2 Marlin quantization from FP32 to INT4.", "content": "Input: $X_{FP32} \\in \\mathbb{R}^{T,C}, s \\in \\mathbb{R}^+$\nOutput: $X_{uINT4}$\n// Shift to span of uint4\n1: $X_{d}^{round} \\doteq round(X_{FP32}/scale)$\n2: $X_{FP32}^{clamp} \\doteq clamp(X_{d}^{round} + 2^3, 0, 2^4 - 1)$\n3: $X_{uint32} \\leftarrow float2uint (X_{FP32}^{clamp})$\n// Pack every 8 $X_{uint32}$ to a single uINT32\n4: for k\u2190 0 to C//8 do\n5:  for i \u2190 0 to 7 do\n6:   $X_{INT4}[:, k] (4*i+3:4*i) \\leftarrow X_{uint32}[:, i + 8 * k] << (4 * i)$\n7:  end for\n8: end for\n// Line 4~8 can also simplify as line 9~11\n// i::8 creates a sequence starts at i, increments by 8, and ends by default\n9: for i \u2190 0 to 7 do\n10:  $X_{uINT4}[:,:](:4*i)\\leftarrow X_{uint32}[:, i::8] << (4* i)$\n11: end for\n12: return $X_{uINT4}$"}, {"title": "Integer Dequantization.", "content": "It means projecting the integers back to the real numbers by multiplying the scaling factors, which can be expressed as\n$X_{FP} = s \\cdot (X_{INT_z} - Z) \\approx X_{FP}$. (9)\nTherefore, in many works s can also be initialized by searching from candidates to find an optimal (Wei et al., 2023b):\n$s_{candidate} = \\{\\frac{i}{numi}s_0, i \\in \\mathbb{Z}^+, i \\in (0, numi)\\}$ (10)\ns.t. $min ||X_{FP} - \\hat{X_{FP}}||_p$. (11)\nwhere numi means the number of candidates, which is always set as 50, 100 and so on (Yuan et al., 2024; Wei et al., 2023b). s can also be a learnable parameter (Wei et al., 2023b; Shao et al., 2023). The way to find a better s has been widely studied before LLMs emerged (Ding et al., 2024; Wei et al., 2023a).\nFor system support, we first unpack the element according to the way we pack them, and then multiply to the corresponding scaling factor, which can be"}, {"title": "Binarization.", "content": "It takes the sign or bool function to abstract the sign:\n$X_{sign} = \\begin{cases} 1, & X_{FP} \\ge 0,\\\\ -1, & X_{FP} < 0,  \\end{cases}$ $X_{bool} = \\begin{cases} 1, & X_{FP} \\ge 0,\\\\ 0, & X_{FP} < 0.  \\end{cases}$ (12)\nUsing sign or bool depends on the algorithm design, i.e. what"}]}