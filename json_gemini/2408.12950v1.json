{"title": "Informational Embodiment: Computational role of information structure in codes and robots", "authors": ["Alexandre Pitti", "Kohei Nakajima", "Yasuo Kuniyoshi"], "abstract": "The body morphology plays an important role in the way information is perceived and processed by an agent. We address an information theory (IT) account on how the precision of sensors, the accuracy of motors, their placement, the body geometry, shape the information structure in robots and computational codes. As an original idea, we envision the robot's body as a physical communication channel through which information is conveyed, in and out, despite intrinsic noise and material limitations. Following this, entropy, a measure of information and uncertainty, can be used to maximize the efficiency of robot design and of algorithmic codes per se. This is known as the principle of Entropy Maximization (PEM) introduced in biology by Barlow in 1969. The Shannon's source coding theorem provides then a framework to compare different types of bodies in terms of sensorimotor information. In line with PME, we introduce a special class of efficient codes used in IT that reached the Shannon limits in terms of information capacity for error correction and robustness against noise, and parsimony. These efficient codes, which exploit insightfully quantization and randomness, permit to deal with uncertainty, redundancy and compacity. These features can be used for perception and control in intelligent systems. In various examples and closing discussions, we reflect on the broader implications of our framework that we called Informational Embodiment to motor theory and bio-inspired robotics, touching upon concepts like motor synergies, reservoir computing, and morphological computation. These insights can contribute to a deeper understanding of how information theory intersects with the embodiment of intelligence in both natural and artificial systems.", "sections": [{"title": "1 Introduction", "content": "Before Claude Shannon's work, engineers believed that reducing communication errors necessitated increased transmission power or repetitive message transmission. Shannon fundamentally demonstrated that it wasn't essential to expend excessive energy, effort and time if appropriate coding schemes were employed [1, 2], even in the face of an unreliable communication channel. We think that similar principles can be applied in robotics to enhance efficiency in perception, control, and body design. Information theory (IT) can provide insights to robotics in terms of efficient codes, information structure and communication between the body (hardware) and the controller (software).\nCurrent robots achieve high accuracy in motion control based on highly precise sensors and motors. For instance, current electronics permit to control the motion behavior of robots at the speed of kHz with high resolution sensors to detect\nerrors, simulate several actions and generate an optimal plan with precise motors. Nonetheless, they rely heavily on high performance devices to work. Therefore, the robot's accuracy in terms of performance Rout is mostly linked to the precision level of its devices Rin, so that Rin \u2248 Rout or even Rin > Rout. In a sense, more precision in devices provides more accuracy in behavior, see Fig. 1 a).\nIn comparison, biological systems don't possess very precise sensors and actuators. However, they are capable to overcome their own limitations and reach higher accuracy in performance, so that Rin Rout. In a sense, they do more with less, see Fig. 1 c). Animals and current robotics use therefore different strategies to achieve efficient control. Besides, contemporary robots still fall short of replicating the smooth and effortless variety and adaptive flexibility displayed by animals and average 18-month-old during everyday activities [3, 4]. In return, bio-inspired robotics and neuro-inspired AI systems, such as soft robots and reservoir computing, may use in the future the same strategies as humans for motion control, in order to achieve high performance based on unreliable devices (e.g. sensors, motors, or neurons), so that Rin Rout, see Fig. 1 d). In this line, we propose that IT can give insights how biological systems may use efficient coding to do it and how robots can reproduce it.\nIn the 60', Horace Barlow introduced the Efficient Coding theory in Biology [5, 6], suggesting that biological systems rely on an efficient coding mechanism to convey maximum information per unit [7, 8]. This theory, inspired by Jaynes' Principle of Entropy Maximization (EM) [9, 10] and IT of Claude Shannon, posits that by reducing redundancy in a signal, we eliminate unnecessary predictable information, enabling storage of more information in a compressed code. Efficient codes are highly informative, compact, and robust over time, aligning closely with the Bayesian treatment of information. Digital codes and current advanced methods in IT that will be presented in section 2 verify the Entropy Maximization principle so that we have also Rin < Rout, see Fig. 1 b). Accordingly, efficient coding can be advantageously used by animals for leveraging accuracy, in sensing, control, and information retrieval [11].\nTransposed into the perspective of embodiment, the body can be conceptualized as a resource, functioning metaphorically as a physical communication channel that conveys information in and out. Its characteristics in terms of information exchange, capacity and bandwidth will depend then on information conveyed by its morphology, its sensors, and its motors; which means its information structure. This view of embodiment is in line with the concept of morphological computation proposed by Rolf Pfeifer and colleagues [12-14], making explicit the link between the robot's body and computation, such that the robot's design has an incidence on the information structure and codes.\nFrom this perspective, effectiveness of one embodiment can be quantified in terms of information exchange on one side and in terms of information storage or encoding on the other side. Entropy, a measure for information, redundancy, and variability across systems, can assess then the physical limit of embodiment between the robot's body and its algorithm. We propose that entropy can quantify the respective performances of different types of code and body, their bandwidth or their information bottleneck.\nFor the robotic engineer, these concepts borrowed from IT will help to answer practical questions such as: where to place the motors and the sensors in the robots in order to increase accuracy? What is an efficient controller for a specific body? Reversely, what is the limit in terms of accuracy of one specific body? What is its nominal speed and bandwidth? And how many sensors or motors do I need for a certain precision level depending on their resolution and their placement? Currently, these questions are solved only empirically based on experience by roboticists. IT can be a solid ally to robotics to conceptualize an informational embodiment theory: an information-based theory to embodied agents and robots.\nThe paper is organized as follows. In section 2, we will present how efficient coding has been introduced in Telecom and Sensing Technologies in the 2000' and gave rise to an IT revolution. We will describe how these codes maximize entropy and how they can inspire embodied AI and robotics with novel concepts.\nIn section 3, we will present different examples supporting our informational embodiment theory. We will explain our framework in the light of quantification of different types of embodiment, of evaluating motor equivalence and information"}, {"title": "2 Efficient Codes, the Body as an Information Channel.", "content": "Our hypothesis draws inspiration from the Embodied Intelligence (EI) paradigm developed by [12, 14-18] and others that explicitly delve into information theory [19-22]. While EI has provided essential principles, it is still in search of a comprehensive theory [23-25].\nWe propose that efficient coding, -in the sense given by Shannon of entropy maximization,- can explicitly describe the design of sensors, controllers or bodies that maximizes information processing.\nEfficient Coding in Biology. Efficiency coding principles have been proposed in Biology in which original signals, redundant, are hypothesized to be transformed into neural codes of uniform distribution [26, 27]. By doing so, codes are more compact and can transmit the same amount of information with far less channel capacity [27, 28]; hence maximizing information.\nFor example, the human optical nerve, which is composed of 1 million ganglions, has to transmit the information coming from the retina, which is composed of 120 millions retinal photo-receptors. Following the hypothesis of efficient encoding that neurons should encode information as efficiently as possible in order to maximize neural resources, it has been shown that visual data can be compressed up to 20 fold without noticeable information loss. The first experiences in testing the theory of efficiency coding or redundancy reduction in Biology came from the work of Laughlin [26] applied to the fly eye. He measured and compared both the contrast distribution in the image and the contrastive cells in the fly eye and predicted that optimal encoding would take the form of maximizing contrast by transforming the original (redundant) distribution into a uniform (uncorrelated) distribution to be transmitted to the fly brain. As each output value becomes equiprobable, the conveyed signal achieves the capacity limit for transmission with optimal bandwidth [11]. As a result, optimal coding makes the signal resemble white noise (Maximum Entropy): a coding effect that is called whitening [27].\nEfficient coding is hypothesized to occur widely in the brain to manipulate natural input, and EM appears to be one important principle of brain dynamics [29-31]. The level of redundancy in each brain region is suggested to relate then to different types of computation and treatment of information; e.g. for memory access, storage, and retrieval [32, 33].\nFor instance, the models of sparse coding and motor synergies, in human sensing and motor control, illustrate as well the efficient coding hypothesis with pattern separation and grouping in the human body [34-37]. Motor synergies correspond then to the dynamical combination of a discrete set of motor groups to realize motion behaviors. They are encoded at the level of the spinal cords in the place of central pattern generators for the rhythmic patterns and of reflex circuits for those driven by sensory feedback. Since their number is limited in comparison to the number of muscles to control, they can be viewed then as sort of \"compressive codes\" to convey information.\nIn this line, the spinal cords system can be seen as a communication bottleneck in terms of motor access for the brain since the ratio between the number of these spinal cord cells and the number of muscles is very low [37]. As a proposal, the efficient coding hypothesis can serve to design a reduced number of motor codes or synergies optimized for control, adapted to one particular body, yielding to minimal energetic cost of transport and best stability.\nRandomness, redundancy reduction and compressive codes.\nTwenty years ago, an IT revolution occurred, introducing methods that exploited advantageously the EM principle with the use of random matrices to transmit and encode information near error-free, see Fig. 3 a). The random nature of these codes y turned out to be critical to correct signals and messages x. Accordingly, although these codes y can be highly corrupted by noise\nor purposefully limited by low resolution Ry, it turns out that a very small number k of units are sufficient to approximate signals x of very high resolution Rx; so that Ry < Rx. Ry and Rx correspond respectively to Rin and Rout of\nso that Ry < Rx. Although each patch cannot represent precisely the item due to their low resolution, the combination of few are enough to recompose it.\nIn comparison, regression methods such as the descent gradient used in artificial neural networks (perceptrons) or Free-Energy Minimization [41-43] require longer time for convergence by removing iteratively errors to approximate the signal, see Fig. 2 c). At the end of the iterative stage, the synaptic weights W approximate the signal x so that the resolution of the synaptic weights Rw has the same resolution as the signal Rx, or higher such that Rw \u2248 Rx. As a result, these methods require very precise devices to work.\nImportantly, these results demonstrate that information structure plays a computational role for capacity storage, and robustness against noise. For instance, while discrete and non-redundant codes can be stored and retrieved very rapidly, analog and redundant codes require more precision or capacity storage. Counter-intuitively, very precise codes are also more prone to noise because they rely on very precise devices in counter part. In comparison, discrete codes, which remove redundancy in the signal, makes them also more robust against variabilities. They are therefore easier to handle for information preservation and retrieval.\nIn computers and modern communication systems, the digital treatment of information proposed by von Neuman and Claude Shannon using binary codes (low resolution) instantiates fully this efficient encoding principle. Because information is encoded using a limited set of discrete values, only binary values, digital encoding instantiates Entropy Maximization, and satisfies the source coding theorem such that \\(log R_x \\approx k log R_y\\), with smallest number k of codes; see Fig. 2 a). It follows that each bit added to the codeword augments the resolution by a power-law factor as it is known.\nEM in random neural networks. In this line, the EM principle was applied in sensory encoding for maximizing information [7, 8] and in artificial neural networks for efficient encoding within random networks [44]. To do so, the mechanisms presented above of discreteness and randomness were used to remove redundancy in\nthe signal and to create orthogonal neural codes, see Fig. 2 b) and Fig. 3 a).\nResults in [44] demonstrated that the neural network reaches Shannon's capacity limit in terms of information storage per neuron within it, see Fig. 3 b). They showed that random neurons can convey maximal information, so that each neuron added to the neural population increases its storage capacity by an exponential scale, like digital processing. As a result, few neurons y are enough to encode one signal x of high resolution so that we have effectively Ry < Rx.\nBecause this neural network verifies the source coding theorem with \\(log R_x \\approx k log R_y\\), it performs a kind of digital computing, in the sense given by Claude Shannon of Entropy Maximization. We expect that these results and their mechanisms can be transposed to embodied AI and robotics. For instance, random networks can be used to maximize information either for fast sampling in few shots (encoding) or for motor control with few primitives (decoding), see Fig. 3 c-e).\nAt the conceptual level, it suggests that the information structure of codes -i.e., their level of redundancy, discreteness, their resolution- has an impact on their computational properties. Hence, poor sensors and unreliable motors (low entropic systems) may approximate accurate sensors and precise motors (higher entropic systems). Accordingly, roboticists can play with the body structure, their combinatorics, their degree of redundancy, the resolution of their sensors, and motors to develop accurate robots, soft and rigid, that maximize information.\nThus, we suggest that the level of embodiment of one system (a robot or a biological agent) correlates with its capability to convey information [21, 23, 24, 45]. Our approach based on EM is in line with two proposals of embodied AI, namely morphological computation and physical reservoir computing, that we will present in the Discussion section."}, {"title": "3 Some examples", "content": "Information equivalence for different robot bodies. Let's use Shannon equation of source coding \\(log R_x = k log R_y\\) applied to a simple robotic example to describe its use, see Fig. 4. Let's have a very precise motor or sensor, X, such that it can take any values within the interval range Rx; its\nresolution. For the purpose of our demonstration, let's assume that Rx = 1024. Accurate sensing or control with such device is difficult since it requires the same amount of precision for its calibration by another sensor or motor.\nAccording to Shannon equation applied to robotics, one such high resolution device is equivalent in terms of entropy to k low resolution devices Y; k being the number of devices. Let's assume two other robotic designs in which we change the number of actuators, their resolution, and their disposition; resp. Fig. 4 b) and c). For one robotic design similar to a kinematic chain, we select the number of states of the devices Y to have only two states so that Ry = 2; i.e., such system may correspond to agonist and antagonist actuators, or on and off sensors. At a biomechanical level, this design resembles to a tentacle. In comparison to the first device, this second one is simpler to control as each motor possesses two states only, which are easier to discriminate than high precision motor. As a kinematic chain, each motor does not affect the state of another. This system has therefore no redundancy.\nThe third morphology corresponds to a five-link redundant robot with four-state actuators; see Fig. 4 c). If the design is fully rigid, there would be some impossible states. So let's assume\nsome flexible wires, in purple, so that all the states are possible. This morphology ressembles roughly the one of the human hand with its five fingers manipulating the same object.\nAccording to Shannon inequality equation of source coding between the three systems, we will have correspondence when \\(log R_x = k log R_y\\). Thus, one equivalence from an IT viewpoint between them will occur for \\(k = log R_x/log R_y = 10\\) and k\u2248 5, respectively for the second and third architecture; k being the number of necessary low resolution devices Y to approximate the device X. In comparison to the first system with one accurate motor, the two last architectures have very few motor states. However, they possess the same number of states and therefore the same complexity level.\nTherefore, in terms of accuracy and information, which are employed as synonyms here, one series of ten switch motors or a five-link parallel device with just two additional states (four states) worths one very precise motor of 1024 states. This indicates that multi-articulated systems can gain in precision and accuracy even with simple and non precise controllers. Furthermore, the relationship between precision and the number of motors is asymmetric in the equation \\(log R_x = k log R_y\\). Precision increases following an exponential scale whereas the number of controllers k augments following a linear scale.\nInformation hierarchies in motor and sensor grouping.\nThe example above in Fig. 4 presented the effects of motor grouping in terms of information gain depending on their organization (serial or parallel) and on their resolution (small or high precision). The source coding theorem explains how this gain grows exponentially when redundancy is removed even for unreliable devices.\nA second property of this theorem is the notion of information hierarchy coming from the inequality between the left and right parts of the equation \\(log R_x \\leq k log R_y\\). For instance, we can chain at different organization level the capacity limit of units so that we can have \\(log R_{x+2} < k log R_{x+1} \\leq l log R_x < m log R_{x-1}\\).\nThe coordination of individual motors, which means the sets of all possible groups, constitutes a combinatorics of k log Ry. If the entropy of longer motor sets (chunks) have a lower entropy of log Rx, then coordination constitutes a beneficial effect. Similarly, this property has been used by Claude Shannon to describe the benefit of grouping letters into pairs, triplets and then into words in the English language for robustness\nagainst noise, faster encoding and better inference [46]. Because the number of words are lesser than the combinatorics of random letters, they save resources during encoding and energy during retrieval.\nSimilarly in the motor domain, the motor synergies limit the variabilities of individual muscles with fewer coordinated patterns to control. Motor synergies are therefore compressive, and represent an economical gain. Motor synergies for robotics\nmight increase information gain (or an entropy reduction). However, algorithms that effectively use mutual information are still scarce and difficult to implement in robotics for control and sensory fusion.\nMotor precision, and learning. Let's take another example of motor control, but this time, with a motor of high resolution Rx such that its angle 0 is within the interval range Rx = [0, 1], see Fig. 6 a). We assume that the motor can change its level of stiffness. Variable stiffness changes the relative force level in Newton theory, but in terms of information, it induces also a change in precision, see Fig. 6 a) and b).\nFor instance, one very unreliable motor controller can be designed by dividing simply the motor space into two sub-spaces, Ryo = [0,0.5] and RY1 = [0.5, 1], so that we can generate sequences of low precision, with motor codes of large uncertainties, similar to a discrete binary code; e.g., y = [010001100]. This large imprecision is similar to an unreliable controller with loose tension and stiffness; see Fig. 6 b) on the left.\nThe spatio-temporal motor grouping and their quantization permit to stabilize the motor synergies at various hierarchical level. For instance, the discrete motor pairing in Fig. 6 a) constitutes a repertoire of the complete 4 possibilities in magenta [00] [01] [10] [11]. However, as the length of the motor chunks augments into triplets and quadruplets, the combinatorics increase respectively to \\(2^3\\) = 9 and to \\(2^4\\) = 16 possibilities in cyan and in green, but the number of observed motor synergies stabilize to motor repertoires of 5 actions only, which is lower than 9 and 16 and slightly higher than 4. Similar to language and the construction of words from letters, the created motor hierarchies help to reduce variability, to organize\ncoherent movement over time, and to exploit them as patterns to diminish the redundancies.\nTo augment the level of precision, we can divide the subspaces Ryo and Ry, into smaller subspaces; see Fig. 6 b) on the right. Its dimension Ry \u2208 N can be very high, which demands to have very accurate sensors and motors of same precision level. Instead, the compositionality of low resolution codes Y can recreate time series X of much higher resolution so that Ry < Rx. In the motor domain, complex and nonlinear motor signals X can be represented then by a small number k of codes Y. k relates then to the desired level of precision and stiffness. The number k of recruited motor units Y becomes a parameter of the system's controllability. The number k of recruited sensorimotor codes Y controls then the robot's information flow and bottleneck.\nFitts law and motor bandwidth. This bottleneck of the information flow in action and control is well examplified in the Fitts law [48]. For instance, Fitts discovered that actions are inherently limited by a relation between speed, distance and precision so that rapid and distal actions become unprecise; see Fig. 6 c). This law, which represents also an index of difficulty and a cognitive charge [47], follows a logarithmic scale so that actions have necessarily a nominal speed for a certain precision level. The Fitts law expresses therefore that actions have a particular bandwidth to perform accurate motions. Similar with the compositional codes presented in Fig. 6 b) for sensory precision, actions can be seen then as the composition of unreliable codes, and the number of recruited motor units k relates then to their cognitive charge based on the source coding equation. In line with the motor synergies paradigm, few synergies can be necessary to reach a high precision level due to the exponential scale.\nMotor adaptivity and development. During development, infants confront the challenge of resolving informational issues arising from the immaturity and unreliability of their sensors, the weakness and softness of their muscles, and the multitude of degrees of freedom they encounter. Despite these apparent constraints of the human body, infants successfully execute motion behaviors with remarkable efficiency, seemingly acquired effortlessly [4].\nTherefore, adapting one's own actions in terms of precision level (Information Theory) instead of motor stiffness (Newton Theory) can be a better parameter for controllability if the task has some uncertainty and variability. For instance, there is no need to be very accurate in dynamic and noisy environments. In such situation, it is better to adapt one's own accuracy, which means Ry or the motor code length or complexity, to the variance or uncertainty level of the environment Rx, see Fig. 6 d) left.\nSimilarly, adaptation under uncertainty is what infants are experiencing the most during development, see Fig. 6 d) right. Therefore, the EM paradigm complies partly with others in cognitive and developmental sciences such as information-seeking, artificial curiosity, active sampling [49, 50], active inference [51-53], and empowerment [54], proposing that infants are constantly and actively seeking novel information and maximizing their actions and their sampling to perceive, adapt and learn faster during development.\nIn contrast from them, however, the EM paradigm departs by predicting that incremental growth during development has to be exponential and not linear. For instance, control dexterity develops rapidly in early life, with competence exponentially growing to encompass skillful abilities like reaching, grasping [55, 56], postural balance [57?, 58], and abstraction for solving embodied problems [59?, 60], and language acquisition [61].\nHence, from an IT viewpoint, we propose that the infant developmental phase characterized by the infant's apparent random movements, what is called motor babbling [4, 56], serve to explore at an incredible rate what the body is capable to do, toward maximizing the efficient information exchange between brain and environment; i.e., acquisition of body image [62-64]. In this view, the infant's body becomes then the channel to interface internal and external dynamics for efficient communication exchange.\nThus, the 'interfacing' between internal dynamics and external dynamics, informational embodiment, may represent then the first learning phase to explore the 'richness' of its potentialities, which means acquisition of its complete combinatorics. By doing so, informational embodiment"}, {"title": "4 Discussion", "content": "Information maximization. In human communication systems, maximizing information corresponds to the design of specific codes efficient for particular channels, with respect to their physical limits in terms of information capacity, redundancy and noise. Based on that, the most robust codes are also those that reach the Shannon limit in terms of information transfer.\nAs an original idea, we consider the body as a communication channel so that the morphology (e.g., the redundancy of the limbs), the motors and the sensors, their placement, their number, and their resolution have an impact on the way information is retrieved, and quantified. Hence, the body has an incidence on information structure and therefore on the algorithms used for perception and control.\nThis idea is in line with the concept of morphological computation in the strict sense [69]: the body is processing information so that it is possible to provide efficient codes for sampling, and control and designing its body purposefully for it.\nInterestingly, the digital treatment of information provided by Claude Shannon shows that when entropy is maximized, the information capacity in algorithms, communication channels and storage devices grows exponentially; e.g., the resolution of binary codes grows exponentially for each bit added.\nFollowing this, our definition of informational embodiment is the maximization of the bodily resources to its full exploitation of information capacity for sensing, learning and control with respect to its own physical characteristics and limitations.\nThis new formalism can be useful for the design of robots, and the understanding of human embodiment. For instance, efficient coding mechanisms should operate at all robotic levels to withstand discrepancies, encompassing not only those related to control but also pertaining to material, morphology, sensors, and their interaction with the environment.\nInformational embodiment. IT utilizes entropy to gauge the level of information transmitted in a communication channel, irrespective of its specific properties. Building upon this concept, we suggest that entropy can be employed to quantify the information exchange within any type of robot's body, regardless of its characteristics.\nWhile this proposal may seem straightforward, it introduces a fresh perspective on certain concepts and theories from IT that are not immediately apparent but can be reinterpreted for robotics:\n1. in his theory of communication, Claude Shannon formulated the source coding theorem to transmit a message along a transmission channel [1]. Importantly, this theorem emphasizes that the message to be transmitted and the transmission channel are interconnected unequally through their respective quantities of entropy, such as the resolution of the message and the channel capacity. This inequality describes the interdependence of these two variables the message and the messenger as part of the same equation. Translated to robotics, this implies that the robot's body and the algorithmic part must be considered in tandem.\n2. the sole quantity exchanged between the two is entropy, the currency of information. To be more precise, it is the logarithm of entropy that is exchanged. This implies that the information exchanged and the relationships among interacting systems follow a power-law scale. For instance, with just a few interacting elements, combinatorics can rapidly grow exponentially. As explored further, this result has been extensively utilized in human-made modern communication systems to approach the theoretical limit set by Shannon (digital processing), whereas it is often overlooked or concealed in robotics. Notably, power laws are pervasive in nature for manipulating large numbers [70]. We propose that these laws of large numbers might be decisive for a robot's embodiment and efficiency, particularly in integrating and controlling numerous interacting systems-an aspect that remains challenging for current roboticists.\n3. Shannon's equation elucidates that entropic elements interact multiplicatively and can approximate high-entropic objects very rapidly but also collapse just as swiftly. This is derived from Boltzmann's equation in Thermodynamics. In sensing, control, and learning, weak and unreliable elements (e.g., sensors, actuators, neurons) are all limited in resolution. Nevertheless, when considered as a group, they can approximate more complex objects and signals, achieving higher resolution. Efficiency, in this context, is attained by enhancing entropy, signifying an information gain, while redundancy corresponds to an information loss. For robotics, this implies (1) that the interaction and coordination of (weak) elements can become combinatorial, enhancing efficiency if properly arranged, and (2) that the organization of (weak) elements among each other is a crucial design principle for rapid sensing, dexterous control, and learning/memory organization.\n4. at the conceptual level, transcribing this theorem to robotics allows us to perceive the robot's body as the communication channel for the information sent and received. Therefore, this equation considers the characteristics of the robot's body as an intrinsic part of it-where structure and function are intertwined. The robot's embodiment, encompassing its morphology, the arrangement of its sensors and motors, and their resolution, can be quantitatively defined to address the challenges of sensing and control as problems of transmission or communication. This equation also expresses that the bodily capacity of a robot/agent is necessarily linked with the task it has to perform, and once again, the only manipulated quantity is entropy.\nFollowing this, we may correlate the level of embodiment of a system (a robot or a biological agent) with its degree of information flow [21, 23, 24, 45]. As one counter-intuitive consequence of it, informational embodiment predicts the long-term superiority of soft robots and reservoir computing in comparison to current robotics and AI architectures if properly designed for information processing. This is because they correspond to higher entropic systems with larger combinatorics.\nMorphological Computation. Morphological computation has been defined as \"computation obtained through the interaction of physical forms\" [69]. Taken literaly, morphological computation makes a direct link between body, information, codes and computation [71, 72]. The concept has been further developped to include reservoir computing, chaotic behavior and soft robotics [13, 17, 18, 73-77]."}]}