{"title": "Informational Embodiment: Computational role of\ninformation structure in codes and robots", "authors": ["Alexandre Pitti", "Kohei Nakajima", "Yasuo Kuniyoshi"], "abstract": "The body morphology plays an important role in the way information is perceived and processed by\nan agent. We address an information theory (IT) account on how the precision of sensors, the accu-\nracy of motors, their placement, the body geometry, shape the information structure in robots and\ncomputational codes. As an original idea, we envision the robot's body as a physical communication\nchannel through which information is conveyed, in and out, despite intrinsic noise and material limita-\ntions. Following this, entropy, a measure of information and uncertainty, can be used to maximize the\nefficiency of robot design and of algorithmic codes per se. This is known as the principle of Entropy\nMaximization (PEM) introduced in biology by Barlow in 1969. The Shannon's source coding theorem\nprovides then a framework to compare different types of bodies in terms of sensorimotor information.\nIn line with PME, we introduce a special class of efficient codes used in IT that reached the Shannon\nlimits in terms of information capacity for error correction and robustness against noise, and parsi-\nmony. These efficient codes, which exploit insightfully quantization and randomness, permit to deal\nwith uncertainty, redundancy and compacity. These features can be used for perception and control in\nintelligent systems. In various examples and closing discussions, we reflect on the broader implications\nof our framework that we called Informational Embodiment to motor theory and bio-inspired robotics,\ntouching upon concepts like motor synergies, reservoir computing, and morphological computation.\nThese insights can contribute to a deeper understanding of how information theory intersects with\nthe embodiment of intelligence in both natural and artificial systems.", "sections": [{"title": "1 Introduction", "content": "Before Claude Shannon's work, engineers believed\nthat reducing communication errors necessi-\ntated increased transmission power or repetitive\nmessage transmission. Shannon fundamentally\ndemonstrated that it wasn't essential to expend\nexcessive energy, effort and time if appropriate\ncoding schemes were employed [1, 2], even in the\nface of an unreliable communication channel. We\nthink that similar principles can be applied in\nrobotics to enhance efficiency in perception, con-\ntrol, and body design. Information theory (IT) can\nprovide insights to robotics in terms of efficient\ncodes, information structure and communication\nbetween the body (hardware) and the controller\n(software).\nCurrent robots achieve high accuracy in\nmotion control based on highly precise sensors and\nmotors. For instance, current electronics permit\nto control the motion behavior of robots at the\nspeed of kHz with high resolution sensors to detect"}, {"title": "2 Efficient Codes, the Body\nas an Information Channel.", "content": "Our hypothesis draws inspiration from the\nEmbodied Intelligence (EI) paradigm developed\nby [12, 14-18] and others that explicitly delve\ninto information theory [19-22]. While EI has pro-\nvided essential principles, it is still in search of a\ncomprehensive theory [23-25].\nWe propose that efficient coding, -in the\nsense given by Shannon of entropy maximization,-\ncan explicitly describe the design of sensors, con-\ntrollers or bodies that maximizes information\nprocessing.\nEfficient Coding in Biology. Efficiency\ncoding principles have been proposed in Biology\nin which original signals, redundant, are hypoth-\nesized to be transformed into neural codes of\nuniform distribution [26, 27]. By doing so, codes\nare more compact and can transmit the same\namount of information with far less channel capac-\ntity [27, 28]; hence maximizing information.\nFor example, the human optical nerve, which\nis composed of 1 million ganglions, has to transmit\nthe information coming from the retina, which is\ncomposed of 120 millions retinal photo-receptors.\nFollowing the hypothesis of efficient encoding\nthat neurons should encode information as effi-\nciently as possible in order to maximize neural\nresources, it has been shown that visual data\ncan be compressed up to 20 fold without notice-\nable information loss. The first experiences in\ntesting the theory of efficiency coding or redun-\ndancy reduction in Biology came from the work of\nLaughlin [26] applied to the fly eye. He measured\nand compared both the contrast distribution in\nthe image and the contrastive cells in the fly eye\nand predicted that optimal encoding would take\nthe form of maximizing contrast by transforming\nthe original (redundant) distribution into a uni-\nform (uncorrelated) distribution to be transmitted\nto the fly brain. As each output value becomes\nequiprobable, the conveyed signal achieves the\ncapacity limit for transmission with optimal band-\nwidth [11]. As a result, optimal coding makes the\nsignal resemble white noise (Maximum Entropy):\na coding effect that is called whitening [27].\nEfficient coding is hypothesized to occur\nwidely in the brain to manipulate natural input,\nand EM appears to be one important principle of\nbrain dynamics [29-31]. The level of redundancy\nin each brain region is suggested to relate then to\ndifferent types of computation and treatment of\ninformation; e.g. for memory access, storage, and\nretrieval [32, 33].\nFor instance, the models of sparse coding and\nmotor synergies, in human sensing and motor con-\ntrol, illustrate as well the efficient coding hypoth-\nesis with pattern separation and grouping in the\nhuman body [34-37]. Motor synergies correspond\nthen to the dynamical combination of a discrete\nset of motor groups to realize motion behaviors.\nThey are encoded at the level of the spinal cords\nin the place of central pattern generators for the\nrhythmic patterns and of reflex circuits for those\ndriven by sensory feedback. Since their number\nis limited in comparison to the number of mus-\ncles to control, they can be viewed then as sort of\n\"compressive codes\" to convey information.\nIn this line, the spinal cords system can be seen\nas a communication bottleneck in terms of motor\naccess for the brain since the ratio between the\nnumber of these spinal cord cells and the num-\nber of muscles is very low [37]. As a proposal, the\nefficient coding hypothesis can serve to design a\nreduced number of motor codes or synergies opti-\nmized for control, adapted to one particular body,\nyielding to minimal energetic cost of transport and\nbest stability.\nRandomness, redundancy reduction and\ncompressive codes.\nTwenty years ago, an IT revolution occurred,\nintroducing methods that exploited advanta-\ngeously the EM principle with the use of random\nmatrices to transmit and encode information near\nerror-free, see Fig. 3 a). The random nature of\nthese codes y turned out to be critical to cor-\nrect signals and messages x. Accordingly, although\nthese codes y can be highly corrupted by noise"}, {"title": "3 Some examples", "content": "Information equivalence for different robot\nbodies. Let's use Shannon equation of source cod-\ning $log R_x = k log R_y$ applied to a simple robotic\nexample to describe its use, see Fig. 4. Let's have\na very precise motor or sensor, X, such that it can\ntake any values within the interval range $R_x$; its"}, {"title": "4 Discussion", "content": "Information maximization. In human commu-\nnication systems, maximizing information corre-\nsponds to the design of specific codes efficient for\nparticular channels, with respect to their physical\nlimits in terms of information capacity, redun-\ndancy and noise. Based on that, the most robust\ncodes are also those that reach the Shannon limit\nin terms of information transfer.\nAs an original idea, we consider the body as\na communication channel so that the morphology\n(e.g., the redundancy of the limbs), the motors\nand the sensors, their placement, their number,\nand their resolution have an impact on the way\ninformation is retrieved, and quantified. Hence,\nthe body has an incidence on information struc-\nture and therefore on the algorithms used for\nperception and control.\nThis idea is in line with the concept of mor-\nphological computation in the strict sense [69]: the\nbody is processing information so that it is pos-\nsible to provide efficient codes for sampling, and\ncontrol and designing its body purposefully for it.\nInterestingly, the digital treatment of informa-\ntion provided by Claude Shannon shows that when\nentropy is maximized, the information capacity in\nalgorithms, communication channels and storage\ndevices grows exponentially; e.g., the resolution\nof binary codes grows exponentially for each bit\nadded.\nFollowing this, our definition of informational\nembodiment is the maximization of the bodily\nresources to its full exploitation of information\ncapacity for sensing, learning and control with\nrespect to its own physical characteristics and\nlimitations.\nThis new formalism can be useful for the\ndesign of robots, and the understanding of human\nembodiment. For instance, efficient coding mecha-\nnisms should operate at all robotic levels to with-\nstand discrepancies, encompassing not only those\nrelated to control but also pertaining to material,\nmorphology, sensors, and their interaction with\nthe environment."}]}